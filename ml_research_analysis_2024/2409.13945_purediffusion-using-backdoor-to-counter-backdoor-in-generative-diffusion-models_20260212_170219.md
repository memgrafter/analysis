---
ver: rpa2
title: 'PureDiffusion: Using Backdoor to Counter Backdoor in Generative Diffusion
  Models'
arxiv_id: '2409.13945'
source_url: https://arxiv.org/abs/2409.13945
tags:
- trigger
- backdoor
- triggers
- shift
- purediffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PureDiffusion is a backdoor defense framework for generative diffusion
  models that focuses on trigger inversion. The core idea is to estimate the scale
  of trigger-related distribution shifts in each denoising step using theoretical
  and empirical analysis, then use gradient descent to learn high-quality backdoor
  triggers over multiple timesteps.
---

# PureDiffusion: Using Backdoor to Counter Backdoor in Generative Diffusion Models

## Quick Facts
- arXiv ID: 2409.13945
- Source URL: https://arxiv.org/abs/2409.13945
- Authors: Vu Tuan Truong; Long Bao Le
- Reference count: 11
- One-line primary result: PureDiffusion inverts backdoor triggers in generative diffusion models with 500x lower uniform scores than baselines in some cases

## Executive Summary
PureDiffusion is a novel backdoor defense framework designed to detect and mitigate backdoor attacks in generative diffusion models by focusing on trigger inversion. The method estimates the scale of trigger-related distribution shifts across all denoising steps and uses gradient descent to learn high-quality backdoor triggers. Experimental results demonstrate that PureDiffusion significantly outperforms existing methods in detecting backdoors while also revealing that inverted triggers can sometimes achieve higher attack success rates than the original triggers.

## Method Summary
PureDiffusion operates by first computing the scale of trigger-related distribution shifts (λt) for all denoising steps using a surrogate trigger, leveraging the invariance of these scales to trigger shape and size. It then employs multi-timestep gradient descent to learn the backdoor trigger by minimizing the difference between predicted noises and the estimated trigger shift across randomly sampled timesteps. The method is evaluated on CIFAR-10 using DDPM with 1000 steps and BadDiffusion backdoor attacks, with performance measured via uniform score, attack success rate, and L2 distance to ground-truth triggers.

## Key Results
- PureDiffusion achieves 500x lower uniform scores compared to baseline methods in some cases
- Inverted triggers match or exceed ground-truth triggers in attack success rate
- L2 distance between inverted and ground-truth triggers is significantly lower than baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Trigger-related distribution shift scales are invariant to trigger shape and size.
- Mechanism: The backdoor forward process injects a trigger-dependent shift that propagates through all denoising steps. Because the shift is linear in the trigger and independent of its specific form, the scale λt can be computed using any surrogate trigger.
- Core assumption: The trigger shift in the reverse process is proportional to the forward shift and depends only on the noise schedule, not the trigger identity.
- Evidence anchors:
  - [abstract] "We propose the first solution to estimate the scale of trigger-related distribution shift for all denoising steps"
  - [section III.B.1] "As λt = (1−√αt)√1−¯αt/1−αt only depends on the fixed noise schedule αt without relying on the trigger δ, the Proposition is proved."
  - [corpus] No direct support found; requires further validation.
- Break condition: If the noise schedule changes or the backdoor method alters the shift structure, λt may no longer be invariant.

### Mechanism 2
- Claim: Multi-timestep gradient descent improves trigger inversion fidelity.
- Mechanism: By sampling timesteps randomly and backpropagating through multiple denoising steps, the learned trigger must satisfy consistency constraints across the diffusion trajectory, reducing noise-induced drift.
- Core assumption: The neural network's predicted noise at each step contains both trigger and noise shifts, and minimizing their difference drives the trigger estimate toward the true trigger.
- Evidence anchors:
  - [section III.C] "As a result, we can use gradient descent to learn δ, with t is sampled randomly from 0 to T"
  - [section IV.B.1] "PureDiffusion achieves lower uniform score in all experimented cases" and "ASR of PureDiffusion's triggers is comparable with the ground-truth triggers"
  - [corpus] No direct support found; requires further validation.
- Break condition: If the timestep sampling is too sparse or the noise schedule is unstable, the multi-step constraint may not converge.

### Mechanism 3
- Claim: PureDiffusion can unintentionally strengthen backdoor attacks.
- Mechanism: The learned trigger, optimized to minimize detection metrics, may align more closely with the model's internal representation of the backdoor target, producing higher attack success rates than the original trigger.
- Core assumption: The optimization objective favors triggers that maximize backdoor behavior, even if they are harder to detect by human inspection.
- Evidence anchors:
  - [abstract] "Notably, in certain cases, backdoor triggers inverted by PureDiffusion even achieve higher attack success rate than the original triggers"
  - [section IV.C] "PureDiffusion's inverted triggers even outperform the ground-truth triggers" and "the ASR of triggers inverted by our method is comparable or even higher than the ground-truth triggers"
  - [corpus] No direct support found; requires further validation.
- Break condition: If detection thresholds or metrics are adjusted, the reinforcement effect may disappear.

## Foundational Learning

- Concept: Diffusion model denoising process and noise schedule
  - Why needed here: Understanding how the forward and reverse transitions operate is essential for estimating trigger shifts and designing the inversion loss.
  - Quick check question: In DDPM, what does βt represent, and how does it influence the noise schedule αt?

- Concept: Backdoor attack mechanics in generative models
  - Why needed here: The backdoor insertion modifies the forward process; knowing how m(t) and n(t) are chosen is critical for computing the trigger shift scales.
  - Quick check question: What is the difference between BadDiffusion and TrojDiff in terms of m(t) and n(t)?

- Concept: Distribution shift estimation and optimization
  - Why needed here: The core of PureDiffusion is estimating how much the backdoor trigger shifts the image distribution at each step and then using gradient descent to learn the trigger.
  - Quick check question: Why does PureDiffusion use a surrogate trigger to estimate λt, and what property makes this possible?

## Architecture Onboarding

- Component map: Surrogate trigger → Shift estimator → Trigger learner → Inverted trigger + detection decision
- Critical path:
  1. Choose surrogate trigger ˆδ
  2. Run backdoor forward process with ˆδ to collect predicted noises
  3. Compute λt = (ϵθ(t) · ˆδ)/||ˆδ||²
  4. Initialize learnable δ
  5. For each epoch, sample t, compute loss = E∥ϵθ(t) - λtδ∥², backpropagate
  6. Return δ and evaluate via uniform score/ASR
- Design tradeoffs:
  - Using more timesteps increases fidelity but requires more compute
  - Choosing a bad surrogate trigger may introduce estimation noise but should not break the method
  - Small λt values can lead to vanishing gradients; learning rate tuning is critical
- Failure signatures:
  - Inverted trigger has high L2 distance to ground truth
  - Uniform score does not drop significantly below baseline
  - ASR is near random chance
  - Training loss plateaus early
- First 3 experiments:
  1. Baseline: Run Elijah trigger inversion on a BadDiffusion-backdoored DM and record metrics
  2. Core: Run PureDiffusion trigger inversion on the same DM and compare L2 distance and ASR
  3. Multi-step ablation: Vary the number of timesteps used for backpropagation (1, 5, 10) and measure impact on trigger fidelity and ASR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can PureDiffusion's multi-timestep trigger inversion method be extended to other types of backdoor attacks beyond BadDiffusion, TrojDiff, and VillanDiffusion?
- Basis in paper: [explicit] The paper mentions that similar analyses can be used for other backdoor methods like TrojDiff and VillanDiffusion, but primarily focuses on BadDiffusion. It also suggests that the method can be adopted for other backdoor methods.
- Why unresolved: The paper only provides detailed analysis and experiments for BadDiffusion, and only mentions that the method can be applied to other backdoor methods without providing specific details or results.
- What evidence would resolve it: Conducting experiments using PureDiffusion on a wider range of backdoor attack methods and comparing the results to the original method's performance on BadDiffusion.

### Open Question 2
- Question: What is the optimal number of timesteps for backpropagation in PureDiffusion's trigger inversion method to achieve the best balance between trigger fidelity and computational efficiency?
- Basis in paper: [inferred] The paper mentions that they use the first 10 steps of the denoising process due to computational resource limitations, but suggests that using more than 10 timesteps could potentially increase trigger fidelity.
- Why unresolved: The paper does not explore the impact of using different numbers of timesteps on the performance of trigger inversion, leaving the optimal number of timesteps unknown.
- What evidence would resolve it: Conducting experiments with varying numbers of timesteps (e.g., 5, 10, 20, 50) and analyzing the trade-off between trigger fidelity and computational efficiency for each setting.

### Open Question 3
- Question: How does PureDiffusion's trigger inversion method perform against more sophisticated backdoor triggers that are designed to be resistant to detection and inversion?
- Basis in paper: [inferred] The paper demonstrates PureDiffusion's effectiveness on various trigger-target pairs, but does not specifically address the performance against more advanced or stealthy backdoor triggers.
- Why unresolved: The paper does not explore the robustness of PureDiffusion against more sophisticated backdoor attacks, leaving the method's effectiveness in such scenarios unknown.
- What evidence would resolve it: Designing and implementing more advanced backdoor triggers that incorporate techniques to resist detection and inversion, then evaluating PureDiffusion's performance against these triggers.

## Limitations

- The method's effectiveness relies on the assumption that trigger-related distribution shifts are invariant to trigger shape and size, which is not empirically validated across diverse trigger types
- The potential for PureDiffusion to inadvertently strengthen backdoor attacks raises security concerns that require further investigation
- The optimal number of timesteps for backpropagation is not explored, leaving a gap in understanding the trade-off between trigger fidelity and computational efficiency

## Confidence

- **High confidence**: The mathematical framework for estimating trigger shift scales (λt) is well-defined and theoretically sound, supported by the derivation in Section III.B.1
- **Medium confidence**: The experimental results demonstrating PureDiffusion's superiority over baseline methods (Elijah) are promising, but the lack of ablation studies on surrogate trigger selection limits generalizability
- **Low confidence**: The claim that PureDiffusion can outperform ground-truth triggers in attack success rate is surprising and may indicate overfitting or unintended side effects, requiring more rigorous validation

## Next Checks

1. **Surrogate Trigger Sensitivity**: Test PureDiffusion with multiple surrogate triggers (e.g., random noise, different shapes) to assess the robustness of λt estimation and trigger inversion quality
2. **Adversarial Trigger Design**: Evaluate whether an attacker can design triggers that exploit PureDiffusion's optimization process to create stronger backdoors, testing the security of the defense
3. **Cross-Dataset Generalization**: Apply PureDiffusion to backdoor attacks on datasets beyond CIFAR-10 (e.g., ImageNet) to verify its scalability and effectiveness in diverse settings