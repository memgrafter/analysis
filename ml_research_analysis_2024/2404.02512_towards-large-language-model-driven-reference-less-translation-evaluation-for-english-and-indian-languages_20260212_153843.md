---
ver: rpa2
title: Towards Large Language Model driven Reference-less Translation Evaluation for
  English and Indian Languages
arxiv_id: '2404.02512'
source_url: https://arxiv.org/abs/2404.02512
tags:
- translation
- language
- evaluation
- reference-less
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using large language models (LLMs) for reference-less
  translation evaluation in English and Indian languages. The authors conduct experiments
  with zero-shot, in-context learning, and fine-tuning approaches, comparing LLM-based
  evaluators (LLaMA-2-13B) to existing methods like COMET, BERT-Scorer, and LABSE.
---

# Towards Large Language Model driven Reference-less Translation Evaluation for English and Indian Languages

## Quick Facts
- **arXiv ID**: 2404.02512
- **Source URL**: https://arxiv.org/abs/2404.02512
- **Reference count**: 20
- **Primary result**: Fine-tuned LLMs achieve comparable or higher correlation with human judgments for reference-less translation evaluation in English-Indian language pairs.

## Executive Summary
This paper explores using large language models (LLMs) for reference-less translation evaluation in English and Indian languages. The authors conduct experiments with zero-shot, in-context learning, and fine-tuning approaches, comparing LLM-based evaluators (LLaMA-2-13B) to existing methods like COMET, BERT-Scorer, and LABSE. Results show that fine-tuned LLMs achieve comparable or higher correlation with human judgments for Indian language pairs. The study highlights the potential of fine-tuned LLMs for reference-less translation evaluation tasks.

## Method Summary
The paper investigates LLM-based reference-less translation evaluation using three approaches: zero-shot learning, in-context learning, and fine-tuning. Models tested include LLaMA-2-7B, LLaMA-2-13B, and Mistral-7B. The authors use LoRa-based fine-tuning and compare against baselines like COMET, BERT-Scorer, and LABSE. They employ the WMT-23 MT-QE corpora and BPCC-Human dataset for English to Hindi, Gujarati, Marathi, Tamil, and Telugu. The evaluation uses Spearman's Rank Correlation, Pearson Correlation, and Kendall's Rank Correlation between LLM scores and human judgments.

## Key Results
- Fine-tuned LLMs achieve comparable or higher correlation with human judgments compared to baseline methods for Indian language pairs
- LoRa-based fine-tuning methods for LLaMA-2-7B and LLaMA-2-13B models achieve the highest correlations
- Zero-shot and in-context learning approaches show poor performance for translation evaluation tasks
- Multi-task learning (translation + evaluation) does not improve translation evaluation performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning LLMs improves reference-less translation evaluation performance by aligning model output distributions with human judgment distributions.
- Mechanism: Fine-tuning adapts the LLM's internal representations and output probabilities to better match the patterns found in human evaluation scores. The min-max scaling on z-scores in the training data normalizes the human judgment distribution, allowing the model to learn the mapping from translation quality to a 1-100 score scale.
- Core assumption: Human evaluation scores follow consistent patterns across different language pairs and can be learned by the model.
- Evidence anchors:
  - [abstract] "fine-tuned LLMs achieve comparable or higher correlation with human judgments"
  - [section] "We utilized the mean of z-scores, followed by language-specific min-max-based re-scaling from 1 to 100 for the entire corpora"
  - [corpus] Weak - The corpus analysis shows related work but doesn't directly address the scaling approach
- Break condition: If human evaluation patterns are inconsistent across languages or annotators, or if the scaling introduces artifacts that the model learns incorrectly.

### Mechanism 2
- Claim: LoRa fine-tuning provides better evaluation performance than full fine-tuning due to parameter-efficient adaptation that preserves general language understanding capabilities.
- Mechanism: LoRa adds low-rank adapter layers that modify the model's behavior for the specific task while keeping the base model's weights frozen. This preserves the general language understanding capabilities while adapting to the evaluation task, avoiding catastrophic forgetting of general knowledge.
- Core assumption: The base LLM already has sufficient general language understanding capabilities, and task-specific adaptation is more effective than complete retraining.
- Evidence anchors:
  - [abstract] "LoRa-based fine-tuning methods for LLaMa-2-7b and LLaMa-2-13b models achieve the highest correlations"
  - [section] "We opted for LLaMA-2-7b, LLaMA-2-13b, and Mistral-7B for the fine-tuning experiment"
  - [corpus] Weak - No direct evidence about LoRa vs full fine-tuning tradeoffs in the corpus
- Break condition: If the base model lacks sufficient general understanding for the target languages, or if the task requires fundamental changes to the model's architecture.

### Mechanism 3
- Claim: Multi-task fine-tuning (translation + evaluation) does not improve performance because the tasks have conflicting optimization objectives.
- Mechanism: The translation task optimizes for generating accurate target language text, while the evaluation task optimizes for scoring existing translations. These different objectives create interference during training, where the model cannot effectively optimize for both simultaneously.
- Core assumption: Translation and evaluation are fundamentally different tasks that don't benefit from shared optimization.
- Evidence anchors:
  - [abstract] "we did not observe any additional benefits when we perform translation evaluation fine-tuning with translation fine-tuning under the multi-task setting"
  - [section] "we conducted another experiment under multi-task settings, where we fine-tuned both LLaMA-2-7b and LLaMA-2-13b for reference-less translation evaluation and translation tasks"
  - [corpus] Weak - No direct evidence about multi-task interference in the corpus
- Break condition: If the tasks have more complementary objectives than initially assumed, or if a different multi-task architecture could resolve the interference.

## Foundational Learning

- Concept: Statistical language modeling fundamentals (predicting next token probabilities)
  - Why needed here: LLMs are fundamentally autoregressive models that predict the next token in a sequence. Understanding this helps explain why they can be adapted for evaluation tasks through fine-tuning.
  - Quick check question: How does an autoregressive language model generate text, and why is this relevant to its ability to score translations?

- Concept: Cross-lingual representation learning and multilingual embeddings
  - Why needed here: The paper involves evaluation across English and multiple Indian languages. Understanding how models learn cross-lingual representations is crucial for why they can handle this multilingual setting.
  - Quick check question: What mechanisms allow language models to understand and evaluate translations across different languages?

- Concept: Parameter-efficient fine-tuning techniques (LoRa, adapters)
  - Why needed here: The paper uses LoRa for fine-tuning, which is a specific approach to adapting large models efficiently. Understanding this helps explain the design choices and their implications.
  - Quick check question: How does LoRa differ from full fine-tuning, and what are the trade-offs between these approaches?

## Architecture Onboarding

- Component map:
  Base LLM (LLaMA-2-7B/13B, Mistral-7B) -> LoRa adapters -> Prompt generator -> LLM (with adapters) -> Raw output -> Reply parser -> Score -> Human judgment -> Correlation calculation -> Performance evaluation

- Critical path:
  1. Input translation pair → Prompt generator
  2. Generated prompt → LLM (with LoRa adapters)
  3. Raw output → Reply parser (extracts score)
  4. Score + human judgment → Correlation calculation
  5. Metrics → Performance evaluation

- Design tradeoffs:
  - LoRa vs full fine-tuning: Parameter efficiency vs potential performance ceiling
  - Single-task vs multi-task: Task specialization vs generalization
  - Zero-shot vs fine-tuned: Flexibility vs performance
  - Temperature=0 vs sampling: Determinism vs natural variation

- Failure signatures:
  - Consistently high scores regardless of input quality → Model not learning evaluation task
  - Very low correlation with human judgments → Scaling or training issues
  - High variance in scores → Temperature or sampling issues
  - Memory errors during fine-tuning → Hardware constraints or inefficient implementation

- First 3 experiments:
  1. Run inference on a small validation set with the fine-tuned model to verify score extraction works correctly
  2. Calculate correlation on a subset of data to confirm fine-tuning was successful
  3. Compare zero-shot vs fine-tuned performance on the same small dataset to quantify improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of fine-tuned LLMs for reference-less translation evaluation compare to human evaluators across different Indian languages?
- Basis in paper: [explicit] The paper mentions that fine-tuned LLMs achieve comparable or higher correlation with human judgments for Indian language pairs, but it doesn't directly compare their performance to human evaluators.
- Why unresolved: The paper focuses on comparing fine-tuned LLMs to existing automatic evaluation methods like COMET, BERT-Scorer, and LABSE, but doesn't provide a direct comparison to human evaluation.
- What evidence would resolve it: Conducting experiments where both fine-tuned LLMs and human evaluators assess the same set of translations, then comparing their scores and correlations with ground truth human judgments.

### Open Question 2
- Question: What is the impact of translation fine-tuning on the translation evaluation capabilities of LLMs?
- Basis in paper: [explicit] The paper explores fine-tuning LLMs on translation corpora alongside translation evaluation corpora to determine if translation-based fine-tuning improves overall translation evaluation performance.
- Why unresolved: The paper states that multi-task learning-based fine-tuning does not lead to performance improvement compared to single-task fine-tuning for translation evaluation, but it doesn't provide a detailed analysis of the impact of translation fine-tuning.
- What evidence would resolve it: Conducting experiments where LLMs are fine-tuned solely on translation corpora and then evaluated on translation evaluation tasks, comparing their performance to LLMs fine-tuned only on translation evaluation corpora.

### Open Question 3
- Question: How does the performance of LLMs for reference-less translation evaluation vary across different Indian language families?
- Basis in paper: [inferred] The paper mentions that the dataset used for evaluation includes languages from two major language families: Indo-Aryan and Dravidian. However, it doesn't analyze the performance of LLMs across these families.
- Why unresolved: The paper presents overall results for the considered Indian language pairs, but doesn't provide a detailed breakdown of performance across different language families.
- What evidence would resolve it: Conducting experiments where LLMs are evaluated on translation tasks involving languages from different families (e.g., Indo-Aryan vs. Dravidian), then comparing their performance across these families.

## Limitations
- Results primarily focus on translation from English to Indian languages, with limited bidirectional testing
- Reliance on specific datasets (WMT-23 MT-QE and BPCC-Human) may limit generalizability to other domains
- Comparison with COMET-QE lacks detailed training information, making state-of-the-art assessment difficult

## Confidence
**High Confidence**: Fine-tuned LLMs achieve comparable or higher correlation with human judgments than baseline methods for Indian language translation evaluation.

**Medium Confidence**: Zero-shot and in-context learning perform poorly compared to fine-tuning for these tasks.

**Low Confidence**: Multi-task learning provides no benefit for translation evaluation.

## Next Checks
1. **Bidirectional Evaluation**: Test the fine-tuned models on translation from Indian languages to English across all language pairs to verify the models' performance isn't directionally biased and to assess generalizability to non-English source languages.

2. **Cross-Domain Robustness**: Evaluate the fine-tuned models on translation data from different domains (news, conversational, technical) not present in the training data to assess whether the models overfit to the specific translation quality patterns in the WMT-23 and BPCC-Human datasets.

3. **Alternative Multi-Task Architectures**: Implement and test alternative multi-task learning approaches, such as alternating training batches, gradient-based task balancing, or separate adapter modules for each task, to determine whether the negative multi-task result is due to the specific implementation rather than a fundamental limitation of the approach.