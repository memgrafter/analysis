---
ver: rpa2
title: Open Set Recognition for Random Forest
arxiv_id: '2408.02684'
source_url: https://arxiv.org/abs/2408.02684
tags:
- classi
- classes
- data
- recognition
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses open-set recognition (OSR) for random forest
  classifiers, which involves identifying samples from unknown classes during testing.
  The proposed method, RF-KOSNN, incorporates distance metric learning and distance-based
  OSR using Random Forest-Geometry- and Accuracy-Preserving (RF-GAP) proximities and
  Gaussian processes.
---

# Open Set Recognition for Random Forest

## Quick Facts
- arXiv ID: 2408.02684
- Source URL: https://arxiv.org/abs/2408.02684
- Authors: Guanchao Feng, Dhruv Desai, Stefano Pasquali, Dhagash Mehta
- Reference count: 40
- Key outcome: Proposed RF-KOSNN method incorporates distance metric learning using RF-GAP proximities and Gaussian processes for open-set recognition with random forests, achieving improved performance over state-of-the-art distance-based OSR methods.

## Executive Summary
This paper addresses the challenge of open-set recognition (OSR) for random forest classifiers, where test samples may come from unknown classes not seen during training. The authors propose RF-KOSNN, a method that leverages Random Forest-Geometry- and Accuracy-Preserving (RF-GAP) proximities and Gaussian processes to learn a task-specific Mahalanobis distance metric. This metric is then used within a KOSNN framework with a global Extreme Value Theory (EVT) model for identifying unknown classes. The approach is validated on both synthetic and real-world datasets, showing improved overall accuracy and better management of the trade-off between empirical risk and open space risk compared to existing distance-based OSR methods.

## Method Summary
The RF-KOSNN method combines closed-set random forest classification with open-set recognition capabilities. It first trains a random forest classifier and computes RF-GAP proximities, which measure how often pairs of samples end up in the same terminal node of out-of-bag trees. These proximities are then used with Gaussian processes to infer a Mahalanobis distance metric that aligns with the random forest's decision boundaries. The method applies KOSNN in the transformed feature space induced by this learned distance, using a global EVT model (instead of class-wise models) to estimate the probability of a sample belonging to an unknown class. The decision function uses the closed-set RF prediction unless the unknown probability exceeds a threshold, balancing empirical risk and open space risk.

## Key Results
- RF-KOSNN outperforms state-of-the-art distance-based open-set recognition methods on both synthetic and real-world datasets
- The method achieves improved overall accuracy and better management of the trade-off between empirical risk and open space risk
- Global EVT model for open-set recognition proves more robust than class-wise models, especially with scarce data or class imbalances

## Why This Works (Mechanism)

### Mechanism 1
RF-GAP proximities preserve both data geometry and random forest classification accuracy, making them suitable for inferring a Mahalanobis distance metric that improves open-set recognition. These proximities capture the similarity structure learned by the random forest, and by modeling their relationship with Euclidean distances using Gaussian processes, a task-specific distance is inferred that aligns with the RF's decision boundaries. The core assumption is that the similarity structure encoded in RF-GAP proximities reflects the optimal geometry for distinguishing known from unknown classes.

### Mechanism 2
Using a global EVT model for open-set recognition is more robust than class-wise models, especially when dealing with scarce data or class imbalances. Instead of fitting separate Weibull distributions for each known class, a single EVT model is trained on distance ratios across all known classes, estimating the probability of a sample belonging to the unknown class based on how extreme its distance ratio is relative to the training data. The core assumption is that the distribution of distance ratios from known classes can be modeled globally without significant loss of discriminative power.

### Mechanism 3
The proposed method achieves a better balance between empirical risk and open space risk by incorporating the learned Mahalanobis distance into the KOSNN framework. The decision function uses the closed-set RF prediction unless the probability of being an unknown (estimated via the global EVT model on the transformed feature space) exceeds a threshold. This allows the classifier to reject samples from unknown classes while maintaining high accuracy on known classes. The core assumption is that managing open-set risk in the linearly transformed feature space effectively manages it in the original space.

## Foundational Learning

- Concept: Open-set recognition and the concept of open space risk.
  - Why needed here: The paper addresses a scenario where test samples may come from unknown classes, requiring the classifier to distinguish them from known classes.
  - Quick check question: What is the difference between open-set and closed-set recognition?

- Concept: Random Forest and its properties, particularly RF-GAP proximities.
  - Why needed here: The method leverages RF-GAP proximities to infer a distance metric for open-set recognition.
  - Quick check question: How are RF-GAP proximities defined and what information do they capture?

- Concept: Gaussian Processes and their use in learning a Mahalanobis distance metric.
  - Why needed here: A GP is used to model the relationship between RF-GAP proximities and Euclidean distances, inferring a task-specific distance metric.
  - Quick check question: How does a GP learn a mapping from input distances to output similarities?

## Architecture Onboarding

- Component map:
  Closed-set Random Forest Classifier (RF-GAP proximities) -> Gaussian Process for distance metric learning -> Distance-based Open-set Recognition (KOSNN in transformed space) -> Extreme Value Theory (global model for unknown class probability)

- Critical path:
  1. Train closed-set RF on known classes.
  2. Compute RF-GAP proximity matrix on training data.
  3. Use GP to infer Mahalanobis distance from RF-GAP proximities.
  4. Transform feature space using learned distance.
  5. Apply KOSNN with global EVT model in transformed space for OSR.

- Design tradeoffs:
  - Global EVT model vs. class-wise models: Simpler and more robust but may lose some class-specific information.
  - Diagonal L in Mahalanobis distance: Reduces hyperparameters and computational cost but assumes feature independence.

- Failure signatures:
  - Poor OSR performance despite good closed-set accuracy: Likely issues with distance metric learning or EVT model fitting.
  - High variance in OSR performance across runs: Potential instability in GP inference or hyperparameter selection.

- First 3 experiments:
  1. Reproduce the synthetic dataset experiment to validate the core OSR mechanism.
  2. Test on the Iris dataset to evaluate performance on a simple, well-known dataset.
  3. Experiment with different EVT threshold values (U) to analyze the trade-off between known class accuracy and OSR performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed RF-KOSNN method perform on high-dimensional datasets with thousands of features, compared to deep learning approaches?
- Basis in paper: [inferred] The paper mentions that tree-based models like RF often outperform deep learning approaches on tabular data, but does not provide empirical evidence on high-dimensional datasets.
- Why unresolved: The experiments in the paper are conducted on relatively low-dimensional datasets (iris with 4 features, handwritten digits with 64 features, and fund data with ~16 features). No experiments are shown on high-dimensional datasets.
- What evidence would resolve it: Experiments comparing RF-KOSNN performance to deep learning methods on high-dimensional tabular datasets with thousands of features.

### Open Question 2
- Question: How sensitive is the RF-KOSNN method to the choice of hyperparameters, particularly the number of trees and maximum depth in the random forest?
- Basis in paper: [explicit] The paper mentions that hyperparameters were selected with grid search and 5-fold cross-validation, but does not discuss the sensitivity of results to these choices.
- Why unresolved: While the paper describes the hyperparameter search process, it does not provide information on how robust the method is to different hyperparameter settings or what the optimal values are.
- What evidence would resolve it: Sensitivity analysis showing RF-KOSNN performance across a range of hyperparameter values, or information on which hyperparameters have the most significant impact on performance.

### Open Question 3
- Question: How does the proposed distance metric learning approach compare to other distance metric learning methods in the context of open set recognition?
- Basis in paper: [inferred] The paper introduces a novel distance metric learning approach using Gaussian processes, but does not compare it to other existing methods.
- Why unresolved: While the paper shows improved performance over KOSNN, it does not provide a comparison with other distance metric learning approaches that could be applied to open set recognition.
- What evidence would resolve it: Experiments comparing the proposed distance metric learning approach to other state-of-the-art distance metric learning methods in the context of open set recognition.

## Limitations

- Computational complexity: The distance metric learning phase using Gaussian processes is computationally intensive, particularly with high-dimensional data or large datasets.
- Dependence on RF-GAP quality: The method's effectiveness depends critically on the quality of RF-GAP proximities, which may not capture meaningful similarity structure in noisy or high-dimensional datasets.
- Limited evaluation scope: Experiments are primarily conducted on relatively simple, low-dimensional datasets, with limited assessment of performance on complex, high-dimensional data typical of real-world applications.

## Confidence

- Confidence in core mechanism (Medium): The paper provides theoretical justification and empirical evidence that RF-GAP proximities preserve both geometry and accuracy, but the evidence is primarily based on synthetic and relatively simple real-world datasets.
- Confidence in OSR performance claims (Medium-High): The experimental results show consistent improvements over state-of-the-art methods across multiple datasets and metrics. However, the comparison is limited to distance-based OSR methods, and the performance on high-dimensional image datasets is not evaluated.

## Next Checks

1. Evaluate RF-KOSNN on high-dimensional image datasets (e.g., CIFAR-10 or MNIST with artificially removed classes) to assess scalability and performance in more realistic scenarios.
2. Conduct ablation studies to isolate the contribution of each component (RF-GAP proximities, GP-based distance learning, global EVT model) to OSR performance.
3. Test the method's robustness to class imbalance and varying proportions of unknown classes in the test set to evaluate real-world applicability.