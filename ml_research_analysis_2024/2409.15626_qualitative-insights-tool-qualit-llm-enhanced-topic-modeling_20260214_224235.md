---
ver: rpa2
title: 'Qualitative Insights Tool (QualIT): LLM Enhanced Topic Modeling'
arxiv_id: '2409.15626'
source_url: https://arxiv.org/abs/2409.15626
tags:
- topic
- modeling
- topics
- data
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QualIT integrates large language models (LLMs) with clustering-based
  topic modeling to overcome limitations of traditional approaches like LDA and BERTopic.
  The method uses LLMs to extract key phrases from documents, perform hallucination
  checks via coherence scoring, and cluster these phrases into main topics and sub-topics.
---

# Qualitative Insights Tool (QualIT): LLM Enhanced Topic Modeling

## Quick Facts
- arXiv ID: 2409.15626
- Source URL: https://arxiv.org/abs/2409.15626
- Authors: Satya Kapoor; Alex Gil; Sreyoshi Bhaduri; Anshul Mittal; Rutu Mulkar
- Reference count: 29
- Key result: Achieved 70% topic coherence and 95.5% topic diversity on 20 NewsGroups dataset, outperforming BERTopic (65%, 85%) and LDA (57%, 72%)

## Executive Summary
QualIT integrates large language models with clustering-based topic modeling to address limitations of traditional approaches like LDA and BERTopic. The method uses LLMs to extract key phrases from documents, performs hallucination checks via coherence scoring, and clusters these phrases into main topics and sub-topics. Tested on the 20 NewsGroups dataset with 20 ground-truth topics, QualIT achieved superior topic coherence and diversity metrics compared to traditional methods. Human evaluators also showed higher agreement on topic classifications when using QualIT outputs.

## Method Summary
QualIT is an LLM-enhanced topic modeling approach that extracts key phrases from documents using Claude-2.1, filters potential hallucinations through coherence scoring based on Amazon Titan embeddings, and applies two-layer K-Means clustering to identify main topics and sub-topics. The method preprocesses the 20 NewsGroups dataset with tokenization, lemmatization, and stop word removal, then uses LLM prompts to synthesize themes from clusters. Evaluation includes topic coherence and diversity metrics, plus human evaluation for classification agreement.

## Key Results
- Achieved 70% topic coherence versus 65% for BERTopic and 57% for LDA
- Attained 95.5% topic diversity versus 85% for BERTopic and 72% for LDA
- Human evaluators agreed more frequently on topic classifications (80% agreement for at least 2 evaluators vs 50% for LDA)
- Processing time: 2-3 hours versus 30 minutes for BERTopic

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-enhanced key phrase extraction reduces noise by capturing semantically relevant phrases instead of raw text
- Mechanism: LLMs extract condensed thematic phrases that better represent document content, filtering out irrelevant tokens
- Core assumption: LLM key phrase extraction is more semantically accurate than bag-of-words approaches
- Evidence anchors:
  - [abstract] "leverages the deep contextual understanding and powerful language generation capabilities of LLMs to enrich the topic modeling process"
  - [section 4.1] "The LLM analyzes the content, discerning patterns and topics that frequently occur within the text"
  - [corpus] Found 25 related papers with average neighbor FMR=0.47, indicating moderate semantic overlap in topic modeling research
- Break condition: If LLM key phrase extraction introduces bias or hallucinates content, the clustering will group incorrect semantic clusters

### Mechanism 2
- Claim: Coherence scoring filters out hallucinated key phrases before clustering
- Mechanism: Low coherence scores indicate phrases that don't align with document content, reducing noise in clustering input
- Core assumption: Cosine similarity on embeddings accurately measures phrase-document alignment
- Evidence anchors:
  - [section 4.2] "A coherence score is calculated for each phrase... Key-phrases with the lowest coherence scores may be flagged for 'hallucination'"
  - [section 4.2] "For our approach, phrases with coherence scores below 10% were excluded"
  - [corpus] No direct evidence of hallucination filtering in related papers, suggesting this is a novel contribution
- Break condition: If the coherence threshold is too strict or too lenient, valid phrases may be excluded or hallucinated phrases retained

### Mechanism 3
- Claim: Two-layer clustering creates hierarchical topic structures that improve interpretability
- Mechanism: Main clustering groups documents by high-level themes, then sub-clustering creates granular sub-topics within each main theme
- Core assumption: K-means clustering on key phrases produces meaningful semantic groupings
- Evidence anchors:
  - [section 4.3.1] "Create the primary cluster and pass the key-phrases of each cluster to another LLM prompt to distill a main theme"
  - [section 4.3.2] "Implement a secondary level of clustering within each primary cluster to uncover more specific sub-topics"
  - [section 5.1] "This pre-processed dataset was used in all methods in this experiment for fair comparisons"
- Break condition: If the number of clusters is poorly estimated, topics will be over- or under-segmented

## Foundational Learning

- Concept: Cosine similarity for coherence scoring
  - Why needed here: Measures semantic alignment between extracted phrases and original document content
  - Quick check question: What does a low coherence score indicate about a key phrase's relationship to the source document?

- Concept: Silhouette score for optimal clustering
  - Why needed here: Automatically determines the number of clusters when K-means requires this parameter
  - Quick check question: What value range does the silhouette score use to evaluate clustering quality?

- Concept: Topic coherence metrics (normalized pointwise mutual information)
  - Why needed here: Evaluates how semantically meaningful topics are based on word co-occurrence patterns
  - Quick check question: What range does topic coherence score fall within, and what does a score of 1 indicate?

## Architecture Onboarding

- Component map: Document → LLM Key Phrase Extraction → Coherence Scoring → Main Clustering → LLM Theme Synthesis → Sub-Clustering → LLM Sub-Topic Extraction → Topic Output
- Critical path: Document → LLM Key Phrase Extraction → Coherence Scoring → Main Clustering (longest processing time)
- Design tradeoffs: Higher coherence thresholds reduce noise but may lose valid content; more clusters increase granularity but reduce coherence
- Failure signatures: Low topic coherence scores across all methods suggest preprocessing issues; high processing time indicates LLM efficiency problems
- First 3 experiments:
  1. Test coherence scoring threshold by running with 5%, 10%, and 15% exclusion rates on sample documents
  2. Compare K-means vs HDBSCAN clustering on the same key phrase dataset to evaluate clustering quality
  3. Measure runtime impact of reducing LLM temperature from 0 to 0.2 while monitoring coherence score changes

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on LLM performance without disclosed prompt engineering details limits reproducibility
- 10% coherence threshold appears arbitrary without systematic optimization or sensitivity analysis
- 4-6x runtime increase over traditional methods (2-3 hours vs 30 minutes) raises scalability concerns
- Human evaluation involved only 3 participants with unspecified expertise levels

## Confidence

**High confidence** in the mechanism of LLM-enhanced phrase extraction improving semantic relevance over bag-of-words approaches

**Medium confidence** in the coherence scoring methodology for hallucination detection, as the threshold selection lacks rigorous justification

**Medium confidence** in the two-layer clustering approach producing interpretable hierarchical topics, though optimal cluster number determination could be more robust

**Low confidence** in the human evaluation methodology due to small sample size and unspecified rater expertise

## Next Checks
1. Conduct ablation studies comparing QualIT performance with different coherence thresholds (5%, 10%, 15%) to determine optimal filtering parameters and their impact on topic quality
2. Test the approach on a larger, more diverse dataset (e.g., full Reuters-21578 or academic abstracts) to evaluate scalability and runtime efficiency
3. Expand human evaluation to include 10-15 participants with specified domain expertise, using inter-rater reliability metrics (Krippendorff's alpha) to strengthen agreement assessment