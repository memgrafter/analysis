---
ver: rpa2
title: 'Modular Sentence Encoders: Separating Language Specialization from Cross-Lingual
  Alignment'
arxiv_id: '2407.14878'
source_url: https://arxiv.org/abs/2407.14878
tags:
- language
- cross-lingual
- monolingual
- training
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Modular training is proposed to mitigate the curse of multilinguality
  and performance trade-offs in multilingual sentence encoders. Language-specific
  embedding layers, language adapters, and sentence encoding adapters are trained
  to specialize for each language, followed by cross-lingual alignment adapters to
  align to English.
---

# Modular Sentence Encoders: Separating Language Specialization from Cross-Lingual Alignment

## Quick Facts
- arXiv ID: 2407.14878
- Source URL: https://arxiv.org/abs/2407.14878
- Reference count: 40
- Primary result: Modular training achieves better and more balanced performance across monolingual and cross-lingual tasks compared to full-parameter training

## Executive Summary
This paper introduces a modular approach to training multilingual sentence encoders that addresses the curse of multilinguality. The method separates language-specific specialization from cross-lingual alignment by using dedicated modules for each function. Language-specific embedding layers and adapters are first trained to specialize for each language, followed by cross-lingual alignment adapters that map representations to English. The approach is evaluated on semantic textual similarity and cross-lingual retrieval tasks across 8 languages, showing improved performance particularly for low-resource languages.

## Method Summary
The modular training approach uses a layered architecture where language-specific modules handle individual language characteristics, while cross-lingual alignment modules handle the mapping between languages. The method involves two training stages: first, language-specific embedding layers, language adapters, and sentence encoding adapters are trained using machine-translated paraphrase and parallel data; second, cross-lingual alignment adapters are trained to align each language to English. This separation allows the model to learn specialized representations for each language while maintaining effective cross-lingual alignment.

## Key Results
- Modular training achieves better and more balanced performance across monolingual and cross-lingual tasks compared to full-parameter training
- Significant performance gains observed for low-resource languages (Finnish, Swahili, Urdu)
- The approach mitigates the trade-off between monolingual and cross-lingual performance that typically occurs with multilingual models

## Why This Works (Mechanism)
The modular approach works by decoupling language specialization from cross-lingual alignment. By training language-specific components separately before alignment, the model can develop specialized representations that capture language-specific nuances without being constrained by the need to maintain cross-lingual compatibility throughout training. The subsequent alignment stage then maps these specialized representations to a common space, allowing for effective cross-lingual transfer while preserving the quality of monolingual representations.

## Foundational Learning
1. **Curse of Multilinguality**: Why needed - explains why multilingual models typically perform worse than monolingual ones as language count increases; Quick check - verify that adding more languages degrades performance in standard multilingual models
2. **Adapter-based training**: Why needed - provides a parameter-efficient way to adapt models to new languages; Quick check - confirm that adapters add minimal parameters compared to full fine-tuning
3. **Cross-lingual alignment**: Why needed - enables transfer of knowledge between languages; Quick check - verify that aligned representations allow for effective zero-shot cross-lingual transfer

## Architecture Onboarding

**Component map**: Language-specific modules (embeddings, language adapters, sentence encoding adapters) -> Cross-lingual alignment adapters -> English space

**Critical path**: Language-specific modules learn specialized representations -> Cross-lingual alignment adapters map to English space -> Downstream tasks use aligned representations

**Design tradeoffs**: 
- Modular vs. full-parameter training: modularity provides specialization but adds complexity
- English-centric alignment vs. multilingual alignment: English-centric is simpler but may limit non-English transfer
- Adapter size vs. performance: larger adapters may improve performance but reduce parameter efficiency

**Failure signatures**:
- Poor monolingual performance indicates inadequate language-specific specialization
- Weak cross-lingual transfer suggests ineffective alignment adapters
- Performance degradation with additional languages may indicate scalability issues

**3 first experiments**:
1. Evaluate monolingual performance on semantic textual similarity for each language separately
2. Test cross-lingual retrieval performance between each language pair
3. Compare parameter efficiency between modular and full-parameter approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on parallel and machine-translated data introduces quality dependencies
- Evaluation focuses primarily on semantic textual similarity and cross-lingual retrieval tasks
- Limited language coverage (8 languages total, only 3 low-resource languages)

## Confidence
- High confidence: The modular architecture design and its theoretical advantages in separating specialization from alignment
- Medium confidence: Performance improvements over full-parameter training on the evaluated tasks
- Medium confidence: The claim that modular training particularly benefits low-resource languages

## Next Checks
1. Evaluate the approach on a broader range of languages including underrepresented language families and scripts to assess true cross-lingual generalization
2. Test the modular encoders on diverse downstream tasks beyond semantic textual similarity and retrieval, such as cross-lingual classification and question answering
3. Conduct ablation studies to quantify the individual contributions of language adapters, sentence encoding adapters, and alignment adapters to the overall performance gains