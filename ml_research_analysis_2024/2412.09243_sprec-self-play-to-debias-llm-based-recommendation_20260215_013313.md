---
ver: rpa2
title: 'SPRec: Self-Play to Debias LLM-based Recommendation'
arxiv_id: '2412.09243'
source_url: https://arxiv.org/abs/2412.09243
tags:
- recommendation
- arxiv
- sprec
- negative
- self-play
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a critical bias issue in large language model
  (LLM)-based recommender systems trained with Direct Preference Optimization (DPO),
  where the model disproportionately recommends popular items, creating filter bubbles
  and harming user experience. To address this, the authors propose SPRec, a self-play
  framework that iteratively refines recommendations by using the model's own outputs
  as negative samples, effectively suppressing biased items.
---

# SPRec: Self-Play to Debias LLM-based Recommendation

## Quick Facts
- arXiv ID: 2412.09243
- Source URL: https://arxiv.org/abs/2412.09243
- Reference count: 40
- Primary result: SPRec improves HR@5 by up to 0.0143 and NDCG@5 by up to 0.0140 while reducing over-recommendation (ORRatio@1 down to 0.1670) and increasing diversity (DivRatio@1 up to 0.3859)

## Executive Summary
This paper addresses a critical bias issue in LLM-based recommender systems trained with Direct Preference Optimization (DPO), where the model disproportionately recommends popular items, creating filter bubbles. The authors propose SPRec, a self-play framework that iteratively refines recommendations by using the model's own outputs as negative samples. SPRec outperforms existing SFT- and DPO-based methods across four real-world datasets, achieving both better accuracy and fairness metrics while enhancing diversity.

## Method Summary
SPRec is a self-play framework that iteratively refines LLM-based recommendations by alternating between SFT and DPO steps. The method treats offline interaction data as positive samples and the model's predicted outputs from the previous iteration as negative samples. This self-play approach adaptively suppresses biased items by increasing their learning rates based on their predicted probabilities, while the SFT step maintains alignment with positive samples to preserve accuracy.

## Key Results
- HR@5 improved by up to 0.0143 over baseline SFT and DPO methods
- NDCG@5 improved by up to 0.0140 while reducing over-recommendation (ORRatio@1 down to 0.1670)
- Diversity (DivRatio@1) increased by up to 0.3859 and fairness (MGU@1) improved to 0.0242
- Ablation studies confirm self-play negative sampling is critical for bias mitigation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPO inherently amplifies popularity bias by optimizing for mode-seeking behavior in the reverse KL-divergence.
- Mechanism: DPO's loss function encourages the model to prioritize items with higher popularity probabilities (p_D(y|x)), leading to over-recommendation of popular items.
- Core assumption: The negative sampling distribution q_D(y|x) is uniform and β < 1, which is typical in recommendation settings.
- Evidence anchors:
  - [abstract] "our theoretical analysis reveals that DPO inherently biases the model towards a few items, exacerbating the filter bubble issue"
  - [section 4.1] "DPO loss naturally encourages the policy to prioritize popular items with higher probabilities p_D(y|x)"
  - [corpus] Weak correlation with related papers on DPO bias

### Mechanism 2
- Claim: SPRec's self-play negative sampling adaptively suppresses biased items by increasing their learning rate.
- Mechanism: In each iteration, the model's own predictions are used as negative samples, weighted by their predicted probabilities. Items ranked higher by the model receive stronger penalties.
- Core assumption: The model's predicted distribution at iteration t reflects current bias patterns that need correction.
- Evidence anchors:
  - [abstract] "treating offline interaction data as positive samples and the predicted outputs from the previous iteration as negative samples"
  - [section 4.2] "objective adaptively pays more attention to biased items by increasing their learning rates if they have higher probabilities"
  - [corpus] Moderate correlation with self-play literature in LLMs

### Mechanism 3
- Claim: The SFT-DPO alternating structure prevents accuracy loss while improving fairness.
- Mechanism: SFT maintains alignment with positive samples and preserves accuracy, while DPO with self-play negatives reduces bias. Alternating prevents either component from dominating.
- Core assumption: Both SFT and DPO contribute positively when used together, rather than interfering.
- Evidence anchors:
  - [abstract] "each self-play iteration begins with an SFT round using positive samples...followed by a DPO step"
  - [section 4.3] "incorporating an SFT step in each self-play iteration can further enhance performance"
  - [corpus] Moderate correlation with iterative fine-tuning approaches

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is the core mechanism being modified to address bias in LLM-based recommendations
  - Quick check question: What is the closed-form solution for the optimal policy under DPO's objective function?

- Concept: Self-play reinforcement learning
  - Why needed here: Self-play provides the negative sampling strategy that enables adaptive bias suppression
  - Quick check question: How does using the model's own predictions as negative samples differ from random negative sampling?

- Concept: KL-divergence properties (forward vs reverse)
  - Why needed here: Understanding why DPO's reverse KL promotes mode-seeking while SFT's forward KL promotes mass-covering is crucial for the theoretical analysis
  - Quick check question: Which type of KL-divergence would you use if you wanted the model to cover the entire distribution rather than focus on peaks?

## Architecture Onboarding

- Component map: Dataset Construction -> SFT Step -> DPO Step -> Repeat
- Critical path: For each iteration: generate negative samples from current model → construct pairwise dataset → run SFT on positive samples → run DPO with self-play negatives → repeat
- Design tradeoffs: SFT preserves accuracy but doesn't address bias; DPO with random negatives amplifies bias; self-play negatives address bias but may hurt accuracy without SFT. The alternating structure balances these tradeoffs.
- Failure signatures: Loss of diversity metrics without improvement in accuracy suggests over-reliance on DPO; degradation in accuracy metrics suggests insufficient SFT; no improvement in either suggests ineffective negative sampling.
- First 3 experiments:
  1. Run baseline SFT-only training and measure popularity bias (ORRatio@1, DivRatio@1)
  2. Run DPO with random negatives and compare bias metrics to baseline
  3. Run SPRec for 1-2 iterations and measure change in bias metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SPRec perform when applied to datasets with different interaction densities or sparsity levels?
- Basis in paper: [inferred] The paper evaluates SPRec on four real-world datasets but does not explore performance across varying levels of interaction density or sparsity.
- Why unresolved: The paper focuses on demonstrating SPRec's effectiveness across different datasets but does not analyze its robustness to varying data characteristics like sparsity.
- What evidence would resolve it: Experiments comparing SPRec's performance on datasets with different interaction densities or sparsity levels, showing how it adapts to varying data characteristics.

### Open Question 2
- Question: Can SPRec be extended to handle multi-modal data, such as combining text, images, and user interactions in recommendations?
- Basis in paper: [inferred] The paper focuses on text-based recommendations using LLMs but does not explore the integration of multi-modal data.
- Why unresolved: The paper demonstrates SPRec's effectiveness in text-based recommendations but does not investigate its applicability to multi-modal scenarios.
- What evidence would resolve it: Experiments showing SPRec's performance when integrating text, images, and user interactions, highlighting its ability to handle multi-modal data.

### Open Question 3
- Question: How does SPRec's performance scale with larger or more diverse item catalogs?
- Basis in paper: [inferred] The paper evaluates SPRec on datasets with up to 32,094 items but does not explore its scalability to much larger or more diverse item catalogs.
- Why unresolved: The paper demonstrates SPRec's effectiveness on moderately sized datasets but does not investigate its performance on significantly larger or more diverse item catalogs.
- What evidence would resolve it: Experiments comparing SPRec's performance on datasets with varying catalog sizes and diversity, showing how it scales with larger or more diverse item sets.

## Limitations
- Theoretical analysis relies on idealized assumptions about uniform negative sampling and β < 1 that may not hold in practice
- Limited empirical evaluation to four datasets without analysis of performance across different data characteristics
- No investigation of convergence stability or diminishing returns across self-play iterations

## Confidence

**Confidence labels:**
- **High confidence**: The alternating SFT-DPO structure's role in balancing accuracy and fairness; the empirical improvements in HR@5 (up to 0.0143) and NDCG@5 (up to 0.0140) over baselines
- **Medium confidence**: The theoretical claim that DPO inherently amplifies popularity bias; the effectiveness of self-play negative sampling for adaptive bias suppression
- **Low confidence**: The generalizability of results across different LLM architectures and recommendation domains

## Next Checks
1. Test SPRec's performance on a held-out dataset with deliberately skewed popularity distributions to stress-test the bias mitigation
2. Implement a random negative sampling baseline with matched computational budget to isolate the contribution of self-play vs. simply having more negatives
3. Analyze the convergence properties by monitoring diversity metrics across iterations to identify potential diminishing returns or instability patterns