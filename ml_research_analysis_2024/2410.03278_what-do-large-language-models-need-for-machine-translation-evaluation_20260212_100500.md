---
ver: rpa2
title: What do Large Language Models Need for Machine Translation Evaluation?
arxiv_id: '2410.03278'
source_url: https://arxiv.org/abs/2410.03278
tags:
- translation
- language
- llms
- quality
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of large language models
  (LLMs) for machine translation evaluation. The authors systematically explore what
  translation information LLMs need, such as source, reference, translation errors,
  and annotation guidelines, to evaluate translation quality.
---

# What do Large Language Models Need for Machine Translation Evaluation?

## Quick Facts
- arXiv ID: 2410.03278
- Source URL: https://arxiv.org/abs/2410.03278
- Authors: Shenbin Qian; Archchana Sindhujan; Minnie Kabra; Diptesh Kanojia; Constantin Orăsan; Tharindu Ranasinghe; Frédéric Blain
- Reference count: 14
- Key outcome: Reference translations are crucial for accurate LLM-based MT evaluation, and larger models benefit more from Chain of Thought prompting than smaller models.

## Executive Summary
This paper systematically investigates what information large language models need to effectively evaluate machine translation quality. The authors experiment with different prompting techniques (zero-shot, Chain of Thought, few-shot) and prompt templates varying the inclusion of source text, reference translations, translation errors, and annotation guidelines across eight language pairs. Their findings reveal that reference translations are essential for accurate evaluation, while larger models show enhanced performance with Chain of Thought prompting. However, the study also highlights a critical limitation: LLMs often fail to provide consistent numerical scores, raising questions about their reliability for MT evaluation tasks.

## Method Summary
The authors use six LLM variants (OpenChat3.5, Llama-2-7B/13B, Gemma-7B, Qwen1.5-14B, Mixtral-8x7B) to predict Direct Assessment (DA) scores for machine translations across eight language pairs. They employ eight different prompt templates varying the information provided (source, reference, error words, guidelines) and test three prompting techniques: zero-shot, Chain of Thought, and few-shot. The evaluation uses Spearman correlation between predicted and human-annotated DA scores from the WMT22 QE shared task dataset. Score extraction is performed using regular expressions from LLM outputs, with performance measured across different model sizes and language resource levels.

## Key Results
- Reference translations are crucial for accurate LLM-based MT evaluation
- Larger models benefit more from Chain of Thought prompting than smaller models
- LLMs do not consistently provide numerical scores, questioning their reliability for this task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reference translations are crucial for accurate translation evaluation by LLMs.
- Mechanism: LLMs perform better when provided with source, MT output, and reference translation, as this allows them to compare the translation against the reference to assess accuracy.
- Core assumption: LLMs can effectively utilize reference translations to identify discrepancies and evaluate translation quality.
- Evidence anchors:
  - [abstract]: "Our findings indicate the importance of reference translations for an LLM-based evaluation."
  - [section]: "We observe that LLM performance is generally better when the source and reference are included in the prompt, as in Templates 3, 5, and 6, compared to prompts without them, such as Templates 1 and 2."
  - [corpus]: Weak - no direct corpus evidence provided.
- Break Condition: If the reference translation is of poor quality or significantly different from the source, the LLM may not accurately assess the translation quality.

### Mechanism 2
- Claim: Larger models benefit more from Chain of Thought (CoT) prompting than smaller models.
- Mechanism: CoT prompting provides reasoning steps that help LLMs analyze translation quality, and larger models with more parameters can better utilize this structured reasoning.
- Core assumption: Larger models have greater capacity to process and benefit from additional reasoning steps provided by CoT prompting.
- Evidence anchors:
  - [abstract]: "While larger models do not necessarily fare better, they tend to benefit more from CoT prompting, than smaller models."
  - [section]: "For the larger 13 billion parameter variants, results were mixed for different language pairs. For language pairs such as EN-DE and EN-MR, CoT prompting improved the performance in the prediction of DA scores."
  - [corpus]: Weak - no direct corpus evidence provided.
- Break Condition: If the CoT prompt is poorly designed or the translation task is too simple, the benefits of CoT prompting may not be significant even for larger models.

### Mechanism 3
- Claim: LLMs are inconsistent in generating numerical scores for translation evaluation.
- Mechanism: LLMs often generate lengthy and unstructured explanations instead of concise numerical scores, making it difficult to extract and compare scores consistently.
- Core assumption: The inconsistency in score generation is due to the way LLMs are trained and the nature of the translation evaluation task.
- Evidence anchors:
  - [abstract]: "We also observe that LLMs do not always provide a numerical score when generating evaluations, which poses a question on their reliability for the task."
  - [section]: "In most cases, LLMs tend to generate scores accompanied by lengthy and unstructured explanations. While using regular expressions for extraction can be helpful, it is not always reliable."
  - [corpus]: Weak - no direct corpus evidence provided.
- Break Condition: If the prompt is specifically designed to elicit numerical scores or if the LLM is fine-tuned for this task, the inconsistency may be reduced.

## Foundational Learning

- Concept: Translation quality evaluation
  - Why needed here: Understanding the basics of translation quality evaluation is crucial to grasp the significance of the study's findings and the challenges in using LLMs for this task.
  - Quick check question: What are some traditional methods used for automatic evaluation of machine translation quality?

- Concept: Chain of Thought (CoT) prompting
  - Why needed here: CoT prompting is a key technique explored in the study to improve LLM performance in translation evaluation, and understanding its mechanism is essential to interpret the results.
  - Quick check question: How does CoT prompting differ from standard prompting, and what are its potential benefits?

- Concept: Spearman correlation
  - Why needed here: Spearman correlation is used as the evaluation metric in the study to assess the agreement between LLM-generated scores and human judgments, making it important to understand its meaning and interpretation.
  - Quick check question: What does a high Spearman correlation between LLM scores and human scores indicate about the LLM's performance in translation evaluation?

## Architecture Onboarding

- Component map: Source text -> MT output -> LLM (with prompt template) -> DA score -> Spearman correlation with human scores
- Critical path: Source text, MT output, and reference translation are provided to LLM via prompt template, which generates a DA score that is compared to human-annotated scores using Spearman correlation
- Design tradeoffs: Using larger LLMs may improve performance but also increases computational cost. Including reference translations improves accuracy but may not always be available. CoT prompting can enhance performance but requires more complex prompts.
- Failure signatures: If the LLM consistently fails to provide numerical scores or if the scores show low correlation with human judgments, it indicates a failure in the evaluation process.
- First 3 experiments:
  1. Compare the performance of different LLMs (OpenChat3.5, Llama-2, Gemma, Qwen1.5, Mixtral-8x7B) on a specific language pair using zero-shot prompting with the same prompt template.
  2. Evaluate the impact of including reference translations in the prompt by comparing the performance of zero-shot prompting with and without references.
  3. Assess the effectiveness of CoT prompting by comparing the performance of a larger LLM (e.g., Llama-2-13B) with and without CoT prompting on a specific language pair.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do larger language models consistently outperform smaller ones for machine translation evaluation across all language pairs?
- Basis in paper: [explicit] The authors state "Larger models may not necessarily perform better than smaller models, but CoT prompting works better on larger than smaller model variants."
- Why unresolved: The paper only tested models up to 14 billion parameters due to computational constraints. The performance of models with 70 billion or more parameters remains unknown.
- What evidence would resolve it: Comprehensive testing of LLMs with varying sizes (e.g., 30B, 70B, 175B parameters) across multiple language pairs to establish a clear correlation between model size and evaluation performance.

### Open Question 2
- Question: What is the impact of including translation errors and annotation guidelines on LLM-based MT evaluation accuracy?
- Basis in paper: [explicit] The authors investigated this by creating prompts with/without error words and guidelines, finding "incorporating error words does not seem to improve performance" and "including error words and annotation guidelines does not consistently help LLMs evaluate MT quality."
- Why unresolved: The experiments were limited to word-level error extraction and a single set of guidelines. Different error granularities or guideline formulations might yield different results.
- What evidence would resolve it: Systematic experiments varying error granularity (word, phrase, sentence level) and using multiple guideline sets, measuring their impact on evaluation accuracy across diverse language pairs.

### Open Question 3
- Question: Why do LLMs show lower correlation with human judgments for high-resource language pairs compared to low-resource ones?
- Basis in paper: [explicit] The authors observed "For high-resource language pairs like EN-DE and EN-ZH, correlation scores tend to be lower than those of medium- and low-resource pairs" and attributed this to "imbalance in the score representation."
- Why unresolved: The analysis was based on density plots and speculation about MT system quality. The underlying causes of this discrepancy remain unclear.
- What evidence would resolve it: In-depth linguistic analysis of high vs. low-resource MT outputs, combined with human evaluation studies to understand how LLM scoring patterns differ from human raters across resource levels.

## Limitations

- Evaluation relies solely on DA scores from WMT22 QE dataset, which may not capture translation quality nuances across different domains
- Regular expression-based score extraction method is unreliable when LLMs generate lengthy, unstructured explanations
- Experiments are limited to English-centric language pairs, raising questions about generalizability to truly multilingual scenarios

## Confidence

- High Confidence: The finding that reference translations significantly improve LLM evaluation performance is well-supported by systematic comparisons across multiple language pairs and model sizes.
- Medium Confidence: The observation that larger models benefit more from CoT prompting, while theoretically sound, is based on mixed results across different language pairs.
- Low Confidence: The claim about LLMs' unreliability due to inconsistent score generation is difficult to quantify precisely.

## Next Checks

1. **Score Extraction Robustness Test:** Implement alternative score extraction methods (such as fine-tuned classifiers or structured output formats) to assess whether the score generation inconsistency is an artifact of the extraction method rather than LLM behavior.

2. **Cross-Domain Generalization Study:** Evaluate the same prompting strategies on translation quality assessment tasks from different domains (e.g., technical documentation, literature, dialogue) to test the robustness of the reference translation finding across diverse contexts.

3. **Prompt Engineering Impact Analysis:** Systematically vary prompt structure, instruction clarity, and formatting across the studied language pairs to quantify the impact of prompt engineering on LLM performance, particularly for cases where current prompts yield inconsistent results.