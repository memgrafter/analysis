---
ver: rpa2
title: "Sequential transport maps using SoS density estimation and $\u03B1$-divergences"
arxiv_id: '2402.17943'
source_url: https://arxiv.org/abs/2402.17943
tags:
- density
- densities
- which
- where
- given
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for density estimation based on sequential
  transport maps. The key idea is to build a sequence of intermediate densities between
  a reference density and the target density, and then compute the exact Knothe-Rosenblatt
  map from the reference to each intermediate density.
---

# Sequential transport maps using SoS density estimation and $α$-divergences

## Quick Facts
- arXiv ID: 2402.17943
- Source URL: https://arxiv.org/abs/2402.17943
- Authors: Benjamin Zanger; Olivier Zahm; Tiangang Cui; Martin Schreiber
- Reference count: 40
- Key outcome: A method for density estimation using sequential transport maps with SoS densities and α-divergences, enabling unnormalized density handling and convex optimization.

## Executive Summary
This paper introduces a method for density estimation based on sequential transport maps, building a sequence of intermediate densities between a reference and target density. The approach uses Sum-of-Squares (SoS) functions and α-divergences to enable working with unnormalized densities and provides convex optimization problems solvable by semidefinite programming. The method breaks down complex density estimation into simpler subproblems using bridging densities, with theoretical convergence guarantees for all α-divergences.

## Method Summary
The method estimates densities by constructing a sequence of Knothe-Rosenblatt transport maps. Starting from a reference density, intermediate densities are estimated using SoS functions and α-divergences, with each transport map computed between the reference and the corresponding intermediate density. The final transport map is the composition of all intermediate maps. The approach handles unnormalized densities, provides convex optimization problems, and offers unified convergence analysis across all α-divergences.

## Key Results
- The method enables density estimation using unnormalized densities through α-divergences
- Sequential transport maps break complex problems into simpler subproblems
- SoS densities with α-divergences yield convex optimization problems solvable by semidefinite programming
- Diffusion-based bridging densities allow estimation from samples without normalization constants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method enables density estimation using unnormalized densities by employing α-divergences.
- Mechanism: α-divergences allow variational problems to be posed directly on unnormalized densities without requiring normalization constants, avoiding the instability of discretizing normalized KL divergence.
- Core assumption: The convex function ϕα(t) is well-defined and positive for all t > 0.
- Evidence anchors:
  - [abstract]: "The main advantage of α-divergences is to enable working with unnormalized densities, which provides benefits both numerically and theoretically."
  - [section]: "One practical advantage of using α-divergences with unnormalized density functions is that there is no need to enforce the approximate density to integrate to one while learning it."
  - [corpus]: Weak corpus support; no direct mention of unnormalized density handling.
- Break condition: If the target density has regions with very low values and α > 1, the zero-avoiding property may fail to capture those regions.

### Mechanism 2
- Claim: Combining Sum-of-Squares (SoS) densities with α-divergences yields convex optimization problems solvable by semidefinite programming.
- Mechanism: The SoS parametrization A ↦ gA is linear, preserving convexity of the α-divergence objective; this enables efficient SDP solvers.
- Core assumption: The basis Φ(x) is chosen such that the resulting SoS functions are well-behaved (e.g., orthonormal).
- Evidence anchors:
  - [abstract]: "Combining SoS densities with α-divergence interestingly yields convex optimization problems which can be efficiently solved using semidefinite programming."
  - [section]: "Using the notion of α-geodesic, we also propose a novel convergence analysis for (3) which relies on the geometry induced by Dα(·∥·)."
  - [corpus]: Weak corpus support; no direct mention of SDP or convex optimization guarantees.
- Break condition: If the chosen basis is not orthonormal or the domain is not semi-algebraic, the positive semidefiniteness constraint may need relaxation.

### Mechanism 3
- Claim: Sequential transport maps break down a complex density estimation problem into a sequence of simpler subproblems using bridging densities.
- Mechanism: By introducing intermediate densities {π(ℓ)} with increasing complexity, each map Qℓ is learned between ρref and a simpler target, reducing the difficulty of each subproblem.
- Core assumption: The sequence of bridging densities is chosen such that consecutive densities are sufficiently similar (bounded by η).
- Evidence anchors:
  - [abstract]: "We build on a sequence of composed Knothe-Rosenblatt (KR) maps... Each of those maps are built by first estimating an intermediate density of moderate complexity."
  - [section]: "Similar to Sequential Monte Carlo Samplers [12], this sequence allows for breaking down the challenging approximation problem (1) into a sequence of intermediate problems of more manageable complexity."
  - [corpus]: Weak corpus support; no direct mention of bridging density methodology.
- Break condition: If the bridging densities are not chosen carefully, the error can accumulate and the final approximation may be poor.

## Foundational Learning

- Concept: α-divergences and their properties (e.g., duality, stability under transport).
  - Why needed here: They generalize many common divergences and allow working with unnormalized densities.
  - Quick check question: What is the relationship between α-divergence and KL divergence when α = 1?

- Concept: Sum-of-Squares (SoS) functions and their parametrization.
  - Why needed here: They provide a flexible function class that remains positive and enables efficient computation of Knothe-Rosenblatt maps.
  - Quick check question: How is the Knothe-Rosenblatt map computed for a SoS density?

- Concept: Bridging densities and their role in sequential density estimation.
  - Why needed here: They decompose a complex estimation problem into simpler subproblems.
  - Quick check question: What is the difference between tempered and diffusion-based bridging densities?

## Architecture Onboarding

- Component map: Reference density ρref -> Basis functions Φ(x) -> Positive semidefinite matrix A -> α-divergence Dα -> Bridging densities {π(ℓ)} -> Knothe-Rosenblatt map Qℓ -> Composition Tℓ

- Critical path:
  1. Choose reference density ρref and basis Φ.
  2. Define sequence of bridging densities {π(ℓ)}.
  3. For each ℓ, estimate intermediate density ρ(ℓ) by solving minA Dα(π(ℓ)∥ρ(ℓ)).
  4. Compute Knothe-Rosenblatt map Qℓ from ρref to ρ(ℓ).
  5. Compose maps to obtain final transport Tℓ.

- Design tradeoffs:
  - Choice of α: Higher α gives zero-avoiding property but may miss low-density regions.
  - Choice of basis: Orthonormal polynomials simplify integration but may not be optimal for all domains.
  - Choice of bridging densities: Tempered densities are simple but diffusion-based may be better for samples-only scenarios.

- Failure signatures:
  - Poor approximation quality: Check if η (max divergence between consecutive bridging densities) is too large.
  - Numerical instability: Check if the SDP solver fails to converge or if A becomes ill-conditioned.
  - High computational cost: Check if the dimension of the SoS parametrization is too large.

- First 3 experiments:
  1. Estimate a simple Gaussian mixture from samples using diffusion-based bridging densities.
  2. Estimate a multimodal density from unnormalized evaluations using tempered bridging densities.
  3. Apply the method to a Bayesian inference problem with a known posterior density.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the curse of dimensionality be effectively addressed for SoS density estimation methods?
- Basis in paper: [inferred] The paper acknowledges that SoS densities do not scale well in high dimensions due to exponential increase of basis functions. The authors propose two methods (lazy maps and exploiting conditional independence) but note these are not fully explored.
- Why unresolved: Both proposed methods are only conceptually introduced without comprehensive evaluation or comparison. The lazy maps use random subspaces without optimization, and the graphical model approach assumes perfect knowledge of the structure.
- What evidence would resolve it: Comparative studies of different subspace selection methods for lazy maps, empirical evaluation of the graphical model approach on various network structures, or novel tensor-based formulations for SoS functions.

### Open Question 2
- Question: What are the optimal bridging densities for different types of target distributions (e.g., multimodal, concentrated, discontinuous)?
- Basis in paper: [explicit] The paper discusses tempered densities and introduces diffusion-based bridging densities, but notes that bridging densities for discontinuous distributions are missing. The authors suggest different bridging methods work better for different applications.
- Why unresolved: While the paper introduces new bridging methods, it doesn't provide systematic comparison across different distribution types or theoretical guarantees for their convergence properties.
- What evidence would resolve it: Comparative analysis of bridging density performance across distribution families, theoretical bounds on convergence rates for different bridging strategies, or adaptive methods for selecting optimal bridging densities.

### Open Question 3
- Question: How can optimal sampling strategies be developed for SoS density estimation that are compatible with the convex optimization framework?
- Basis in paper: [explicit] The paper mentions that optimal sampling is well-established for least squares in linear approximation spaces but is unknown for SoS functions, despite using Monte Carlo estimation.
- Why unresolved: The paper uses standard Monte Carlo sampling without exploring whether importance sampling or other strategies could improve convergence or reduce variance in the estimation process.
- What evidence would resolve it: Development of importance sampling strategies tailored to SoS functions, theoretical analysis of variance reduction techniques, or empirical comparison of different sampling strategies on benchmark problems.

## Limitations
- Limited empirical validation with only a few toy examples and one real-world application
- Computational complexity not thoroughly analyzed, particularly for high-dimensional problems
- Several technical assumptions stated without verification or discussion of practical implications

## Confidence
- **High confidence**: The theoretical framework connecting α-divergences, SoS densities, and convex optimization is sound and well-established in the literature.
- **Medium confidence**: The sequential transport map approach is a reasonable strategy, but its practical advantages over existing methods need more empirical validation.
- **Low confidence**: The claimed benefits of using unnormalized densities and the unified convergence analysis for all α-divergences are not thoroughly validated with experiments.

## Next Checks
1. **Numerical stability analysis**: Systematically test the method on a range of problems with varying dimensionality and complexity to assess numerical stability and scalability.
2. **Comparison with baseline methods**: Conduct a thorough comparison with existing density estimation methods (e.g., normalizing flows, score-based models) on benchmark datasets to quantify the practical benefits of the proposed approach.
3. **Investigation of failure modes**: Identify and analyze specific scenarios where the method may fail or perform poorly, such as highly multimodal distributions or distributions with complex dependencies.