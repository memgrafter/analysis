---
ver: rpa2
title: 'Fleximo: Towards Flexible Text-to-Human Motion Video Generation'
arxiv_id: '2411.19459'
source_url: https://arxiv.org/abs/2411.19459
tags:
- motion
- video
- videos
- skeleton
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fleximo, a new framework for generating human
  motion videos from text and reference images. Unlike prior methods that rely on
  extracted pose sequences from videos, Fleximo uses pre-trained text-to-3D motion
  models combined with a skeleton adapter and anchor-point rescaling to address scale
  and detail issues.
---

# Fleximo: Towards Flexible Text-to-Human Motion Video Generation

## Quick Facts
- arXiv ID: 2411.19459
- Source URL: https://arxiv.org/abs/2411.19459
- Reference count: 40
- Key outcome: Fleximo achieves MotionScore of 0.699, outperforming state-of-the-art text-conditioned image-to-video methods in identity fidelity, video quality, and motion accuracy.

## Executive Summary
Fleximo introduces a novel framework for generating human motion videos from text descriptions and reference images. Unlike previous methods that extract pose sequences from videos, Fleximo leverages pre-trained text-to-3D motion models combined with a skeleton adapter and anchor-point rescaling to address scale and detail issues. The system uses LLM planning for long motion sequences and a video refinement process to improve quality. Fleximo outperforms existing text-conditioned image-to-video methods in identity preservation, video quality, and motion accuracy.

## Method Summary
Fleximo is a text-to-human motion video generation framework that uses LLM planning to decompose natural language into motion segments, a text-to-motion model (T2M-GPT) to generate 3D meshes, an anchor-point rescaling module to align generated skeletons with reference images, a skeleton adapter to add realistic hand and face details, and a motion-to-video model (MimicMotion) to generate the final video with refinement. The method is trained on the HumanVid dataset and evaluated on a new MotionBench benchmark using MotionScore and standard metrics like PSNR, SSIM, and FID.

## Key Results
- MotionScore of 0.699 on MotionBench benchmark
- Outperforms state-of-the-art text-conditioned image-to-video methods
- Better identity fidelity and motion accuracy compared to existing approaches
- Introduces MotionBench benchmark with 400 videos across 20 identities and 20 motions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Anchor point based rescaling aligns the scale and proportions of the generated 2D skeleton with the reference image.
- Mechanism: The method first applies an affine transformation for rough alignment, then fixes the neck as an anchor and individually rescales connected body parts to match exact lengths.
- Core assumption: Affine transformations cannot perfectly match body part lengths between generated and reference skeletons.
- Evidence anchors:
  - [abstract] "To address the first problem, we propose an anchor point based rescale method that uses keypoints from the reference image and frame-by-frame affine transformations for scale adjustment."
  - [section 3.2] "Since affine transformations cannot fully match body part lengths, we further designate the neck as the anchor and individually adjust each connected keypoint, improving alignment with the reference image."
- Break condition: The method fails if the reference image is heavily occluded or if the pose differs significantly from the generated skeleton's default pose.

### Mechanism 2
- Claim: The skeleton adapter bridges the gap between text-to-motion models and motion-to-video models by adding realistic hand and face details.
- Mechanism: The adapter uses a latent video diffusion model with a U-Net, frozen CLIP for reference image encoding, and a PoseNet for handless pose video features to generate complete skeleton videos with hands.
- Core assumption: Hand and face details cannot be represented by the sparse keypoints generated by text-to-motion models.
- Evidence anchors:
  - [abstract] "To overcome these challenges, we introduce an anchor point based rescale method and design a skeleton adapter to fill in missing details and bridge the gap between text-to-motion and motion-to-video generation."
  - [section 3.3] "To solve this problem, we design a skeleton adapter to generate hand skeletons based on handless skeleton videos."
- Break condition: The method fails if the handless skeleton videos lack sufficient motion information for the adapter to generate realistic hands.

### Mechanism 3
- Claim: LLM planning enables generation of long motion sequences by decomposing natural language into discrete motion segments.
- Mechanism: The LLM processes long input text, breaking it into sequential motion segments that can be generated independently and then concatenated.
- Core assumption: Long motion sequences can be effectively divided into smaller, coherent segments that maintain temporal consistency when concatenated.
- Evidence anchors:
  - [abstract] "A large language model (LLM) is employed to decompose natural language into discrete motion sequences, enabling the generation of motion videos of any desired length."
  - [section 3.4] "Similar to RPG Diffusion [37], we use a template and ask an LLM to divide long input motion texts into smaller motion segments."
- Break condition: The method fails if the LLM cannot properly segment the text or if the transitions between segments are not smooth.

## Foundational Learning

- Concept: Affine transformations and their limitations in image alignment
  - Why needed here: Understanding why affine transformations alone are insufficient for precise skeleton alignment is crucial for grasping the need for the anchor point based rescaling.
  - Quick check question: Why can't affine transformations perfectly align the generated skeleton with the reference image?

- Concept: Latent diffusion models and their application in video generation
  - Why needed here: The skeleton adapter uses a latent video diffusion model, so understanding how these models work is essential for comprehending the adapter's functionality.
  - Quick check question: How does a latent video diffusion model differ from a standard diffusion model?

- Concept: Motion capture and skeletal animation
  - Why needed here: Understanding the basics of motion capture and skeletal animation is important for grasping the challenges in text-to-motion and motion-to-video generation.
  - Quick check question: What are the key differences between 3D mesh-based motion capture and 2D skeleton-based motion representation?

## Architecture Onboarding

- Component map: LLM planner -> T2M-GPT -> anchor point based rescale -> skeleton adapter -> MimicMotion -> video refinement
- Critical path: The critical path for generating a motion video is: LLM planning → text-to-motion generation → anchor point based rescaling → skeleton adapter → motion-to-video generation → video refinement.
- Design tradeoffs: The framework trades off training data requirements for computational complexity by leveraging pre-trained models and focusing on bridging the gap between them rather than training end-to-end.
- Failure signatures: Poor identity preservation, unrealistic hand movements, scale jitter, and lack of motion-text alignment are key failure signatures.
- First 3 experiments:
  1. Generate a simple motion video with a single, well-defined motion to verify basic functionality.
  2. Test the anchor point based rescaling by comparing the scale and proportions of the generated skeleton with the reference image.
  3. Evaluate the skeleton adapter by generating handless skeleton videos and checking if the adapter produces realistic hand movements.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the content, several areas warrant further investigation.

## Limitations
- Limited ability to generate large-scale movements like sports or dynamic positional changes
- Dependency on quality of pre-trained models (T2M-GPT and MimicMotion)
- Requirement for reference image limits applications where identity is unknown or irrelevant
- Potential failure with heavily occluded reference images or extreme pose differences

## Confidence
- **High confidence**: Anchor point rescaling improves scale alignment (supported by visual comparisons in paper)
- **Medium confidence**: Skeleton adapter generates realistic hand and face details (limited evaluation on hand diversity)
- **Low confidence**: LLM planning produces temporally coherent long sequences (no independent evaluation of segmentation quality)

## Next Checks
1. Evaluate anchor point rescaling performance with reference images containing occlusions, profile views, and extreme poses to establish failure boundaries
2. Test skeleton adapter generalization by generating hands in poses and orientations not present in training data
3. Measure LLM segmentation quality independently by evaluating coherence of generated segments before concatenation