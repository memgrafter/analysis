---
ver: rpa2
title: 'OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks with
  Reinforcement Fine-Tuning'
arxiv_id: '2412.16849'
source_url: https://arxiv.org/abs/2412.16849
tags:
- reasoning
- data
- policy
- samples
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OpenRFT is a framework for adapting reasoning foundation models
  to domain-specific tasks using reinforcement fine-tuning. It addresses the challenges
  of lacking reasoning step data and limited training samples by leveraging domain-specific
  samples through question augmentation, synthesizing reasoning-process data, and
  few-shot in-context learning.
---

# OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks with Reinforcement Fine-Tuning

## Quick Facts
- arXiv ID: 2412.16849
- Source URL: https://arxiv.org/abs/2412.16849
- Reference count: 9
- Primary result: Achieves 11% average performance gain over vanilla models using only 100 domain-specific samples per task

## Executive Summary
OpenRFT is a framework that adapts reasoning foundation models to domain-specific tasks through reinforcement fine-tuning. It addresses the challenges of limited training data and absence of reasoning step supervision by leveraging domain-specific samples through question augmentation, synthesizing reasoning-process data, and few-shot in-context learning. The framework uses a process reward model to supervise reasoning steps and incorporates data augmentation and knowledge embedding techniques. Evaluated on SciKnowEval, OpenRFT demonstrates significant performance improvements while requiring minimal domain-specific training data.

## Method Summary
OpenRFT adapts reasoning foundation models to domain-specific tasks by synthesizing reasoning process data from a stronger teacher model, performing supervised fine-tuning on this synthesized data, and then applying reinforcement learning with process reward model supervision. The framework incorporates data augmentation through question rewriting and option shuffling, and uses few-shot in-context learning to embed domain knowledge. The RL stage combines outcome-based rewards with process-based rewards from a process reward model to guide the policy model's exploration and optimization.

## Key Results
- Achieves 11% average performance gain over vanilla models
- Effective with only 100 domain-specific samples per task
- Synthesizing reasoning process data contributes 2.8% to overall performance
- Data augmentation provides 2.5% improvement
- Process reward model supervision stabilizes training by evaluating reasoning steps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthesizing reasoning step data through a stronger model addresses the absence of reasoning supervision in domain-specific samples.
- Mechanism: A stronger teacher model (e.g., QwQ-32B) is used to generate reasoning steps for domain-specific samples. This synthesized data is then used to fine-tune the policy model via SFT, providing it with domain-specific reasoning patterns.
- Core assumption: The teacher model can generate high-quality reasoning steps that align with the expected reasoning process for the domain-specific task.
- Evidence anchors:
  - [abstract] "OpenRFT addresses two key challenges of lacking reasoning step data and the limited quantity of training samples, by leveraging the domain-specific samples in three ways: question augmentation, synthesizing reasoning-process data, and few-shot ICL."
  - [section] "To enhance the understanding and response capabilities of a reasoning foundation model in specific domains, we adopt a teacher reasoning foundation model to synthesize reasoning process data and leverage this data for SFT of the reasoning foundational model."
- Break condition: If the teacher model's action space does not align with the student policy model, the synthesized data may degrade training performance, as observed in the experiments.

### Mechanism 2
- Claim: Reinforcement learning with process reward model (PRM) stabilizes training by providing feedback on reasoning steps, not just final answers.
- Mechanism: During RL, a PRM evaluates the rationality of each reasoning step, providing a process-based reward score. This reward is combined with the outcome-based reward to guide the policy model's exploration and optimization.
- Core assumption: The PRM can accurately assess the quality of reasoning steps, distinguishing correct reasoning processes from flawed ones, even when the final answer is correct.
- Evidence anchors:
  - [abstract] "OpenRFT addresses two key challenges of lacking reasoning step data and the limited quantity of training samples, by leveraging the domain-specific samples in three ways: question augmentation, synthesizing reasoning-process data, and few-shot ICL."
  - [section] "To mitigate the negative impact of this phenomenon to some extent, we integrate a process-based reward model ρPRM into the reward design of reinforcement learning."
- Break condition: If the PRM is not well-aligned with the policy model's action space, it may provide incorrect or misleading feedback, leading to suboptimal training.

### Mechanism 3
- Claim: Data augmentation and few-shot ICL address the limited quantity of domain-specific samples by increasing data diversity and embedding domain knowledge.
- Mechanism: Data augmentation involves rewriting questions and shuffling options to generate new samples. Few-shot ICL uses domain-specific samples as context to guide the policy model's exploration during RL.
- Core assumption: The augmented data retains the original meaning and difficulty of the domain-specific tasks, and the ICL context effectively steers the policy model towards relevant reasoning paths.
- Evidence anchors:
  - [abstract] "OpenRFT addresses two key challenges of lacking reasoning step data and the limited quantity of training samples, by leveraging the domain-specific samples in three ways: question augmentation, synthesizing reasoning-process data, and few-shot ICL."
  - [section] "For each training task, we construct a corresponding vector database. For each training sample Qi, we retrieve the top k most similar question-answer pairs..."
- Break condition: If data augmentation introduces significant noise or the ICL context is not well-curated, it may confuse the policy model or lead it astray.

## Foundational Learning

- Concept: Reinforcement Learning (RL)
  - Why needed here: RL is used to optimize the policy model's reasoning process through trial and error, guided by rewards from the PRM.
  - Quick check question: What is the difference between on-policy and off-policy RL, and which is used in OpenRFT?

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT is used to pre-adapt the policy model to domain-specific reasoning patterns using the synthesized reasoning process data.
  - Quick check question: How does SFT differ from RL in terms of training objective and data dependency?

- Concept: In-Context Learning (ICL)
  - Why needed here: ICL is used to embed domain knowledge into the policy model by providing relevant context during RL training.
  - Quick check question: What is the role of similarity search in ICL, and how does it contribute to the effectiveness of OpenRFT?

## Architecture Onboarding

- Component map:
  Data Augmentation -> Reasoning Process Synthesis -> SFT-based Imitation -> RL-based Exploration and Self-Improvement -> Process Reward Model (PRM) -> Few-shot ICL

- Critical path:
  1. Data augmentation to increase sample diversity
  2. Reasoning process synthesis to provide supervision
  3. SFT to pre-adapt the policy model
  4. RL with PRM and ICL to optimize the policy model

- Design tradeoffs:
  - Using a stronger teacher model for synthesis may lead to action space misalignment with the student policy model
  - Data augmentation may introduce noise if not carefully implemented
  - ICL may be ineffective if the context is not well-curated or relevant

- Failure signatures:
  - If the teacher model's action space does not align with the student policy model, SFT performance may degrade
  - If the PRM is not well-aligned with the policy model's action space, RL training may be unstable or suboptimal
  - If data augmentation introduces significant noise or ICL context is not well-curated, the policy model may be confused or led astray

- First 3 experiments:
  1. Test data augmentation on a small subset of domain-specific samples to assess its impact on sample diversity and quality
  2. Evaluate the effectiveness of the PRM in distinguishing correct reasoning processes from flawed ones
  3. Assess the contribution of ICL to the policy model's performance by comparing results with and without ICL

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of synthesized reasoning process data affect the final performance of OpenRFT?
- Basis in paper: [explicit] The paper mentions that the quality of synthesized reasoning process data is crucial for SFT-based imitation and that using a stronger teacher model (QwQ-32B) with inconsistent action spaces led to worse performance than self-distillation.
- Why unresolved: While the paper demonstrates that self-distillation is better than using a stronger model with misaligned action spaces, it does not explore other factors affecting the quality of synthesized data, such as prompt engineering, sampling strategies, or the impact of data noise.
- What evidence would resolve it: Experiments comparing different methods of synthesizing reasoning process data, including varying prompt complexity, sampling temperatures, and filtering strategies for selecting high-confidence data, would provide insights into the optimal approach for data synthesis.

### Open Question 2
- Question: What is the optimal balance between outcome rewards and process rewards in the reinforcement learning stage of OpenRFT?
- Basis in paper: [explicit] The paper mentions using a linear combination of outcome rewards and process rewards with a weighted coefficient (α) set to 0.7, but also notes that different aggregation strategies (mean, minimum, etc.) can be used.
- Why unresolved: The paper does not provide a systematic exploration of how different values of α or alternative reward aggregation strategies impact the final performance. Additionally, the effectiveness of process rewards may vary depending on the complexity of the task and the quality of the process reward model.
- What evidence would resolve it: A comprehensive ablation study varying the value of α and testing different reward aggregation strategies across multiple tasks would reveal the optimal balance between outcome and process rewards for different scenarios.

### Open Question 3
- Question: How does the inclusion of domain knowledge through few-shot ICL impact the efficiency and effectiveness of the RL exploration in OpenRFT?
- Basis in paper: [explicit] The paper introduces few-shot ICL to embed domain knowledge into the policy model and guide exploration, but notes that it led to a slight decrease in performance, possibly due to inconsistent prompts between SFT and RL stages.
- Why unresolved: The paper does not investigate the reasons behind the performance decrease or explore alternative methods for incorporating domain knowledge, such as using different retrieval strategies, adjusting the number of examples in the prompt, or exploring other knowledge embedding techniques.
- What evidence would resolve it: Experiments comparing different ICL configurations, including varying the number of examples, adjusting the similarity threshold for retrieval, and exploring alternative knowledge embedding methods, would determine the optimal approach for incorporating domain knowledge and its impact on RL efficiency and effectiveness.

## Limitations
- Framework effectiveness depends on availability of a stronger reasoning model for data synthesis
- Sensitive to alignment between teacher and student model action spaces
- Data augmentation requires careful implementation to avoid introducing semantic noise
- Process reward model effectiveness contingent on alignment with policy model's reasoning structure

## Confidence
- High confidence: The framework's overall approach and its effectiveness in leveraging limited domain-specific samples through multiple strategies
- Medium confidence: The specific contribution of each component (data augmentation, reasoning synthesis, ICL) to the overall performance gains
- Medium confidence: The generalizability of results across different reasoning foundation models and domain types

## Next Checks
1. Conduct ablation studies to isolate the individual contributions of data augmentation, reasoning process synthesis, and few-shot ICL to overall performance
2. Test the framework with multiple teacher-student model pairs to quantify the impact of action space misalignment on SFT effectiveness
3. Evaluate the framework's robustness to varying levels of data quality in the domain-specific samples to establish practical deployment thresholds