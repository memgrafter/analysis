---
ver: rpa2
title: 'Towards Reliable and Factual Response Generation: Detecting Unanswerable Questions
  in Information-Seeking Conversations'
arxiv_id: '2401.11452'
source_url: https://arxiv.org/abs/2401.11452
tags:
- answerability
- passages
- answer
- question
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting unanswerable questions
  in conversational information seeking systems. The authors propose a two-step process
  where relevant passages are first identified and then summarized, with an intermediate
  step to assess whether the answer to the user's question is present in the corpus.
---

# Towards Reliable and Factual Response Generation: Detecting Unanswerable Questions in Information-Seeking Conversations

## Quick Facts
- arXiv ID: 2401.11452
- Source URL: https://arxiv.org/abs/2401.11452
- Reference count: 0
- Key outcome: Simple sentence-level classification approach outperforms ChatGPT-3.5 on detecting unanswerable questions in conversational search

## Executive Summary
This paper addresses the challenge of detecting unanswerable questions in conversational information seeking systems. The authors propose a two-step process where relevant passages are first identified and then summarized, with an intermediate step to assess whether the answer to the user's question is present in the corpus. Their method employs a sentence-level classifier to detect if the answer is present, then aggregates these predictions on the passage level, and eventually across the top-ranked passages to arrive at a final answerability estimate. To train and evaluate this approach, the authors develop a dataset based on the TREC CAsT benchmark that includes answerability labels on the sentence, passage, and ranking levels. Their results demonstrate that this simple method provides a strong baseline, outperforming a state-of-the-art LLM on the answerability prediction task.

## Method Summary
The authors propose a two-step process for detecting unanswerable questions in conversational information seeking systems. First, relevant passages are retrieved and identified. Second, a sentence-level classifier is used to detect if the answer to the user's question is present in each sentence of the top-ranked passages. The predictions are then aggregated on the passage level using max or mean aggregation, and further on the ranking level to arrive at a final answerability estimate. The sentence-level classifier is trained using a BERT transformer model with a sequence classification head on top, and can be augmented with additional training examples from the SQuAD 2.0 dataset. The authors develop a dataset based on the TREC CAsT benchmark, named CAsT-answerability, which includes answerability labels on three levels: sentences, passages, and rankings.

## Key Results
- Simple sentence-level classification approach achieves up to 0.891 accuracy on sentence-level answerability detection
- The proposed method outperforms ChatGPT-3.5 on ranking-level answerability prediction (0.829 vs. 0.712 accuracy)
- Aggregating sentence-level predictions using max aggregation on the passage level followed by mean aggregation on the ranking level gives the best results

## Why This Works (Mechanism)
The approach works by breaking down the answerability detection task into manageable sub-tasks. By first classifying individual sentences, the model can focus on granular evidence of answerability before aggregating these predictions to higher levels. This hierarchical approach allows for more nuanced detection of when answers are truly absent versus when they might be present but distributed across multiple sentences or passages.

## Foundational Learning
- **BERT transformer model**: Pre-trained language model used as the base for sentence-level classification; needed because it provides strong contextual understanding of text; quick check: verify pre-trained weights are properly loaded
- **Sequence classification head**: Additional layer on top of BERT for binary classification tasks; needed to convert BERT's contextual embeddings into answerability predictions; quick check: ensure output layer has correct dimensions
- **Max and mean aggregation**: Methods for combining multiple predictions into a single score; needed to aggregate sentence-level predictions to passage and ranking levels; quick check: verify aggregation functions handle edge cases correctly

## Architecture Onboarding

### Component Map
BERT transformer -> Sentence classifier -> Max aggregation (passage level) -> Mean aggregation (ranking level) -> Final answerability prediction

### Critical Path
The critical path flows from the sentence-level classification through the aggregation layers to the final ranking-level prediction. Each stage must successfully complete before the next can proceed.

### Design Tradeoffs
- Simple aggregation vs. more complex fusion techniques
- Using pre-trained BERT vs. fine-tuning on the specific task
- Sentence-level granularity vs. passage-level or document-level classification

### Failure Signatures
- Poor sentence-level classifier performance will cascade through all higher-level predictions
- Incorrect threshold selection in aggregation steps can lead to systematic over/under-prediction
- Domain mismatch between training data (SQuAD 2.0) and target conversational queries may reduce effectiveness

### First Experiments
1. Train sentence-level classifier on CAsT-answerability dataset and evaluate on held-out test set
2. Implement and test both max and mean aggregation methods on passage-level predictions
3. Compare baseline BERT approach against ChatGPT-3.5 on ranking-level answerability prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does augmenting the CAsT-answerability dataset with SQuAD 2.0 data improve ranking-level answerability prediction in conversational search?
- Basis in paper: The authors observe that while sentence-level and passage-level answerability prediction benefits from augmented data, this does not translate to effective passage or ranking-level answerability prediction.
- Why unresolved: The paper does not provide a detailed analysis of why the augmented data does not improve ranking-level prediction. It only suggests that the focus on short-span answers in SQuAD 2.0 might mislead the classifier.
- What evidence would resolve it: Further analysis of the types of questions and answers in SQuAD 2.0 compared to CAsT-answerability, and how they affect the classifier's performance on ranking-level prediction.

### Open Question 2
- Question: How do different aggregation methods (max, mean, and potentially more advanced techniques) perform on ranking-level answerability prediction in conversational search?
- Basis in paper: The authors experiment with max and mean aggregation methods and find that max aggregation on the passage level followed by mean aggregation on the ranking level gives the best results.
- Why unresolved: The paper does not explore other aggregation methods or provide a detailed comparison of their performance. It only suggests that more advanced score- and/or content-based fusion techniques could be applied in the future.
- What evidence would resolve it: A comprehensive comparison of different aggregation methods on ranking-level answerability prediction, including more advanced techniques.

### Open Question 3
- Question: How do large language models (LLMs) like ChatGPT compare to the proposed baseline methods for ranking-level answerability prediction in conversational search?
- Basis in paper: The authors compare their baseline methods to ChatGPT-3.5 and find that their methods outperform ChatGPT on ranking-level answerability prediction.
- Why unresolved: The paper does not provide a detailed analysis of why ChatGPT performs worse than the proposed methods. It only suggests that ChatGPT has a limited ability to detect answerability without additional guidance.
- What evidence would resolve it: Further analysis of the types of questions and answers that ChatGPT struggles with, and how the proposed methods address these challenges.

## Limitations
- The dataset used (CAsT-answerability) is based on the TREC CAsT benchmark and may not fully represent real-world conversational information seeking scenarios
- The approach focuses on a two-step process and does not explicitly address the impact of passage retrieval quality on answerability detection performance
- The paper relies on a pre-trained BERT model without exploring the potential benefits of fine-tuning on the specific task of answerability detection

## Confidence
- High: The overall methodology and experimental setup are clearly described, and the results are presented with appropriate metrics and comparisons
- Medium: The dataset development process and the specific implementation details of the aggregation functions are not fully specified, which may limit the reproducibility of the results
- Low: The paper does not discuss the potential limitations and assumptions of the proposed approach, such as the impact of passage retrieval quality and the generalizability to other domains

## Next Checks
1. Evaluate the proposed approach on additional datasets from different domains and with varying levels of complexity to assess its generalizability and robustness
2. Investigate the impact of passage retrieval quality on the answerability detection performance by conducting experiments with different retrieval models or by introducing controlled noise in the retrieved passages
3. Explore the potential benefits of fine-tuning the BERT transformer model on the specific task of answerability detection and compare the results with the proposed approach that relies on a pre-trained model