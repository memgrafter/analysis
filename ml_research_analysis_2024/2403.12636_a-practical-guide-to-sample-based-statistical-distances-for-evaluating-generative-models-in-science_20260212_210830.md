---
ver: rpa2
title: A Practical Guide to Sample-based Statistical Distances for Evaluating Generative
  Models in Science
arxiv_id: '2403.12636'
source_url: https://arxiv.org/abs/2403.12636
tags:
- distance
- data
- learning
- c2st
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides a practical guide to sample-based statistical\
  \ distances for evaluating generative models across scientific domains. The authors\
  \ focus on four commonly used distances\u2014Sliced-Wasserstein (SW), Classifier\
  \ Two-Sample Tests (C2ST), Maximum Mean Discrepancy (MMD), and Fr\xE9chet Inception\
  \ Distance (FID)\u2014each representing a different methodological approach to comparing\
  \ distributions."
---

# A Practical Guide to Sample-based Statistical Distances for Evaluating Generative Models in Science

## Quick Facts
- arXiv ID: 2403.12636
- Source URL: https://arxiv.org/abs/2403.12636
- Reference count: 40
- Primary result: Sample-based statistical distances (SW, C2ST, MMD, FID) provide complementary evaluation of generative models, with performance depending on sample size, dimensionality, and data modality

## Executive Summary
This practical guide addresses the challenge of evaluating generative models in scientific applications by examining four commonly used sample-based statistical distances. The authors systematically investigate how Sliced-Wasserstein, Classifier Two-Sample Tests, Maximum Mean Discrepancy, and Fréchet Inception Distance perform across different data characteristics and model types. Through both synthetic experiments and real-world applications in neuroscience and medical imaging, they demonstrate that different distances capture distinct aspects of distributional similarity. The guide provides concrete recommendations for choosing appropriate metrics based on data properties and evaluation goals, emphasizing the importance of using multiple complementary measures for robust model assessment.

## Method Summary
The paper implements and compares four statistical distance measures (SW, C2ST, MMD, FID) to evaluate generative models in scientific domains. The method involves generating synthetic samples from models, comparing them to real data using statistical distances, and using multiple complementary metrics for comprehensive evaluation. The approach includes systematic scaling experiments varying sample size and dimensionality, analysis of embedding network choices for structured data, and application to real scientific problems including primate decision-making models and chest X-ray image generation. Implementation requires obtaining the four statistical distance measures, datasets (primate decision times and chest X-ray images or similar), and generative models for scientific applications.

## Key Results
- Different statistical distances (SW, C2ST, MMD, FID) yield varying results on the same data, demonstrating complementary strengths and sensitivities
- SW and MMD show better scaling properties in high dimensions compared to C2ST, while FID can be biased for small sample sizes
- In real applications, each distance captures different aspects of model performance - SW detects overall distributional shape, C2ST identifies class separability, MMD measures kernel-based similarity, and FID evaluates deep feature distributions

## Why This Works (Mechanism)

### Mechanism 1
Different sample-based distances capture different aspects of the distribution, so using multiple distances avoids over-reliance on a single sensitivity or failure mode. Each distance is based on a different embedding or projection (e.g., linear slices for SW, classifier outputs for C2ST, kernel means for MMD, deep features for FID). Because these embeddings highlight different features, one distance may detect a subtle shift that others miss, while another may be more sensitive to overall shape or mode coverage. The core assumption is that the generative model's error is not uniformly distributed across all features captured by the embeddings; otherwise all distances would agree.

### Mechanism 2
Sample size and dimensionality affect each distance differently, so scaling experiments guide the choice of metric for a given dataset. SW and MMD have explicit sample complexity bounds and are less sensitive to high dimensionality for well-chosen hyperparameters, while C2ST becomes overly sensitive in high dimensions and FID can be biased for small sample sizes. The core assumption is that the practitioner can estimate dataset size and dimensionality before choosing a metric.

### Mechanism 3
Using well-established embedding networks (e.g., InceptionV3 for images) provides semantically meaningful distance estimates for structured data. The network acts as a feature extractor that maps high-dimensional data into a lower-dimensional space where simpler distances (e.g., Gaussian Wasserstein) are meaningful; the network's training objective ensures these features align with human perception or domain relevance. The core assumption is that the embedding network is pretrained on a dataset whose statistics are representative of the target data distribution.

## Foundational Learning

- Concept: Wasserstein distance as optimal transport cost
  - Why needed here: SW is a sliced approximation; understanding the base Wasserstein metric explains why SW is a valid lower bound
  - Quick check question: In the Earth Mover's analogy, what is the "cost" being minimized?

- Concept: Kernel trick and characteristic kernels
  - Why needed here: MMD relies on implicit feature maps via kernels; knowing which kernels are characteristic ensures injective embeddings
  - Quick check question: If a kernel is not characteristic, what can happen to MMD values?

- Concept: Classifier two-sample test and binary classification accuracy
  - Why needed here: C2ST interprets distribution similarity as classifier performance; knowing how accuracy maps to distributional overlap is critical
  - Quick check question: What C2ST value indicates that the two datasets are indistinguishable?

## Architecture Onboarding

- Component map: Data → Embedding network (optional) → Distance computation (SW, C2ST, MMD, or FID) → Summary statistics → Model comparison
- Critical path: Load datasets → Embed if needed → Compute distance → Repeat for multiple subsets → Aggregate results
- Design tradeoffs: SW is fastest but may miss low-dimensional differences; C2ST is interpretable but computationally heavy; MMD is flexible but sensitive to kernel choice; FID is domain-specific but biased
- Failure signatures: SW variance high with few projections; C2ST accuracy <0.5 due to duplicates; MMD near zero for inappropriate bandwidth; FID unstable with small sample size
- First 3 experiments:
  1. Compare two small synthetic Gaussian datasets with all four distances to see scaling and sensitivity
  2. Evaluate the same real chest X-ray datasets using both InceptionV3 and CheXzero embeddings to compare FID results
  3. Run C2ST with different classifiers (MLP vs. ResNet) on the same 2D toy data to observe sensitivity to classifier choice

## Open Questions the Paper Calls Out

### Open Question 1
How do different kernel choices in MMD affect its performance across various data modalities and dimensionality ranges? The paper discusses MMD's dependence on kernel selection and shows sensitivity to bandwidth parameters through experiments, but only tests a few kernels (Gaussian, linear, energy-based) and doesn't systematically explore the full space of possible kernels for different data types.

### Open Question 2
What is the optimal number of slices for SW distance when comparing high-dimensional distributions with subtle differences? The paper mentions that SW may require many slices when distributions differ along small subsets of directions, but doesn't provide concrete guidelines or establish how many slices are needed for reliable detection in different scenarios.

### Open Question 3
How can we determine when C2ST becomes oversensitive to minor variations in high-dimensional data, making it practically useless for evaluation? While the paper illustrates C2ST's high sensitivity through examples and shows it can remain very high even for seemingly good generative models in high dimensions, it doesn't provide quantitative thresholds or guidelines for when this oversensitivity becomes problematic.

## Limitations
- Lack of theoretical bounds on combined metric performance when using multiple distances together
- Limited exploration of high-dimensional curse effects beyond tested sample size and dimensionality ranges
- Dependence on quality of pretrained embedding networks without systematic comparison across different architectures

## Confidence
Our findings about the complementary nature of statistical distances (High confidence) are supported by direct experiments, though the claim that no single metric suffices remains partly theoretical since we did not exhaustively test pathological cases where one distance might dominate. The scaling analysis (Medium confidence) is based on controlled synthetic experiments, but real-world data often violates the assumed distributional properties. The embedding network claims (Medium confidence) are empirically validated on specific datasets but lack systematic comparison across different embedding architectures.

## Next Checks
1. Test all four distances on a known pathological generative model where one metric clearly fails while others succeed
2. Systematically vary embedding network architecture choices on the same dataset to quantify sensitivity
3. Evaluate metric performance on datasets with known multi-modal structure to test mode coverage detection