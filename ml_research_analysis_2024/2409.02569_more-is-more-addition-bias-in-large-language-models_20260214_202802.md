---
ver: rpa2
title: 'More is More: Addition Bias in Large Language Models'
arxiv_id: '2409.02569'
source_url: https://arxiv.org/abs/2409.02569
tags:
- bias
- additive
- responses
- addition
- ingredients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigated the presence of addition bias in Large\
  \ Language Models (LLMs), drawing parallels to the cognitive bias observed in humans\
  \ where individuals tend to favor additive over subtractive changes. Through a series\
  \ of controlled experiments across various tasks, we tested several prominent LLMs,\
  \ including GPT-3.5 Turbo, Claude 3.5 Sonnet, Mistral, Math\u03A3tral, and Llama\
  \ 3.1."
---

# More is More: Addition Bias in Large Language Models

## Quick Facts
- arXiv ID: 2409.02569
- Source URL: https://arxiv.org/abs/2409.02569
- Reference count: 0
- Primary result: Significant addition bias observed across multiple LLM families in controlled experiments

## Executive Summary
This study investigates addition bias in Large Language Models (LLMs), where models demonstrate a systematic preference for additive over subtractive changes. The research tested several prominent LLMs including GPT-3.5 Turbo, Claude 3.5 Sonnet, Mistral, MathÎ£tral, and Llama 3.1 across various controlled tasks. Results consistently showed a significant tendency for models to favor adding elements rather than removing them, mirroring cognitive biases observed in humans. This finding has important implications for how LLMs approach problem-solving, decision-making, and content generation tasks.

The presence of addition bias could lead to unnecessarily complex solutions and inefficiencies in LLM applications. While the bias was consistent across most tasks and models tested, some exceptions were noted, particularly in text summarization tasks where certain models showed a preference for reduction. These findings suggest that model developers should consider this inherent bias when designing and deploying LLMs for practical applications.

## Method Summary
The study employed controlled experiments across multiple tasks to test for addition bias in various LLMs. Researchers presented models with scenarios requiring either additive or subtractive changes and analyzed their responses. Tasks included palindrome modification, Lego tower construction, recipe modification, and text summarization. The experiments systematically varied parameters such as the number of ingredients or complexity levels to assess whether bias persisted across different contexts. Multiple model families were tested to establish the prevalence of this phenomenon across different architectures and training approaches.

## Key Results
- Across all tested models and tasks, LLMs demonstrated a significant preference for additive changes over subtractive ones
- In palindrome tasks, models like GPT-3.5 Turbo and Claude 3.5 Sonnet strongly preferred adding letters rather than removing them
- Most models favored adding a brick to the shorter tower rather than removing one from the taller tower in Lego construction tasks
- Addition bias persisted even in complex scenarios like modifying recipes with unusual ingredients
- Some models showed exceptions to the bias, particularly in text summarization where reduction was preferred over addition

## Why This Works (Mechanism)
The study did not provide a detailed mechanism explanation for why addition bias occurs in LLMs, as the focus was on empirical observation rather than theoretical explanation of the underlying causes.

## Foundational Learning

**Additive vs Subtractive Problem Solving**
- Why needed: Understanding the fundamental cognitive bias that addition bias represents
- Quick check: Can identify examples of additive and subtractive solutions in everyday problems

**LLM Task Evaluation Methods**
- Why needed: Framework for designing controlled experiments to test model behavior
- Quick check: Can design simple A/B tests to compare model preferences

**Cognitive Biases in AI Systems**
- Why needed: Context for understanding how human cognitive biases manifest in AI
- Quick check: Can explain at least three known cognitive biases in AI systems

## Architecture Onboarding

**Component Map**
Input Task -> Model Processing -> Output Generation -> Bias Analysis

**Critical Path**
Task presentation -> Model response generation -> Response classification (additive/subtractive) -> Statistical analysis of preferences

**Design Tradeoffs**
- Controlled experiments provide clear causal evidence but may not reflect real-world complexity
- Testing multiple models increases generalizability but requires more resources
- English-only tasks ensure consistency but limit cross-linguistic insights

**Failure Signatures**
- Models showing equal preference for addition and subtraction may indicate task ambiguity
- Unexpected subtractive preferences could suggest task-specific training effects
- Inconsistent results across similar tasks may indicate model sensitivity to prompt phrasing

**First Experiments**
1. Test the same models on a simple arithmetic task requiring either addition or subtraction
2. Present models with a scenario where subtraction is clearly more efficient than addition
3. Compare model responses across different prompt formulations of the same task

## Open Questions the Paper Calls Out
The study did not identify specific open questions for future research.

## Limitations
- Limited to English-language tasks, raising questions about cross-linguistic generalizability
- Focused on specific model families, may not apply to all LLM architectures
- Controlled experimental conditions may not capture real-world complexity

## Confidence

High:
- Core observation of addition bias in tested models and tasks
- Consistency of findings across multiple model families
- Statistical significance of additive preferences

Medium:
- Claims about universality of addition bias across all LLMs
- Applicability of findings to real-world applications
- Generalizability to languages beyond English

## Next Checks
1. Test additional model architectures beyond the current selection to establish broader generalizability
2. Conduct experiments with multilingual tasks to assess cross-linguistic variations in addition bias
3. Implement longitudinal studies to examine whether addition bias persists or changes as models are updated and fine-tuned