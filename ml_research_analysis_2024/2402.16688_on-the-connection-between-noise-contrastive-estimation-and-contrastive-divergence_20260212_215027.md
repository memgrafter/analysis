---
ver: rpa2
title: On the connection between Noise-Contrastive Estimation and Contrastive Divergence
arxiv_id: '2402.16688'
source_url: https://arxiv.org/abs/2402.16688
tags:
- cnce
- rnce
- data
- distribution
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper establishes a direct connection between two classes
  of unnormalised model estimation methods: noise-contrastive estimation (NCE) and
  contrastive divergence (CD). It shows that ranking NCE (RNCE) is equivalent to maximum
  likelihood estimation using conditional importance sampling (CIS), while both RNCE
  and conditional NCE (CNCE) are special cases of CD with specific MCMC kernels.'
---

# On the connection between Noise-Contrastive Estimation and Contrastive Divergence

## Quick Facts
- arXiv ID: 2402.16688
- Source URL: https://arxiv.org/abs/2402.16688
- Reference count: 40
- This paper establishes a direct connection between two classes of unnormalised model estimation methods: noise-contrastive estimation (NCE) and contrastive divergence (CD).

## Executive Summary
This paper reveals fundamental connections between noise-contrastive estimation (NCE) and contrastive divergence (CD), two major approaches for training unnormalised probabilistic models. The authors show that ranking NCE (RNCE) is mathematically equivalent to maximum likelihood estimation using conditional importance sampling, while both RNCE and conditional NCE (CNCE) can be viewed as special cases of CD with specific Markov chain Monte Carlo kernels. This theoretical framework provides new insights into NCE methods and suggests practical improvements like adaptive proposal distributions and persistent sampling strategies. Empirical results on autoregressive energy-based models demonstrate that these insights can lead to improved performance compared to traditional NCE approaches.

## Method Summary
The paper connects NCE methods to contrastive divergence by showing that RNCE corresponds to maximum likelihood estimation with conditional importance sampling, and both RNCE and CNCE are special cases of CD with specific MCMC kernels. The key insight is that when the noise distribution q is chosen to resemble the model distribution pθ rather than the data distribution pd, NCE methods provide unbiased gradient estimates. The authors propose several extensions based on this connection, including adaptive proposal distributions that evolve towards pθ during training, persistent sampling strategies, and MCMC variants like Metropolis-Hastings. These extensions are tested on autoregressive energy-based models using datasets like Power, Gas, Hepmass, Miniboone, and BSDS300, with performance evaluated using test log-likelihoods and Wasserstein distances.

## Key Results
- RNCE is mathematically equivalent to maximum likelihood estimation using conditional importance sampling
- Both RNCE and CNCE are special cases of CD-1 with specific pθ-invariant MCMC kernels
- Adaptive proposal distributions that target pθ rather than pd lead to improved performance on autoregressive EBMs
- The theoretical framework provides justification for choosing the model distribution as the noise distribution in NCE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RNCE can be interpreted as maximum likelihood estimation using conditional importance sampling (CIS)
- Mechanism: By conditioning on a data sample x₀ in the importance sampling procedure, RNCE implicitly performs CIS, which provides an unbiased gradient estimate of the log-normalization constant under the idealized condition that x₀ comes from the model distribution pθ
- Core assumption: The conditioned sample x₀ ~ pθ, making the CIS estimator unbiased for ∇θ log Zθ
- Evidence anchors:
  - [abstract]: "RNCE is equivalent to ML estimation combined with conditional importance sampling"
  - [section]: "RNCE corresponds to an ML criterion based on conditional importance sampling (CIS)"
  - [corpus]: Weak evidence - corpus contains related NCE and IS papers but no direct CIS-RNCE connections
- Break condition: If q does not cover the support of pθ, the CIS procedure fails and the gradient estimate becomes biased

### Mechanism 2
- Claim: Both RNCE and CNCE are special cases of contrastive divergence (CD) with specific MCMC kernels
- Mechanism: RNCE uses a kernel based on CIS sampling, while CNCE uses a kernel similar to Metropolis-Hastings but with a different acceptance probability. Both kernels are pθ-invariant, making RNCE and CNCE equivalent to CD-1.
- Core assumption: The constructed kernels are pθ-invariant (satisfy detailed balance or similar conditions)
- Evidence anchors:
  - [abstract]: "both RNCE and CNCE are special cases of CD with specific MCMC kernels"
  - [section]: "we connect RNCE and CNCE to the family of CD methods" and "kernel Kθ(x′|x0) is known to be pθ-invariant"
  - [corpus]: Weak evidence - corpus contains CD and NCE papers but no explicit kernel constructions showing this connection
- Break condition: If the kernels are not properly constructed to be pθ-invariant, the equivalence to CD breaks down

### Mechanism 3
- Claim: For optimal learning, the noise distribution q should resemble the model distribution pθ rather than the data distribution pd
- Mechanism: When q = pθ, both RNCE and CNCE provide unbiased gradient estimates of the negative log-likelihood up to a constant scaling factor. This makes the learning process more stable and efficient.
- Core assumption: The model distribution pθ is a good approximation of the data distribution pd during training
- Evidence anchors:
  - [abstract]: "motivation why, for optimal learning, the noise distribution should resemble the model distribution, and not the data distribution"
  - [section]: "our interpretation of both RNCE and CNCE as special cases of CD-1... suggests that we should choose q as close as possible to pθ"
  - [corpus]: Weak evidence - corpus contains papers on adaptive proposals and NCE but no direct theoretical justification for q ≈ pθ
- Break condition: If pθ diverges significantly from pd, using q = pθ may lead to poor exploration of the data distribution and suboptimal learning

## Foundational Learning

- Concept: Importance sampling and self-normalized importance sampling
  - Why needed here: Understanding how IS approximates the normalization constant Zθ and its biased gradient estimates is crucial for grasping why RNCE (as CIS) provides an advantage
  - Quick check question: What is the difference between the IS estimate of Zθ and its gradient? Why is the gradient biased?

- Concept: Markov chain Monte Carlo (MCMC) and detailed balance
  - Why needed here: The construction of pθ-invariant kernels for RNCE and CNCE relies on MCMC principles and detailed balance conditions
  - Quick check question: What conditions must a transition kernel satisfy to be pθ-invariant? How does detailed balance relate to this?

- Concept: Contrastive divergence and its gradient approximation
  - Why needed here: The core connection between NCE methods and CD, and how the gradient of the NCE criteria can be interpreted as CD gradients with specific kernels
  - Quick check question: How does CD approximate the gradient of the log-likelihood? What is the role of the MCMC kernel in this approximation?

## Architecture Onboarding

- Component map:
  - Data sample x₀ ~ pd(·)
  - Noise samples x₁:J ~ q(·)
  - Model pθ(x) = ẽpθ(x)/Zθ
  - Proposal distribution q (design choice)
  - MCMC kernels Kθ (constructed from CIS or MH-like methods)
  - Learning objective (RNCE, CNCE, or their extensions)

- Critical path:
  1. Sample x₀ from data distribution pd
  2. Sample x₁:J from proposal q
  3. Compute weights wⱼ = ẽpθ(xⱼ)/q(xⱼ) (normalized appropriately)
  4. Calculate gradient of NCE criterion using these weights
  5. Update model parameters θ using the gradient

- Design tradeoffs:
  - Choice of q: q ≈ pd (harder classification, traditional NCE) vs. q ≈ pθ (easier classification, CD interpretation, potentially better gradients)
  - Number of noise samples J: More samples reduce variance but increase computational cost
  - MCMC kernel design: Simple kernels (CIS-based) vs. more sophisticated ones (MH-based) for better mixing

- Failure signatures:
  - Divergence or instability in training: May indicate poor choice of q or kernel
  - Slow convergence: Could be due to insufficient noise samples or suboptimal kernel
  - Poor final performance: Might suggest the model distribution pθ is not well-approximating pd

- First 3 experiments:
  1. Implement RNCE with different choices of q (pd vs. pθ) on a simple 2D dataset to observe the impact on convergence
  2. Compare RNCE and CNCE on a small energy-based model to verify their equivalence to CD-1 with different kernels
  3. Test the proposed adaptive proposal strategy (qϕ → pθ) on a moderately sized dataset to validate the theoretical justification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of MCMC kernel affect the performance of NCE methods when extended beyond CD-1 to CD-k with multiple steps?
- Basis in paper: [explicit] The paper mentions that extending NCE methods to CD-k (multiple MCMC steps) is a natural extension but does not provide experimental results or theoretical analysis of how different kernels impact performance.
- Why unresolved: The paper only briefly mentions this extension and leaves detailed investigation for future work. The effect of kernel choice on convergence and sample quality in multi-step settings remains unexplored.
- What evidence would resolve it: Experimental comparisons of NCE methods with different MCMC kernels (e.g., Gibbs sampling, Hamiltonian Monte Carlo) extended to CD-k, measuring convergence speed, sample quality, and computational efficiency.

### Open Question 2
- Question: What is the optimal way to balance the trade-off between proposal distribution similarity to the model distribution and data distribution in NCE methods?
- Basis in paper: [explicit] The paper argues that the proposal distribution should resemble the model distribution rather than the data distribution, but acknowledges this creates a chicken-and-egg problem since the model is unknown.
- Why unresolved: While the paper proposes adapting the proposal distribution towards the model, it doesn't provide a theoretical framework for determining the optimal balance or practical guidelines for implementation.
- What evidence would resolve it: Theoretical analysis of the bias-variance tradeoff in NCE methods as a function of proposal distribution choice, coupled with empirical studies comparing different adaptation strategies.

### Open Question 3
- Question: How do NCE methods perform on high-dimensional data compared to other unnormalized model estimation techniques?
- Basis in paper: [explicit] The paper mentions that ML-IS becomes highly unstable for high-dimensional datasets like Miniboone and BSDS300, but doesn't provide comprehensive comparisons with other methods.
- Why unresolved: The paper only briefly touches on this limitation of ML-IS and mentions omitting results for these datasets, but doesn't explore how NCE methods scale with dimensionality or compare their performance to other techniques.
- What evidence would resolve it: Systematic experiments comparing NCE methods, contrastive divergence, and other unnormalized model estimation techniques across a range of high-dimensional datasets, measuring performance metrics like log-likelihood and computational efficiency.

## Limitations
- The paper assumes the model distribution pθ adequately covers the data support, which may fail in practice
- Computational overhead from MCMC sampling in CD-like methods is not thoroughly analyzed
- Limited empirical validation on complex, non-autoregressive models and very high-dimensional data

## Confidence
- Theoretical connection (RNCE=CIS): Medium - mathematically grounded but with weak empirical validation
- Practical advantages of adaptive proposals: Low-to-medium - modest improvements shown on standard benchmarks
- Scalability to high-dimensional data: Low - paper acknowledges limitations but doesn't provide comprehensive analysis

## Next Checks
1. Test the proposed adaptive proposal strategy on more complex, non-autoregressive EBMs to validate scalability
2. Conduct systematic ablation studies on proposal design choices (q ≈ pd vs. q ≈ pθ) across diverse model classes
3. Compare NCE methods with modern score-based/diffusion alternatives in terms of both performance and computational efficiency on high-dimensional datasets