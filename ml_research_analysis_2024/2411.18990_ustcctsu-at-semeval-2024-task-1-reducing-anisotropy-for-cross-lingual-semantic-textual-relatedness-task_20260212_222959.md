---
ver: rpa2
title: 'USTCCTSU at SemEval-2024 Task 1: Reducing Anisotropy for Cross-lingual Semantic
  Textual Relatedness Task'
arxiv_id: '2411.18990'
source_url: https://arxiv.org/abs/2411.18990
tags:
- language
- data
- semantic
- whitening
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the cross-lingual semantic textual relatedness
  task by tackling two main challenges: the anisotropy of sentence embeddings from
  pre-trained language models and the curse of multilingualism. The authors propose
  a method that combines whitening techniques to transform anisotropic sentence vectors
  into isotropic ones, improving semantic similarity measurement.'
---

# USTCCTSU at SemEval-2024 Task 1: Reducing Anisotropy for Cross-lingual Semantic Textual Relatedness Task

## Quick Facts
- arXiv ID: 2411.18990
- Source URL: https://arxiv.org/abs/2411.18990
- Reference count: 9
- Primary result: 2nd place in Spanish and 3rd place in Indonesian at SemEval-2024 Task 1 using whitening and data filtering

## Executive Summary
This paper addresses the cross-lingual semantic textual relatedness task by tackling two main challenges: the anisotropy of sentence embeddings from pre-trained language models and the curse of multilingualism. The authors propose a method that combines whitening techniques to transform anisotropic sentence vectors into isotropic ones, improving semantic similarity measurement. Additionally, they design a data filtering approach to mitigate the negative effects of multilingualism by selectively excluding languages that do not enhance the target language's performance. Experiments on SemEval-2024 Task 1 show that their method achieves 2nd place in Spanish and 3rd place in Indonesian, with significant improvements in Spearman correlation coefficients.

## Method Summary
The approach uses XLM-RoBERTa-base as the encoder, applying whitening transformation to reduce anisotropy in sentence embeddings. The whitening module transforms sentence vectors from anisotropic to isotropic distributions by applying a linear transformation that makes the covariance matrix identity. A data filtering method is implemented to exclude languages that negatively impact target language performance, using Spearman correlation between gold and predicted labels as the selection criterion. The system processes sentence pairs through the encoder, applies whitening, computes cosine similarity between whitened embeddings, and outputs relatedness scores.

## Key Results
- Achieved 2nd place in Spanish and 3rd place in Indonesian at SemEval-2024 Task 1
- Whitened embeddings improved Spearman correlation by 0.0511 compared to non-whitened embeddings for Spanish
- Data filtering improved Spearman correlation from 0.6375 to 0.6886 for Spanish by excluding languages that decreased performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Whitening transforms anisotropic sentence vectors into isotropic ones, improving semantic similarity measurement.
- Mechanism: Whitening applies a linear transformation to sentence embeddings, making their covariance matrix identity. This spreads vectors more evenly across the space, reducing the clustering that occurs with anisotropic embeddings. When vectors are more isotropically distributed, cosine similarity becomes a more reliable metric for semantic relatedness.
- Core assumption: The anisotropic distribution of sentence embeddings from pre-trained language models is the primary cause of poor semantic similarity measurement with cosine similarity.
- Evidence anchors:
  - [abstract]: "we choose the XLM-R-base as our base model and use pre-trained sentence representations based on whitening to reduce anisotropy"
  - [section 3.3]: Detailed mathematical explanation of whitening transformation including covariance matrix manipulation and Singular Value Decomposition
  - [corpus]: Weak evidence - corpus contains related papers on semantic textual relatedness but no direct mention of whitening or anisotropy reduction
- Break condition: If the sentence embeddings are already approximately isotropic, whitening would provide minimal benefit and could potentially degrade performance by unnecessarily altering the embedding space.

### Mechanism 2
- Claim: Selective data filtering based on language interactions improves cross-lingual performance by mitigating the curse of multilingualism.
- Mechanism: The approach tests each language's impact on the target language by training with and without that language, then measuring performance changes. Languages that decrease performance when included are excluded from the training set. This creates an optimized multilingual training set for each target language.
- Core assumption: There is mutual interdependence between languages, and some language combinations are detrimental to cross-lingual performance beyond a certain point.
- Evidence anchors:
  - [abstract]: "we design a delicate data filtering method to alleviate the curse of multilingualism"
  - [section 3.4]: Experimental evidence showing improved performance (0.6886 vs 0.6375 Spearman coefficient for Spanish) when using filtered data versus unfiltered data
  - [corpus]: Weak evidence - corpus papers discuss semantic textual relatedness but don't specifically address language filtering or curse of multilingualism
- Break condition: If the relationship between languages is not mutual or if performance improvements are due to chance rather than systematic effects, the filtering method may not generalize or could lead to overfitting.

### Mechanism 3
- Claim: XLM-R-base provides superior multilingual representation capabilities compared to other models like mBERT.
- Mechanism: XLM-R-base uses larger-scale pre-training data and RoBERTa improvements, including dynamic masking and adversarial training for cross-lingual consistency. This results in better semantic understanding across languages.
- Core assumption: Model architecture and training scale directly impact multilingual performance in semantic textual relatedness tasks.
- Evidence anchors:
  - [section 3.2]: Comparative analysis showing XLM-R-base outperforms mBERT, XLM, and even XLM-R-large in their experiments
  - [section 5]: Results table showing XLM-R-base-whitening achieving 0.6886 Spearman coefficient for Indonesian compared to lower scores for other models
  - [corpus]: Weak evidence - related papers focus on semantic textual relatedness but don't provide direct comparisons of multilingual models
- Break condition: If the task domain or language pairs differ significantly from the pre-training data, the model's multilingual capabilities may not transfer effectively.

## Foundational Learning

- Concept: Anisotropy in sentence embeddings
  - Why needed here: Understanding why whitening is necessary requires grasping how BERT-based models produce vectors clustered in narrow regions of space
  - Quick check question: What geometric property of sentence embeddings causes cosine similarity to become unreliable for measuring semantic relatedness?

- Concept: Cross-lingual representation learning
  - Why needed here: The task requires mapping text from different languages into a shared semantic space for similarity calculation
  - Quick check question: How do multilingual models like XLM-R-base create representations that can be compared across languages?

- Concept: Curse of multilingualism
  - Why needed here: Explains why simply adding more languages to training data can degrade performance beyond a certain point
  - Quick check question: Why might adding a new language to a multilingual model's training data sometimes decrease performance on existing languages?

## Architecture Onboarding

- Component map: Input sentence pairs -> XLM-R-base encoder -> Whitening module -> Cosine similarity calculation -> Output relatedness score
- Critical path: Input -> XLM-R-base -> Whitening -> Similarity calculation -> Output
- Design tradeoffs: The system trades computational complexity (whitening calculations, data filtering experiments) for improved accuracy. The whitening transformation adds preprocessing overhead but enables more reliable similarity measurement.
- Failure signatures: If whitening parameters are incorrectly calculated, vectors may become overly dispersed or compressed. Poor data filtering choices can lead to missing beneficial languages or including detrimental ones.
- First 3 experiments:
  1. Test baseline XLM-R-base without whitening on a small validation set to establish performance baseline
  2. Apply whitening to the same validation set and measure improvement in cosine similarity reliability
  3. Run data filtering experiments to identify which languages positively impact target language performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the whitening module specifically affect the anisotropy of sentence embeddings in XLM-Rbase, and what are the mathematical properties of the transformation?
- Basis in paper: [explicit] The paper discusses the use of whitening techniques to transform sentence vectors from anisotropic to isotropic, detailing the mathematical transformation in section 3.3.
- Why unresolved: While the paper describes the transformation process, it does not provide empirical evidence or a detailed analysis of how this transformation affects the distribution of sentence embeddings.
- What evidence would resolve it: Experimental results showing the distribution of sentence embeddings before and after whitening, along with statistical measures of anisotropy, would clarify the impact of the whitening module.

### Open Question 2
- Question: What are the specific criteria used in the data filtering method to determine which languages are excluded from the training set, and how does this impact the performance on the target language?
- Basis in paper: [explicit] The paper mentions a data filtering method that uses the Spearman correlation between gold and predicted labels to exclude languages that do not enhance performance, as detailed in section 3.4.
- Why unresolved: The paper does not provide a detailed explanation of the criteria or thresholds used to exclude languages, nor does it analyze the impact of this filtering on model performance.
- What evidence would resolve it: A detailed analysis of the filtering criteria, along with comparative performance metrics with and without data filtering, would provide insights into its effectiveness.

### Open Question 3
- Question: How does the curse of multilingualism manifest in cross-lingual tasks, and what are the underlying mechanisms that cause performance to decline when more languages are added to the training set?
- Basis in paper: [explicit] The paper references the curse of multilingualism and its impact on performance, citing Conneau et al. (2020) in section 1.
- Why unresolved: The paper does not explore the theoretical underpinnings or provide empirical evidence of how multilingualism affects model performance in cross-lingual tasks.
- What evidence would resolve it: A theoretical analysis combined with empirical studies on model performance across different numbers of languages would elucidate the mechanisms behind the curse of multilingualism.

## Limitations

- The approach relies heavily on the assumption that anisotropy is the primary limiting factor, without exploring alternative normalization techniques
- Data filtering methodology is under-specified, particularly regarding evaluation when target language labels are unavailable
- Experiments are limited to only two target languages (Spanish and Indonesian), raising questions about generalizability across different language families and script systems

## Confidence

- High confidence: The whitening mechanism for reducing anisotropy and its mathematical formulation
- Medium confidence: The effectiveness of data filtering approach due to limited experimental details
- Medium confidence: The superiority of XLM-R-base over other multilingual models based on reported results
- Low confidence: Generalizability of results beyond the specific language pairs tested

## Next Checks

1. Test the whitening approach on a controlled synthetic dataset where the degree of anisotropy is systematically varied, to verify that improvements in cosine similarity reliability directly correlate with reduced anisotropy.

2. Implement an ablation study comparing the proposed data filtering method against simpler alternatives (e.g., removing languages with fewest training examples) to determine if the complexity of the filtering approach is justified.

3. Conduct experiments with additional target languages from different language families (e.g., Slavic, Semitic, or East Asian languages) to assess whether the reported benefits generalize beyond Indo-European languages with Latin script.