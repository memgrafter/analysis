---
ver: rpa2
title: 'SocialBench: Sociality Evaluation of Role-Playing Conversational Agents'
arxiv_id: '2403.13679'
source_url: https://arxiv.org/abs/2403.13679
tags:
- group
- role-playing
- social
- agents
- role
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SocialBench, the first benchmark designed
  to evaluate the social intelligence of role-playing conversational agents at both
  individual and group levels. SocialBench contains 500 characters and over 6,000
  questions with 30,800 multi-turn role-playing utterances.
---

# SocialBench: Sociality Evaluation of Role-Playing Conversational Agents

## Quick Facts
- arXiv ID: 2403.13679
- Source URL: https://arxiv.org/abs/2403.13679
- Reference count: 6
- SocialBench introduces the first benchmark for evaluating social intelligence in role-playing conversational agents at both individual and group levels

## Executive Summary
This paper presents SocialBench, a comprehensive benchmark designed to evaluate the social intelligence of role-playing conversational agents. The benchmark contains 500 characters and over 6,000 questions with 30,800 multi-turn role-playing utterances, assessing agents across six dimensions of social intelligence. SocialBench employs fully automatic evaluation metrics to test 10 mainstream open-source and closed-source LLMs, revealing that while agents perform adequately at individual level interactions, their social interaction capabilities significantly deteriorate when handling group dynamics, particularly as group complexity increases.

## Method Summary
SocialBench is constructed through a systematic approach involving character creation, question generation, and multi-turn conversation simulation. The benchmark defines 500 distinct characters with detailed role descriptions and generates over 6,000 questions targeting six dimensions of social intelligence: self-awareness on role description, emotional perception, long-term conversation memory, and social preference towards group dynamics. Multi-turn role-playing conversations are simulated to create 30,800 utterances. The evaluation framework uses automated metrics to assess agent responses across individual and group scenarios, with group complexity varying to test performance under different social interaction conditions.

## Key Results
- Role-playing conversational agents show strong performance at individual interaction level but exhibit significant deficiencies in group social interactions
- Agent performance consistently declines as group size and complexity increase
- Automated evaluation across 10 mainstream LLMs reveals varying degrees of social intelligence capabilities, with closed-source models generally outperforming open-source alternatives

## Why This Works (Mechanism)
SocialBench works by providing a structured framework that systematically evaluates multiple dimensions of social intelligence through controlled role-playing scenarios. The benchmark's effectiveness stems from its comprehensive coverage of social interaction types, from individual conversations to complex group dynamics, allowing for granular assessment of agent capabilities. The automated evaluation metrics provide consistent and scalable measurement of agent responses against predefined social intelligence criteria.

## Foundational Learning

1. **Social Intelligence Dimensions** - Understanding self-awareness, emotional perception, memory retention, and group dynamics is essential for evaluating conversational agents' social capabilities. Quick check: Can the agent correctly identify its role and maintain consistent personality across conversations?

2. **Role-Playing Evaluation Framework** - The benchmark relies on structured character descriptions and scenario-based questioning to create realistic social interaction contexts. Quick check: Are character backgrounds sufficiently detailed to enable authentic social responses?

3. **Multi-Turn Conversation Analysis** - Evaluating long-term memory and conversation coherence requires tracking dialogue progression across multiple turns. Quick check: Does the agent maintain contextual awareness and reference previous conversation elements appropriately?

## Architecture Onboarding

**Component Map:** Character Database -> Question Generator -> Conversation Simulator -> Response Evaluator -> Performance Metrics

**Critical Path:** Character Database -> Question Generator -> Conversation Simulator -> Response Evaluator

**Design Tradeoffs:** The benchmark prioritizes automated evaluation for scalability but sacrifices nuanced human judgment. The focus on predefined characters limits generalizability but ensures consistency across evaluations.

**Failure Signatures:** Performance degradation occurs primarily in group scenarios with complex social dynamics. Agents struggle with maintaining consistent personality across multiple interlocutors and fail to appropriately manage emotional nuances in group settings.

**First 3 Experiments:**
1. Evaluate agent performance on individual character interactions to establish baseline social intelligence capabilities
2. Test group interaction performance with varying group sizes to identify performance degradation patterns
3. Compare automated evaluation metrics against human judgments to validate metric reliability

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark's reliance on automated metrics may not fully capture the nuances of social interaction quality that human evaluation would provide
- The performance degradation in group scenarios is observed but not thoroughly explained - the underlying causes remain unclear
- The benchmark's focus on 500 pre-defined characters may limit generalizability to other character types or domains not represented in the dataset

## Confidence
- **High confidence**: Benchmark construction methodology and dataset size claims are well-documented and verifiable
- **Medium confidence**: Automated evaluation metrics provide consistent measurements, but correlation with human-perceived social intelligence is not established
- **Medium confidence**: Performance trends across different LLM models are observable, but causal explanations for performance differences are limited

## Next Checks
1. Conduct human evaluation studies to validate the correlation between automated metrics and human judgments of social intelligence in role-playing scenarios
2. Test the benchmark's sensitivity by introducing controlled variations in character descriptions and question formulations to assess robustness
3. Evaluate whether the observed performance degradation with group size persists when using specialized multi-agent architectures versus single-agent approaches