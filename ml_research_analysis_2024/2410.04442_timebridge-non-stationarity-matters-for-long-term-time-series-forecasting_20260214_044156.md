---
ver: rpa2
title: 'TimeBridge: Non-Stationarity Matters for Long-term Time Series Forecasting'
arxiv_id: '2410.04442'
source_url: https://arxiv.org/abs/2410.04442
tags:
- time
- long-term
- forecasting
- series
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of non-stationarity in multivariate
  time series forecasting, which leads to spurious regressions in short-term modeling
  and obscures long-term cointegration relationships. To tackle this, the authors
  propose TimeBridge, a framework that segments input series into patches and applies
  Integrated Attention to mitigate short-term non-stationarity and capture stable
  dependencies within each variate, while using Cointegrated Attention to preserve
  non-stationarity and model long-term cointegration across variates.
---

# TimeBridge: Non-Stationarity Matters for Long-term Time Series Forecasting

## Quick Facts
- arXiv ID: 2410.04442
- Source URL: https://arxiv.org/abs/2410.04442
- Reference count: 40
- Primary result: State-of-the-art performance in both short-term and long-term multivariate time series forecasting by explicitly handling non-stationarity through patch-based attention mechanisms.

## Executive Summary
TimeBridge addresses the challenge of non-stationarity in multivariate time series forecasting by proposing a novel framework that segments input series into patches and applies different attention mechanisms for short-term and long-term modeling. The framework uses Integrated Attention to mitigate short-term non-stationarity within each variate, while Cointegrated Attention preserves non-stationarity to capture long-term cointegration relationships across variates. Through comprehensive experiments on multiple datasets including Electricity, Traffic, Weather, Solar, and financial indices, TimeBridge demonstrates superior performance in both short-term and long-term forecasting tasks, achieving state-of-the-art results across diverse domains.

## Method Summary
TimeBridge processes multivariate time series through a three-stage attention framework. First, Patch Embedding segments input sequences into non-overlapping patches. Integrated Attention then applies patch-wise normalization to remove short-term non-stationarity, converting each patch from I(1) to I(0) before applying attention. Patch Downsampling reduces the number of patches while preserving long-term information through an MLP layer. Finally, Cointegrated Attention operates on the downsampled, non-stationary patches to model long-term cointegration relationships across variates. The framework uses a hybrid MAE loss combining time and frequency domain components, trained with Adam optimizer.

## Key Results
- Achieves state-of-the-art performance in both short-term and long-term forecasting across multiple datasets including Electricity, Traffic, Weather, and Solar
- Excels in financial forecasting on CSI 500 and S&P 500 indices, demonstrating robustness to complex volatility patterns
- Outperforms existing methods by effectively handling spurious regressions in short-term modeling while capturing stable long-term cointegration relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrated Attention mitigates short-term non-stationarity by normalizing within each patch to avoid spurious regressions.
- Mechanism: Patch-wise detrending (moving average subtraction) converts each patch from I(1) to I(0), then attention uses the stationary residuals as Query/Key while the original patch is the Value.
- Core assumption: Short-term causal relationships exist primarily within each variate, not across variates, so stabilizing intra-variate sequences is sufficient.
- Evidence anchors: Abstract mentions mitigating short-term non-stationarity; section describes patch-wise normalization with normalized patches as Query/Key and original as Value.
- Break condition: If short-term cross-variate causal links are strong, the assumption fails and Integrated Attention underperforms.

### Mechanism 2
- Claim: Cointegrated Attention preserves non-stationarity to model long-term cointegration across variates.
- Mechanism: After downsampling, each patch carries richer long-term context; attention is applied directly to non-stationary patch embeddings so that the model can detect and exploit stable long-run equilibrium relationships (cointegration).
- Core assumption: Long-term cointegration signals are embedded in the non-stationary structure; removing non-stationarity destroys these signals.
- Evidence anchors: Abstract mentions preserving non-stationarity for long-term cointegration; section describes leveraging attention on non-stationary patches to model cointegration relationships.
- Break condition: If long-term dependencies are weak or dominated by noise, preserving non-stationarity may add variance without benefit.

### Mechanism 3
- Claim: Patch Downsampling aggregates long-term information and reduces computational cost before Cointegrated Attention.
- Mechanism: MLP reduces N patches to M (M<N) patches; the downsampled set serves as Query to aggregate global context from the original patch set (Key/Value), enabling richer temporal coverage per patch.
- Core assumption: Fewer, information-rich patches suffice to capture cointegration; aggregation does not erase critical fine-grained details.
- Evidence anchors: Section describes reducing patches and aggregating global information through attention; abstract mentions downsampling to enrich each patch with more long-term information.
- Break condition: Excessive downsampling may oversimplify and lose essential cointegration patterns.

## Foundational Learning

- Concept: Integration and cointegration in time series
  - Why needed here: The paper explicitly contrasts I(1) (integrated) vs I(0) (stationary) series and exploits cointegration for long-term modeling.
  - Quick check question: If Xt ~ I(1) and Yt ~ I(1) are cointegrated, what is the order of integration of Zt = Xt - βYt?

- Concept: Augmented Dickey-Fuller (ADF) test
  - Why needed here: ADF is used to test stationarity of individual series and residuals in the Engle-Granger cointegration test, justifying the I(1) vs I(0) claims.
  - Quick check question: What does rejecting the ADF null hypothesis imply about the time series?

- Concept: Engle-Granger (EG) test for cointegration
  - Why needed here: EG tests residuals from regressing one I(1) series on another; stationarity of residuals confirms cointegration, underpinning the long-term modeling rationale.
  - Quick check question: In the EG test, if the residuals are I(0), what does that say about the original series?

## Architecture Onboarding

- Component map: Input → Patch Embedding (C×N×D) → Integrated Attention (stationary residuals → attention) → Patch Downsampling (N→M) → Cointegrated Attention (non-stationary M patches across variates) → Output projection
- Critical path: Patch Embedding → Integrated Attention → Patch Downsampling → Cointegrated Attention → Output
- Design tradeoffs: (1) Patch size S vs. number of patches N: larger S → fewer patches but risk losing short-term resolution. (2) Downsampling ratio M/N: more patches → richer cointegration capture but higher cost. (3) Retaining vs. removing non-stationarity at each stage: stability vs. long-term signal preservation.
- Failure signatures: (1) High MSE/MAE with stable loss curves → patch normalization or downsampling too aggressive. (2) Volatile training loss → insufficient normalization in Integrated Attention. (3) Underfitting on datasets with strong cross-variate links → Integrated-only mode without Cointegrated Attention.
- First 3 experiments:
  1. Baseline: Run with Integrated Attention only (no downsampling, no Cointegrated Attention) to confirm short-term gains.
  2. Add Patch Downsampling but keep Integrated Attention only to measure effect of long-term context aggregation.
  3. Enable Cointegrated Attention on a multi-channel dataset (e.g., Traffic) to validate cross-variate cointegration capture.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal patch size S in TimeBridge scale with the inherent non-stationarity (ADF test statistic) of different datasets?
- Basis in paper: The authors note that non-stationarity varies across datasets (ADF values range from -1.53 for random walk to -97.54 for Gaussian white noise) and use different patch sizes for different datasets, but do not systematically study the relationship between non-stationarity level and optimal patch size.
- Why unresolved: The paper treats patch size as a hyperparameter tuned per dataset without establishing a principled connection to the dataset's non-stationarity characteristics.
- What evidence would resolve it: A comprehensive study varying patch sizes across datasets with different ADF values to establish a scaling relationship, or a theoretical model predicting optimal patch size from ADF statistics.

### Open Question 2
- Question: Does TimeBridge's performance advantage over stationary baselines increase with the degree of cointegration (EG test statistic) in the dataset?
- Basis in paper: The authors observe that datasets with more channels (like Electricity and Traffic) show stronger cointegration relationships and that TimeBridge excels on these datasets, but do not quantify this relationship.
- Why unresolved: While the paper demonstrates superior performance on datasets with high cointegration, it does not systematically measure how performance scales with cointegration strength across multiple datasets.
- What evidence would resolve it: A regression analysis correlating TimeBridge's performance gains against the EG test values across all tested datasets.

### Open Question 3
- Question: What is the computational complexity trade-off between patch downsampling rate and forecasting accuracy in TimeBridge?
- Basis in paper: The authors discuss that increasing downsampled patches initially improves predictions but adds computational cost, and they carefully balanced downsampling rates, but do not provide a detailed complexity-accuracy analysis.
- Why unresolved: The paper mentions computational considerations but does not quantify the exact trade-off between the number of downsampled patches M, computational cost, and forecasting accuracy.
- What evidence would resolve it: A detailed analysis measuring wall-clock time, memory usage, and accuracy across different downsampling rates M for representative datasets.

## Limitations

- Critical hyperparameters like patch sizes S and downsampling ratios M/N are not fully specified for all datasets, making exact reproduction challenging.
- The framework assumes short-term causal relationships are primarily intra-variate, which may not hold for all datasets with strong cross-variate dependencies.
- The exact implementation details of the hybrid MAE loss combining time and frequency domain components are not fully described, particularly the weighting parameter α.

## Confidence

- **High Confidence**: The overall framework architecture (Integrated Attention → Patch Downsampling → Cointegrated Attention) is clearly specified and the state-of-the-art results across multiple benchmarks provide strong empirical support for the approach's effectiveness.
- **Medium Confidence**: The theoretical justification for separating short-term and long-term modeling through non-stationarity preservation/removal is sound, but the exact conditions under which this separation is optimal remain unclear. The assumption that short-term causal relationships are primarily intra-variate may not hold for all datasets.
- **Low Confidence**: The specific hyperparameter choices (patch sizes, downsampling ratios, loss weighting) that led to optimal performance are not fully disclosed, making it difficult to assess whether the results are robust across different configurations.

## Next Checks

1. **Sensitivity Analysis**: Systematically vary patch sizes S and downsampling ratios M/N across datasets to determine the stability of performance gains and identify optimal ranges for each dataset type.

2. **Ablation on Cointegration Strength**: Test TimeBridge on synthetic datasets with varying degrees of cointegration (from none to strong) to verify that the Cointegrated Attention component provides proportional benefits only when meaningful cointegration exists.

3. **Cross-Variate Causal Link Assessment**: Implement and compare against variants that preserve short-term cross-variate relationships (rather than treating them as noise) to test whether the assumption of primarily intra-variate short-term causality holds across all benchmark datasets.