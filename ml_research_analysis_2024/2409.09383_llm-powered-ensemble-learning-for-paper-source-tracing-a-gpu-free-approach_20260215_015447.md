---
ver: rpa2
title: 'LLM-Powered Ensemble Learning for Paper Source Tracing: A GPU-Free Approach'
arxiv_id: '2409.09383'
source_url: https://arxiv.org/abs/2409.09383
tags:
- u1d456
- source
- u1d457
- quotedbl
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of identifying reference sources
  for academic papers in the KDD CUP 2024 competition. The proposed method leverages
  closed-source large language models (LLMs) to generate predicted reference sources
  directly from paper content, without requiring GPU resources.
---

# LLM-Powered Ensemble Learning for Paper Source Tracing: A GPU-Free Approach

## Quick Facts
- **arXiv ID:** 2409.09383
- **Source URL:** https://arxiv.org/abs/2409.09383
- **Reference count:** 2
- **Key outcome:** GPU-free LLM ensemble approach achieved MAP score of 0.50 on KDD CUP 2024 paper source tracing task

## Executive Summary
This paper presents a novel GPU-free approach for identifying reference sources in academic papers using ensemble learning with large language models (LLMs). The method leverages four closed-source LLMs (GPT-4 Turbo, GPT-4o, Gemini 1.5 Pro, and Claude 3 Opus) to generate predicted reference sources from paper content, then refines these predictions using traditional machine learning classifiers (LightGBM and CatBoost). The final predictions combine base model scores with LLM-based bonuses, weighted by confidence and percentile rankings. Notably, this was the only GPU-free approach among award-winning entries in the KDD CUP 2024 competition, demonstrating that sophisticated ensemble learning can effectively harness multiple LLMs for complex reasoning tasks in resource-constrained environments.

## Method Summary
The method combines LLM-generated predictions with traditional machine learning classifiers through an ensemble approach. First, paper metadata, citation statistics, and contextual keywords are extracted as features. Four closed-source LLMs generate probability scores for each citation being a source, using multiple prompt variants. These LLM outputs are refined through grouping by prompt type and base model, with confidence scores and expert weights applied. Separately, LightGBM and CatBoost classifiers are trained on the engineered features to provide probability estimates. The final prediction model combines the base model scores (weighted combination of LightGBM and CatBoost probabilities) with LLM-based bonuses, with adjustments based on percentile rankings and confidence thresholds.

## Key Results
- Achieved MAP score of 0.50 on validation set, outperforming individual LLM or LightGBM approaches
- Only GPU-free approach among award-winning entries in KDD CUP 2024 competition
- Demonstrated effective combination of semantic understanding (LLMs) with structured feature analysis (traditional ML)
- Successfully handled the task of identifying source references from 55,014 citations across 1,576 computer science papers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-based zero-shot reasoning can effectively identify source references by analyzing semantic context and citation cues in academic papers
- **Mechanism:** Closed-source LLMs like GPT-4 Turbo, GPT-4o, Gemini 1.5 Pro, and Claude 3 Opus are prompted to analyze paper text and identify key citations based on contextual keywords (e.g., "inspired by," "motivated by") and their proximity to cited references
- **Core assumption:** LLMs with MMLU scores above 85 possess sufficient logical reasoning capabilities for academic context analysis
- **Evidence anchors:** "closed-source large language models (LLMs) to directly generate predicted reference sources from the provided papers"; "we employ four state-of-the-art LLMs... Our selection criterion was based on empirical evidence suggesting that LLMs with an MMLU score above 85 demonstrate superior logical reasoning capabilities in academic contexts."
- **Break condition:** If the LLM's understanding of academic context or citation patterns is insufficient, or if the prompt engineering fails to elicit relevant responses

### Mechanism 2
- **Claim:** Feature engineering combined with traditional machine learning classifiers provides complementary evidence to LLM-generated predictions
- **Mechanism:** Paper metadata, citation statistics, reference metadata, and contextual keywords are extracted as features. These features are used to train LightGBM and CatBoost classifiers, which provide probability estimates for each citation being a source citation based on structured data representations
- **Core assumption:** Structured features from papers and citations can capture relevant patterns for source tracing that complement LLM semantic understanding
- **Evidence anchors:** "Key features are extracted through feature engineering, including paper metadata, citation statistics, and contextual keywords"; "We compute the total occurrences of each cited reference within the paper... These features provide a rich, structured representation of the papers and their citations, complementing the semantic understanding capabilities of the LLMs."
- **Break condition:** If the engineered features fail to capture relevant patterns or if the traditional ML models underperform compared to LLM-only approaches

### Mechanism 3
- **Claim:** Ensemble learning combining LLM predictions and traditional ML classifier outputs achieves better performance than either approach alone
- **Mechanism:** The final prediction model combines base model scores (weighted combination of LightGBM and CatBoost probabilities) with LLM-based bonuses. LLM outputs are grouped by prompt type and base model, with confidence scores and expert weights applied. The LLM bonuses are then aggregated and added to the base score, with adjustments based on percentile rankings and confidence thresholds
- **Core assumption:** The strengths of LLM semantic understanding and traditional ML structured feature analysis are complementary and can be effectively combined through weighted averaging
- **Evidence anchors:** "The final predictions combine base model scores with LLM-based bonuses, weighted by confidence and percentile rankings"; "Our final prediction model combines the strengths of LLM-generated answers and traditional machine learning classifiers... This ensemble approach allows us to leverage the semantic understanding of LLMs while benefiting from the structured feature representations captured by traditional models."
- **Break condition:** If the combination weights are poorly calibrated or if one component (LLM or traditional ML) consistently underperforms, leading to suboptimal ensemble performance

## Foundational Learning

- **Concept:** Zero-shot learning with large language models
  - **Why needed here:** The approach relies on LLMs' ability to perform reasoning tasks without task-specific fine-tuning, which is crucial for the GPU-free constraint and allows leveraging pre-trained models' general capabilities
  - **Quick check question:** How does zero-shot learning differ from few-shot or fine-tuning approaches, and what are the advantages and limitations of each in the context of paper source tracing?

- **Concept:** Feature engineering and traditional machine learning
  - **Why needed here:** While LLMs provide semantic understanding, structured features from papers and citations capture additional patterns that can complement LLM predictions and improve overall performance
  - **Quick check question:** What types of features are most relevant for paper source tracing, and how do they relate to the semantic understanding provided by LLMs?

- **Concept:** Ensemble learning and model combination
  - **Why needed here:** Combining predictions from multiple models (LLMs and traditional ML classifiers) with different strengths can achieve better performance than any individual model, addressing the limitations of each approach
  - **Quick check question:** What are the key considerations when combining predictions from models with different characteristics (e.g., semantic understanding vs. structured feature analysis), and how can confidence scores and weights be effectively calibrated?

## Architecture Onboarding

- **Component map:** Academic paper text and citation list -> Feature engineering module -> LLM prediction module -> Traditional ML module -> Ensemble module -> Final probability scores
- **Critical path:**
  1. Extract features from paper text and citations
  2. Generate LLM predictions for each citation using multiple models and prompts
  3. Train traditional ML classifiers on engineered features
  4. Combine base model scores and LLM bonuses in the ensemble module
  5. Output final probability scores
- **Design tradeoffs:**
  - LLM vs. traditional ML: LLMs provide semantic understanding but may lack structured feature analysis, while traditional ML models excel at structured data but may miss semantic nuances
  - Number of LLMs and prompts: More models and prompts can provide diverse perspectives but increase computational cost and complexity
  - Ensemble weighting: Optimal weights for combining base model scores and LLM bonuses require careful calibration to balance the strengths of each component
- **Failure signatures:**
  - Poor LLM performance: If LLM predictions consistently miss key citations or include irrelevant ones, it may indicate insufficient prompt engineering or LLM limitations in academic context understanding
  - Weak traditional ML performance: If traditional ML classifiers underperform, it may suggest that the engineered features are not capturing relevant patterns or that the models are not suitable for this task
  - Suboptimal ensemble performance: If the ensemble approach fails to improve upon individual models, it may indicate poorly calibrated weights or a lack of complementarity between LLM and traditional ML predictions
- **First 3 experiments:**
  1. Evaluate individual LLM performance: Generate predictions using each LLM separately and compare their MAP scores to assess their relative strengths and weaknesses in paper source tracing
  2. Assess traditional ML performance: Train LightGBM and CatBoost classifiers on engineered features and evaluate their performance independently to determine the value of structured feature analysis
  3. Test ensemble combinations: Experiment with different weighting schemes and confidence adjustment strategies to find the optimal combination of base model scores and LLM bonuses for maximizing MAP score

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How would the proposed method perform if fine-tuned on a domain-specific corpus rather than using zero-shot learning?
- **Basis in paper:** [inferred] The paper explicitly compares zero-shot LLM performance with traditional fine-tuning approaches and mentions this as a key differentiator, stating "Unlike most teams that addressed this challenge by fine-tuning pre-trained neural language models such as BERT or ChatGLM, our primary approach utilized closed-source large language models (LLMs)."
- **Why unresolved:** The paper only evaluates zero-shot performance and doesn't explore whether domain-specific fine-tuning would improve results. The authors position their approach as an alternative to fine-tuning but don't empirically compare the two.
- **What evidence would resolve it:** A controlled experiment comparing zero-shot LLM performance against domain-finetuned LLMs on the same task, measuring MAP scores and computational requirements for both approaches.

### Open Question 2
- **Question:** What is the optimal combination of LLM confidence scores and feature-based scores in the ensemble model?
- **Basis in paper:** [explicit] The authors mention they use weighted combinations (e.g., "/u1D45Dbase/u1D456,/u1D457= /u1D45D/u1D459/u1D454/u1D44F/u1D456,/u1D457× /u1D714 /u1D459/u1D454/u1D44F+ /u1D45D/u1D450/u1D44F/u1D456,/u1D457× /u1D714 /u1D450/u1D44F") but don't explore whether these weights are optimal or could be learned.
- **Why unresolved:** The paper presents fixed weights derived from expert knowledge but doesn't investigate whether these could be optimized through validation or whether different weighting schemes might yield better performance.
- **What evidence would resolve it:** Systematic experimentation with different weighting schemes (including learned weights through validation) to determine the optimal combination strategy, measuring impact on MAP scores.

### Open Question 3
- **Question:** How sensitive is the method to different percentile thresholds (/u1D45D/u1D45B/u1D452/u1D454/u1D461ℎ/u1D45F/u1D452/u1D460ℎ/u1D45C/u1D459/u1D451) and division constants (/u1D436 /u1D45B/u1D452/u1D454) used to adjust LLM scores?
- **Basis in paper:** [explicit] The authors specify particular values (0.2 and 4 respectively) in Table 2 but don't discuss sensitivity analysis or whether these values were optimized.
- **Why unresolved:** The paper presents these as fixed parameters without exploring their impact on performance or discussing the methodology used to select them.
- **What evidence would resolve it:** Comprehensive sensitivity analysis showing how MAP scores vary with different percentile thresholds and division constants, identifying optimal ranges and potential overfitting to the validation set.

## Limitations
- Relies entirely on closed-source LLMs, making exact reproduction difficult without access to the same model versions and pricing constraints
- Ensemble weighting scheme appears somewhat arbitrary with multiple hyperparameters lacking clear justification or sensitivity analysis
- Limited generalizability to other academic domains beyond computer science papers, given the task-specific nature of academic citation patterns

## Confidence
- **High confidence:** The MAP score of 0.50 on validation set is clearly reported and the GPU-free constraint is verifiable as the approach uses only API calls to closed-source models
- **Medium confidence:** The mechanism combining LLM semantic understanding with traditional ML feature analysis is logically sound, though the optimal weighting scheme is not rigorously validated
- **Low confidence:** The generalizability of this approach to other domains beyond computer science papers, given the task-specific nature of academic citation patterns

## Next Checks
1. **Replicate the baseline performance:** Generate predictions using individual LLMs and traditional ML models separately to verify that the ensemble approach provides meaningful improvement over individual components
2. **Test the sensitivity to ensemble weights:** Systematically vary the weighting parameters (/u1D714 /u1D453, /u1D45D/u1D45B/u1D452/u1D454, /u1D436 /u1D45B/u1D452/u1D454) to determine if the reported combination is optimal or if simpler weighting schemes perform similarly
3. **Evaluate on different academic domains:** Apply the same methodology to papers from other fields (e.g., biology, physics) to assess whether the approach generalizes beyond computer science or requires domain-specific adaptation