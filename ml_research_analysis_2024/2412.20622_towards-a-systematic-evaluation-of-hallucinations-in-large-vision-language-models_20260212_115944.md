---
ver: rpa2
title: Towards a Systematic Evaluation of Hallucinations in Large-Vision Language
  Models
arxiv_id: '2412.20622'
source_url: https://arxiv.org/abs/2412.20622
tags:
- hallucination
- lvlms
- visual
- image
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HALLUCINOGEN, a novel benchmark for evaluating
  hallucinations in large vision-language models (LVLMs). The benchmark focuses on
  implicit hallucination attacks that require models to perform contextual reasoning
  before generating responses, contrasting with prior benchmarks that use explicit
  yes/no prompts about object presence.
---

# Towards a Systematic Evaluation of Hallucinations in Large-Vision Language Models

## Quick Facts
- arXiv ID: 2412.20622
- Source URL: https://arxiv.org/abs/2412.20622
- Reference count: 26
- Primary result: HALLUCINOGEN benchmark reveals LVLMs hallucinate significantly more on implicit attacks requiring contextual reasoning

## Executive Summary
This paper introduces HALLUCINOGEN, a novel benchmark for systematically evaluating hallucinations in large vision-language models (LVLMs). Unlike prior benchmarks that use explicit yes/no prompts about object presence, HALLUCINOGEN employs implicit hallucination attacks that require contextual reasoning before generating responses. The benchmark categorizes visual entities as salient (visibly recognizable objects) or latent (requiring domain knowledge for inference) and tests LVLMs across three vision-language tasks: localization, visual context reasoning, and counterfactual reasoning. Extensive evaluations on 11 state-of-the-art LVLMs demonstrate that current models perform close to random guessing on HALLUCINOGEN, with implicit attacks resulting in significantly higher hallucination rates compared to explicit attacks.

## Method Summary
HALLUCINOGEN evaluates LVLMs using 90,000 image-prompt pairs from MS-COCO (salient entities) and NIH Chest X-ray (latent entities) datasets. For each image-entity pair, the benchmark employs 15 diverse implicit hallucination attack prompts across three task categories: localization, visual context reasoning, and counterfactual reasoning. The evaluation uses string matching to convert open-ended responses to binary "Yes/No" labels, though LLM-as-judge evaluation with GPT-4o is also mentioned as an alternative. The benchmark tests 11 state-of-the-art LVLMs including LLaVA-1.5, mPLUG-OWL2, Qwen2VL, Llama3.2-VL, Gemini, and two hallucination mitigation strategies (RLHF and LURE).

## Key Results
- LVLMs perform close to random guessing on HALLUCINOGEN benchmark, demonstrating severe hallucination susceptibility
- Implicit attacks result in significantly higher hallucination rates compared to explicit attacks (6.8%-29.0% difference)
- As task difficulty increases from localization to counterfactual reasoning, hallucination errors increase substantially
- Even medical domain expert models like LLaVA-Med exhibit poor performance on latent entity tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Implicit hallucination attacks require LVLMs to perform contextual reasoning before generating responses, making them more susceptible to hallucination than explicit yes/no prompts.
- Mechanism: The model must first implicitly verify whether a queried entity exists in the image before answering questions about its context, location, or counterfactual scenarios. This additional reasoning step increases the likelihood of hallucination.
- Core assumption: LVLMs rely heavily on language priors and spurious correlations rather than true visual understanding, making them vulnerable when required to perform multi-step reasoning.
- Evidence anchors:
  - [abstract] "implicit attacks result in significantly higher hallucination rates compared to explicit attacks (6.8%-29.0% difference)"
  - [section 1] "HALLUCINOGEN introduces implicit open-form hallucination attacks, which pose a more significant challenge for LVLMs to defend against"
  - [section 3.2] "These attacks test the LVLM's spatial reasoning and its susceptibility to context-induced hallucinations"

### Mechanism 2
- Claim: LVLMs allocate minimal attention to visual tokens compared to textual tokens when generating responses, leading to hallucination.
- Mechanism: During next-token prediction, the model's attention to visual tokens remains near zero while attention to query and previously generated tokens dominates, indicating overreliance on language priors rather than visual understanding.
- Core assumption: The transformer architecture's attention mechanism shows that visual features are not effectively integrated into the reasoning process.
- Evidence anchors:
  - [section 4.5] "During next-token prediction, the model's attention to visual tokens remains near zero, while attention to query tokens decreases significantly"
  - [section 4.5] "LVLMs allocate very little attention to visual tokens when responding to our hallucination attacks"

### Mechanism 3
- Claim: Chain-of-Thought reasoning amplifies hallucination in LVLMs by increasing the length and complexity of generated responses.
- Mechanism: CoT prompts make LVLMs generate longer, multi-step responses, which increases the opportunity for errors to accumulate and for the model to hallucinate intermediate reasoning steps.
- Core assumption: Longer reasoning chains provide more opportunities for hallucination to manifest and compound.
- Evidence anchors:
  - [section 4.4] "while CoT is ineffective against our hallucination attacks, it increases hallucination in the four best-performing LVLMs"
  - [section 4.4] "we hypothesize that since CoT prompts make LVLMs generate longer, multi-step responses, it increases the likelihood of hallucination as errors can accumulate over extended reasoning"

## Foundational Learning

- Concept: Visual grounding in multimodal models
  - Why needed here: Understanding how visual features are integrated with language representations is crucial for diagnosing why LVLMs hallucinate
  - Quick check question: What architectural component in LVLMs determines how much attention is paid to visual versus textual features during generation?

- Concept: Implicit vs explicit reasoning in language models
  - Why needed here: The distinction between direct identification tasks and those requiring contextual inference explains why certain prompts induce more hallucination
  - Quick check question: How does requiring a model to first verify entity presence before answering a contextual question differ from directly asking about entity presence?

- Concept: Attention mechanisms in transformer architectures
  - Why needed here: Understanding attention patterns reveals how LVLMs process multimodal inputs and where they may fail to properly integrate visual information
  - Quick check question: What does it mean when a model shows near-zero attention to visual tokens during response generation?

## Architecture Onboarding

- Component map: Vision encoder (CNN or ViT) -> Visual features -> Cross-modal attention layers -> Language model (LLM) -> Textual features -> Cross-modal attention layers -> Integrated representation -> Output head -> Text generation
- Critical path: 1. Image -> Vision encoder -> Visual tokens, 2. Prompt -> LLM embedding -> Query tokens, 3. Cross-attention -> Integrated representation, 4. Generation -> Text output
- Design tradeoffs: Visual feature resolution vs. computational cost, Language model size vs. hallucination susceptibility, Explicit verification mechanisms vs. generation fluency
- Failure signatures: Near-zero visual attention scores during generation, High accuracy on explicit prompts but low on implicit ones, Performance degradation as task complexity increases
- First 3 experiments: 1. Visualize attention maps for visual vs. textual tokens during HALLUCINOGEN evaluation to confirm minimal visual attention, 2. Test CoT variants that explicitly require visual verification before reasoning to see if hallucination decreases, 3. Implement a post-generation visual consistency check to filter hallucinated responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of latent entities in HALLUCINOGEN affect the generalizability of hallucination detection across different domains?
- Basis in paper: [explicit] The paper introduces latent entities as a new category requiring domain knowledge for inference, such as diagnosing diseases from medical images.
- Why unresolved: While the paper demonstrates that LVLMs perform poorly on latent entities, it does not explore how these results translate to other domains beyond medical imaging or how latent entities might improve the robustness of hallucination detection in diverse applications.
- What evidence would resolve it: Conducting experiments on latent entities across multiple domains (e.g., engineering, biology, or finance) and comparing performance metrics with those on salient entities would clarify the generalizability of HALLUCINOGEN's approach.

### Open Question 2
- Question: What is the impact of task difficulty on hallucination rates when using more complex vision-language tasks beyond those tested in HALLUCINOGEN?
- Basis in paper: [explicit] The paper shows that hallucination errors increase as task difficulty rises from localization to counterfactual reasoning, but it does not explore tasks beyond these three categories.
- Why unresolved: The paper focuses on three specific tasks, leaving open the question of whether hallucination rates continue to increase with even more complex tasks or if there is a plateau or decline in performance.
- What evidence would resolve it: Evaluating LVLMs on additional complex tasks, such as multi-step reasoning or cross-modal understanding, and analyzing hallucination rates across these tasks would provide insights into the relationship between task complexity and hallucination.

### Open Question 3
- Question: How do different hallucination mitigation strategies perform when evaluated on HALLUCINOGEN's implicit attacks compared to explicit attacks?
- Basis in paper: [explicit] The paper evaluates two mitigation strategies (RLHF and LURE) and two inference-time defenses (post-prompting and CoT) on HALLUCINOGEN, showing limited effectiveness.
- Why unresolved: The paper does not compare the performance of these strategies specifically on implicit attacks versus explicit attacks, nor does it explore the potential for combining multiple strategies to enhance effectiveness.
- What evidence would resolve it: Conducting a comparative analysis of mitigation strategies on both implicit and explicit attacks within HALLUCINOGEN and testing hybrid approaches could reveal more effective methods for reducing hallucinations in LVLMs.

## Limitations
- Evaluation methodology relies on binary conversion of open-ended responses using string matching, which may introduce labeling errors
- Benchmark focuses on two specific datasets (MS-COCO and NIH Chest X-ray), limiting generalizability to other visual domains
- Does not explore whether performance varies with image complexity, entity salience within images, or prompt phrasing variations

## Confidence
**High Confidence**: The claim that implicit hallucination attacks result in significantly higher hallucination rates compared to explicit attacks (6.8%-29.0% difference) is well-supported by extensive experimental data across 11 LVLMs.

**Medium Confidence**: The assertion that task difficulty increases hallucination errors from localization to counterfactual reasoning is supported by the data, though the specific numerical differences could be influenced by dataset selection and prompt design.

**Low Confidence**: The hypothesis that chain-of-thought reasoning amplifies hallucination by increasing response length is based on limited experimental evidence with only four LVLMs showing this effect.

## Next Checks
1. Conduct ablation studies where visual tokens are explicitly highlighted in prompts to measure whether attention scores increase and hallucination rates decrease, validating the core mechanism of visual grounding failure.
2. Evaluate HALLUCINOGEN on additional datasets beyond MS-COCO and NIH Chest X-ray to determine if the observed hallucination patterns hold across diverse visual domains including natural scenes, medical imaging, and technical diagrams.
3. Implement and test alternative hallucination mitigation approaches such as visual grounding rewards during training or post-hoc visual consistency checking to assess whether the proposed mechanisms can be effectively addressed.