---
ver: rpa2
title: 'When Representations Align: Universality in Representation Learning Dynamics'
arxiv_id: '2402.09142'
source_url: https://arxiv.org/abs/2402.09142
tags:
- learning
- dynamics
- theory
- neural
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an effective theory to model representation
  learning dynamics in deep neural networks by treating the encoding and decoding
  maps as arbitrary smooth functions. The theory predicts interactions between nearby
  representations in the expressive regime where large architectures are not constrained
  by their parametrization.
---

# When Representations Align: Universality in Representation Learning Dynamics

## Quick Facts
- arXiv ID: 2402.09142
- Source URL: https://arxiv.org/abs/2402.09142
- Reference count: 40
- Primary result: Introduces effective theory modeling representation learning dynamics by treating encoding/decoding maps as arbitrary smooth functions

## Executive Summary
This paper presents an effective theory for modeling representation learning dynamics in deep neural networks by treating encoding and decoding maps as arbitrary smooth functions. The theory predicts interactions between nearby representations in the expressive regime where large architectures are not constrained by their parametrization. Through experiments across various architectures (fully connected, convolutional, skip connections) and activation functions (ReLU, tanh), the theory accurately describes learning dynamics after fitting two effective learning rates. The work identifies both "lazy" and "rich" learning regimes depending on initialization scale, predicting structured representations based on data structure rather than random initialization.

## Method Summary
The authors develop an effective theory that models representation learning dynamics by treating the encoding and decoding maps of neural networks as arbitrary smooth functions. This approach allows them to predict interactions between nearby representations in the expressive regime where architectures are sufficiently large to be unconstrained by parametrization. The theory is validated through experiments on various architectures including fully connected networks, convolutional networks, and networks with skip connections, using different activation functions such as ReLU and tanh. The key insight is that after fitting two effective learning rates, the theory can accurately describe the learning dynamics across these diverse settings.

## Key Results
- Theory accurately describes learning dynamics across various architectures after fitting two effective learning rates
- Predicts both "lazy" and "rich" learning regimes depending on initialization scale
- Successfully predicts final representational structures in toy tasks like XOR and feature collapse experiments
- Shows some success on MNIST while acknowledging limitations in capturing all architectural details

## Why This Works (Mechanism)
The theory works by abstracting away specific architectural details and focusing on the universal aspects of representation learning dynamics. By treating encoding and decoding maps as arbitrary smooth functions, the theory captures the essential interactions between representations that occur regardless of the specific implementation. This abstraction allows the theory to identify conserved behaviors across different models once they are sufficiently flexible. The two effective learning rates act as universal parameters that can be fitted to describe the dynamics across diverse architectures, explaining why the same theoretical framework applies to both fully connected and convolutional networks with different activation functions.

## Foundational Learning
- **Effective Theory**: A simplified mathematical framework that captures essential behaviors while ignoring unnecessary details. Needed to make the complex dynamics of deep learning tractable and to identify universal patterns.
- **Lazy vs Rich Learning Regimes**: Two distinct phases of neural network training depending on initialization scale. Lazy learning occurs with small initial weights where networks change little, while rich learning involves substantial representation changes.
- **Expressive Regime**: The regime where neural networks are large enough that their behavior is not constrained by parametrization. Critical for the theory's applicability as it assumes sufficient model capacity.
- **Smooth Function Approximation**: The assumption that encoding and decoding maps can be treated as arbitrary smooth functions. Enables the mathematical treatment while maintaining generality across architectures.
- **Representation Collapse**: A phenomenon where representations become degenerate or lose information. The theory successfully predicts when and how this occurs in various settings.
- **Effective Learning Rates**: Two parameters fitted to describe the dynamics of representation learning. Act as universal constants that capture the essential timescale of learning across architectures.

## Architecture Onboarding
- **Component Map**: Input Data -> Encoder Network -> Representation Space -> Decoder Network -> Output Predictions
- **Critical Path**: The key dynamics occur in the representation space where the theory predicts interactions between nearby representations, with the encoder and decoder serving as smooth transformations of this space.
- **Design Tradeoffs**: The theory sacrifices architectural specificity for universality, meaning it cannot capture all architectural details but gains the ability to describe conserved behaviors across models.
- **Failure Signatures**: The theory is expected to fail when architectures are too small to be in the expressive regime, or when specific architectural innovations (like attention mechanisms) introduce dynamics not captured by smooth function approximations.
- **First Experiments**: 1) Test XOR task to verify basic predictions of representation structure, 2) Run feature collapse experiment to validate predictions about representation degeneracy, 3) Apply to simple MNIST classification to assess practical utility beyond toy tasks.

## Open Questions the Paper Calls Out
None

## Limitations
- The theory shows limited success on complex real-world tasks, with only "some success" reported on MNIST
- Cannot capture all architectural details, raising questions about which specific features are missed and their significance
- Claims of identifying "universal behaviors" may be overstated given the limited scope of tested architectures and tasks
- Applicability appears restricted to relatively simple tasks, with no evidence provided for more complex datasets

## Confidence
- High confidence in: Theory's ability to describe learning dynamics after fitting two effective learning rates in controlled experimental settings (XOR, feature collapse)
- Medium confidence in: Predictions about "lazy" versus "rich" learning regimes based on initialization scale, as this is a theoretical prediction that may not hold across all architectures and tasks
- Low confidence in: Claim of identifying truly "universal behaviors" across different models, given the limited scope of tested architectures and tasks

## Next Checks
1. Test the theory's predictions on larger-scale image classification tasks beyond MNIST, such as CIFAR-10 or ImageNet, to assess its scalability and practical utility.
2. Evaluate the theory's performance across a broader range of architectural innovations, including attention mechanisms, transformers, and other modern deep learning components not mentioned in the current study.
3. Conduct ablation studies to systematically identify which architectural details the theory fails to capture and quantify the impact of these omissions on learning dynamics predictions.