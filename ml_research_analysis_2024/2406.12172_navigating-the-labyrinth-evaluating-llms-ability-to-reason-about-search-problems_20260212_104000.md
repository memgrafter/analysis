---
ver: rpa2
title: 'Navigating the Labyrinth: Evaluating LLMs'' Ability to Reason About Search
  Problems'
arxiv_id: '2406.12172'
source_url: https://arxiv.org/abs/2406.12172
tags:
- state
- goal
- cost
- visited
- initial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SearchBench, a new benchmark of combinatorial
  search problems that require backtracking and multi-step reasoning. The authors
  show that even advanced LLMs struggle with these problems when solving them end-to-end
  in natural language.
---

# Navigating the Labyrinth: Evaluating LLMs' Ability to Reason About Search Problems

## Quick Facts
- **arXiv ID**: 2406.12172
- **Source URL**: https://arxiv.org/abs/2406.12172
- **Reference count**: 40
- **Primary result**: Even advanced LLMs struggle with combinatorial search problems requiring backtracking when solving them end-to-end in natural language, but performance improves significantly when generating complete A* search algorithms with code execution.

## Executive Summary
This paper introduces SearchBench, a new benchmark of combinatorial search problems that require backtracking and multi-step reasoning. The authors demonstrate that even advanced LLMs struggle with these problems when solving them end-to-end in natural language. However, performance improves significantly when models are prompted to generate complete A* search algorithms - offloading the iterative search process to code execution. An enhanced Multi-Stage-Multi-Try (MSMT) inference method further boosts performance, with GPT-4 solving over 57% of problems correctly. This reveals a key LLM bottleneck in iterative reasoning while highlighting their strength in structured code generation.

## Method Summary
The authors evaluate multiple LLMs (GPT-4, GPT-3.5, Code Llama, Llama 3.1 variants, Mixtral, Mistral) on SearchBench, a benchmark of 11 unique search problem types with 1,107 instances. They compare five prompting methods: 0-shot text, 4-shot CoT text, 0-shot code, 4-shot A*, and 4-shot MSMT A*. The MSMT A* approach decomposes algorithm generation into stages with unit test validation. Performance is assessed on feasibility (rule compliance), correctness (goal achievement), and optimality (minimum cost) using automated evaluation pipelines. All experiments use in-context learning without additional training.

## Key Results
- LLMs struggle with iterative reasoning in text, failing to solve over 70% of search problems even with chain-of-thought prompting
- Code-based prompting (A* generation) significantly improves performance by offloading iterative search to code execution
- MSMT A* inference method further improves results, with GPT-4 solving over 57% of problems correctly
- The approach reveals LLMs' strength in structured code generation versus natural language reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs struggle with iterative reasoning in text because autoregressive architecture forces sequential problem solving, making backtracking difficult.
- Mechanism: When solving combinatorial search problems step-by-step in natural language, the model must maintain and update intermediate states in text, which is error-prone and prevents efficient exploration of alternative paths when dead ends are reached.
- Core assumption: Natural language is not an ideal medium for accurately representing and updating intermediate states in search problems.
- Evidence anchors:
  - [abstract]: "SearchBench problems require considering multiple pathways and performing backtracking, posing a significant challenge to auto-regressive models."
  - [section]: "The autoregressive architecture of current LLMs forces solving problems sequentially, which makes tasks requiring backtracking challenging."
  - [corpus]: Weak evidence - no corpus neighbors directly discuss autoregressive architecture limitations.
- Break condition: If a model architecture or prompting strategy could maintain multiple parallel reasoning paths or efficiently backtrack without re-generating from scratch.

### Mechanism 2
- Claim: LLMs excel at structured code generation, which offloads iterative search and backtracking to code execution.
- Mechanism: When prompted to generate complete A* search algorithms, LLMs leverage their strength in structured code generation, shifting the burden of iterative state space exploration from text-based reasoning to code execution where loops and data structures handle backtracking naturally.
- Core assumption: Code generation is not an iterative task for LLMs in the same way text-based reasoning is, allowing them to implement complex algorithms despite struggling with step-by-step reasoning.
- Evidence anchors:
  - [abstract]: "performance is significantly boosted when we prompt models to generate a complete A* search algorithm—a comparatively more cognitively difficult task. This approach effectively offloads the iterative search and backtracking process from the models, which they struggle with in text."
  - [section]: "This approach succeeds because it leverages the model's strength in code generation, which is not an iterative task, offloading the exploration of a large action space from LLMs to code execution."
  - [corpus]: Weak evidence - corpus neighbors don't discuss code generation strengths of LLMs.
- Break condition: If code generation becomes as challenging for LLMs as text-based reasoning, or if the generated code cannot be executed correctly.

### Mechanism 3
- Claim: The Multi-Stage-Multi-Try (MSMT) inference method improves code quality by decomposing algorithm generation into stages with validation.
- Mechanism: MSMT divides code generation into two stages - first generating instance-agnostic A* algorithm, then implementing problem-specific initialization. Simple unit tests validate each stage, filtering out erroneous generations and ensuring correctness before proceeding.
- Core assumption: Decomposing complex tasks and validating intermediate outputs improves LLM performance on combinatorial problems.
- Evidence anchors:
  - [abstract]: "Our MSMT A* method (Fig. 2) significantly improves LLMs' ability to solve search problems, outperforming other prompting strategies we used"
  - [section]: "This approach decomposes the task of implementing the search algorithm into two stages and checks the model's generations against external validators; we use simple unit tests instead of external data sources or solved solution instances."
  - [corpus]: Weak evidence - corpus neighbors don't discuss decomposition strategies for LLMs.
- Break condition: If the unit tests are insufficient to catch all errors or if the decomposition adds too much complexity for the model to handle.

## Foundational Learning

- Concept: Combinatorial search problems requiring backtracking
  - Why needed here: Understanding why LLMs fail on these problems requires grasping the nature of combinatorial search spaces and why backtracking is essential but challenging for autoregressive models.
  - Quick check question: What makes combinatorial search problems different from sequential reasoning tasks, and why does this difference matter for LLM performance?

- Concept: A* search algorithm and admissible heuristics
  - Why needed here: The paper's solution approach relies on generating A* algorithms with proper heuristics, so understanding this algorithm and what makes a heuristic admissible and consistent is crucial.
  - Quick check question: What properties must a heuristic have to be both admissible and consistent in A* search, and why are these properties important for finding optimal solutions?

- Concept: Evaluation metrics beyond correctness (feasibility, optimality)
  - Why needed here: The paper uses multi-dimensional evaluation, so understanding what feasibility and optimality mean in this context is important for interpreting results.
  - Quick check question: How do feasibility, correctness, and optimality differ as evaluation metrics, and what does each reveal about LLM performance on search problems?

## Architecture Onboarding

- Component map: SearchBench benchmark generation pipeline -> Evaluation pipeline (feasibility, correctness, optimality checks) -> LLM prompting strategies (text-based, code-based, A*, MSMT A*) -> Code execution environment for generated algorithms -> Unit test validation system for MSMT approach

- Critical path: Problem instance → LLM prompt → Code generation → Code execution → Solution validation → Metric calculation

- Design tradeoffs:
  - Text-based prompting vs code-based prompting: Text allows natural reasoning but struggles with state updates; code enables systematic search but requires correct implementation.
  - Single-shot vs multi-stage prompting: Single-shot is simpler but less robust; multi-stage with validation improves quality but adds complexity.
  - Fixed vs dynamic instance generation: Fixed instances ensure reproducibility; dynamic generation could provide more comprehensive evaluation but risks contamination.

- Failure signatures:
  - Infeasible solutions: Generated code violates problem rules or constraints
  - Incorrect solutions: Code executes but doesn't reach goal state
  - Non-optimal solutions: Code finds feasible solution but not minimum cost
  - Execution errors: Generated code doesn't compile or runs into runtime errors
  - Unit test failures: MSMT stage 1 or stage 2 code fails validation

- First 3 experiments:
  1. Run SearchBench problems with 0-shot text prompting to establish baseline LLM performance
  2. Implement and test 4-shot A* prompting on a subset of SearchBench problems to verify code generation approach
  3. Add unit test validation to A* generation and measure improvement in solution quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the fundamental limitation in LLMs' reasoning that prevents them from solving combinatorial search problems end-to-end in natural language?
- Basis in paper: [explicit] The authors show that even advanced LLMs fail at iterative reasoning and backtracking when solving search problems step-by-step in text, but significantly improve when generating complete A* algorithms.
- Why unresolved: While the paper demonstrates this limitation empirically, it doesn't explain the underlying architectural or cognitive reason why auto-regressive models struggle with iterative reasoning.
- What evidence would resolve it: Systematic ablation studies varying model architecture (e.g., comparing transformers with different attention mechanisms) or training paradigms could reveal whether this limitation is inherent to auto-regressive models or can be mitigated through architectural changes.

### Open Question 2
- Question: How can the gap between LLM-generated A* implementations and optimal solutions be further reduced?
- Basis in paper: [explicit] The authors note that GPT-4's A* implementations were 213x slower than optimal solutions, and even with MSMT A* prompting, optimal performance was only 28.6%.
- Why unresolved: The paper shows that while code generation helps, the heuristics and algorithms generated by LLMs are still significantly suboptimal compared to hand-crafted solutions.
- What evidence would resolve it: Developing more sophisticated prompting strategies or training objectives specifically for algorithm generation could improve the quality of generated search algorithms and heuristics.

### Open Question 3
- Question: Can the MSMT A* framework be generalized to other types of combinatorial problems beyond search?
- Basis in paper: [inferred] The authors present MSMT A* as a successful approach for search problems, but the framework's applicability to other problem types like constraint satisfaction or planning is unexplored.
- Why unresolved: The paper focuses specifically on search problems with backtracking, but the multi-stage, multi-try approach could potentially be adapted to other combinatorial domains.
- What evidence would resolve it: Applying the MSMT framework to other combinatorial problem categories in SearchBench (like subset sum or under-determined systems) and measuring performance improvements would demonstrate its generalizability.

## Limitations

- The study demonstrates LLM limitations in iterative reasoning through controlled experiments, but the exact mechanisms behind why autoregressive architectures struggle with backtracking remain incompletely understood.
- The SearchBench benchmark, though carefully constructed, may not capture all aspects of real-world search problems, limiting generalizability.
- While code-based approaches significantly outperform text-based reasoning, the generality of these findings across different problem domains and LLM architectures requires further validation.

## Confidence

- **High confidence**: The empirical finding that LLMs perform poorly on search problems when solving them end-to-end in natural language, and that code generation approaches significantly improve performance. This is well-supported by systematic experiments across multiple models and problem types.
- **Medium confidence**: The interpretation that autoregressive architecture inherently limits backtracking capabilities. While the evidence supports this, alternative explanations (such as prompting strategies or problem representation) cannot be completely ruled out.
- **Medium confidence**: The claim that code generation is "not an iterative task" for LLMs. While code generation appears to help, the underlying cognitive mechanisms require deeper investigation.

## Next Checks

1. **Architecture Ablation**: Test whether the performance gap between text and code approaches persists when using non-autoregressive or state-tracking LLM variants, to isolate whether the limitation is truly architectural or can be mitigated through different model designs.

2. **Cross-Domain Generalization**: Apply the MSMT A* approach to search problems from different domains (e.g., planning, optimization, game solving) to determine whether the observed improvements transfer beyond the specific SearchBench problems.

3. **Human Evaluation of Generated Code**: Conduct qualitative analysis of LLM-generated A* implementations to identify common error patterns and assess whether the generated code demonstrates genuine algorithmic understanding or surface-level pattern matching.