---
ver: rpa2
title: 'PHLP: Sole Persistent Homology for Link Prediction - Interpretable Feature
  Extraction'
arxiv_id: '2404.15225'
source_url: https://arxiv.org/abs/2404.15225
tags:
- graph
- phlp
- node
- drnl
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the interpretability challenge in link prediction
  (LP) for graph data by proposing a novel method, PHLP, which uses persistent homology
  (PH) to extract topological features without relying on neural networks. The key
  idea is to analyze how the presence or absence of target links affects the overall
  graph topology by utilizing angle hop subgraphs and a new node labeling method called
  Degree DRNL.
---

# PHLP: Sole Persistent Homology for Link Prediction - Interpretable Feature Extraction

## Quick Facts
- **arXiv ID**: 2404.15225
- **Source URL**: https://arxiv.org/abs/2404.15225
- **Reference count**: 40
- **Primary result**: PHLP achieves state-of-the-art performance on Power dataset and improves GNN models when hybridized

## Executive Summary
This paper addresses the interpretability challenge in link prediction for graph data by proposing PHLP, a method that uses persistent homology to extract topological features without relying on neural networks. The key innovation is analyzing how the presence or absence of target links affects graph topology using angle hop subgraphs and a new node labeling method called Degree DRNL. PHLP performs comparably to state-of-the-art models across benchmark datasets and achieves the highest AUC score on the Power dataset. Additionally, incorporating PHLP's outputs into existing GNN-based SOTA models improves their performance across all datasets, demonstrating the effectiveness of the topological features extracted by PHLP.

## Method Summary
PHLP extracts topological features for link prediction by computing persistence images from angle hop subgraphs using Degree DRNL node labeling. For each candidate link, the method generates subgraphs both with and without the link, computes their topological signatures via persistent homology, and uses the difference between these signatures as predictive features. These features are fed to a simple MLP classifier. The multi-angle variant (MA-PHLP) aggregates information across different angular perspectives. The method can also be hybridized with GNN-based models by concatenating the persistence image vectors with existing features before classification.

## Key Results
- PHLP achieves competitive AUC scores compared to state-of-the-art models across eight benchmark datasets
- PHLP obtains the highest AUC score on the Power dataset, which has low graph density
- Incorporating PHLP features into existing GNN-based SOTA models improves performance across all benchmark datasets
- The method provides interpretable topological features while maintaining competitive predictive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using Degree DRNL with angle hop subgraphs captures topological differences in link existence that standard node labeling misses.
- Mechanism: Standard DRNL produces identical node label patterns for structurally different graphs, leading to identical persistence images. Degree DRNL incorporates node degree, creating distinct topological signatures when links are present vs. absent. Angle hop subgraphs allow viewing connectivity from multiple perspectives, capturing richer topological features.
- Core assumption: Topological features extracted from (k,l)-angle hop subgraphs are informative for link prediction and distinguishable when Degree DRNL is applied.
- Evidence anchors:
  - [abstract] "PHLP utilizes the angle hop subgraph and new node labeling called degree double radius node labeling (Degree DRNL), distinguishing the information of graphs better than DRNL."
  - [section] "Degree DRNL encounters limitations when the graph is transformed into node-label information...To incorporate the local topology of each node with the effects of DRNL, we introduced Degree DRNL."
- Break condition: If topological features from different link states are not distinguishable in persistence images, the method fails to capture link-specific topology.

### Mechanism 2
- Claim: Comparing persistence images with and without target links creates effective features for link prediction.
- Mechanism: For each candidate link, the method computes persistence images on the subgraph both with and without the link. The difference between these images captures how link existence changes the graph's topology. These difference features are then fed to a simple classifier.
- Core assumption: Topological changes induced by link presence/absence are detectable and predictive of link existence.
- Evidence anchors:
  - [abstract] "The key idea is to analyze how the presence or absence of target links affects the overall graph topology"
  - [section] "To use the topological information of subgraphs for LP, we measure how the topological information changes depending on the existence of the target link"
- Break condition: If topological changes are not statistically significant or if simple classifiers cannot distinguish these changes, the approach fails.

### Mechanism 3
- Claim: Hybridizing PHLP features with GNN-based methods improves performance across datasets.
- Mechanism: PHLP computes persistence images that are transformed to match GNN feature dimensions and concatenated with existing features before classification. This adds topological information that GNNs might miss.
- Core assumption: GNNs benefit from additional topological features that complement learned representations.
- Evidence anchors:
  - [abstract] "Incorporating the outputs calculated using PHLP into the existing GNN-based SOTA models improves performance across all benchmark datasets"
  - [section] "Simply concatenating the PI vector calculated using PHLP with the final output of the SEAL model increases AUC scores for all datasets"
- Break condition: If concatenated features create noise or redundancy, performance gains disappear or degrade.

## Foundational Learning

- **Concept: Persistent Homology**
  - Why needed here: Provides a way to quantify topological features (connected components, loops) across multiple scales in graph data
  - Quick check question: What do the birth and death values in a persistence diagram represent in graph filtration?

- **Concept: Rips Filtration**
  - Why needed here: Creates a sequence of simplicial complexes from a graph based on edge weights, enabling topological analysis
  - Quick check question: How does the Rips filtration construct higher-dimensional simplices from pairwise connections?

- **Concept: Persistence Images**
  - Why needed here: Converts persistence diagrams into fixed-size vectors that can be used with standard machine learning classifiers
  - Quick check question: What is the purpose of the weight function ϕ(b,d′) in the persistence surface definition?

## Architecture Onboarding

- **Component map**: Input Graph -> Angle Hop Subgraph Extractor -> Degree DRNL Labeling -> Edge-weight Function -> Rips Filtration -> Persistence Image Generator -> Classifier (MLP)

- **Critical path**:
  1. Extract angle hop subgraph
  2. Apply Degree DRNL labeling
  3. Compute edge weights and Rips filtration
  4. Generate persistence images
  5. Concatenate with/without link images
  6. Classify with MLP

- **Design tradeoffs**:
  - Single vs multi-angle: MA-PHLP aggregates across angles but increases computation
  - k vs l in angle hops: Different combinations capture different topological perspectives
  - Dimension of PH: Zero-dimensional PH is faster but may miss important features; higher dimensions capture more but are computationally expensive

- **Failure signatures**:
  - Similar persistence images for positive and negative links indicate insufficient topological discrimination
  - Performance drops when max hop increases suggest overfitting to larger subgraphs
  - No improvement from hybrid methods suggests topological features are redundant with existing features

- **First 3 experiments**:
  1. Verify angle hop subgraph extraction produces expected vertex/edge sets for simple test graphs
  2. Compare persistence images from Degree DRNL vs DRNL on graphs where they should differ
  3. Test classifier performance on synthetic graphs where topological differences are known to be predictive

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PHLP vary with different graph densities and structures, such as scale-free versus random networks?
- Basis in paper: [inferred] The paper demonstrates PHLP's effectiveness on various datasets, but does not systematically analyze performance across different graph types or densities.
- Why unresolved: The paper does not provide a comparative analysis of PHLP's performance across diverse graph structures or densities.
- What evidence would resolve it: Experiments comparing PHLP's performance on datasets with varying densities and structures, such as scale-free, random, and small-world networks.

### Open Question 2
- Question: Can PHLP be effectively extended to dynamic graphs where links are added or removed over time?
- Basis in paper: [inferred] The paper focuses on static link prediction, but does not address the challenge of dynamic graphs.
- Why unresolved: The paper does not explore the applicability of PHLP to dynamic graph scenarios.
- What evidence would resolve it: Experiments demonstrating PHLP's performance on dynamic graph datasets, such as temporal social networks or evolving biological networks.

### Open Question 3
- Question: How does the choice of angle hop subgraph parameters (k and l) affect the interpretability and performance of PHLP?
- Basis in paper: [explicit] The paper discusses the use of angle hop subgraphs and proposes MA-PHLP to aggregate information from multiple angles, but does not provide a detailed analysis of the impact of k and l choices.
- Why unresolved: The paper does not systematically explore the relationship between angle hop parameters and PHLP's performance or interpretability.
- What evidence would resolve it: Experiments varying k and l parameters across different datasets, analyzing the trade-offs between performance and interpretability.

## Limitations

- **Implementation details missing**: The Degree DRNL node labeling function is not fully specified, making exact reproduction difficult.
- **Limited ablation studies**: The paper lacks detailed analysis of which topological features are most predictive and whether PHLP features complement or simply add noise to GNN models.
- **Computational efficiency**: The paper does not thoroughly analyze computational costs compared to neural methods, particularly for larger graphs or higher-dimensional PH computations.

## Confidence

- **PHLP's interpretability claim (High confidence)**: The use of persistent homology with clear topological interpretation is well-established in the literature. The Degree DRNL modification appears sound, though implementation details are lacking.
- **Performance improvements over baselines (Medium confidence)**: While AUC scores are reported, the lack of detailed ablation studies and hyperparameter tuning makes it difficult to assess whether improvements are robust or dataset-specific.
- **Hybrid method effectiveness (Medium confidence)**: The claim that PHLP features improve GNN models is supported by results, but the mechanism (complementary information vs. noise injection) is not clearly established.

## Next Checks

1. **Implement and verify Degree DRNL**: Create test cases where Degree DRNL should produce different outputs than standard DRNL, and verify the implementation captures these differences correctly.

2. **Ablation study on homology dimensions**: Systematically test PHLP performance using only 0-dimensional PH, only 1-dimensional PH, and their combination to understand which topological features are most predictive.

3. **Feature importance analysis**: Apply permutation importance or SHAP values to the hybrid models to determine whether PHLP features are truly complementary to GNN features or simply adding regularization noise.