---
ver: rpa2
title: A Parameter Update Balancing Algorithm for Multi-task Ranking Models in Recommendation
  Systems
arxiv_id: '2410.05806'
source_url: https://arxiv.org/abs/2410.05806
tags:
- task
- multi-task
- tasks
- methods
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the seesaw problem in multi-task learning
  for recommendation systems, where training multiple tasks together leads to imbalanced
  performance across tasks. The authors identify that conventional gradient balancing
  methods assume optimal joint gradients lead to optimal parameter updates, but this
  assumption breaks down when using momentum-based optimizers like Adam due to significant
  deviation between gradients and actual parameter updates.
---

# A Parameter Update Balancing Algorithm for Multi-task Ranking Models in Recommendation Systems

## Quick Facts
- arXiv ID: 2410.05806
- Source URL: https://arxiv.org/abs/2410.05806
- Reference count: 40
- Multi-task ranking models suffer from task imbalance due to momentum-based optimizer deviations

## Executive Summary
This paper addresses the seesaw problem in multi-task learning for recommendation systems, where training multiple tasks together leads to imbalanced performance across tasks. The authors identify that conventional gradient balancing methods assume optimal joint gradients lead to optimal parameter updates, but this assumption breaks down when using momentum-based optimizers like Adam due to significant deviation between gradients and actual parameter updates. They propose Parameter Update Balancing (PUB), a novel multi-task optimization method that directly balances parameter updates rather than gradients, formalizing the problem as a convex optimization task.

The authors demonstrate PUB's effectiveness through comprehensive experiments on AliExpress ranking datasets, showing significant improvements across four countries (US, NL, ES, FR) and multiple backbone architectures (MMOE and PLE). The method also shows strong generalization on NYUv2 scene understanding dataset and achieves significant improvements in real-world deployment on HUAWEI AppGallery, increasing eCPM by 4.06% on average in online A/B testing. PUB demonstrates flexibility by integrating with update manipulation methods like Clippy and AdaTask for further improvements.

## Method Summary
The paper introduces Parameter Update Balancing (PUB), a multi-task optimization method that directly balances parameter updates rather than gradients. The key insight is that conventional gradient balancing methods fail when using momentum-based optimizers like Adam because the relationship between gradients and actual parameter updates becomes significantly distorted. PUB formulates the parameter update balancing problem as a convex optimization task, using an efficient approximation method to find optimal task weights for update combination. The method can be applied to any backbone architecture (such as MMOE and PLE) and demonstrates flexibility by integrating with existing update manipulation techniques like Clippy and AdaTask.

## Key Results
- PUB significantly improves both MMOE and PLE backbones across four countries (US, NL, ES, FR)
- Consistently outperforms state-of-the-art baselines at all task metrics
- Achieves 4.06% average eCPM improvement in real-world deployment on HUAWEI AppGallery
- Demonstrates strong generalization on NYUv2 scene understanding dataset

## Why This Works (Mechanism)
The core mechanism addresses a fundamental flaw in gradient-based multi-task optimization: when using momentum-based optimizers like Adam, the actual parameter updates deviate significantly from the raw gradients. This deviation breaks the assumption underlying conventional gradient balancing methods, which assume that optimizing gradients directly optimizes parameter updates. PUB circumvents this issue by directly optimizing the parameter updates themselves through convex optimization, ensuring that the final model parameters reflect the intended balance across tasks.

## Foundational Learning
- Multi-task Learning Optimization: Understanding how to balance multiple objectives in neural networks is crucial for recommendation systems where different metrics (click-through rate, conversion rate, etc.) need to be optimized simultaneously.
  - Why needed: Recommendation systems must optimize multiple competing objectives simultaneously
  - Quick check: Can identify seesaw problem where optimizing one task degrades others

- Momentum-based Optimizers (Adam): These optimizers use historical gradient information to accelerate convergence, but introduce non-linear transformations between gradients and parameter updates.
  - Why needed: Understanding why gradient balancing fails requires knowledge of optimizer mechanics
  - Quick check: Can explain difference between gradient and parameter update in Adam

- Convex Optimization: The mathematical framework used to solve the parameter update balancing problem efficiently.
  - Why needed: Provides tractable solution to the otherwise intractable balancing problem
  - Quick check: Can formulate the balancing as a convex optimization problem

## Architecture Onboarding

Component Map: Task heads -> Shared backbone -> Parameter update balancing layer -> Optimizer

Critical Path: Task-specific gradients → PUB convex optimization → Combined parameter updates → Model parameter update

Design Tradeoffs:
- Direct update balancing vs gradient balancing: PUB trades computational complexity for accuracy
- Convex approximation vs exact solution: Balances efficiency with optimality
- Task weight optimization vs fixed weights: Allows dynamic adaptation but requires more computation

Failure Signatures:
- Performance degradation when task correlations are highly non-linear
- Increased training instability with very high task imbalance ratios
- Computational overhead becomes prohibitive for extremely large parameter spaces

First Experiments:
1. Ablation study comparing PUB vs gradient balancing on synthetic multi-task data with known optimal solutions
2. Sensitivity analysis of PUB performance across different task imbalance ratios
3. Integration test with Clippy and AdaTask to verify compatibility with existing optimization methods

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation scope remains relatively narrow, limited to four AliExpress markets and one scene understanding dataset
- Computational overhead of the convex optimization step for parameter update balancing was not thoroughly analyzed for large-scale production systems
- Generalization to other recommendation domains (music streaming, social media feeds) remains untested

## Confidence
High confidence in technical contribution and empirical improvements, as the mathematical formulation is rigorous and experimental results are comprehensive.
Medium confidence in claims about PUB being "parameter-agnostic" since only MMOE and PLE backbones were tested.
Medium confidence in generalization claim to NYUv2 dataset as it represents a single non-recommendation task.

## Next Checks
1. Evaluate PUB's performance across diverse recommendation domains (music, video, social media) to verify cross-domain generalization.
2. Conduct ablation studies to quantify the computational overhead introduced by the convex optimization step, particularly in distributed training scenarios.
3. Test PUB's robustness to different task imbalance ratios and correlation structures between tasks to identify potential failure modes.