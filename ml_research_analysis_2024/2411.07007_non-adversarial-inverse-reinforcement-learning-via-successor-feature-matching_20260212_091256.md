---
ver: rpa2
title: Non-Adversarial Inverse Reinforcement Learning via Successor Feature Matching
arxiv_id: '2411.07007'
source_url: https://arxiv.org/abs/2411.07007
tags:
- learning
- policy
- features
- expert
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Successor Feature Matching (SFM), a novel
  non-adversarial approach to inverse reinforcement learning (IRL) that learns from
  expert demonstrations without requiring action labels. The method leverages successor
  features to match the expert's feature occupancy through direct policy optimization,
  eliminating the need for costly adversarial game-solving or explicit reward function
  learning.
---

# Non-Adversarial Inverse Reinforcement Learning via Successor Feature Matching
## Quick Facts
- arXiv ID: 2411.07007
- Source URL: https://arxiv.org/abs/2411.07007
- Reference count: 40
- Key outcome: SFM outperforms state-only adversarial methods (GAIfO, MM, OPOLO) and non-adversarial baselines (BC, IQ-Learn) by 16% in mean normalized return on DeepMind Control tasks

## Executive Summary
This paper introduces Successor Feature Matching (SFM), a novel non-adversarial approach to inverse reinforcement learning that learns from expert demonstrations without requiring action labels. SFM leverages successor features to match the expert's feature occupancy through direct policy optimization, eliminating the need for costly adversarial game-solving or explicit reward function learning. The method demonstrates superior performance compared to state-of-the-art baselines, particularly in sample-efficient scenarios with limited demonstrations.

## Method Summary
SFM learns a policy by matching the feature occupancy between the agent and expert trajectories. It parameterizes both agent and expert policies, then uses policy gradient updates to align the agent's successor features with those of the expert. The method optimizes the objective L(π_θ) = 1/2 ||Φ(π_θ) - Φ(π_E)||^2, where Φ represents the successor features. Unlike adversarial approaches, SFM avoids the min-max optimization problem and instead directly optimizes for feature matching through policy updates.

## Key Results
- SFM outperforms state-only adversarial methods (GAIfO, MM, OPOLO) and non-adversarial baselines (BC, IQ-Learn) by 16% in mean normalized return
- Demonstrates strong performance with as few as a single state-only demonstration
- Shows robustness to policy optimizer choice, maintaining performance even with weaker RL algorithms like TD3

## Why This Works (Mechanism)
SFM works by leveraging the mathematical relationship between successor features and occupancy measures. Successor features capture the expected cumulative discounted sum of features when following a policy, providing a compressed representation of the policy's behavior. By matching these features between expert and agent policies, SFM indirectly aligns their occupancy measures without needing to estimate the reward function explicitly. This approach avoids the instability and computational cost of adversarial training while maintaining the theoretical benefits of feature matching.

## Foundational Learning
- **Successor Features**: Expected cumulative discounted sum of features when following a policy; needed to capture long-term behavior without explicit reward estimation; quick check: verify the Bellman equation holds for your feature representation
- **Occupancy Measures**: Distribution over states visited when following a policy; needed as the underlying quantity being matched; quick check: ensure your feature expectations correspond to weighted state visitation
- **Policy Gradients**: Method for optimizing policies through gradient descent; needed for direct policy optimization without reward functions; quick check: verify your gradient estimator has low variance on simple tasks
- **Feature Matching**: Technique for aligning distributions through summary statistics; needed to avoid adversarial training instability; quick check: test feature matching on synthetic distributions first
- **Inverse Reinforcement Learning**: Learning from expert demonstrations without action labels; needed as the target problem; quick check: validate your method recovers known reward functions in controlled settings

## Architecture Onboarding
**Component Map**: Expert demonstrations -> Feature extractor -> Successor feature generator -> Policy optimizer -> Agent policy

**Critical Path**: 1) Extract features from expert demonstrations 2) Compute expert successor features 3) Initialize agent policy 4) Generate agent trajectories 5) Compute agent successor features 6) Calculate feature matching loss 7) Update agent policy via gradient descent 8) Repeat until convergence

**Design Tradeoffs**: SFM trades off the theoretical optimality guarantees of reward-based IRL for practical stability and sample efficiency. The method sacrifices some flexibility in handling arbitrary reward structures for the benefit of avoiding adversarial training and explicit reward learning.

**Failure Signatures**: Poor feature selection leading to insufficient state information representation, instability when expert demonstrations are noisy or suboptimal, degraded performance when the feature space dimensionality is too high relative to available data.

**First Experiments**: 1) Validate successor feature computation on a known MDP with ground truth rewards 2) Test feature matching on synthetic state distributions before adding policy optimization 3) Compare against behavior cloning on a simple continuous control task

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical convergence guarantees remain limited to specific reward function classes
- Performance in high-dimensional continuous state spaces beyond tested DeepMind Control suite is unclear
- Sensitivity to feature selection and potential for suboptimal policies with noisy demonstrations not thoroughly explored
- Computational overhead compared to direct imitation methods in sample-constrained settings requires further investigation

## Confidence
High confidence: Superior empirical performance compared to state-only adversarial methods and non-adversarial baselines across multiple DeepMind Control tasks; clear methodological description

Medium confidence: Performance maintenance with weaker RL algorithms like TD3; robustness claims regarding policy optimizer choice

Low confidence: Fundamental superiority to all existing state-only IRL methods in all scenarios due to limited benchmark comparisons

## Next Checks
1. Test SFM's performance on continuous control tasks with significantly higher dimensional state spaces, such as Humanoid or Ant from the DeepMind Control suite, to evaluate scalability

2. Conduct ablation studies to quantify the impact of feature selection on SFM's performance, including comparisons with randomly initialized features and features learned from unsupervised pre-training

3. Evaluate SFM's robustness to noisy demonstrations by systematically adding varying levels of noise to expert trajectories and measuring performance degradation