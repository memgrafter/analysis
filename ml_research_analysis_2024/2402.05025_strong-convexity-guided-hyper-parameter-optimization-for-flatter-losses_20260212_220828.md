---
ver: rpa2
title: Strong convexity-guided hyper-parameter optimization for flatter losses
arxiv_id: '2402.05025'
source_url: https://arxiv.org/abs/2402.05025
tags:
- strong
- convexity
- optimization
- learning
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel white-box hyper-parameter optimization
  algorithm that aims to minimize the strong convexity of the loss function. The authors
  establish a theoretical connection between the strong convexity of the loss and
  its flatness, and use this to motivate a method that searches for hyper-parameter
  configurations that minimize the strong convexity.
---

# Strong convexity-guided hyper-parameter optimization for flatter losses

## Quick Facts
- arXiv ID: 2402.05025
- Source URL: https://arxiv.org/abs/2402.05025
- Reference count: 39
- Primary result: Novel white-box HPO algorithm that minimizes strong convexity of loss function, achieving strong performance with fewer training runs compared to popular HPO methods

## Executive Summary
This paper introduces a novel white-box hyper-parameter optimization algorithm that targets the strong convexity of the loss function to achieve flatter minima and better generalization. The authors establish a theoretical connection between strong convexity and loss landscape flatness, then develop a method that searches for hyper-parameter configurations minimizing this measure. The approach uses closed-form equations to approximate strong convexity based on network structure and performs a randomized search over configurations, requiring only one-epoch training runs for evaluation rather than full training cycles.

The method is evaluated across 14 classification datasets, demonstrating competitive performance with significantly reduced computational cost. On MNIST, it achieves 98.65% accuracy using only 10 full training runs, outperforming Hyperopt (97.22%) and approaching Random Search (98.95%). The authors also provide theoretical results connecting strong convexity to the Polyak-Łojasiewicz inequality, showing that minimizing strong convexity implies more iterations are required for convergence.

## Method Summary
The method samples N1 random hyper-parameter configurations, trains each for one epoch, computes strong convexity across mini-batches using the supremum of the Frobenius norm of the Hessian, selects the top N2 configurations with lowest strong convexity values, and fully trains these selected configurations to return the best performer. The approach requires 14 classification datasets (8 tabular from Bayesmark/MLP benchmarks, 2 image datasets MNIST and SVHN, 4 OpenML datasets) and uses accuracy or AUC as performance metrics depending on the dataset type.

## Key Results
- Achieves 98.65% accuracy on MNIST with only 10 full training runs
- Outperforms Hyperopt (97.22%) while requiring significantly fewer training iterations
- Demonstrates competitive performance with Random Search (98.95%) at lower computational cost
- Shows strong performance across 14 diverse classification datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Minimizing the strong convexity parameter improves the flatness of the loss landscape.
- **Mechanism:** Strong convexity is inversely related to the sharpness of the loss surface. Lower strong convexity means the Hessian has smaller eigenvalues, implying a flatter loss landscape. The paper derives closed-form equations to approximate strong convexity based on network structure, allowing efficient computation during HPO.
- **Core assumption:** The strong convexity of the loss can be computed efficiently for a given network architecture and hyper-parameter configuration, and this measure is a reliable proxy for flatness.
- **Evidence anchors:**
  - [abstract] "we first establish a relationship between the strong convexity of the loss and its flatness"
  - [section] "From Lemma 1, we can rewrite this as ∥∇²f(x)∥² ≥ μ ⇒ ∥∇²f(x)∥F ≥ μ"
  - [corpus] Weak anchor - corpus focuses on sharpness but not strong convexity directly
- **Break condition:** If the strong convexity computation becomes intractable for deeper or more complex architectures, or if the assumption that strong convexity maps directly to flatness fails in practice.

### Mechanism 2
- **Claim:** Using a one-epoch training run to estimate strong convexity is a good trade-off between accuracy and computational cost.
- **Mechanism:** Training for a single epoch moves the network weights closer to their final positions, providing a more accurate estimate of strong convexity than random initialization, while avoiding the full computational cost of training to convergence.
- **Core assumption:** One epoch of training is sufficient to bring the network weights close enough to their final values that the strong convexity estimate is meaningful.
- **Evidence anchors:**
  - [section] "Training for a single epoch provides a balance between the cost associated with training fully... and not training at all"
  - [abstract] "requires fewer full-length training runs... instead relying on one-epoch cycles"
  - [corpus] Weak anchor - corpus discusses sharpness but not training budget trade-offs
- **Break condition:** If one epoch is insufficient for the network to escape poor local minima, the strong convexity estimate may be misleading.

### Mechanism 3
- **Claim:** The supremum of strong convexity across mini-batches is a useful objective for HPO.
- **Mechanism:** Using the supremum across mini-batches ensures that the hyper-parameter configuration is robust across different data subsets, avoiding configurations that are only good for specific batches.
- **Core assumption:** The mini-batch approximation of the supremum is a good proxy for the full-batch supremum.
- **Evidence anchors:**
  - [section] "it is important that we look at the highest value across mini-batches, and aim to minimize that upper bound"
  - [abstract] "we seek to find hyper-parameter configurations that improve flatness by minimizing the strong convexity of the loss"
  - [corpus] Weak anchor - corpus focuses on sharpness-aware methods but not mini-batch strategies
- **Break condition:** If the mini-batch supremum is highly variable, the search may be unstable or misleading.

## Foundational Learning

- **Concept:** Strong convexity in optimization
  - **Why needed here:** The paper's entire method relies on understanding the relationship between strong convexity and loss landscape flatness. The theoretical framework depends on strong convexity as a measure of curvature.
  - **Quick check question:** If a function is μ-strongly convex, what is the lower bound on the curvature of the function?
- **Concept:** Hessian matrix and its spectral properties
  - **Why needed here:** The paper uses the Frobenius norm of the Hessian to define and compute strong convexity. Understanding eigenvalues and matrix norms is essential for grasping the theoretical derivations.
  - **Quick check question:** How does the Frobenius norm of the Hessian relate to the largest eigenvalue of the Hessian?
- **Concept:** Neural network loss surfaces and generalization
  - **Why needed here:** The motivation for the method is the established link between flat minima and generalization. Without this context, the search for low strong convexity would seem arbitrary.
  - **Quick check question:** What is the empirical relationship between the sharpness of a loss minimum and the generalization error of the model?

## Architecture Onboarding

- **Component map:** Random configuration sampler -> One-epoch trainer with strong convexity estimator -> Mini-batch strong convexity calculator (supremum across batches) -> Top-N2 selector based on strong convexity -> Full trainer for selected configurations -> Performance evaluator (accuracy/AUC)
- **Critical path:** 1. Sample random hyper-parameters 2. Train for one epoch 3. Compute strong convexity across mini-batches 4. Select top 10 lowest values 5. Train full models on selected configs 6. Return best performer
- **Design tradeoffs:**
  - One-epoch vs full training for strong convexity estimation (speed vs accuracy)
  - Supremum vs average strong convexity (robustness vs smoothness)
  - Fixed N1 vs adaptive N1 (exploration vs cost)
- **Failure signatures:**
  - All strong convexity values near zero → network may not be strongly convex
  - High variance in mini-batch strong convexity → batch size too small or unstable training
  - Top configurations perform poorly → strong convexity not a good proxy for generalization in this domain
- **First 3 experiments:**
  1. Run on a small tabular dataset (e.g., iris) with default N1=50, N2=10 to verify pipeline runs
  2. Compare strong convexity vs sharpness on a few configurations to validate the proxy
  3. Test with N1=20 to see impact of fewer configurations on final performance and runtime

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The theoretical connection between strong convexity and generalization remains primarily theoretical rather than empirically verified
- The strong convexity computation method may not scale well to very large models or datasets
- The method's performance gains come at the cost of introducing a potentially complex strong convexity computation

## Confidence
- **High confidence**: The method's computational efficiency advantage over traditional HPO methods is well-supported by the experimental results
- **Medium confidence**: The theoretical relationship between strong convexity and flatness is mathematically established, but the practical implications for generalization need more rigorous testing
- **Medium confidence**: The one-epoch training approach for strong convexity estimation appears reasonable, though the optimal training duration is not thoroughly explored

## Next Checks
1. Test the method on larger datasets (e.g., CIFAR-10/100, ImageNet) to verify if the strong convexity computation remains tractable and beneficial at scale.
2. Compare strong convexity minimization against other flatness measures (e.g., PAC-Bayes bounds, sharpness-aware minimization) to establish whether it provides unique benefits beyond existing approaches.
3. Systematically vary the training duration used for strong convexity estimation (1 epoch, 5 epochs, 10 epochs) to determine the optimal trade-off between computational cost and estimation accuracy.