---
ver: rpa2
title: Understanding and Patching Compositional Reasoning in LLMs
arxiv_id: '2402.14328'
source_url: https://arxiv.org/abs/2402.14328
tags:
- reasoning
- compositional
- llms
- query
- implicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes compositional reasoning failures in LLMs, attributing
  them to improper generation or leveraging of implicit reasoning results. Through
  Logit Lens and intervention experiments, the authors identify middle-layer MHSA
  modules as crucial for generating and utilizing implicit reasoning results.
---

# Understanding and Patching Compositional Reasoning in LLMs

## Quick Facts
- arXiv ID: 2402.14328
- Source URL: https://arxiv.org/abs/2402.14328
- Authors: Zhaoyi Li; Gangwei Jiang; Hong Xie; Linqi Song; Defu Lian; Ying Wei
- Reference count: 37
- One-line primary result: CREME achieves +17.0% correction rate, +7.99% paraphrasing accuracy, and +1.27% generalization accuracy on LLaMA-2-7B for compositional reasoning

## Executive Summary
This paper analyzes compositional reasoning failures in LLMs, attributing them to improper generation or leveraging of implicit reasoning results. Through Logit Lens and intervention experiments, the authors identify middle-layer MHSA modules as crucial for generating and utilizing implicit reasoning results. They propose CREME, a lightweight method that edits these MHSA parameters to correct compositional reasoning errors, achieving significant improvements while maintaining low specificity scores.

## Method Summary
The authors use Logit Lens to inspect hidden states at different layers for compositional and second-hop queries, observing the emergence of implicit reasoning results (o1) before explicit results (o2). They perform intervention experiments to verify the causal role of implicit results by eliminating o1 information in middle layers. Causal mediation analysis locates important MHSA modules by comparing compositional and second-hop query outputs. CREME then edits these located MHSA parameters using a closed-form solution to insert corrected key-value pairs while preserving existing memories.

## Key Results
- CREME achieves +17.0% correction rate, +7.99% paraphrasing accuracy, and +1.27% generalization accuracy on LLaMA-2-7B
- The method maintains low specificity scores, indicating minimal impact on irrelevant queries
- Middle-layer MHSA modules are identified as critical for generating and leveraging implicit reasoning results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Middle-layer MHSA modules generate and leverage implicit reasoning results that causally enable correct compositional reasoning.
- Mechanism: The model first generates an implicit reasoning result (first-hop answer) in middle layers via MHSA, then uses this to produce the final explicit reasoning result (second-hop answer) in later layers.
- Core assumption: The emergence of implicit reasoning results in hidden states precedes and influences the generation of explicit reasoning results.
- Evidence anchors:
  - [abstract] "implicit reasoning results indeed surface within middle layers and play a causative role in shaping the final explicit reasoning results"
  - [section 4.1] "implicit reasoning results not only manifest within the LLMs' intermediate layers but also tend to precede the generation of explicit reasoning results"
  - [corpus] Weak - no direct evidence in related papers
- Break condition: If the MHSA modules in middle layers are removed or disrupted, the model loses the ability to generate implicit reasoning results, causing compositional failures.

### Mechanism 2
- Claim: CREME edits the MHSA output matrix to insert corrected key-value pairs, minimally disrupting existing memories while improving compositional reasoning.
- Mechanism: By replacing the output of specific MHSA modules with their counterparts from second-hop computation graphs, CREME inserts the correct implicit reasoning result into the model's associative memory.
- Core assumption: The MHSA output matrix can be treated as a linear associative memory that stores factual associations.
- Evidence anchors:
  - [abstract] "develop CREME, a lightweight method to patch errors in compositional reasoning via editing the located MHSA modules"
  - [section 6] "we view W l O as a linear associative memory... we aim to edit W l O to ˆW l O such that... the constraint implements the edit as an insertion of (k∗, v∗) into the linear memory"
  - [corpus] Weak - no direct evidence in related papers
- Break condition: If the edited MHSA matrix becomes too distorted or the inserted key-value pairs conflict with existing knowledge, the model may fail on unrelated queries.

### Mechanism 3
- Claim: The intervention experiment proves that implicit reasoning results in middle layers have a causal effect on generating explicit reasoning results.
- Mechanism: By eliminating the information about implicit reasoning results in middle layers through intervention and observing the impact on explicit reasoning results, we demonstrate causality.
- Core assumption: The intervention can effectively remove the bias on implicit reasoning results without causing unintended side effects.
- Evidence anchors:
  - [abstract] "we resort to Logit Lens and an intervention experiment to dissect the inner hidden states of LLMs"
  - [section 4.2] "we strategically intervene on these inner hidden states to eliminate the information related to o1... and observe the resultant impact on predicting o2"
  - [corpus] Weak - no direct evidence in related papers
- Break condition: If the intervention causes numerical instability or the elimination of implicit reasoning results is incomplete, the causal relationship may not be accurately demonstrated.

## Foundational Learning

- Concept: Compositional reasoning in LLMs
  - Why needed here: Understanding how LLMs decompose complex tasks into manageable subtasks is crucial for analyzing their failures and developing solutions.
  - Quick check question: What are the two types of reasoning results involved in compositional reasoning, and how do they relate to each other?

- Concept: Logit Lens
  - Why needed here: Logit Lens is a key tool for inspecting hidden states of LLMs and understanding how information about implicit and explicit reasoning results emerges in different layers.
  - Quick check question: How does Logit Lens project hidden states into the output vocabulary space, and what does it reveal about the information content at different layers?

- Concept: Causal mediation analysis
  - Why needed here: This technique helps locate the specific modules responsible for generating and leveraging implicit reasoning results by comparing computation graphs for compositional and second-hop queries.
  - Quick check question: What is the main idea behind using causal mediation analysis to identify important modules in LLMs?

## Architecture Onboarding

- Component map: Input → Embedding → Transformer blocks (with key MHSA modules in middle layers) → Output LM head → Final answer
- Critical path: Input → Embedding → Transformer blocks (with key MHSA modules in middle layers) → Output LM head → Final answer
- Design tradeoffs: Editing MHSA modules to improve compositional reasoning may have minimal impact on unrelated queries, but excessive editing could disrupt existing knowledge.
- Failure signatures: If the model takes shortcuts in reasoning, fails to generate implicit reasoning results, or produces incomplete reasoning, it indicates problems with the MHSA modules in middle layers.
- First 3 experiments:
  1. Use Logit Lens to inspect hidden states at different layers for compositional and second-hop queries, observing the emergence of implicit and explicit reasoning results.
  2. Perform intervention experiments by eliminating information about implicit reasoning results in middle layers and measuring the impact on explicit reasoning results.
  3. Apply CREME to edit the located MHSA modules and evaluate the improvement in compositional reasoning accuracy, generalization, and specificity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do implicit reasoning results emerge and get utilized in middle layers of LLMs during compositional reasoning?
- Basis in paper: [explicit] The paper identifies middle-layer MHSA modules as crucial for generating and utilizing implicit reasoning results, with CREME achieving significant improvements by editing these modules.
- Why unresolved: While the paper locates the important MHSA modules and demonstrates their effectiveness through CREME, it does not fully explain the underlying mechanisms of how implicit reasoning results emerge and are utilized in these layers.
- What evidence would resolve it: Further analysis of the inner workings of LLMs during compositional reasoning, potentially using techniques like attention visualization or feature importance analysis, could provide insights into the emergence and utilization of implicit reasoning results.

### Open Question 2
- Question: Can the findings on compositional reasoning in LLMs be generalized to other types of reasoning tasks beyond factual knowledge?
- Basis in paper: [explicit] The paper focuses on compositional reasoning for factual knowledge and suggests future work to validate conclusions on larger-scale LLMs and other types of compositional reasoning tasks.
- Why unresolved: The paper's experiments and analysis are primarily based on factual knowledge compositional reasoning. It is unclear whether the observed patterns and mechanisms would hold for other types of reasoning, such as arithmetic reasoning or logical inference.
- What evidence would resolve it: Conducting experiments on various types of compositional reasoning tasks and comparing the results with the findings on factual knowledge could help determine the generalizability of the observed patterns and mechanisms.

### Open Question 3
- Question: How does the scale of LLMs affect the effectiveness of compositional reasoning and the impact of interventions like CREME?
- Basis in paper: [explicit] The paper acknowledges limitations in conducting experiments with larger-scale LLMs due to computational constraints and suggests future work to validate conclusions on larger-scale LLMs.
- Why unresolved: The paper's experiments are conducted on relatively smaller-scale LLMs (7B and 3B parameters). It is unclear whether the observed patterns and the effectiveness of interventions like CREME would scale up to larger models.
- What evidence would resolve it: Performing experiments on larger-scale LLMs and comparing the results with those obtained from smaller models could provide insights into the impact of model scale on compositional reasoning and the effectiveness of interventions.

## Limitations

- The paper's experiments are primarily based on factual knowledge compositional reasoning, limiting the generalizability to other types of reasoning tasks.
- The exact mechanism by which CREME's closed-form solution edits MHSA parameters without disrupting existing knowledge is not fully explained.
- The effectiveness of CREME on larger-scale LLMs (beyond 7B and 3B parameters) is unknown due to computational constraints.

## Confidence

- **High confidence**: The identification of compositional reasoning failures as stemming from improper generation or leveraging of implicit reasoning results is well-supported by experimental evidence.
- **Medium confidence**: The specific identification of middle-layer MHSA modules as critical for compositional reasoning is supported but could benefit from additional ablation studies across different model architectures.
- **Low confidence**: The exact mechanism by which CREME's closed-form solution edits MHSA parameters without disrupting existing knowledge is not fully explained, and the generality of this approach across different types of compositional reasoning tasks is uncertain.

## Next Checks

1. **Architecture Ablation**: Test CREME across different model sizes (e.g., LLaMA-2-13B, LLaMA-2-70B) and architectures to verify that middle-layer MHSA modules are consistently critical for compositional reasoning.

2. **Robustness Testing**: Evaluate CREME's performance on out-of-distribution compositional reasoning tasks not seen during training, including more complex reasoning chains and different knowledge domains.

3. **Alternative Interventions**: Compare CREME with alternative editing approaches (e.g., full parameter retraining, layer-wise intervention) to isolate whether the specific closed-form solution is necessary or if simpler interventions could achieve similar results.