---
ver: rpa2
title: Open-Endedness is Essential for Artificial Superhuman Intelligence
arxiv_id: '2406.04268'
source_url: https://arxiv.org/abs/2406.04268
tags:
- open-ended
- open-endedness
- system
- systems
- observer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal definition of open-endedness for
  AI systems, framing it as the ability to produce increasingly novel and learnable
  artifacts from the perspective of an observer. Novelty means that artifacts become
  harder to predict over time, while learnability means that longer observation histories
  improve prediction accuracy.
---

# Open-Endedness is Essential for Artificial Superhuman Intelligence

## Quick Facts
- arXiv ID: 2406.04268
- Source URL: https://arxiv.org/abs/2406.04268
- Reference count: 40
- Key outcome: Open-endedness—defined as producing increasingly novel and learnable artifacts—is essential for ASI, and combining foundation models with open-ended algorithms offers a path forward while raising critical safety considerations.

## Executive Summary
This paper introduces a formal definition of open-endedness for AI systems, framing it as the ability to produce increasingly novel and learnable artifacts from the perspective of an observer. Novelty means that artifacts become harder to predict over time, while learnability means that longer observation histories improve prediction accuracy. The authors argue that such open-endedness is essential for creating artificial superhuman intelligence (ASI), as it enables continual, self-improving discovery beyond static training datasets. They illustrate how existing AI systems—like AlphaGo, AdA, and POET—exhibit open-endedness in narrow domains, but lack the generality needed for ASI. The paper proposes combining foundation models with open-ended algorithms (via reinforcement learning, self-improvement, task generation, and evolutionary methods) as a path toward ASI. It also highlights critical safety considerations, emphasizing the need for interpretability and human oversight as systems become increasingly capable.

## Method Summary
The paper formalizes open-endedness as a system property where artifacts become both increasingly unpredictable (novel) and more predictable with longer observation histories (learnable) to an observer. This definition is operationalized through statistical modeling of artifact predictability and is applied to analyze existing systems and propose future directions for ASI. The authors advocate combining foundation models with open-ended algorithms to generate novel, human-relevant artifacts, while addressing safety via interpretability and human oversight.

## Key Results
- Open-endedness is formally defined as novelty (increasing unpredictability) plus learnability (improving predictability with longer histories).
- ASI requires open-endedness because static datasets will soon be exhausted and self-directed discovery is needed for continual improvement.
- Foundation models combined with open-ended algorithms can enable ASI by merging general knowledge with self-directed exploration.
- Human observers are critical for ASI utility and safety, as artifacts must remain comprehensible to be useful and interpretable.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open-endedness is essential for ASI because static datasets will soon be exhausted, and ASI must generate its own learning signals.
- Mechanism: The paper argues that foundation models trained on static, internet-scale data will plateau because novelty cannot be sustained once the epistemic uncertainty is modeled. Open-ended systems, by contrast, continuously produce novel and learnable artifacts, enabling indefinite self-improvement.
- Core assumption: Open-endedness can be formally defined and measured using novelty (increasing unpredictability) and learnability (improving predictability with more history).
- Evidence anchors:
  - [abstract] "extrapolating, we may soon be running out of high-quality textual and visual data for training such models... open-endedness is unlikely to arise for free by training on ever-larger datasets."
  - [section 2.1] "A system displays novelty if artifacts become increasingly unpredictable with respect to the observer’s model... The system is learnable whenever conditioning on a longer history makes artifacts more predictable."
- Break condition: If the observer's model cannot improve over time due to inherent stochasticity (e.g., pure noise), the system fails novelty or learnability, breaking open-endedness.

### Mechanism 2
- Claim: Foundation models + open-ended algorithms enable ASI by combining general knowledge with self-directed exploration.
- Mechanism: Foundation models provide broad, human-relevant priors and mutation operators; open-ended algorithms (e.g., RL, evolutionary methods) generate novel tasks and artifacts. This synergy allows systems to explore beyond static datasets while staying aligned with human concepts of "interestingness."
- Core assumption: Foundation models capture human notions of interestingness and can serve as general sequence modelers for variation and selection.
- Evidence anchors:
  - [abstract] "open-ended algorithms endow foundation models with the ability to uncover new knowledge, while foundation models guide the search space for open-ended AI towards discovering human-relevant artifacts efficiently."
  - [section 3] "foundation models have been trained on vast amounts of human data, they capture human notions of interestingness... serve as general mutation operators."
- Break condition: If the mutation/selection operators fail to produce artifacts that are both novel and learnable to a human observer, the open-endedness property breaks down.

### Mechanism 3
- Claim: Open-endedness is observer-dependent, and ASI must remain learnable to humans to be useful and safe.
- Mechanism: The definition requires that artifacts be novel and learnable to an external observer. ASI must generate artifacts that continue to surprise and educate human observers; otherwise, it loses relevance or safety.
- Core assumption: Human observers are pre-eminent for ASI utility and safety, and their cognitive limitations bound the timescale of open-endedness.
- Evidence anchors:
  - [section 2.3] "the choice of observer is a free parameter of great importance... there is a pre-eminent class of observers, namely humans."
  - [section 4.2] "In order to provide informed oversight and direction when guiding an open-ended system, human observers need to at least partially understand the significance of the new artifacts."
- Break condition: If the ASI produces artifacts beyond human comprehension or memory capacity, it ceases to be learnable, breaking open-endedness from the human perspective.

## Foundational Learning

- Concept: Formal definition of open-endedness (novelty + learnability).
  - Why needed here: Provides a measurable, actionable criterion to evaluate whether a system is truly open-ended rather than just producing random or repetitive outputs.
  - Quick check question: Can you state the two conditions (novelty and learnability) that together define open-endedness for an observer?

- Concept: Observer dependency in open-endedness.
  - Why needed here: Clarifies that open-endedness is not an intrinsic property of the system alone but depends on the observer's ability to learn from and be surprised by the artifacts.
  - Quick check question: Why might the same system be open-ended to a human but not to an AI oracle?

- Concept: Synergy between foundation models and open-ended algorithms.
  - Why needed here: Explains how combining general knowledge (foundation models) with self-directed exploration (open-ended algorithms) can overcome the limitations of static training data.
  - Quick check question: What roles do foundation models and open-ended algorithms play in generating novel and learnable artifacts?

## Architecture Onboarding

- Component map:
  - Foundation Model (FM) -> Open-Ended Algorithm (OEA) -> Observer Module -> Feedback Loop -> Safety Interface

- Critical path:
  1. FM generates candidate artifacts or variations.
  2. OEA proposes new tasks or environments.
  3. Observer evaluates novelty (unpredictability) and learnability (predictability improvement).
  4. Feedback adjusts FM and OEA to maximize both novelty and learnability.
  5. Safety checks ensure artifacts are interpretable and beneficial.

- Design tradeoffs:
  - Broad vs. narrow domain: Broader domains increase novelty potential but risk losing learnability for human observers.
  - Observer complexity: More sophisticated observers can detect finer novelty but increase computational cost.
  - Safety vs. capability: Stronger open-endedness may produce harder-to-interpret artifacts, challenging oversight.

- Failure signatures:
  - Novelty plateau: Artifacts stop becoming more unpredictable.
  - Learnability loss: Observer cannot improve predictions even with more history.
  - Safety breach: Artifacts become incomprehensible or misaligned with human values.

- First 3 experiments:
  1. Implement a simple open-ended system (e.g., evolving text prompts) and measure novelty/learnability for a human observer over time.
  2. Add a foundation model to the mutation step and compare novelty/learnability gains vs. baseline.
  3. Introduce an observer module that learns to predict artifacts and use its loss to guide the open-ended algorithm; measure improvements in both novelty and learnability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does a self-observing open-ended system (where the observer is part of the system) produce artifacts that remain novel and learnable indefinitely?
- Basis in paper: [explicit] The paper discusses self-observing systems like AlphaGo and humans experiencing Eureka moments, where the observer generates and learns from its own artifacts.
- Why unresolved: The paper does not provide formal conditions under which self-observation leads to indefinite open-endedness, particularly regarding the balance between exploration and exploitation.
- What evidence would resolve it: Empirical studies showing sustained novelty and learnability in self-improving systems over long timescales, or theoretical proofs bounding the conditions for indefinite open-endedness in self-observing systems.

### Open Question 2
- Question: How can we design open-ended foundation models that maintain human-understandability of artifacts as their complexity grows?
- Basis in paper: [explicit] Section 4.2 discusses the challenge of humans understanding increasingly complex AI creations, emphasizing the need for interpretability and explainability.
- Why unresolved: The paper identifies this as a critical safety concern but does not propose concrete methods to ensure human comprehension keeps pace with system complexity.
- What evidence would resolve it: Demonstrations of open-ended systems where human observers consistently understand and interpret novel artifacts, or metrics quantifying the gap between system complexity and human comprehension over time.

### Open Question 3
- Question: What is the relationship between the formal definition of open-endedness (novelty + learnability) and the rate-distortion curve formulation proposed in Appendix B?
- Basis in paper: [explicit] Appendix B introduces an alternative definition using rate-distortion curves, conjecturing equivalence under appropriate conditions.
- Why unresolved: The paper does not prove or empirically validate the equivalence between the statistical learning definition and the rate-distortion formulation.
- What evidence would resolve it: Formal proofs showing equivalence under specific conditions, or empirical studies comparing both definitions on benchmark open-ended systems to identify cases of agreement or divergence.

## Limitations
- The formal definition of open-endedness, while elegant, is challenging to implement and measure in real-world systems.
- The reliance on human observers for ASI utility and safety introduces cognitive limitations that may cap the timescale of open-endedness.
- The paper does not fully address how to balance safety and capability in increasingly open-ended systems.

## Confidence
- High confidence in the formal definition of open-endedness (novelty + learnability) as a useful framework for evaluating AI systems.
- Medium confidence in the claim that foundation models + open-ended algorithms can enable ASI, given the theoretical synergy but lack of empirical validation.
- Medium confidence in the assertion that ASI must remain learnable to humans for utility and safety, as this depends on assumptions about human cognitive limitations and the value of interpretability.

## Next Checks
1. Implement a prototype system that combines a foundation model with an open-ended algorithm (e.g., evolving text prompts) and measure novelty and learnability over time using a human observer.
2. Test the formal definition of open-endedness on a broader range of systems (e.g., generative art, game-playing agents) to validate its generalizability and identify edge cases.
3. Explore the scalability of observer-based evaluation by comparing human and AI oracle assessments of novelty and learnability in increasingly complex artifacts.