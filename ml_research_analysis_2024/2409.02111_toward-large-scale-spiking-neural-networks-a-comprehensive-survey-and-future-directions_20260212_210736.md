---
ver: rpa2
title: 'Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future
  Directions'
arxiv_id: '2409.02111'
source_url: https://arxiv.org/abs/2409.02111
tags:
- spiking
- neural
- networks
- snns
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of deep spiking neural
  networks (SNNs) and emerging Spiking Transformers, addressing the challenge of developing
  energy-efficient large-scale neural networks. The authors review learning methods
  for deep SNNs, categorizing them into ANN-to-SNN conversion and direct training
  with surrogate gradients.
---

# Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions

## Quick Facts
- arXiv ID: 2409.02111
- Source URL: https://arxiv.org/abs/2409.02111
- Authors: Yangfan Hu; Qian Zheng; Guoqi Li; Huajin Tang; Gang Pan
- Reference count: 40
- Key outcome: Comprehensive survey of deep SNNs and Spiking Transformers, addressing energy-efficient large-scale neural networks

## Executive Summary
This paper provides a comprehensive survey of deep spiking neural networks (SNNs) and emerging Spiking Transformers, focusing on learning methods and network architectures. The authors review two primary learning approaches: ANN-to-SNN conversion and direct training with surrogate gradients. They examine how Spiking Transformers combine event-driven computation with attention mechanisms to achieve energy-efficient sequence modeling. The survey highlights the performance gap between SNNs and state-of-the-art ANNs while discussing future directions for building large-scale SNNs that can match ANN performance while maintaining energy efficiency.

## Method Summary
The paper synthesizes existing research on deep SNNs by categorizing learning methods into ANN-to-SNN conversion and direct training with surrogate gradients. For reproduction, the minimum viable plan involves preparing the ImageNet dataset with 224x224 resolution, choosing either direct training methods (like Spikformer with surrogate gradients) or ANN-to-SNN conversion pipelines, and training the SNN model with standard optimization algorithms while evaluating classification accuracy and inference latency. Key unknowns include specific surrogate gradient function parameters and detailed neuron model parameters, which could affect faithful reproduction.

## Key Results
- Review of learning methods for deep SNNs, categorizing them into ANN-to-SNN conversion and direct training with surrogate gradients
- Survey of network architectures, focusing on deep convolutional neural networks (DCNNs) and Transformer architectures
- Comprehensive comparison of state-of-the-art deep SNNs, with particular focus on Spiking Transformers
- Discussion of future directions toward building large-scale SNNs, highlighting the potential of Spiking Transformers in achieving energy-efficient machine intelligence systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ANN-to-SNN conversion leverages pre-trained ANN models to bootstrap deep SNN training, reducing the energy cost of training from scratch.
- Mechanism: By mapping real-valued activations to discrete spikes under the assumption that firing rates approximate activations, converted SNNs inherit the learned representations of ANNs while gaining event-driven efficiency.
- Core assumption: The equivalence between ReLU activations and IF neuron firing rates holds across layers, enabling accurate functional mapping.
- Evidence anchors:
  - [abstract] "we primarily examine two aspects that are heavily studied and of great importance: learning rules and network architecture. For learning rules, we focus on two popular approaches: ANN-to-SNN conversion and direct training with surrogate gradients."
  - [section] "Based on the assumption that ANN activations approximate SNN firing rates, researchers proposed various conversion methods to exploit the advantages of deep neural networks and build deep SNNs by mapping real-valued activation neurons into discrete spiking neurons."
  - [corpus] Weak: corpus neighbors discuss energy efficiency but do not directly confirm the firing rate-activation equivalence.
- Break condition: If the conversion error accumulates significantly in deeper layers, the firing rate-activation equivalence breaks down, degrading performance.

### Mechanism 2
- Claim: Direct training with surrogate gradients enables gradient-based optimization of SNNs without requiring discontinuous spike function derivatives.
- Mechanism: Surrogate gradient functions approximate the derivative of the spiking nonlinearity, allowing backpropagation through time to update weights based on temporal spike patterns.
- Core assumption: The surrogate gradient closely approximates the true gradient of the spiking function across the relevant membrane potential range.
- Evidence anchors:
  - [abstract] "Our main contributions are as follows: (1) an overview of learning methods for deep spiking neural networks, categorized by ANN-to-SNN conversion and direct training with surrogate gradients."
  - [section] "To address the discontinuous spiking function, researchers employ surrogate gradients (derivatives of continuously differentiable functions) to approximate the derivative of the spiking nonlinearity."
  - [corpus] Weak: corpus neighbors mention surrogate gradients but do not provide quantitative validation of approximation accuracy.
- Break condition: If the surrogate gradient mismatch with true gradients is too large, optimization becomes ineffective and SNN performance degrades.

### Mechanism 3
- Claim: Spiking Transformers combine event-driven computation with attention mechanisms to achieve energy-efficient sequence modeling.
- Mechanism: By implementing self-attention using spiking operations (e.g., matrix dot-products on spike forms of Q, K, V), Spiking Transformers reduce computational overhead while maintaining representational power.
- Core assumption: The removal of softmax operations in favor of spike-based computations does not significantly impair attention quality.
- Evidence anchors:
  - [abstract] "With the recent success of large language models (LLMs), research on deep SNNs with transformer architectures has become a focal point in the neuromorphic computing community."
  - [section] "For the first time, they introduced a spiking self-attention mechanism and proposed a framework, i.e., Spikformer, to build deep SNNs with a transformer architecture."
  - [corpus] Weak: corpus neighbors do not discuss Spiking Transformers or their attention mechanisms.
- Break condition: If the simplified attention mechanism fails to capture long-range dependencies effectively, model performance suffers.

## Foundational Learning

- Concept: Leaky Integrate-and-Fire (LIF) neuron dynamics
  - Why needed here: LIF neurons are the standard building block for deep SNNs, and understanding their dynamics is crucial for designing and training SNN architectures.
  - Quick check question: What happens to the membrane potential of an LIF neuron when it receives no input spikes for an extended period?

- Concept: Backpropagation through time (BPTT) for recurrent networks
  - Why needed here: BPTT is the primary algorithm for training deep SNNs directly, as it handles the temporal dependencies inherent in spiking activity.
  - Quick check question: How does BPTT unroll a recurrent network for gradient computation?

- Concept: Attention mechanisms in Transformers
  - Why needed here: Attention is a core component of Spiking Transformers, and understanding its role is essential for grasping how these models achieve state-of-the-art performance.
  - Quick check question: What is the purpose of the softmax operation in vanilla self-attention, and why is it challenging to implement in SNNs?

## Architecture Onboarding

- Component map: Spiking neurons (LIF/IF) → Synaptic connections (convolution, attention) → Normalization layers (tdBN, MPBN) → Output layer (classification)
- Critical path: Input spikes → Neuron membrane integration → Spike generation → Attention computation (for Spiking Transformers) → Classification
- Design tradeoffs: Balancing accuracy vs. latency (time steps), biological plausibility vs. computational efficiency, model size vs. scalability
- Failure signatures: Performance degradation with increased depth, poor convergence during training, high latency for achieving target accuracy
- First 3 experiments:
  1. Implement a simple CNN-to-SNN conversion on MNIST and measure accuracy vs. time steps.
  2. Train a small SNN directly using surrogate gradients on CIFAR-10 and compare with converted SNN performance.
  3. Build a Spiking Transformer with a single attention head and evaluate on a small image classification task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can large-scale spiking neural networks achieve comparable performance to state-of-the-art ANNs while maintaining energy efficiency?
- Basis in paper: [explicit] The paper discusses the potential of Spiking Transformers in achieving energy-efficient machine intelligence systems and highlights the performance gap between SNNs and ANNs.
- Why unresolved: Current SNNs, especially those with Transformer architectures, still face challenges in fully leveraging event-driven computation and reducing resource consumption.
- What evidence would resolve it: Demonstration of large-scale SNNs with Transformer architectures achieving state-of-the-art performance on benchmarks while significantly reducing energy consumption compared to ANNs.

### Open Question 2
- Question: What are the most effective learning rules for training deep spiking neural networks that do not rely on traditional backpropagation mechanisms?
- Basis in paper: [explicit] The paper mentions exploring gradient rules that do not rely on traditional backpropagation mechanisms, such as equilibrium propagation and the Forward-Forward approach.
- Why unresolved: While these methods show promise, they are still in early stages and their effectiveness for deep SNNs is not yet fully established.
- What evidence would resolve it: Successful implementation and validation of deep SNNs trained with non-backpropagation learning rules, demonstrating competitive performance with backpropagation-based methods.

### Open Question 3
- Question: How can the scalability of spiking neural networks be improved to match the parameter counts of state-of-the-art artificial neural networks?
- Basis in paper: [explicit] The paper discusses the limited number of parameters that deep SNNs can effectively utilize compared to ANNs with billions of parameters.
- Why unresolved: Challenges associated with training deep SNNs with a large number of parameters remain, including issues with information loss and gradient vanishing.
- What evidence would resolve it: Development and successful training of deep SNNs with parameter counts comparable to state-of-the-art ANNs, achieving similar or better performance on complex tasks.

## Limitations

- The survey relies heavily on reported results from individual papers without providing consolidated benchmarks across different SNN architectures.
- The biological plausibility of deep SNNs is not rigorously evaluated against known cortical constraints.
- Major uncertainties remain around the practical scalability of Spiking Transformers, particularly regarding attention mechanism efficiency and training stability with surrogate gradients.

## Confidence

- ANN-to-SNN conversion mechanisms: High
- Direct training with surrogate gradients: Medium (empirical validation varies across studies)
- Spiking Transformers performance claims: Low (limited empirical evidence in the survey)

## Next Checks

1. Benchmark Spiking Transformers against standard ANNs on the same hardware platform to quantify claimed energy efficiency improvements.
2. Systematically evaluate the impact of different surrogate gradient functions on training convergence and final accuracy across multiple datasets.
3. Measure the latency-accuracy tradeoff curve for converted vs. directly trained SNNs to identify practical deployment thresholds.