---
ver: rpa2
title: 'OLAPH: Improving Factuality in Biomedical Long-form Question Answering'
arxiv_id: '2405.12701'
source_url: https://arxiv.org/abs/2405.12701
tags:
- evaluation
- factuality
- language
- answers
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OLAPH, a framework that leverages cost-effective
  and multifaceted automatic evaluation to construct synthetic preference sets and
  iteratively trains LLMs to reduce hallucinations and include crucial medical claims
  in biomedical long-form question answering. The key contributions include MedLFQA,
  a benchmark dataset reconstructed from existing LFQA datasets with long-form answers
  and crucial statements, and the OLAPH framework which utilizes supervised fine-tuning,
  temperature sampling, and direct preference optimization to generate preferred responses.
---

# OLAPH: Improving Factuality in Biomedical Long-form Question Answering

## Quick Facts
- arXiv ID: 2405.12701
- Source URL: https://arxiv.org/abs/2405.12701
- Authors: Minbyul Jeong; Hyeon Hwang; Chanwoong Yoon; Taewhoo Lee; Jaewoo Kang
- Reference count: 40
- Primary result: 7B LLMs trained with OLAPH can generate long-form answers comparable to medical experts' answers in terms of factuality

## Executive Summary
This paper introduces OLAPH, a framework that leverages cost-effective and multifaceted automatic evaluation to construct synthetic preference sets and iteratively trains LLMs to reduce hallucinations and include crucial medical claims in biomedical long-form question answering. The framework uses supervised fine-tuning, temperature sampling, and direct preference optimization to generate preferred responses without expensive human annotation. Experimental results show that 7B models trained with OLAPH achieve factuality scores comparable to medical experts, with significant improvements in automatic evaluation metrics not used during training.

## Method Summary
OLAPH addresses factuality issues in biomedical long-form question answering by first reconstructing the MedLFQA benchmark from existing LFQA datasets with long-form answers and crucial statements. The framework then applies supervised fine-tuning (SFT) to adapt pre-trained LLMs to the question-answering task using self-generated labels. Temperature sampling generates k predictions, which are evaluated using weighted automatic metrics (Rouge, BERTScore, BLEURT, hallucination detection) to construct preference pairs. Direct Preference Optimization (DPO) is then applied iteratively until convergence, progressively improving the model's ability to generate fact-based responses.

## Key Results
- 7B LLMs trained with OLAPH achieve factuality scores comparable to medical experts' answers
- Significant improvements in automatic evaluation metrics not used during training
- The framework successfully reduces hallucinations and includes crucial medical claims in biomedical long-form answers
- Cost-effective automatic evaluation substitutes for expensive human annotation in preference learning

## Why This Works (Mechanism)

### Mechanism 1
- Iterative preference optimization using self-generated responses progressively improves factuality in 7B models by generating k predictions, evaluating them with weighted automatic metrics, and constructing preference pairs for DPO training. The core assumption is that the model can generate responses containing correct answers among k sampled predictions. Break condition: If the model consistently generates low-quality responses across all k samples, preference set construction fails.

### Mechanism 2
- Cost-effective automatic evaluation metrics substitute for expensive human annotation by using Rouge scores for word composition, BERTScore and BLEURT for semantic similarity, and HALLUCINATION/COMPREHENSIVENESS metrics for factuality. The core assumption is that these metrics correlate well with human judgment. Break condition: If automatic metrics poorly correlate with human preferences, preference set construction becomes unreliable.

### Mechanism 3
- Starting with SFT on question-answering task before DPO training helps the model understand task format by familiarizing it with question-answering format using relatively small data samples. The core assumption is that the model needs task-specific pretraining to understand the question-answering format. Break condition: If initial SFT training fails to properly align the model with the task, subsequent DPO training may not be effective.

## Foundational Learning

- **Temperature sampling for diverse predictions**: Generates multiple response variations to capture the model's range of possible outputs and identify high-quality responses. Quick check: What happens to response diversity when temperature approaches 0 versus when it's high?

- **Preference optimization and Direct Preference Optimization (DPO)**: Aligns model outputs with human preferences without requiring explicit reward modeling. Quick check: How does DPO differ from standard supervised fine-tuning in terms of training objective?

- **Evaluation metrics for text generation**: Provides automatic, cost-effective way to evaluate response quality across multiple dimensions (ROUGE, BERTScore, BLEURT, hallucination detection). Quick check: Which metric would be most appropriate for measuring semantic similarity versus word overlap?

## Architecture Onboarding

- **Component map**: Data preparation → SFT pretraining → Temperature sampling → Automatic evaluation → Preference set construction → DPO training → Iterative loop
- **Critical path**: Temperature sampling → Automatic evaluation → Preference set construction → DPO training
- **Design tradeoffs**: Using automatic metrics saves cost but may miss nuanced quality aspects; temperature sampling increases computation but improves diversity
- **Failure signatures**: Low variance in sampled predictions, preference scores clustering around threshold, factuality metrics not improving across iterations
- **First 3 experiments**:
  1. Run single iteration with k=3 predictions to verify temperature sampling works and automatic evaluation produces reasonable scores
  2. Test preference set construction by manually inspecting high vs low scoring responses
  3. Run SFT on small subset to verify model learns question-answering format before proceeding to DPO

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OLAPH scale with model size beyond 7B parameters?
- Basis: The paper focuses on 7B models due to resource constraints and notes that smaller models have lower probability of generating correct predictions.
- Why unresolved: The paper does not test OLAPH on larger or smaller models.
- What evidence would resolve it: Experiments testing OLAPH on models ranging from 1B to 70B+ parameters, comparing factuality and other metrics.

### Open Question 2
- Question: How sensitive is OLAPH to the choice of evaluation metrics in the preference set construction?
- Basis: The paper uses a weighted sum of evaluation metrics (word composition, semantic similarity, factuality) to construct the preference set.
- Why unresolved: The paper does not explore the impact of using different evaluation metrics or varying the weights.
- What evidence would resolve it: Experiments testing OLAPH with different combinations of evaluation metrics and weightings.

### Open Question 3
- Question: How does OLAPH perform on datasets with more recent or domain-specific knowledge?
- Basis: The paper uses MedLFQA, which contains biomedical knowledge up to a fixed timestamp, and notes the possibility of outdated information.
- Why unresolved: The paper does not test OLAPH on datasets with more recent or domain-specific knowledge beyond the biomedical domain.
- What evidence would resolve it: Experiments testing OLAPH on datasets with recent or domain-specific knowledge.

## Limitations

- The effectiveness depends heavily on the quality and correlation of automatic evaluation metrics with human judgment, which is not explicitly validated
- The assumption that temperature sampling will generate diverse enough predictions containing the correct answer is critical but unverified
- The framework has not been tested on larger model sizes or datasets with more recent/domain-specific knowledge

## Confidence

- **High confidence**: The overall framework architecture (SFT → temperature sampling → automatic evaluation → DPO) is sound and follows established practices
- **Medium confidence**: The claim that cost-effective automatic evaluation can substitute for human annotation - while metrics are used, their correlation with human preferences is not directly tested
- **Medium confidence**: The assertion that 7B models can achieve medical expert-level factuality - this is based on automatic metrics rather than direct human expert evaluation

## Next Checks

1. **Metric Correlation Validation**: Conduct a small-scale human evaluation study comparing automatic metric scores against expert human judgments on a subset of responses to quantify the correlation between automatic metrics and human preferences for biomedical LFQA.

2. **Temperature Sampling Diversity Analysis**: Analyze the variance and diversity of responses generated across k samples at different temperature settings to verify that the model produces a meaningful range of outputs that can contain high-quality responses for preference learning.

3. **Failure Mode Analysis**: Test the framework on a challenging subset of questions where the base model is likely to hallucinate, and examine whether the iterative DPO training actually improves factuality or if the preference learning fails due to poor initial response quality.