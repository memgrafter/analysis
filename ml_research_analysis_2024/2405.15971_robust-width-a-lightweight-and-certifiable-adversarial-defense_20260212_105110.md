---
ver: rpa2
title: 'Robust width: A lightweight and certifiable adversarial defense'
arxiv_id: '2405.15971'
source_url: https://arxiv.org/abs/2405.15971
tags:
- robustness
- adversarial
- robust
- defense
- classifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel adversarial defense method based on
  the robust width property (RWP), a geometric criterion from compressed sensing.
  The defense uses a lightweight input purification scheme that exploits the sparsity
  of natural images in appropriate bases, requiring no additional training or data.
---

# Robust width: A lightweight and certifiable adversarial defense

## Quick Facts
- arXiv ID: 2405.15971
- Source URL: https://arxiv.org/abs/2405.15971
- Authors: Jonathan Peck; Bart Goossens
- Reference count: 34
- Primary result: A lightweight, certifiable adversarial defense using RWP-based denoising that achieves state-of-the-art black-box robustness without additional training

## Executive Summary
This paper proposes a novel adversarial defense method based on the robust width property (RWP) from compressed sensing. The defense uses a lightweight input purification scheme that exploits the sparsity of natural images in appropriate bases, requiring no additional training or data. The method involves applying a random partial Fourier measurement operator and a reconstruction algorithm based on the RWP to denoise inputs before classification. Theoretical robustness guarantees are provided, showing that the defense improves robustness bounds for approximately sparse images. Experiments on ImageNet demonstrate significant improvements in black-box robustness, especially for large perturbation budgets, closely matching state-of-the-art white-box performance without the need for adversarial training or larger models.

## Method Summary
The defense method is based on the robust width property (RWP) from compressed sensing. It uses a random partial Fourier measurement operator to compress the input, followed by a reconstruction algorithm that exploits the sparsity of natural images in appropriate bases (e.g., wavelets, Fourier, shearlets). The reconstructed input is then fed to the base classifier. The method requires no additional training or data and can be applied to any existing model. Theoretical analysis provides robustness guarantees, showing that the defense improves robustness bounds for approximately sparse images. The defense is easy to implement and has been shown to achieve significant improvements in black-box robustness, particularly for large perturbation budgets.

## Key Results
- The RWP-based defense achieves state-of-the-art black-box robustness on ImageNet without requiring additional training or data
- For large perturbation budgets (32/255), the defense closely matches white-box performance, achieving 38.3% robust accuracy compared to 38.7% for white-box attacks
- The defense shows significant robustness gains across different perturbation budgets (4/255 to 32/255) and is particularly effective for larger perturbations
- The method is lightweight, easy to implement, and can be applied to any existing model without modification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RWP-based denoising improves robustness by reconstructing the clean input from compressed measurements that are invariant to small adversarial perturbations.
- Mechanism: The random partial Fourier measurement operator Φ compresses the input, and the RWP guarantees that the reconstruction error is bounded when the perturbation is small and the input is approximately sparse in the chosen basis.
- Core assumption: Natural images are approximately sparse in some transform domain (e.g., wavelets, Fourier, shearlets).
- Evidence anchors:
  - [abstract] "The defense uses a lightweight input purification scheme that exploits sparsity of natural images in appropriate bases"
  - [section 4.2] "we can verify if the CS-RWP denoiser ∆ε from (2.4) is an adversarial denoiser for a τ-robust classifier f on S ⊂ V"
  - [corpus] No direct corpus evidence for RWP-based denoising improving robustness.
- Break condition: If the input is not approximately sparse in the chosen basis, the reconstruction error bound grows and the defense fails.

### Mechanism 2
- Claim: The defense improves robustness by leveraging the inherent robustness of the base classifier to tolerate the reconstruction error introduced by denoising.
- Mechanism: If the base classifier is τ-robust, and the reconstruction error from denoising is bounded by C(ε) < τ, then the smoothed classifier f ◦ gε is ε-robust.
- Core assumption: The base classifier already has some minimum robustness τ, and the reconstruction error does not exceed this bound.
- Evidence anchors:
  - [section 4.2] "Let f : V → W be a τ-robust classifier and let gε : V → V be an adversarial denoiser for f on S ⊂ V with performance bound C(ε) = εκ(ε) ≤ τ"
  - [section 4.2] "If additionally τ = εκ(ε) is a tight robustness bound and κ(ε) < 1, then the robustness gain of the defense method in S is at least 1/κ(ε)"
  - [corpus] No direct corpus evidence for the relationship between base classifier robustness and denoising reconstruction error.
- Break condition: If the base classifier is not τ-robust, or if the reconstruction error exceeds the base robustness bound, the defense cannot improve robustness.

### Mechanism 3
- Claim: The probabilistic nature of the RWP allows for robustness certification with high probability when using random Fourier matrices.
- Mechanism: Random Fourier matrices satisfy the RWP with high probability, and as input dimension increases, this probability approaches 1 exponentially fast.
- Core assumption: The sensing operator Φ is sampled from a random matrix ensemble that satisfies the RWP with high probability.
- Evidence anchors:
  - [section 4.3] "The practical utility of theorem 4.8 is limited by the fact that, in practice, the sensing operator Φ is not deterministic but rather sampled from some random matrix ensemble"
  - [section 4.3] "random Fourier matrices are known to satisfy the so-called restricted isometry property (RIP) with high probability [Haviv and Regev, 2017]. As shown by Cahill and Mixon [2021], the RIP implies the RWP"
  - [corpus] No direct corpus evidence for the probabilistic nature of RWP-based robustness certification.
- Break condition: If the sensing operator does not satisfy the RWP with sufficient probability, the robustness certification becomes unreliable.

## Foundational Learning

- Concept: Compressed sensing and the robust width property (RWP)
  - Why needed here: The defense relies on reconstructing the clean input from compressed measurements using RWP-based compressed sensing.
  - Quick check question: Can you explain how RWP differs from the restricted isometry property (RIP) and why it's important for approximately sparse signals?

- Concept: Sparsity in transform domains (wavelets, Fourier, shearlets)
  - Why needed here: The defense assumes that natural images are approximately sparse in some transform domain, which is crucial for the RWP-based reconstruction to work.
  - Quick check question: What are some common transform bases used for image sparsity, and how do they differ in their ability to sparsify natural images?

- Concept: Adversarial robustness and certified defenses
  - Why needed here: The goal of the defense is to improve the adversarial robustness of classifiers, so understanding the concepts of robustness bounds and certified defenses is essential.
  - Quick check question: How does the robustness gain of the RWP-based defense compare to other certified defense methods like randomized smoothing?

## Architecture Onboarding

- Component map:
  Input -> Random partial Fourier matrix Φ -> RWP-based compressed sensing denoiser ∆ε -> Base classifier f

- Critical path:
  1. Compute compressed measurements: y = Φx
  2. Reconstruct denoised input: x̂ = ∆ε(y)
  3. Classify denoised input: ŷ = f(x̂)

- Design tradeoffs:
  - Sparsity basis selection: Different bases (wavelets, Fourier, shearlets) have different sparsity properties for natural images, affecting the reconstruction quality and robustness gain.
  - Measurement budget: The number of retained Fourier coefficients (subsampling probability) affects the reconstruction error and computational cost.
  - Base classifier robustness: The inherent robustness of the base classifier determines the maximum achievable robustness gain.

- Failure signatures:
  - High reconstruction error: If the reconstruction error exceeds the base classifier's robustness bound, the defense fails to improve robustness.
  - Sensitivity to basis choice: If the chosen sparsity basis does not sparsify the input well, the defense performance degrades.
  - Dependence on classifier architecture: The defense may work better with some classifier architectures (e.g., CNNs) than others (e.g., Vision Transformers).

- First 3 experiments:
  1. Verify sparsity: Check if natural images are approximately sparse in the chosen transform basis by computing the L1 norm of the transformed coefficients.
  2. Measure reconstruction error: Compute the reconstruction error of the RWP-based denoiser on clean images and adversarial examples with varying perturbation budgets.
  3. Evaluate robustness gain: Compare the robust accuracy of the base classifier and the defended classifier on adversarial examples with different attack methods and perturbation budgets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much robustness can be guaranteed when the underlying classifier has less than the minimum required robustness threshold?
- Basis in paper: [explicit] Theorem 4.9 requires a base classifier with at least 2ε/α robustness
- Why unresolved: The paper only provides bounds for classifiers meeting the minimum robustness requirement, but many practical models may fall below this threshold
- What evidence would resolve it: Empirical evaluation of the defense on classifiers with varying levels of base robustness, particularly those below the 2ε/α threshold

### Open Question 2
- Question: Why do Vision Transformers show significantly worse performance with this defense compared to convolutional neural networks?
- Basis in paper: [explicit] Section 6 notes that ViTs perform substantially worse than CNNs when using the defense
- Why unresolved: The paper attributes this to potential issues with the Fourier transform basis, but doesn't provide a definitive explanation
- What evidence would resolve it: Systematic evaluation using different sensing matrices (beyond Fourier) with ViTs, or architectural modifications to make ViTs more compatible with the defense

### Open Question 3
- Question: Can the theoretical robustness bounds be tightened beyond the current Ω(ε/α) lower bound?
- Basis in paper: [explicit] Section 6 conjectures that any lower bound based on robust width will satisfy Ω(ε/α) but the constants may be improved
- Why unresolved: The paper establishes the theoretical framework but doesn't explore whether the constants can be optimized
- What evidence would resolve it: Derivation of tighter bounds using alternative mathematical approaches to robust width, or empirical demonstration that the current bounds are loose

## Limitations

- The defense's effectiveness relies on the assumption that natural images are approximately sparse in some transform domain, which may not hold for all types of images or inputs.
- The theoretical analysis assumes that the base classifier has some minimum robustness τ, which may not always be the case in practice, potentially limiting the defense's ability to improve robustness.
- The defense may be sensitive to the choice of hyperparameters, such as the sparsity basis, measurement budget, and denoising parameters, requiring careful tuning for optimal performance.

## Confidence

- High Confidence: The RWP-based denoising improves robustness for approximately sparse images by leveraging the inherent sparsity of natural images in transform domains.
- Medium Confidence: The defense achieves significant robustness gains, particularly for large perturbation budgets, by exploiting the relationship between the base classifier's robustness and the reconstruction error from denoising.
- Medium Confidence: The probabilistic nature of the RWP allows for robustness certification with high probability when using random Fourier matrices.

## Next Checks

1. Verify the sparsity assumption: Evaluate the sparsity of natural images in different transform bases (e.g., wavelets, Fourier, shearlets) by computing the L1 norm of the transformed coefficients and comparing the results.

2. Analyze the reconstruction error: Measure the reconstruction error of the RWP-based denoiser on clean images and adversarial examples with varying perturbation budgets, and investigate its relationship with the base classifier's robustness.

3. Evaluate the impact of sparsity basis selection: Compare the defense performance using different sparsity bases and analyze how the choice of basis affects the robustness gain and computational cost.