---
ver: rpa2
title: Coal Mining Question Answering with LLMs
arxiv_id: '2410.02959'
source_url: https://arxiv.org/abs/2410.02959
tags:
- mining
- coal
- prompt
- safety
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a novel approach to coal mining question answering
  using large language models (LLMs) enhanced with multi-turn prompt engineering.
  The method addresses the challenge of providing accurate, context-aware information
  in a high-risk industry where technical complexity and dynamic conditions make generic
  QA systems ineffective.
---

# Coal Mining Question Answering with LLMs

## Quick Facts
- arXiv ID: 2410.02959
- Source URL: https://arxiv.org/abs/2410.02959
- Reference count: 16
- Key result: Multi-turn prompt engineering improves LLM accuracy by 15-18% on coal mining QA tasks

## Executive Summary
This paper presents a novel approach to coal mining question answering using large language models enhanced with multi-turn prompt engineering. The method addresses the challenge of providing accurate, context-aware information in a high-risk industry where technical complexity and dynamic conditions make generic QA systems ineffective. By breaking down complex queries into structured, sequential prompts, the approach guides LLMs like GPT-4 to process nuanced mining-specific information more effectively. Experiments on a manually curated dataset of 500 real-world mining questions show significant performance improvements, with an average accuracy increase of 15-18% and a notable rise in GPT-4-based scoring (0.7-1.0 points) compared to baseline models.

## Method Summary
The approach uses multi-turn prompt engineering to break down complex mining queries into sequential, focused prompts that guide LLMs through structured reasoning. The system decomposes each query into multiple stages: initial context extraction, specific topic focus, and operational procedures. For evaluation, structured questions use standard accuracy metrics while open-ended queries employ GPT-4-based scoring on a 1-5 scale. The method was tested against baseline models and chain-of-thought prompting using a manually curated dataset of 500 coal mining questions covering safety, environmental, and operational topics.

## Key Results
- Multi-turn prompt engineering achieves 15-18% accuracy improvement over baseline models
- GPT-4-based scoring shows 0.7-1.0 point increase compared to baseline methods
- Method consistently outperforms chain-of-thought prompting across structured and open-ended queries
- Robust performance in handling dynamic mining scenarios and emergency conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-turn prompt engineering improves accuracy by breaking down complex mining queries into structured sub-components
- Mechanism: The system decomposes a single complex query into multiple focused prompts, each addressing a specific aspect of the mining question
- Core assumption: LLMs can maintain contextual continuity across sequential prompts while improving precision on domain-specific technical details
- Evidence anchors:
  - [abstract] "By breaking down complex queries into structured components, our approach allows LLMs to process nuanced technical information more effectively"
  - [section] "This design ensures that the LLM remains focused on key aspects of the query without losing contextual relevance"
  - [corpus] Weak evidence - corpus neighbors focus on different domains (legal, geospatial) without specific multi-turn prompt validation

### Mechanism 2
- Claim: GPT-4-based scoring provides more nuanced evaluation than simple accuracy metrics for open-ended mining queries
- Mechanism: The system uses GPT-4 itself to evaluate responses on a 1-5 scale based on domain-specific accuracy, depth of explanation, and handling of ambiguities
- Core assumption: GPT-4 can reliably assess the quality of its own domain-specific outputs using predefined rubric criteria
- Evidence anchors:
  - [abstract] "For more open-ended, complex queries where the answer may involve nuanced reasoning or where no single 'correct' answer exists, we utilize GPT-4-based scoring"
  - [section] "Each answer is scored on a scale of 1 to 5, where higher scores indicate more accurate and contextually aware answers"
  - [corpus] No direct evidence in corpus - neighbors don't discuss self-evaluation methodologies

### Mechanism 3
- Claim: Iterative refinement through multi-turn prompts enables better handling of dynamic mining scenarios and emergency conditions
- Mechanism: The system allows for real-time adjustments by having subsequent prompts query and refine previous outputs, particularly useful for handling emergency responses or environmental constraints
- Core assumption: Mining scenarios often require dynamic reasoning that benefits from iterative information refinement rather than single-pass responses
- Evidence anchors:
  - [abstract] "Additionally, iterative refinement prompts enable the model to query its output, improving accuracy by iterating over possible constraints or variables"
  - [section] "The iterative nature of the prompts helps the model reason more effectively and adapt its answers to the context-specific nuances"
  - [corpus] Weak evidence - corpus neighbors don't specifically address iterative refinement in industrial contexts

## Foundational Learning

- Concept: Prompt engineering fundamentals
  - Why needed here: The entire approach relies on carefully crafted prompt sequences to guide LLM behavior in a specialized domain
  - Quick check question: What's the difference between single-turn and multi-turn prompting in terms of information flow?

- Concept: Domain-specific terminology and context
  - Why needed here: Coal mining involves highly technical vocabulary (methane detection, ventilation systems, safety protocols) that general LLMs may not handle accurately without guidance
  - Quick check question: How would you distinguish between methane gas detection and other gas detection protocols in mining?

- Concept: Evaluation metric design for specialized domains
  - Why needed here: Standard accuracy metrics are insufficient for assessing nuanced, context-aware responses in safety-critical environments
  - Quick check question: Why might a 1-5 scoring rubric be more appropriate than binary accuracy for emergency response queries?

## Architecture Onboarding

- Component map: Input layer → Prompt engineering module → LLM engine → Scoring module → Output layer
- Critical path: Query → Multi-turn prompt decomposition → Sequential LLM processing → Iterative refinement → GPT-4 scoring → Final answer
- Design tradeoffs:
  - Prompt complexity vs. processing time: More turns improve accuracy but increase latency
  - Scoring subjectivity vs. nuance capture: GPT-4 scoring captures context but may introduce bias
  - Dataset specificity vs. generalizability: Highly curated mining dataset improves domain performance but limits broader application
- Failure signatures:
  - Context loss between prompt turns (answers become inconsistent)
  - Over-specialization to training dataset patterns
  - Scoring inflation/deflation due to rubric misalignment
  - Computational bottlenecks in iterative refinement
- First 3 experiments:
  1. Compare single-turn baseline vs. 2-turn prompt structure on 50 structured mining questions
  2. Test GPT-4 scoring rubric consistency by having multiple annotators score the same responses
  3. Evaluate emergency scenario handling by introducing time-constrained queries requiring dynamic reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multi-turn prompt engineering method perform on other specialized industrial domains beyond coal mining, such as chemical processing or oil and gas?
- Basis in paper: [explicit] The paper discusses the potential for extending the method to other technical domains where LLMs need to be guided through multi-dimensional reasoning
- Why unresolved: The experiments were conducted solely on coal mining data, leaving the generalizability of the approach to other industries untested
- What evidence would resolve it: Conducting similar experiments on other industrial domains and comparing the performance improvements relative to baseline models would provide insights into the method's generalizability

### Open Question 2
- Question: What are the computational costs and efficiency implications of using multi-turn prompt engineering compared to single-turn approaches in real-time applications?
- Basis in paper: [inferred] The paper mentions the iterative nature of multi-turn prompts but does not discuss the computational resources required or the impact on response times
- Why unresolved: The trade-off between accuracy improvements and computational efficiency is not explored, which is crucial for real-time decision-making environments
- What evidence would resolve it: Analyzing the computational overhead, latency, and resource utilization of the multi-turn approach compared to baseline methods in real-time scenarios would clarify its practical applicability

### Open Question 3
- Question: How does the performance of the multi-turn prompt engineering method vary with different types of queries, such as those requiring complex reasoning versus simple factual recall?
- Basis in paper: [explicit] The paper discusses the use of structured and unstructured questions but does not provide a detailed breakdown of performance across different query types
- Why unresolved: Understanding how the method handles various query complexities is essential for assessing its versatility and limitations
- What evidence would resolve it: Conducting experiments that categorize queries by complexity and analyzing the method's performance on each category would reveal its strengths and weaknesses in handling diverse query types

## Limitations

- The multi-turn prompt engineering methodology lacks detailed implementation specifications and exact prompt templates
- GPT-4-based scoring introduces potential bias as the model evaluates its own outputs without external validation
- Results are based on a manually curated dataset of 500 questions, with no information on dataset diversity or potential overfitting

## Confidence

- **Medium confidence** in accuracy improvements: The 15-18% performance gain is reported but lacks detailed statistical validation and confidence intervals
- **Medium confidence** in GPT-4 scoring methodology: While innovative, the self-evaluation approach lacks external validation and may introduce systematic bias
- **Low confidence** in generalizability: Results are based on a manually curated dataset with no cross-domain testing

## Next Checks

1. **Statistical validation**: Request detailed statistical analysis including confidence intervals, p-values, and effect sizes for the reported accuracy improvements across different query types
2. **External scoring validation**: Implement human expert evaluation alongside GPT-4 scoring to assess potential bias and validate the rubric's effectiveness
3. **Cross-domain testing**: Apply the multi-turn prompting approach to at least one other technical domain (e.g., oil and gas or construction safety) to evaluate generalizability beyond coal mining