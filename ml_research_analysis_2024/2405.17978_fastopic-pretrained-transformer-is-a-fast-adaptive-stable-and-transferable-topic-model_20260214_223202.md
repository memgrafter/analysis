---
ver: rpa2
title: 'FASTopic: Pretrained Transformer is a Fast, Adaptive, Stable, and Transferable
  Topic Model'
arxiv_id: '2405.17978'
source_url: https://arxiv.org/abs/2405.17978
tags:
- topic
- topics
- https
- fastopic
- transport
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FASTopic, a topic modeling framework that
  addresses limitations of existing methods in terms of effectiveness, efficiency,
  and stability. FASTopic employs a novel Dual Semantic-relation Reconstruction (DSR)
  paradigm that models semantic relations among document, topic, and word embeddings,
  and uses an Embedding Transport Plan (ETP) method to regularize these relations
  as optimal transport plans.
---

# FASTopic: Pretrained Transformer is a Fast, Adaptive, Stable, and Transferable Topic Model

## Quick Facts
- **arXiv ID**: 2405.17978
- **Source URL**: https://arxiv.org/abs/2405.17978
- **Reference count**: 40
- **Primary result**: FASTopic achieves superior topic quality (CV=0.426, TD=0.983), efficiency, adaptivity, stability, and transferability compared to state-of-the-art baselines

## Executive Summary
FASTopic introduces a novel topic modeling framework that addresses key limitations of existing methods through a Dual Semantic-relation Reconstruction (DSR) paradigm with Embedding Transport Plan (ETP) regularization. By modeling semantic relations among document, topic, and word embeddings as optimal transport plans, FASTopic avoids relation bias issues and produces distinct, coherent topics with accurate document-topic distributions. The approach demonstrates superior performance across multiple dimensions including effectiveness, efficiency, adaptivity, stability, and transferability, validated through extensive experiments on benchmark datasets and downstream tasks.

## Method Summary
FASTopic employs a Dual Semantic-relation Reconstruction (DSR) paradigm that leverages pretrained document embeddings and learnable topic and word embeddings. The core innovation lies in using Embedding Transport Plan (ETP) regularization to model semantic relations among documents, topics, and words as optimal transport plans. This approach addresses the relation bias issue present in existing methods by enforcing a dual relation reconstruction process. The framework uses Sinkhorn algorithm for optimal transport computation and is trained with Adam optimizer for 200 epochs. FASTopic infers document-topic distributions for new documents through a temperature-controlled mechanism, enabling both topic discovery and practical deployment.

## Key Results
- **Topic quality**: Achieves topic coherence CV=0.426 and topic diversity TD=0.983 on benchmark datasets
- **Efficiency**: Demonstrates fastest running time across all evaluated datasets
- **Transferability**: Shows strong performance in cross-domain scenarios through fine-tuning experiments

## Why This Works (Mechanism)
FASTopic works by addressing the fundamental limitation of existing topic models that rely on simple cosine similarity or KL divergence for semantic relation modeling, which leads to relation bias and repetitive topics. The DSR paradigm with ETP regularization models semantic relations as optimal transport plans, which inherently accounts for the global structure of the embedding space and prevents the collapse of topic embeddings. By regularizing the semantic relations through optimal transport, FASTopic ensures that topics are both distinct and representative of the document collection's semantic structure.

## Foundational Learning

### Optimal Transport
- **Why needed**: To model semantic relations as probability distributions over embedding spaces, enabling more robust and globally consistent topic modeling
- **Quick check**: Verify that the Sinkhorn algorithm implementation produces stable transport plans and converges within reasonable iterations

### Semantic Relation Reconstruction
- **Why needed**: To capture the bidirectional relationship between documents and topics, avoiding the one-sided biases of traditional methods
- **Quick check**: Confirm that topic embeddings are well-separated in the embedding space and not collapsing to a single point

### Embedding Transport Plan
- **Why needed**: To regularize the semantic relations using optimal transport, ensuring topic diversity and coherence
- **Quick check**: Validate that the ETP regularization improves topic quality metrics (CV, TD) compared to baseline without regularization

## Architecture Onboarding

### Component Map
Document Embeddings (pretrained) -> DSR Framework -> ETP Regularization -> Topic Embeddings + Document-Topic Distributions

### Critical Path
1. Obtain pretrained document embeddings
2. Initialize learnable topic and word embeddings
3. Apply DSR with ETP regularization
4. Optimize using Adam for 200 epochs
5. Infer document-topic distributions for new documents

### Design Tradeoffs
- **Speed vs. quality**: FASTopic prioritizes speed through efficient ETP computation while maintaining high topic quality
- **Flexibility vs. complexity**: The framework accepts various pretrained embeddings but requires careful tuning of ETP parameters

### Failure Signatures
- **Relation bias**: Repetitive or semantically similar topics indicate ETP regularization failure
- **Poor downstream performance**: Suggests issues with document embedding quality or insufficient training

### First Experiments
1. Verify topic coherence and diversity on 20NewsGroups dataset
2. Test stability by running multiple training sessions with different random seeds
3. Evaluate transferability by applying pretrained model to a novel domain

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of document embedding model (e.g., Sentence-BERT variants) quantitatively affect the quality of topics discovered by FASTopic?
- **Basis in paper**: The paper reports results using all-MiniLM-L6-v2, all-mpnet-base-v2, and all-distilroberta-v1 embeddings, noting that performance grows with the latter two.
- **Why unresolved**: The paper only shows qualitative differences in performance, not the magnitude of impact or relative contributions of different embedding models.
- **What evidence would resolve it**: A systematic comparison showing topic quality metrics (CV, TD) and downstream task performance for FASTopic trained with each embedding model, controlling for other variables.

### Open Question 2
What is the impact of the temperature hyperparameter τ on the quality and diversity of discovered topics in FASTopic?
- **Basis in paper**: The paper mentions τ=1.0 in the equation for inferring doc-topic distributions for new documents but does not explore its sensitivity.
- **Why unresolved**: The paper uses a fixed value without justification or exploration of the parameter space.
- **What evidence would resolve it**: A sensitivity analysis varying τ and measuring its effect on topic coherence, diversity, and downstream task performance.

### Open Question 3
How does FASTopic's performance scale with document length, particularly for very long documents (e.g., full research papers vs. news articles)?
- **Basis in paper**: The paper mentions that the max input length of pretrained document embedding models may hamper performance on extremely long documents, but shows FASTopic can handle long documents like academic papers.
- **Why unresolved**: The paper doesn't quantify the degradation in performance with increasing document length or provide thresholds.
- **What evidence would resolve it**: Experiments measuring topic quality and downstream task performance across documents of varying lengths, including very long documents, to identify performance limits and potential degradation patterns.

## Limitations

- **Reproducibility concerns**: Implementation details of Sinkhorn algorithm and preprocessing pipeline are not fully specified
- **Scalability validation**: Extensive experiments use relatively small-scale benchmark datasets
- **Transferability scope**: Cross-domain validation is limited to fine-tuning experiments rather than comprehensive testing

## Confidence

- **Topic modeling effectiveness claims (High)**: Well-supported by standard metrics (CV=0.426, TD=0.983) on established benchmark datasets
- **Efficiency and stability claims (Medium)**: Supported by runtime comparisons but lacks detailed variance analysis across multiple runs
- **Transferability claims (Medium)**: Preliminary evidence from fine-tuning experiments, but limited scope of cross-domain testing

## Next Checks

1. Implement a small-scale reproduction using 20NewsGroups dataset to verify the DSR framework's ability to generate coherent and diverse topics without relation bias
2. Conduct multiple runs with different random seeds to quantify stability and variance in topic quality metrics
3. Test the transferability claim by applying pretrained FASTopic to a novel domain (e.g., biomedical abstracts) and measuring performance degradation compared to domain-specific training