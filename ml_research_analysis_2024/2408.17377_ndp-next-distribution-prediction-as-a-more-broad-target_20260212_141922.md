---
ver: rpa2
title: 'NDP: Next Distribution Prediction as a More Broad Target'
arxiv_id: '2408.17377'
source_url: https://arxiv.org/abs/2408.17377
tags:
- distribution
- training
- language
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a key limitation of the Next Token Prediction
  (NTP) paradigm: its reliance on a narrow one-hot target distribution. The authors
  argue that this limits learning efficiency and task planning.'
---

# NDP: Next Distribution Prediction as a More Broad Target

## Quick Facts
- arXiv ID: 2408.17377
- Source URL: https://arxiv.org/abs/2408.17377
- Reference count: 4
- Next Distribution Prediction (NDP) improves over Next Token Prediction (NTP) by up to +2.97 COMET for translation, +0.61 average for general tasks, and +10.75 for medical domain adaptation

## Executive Summary
This paper addresses a fundamental limitation of the Next Token Prediction (NTP) paradigm in large language models: its reliance on narrow one-hot target distributions. The authors propose Next Distribution Prediction (NDP), which replaces the traditional one-hot target with n-gram distributions learned from the dataset. This broader target distribution allows the model to consider a wider range of likely next tokens during training, improving learning efficiency and task planning capabilities. The method demonstrates significant performance improvements across translation, general tasks, and domain adaptation benchmarks while also enabling unified pre-training and fine-tuning through the incorporation of unsupervised data.

## Method Summary
NDP replaces the one-hot target distribution in NTP with n-gram distributions compiled from the training data. These distributions are converted to probability distributions using softmax with temperature control, then fused with language model (CLM) distributions using weighted linear combination. The method enables unified training by enriching CLM tables with unsupervised data during fine-tuning. Experiments use datasets including IWSLT17 for translation, Alpaca-GPT4 for instruction tuning, and various medical datasets for domain adaptation.

## Key Results
- Translation: Up to +2.97 COMET improvement over NTP baseline
- General tasks: +0.61 average improvement across benchmarks including GSM8k, MMLU, HumanEval
- Domain adaptation: +10.75 average improvement in medical domain with MedQA, MedMCQA, CareQA
- Unified pre-training and fine-tuning enabled through enriched CLM tables

## Why This Works (Mechanism)

### Mechanism 1
n-gram distributions align better with LLM world data compression than one-hot targets. By using n-gram statistics to create broader target distributions, NDP better matches the probabilistic nature of real language. The assumption is that LLM distributions represent efficient compression of world data, and empirical observations show n-gram distributions align more closely with LLM output distributions than one-hot targets.

### Mechanism 2
Combining supervised and CLM distributions improves learning efficiency through weighted linear fusion. Both supervised table distributions and CLM distributions contribute valuable learning signals, with the fusion allowing the model to benefit from both task-specific and general language modeling information.

### Mechanism 3
NDP enables unified pre-training and fine-tuning by incorporating unsupervised data. Additional unsupervised text can enrich the CLM table during training, allowing continued pre-training during fine-tuning. This assumes that unsupervised data provides valuable language modeling signals that enhance task-specific performance.

## Foundational Learning

- **Probability distributions and softmax**: Understanding how n-gram frequencies are converted to probability distributions for training targets is crucial for implementing NDP. Quick check: How does the temperature coefficient t affect the probability distribution in Equation 2?

- **Knowledge distillation and soft labels**: NDP is conceptually similar to knowledge distillation, replacing hard one-hot targets with softer distributions. Quick check: What's the key difference between NDP and traditional knowledge distillation approaches?

- **Language modeling and n-gram statistics**: NDP relies on n-gram statistics to create more informative training targets. Quick check: Why might n-gram statistics provide better training targets than one-hot labels for next-token prediction?

## Architecture Onboarding

- **Component map**: Data preprocessing pipeline (n-gram table construction) -> Distribution conversion module (converts n-gram counters to probability distributions) -> Training loop (replaces one-hot targets with fused distributions) -> Optional unsupervised data module (enriches CLM table during training)

- **Critical path**: Data preprocessing → Distribution conversion → Training loop → Evaluation

- **Design tradeoffs**: Table size vs. memory usage (larger n-grams capture more context but require more memory), fusion weight α needs tuning to balance supervised vs. CLM information, larger vocabularies make distribution management more challenging

- **Failure signatures**: Poor performance on specific tasks (check if fusion weight is appropriate for task type), memory issues (reduce n-gram size or vocabulary size), slow convergence (verify temperature coefficient t is properly computed)

- **First 3 experiments**: 1) Ablation study comparing NDP with NTP baseline on small translation task, 2) Fusion weight sensitivity testing different α values on general task benchmark, 3) Measuring performance gains from adding unsupervised text to CLM table

## Open Questions the Paper Calls Out

- **What is the theoretical limit of NDP's improvement over NTP as model scale increases?**: The paper focuses on empirical results but doesn't provide theoretical analysis of convergence behavior or maximum potential improvement. Further theoretical analysis and experiments with extremely large-scale models could provide insights.

- **How does NDP perform on tasks requiring long-term planning or reasoning?**: While the paper discusses NTP limitations in advanced planning tasks, it doesn't specifically test NDP's performance on such tasks. Experiments on complex problem-solving or decision-making tasks could reveal NDP's potential in these areas.

- **What is the optimal n-gram size for different types of tasks and datasets?**: The paper empirically chooses 5-gram for CLM based on simple experimental results but doesn't explore the impact of different n-gram sizes. A comprehensive study across various tasks and datasets could determine optimal n-gram sizes for different scenarios.

## Limitations

- Hyperparameter specification lacks precision, particularly regarding n-gram order, temperature coefficient calculation, and fusion weight α values
- Performance improvements may be dataset-dependent and haven't been validated across diverse domains
- Computational overhead from n-gram table compilation and increased memory requirements isn't thoroughly analyzed

## Confidence

- **High Confidence**: The core mechanism of replacing one-hot targets with n-gram distributions is technically sound and well-motivated
- **Medium Confidence**: Reported performance improvements are likely valid but may be partially attributed to task-specific hyperparameter optimization
- **Low Confidence**: Claims about unified pre-training and fine-tuning through unsupervised data incorporation lack empirical validation

## Next Checks

1. **Ablation Study on Hyperparameters**: Systematically vary n-gram order, temperature coefficient, and fusion weight α to quantify their individual contributions to performance gains and identify optimal configurations for different task types.

2. **Computational Cost Analysis**: Measure and compare training time, memory usage, and inference latency between NTP and NDP implementations across different model sizes and vocabulary sizes to assess practical deployment feasibility.

3. **Cross-Domain Generalization Test**: Evaluate NDP performance on diverse, out-of-distribution datasets not used in the original experiments (e.g., medical, legal, scientific domains) to assess robustness and identify failure modes when n-gram statistics don't capture domain-specific patterns effectively.