---
ver: rpa2
title: 'SysBench: Can Large Language Models Follow System Messages?'
arxiv_id: '2408.10943'
source_url: https://arxiv.org/abs/2408.10943
tags:
- system
- user
- messages
- message
- constraints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SysBench, a comprehensive benchmark designed
  to evaluate how well large language models follow system messages. The benchmark
  addresses three key limitations of existing models: constraint violation, instruction
  misjudgment, and multi-turn instability.'
---

# SysBench: Can Large Language Models Follow System Messages?

## Quick Facts
- **arXiv ID**: 2408.10943
- **Source URL**: https://arxiv.org/abs/2408.10943
- **Reference count**: 40
- **Primary result**: Best model achieves only 54.4% session stability rate on system message following task

## Executive Summary
This paper introduces SysBench, a comprehensive benchmark designed to evaluate how well large language models follow system messages. The benchmark addresses three key limitations of existing models: constraint violation, instruction misjudgment, and multi-turn instability. SysBench includes a high-quality dataset of 500 system messages with corresponding multi-turn user conversations, covering six types of constraints. The evaluation protocol uses three metrics - constraint satisfaction rate, instruction satisfaction rate, and session stability rate - to assess model performance at different granularities.

Extensive experiments on 16 popular LLMs reveal that following system messages remains challenging, with the best model achieving only 54.4% session stability rate. The study also reveals a positive correlation between attention scores allocated to system messages and the models' ability to follow them. The paper provides insights into which constraint types are most challenging and identifies that multi-turn dependent conversations show steeper performance degradation as interactions progress.

## Method Summary
The SysBench benchmark evaluates LLM performance on system message following through a three-stage process. First, models generate responses to user queries in multi-turn conversations while adhering to system messages containing constraints. Second, a model-based verifier (GPT-4o) evaluates generated responses against three metrics: Constraint Satisfaction Rate (CSR) for constraint adherence, Instruction Satisfaction Rate (ISR) for instruction following, and Session Stability Rate (SSR) for multi-turn consistency. Third, attention score analysis quantifies how much focus models allocate to system messages versus other context. The benchmark uses 500 system messages with 5-turn conversations, covering six constraint types: action, content, background, role, format, and style.

## Key Results
- Best model achieves only 54.4% session stability rate on 5-turn conversations
- Correlation between attention scores allocated to system messages and compliance performance
- GPT-3.5 shows better performance on misaligned instructions than aligned ones
- Multi-turn dependent conversations show steeper performance degradation than parallel ones

## Why This Works (Mechanism)
The benchmark works by creating a controlled environment where system messages with explicit constraints are paired with user queries across multiple conversation turns. The three-metric evaluation framework captures different aspects of system message compliance at varying granularities. The use of a model-based verifier with structured evaluation checklists provides consistent and scalable assessment of constraint adherence. Attention score analysis reveals that models' ability to follow system messages correlates with how much computational focus they allocate to these messages during processing.

## Foundational Learning

### Constraint Types
**Why needed**: System messages contain various constraints that models must interpret and follow correctly
**Quick check**: Can identify action, content, background, role, format, and style constraints in sample system messages

### Multi-turn Conversation Dynamics
**Why needed**: System message compliance must be maintained across extended interactions
**Quick check**: Understands difference between parallel (independent) and dependent (contextually linked) conversation turns

### Attention Mechanism Analysis
**Why needed**: Reveals how models prioritize system messages during processing
**Quick check**: Can interpret attention heatmaps showing focus distribution between system messages and other context

### Evaluation Metrics Framework
**Why needed**: Provides standardized measurement of compliance at different levels
**Quick check**: Understands difference between CSR (constraint adherence), ISR (instruction following), and SSR (multi-turn stability)

## Architecture Onboarding

### Component Map
System Messages -> LLM Models -> Generated Responses -> GPT-4o Verifier -> Evaluation Metrics (CSR, ISR, SSR) -> Attention Analysis

### Critical Path
System Message + User Query -> LLM Generation -> Response Output -> Verifier Evaluation -> Performance Assessment

### Design Tradeoffs
Uses GPT-4o verifier for consistency and scalability vs. potential bias; balances evaluation comprehensiveness with computational cost; prioritizes controlled constraints over naturalistic conversations.

### Failure Signatures
Models showing high attention to user queries but low attention to system messages; inconsistent constraint application across conversation turns; preference for user instructions over system constraints when conflicts arise.

### Three First Experiments
1. Compare CSR, ISR, and SSR across different model families (open-source vs. proprietary)
2. Analyze attention distribution patterns for models with highest vs. lowest session stability rates
3. Test performance variation when system messages contain 1 vs. 3 vs. 5 constraints simultaneously

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between the number and complexity of constraints in system messages and the ability of LLMs to follow them accurately?
- Basis in paper: The paper identifies constraint violation as a key limitation and categorizes constraints into six types, noting that each system message typically contains 2-3 constraints on average.
- Why unresolved: The paper doesn't explore how varying the number or complexity of constraints affects model performance, or whether there's a threshold beyond which models struggle significantly.
- What evidence would resolve it: Systematic experiments varying the number and complexity of constraints in system messages, measuring CSR and ISR across different constraint configurations.

### Open Question 2
- Question: How does the position and prominence of system messages in the input sequence affect model attention and constraint-following ability?
- Basis in paper: The paper finds a correlation between attention scores allocated to system messages and the models' ability to follow them, and explores attention distribution through experiments.
- Why unresolved: The paper doesn't investigate whether changing the position of system messages (e.g., moving them to the beginning vs. end of the prompt) or emphasizing them through formatting affects attention allocation and performance.
- What evidence would resolve it: Experiments manipulating system message position and formatting, measuring attention scores and constraint-following metrics across different configurations.

### Open Question 3
- Question: What training strategies could improve LLMs' ability to prioritize system messages over conflicting user instructions?
- Basis in paper: The paper observes that some models show minimal performance variation between aligned and misaligned instructions, and notes that GPT-3.5 actually performs better on misaligned instructions.
- Why unresolved: The paper identifies this as a potential area for improvement but doesn't propose or test specific training methods to enhance system message prioritization.
- What evidence would resolve it: Training experiments using techniques like instruction hierarchy learning, reinforcement learning from human feedback focused on system message compliance, or curriculum learning approaches that progressively introduce conflicting instructions.

### Open Question 4
- Question: How does the length and complexity of multi-turn conversations affect the stability of system message following over extended interactions?
- Basis in paper: The paper measures session stability rate across 5 turns and observes that performance degrades as conversations lengthen, with multi-turn dependent conversations showing steeper declines than parallel ones.
- Why unresolved: The paper only tests up to 5 turns and doesn't explore whether this degradation continues linearly or if there's a point where performance stabilizes or catastrophically fails.
- What evidence would resolve it: Extended experiments testing system message following over 10-20 conversation turns, measuring SSR at each step to identify degradation patterns and potential breaking points.

### Open Question 5
- Question: Are there specific architectural modifications that could improve system message following without requiring full model retraining?
- Basis in paper: The paper's findings on attention allocation suggest that architectural factors influence system message following, and the observation that marker tokens don't significantly affect attention implies architectural rather than input-based solutions might be possible.
- Why unresolved: The paper focuses on evaluating existing models rather than exploring architectural changes that could enhance system message processing.
- What evidence would resolve it: Experiments testing architectural modifications like specialized attention mechanisms for system messages, prompt compression techniques, or context window management strategies specifically designed to preserve system message information.

## Limitations
- Reliance on GPT-4o verifier may introduce bias in evaluation results
- Benchmark focuses on six specific constraint types that may not capture all real-world scenarios
- Dataset size of 500 system messages may not represent full diversity of system message applications

## Confidence
- **Medium confidence**: Core claims about LLMs' difficulties with system message following
- **High confidence**: Correlation analysis between attention scores and performance
- **Low confidence**: Conclusions about which specific constraint types are most challenging

## Next Checks
1. **Human verification study**: Conduct human evaluation comparing GPT-4o verifier assessments against human annotators on 100 random responses to validate automatic evaluation pipeline.

2. **Cross-domain robustness testing**: Evaluate same models on system messages from medical, legal, and creative writing domains not in current dataset to assess generalizability.

3. **Attention visualization analysis**: Generate attention heatmaps for all 16 models on standardized system messages to verify reported correlation between attention allocation and compliance performance.