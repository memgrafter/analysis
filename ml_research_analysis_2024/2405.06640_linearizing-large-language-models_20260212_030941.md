---
ver: rpa2
title: Linearizing Large Language Models
arxiv_id: '2405.06640'
source_url: https://arxiv.org/abs/2405.06640
tags:
- linear
- attention
- arxiv
- softmax
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SUPRA is a method to uptrain large pre-trained transformers into
  linear transformers (RNNs) with minimal additional training cost. By replacing softmax
  normalization with GroupNorm and introducing a trainable MLP for query/key projections,
  SUPRA converts attention blocks to linear attention while retaining RNN inference
  efficiency.
---

# Linearizing Large Language Models

## Quick Facts
- arXiv ID: 2405.06640
- Source URL: https://arxiv.org/abs/2405.06640
- Reference count: 8
- Primary result: SUPRA uptrains transformers into linear RNNs with 5% of pre-training compute, achieving competitive performance on NLU benchmarks while improving inference efficiency.

## Executive Summary
SUPRA presents a method to convert large pre-trained transformers into linear transformers (RNNs) with minimal additional training cost. By replacing softmax normalization with GroupNorm and introducing a trainable MLP for query/key projections, SUPRA converts attention blocks to linear attention while retaining RNN inference efficiency. The approach achieves competitive performance on standard benchmarks using only 5% of pre-training compute, outperforming RWKV on HellaSwag. However, SUPRA models inherit the limitations of RNNs, showing gaps in in-context learning (MMLU) and long-context tasks (NarrativeQA, Qasper), even with 7B-scale models.

## Method Summary
SUPRA uptrains pre-trained transformers by replacing softmax normalization with GroupNorm and adding a trainable MLP kernel with shared weights for queries and keys. The method uses rotary positional encoding (RoPE) to maintain positional information in the linearized space. During training, the modified attention mechanism is optimized with Adam, while at inference time, the model operates in recurrent mode for constant-memory processing. The approach requires only 5% of the original pre-training compute while converting the transformer into an RNN with linear attention.

## Key Results
- SUPRA achieves competitive performance on standard NLU benchmarks (HellaSwag, PIQA, ARC-E, ARC-C, MMLU) using only 5% of pre-training compute
- SUPRA outperforms RWKV on HellaSwag, demonstrating superior linearization effectiveness
- SUPRA models show persistent gaps in in-context learning (MMLU) and long-context tasks (NarrativeQA, Qasper) compared to standard transformers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing softmax normalization with GroupNorm stabilizes training and preserves transformer performance during linearization.
- Mechanism: GroupNorm applies per-group normalization instead of computing the sum of all attention weights, avoiding numerical instability in large models.
- Core assumption: GroupNorm can replace the softmax denominator without harming representational capacity.
- Evidence anchors:
  - [abstract] "By replacing softmax normalization with GroupNorm and introducing a trainable MLP for query/key projections, SUPRA converts attention blocks to linear attention while retaining RNN inference efficiency."
  - [section] "Rather than pre-training linear models from scratch, we choose to instead uptrain state-of-the-art transformers. ... we identify two major issues and proposing SUPRA, an approach to fine-tuning very large transformers into RNNs."
  - [corpus] Weak. No direct mention of GroupNorm in neighbor papers, though normalization strategies are common in transformer linearization discussions.
- Break condition: If the normalization term in GroupNorm fails to approximate the attention weight distribution, model performance degrades.

### Mechanism 2
- Claim: Trainable MLP kernel with ReLU activation and shared weights for queries and keys enables effective linearization without approximating softmax attention.
- Mechanism: The MLP projects inputs into a higher-dimensional space where dot products mimic attention patterns, but without computing full softmax attention matrices.
- Core assumption: A small MLP with ReLU is expressive enough to map queries/keys into a space where linear dot products capture sufficient attention-like behavior.
- Evidence anchors:
  - [section] "We use a trainable layer: ϕ(x) = relu(Wx + b). The weights are shared between keys and queries for a given attention head."
  - [abstract] "By replacing softmax normalization with GroupNorm and introducing a trainable MLP for query/key projections, SUPRA converts attention blocks to linear attention while retaining RNN inference efficiency."
  - [corpus] Weak. Neighbor papers discuss MLP kernels in linearization but not in the specific context of SUPRA's design.
- Break condition: If the MLP kernel cannot project queries and keys into a space where linear similarity captures necessary dependencies, linearization fails.

### Mechanism 3
- Claim: Rotary positional encoding (RoPE) preserves positional information better than absolute encoding in linearized models.
- Mechanism: RoPE modulates queries and keys by rotating them according to their position, maintaining relative positional relationships in the linear dot product space.
- Core assumption: Relative positional encoding is necessary because linear attention lacks the softmax normalization that previously helped encode position implicitly.
- Evidence anchors:
  - [section] "we note that linear attention suffers more with absolute positional encoding than softmax attention, and a modern relative positional encoding scheme like RoPE (Su et al., 2021) is crucial for competitive performance."
  - [abstract] "we identify two major issues and proposing SUPRA, an approach to fine-tuning very large transformers into RNNs."
  - [corpus] Weak. Neighbor papers mention positional encoding in linearization contexts but don't specifically discuss RoPE's role in SUPRA.
- Break condition: If RoPE cannot maintain sufficient positional discrimination in the linearized space, the model loses sequence ordering capabilities.

## Foundational Learning

- Concept: Linear attention and its relationship to recurrence
  - Why needed here: Understanding how linear attention can be reformulated as an RNN is essential for grasping SUPRA's inference efficiency claims.
  - Quick check question: How does the recurrence formulation of linear attention enable constant-memory inference compared to standard transformers?

- Concept: Group normalization and its advantages over batch normalization
  - Why needed here: GroupNorm's role in stabilizing linearized attention is central to SUPRA's design.
  - Quick check question: What problem does GroupNorm solve that makes it preferable to the standard softmax denominator in large-scale linearization?

- Concept: Rotary positional encoding (RoPE) and its mathematical formulation
  - Why needed here: RoPE is critical for maintaining positional information in linearized attention, especially given the removal of softmax normalization.
  - Quick check question: How does RoPE modify the dot product to encode relative positions without increasing computational complexity?

## Architecture Onboarding

- Component map: Pre-trained transformer weights -> MLP kernel (ϕ) with shared weights -> GroupNorm layer -> RoPE positional encoding -> Decay vector γ
- Critical path:
  1. Initialize with pre-trained transformer weights
  2. Add MLP kernel and GroupNorm layers
  3. Apply RoPE to queries and keys
  4. Train with modified attention mechanism
  5. Use recurrent formulation at inference time
- Design tradeoffs:
  - Trade softmax normalization stability for GroupNorm's computational efficiency
  - Use shared MLP weights to reduce parameters while maintaining expressiveness
  - Accept potential loss of in-context learning capability for improved inference efficiency
- Failure signatures:
  - Training instability or divergence (indicates normalization issues)
  - Poor performance on tasks requiring long-range dependencies (suggests decay vector or MLP kernel problems)
  - Inability to maintain positional information (suggests RoPE implementation issues)
- First 3 experiments:
  1. Compare SUPRA linearization with and without GroupNorm on a small transformer to verify normalization impact
  2. Test different MLP kernel architectures (depth, width) to find optimal expressiveness vs efficiency balance
  3. Evaluate RoPE vs absolute positional encoding in linearized attention to quantify positional information preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specialized training strategies or architectural modifications overcome the persistent in-context learning and long-context modeling limitations of linear transformers?
- Basis in paper: [explicit] The authors identify a "persistent gap" in in-context learning (MMLU) and long-context tasks (NarrativeQA, Qasper) even for large linear models, despite their advantages in inference efficiency.
- Why unresolved: While the authors demonstrate these limitations, they do not explore solutions beyond noting the potential of more sophisticated gating mechanisms, higher order linear attention, or associative binding.
- What evidence would resolve it: Comparative studies of linear transformers with and without specialized training for in-context learning, or with and without advanced architectural modifications for long-context modeling, would reveal the effectiveness of these approaches.

### Open Question 2
- Question: Is the performance gap between transformers and linear transformers on long-context tasks primarily due to the inherent limitations of linear attention or the lack of effective training strategies for long-context modeling in linear models?
- Basis in paper: [inferred] The authors observe that linear models underperform transformers on long-context tasks, even when using techniques like YaRN for context scaling. They also note that the training context lengths for these models do not exceed 8k tokens.
- Why unresolved: The paper does not isolate the effects of inherent limitations of linear attention from the lack of effective training strategies for long-context modeling in linear models.
- What evidence would resolve it: Experiments comparing the performance of linear transformers trained on longer sequences with transformers on long-context tasks would help disentangle the effects of architecture and training.

### Open Question 3
- Question: Does the decay factor in linear attention significantly impact the model's ability to handle long-range dependencies, and if so, what is the optimal decay strategy for different task types?
- Basis in paper: [explicit] The authors discuss the impact of decay factors on long-range performance, noting that the default decay factors proposed in Qin et al. (2024) lead to a plateau in performance at longer context lengths.
- Why unresolved: The paper does not explore the impact of different decay strategies on a variety of tasks or investigate the optimal decay strategy for different types of long-range dependencies.
- What evidence would resolve it: Comparative studies of linear transformers with different decay strategies on tasks with varying types of long-range dependencies would reveal the optimal decay strategy for each task type.

## Limitations

- SUPRA models show significant degradation on long-context tasks (NarrativeQA, Qasper) even with 7B-scale models, suggesting fundamental limitations of RNN-style architectures
- The approach exhibits a persistent 10-20 point gap on MMLU, indicating incomplete preservation of in-context learning capabilities
- Limited experimental scope (only 7B models) makes it unclear whether SUPRA scales effectively to larger transformer sizes

## Confidence

**High confidence**: The core claim that SUPRA can uptrain pre-trained transformers into linear attention models with competitive performance on standard NLU benchmarks.

**Medium confidence**: The assertion that GroupNorm stabilization and MLP kernel projections are the primary drivers of SUPRA's success, though ablation studies could be more comprehensive.

**Low confidence**: Claims about the general applicability of SUPRA to very large models beyond 7B parameters, as scaling effects are not thoroughly explored.

## Next Checks

1. **Ablation study of normalization strategies**: Compare SUPRA with exact softmax normalization (but still linear recurrence) to isolate whether GroupNorm provides efficiency gains versus being necessary for stability.

2. **Extended long-context evaluation**: Test SUPRA models on tasks requiring 16K+ context lengths with systematic analysis of performance degradation patterns.

3. **Cross-model linearization transfer**: Apply SUPRA to a wider range of pre-trained transformer sizes (1B, 13B, 30B) to assess scaling properties and identify at what scale the linearization process becomes unstable or loses effectiveness.