---
ver: rpa2
title: 'MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation
  via Knowledge Distillation'
arxiv_id: '2403.14171'
source_url: https://arxiv.org/abs/2403.14171
tags:
- multimodal
- misinformation
- llms
- evidence
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MMIDR addresses the challenge of interpreting multimodal misinformation
  by teaching large language models (LLMs) to provide fluent and high-quality textual
  explanations for their decision-making processes. The framework employs a data augmentation
  pipeline that converts multimodal misinformation into an instruction-following format
  through visual information processing and evidence retrieval.
---

# MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation

## Quick Facts
- arXiv ID: 2403.14171
- Source URL: https://arxiv.org/abs/2403.14171
- Authors: Longzheng Wang; Xiaohan Xu; Lei Zhang; Jiarui Lu; Yongxiu Xu; Hongbo Xu; Minghao Tang; Chuang Zhang
- Reference count: 40
- One-line primary result: MMIDR achieves 93.63% accuracy for LLaMA2 and 94.04% for MiniGPT-v2 in multimodal misinformation detection

## Executive Summary
MMIDR introduces a framework that teaches large language models to interpret multimodal misinformation by generating fluent textual explanations for decision-making processes. The approach combines data augmentation through visual processing and evidence retrieval with knowledge distillation from proprietary to open-source LLMs. By converting multimodal misinformation into instruction-following format and leveraging proprietary LLM capabilities, MMIDR achieves significant performance improvements in detection accuracy while providing compelling rationales to support assessments.

## Method Summary
MMIDR employs a three-stage approach: first, a data augmentation pipeline processes multimodal misinformation through OCR, image captioning, and evidence retrieval to extract relevant textual and visual context; second, proprietary LLMs (GPT-3.5) are prompted with this processed content using a labeling template to generate rationales explaining authenticity assessments; third, knowledge distillation via LoRA fine-tunes open-source LLMs (LLaMA2, MiniGPT-v2) to align with the teacher model's outputs, enabling them to generate similar explanations while maintaining strong detection performance.

## Key Results
- Achieves 93.63% accuracy for LLaMA2 and 94.04% for MiniGPT-v2 in multimodal misinformation detection
- Demonstrates significant performance improvements over baseline models (3.21% for LLaMA2, 3.41% for MiniGPT-v2)
- Successfully bridges the gap between proprietary and open-source LLMs while maintaining strong detection capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation from proprietary LLMs to open-source LLMs can bridge the performance gap in multimodal misinformation detection.
- Mechanism: Proprietary LLMs generate rationales for misinformation detection, which are then used to fine-tune open-source LLMs using LoRA.
- Core assumption: Rationales generated by proprietary LLMs contain sufficient information to guide the training of open-source LLMs.
- Evidence anchors:
  - [abstract]: "we design an efficient knowledge distillation approach to distill the capability of proprietary LLMs in explaining multimodal misinformation into open-source LLMs."
  - [section 3.3]: "Knowledge distillation refers to fine-tune the student LLM by maximizing the likelihood of sequences generated by the teacher LLM, ensuring alignment between the predictions of the student and the teacher."
  - [corpus]: Weak. The corpus contains papers on multimodal misinformation detection, but none directly address knowledge distillation from proprietary to open-source LLMs.

### Mechanism 2
- Claim: Data augmentation through visual information processing and evidence retrieval enhances the performance of LLMs in multimodal misinformation detection.
- Mechanism: Images are processed using OCR and image captioning, and textual and visual evidence is retrieved from the internet to provide context for the LLMs.
- Core assumption: The retrieved evidence provides relevant and accurate information to support the LLMs' decision-making process.
- Evidence anchors:
  - [abstract]: "To convert multimodal misinformation into an appropriate instruction-following format, we present a data augmentation perspective and pipeline. This pipeline consists of a visual information processing module and an evidence retrieval module."
  - [section 3.1]: "We employ OCR and image captioning techniques to process visual information. Additionally, we obtain both visual and textual evidence through evidence retrieval."
  - [corpus]: Weak. The corpus contains papers on multimodal misinformation detection, but none directly address the use of data augmentation through visual information processing and evidence retrieval.

### Mechanism 3
- Claim: Instruction-following capabilities of LLMs can be leveraged to generate high-quality rationales for multimodal misinformation detection.
- Mechanism: A labeling prompt template is used to guide the proprietary LLMs in generating rationales for the given multimodal misinformation.
- Core assumption: The instruction-following capabilities of LLMs are sufficient to generate high-quality rationales for multimodal misinformation detection.
- Evidence anchors:
  - [abstract]: "we present a data augmentation perspective and pipeline. This pipeline consists of a visual information processing module and an evidence retrieval module. Subsequently, we prompt the proprietary LLMs with processed contents to extract rationales for interpreting the authenticity of multimodal misinformation."
  - [section 3.2]: "To utilize the instruction-following ability of LLMs, we design a simple labeling prompt template that prompts the teacher LLM to generate rationales."
  - [corpus]: Weak. The corpus contains papers on the use of LLMs in misinformation detection, but none directly address the use of instruction-following capabilities to generate rationales.

## Foundational Learning

- Concept: Multimodal misinformation detection
  - Why needed here: The paper focuses on detecting misinformation that contains both textual and visual elements.
  - Quick check question: What are the key challenges in detecting multimodal misinformation compared to unimodal misinformation?

- Concept: Knowledge distillation
  - Why needed here: The paper uses knowledge distillation to transfer the capabilities of proprietary LLMs to open-source LLMs.
  - Quick check question: What are the key components of a knowledge distillation process?

- Concept: Instruction-following capabilities of LLMs
  - Why needed here: The paper leverages the instruction-following capabilities of LLMs to generate rationales for multimodal misinformation detection.
  - Quick check question: How can instruction-following capabilities be utilized to improve the performance of LLMs in specific tasks?

## Architecture Onboarding

- Component map: Multimodal misinformation -> Data augmentation pipeline (OCR, image captioning, evidence retrieval) -> Rationale elicitation (proprietary LLM) -> Knowledge distillation (LoRA) -> Open-source LLM
- Critical path: Input multimodal misinformation flows through data augmentation, rationale generation, and knowledge distillation to produce an open-source LLM capable of detection and explanation
- Design tradeoffs: Proprietary LLM usage introduces privacy and cost concerns; open-source LLM limitations may constrain ultimate performance
- Failure signatures: Incomplete or inaccurate evidence retrieval leads to poor rationale quality, which degrades the distilled model's performance
- First 3 experiments:
  1. Evaluate the performance of the data augmentation pipeline in extracting relevant evidence from multimodal misinformation
  2. Assess the quality of the rationales generated by the proprietary LLMs using the labeling prompt template
  3. Measure the performance of the open-source LLMs after knowledge distillation in multimodal misinformation detection tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MMIDR vary with different quantities of retrieved visual and textual evidence?
- Basis in paper: [explicit] The paper states that the quantity of evidence significantly impacts the decisions made by LLMs, reaching an optimal effect when the cumulative quantity of visual and textual evidence is 6, as depicted by the red and blue lines in Figure 3.
- Why unresolved: The paper does not provide a detailed analysis of the performance of MMIDR with varying quantities of evidence beyond the observation of the optimal effect at 6 pieces of evidence.
- What evidence would resolve it: Conducting experiments with different quantities of evidence and comparing the performance of MMIDR in terms of accuracy, precision, recall, and F1-score.

### Open Question 2
- Question: How does the quality of rationales generated by student LLMs compare to those generated by the teacher LLM?
- Basis in paper: [inferred] The paper mentions that the distilled student model can generate fluent expressions and provide explanations to some extent, but its ability to capture crucial information remains inferior to that of the teacher model.
- Why unresolved: The paper does not provide a quantitative comparison of the quality of rationales generated by student LLMs and the teacher LLM.
- What evidence would resolve it: Developing a metric to measure the quality of rationales and conducting experiments to compare the rationales generated by student LLMs and the teacher LLM.

### Open Question 3
- Question: How does the performance of MMIDR vary with different types of multimodal misinformation?
- Basis in paper: [explicit] The paper does not explicitly address this question, but it mentions that MMIDR is evaluated on a dataset that includes both textual and visual components, and incorporates two languages, English and Chinese.
- Why unresolved: The paper does not provide a detailed analysis of the performance of MMIDR with different types of multimodal misinformation.
- What evidence would resolve it: Conducting experiments with different types of multimodal misinformation, such as fake news, rumors, and altered images, and comparing the performance of MMIDR in terms of accuracy, precision, recall, and F1-score.

## Limitations

- Heavy reliance on proprietary LLMs (GPT-3.5) for rationale generation creates reproducibility constraints
- Evidence retrieval module depends on external APIs (Google Reverse Image Search and Google Programmable Search Engine) with uncertain availability and cost
- Limited quantitative evaluation of rationale quality metrics such as coherence, relevance, and factual accuracy

## Confidence

**High Confidence**: The core knowledge distillation mechanism and data augmentation pipeline are well-supported by experimental results with reported accuracy improvements of 3.21% for LLaMA2 and 3.41% for MiniGPT-v2 over baseline models.

**Medium Confidence**: The instruction-following capabilities of LLMs are demonstrated through rationale generation, but quality assessment remains largely qualitative rather than quantitative.

**Low Confidence**: The generalizability to other multimodal misinformation datasets beyond MR2 is not empirically validated, and computational cost implications of the data augmentation pipeline are not thoroughly discussed.

## Next Checks

1. **Rationale Quality Assessment**: Conduct systematic evaluation of rationales generated by GPT-3.5 using ROUGE scores, human evaluation for factual accuracy, and coherence measures to quantify quality of distilled knowledge.

2. **Evidence Retrieval Robustness**: Test framework performance with alternative evidence retrieval methods (open-source search APIs or local knowledge bases) to assess dependency on proprietary services and evaluate robustness to retrieval failures.

3. **Cross-Dataset Generalization**: Validate framework on additional multimodal misinformation datasets with varying characteristics (different image-to-text ratios, cultural contexts) to establish generalization capabilities and identify domain-specific limitations.