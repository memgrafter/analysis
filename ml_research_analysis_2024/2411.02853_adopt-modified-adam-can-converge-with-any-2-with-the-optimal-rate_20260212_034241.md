---
ver: rpa2
title: "ADOPT: Modified Adam Can Converge with Any $\u03B2_2$ with the Optimal Rate"
arxiv_id: '2411.02853'
source_url: https://arxiv.org/abs/2411.02853
tags:
- adopt
- adam
- convergence
- gradient
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the non-convergence issue of Adam and related
  adaptive gradient methods in stochastic optimization, particularly for nonconvex
  problems. The authors identify that the core problem stems from the correlation
  between the current gradient and the second-moment estimate in the Adam algorithm.
---

# ADOPT: Modified Adam Can Converge with Any $β_2$ with the Optimal Rate

## Quick Facts
- arXiv ID: 2411.02853
- Source URL: https://arxiv.org/abs/2411.02853
- Reference count: 40
- Primary result: ADOPT achieves optimal O(1/√T) convergence for smooth nonconvex optimization without relying on β₂ choice or bounded noise assumptions

## Executive Summary
This paper addresses the non-convergence issue of Adam and related adaptive gradient methods in stochastic optimization, particularly for nonconvex problems. The authors identify that the core problem stems from the correlation between the current gradient and the second-moment estimate in the Adam algorithm. To fix this, they propose ADOPT, which removes the current gradient from the second-moment estimate and changes the order of momentum update and normalization. Theoretically, ADOPT achieves the optimal O(1/√T) convergence rate for smooth nonconvex optimization without relying on the choice of β₂ or the strong assumption that gradient noise is uniformly bounded. Empirically, ADOPT demonstrates superior performance across various tasks including image classification, generative modeling, natural language processing, and reinforcement learning, compared to Adam and its variants.

## Method Summary
ADOPT is a modification of Adam that addresses its non-convergence issue by making two key changes to the update rule. First, it removes the current gradient from the second-moment estimate, using v_{t-1} instead of v_t for normalization. Second, it changes the order of momentum update and normalization, normalizing the gradient before updating the momentum. These changes ensure that the momentum estimate m_t is conditionally independent of the current second-moment estimate, eliminating the statistical correlation that causes Adam to diverge. The algorithm maintains the same computational complexity as Adam and requires no additional memory overhead.

## Key Results
- ADOPT achieves the optimal O(1/√T) convergence rate for smooth nonconvex optimization with any choice of β₂
- The method eliminates the constant factor in the convergence bound that prevents Adam from converging
- ADOPT outperforms Adam, AMSGrad, and AdaShift across multiple tasks including image classification, generative modeling, NLP, and RL
- The method works without relying on the bounded noise assumption, using only standard smoothness and bounded variance assumptions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Removing the current gradient from the second-moment estimate eliminates the correlation that causes Adam's non-convergence.
- **Mechanism:** In Adam, the second-moment estimate `v_t` includes the current gradient `g_t`, creating statistical dependence between the normalization term and the gradient being normalized. By using `v_{t-1}` instead of `v_t` for normalization, ADOPT ensures that the scaling factor is conditionally independent of the current gradient, removing the source of non-convergence.
- **Core assumption:** The stochastic gradient `g_t` and the past second-moment estimate `v_{t-1}` are conditionally independent given past gradients.
- **Evidence anchors:**
  - [abstract]: "ADOPT addresses the non-convergence issue of Adam by removing the current gradient from the second moment estimate"
  - [section]: "RMSprop can be modified to be convergent by removing the current gradient gt from the second moment estimate vt"
  - [corpus]: No direct evidence in corpus; this is a novel theoretical contribution of the paper
- **Break condition:** If future gradients become dependent on past second-moment estimates through some other mechanism, or if the conditional independence assumption is violated.

### Mechanism 2
- **Claim:** Changing the order of momentum update and normalization fixes the remaining non-convergence issue in Adam-style methods.
- **Mechanism:** In standard Adam, the normalization by the second-moment estimate happens after the momentum update, which means the momentum `m_t` still contains information about the current gradient through the normalization process. ADOPT normalizes the current gradient before updating the momentum, ensuring that `m_t` depends only on past normalized gradients and is independent of the current second-moment estimate.
- **Core assumption:** The momentum update can be separated from the normalization step without losing convergence benefits.
- **Evidence anchors:**
  - [abstract]: "changing the order of the momentum update and the normalization by the second moment estimate"
  - [section]: "we propose to change the order of the momentum update and the normalization by the second moment estimate"
  - [corpus]: No direct evidence in corpus; this is a novel theoretical contribution of the paper
- **Break condition:** If the momentum update requires information about the current gradient's scale for proper direction, or if the computational order affects gradient flow in ways not captured by the analysis.

### Mechanism 3
- **Claim:** ADOPT achieves the optimal O(1/√T) convergence rate for smooth nonconvex optimization without relying on problem-dependent hyperparameter tuning.
- **Mechanism:** By combining the two algorithmic changes (removing current gradient from second-moment estimate and changing update order), ADOPT eliminates the constant factor in the convergence bound that prevents Adam from converging. This allows the algorithm to achieve the minimax optimal rate regardless of the choice of β₂.
- **Core assumption:** The convergence analysis holds under standard assumptions (smoothness, bounded variance, unbiased gradients) without requiring the stronger bounded noise assumption.
- **Evidence anchors:**
  - [abstract]: "ADOPT achieves the optimal convergence rate of O(1/√T) with any choice of β2 without depending on the bounded noise assumption"
  - [section]: "ADOPT can converge with the optimal rate for smooth nonconvex optimization as follows"
  - [corpus]: No direct evidence in corpus; this is a novel theoretical contribution of the paper
- **Break condition:** If the problem violates the smoothness assumption, or if the variance grows faster than bounded, or if the objective is not lower-bounded.

## Foundational Learning

- **Concept:** Exponential moving averages in adaptive gradient methods
  - Why needed here: Understanding how Adam maintains running estimates of gradient moments is crucial to identifying why the correlation between current gradient and second-moment estimate causes non-convergence
  - Quick check question: In Adam's update, which terms are used to compute the second-moment estimate v_t at time step t?

- **Concept:** Statistical independence and conditional probability
  - Why needed here: The core insight about non-convergence relies on understanding when two random variables are statistically independent or conditionally independent
  - Quick check question: If X and Y are independent, are they also conditionally independent given Z? (Answer: Not necessarily)

- **Concept:** Convergence analysis for stochastic optimization
  - Why needed here: Understanding how convergence rates are derived and what factors affect them is essential for grasping why ADOPT achieves optimal rates while Adam does not
  - Quick check question: What is the difference between the bounded noise assumption (Assumption 2.6) and the bounded variance assumption (Assumption 2.5)?

## Architecture Onboarding

- **Component map:** θ_t = θ_{t-1} - α_t * m_t
- **Critical path:** θ_t = θ_{t-1} - α_t * m_t, where m_t = β1 * m_{t-1} + (1-β1) * g_t / max(√v_{t-1}, ε)
- **Design tradeoffs:**
  - Memory: ADOPT requires storing both m_t and v_t like Adam, with no additional memory overhead
  - Computational cost: Same as Adam (one extra division and square root per parameter)
  - Hyperparameter sensitivity: Less sensitive to β₂ choice, but still requires tuning β₁, learning rate, and potentially clipping threshold
- **Failure signatures:**
  - Numerical instability when v_{t-1} contains near-zero values (solved by clipping or max operation)
  - Divergence if learning rate is too large or β₂ is too small
  - Slow convergence if β₁ is too close to 1 (excessive momentum)
- **First 3 experiments:**
  1. Run ADOPT on the toy convex problem from the paper (f(θ) = θ for θ ∈ [-1, 1]) with different β₂ values to verify convergence where Adam fails
  2. Compare training loss curves of ADOPT vs Adam on a simple MLP for MNIST classification
  3. Test ADOPT on a small ResNet for CIFAR-10 to verify practical performance benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the convergence analysis of ADOPT be extended to the bounded variance assumption (Assumption 2.4) instead of the bounded second moment assumption (Assumption 2.5)?
- Basis in paper: [explicit] The authors state that relaxing the bounded second moment assumption to the bounded variance assumption is a limitation of their analysis and suggest it as future work.
- Why unresolved: The current proof techniques rely on the stronger bounded second moment assumption, and the authors have not yet derived a convergence bound under the weaker bounded variance assumption.
- What evidence would resolve it: A rigorous convergence proof of ADOPT under the bounded variance assumption, demonstrating the optimal O(1/√T) rate.

### Open Question 2
- Question: How does ADOPT perform in training large-scale foundation models (e.g., large language models) compared to Adam and other adaptive gradient methods?
- Basis in paper: [inferred] The authors mention that adaptive gradient methods like Adam are widely used for training large-scale foundation models, and they discuss the potential of ADOPT to improve training stability and model performance in practice.
- Why unresolved: The authors have not conducted experiments specifically on training large-scale foundation models with ADOPT.
- What evidence would resolve it: Extensive experiments comparing the performance of ADOPT to Adam and other adaptive gradient methods in training large-scale foundation models, including training stability, convergence speed, and final model quality.

### Open Question 3
- Question: How sensitive is ADOPT to hyperparameter choices, particularly the learning rate schedule and the clipping value?
- Basis in paper: [inferred] The authors provide recommended hyperparameter settings for ADOPT and conduct experiments with different learning rate schedules, but they do not perform an extensive sensitivity analysis of ADOPT to various hyperparameters.
- Why unresolved: The authors have not systematically investigated the impact of different hyperparameter choices on the performance of ADOPT.
- What evidence would resolve it: A comprehensive sensitivity analysis of ADOPT to various hyperparameters, including learning rate schedules, clipping values, and other relevant hyperparameters, demonstrating the robustness of ADOPT to hyperparameter choices.

## Limitations

- The convergence analysis relies on the bounded second moment assumption rather than the weaker bounded variance assumption
- Empirical validation, while extensive, does not include large-scale foundation model training experiments
- The paper does not provide a comprehensive sensitivity analysis of ADOPT to hyperparameter choices

## Confidence

**High Confidence:** The theoretical analysis showing that removing the current gradient from the second-moment estimate eliminates the source of non-convergence is well-founded and mathematically rigorous. The mechanism of conditional independence between the normalization term and current gradient is clearly established.

**Medium Confidence:** The practical performance improvements demonstrated across multiple tasks are convincing, but the extent of these improvements may depend on specific problem characteristics and hyperparameter choices that weren't exhaustively explored.

**Medium Confidence:** The claim that ADOPT achieves optimal O(1/√T) convergence without relying on the bounded noise assumption is theoretically proven, but the practical significance of avoiding this assumption depends on the specific noise characteristics of real-world problems.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary β1, β2, and learning rate across a grid for both ADOPT and Adam on a standard benchmark (e.g., CIFAR-10 ResNet) to quantify the practical impact of being less sensitive to β2 choices.

2. **Early Training Behavior:** Compare the first 100-1000 iterations of ADOPT vs Adam on a simple convex problem to empirically verify that ADOPT maintains stable behavior when Adam would diverge due to the correlation issue.

3. **Non-smooth Problem Testing:** Evaluate ADOPT on tasks with non-smooth objectives (e.g., hinge loss, ReLU networks with dead neurons) to test the practical limits of the smoothness assumptions in the theoretical analysis.