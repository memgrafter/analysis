---
ver: rpa2
title: Fine-tuning Multimodal Large Language Models for Product Bundling
arxiv_id: '2407.11712'
source_url: https://arxiv.org/abs/2407.11712
tags:
- bundle
- bundling
- multimodal
- product
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Bundle-MLLM, a novel framework that fine-tunes
  multimodal large language models (LLMs) for product bundling by integrating textual,
  visual/acoustic, and relational data through hybrid item tokenization. The method
  employs a progressive optimization strategy that first learns bundle patterns using
  LoRA and then adapts multimodal semantic understanding, achieving significant improvements
  over state-of-the-art methods with up to 18.81% relative performance gain in HitRate@1
  across four datasets in two domains.
---

# Fine-tuning Multimodal Large Language Models for Product Bundling

## Quick Facts
- **arXiv ID**: 2407.11712
- **Source URL**: https://arxiv.org/abs/2407.11712
- **Reference count**: 40
- **Primary result**: Bundle-MLLM achieves up to 18.81% relative improvement in HitRate@1 across four datasets in two domains

## Executive Summary
This paper proposes Bundle-MLLM, a novel framework that fine-tunes multimodal large language models (LLMs) for product bundling by integrating textual, visual/acoustic, and relational data through hybrid item tokenization. The method employs a progressive optimization strategy that first learns bundle patterns using LoRA and then adapts multimodal semantic understanding, achieving significant improvements over state-of-the-art methods. By reformulating product bundling as a multiple-choice question format, the framework enables effective LLM adaptation while maintaining computational efficiency.

## Method Summary
Bundle-MLLM fine-tunes LLMs for product bundling by first extracting textual, visual/acoustic, and relational features using specialized encoders (BLIP2, CLAP, LightGCN). These features are fused through a self-attention mechanism into a single multimodal token, which is combined with textual tokens using a soft separation token. The model is then trained using a progressive optimization strategy: first optimizing LoRA weights for bundle pattern learning with text-only prompts, then freezing the LLM and training the multimodal fusion module for semantic understanding.

## Key Results
- Achieves up to 18.81% relative improvement in HitRate@1 over state-of-the-art methods
- Demonstrates effectiveness across four datasets in two domains (product and music)
- Shows progressive optimization (S1→S2) outperforms single-step optimization strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid item tokenization with soft separation token improves multimodal semantic integration
- Mechanism: Single fused multimodal token via self-attention captures inter-modal relationships while maintaining efficiency; soft separator dynamically adapts to different semantic structures
- Core assumption: Single token can represent complex multimodal interactions without information loss
- Evidence: Abstract and section 3.1 describe the fusion approach and soft separator
- Break condition: When modalities are highly diverse and require distinct processing

### Mechanism 2
- Claim: Progressive optimization outperforms single-step optimization
- Mechanism: Sequential training (bundle patterns first, then multimodal understanding) preserves task-specific adaptations and avoids parameter corruption
- Core assumption: Bundle pattern learning and multimodal understanding can be optimized independently
- Evidence: Abstract and section 4.3.3 show S1→S2 consistently outperforms other strategies
- Break condition: When datasets have extremely dense relational data requiring simultaneous optimization

### Mechanism 3
- Claim: Multiple-choice reformulation enables effective LLM adaptation
- Mechanism: Structuring task as "Given partial bundle X, which candidate should be included?" leverages LLM instruction-following capabilities
- Core assumption: LLMs can effectively reason about bundling relationships in multiple-choice format
- Evidence: Abstract and section 3.2 describe the reformulation approach
- Break condition: When candidate sets are extremely large or require complex multi-step reasoning

## Foundational Learning

- **Multimodal representation learning and fusion**: Combines textual, media, and relational features into unified representations; Quick check: How does self-attention fusion differ from simple concatenation?
- **Parameter-efficient fine-tuning (PEFT) techniques**: Uses LoRA to adapt LLMs without full fine-tuning; Quick check: What are key differences between LoRA and other PEFT methods?
- **Instruction tuning and prompt engineering**: Reformulates bundling as multiple-choice questions; Quick check: How does progressive optimization differ from standard instruction tuning?

## Architecture Onboarding

- **Component map**: Textual tokenizer → foundation encoders (BLIP2, CLAP, LightGCN) → multimodal fusion module → soft separation token → hybrid tokenization → LLM with LoRA → prediction
- **Critical path**: Item features → foundation encoders → multimodal fusion → soft separation token → hybrid tokenization → LLM with LoRA → prediction
- **Design tradeoffs**: Single fused token vs. multiple modality-specific tokens (efficiency vs. expressiveness); soft vs. hard separator (flexibility vs. clarity); progressive vs. simultaneous optimization (stability vs. complementarity)
- **Failure signatures**: Poor performance on structured textual datasets (separator issues); degradation with dense relational data (optimization limitations); inefficiency with large candidate sets (architectural bottlenecks)
- **First 3 experiments**: 1) Test different separator strategies on datasets with varying textual structure; 2) Compare progressive vs. simultaneous optimization across relational data densities; 3) Evaluate multimodal fusion with varying modality combinations

## Open Questions the Paper Calls Out

1. **Scalability to larger LLMs**: How does progressive optimization affect performance when scaling beyond Llama2-7B? The paper shows improvements up to 13B but doesn't explore larger models.

2. **Extension to additional modalities**: Can the multimodal fusion approach handle sensor data or user behavior patterns beyond text, visual, and audio? The paper only tests three specific modalities.

3. **Candidate item size limits**: What's the theoretical upper limit of candidate items Bundle-MLLM can handle effectively? The paper tests up to 20 candidates but doesn't systematically explore performance limits.

## Limitations

- Hybrid tokenization with single fused token may lose modality-specific nuances when features are highly diverse
- Progressive optimization may not generalize to domains with extremely dense relational structures
- Multiple-choice reformulation constrains task to pairwise selection rather than complex bundle relationships

## Confidence

- **High Confidence**: Progressive optimization (S1→S2) outperforming single-step optimization is well-supported by ablation studies
- **Medium Confidence**: Hybrid item tokenization with soft separation token improving multimodal integration lacks direct comparative evidence
- **Medium Confidence**: Multiple-choice reformulation enabling effective LLM adaptation is logically sound but lacks comparative analysis with alternatives

## Next Checks

1. Conduct controlled experiments comparing soft separation token against hard separation and no separation strategies across datasets with varying textual structure quality.

2. Test the progressive optimization strategy on datasets with artificially manipulated relational data densities to determine optimal optimization approach.

3. Evaluate alternative task formulations beyond multiple-choice (e.g., direct generation, ranking-based approaches) to assess whether the reformulation is genuinely optimal.