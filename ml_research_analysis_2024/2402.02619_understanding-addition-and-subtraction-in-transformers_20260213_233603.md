---
ver: rpa2
title: Understanding Addition and Subtraction in Transformers
arxiv_id: '2402.02619'
source_url: https://arxiv.org/abs/2402.02619
tags:
- addition
- subtraction
- answer
- training
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Small transformers trained from scratch can solve n-digit addition\
  \ and subtraction with 99.999% accuracy. Using 49 models (2\u20133 layers, 3\u2013\
  4 heads), the authors extend prior work on addition circuits to subtraction, presenting\
  \ a unified algorithm based on cascading carry and borrow circuits."
---

# Understanding Addition and Subtraction in Transformers

## Quick Facts
- **arXiv ID**: 2402.02619
- **Source URL**: https://arxiv.org/abs/2402.02619
- **Reference count**: 40
- **Primary result**: Small transformers (2-3 layers, 3-4 heads) achieve 99.999% accuracy on n-digit addition and subtraction through cascading carry/borrow circuits

## Executive Summary
This paper demonstrates that small transformers can achieve near-perfect accuracy on n-digit addition and subtraction by implementing specific algorithmic circuits. Building on prior work uncovering addition circuits, the authors extend the analysis to subtraction and present a unified mechanistic account based on cascading carry and borrow circuits. Through systematic ablations and node-level constraints across 49 models, they validate the learned mechanisms and release a reproducibility toolkit. The study also surveys 180 public LLMs, finding that only 7% achieve reliable addition performance, highlighting the gap between specialized small models and general-purpose LLMs.

## Method Summary
The authors train 2-3 layer transformers with 3-4 attention heads on synthetic data enriched with edge cases for n-digit addition and subtraction (5-15 digits). They use AdamW optimizer with batch size 64, learning rate 0.00008, and weight decay 0.1, targeting loss < 2×10^-8. The methodology includes systematic ablations, PCA clustering for node identification, and causal interventions through activation swapping to validate mechanistic interpretations. A unified algorithmic framework handles both operations by replacing "cascading carry one" with "cascading borrow one," with nodes becoming polysemantic during mixed training.

## Key Results
- Small transformers achieve 99.999% accuracy on n-digit addition and subtraction
- Unified algorithmic framework handles both addition and subtraction through shared circuit structure
- Systematic ablations validate learned mechanisms with high precision
- Only 7% of surveyed LLMs achieve reliable addition performance despite having far more parameters

## Why This Works (Mechanism)

### Mechanism 1
Small transformers implement exact arithmetic through cascading carry and borrow circuits using specific attention heads and MLP layers to compute subtasks like single-digit carry/borrow classification (ST/MB) and multidigit carry/borrow propagation (SV/MV) across multiple tokens. Nodes must appear in specific token positions and layers to respect computational dependencies. Evidence includes perfect accuracy of SVn multidigit cascade carry values completed by the '=' token, though only 5/8 corpus papers directly address transformer arithmetic circuits.

### Mechanism 2
Models generalize arithmetic algorithms across different digit lengths and operations through a single algorithmic framework that handles both addition and subtraction by replacing "cascading carry one" with "cascading borrow one." Nodes become polysemantic during mixed training, handling multiple subtasks (SA/MD/ND or ST/MB) simultaneously. Evidence includes nodes learning to handle three subtasks in mixed models, with Zhang et al. [2024] identifying symmetries between addition and subtraction circuits.

### Mechanism 3
Systematic ablations and node-level constraints validate learned mechanisms through an automated framework that searches for candidate nodes, tests behavior through PCA clustering and question sets, then validates through causal intervention by swapping activations. The framework can predict model output from nearly 2 million possible answers. Evidence includes multiple papers using similar mechanistic interpretability approaches, though validation is based on internal consistency rather than independent verification.

## Foundational Learning

- **Concept**: Modular arithmetic and digit-wise operations
  - Why needed here: The algorithms rely on computing (D_n + D'_n) mod 10 for addition and (D_n - D'_n) mod 10 for subtraction
  - Quick check question: What is (7 + 8) mod 10? What is (3 - 9) mod 10?

- **Concept**: Cascading carry/borrow propagation
  - Why needed here: The core challenge is handling cases where a single digit operation affects multiple higher-order digits
  - Quick check question: In 999 + 1 = 1000, how many digits does the carry propagate through? In 1000 - 1 = 999, how many digits does the borrow propagate through?

- **Concept**: Attention mechanism and residual streams
  - Why needed here: Understanding how nodes attend to specific digit positions and combine information across layers is crucial for interpreting the circuits
  - Quick check question: If an attention head at position P10 attends to tokens D3 and D'3, what kind of subtask might it be implementing?

## Architecture Onboarding

- **Component map**: Digit positions → Attention heads (SA/ST/MB/MD/ND) → MLP layers → Answer digits
- **Critical path**: Computation flows from digit positions through attention heads implementing subtasks to MLP layers that combine information and produce final answer digits
- **Design tradeoffs**: Fewer layers/heads make circuits more interpretable but may limit accuracy on very long numbers; more layers enable better handling of cascading operations but increase complexity
- **Failure signatures**: Loss spikes during training indicate missing edge cases; post-training ablations that don't produce expected effects suggest node misidentification
- **First 3 experiments**:
  1. Train a 2-layer, 3-head model on 5-digit addition and visualize attention patterns at each token position
  2. Implement PCA clustering on attention head activations for a specific subtask (e.g., ST2) and verify three distinct clusters
  3. Perform ablation on a candidate node for ST2 and verify that output changes match predictions based on the algorithm

## Open Questions the Paper Calls Out

### Open Question 1
Why do 93% of surveyed LLMs fail at basic arithmetic while small transformers achieve near-perfect accuracy? The paper identifies this gap but doesn't investigate underlying architectural or training differences between small specialized models and large general-purpose LLMs. Comparative analysis of attention patterns, training data composition, and architectural choices could reveal key differences.

### Open Question 2
What specific representations and transformations do polysemantic nodes use to handle multiple arithmetic operations? The paper focuses on algorithmic-level analysis rather than mechanistic details of how individual nodes encode and manipulate information for different operations. Detailed analysis of residual stream activations and attention patterns would reveal internal representations.

### Open Question 3
How generalizable are these arithmetic circuits across different model architectures and training approaches? While consistent circuit patterns emerge across 49 models, the study doesn't test whether these same circuits emerge in transformers with different layer counts, attention mechanisms, or training curricula. Testing varying architectures would reveal circuit generalizability.

## Limitations
- Data generation strategy for edge cases remains unspecified, which is critical for model convergence
- Generalization to real-world settings untested despite survey showing LLMs struggle with arithmetic
- Node identification validity depends heavily on automated frameworks whose assumptions aren't fully validated

## Confidence

**High Confidence**: Small transformers can achieve 99.999% accuracy on n-digit addition and subtraction with clearly specified training procedures and evaluation methodology.

**Medium Confidence**: The mechanistic explanation of cascading carry and borrow circuits is supported by systematic ablations, though node identification relies on automated frameworks.

**Low Confidence**: Generalization claims about polysemantic nodes handling multiple operations depend on node identification framework working correctly.

## Next Checks
1. Replicate the core finding with independent data generation to verify the 99.999% accuracy claim and test whether data enrichment strategy is as critical as suggested.
2. Validate node identification through alternative methods by applying different approaches to verify that the same nodes are identified for key subtasks.
3. Test generalization to naturalistic data by fine-tuning a model on naturalistic arithmetic questions and measuring accuracy degradation.