---
ver: rpa2
title: Smooth Pseudo-Labeling
arxiv_id: '2405.14313'
source_url: https://arxiv.org/abs/2405.14313
tags:
- fixmatch
- smooth
- loss
- class
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Smooth Pseudo-Labeling (SP L) to address discontinuities
  in the loss function of FixMatch that lead to instabilities in scarce label regimes.
  SP L replaces the thresholding in pseudo-labeling with a smooth multiplicative factor
  that ramps up continuously from 0 to 1, making the loss continuously differentiable.
---

# Smooth Pseudo-Labeling

## Quick Facts
- arXiv ID: 2405.14313
- Source URL: https://arxiv.org/abs/2405.14313
- Authors: Nikolaos Karaliolios; Hervé Le Borgne; Florian Chabot
- Reference count: 37
- Key outcome: Smooth Pseudo-Labeling (SP L) improves FixMatch stability in scarce label regimes by replacing thresholding with smooth multiplicative factors, reducing error rates from ~9.28% to ~7.24% on CIFAR-10 with 40 labels

## Executive Summary
This paper addresses discontinuities in the loss function of FixMatch that cause instabilities when labeled data is scarce. The authors introduce Smooth Pseudo-Labeling (SP L), which replaces the hard threshold in pseudo-labeling with a smooth multiplicative factor that ramps continuously from 0 to 1. This modification makes the loss continuously differentiable, improving training stability without adding computational overhead or hyperparameters. Experiments on CIFAR-10 and CIFAR-100 show significant improvements in error rates and robustness to hyperparameter changes.

## Method Summary
The method replaces the hard thresholding mechanism in FixMatch's pseudo-labeling with a smooth multiplicative factor Φ that continuously ramps from 0 to 1 as prediction confidence increases from τ to 1. The loss function becomes continuously differentiable, addressing the discontinuities that cause training instabilities. The smooth function maintains the same expected behavior as the original threshold but eliminates abrupt changes in the loss derivative. The approach is implemented within the FixMatch framework using standard semi-supervised learning components (weak/strong augmentations, EMA, consistency loss) with the modification focused solely on the pseudo-labeling component.

## Key Results
- SP L reduces CIFAR-10 error rates from ~9.28% to ~7.24% with 40 labels (significant improvement with lower standard deviation)
- SP L shows improved robustness to threshold, momentum, and class imbalance changes compared to FixMatch
- Both methods can degrade with more labeled data in random sampling, but SP L mitigates this to some extent
- Improvements achieved without extra modules, hyperparameters, or computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing hard threshold with smooth multiplicative factor reduces discontinuities in loss derivative, improving stability in scarce label regimes
- Mechanism: The original pseudo-labeling loss includes a step function that equals 1 when predicted probability exceeds τ and 0 otherwise, creating discontinuities. By using a smooth function that ramps from 0 to 1 as probability increases from τ to 1, the derivative becomes continuous, allowing gradient descent to proceed without abrupt changes in direction
- Core assumption: Discontinuities in loss derivative are a significant cause of instability when labels are scarce
- Evidence anchors: Abstract states SP L makes loss "continuously differentiable"; section explains derivative discontinuity at max fθ(x) = τ
- Break condition: If discontinuities are not the primary source of instability, or smooth function introduces other issues (e.g., vanishing gradients)

### Mechanism 2
- Claim: Smooth pseudo-labeling loss improves robustness to hyperparameter changes and random initialization
- Mechanism: Discontinuities in loss derivative cause high sensitivity to batch sequence and initial weights. Smoothing the loss reduces this dependence, leading to more consistent results across different runs with same hyperparameters
- Core assumption: Sensitivity to hyperparameters and initialization is caused by discontinuities in loss derivative
- Evidence anchors: Abstract mentions "more robust to threshold, momentum, and class imbalance changes"; section discusses signal-to-noise ratio problem from iterative discontinuities
- Break condition: If improved robustness is due to factors other than smoothing of loss derivative

### Mechanism 3
- Claim: Smooth pseudo-labeling loss mitigates performance degradation when adding more labeled data in random sampling scenarios
- Mechanism: In random sampling, class distribution of labeled data can vary significantly from true distribution. Discontinuities in loss derivative cause model to be overly sensitive to this noise, leading to performance drops when labeled data expands. Smoothing the loss makes model more stable and less likely to degrade
- Core assumption: Performance degradation in random sampling is caused by discontinuities in loss derivative
- Evidence anchors: Abstract states "SP L mitigates this to some extent"; section observes both implementations can regress to higher error rates when labeled images increase
- Break condition: If performance degradation is due to factors other than discontinuities in loss derivative

## Foundational Learning

- Concept: Continuous differentiability of loss functions
  - Why needed here: Core contribution is introducing smooth loss function to replace non-differentiable one. Understanding continuous differentiability implications is crucial for appreciating improvement
  - Quick check question: What is the difference between a C0 and a C1 function? How does this difference affect gradient descent?

- Concept: Semi-supervised learning and pseudo-labeling
  - Why needed here: Paper builds upon pseudo-labeling method in semi-supervised learning. Understanding basics of these concepts is necessary for understanding problem addressed
  - Quick check question: How does pseudo-labeling work in semi-supervised learning? What are the challenges associated with it?

- Concept: Signal-to-noise ratio in training
  - Why needed here: Paper discusses signal-to-noise ratio problem in context of pseudo-labeling. Understanding this concept helps in understanding source of instability in original method
  - Quick check question: What is the signal-to-noise ratio in context of training neural network? How does it affect stability of training process?

## Architecture Onboarding

- Component map: WideResNet-28-2 architecture -> FixMatch framework (weak/strong augmentations, EMA, consistency loss) -> Smooth pseudo-labeling component (smooth multiplicative factor Φ replacing hard threshold) -> Loss computation -> Gradient descent parameter updates
- Critical path: Computation of loss function and its gradient. Loss function depends on pseudo-labels, which depend on model's predictions on unlabeled data. Gradient of loss used to update model's parameters via gradient descent
- Design tradeoffs: Main tradeoff is between stability and expressiveness. Smoother loss function is more stable but may be less expressive, potentially leading to slower convergence or lower final performance. Choice of smoothing function involves tradeoff between smoothness and rate at which loss increases as prediction confidence increases
- Failure signatures: Model may fail to converge or converge to poor solution (high error rates, low accuracy, high variance across runs). Model may be too smooth, leading to slow convergence or inability to fit data well
- First 3 experiments:
  1. Implement smooth pseudo-labeling loss and compare performance to original pseudo-labeling loss on simple dataset (e.g., MNIST) with small number of labeled examples
  2. Vary smoothing function (linear, quadratic, square root) and compare performance to understand impact of smoothing function on training process
  3. Test robustness of smooth pseudo-labeling loss to hyperparameter changes (threshold, learning rate) and random initialization to understand improved robustness claimed in paper

## Open Questions the Paper Calls Out

- Question: How does smooth pseudo-labeling method scale to more complex datasets beyond CIFAR-10 and CIFAR-100, particularly when performance is far from fully supervised baseline?
- Basis in paper: Explicit statement that method is significant only in simple datasets since for datasets close to real-life applications either demand strong supervision or performance stays far from fully supervised baseline
- Why unresolved: Experiments limited to CIFAR-10 and CIFAR-100 datasets. Performance on more complex datasets like ImageNet not thoroughly explored
- What evidence would resolve it: Experiments on more complex datasets like ImageNet with varying levels of supervision, comparing performance of Smooth FixMatch to original FixMatch

- Question: What is theoretical justification for monotonicity property in semi-supervised learning algorithms based on pseudo-labeling, and how can it be built into construction of such algorithms?
- Basis in paper: Inferred from observation that both FixMatch and Smooth FixMatch can regress to higher error rates when number of labeled images increases, which is not expected behavior
- Why unresolved: Paper does not provide theoretical justification for monotonicity property, nor propose concrete method to build this property into SSL algorithms
- What evidence would resolve it: Theoretical framework explaining why SSL algorithms should exhibit monotonicity with respect to labeled dataset, and concrete method to enforce this property in construction of SSL algorithms

- Question: How does shape of smoothing factor (linear, quadratic, square root) affect performance of Smooth FixMatch in different scenarios, and is there optimal shape for different types of datasets or supervision regimes?
- Basis in paper: Explicit exploration of three cases for shape of smoothing factor: linear (µ = 1), quadratic (µ = 2), and square root (µ = 1/2). Mentions linear function performs better overall, but different hyperparameters gave good results for quadratic and square root factors
- Why unresolved: Paper does not provide comprehensive analysis of how shape of smoothing factor affects performance across different datasets or supervision regimes
- What evidence would resolve it: Systematic study comparing performance of Smooth FixMatch with different shapes of smoothing factor across variety of datasets and supervision regimes

## Limitations

- Limited to CIFAR-10 and CIFAR-100 datasets with specific architectures; results may not generalize to other domains or more complex models
- Narrow focus on discontinuities as primary source of instability without exploring alternative explanations for training instability in semi-supervised settings
- Performance degradation with increasing labeled data in random sampling is concerning but not fully explained

## Confidence

- High confidence in technical correctness of smooth pseudo-labeling implementation and its differentiability properties
- Medium confidence in claim that discontinuities are primary source of instability in scarce label regimes
- Medium confidence in robustness improvements across different hyperparameters and initialization conditions
- Low confidence in explanation for performance degradation with more labeled data in random sampling scenarios

## Next Checks

1. **Ablation study on smoothing function**: Systematically compare different smoothing functions (linear, quadratic, sigmoid) to isolate effect of smoothness on training stability and final performance

2. **Transfer to other SSL methods**: Apply smooth pseudo-labeling technique to other semi-supervised learning frameworks like MixMatch or ReMixMatch to assess generalizability beyond FixMatch

3. **Investigation of performance degradation**: Conduct detailed analysis of random sampling benchmark results to identify whether degradation is due to class imbalance, sampling variance, or other factors, and test whether alternative sampling strategies mitigate this issue