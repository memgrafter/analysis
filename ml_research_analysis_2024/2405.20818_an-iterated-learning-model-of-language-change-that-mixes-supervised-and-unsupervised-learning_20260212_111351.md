---
ver: rpa2
title: An iterated learning model of language change that mixes supervised and unsupervised
  learning
arxiv_id: '2405.20818'
source_url: https://arxiv.org/abs/2405.20818
tags:
- language
- learning
- meaning
- iterated
- meanings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new iterated learning model that avoids
  the computational inefficiency and unrealistic requirements of obversion in prior
  models. The model uses a semi-supervised learning approach with both supervised
  and unsupervised (autoencoder) training phases, allowing for the evolution of expressive,
  compositional, and stable languages without requiring agents to compute all possible
  meaning-signal mappings.
---

# An iterated learning model of language change that mixes supervised and unsupervised learning

## Quick Facts
- arXiv ID: 2405.20818
- Source URL: https://arxiv.org/abs/2405.20818
- Reference count: 40
- Key outcome: Introduces semi-supervised ILM that avoids obversion computational burden while evolving expressive, compositional, stable languages

## Executive Summary
This paper presents a new iterated learning model that addresses computational inefficiencies in prior approaches by mixing supervised and unsupervised learning. The model uses neural networks for encoding meanings into signals and decoding signals back into meanings, with an autoencoder structure that eliminates the need for computationally expensive obversion. Results show the model produces XCS (expressive, compositional, stable) languages while demonstrating a linear relationship between bottleneck size and language dimensionality.

## Method Summary
The model employs three neural networks: an encoder mapping meanings to signals, a decoder mapping signals to meanings, and an autoencoder combining both. During training, the encoder and decoder receive supervised learning from a bottleneck set of meaning-signal pairs, while the autoencoder performs unsupervised learning on arbitrary meanings. This semi-supervised approach avoids the computational burden of obversion while better reflecting actual language learning dynamics where children experience both explicit instruction and environmental exposure.

## Key Results
- The model produces XCS languages without requiring obversion computation
- Reveals linear relationship between optimal bottleneck size and language dimensionality
- Demonstrates strong performance even without bottleneck, though compositionality decreases
- Shows compositional languages are easier to learn than non-compositional ones

## Why This Works (Mechanism)

### Mechanism 1
Semi-supervised learning avoids obversion computational burden by training encoder and decoder separately and coupling them via an autoencoder. The autoencoder maps meanings → signals → meanings, allowing implicit learning of the inverse mapping without requiring explicit inversion of the full meaning-signal table. Core assumption: Autoencoder reconstruction loss provides useful training signal. Break condition: Autoencoder training dominates and overwhelms supervised signal.

### Mechanism 2
The mixed learning approach better reflects actual language acquisition in children. During learning, the model experiences both explicit instruction (meaning-signal pairs) and implicit observation (autoencoder on arbitrary meanings), mirroring how children learn from directed speech and environmental exposure. Core assumption: Semi-supervised learning is more representative of real language learning. Break condition: Autoencoder's arbitrary meaning set diverges too much from tutor's language.

### Mechanism 3
Expressivity and compositionality emerge because autoencoder encourages generalization toward compositional rules. The autoencoder forces reconstruction of meanings from signals not explicitly seen, pushing encoder and decoder toward systematic, compositional mappings rather than arbitrary ones. Core assumption: Autoencoder reconstruction loss acts as regularizer favoring compositional mappings. Break condition: Bottleneck too large or autoencoder set too small, leading to expressivity collapse.

## Foundational Learning

- Concept: Iterated Learning Model (ILM) dynamics
  - Why needed here: Central to understanding how language properties emerge over generations of tutor-pupil interactions
  - Quick check question: In the ILM, what role does the transmission bottleneck play in driving generalization?

- Concept: Semi-supervised learning
  - Why needed here: The model's novel approach combines supervised and unsupervised learning; understanding this is crucial to grasp how it avoids obversion
  - Quick check question: What is the key difference between how the Semi-Supervised ILM and the Obverter ILM train their encoder networks?

- Concept: Autoencoder architecture
  - Why needed here: The autoencoder is the core innovation that replaces obversion; understanding its structure and training is essential
  - Quick check question: In the Semi-Supervised ILM, how is the autoencoder constructed from the encoder and decoder networks?

## Architecture Onboarding

- Component map:
  - Meaning space M: Binary vectors of length n representing states of the world
  - Signal space S: Binary vectors of length n representing utterances
  - Encoder network ˆe: Maps meanings to probability distributions over signals
  - Decoder network ˆd: Maps signals to probability distributions over meanings
  - Autoencoder network ˆa: Concatenation of ˆe and ˆd, mapping meanings to meanings via signals
  - Bottleneck set B: Subset of meaning-signal pairs used for supervised training
  - Autoencoder set A: Subset of meanings used for unsupervised training

- Critical path:
  1. Initialize encoder and decoder networks
  2. For each epoch:
     - Present meaning-signal pairs from B to encoder
     - Present signal-meaning pairs from B to decoder
     - Present meanings from A to autoencoder
  3. Train all three networks using backpropagation
  4. After training, the pupil becomes the tutor for the next generation

- Design tradeoffs:
  - Larger autoencoder set A improves expressivity but increases computational cost
  - Increasing bottleneck size B can reduce performance in Obverter ILM but not necessarily in Semi-Supervised ILM
  - Autoencoder can be constructed as meaning→meaning or signal→signal; paper uses meaning→meaning

- Failure signatures:
  - Expressivity collapse: All meanings map to same or small set of signals
  - Instability: Language properties (x, c, s) fluctuate wildly between generations
  - Slow convergence: Model takes many generations to reach high x, c, and s values

- First 3 experiments:
  1. Verify Semi-Supervised ILM can evolve XCS language for small n (e.g., n=4) with varying bottleneck sizes
  2. Test effect of autoencoder set size A on expressivity and compositionality for larger n (e.g., n=8)
  3. Compare performance of Semi-Supervised ILM and Obverter ILM when bottleneck size set to |M| (no bottleneck)

## Open Questions the Paper Calls Out

### Open Question 1
How does performance of the Semi-Supervised ILM scale with larger languages (e.g., n > 20) and what is optimal architecture for such languages?
- Basis in paper: For n = 20, language becomes expressive and compositional but not stable unless hidden layer size increased to 30 nodes
- Why unresolved: Paper only explores n = 20 with specific hidden layer size without systematic investigation of larger languages or different architectures
- What evidence would resolve it: Systematic experiments varying n and hidden layer size, plus exploring different neural network architectures

### Open Question 2
What is impact of varying ratio of supervised to unsupervised learning examples on evolution of XCS languages in Semi-Supervised ILM?
- Basis in paper: Paper mentions autoencoder trained using mix of supervised and unsupervised examples but doesn't explore effect of varying this ratio
- Why unresolved: Paper doesn't investigate how changing balance between supervised and unsupervised learning affects emergence of XCS languages
- What evidence would resolve it: Experiments systematically varying ratio of supervised to unsupervised learning examples and measuring resulting expressivity, compositionality, and stability

### Open Question 3
How does Semi-Supervised ILM perform in population-based setting where agents learn from multiple tutors and teach multiple pupils simultaneously?
- Basis in paper: Paper mentions Semi-Supervised ILM could be extended to incorporate population-based models but doesn't present experiments in this direction
- Why unresolved: Paper only explores Semi-Supervised ILM in tutor-pupil setting without investigating behavior in more complex social learning environment
- What evidence would resolve it: Simulations of Semi-Supervised ILM in population-based setting where agents interact with multiple peers

## Limitations

- Several implementation details remain underspecified including exact neural network architecture beyond basic dimensions and precise training parameters
- Claim that model "works well even without a bottleneck" needs qualification as compositionality does decrease
- Relationship between autoencoder set size A and expressivity, while observed, lacks theoretical grounding for why |A|=3|B| emerges as optimal

## Confidence

**High Confidence**: Mechanism by which autoencoder avoids explicit obversion is well-specified and technically sound; linear relationship between bottleneck size and language dimensionality is clearly demonstrated

**Medium Confidence**: Claim that model better reflects child language learning is supported by general child development literature but lacks direct empirical validation; assertion that compositional languages are easier to learn than non-compositional ones is plausible but not rigorously proven within model itself

**Low Confidence**: Exact conditions under which model fails or produces degenerate languages are not well-characterized; optimal size relationship |A|=3|B| appears empirical rather than theoretically derived

## Next Checks

1. **Architecture Sensitivity Test**: Systematically vary neural network architecture (number of hidden layers, nodes per layer) while holding all else constant to determine sensitivity to implementation details

2. **Autoencoder Size Sweep**: Conduct comprehensive experiment varying |A| from |B|/2 to 10|B| to precisely map relationship between autoencoder set size and both expressivity and compositionality

3. **No-Bottleneck Edge Case**: Run extended simulations without any bottleneck (|B|=0) to characterize exact degradation pattern in compositionality and stability, and determine if threshold exists below which model fails entirely