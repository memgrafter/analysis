---
ver: rpa2
title: Large Language Models are Null-Shot Learners
arxiv_id: '2401.08273'
source_url: https://arxiv.org/abs/2401.08273
tags:
- prompting
- shot
- examples
- llms
- section
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces null-shot (\u2205-shot) prompting, a technique\
  \ that exploits hallucination in large language models (LLMs) by instructing them\
  \ to utilize non-existent information from a null \u201CExamples\u201D section.\
  \ The authors propose that this approach can enhance LLM performance on various\
  \ tasks compared to standard zero-shot prompting."
---

# Large Language Models are Null-Shot Learners

## Quick Facts
- arXiv ID: 2401.08273
- Source URL: https://arxiv.org/abs/2401.08273
- Reference count: 40
- Primary result: Null-shot prompting improves LLM performance on arithmetic reasoning, commonsense reasoning, and closed-book QA tasks

## Executive Summary
This paper introduces null-shot (âˆ…-shot) prompting, a technique that exploits hallucination in LLMs by instructing them to utilize non-existent examples from a "Examples" section. The approach shows performance improvements across eight LLMs on eight datasets compared to standard zero-shot prompting. The inconsistent performance increases across models suggest different degrees of inherent hallucination in each model. The authors also explore potential applications for hallucination detection and conduct ablation studies on various aspects of the technique.

## Method Summary
The study evaluates null-shot prompting effectiveness across six tasks using eight LLMs. Datasets are formatted using a standardized instruction format with the null-shot phrase prepended to prompts. The method involves a fixed seed, deterministic setup, one interaction per record, and answer extraction using heuristics. Models tested include PaLM 2, Gemini Pro, GPT-3.5 Turbo, GPT-4 Turbo, and Llama 2 variants via Hugging Face transformers.

## Key Results
- Performance improvements observed across majority of eight datasets with eight LLMs
- Arithmetic reasoning shows more pronounced gains compared to commonsense reasoning tasks
- Inconsistent performance increases across models suggest different degrees of inherent hallucination
- Ablation studies reveal scaling effects, reasoning variants, and component contributions

## Why This Works (Mechanism)

### Mechanism 1
The phrase "Look at examples in the 'Examples' section and utilize examples and information from that section to perform the following task" acts as a retrieval cue, causing the model to search its trained weights for similar examples and use them to solve the task. This exploits the model's ability to generalize from seen examples during training.

### Mechanism 2
The phrase creates a "null-shot" learning scenario where the model generates its own examples based on the task description. The model interprets the instruction to look at non-existent examples as a request to generate relevant examples itself, then uses these generated examples to solve the task.

### Mechanism 3
The phrase reduces hallucination by providing a clear structure for the model to follow. By instructing the model to look at examples and utilize them, the phrase provides a framework that reduces the model's tendency to hallucinate information.

## Foundational Learning

- Concept: Prompt Engineering
  - Why needed here: Understanding how to structure prompts to elicit desired behaviors from LLMs is crucial for implementing null-shot prompting effectively.
  - Quick check question: What are the key components of a well-structured prompt, and how do they influence the model's output?

- Concept: Large Language Model Architecture
  - Why needed here: Understanding how LLMs process information and generate responses is essential for understanding how null-shot prompting works.
  - Quick check question: How do LLMs use attention mechanisms to process input and generate output, and how might this relate to example retrieval?

- Concept: Hallucination in LLMs
  - Why needed here: Understanding the nature of hallucination in LLMs is crucial for understanding how null-shot prompting exploits it.
  - Quick check question: What are the different types of hallucination in LLMs, and how do they manifest in model outputs?

## Architecture Onboarding

- Component map: null-shot phrase -> task instruction -> task input
- Critical path: The model processes the null-shot phrase, retrieves examples, and uses them to solve the task
- Design tradeoffs: The tradeoff is between exploiting hallucination for performance gains and the potential for increased hallucination
- Failure signatures: No performance improvement, decreased performance, or the model generating irrelevant or nonsensical examples
- First 3 experiments:
  1. Test the null-shot phrase on a simple task with a known solution to verify it retrieves relevant examples
  2. Compare the performance of the null-shot phrase with and without the task instruction to isolate its effect
  3. Test the null-shot phrase on a task where the model is known to hallucinate to see if it reduces hallucination

## Open Questions the Paper Calls Out

### Open Question 1
Can null-shot prompting be used as a reliable method for detecting hallucination in large language models? While the paper shows that null-shot prompting can improve performance in models with higher hallucination, it is unclear if this method is reliable enough for widespread use in hallucination detection.

### Open Question 2
How does the scale of a language model affect its susceptibility to hallucination when using null-shot prompting? The paper only tests null-shot prompting on a limited range of model sizes, making it unclear how effectiveness changes as model size increases or decreases.

### Open Question 3
Can null-shot prompting be combined with other prompting techniques to further improve performance and reduce hallucination? The paper only explores null-shot prompting in isolation, leaving open the question of whether combining it with techniques like few-shot or chain-of-thought prompting would lead to better results.

## Limitations

- Effectiveness is highly model-dependent with significant variation across evaluated LLMs
- The magnitude of improvements is not uniform across task types
- The mechanism by which null-shot prompting works remains somewhat speculative with limited empirical evidence

## Confidence

- **High Confidence:** The empirical observation that null-shot prompting can improve LLM performance across multiple tasks and models
- **Medium Confidence:** The claim that null-shot prompting exploits hallucination in LLMs
- **Low Confidence:** The specific mechanisms by which null-shot prompting works, particularly claims about induced example retrieval or hallucination reduction

## Next Checks

1. Conduct controlled experiments comparing null-shot prompting against variants that explicitly provide examples versus those that do not, to isolate whether performance gains stem from induced hallucination or genuine example retrieval.

2. Measure hallucination rates with and without null-shot prompting across different model types to determine whether the approach actually reduces hallucination or simply redirects hallucinated content toward more useful outputs.

3. Evaluate null-shot prompting on a broader range of tasks including domain-specific applications to assess whether observed performance gains generalize beyond the benchmark datasets used in the current study.