---
ver: rpa2
title: Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation
  Problem
arxiv_id: '2402.02868'
source_url: https://arxiv.org/abs/2402.02868
tags:
- learning
- fine-tuning
- forgetting
- pre-trained
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Forgetting of pre-trained capabilities (FPC) is a major issue
  in fine-tuning RL models, where performance on parts of the state space not visited
  early in fine-tuning deteriorates due to interference. This study identifies two
  common FPC scenarios: state coverage gap and imperfect cloning gap.'
---

# Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem

## Quick Facts
- arXiv ID: 2402.02868
- Source URL: https://arxiv.org/abs/2402.02868
- Reference count: 40
- Primary result: Knowledge retention techniques mitigate forgetting of pre-trained capabilities, improving RL fine-tuning performance 2x in NetHack and other domains.

## Executive Summary
This paper identifies forgetting of pre-trained capabilities (FPC) as a major issue in fine-tuning reinforcement learning models. The authors show that during fine-tuning, performance on parts of the state space not visited early deteriorates due to interference in the function approximator. They propose that FPC manifests in two scenarios - state coverage gap and imperfect cloning gap - and demonstrate that knowledge retention techniques like elastic weight consolidation, behavioral cloning, and kickstarting can effectively mitigate FPC. These techniques significantly improve fine-tuning performance, achieving state-of-the-art results in NetHack (from 5K to over 10K points) and showing similar improvements in Montezuma's Revenge and Meta-World robotic tasks.

## Method Summary
The approach involves fine-tuning pre-trained RL models on downstream tasks while applying knowledge retention techniques to mitigate forgetting. The methods tested include Elastic Weight Consolidation (EWC), behavioral cloning (BC), kickstarting (KS), and episodic memory (EM). The paper frames the problem using a state space partition into CLOSE states (visited early in fine-tuning) and FAR states (visited later), analyzing how forgetting manifests differently in two gap scenarios: state coverage gap where the pre-trained policy is good on FAR but poor on CLOSE, and imperfect cloning gap where the pre-trained policy is close but not identical to the optimal policy. Experiments are conducted across multiple domains including NetHack, Montezuma's Revenge, and Meta-World robotic tasks.

## Key Results
- Knowledge retention techniques (EWC, BC, KS, EM) significantly improve fine-tuning performance across all tested environments
- In NetHack, BC and KS achieve state-of-the-art results, improving performance from 5K to over 10K points (2x improvement)
- The paper identifies two distinct forgetting scenarios: state coverage gap and imperfect cloning gap
- BC performs best for state coverage gap while KS excels in imperfect cloning gap scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Forgetting of pre-trained capabilities (FPC) occurs when fine-tuning RL models on a task where parts of the state space not visited early in training are lost due to interference in the function approximator.
- **Mechanism:** Interference in function approximator weights during fine-tuning causes the policy to deteriorate on previously mastered regions of the state space. This happens because the policy updates based on newly encountered states can overwrite representations important for handling states from pre-training that are not visited immediately.
- **Core assumption:** RL agents visit states non-uniformly, and states that are infrequently visited early in fine-tuning are vulnerable to forgetting if they rely on the same network parameters as frequently visited states.
- **Evidence anchors:**
  - [abstract] "Forgetting of pre-trained capabilities (FPC) is a major issue in fine-tuning RL models, where performance on parts of the state space not visited early in fine-tuning deteriorates due to interference."
  - [section] "The states in FAR are reachable only by going through CLOSE; hence, they are infrequently visited as they can be reached only once some learning on CLOSE happens."
  - [corpus] Weak. While corpus papers discuss catastrophic forgetting, none directly anchor the specific RL fine-tuning FPC mechanism described here.
- **Break condition:** If state visitation during fine-tuning is uniform or if the network architecture perfectly isolates representations for different state regions, this mechanism may not manifest.

### Mechanism 2
- **Claim:** Knowledge retention techniques mitigate FPC by applying auxiliary losses that preserve behavior on important state subspaces.
- **Mechanism:** Techniques like Elastic Weight Consolidation (EWC), behavioral cloning (BC), and kickstarting (KS) introduce additional loss terms that regularize the policy to maintain performance on states relevant to pre-training. EWC penalizes changes to parameters important for pre-training, BC uses expert trajectories to guide policy updates, and KS uses online policy data to preserve pre-trained behavior.
- **Core assumption:** Preserving pre-trained capabilities on relevant state subspaces directly improves downstream task performance, and auxiliary losses can effectively regularize the network to maintain these capabilities.
- **Evidence anchors:**
  - [abstract] "Knowledge retention techniques, such as elastic weight consolidation, behavioral cloning, and kickstarting, mitigate FPC and significantly improve fine-tuning performance."
  - [section] "We consider the following popular methods for knowledge retention: Elastic Weight Consolidation (EWC), replay by behavioral cloning (BC), kickstarting (KS), and episodic memory (EM)."
  - [corpus] Missing. The cited corpus does not directly address knowledge retention in RL fine-tuning, though it discusses forgetting more generally.
- **Break condition:** If the auxiliary loss coefficients are poorly tuned, or if the pre-trained policy is suboptimal, these techniques may hinder rather than help learning.

### Mechanism 3
- **Claim:** The severity of FPC depends on the relationship between "CLOSE" and "FAR" states in the task, with imperfect cloning gap and state coverage gap representing two key scenarios.
- **Mechanism:** In imperfect cloning gap, the pre-trained policy is close but not identical to the optimal policy for the downstream task. Fine-tuning on CLOSE states causes interference that degrades performance on FAR states. In state coverage gap, the pre-trained policy is good on FAR but poor on CLOSE; fine-tuning on CLOSE causes forgetting on FAR. The degree of forgetting is related to how much the CLOSE and FAR states rely on overlapping network representations.
- **Core assumption:** The structural relationship between parts of the state space (CLOSE vs FAR) determines the likelihood and severity of forgetting, and these two specific gap types capture the main ways forgetting manifests in RL fine-tuning.
- **Evidence anchors:**
  - [abstract] "We identify two important instances of FPC: state coverage gap and imperfect cloning gap."
  - [section] "In state coverage gap, we consider a pre-trained agent that is performing well mostly on FAR and does not know how to behave on CLOSE. However, when fine-tuned on CLOSE, its behavior on FAR will deteriorate considerably due to forgetting."
  - [corpus] Weak. The corpus discusses forgetting in general but does not anchor these specific gap types in RL fine-tuning.
- **Break condition:** If the task structure does not naturally partition into CLOSE and FAR states, or if the pre-trained policy is already optimal, these gap types may not apply.

## Foundational Learning

- **Concept:** Markov Decision Processes (MDPs)
  - **Why needed here:** The paper frames RL fine-tuning as an MDP problem where the state space can be partitioned into regions (CLOSE/FAR) and policies are learned to maximize cumulative reward. Understanding MDPs is essential for grasping how forgetting manifests in RL.
  - **Quick check question:** In an MDP, what is the difference between the state space and the observation space, and why does this distinction matter for understanding FPC?

- **Concept:** Catastrophic forgetting in neural networks
  - **Why needed here:** FPC is framed as a specific instance of catastrophic forgetting in the context of RL fine-tuning. Understanding the general mechanisms of forgetting in neural networks helps contextualize the specific RL case.
  - **Quick check question:** What is the primary difference between catastrophic forgetting in supervised learning and FPC in RL fine-tuning, based on the paper's framing?

- **Concept:** Knowledge retention and continual learning
  - **Why needed here:** The paper applies techniques from continual learning (EWC, BC, KS) to mitigate FPC in RL fine-tuning. Understanding how these techniques work in continual learning contexts is crucial for adapting them to RL.
  - **Quick check question:** How does the goal of knowledge retention in RL fine-tuning differ from its goal in traditional continual learning, according to the paper?

## Architecture Onboarding

- **Component map:** Pre-trained policy (π*) -> Fine-tuning algorithm (APPO/PPO/SAC) -> Knowledge retention modules (EWC/BC/KS/EM) -> State space partition (CLOSE/FAR) -> Performance metrics (scores, success rates, CKA)

- **Critical path:** Load pre-trained policy π* -> Initialize fine-tuning algorithm with π* -> Apply knowledge retention method during training -> Monitor performance on CLOSE and FAR states separately -> Evaluate final policy on downstream task

- **Design tradeoffs:** Using BC requires access to pre-training data but can preserve behavior on FAR states; KS uses online data but may not cover FAR states early; EWC adds computational overhead but requires no additional data; BC and KS require processing additional examples; Episodic memory only works with off-policy algorithms due to replay buffer requirements

- **Failure signatures:** Rapid decline in performance on FAR states during early fine-tuning indicates FPC; Fine-tuning performance plateaus below pre-trained policy suggests interference; Knowledge retention methods that hurt performance may indicate suboptimal pre-trained policy

- **First 3 experiments:** Run vanilla fine-tuning without knowledge retention to establish baseline FPC severity; Apply BC with varying buffer sizes to determine optimal data retention; Test EWC with different regularization coefficients to find stability-accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective knowledge retention methods for different RL fine-tuning scenarios, and how do their trade-offs (e.g., computational cost, memory usage) impact their practical applicability?
- Basis in paper: [explicit] The paper compares several knowledge retention methods (EWC, BC, KS, EM) and finds that their effectiveness varies depending on the environment and problem type (e.g., BC is better for state coverage gap, KS for imperfect cloning gap). However, the choice of method also depends on constraints like memory and computational resources.
- Why unresolved: While the paper provides insights into the effectiveness of different methods, it does not provide a comprehensive framework for selecting the most appropriate method based on specific problem characteristics and resource constraints. Additionally, the paper does not explore more advanced knowledge retention techniques that might be even more effective.
- What evidence would resolve it: A systematic study comparing a wider range of knowledge retention methods across various RL fine-tuning scenarios, considering both performance and practical considerations like computational cost and memory usage. This could involve developing a decision tree or scoring system to guide the selection of the most appropriate method for a given problem.

### Open Question 2
- Question: How does the interplay between actions and observations in RL contribute to forgetting of pre-trained capabilities, and can this phenomenon be mitigated by designing RL algorithms that explicitly account for this interplay?
- Basis in paper: [explicit] The paper argues that the interplay between actions and observations in RL leads to a changing visitation of states during fine-tuning, which causes forgetting of pre-trained capabilities. This is in contrast to supervised learning, where the data distribution is i.i.d. and forgetting is not a factor.
- Why unresolved: The paper does not provide a detailed theoretical analysis of how the interplay between actions and observations contributes to forgetting, nor does it propose specific RL algorithm modifications to address this issue. Understanding this interplay and developing algorithms that mitigate its negative effects is crucial for improving RL fine-tuning.
- What evidence would resolve it: A theoretical analysis of the relationship between actions, observations, and forgetting in RL, followed by the development and evaluation of RL algorithms that explicitly account for this interplay. This could involve designing algorithms that prioritize exploration of under-visited states or that incorporate mechanisms to prevent interference between different parts of the state space.

### Open Question 3
- Question: How does the severity of forgetting in RL fine-tuning scale with the complexity of the environment and the size of the neural network, and what are the underlying mechanisms that drive this relationship?
- Basis in paper: [explicit] The paper provides some initial evidence that forgetting becomes more severe as the complexity of the environment increases (e.g., more states in CLOSE) and that the size of the neural network might play a role (e.g., smaller networks might be more prone to forgetting). However, the paper does not provide a comprehensive analysis of this relationship.
- Why unresolved: The paper does not provide a detailed investigation of how the severity of forgetting scales with environment complexity and network size, nor does it explore the underlying mechanisms that drive this relationship. Understanding these factors is crucial for designing effective RL fine-tuning strategies.
- What evidence would resolve it: A systematic study examining the relationship between environment complexity, network size, and forgetting severity in RL fine-tuning. This could involve varying the size of the neural network and the complexity of the environment (e.g., number of states, reward sparsity) and measuring the resulting forgetting. Additionally, analyzing the internal representations of the neural network during fine-tuning could provide insights into the mechanisms that drive forgetting.

## Limitations
- The paper assumes the pre-trained policy is "good" without defining what this means quantitatively
- The relationship between representation similarity and forgetting is asserted but not directly measured
- The paper doesn't explore whether alternative fine-tuning strategies might achieve similar results without explicit forgetting mitigation

## Confidence

**Confidence labels:**
- Mechanism 1 (FPC due to interference): Medium - theoretically sound but lacks direct empirical validation
- Mechanism 2 (knowledge retention effectiveness): High - strong empirical results across multiple domains
- Mechanism 3 (CLOSE/FAR gap scenarios): Medium - useful conceptual framework but limited validation of its universality

## Next Checks

1. Measure representation similarity (e.g., CKA) between pre-trained and fine-tuned policies on CLOSE vs FAR states to directly test the interference hypothesis
2. Conduct ablation studies varying the timing and frequency of state visits during fine-tuning to determine when forgetting is most severe
3. Test whether simpler regularization techniques (L2, dropout) achieve comparable results to the more complex knowledge retention methods