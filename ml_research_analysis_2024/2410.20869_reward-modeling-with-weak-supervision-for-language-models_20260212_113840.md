---
ver: rpa2
title: Reward Modeling with Weak Supervision for Language Models
arxiv_id: '2410.20869'
source_url: https://arxiv.org/abs/2410.20869
tags:
- baseline
- dataset
- weakly
- datasets
- labeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates extending RLHF datasets through weak supervision,
  employing noisy labeling functions based on response characteristics like length,
  readability, lexical diversity, and sentiment. The authors developed labeling functions
  via data analysis and used a Snorkel label model to weakly annotate unlabeled data.
---

# Reward Modeling with Weak Supervision for Language Models

## Quick Facts
- arXiv ID: 2410.20869
- Source URL: https://arxiv.org/abs/2410.20869
- Authors: Ben Hauptvogel; Malte Ostendorff; Georg Rehm; Sebastian Möller
- Reference count: 19
- Primary result: Weak supervision improves reward model performance when training sets are small, with larger gains for AI-annotated than human-annotated data

## Executive Summary
This paper investigates extending RLHF datasets through weak supervision using noisy labeling functions based on response characteristics like length, readability, lexical diversity, and sentiment. The authors developed labeling functions via data analysis and used a Snorkel label model to weakly annotate unlabeled data. Experiments across four RLHF datasets show that weak supervision improves reward model performance when training sets are small, with larger gains for AI-annotated than human-annotated data. Confidence-threshold filtering of weakly labeled samples further enhances results.

## Method Summary
The authors analyze RLHF datasets to identify heuristics correlating with response preference, implement labeling functions based on these heuristics (length, readability, lexical diversity, sentiment), and use Snorkel's label model to combine noisy signals into weak supervision labels. They apply confidence threshold filtering to select high-quality weak samples and train reward models on combined original and weakly labeled datasets. The approach is tested on four RLHF datasets, with MT-BENCH experiments using LLM-generated data that is then weakly labeled.

## Key Results
- Weak supervision significantly improves reward model performance when training sets are small
- Larger performance gains observed for AI-annotated data compared to human-annotated data
- Confidence-threshold filtering of weakly labeled samples further enhances reward model accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weak supervision improves reward model performance when training sets are small by providing additional preference signals without costly manual labeling.
- Mechanism: Labeling functions based on simple heuristics (text length, readability, lexical diversity, sentiment, numbers, keywords) generate noisy but informative labels. Snorkel's label model denoises these signals and creates a weak supervision signal that augments the originally labeled data.
- Core assumption: Simple heuristics correlate with human preference signals, and the label model can effectively denoise the noisy signals.
- Evidence anchors: [abstract] "By analyzing RLHF datasets to identify heuristics that correlate with response preference, we wrote simple labeling functions and then calibrated a label model to weakly annotate unlabeled data."

### Mechanism 2
- Claim: Confidence-threshold filtering of weakly labeled samples enhances results by selecting high-quality weak labels while rejecting uncertain ones.
- Mechanism: The label model outputs prediction probabilities that are converted to confidence values. Samples below a threshold are discarded, improving average accuracy at the cost of reduced sample count.
- Core assumption: High-confidence predictions from the label model are more likely to be correct than low-confidence ones.
- Evidence anchors: [section 3.6] "To improve the accuracy of our labeling, we can implement a confidence threshold. We specify a particular threshold value and exclude any samples with confidence levels below this value."

### Mechanism 3
- Claim: Generating new data and applying weak labels offers scalable method to extend preference datasets when original data is limited.
- Mechanism: LLMs generate prompt-response pairs, which are then weakly labeled using the same labeling function and label model approach. This creates synthetic preference data that augments the original dataset.
- Core assumption: Generated data is sufficiently similar to real preference data that weak labels remain meaningful.
- Evidence anchors: [abstract] "Additionally, using an LLM to generate and then weakly label responses offers a promising method for extending preference data."

## Foundational Learning

- Concept: Snorkel label model calibration and denoising
  - Why needed here: The label model combines multiple noisy labeling functions into a coherent weak supervision signal
  - Quick check question: How does Snorkel handle correlations between labeling functions during calibration?

- Concept: Statistical significance testing for heuristic validation
  - Why needed here: Independent t-tests determine whether heuristic differences between chosen/rejected responses are meaningful
  - Quick check question: What p-value threshold was used to determine statistical significance?

- Concept: Confidence threshold optimization
  - Why needed here: Balancing precision vs recall of weak labels through confidence-based filtering
  - Quick check question: What happens to performance when confidence threshold is set too high or too low?

## Architecture Onboarding

- Component map: Data analysis module → labeling functions → Snorkel label model → weak dataset generation → reward model training → evaluation
- Critical path: 1. Analyze baseline dataset to identify heuristics 2. Implement labeling functions with appropriate thresholds 3. Calibrate Snorkel label model on baseline data 4. Apply label model to unlabeled data 5. Filter by confidence threshold 6. Train reward model on combined dataset 7. Evaluate performance
- Design tradeoffs: Coverage vs accuracy in labeling functions (higher thresholds reduce coverage but may improve accuracy), dataset size vs weak supervision benefit (diminishing returns as original dataset grows), confidence threshold vs sample count (higher thresholds improve quality but reduce quantity)
- Failure signatures: No improvement over baseline → labeling functions poorly correlated with preference, performance degradation with weak samples → label model failing to denoise effectively, high variance in results → insufficient data for stable label model calibration
- First 3 experiments: 1. Test labeling functions on small baseline subset to verify accuracy/coverage tradeoffs 2. Calibrate label model and validate accuracy on held-out weak samples 3. Train reward model with varying confidence thresholds to find optimal filtering point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors beyond simple heuristics influence human preference in response selection?
- Basis in paper: [explicit] The authors note that "the human (or AI) labeling process is inherently more complex and likely extends beyond these simple factors" and cite their qualitative analysis showing limitations of heuristics
- Why unresolved: Current labeling functions rely on basic features like length and sentiment, but the authors found cases where these failed to capture true preference
- What evidence would resolve it: A systematic study identifying additional factors that influence preference decisions, or development of more sophisticated labeling functions that capture these factors

### Open Question 2
- Question: How do different confidence threshold values affect the optimal balance between accuracy and dataset size in weakly supervised RLHF?
- Basis in paper: [explicit] The authors mention "We conducted experiments with different confidence thresholds to assess their impact on the reward model performance" but don't provide a comprehensive analysis
- Why unresolved: The paper shows that confidence thresholds affect performance but doesn't determine optimal thresholds across different dataset sizes or types
- What evidence would resolve it: A systematic study varying confidence thresholds across multiple dataset sizes and types to identify optimal thresholds for maximizing performance

### Open Question 3
- Question: Can LLMs reliably generate high-quality preference data when used for both generation and weak supervision?
- Basis in paper: [explicit] The MT-BENCH experiments showed promising results using generated data, but the authors note "The small size of the MT-BENCH datasets...likely contribute to the noise in the results"
- Why unresolved: The experiments were limited in scale and the results showed volatility, suggesting potential limitations in this approach
- What evidence would resolve it: Large-scale experiments testing LLM-generated data across multiple model sizes and domains, with comparison to human-annotated baselines

## Limitations
- Weak supervision effectiveness depends on correlation strength between heuristics and actual human preferences, showing minimal gains for complex human preference patterns
- Snorkel label model performance in this specific RLHF context lacks external validation
- Confidence threshold filtering may introduce bias by excluding potentially useful samples with lower confidence scores

## Confidence

**High confidence**: The basic mechanism of using heuristics-based labeling functions and Snorkel label model for weak supervision is well-established and shows consistent improvements across multiple datasets. The observation that weak supervision helps more with AI-annotated than human-annotated data is supported by clear experimental evidence.

**Medium confidence**: The specific choice of heuristics (length, readability, lexical diversity, sentiment) and their implementation details may affect generalizability. The optimal confidence threshold values and their impact on different dataset characteristics require further validation.

**Low confidence**: The scalability claims for generating synthetic preference data through LLMs need more extensive validation, particularly regarding how generated data distributions compare to real preference data and whether weak labels remain meaningful across different domains.

## Next Checks
1. **Cross-dataset validation**: Test the same weak supervision approach on additional RLHF datasets from different domains to verify generalizability beyond the four studied datasets.

2. **Label model robustness**: Evaluate how Snorkel's performance varies with different labeling function combinations and whether alternative label models (e.g., probabilistic graphical models) might perform better for RLHF-specific characteristics.

3. **Human evaluation study**: Conduct systematic human evaluation comparing weakly supervised reward models against fully supervised baselines to quantify actual preference alignment rather than just classification accuracy metrics.