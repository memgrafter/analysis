---
ver: rpa2
title: An Automated Explainable Educational Assessment System Built on LLMs
arxiv_id: '2412.13381'
source_url: https://arxiv.org/abs/2412.13381
tags:
- assessment
- llms
- student
- automated
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AERA Chat is an interactive platform for automated explainable
  student answer assessment using large language models (LLMs). It addresses the challenge
  of limited explainability in educational assessment by generating automated marking
  with natural language rationales.
---

# An Automated Explainable Educational Assessment System Built on LLMs

## Quick Facts
- arXiv ID: 2412.13381
- Source URL: https://arxiv.org/abs/2412.13381
- Reference count: 8
- AERA Chat is an interactive platform for automated explainable student answer assessment using large language models (LLMs).

## Executive Summary
AERA Chat addresses the challenge of limited explainability in educational assessment by leveraging large language models to generate automated marking with natural language rationales. The system provides a bulk marking interface for batch processing student responses, LLM-powered rationale generation, and visual highlighting of key answer elements and assessment aspects. Built on a microservices architecture with Docker, it supports both public and private LLM models while facilitating research through unified benchmarking capabilities and rationale collection.

## Method Summary
The system implements a microservices architecture using Docker containers for frontend (Remix framework), backend (Flask), LLM services, and PostgreSQL database. It enables automated student answer scoring through LLM-based assessment with in-context learning capabilities, generating natural language rationales that justify scoring decisions. The platform supports both public LLM APIs (OpenAI) and private models via HuggingFace, providing visual highlighting of key answer elements and assessment aspects through GPT-4o's word-level tagging functionality.

## Key Results
- Automated marking system generates natural language rationales for student answer assessments
- Visual highlighting feature identifies key answer elements and assessment aspects
- Supports both public and private LLM models through unified microservices architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate natural language rationales for student answer assessments, improving explainability.
- Mechanism: LLMs leverage in-context learning and reasoning capabilities to produce explanations that justify their scoring decisions in human-readable form.
- Core assumption: LLMs can accurately assess student responses and articulate reasoning in natural language without requiring extensive task-specific training.
- Evidence anchors:
  - [abstract] "This system leverages large language models (LLMs) to generate automated marking and rationale explanations, addressing the challenge of limited explainability in automated educational assessment"
  - [section] "The recent development of large language models (LLMs) has introduced a new approach that leverages in-context learning and reasoning capabilities... to generate natural language rationales that justify model decisions"
- Break condition: When LLMs generate factually incorrect rationales or when the generated explanations do not accurately reflect the reasoning behind the scores assigned.

### Mechanism 2
- Claim: Visual highlighting of key answer elements and assessment aspects enhances user understanding and verification.
- Mechanism: The system uses GPT-4o to perform word-level tagging and generate context highlights in JSON format, which are then visually displayed to users.
- Core assumption: Visual cues can effectively communicate the relationship between student answer content and assessment rationale without requiring technical expertise.
- Evidence anchors:
  - [section] "Our platform aims to enhance user experience by providing clear, high-contrast visual cues within student answers and assessment rationales... Users can choose to visualize the key answer elements mentioned in the student answers or visualize the positive aspects... and negative aspects... within rationales"
  - [section] "This functionality is powered by GPT-4o, which performs word-level tagging and generates context highlights in a JSON format, ensuring effective and efficient context visualization"
- Break condition: When the highlighting system fails to accurately identify key elements or when visual cues become too complex for non-technical users to interpret.

### Mechanism 3
- Claim: A microservices architecture with Docker enables scalability and flexibility in LLM integration.
- Mechanism: The system separates frontend, backend, LLM services, and database into independent Docker containers that communicate via REST API.
- Core assumption: Microservices architecture provides better scalability and maintainability than monolithic approaches for educational assessment platforms.
- Evidence anchors:
  - [section] "AERA Chat is designed using a microservices architecture that integrates multiple LLMs through a unified web interface. We use Docker to modularize services"
  - [section] "Frontend: The frontend provides a responsive web interface... Backend: The backend layer serves as the backbone... LLM Services: Users of our system can opt to employ publicly available API-based LLMs, or they may choose to develop and utilize privately trained, customized LLMs... Database: We utilize a PostgreSQL relational database"
- Break condition: When inter-service communication overhead becomes too high or when service orchestration complexity outweighs the benefits of modularity.

## Foundational Learning

- Concept: Large Language Models and In-Context Learning
  - Why needed here: Understanding how LLMs can perform educational assessment tasks without extensive fine-tuning is fundamental to grasping the system's approach.
  - Quick check question: How do LLMs leverage in-context learning to perform tasks they weren't explicitly trained on, and what are the limitations of this approach?

- Concept: Automated Student Answer Scoring (ASAS) Systems
  - Why needed here: The system builds on existing ASAS methodologies while adding explainability features, so understanding traditional approaches is essential.
  - Quick check question: What are the key differences between traditional text classifier-based ASAS systems and LLM-based approaches in terms of explainability?

- Concept: Explainability and Interpretability in AI
  - Why needed here: The core value proposition of AERA Chat is providing transparent assessments, which requires understanding different approaches to AI explainability.
  - Quick check question: What are the trade-offs between feature analysis, attention visualization, and natural language rationale generation as methods for making AI systems explainable?

## Architecture Onboarding

- Component map:
  - Frontend (Remix/React) -> Backend (Flask) -> LLM Services (OpenAI/HuggingFace) -> Database (PostgreSQL)

- Critical path:
  1. User uploads questions and student answers via frontend
  2. Frontend sends request to backend API
  3. Backend compiles prompts and routes to selected LLM service
  4. LLM generates scores and rationales
  5. Results stored in database and returned to frontend
  6. Frontend displays results with optional highlighting

- Design tradeoffs:
  - Using multiple LLMs (public and private) provides flexibility but increases complexity
  - Docker microservices enable scalability but add orchestration overhead
  - PostgreSQL provides relational structure but may limit horizontal scaling compared to NoSQL

- Failure signatures:
  - LLM service timeouts indicate model capacity or API issues
  - Frontend not updating suggests backend API communication failure
  - Inconsistent scoring across runs may indicate LLM prompt instability
  - Database connection errors prevent assessment record storage

- First 3 experiments:
  1. Deploy minimal working system with single public LLM (GPT-3.5-turbo) and test basic assessment functionality
  2. Add visual highlighting feature and validate JSON output structure and rendering
  3. Integrate private LLM support using HuggingFace and compare performance against public models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the system perform when using privately trained, customized LLMs versus publicly available API-based models for educational assessment?
- Basis in paper: [explicit] The paper mentions users can opt to employ publicly available API-based LLMs or develop and utilize privately trained, customized LLMs.
- Why unresolved: The paper does not provide comparative performance data between public and private LLM models.
- What evidence would resolve it: Empirical studies comparing assessment accuracy, rationale quality, and computational efficiency between public and private LLM implementations on standardized educational datasets.

### Open Question 2
- Question: What is the impact of human-verified ground truth annotations on the quality of generated rationales compared to noisy LLM-generated rationales?
- Basis in paper: [explicit] The paper notes that most ground truth labels used for training rationale generation models rely on noisy rationales generated by LLMs, often without human verification.
- Why unresolved: The paper does not present a systematic comparison between human-verified and LLM-generated rationales.
- What evidence would resolve it: Controlled experiments comparing rationale quality metrics (factual accuracy, coherence, helpfulness) between systems trained on human-verified versus LLM-generated annotations.

### Open Question 3
- Question: How does the highlighting technique for key answer elements and rationale aspects affect user trust and understanding of the assessment process?
- Basis in paper: [explicit] The paper describes a highlighting technique that visualizes key answer elements and positive/negative aspects within rationales.
- Why unresolved: The paper does not include user studies measuring the effectiveness of these visual aids on educator comprehension or trust in the system.
- What evidence would resolve it: User studies with educators evaluating their understanding of rationales with and without highlighting, including measures of trust, comprehension, and time to verify assessments.

## Limitations

- System effectiveness constrained by LLM hallucination risks, particularly in rationale generation where models may produce plausible but incorrect explanations
- Reliance on in-context learning without task-specific fine-tuning leads to performance variation based on prompt quality and input formatting
- Microservices architecture introduces operational complexity that may impact system reliability in production environments

## Confidence

**High Confidence**: The core microservices architecture implementation and basic LLM integration are technically sound, as evidenced by the modular design and Docker-based deployment approach. The platform successfully demonstrates the feasibility of combining multiple LLM services for educational assessment.

**Medium Confidence**: The visual highlighting mechanism's effectiveness for non-technical users is plausible based on the described implementation, but user experience validation data is not provided. The automated performance evaluation framework appears methodologically sound, though specific evaluation metrics and results are not detailed.

**Low Confidence**: The claim that the system significantly enhances educational assessment transparency compared to existing solutions lacks empirical validation. The generalization capability of the AERA model across diverse educational domains remains unproven without broader testing on varied assessment types.

## Next Checks

1. **Hallucination Audit**: Conduct systematic testing of rationale generation accuracy by comparing LLM explanations against human expert assessments for a diverse sample of student answers, measuring the rate of factually incorrect or misleading explanations.

2. **User Comprehension Study**: Evaluate the visual highlighting feature's effectiveness by testing comprehension rates among educators with varying technical backgrounds, comparing understanding of highlighted versus plain-text rationales.

3. **Cross-Domain Performance Testing**: Validate the system's generalization capability by deploying it across at least three distinct educational domains (e.g., mathematics, literature, science) and measuring consistency in assessment quality and explanation accuracy.