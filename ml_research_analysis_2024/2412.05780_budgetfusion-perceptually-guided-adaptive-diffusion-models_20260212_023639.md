---
ver: rpa2
title: 'BudgetFusion: Perceptually-Guided Adaptive Diffusion Models'
arxiv_id: '2412.05780'
source_url: https://arxiv.org/abs/2412.05780
tags:
- diffusion
- image
- quality
- denoising
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BudgetFusion presents a perceptually-guided approach for adaptive
  step selection in diffusion models, addressing the computational inefficiency of
  generating high-quality images at fixed step counts. The method predicts optimal
  denoising steps by analyzing perceptual quality metrics (pixel-level L-SNR, mid-level
  D-SIM, and semantic I-CLIP) as a function of diffusion steps using a bidirectional
  LSTM model.
---

# BudgetFusion: Perceptually-Guided Adaptive Diffusion Models

## Quick Facts
- arXiv ID: 2412.05780
- Source URL: https://arxiv.org/abs/2412.05780
- Reference count: 40
- Primary result: 63.9% reduction in diffusion step count while maintaining perceptual quality

## Executive Summary
BudgetFusion introduces a perceptually-guided approach for adaptive step selection in diffusion models, addressing the computational inefficiency of generating high-quality images at fixed step counts. The method predicts optimal denoising steps by analyzing perceptual quality metrics (pixel-level L-SNR, mid-level D-SIM, and semantic I-CLIP) as a function of diffusion steps using a bidirectional LSTM model. For each text prompt, the model identifies the plateau point where additional steps yield minimal perceptual gain. Evaluated on Stable Diffusion 2, BudgetFusion achieves a 63.9% reduction in generation time (from 8.00 to 2.89 seconds per image) while maintaining comparable perceptual quality, with a 6.6-8.7% quality gain per step over uniform step allocation.

## Method Summary
BudgetFusion presents a perceptually-guided approach for adaptive step selection in diffusion models, addressing the computational inefficiency of generating high-quality images at fixed step counts. The method predicts optimal denoising steps by analyzing perceptual quality metrics (pixel-level L-SNR, mid-level D-SIM, and semantic I-CLIP) as a function of diffusion steps using a bidirectional LSTM model. For each text prompt, the model identifies the plateau point where additional steps yield minimal perceptual gain. Evaluated on Stable Diffusion 2, BudgetFusion achieves a 63.9% reduction in generation time (from 8.00 to 2.89 seconds per image) while maintaining comparable perceptual quality, with a 6.6-8.7% quality gain per step over uniform step allocation. User studies confirm that images generated with BudgetFusion are subjectively preferred over both fixed-step and uniform approaches, demonstrating significant efficiency improvements without perceptible quality loss.

## Key Results
- 63.9% reduction in generation time (from 8.00 to 2.89 seconds per image)
- 6.6-8.7% quality gain per step compared to uniform step allocation
- User studies confirm subjective preference for BudgetFusion-generated images

## Why This Works (Mechanism)
BudgetFusion works by leveraging perceptual quality metrics that capture different aspects of image fidelity at various stages of the diffusion process. The bidirectional LSTM model learns to predict the optimal stopping point by analyzing how pixel-level (L-SNR), mid-level (D-SIM), and semantic (I-CLIP) metrics evolve across diffusion steps. This approach identifies the "plateau" region where additional denoising steps provide diminishing perceptual returns, allowing the system to stop early without sacrificing quality. The adaptive nature means different prompts can use different step counts based on their specific perceptual evolution patterns.

## Foundational Learning
- **Perceptual quality metrics (L-SNR, D-SIM, I-CLIP)**: Needed to quantify image quality at different abstraction levels; quick check: verify metrics capture human perception of image quality.
- **Bidirectional LSTM architecture**: Required for learning temporal patterns in perceptual metric evolution; quick check: confirm model can predict step plateaus from metric sequences.
- **Diffusion model denoising process**: Essential for understanding where perceptual gains plateau; quick check: visualize quality improvement curves across steps.
- **Text-to-image generation pipeline**: Needed to integrate adaptive step selection; quick check: ensure compatibility with existing diffusion frameworks.
- **Computational efficiency metrics**: Important for quantifying time savings; quick check: verify timing measurements are consistent and reproducible.
- **User study methodology**: Critical for validating perceptual quality claims; quick check: confirm study design minimizes bias and captures meaningful preferences.

## Architecture Onboarding

**Component Map**: Text Prompt -> Perceptual Metrics (L-SNR, D-SIM, I-CLIP) -> Bidirectional LSTM -> Optimal Step Count -> Diffusion Model

**Critical Path**: The core inference pipeline runs text prompt → perceptual metric computation → LSTM prediction → diffusion step allocation. The LSTM must process metric sequences in real-time to avoid bottlenecking the generation process.

**Design Tradeoffs**: 
- Computational overhead of metric computation vs. step savings
- LSTM model complexity vs. prediction accuracy
- Number of perceptual metrics vs. model generalization
- Fixed vs. adaptive threshold for plateau detection

**Failure Signatures**: 
- Under-generation: perceptual metrics plateau too early, producing low-quality images
- Over-generation: LSTM fails to detect plateau, wasting computation
- Inconsistent predictions: metric evolution patterns vary unpredictably across similar prompts
- Timing overhead: metric computation and LSTM inference negate computational savings

**First Experiments**:
1. Benchmark perceptual metric computation time across different image resolutions
2. Train LSTM on synthetic metric sequences to verify plateau detection capability
3. Profile end-to-end generation pipeline to identify computational bottlenecks

## Open Questions the Paper Calls Out
None provided in the input.

## Limitations
- Narrow evaluation scope: tested only on Stable Diffusion 2 without cross-architecture validation
- Perceptual metric limitations: specific choices may not capture all aspects of image quality
- Training data dependency: LSTM performance heavily depends on quality and diversity of training data
- Subjective quality claims: "no perceptible quality loss" supported by user studies but lacks detailed methodology

## Confidence
- Computational efficiency gains: High
- Perceptual quality comparisons: Medium
- Adaptive algorithm's general applicability: Low

## Next Checks
1. Test BudgetFusion across multiple diffusion model architectures (e.g., SD1.x, SDXL, Kandinsky) to assess cross-model generalization
2. Conduct ablation studies to isolate the contribution of each perceptual metric to step prediction accuracy
3. Implement a long-term memory component that learns from past prompt successes to improve predictions for recurring text prompts