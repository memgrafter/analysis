---
ver: rpa2
title: Manual Verbalizer Enrichment for Few-Shot Text Classification
arxiv_id: '2410.06173'
source_url: https://arxiv.org/abs/2410.06173
tags:
- words
- language
- label
- verbalizer
- maven
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MaVEN, a method to enrich manual verbalizers
  for few-shot text classification by incorporating semantically related words from
  the embedding space of the language model. The core idea is to automatically expand
  the label words by including their nearest neighbors in the embedding space, initialized
  with similarity weights and fine-tuned during prompt-based fine-tuning.
---

# Manual Verbalizer Enrichment for Few-Shot Text Classification

## Quick Facts
- arXiv ID: 2410.06173
- Source URL: https://arxiv.org/abs/2410.06173
- Authors: Quang Anh Nguyen; Nadi Tomeh; Mustapha Lebbah; Thierry Charnois; Hanene Azzag; Santiago Cordoba Muñoz
- Reference count: 24
- The paper proposes MaVEN, a method to enrich manual verbalizers for few-shot text classification by incorporating semantically related words from the embedding space of the language model, achieving state-of-the-art results especially in extremely low-data regimes (N=32).

## Executive Summary
This paper addresses the challenge of few-shot text classification by proposing MaVEN (Manual veRbalizer ENrichment), a method that automatically expands manual verbalizers using semantically related words from the embedding space of language models. MaVEN initializes verbalizers with core words from class names, then enriches them by finding nearest neighbors in the embedding space and optimizing similarity weights during fine-tuning. The approach achieves significant improvements over manual, soft, and automatic verbalizers, particularly in extremely low-data settings (N=32), with accuracy gains of 2.3-10.0 percentage points.

## Method Summary
MaVEN enriches manual verbalizers by expanding label words with their nearest neighbors in the language model's embedding space. For each core label word from class names, the method finds semantically related words using cosine similarity and includes them in the verbalizer with similarity-based weights. These weights are fine-tuned during prompt-based fine-tuning along with the language model parameters. The approach uses an ensemble of multiple textual templates to reduce sensitivity to prompt selection and improve stability. MaVEN operates on RoBERTa-large and CamemBERT-large models and is evaluated across five datasets in various few-shot settings (zero-shot, 32, 64, 128 examples).

## Key Results
- MaVEN achieves state-of-the-art results compared to manual, soft, and automatic verbalizers across five datasets
- In extremely low-data regimes (N=32), MaVEN improves accuracy by 2.3-10.0 percentage points over manual verbalizers
- Performance gains diminish as training data increases (N≥128), with the model learning to focus on core words
- The ensemble approach using multiple templates improves stability and reduces prompt sensitivity
- MaVEN is particularly effective when class names are meaningful representations of their concepts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MaVEN enriches manual verbalizers by incorporating semantically related words from the embedding space, which captures more probability mass across label words than manual verbalizers alone.
- Mechanism: For each core label word w0 ∈ v(y), MaVEN finds its nearest neighbors in the embedding space and includes them with similarity-based weights, optimized during fine-tuning.
- Core assumption: The semantic relationships encoded in the language model's embedding space reflect true relationships between words and class concepts.
- Evidence anchors: Abstract statement about automatic expansion via nearest neighbors; section 3.2 definition of neighborhood expansion using embedding similarity.

### Mechanism 2
- Claim: Fine-tuning with MaVEN's expanded verbalizers is particularly effective in extremely low-data regimes because the model can leverage semantic richness of multiple related words rather than sparse training signals.
- Mechanism: With limited training data, the model benefits from multiple semantically related words per class, providing diverse context for learning class boundaries.
- Core assumption: In few-shot settings, semantic information from related words provides valuable signal not learnable from limited data alone.
- Evidence anchors: Abstract showing 2.3-10.0 percentage point improvements for N=32; section 4.3 showing clear superiority in low-data settings.

### Mechanism 3
- Claim: The ensemble approach using multiple templates reduces sensitivity to prompt selection and improves overall stability and performance.
- Mechanism: Training separate models on different textual templates and aggregating predictions through voting/averaging mitigates variability from different template constructions.
- Core assumption: Different templates elicit different aspects of knowledge from the language model, and combining perspectives provides more robust classification.
- Evidence anchors: Section 4.5 showing ensemble models yield more accurate predictions and enhance stability; abstract noting template-independent comparison.

## Foundational Learning

- **Masked Language Modeling (MLM)**: The prompting framework relies on predicting masked tokens, which is the core pre-training objective of models like BERT and RoBERTa. Quick check: What is the relationship between the probability of a class and the masked token predictions in the prompting framework?

- **Prompt-based fine-tuning vs traditional fine-tuning**: Understanding why prompt-based methods are more effective in few-shot scenarios requires grasping fundamental differences in how knowledge is accessed and adapted. Quick check: How does prompt-based fine-tuning reduce the gap between pre-training and fine-tuning compared to adding task-specific heads?

- **Embedding space similarity and semantic relationships**: MaVEN's core mechanism depends on finding semantically related words in the embedding space. Quick check: Why might cosine similarity in embedding space be a reasonable measure for finding semantically related words for class expansion?

## Architecture Onboarding

- **Component map**: Class names → Core word extraction → Embedding similarity search → Verbalizer expansion → Template wrapping → Fine-tuning with similarity weight optimization → Ensemble aggregation.

- **Critical path**: The method starts with extracting core words from class names, then uses embedding similarity to find semantically related neighbors, constructs expanded verbalizers with similarity weights, wraps them in textual templates, fine-tunes the language model while optimizing weights, and finally aggregates predictions through ensemble methods.

- **Design tradeoffs**: MaVEN trades increased vocabulary size and parameter count (for similarity weights) against improved semantic coverage and few-shot performance. The choice of k (neighborhood size) represents a key hyperparameter tradeoff between semantic coverage and noise introduction.

- **Failure signatures**: Poor performance occurs when class names are ambiguous or non-descriptive, when the embedding space doesn't capture relevant semantic relationships, or when neighborhood size k is poorly chosen (too small misses connections, too large introduces noise).

- **First 3 experiments**:
  1. Zero-shot evaluation with different neighborhood sizes k to find optimal balance between semantic coverage and noise.
  2. Comparison of different embedding spaces (LM embeddings vs word2vec vs GloVe) to validate importance of using same embedding space as fine-tuned model.
  3. Ablation study comparing individual templates vs ensemble methods to quantify benefit of aggregation strategy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MaVEN scale with different embedding spaces (LM, word2vec, GloVe) across languages beyond English and French?
- Basis in paper: [explicit] The paper compares MaVEN performance using different embedding spaces, showing LM embeddings generally perform best, especially for zero-shot learning, but experiments are limited to English and French datasets.
- Why unresolved: The effectiveness of different embedding spaces across diverse languages, particularly low-resource languages, remains unexplored.
- What evidence would resolve it: Testing MaVEN with different embedding spaces on multilingual datasets, especially low-resource languages, would clarify how embedding choice impacts performance across linguistic contexts.

### Open Question 2
- Question: What is the optimal neighborhood size (k) for MaVEN across different datasets and template types, and how does this vary with dataset characteristics?
- Basis in paper: [explicit] The paper examines impact of varying k, finding values between 10 and 15 work well generally, but acknowledges performance varies with templates and dataset characteristics.
- Why unresolved: The study uses fixed k=15 in most experiments and only briefly explores variations, leaving the relationship between optimal k, dataset complexity, and template effectiveness uncharacterized.
- What evidence would resolve it: A systematic study varying k across different dataset types and template strategies would identify optimal k ranges for different scenarios.

### Open Question 3
- Question: How does MaVEN's performance compare to other few-shot learning methods that use additional unlabeled data or knowledge bases?
- Basis in paper: [explicit] The paper notes some methods like PETAL use semi-supervised learning with unlabeled data, and KPT uses external knowledge bases like WordNet, while MaVEN operates without these resources, but provides no direct comparisons.
- Why unresolved: The paper positions MaVEN as resource-efficient alternative but doesn't benchmark it against methods leveraging additional data or knowledge sources.
- What evidence would resolve it: Head-to-head comparisons of MaVEN against semi-supervised approaches and knowledge-enhanced methods on the same datasets would clarify relative strengths and weaknesses.

### Open Question 4
- Question: How sensitive is MaVEN to the quality and representativeness of initial manual verbalizers, and can it effectively recover from poor initial label word choices?
- Basis in paper: [explicit] The paper discusses MaVEN relies on initial manual label words and conducts experiments with automatically generated verbalizers (PETAL) as initialization, showing MaVEN can improve upon automatic verbalizers.
- Why unresolved: While the paper shows MaVEN improves automatic verbalizers, it doesn't systematically test how it performs when starting from poor manual verbalizers or explore limits of recovery capabilities.
- What evidence would resolve it: Experiments deliberately constructing suboptimal initial verbalizers and measuring MaVEN's ability to recover performance would establish its robustness to initialization quality.

### Open Question 5
- Question: Can MaVEN be effectively adapted for generative language models (like GPT-3, LLaMA) in decode mode, and how would this compare to its performance with masked LMs?
- Basis in paper: [explicit] The paper notes MaVEN is designed for masked LMs used in encoder mode, but recent powerful PLMs are autoregressive models, suggesting potential for adaptation to generative models.
- Why unresolved: The paper focuses exclusively on masked LMs and doesn't explore how the verbalizer enrichment approach might work with generative models or what adaptations would be necessary.
- What evidence would resolve it: Implementing MaVEN for generative models by adapting verbalizer construction for decode mode, then benchmarking against both original MaVEN and standard approaches, would determine cross-model applicability.

## Limitations

- The approach is constrained by the quality of initial class names and the semantic relationships captured in the LM's embedding space, potentially failing when class names are ambiguous or non-descriptive.
- The method may not generalize well to domains with substantially different vocabulary or semantic structures than the evaluated datasets.
- The optimal neighborhood size k varies with dataset characteristics and template types, requiring careful tuning for different applications.

## Confidence

- **High Confidence**: The empirical results demonstrating MaVEN's superiority over manual, soft, and automatic verbalizers in few-shot settings (2.3-10.0 percentage point improvements); the ablation studies showing importance of semantic relationships in embedding space; the observation that benefits diminish as training data increases.

- **Medium Confidence**: The mechanism explanation for why embedding space expansion works (semantic richness capturing probability mass); the ensemble approach's effectiveness in reducing template sensitivity; the claim that the method is particularly effective when class names are meaningful representations.

- **Low Confidence**: The generalizability of results to domains with very different vocabulary or semantic structures; the optimal neighborhood size k across different tasks and domains; the long-term stability of similarity weights learned during fine-tuning.

## Next Checks

1. **Cross-domain robustness test**: Apply MaVEN to datasets from domains with substantially different vocabulary (e.g., biomedical or legal text) to assess whether embedding space expansion remains effective when semantic relationships differ from the original training corpus.

2. **Class name quality ablation**: Systematically evaluate MaVEN's performance with progressively degraded class names (e.g., replacing descriptive names with random or nonsensical terms) to quantify the dependency on initial label word quality and identify failure thresholds.

3. **Embedding space comparison**: Compare MaVEN's performance when using different embedding sources (frozen LM embeddings vs. static embeddings like GloVe vs. task-specific embeddings) to isolate whether the effectiveness comes from the specific LM's embedding space or from the general principle of semantic expansion.