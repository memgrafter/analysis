---
ver: rpa2
title: 'Enhancing Large Language Models with Domain-Specific Knowledge: The Case in
  Topological Materials'
arxiv_id: '2409.13732'
source_url: https://arxiv.org/abs/2409.13732
tags:
- topological
- materials
- data
- information
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TopoChat, a specialized dialogue system for
  topological materials that integrates domain-specific knowledge graphs with large
  language models. The authors address the limitation of general LLMs in domain-specific
  applications by constructing MaterialsKG, a knowledge graph containing 29,577 materials
  with six node categories and five relationship types.
---

# Enhancing Large Language Models with Domain-Specific Knowledge: The Case in Topological Materials

## Quick Facts
- arXiv ID: 2409.13732
- Source URL: https://arxiv.org/abs/2409.13732
- Reference count: 34
- Primary result: TopoChat achieves over 78% accuracy across three task types, outperforming naive LLMs in topological materials queries

## Executive Summary
This paper presents TopoChat, a specialized dialogue system for topological materials that integrates domain-specific knowledge graphs with large language models. The authors address the limitation of general LLMs in domain-specific applications by constructing MaterialsKG, a knowledge graph containing 29,577 materials with six node categories and five relationship types. They develop a two-phase prompt learning algorithm that combines structured knowledge graph queries with literature retrieval-augmented generation. Evaluation shows TopoChat achieves over 78% accuracy across three task types (entity, relationship, and property selection), with GPT-3.5-turbo performing best as both the Cypher query generator and response generator. The system enables efficient querying of material properties, recommendations, and complex relational reasoning, demonstrating superior performance compared to naive LLMs in answering material-related questions.

## Method Summary
The method involves constructing MaterialsKG, a knowledge graph containing 29,577 materials with six node categories and five relationship types, built through multi-source data management flow integrating data from CMPDC databases. A literature vector database with 1,000 QA pairs extracted from arXiv papers is created using FAISS for efficient retrieval. The two-phase prompt learning algorithm works in stages: Stage 1 uses an LLM to generate Cypher queries from user questions, Stage 2 retrieves data from MaterialsKG and relevant literature from the vector database, and Stages 3-4 combine structured and unstructured information for retrieval-augmented generation using GPT-3.5-turbo. The "NOTICE" principle (Task, Instruction, Example, Context, Note) guides prompt design for effective domain-specific knowledge retrieval.

## Key Results
- TopoChat achieves over 78% accuracy across three task types (entity, relationship, and property selection)
- GPT-3.5-turbo performs best as both Cypher query generator and response generator
- System demonstrates superior performance compared to naive LLMs in structural and property querying, material recommendation, and complex relational reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-phase prompt learning algorithm effectively combines structured knowledge graph queries with literature retrieval-augmented generation to overcome LLM hallucinations in domain-specific applications.
- Mechanism: The system first converts user questions into Cypher queries (Stage 1) to retrieve structured data from MaterialsKG, then retrieves relevant literature from a vector database (Stage 2-1), and finally combines both sources for text generation (Stage 3-4). This multi-source approach grounds LLM responses in verifiable domain knowledge.
- Core assumption: LLMs can be effectively guided by structured prompts and context to generate accurate domain-specific responses when provided with relevant knowledge graph and literature information.
- Evidence anchors:
  - [abstract]: "Using large language models and prompt learning, we develop a specialized dialogue system for topological materials called TopoChat. Compared to naive LLMs, TopoChat exhibits superior performance in structural and property querying, material recommendation, and complex relational reasoning."
  - [section]: "The aim of this paper is to effectively address the mentioned challenges by integrating multi-source knowledge into large language models and proposing an expert system named TopoChat."
  - [corpus]: Weak evidence - no direct citations in the corpus papers about this specific two-phase prompt learning approach.
- Break condition: The mechanism breaks if the Cypher generation fails to accurately capture user intent, if the knowledge graph lacks relevant information for the query, or if the literature retrieval fails to find pertinent context.

### Mechanism 2
- Claim: The "NOTICE" principle in prompt design (Task, Instruction, Example, Context, Note) provides a systematic framework for creating effective prompts that guide LLMs in domain-specific knowledge retrieval.
- Mechanism: By explicitly defining each component of the prompt, the system ensures LLMs understand their role (Task), know what to do (Instruction), have examples to learn from (Example), receive relevant context, and follow specific guidelines (Note). This structured approach reduces ambiguity and improves response quality.
- Core assumption: LLMs respond predictably to well-structured prompts with clear instructions and examples, particularly when domain-specific context is provided.
- Evidence anchors:
  - [section]: "To accomplish specific tasks in material science, we propose the 'NOTICE' principle, which guides us in designing the LLM prompt according to actual needs."
  - [section]: "In Stage 1, we design the prompt that needs to be passed to the LLM as follows: • Task:Generate Cypher statement to query a graph database. • Instruction:Describe the properties for a given or recommend materials according to human question."
  - [corpus]: Weak evidence - corpus papers discuss prompt engineering but don't specifically reference the "NOTICE" principle.
- Break condition: The mechanism fails if the examples provided don't adequately cover the query space, if the context is too complex for the LLM to process, or if the notes create conflicting instructions.

### Mechanism 3
- Claim: The integration of multi-source data management flow creates a unified knowledge representation that enables comprehensive domain-specific querying.
- Mechanism: The system integrates data from multiple databases through a structured flow (acquisition, cleaning, storage, maintenance, reuse), creating MaterialsKG with six node categories and five relationship types. This unified representation allows for complex queries that span different data sources.
- Core assumption: Heterogeneous data sources can be effectively normalized and integrated into a coherent knowledge graph structure that preserves relationships and enables meaningful queries.
- Evidence anchors:
  - [section]: "Multi-source data fusion and management flow is aimed at bridging the data barriers between sites to build a unified high-quality database that enables seamless data connectivity between different sites."
  - [section]: "Based on the condensed matter data center, we establish a material knowledge graph (MaterialsKG) and integrate it with literature."
  - [corpus]: Moderate evidence - several corpus papers discuss knowledge graph construction for materials science, though not specifically this multi-source integration approach.
- Break condition: The mechanism breaks if data normalization introduces errors, if relationships between different data sources are incorrectly mapped, or if the knowledge graph becomes too complex for efficient querying.

## Foundational Learning

- Concept: Knowledge Graph Construction
  - Why needed here: MaterialsKG provides the structured domain knowledge that grounds LLM responses in verifiable facts rather than hallucinations.
  - Quick check question: Can you explain the difference between nodes and relationships in a knowledge graph, and why both are essential for representing material properties?

- Concept: Prompt Engineering
  - Why needed here: Effective prompts are crucial for guiding LLMs to perform specific tasks like generating Cypher queries or synthesizing information from multiple sources.
  - Quick check question: What are the key components of the "NOTICE" principle, and how does each contribute to prompt effectiveness?

- Concept: Vector Database Retrieval
  - Why needed here: FAISS vector database enables efficient retrieval of relevant literature information to augment knowledge graph data in LLM responses.
  - Quick check question: How does FAISS use L2 distance to determine similarity between question embeddings, and why is this important for literature retrieval?

## Architecture Onboarding

- Component map:
  - User Interface (Vue3/Flask) → Query Processor → Text2Cypher LLM → MaterialsKG Database → Literature Vector Database (FAISS) → QA LLM → Response Generator
  - Key components: MaterialsKG (Neo4j), FAISS vector database, GPT-3.5-turbo for both Cypher generation and QA generation, Langchain framework

- Critical path:
  1. User question → Text2Cypher prompt generation → Cypher query execution → Database results
  2. User question → Literature vector search → Relevant QA pairs
  3. Combine both results → Retrieval-augmented generation → Final response

- Design tradeoffs:
  - Using GPT-3.5-turbo for both Cypher generation and QA provides consistency but may limit specialized capabilities
  - Literature vector database provides quick retrieval but may miss nuanced context compared to full-text search
  - Knowledge graph structure enables complex queries but requires significant upfront data integration effort

- Failure signatures:
  - Cypher generation errors: LLM produces non-executable or incorrect Cypher statements
  - Knowledge graph gaps: Missing data in MaterialsKG leads to incomplete responses
  - Vector database limitations: FAISS similarity search returns irrelevant literature
  - LLM context window overflow: Combined context exceeds LLM processing capacity

- First 3 experiments:
  1. Test Cypher generation accuracy: Provide 10 diverse material-related questions and measure how often the generated Cypher queries return correct results
  2. Evaluate literature retrieval relevance: For 5 test questions, measure the precision@3 of FAISS vector database retrieval
  3. Assess end-to-end performance: Compare TopoChat responses to naive LLM responses on 20 material-related questions using human evaluation of accuracy and completeness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TopoChat compare to other domain-specific LLM applications in materials science or related fields?
- Basis in paper: [inferred] The paper focuses on comparing TopoChat's performance to naive LLMs like ChatGPT but does not compare it to other specialized LLM systems in materials science or related domains.
- Why unresolved: The paper does not provide a comparative analysis with other domain-specific LLM applications, which would help establish TopoChat's relative effectiveness.
- What evidence would resolve it: A direct comparison of TopoChat's accuracy, response quality, and efficiency against other specialized LLM systems for materials science or similar domains.

### Open Question 2
- Question: What is the impact of expanding the MaterialsKG to include more diverse material properties and relationships on the accuracy and capabilities of TopoChat?
- Basis in paper: [explicit] The paper mentions that future work will focus on incorporating additional sources and fusing more data to enrich the knowledge base.
- Why unresolved: The current study uses a limited set of material properties and relationships, and it is unclear how expanding this scope would affect the system's performance.
- What evidence would resolve it: An experimental study showing the changes in TopoChat's accuracy and capabilities when the MaterialsKG is expanded to include a broader range of material properties and relationships.

### Open Question 3
- Question: How does the integration of multimodal data (e.g., images, spectra) into the TopoChat system affect its performance in material property prediction and recommendation?
- Basis in paper: [inferred] The paper focuses on text-based data and does not explore the potential benefits of integrating multimodal data into the system.
- Why unresolved: The study does not investigate the impact of incorporating multimodal data on the system's performance, which could provide valuable insights for improving material property prediction and recommendation.
- What evidence would resolve it: An experimental study comparing the performance of TopoChat with and without the integration of multimodal data, focusing on material property prediction and recommendation tasks.

## Limitations

- Data Integration Complexity: The MaterialsKG construction relies on multi-source data fusion from heterogeneous databases, presenting significant normalization challenges without provided error rates for data integration.
- Evaluation Scope: The evaluation focuses on accuracy metrics across three task types but lacks comprehensive assessment of response quality, including hallucination detection and handling of ambiguous queries.
- System Generalization: While effective for topological materials, the architecture's generalizability to other scientific domains remains unproven due to domain-specific terminology and relationships.

## Confidence

**High Confidence (95%+)**: The core methodology of combining structured knowledge graph queries with literature retrieval-augmented generation is technically sound and represents a valid approach to addressing LLM hallucinations in domain-specific applications. The two-phase prompt learning algorithm is well-specified with clear implementation steps.

**Medium Confidence (70-95%)**: The empirical results showing superior performance compared to naive LLMs are credible but limited by the evaluation methodology. The accuracy metrics are promising but based on a constrained set of test cases without comprehensive error analysis or comparison to alternative approaches.

**Low Confidence (0-70%)**: The system's robustness under real-world usage conditions, including handling edge cases, dealing with incomplete knowledge graph data, and maintaining consistent performance across diverse query types, cannot be fully assessed from the current evaluation framework.

## Next Checks

1. **Error Analysis and Hallucination Detection**: Conduct a comprehensive error analysis on 100+ diverse material-related queries to identify specific failure modes, including hallucination patterns, Cypher generation errors, and knowledge graph gaps. Measure false positive rates and analyze the types of questions where TopoChat performs worse than naive LLMs.

2. **Cross-Domain Transferability**: Adapt the TopoChat architecture to a different scientific domain (e.g., organic chemistry or computational biology) by constructing a new domain-specific knowledge graph and evaluating whether the same two-phase prompt learning approach achieves comparable performance improvements.

3. **Real-World Deployment Testing**: Deploy TopoChat in an actual research environment for 30 days, collecting user feedback on response quality, system reliability, and handling of complex queries that require domain expertise. Compare user satisfaction metrics with naive LLM interactions for material-related research tasks.