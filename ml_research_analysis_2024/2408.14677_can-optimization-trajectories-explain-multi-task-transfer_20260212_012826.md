---
ver: rpa2
title: Can Optimization Trajectories Explain Multi-Task Transfer?
arxiv_id: '2408.14677'
source_url: https://arxiv.org/abs/2408.14677
tags:
- training
- task
- loss
- generalization
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether optimization trajectories can explain
  multi-task transfer in deep learning. The authors compare single-task and multi-task
  generalization by total task training loss at each epoch across 5 multi-task settings.
---

# Can Optimization Trajectories Explain Multi-Task Transfer?

## Quick Facts
- arXiv ID: 2408.14677
- Source URL: https://arxiv.org/abs/2408.14677
- Authors: David Mueller; Mark Dredze; Nicholas Andrews
- Reference count: 40
- Primary result: Multi-task learning shows early generalization gaps observable before convergence, but trajectory properties don't explain transfer effects

## Executive Summary
This paper investigates whether optimization trajectories can explain multi-task transfer in deep learning. The authors compare single-task and multi-task generalization by total task training loss at each epoch across 5 multi-task settings. They find that transfer (both positive & negative) is observable as a generalization gap between single-task and multi-task trajectories early into training, well before either trajectory converges. The study then examines whether factors of the optimization trajectory previously connected to generalization in deep learning (gradient coherence, early-stage Fisher information, and loss surface sharpness) are correlated with the impact of MTL on generalization.

## Method Summary
The authors conduct empirical studies across 5 multi-task settings, comparing single-task and multi-task optimization trajectories. They measure total task training loss at each epoch to identify generalization gaps between the two approaches. The study examines three trajectory properties previously linked to generalization: gradient coherence, early-stage Fisher information, and loss surface sharpness. These properties are analyzed to determine their correlation with transfer effects observed in multi-task learning scenarios.

## Key Results
- Transfer effects (positive and negative) appear as generalization gaps between single-task and multi-task trajectories early in training, before convergence
- Multi-task learning impacts trajectory properties like gradient coherence and loss surface sharpness, but these effects don't explain performance changes
- Gradient conflict between tasks correlates with optimization difficulty but is not predictive of generalization outcomes

## Why This Works (Mechanism)
The paper suggests that optimization trajectories diverge between single-task and multi-task learning early in training, creating observable generalization gaps. However, the mechanism by which these early divergences might cause transfer effects remains unclear. The authors find that while MTL affects trajectory properties, these properties cannot explain the observed transfer effects, suggesting the relationship between optimization dynamics and generalization may be more complex than previously thought.

## Foundational Learning
- Multi-task learning (MTL): Training on multiple tasks simultaneously - needed to understand the baseline comparison; quick check: verify that task losses are properly normalized
- Optimization trajectories: The path of model parameters during training - needed to track how learning dynamics differ between single-task and multi-task settings; quick check: confirm trajectory sampling frequency is sufficient
- Generalization gap: Difference in performance between training and test sets - needed to measure transfer effectiveness; quick check: ensure proper train/test split validation
- Gradient coherence: Consistency of gradient directions across training steps - needed to assess optimization stability; quick check: verify gradient computation correctness
- Fisher information: Measure of parameter sensitivity - needed to understand early-stage learning dynamics; quick check: confirm Fisher matrix computation implementation
- Loss surface sharpness: Curvature properties of the loss landscape - needed to evaluate optimization difficulty; quick check: validate sharpness metrics across multiple random seeds

## Architecture Onboarding
Component map: Data loader -> Model architecture -> Optimizer -> Trajectory monitor -> Evaluation metrics
Critical path: Data preprocessing → Model training → Trajectory property computation → Performance evaluation → Correlation analysis
Design tradeoffs: The study uses convolutional networks rather than more modern architectures, potentially limiting generalizability to transformers and other architectures where optimization dynamics differ significantly
Failure signatures: If trajectory properties show no correlation with transfer effects across multiple datasets and architectures, the optimization trajectory framework may be fundamentally flawed
First experiments: 1) Replicate results on additional datasets to test robustness, 2) Apply same analysis to transformer architectures, 3) Test whether artificially modifying trajectories can induce transfer effects

## Open Questions the Paper Calls Out
None

## Limitations
- Major uncertainties remain regarding whether observed early generalization gaps truly explain transfer or merely correlate with it
- The analysis shows trajectory properties are affected by MTL but cannot explain performance changes, suggesting the framework may be incomplete
- The study focuses on convolutional networks, potentially limiting generalizability to modern architectures like transformers

## Confidence
- Whether early generalization gaps explain transfer: Medium (empirical observations are clear but causal relationship unproven)
- Whether trajectory properties explain transfer: Low to Medium (framework shows interesting patterns but lacks theoretical justification)
- Overall framework validity: Low to Medium (empirical observations interesting but theoretical grounding underdeveloped)

## Next Checks
1. Conduct ablation studies that artificially modify optimization trajectories (e.g., through learning rate scheduling or optimization algorithm changes) to test whether trajectory modifications alone can induce transfer effects
2. Extend the analysis to architectures beyond convolutional networks, particularly transformers and other modern architectures where optimization dynamics differ significantly
3. Develop and test alternative metrics for measuring gradient conflict and trajectory coherence that might better capture the relationship to generalization outcomes