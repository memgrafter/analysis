---
ver: rpa2
title: Have You Poisoned My Data? Defending Neural Networks against Data Poisoning
arxiv_id: '2403.13523'
source_url: https://arxiv.org/abs/2403.13523
tags:
- poisons
- attacks
- poisoning
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel defense against clean-label poisoning
  attacks in transfer learning settings. The approach leverages characteristic vectors
  based on Batch Normalization (BN) statistics to distinguish poisoned datapoints
  from clean ones.
---

# Have You Poisoned My Data? Defending Neural Networks against Data Poisoning

## Quick Facts
- arXiv ID: 2403.13523
- Source URL: https://arxiv.org/abs/2403.13523
- Reference count: 40
- This paper proposes a novel defense against clean-label poisoning attacks in transfer learning settings using characteristic vectors based on Batch Normalization statistics.

## Executive Summary
This paper introduces a novel defense against clean-label poisoning attacks in transfer learning scenarios. The approach leverages characteristic vectors computed from Batch Normalization statistics to distinguish poisoned data points from clean ones. By measuring convergence toward target class distributions rather than deviation from base classes, the method achieves robust detection across multiple attack types, datasets, and architectures. Experimental results demonstrate significant improvements over existing defenses, reducing attack success rates to 3.67% on average while maintaining near-clean test accuracy.

## Method Summary
The defense mechanism works by computing characteristic vectors for each data point based on channel-wise mean and variance statistics from Batch Normalization layers across the feature extractor. These vectors capture the distributional properties of features at different network depths. For each class, centroids are computed from clean training data, and each sample's distance to these centroids determines its "real label." Samples whose assigned label differs from their computed real label are filtered out as potential poisons. This approach exploits the fact that effective poisons shift their feature distributions toward the target class across multiple layers, making them distinguishable from clean samples of the base class.

## Key Results
- The proposed method reduces attack success rates to an average of 3.67% across multiple attack types
- Maintains near-clean test accuracy (90.21%) compared to significantly degraded performance of undefended models (52.80%)
- Outperforms existing state-of-the-art defenses, particularly against stronger attacks and with large poison budgets

## Why This Works (Mechanism)

### Mechanism 1
Characteristic vectors based on Batch Normalization statistics can effectively distinguish poisoned samples from clean ones by capturing shifts in feature distribution across network layers. For each data point, compute channel-wise mean and variance at every BN layer as it passes through the feature extractor. Compare this "characteristic vector" to centroid vectors representing each class's distribution. Poisons converge toward target class distribution in this space while diverging from base class. Core assumption: The perturbation injected by poisoning attacks is sufficient to meaningfully shift the distribution of poisoned samples from clean samples across multiple layers of the network, and this shift can be captured by BN statistics.

### Mechanism 2
Real poisons (effective ones) reside in the class manifold of the target class in the characteristic vector space, while failed poisons overlap with clean points of the same base class. By computing distances between each sample's characteristic vector and class centroids, real poisons are identified as those whose "true label" (minimum distance) differs from their assigned label. This reveals that effective poisons have shifted toward target class characteristics. Core assumption: The optimization process used to generate poisons inherently pushes the poisoned samples' feature distributions toward the target class across all layers, not just the final layer.

### Mechanism 3
The proposed defense generalizes across different poison generation algorithms because it measures convergence toward target class rather than deviation from base class. Unlike defenses that detect feature space deviations from the majority distribution (which can be influenced by perturbation budget or specific attack techniques), this approach measures whether poisoned samples have characteristic vectors closer to the target class centroid than their assigned base class centroid. Core assumption: Any successful poison must cause the model to misclassify target samples as the base class, which requires the poisoned base samples to shift their feature distribution toward the target class in a way that's detectable by BN statistics.

## Foundational Learning

- Concept: Transfer learning with frozen feature extractors
  - Why needed here: The defense relies on a pre-trained feature extractor whose BN statistics encode meaningful feature distributions. The approach assumes the extractor is robust and that only the downstream classifier is trained.
  - Quick check question: Why does the defense require the feature extractor to be frozen during fine-tuning?

- Concept: Batch Normalization statistics as distribution descriptors
  - Why needed here: BN layers' mean and variance at each layer serve as a summary of the feature distribution for each class. These statistics are used to build characteristic vectors that capture low- and high-level features.
  - Quick check question: How do BN mean and variance statistics capture the intrinsic properties of data distributions at different network depths?

- Concept: Bilevel optimization in poisoning attacks
  - Why needed here: Understanding the formal optimization problem behind clean-label poisoning helps explain why poisons must shift feature distributions toward the target class, which is what the defense detects.
  - Quick check question: In the bilevel optimization formulation, what are the two nested optimization problems that the attacker must solve?

## Architecture Onboarding

- Component map: Pre-trained feature extractor (frozen) -> BN layers at each depth -> Characteristic vector computation -> Centroid computation for each class -> Distance computation to centroids -> Label mismatch detection -> Filtering mechanism

- Critical path:
  1. Pre-train feature extractor on source dataset
  2. Compute class centroids by passing all clean training data through extractor and aggregating BN statistics
  3. For each sample in training set, compute characteristic vector (BN stats)
  4. Compute distance to each class centroid
  5. Assign "real label" as class with minimum distance
  6. Filter samples where computed real label differs from dataset label

- Design tradeoffs:
  - Using multiple BN layers provides robustness but increases computational cost
  - Choice of distance metric (cosine vs L2) affects sensitivity to distribution shifts
  - Weighting different BN layers differently could improve detection but requires hyperparameter tuning
  - Requires pre-trained feature extractor but enables detection without model training

- Failure signatures:
  - High false positive rate: characteristic vectors of clean samples are being misclassified as belonging to wrong classes
  - High false negative rate: poisons are not being detected (their characteristic vectors remain close to base class centroids)
  - Performance degradation: filtering removes too many clean samples or the remaining data is insufficient for good model performance
  - Inconsistent results across architectures: defense works well on some feature extractors but poorly on others

- First 3 experiments:
  1. Verify characteristic vectors capture class distributions by visualizing t-SNE projections of clean samples' characteristic vectors colored by class
  2. Test detection performance on synthetic data where you know the "ground truth" labels based on characteristic vector distances
  3. Evaluate sensitivity to perturbation budget by generating poisons with varying epsilon values and measuring detection rates

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed characteristic vector approach perform when applied to non-image domains like text or tabular data? Basis in paper: The paper focuses exclusively on image classification tasks with CIFAR10 and CINIC10 datasets, leaving the generalization to other data modalities unexplored. Why unresolved: The paper's experiments are limited to visual data where BN statistics capture meaningful distributional information, but the approach's effectiveness on text or tabular data where BN statistics may be less informative remains unknown. What evidence would resolve it: Experiments applying the characteristic vector defense to poison detection in text classification or tabular datasets, comparing performance against existing defenses.

### Open Question 2
What is the computational overhead of computing characteristic vectors during both training and inference phases? Basis in paper: The paper describes the method but doesn't quantify runtime performance, memory requirements, or impact on training/inference speed. Why unresolved: While the paper demonstrates effectiveness, the practical deployment implications of additional computation for BN statistics at each layer are not addressed. What evidence would resolve it: Benchmark measurements showing time and memory overhead compared to baseline training, both during the defense computation and for the final model.

### Open Question 3
How does the defense perform against adaptive adversaries who can modify their attack strategy to evade characteristic vector detection? Basis in paper: The paper mentions evaluating against multiple attack types but doesn't explore whether attackers could optimize poisons to minimize shifts in BN statistics specifically. Why unresolved: The evaluation assumes static attack algorithms without considering whether adversaries could craft poisons that specifically minimize changes to the characteristic vectors while still achieving misclassification. What evidence would resolve it: Experiments where attackers are given knowledge of the defense mechanism and optimize their poisons to minimize characteristic vector distance changes while maintaining attack success.

## Limitations

- The defense's effectiveness may degrade against adaptive attacks specifically designed to maintain BN statistics close to clean base class distributions
- The approach requires a pre-trained feature extractor, which may not always be available or may introduce bias based on the source dataset
- Computational overhead from computing characteristic vectors at multiple BN layers could be significant for large models or datasets

## Confidence

- High confidence in the core mechanism of using BN statistics to capture distributional shifts, as this is well-established and the experimental results consistently show detection capability across multiple datasets and architectures
- Medium confidence in the generalization claims, as the experiments cover a range of attacks but may not encompass all possible future attack strategies
- Medium confidence in the computational efficiency claims, as while the approach avoids training classifiers, computing characteristic vectors for all training samples adds overhead

## Next Checks

1. Test the defense against adaptive poisoning attacks specifically designed to maintain BN statistics close to the base class distribution while still achieving misclassification
2. Evaluate detection performance when the feature extractor undergoes partial fine-tuning rather than being completely frozen
3. Measure the computational overhead of characteristic vector computation across different batch sizes and network depths to validate scalability claims