---
ver: rpa2
title: 'Shaving Weights with Occam''s Razor: Bayesian Sparsification for Neural Networks
  Using the Marginal Likelihood'
arxiv_id: '2402.15978'
source_url: https://arxiv.org/abs/2402.15978
tags:
- pruning
- sparsity
- spam
- neural
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SpaM, a Bayesian training framework that
  improves the sparsifiability of neural networks by optimizing prior hyperparameters
  using the marginal likelihood. This automatically regularizes parameters to small
  magnitudes, making them easier to prune later.
---

# Shaving Weights with Occam's Razor: Bayesian Sparsification for Neural Networks Using the Marginal Likelihood

## Quick Facts
- arXiv ID: 2402.15978
- Source URL: https://arxiv.org/abs/2402.15978
- Reference count: 40
- Primary result: Bayesian sparsification method (SpaM) that improves post-pruning accuracy at extreme sparsity levels by optimizing prior hyperparameters via marginal likelihood.

## Executive Summary
This paper introduces SpaM, a Bayesian training framework that improves the sparsifiability of neural networks by optimizing prior hyperparameters using the marginal likelihood. This automatically regularizes parameters to small magnitudes, making them easier to prune later. The framework uses Laplace approximations for efficient marginal likelihood estimation and allows various prior structures (scalar, layer-wise, unit-wise, parameter-wise). A key contribution is OPD, a pruning criterion that reuses the Laplace posterior precision and often outperforms other methods at high sparsity levels. Experiments across multiple architectures and datasets show SpaM significantly improves accuracy retention after pruning, especially at 95-99% sparsity, with OPD maintaining near-baseline performance where other methods fail. Structured pruning achieves up to 20x computational savings. The method combines strong empirical performance with computational efficiency, making it suitable for resource-constrained deployment.

## Method Summary
SpaM trains networks in a Bayesian framework where prior hyperparameters are learned by maximizing the marginal likelihood, approximated via Laplace method. This induces a natural regularization effect, pushing parameters toward small magnitudes and thus improving their sparsifiability. After training, structured pruning is applied using the Optimal Pruning Direction (OPD) criterion, which leverages the Laplace posterior precision matrix to identify and remove weights that minimally impact the model's performance. The framework supports various prior structures and demonstrates significant improvements in accuracy retention after pruning, especially at extreme sparsity levels.

## Key Results
- SpaM improves post-pruning accuracy significantly at 95-99% sparsity, outperforming standard pruning methods.
- OPD pruning criterion, reusing Laplace posterior precision, achieves near-baseline performance at extreme sparsity where other methods fail.
- Structured pruning with SpaM yields up to 20x computational savings while maintaining model accuracy.

## Why This Works (Mechanism)
By optimizing prior hyperparameters via marginal likelihood, SpaM induces a natural regularization that shrinks parameters toward zero, making them easier to prune without significant performance loss. The Laplace approximation provides an efficient way to estimate the marginal likelihood and compute the posterior precision, which is then reused by the OPD criterion to make informed pruning decisions. This Bayesian approach ensures that the model's uncertainty is properly accounted for during pruning, leading to better generalization after sparsification.

## Foundational Learning
- **Marginal likelihood**: The probability of the data given the model, integrated over all possible parameter values. Needed to objectively compare different models or hyperparameter settings. Quick check: verify that the marginal likelihood decreases as model complexity increases, following Occam's razor.
- **Laplace approximation**: A method to approximate intractable integrals (like the marginal likelihood) by fitting a Gaussian around the posterior mode. Quick check: ensure the posterior is unimodal and approximately Gaussian for the approximation to be valid.
- **Prior hyperparameters**: Parameters controlling the prior distribution over network weights (e.g., precision of a Gaussian prior). Needed to regularize the model and influence its sparsifiability. Quick check: experiment with different prior structures (scalar, layer-wise, etc.) and observe their impact on pruning performance.
- **Structured pruning**: Removing entire groups of weights (e.g., channels, filters) rather than individual weights. Needed to achieve significant computational savings. Quick check: measure the actual speedup and memory reduction after structured pruning.
- **Posterior precision**: The inverse of the posterior covariance matrix, indicating the certainty of parameter estimates. Needed by the OPD criterion to identify weights that can be pruned with minimal impact. Quick check: verify that weights with low posterior precision are indeed less important for the model's performance.
- **Occam's razor**: The principle that simpler explanations are preferable to complex ones. Needed to justify the use of marginal likelihood for model selection and hyperparameter optimization. Quick check: ensure that the chosen model complexity balances fit to the data with simplicity.

## Architecture Onboarding
- **Component map**: Neural network -> Bayesian training with marginal likelihood optimization -> Laplace approximation -> Posterior precision -> Structured pruning (OPD) -> Sparse model
- **Critical path**: Marginal likelihood optimization (SpaM training) -> Laplace approximation -> OPD pruning -> Sparse model evaluation
- **Design tradeoffs**: SpaM trades off longer training time for better post-pruning accuracy and computational efficiency. The choice of prior structure (scalar, layer-wise, etc.) balances flexibility with computational cost.
- **Failure signatures**: Poor performance if the posterior is not well-approximated by a Gaussian (invalidating Laplace approximation), or if the marginal likelihood optimization does not converge properly.
- **First experiments**: 1) Train a small network (e.g., LeNet) with SpaM and compare post-pruning accuracy to standard pruning methods. 2) Vary the prior structure and observe its impact on pruning performance. 3) Apply SpaM to a larger architecture (e.g., ResNet) and measure computational savings from structured pruning.

## Open Questions the Paper Calls Out
None

## Limitations
- The effectiveness of the Laplace approximation may degrade for very deep or highly non-linear models where the posterior deviates significantly from Gaussian.
- The computational efficiency gains assume that the marginal likelihood optimization converges quickly, but this may not hold for all architectures or datasets.
- The robustness of the method under domain shifts or in continual learning scenarios is not addressed and requires further investigation.

## Confidence
- High: The empirical improvements in accuracy retention after pruning, especially at 95-99% sparsity levels, are well-supported by the presented experiments.
- Medium: The computational efficiency claims and the general applicability of the Laplace approximation across different architectures are plausible but require further validation.
- Low: The long-term stability and robustness of the method under domain shifts or in continual learning scenarios are not addressed and remain uncertain.

## Next Checks
1. Test the method on a broader range of architectures, including very deep networks and those with complex activation functions, to assess the robustness of the Laplace approximation.
2. Evaluate the method's performance under domain shifts or in continual learning scenarios to understand its robustness beyond the original training distribution.
3. Benchmark the computational efficiency gains on a wider variety of hardware platforms and compare against other state-of-the-art sparsification techniques to confirm the claimed advantages.