---
ver: rpa2
title: 'Navigating Data Scarcity using Foundation Models: A Benchmark of Few-Shot
  and Zero-Shot Learning Approaches in Medical Imaging'
arxiv_id: '2408.08058'
source_url: https://arxiv.org/abs/2408.08058
tags:
- data
- in21k
- medical
- resnet-18
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks few-shot and zero-shot learning performance
  of 16 pretrained foundation models across 19 diverse medical imaging datasets. Models
  were evaluated using linear probing and fine-tuning strategies with varying numbers
  of labeled samples per class.
---

# Navigating Data Scarcity using Foundation Models: A Benchmark of Few-Shot and Zero-Shot Learning Approaches in Medical Imaging

## Quick Facts
- arXiv ID: 2408.08058
- Source URL: https://arxiv.org/abs/2408.08058
- Authors: Stefano Woerner; Christian F. Baumgartner
- Reference count: 24
- Key result: BiomedCLIP excels with ≤5 samples/class, large CLIP models with 2B pretraining samples perform best with more data, and fine-tuning ResNet-18 matches foundation models with ≥20 samples/class

## Executive Summary
This comprehensive benchmark evaluates 16 pretrained foundation models across 19 diverse medical imaging datasets using few-shot and zero-shot learning approaches. The study systematically compares linear probing and fine-tuning strategies across varying numbers of labeled samples per class (1-30 samples). Results demonstrate that domain-specific pretraining (BiomedCLIP) provides the strongest performance for very small training sets, while large general-purpose models (CLIP-ViT-H pretrained on LAION-2B) excel with moderate training data. Fine-tuning smaller models like ResNet-18 matches or exceeds foundation model performance when sufficient labeled data is available. Zero-shot learning consistently underperformed few-shot approaches across all evaluated scenarios.

## Method Summary
The study evaluates 16 pretrained foundation models on 19 medical imaging datasets from the MedIMeta meta-dataset. Models are adapted using two strategies: linear probing (frozen backbone with trainable classification head) and fine-tuning (all weights trainable). Few-shot learning is evaluated across sample sizes from 1 to 30 per class, with 10 query samples per class. Hyperparameter search optimizes learning rate, optimizer, and training steps. Zero-shot learning uses CLIP/BiomedCLIP similarity matching with three prompt templates. Performance is measured using AUROC averaged over 100 task instances per dataset.

## Key Results
- BiomedCLIP, trained exclusively on medical data, achieves best average performance for very small training sets (≤5 samples/class)
- Large CLIP models pretrained on LAION-2B perform best with moderate training data (5-20 samples/class) when using linear probing
- Fine-tuning ResNet-18 matches or exceeds foundation model performance with ≥20 samples per class
- Zero-shot learning consistently underperforms few-shot approaches across all datasets
- Model performance correlates with parameter count and pretraining dataset size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BiomedCLIP achieves superior few-shot performance with very few labeled samples due to domain-specific pretraining on medical image-text pairs.
- Mechanism: The model's training exclusively on biomedical data (PMC-15M dataset) aligns its representations with medical domain semantics, reducing the semantic gap between pretraining and target tasks.
- Core assumption: Medical pretraining data provides sufficient diversity and coverage to learn generalizable medical visual features.
- Evidence anchors:
  - [abstract]: "BiomedCLIP, a model pretrained exclusively on medical data, performs best on average for very small training set sizes"
  - [section]: "BiomedCLIP, which was the only foundation model in our comparison trained entirely with medical data, outperformed its larger CLIP counterparts"
  - [corpus]: Weak - no direct comparison of domain-specific vs general pretraining in corpus
- Break Condition: If target medical tasks involve rare conditions or imaging modalities not well-represented in biomedical pretraining data.

### Mechanism 2
- Claim: Linear probing of large CLIP models (ViT-H/14) achieves best average few-shot performance when more labeled samples are available.
- Mechanism: Large models trained on massive general datasets (LAION-2B) develop rich visual representations that transfer well to diverse medical tasks when combined with domain-appropriate text prompts.
- Core assumption: General visual representations learned from 2B image-text pairs capture sufficient semantic diversity for medical task adaptation.
- Evidence anchors:
  - [abstract]: "very large CLIP models pretrained on LAION-2B perform best with slightly more training samples"
  - [section]: "linear probing with CLIP-ViT-H on average outperformed fine-tuning of the ResNet-18 and ResNet-50 for all n"
  - [corpus]: Weak - no direct comparison of linear probing vs fine-tuning strategies in corpus
- Break Condition: If target medical tasks require fine-grained feature learning that only task-specific fine-tuning can capture.

### Mechanism 3
- Claim: Fine-tuning smaller models (ResNet-18) matches or exceeds foundation model performance when sufficient labeled data (≥20 samples/class) is available.
- Mechanism: Task-specific adaptation through fine-tuning allows the model to learn task-specific decision boundaries that may be more optimal than frozen foundation model representations.
- Core assumption: ResNet-18 architecture has sufficient capacity to learn effective representations when provided adequate labeled data.
- Evidence anchors:
  - [abstract]: "simply fine-tuning a ResNet-18 pretrained on ImageNet performs similarly with more than five training examples per class"
  - [section]: "with more training data fine-tuning the ResNet-18 performed almost as well as CLIP-ViT-H, and for n ≥ 20 the fine-tuned ResNet-18 outperformed BiomedCLIP"
  - [corpus]: Weak - no direct performance comparison of fine-tuning vs linear probing strategies in corpus
- Break Condition: If target medical tasks require the rich visual representations only large foundation models can provide.

## Foundational Learning

- Concept: Few-shot learning adaptation strategies (linear probing vs fine-tuning)
  - Why needed here: Different adaptation strategies have vastly different computational requirements and performance characteristics across sample regimes
  - Quick check question: When would you choose linear probing over fine-tuning for a new medical imaging task?

- Concept: Contrastive language-image pretraining (CLIP)
  - Why needed here: CLIP's zero-shot capabilities and strong few-shot performance make it central to this benchmark study
  - Quick check question: What is the key architectural difference between CLIP and traditional supervised vision models?

- Concept: Zero-shot learning limitations in medical imaging
  - Why needed here: Understanding why ZSL performs poorly in medical tasks helps focus research efforts on more promising approaches
  - Quick check question: Why might ZSL work well on general computer vision tasks but poorly on medical imaging tasks?

## Architecture Onboarding

- Component map:
  - Foundation models (BiomedCLIP, CLIP variants, DINOv2, ViT variants, ResNet variants)
  - Adaptation strategies (linear probing, fine-tuning)
  - Evaluation framework (MedIMeta meta-dataset, 19 diverse medical imaging datasets)
  - Hyperparameter search system (optimizers, learning rates, training steps)

- Critical path:
  1. Load pretrained foundation model
  2. Apply adaptation strategy (linear probe or fine-tune)
  3. Evaluate on sampled few-shot tasks
  4. Aggregate results across datasets and sample sizes

- Design tradeoffs:
  - Model size vs computational feasibility: Large models offer better performance but require more resources
  - Adaptation strategy: Linear probing is computationally cheaper but may underperform fine-tuning with sufficient data
  - Pretraining data domain: Medical pretraining helps with small sample sizes but general pretraining scales better with more data

- Failure signatures:
  - Poor performance across all datasets suggests issues with model initialization or adaptation strategy
  - High variance between datasets indicates domain mismatch between pretraining and target tasks
  - Performance degradation with increasing sample size suggests overfitting or optimization issues

- First 3 experiments:
  1. Reproduce BiomedCLIP 1-shot performance on chest X-ray dataset
  2. Compare linear probing vs fine-tuning on dermatology dataset with 10 samples/class
  3. Test zero-shot performance using different prompt templates on fundus imaging dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BiomedCLIP's performance compare to other foundation models when trained on datasets with varying levels of medical domain overlap with its pretraining data (PMC-15M)?
- Basis in paper: [explicit] The paper notes BiomedCLIP performs exceptionally well on some datasets but poorly on others, hypothesizing this may be due to dataset overlap with its pretraining corpus.
- Why unresolved: The study doesn't systematically analyze performance correlation with dataset-medical domain overlap.
- What evidence would resolve it: Performance analysis stratified by dataset pretraining overlap, or ablation studies on BiomedCLIP using datasets with known pretraining overlap.

### Open Question 2
- Question: What is the optimal number of training samples per class for transitioning from linear probing of foundation models to fine-tuning smaller models like ResNet-18?
- Basis in paper: [explicit] The paper shows ResNet-18 fine-tuning outperforms linear probing on CLIP-ViT-H at n≥20, but doesn't identify an optimal transition point.
- Why unresolved: The study doesn't analyze the trade-off between computational cost and performance at different sample sizes.
- What evidence would resolve it: Cost-benefit analysis across a wider range of sample sizes, including computational time and resource requirements.

### Open Question 3
- Question: Can zero-shot learning performance be improved for medical imaging through domain-specific prompt engineering or retrieval augmentation?
- Basis in paper: [explicit] Zero-shot performance was significantly worse than few-shot learning, contradicting findings in general computer vision.
- Why unresolved: The study only tested basic prompt templates without exploring medical domain adaptation techniques.
- What evidence would resolve it: Experiments with medical domain-specific prompts, few-shot prompt tuning, or retrieval-augmented generation approaches.

## Limitations

- Narrow evaluation scope focused exclusively on AUROC, potentially missing clinically relevant metrics
- Unclear methodology for ensuring complete subject-level independence across few-shot task splits
- High computational requirements for evaluating large foundation models limit practical accessibility

## Confidence

**High Confidence Claims:**
- BiomedCLIP's superior performance with ≤5 samples per class due to domain-specific pretraining
- Linear probing of large CLIP models outperforming fine-tuning for n < 20
- Zero-shot learning consistently underperforming few-shot approaches across all datasets

**Medium Confidence Claims:**
- Correlation between model performance and parameter count/pretraining dataset size
- ResNet-18 fine-tuning matching foundation model performance with ≥20 samples per class
- Generalizability of findings across the 19 diverse medical imaging datasets

**Low Confidence Claims:**
- Extrapolation of trends to sample sizes beyond the evaluated range (n > 30)
- Performance predictions for foundation models not included in the benchmark
- Clinical utility of the observed performance differences in real-world diagnostic settings

## Next Checks

1. **Subject-Level Independence Verification**: Implement a rigorous cross-validation framework that explicitly tracks patient/subject identifiers across all splits to ensure no data leakage between training and evaluation sets. Verify that the reported performance holds under this stricter independence constraint.

2. **Multi-Metric Clinical Validation**: Re-evaluate the top-performing models using clinically relevant metrics beyond AUROC, including precision at high recall thresholds, F1-score across different operating points, and calibration curves. This will assess whether AUROC-based rankings align with clinical utility.

3. **Parameter-Efficient Fine-Tuning Comparison**: Extend the benchmark to include LoRA, prefix tuning, and other parameter-efficient fine-tuning methods for the large CLIP models. This will determine whether the computational advantages of smaller models can be combined with the representational power of large foundation models.