---
ver: rpa2
title: Revealing Fine-Grained Values and Opinions in Large Language Models
arxiv_id: '2406.19238'
source_url: https://arxiv.org/abs/2406.19238
tags:
- tropes
- responses
- love
- proposition
- trope
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work examines values and opinions embedded in large language\
  \ models by analyzing their responses to the Political Compass Test (PCT) across\
  \ 420 demographic and instruction prompt variations, yielding 156,240 responses\
  \ from 6 models. The authors propose analyzing both coarse-grained stances and fine-grained\
  \ tropes\u2014recurrent justifications across prompts\u2014to reveal latent values."
---

# Revealing Fine-Grained Values and Opinions in Large Language Models

## Quick Facts
- arXiv ID: 2406.19238
- Source URL: https://arxiv.org/abs/2406.19238
- Authors: Dustin Wright; Arnav Arora; Nadav Borenstein; Srishti Yadav; Serge Belongie; Isabelle Augenstein
- Reference count: 40
- One-line primary result: Demographic prompts significantly shift LLM political stances on the Political Compass Test, with shared justification patterns (tropes) across models revealing latent values.

## Executive Summary
This work examines values and opinions embedded in large language models by analyzing their responses to the Political Compass Test (PCT) across 420 demographic and instruction prompt variations, yielding 156,240 responses from 6 models. The authors propose analyzing both coarse-grained stances and fine-grained tropes—recurrent justifications across prompts—to reveal latent values. They find that demographic features significantly shift PCT outcomes, with political orientation and gender having the largest effects. Open-ended vs. closed-form responses differ substantially, especially for right-leaning demographics. Trope analysis reveals shared reasoning patterns across models, suggesting common biases in generated text. The dataset and code are publicly released for further research.

## Method Summary
The study analyzes 156,240 responses from 6 LLMs (Mistral, Mixtral, Zephyr, Llama 2, Llama 3, OLMo) to 62 PCT propositions using 420 prompt variations combining 21 demographic options and 20 instruction variations. For coarse-grained analysis, responses are classified into PCT categories (post-hoc for open-ended). For fine-grained analysis, sentences are clustered semantically using DBSCAN (ε=0.15, minPts=8), centroids distilled to tropes, and non-justifications filtered by LLM. The method reveals how demographic features affect political stance outputs and identifies shared reasoning patterns across models.

## Key Results
- Demographic features added to prompts significantly affect PCT outcomes, with political orientation and gender having the largest effects
- Open-ended and closed-form response formats elicit different political stances, with closed-form yielding more definitive positions
- Trope analysis reveals shared justification patterns across models, with most identified tropes generated by at least two models
- Right-leaning demographics show the largest disparities between open and closed response formats

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Demographic prompts significantly alter the political stance outputs of LLMs on the Political Compass Test (PCT).
- Mechanism: When demographic features (e.g., political orientation, gender, class) are added to the prompt, the LLM simulates the perspective of that demographic, leading to shifts in the generated stance towards the PCT propositions.
- Core assumption: The LLM has been trained on data that associates certain viewpoints with specific demographic groups, and it can simulate those perspectives when prompted.
- Evidence anchors:
  - [abstract] "We find that demographic features added to prompts significantly affect outcomes on the PCT, reflecting bias..."
  - [section] "We find significant effects across most of the demographic categories tested... Specifying an explicit political orientation significantly affects placement in almost all cases, with a large effect size."
  - [corpus] "Covert Bias: The Severity of Social Views' Unalignment in Language Models Towards Implicit and Explicit Opinion" (suggests demographic-based biases exist in LLMs)
- Break condition: If the LLM lacks sufficient training data associating viewpoints with the prompted demographic, or if the demographic prompt is not well-formed, the stance may not shift as expected.

### Mechanism 2
- Claim: Open-ended and closed-form response formats elicit different political stances from LLMs, even for the same demographic prompt.
- Mechanism: When prompted with a closed-form format (choose from Strongly Agree, Agree, Disagree, Strongly Disagree), the LLM tends to provide a more definitive stance. In contrast, open-ended prompts allow for more nuanced or neutral responses, including refusals to answer.
- Core assumption: The LLM's generation process is influenced by the format constraints of the prompt, and it can generate different types of responses (e.g., categorical vs. explanatory) based on the prompt structure.
- Evidence anchors:
  - [abstract] "...disparities between the results of tests when eliciting closed-form vs. open domain responses."
  - [section] "Fig. 5 shows responses from Llama 3 across the 62 PCT propositions for the open-ended and closed-form settings... We also observe that in the open setting the model more often either a) refuses to answer, or b) tends to output a neutral stance."
  - [corpus] "Fine-Grained Interpretation of Political Opinions in Large Language Models" (suggests differences in LLM responses based on prompt format)
- Break condition: If the LLM is highly consistent in its responses regardless of format, or if the prompt is ambiguous, the format may not significantly impact the stance.

### Mechanism 3
- Claim: LLMs generate similar justifications (tropes) for their stances across different models and prompts, even when the stances themselves differ.
- Mechanism: By clustering semantically similar sentences in the open-ended responses, we can identify recurrent themes (tropes) that represent common justifications for stances. These tropes are shared across models and prompts, revealing underlying reasoning patterns.
- Core assumption: The LLM's training data contains common arguments and justifications for various viewpoints, and it can generate these patterns when prompted, regardless of the specific stance or demographic.
- Evidence anchors:
  - [abstract] "Additionally, patterns in the plain text rationales via tropes show that similar justifications are repeatedly generated across models and prompts even with disparate stances."
  - [section] "We see that many models share their most prevalent tropes... In fact, most identified tropes are generated by at least two models."
  - [corpus] "Whose Emotions and Moral Sentiments Do Language Models Reflect?" (suggests LLMs reflect common sentiments and justifications)
- Break condition: If the LLM's training data lacks diversity in arguments and justifications, or if the clustering method fails to capture meaningful patterns, the trope analysis may not reveal shared reasoning.

## Foundational Learning

- Concept: Political Compass Test (PCT)
  - Why needed here: The PCT is used as a standardized survey to elicit political stances from LLMs, allowing for quantitative analysis of their biases.
  - Quick check question: What are the two axes used in the PCT to measure political orientation?

- Concept: Trope analysis
  - Why needed here: Trope analysis is a method for identifying common justifications and explanations in LLM-generated text, providing insight into the underlying reasoning patterns.
  - Quick check question: How does the trope extraction process cluster semantically similar sentences to identify recurrent themes?

- Concept: Demographic prompting
  - Why needed here: Demographic prompting is used to simulate different perspectives in LLM responses, allowing for analysis of how biases vary across demographic groups.
  - Quick check question: What demographic categories were used in this study to prompt the LLMs?

## Architecture Onboarding

- Component map: 6 LLMs (Mistral, Mixtral, Zephyr, Llama 2, Llama 3, OLMo) -> 62 PCT propositions -> 420 prompt variations (21 demographic options × 20 instruction variations) -> 156,240 responses -> Trope extraction pipeline (clustering + LLM filtering) -> Coarse-grained stance analysis + Fine-grained trope analysis

- Critical path: Generate responses to PCT propositions using 6 LLMs and 420 prompt variations → Analyze coarse-grained stances (PCT placement) and fine-grained tropes → Draw conclusions about biases and reasoning patterns

- Design tradeoffs: The study uses a large number of prompt variations to ensure robustness, but this also increases computational cost. The trope analysis relies on unsupervised clustering, which may be sensitive to parameter settings

- Failure signatures: Inconsistent responses across prompt variations, failure to generate responses for some propositions, poor clustering of tropes, or difficulty in filtering non-trope sentences

- First 3 experiments:
  1. Generate responses to PCT propositions using a single LLM and a subset of prompt variations to validate the generation process
  2. Analyze the coarse-grained stances (PCT placement) for a single demographic category to verify the impact of demographic prompting
  3. Apply the trope extraction pipeline to a small set of responses to ensure the clustering and filtering steps are working as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different pre-training dataset compositions affect the tropes LLMs generate?
- Basis in paper: [inferred] The paper notes that models with similar architectures or training data (e.g., Llama 2 & Llama 3, Mistral & Mixtral) share more tropes, suggesting pre-training data impacts generated justifications.
- Why unresolved: The study uses a fixed set of 6 models without analyzing their specific training data characteristics. It's unclear which aspects of the data (e.g., source diversity, political slant, domain coverage) drive trope generation.
- What evidence would resolve it: A systematic study varying pre-training data composition while keeping model architecture constant, measuring trope overlap and political alignment changes.

### Open Question 2
- Question: How does trope generation scale with model size beyond 13B parameters?
- Basis in paper: [explicit] The authors note computational constraints limited them to models under 13B parameters and suggest future work should examine larger models.
- Why unresolved: The paper's models represent a limited parameter range. Larger models may exhibit different trope patterns or political biases.
- What evidence would resolve it: Testing the same methodology on models ranging from 7B to 70B+ parameters, comparing trope diversity, prevalence, and alignment with political stances.

### Open Question 3
- Question: How transferable are trope extraction methods across languages and cultures?
- Basis in paper: [inferred] The PCT focuses on Western-specific topics in English, and the authors note this limits cross-cultural applicability of their findings.
- Why unresolved: The current methodology relies on English-specific semantic embeddings and clustering approaches that may not generalize.
- What evidence would resolve it: Applying the same trope extraction pipeline to multilingual models tested on culturally diverse political surveys, comparing extracted patterns across languages.

## Limitations
- Unknown demographic model bias: The study assumes LLMs can accurately simulate demographic perspectives but doesn't validate whether generated responses reflect real-world viewpoints versus stereotypes in training data
- Trope extraction validity: The quality of extracted tropes depends heavily on DBSCAN parameter choices and lacks ground truth validation of whether they represent meaningful justifications
- Response completeness: The analysis excludes refusals and non-compliant responses without fully characterizing these exclusions, which could bias results if certain prompts trigger systematic refusals

## Confidence

**High confidence:** The finding that demographic prompts significantly affect PCT outcomes is well-supported by the large-scale data (156,240 responses) and consistent regression results showing political orientation and gender as primary drivers. The open vs. closed format differences are clearly observable across all models.

**Medium confidence:** The trope analysis revealing shared reasoning patterns across models is methodologically sound but relies on unsupervised clustering whose results are sensitive to parameter settings. The claim that similar justifications appear across disparate stances is plausible but would benefit from manual validation.

**Low confidence:** The interpretation that identified tropes represent "latent values" in LLMs overstates what can be concluded from surface-level textual patterns. The study doesn't establish causal links between training data, trope generation, and underlying model values.

## Next Checks
1. **Manual trope validation:** Sample 50 clusters from each model and have human annotators rate whether the distilled tropes represent coherent, meaningful justifications versus arbitrary semantic groupings

2. **Demographic bias ground truth:** Compare LLM-generated responses for specific demographic prompts against real-world survey data for those demographics to assess simulation accuracy versus stereotype reproduction

3. **Parameter sensitivity analysis:** Systematically vary DBSCAN parameters (ε from 0.1 to 0.3, minPts from 5 to 15) and measure stability of top tropes to establish robustness of the fine-grained analysis