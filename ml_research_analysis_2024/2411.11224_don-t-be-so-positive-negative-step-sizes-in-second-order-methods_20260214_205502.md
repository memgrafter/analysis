---
ver: rpa2
title: 'Don''t Be So Positive: Negative Step Sizes in Second-Order Methods'
arxiv_id: '2411.11224'
source_url: https://arxiv.org/abs/2411.11224
tags:
- step
- methods
- negative
- sizes
- wolfe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that second-order and second-order-like methods
  for neural network training are globally convergent when combined with a Wolfe line
  search that allows negative step sizes, even if the method produces ascent directions.
  The key insight is that when the search direction points uphill, taking a negative
  step size allows the algorithm to descend while still using full curvature information,
  avoiding the need for expensive Hessian modifications like damping.
---

# Don't Be So Positive: Negative Step Sizes in Second-Order Methods

## Quick Facts
- arXiv ID: 2411.11224
- Source URL: https://arxiv.org/abs/2411.11224
- Authors: Betty Shea; Mark Schmidt
- Reference count: 36
- Key outcome: SR1 with negative step sizes outperforms BFGS and Adam in deep learning training, especially for deeper networks

## Executive Summary
This paper challenges the conventional wisdom that second-order optimization methods must use positive step sizes by demonstrating that allowing negative step sizes with Wolfe line search can lead to better performance. The authors show that when search directions point uphill, negative step sizes enable the algorithm to descend while still leveraging full curvature information. This approach eliminates the need for computationally expensive Hessian modifications like damping, providing a simple way to incorporate negative curvature information for improved optimization in deep learning.

## Method Summary
The paper proposes using Wolfe line search with negative step sizes in second-order and quasi-Newton methods, particularly SR1. Instead of enforcing positive definiteness through damping or other modifications, the algorithm allows both positive and negative step sizes when searching for the optimal step. When the search direction points uphill (ascent direction), a negative step size is taken to achieve descent. This approach maintains computational efficiency while potentially accessing more curvature information than traditional methods that only use positive step sizes.

## Key Results
- SR1 quasi-Newton with negative step sizes often outperforms BFGS and Adam, particularly as network depth increases
- Limited-memory SR1 with negative step sizes achieved the lowest training error in most tested datasets
- Standard SR1 with only positive step sizes often diverged during training, while the negative step size variant remained stable
- The approach provides a computationally inexpensive alternative to Hessian modifications like damping

## Why This Works (Mechanism)
The key insight is that second-order methods can still achieve descent even when the search direction points uphill, by taking a negative step size. Traditional methods enforce positive definiteness to ensure ascent directions, then modify the Hessian (e.g., through damping) to make directions descent. However, this paper shows that allowing negative step sizes eliminates the need for these modifications while still achieving descent. The Wolfe line search framework naturally accommodates negative step sizes, ensuring sufficient decrease and curvature conditions are met regardless of step sign.

## Foundational Learning
1. **Second-order methods and Hessian approximations** - Why needed: Understanding how methods like Newton's method and quasi-Newton use curvature information; Quick check: Can you explain the difference between BFGS and SR1 updates?
2. **Line search algorithms and Wolfe conditions** - Why needed: The paper relies on Wolfe line search to find appropriate step sizes; Quick check: What are the sufficient decrease and curvature conditions in Wolfe line search?
3. **Positive definiteness in optimization** - Why needed: Traditional methods enforce this to ensure descent; Quick check: Why do most quasi-Newton methods enforce positive definiteness in the Hessian approximation?

## Architecture Onboarding
**Component Map:** Wolfe Line Search -> Step Size Determination -> Search Direction (can be ascent) -> Function Evaluation
**Critical Path:** Compute search direction → Apply Wolfe line search (allowing negative steps) → Update parameters
**Design Tradeoffs:** Negative step sizes avoid damping costs but may require more line search iterations; Positive definiteness enforcement guarantees descent but adds computational overhead
**Failure Signatures:** Divergence with positive-only step sizes; Poor performance if line search parameters are too restrictive
**First Experiments:**
1. Compare SR1 with positive-only vs. negative-allowed step sizes on a simple convex quadratic
2. Test Wolfe line search with negative step sizes on a neural network with known saddle points
3. Benchmark training convergence speed between SR1 (negative steps) and BFGS on a small CNN

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Theoretical analysis primarily limited to convex quadratic problems, lacking formal convergence guarantees for non-convex deep learning objectives
- Experiments focus on training error rather than generalization performance, which is critical for neural network optimization
- The claim that negative step sizes avoid damping needs while maintaining computational efficiency remains empirically supported but theoretically unproven for general non-convex functions

## Confidence
- **High Confidence**: Empirical observation that negative step sizes improve SR1 performance and prevent divergence on neural networks
- **Medium Confidence**: Theoretical convergence results for convex quadratics and their relevance to deep learning
- **Medium Confidence**: Claim that negative step sizes avoid damping needs while maintaining computational efficiency

## Next Checks
1. Conduct experiments measuring generalization performance (test accuracy) alongside training error to evaluate if negative step sizes improve or degrade model generalization
2. Analyze sensitivity of negative step sizes to learning rate initialization and Wolfe line search parameters across different network architectures
3. Extend theoretical analysis to characterize convergence behavior on non-convex functions with saddle points and negative curvature regions