---
ver: rpa2
title: 'More Experts Than Galaxies: Conditionally-overlapping Experts With Biologically-Inspired
  Fixed Routing'
arxiv_id: '2410.08003'
source_url: https://arxiv.org/abs/2410.08003
tags:
- comet
- standard
- learning
- network
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COMET introduces a fixed, biologically inspired random projection
  followed by a k-winner-take-all cap operation to replace trainable gating functions
  in sparse neural networks. This approach creates an exponential number of overlapping
  experts whose overlap depends on input similarity, enabling faster learning and
  improved generalization without increasing trainable parameters.
---

# More Experts Than Galaxies: Conditionally-overlapping Experts With Biologically-Inspired Fixed Routing

## Quick Facts
- arXiv ID: 2410.08003
- Source URL: https://arxiv.org/abs/2410.08003
- Reference count: 40
- Primary result: Fixed random routing with k-winner-take-all creates exponential overlapping experts that improve learning speed and generalization without increasing trainable parameters

## Executive Summary
COMET introduces a novel approach to sparse neural networks that replaces trainable gating functions with fixed, biologically inspired random projections followed by k-winner-take-all cap operations. This design creates an exponential number of overlapping experts whose overlap depends on input similarity, enabling faster learning and improved generalization without increasing trainable parameters. The method demonstrates consistent performance gains across image classification, language modeling, and regression tasks using various architectures including ViT, MLP-Mixer, GPT, and MLP.

## Method Summary
COMET uses fixed random projections and k-winner-take-all (k-WTA) cap operations to create input-dependent sparsity in neural networks. The routing network generates binary masks that are applied to both the backbone and routing networks' activations. Similar inputs project to similar locations in the random projection space, resulting in masks with higher overlap. This causes the model to use more shared parameters for similar inputs while maintaining computational efficiency through sparsity. The method is tested across multiple tasks and architectures, showing improved performance particularly for larger models.

## Key Results
- Up to 9% higher accuracy on ViT Large for CIFAR100 compared to standard models
- Faster convergence rates across all tested tasks and architectures
- Consistent performance gains for larger models while maintaining computational efficiency
- Exponential expert space (binomial(N,k) possible combinations) without increasing trainable parameters

## Why This Works (Mechanism)

### Mechanism 1
Fixed random projection followed by k-winner-take-all (k-WTA) cap operation creates overlapping experts whose degree of overlap depends on input similarity. Random projection transforms input representations into a higher-dimensional space, and the k-WTA operation selects top k activations to create a binary mask. Similar inputs project to similar locations in this space, resulting in masks with higher overlap. This causes the model to use more shared parameters for similar inputs.

### Mechanism 2
Input-dependent sparsity enables faster learning and improved generalization without increasing trainable parameters. By routing similar inputs to overlapping experts, the model can transfer knowledge between similar examples, reducing the effective learning problem size and allowing generalization from fewer examples. The fixed routing provides stable learning targets since each input always maps to the same expert.

### Mechanism 3
Exponential number of implicit experts enables handling more complex tasks than traditional MoE with limited experts. The number of possible expert combinations is binomial(N,k) which grows exponentially with model size, creating a much larger effective expert space than traditional MoE which is limited to the number of predefined experts.

## Foundational Learning

- Concept: Random projections and Johnson-Lindenstrauss lemma
  - Why needed here: Understanding how random projections can preserve distances between high-dimensional points is crucial for grasping why similar inputs map to similar masks
  - Quick check question: If two points are distance d apart in the original space, what is the expected distance between their random projections? (Answer: proportional to d, with high probability)

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: COMET is a variant of MoE, so understanding the standard MoE framework helps contextualize the innovations
  - Quick check question: In standard MoE, what component decides which experts to activate for a given input? (Answer: The gating/network)

- Concept: Catastrophic forgetting and continual learning
  - Why needed here: The paper mentions that trainable gating functions can cause forgetfulness, which COMET avoids with fixed routing
  - Quick check question: What problem occurs when a model trained on task A is then trained on task B without any mechanism to preserve task A knowledge? (Answer: Catastrophic forgetting)

## Architecture Onboarding

- Component map: Input -> Backbone network & Routing network -> k-WTA cap operation -> Masking layer -> Masked activations -> Output

- Critical path:
  1. Input passes through both backbone and routing networks
  2. Routing network generates mask via random projection + k-WTA
  3. Mask is applied to both networks' activations
  4. Computation proceeds with masked activations
  5. Only backbone network produces final output (no mask on last layer)

- Design tradeoffs:
  - Fixed vs trainable routing: Fixed avoids representation collapse but cannot adapt to task-specific routing needs
  - Sparsity level (pk): Higher sparsity means more efficient computation but potentially less capacity
  - Random projection dimension: Should match backbone layer size for simplicity, but could be adjusted

- Failure signatures:
  - All-zero masks: Routing network failing to activate any neurons (should be rare with k-WTA)
  - Very low mask overlap: Inputs not grouping by similarity as intended
  - Degraded performance vs standard model: Routing not providing benefit for this task

- First 3 experiments:
  1. Verify mask generation: Feed simple inputs through routing network and check that similar inputs produce similar masks
  2. Compare learning curves: Train COMET vs standard model on a simple task and verify faster initial learning
  3. Test sparsity effects: Vary pk and measure impact on performance and computation time to find optimal sparsity level

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of COMET scale with extremely large models (e.g., trillion-parameter models) and what are the theoretical limits of its effectiveness? The paper mentions that current systems are limited to a few thousand experts, while COMET has an exponential number of experts, but does not test scaling to trillion-parameter models. Systematic scaling experiments showing performance and efficiency gains on models with trillions of parameters would clarify COMET's effectiveness at extreme scales.

### Open Question 2
What is the optimal routing network architecture for COMET, and how does it compare to the fixed random projection approach used in the current implementation? The paper notes that COMET only requires the routing network to generate a mask with the same shape as the backbone layer, leaving room for alternative architectures. Comparative experiments testing different routing network architectures (e.g., attention-based, learned projections) against the fixed random projection approach would identify optimal designs.

### Open Question 3
How does COMET perform in multi-task and continual learning scenarios compared to state-of-the-art methods designed specifically for these settings? The paper conjectures that COMET's input-dependent sparsity will yield advantages for multi-task settings but defers this to future work, only providing preliminary transfer learning results. Comprehensive experiments in continual learning and multi-task settings comparing COMET to specialized methods like MAML, EWC, or MER would demonstrate its effectiveness in these scenarios.

## Limitations
- Fixed random routing cannot adapt to task-specific input patterns, potentially missing optimal routing configurations
- Exponential expert space may contain significant redundancy without empirical validation of true expert diversity
- No ablation studies on the k-WTA operation parameters to determine sensitivity to this hyperparameter

## Confidence
- High Confidence: Exponential expert space claim (mathematically proven), improved learning rate observations (directly measured)
- Medium Confidence: Input similarity-based overlap mechanism (supported by controlled experiments but not comprehensive), generalization improvements (task-dependent)
- Low Confidence: Catastrophic forgetting claims (not directly tested), claims about biological plausibility (not empirically validated)

## Next Checks
1. Test COMET performance across multiple random seeds to verify stability of the fixed routing approach
2. Quantify actual diversity in expert usage across inputs to validate the practical value of the exponential expert space
3. Test whether COMET models retain performance better when fine-tuned on new tasks compared to standard models, validating the catastrophic forgetting claim