---
ver: rpa2
title: Disease Classification and Impact of Pretrained Deep Convolution Neural Networks
  on Diverse Medical Imaging Datasets across Imaging Modalities
arxiv_id: '2408.17011'
source_url: https://arxiv.org/abs/2408.17011
tags:
- performance
- training
- layers
- medical
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the performance of deep convolutional
  neural networks (DCNNs) for medical image classification across three imaging modalities:
  chest X-rays (CXR), optical coherence tomography (OCT), and whole slide images (WSI).
  Ten pretrained architectures (VGG, ResNet, Inception, Xception, DenseNet) were evaluated
  under five experimental settings using both pretraining and random initialization.'
---

# Disease Classification and Impact of Pretrained Deep Convolution Neural Networks on Diverse Medical Imaging Datasets across Imaging Modalities

## Quick Facts
- arXiv ID: 2408.17011
- Source URL: https://arxiv.org/abs/2408.17011
- Authors: Jutika Borah; Kumaresh Sarmah; Hidam Kumarjit Singh
- Reference count: 21
- Primary result: Pretraining provides variable benefits across medical imaging modalities, with deeper architectures not necessarily yielding better performance.

## Executive Summary
This study investigates the performance of ten pretrained deep convolutional neural networks (VGG, ResNet, Inception, Xception, DenseNet) for medical image classification across chest X-rays, optical coherence tomography, and whole slide images. Five experimental settings combining pretraining and random initialization were evaluated. Results demonstrate that improvements in ImageNet do not directly translate to medical imaging tasks, and deeper, more complex architectures do not guarantee better performance. The study highlights that pretraining benefits vary significantly across imaging modalities and datasets.

## Method Summary
The study evaluates ten pretrained DCNN architectures across three medical imaging datasets using five experimental settings: pretraining with frozen layers, pretraining with fine-tuning top layers, pretraining with fine-tuning all layers, random initialization with fine-tuning top layers, and random initialization with fine-tuning all layers. Models were trained for 300 epochs using Adam optimizer with learning rate decay, and evaluated using AUC, precision, sensitivity, and specificity with 95% confidence intervals.

## Key Results
- Deeper and more complex architectures did not necessarily result in the best performance
- Pretraining benefits varied significantly across different medical imaging modalities
- Models performed inconsistently across datasets within the same modality
- Improvements in ImageNet did not parallel improvements in medical imaging tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining provides less benefit when the target medical imaging domain has fundamentally different visual features than ImageNet.
- Mechanism: Transfer learning relies on the assumption that learned features from a source domain (ImageNet) will be useful in a target domain (medical images). If the source and target domains differ significantly in image characteristics, the pretrained features may not align well with the target task, reducing the benefit of pretraining.
- Core assumption: Feature transferability is proportional to domain similarity between source and target datasets.
- Evidence anchors:
  - [abstract] "improvements in ImageNet are not parallel to the medical imaging tasks"
  - [section] "Improvement in pretraining will be pronounced in modalities where visual features are more or less complementary to those in natural images"
  - [corpus] Weak or missing; no direct comparison of ImageNet vs. medical image feature distributions.

### Mechanism 2
- Claim: Model complexity (number of parameters) does not guarantee better performance on medical imaging tasks.
- Mechanism: Increasing model depth and parameters can lead to overfitting on small medical datasets, especially when the dataset lacks diversity. Simpler models may generalize better by avoiding overfitting to noise or irrelevant patterns in the data.
- Core assumption: Model capacity should match dataset size and complexity for optimal generalization.
- Evidence anchors:
  - [abstract] "deeper and more complex architectures did not necessarily result in the best performance"
  - [section] "the largest and smallest models show slight differences in performance"
  - [corpus] Weak or missing; no direct evidence of overfitting vs. underfitting analysis.

### Mechanism 3
- Claim: Performance improvements from pretraining vary significantly across different medical imaging modalities.
- Mechanism: Each imaging modality (CXR, OCT, WSI) has unique image characteristics, resolution, and diagnostic information. Pretraining benefits depend on how well the pretrained features align with the specific modality's visual patterns and task requirements.
- Core assumption: The effectiveness of transfer learning is modality-dependent, requiring tailored approaches for each imaging type.
- Evidence anchors:
  - [abstract] "Models performed inconsistently across datasets within the same modality"
  - [section] "the performance of deep learning models across CXR, WSI, and OCT modalities is due to differences in image resolution, information content, data availability, pre-processing needs"
  - [corpus] Weak or missing; no direct evidence of modality-specific feature alignment.

## Foundational Learning

- Concept: Domain shift in transfer learning
  - Why needed here: Understanding how differences between source (ImageNet) and target (medical imaging) domains affect model performance is critical for interpreting the study's results.
  - Quick check question: What are the key differences between natural images and medical images that could cause domain shift?

- Concept: Model overfitting and generalization
  - Why needed here: The study highlights that deeper models do not always perform better, suggesting overfitting may be a concern. Understanding this concept helps explain why simpler models can generalize better on smaller datasets.
  - Quick check question: How does model complexity relate to the risk of overfitting on small datasets?

- Concept: Modality-specific feature extraction
  - Why needed here: Each medical imaging modality has unique characteristics that require tailored feature extraction approaches. Understanding this concept helps explain why performance varies across modalities.
  - Quick check question: What are the key differences between CXR, OCT, and WSI that affect feature extraction?

## Architecture Onboarding

- Component map:
  - Data preprocessing: resizing, normalization
  - Model architectures: VGG, ResNet, Inception, Xception, DenseNet
  - Transfer learning settings: pretraining (fixed, top layers, all layers), random initialization
  - Training: Adam optimizer, cross-entropy loss, learning rate decay
  - Evaluation: AUC, precision, sensitivity, specificity

- Critical path:
  1. Load and preprocess datasets (resize, normalize)
  2. Select model architecture and transfer learning setting
  3. Train model with specified hyperparameters
  4. Evaluate performance using standard metrics
  5. Compare results across models and settings

- Design tradeoffs:
  - Model complexity vs. dataset size: Deeper models may overfit on smaller datasets
  - Pretraining vs. random initialization: Pretraining may not always improve performance due to domain shift
  - Fine-tuning settings: Different levels of fine-tuning (fixed, top layers, all layers) affect performance differently

- Failure signatures:
  - Overfitting: High training accuracy but low validation/test accuracy
  - Underfitting: Low accuracy on both training and validation/test sets
  - Domain shift: Pretraining does not improve performance due to significant differences between source and target domains

- First 3 experiments:
  1. Train a simple model (e.g., VGG16) with random initialization on the CXR dataset to establish a baseline
  2. Train the same model with pretraining (all layers) on the CXR dataset to compare performance
  3. Train a more complex model (e.g., ResNet152) with random initialization on the CXR dataset to test the effect of model complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does pretraining on ImageNet fail to improve medical image classification performance, and when does it actually harm performance?
- Basis in paper: [explicit] The paper observes that pretrained models as fixed feature extractors yield poor performance across datasets, while WSI shows better performance with pretraining. It notes that improvements in ImageNet do not parallel medical imaging tasks.
- Why unresolved: The paper identifies variability in pretraining benefits across modalities but does not systematically identify the characteristics of datasets or tasks where pretraining fails or causes harm.
- What evidence would resolve it: Systematic experiments varying dataset characteristics (size, domain similarity to ImageNet, class complexity) to identify thresholds where pretraining transitions from beneficial to neutral to harmful.

### Open Question 2
- Question: What is the optimal fine-tuning strategy for medical image classification when using pretrained models - should we fine-tune all layers, only top layers, or use a hybrid approach?
- Basis in paper: [explicit] The paper evaluates three fine-tuning strategies (freeze all layers, fine-tune top layers, fine-tune all layers) and finds performance varies by dataset and architecture, but doesn't identify optimal strategy.
- Why unresolved: The paper shows that different strategies work better for different datasets and architectures, but doesn't provide guidance on selecting the optimal strategy for a given medical imaging task.
- What evidence would resolve it: Empirical studies systematically comparing fine-tuning strategies across diverse medical imaging datasets with varying characteristics, potentially leading to a decision framework.

### Open Question 3
- Question: Can a single deep learning model architecture effectively perform well across all medical imaging modalities (CXR, OCT, WSI), or do we need modality-specific architectures?
- Basis in paper: [explicit] The paper observes that "the performance of models varies significantly across modalities like CXR, OCT, and WSI" and that "each modality presents unique challenges that require tailored approaches."
- Why unresolved: While the paper demonstrates performance differences across modalities, it doesn't test whether architecture modifications or attention mechanisms could create a universal model, or definitively prove modality-specific architectures are necessary.
- What evidence would resolve it: Experiments with advanced architectures incorporating attention mechanisms, dynamic networks, and cross-modal learning to test if universal performance can be achieved, compared against modality-specific models.

## Limitations

- Lack of detailed experimental methodology with unspecified data split ratios and layer freezing strategies
- Limited empirical evidence for theoretical claims about domain shift between ImageNet and medical images
- No analysis of potential confounding factors such as dataset size, quality, and preprocessing variations across modalities

## Confidence

- High Confidence: The observation that deeper architectures do not always yield better performance is well-supported by empirical results across multiple datasets and modalities
- Medium Confidence: The claim that pretraining benefits vary across modalities is supported by data, but underlying mechanisms require further investigation
- Low Confidence: Theoretical explanation for why ImageNet improvements don't translate to medical imaging tasks lacks empirical substantiation

## Next Checks

1. Conduct comparative analysis of feature distributions between ImageNet and each medical imaging modality to quantify domain shift
2. Perform detailed experiments measuring overfitting by comparing training and validation performance across different model complexities and dataset sizes
3. Design experiments to isolate and compare feature extraction capabilities of different architectures for each modality, testing whether unified models can effectively capture common features across all modalities