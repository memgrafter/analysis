---
ver: rpa2
title: Self-Powered LLM Modality Expansion for Large Speech-Text Models
arxiv_id: '2410.03798'
source_url: https://arxiv.org/abs/2410.03798
tags:
- speech
- self-powered
- data
- training
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses speech anchor bias in large speech-text models
  (LSMs), where models overly rely on speech inputs and ignore textual instructions.
  The authors propose a self-powered LSM that generates augmented training data using
  the model itself to mitigate this bias.
---

# Self-Powered LLM Modality Expansion for Large Speech-Text Models

## Quick Facts
- arXiv ID: 2410.03798
- Source URL: https://arxiv.org/abs/2410.03798
- Reference count: 40
- Primary result: Self-powered LSM improves instruction-following across speech recognition, translation, understanding, and question answering tasks by mitigating speech anchor bias

## Executive Summary
This paper addresses speech anchor bias in large speech-text models (LSMs), where models overly rely on speech inputs and ignore textual instructions. The authors propose a self-powered LSM that generates augmented training data using the model itself to mitigate this bias. Experiments show that self-powered LSM significantly improves instruction-following capabilities across speech recognition, translation, understanding, and question answering tasks, achieving better alignment between speech and text modalities. The method effectively reduces speech anchor bias and enhances the model's responsiveness to instructions while maintaining general textual performance.

## Method Summary
The self-powered LSM uses a frozen Whisper encoder to process speech, a Q-former to bridge speech embeddings to LLM input space, and a Vicuna-7B LLM to generate responses. Self-powered data is generated by prompting the LLM with instruction-transcript pairs to create pseudo-targets, which replace original transcripts during fine-tuning. The model is trained for 2 epochs with batch size 512, learning rate 2e-5, freezing the speech encoder while fine-tuning Q-former and LLM components.

## Key Results
- Self-powered LSM achieves significant improvements in instruction-following tasks across speech recognition, translation, understanding, and question answering
- Layer-wise attention analysis confirms reduction in speech anchor bias with increased instruction focus in deeper layers
- Model maintains general textual performance on MMLU benchmark while adding speech modality capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-powered augmentation redirects training from speech-centric to instruction-constrained likelihood maximization
- Mechanism: Model generates pseudo-targets conditioned on both transcript and task-specific instructions, forcing attention to instructions during fine-tuning
- Core assumption: LLM can generate reasonable pseudo-targets reflecting instruction without relying solely on speech modality
- Evidence anchors: Abstract mentions self-powered LSM uses augmented ASR data for effective instruction tuning; section explains speech anchor bias skews distribution P(t|s,i;θ) to align with P(t|s;θ)

### Mechanism 2
- Claim: Layer-wise attention dynamics shift from speech-dominant to instruction-aware as training progresses
- Mechanism: Early layers process speech heavily, but deeper layers increasingly attend to instructions, mirroring instruction-following LLM behavior
- Core assumption: Attention norms reliably indicate information flow from speech vs instruction to output
- Evidence anchors: Section reveals LSMs trained with speech data overly concentrate on speech, neglecting instructions; analysis shows augmentation data improves sensitivity to instructional cues, especially in terminal layers

### Mechanism 3
- Claim: Freezing speech encoder preserves pre-trained features while fine-tuning Q-former and LLM aligns representations
- Mechanism: Frozen encoder ensures stable acoustic representations, while joint fine-tuning adapts modality connector and language generation to augmented instruction-target pairs
- Core assumption: Pre-trained encoder representations are sufficiently rich to support new instruction-following tasks
- Evidence anchors: Section states speech encoder is frozen to ensure stability while Q-former and LLM are fine-tuned for alignment; training objective section describes frozen Whisper encoder producing contextual representation H

## Foundational Learning

- Concept: Attention mechanism interpretation in multimodal models
  - Why needed here: Understanding how attention weights reflect modality dominance is critical to diagnosing speech anchor bias
  - Quick check question: If attention norm from instruction tokens to output increases in deeper layers, what does that indicate about the model's instruction-following behavior?

- Concept: Self-distillation and pseudo-label generation
  - Why needed here: Self-powered augmentation relies on generating pseudo-targets from the model itself, a form of self-distillation in multimodal context
  - Quick check question: What is the difference between standard self-distillation and the self-powered approach used here?

- Concept: Catastrophic forgetting in modality adaptation
  - Why needed here: Model must retain general text-only capabilities while adding speech modality without degrading MMLU performance
  - Quick check question: How does freezing the speech encoder help mitigate catastrophic forgetting of text modality?

## Architecture Onboarding

- Component map: Whisper encoder (frozen) -> Q-former -> LLM (Vicuna) with instruction -> Output
- Critical path: 1) Input speech → Whisper encoder → Q-former → LLM with instruction → Output; 2) During self-powered fine-tuning, LLM generates pseudo-targets using instruction-transcript pairs, replacing original transcripts
- Design tradeoffs: Freezing speech encoder reduces complexity but may limit adaptability to new acoustic patterns; full fine-tuning on LLM/Q-former achieved better performance than LoRA but is more resource-intensive; self-powered data improves instruction-following but may degrade ASR metrics
- Failure signatures: High WER on ASR tasks after fine-tuning indicates speech anchor bias not fully mitigated; poor MMLU performance suggests catastrophic forgetting; low instruction-following accuracy indicates pseudo-target generation or fine-tuning ineffective
- First 3 experiments: 1) Compare layer-wise attention norms between vanilla IT LSM and self-powered LSM to verify reduction in speech anchor bias; 2) Evaluate instruction-following accuracy on SLU/QA tasks to confirm improved responsiveness; 3) Test MMLU performance to ensure no degradation in general text capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different instruction types on self-powered LSM performance?
- Basis in paper: Experiments show training with different instruction types yields varying performance improvements
- Why unresolved: Paper lacks detailed analysis of how each instruction type contributes to overall performance
- What evidence would resolve it: Experiments training and evaluating LSMs using each instruction type separately, comparing performance across ASR, ST, SLU, and QA tasks

### Open Question 2
- Question: How does speech encoder size affect self-powered LSM performance in different tasks?
- Basis in paper: Experiments using whisper encoders of different sizes (small, medium, large-v2) show larger encoders yield better performance
- Why unresolved: Paper doesn't explore specific task-level performance differences or trade-off between model size and efficiency
- What evidence would resolve it: Detailed analysis of performance across different tasks using each encoder size, with assessment of computational resources required

### Open Question 3
- Question: Can self-powered LSM be further improved by incorporating LoRA fine-tuning for LLM component?
- Basis in paper: Experiments compare self-powered LSM with and without LoRA fine-tuning, showing improved performance in some tasks
- Why unresolved: Paper doesn't explore benefits of combining self-powered data generation with LoRA fine-tuning or optimal configuration for different tasks
- What evidence would resolve it: Experiments training self-powered LSM with various combinations of self-powered data generation and LoRA fine-tuning, evaluating performance across all task types

## Limitations

- Self-powered augmentation mechanism heavily relies on quality of pseudo-target generation, which could reinforce incorrect behaviors if generated targets contain hallucinations
- Freezing speech encoder may limit adaptability to new acoustic patterns that differ from pre-training data
- Method's effectiveness depends on quality of instruction generation and may not generalize across all instruction types or languages

## Confidence

- High confidence in identification of speech anchor bias as real phenomenon affecting LSMs, supported by layer-wise attention analysis
- Medium confidence in effectiveness of self-powered augmentation, as results show improvements but mechanism's dependence on LLM-generated pseudo-targets introduces uncertainty
- Low confidence in claim that freezing speech encoder is optimal, as paper only compares against full fine-tuning without exploring intermediate approaches

## Next Checks

1. Conduct ablation studies comparing self-powered data quality against ground-truth instruction-target pairs to quantify impact of pseudo-target generation errors
2. Test approach with different LLM backbones beyond Vicuna to verify self-powered mechanism generalizes across model architectures and sizes
3. Perform extended evaluation on out-of-domain speech datasets to assess whether model maintains instruction-following capabilities with different acoustic patterns