---
ver: rpa2
title: On-the-fly Definition Augmentation of LLMs for Biomedical NER
arxiv_id: '2404.00152'
source_url: https://arxiv.org/abs/2404.00152
tags:
- entity
- definitions
- entities
- output
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates improving large language models (LLMs) on
  biomedical named entity recognition (NER) tasks through on-the-fly definition augmentation.
  The authors first establish baseline LLM performance on six biomedical NER datasets,
  then propose a method that provides models with relevant concept definitions at
  inference time, enabling them to revise their predictions.
---

# On-the-fly Definition Augmentation of LLMs for Biomedical NER

## Quick Facts
- **arXiv ID**: 2404.00152
- **Source URL**: https://arxiv.org/abs/2404.00152
- **Reference count**: 18
- **Primary result**: GPT-4 achieves 15% relative F1 improvement on biomedical NER tasks through definition augmentation

## Executive Summary
This paper investigates improving large language models on biomedical named entity recognition tasks through on-the-fly definition augmentation. The authors establish baseline LLM performance on six biomedical NER datasets, then propose a method that provides models with relevant concept definitions at inference time, enabling them to revise their predictions. Experiments with zero- and few-shot settings show consistent improvements across both open-source and closed LLMs, with GPT-4 achieving an average 15% relative F1 improvement. Ablation studies confirm that performance gains stem from adding relevant definitions rather than additional context alone.

## Method Summary
The approach involves two main steps: first prompting the LLM to extract entities from input text, then providing follow-up prompts augmented with definitions of biomedical concepts identified in the text. The method explores both single-turn and iterative prompting strategies, where iterative prompting allows the model to revise predictions one entity at a time with relevant contextual information. The authors evaluate their approach across six biomedical NER datasets using zero-shot and few-shot settings, testing different knowledge sources including UMLS, Wikidata, and GPT-4-generated definitions.

## Key Results
- GPT-4 achieves 15% relative F1 improvement with definition augmentation in zero-shot settings
- Iterative prompting shows significant improvements (32.6% for Llama 2, 33.9% for GPT-4 in zero-shot settings)
- Definitions from domain-specific sources like UMLS outperform general sources like Wikidata
- GPT-4-generated definitions show little to no impact on model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing definitions during inference improves model precision and recall on biomedical NER tasks.
- Mechanism: LLMs lack domain-specific knowledge about biomedical concepts. By augmenting prompts with relevant definitions, models gain access to precise contextual information that helps them correctly identify entities and assign proper types.
- Core assumption: LLMs can effectively incorporate new definitional knowledge at inference time to revise their predictions.
- Evidence anchors:
  - Experiments show consistent improvements across both open-source and closed LLMs, with GPT-4 achieving an average 15% relative F1 improvement with definition augmentation.
  - The approach focuses on identifying and providing definitions of relevant biomedical concepts as a follow-up step at inference time, allowing the model to correct its entity extractions.

### Mechanism 2
- Claim: Iterative prompting with entity-specific definitions improves correction accuracy compared to single-turn approaches.
- Mechanism: Breaking down the correction process into atomic steps (one entity at a time) simplifies the task for the model, allowing it to focus on specific corrections with relevant contextual information.
- Core assumption: LLMs perform better on focused, simpler tasks than complex multi-step corrections.
- Evidence anchors:
  - Iterative prompts augmented with the definition of a single concept and asking the model to make corrections to a single extracted entity (if needed) at a time.
  - An average increase of 32.6% and 33.9% for Llama 2 and 15% and 13.7% for GPT-4 using single turn and iterative prompting, respectively.

### Mechanism 3
- Claim: The source and relevance of definition knowledge significantly impact performance improvements.
- Mechanism: Domain-specific knowledge sources (like UMLS) provide more accurate and relevant definitions than general sources (like Wikidata) or automatically generated definitions, leading to better performance gains.
- Core assumption: The quality and specificity of definitional knowledge directly correlates with the magnitude of performance improvements.
- Evidence anchors:
  - Definitions from Wikidata also improve over the zero-shot baseline, albeit to a lesser degree than UMLS.
  - The definitions generated by GPT-4 seem to have little to no impact on the model's performance.

## Foundational Learning

- **Concept**: In-context learning (ICL)
  - Why needed here: The paper evaluates LLM performance in zero- and few-shot settings where models learn from examples provided in the prompt rather than fine-tuning.
  - Quick check question: What is the key difference between zero-shot and few-shot learning in the context of this paper?

- **Concept**: Entity linking and knowledge bases
  - Why needed here: The approach relies on mapping biomedical concepts in text to entries in knowledge bases (like UMLS) to retrieve relevant definitions.
  - Quick check question: How does the entity linker contribute to the definition augmentation process?

- **Concept**: Named Entity Recognition (NER)
  - Why needed here: The paper focuses on improving LLM performance on biomedical NER tasks, which involves identifying and classifying named entities in text.
  - Quick check question: What makes biomedical NER particularly challenging compared to general NER tasks?

## Architecture Onboarding

- **Component map**: Entity linking (SciSpaCy) → Knowledge base (UMLS) → Prompt construction → LLM inference → Entity correction
- **Critical path**: Entity linking → Definition retrieval → Prompt construction → LLM inference → Entity correction
- **Design tradeoffs**:
  - Single-turn vs. iterative prompting: Simplicity vs. potential performance gains
  - Knowledge source selection: Domain-specific accuracy vs. general applicability
  - Prompt complexity: Information richness vs. model context window limitations
- **Failure signatures**:
  - Poor entity linking performance leading to irrelevant definitions
  - Over-reliance on definitions causing models to ignore context
  - Context window overflow due to large number of definitions
- **First 3 experiments**:
  1. Benchmark baseline LLM performance on biomedical NER datasets in zero-shot settings
  2. Evaluate the impact of definition augmentation on zero-shot performance
  3. Test different knowledge sources (UMLS, Wikidata, GPT-4 generated) for definition quality

## Open Questions the Paper Calls Out

- How does the performance of definition augmentation vary across different biomedical subdomains (e.g., clinical trials vs. molecular biology)?
- What is the optimal strategy for selecting which biomedical concepts to provide definitions for during inference?
- How does definition augmentation performance scale with entity type schema complexity?

## Limitations

- Knowledge source generalization: The approach shows mixed results with non-domain-specific knowledge sources, raising questions about cross-domain applicability
- Iterative prompting overhead: The paper doesn't adequately address computational overhead and latency implications in production scenarios
- Error propagation risk: Poor initial entity extraction can lead to incorrect entity mappings, potentially amplifying errors rather than correcting them

## Confidence

- **High Confidence**: The core finding that definition augmentation improves biomedical NER performance across both zero- and few-shot settings
- **Medium Confidence**: The specific performance improvements (15% relative F1 improvement for GPT-4) and the superiority of iterative prompting
- **Low Confidence**: The generalizability of results to non-biomedical domains and long-term stability with evolving knowledge bases

## Next Checks

1. Test the definition augmentation approach on non-biomedical NER datasets (e.g., legal, financial) to assess cross-domain applicability
2. Evaluate how performance changes over time as biomedical knowledge bases like UMLS are updated
3. Conduct detailed error analysis of initial entity extractions to quantify error types and their impact on potential improvements