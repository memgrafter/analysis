---
ver: rpa2
title: 'A New Dynamic Distributed Planning Approach: Application to DPDP Problems'
arxiv_id: '2403.00805'
source_url: https://arxiv.org/abs/2403.00805
tags:
- 'false'
- agent
- agents
- move
- dans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new dynamic distributed planning approach
  for multi-agent systems that can adapt to changes in the agents' action sets and
  environment. The approach uses genetic algorithms to generate plans that satisfy
  constraints, with each agent generating a new plan whenever its action set changes.
---

# A New Dynamic Distributed Planning Approach: Application to DPDP Problems

## Quick Facts
- arXiv ID: 2403.00805
- Source URL: https://arxiv.org/abs/2403.00805
- Authors: Zakaria Tolba
- Reference count: 0
- One-line primary result: Proposes a dynamic distributed planning approach using genetic algorithms that adapts to changes in agents' action sets and environment.

## Executive Summary
This paper introduces a dynamic distributed planning approach for multi-agent systems that enables agents to adapt their plans when action sets change due to environmental dynamics. The approach leverages genetic algorithms to generate plans that satisfy constraints, with each agent independently regenerating plans whenever its action set changes. The method is validated on a Dynamic Pick Up and Delivery Problem (DPDP) case study, demonstrating improved plan quality and adaptability compared to static planning methods.

## Method Summary
The approach employs genetic algorithms for plan generation, where each agent creates an initial population of randomly ordered action sequences and evaluates fitness using a weighted sum of constraint satisfaction functions. When an agent's action set changes during execution, it dynamically regenerates a new plan incorporating both unexecuted actions from the previous plan and newly introduced actions. The system operates in a fully distributed manner, with each agent independently planning and executing without central coordination, though arbitration mechanisms exist for conflict resolution.

## Key Results
- Genetic algorithms effectively optimize action sequences under multiple constraints (distance and obstacle minimization)
- Dynamic replanning successfully incorporates unexecuted actions from previous plans when action sets change
- Distributed approach minimizes communication overhead while maintaining plan adaptability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The approach dynamically regenerates agent plans whenever the agent's action set changes, ensuring plans remain executable in a changing environment.
- Mechanism: At each change in the agent's action set, the agent treats all previously unexecuted actions plus new actions as a new planning problem, and applies genetic algorithms to find a new optimal action sequence under updated constraints.
- Core assumption: The agent can reliably detect changes in its action set and that the unexecuted portion of the previous plan remains valid to include in the new planning set.
- Evidence anchors:
  - [abstract] "The approach uses genetic algorithms to generate plans that satisfy constraints, with each agent generating a new plan whenever its action set changes."
  - [section 2.3] "During the execution of the initial plan, if the agent encounters a change requiring new actions, it dynamically generates a new plan including unexecuted old actions and new actions."
- Break condition: If change detection is delayed or if action interdependencies make the unexecuted portion incompatible with new actions, the approach may fail.

### Mechanism 2
- Claim: Genetic algorithms are used to optimize the ordering of actions within each agent's plan, balancing multiple constraint functions such as distance and obstacle minimization.
- Mechanism: Each agent creates an initial population of randomly ordered action sequences, evaluates fitness using a weighted sum of constraint satisfaction functions, and iteratively selects, crosses over, and mutates plans to converge on the highest-fitness plan.
- Core assumption: The fitness function accurately captures the agent's operational constraints and that the genetic algorithm's operators preserve valid action sequences.
- Evidence anchors:
  - [abstract] "The approach uses genetic algorithms to generate plans that satisfy constraints."
  - [section 2.2] "The agent evaluates each plan by computing the fitness function f_Ai defined as the weighted sum of constraint fitness functions."
- Break condition: If the constraint space is too complex or constraints conflict, the genetic algorithm may converge to suboptimal or invalid plans.

### Mechanism 3
- Claim: The system is fully distributed: each agent independently plans and executes without central coordination, minimizing communication overhead.
- Mechanism: Each agent maintains its own action set, constraints, and state; when a change occurs, it locally regenerates its plan and executes without informing other agents unless arbitration is needed.
- Core assumption: The environment changes are local enough that agents can resolve conflicts through pre-defined arbitration rather than constant coordination.
- Evidence anchors:
  - [abstract] "The approach fits into the context of distributed planning for distributed plans where each agent can produce its own plans."
  - [section 5.1.3] "Planification distribuée pour des plans distribués (PDPD) [Plusieurs planificateurs, plusieurs exécutants]: se focalise sur le fait que chaque agent est capable de produire ses propres plans indépendamment."
- Break condition: If conflicts are frequent or global coordination is required, the distributed approach may lead to plan inconsistencies or resource conflicts.

## Foundational Learning

- Concept: Multi-agent systems and agent autonomy
  - Why needed here: The approach relies on agents being autonomous and capable of local decision-making and plan regeneration.
  - Quick check question: Can an agent independently detect changes in its environment and update its action set without external input?

- Concept: Genetic algorithms for optimization
  - Why needed here: The plan generation process uses genetic algorithms to search for optimal action sequences under multiple constraints.
  - Quick check question: Does the fitness function properly encode all relevant constraints, and are genetic operators applied correctly?

- Concept: Dynamic planning and state management
  - Why needed here: The approach must handle changing states and action sets while preserving partial plan progress.
  - Quick check question: How does the agent track which actions remain unexecuted and how are they integrated into the new plan?

## Architecture Onboarding

- Component map: Agent (state, action set, constraints, fitness function, revision function) -> Environment (dynamic changes) -> Plan Generator (genetic algorithm) -> Coordination Manager (arbitration)
- Critical path: Change detection → Action set update → Plan regeneration (genetic algorithm) → Execution
- Design tradeoffs: Distributed autonomy vs. potential for plan conflicts; genetic algorithm exploration vs. convergence time; dynamic plan regeneration vs. execution continuity
- Failure signatures: Stale or invalid plans due to missed change detection; plan divergence among agents leading to resource conflicts; genetic algorithm stagnation or slow convergence
- First 3 experiments:
  1. Simulate a single agent with a static action set and verify initial plan generation and execution.
  2. Introduce a single change in the agent's action set and verify dynamic plan regeneration includes unexecuted actions.
  3. Scale to multiple agents and simulate environment changes, checking for conflict arbitration and distributed plan execution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed approach handle the scalability of plan generation when the number of agents and actions increases significantly?
- Basis in paper: [explicit] The paper does not discuss the scalability of the approach in terms of computational complexity or performance with a large number of agents and actions.
- Why unresolved: The paper focuses on demonstrating the approach's ability to handle dynamic changes and generate optimal plans for a specific case study (DPDP) but does not address the scalability aspect.
- What evidence would resolve it: Conducting experiments with varying numbers of agents and actions to measure the approach's performance and computational complexity would provide insights into its scalability.

### Open Question 2
- Question: How does the proposed approach handle conflicts between agents when they have conflicting goals or preferences?
- Basis in paper: [explicit] The paper mentions the use of coordination mechanisms, such as arbitration, but does not provide details on how conflicts are resolved or how the approach handles conflicting goals or preferences.
- Why unresolved: The paper focuses on the dynamic plan generation aspect and does not delve into the specifics of conflict resolution in multi-agent systems.
- What evidence would resolve it: Providing examples or scenarios where agents have conflicting goals or preferences and demonstrating how the approach resolves these conflicts would address this question.

### Open Question 3
- Question: How does the proposed approach compare to other dynamic planning approaches in terms of plan quality and adaptability?
- Basis in paper: [inferred] The paper claims that the approach improves plan quality and adaptability compared to static planning methods but does not provide a direct comparison with other dynamic planning approaches.
- Why unresolved: The paper focuses on the proposed approach and its advantages over static planning but does not benchmark it against other dynamic planning methods.
- What evidence would resolve it: Conducting a comparative study with other dynamic planning approaches using the same case study or similar scenarios would provide insights into the approach's performance relative to existing methods.

## Limitations

- Limited empirical validation across diverse scenarios beyond the DPDP case study
- No detailed analysis of genetic algorithm parameter sensitivity and convergence quality
- Assumption of reliable change detection mechanisms without detailed implementation discussion

## Confidence

- High confidence: The core mechanism of using genetic algorithms for plan optimization under constraints is well-established and theoretically sound.
- Medium confidence: The dynamic replanning mechanism incorporating unexecuted actions is plausible but requires empirical validation for edge cases.
- Medium confidence: The distributed architecture's ability to handle conflicts through arbitration without central coordination needs more rigorous testing.

## Next Checks

1. **Stress Test Change Detection**: Simulate scenarios with rapid, cascading changes to action sets and measure the system's ability to maintain valid plans without missing changes or including obsolete actions.

2. **Conflict Resolution Benchmarking**: Create multi-agent scenarios with frequent resource conflicts and compare the distributed approach's performance against a centralized coordinator in terms of plan quality and execution time.

3. **Genetic Algorithm Parameter Sensitivity**: Systematically vary mutation rates, population sizes, and selection thresholds across multiple DPDP instances to identify optimal configurations and test algorithm robustness to parameter changes.