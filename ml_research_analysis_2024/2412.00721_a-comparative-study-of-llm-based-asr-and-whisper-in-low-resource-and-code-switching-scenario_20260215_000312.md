---
ver: rpa2
title: A Comparative Study of LLM-based ASR and Whisper in Low Resource and Code Switching
  Scenario
arxiv_id: '2412.00721'
source_url: https://arxiv.org/abs/2412.00721
tags:
- speech
- whisper
- llm-based
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the effectiveness of LLM-based ASR systems
  compared to Whisper in low-resource and Mandarin-English code-switching scenarios.
  LLM-based ASR models leverage large language models to enhance speech recognition
  performance, particularly in languages where Whisper underperforms.
---

# A Comparative Study of LLM-based ASR and Whisper in Low Resource and Code Switching Scenario

## Quick Facts
- arXiv ID: 2412.00721
- Source URL: https://arxiv.org/abs/2412.00721
- Reference count: 0
- LLM-based ASR achieves 12.8% relative WER improvement over Whisper in low-resource languages

## Executive Summary
This study compares LLM-based ASR systems with Whisper in low-resource and Mandarin-English code-switching scenarios. The researchers implement an LLM-based ASR framework that uses Whisper's encoder with a linear projector and language-specific LLMs. Experiments on languages including Thai, Vietnamese, Arabic, Welsh, and Mandarin-English reveal that LLM-based ASR outperforms Whisper in low-resource settings by 12.8% relative WER, while Whisper shows superior performance in Mandarin-English code-switching after fine-tuning. The study emphasizes the importance of selecting language-appropriate LLMs to maximize recognition accuracy.

## Method Summary
The researchers implement LLM-based ASR by combining Whisper's encoder with a linear projector and language-specific LLMs, using prompt templates for speech-to-text tasks. For low-resource languages, they use language-specific LLMs (Sailor-7b for Thai/Vietnamese, Jais-7b for Arabic, Mixtral-7b for Welsh), while Qwen2.5-7b is used for Mandarin-English. Whisper-finetune employs LoRA (r=16) while LLM-based ASR uses LoRA (r=24). Both models are trained for 10 epochs with AdamW optimizer and evaluated using beam search decoding. Datasets include GigaSpeech2, Common Voice, and ASRU 2019 Mandarin-English code-switching Challenge.

## Key Results
- LLM-based ASR achieves 12.8% relative WER improvement over Whisper in low-resource languages (Thai, Vietnamese, Arabic, Welsh)
- Whisper performs better than LLM-based ASR in Mandarin-English code-switching tasks after fine-tuning
- Optimal performance achieved when LLM is closely aligned with target language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based ASR outperforms Whisper in low-resource languages due to better linguistic reasoning for rare vocabulary
- Mechanism: The LLM component leverages its pre-trained language understanding to fill in recognition gaps where the speech encoder alone struggles with limited data
- Core assumption: The LLM has sufficient language-specific training data to recognize low-resource vocabulary
- Evidence anchors:
  - [abstract] "LLM-based ASR achieves a 12.8% relative improvement in Word Error Rate (WER) over Whisper in low-resource languages like Thai, Vietnamese, and Welsh."
  - [section] "In languages where Whisper performs poorly, LLM-based ASR shows certain advantages."
  - [corpus] Weak - no direct corpus evidence for specific linguistic reasoning mechanisms
- Break condition: If the LLM lacks sufficient language-specific training data, the performance advantage disappears

### Mechanism 2
- Claim: Whisper excels at Mandarin-English code-switching due to its multilingual pretraining
- Mechanism: Whisper's architecture and training on diverse multilingual data allows it to handle language alternation patterns effectively
- Core assumption: The multilingual pretraining captures sufficient code-switching patterns
- Evidence anchors:
  - [abstract] "Whisper performs better in Mandarin-English code-switching tasks after fine-tuning."
  - [section] "Whisper excels in tasks such as Mandarin-English code-switching ASR."
  - [corpus] Weak - no specific corpus evidence about code-switching pattern handling
- Break condition: If the code-switching patterns differ significantly from Whisper's pretraining distribution

### Mechanism 3
- Claim: Selecting language-appropriate LLMs is critical for optimal performance
- Mechanism: The LLM's proficiency in the target language directly impacts recognition accuracy through better language modeling
- Core assumption: Different LLMs have varying levels of language proficiency based on their pretraining data
- Evidence anchors:
  - [section] "The performance of the ASR model is positively correlated with the LLM's proficiency in the specific language being recognized."
  - [section] "It is evident that the model achieves optimal performance when the large language model is closely aligned with the target language."
  - [corpus] Weak - no direct corpus evidence linking LLM pretraining data to recognition performance
- Break condition: When using a general-purpose LLM instead of a language-specific one for low-resource languages

## Foundational Learning

- Concept: Speech-to-text pipeline architecture
  - Why needed here: Understanding how Whisper and LLM-based ASR differ in their processing paths
  - Quick check question: What are the key architectural differences between encoder-decoder and encoder-LLM approaches?

- Concept: Language model pretraining data composition
  - Why needed here: Explains why certain LLMs work better for specific languages
  - Quick check question: How does the composition of pretraining data affect a language model's performance on low-resource languages?

- Concept: Code-switching patterns in speech
  - Why needed here: Critical for understanding why Whisper performs well on Mandarin-English tasks
  - Quick check question: What are the typical patterns in code-switching speech that ASR systems need to handle?

## Architecture Onboarding

- Component map:
  Speech → Encoder → Projector → LLM → Text output

- Critical path:
  Speech → Encoder → Projector → LLM → Text output

- Design tradeoffs:
  - Fixed vs. trainable speech encoder layers
  - LLM size vs. inference speed
  - Language-specific vs. general-purpose LLM
  - Fine-tuning approach (LoRA vs. full fine-tuning)

- Failure signatures:
  - Poor WER in target language → LLM language proficiency issue
  - High latency → Large LLM or inefficient projector
  - Code-switching errors → Whisper might be better choice
  - Overfitting on small datasets → Too many trainable parameters

- First 3 experiments:
  1. Compare WER on target language using different LLMs (Sailor, Mixtral, Llama3, Vicuna)
  2. Test Whisper vs. LLM-based ASR on code-switching subset
  3. Evaluate impact of freezing vs. training speech encoder layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM-based ASR systems perform when scaled to languages with even fewer resources than those tested in this study?
- Basis in paper: [explicit] The study focuses on languages with 60-200 hours of training data but does not explore extremely low-resource scenarios with less than 60 hours of data
- Why unresolved: The paper's experiments are limited to a specific range of low-resource languages, leaving the performance of LLM-based ASR in ultra-low-resource settings unexplored
- What evidence would resolve it: Additional experiments testing LLM-based ASR on languages with less than 60 hours of training data, comparing results with Whisper and other baseline models

### Open Question 2
- Question: What is the impact of using different speech encoder architectures (e.g., Wav2Vec2, HuBERT) instead of Whisper encoder in LLM-based ASR systems?
- Basis in paper: [inferred] The study uses Whisper encoder for LLM-based ASR but does not compare it with other speech encoder architectures, which could affect performance
- Why unresolved: The paper assumes Whisper encoder is optimal but does not validate this assumption by testing alternative encoders
- What evidence would resolve it: Comparative experiments using different speech encoder architectures (e.g., Wav2Vec2, HuBERT) in LLM-based ASR systems, measuring performance across various languages and tasks

### Open Question 3
- Question: How does the performance of LLM-based ASR systems vary with different prompt engineering strategies for code-switching tasks?
- Basis in paper: [explicit] The study mentions using prompts in LLM-based ASR but does not explore the impact of different prompt engineering strategies on code-switching performance
- Why unresolved: The paper uses a generic prompt ("transcribe speech to text") but does not investigate whether task-specific or language-specific prompts could improve results
- What evidence would resolve it: Experiments testing various prompt engineering strategies (e.g., task-specific, language-specific, or contextual prompts) in LLM-based ASR systems for code-switching tasks, measuring their impact on recognition accuracy

## Limitations
- Dataset size and representation details are not provided for each low-resource language, limiting generalizability
- Model architecture and hyperparameter details are insufficient for faithful reproduction
- Code-switching evaluation is limited to Mandarin-English without exploring other language pairs

## Confidence

**High Confidence**: The claim that LLM-based ASR outperforms Whisper in low-resource languages is supported by the reported 12.8% relative improvement in WER.

**Medium Confidence**: The assertion that Whisper excels in Mandarin-English code-switching tasks after fine-tuning is supported by the study's results.

**Low Confidence**: The claim that selecting language-appropriate LLMs is critical for optimal performance lacks direct evidence linking LLM pretraining data composition to recognition accuracy.

## Next Checks
1. Conduct a detailed analysis of the datasets used for each low-resource language, including statistics on the number of speakers, utterances, and linguistic diversity
2. Perform ablation studies to isolate the impact of different model components (e.g., LLM size, projector architecture, fine-tuning approach) on the performance of LLM-based ASR and Whisper
3. Extend the evaluation to include code-switching scenarios involving other language pairs (e.g., Spanish-English, Hindi-English) and analyze the specific code-switching patterns handled by the models