---
ver: rpa2
title: Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning
arxiv_id: '2410.19290'
source_url: https://arxiv.org/abs/2410.19290
tags:
- knowledge
- fictitious
- data
- lora
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucinations in large language
  models (LLMs) caused by knowledge inconsistency between pre-training and fine-tuning
  stages. The authors propose PREREQ-TUNE, a two-stage fine-tuning strategy that first
  learns prerequisite knowledge through a LoRA module, then focuses on downstream
  task skills without being affected by knowledge inconsistency.
---

# Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning

## Quick Facts
- arXiv ID: 2410.19290
- Source URL: https://arxiv.org/abs/2410.19290
- Reference count: 27
- Key outcome: PREREQ-TUNE achieves up to 45.30% accuracy on biography generation compared to 32.70% for standard SFT, and outperforms strong baselines while using fictitious data.

## Executive Summary
This paper addresses hallucinations in LLMs caused by knowledge inconsistency between pre-training and fine-tuning stages. The authors propose PREREQ-TUNE, a two-stage fine-tuning strategy that first learns prerequisite knowledge through a LoRA module, then focuses on downstream task skills without being affected by knowledge inconsistency. The method can be combined with fictitious synthetic data to create multiple knowledge LoRA versions, forcing the model to ground responses in its internal knowledge. Experiments show PREREQ-TUNE outperforms strong baselines like POPULAR, FLAME, FACTTUNE, and RL, achieving significant improvements in factuality across biography generation, medical QA, and short QA tasks.

## Method Summary
PREREQ-TUNE is a two-stage fine-tuning strategy that disentangles knowledge acquisition from skill learning using LoRA modules. First, a knowledge LoRA is trained on prerequisite knowledge dataset Dknow. Then, with the knowledge LoRA frozen, a skill LoRA is trained on the task dataset DT. This prevents knowledge inconsistency from interfering with skill learning. The method can be enhanced with fictitious synthetic data to create multiple knowledge LoRA versions, forcing the skill LoRA to ground responses in the active knowledge LoRA rather than relying on pre-trained knowledge. During inference, only the skill LoRA is used, eliminating knowledge pollution.

## Key Results
- Biography generation: 45.30% accuracy with PREREQ-TUNE vs 32.70% with standard SFT
- Medical QA: PREREQ-TUNE outperforms FLAME, FACTTUNE, and RL baselines on MeQSum dataset
- Short QA: Fictitious data version achieves 62.70% accuracy vs 58.50% for real data baseline
- Multi-version grounding: Skill LoRA correctly grounds in active knowledge LoRA over 90% of time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prerequisite learning disentangles knowledge acquisition from skill learning by training LoRA modules separately
- Mechanism: First train a knowledge LoRA on prerequisite knowledge dataset Dknow, then freeze it and train a skill LoRA on task dataset DT. This prevents knowledge inconsistency from interfering with skill learning.
- Core assumption: Knowledge and skills can be effectively separated into different LoRA modules
- Evidence anchors: [abstract], [section 3.2], [corpus]
- Break condition: If knowledge and skills cannot be separated cleanly into different LoRA modules, or if LoRA modules interfere with each other during training

### Mechanism 2
- Claim: Multi-version PREREQ-TUNE forces groundedness by requiring skill LoRA to produce different answers based on different knowledge LoRA versions
- Mechanism: Create multiple knowledge LoRA versions with conflicting information about same entities, then train skill LoRA to match answers to corresponding knowledge LoRA. This forces the model to ground responses in the knowledge LoRA rather than hallucinating.
- Core assumption: The skill LoRA can learn to ground responses in the active knowledge LoRA rather than falling back to pre-trained knowledge
- Evidence anchors: [abstract], [section 3.3], [section 4.3]
- Break condition: If the skill LoRA fails to ground in the knowledge LoRA and instead relies on pre-trained knowledge, or if knowledge pollution occurs

### Mechanism 3
- Claim: Fictitious synthetic data prevents knowledge pollution by ensuring removed knowledge is truly unknown
- Mechanism: Using completely fictitious entities means that when knowledge LoRA is removed during inference, the knowledge is guaranteed unknown rather than possibly still in pre-trained weights. This prevents skill LoRA from accidentally learning fictitious knowledge.
- Core assumption: Knowledge from fictitious data cannot exist in pre-trained weights
- Evidence anchors: [abstract], [section 3.3], [section 4.4]
- Break condition: If fictitious knowledge accidentally overlaps with real knowledge in pre-trained weights, or if skill LoRA learns to hallucinate fictitious knowledge

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation) fine-tuning
  - Why needed here: PREREQ-TUNE relies on LoRA modules to separately store knowledge and skills, allowing for modular fine-tuning
  - Quick check question: What is the key advantage of using LoRA over full fine-tuning for PREREQ-TUNE's two-stage approach?

- Concept: Knowledge disentanglement in LLM fine-tuning
  - Why needed here: Understanding why separating knowledge acquisition from skill learning is critical for reducing hallucinations
  - Quick check question: How does knowledge inconsistency between pre-training and fine-tuning lead to hallucinations?

- Concept: Synthetic data generation for training
  - Why needed here: PREREQ-TUNE uses fictitious synthetic data to create controlled knowledge scenarios and enable multi-version training
  - Quick check question: Why can't real data be used for multi-version PREREQ-TUNE in the same way as fictitious data?

## Architecture Onboarding

- Component map: Base LLM (θ0) -> Knowledge LoRA (Δθknow) -> Skill LoRA (Δθskill)
- Critical path:
  1. Create dataset pairs (Dknow, DT) using top-down or bottom-up strategy
  2. Prerequisite learning: Train knowledge LoRA on Dknow
  3. SFT: Freeze knowledge LoRA, train skill LoRA on DT
  4. Inference: Drop knowledge LoRA, use only skill LoRA
- Design tradeoffs:
  - Using fictitious vs real data: Fictitious enables stronger control but may miss real-world edge cases
  - Statement-based vs passage-based knowledge: Statement-based provides finer control but may be harder to generate
  - Number of knowledge versions: More versions improve grounding but increase training complexity
- Failure signatures:
  - High knowledge pollution: Skill LoRA learns knowledge even without knowledge LoRA present
  - Poor generalization: Skill LoRA fails to ground in pre-trained knowledge when knowledge LoRA is removed
  - Training instability: LoRA modules interfere with each other during training
- First 3 experiments:
  1. Test knowledge grounding: Plug different knowledge LoRAs into trained skill LoRA and verify it produces matching answers
  2. Test knowledge pollution: Remove knowledge LoRA and check if skill LoRA answers questions correctly
  3. Test multi-version grounding: Create conflicting knowledge versions and verify skill LoRA grounds in the active version

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the knowledge LoRA learned through prerequisite training contain knowledge that interferes with the base model's pre-trained knowledge?
- Basis in paper: Inferred
- Why unresolved: The paper hypothesizes that the skill LoRA can ground responses in the base model's pre-trained knowledge when the knowledge LoRA is removed, but this remains an assumption without direct evidence.
- What evidence would resolve it: Direct comparison of knowledge representations between base model and knowledge LoRA, or ablation studies showing performance degradation when using knowledge LoRA that overlaps with base model knowledge.

### Open Question 2
- Question: How does the model's performance scale with the number of knowledge versions (K) in multi-version PREREQ-TUNE?
- Basis in paper: Inferred
- Why unresolved: The paper mentions creating multiple knowledge versions for multi-version PREREQ-TUNE but doesn't systematically study how performance changes as K increases.
- What evidence would resolve it: Experiments varying K from 1 to 10+ while measuring accuracy and grounding quality, with analysis of computational costs.

### Open Question 3
- Question: What is the optimal balance between real and fictitious data in the mixed training strategy for short QA tasks?
- Basis in paper: Explicit
- Why unresolved: The paper uses a combination of fictitious and real data for short QA but doesn't explore different mixing ratios.
- What evidence would resolve it: Systematic experiments varying the proportion of real to fictitious data while measuring factuality and grounding.

## Limitations
- The evaluation focuses on biography generation, medical QA, and short QA tasks, limiting generalizability to other domains like code generation or creative writing.
- Multi-version approach requires fictitious data and increases computational overhead, potentially limiting practical applicability.
- Long-term stability of knowledge-skill disentanglement after continued fine-tuning on new tasks remains untested.

## Confidence

**Low Confidence** on generalizability across diverse tasks and domains beyond the evaluated biography generation, medical QA, and short QA tasks.

**Medium Confidence** on scalability of the multi-version approach, as computational overhead increases significantly with more knowledge versions, though the paper only demonstrates success with two versions.

**Medium Confidence** on the effectiveness of knowledge-skill disentanglement, supported by experimental results showing improved accuracy, but relying on the assumption that knowledge and skills can be cleanly separated.

## Next Checks

1. **Knowledge-Skill Interference Test**: Train PREREQ-TUNE on a task with clearly separable knowledge and skills, then deliberately create interference by introducing overlapping knowledge in both LoRA modules. Measure whether the disentanglement breaks down and hallucinations increase.

2. **Cross-Domain Generalization Test**: Apply PREREQ-TUNE to a fundamentally different domain (e.g., code generation or mathematical reasoning) and compare performance against baselines. This would validate whether the knowledge-skill separation assumption holds beyond the evaluated domains.

3. **Long-Term Stability Evaluation**: Fine-tune PREREQ-TUNE models on additional tasks over multiple iterations, then evaluate whether the original knowledge-skill disentanglement degrades. This would assess the method's robustness to continued adaptation.