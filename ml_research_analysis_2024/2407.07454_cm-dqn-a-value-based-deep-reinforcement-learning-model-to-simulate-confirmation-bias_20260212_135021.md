---
ver: rpa2
title: 'CM-DQN: A Value-Based Deep Reinforcement Learning Model to Simulate Confirmation
  Bias'
arxiv_id: '2407.07454'
source_url: https://arxiv.org/abs/2407.07454
tags:
- bias
- learning
- confirmation
- reward
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces CM-DQN, a deep reinforcement learning model
  designed to simulate confirmation bias in decision-making tasks with continuous
  states and discrete actions. The core idea is to apply different update strategies
  for positive and negative prediction errors, inspired by how individuals with confirmation
  bias weigh good and bad outcomes differently.
---

# CM-DQN: A Value-Based Deep Reinforcement Learning Model to Simulate Confirmation Bias

## Quick Facts
- arXiv ID: 2407.07454
- Source URL: https://arxiv.org/abs/2407.07454
- Reference count: 3
- Primary result: CM-DQN with confirmatory bias (αC > αD) achieved highest learning rewards in Lunar Lander and multi-armed bandit experiments

## Executive Summary
This paper introduces CM-DQN, a deep reinforcement learning algorithm that simulates confirmation bias by applying asymmetric update strategies to positive and negative prediction errors. The model uses larger learning rates for positive errors (confirming outcomes) and smaller rates for negative errors (disconfirming outcomes), effectively reinforcing beliefs aligned with good outcomes. Experiments in Lunar Lander and multi-armed bandit environments demonstrate that confirmatory bias improves learning performance compared to disconfirmatory bias or non-bias baselines. The bias constraint parameter K plays a critical role in stabilizing learning, with K=1e-1 yielding optimal results.

## Method Summary
CM-DQN extends standard DQN by implementing different update strategies for positive and negative temporal difference (TD) errors. For confirmatory bias, positive TD errors trigger gradient descent with step size αC, while negative TD errors trigger gradient ascent with step size αD=K·αC. The algorithm uses experience replay and target networks for stability. Experiments were conducted in Lunar Lander (continuous states, discrete actions) and a 2-armed bandit task, with ablation studies on the bias constraint parameter K across multiple orders of magnitude.

## Key Results
- Confirmatory bias (αC > αD) achieved highest testing rewards in both Lunar Lander and multi-armed bandit experiments
- In bandit task, optimal performance occurred when αC exceeded αD across tested learning rates
- K=1e-1 provided optimal bias constraint in Lunar Lander ablation study
- Disconfirmatory bias (αC < αD) produced worse performance than both confirmatory and non-bias conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different update rates for positive and negative prediction errors simulate confirmation bias by reinforcing beliefs aligned with good outcomes more strongly than bad outcomes.
- Mechanism: When prediction error δ is positive (better-than-expected), the algorithm uses larger learning rate αC; when δ is negative (worse-than-expected), it uses smaller rate αD. This asymmetry causes value function to adapt more strongly to confirming evidence.
- Core assumption: Learning rate asymmetry alone is sufficient to mimic human-like confirmation bias in RL without explicit belief modeling.
- Evidence anchors: [abstract] "apply different update strategies for positive or negative prediction errors", [section] "αC > αD" for confirmatory bias
- Break condition: If αC ≈ αD or both are too small, asymmetry disappears and bias effects vanish.

### Mechanism 2
- Claim: Gradient ascent on negative TD errors (when confirmatory bias is active) acts as "belief reinforcement" step that counteracts normal gradient descent on positive errors.
- Mechanism: For confirmatory bias, negative TD errors trigger gradient ascent step with step size αD, while positive TD errors use gradient descent with step size αC. This asymmetric treatment shifts value estimates toward beliefs agent already holds.
- Core assumption: Gradient ascent can be safely combined with gradient descent in same update without destabilizing learning, provided K scales αD appropriately.
- Evidence anchors: [section] "Perform a gradient ascent with step size αd on TD2 error ... where αd = Kαc", [section] "K = 1e-1 has highest testing reward"
- Break condition: If K is too large, gradient ascent dominates and destabilizes learning; if too small, bias effect is negligible.

### Mechanism 3
- Claim: In discrete action tasks, softmax action selection with low temperature preserves bias effects by making agent more deterministic, amplifying impact of value differences.
- Mechanism: Temperature parameter set to 0.1 in bandit experiment reduces exploration randomness, so higher valued arms (shaped by biased updates) are chosen more often, reinforcing bias cycle.
- Core assumption: Lower temperature does not eliminate exploration entirely; it only reduces noise enough to let bias shape policy.
- Evidence anchors: [section] "We set the temperature parameter to be 0.1", [section] "heatmap ... when αC is larger than αD, agent tends to learn better result"
- Break condition: If temperature → 0, policy becomes brittle and may lock into suboptimal actions before learning complete.

## Foundational Learning

- Concept: Bellman equation and Q-learning update rule
  - Why needed here: CM-DQN extends Q-learning to continuous states using neural nets; understanding TD target is essential to see how bias modifies gradient
  - Quick check question: In Bellman optimality equation, what does max_a' Q(s',a') represent?

- Concept: Experience replay and target networks
  - Why needed here: Stabilizes learning in CM-DQN; without them, dual ascent/descent updates could diverge
  - Quick check question: Why do we update θ_target slowly (τ << 1) instead of copying θ directly?

- Concept: Gradient ascent vs. descent in deep RL
  - Why needed here: CM-DQN uses gradient ascent on negative TD errors to simulate confirmation bias; mixing ascent/descent is unusual and requires careful tuning
  - Quick check question: What would happen if we applied gradient ascent to all TD errors regardless of sign?

## Architecture Onboarding

- Component map: Input state → Neural net Q(s,a;θ) → TD error calculation → Conditional gradient step (descent for positive, ascent for negative under bias) → Replay buffer update → Target network sync → Action selection via ε-greedy/softmax
- Critical path: Forward pass → TD error → Bias condition check → Gradient step → Parameter update → Target sync
- Design tradeoffs: Using gradient ascent for bias increases representational flexibility but risks instability; simpler alternative is to just scale αC/αD in tabular settings
- Failure signatures: Oscillating rewards, divergence of Q-values, or agent getting stuck selecting same sub-optimal action repeatedly
- First 3 experiments:
  1. Run CM-DQN with αC = αD = 0.01, K = 0, confirm baseline performance matches DQN
  2. Enable confirmatory bias (αC > αD, K = 1e-1), measure reward improvement in LunarLander
  3. Test disconfirmatory bias (αC < αD, K = 1e-1), verify reward drops relative to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the bias constraint parameter K in CM-DQN influence the agent's learning dynamics in environments with different reward structures?
- Basis in paper: [explicit] The paper discusses the role of K in controlling step size of gradient ascent and its impact on learning outcomes, particularly noting that K=1e-1 yielded optimal performance in Lunar Lander environment
- Why unresolved: The paper does not explore how K's influence varies with different reward structures or environments, limiting understanding of its broader applicability
- What evidence would resolve it: Conducting experiments across diverse environments with varying reward structures to analyze how different values of K affect learning dynamics and outcomes

### Open Question 2
- Question: Can the CM-DQN algorithm be effectively extended to handle continuous action spaces, and what modifications would be necessary?
- Basis in paper: [inferred] The paper mentions potential for future work in integrating confirmation model with Deep Deterministic Policy Gradient (DDPG) to study confirmation bias in continuous state and continuous action decision processes
- Why unresolved: The paper does not provide experimental results or theoretical analysis on extending CM-DQN to continuous action spaces
- What evidence would resolve it: Implementing CM-DQN with DDPG or similar algorithms and evaluating its performance in environments with continuous action spaces

### Open Question 3
- Question: How does the performance of CM-DQN with confirmatory bias compare to other reinforcement learning algorithms that incorporate cognitive biases, such as optimism or pessimism?
- Basis in paper: [inferred] The paper highlights significance of understanding confirmation bias in reinforcement learning but does not compare CM-DQN's performance with other bias-incorporating algorithms
- Why unresolved: The paper focuses on effects of confirmation bias without benchmarking against other cognitive biases in reinforcement learning
- What evidence would resolve it: Conducting comparative studies between CM-DQN and other bias-incorporating algorithms to evaluate their respective impacts on learning efficiency and decision-making quality

## Limitations

- The study lacks detailed neural network architecture specifications beyond a single layer dimension (128 units), making exact reproduction challenging
- The gradient ascent implementation for bias simulation in continuous state spaces is described but not fully specified in terms of loss function formulation
- The ablation study on parameter K, while showing optimal performance at K=1e-1, does not explore stability implications of this choice across different environments

## Confidence

- High confidence: The core mechanism of asymmetric learning rates (αC > αD) for simulating confirmation bias is well-supported by experimental results in both Lunar Lander and multi-armed bandit tasks
- Medium confidence: The effectiveness of gradient ascent on negative TD errors combined with descent on positive errors, as this mixing approach is unusual in deep RL and requires careful tuning
- Medium confidence: The optimal value of K=1e-1 for bias constraint, as this is based on limited ablation testing and may not generalize to all environments

## Next Checks

1. Conduct stability analysis of CM-DQN across different K values in Lunar Lander to determine the range where learning remains stable
2. Implement and test a variant where only learning rate scaling (not gradient ascent) is used to simulate confirmation bias, comparing performance
3. Perform ablation on neural network architecture depth/width to assess sensitivity of bias effects to representational capacity