---
ver: rpa2
title: Improving Mathematical Reasoning Capabilities of Small Language Models via
  Feedback-Driven Distillation
arxiv_id: '2411.14698'
source_url: https://arxiv.org/abs/2411.14698
tags:
- reasoning
- mathematical
- distillation
- slms
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of deploying large language models
  in resource-constrained environments by transferring reasoning capabilities to small
  language models (SLMs). The authors propose a Feedback-Driven Distillation (FDD)
  framework that iteratively constructs a complex and diverse distillation dataset.
---

# Improving Mathematical Reasoning Capabilities of Small Language Models via Feedback-Driven Distillation

## Quick Facts
- arXiv ID: 2411.14698
- Source URL: https://arxiv.org/abs/2411.14698
- Reference count: 38
- Authors: Xunyu Zhu, Jian Li, Can Ma, Weiping Wang
- Key outcome: Enables small language models to achieve state-of-the-art performance in mathematical reasoning tasks through iterative feedback-driven distillation

## Executive Summary
This paper addresses the challenge of deploying large language models in resource-constrained environments by transferring reasoning capabilities to small language models (SLMs). The authors propose a Feedback-Driven Distillation (FDD) framework that iteratively constructs a complex and diverse distillation dataset. The method involves generating mathematical questions based on the performance of SLMs on existing questions, with easy questions leading to more complex variations and hard questions leading to similar complexity questions. The approach uses a multi-round distillation paradigm to progressively improve the mathematical reasoning abilities of SLMs. Experimental results show that the method enables SLMs to achieve state-of-the-art performance in mathematical reasoning tasks, with a FlanT5-Large model achieving 49.43% accuracy on GSM8K and 67.55% average accuracy across multiple datasets.

## Method Summary
The Feedback-Driven Distillation framework consists of three main stages: initialization, question generation, and fine-tuning, repeated in multiple rounds. The process begins with an LLM (ChatGPT) generating Program-of-Thought (PoT) rationales for mathematical questions in the GSM8K dataset. These PoTs are used to fine-tune the SLM. The fine-tuned SLM then classifies questions as easy or hard based on performance. Easy questions trigger the generation of more complex variations, while hard questions lead to similar complexity questions. This enriched dataset is used to fine-tune the SLM from scratch. The multi-round paradigm repeats this process, progressively improving the SLM's reasoning capabilities by focusing on its weaknesses and expanding the complexity of the training data.

## Key Results
- FlanT5-Large achieves 49.43% accuracy on GSM8K, significantly outperforming previous SLM approaches
- The multi-round distillation paradigm consistently improves performance across all SLM sizes
- Average accuracy of 67.55% across GSM8K, ASDiv, SVAMP, and MultiArith datasets demonstrates strong generalization
- The framework achieves state-of-the-art performance while using significantly fewer parameters than competing approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative dataset expansion based on student performance feedback improves SLM mathematical reasoning.
- Mechanism: The method uses SLMs' performance on existing questions to guide LLM generation of new, more targeted questions. Easy questions lead to more complex variations, while hard questions lead to similar complexity questions. This creates a progressively more challenging and diverse training dataset.
- Core assumption: Performance-based classification of questions (easy/hard) is a reliable indicator of knowledge gaps and learning readiness.
- Evidence anchors:
  - [abstract] "We classify problems into easy and hard categories based on SLM performance. For easy problems, LLMs generate more complex variations, while for hard problems, new questions of similar complexity are synthesized."
  - [section 3.2] "We use the fine-tuned SLM from the Initialization Stage to perform inference on each data in the mathematical distillation dataset, generating corresponding answer. We then compare the generated answer with the gold answer. If the generated answer matches the gold answer, we consider the question to be easy, meaning the SLM have successfully learned how to solve it. Conversely, if the generated answer does not match, we classify the question as hard."
- Break condition: If the SLM cannot reliably distinguish between easy and hard questions, the feedback mechanism breaks down and the generated questions may not be appropriately targeted.

### Mechanism 2
- Claim: Using Program-of-Thought (PoT) instead of Chain-of-Thought (CoT) improves SLM reasoning by offloading computation.
- Mechanism: PoT focuses the SLM on generating Python programs rather than performing all reasoning steps internally. The actual computation is delegated to an external Python interpreter, reducing the cognitive load on the SLM.
- Core assumption: SLMs perform better when they can offload computational steps to external tools rather than handling everything internally.
- Evidence anchors:
  - [section 3.1] "We guide LLMs to generate rationales in PoT format rather than CoT format. The primary advantage of PoT is that it focuses solely on generating the program needed to solve the question, while delegating the actual computational process to an external Python interpreter."
- Break condition: If the Python interpreter fails or introduces latency, or if the SLM cannot effectively translate problems into executable programs.

### Mechanism 3
- Claim: Multi-round distillation progressively improves reasoning by continuously updating the dataset based on current SLM capabilities.
- Mechanism: Each round uses the current SLM's performance to generate new questions, which are added to the dataset. This ensures the dataset evolves with the SLM's growing capabilities and avoids redundancy by excluding questions the SLM has already mastered.
- Core assumption: SLMs learn progressively, and a dataset that evolves with their current capabilities is more effective than a static dataset.
- Evidence anchors:
  - [section 3.4] "In each round, we integrate the hard questions and generated questions from the previous round, leveraging the fine-tuned SLMs from that round to reclassify these questions into hard and easy categories."
- Break condition: If the SLM plateaus in performance, additional rounds may not yield improvements and could waste computational resources.

## Foundational Learning

- Concept: Knowledge distillation and teacher-student model training
  - Why needed here: The entire framework relies on transferring reasoning capabilities from LLMs (teachers) to SLMs (students) through carefully constructed datasets.
  - Quick check question: What is the fundamental difference between knowledge distillation and supervised fine-tuning?

- Concept: Chain-of-Thought vs Program-of-Thought reasoning
  - Why needed here: The paper explicitly chooses PoT over CoT for efficiency reasons, and understanding this distinction is crucial for implementing the framework correctly.
  - Quick check question: How does offloading computation to an external Python interpreter change the SLM's task during reasoning?

- Concept: Iterative dataset construction and feedback loops
  - Why needed here: The core innovation involves using SLM performance to guide the generation of new training data, requiring understanding of how to create and maintain such feedback loops.
  - Quick check question: What metrics would you use to determine when to stop adding new questions to the dataset?

## Architecture Onboarding

- Component map: LLM (ChatGPT/gpt-3.5-turbo) -> Question Generation -> Python Interpreter -> SLM (FlanT5 variants) -> Classification Module -> Dataset Construction -> Fine-tuning

- Critical path: Dataset construction → SLM fine-tuning → Performance evaluation → Question generation → Dataset expansion → Repeat

- Design tradeoffs:
  - PoT vs CoT: PoT reduces SLM burden but requires reliable Python execution
  - Question generation frequency: More rounds improve performance but increase cost
  - Dataset size: Larger datasets improve coverage but increase training time

- Failure signatures:
  - SLM performance plateaus despite additional rounds
  - Generated questions are too similar to existing ones (checked via ROUGE-L similarity)
  - Python interpreter produces inconsistent results for identical PoTs

- First 3 experiments:
  1. Implement single-round distillation with PoT and measure GSM8K performance baseline
  2. Add question generation for easy questions only and measure impact on performance
  3. Implement full multi-round approach and compare against single-round results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of generated questions impact the mathematical reasoning performance of SLMs in the Feedback-Driven Distillation framework?
- Basis in paper: [inferred] The paper discusses the importance of data quality and quantity for effective knowledge distillation, but does not explicitly analyze the quality of generated questions.
- Why unresolved: The paper focuses on the quantity and diversity of generated questions but does not investigate how the quality of these questions affects the performance of SLMs.
- What evidence would resolve it: Conduct experiments comparing the performance of SLMs trained on high-quality versus low-quality generated questions, measuring the impact on mathematical reasoning accuracy.

### Open Question 2
- Question: What is the optimal number of reasoning paths per question for maximizing the mathematical reasoning performance of SLMs?
- Basis in paper: [explicit] The paper mentions that generating multiple reasoning paths can improve performance but does not determine the optimal number.
- Why unresolved: The paper explores the effect of varying the number of reasoning paths but does not identify the point of diminishing returns or the optimal number for best performance.
- What evidence would resolve it: Perform experiments with different numbers of reasoning paths per question and analyze the performance gains to find the optimal number.

### Open Question 3
- Question: How does the multi-round distillation paradigm affect the scalability and efficiency of the Feedback-Driven Distillation framework?
- Basis in paper: [explicit] The paper introduces a multi-round distillation paradigm but does not extensively analyze its impact on scalability and efficiency.
- Why unresolved: While the paper mentions the potential benefits of multiple rounds, it does not provide a detailed analysis of how this affects the overall scalability and efficiency of the framework.
- What evidence would resolve it: Conduct experiments comparing the performance and resource usage of the framework with different numbers of rounds to assess scalability and efficiency.

## Limitations

- The framework's effectiveness depends heavily on the quality and diversity of LLM-generated questions, which may vary across different LLMs or prompt engineering approaches
- The computational cost of multiple rounds of distillation, particularly the LLM interaction costs, is not fully explored in terms of scalability
- The generalization of results to domains beyond mathematical reasoning is not established, limiting the framework's applicability

## Confidence

- **High Confidence:** The core mechanism of using PoT instead of CoT for mathematical reasoning is well-supported by existing literature and the paper's results. The improvement from 35.88% to 49.43% accuracy on GSM8K for FlanT5-Large is substantial and statistically significant.

- **Medium Confidence:** The iterative dataset construction approach shows promise, but the paper lacks ablation studies to isolate the contribution of each component (PoT, question generation strategies, multi-round refinement). The claim that performance-based classification reliably identifies knowledge gaps needs further validation across diverse problem types.

- **Low Confidence:** The generalization of results to other domains beyond mathematical reasoning is not established. The paper focuses exclusively on GSM8K and related math datasets, leaving open questions about applicability to logical reasoning, commonsense reasoning, or other knowledge-intensive tasks.

## Next Checks

1. **Ablation Study on Distillation Components:** Run experiments isolating PoT vs CoT, single-round vs multi-round distillation, and with/without question generation to quantify each component's contribution to the final performance.

2. **Robustness Testing of Classification System:** Systematically introduce noise into the easy/hard classification (e.g., randomly misclassifying 10-30% of questions) and measure the impact on final SLM performance to assess the feedback mechanism's reliability.

3. **Cross-Domain Generalization Test:** Apply the FDD framework to a non-mathematical reasoning dataset (e.g., strategyQA or CommonsenseQA) to evaluate whether the approach generalizes beyond mathematical problem-solving.