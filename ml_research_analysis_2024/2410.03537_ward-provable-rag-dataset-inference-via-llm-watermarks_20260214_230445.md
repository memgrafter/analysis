---
ver: rpa2
title: 'Ward: Provable RAG Dataset Inference via LLM Watermarks'
arxiv_id: '2410.03537'
source_url: https://arxiv.org/abs/2410.03537
tags:
- ward
- rag-di
- queries
- zhao
- farad
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces WARD, a watermarking-based method for detecting\
  \ unauthorized use of datasets in Retrieval-Augmented Generation (RAG) systems.\
  \ The authors formalize the RAG Dataset Inference (RAG-DI) problem, create FARAD\u2014\
  a novel dataset designed to realistically benchmark RAG-DI methods\u2014and adapt\
  \ prior RAG Membership Inference baselines to the black-box RAG-DI setting."
---

# Ward: Provable RAG Dataset Inference via LLM Watermarks

## Quick Facts
- **arXiv ID**: 2410.03537
- **Source URL**: https://arxiv.org/abs/2410.03537
- **Reference count**: 40
- **Primary result**: WARD achieves 100% accuracy detecting unauthorized RAG dataset usage via statistical watermark detection, outperforming all baselines.

## Executive Summary
This paper introduces WARD, a watermarking-based method for detecting unauthorized use of datasets in Retrieval-Augmented Generation (RAG) systems. The authors formalize the RAG Dataset Inference (RAG-DI) problem, create FARAD—a novel dataset designed to realistically benchmark RAG-DI methods—and adapt prior RAG Membership Inference baselines to the black-box RAG-DI setting. WARD proactively watermarks data owner documents and audits RAG systems by detecting watermark traces in LLM responses via statistical tests, achieving 100% accuracy on both easy (no fact redundancy) and hard (fact redundancy) settings, outperforming all baselines in accuracy, query efficiency, and robustness. WARD also provides rigorous statistical guarantees with controlled false positive rates, even under strong defenses.

## Method Summary
WARD solves RAG Dataset Inference by proactively watermarking documents at creation time using red-green token techniques, then auditing RAG systems through statistical detection of these watermarks in responses. The method generates questions based on each document, queries the RAG system, and uses z-score tests on green token frequencies to compute joint p-values across multiple queries. This approach is evaluated against adapted baselines (SIB, AAG, FACTS) on the novel FARAD dataset containing 3591 groups of 4 articles each with controlled fact redundancy. The evaluation demonstrates WARD's superiority in both easy and hard settings across different RAG configurations and system prompts.

## Key Results
- WARD achieves 100% accuracy in detecting unauthorized dataset usage in both easy (no fact redundancy) and hard (fact redundancy) settings
- All baseline methods (SIB, AAG, FACTS) fail to perform consistently well in the hard setting, inducing both false positives and negatives
- WARD provides rigorous statistical guarantees with controlled false positive rates via joint p-value computation
- The method maintains high accuracy even under defensive system prompts designed to prevent watermark detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WARD uses red-green watermarking to make ownership traces detectable through statistical z-score tests
- Mechanism: During text generation, token vocabulary is split into encouraged green and discouraged red tokens. Logits of green tokens are increased by δ, boosting their probability of being sampled. Detection uses z-score based on observed green token ratio vs expected γ
- Core assumption: The watermark signal remains detectable after propagation through RAG pipeline
- Evidence anchors:
  - [abstract] "WARD proactively watermarks data owner documents and audits RAG systems by detecting watermark traces in LLM responses via statistical tests"
  - [section 2] "We use the z-score z = |s|g − γTp γ(1 − γ)T , where |s|g is the number of green tokens in a given text s, and T = |s|"
  - [corpus] Weak - only 1 neighbor paper mentions watermarking, no direct comparisons
- Break condition: Text transformations destroy watermark signal or change green token ratio below detection threshold

### Mechanism 2
- Claim: Joint p-value aggregation allows detection of weak watermark signals across multiple queries
- Mechanism: Individual responses may not individually exceed detection threshold, but joint test across n responses can reject null hypothesis with controlled false positive rate
- Core assumption: Token independence can be approximated by ignoring duplicate h-grams across responses
- Evidence anchors:
  - [abstract] "WARD...audits RAG systems by detecting watermark traces in LLM responses via statistical tests, achieving 100% accuracy"
  - [section 4] "Following Sander et al. (2024), after n queries we compute a joint p-value R = {r1, ..., rn}"
  - [corpus] Weak - no neighbor papers discuss joint watermark detection across multiple queries
- Break condition: Dependencies between tokens violate statistical test assumptions

### Mechanism 3
- Claim: Watermarking provides reliable signal while baselines fail under fact redundancy
- Mechanism: Unlike similarity-based methods (AAG, SIB, FACTS) that struggle when documents share similar facts, watermarking creates unique traceable patterns independent of content similarity
- Core assumption: Fact redundancy makes semantic similarity methods unreliable for dataset inference
- Evidence anchors:
  - [abstract] "WARD consistently outperforms all baselines...achieving higher accuracy, superior query efficiency and robustness"
  - [section 5.1] "all baselines fail to perform consistently well, inducing both false positives and negatives...only WARD achieves 100% accuracy"
  - [corpus] Moderate - neighbor papers discuss watermarking but not specifically for RAG dataset inference
- Break condition: Watermark signal cannot propagate through RAG pipeline or is removed by defenses

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) pipeline
  - Why needed here: Understanding how documents flow from corpus through retrieval to LLM context is essential for understanding watermark propagation
  - Quick check question: In RAG, what happens to retrieved documents before they reach the LLM?

- Concept: Statistical hypothesis testing and p-values
  - Why needed here: WARD's detection mechanism relies on computing p-values and joint tests across multiple queries
  - Quick check question: What does a p-value represent in the context of watermark detection?

- Concept: Text watermarking techniques
  - Why needed here: Understanding red-green watermarking is fundamental to understanding how WARD works
  - Quick check question: How does increasing δ affect the probability of sampling green tokens?

## Architecture Onboarding

- Component map: Data owner -> Watermarked documents -> RAG system -> LLM responses -> Token counting -> Statistical testing -> Decision
- Critical path:
  1. Data owner watermarks their documents
  2. WARD generates questions based on each document
  3. Questions are sent to RAG system
  4. Responses are collected and tokenized
  5. Green token counts are computed
  6. Joint p-value is calculated
  7. Decision is made based on threshold
- Design tradeoffs:
  - Watermark strength (δ) vs text quality
  - Context width (h) vs robustness to transformations
  - Number of queries vs detection speed
  - Joint testing vs computational cost
- Failure signatures:
  - Low green token ratio in responses indicates watermark not propagating
  - High false positive rate suggests statistical test assumptions violated
  - Inconsistent accuracy across different RAG configurations indicates sensitivity issues
- First 3 experiments:
  1. Test watermark propagation on single document through RAG pipeline
  2. Measure detection accuracy with varying number of queries
  3. Evaluate robustness against different system prompts and defenses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would WARD perform in settings where the watermark signal is degraded not just by the RAG pipeline but also by the presence of adversarial transformations applied to the RAG system's responses?
- Basis in paper: [inferred] The paper discusses watermark robustness under "moderate text transformations" and mentions future work on combining WARD with other watermarking approaches, suggesting open questions about more extreme transformations.
- Why unresolved: The paper primarily focuses on natural propagation of watermarks through the RAG pipeline. While it mentions defenses like MEMFREE that prevent n-gram overlap, it doesn't explore more sophisticated adversarial attacks that could target the watermark detection mechanism itself.
- What evidence would resolve it: Experiments testing WARD against adversarially crafted responses designed to break watermark detection while maintaining semantic coherence would clarify the method's limits.

### Open Question 2
- Question: What is the optimal strategy for selecting the number of queries per document (qpd) to maximize both accuracy and efficiency across different types of RAG corpora?
- Basis in paper: [explicit] The paper presents preliminary results on qpd in Figure 9 and mentions "a promising avenue for future work," indicating this is an open research direction.
- Why unresolved: The paper only provides limited ablation results showing diminishing returns for high qpd values, but doesn't provide a systematic framework for choosing qpd based on corpus characteristics like size, redundancy, or topic diversity.
- What evidence would resolve it: A comprehensive study mapping qpd performance to corpus properties and developing guidelines for qpd selection would address this gap.

### Open Question 3
- Question: How does WARD's performance degrade when the percentage of documents from Ddo that are actually present in the RAG corpus (D) drops below 20%, and what are the theoretical bounds on this degradation?
- Basis in paper: [explicit] The paper presents results showing some robustness at 20% inclusion (Figure 10) and mentions "an interesting avenue for future work" regarding relaxed RAG-DI assumptions.
- Why unresolved: The paper only tests down to 20% inclusion, showing initial false positives, but doesn't establish theoretical limits or explore the full degradation curve as inclusion percentage approaches zero.
- What evidence would resolve it: Theoretical analysis combined with experimental results showing performance across the full range of inclusion percentages would establish the method's practical limits.

## Limitations
- FARAD dataset construction details are underspecified, particularly exact prompts for fact extraction and article generation
- Statistical test assumptions rely on token independence approximation that remains unverified
- Baseline adaptations lack complete parameter specification, making fair evaluation uncertain
- Generalizability beyond tested RAG configurations remains unproven

## Confidence

**High confidence** in the core claim that watermarking provides a reliable signal for dataset inference that outperforms content-based methods under fact redundancy. This is supported by controlled experiments showing 100% accuracy versus baselines failing in hard settings.

**Medium confidence** in the statistical guarantees and robustness claims. While p-values and controlled false positive rates are demonstrated, the assumptions underlying the statistical tests (particularly token independence) require further validation.

**Low confidence** in generalizability beyond the specific RAG configurations tested. Performance may vary significantly with different LLMs, retrieval mechanisms, or document types not represented in FARAD.

## Next Checks

1. **Verify FARAD construction reproducibility**: Recreate a subset of FARAD using only the information provided, focusing on fact extraction prompts and article generation procedures. Measure fact redundancy levels and check for training data overlap.

2. **Test statistical assumptions**: Evaluate the independence approximation by measuring duplicate h-gram frequencies across responses in real usage scenarios. Compare joint p-value distributions with and without the independence assumption to quantify impact on false positive rates.

3. **Evaluate across diverse RAG configurations**: Test WARD's accuracy on RAG systems using different retrieval strategies (e.g., dense vs sparse retrieval), varying context window sizes, and with non-English documents to assess robustness beyond the tested configurations.