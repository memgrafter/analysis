---
ver: rpa2
title: Enhancing SLM via ChatGPT and Dataset Augmentation
arxiv_id: '2409.12599'
source_url: https://arxiv.org/abs/2409.12599
tags:
- dataset
- language
- augmentation
- training
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of improving small language
  models (SLMs) for natural language inference (NLI) tasks without the need for costly
  human annotation. The core method involves augmenting the ANLI dataset using ChatGPT-3.5-Turbo
  to generate two types of rationales: information extraction (SMANLI) and informed
  reasoning (EMANLI).'
---

# Enhancing SLM via ChatGPT and Dataset Augmentation

## Quick Facts
- arXiv ID: 2409.12599
- Source URL: https://arxiv.org/abs/2409.12599
- Reference count: 30
- Primary result: Models trained on augmented datasets achieve 1.3% and 2.3% higher classification accuracy compared to baseline T5-Small without rationales

## Executive Summary
This paper addresses the challenge of improving small language models (SLMs) for natural language inference (NLI) tasks without costly human annotation. The authors augment the ANLI dataset using ChatGPT-3.5-Turbo to generate two types of rationales: information extraction (SMANLI) and informed reasoning (EMANLI). These rationales are used to fine-tune a T5-Small model with a custom split loss function. The results show that models trained on the augmented datasets achieve 1.3% and 2.3% higher classification accuracy, respectively, compared to a baseline T5-Small model without rationales. The study demonstrates the potential of leveraging large language models for cost-effective dataset augmentation, leading to enhanced performance and efficiency in SLMs for complex NLP tasks.

## Method Summary
The authors augment the ANLI dataset using ChatGPT-3.5-Turbo to generate two types of rationales: information extraction (SMANLI) and informed reasoning (EMANLI). These rationales are then used to fine-tune a T5-Small model using a custom split loss function that balances label prediction and rationale generation. The fine-tuning process tests different combinations of split ratios (e.g., 0.25/0.75, 0.5/0.5, 0.75/0.25) and learning rates (6e-4, 1.2e-3, 2.4e-3) over 5 epochs. The models are evaluated on the ANLI test set, comparing classification accuracy against a baseline T5-Small model without rationales.

## Key Results
- T5-Small models trained on ANLI with SMANLI rationales achieved 1.3% higher classification accuracy than baseline
- T5-Small models trained on ANLI with EMANLI rationales achieved 2.3% higher classification accuracy than baseline
- Different rationale types (information extraction vs. informed reasoning) showed distinct impacts on model performance and BLEU scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic rationales from a teacher LLM improve the semantic reasoning capabilities of a student SLM.
- Mechanism: Knowledge distillation through dataset augmentation transfers complex reasoning patterns from the teacher to the student via synthetic rationales.
- Core assumption: The rationales generated by the teacher model capture valid and useful reasoning patterns that can be learned by the smaller model.
- Evidence anchors:
  - [abstract] "incorporation of synthetic rationales significantly improves the model's ability to comprehend natural language, leading to 1.3% and 2.3% higher classification accuracy"
  - [section] "the core differences in the impact of augmenting ANLI with different types of rationales on the training process"
- Break condition: If rationales contain hallucinated or irrelevant reasoning, the student model may learn incorrect patterns.

### Mechanism 2
- Claim: Split loss function balances label prediction and rationale generation during training.
- Mechanism: Custom loss function weights cross-entropy losses for labels and rationales differently, allowing the model to maintain classification accuracy while learning to generate rationales.
- Core assumption: The split loss function can effectively balance the dual objectives without one dominating the other.
- Evidence anchors:
  - [section] "custom split-loss defined in equation 2 to make use of the text-to-text architecture while maintaining adequate focus on the label"
  - [section] "SplitRatio = (FLabel, FRationale), where FLabel + FRationale = 1"
- Break condition: If split ratio is poorly chosen, the model may prioritize one objective at the expense of the other.

### Mechanism 3
- Claim: Different types of rationales (information extraction vs. informed reasoning) have distinct impacts on model performance.
- Mechanism: Information extraction rationales provide distilled facts while informed reasoning rationales provide contextual explanations, each influencing the model's learning process differently.
- Core assumption: The nature of the rationale type affects how the student model learns and generalizes.
- Evidence anchors:
  - [section] "variations in BLEU score between the two versions of the dataset, as well as the progression of the score throughout the training phase"
  - [section] "SMANLI achieves an overall much higher rationale score â€“ originating from the smaller possible variance in information extraction"
- Break condition: If rationale type is mismatched with the task, performance gains may be limited or negative.

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: The entire approach is applied to NLI tasks, so understanding premise-hypothesis relationships is fundamental.
  - Quick check question: What are the three possible relationships between premise and hypothesis in NLI?

- Concept: Knowledge Distillation
  - Why needed here: The method relies on transferring knowledge from a large model to a smaller one through synthetic data.
  - Quick check question: How does knowledge distillation differ from traditional supervised learning?

- Concept: Dataset Augmentation
  - Why needed here: The approach creates new training data by generating rationales, expanding the dataset without manual annotation.
  - Quick check question: What are the potential risks of synthetic data augmentation compared to human-annotated data?

## Architecture Onboarding

- Component map: T5-Small model -> Custom split loss function -> ANLI dataset (augmented with rationales) -> Evaluation on test set
- Critical path: Data augmentation -> Model training with split loss -> Evaluation and comparison to baseline
- Design tradeoffs: Balance between rationale quality and quantity, split ratio selection, and computational efficiency
- Failure signatures: Low BLEU scores indicating poor rationale generation, negligible performance improvement over baseline, overfitting to rationales
- First 3 experiments:
  1. Fine-tune T5-Small on baseline ANLI without rationales to establish performance baseline
  2. Fine-tune T5-Small on ANLI with SMANLI rationales using different split ratios
  3. Fine-tune T5-Small on ANLI with EMANLI rationales using different split ratios and learning rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the rationales generated by ChatGPT compare to human-generated rationales in terms of quality and effectiveness for improving SLM performance?
- Basis in paper: [explicit] The paper mentions that human-generated rationales have been used in other studies and that future work should involve a deeper analysis of the generated data to exclude potential biases and limitations.
- Why unresolved: The study only evaluates the quality of the augmented dataset by its effects on the student model's performance, not by comparing the generated rationales to human-generated ones.
- What evidence would resolve it: A direct comparison of the rationales generated by ChatGPT and human annotators, using established benchmarks for rationale quality and coherence.

### Open Question 2
- Question: What is the impact of using different teacher models (e.g., GPT-4, Claude) on the quality of the generated rationales and the performance of the student model?
- Basis in paper: [inferred] The paper suggests testing different teacher models in future work to see how they influence the student model's performance.
- Why unresolved: The study only uses ChatGPT-3.5-Turbo as the teacher model and does not explore the effects of using other models.
- What evidence would resolve it: Conducting experiments with different teacher models and comparing the resulting student model performance and rationale quality.

### Open Question 3
- Question: How does the size of the student model (e.g., T5-Small, T5-Base) affect the benefits gained from rationale-based knowledge distillation?
- Basis in paper: [explicit] The paper suggests testing differently sized models in future work to see how they respond to the augmentation process.
- Why unresolved: The study only uses T5-Small as the student model and does not explore the effects of using larger or smaller models.
- What evidence would resolve it: Comparing the performance of different-sized student models on the augmented datasets and analyzing the correlation between model size and rationale-based improvements.

### Open Question 4
- Question: What are the specific characteristics of the rationales that contribute most to improving the student model's performance?
- Basis in paper: [explicit] The paper mentions that the variations in BLEU score between the two versions of the dataset reveal differences in the impact of augmenting ANLI with different types of rationales on the training process.
- Why unresolved: The study does not provide a detailed analysis of the rationale characteristics that lead to better performance.
- What evidence would resolve it: Conducting an in-depth analysis of the generated rationales, identifying key features (e.g., length, complexity, relevance) that correlate with improved student model performance.

## Limitations

- Evaluation limited to single dataset (ANLI) and single model architecture (T5-Small)
- No ablation studies examining individual contribution of rationale types versus other factors
- Does not address potential biases introduced through synthetic data generation

## Confidence

- High Confidence: Technical feasibility of using ChatGPT to generate rationales and basic implementation of split loss function
- Medium Confidence: Performance improvements (1.3% and 2.3% accuracy gains) are statistically meaningful but may not generalize
- Low Confidence: Theoretical claims about why different rationale types produce distinct learning effects lack rigorous validation

## Next Checks

1. Test the augmentation approach on multiple NLI datasets (e.g., SNLI, MNLI) and non-NLI tasks to assess generalizability
2. Conduct ablation studies comparing synthetic rationales against other augmentation methods like back-translation or adversarial examples
3. Implement a controlled study with human evaluation of rationale quality and its correlation with downstream performance to validate the knowledge transfer mechanism