---
ver: rpa2
title: Uncertainty estimation via ensembles of deep learning models and dropout layers
  for seismic traces
arxiv_id: '2410.06120'
source_url: https://arxiv.org/abs/2410.06120
tags:
- training
- networks
- polarity
- data
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents Convolutional Neural Networks (CNNs) to classify
  seismic waveforms based on first-motion polarity. Multiple training settings were
  explored, varying optimizers, dropout layers, and training datasets.
---

# Uncertainty estimation via ensembles of deep learning models and dropout layers for seismic traces

## Quick Facts
- arXiv ID: 2410.06120
- Source URL: https://arxiv.org/abs/2410.06120
- Reference count: 15
- Primary result: CNNs with dropout and ensemble methods achieve >97.4% accuracy in seismic polarity classification while significantly improving uncertainty estimation

## Executive Summary
This study presents Convolutional Neural Networks (CNNs) to classify seismic waveforms based on first-motion polarity. Multiple training settings were explored, varying optimizers, dropout layers, and training datasets. Accuracies above 97.4% were achieved across all settings. A SOM visualization procedure successfully identified and removed mislabeled and ambiguous data points. Notably, dropout layers improved the network's capability to handle mislabeled data. Ensemble methods were introduced to improve uncertainty estimation, with ensembles outperforming individual networks. The uncertainty estimation capability of ensembles incorporating models with dropout layers was significantly enhanced. The study demonstrates the effectiveness of CNNs in determining first-motion polarities, the importance of data cleaning procedures, and the value of ensemble methods in uncertainty estimation.

## Method Summary
The study used the INSTANCE dataset containing seismic traces labeled with 'upward', 'downward', or 'undecidable' polarity. Eight CNN training sessions were conducted with variations in optimizer (SGD/ADAM), dropout inclusion (yes/no), and training dataset (complete/SOM-cleaned). The CNN architecture consisted of five convolutional layers followed by two fully connected layers with sigmoid output. Data cleaning employed Self-Organizing Maps (SOM) to identify and remove mislabeled or ambiguous traces through cyclic applications and manual inspection. Ensemble uncertainty estimation was performed by averaging predictions across all trained models.

## Key Results
- CNNs achieved accuracies above 97.4% across all training configurations
- Dropout layers significantly improved robustness to mislabeled examples, with SGD+dropout sessions correcting 310 (92%) mislabeled labels versus 290 maximum for other configurations
- Ensemble methods outperformed individual networks in uncertainty estimation, particularly when incorporating dropout models
- SOM-based cleaning successfully identified and removed ambiguous/mislabeled data points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensemble methods improve uncertainty estimation by averaging predictions across multiple trained models.
- Mechanism: Multiple models trained with different initializations and hyperparameters produce varied outputs for the same input. Averaging these outputs smooths overconfident predictions and better reflects epistemic uncertainty.
- Core assumption: Individual model predictions are independent and capture different aspects of the data distribution.
- Evidence anchors:
  - [abstract] "The ensemble method outperformed individual networks in uncertainty estimation."
  - [section] "We first compute the mean prediction of all networks on the complete test set with defined polarity... The values in the distribution of Fig. 4b appear to be filled more evenly, with the peaks much reduced."
  - [corpus] Weak or missing direct evidence for this specific ensemble averaging claim in corpus.
- Break condition: If models are highly correlated or trained identically, averaging provides little uncertainty gain.

### Mechanism 2
- Claim: Dropout layers improve robustness to mislabeled examples by preventing co-adaptation of neurons.
- Mechanism: During training, dropout randomly disables neurons, forcing the network to learn redundant, robust features that are less sensitive to noise or mislabeled data.
- Core assumption: Mislabeled data are not systematic but random noise that dropout can average out.
- Evidence anchors:
  - [abstract] "comparisons among different training settings revealed that the use of dropout improved the robustness of networks to mislabeled examples."
  - [section] "SGD&dropout training sessions, on average, were able to assign 310 (92%) of them to the correct class. Conversely, other training sessions corrected no more than 290 labels."
  - [corpus] Weak or missing direct evidence for dropout improving robustness in corpus.
- Break condition: If mislabeled data form a structured pattern, dropout cannot compensate.

### Mechanism 3
- Claim: Combining dropout with ensembles enhances uncertainty estimation beyond either method alone.
- Mechanism: Dropout during inference approximates Bayesian model averaging, and ensembles provide multiple such approximations. Their combination yields richer uncertainty estimates by sampling different model architectures and weight configurations.
- Core assumption: Dropout-induced stochasticity and ensemble diversity are complementary sources of uncertainty modeling.
- Evidence anchors:
  - [abstract] "we discovered that, even if individual models using dropout layers did not show a significant improvement in the distribution of predictions, the uncertainty estimation capability of ensembles incorporating models with dropout layers was significantly enhanced."
  - [section] "The best estimation of uncertainty is achieved when using the complete INSTANCE training set with SGD&dropout setting... which even outperforms the ensemble of all the training sessions."
  - [corpus] Weak or missing direct evidence for this synergistic claim in corpus.
- Break condition: If dropout variance is too low or ensembles too similar, synergy collapses.

## Foundational Learning

- Concept: Aleatoric vs. epistemic uncertainty
  - Why needed here: The study explicitly distinguishes between stochastic noise (aleatoric) and lack of knowledge (epistemic) in model uncertainty, which guides evaluation and method choice.
  - Quick check question: What type of uncertainty cannot be reduced by more data?
- Concept: Dropout as approximate Bayesian inference
  - Why needed here: Dropout is used both for regularization and for estimating model uncertainty by sampling stochastic forward passes during inference.
  - Quick check question: How does dropout approximate sampling from a posterior distribution?
- Concept: Self-Organizing Maps (SOM) for data cleaning
  - Why needed here: SOM is used to identify ambiguous or mislabeled data points before training, improving dataset quality.
  - Quick check question: What property of SOM makes it suitable for detecting outliers in labeled datasets?

## Architecture Onboarding

- Component map:
  INSTANCE dataset → vertical component extraction → polarity labeling → SOM cleaning → CNN training (8 configurations) → ensemble aggregation → uncertainty estimation
- Critical path:
  Training → Prediction → Uncertainty estimation (individual) → Ensemble aggregation → Uncertainty assessment
- Design tradeoffs:
  - Dropout vs. no dropout: Regularization vs. deterministic training
  - Complete vs. cleaned dataset: More data vs. higher quality
  - Ensemble size vs. computational cost: Better uncertainty vs. resource use
- Failure signatures:
  - Overconfident predictions on undecidable data → insufficient uncertainty modeling
  - Large accuracy drop on cleaned test set → overfitting to noise
  - Dropout ensembles not improving uncertainty → insufficient diversity or dropout rate too low
- First 3 experiments:
  1. Train one CNN with SGD + dropout on complete dataset; evaluate accuracy and uncertainty histogram.
  2. Train same architecture without dropout; compare robustness to mislabeled examples.
  3. Build ensemble of 4 models (2 with dropout, 2 without); compare uncertainty estimates on undecidable data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ensemble models with dropout layers compare to single models with dropout layers when tested on out-of-distribution data beyond undecidable polarities?
- Basis in paper: [explicit] The paper observes that ensembles incorporating models with dropout layers significantly enhance uncertainty estimation, but individual models with dropout layers alone do not achieve reliable uncertainty estimation.
- Why unresolved: The study only evaluates the performance of ensembles and single models on undecidable polarity data, which is a specific type of out-of-distribution data. It is unclear whether the same conclusions hold for other types of out-of-distribution data.
- What evidence would resolve it: Testing the performance of ensemble models with dropout layers and single models with dropout layers on various types of out-of-distribution data beyond undecidable polarities, such as data from different seismic regions or with different noise characteristics.

### Open Question 2
- Question: What is the optimal number of models to include in an ensemble for maximizing uncertainty estimation in seismic trace classification?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of ensemble methods in uncertainty estimation but does not explore the impact of ensemble size on performance.
- Why unresolved: The study uses a fixed ensemble size (56 networks) without investigating whether increasing or decreasing the number of models would improve uncertainty estimation.
- What evidence would resolve it: Conducting experiments with ensembles of varying sizes and comparing their uncertainty estimation performance to determine the optimal number of models.

### Open Question 3
- Question: How do different data cleaning strategies, beyond the SOM-based approach, affect the robustness of CNN models to mislabeled examples in seismic trace classification?
- Basis in paper: [explicit] The paper describes a cleaning strategy based on cyclic applications of Self-Organized Maps (SOM) and manual visual inspection, which successfully identified and removed mislabeled and ambiguous data points.
- Why unresolved: The study only explores one data cleaning strategy (SOM-based) and does not compare its effectiveness to other potential approaches, such as active learning or semi-supervised learning methods.
- What evidence would resolve it: Implementing and comparing the performance of different data cleaning strategies, including but not limited to SOM-based, active learning, and semi-supervised learning approaches, on the robustness of CNN models to mislabeled examples in seismic trace classification.

## Limitations
- The specific CNN architecture details beyond layer count (kernel sizes, strides, activation functions) remain unspecified
- The SOM cleaning procedure implementation details (iterations, grid size, distance threshold) are not fully described
- The uncertainty estimation claims lack external benchmarking against established uncertainty quantification methods
- The study only evaluates ensemble size as a fixed configuration without exploring optimal ensemble size

## Confidence
- CNN architecture design and performance claims: Medium
- Dropout improving mislabeled data robustness: Medium
- Ensemble uncertainty estimation claims: Medium-High
- SOM data cleaning effectiveness: Medium

## Next Checks
1. Benchmark ensemble uncertainty estimates against established Bayesian methods on identical datasets
2. Test dropout robustness on synthetic mislabeled datasets with controlled noise patterns
3. Compare SOM cleaning effectiveness against automated label noise correction algorithms