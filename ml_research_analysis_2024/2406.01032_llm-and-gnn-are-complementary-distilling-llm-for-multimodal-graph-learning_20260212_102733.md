---
ver: rpa2
title: 'LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning'
arxiv_id: '2406.01032'
source_url: https://arxiv.org/abs/2406.01032
tags:
- graph
- molecule
- molecular
- smiles
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of molecular property prediction
  using Graph Neural Networks (GNNs), which struggle to effectively process multimodal
  molecular data such as SMILES strings and molecular diagrams. To bridge this gap,
  the authors propose GALLON, a framework that distills knowledge from Large Language
  Models (LLMs) and GNNs into a Multilayer Perceptron (MLP).
---

# LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning

## Quick Facts
- arXiv ID: 2406.01032
- Source URL: https://arxiv.org/abs/2406.01032
- Reference count: 40
- GALLON achieves state-of-the-art performance on molecular property prediction, outperforming both GNNs and LLMs in accuracy and efficiency

## Executive Summary
This paper addresses the challenge of molecular property prediction using Graph Neural Networks (GNNs), which struggle to effectively process multimodal molecular data such as SMILES strings and molecular diagrams. To bridge this gap, the authors propose GALLON, a framework that distills knowledge from Large Language Models (LLMs) and GNNs into a Multilayer Perceptron (MLP). GALLON integrates multimodal molecular data to extract insights from LLMs and synergizes their capabilities with GNNs, resulting in a unified model that captures rich textual, visual, and structural information. Extensive experiments on seven datasets demonstrate that GALLON achieves state-of-the-art performance, outperforming both GNNs and LLMs in terms of accuracy and efficiency, while also reducing model size and inference time.

## Method Summary
GALLON integrates multimodal molecular data (SMILES strings, molecular diagrams, and graph structures) to extract insights from LLMs and synergize their capabilities with GNNs. The framework distills knowledge from both teacher models (GNN and LLM) into an MLP using representation distillation for regression and label distillation for classification. The multimodal inputs enable LLMs to extract richer chemical context, while the MLP architecture with comprehensive node features can achieve GNN-level performance without explicit graph structure. The approach leverages the complementary strengths of LLMs and GNNs to provide a more efficient and effective solution for molecular property prediction.

## Key Results
- GALLON achieves state-of-the-art performance across seven molecular datasets, outperforming both GNNs and LLMs
- The multimodal approach demonstrates significant improvements in accuracy and efficiency compared to single-modality methods
- The framework reduces model size and inference time while maintaining or improving prediction performance

## Why This Works (Mechanism)

### Mechanism 1
GALLON's multimodal input (SMILES, diagrams, graph structure) enables LLMs to extract richer chemical context than any single modality alone. The LLM processes each modality independently, generating detailed structural and functional group descriptions. These complementary insights are encoded by a fine-tuned LM, creating embeddings that capture both atomic-level and holistic molecular properties.

### Mechanism 2
Distilling from both GNN and LLM creates MLP representations that capture complementary strengths: structural relationships from GNNs and contextual knowledge from LLMs. GALLON trains two teacher models (GNN and LLM/LM encoder) independently, then distills their knowledge into an MLP using both label distillation for classification and representation distillation for regression tasks.

### Mechanism 3
The MLP architecture with comprehensive node features (chirality, degree, aromaticity, etc.) can achieve GNN-level performance without explicit graph structure. Node features include structural information that captures graph topology implicitly. The MLP learns to leverage these features through distillation from the GNN teacher.

## Foundational Learning

- **Concept**: Graph Neural Networks for molecular representation
  - Why needed here: Understanding GNN limitations with multimodal data motivates the LLM integration
  - Quick check question: Why do GNNs struggle with SMILES strings and molecular diagrams compared to graph structures?

- **Concept**: Knowledge distillation principles
  - Why needed here: The core innovation involves distilling from multiple teacher models into a student MLP
  - Quick check question: What's the difference between label distillation and representation distillation, and when would you use each?

- **Concept**: Multimodal learning integration
  - Why needed here: Combining SMILES, diagrams, and graph structure requires understanding how different modalities complement each other
  - Quick check question: How does adding visual molecular diagrams improve LLM understanding compared to text-only representations?

## Architecture Onboarding

- **Component map**: SMILES + diagram + graph structure -> LLM -> fine-tuned LM encoder -> hLM; Node features + graph structure -> GNN -> hGNN; Node features -> MLP -> hMLP; MLP distillation from both hLM and hGNN -> final prediction

- **Critical path**: Multimodal inputs → LLM → fine-tuned LM encoder → hLM; Node features + graph structure → GNN → hGNN; Node features → MLP → hMLP; MLP distillation from both hLM and hGNN → final prediction

- **Design tradeoffs**: Using LLM adds knowledge but increases inference cost; distillation mitigates this. Multimodal inputs improve understanding but increase prompt complexity. MLP reduces model size but requires comprehensive node features to compensate for lack of explicit graph structure.

- **Failure signatures**: Poor performance on datasets with subtle structural differences: likely insufficient node features. Large gap between LLM-only and combined results: possible redundancy between GNN and LLM knowledge. Inconsistent results across seeds: likely hyperparameter sensitivity in distillation process.

- **First 3 experiments**:
  1. Ablation study: Remove molecular diagrams and observe performance drop on datasets where visual structure matters
  2. Teacher comparison: Train MLP with only GNN teacher vs only LLM teacher vs both teachers
  3. Node feature analysis: Remove key structural features (chirality, aromaticity) and measure impact on isomer classification

## Open Questions the Paper Calls Out

### Open Question 1
How do the different components of the LLM-generated explanations contribute to the final performance of the MLP in molecular property prediction? The paper mentions that both the prior knowledge embedded in LLMs and the graph structural information from GNNs contribute to the final results, but it does not quantify the individual contributions of each component.

### Open Question 2
What are the limitations of using only node features without explicit graph structure in the MLP for molecular property prediction? The paper acknowledges that using MLP with atom input without a graph structure cannot differentiate isomers, but it does not explore the extent of this limitation or its impact on performance.

### Open Question 3
How does the choice of the smaller Language Model (LM) for encoding LLM outputs affect the final performance of the MLP? The paper mentions that a smaller LM is fine-tuned to encode the text outputs of the LLM, but it does not explore the impact of different LM choices or architectures on the final performance.

## Limitations
- Primary uncertainty lies in the LLM component's dependency on external API calls (GPT-4V and Claude3-Haiku), which introduces potential reproducibility issues and cost barriers
- The paper doesn't provide detailed error analysis to understand failure cases or the types of molecular properties where the multimodal approach excels versus traditional GNNs
- Scalability claims regarding reduced model size and inference time are somewhat speculative, as the paper doesn't provide direct comparisons of computational efficiency

## Confidence
**High confidence**: The distillation methodology (both label and representation distillation) is well-established and the experimental results showing performance improvements over baseline GNNs are statistically significant with multiple random seeds.

**Medium confidence**: The claim that multimodal integration provides complementary information is supported by ablation studies, but the exact contribution of each modality (SMILES, diagrams, graph structure) to overall performance is not fully quantified.

**Low confidence**: The scalability claims regarding reduced model size and inference time are somewhat speculative, as the paper doesn't provide direct comparisons of computational efficiency between the proposed method and traditional GNNs, particularly accounting for the LLM inference costs.

## Next Checks
1. **Ablation study replication**: Independently verify the contribution of each modality by removing molecular diagrams and graph structure from the LLM prompts, then measuring performance degradation across all seven datasets.

2. **Teacher comparison analysis**: Conduct controlled experiments comparing MLP performance when trained with only GNN teacher, only LLM teacher, and both teachers to quantify the true complementarity of the knowledge sources.

3. **Cross-dataset generalization test**: Evaluate GALLON's performance on molecular datasets not seen during training (such as from different chemical domains) to assess whether the multimodal approach provides consistent benefits beyond the specific MoleculeNet datasets used in the study.