---
ver: rpa2
title: 'Language Model Council: Democratically Benchmarking Foundation Models on Highly
  Subjective Tasks'
arxiv_id: '2406.08598'
source_url: https://arxiv.org/abs/2406.08598
tags:
- more
- qwen1
- llms
- gemini-1
- council
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Language Model Council (LMC) introduces a democratic evaluation
  framework where diverse LLMs collaborate to generate tests, respond to them, and
  judge each other's outputs. Applied to emotional intelligence assessment, 20 LLMs
  collectively produced 100 interpersonal conflict scenarios, responded to them, and
  evaluated each other's responses.
---

# Language Model Council: Democratically Benchmarking Foundation Models on Highly Subjective Tasks

## Quick Facts
- arXiv ID: 2406.08598
- Source URL: https://arxiv.org/abs/2406.08598
- Reference count: 40
- The LMC achieved higher alignment with human evaluations (0.92 Spearman correlation) than individual judges or other benchmarks

## Executive Summary
The Language Model Council (LMC) introduces a democratic evaluation framework where diverse LLMs collaborate to generate tests, respond to them, and judge each other's outputs. Applied to emotional intelligence assessment, 20 LLMs collectively produced 100 interpersonal conflict scenarios, responded to them, and evaluated each other's responses. The framework achieves strong human alignment (0.92 Spearman correlation) while enabling cost-effective evaluation through strategic sub-council selection. Key findings include optimal council sizes with diminishing returns beyond ~9 members and the surprising result that models excelling at EI tasks don't necessarily make the best judges.

## Method Summary
The LMC framework operates in three stages: test set formulation where LLMs expand base scenarios into detailed interpersonal conflicts, response gathering where all LLMs respond to these scenarios, and collective judging where all LLMs pairwise compare each other's responses. The system uses pairwise comparisons with position flipping and aggregation methods like majority vote to produce final rankings. The approach is designed to be scalable and robust, with Monte Carlo simulations revealing optimal council sizes and the potential for cost reduction through strategic sub-council selection.

## Key Results
- Democratic consensus from 20 diverse LLMs achieved 0.92 Spearman correlation with human evaluations
- Optimal council size shows diminishing returns beyond ~9 members for separability and stability
- Sub-councils (smalls, top-4) can maintain strong performance (0.88+ correlation) while reducing costs
- Models excelling at EI tasks don't necessarily make the best judges for pairwise comparisons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Democratic consensus from diverse LLM judges produces more reliable subjective rankings than single-model judgments
- Mechanism: Each LLM in the council acts as a judge, respondent, and test-set contributor, creating a closed-loop evaluation system where multiple perspectives mitigate individual model biases
- Core assumption: Different LLMs have complementary strengths and weaknesses in subjective judgment that average out when aggregated democratically
- Evidence anchors:
  - [abstract]: "The LMC achieved higher alignment with human evaluations (0.92 Spearman correlation) than individual judges or other benchmarks"
  - [section 3.5]: "EI Score, Separability, Consistency, Conviction, Contrarianism, Polarization" metrics show collective performance

### Mechanism 2
- Claim: Monte Carlo simulations reveal optimal council size for balancing cost and evaluation quality
- Mechanism: Random sampling of council compositions and test set sizes allows systematic study of how separability and stability change with different configurations
- Core assumption: Random sampling with replacement adequately represents the space of possible council compositions
- Evidence anchors:
  - [section 5.1]: "Both separability and stability improve as the number of test examples and the number of LLM judges increase"
  - [section 4]: "The added benefit of including either an additional judge or more test examples diminishes significantly in a concentric shape that starts ~50 examples and ~9 judges"

### Mechanism 3
- Claim: Hand-curated sub-councils can maintain strong performance while reducing costs
- Mechanism: Strategic selection of council members based on model characteristics (size, family, performance) can achieve high correlation with human rankings without full council participation
- Core assumption: Certain combinations of models capture the diversity benefits of full council participation
- Evidence anchors:
  - [section 5.2]: "smalls, a council composed of the smallest LLMs, achieves a separability of 71%—exceeding the average judge's 53.3%—and a Spearman correlation of 0.88 with human rankings"
  - [section 4]: "top-4 achieves the same correlation with human rankings as smalls (0.88) but with significantly higher separability (79%)"

## Foundational Learning

- Concept: Bradley-Terry pairwise comparison algorithm
  - Why needed here: Enables estimation of model win rates without requiring all possible pairwise comparisons
  - Quick check question: How does the Bradley-Terry algorithm estimate relative model strengths when not all pairs are directly compared?

- Concept: Spearman correlation as evaluation metric
  - Why needed here: Measures rank correlation between LMC rankings and human judgments
  - Quick check question: Why is Spearman correlation more appropriate than Pearson correlation for evaluating ranking systems?

- Concept: Monte Carlo simulation methodology
  - Why needed here: Allows systematic exploration of council composition effects without exhaustive experimentation
  - Quick check question: What are the key assumptions when using Monte Carlo simulations to study evaluation system properties?

## Architecture Onboarding

- Component map: Council formation -> Test set generation -> Response collection -> Collective judging -> Aggregation -> Ranking output
- Critical path: Council formation → Test set generation → Response collection → Collective judging → Aggregation → Ranking output
- Design tradeoffs:
  - Full council vs. sub-councils (inclusivity vs. cost)
  - Single reference model vs. multiple references (consistency vs. bias)
  - Granular vs. coarse rating scales (discrimination vs. judge consistency)
  - Position flipping vs. single position (bias mitigation vs. efficiency)
- Failure signatures:
  - Low separability (poor ranking discrimination)
  - High position bias (systematic judge preferences)
  - Poor human correlation (misalignment with human preferences)
  - Excessive cost (council too large or test set too extensive)
- First 3 experiments:
  1. Run calibration study with 3 different temperatures and rating scales to optimize judge consistency
  2. Perform dry run with 5% of data using different reference models to optimize separability
  3. Test sub-council compositions (flagships, smalls, top-4) to validate cost-performance tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the choice of reference model in arena-style evaluations systematically bias results toward certain model families or successors?
- Basis in paper: Explicit - The paper identifies that using Qwen-1.5-32B as reference model may have given outsized advantage to larger Qwen models, and speculates this could be a form of "successor bias."
- Why unresolved: The paper acknowledges this as a possibility but does not empirically test different reference model choices or analyze the systematic effects across multiple evaluations.
- What evidence would resolve it: Controlled experiments comparing rankings using different reference models (including cross-family and adversarial choices) to measure systematic bias effects.

### Open Question 2
- Question: What is the optimal council size for balancing evaluation quality, robustness to adversarial judges, and cost-effectiveness?
- Basis in paper: Explicit - The paper finds diminishing returns beyond ~9 members but acknowledges this is based on their specific EI task and 100 examples, suggesting task-dependency.
- Why unresolved: The paper's Monte Carlo simulations show general trends but don't establish task-specific optimal sizes or account for variations in task complexity, judge quality heterogeneity, or cost constraints.
- What evidence would resolve it: Task-specific experiments varying council sizes across different subjective domains while measuring separability, stability (MERV), human alignment, and cost metrics.

### Open Question 3
- Question: How does the Language Model Council framework generalize to non-English languages and multimodal tasks?
- Basis in paper: Inferred - The paper explicitly limits its case study to English and single-turn interactions, noting these as limitations while suggesting broader applicability.
- Why unresolved: The paper doesn't provide evidence that the democratic voting mechanism, CoT prompting, or pairwise comparison framework works equivalently across languages with different grammatical structures or in multimodal contexts.
- What evidence would resolve it: Empirical validation of LMC across multiple languages (especially non-Indo-European) and multimodal tasks (vision-language, audio-text) measuring whether democratic consensus improves over single judges.

## Limitations

- Framework validation limited to emotional intelligence tasks in English, limiting generalizability to other subjective domains
- Monte Carlo simulation assumptions may not capture systematic interactions between specific model types
- Council size optimization findings may be specific to the 20-LLM configuration and may not generalize across different tasks

## Confidence

**High Confidence:**
- The LMC framework can produce rankings that correlate well with human evaluations (0.92 Spearman)
- Sub-councils can achieve strong performance while reducing costs (smalls: 0.88 correlation, top-4: 79% separability)
- Council size has diminishing returns beyond ~9 members for the studied configuration

**Medium Confidence:**
- The concentric diminishing returns pattern applies broadly across different task types
- Position flipping effectively mitigates positional bias in pairwise comparisons
- The aggregation methods (majority vote, mean pooling) perform similarly across different council sizes

**Low Confidence:**
- The optimal council size of ~9 members generalizes to other subjective domains
- The framework's robustness against adversarial judges scales linearly with council size
- Sub-council selection strategies based on model characteristics will generalize to different model pools

## Next Checks

1. **Cross-domain validation study**: Apply the LMC framework to at least three additional subjective domains (e.g., creative writing, ethical reasoning, technical problem-solving) to test whether the 9-judge diminishing returns pattern and council composition effects hold across different types of subjective tasks.

2. **Adversarial judge robustness test**: Systematically introduce known adversarial judges into the council (models designed to be inconsistent or biased) at varying proportions (10%, 25%, 50%) to empirically measure how council performance degrades and whether the diminishing returns pattern changes.

3. **Alternative aggregation method comparison**: Implement and compare additional aggregation methods beyond majority vote and mean pooling, such as weighted voting based on judge consistency scores or Bayesian inference methods, to determine if better aggregation approaches exist that could reduce the required council size while maintaining performance.