---
ver: rpa2
title: Fine-Tuned Language Models Generate Stable Inorganic Materials as Text
arxiv_id: '2402.04379'
source_url: https://arxiv.org/abs/2402.04379
tags:
- materials
- llama-2
- language
- crystal
- stable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fine-tuning large language models on text-encoded crystal structures
  enables high-rate generation of stable inorganic materials. The method leverages
  existing LLM tokenization, fine-tunes on bulk crystal strings with task-specific
  prompts, and uses translation augmentations to encourage learned symmetries.
---

# Fine-Tuned Language Models Generate Stable Inorganic Materials as Text

## Quick Facts
- arXiv ID: 2402.04379
- Source URL: https://arxiv.org/abs/2402.04379
- Reference count: 40
- One-line primary result: Fine-tuned LLaMA-2 70B models generate metastable materials at ~49% vs 28% for CDVAE diffusion models

## Executive Summary
Fine-tuning large language models on text-encoded crystal structures enables high-rate generation of stable inorganic materials. The method leverages existing LLM tokenization, fine-tunes on bulk crystal strings with task-specific prompts, and uses translation augmentations to encourage learned symmetries. Evaluated against diffusion-based CDVAE, fine-tuned LLaMA-2 70B models generate metastable materials at ~49% vs 28% for CDVAE, with ~90% structural/compositional validity. Larger models show improved translation invariance (IPT metric) and diverse, novel stable materials. The approach also supports text-conditional generation and infilling, making it flexible for materials design.

## Method Summary
The method encodes crystal structures as newline-separated text strings (lattice parameters, atom identities, fractional coordinates) and fine-tunes pretrained LLaMA-2 models using LoRA adapters. Task-specific prompts enable unconditional generation, text-conditional generation, and infilling within a single framework. Training incorporates stochastic prompts and translation augmentations to encourage symmetry learning. Models are evaluated on structural/compositional validity (>90%), stability (energy above hull), diversity, novelty, and translation invariance metrics. The approach is compared against CDVAE diffusion models and validated using M3GNet and DFT calculations.

## Key Results
- Fine-tuned LLaMA-2 70B models generate metastable materials at ~49% vs 28% for CDVAE diffusion models
- Generated materials show ~90% structural and compositional validity
- Larger LLaMA models demonstrate improved translation invariance and symmetry learning (IPT metric)

## Why This Works (Mechanism)

### Mechanism 1
Large language models trained on text can learn generalizable patterns for atomistic data when fine-tuned with minimal task-specific modifications. Pretraining on large text corpora equips LLMs with biases toward abstract and compressible patterns, which translates to effective modeling of numerical sequences representing atomic coordinates and material properties. The fine-tuning step adapts these biases to the specific structure of crystal data encoded as text strings.

### Mechanism 2
Text prompting's inherent flexibility enables the model to perform multiple tasks (unconditional generation, text-conditional generation, and infilling) within a single framework. By incorporating task-specific prompts as input to the model, the LLM can be conditioned on desired properties or constraints while still leveraging its generative capabilities. This allows for seamless switching between different material design tasks without architectural changes.

### Mechanism 3
Larger language models demonstrate improved ability to learn symmetries from training data and augmentations, leading to higher rates of stable material generation. Larger models are more effective compressors of data, allowing them to capture more complex patterns and symmetries present in the training data. This improved understanding of symmetries translates to better modeling of atomic structures and their properties.

## Foundational Learning

- Concept: Text tokenization and encoding of numerical data
  - Why needed here: The method relies on converting crystal structures into text strings, which requires understanding how numbers are tokenized and encoded by the LLM
  - Quick check question: How does the LLaMA-2 tokenizer handle numbers, and why is this important for representing atomic coordinates?

- Concept: Fine-tuning vs. training from scratch
  - Why needed here: The approach uses fine-tuning of a pretrained LLM, which has different implications and requirements compared to training a model from scratch
  - Quick check question: What are the advantages and disadvantages of fine-tuning a pretrained LLM versus training a model from scratch for this task?

- Concept: Symmetry in crystal structures
  - Why needed here: Understanding the symmetries present in crystal structures is crucial for evaluating the model's ability to learn and generate stable materials
  - Quick check question: What are the key symmetries present in crystal structures, and how do they impact the generation of stable materials?

## Architecture Onboarding

- Component map: Pretrained LLaMA-2 model with LoRA adapters -> Text encoder for crystal structures -> Task-specific prompts -> Evaluation pipeline

- Critical path: 1) Convert crystal structures to text strings using specified format, 2) Fine-tune pretrained LLM with LoRA adapters on encoded crystal data, 3) Generate materials using fine-tuned model with task-specific prompts, 4) Evaluate generated materials for validity, stability, and diversity

- Design tradeoffs: Model size vs. computational cost (larger models may generate more stable materials but are more expensive to train), prompt specificity vs. model flexibility (more specific prompts may improve task performance but reduce model's ability to handle diverse tasks), fine-tuning duration vs. overfitting (longer fine-tuning may improve performance but increase risk of overfitting)

- Failure signatures: Low validity rates (<90%) indicating issues with text encoding or model's understanding of atomic structures, low stability rates (<30%) suggesting model is not effectively learning required symmetries and properties, slow sampling speed indicating inefficiencies in generation process

- First 3 experiments: 1) Fine-tune small LLaMA-2 model (7B) on subset of Materials Project dataset and evaluate validity and stability rates, 2) Experiment with different text prompts to assess impact on model's ability to generate stable materials, 3) Compare performance of fine-tuned LLaMA-2 models with different sizes (7B, 13B, 70B) to understand effect of model scale on stability and diversity

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LLM-based crystal generation scale with model size beyond 70B parameters, and is there a point of diminishing returns? The study only evaluates up to LLaMA-2 70B, leaving open the question of whether further scaling would yield proportional gains in stability, diversity, or symmetry learning.

### Open Question 2
Can the proposed IPT metric be generalized to other generative modeling tasks beyond atomistic data, and how does it compare to existing invariance metrics? The paper applies IPT specifically to crystal structures and does not explore its applicability to other domains or compare it to established invariance metrics.

### Open Question 3
How do alternative sampling strategies (e.g., classifier-free guidance or PPLM variants) impact the trade-off between conditional generation accuracy and sample diversity in LLM-based crystal generation? The paper mentions these approaches might be useful but does not implement or evaluate them.

## Limitations

- The approach relies on specific text encoding conventions (coordinate precision, tokenization) that introduce brittleness - small changes in formatting could significantly impact model performance
- Evaluation focuses primarily on energy-above-hull metrics and structural validity, lacking comprehensive assessment of material properties like electronic structure or synthesizability
- Translation invariance metric (IPT) shows improvement with scale but lacks clear thresholds for "good" invariance, making performance comparisons difficult

## Confidence

- High Confidence: Structural and compositional validity rates (>90%), direct comparison with CDVAE showing improved stability (49% vs 28% metastable), and translation invariance improvements with model scale
- Medium Confidence: The claim that LLMs can simultaneously perform unconditional generation, infilling, and text-conditional generation relies on task-specific prompts that aren't fully specified
- Medium Confidence: The assertion that larger models inherently learn better symmetries is supported by IPT metrics but requires more systematic analysis across different crystal families

## Next Checks

1. Ablation study on text encoding precision: Systematically vary coordinate precision (from 2 to 6 decimal places) and measure impacts on validity, stability, and generation speed to determine minimum precision required for robust performance

2. Out-of-distribution stability testing: Generate materials with specified target properties (e.g., specific space groups or band gaps) and validate stability using both M3GNet and full DFT calculations to assess reliability of proxy stability metric

3. Scaling behavior analysis: Train models at intermediate scales (e.g., 1.3B, 30B) to characterize precise relationship between parameter count and translation invariance/stability rate, determining whether improvements show diminishing returns