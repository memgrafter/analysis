---
ver: rpa2
title: White Men Lead, Black Women Help? Benchmarking and Mitigating Language Agency
  Social Biases in LLMs
arxiv_id: '2404.10508'
source_url: https://arxiv.org/abs/2404.10508
tags:
- agency
- language
- dataset
- gender
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes and investigates gender, racial, and intersectional
  biases in language agency across 6 human-written and LLM-generated datasets. To
  accurately measure agency, the authors introduce a Language Agency Classification
  dataset and train an agency classifier, finding that LLM-generated texts exhibit
  notably higher levels of gender bias than human-written texts.
---

# White Men Lead, Black Women Help? Benchmarking and Mitigating Language Agency Social Biases in LLMs

## Quick Facts
- arXiv ID: 2404.10508
- Source URL: https://arxiv.org/abs/2404.10508
- Authors: Yixin Wan; Kai-Wei Chang
- Reference count: 21
- Primary result: LLM-generated texts exhibit notably higher levels of gender and intersectional bias compared to human-written texts

## Executive Summary
This study investigates gender, racial, and intersectional biases in language agency across human-written and LLM-generated datasets. The authors introduce a Language Agency Classification dataset and train an agency classifier to measure bias levels. Their findings reveal that LLM-generated texts show significantly higher gender bias than human-written texts, with particularly pronounced intersectional bias targeting Black females. Based on these results, the authors propose a Mitigation via Selective Rewrite strategy that proves more effective than traditional prompt-based mitigation methods.

## Method Summary
The study develops a Language Agency Classification dataset to train an agency classifier using Roberta-Large, achieving 93.2% accuracy and 93.3% F1 score. The researchers benchmark agency bias across six human-written and six LLM-generated datasets, comparing bias levels between different demographic groups. They evaluate multiple mitigation strategies including prompt-based approaches and introduce a novel Mitigation via Selective Rewrite technique that selectively modifies biased portions of text while preserving overall meaning and fluency.

## Key Results
- LLM-generated texts exhibit notably higher levels of gender bias than human-written texts
- LLM-generated texts demonstrate remarkably higher levels of intersectional bias, particularly targeting Black females
- Mitigation via Selective Rewrite proves more effective and reliable than prompt-based mitigation methods

## Why This Works (Mechanism)
The study's effectiveness stems from its precise measurement of agency bias through a dedicated classification model trained on carefully curated data. By quantifying agency attributions across demographic groups, the research reveals systematic patterns of bias that would be difficult to detect through qualitative analysis alone. The Selective Rewrite mitigation works by targeting specific biased segments rather than attempting to rewrite entire passages, preserving contextual coherence while eliminating problematic content.

## Foundational Learning
1. **Language Agency Classification** - The process of determining whether language attributes agency (action-oriented, independent) or lacks it (passive, dependent). Why needed: Provides measurable metric for bias detection. Quick check: Does the text portray subjects as active decision-makers or passive recipients?

2. **Intersectional Bias Measurement** - Analysis of bias that occurs at the intersection of multiple demographic attributes (e.g., race and gender). Why needed: Reveals compound discrimination patterns invisible in single-attribute analyses. Quick check: Are certain demographic combinations (e.g., Black females) disproportionately affected compared to individual attribute biases?

3. **Selective Rewrite Strategy** - Targeted modification of only the biased portions of text while maintaining overall context and fluency. Why needed: More precise and less disruptive than full-text rewrites. Quick check: Does the rewritten text maintain original meaning while eliminating identified biases?

## Architecture Onboarding
**Component Map:** Agency Classifier -> Bias Measurement -> Mitigation Strategy -> Evaluation
**Critical Path:** Data collection → Agency classifier training → Bias benchmarking → Mitigation implementation → Effectiveness evaluation
**Design Tradeoffs:** Selective Rewrite trades some computational overhead for higher precision versus prompt-based methods; classifier accuracy vs. generalization across domains
**Failure Signatures:** Classifier misclassifies agency in complex sentences; mitigation introduces awkward phrasing; bias patterns not captured due to dataset limitations
**First Experiments:** 1) Validate agency classifier on diverse sentence structures; 2) Test mitigation across different bias types; 3) Compare computational efficiency of selective vs. full-text rewriting

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis constrained by specific datasets and models examined, limiting generalizability
- Results may not extend to other model architectures or training paradigms
- Does not address potential feedback loops where biased training data perpetuates and amplifies biases over time

## Confidence
- High confidence in existence of measurable gender and intersectional bias in LLM-generated text
- Medium confidence in mitigation strategy's effectiveness across diverse scenarios
- Low confidence in real-world deployment implications and user interaction manifestations

## Next Checks
1. Test the Selective Rewrite mitigation strategy across diverse model families beyond Llama-3 and different types of agency-related prompts to assess generalizability.

2. Conduct a longitudinal study examining whether LLM-generated content introduces new bias patterns into training corpora over multiple generations, potentially creating feedback loops.

3. Expand intersectional bias analysis to include additional demographic intersections and investigate the linguistic patterns that generate the most severe biases against specific groups.