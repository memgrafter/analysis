---
ver: rpa2
title: 'Graph and Sequential Neural Networks in Session-based Recommendation: A Survey'
arxiv_id: '2408.14851'
source_url: https://arxiv.org/abs/2408.14851
tags:
- recommendation
- graph
- session
- item
- proc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive overview of session-based
  recommendation (SR) using graph and sequential neural networks. It categorizes existing
  methods into sequential neural network-based approaches (RNN, LSTM, GRU, attention
  mechanisms, transformers) and graph neural network-based approaches (GCN, GAT, hypergraphs,
  heterogeneous graphs).
---

# Graph and Sequential Neural Networks in Session-based Recommendation: A Survey

## Quick Facts
- arXiv ID: 2408.14851
- Source URL: https://arxiv.org/abs/2408.14851
- Authors: Zihao Li; Chao Yang; Yakun Chen; Xianzhi Wang; Hongxu Chen; Guandong Xu; Lina Yao; Quan Z. Sheng
- Reference count: 40
- Primary result: Comprehensive survey categorizing SR methods into sequential (RNN, LSTM, GRU, attention, transformers) and graph-based (GCN, GAT, hypergraphs, heterogeneous graphs) approaches

## Executive Summary
This survey provides a comprehensive overview of session-based recommendation (SR) using graph and sequential neural networks. The paper categorizes existing methods into two main approaches: sequential neural network-based methods that model item order patterns, and graph neural network-based methods that capture co-occurrence relationships. The survey introduces unified frameworks for both approaches and discusses key modules including neighbor session selection, graph construction, information propagation, and session representation. It analyzes eight public datasets, highlighting that session lengths are typically short (average 3.95-17.03 items), suggesting graph-based methods may be more suitable than sequential models. The survey identifies challenges and future directions, emphasizing the need for better external information fusion, dynamic graph structures, and diverse user interest representations.

## Method Summary
The survey systematically reviews session-based recommendation approaches by categorizing them into sequential neural networks (RNN, LSTM, GRU, attention mechanisms, transformers) and graph neural networks (GCN, GAT, hypergraphs, heterogeneous graphs). For sequential models, the framework involves item embeddings, sequence modeling, session representation, and prediction layers. For graph models, the framework consists of neighbor session selection, graph construction, information propagation and aggregation, session representation, and prediction. The paper provides detailed analysis of eight public datasets, discusses evaluation metrics including HR@K, MRR@K, NDCG@K for accuracy and ILD, coverage for diversity, and identifies key challenges such as external information fusion, scalability with large graphs, and dynamic relationship modeling.

## Key Results
- Session lengths are typically short (average 3.95-17.03 items), suggesting graph-based methods may be more suitable than sequential models
- Common evaluation metrics include HR@K, MRR@K, NDCG@K for accuracy and ILD, coverage for diversity
- The survey identifies three main future directions: better external information fusion, dynamic graph structures, and diverse user interest representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph neural networks (GNNs) outperform sequential neural networks for session-based recommendation when session lengths are short and item order dependency is weak.
- Mechanism: GNNs capture co-occurrence patterns and multi-hop item relationships through information propagation and aggregation, which is more suitable than modeling sequential order when sessions are short and order patterns are ambiguous.
- Core assumption: Session lengths are typically short (average 3.95-17.03 items) and lack obvious sequential order patterns.
- Evidence anchors:
  - [abstract] "session lengths are typically short (average 3.95-17.03 items), suggesting graph-based methods may be more suitable than sequential models"
  - [section] "we could find that the length of sessions is rather limited and there is no obvious dependency between two items in a session, in spite of the items organized in chronological order"
  - [corpus] Weak evidence - corpus does not directly address this specific comparison.
- Break condition: If session lengths become longer and clear sequential order patterns emerge, sequential models would regain advantage.

### Mechanism 2
- Claim: External information (item attributes, user behaviors, time/location) significantly improves recommendation performance when properly fused.
- Mechanism: External information provides additional context about items and user interactions, which can be embedded and fused with item representations through various strategies (concatenation, addition, gating, attention) to enrich the recommendation model.
- Core assumption: External information contains valuable signals that complement item ID information alone.
- Evidence anchors:
  - [abstract] "need for better external information fusion" and "fusing other external information, e.g., item attributes, and interaction behaviors"
  - [section] "the text information, e.g., item description, title, and user's comments, contains fertile information about items' features and users' preferences"
  - [corpus] Weak evidence - corpus does not provide specific performance comparisons of external information fusion.
- Break condition: If external information is noisy, irrelevant, or the fusion method is poorly designed, performance gains may be minimal or negative.

### Mechanism 3
- Claim: Neighbor session selection strategies improve recommendation by providing additional context beyond the current session.
- Mechanism: By identifying and incorporating sessions that are similar to or adjacent to the current session (through item sharing, time proximity, or similarity metrics), the model gains access to broader user behavior patterns and preferences.
- Core assumption: User preferences can be inferred not just from the current session but also from related sessions.
- Evidence anchors:
  - [abstract] "neighbor session selection, graph construction, information propagation, and session representation" as key modules
  - [section] "it is of critical importance to define a proper similarity measurement" for neighbor session selection
  - [corpus] Weak evidence - corpus does not provide specific results on neighbor session selection effectiveness.
- Break condition: If neighbor sessions are not truly representative of the current user's intent or if the selection process introduces noise, performance may degrade.

## Foundational Learning

- Graph neural networks (GNNs)
  - Why needed here: GNNs are essential for modeling complex item relationships and capturing co-occurrence patterns in session data, which is more effective than sequential modeling for short sessions.
  - Quick check question: What is the key difference between how GNNs and sequential neural networks model item relationships in session data?

- Attention mechanisms
  - Why needed here: Attention mechanisms allow the model to focus on relevant items and relationships within sessions, improving representation learning and prediction accuracy.
  - Quick check question: How does the soft-attention mechanism in GNNs differ from the self-attention used in sequential models?

- Embedding techniques
  - Why needed here: Proper embedding of items, attributes, and external information is crucial for representing complex data in a format that neural networks can process effectively.
  - Quick check question: What are the three main categories of embeddings used in session-based recommendation systems?

## Architecture Onboarding

- Component map: Sequential models: item embeddings → sequence modeling → session representation → prediction. GNN models: neighbor session selection → graph construction → information propagation and aggregation → session representation → prediction.

- Critical path: For GNN-based systems, the critical path is: neighbor session selection → graph construction → information propagation and aggregation → session representation → prediction. For sequential models, it's: sequence modeling → session representation → prediction.

- Design tradeoffs: GNNs excel at capturing complex item relationships but require careful graph construction and can be computationally expensive. Sequential models are simpler and faster but may miss important co-occurrence patterns in short sessions.

- Failure signatures: Poor performance often indicates issues with neighbor session selection (irrelevant sessions), graph construction (missing important relationships), or information propagation (inadequate aggregation of neighbor information).

- First 3 experiments:
  1. Compare GNN vs sequential model performance on datasets with varying session lengths to validate the hypothesis about session length impact.
  2. Test different neighbor session selection strategies (item sharing vs. similarity-based) to identify the most effective approach.
  3. Evaluate various external information fusion methods (concatenation vs. attention vs. gating) to determine optimal information integration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific types of external information (beyond item embeddings and attributes) most significantly improve SR performance across different recommendation domains?
- Basis in paper: [explicit] The paper discusses external information fusion but notes that not all external information is equally beneficial and effectiveness varies by domain
- Why unresolved: Different recommendation scenarios (e-commerce, music, news) may require different external signals, but no systematic study comparing effectiveness across domains
- What evidence would resolve it: Controlled experiments comparing various external information types (behavior embeddings, time/location data, KG relations) across multiple recommendation domains while measuring both accuracy and computational efficiency

### Open Question 2
- Question: How can we design scalable and dynamic graph structures that effectively capture evolving item relationships in real-time SR applications?
- Basis in paper: [explicit] The paper identifies scalability challenges with large graphs and notes that most existing methods use static graphs despite real-world dynamic relationships
- Why unresolved: Current sampling approaches introduce randomness and instability; no framework exists for automatically updating graph structures as new sessions and items emerge
- What evidence would resolve it: A framework demonstrating stable performance with significantly larger graphs while maintaining accuracy through dynamic updates, validated on streaming session data

### Open Question 3
- Question: What is the optimal balance between diverse and uncertain user interest representations versus single fixed representations for SR?
- Basis in paper: [explicit] The paper discusses the limitations of single fixed representations and the potential benefits of diverse/uncertain representations, but notes this remains largely unexplored
- Why unresolved: No empirical comparison of multi-interest distributions versus single vectors across various session lengths and dataset characteristics; no guidance on when diversity matters most
- What evidence would resolve it: Comprehensive evaluation showing performance gains (accuracy, diversity, robustness) of diverse/uncertain representations versus single vectors across datasets with varying session characteristics and user behavior patterns

## Limitations

- The survey exclusively focuses on graph and sequential neural network approaches, omitting other promising directions like metric learning and reinforcement learning methods
- While the paper identifies future directions, it lacks specific empirical evidence for many proposed improvements, particularly around external information fusion benefits
- The analysis is limited to existing methodologies without providing new experimental validation or comparative performance studies

## Confidence

- Mechanism 1 (GNNs outperform sequential for short sessions): Medium-High
- Mechanism 2 (external information fusion benefits): Medium-Low
- Mechanism 3 (neighbor session selection benefits): Low-Medium

## Next Checks

1. **Direct comparative study**: Conduct controlled experiments comparing GNN and sequential models across datasets with varying session length distributions to quantify the impact of session characteristics on method selection.

2. **External information fusion ablation**: Systematically test different external information types (item attributes, user behaviors, time/location) and fusion strategies (concatenation, attention, gating) to identify which combinations yield measurable performance improvements.

3. **Neighbor session selection optimization**: Evaluate the impact of different neighbor selection strategies (item overlap thresholds, similarity metrics, temporal proximity) on recommendation accuracy across diverse session types to establish best practices.