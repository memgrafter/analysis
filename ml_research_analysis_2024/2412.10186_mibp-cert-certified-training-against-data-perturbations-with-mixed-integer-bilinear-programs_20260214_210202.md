---
ver: rpa2
title: 'MIBP-Cert: Certified Training against Data Perturbations with Mixed-Integer
  Bilinear Programs'
arxiv_id: '2412.10186'
source_url: https://arxiv.org/abs/2412.10186
tags:
- training
- bounds
- parameter
- constraints
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BiCert, a method for provably certifying
  robustness of machine learning models against data poisoning attacks during training.
  The approach formulates the certification problem as a mixed-integer bilinear program
  (MIBP) that computes exact bounds on model parameters after training, avoiding over-approximations
  common in prior methods.
---

# MIBP-Cert: Certified Training against Data Perturbations with Mixed-Integer Bilinear Programs

## Quick Facts
- arXiv ID: 2412.10186
- Source URL: https://arxiv.org/abs/2412.10186
- Authors: Tobias Lorenz; Marta Kwiatkowska; Mario Fritz
- Reference count: 13
- Primary result: BiCert achieves higher certified accuracy than state-of-the-art methods for larger perturbation radii while preventing training divergence through exact parameter bound computation.

## Executive Summary
This paper introduces BiCert, a method for provably certifying robustness of machine learning models against data poisoning attacks during training. The approach formulates the certification problem as a mixed-integer bilinear program (MIBP) that computes exact bounds on model parameters after training, avoiding over-approximations common in prior methods. By encoding the entire training process as constraints in an optimization problem, BiCert overcomes fundamental limitations of interval-based methods that cause training to diverge. Experiments on the Two-Moons dataset show that BiCert achieves higher certified accuracy than state-of-the-art methods for larger perturbation radii, while demonstrating more stable training with lower variance.

## Method Summary
BiCert formulates certified training as a mixed-integer bilinear program that computes exact bounds on model parameters after each training iteration. The method encodes the forward pass, loss computation, backpropagation, and parameter updates as constraints in an optimization problem. To maintain computational feasibility, bounds are relaxed to convex sets after each iteration, cutting the deep computational graph into manageable pieces while preserving soundness. This approach overcomes the fundamental limitation of prior interval-based methods where parameter bounds could only grow, causing training to diverge. The exact computation allows parameter intervals to shrink when appropriate, preventing uncontrolled growth of bounds.

## Key Results
- BiCert achieves higher certified accuracy than FullCert and Sosnin et al. for larger perturbation radii on the Two-Moons dataset
- The method demonstrates more stable training with lower variance compared to baseline approaches
- BiCert overcomes the fundamental limitation of prior methods where parameter bounds could only grow, preventing training divergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BiCert computes exact bounds for each training iteration using Bilinear Mixed Integer Programming (BMIP), avoiding over-approximations.
- Mechanism: The optimization problem encodes the entire forward pass, loss computation, backpropagation, and parameter updates as constraints. By solving this BMIP exactly, BiCert obtains precise bounds without the interval over-approximations that cause divergence in prior methods.
- Core assumption: The BMIP solver can handle the computational complexity of solving these bilinear constraints exactly for each parameter update.
- Evidence anchors: [abstract] "By formulating the certification problem as a mixed-integer bilinear program (MIBP) that computes exact bounds on model parameters after training, avoiding over-approximations common in prior methods."

### Mechanism 2
- Claim: The relaxation scheme bounds each training step without sacrificing soundness by cutting the verification graph after each parameter update.
- Mechanism: After computing exact bounds for one training iteration, BiCert relaxes these bounds to convex sets. This relaxation cuts the deep computational graph into manageable pieces while maintaining sound bounds, as the relaxation is performed after the parameter update step.
- Core assumption: Relaxing bounds to convex sets after parameter updates preserves soundness while making the problem computationally feasible.
- Evidence anchors: [abstract] "To make solving this optimization problem tractable, we propose a novel relaxation scheme that bounds each training step without sacrificing soundness."

### Mechanism 3
- Claim: BiCert eliminates the fundamental limitation of prior methods where parameter bounds could only grow, causing training to diverge.
- Mechanism: By computing exact bounds for each parameter update step and preserving the relationship between operands in the update equation, BiCert allows parameter intervals to shrink when appropriate. This prevents the uncontrolled growth of bounds that causes divergence in interval-based methods.
- Core assumption: Exact computation of parameter bounds allows for more precise tracking of parameter relationships, enabling bounds to shrink when the update step would reduce parameter values.
- Evidence anchors: [abstract] "Crucially, our approach overcomes the fundamental limitation of prior approaches where parameter bounds could only grow, often uncontrollably."

## Foundational Learning

- Concept: Mixed-Integer Bilinear Programming (BMIP)
  - Why needed here: BMIP allows encoding of piecewise linear constraints (like ReLUs) and bilinear terms (from parameter-input multiplications) exactly, which is essential for computing precise bounds through the entire training process.
  - Quick check question: Why can't we simply use linear programming for this certification problem? (Answer: Because we need to handle ReLUs which are piecewise linear and require binary decision variables, plus the bilinear terms from weight-input multiplications.)

- Concept: Interval Bound Propagation (IBP) limitations
  - Why needed here: Understanding why prior methods fail is crucial for appreciating BiCert's contributions. IBP over-approximates even simple operations and causes parameter bounds to only grow, leading to divergence.
  - Quick check question: What are the two main causes of divergence in IBP-based certified training? (Answer: 1) IBP doesn't preserve relationships between input/output variables, over-approximating operations like addition/subtraction. 2) Parameter bounds can only grow due to interval subtraction in parameter updates.)

- Concept: Lyapunov sequences and convergence analysis
  - Why needed here: The theoretical analysis of why BiCert overcomes divergence issues relies on understanding Lyapunov sequences and the conditions for SGD convergence.
  - Quick check question: Under what condition does Lorenz et al. [2024] show that IBP-based training diverges? (Answer: When the optimum intersects with the current parameter intervals, i.e., Θi ∩ Θ* ≠ ∅.)

## Architecture Onboarding

- Component map:
  Input constraints module -> Parameter constraints module -> Layer constraints module -> Loss constraints module -> Gradient constraints module -> Parameter update constraints module -> BMIP solver interface -> BoundFlow integration

- Critical path:
  1. Initialize parameter bounds to initial values
  2. For each training iteration:
     - Add current parameter bounds as constraints
     - For each data point: add input constraints
     - For each layer: add layer constraints
     - Add loss and gradient constraints
     - Add parameter update constraints
     - Solve BMIP to get new parameter bounds
  3. Return final parameter bounds for certification

- Design tradeoffs:
  - Precision vs. computational cost: BMIP provides exact bounds but is more expensive than IBP
  - Soundness vs. tightness: Convex relaxation after each iteration maintains soundness but introduces some imprecision
  - Model complexity vs. scalability: Larger models increase BMIP complexity exponentially
  - Loss function choice: Piecewise linear losses can be encoded exactly, while others require relaxations

- Failure signatures:
  - Training divergence: If parameter bounds approach ±∞ during training (though BiCert's design prevents this)
  - Solver timeouts: If Gurobi cannot solve the BMIP within allocated time
  - Numerical instability: If the optimization problem becomes ill-conditioned
  - Memory exhaustion: If the BMIP becomes too large to fit in memory

- First 3 experiments:
  1. Verify basic functionality on a tiny model (e.g., 1 hidden layer, 2 neurons) with known analytical solution to confirm BMIP computes correct bounds
  2. Compare BiCert's certified accuracy against FullCert and Sosnin et al. on Two-Moons dataset for small perturbation radii to confirm similar performance
  3. Test BiCert's certified accuracy for larger perturbation radii to verify the improved precision advantage over baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BiCert scale with model size (depth and width) compared to baseline methods?
- Basis in paper: [inferred] The paper discusses computational complexity and runtime increases but doesn't provide scaling analysis for larger models.
- Why unresolved: The experiments only evaluate on a small 2-layer, 20-neuron network, leaving questions about scalability unanswered.
- What evidence would resolve it: Systematic experiments varying network depth and width while measuring certified accuracy and runtime.

### Open Question 2
- Question: What is the impact of different bound relaxation strategies between training iterations on final certified accuracy?
- Basis in paper: [explicit] The paper mentions using convex relaxation between iterations but doesn't explore alternative strategies.
- Why unresolved: The paper only describes one relaxation approach without comparing to alternatives or analyzing trade-offs.
- What evidence would resolve it: Experiments comparing different relaxation strategies (e.g., tighter vs looser bounds, adaptive relaxation) with their impact on accuracy and stability.

### Open Question 3
- Question: How does BiCert perform on non-convex loss landscapes compared to convex ones?
- Basis in paper: [explicit] The paper uses margin loss (piecewise linear/convex) but mentions general losses can be supported through linear relaxations.
- Why unresolved: The paper only evaluates on convex loss functions, leaving open how the method performs on more complex, non-convex losses.
- What evidence would resolve it: Experiments training with non-convex losses (e.g., cross-entropy) and measuring certified accuracy compared to convex losses.

### Open Question 4
- Question: What is the theoretical relationship between the tightness of parameter bounds and the certified accuracy?
- Basis in paper: [inferred] The paper shows BiCert achieves tighter bounds than baselines and correlates this with higher accuracy, but doesn't establish a formal relationship.
- Why unresolved: While empirical correlation is shown, no theoretical analysis connects bound tightness to certification guarantees.
- What evidence would resolve it: Formal theoretical analysis proving bounds on how bound tightness affects certified accuracy, potentially with probabilistic guarantees.

## Limitations

- Computational complexity of solving BMIPs at each training iteration makes the method practical only for small-scale problems
- Experimental validation is limited to a single synthetic dataset (Two-Moons), with no evaluation on real-world datasets or larger models
- The exact solver parameters and implementation details for handling mini-batches are underspecified, potentially affecting reproducibility

## Confidence

- Mechanism 1 (Exact BMIP bounds): Medium - Theoretical formulation is sound, but computational tractability evidence is limited
- Mechanism 2 (Relaxation scheme): Medium - Soundness is theoretically established, but practical tightness impacts are not fully explored
- Mechanism 3 (Overcoming divergence): High - Theoretical analysis is rigorous and addresses known limitations of prior methods

## Next Checks

1. Reproduce the Two-Moons experiments with the exact solver parameters and compare certified accuracy across all perturbation radii to validate the reported improvements over baselines
2. Implement and test on a larger, real-world dataset (e.g., MNIST or CIFAR-10) to assess scalability limitations and practical utility
3. Conduct ablation studies comparing BiCert with and without the exact BMIP formulation versus using standard IBP to quantify the practical benefit of exact bounds