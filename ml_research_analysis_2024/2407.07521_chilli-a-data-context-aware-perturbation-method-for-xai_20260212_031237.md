---
ver: rpa2
title: 'CHILLI: A data context-aware perturbation method for XAI'
arxiv_id: '2407.07521'
source_url: https://arxiv.org/abs/2407.07521
tags:
- data
- chilli
- feature
- arxiv
- lime
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CHILLI improves explanation faithfulness by generating context-aware
  perturbations that respect feature dependencies and realistic bounds, addressing
  a key limitation of methods like LIME. The approach incorporates data context through
  feature-specific distance functions and perturbation generation inspired by SMOTE,
  producing synthetic samples that are both local and representative of training data.
---

# CHILLI: A data context-aware perturbation method for XAI

## Quick Facts
- arXiv ID: 2407.07521
- Source URL: https://arxiv.org/abs/2407.07521
- Reference count: 23
- Primary result: Reduces RMSE by up to 75% and MAE by up to 87% compared to LIME through context-aware perturbations

## Executive Summary
CHILLI addresses a critical limitation in post-hoc XAI methods by generating context-aware perturbations that respect feature dependencies and realistic bounds. Unlike LIME, which generates unrealistic data through generic sampling, CHILLI incorporates data context through feature-specific distance functions and perturbation generation inspired by SMOTE. This approach produces synthetic samples that are both local to the instance being explained and representative of the training data distribution, resulting in explanations that are more faithful to the base model's true behavior.

## Method Summary
CHILLI is a post-hoc XAI method that improves explanation faithfulness by incorporating data context into perturbation generation. The framework uses feature-specific distance functions to accurately measure locality, avoiding the pitfalls of generic distance metrics. Perturbations are generated through SMOTE-inspired interpolation between the instance being explained and randomly selected training samples, weighted by proximity. This produces realistic synthetic data that better reflects the training distribution. A local linear regression model is then fit to these perturbations to generate feature coefficients as explanations, with the approach validated on WebTRIS (traffic) and MIDAS (weather) datasets.

## Key Results
- Reduces RMSE by up to 75% compared to LIME on test datasets
- Reduces MAE by up to 87% compared to LIME on test datasets
- Produces explanations with greater disparity between feature coefficients, leading to more faithful interpretations of the base model's behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CHILLI's context-aware proximity measure ensures locality is accurately defined and constrained by incorporating feature-specific scales and bounds.
- Mechanism: Instead of using a generic distance function like Euclidean distance, CHILLI calculates the distance between instances independently for each feature dimension using a specified distance function that respects the feature's scale and bounds. This normalized distance is then averaged across all dimensions to give a single distance value, which is used to calculate a proximity measure.
- Core assumption: The absolute difference between two values does not always reflect their true distance, especially if the units are not equidistant, the feature is not measured linearly, or it's cyclic/temporal.
- Evidence anchors:
  - [abstract] "The approach incorporates data context through feature-specific distance functions"
  - [section 3.1] "We propose that the context of features should be considered independently by incorporating the scale and bounds of each feature to ensure the calculated distance is truly representative."
  - [corpus] Weak - no direct mentions of feature-specific distance functions in related papers.

### Mechanism 2
- Claim: CHILLI generates perturbations that are representative of the training data and local to the instance being explained by interpolating between the instance and randomly selected instances from the training data.
- Mechanism: CHILLI uses an approach inspired by the SMOTE algorithm. It selects a random instance from the training data, calculates the proximity between this instance and the instance being explained, and then generates a perturbation by linearly interpolating between the two instances. The selection of the random instance is weighted by proximity, ensuring perturbations are more likely to contain values closer to the instance being explained.
- Core assumption: Perturbations generated by interpolating between instances in the training data will be more representative of the training data distribution and realistic compared to perturbations generated by sampling from a Normal distribution.
- Evidence anchors:
  - [abstract] "The approach incorporates data context through feature-specific distance functions and perturbation generation inspired by SMOTE"
  - [section 3.2] "We use this approach to generate perturbations, and to ensure perturbations fall within realistic bounds, they are produced by interpolating between an instance being explained, x, and some other randomly selected instance, x′, in the training data."
  - [corpus] Weak - no direct mentions of SMOTE-inspired perturbation generation in related papers.

### Mechanism 3
- Claim: CHILLI produces explanations with greater disparity between feature coefficients, leading to more faithful explanations compared to LIME.
- Mechanism: By generating perturbations that are representative of the training data and local to the instance being explained, CHILLI's explanations are fit to data that better reflects the true behavior of the base model. This results in feature coefficients that more accurately represent the contribution of each feature towards the prediction.
- Core assumption: Explanations that are fit to perturbations that better represent the training data and locality will be more faithful to the base model's behavior.
- Evidence anchors:
  - [abstract] "Experiments on WebTRIS and MIDAS datasets show that CHILLI reduces RMSE by up to 75% and MAE by up to 87% compared to LIME, yielding explanations that are more faithful to the base model’s true behavior."
  - [section 5.3] "The explanation produced by CHILLI predicted the ‘Total Volume’ of traffic for the instance to be 81 whilst LIME’s explanation predicted 93. The lower error of the CHILLI explanation, combined with a prediction closer to the base model, supports the conclusion that CHILLI produces a more faithful explanation."
  - [corpus] Weak - no direct mentions of CHILLI's explanation performance compared to LIME in related papers.

## Foundational Learning

- Concept: Feature scaling and normalization
  - Why needed here: CHILLI's context-aware proximity measure requires features to be scaled appropriately to ensure equal contribution to the distance calculation.
  - Quick check question: How would you normalize a feature with a range of [0, 10] to be in the range [0, 1]?

- Concept: Distance metrics and their properties
  - Why needed here: CHILLI uses different distance metrics for different features based on their properties (e.g., cyclic, linear, categorical).
  - Quick check question: What distance metric would you use for a cyclic feature like hour of day, and why?

- Concept: SMOTE algorithm and its variants
  - Why needed here: CHILLI's perturbation generation is inspired by SMOTE, which generates synthetic data by interpolating between instances.
  - Quick check question: How does SMOTE handle categorical features when generating synthetic instances?

## Architecture Onboarding

- Component map: Data preprocessing -> Feature-specific distance calculation -> SMOTE-inspired perturbation generation -> Local linear regression fitting -> Feature coefficient output
- Critical path:
  1. Preprocess data (scaling, normalization)
  2. Calculate feature-specific distances
  3. Generate perturbations using SMOTE-inspired interpolation
  4. Fit proxy model to perturbations
  5. Output feature coefficients as explanation
- Design tradeoffs:
  - Choice of distance function for each feature: Balance between accuracy and computational complexity
  - Number of perturbations to generate: Tradeoff between explanation quality and computational cost
  - Locality hyperparameter (σ): Balance between locality and stability of explanation
- Failure signatures:
  - High RMSE/MAE on perturbations: Explanation is not faithful to base model
  - Unrealistic perturbations: Feature bounds or interpolation method is incorrect
  - Inconsistent feature coefficients: Distance function or perturbation generation is not capturing feature relationships
- First 3 experiments:
  1. Generate perturbations for a single instance using both LIME and CHILLI, and visualize the perturbations in feature space to compare their realism and locality.
  2. Fit proxy models to perturbations generated by LIME and CHILLI for a single instance, and compare the RMSE/MAE of the explanations.
  3. Vary the locality hyperparameter (σ) for CHILLI and observe its effect on the RMSE/MAE of the explanations and the distribution of perturbations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the incorporation of data context in CHILLI specifically affect the trustworthiness of explanations in high-risk applications compared to baseline methods?
- Basis in paper: [explicit] The paper highlights the importance of incorporating data context in high-risk domains and claims that CHILLI improves explanation faithfulness by generating context-aware perturbations.
- Why unresolved: While the paper demonstrates improved faithfulness metrics, it does not explicitly address the impact on trustworthiness perceptions in specific high-risk scenarios.
- What evidence would resolve it: User studies or case studies in high-risk domains (e.g., healthcare, finance) comparing trust levels with explanations generated by CHILLI versus baseline methods.

### Open Question 2
- Question: To what extent do the improvements in explanation faithfulness translate to better decision-making by end-users?
- Basis in paper: [inferred] The paper focuses on the technical improvements in explanation faithfulness but does not directly address the practical implications for end-user decision-making.
- Why unresolved: Faithfulness is a necessary but not sufficient condition for improved decision-making; the paper does not explore how users utilize the explanations in practice.
- What evidence would resolve it: Empirical studies measuring decision accuracy or confidence in high-stakes scenarios when using CHILLI-generated explanations versus baseline explanations.

### Open Question 3
- Question: How does CHILLI's performance scale with increasing feature dimensionality and complexity of feature dependencies?
- Basis in paper: [inferred] The paper evaluates CHILLI on datasets with moderate complexity, but does not explore performance under high-dimensional or highly interdependent feature spaces.
- Why unresolved: The computational and methodological challenges of scaling context-aware perturbation generation to complex, high-dimensional data are not addressed.
- What evidence would resolve it: Experiments on high-dimensional datasets with known complex feature dependencies, comparing explanation faithfulness and computational efficiency of CHILLI to baseline methods.

## Limitations
- Experimental validation is limited to only two datasets (WebTRIS and MIDAS), raising concerns about generalizability across diverse domains.
- The method for handling categorical features during interpolation is underspecified, which could affect practical implementation.
- No comparison is made with other contemporary XAI methods like SHAP or Anchors that address similar concerns about locality and realism.

## Confidence
- **High confidence**: The core mechanism of using feature-specific distance functions and SMOTE-inspired perturbation generation is technically sound and well-grounded in the literature on distance metrics and synthetic data generation.
- **Medium confidence**: The experimental results showing improved faithfulness metrics are promising but limited in scope and require broader validation across different model types and datasets.
- **Low confidence**: The claim that CHILLI will consistently produce more trustworthy explanations in real-world applications lacks sufficient empirical backing beyond the specific use cases presented.

## Next Checks
1. Test CHILLI's performance across at least five additional diverse datasets (e.g., healthcare, finance, image classification) to assess generalizability beyond traffic and weather domains.
2. Implement and compare CHILLI against other locality-aware XAI methods like SHAP or Anchors to establish its relative performance in explaining complex model behaviors.
3. Conduct a user study with domain experts to evaluate whether CHILLI's explanations lead to better decision-making and understanding compared to LIME and other baseline methods.