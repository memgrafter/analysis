---
ver: rpa2
title: 'Babysit A Language Model From Scratch: Interactive Language Learning by Trials
  and Demonstrations'
arxiv_id: '2405.13828'
source_url: https://arxiv.org/abs/2405.13828
tags:
- language
- learning
- words
- word
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how corrective feedback from interactions
  influences neural language acquisition from scratch through controlled experiments.
  The authors introduce a trial-and-demonstration (TnD) learning framework that incorporates
  three components: student trials, teacher demonstrations, and a reward conditioned
  on language competence at various developmental stages.'
---

# Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations

## Quick Facts
- arXiv ID: 2405.13828
- Source URL: https://arxiv.org/abs/2405.13828
- Reference count: 40
- Key outcome: Trial-and-Demonstration learning accelerates word acquisition through teacher demonstrations and active student trials under age-conditioned rewards

## Executive Summary
This paper investigates how corrective feedback from interactions influences neural language acquisition from scratch through controlled experiments. The authors introduce a trial-and-demonstration (TnD) learning framework that incorporates three components: student trials, teacher demonstrations, and a reward conditioned on language competence at various developmental stages. Experiments reveal that the TnD approach accelerates word acquisition for student models of equal and smaller numbers of parameters, highlighting the significance of both trials and demonstrations. The teacher's choices of words influence students' word-specific learning efficiency, and a strong correlation exists between the frequency of words in trials and their respective learning curves. The findings suggest that interactive language learning with teacher demonstrations and active trials can facilitate efficient word learning in language models.

## Method Summary
The TnD framework uses a student model (GPT-2 from scratch) that generates trials from 5-token prompts, while a teacher model (pre-trained GPT-2) provides demonstrations for the same prompts. Both are scored by a reward model (LLaMA-2-7B fine-tuned to predict training step from text) using an age-conditioned reward function: r = log(ŝ/n), where ŝ is the predicted step and n is the current step. Training alternates between c steps of CLM and r steps of PPO (c=3, r=1) for 10,000 steps with 5 random seeds. The reward encourages accelerated learning by giving higher scores when the predicted step exceeds the current step.

## Key Results
- TnD learning accelerates word acquisition compared to CLM alone, especially for student models of equal or smaller parameter sizes
- Teacher demonstrations significantly influence word-specific learning efficiency, with words excluded from demonstrations learned more slowly
- Strong correlation exists between trial frequency and word learning curves, indicating a practice-makes-perfect effect
- nAoA (neural Age of Acquisition) provides a quantitative measure of learning efficiency across different words

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Trial-and-Demonstration (TnD) learning accelerates word acquisition by combining student-generated trials with teacher-generated demonstrations under an age-conditioned reward signal.
- Mechanism: The student model produces a trial completion from a 5-token prompt, the teacher model provides a high-quality demonstration, and both are scored by a reward model predicting the model's developmental step. Reinforcement learning updates the student policy to align its outputs with the teacher's, while the reward encourages faster learning by comparing current step to predicted step.
- Core assumption: The age-conditioned reward (log(ŝ/n)) meaningfully distinguishes developmentally appropriate language quality and provides gradient signals for efficient learning.
- Evidence anchors: [abstract] "We introduce a trial-and-demonstration (TnD) learning framework that incorporates three distinct components: student trials, teacher demonstrations, and a reward conditioned on language competence at various developmental stages." [section 2.3] "The age-conditioned reward r(S, n) is given by: r := log(ŝ/n) = Rϕ(S) − log n"

### Mechanism 2
- Claim: Active trials provide practice-makes-perfect effects, particularly for function words and predicates, by increasing word frequency in production.
- Mechanism: Student trials generate repeated exposure to specific words in active production contexts, and cumulative word frequency in trials predicts lower surprisal (better acquisition). This effect is strongest for closed-class words that depend on other words for full meaning.
- Core assumption: Production frequency in trials correlates with learning efficiency more strongly than passive exposure in corpus or demonstrations.
- Evidence anchors: [abstract] "a practice-makes-perfect effect is evident by a strong correlation between the frequency of words in trials and their respective learning curves." [section 4.4] "We observe, interestingly, that the learning curves for certain words exhibit a pronounced correlation with the frequency of these words in trials."

### Mechanism 3
- Claim: Teacher demonstrations influence word-specific learning efficiency by selectively exposing students to preferred vocabulary.
- Mechanism: The teacher model, acting as a proxy caregiver, chooses words to demonstrate. Words excluded from demonstrations are learned more slowly, indicating that teacher word selection biases student acquisition.
- Core assumption: The teacher model's word choice is not random but reflects a preference that shapes student learning.
- Evidence anchors: [abstract] "the teacher's choices of words influence students' word-specific learning efficiency." [section 4.3] "the teacher model's word choices significantly influence the efficiency of word acquisition by the student model."

## Foundational Learning

- Concept: Reinforcement Learning with Proximal Policy Optimization (PPO)
  - Why needed here: To update the student model's policy based on the age-conditioned reward from both trials and demonstrations, enabling interactive learning beyond static supervised objectives.
  - Quick check question: How does PPO's clipped surrogate objective prevent large policy updates that could destabilize learning?

- Concept: Age-conditioned reward modeling
  - Why needed here: To provide developmental-stage-appropriate feedback by predicting the training step at which a given text typically emerges, enabling the system to reward accelerated learning.
  - Quick check question: What happens to the reward signal if the neural age predictor is poorly calibrated?

- Concept: Double-sigmoid learning curve fitting
  - Why needed here: To capture the plateau behavior in learning curves where surprisal levels off at unigram frequencies, providing more accurate modeling of acquisition dynamics.
  - Quick check question: Why might a single-sigmoid function be insufficient for modeling word learning curves in this context?

## Architecture Onboarding

- Component map:
  Student model (GPT-2 from scratch) -> Trial generation -> PPO update
  Teacher model (pre-trained GPT-2) -> Demonstration generation -> PPO update
  Reward model (LLaMA-2-7B) -> Age prediction -> Reward computation
  Alternating scheduler -> CLM/PPO steps -> Model parameter updates

- Critical path:
  1. Pre-train teacher model on corpus (CLM)
  2. Sample checkpoints, generate (text, step) pairs, fine-tune reward model
  3. Initialize student model from scratch
  4. For each training step: with probability c/(c+r) update student via CLM, else generate trial/demonstration, score with reward, update student via PPO
  5. Evaluate learning curves, nAoA, vocabulary size

- Design tradeoffs:
  - Fixed alternating schedule vs. adaptive scheduling based on learning progress
  - Using teacher-generated vs. ground-truth corpus sentences as demonstrations
  - Age-conditioned reward vs. other reward signals (BLEU, human feedback)

- Failure signatures:
  - Learning curves plateau too early → reward model under-predicting step or insufficient trial frequency
  - nAoA increases over training → reward model over-predicting step or PPO learning rate too high
  - No difference between TnD and CLM → teacher demonstrations too similar to student trials or reward signal too weak

- First 3 experiments:
  1. Run TnD vs. CLM ablation on small dataset with 1 random seed, check if learning curves differ
  2. Test impact of removing demonstrations (Trial-only) vs. removing trials (Demo-only) on vocabulary acquisition
  3. Vary alternating frequency [c,r] and learning rate to find stable training regime

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different reward models impact the effectiveness of the TnD framework in neural language acquisition?
- Basis in paper: [explicit] The paper acknowledges limitations with the current reward model and suggests future research should explore the impact of using less accurate reward models.
- Why unresolved: The study relies on a robust LLaMA-2-7B model as a reward model, but doesn't investigate how different reward models might affect learning efficiency or outcomes.
- What evidence would resolve it: Comparative experiments using various reward models (including less accurate ones) and analyzing their impact on word acquisition speed, final model performance, and learning curve shapes.

### Open Question 2
- Question: Does the TnD framework maintain its effectiveness when extended to languages other than English?
- Basis in paper: [explicit] The paper explicitly states it focuses on English due to available corpus resources and suggests future research should explore other languages.
- Why unresolved: The study's findings may be specific to English language structure and corpus characteristics, limiting generalizability to other languages with different grammatical structures or morphological complexity.
- What evidence would resolve it: Replication studies using the TnD framework with corpora from multiple languages, comparing acquisition patterns and efficiency metrics across different language families.

### Open Question 3
- Question: What is the long-term developmental trajectory of models trained with the TnD framework beyond 10,000 training steps?
- Basis in paper: [inferred] The paper focuses on early word acquisition (up to 10k steps) but acknowledges that models eventually converge to CLM baseline performance, suggesting potential limitations in long-term development.
- Why unresolved: The study's scope is limited to early developmental stages, leaving questions about whether the TnD approach provides lasting advantages or if benefits diminish over extended training periods.
- What evidence would resolve it: Extended training experiments tracking model performance and vocabulary acquisition patterns at various stages (e.g., 50k, 100k, 1M steps) to identify whether TnD provides sustained advantages or plateaus like CLM.

## Limitations
- The reward model's accuracy in predicting developmental stages is not thoroughly validated across different developmental stages
- Experimental setup uses a relatively small corpus (10M tokens) and limited training duration (10,000 steps)
- Comparison between TnD and baseline CLM is conducted only at the final training step rather than tracking the learning trajectory
- The teacher model is a pre-trained GPT-2 rather than a true "caregiver" model that would have learned from scratch alongside the student

## Confidence

**High Confidence** (supported by direct evidence):
- The TnD framework successfully accelerates word acquisition compared to CLM alone
- Teacher demonstrations influence word-specific learning efficiency
- A strong correlation exists between trial frequency and word learning curves

**Medium Confidence** (mechanisms plausible but need more validation):
- The practice-makes-perfect effect through active trials is the primary driver of efficiency gains
- The age-conditioned reward provides meaningful developmental feedback
- The PPO optimization effectively learns from the combined trial-demonstration signal

**Low Confidence** (mechanisms under-specified or weakly supported):
- The reward model's predictions accurately reflect developmental stages
- The alternating schedule (3 CLM : 1 PPO) is optimal for learning
- The student model would scale effectively to larger parameter sizes

## Next Checks

1. **Reward Model Calibration Analysis**: Conduct a detailed evaluation of the age predictor's calibration by testing its predictions on held-out validation sets across different developmental stages. Plot predicted vs. actual training steps to assess whether the reward signal meaningfully distinguishes between developmental levels.

2. **Learning Trajectory Comparison**: Modify the experimental design to compare TnD and CLM learning curves at multiple intermediate checkpoints (e.g., every 1,000 steps) rather than only at the final 10,000 steps. This would reveal when interactive benefits emerge and whether they persist or diminish over time.

3. **Teacher Model Ablation Study**: Replace the pre-trained teacher with a teacher that also learns from scratch to assess whether the current setup underestimates or overestimates the true impact of teacher demonstrations on student learning. Compare word acquisition patterns between the pre-trained teacher and scratch teacher conditions.