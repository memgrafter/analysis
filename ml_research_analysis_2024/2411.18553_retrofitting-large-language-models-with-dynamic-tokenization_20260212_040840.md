---
ver: rpa2
title: Retrofitting Large Language Models with Dynamic Tokenization
arxiv_id: '2411.18553'
source_url: https://arxiv.org/abs/2411.18553
tags:
- tokenization
- embeddings
- dynamic
- token
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiencies and cross-lingual performance
  disparities caused by static subword tokenizers in large language models. The authors
  propose retrofitting LMs with dynamic tokenization, which adaptively adjusts token
  boundaries using a hypernetwork to generate embeddings on-the-fly.
---

# Retrofitting Large Language Models with Dynamic Tokenization

## Quick Facts
- **arXiv ID**: 2411.18553
- **Source URL**: https://arxiv.org/abs/2411.18553
- **Reference count**: 40
- **Primary result**: Dynamic tokenization reduces sequence lengths by >20% in encoder models and up to 17% in decoder models with <2% performance loss

## Executive Summary
This paper proposes a novel method to address the inefficiencies of static subword tokenization in large language models (LMs) by introducing dynamic tokenization. The approach uses a hypernetwork to adaptively generate token embeddings on-the-fly, reducing sequence lengths and improving computational efficiency across multiple languages. The method is evaluated on both encoder (XLM-R) and decoder (Mistral-7B) models, showing significant sequence reduction with minimal performance degradation. This work highlights the potential for dynamic tokenization to enable more equitable compute allocation and faster inference, particularly in multilingual settings.

## Method Summary
The authors propose retrofitting LMs with dynamic tokenization by replacing static token embeddings with a hypernetwork that generates embeddings dynamically. This hypernetwork takes a seed token and an expansion length as input, producing embeddings for the expanded subword sequence. For encoder models, the hypernetwork is trained to minimize the difference between the dynamic and static tokenization outputs. For decoder models, the hypernetwork is integrated into the causal attention mechanism, allowing for adaptive token generation during inference. The method is evaluated on multilingual benchmarks, demonstrating reduced sequence lengths and improved efficiency with minimal performance loss.

## Key Results
- Dynamic tokenization reduces sequence lengths by >20% across 14 languages for encoder models with <2% performance loss.
- For decoder models, sequence reduction reaches up to 17% with minimal degradation in prefilling and scoring tasks.
- The method enables inference speed improvements and promotes equitable compute allocation across languages.

## Why This Works (Mechanism)
Dynamic tokenization works by replacing static token embeddings with a hypernetwork that generates embeddings adaptively. This allows the model to adjust token boundaries based on context, reducing sequence lengths and improving efficiency. The hypernetwork is trained to minimize the difference between dynamic and static tokenization outputs, ensuring that performance is preserved while achieving computational gains.

## Foundational Learning

### Hypernetworks
- **Why needed**: To generate token embeddings dynamically based on context.
- **Quick check**: Verify that the hypernetwork can produce embeddings that closely match static tokenization outputs.

### Causal Attention
- **Why needed**: To integrate dynamic tokenization into decoder models without breaking the autoregressive nature of the model.
- **Quick check**: Ensure that the hypernetwork's output is compatible with the causal attention mechanism.

### Subword Tokenization
- **Why needed**: To understand the limitations of static tokenization and the potential benefits of dynamic approaches.
- **Quick check**: Compare the efficiency and performance of static vs. dynamic tokenization across multiple languages.

## Architecture Onboarding

### Component Map
Hypernetwork -> Dynamic Token Embeddings -> LM Embeddings -> LM Output

### Critical Path
The critical path involves the hypernetwork generating token embeddings, which are then used by the LM for processing. The efficiency of this path directly impacts the overall performance of the dynamic tokenization approach.

### Design Tradeoffs
- **Performance vs. Efficiency**: Dynamic tokenization achieves sequence reduction but may introduce computational overhead due to the hypernetwork.
- **Generalization vs. Specificity**: The method must balance adaptability to different languages with the need for consistent performance across tasks.

### Failure Signatures
- **Performance Degradation**: If the hypernetwork fails to generate embeddings that closely match static tokenization outputs, performance may degrade.
- **Computational Overhead**: If the hypernetwork introduces significant computational overhead, the efficiency gains may be negated.

### 3 First Experiments
1. Evaluate the hypernetwork's ability to generate embeddings that match static tokenization outputs across multiple languages.
2. Test the integration of dynamic tokenization into decoder models, ensuring compatibility with causal attention.
3. Measure the sequence reduction and performance impact of dynamic tokenization on a multilingual benchmark.

## Open Questions the Paper Calls Out
None

## Limitations
- The computational overhead and scalability of the hypernetwork with larger models remain under-characterized.
- The method's performance in multi-task or multi-stage fine-tuning scenarios is unclear.
- Real-world deployment benefits, such as inference speed and memory efficiency, may vary with hardware and workload characteristics.

## Confidence
- **High Confidence**: Claims about sequence length reduction and general feasibility are well-supported by controlled experiments.
- **Medium Confidence**: Performance preservation claims are credible but contingent on the specific evaluation suite used.
- **Low Confidence**: Real-world deployment benefits are supported by limited ablation studies and may vary significantly with hardware and workload.

## Next Checks
1. Evaluate the method's robustness on a broader multilingual benchmark, including low-resource languages and diverse linguistic phenomena.
2. Conduct end-to-end latency and memory profiling on target hardware under realistic batch sizes and sequence lengths.
3. Test the approach's stability and performance when integrated into multi-stage fine-tuning pipelines or applied to encoder-decoder models.