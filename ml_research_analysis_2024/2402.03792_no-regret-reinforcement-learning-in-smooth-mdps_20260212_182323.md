---
ver: rpa2
title: No-Regret Reinforcement Learning in Smooth MDPs
arxiv_id: '2402.03792'
source_url: https://arxiv.org/abs/2402.03792
tags:
- mdps
- smooth
- regret
- function
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies reinforcement learning in Markov decision processes
  with continuous state and action spaces under a smoothness assumption. The authors
  introduce two new algorithms, LEGENDRE-ELEANOR and LEGENDRE-LSVI, which use orthogonal
  feature maps based on Legendre polynomials to achieve no-regret guarantees.
---

# No-Regret Reinforcement Learning in Smooth MDPs

## Quick Facts
- arXiv ID: 2402.03792
- Source URL: https://arxiv.org/abs/2402.03792
- Reference count: 40
- Key result: Two new algorithms achieve no-regret guarantees in continuous-state MDPs using Legendre polynomial features

## Executive Summary
This paper addresses reinforcement learning in Markov decision processes with continuous state and action spaces by leveraging smoothness assumptions. The authors introduce two algorithms, LEGENDRE-ELEANOR and LEGENDRE-LSVI, which use orthogonal feature maps based on Legendre polynomials to achieve improved regret bounds compared to existing methods. LEGENDRE-ELEANOR works under weaker assumptions but is computationally inefficient, while LEGENDRE-LSVI runs in polynomial time but requires stronger smoothness conditions. Both algorithms demonstrate better performance than standard polynomial approaches in modified linear quadratic regulator environments.

## Method Summary
The paper develops reinforcement learning algorithms for smooth MDPs using Legendre polynomial feature maps. LEGENDRE-ELEANOR operates under weaker smoothness assumptions but suffers from computational inefficiency, while LEGENDRE-LSVI achieves polynomial-time complexity at the cost of requiring stronger smoothness conditions. Both algorithms build MDP representations through orthogonal feature maps, with regret bounds that improve as the smoothness parameter increases. The methods are validated on modified linear quadratic regulators with continuous state and action spaces.

## Key Results
- Both LEGENDRE-ELEANOR and LEGENDRE-LSVI achieve no-regret guarantees in smooth MDPs
- LEGENDRE-LSVI runs in polynomial time while LEGENDRE-ELEANOR is computationally inefficient
- Legendre polynomial features outperform standard polynomial bases in experimental evaluations
- Regret bounds improve with higher smoothness parameters (ν)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Legendre polynomial feature maps enable low inherent Bellman error in smooth MDPs
- Mechanism: Orthogonal Legendre polynomials provide an optimal basis for approximating smooth functions in continuous spaces. The orthogonality property ensures that the Bellman operator's output stays close to the span of the feature map, resulting in bounded inherent Bellman error.
- Core assumption: The MDP transition and reward functions are sufficiently smooth (ν-smooth)
- Evidence anchors:
  - [abstract]: "Both algorithms build upon the idea of constructing an MDP representation through an orthogonal feature map based on Legendre polynomials"
  - [section 4]: "As for the Fourier series, we can use a linear combination of Legendre polynomials to approximate any smooth function"
  - [corpus]: Weak - only tangentially related works on orthogonal polynomials, no direct evidence about Bellman error
- Break condition: When smoothness assumption fails (e.g., discontinuous dynamics) or when ν < d/2 - 1 for LEGENDRE-ELEANOR

### Mechanism 2
- Claim: Legendre representation converts strongly smooth MDPs to linear MDPs
- Mechanism: Under strong smoothness, the Bellman operator maps smooth functions to smooth functions. Using Legendre features, this smoothness translates to linear structure where transition dynamics can be expressed as inner products with unknown parameters.
- Core assumption: Strong smoothness condition holds and ν ≥ d - 1
- Evidence anchors:
  - [abstract]: "LEGENDRE -LSVI runs in polynomial time, although for a smaller class of problems"
  - [section 4.2]: "when we apply the Legendre representation of a Strongly Smooth MDP, we get not only an MDP with low inherent Bellmann error, but a Linear MDP"
  - [corpus]: Weak - related works mention linear MDPs but don't connect directly to Legendre polynomials
- Break condition: When strong smoothness condition fails or when ν < d - 1

### Mechanism 3
- Claim: Orthogonal features provide better approximation than standard polynomial bases
- Mechanism: Orthogonal polynomials minimize the condition number of the feature matrix, leading to more stable learning and faster convergence compared to non-orthogonal bases where terms can be highly correlated.
- Core assumption: Smoothness of the underlying functions being approximated
- Evidence anchors:
  - [section 4.3]: "Using standard polynomials as feature maps is common in practice. However, the results show that baselines using Legendre polynomials achieve much superior episodic return"
  - [section 4.1]: "Legendre polynomials are such that ∫ φL,i(x)φL,j(x) dx = δij, which is 1 if i = j, 0 otherwise"
  - [corpus]: Weak - no direct evidence in corpus about comparison between orthogonal and non-orthogonal bases
- Break condition: When approximation order is too low to capture the function complexity

## Foundational Learning

- Concept: Banach space structure of Cν,1 functions
  - Why needed here: The smoothness assumption is formalized using the Cν,1 norm, which requires understanding Banach spaces and function space norms
  - Quick check question: Why does the definition of Cν,1 include derivatives up to order ν+1 rather than just ν?

- Concept: Legendre polynomial orthogonality and completeness
  - Why needed here: The effectiveness of Legendre features relies on their orthogonality property and ability to approximate smooth functions
  - Quick check question: What is the key orthogonality property of Legendre polynomials that makes them useful for function approximation?

- Concept: Bellman operators and their properties
  - Why needed here: The algorithms rely on understanding how Bellman operators behave under smoothness conditions
  - Quick check question: How does the Bellman optimality operator differ from the Bellman policy operator in terms of their action on value functions?

## Architecture Onboarding

- Component map:
  - Legendre feature map generator (φd_L,N) -> Bellman error computation module -> Algorithm core (LEGENDRE-ELEANOR or LEGENDRE-LSVI) -> Regret analysis component -> Experimental validation suite

- Critical path:
  1. Generate Legendre feature map based on problem dimension d and approximation order N
  2. Compute inherent Bellman error bound using smoothness properties
  3. Run algorithm with feature map to minimize regret
  4. Analyze regret bounds and verify theoretical guarantees

- Design tradeoffs:
  - LEGENDRE-ELEANOR: Weaker assumptions but computationally inefficient vs LEGENDRE-LSVI: Stronger assumptions but polynomial-time
  - Higher N improves approximation but increases computational cost
  - Choice of smoothness order ν affects both theoretical guarantees and practical performance

- Failure signatures:
  - Poor performance when smoothness assumption violated (e.g., discontinuous dynamics)
  - Numerical instability when N is too large relative to available data
  - Suboptimal performance when ν is too small relative to problem dimension d

- First 3 experiments:
  1. Verify Legendre polynomial orthogonality numerically by computing inner products
  2. Test approximation quality on known smooth functions using Legendre vs standard polynomial bases
  3. Run LEGENDRE-ELEANOR on a simple 1D smooth MDP to verify no-regret property

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact computational complexity of LEGENDRE-ELEANOR compared to LEGENDRE-LSVI?
- Basis in paper: [inferred] The paper states LEGENDRE-ELEANOR is "computationally inefficient" while LEGENDRE-LSVI has "polynomial time complexity".
- Why unresolved: The paper doesn't provide explicit time complexity analysis for LEGENDRE-ELEANOR or direct comparison with LEGENDRE-LSVI.
- What evidence would resolve it: A detailed time complexity analysis of both algorithms, including concrete Big-O bounds.

### Open Question 2
- Question: Can the no-regret guarantees be extended to MDPs with non-smooth reward functions?
- Basis in paper: [explicit] The paper assumes smooth reward functions in both Strongly and Weakly Smooth MDPs.
- Why unresolved: The paper doesn't explore what happens when the reward function is non-smooth while maintaining smooth transitions.
- What evidence would resolve it: Theoretical analysis showing whether no-regret bounds still hold or break down with non-smooth rewards.

### Open Question 3
- Question: How does the choice of polynomial degree N affect the exploration-exploitation tradeoff in practice?
- Basis in paper: [inferred] The paper mentions N = rK^{1/(d+2(ν+1))} as the choice for both algorithms, but doesn't discuss practical exploration-exploitation dynamics.
- Why unresolved: The theoretical analysis focuses on regret bounds but doesn't explore how N impacts the balance between exploring new policies and exploiting known good ones.
- What evidence would resolve it: Empirical studies showing how different values of N affect learning curves and policy quality during training.

## Limitations
- Theoretical analysis relies on strong smoothness assumptions that may not hold in practical RL settings
- Computational inefficiency of LEGENDRE-ELEANOR limits its practical applicability
- Experimental validation is limited to modified linear quadratic regulators, a narrow class of problems

## Confidence

**High Confidence**: The Legendre polynomial approximation theory is well-established in numerical analysis literature. The orthogonality property and completeness of Legendre polynomials are mathematically proven.

**Medium Confidence**: The regret bounds and their dependence on smoothness parameters are derived under idealized assumptions. While the mathematical derivations appear sound, the practical applicability depends on how well real MDPs satisfy the smoothness conditions.

**Low Confidence**: The experimental validation is limited to modified LQRs, which represent a narrow class of problems. The comparison with standard polynomial bases, while promising, lacks systematic evaluation across diverse problem types.

## Next Checks
1. Test algorithm performance on MDPs with known discontinuities to verify the claimed break conditions when smoothness assumptions fail.

2. Implement a systematic comparison of Legendre features against other orthogonal polynomial bases (e.g., Chebyshev) and standard non-orthogonal bases across multiple smooth function approximation tasks.

3. Evaluate the algorithms on non-linear control problems beyond LQRs to assess practical performance in more general settings.