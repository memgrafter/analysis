---
ver: rpa2
title: 'Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement
  Learning'
arxiv_id: '2402.07107'
source_url: https://arxiv.org/abs/2402.07107
tags:
- uncertainty
- learning
- evidential
- quantile
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CEQR-DQN, a novel approach that combines deep
  evidential learning with conformalized quantile regression to estimate aleatoric
  and epistemic uncertainty in distributional reinforcement learning. The method addresses
  key challenges in separately estimating these uncertainties in stochastic environments
  and handling out-of-distribution observations.
---

# Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning

## Quick Facts
- arXiv ID: 2402.07107
- Source URL: https://arxiv.org/abs/2402.07107
- Authors: Alex Christopher Stutts; Danilo Erricolo; Theja Tulabandhula; Amit Ranjan Trivedi
- Reference count: 12
- Key outcome: CEQR-DQN combines deep evidential learning with conformalized quantile regression to estimate aleatoric and epistemic uncertainty in distributional RL, achieving faster learning and superior performance on MinAtar games compared to UA-DQN.

## Executive Summary
This paper introduces CEQR-DQN, a novel approach that addresses the challenge of estimating aleatoric and epistemic uncertainty in distributional reinforcement learning. By combining deep evidential learning with conformalized quantile regression, the method provides explicit, sample-free computations of global uncertainty. The approach is tested on MinAtar, a miniaturized Atari suite, demonstrating significant improvements in both scores and learning speed compared to existing methods like UA-DQN.

## Method Summary
CEQR-DQN combines conformalized quantile regression with deep evidential learning to estimate both aleatoric and epistemic uncertainty in distributional reinforcement learning. The method uses a single-layer CNN feature extractor feeding separate heads for action quantiles and evidential parameters. Thompson sampling based on calibrated uncertainty estimates guides action selection. The algorithm incorporates quantile calibration using conformal inference principles and evidential learning to provide robust uncertainty quantification. Training involves optimizing a combination of quantile regression loss, evidential learning loss, and calibration losses.

## Key Results
- CEQR-DQN outperforms UA-DQN on all five MinAtar games tested
- The method achieves faster learning speeds across all environments
- Superior final performance is demonstrated, with significant score improvements on games like Breakout and Seaquest
- The approach provides reliable uncertainty estimates that correlate with true model uncertainty

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining conformal inference with evidential learning enables both accurate quantile calibration and robust OOD uncertainty handling.
- **Mechanism:** Conformal inference provides statistical guarantees for marginal coverage of target quantiles, while evidential learning models higher-order distributions over likelihoods, allowing explicit computation of aleatoric and epistemic uncertainty without sampling. Together, they refine Q-value estimates and maintain reliable uncertainty estimates even for OOD observations.
- **Core assumption:** Conformal inference can be adapted to reinforcement learning despite non-i.i.d. data, and evidential distributions can be efficiently parameterized and trained with deep networks.
- **Evidence anchors:**
  - [abstract] "combines deep evidential learning with quantile calibration based on principles of conformal inference to provide explicit, sample-free computations of global uncertainty"
  - [section] "We adapt a conformal inference-based calibration method proposed in [Stutts et al., 2023] and referred to as conformalized joint prediction (CJP). Given inputs X, this method jointly trains a network to simultaneously output multivariate point predictions ˆY and lower and upper bound conditional quantiles that form an uncertainty-aware confidence interval C(X)"
  - [corpus] Weak: No direct corpus papers discussing conformal inference + evidential learning in RL; this appears novel.
- **Break condition:** If the marginal coverage guarantee of conformal inference fails in RL due to strong distributional shifts, or if evidential learning becomes computationally intractable for high-dimensional state spaces.

### Mechanism 2
- **Claim:** Calibrating both the target distribution quantiles (Zθ) and evidential distribution quantiles improves action selection accuracy and exploration efficiency.
- **Mechanism:** Calibration loss functions (Lcal, Linterval) adjust predicted quantiles to match desired coverage rates and center the target within the confidence interval. This leads to more accurate Q-value estimates and tighter uncertainty bounds, enabling the agent to make better-informed exploration-exploitation decisions.
- **Core assumption:** Calibration objectives can be optimized alongside the main reinforcement learning loss without destabilizing training, and centering the TD target within the evidential confidence interval improves exploration.
- **Evidence anchors:**
  - [abstract] "incorporates separate, explicit, and statistically robust computations of aleatoric and epistemic uncertainty as feedback for balancing exploration and exploitation in action selection"
  - [section] "For the evidential sample mean γ we only predict and calibrate the 5th and 95th percentiles so as to establish a 90% evidence-based confidence interval Cevi(X) around the TD target from which we gauge uncertainty"
  - [corpus] Weak: No direct corpus papers discussing calibration of evidential distribution quantiles in RL; this appears novel.
- **Break condition:** If calibration objectives interfere with the main RL loss, leading to poor performance, or if the centering assumption does not hold in complex environments.

### Mechanism 3
- **Claim:** Thompson sampling using calibrated evidential uncertainty estimates leads to more efficient exploration than epsilon-greedy or random exploration strategies.
- **Mechanism:** The algorithm computes action-specific epistemic and aleatoric uncertainty, uses them to form a covariance matrix, and samples actions from a multivariate normal distribution. This allows the agent to explore actions with high uncertainty while exploiting actions with high expected returns, leading to faster learning and better final performance.
- **Core assumption:** The learned evidential uncertainty estimates are meaningful and correlate with the true uncertainty in the Q-value estimates, and Thompson sampling is an effective exploration strategy in this context.
- **Evidence anchors:**
  - [abstract] "Combining calibrated quantile regression with deep evidential learning enhances the algorithm’s uncertainty awareness by providing global network uncertainty estimates over the entire known sample space as opposed to local estimates based solely on the variance and expectation of Q-value quantiles"
  - [section] "Actions are selected via Thompson sampling of a multivariate normal distribution based on the expectation of action quantiles and the covariance between the evidential uncertainty associated with each action"
  - [corpus] Weak: No direct corpus papers discussing Thompson sampling with calibrated evidential uncertainty in RL; this appears novel.
- **Break condition:** If the evidential uncertainty estimates are poorly calibrated or do not capture the true uncertainty, leading to suboptimal exploration, or if Thompson sampling is not well-suited to the specific characteristics of the RL problem.

## Foundational Learning

- **Concept:** Quantile Regression
  - Why needed here: To learn the distribution of returns (Q-values) rather than just point estimates, enabling uncertainty quantification.
  - Quick check question: What is the loss function used to train quantile regression models, and how does it differ from mean squared error?

- **Concept:** Conformal Inference
  - Why needed here: To provide statistical guarantees on the coverage of predicted quantiles, ensuring reliable uncertainty estimates.
  - Quick check question: What is the key assumption of conformal inference, and how is it adapted to handle non-i.i.d. data in reinforcement learning?

- **Concept:** Evidential Learning
  - Why needed here: To model higher-order distributions over likelihoods, enabling explicit computation of aleatoric and epistemic uncertainty without sampling.
  - Quick check question: What is the evidential distribution used in this work, and how are its parameters interpreted in terms of evidence and uncertainty?

## Architecture Onboarding

- **Component map:**
  CNN feature extractor -> Action quantile head + Evidential parameter head -> Uncertainty computation module -> Thompson sampling module -> Environment interaction -> Reward + next state -> Target quantile computation -> Calibration loss computation -> Parameter update

- **Critical path:**
  1. State observation → CNN features
  2. Features → action quantiles + evidential parameters
  3. Quantiles + parameters → uncertainty estimates
  4. Uncertainty + action means → Thompson sampled action
  5. Action → environment interaction → reward + next state
  6. Next state → target quantile computation
  7. Quantiles + targets → calibration loss
  8. Calibration loss + main RL loss → parameter update

- **Design tradeoffs:**
  - Using a single network vs. ensemble methods for uncertainty estimation (computational efficiency vs. potential overconfidence)
  - Calibrating both target and evidential quantiles (improved accuracy vs. increased complexity)
  - Thompson sampling vs. other exploration strategies (principled uncertainty-based exploration vs. potential computational overhead)

- **Failure signatures:**
  - Poor performance on simple tasks where uncertainty is less critical
  - Instability during training, particularly when calibrating evidential parameters
  - Overconfidence in OOD situations despite evidential learning

- **First 3 experiments:**
  1. Run CEQR-DQN on a simple gridworld task to verify basic functionality and compare to DQN.
  2. Test the calibration of predicted quantiles on a synthetic regression dataset to ensure coverage guarantees.
  3. Evaluate the algorithm on a single MinAtar game (e.g., Breakout) to assess improvements in learning speed and final performance.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the discussion and results, several natural questions arise:
1. How does the performance of CEQR-DQN scale with different hyperparameter values for λal and λep across different Atari games?
2. How does the CEQR-DQN algorithm perform on larger, more complex Atari games compared to MinAtar?
3. How does the CEQR-DQN algorithm handle environments with continuous action spaces?

## Limitations
- Computational overhead from evidential learning and quantile calibration increases training time
- Reliance on the assumption that evidential uncertainty correlates well with true model uncertainty
- Calibration methods assume approximate exchangeability of data points, which may be violated in sequential decision-making settings

## Confidence
- **High Confidence:** The core theoretical foundation combining conformal inference with evidential learning is well-grounded, and the empirical improvements over UA-DQN on MinAtar are clearly demonstrated.
- **Medium Confidence:** The scalability of the approach to more complex environments and the robustness of uncertainty estimates in highly stochastic settings require further validation.
- **Low Confidence:** The sensitivity of the method to hyperparameter choices (particularly λcal and λreg) and its performance on non-Atari benchmark tasks remain uncertain.

## Next Checks
1. Test CEQR-DQN on the full Atari benchmark suite to assess scalability and robustness beyond the MinAtar environment.
2. Conduct ablation studies to quantify the individual contributions of conformal calibration and evidential learning to overall performance.
3. Evaluate the method's uncertainty estimates against ground truth uncertainty in a controlled synthetic environment where true aleatoric and epistemic uncertainty can be measured.