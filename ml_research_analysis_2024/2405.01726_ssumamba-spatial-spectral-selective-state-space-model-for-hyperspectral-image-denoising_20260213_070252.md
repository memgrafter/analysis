---
ver: rpa2
title: 'SSUMamba: Spatial-Spectral Selective State Space Model for Hyperspectral Image
  Denoising'
arxiv_id: '2405.01726'
source_url: https://arxiv.org/abs/2405.01726
tags:
- denoising
- spectral
- methods
- ssumamba
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hyperspectral image (HSI) denoising
  by introducing a memory-efficient spatial-spectral UMamba (SSUMamba) model. The
  core method, spatial-spectral continuous scan (SSCS) Mamba, alternates row, column,
  and band in six different orders to generate sequences and uses bidirectional state
  space models (SSMs) to exploit long-range spatial-spectral dependencies.
---

# SSUMamba: Spatial-Spectral Selective State Space Model for Hyperspectral Image Denoising

## Quick Facts
- arXiv ID: 2405.01726
- Source URL: https://arxiv.org/abs/2405.01726
- Authors: Guanyiman Fu; Fengchao Xiong; Jianfeng Lu; Jun Zhou
- Reference count: 40
- Primary result: SSUMamba achieves superior HSI denoising results with lower memory consumption than transformer-based methods

## Executive Summary
This paper introduces SSUMamba, a memory-efficient spatial-spectral UMamba model for hyperspectral image denoising. The core innovation is the spatial-spectral continuous scan (SSCS) Mamba, which alternates row, column, and band scans in six different orders while using bidirectional state space models to exploit long-range spatial-spectral dependencies. The method incorporates 3D convolutions to enhance local spatial-spectral modeling, achieving superior denoising performance with lower memory consumption compared to transformer-based approaches.

## Method Summary
SSUMamba addresses HSI denoising through a multi-scale architecture combining 3D convolutions with bidirectional state space models. The method processes hyperspectral images by alternating six different spatial-spectral scan orders to generate sequences that preserve local pixel relationships. Bidirectional SSM processing captures dependencies in both forward and backward directions, while residual 3D convolution blocks maintain local texture. The model is trained on ICVL dataset patches with Adam optimizer (learning rate 3e-4, batch size 14) for 45 epochs, with learning rate reduction at epochs 20 and 35.

## Key Results
- Achieves superior denoising performance with higher PSNR than transformer-based methods
- Demonstrates lower memory consumption per batch during training
- Outperforms state-of-the-art methods on synthetic and real-world HSI datasets including ICVL, Houston 2018, Pavia City Center, Gaofen-5 Wuhan, and Earth Observing-1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SSCS preserves local pixel relationships during flattening
- Mechanism: SSCS arranges HSIs using zigzag tail-to-tail or head-to-head connections, preventing spatially adjacent pixels from being scattered far apart in the 1D sequence
- Core assumption: HSI pixels depend on both spatial neighbors and corresponding spectral bands, so preserving adjacency in the sequence is crucial
- Evidence anchors:
  - [abstract] "SSCS Mamba alternates the row, column, and band in six different orders to generate the sequence and uses the bidirectional SSM to exploit long-range spatial-spectral dependencies. In each order, the images are rearranged between adjacent scans to ensure spatial-spectral continuity."
  - [section] "This approach ensures that spatial-spectral continuity is maintained in the resulting sequences."
- Break condition: If the sequence length becomes too large or if scans are not bidirectional, the local continuity could be lost and performance degrades

### Mechanism 2
- Claim: Bidirectional SSM captures dependencies in both forward and backward directions
- Mechanism: After SSCS generates the sequence, the bidirectional SSM applies SSM processing to both the original and flipped sequence, then merges results with gating
- Core assumption: Long-range spatial-spectral dependencies exist in both directions; unidirectional processing misses backward dependencies
- Evidence anchors:
  - [abstract] "uses the bidirectional SSM to exploit long-range spatial-spectral dependencies"
  - [section] "a bidirectional SSM layer flips the generated sequence to facilitate backward scanning, effectively exploiting long-range spatial-spectral dependencies in 12 directions."
- Break condition: If gating fails to balance forward/backward contributions, one direction may dominate and harm denoising

### Mechanism 3
- Claim: Residual 3D convolution blocks preserve local texture while long-range SSM models global dependencies
- Mechanism: Each SSCS Mamba block contains a residual 3D conv block before SSM, capturing local spatial-spectral patterns. The SSM then models long-range dependencies
- Core assumption: Local texture preservation is essential for HSI denoising; pure long-range modeling loses local detail
- Evidence anchors:
  - [abstract] "3D convolutions are embedded into the SSCS Mamba to enhance local spatial-spectral modeling."
  - [section] "The residual block enhances the local texture exploration by maintaining spatial-spectral correlation within a localized context."
- Break condition: If residual blocks are too shallow or skip connections too weak, local texture could be lost

## Foundational Learning

- **State Space Models (SSMs) and Mamba**: Linear complexity models for long-range sequence modeling vs. quadratic attention complexity. *Why needed*: HSI denoising requires efficient modeling of long-range spatial-spectral dependencies. *Quick check*: What is the computational complexity difference between SSM/Mamba and self-attention?

- **Spatial-spectral continuity in hyperspectral data**: HSIs have both spatial adjacency and spectral band continuity. *Why needed*: Scan order must preserve both spatial and spectral relationships for effective modeling. *Quick check*: Why does a naive row-column-band sweep scan harm HSI processing?

- **Bidirectional sequence processing**: Dependencies can propagate in both directions in HSI data. *Why needed*: Bidirectional processing captures dependencies missed by unidirectional approaches. *Quick check*: How does flipping the sequence help in bidirectional SSM?

## Architecture Onboarding

- **Component map**: FeatureExtractor -> Encoder blocks -> SSCS Mamba blocks (6) -> Decoder blocks -> Reconstructor
- **Critical path**: FeatureExtractor → Encoder → SSCS Mamba (all 6) → Decoder → Reconstructor
- **Design tradeoffs**:
  - Linear complexity vs. quadratic attention: faster and more memory efficient
  - Six scan schemes vs. single scheme: better coverage but more parameters
  - Bidirectional SSM vs. unidirectional: captures more dependencies but doubles computation
- **Failure signatures**:
  - Poor PSNR/SSIM: likely loss of local texture or long-range continuity
  - High memory usage: too many channels or inefficient sequence handling
  - Slow inference: insufficient parallelization or overly complex scan schemes
- **First 3 experiments**:
  1. Compare PSNR with and without bidirectional SSM on synthetic ICVL data
  2. Test single sweep scan vs. six SSCS schemes to quantify continuity benefit
  3. Vary number of channels in SSCS Mamba blocks to find optimal memory/performance balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SSCS affect the trade-off between denoising performance and computational efficiency in real-world hyperspectral imaging scenarios?
- Basis in paper: [explicit] The paper states that SSUMamba achieves superior denoising results with lower memory consumption per batch compared to transformer-based methods
- Why unresolved: While the paper demonstrates superior performance in synthetic and real-world datasets, it does not provide detailed analysis of how SSCS affects computational efficiency in different imaging environments or varying noise levels
- What evidence would resolve it: Systematic experiments comparing SSUMamba's performance and computational efficiency across diverse real-world imaging scenarios with varying noise characteristics and data volumes

### Open Question 2
- Question: Can SSCS Mamba block be effectively adapted for other hyperspectral image processing tasks beyond denoising?
- Basis in paper: [explicit] The paper mentions future work to explore integrating SSCS Mamba block into other tasks like HSI superresolution and classification
- Why unresolved: The paper focuses on denoising applications and does not provide evidence or analysis of SSCS Mamba block performance in other hyperspectral image processing tasks
- What evidence would resolve it: Experimental results demonstrating the effectiveness of SSCS Mamba block in tasks such as HSI super-resolution, classification, or other image processing applications

### Open Question 3
- Question: How does SSUMamba compare to other state-of-the-art methods when dealing with HSIs with significantly different numbers of spectral bands than training data?
- Basis in paper: [explicit] The paper mentions that T3SC, TRQ3D, SST, and SERT may face challenges when handling HSIs with distinct bands compared to the training data
- Why unresolved: While the paper acknowledges the challenge, it does not provide comprehensive comparison of SSUMamba's performance with other methods on HSIs with varying numbers of bands
- What evidence would resolve it: Detailed comparative analysis of SSUMamba's performance against other methods on hyperspectral images with varying spectral band counts, particularly those significantly different from the training data

## Limitations
- Theoretical uncertainty about quantitative analysis of how much spatial-spectral continuity is actually preserved
- Lack of ablation studies showing individual contributions of SSCS, bidirectional SSM, and 3D convolutions
- Critical implementation details of bidirectional SSM with six alternating scan schemes are not fully specified

## Confidence
- **High confidence**: Core architecture combining SSCS with bidirectional Mamba blocks is sound
- **Medium confidence**: Claim of superior PSNR performance is supported by experimental results
- **Low confidence**: Specific benefits of alternating six scan orders versus fewer schemes are not well-justified

## Next Checks
1. Implement SSUMamba with 1, 3, and 6 scan schemes to quantify marginal benefit of increased scan variety on PSNR/SSIM metrics
2. Train identical models with only forward SSM and with bidirectional SSM to measure exact performance gain from bidirectional processing
3. Profile memory usage per batch during training for SSUMamba versus transformer-based alternatives using identical hardware and batch sizes to verify claimed efficiency gains