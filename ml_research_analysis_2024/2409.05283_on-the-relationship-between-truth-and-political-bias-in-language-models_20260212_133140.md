---
ver: rpa2
title: On the Relationship between Truth and Political Bias in Language Models
arxiv_id: '2409.05283'
source_url: https://arxiv.org/abs/2409.05283
tags:
- political
- bias
- statements
- reward
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines whether optimizing language models for truthfulness\
  \ inadvertently introduces political bias. The authors train reward models on four\
  \ truthfulness datasets\u2014TruthfulQA, FEVER, SciQ, and a custom dataset of factual\
  \ statements\u2014and evaluate their political bias using TwinViews-13k, a dataset\
  \ of 13,855 left-leaning and right-leaning statement pairs matched by topic."
---

# On the Relationship between Truth and Political Bias in Language Models

## Quick Facts
- arXiv ID: 2409.05283
- Source URL: https://arxiv.org/abs/2409.05283
- Reference count: 11
- Primary result: Reward models trained on truthfulness datasets show consistent left-leaning political bias that increases with model size

## Executive Summary
This paper examines whether optimizing language models for truthfulness inadvertently introduces political bias. The authors train reward models on four truthfulness datasets—TruthfulQA, FEVER, SciQ, and a custom dataset of factual statements—and evaluate their political bias using TwinViews-13k, a dataset of 13,855 left-leaning and right-leaning statement pairs matched by topic. The primary finding is that reward models trained on truthfulness datasets consistently show a left-leaning political bias, with the bias increasing for larger models. This bias persists even after removing explicitly political content from the datasets and controlling for stylistic artifacts. The authors also find that existing open-source reward models already exhibit similar left-leaning bias, suggesting that the alignment process itself may contribute to political skew.

## Method Summary
The authors train reward models on four truthfulness datasets (TruthfulQA, FEVER, SciQ, and a custom LLM-generated factual statements dataset) using Pythia models of varying sizes (160M, 2.8B, 6.9B parameters). They evaluate political bias by measuring reward score differences between left-leaning and right-leaning statements in the TwinViews-13k dataset, which contains 13,855 paired statements matched by topic. The training uses standard reward model fine-tuning procedures with PEFT/LoRA for larger models, and bias is measured as the difference in average reward scores between left and right statements across various topics.

## Key Results
- Reward models trained on truthfulness datasets show consistent left-leaning political bias
- The political bias increases with model size, demonstrating inverse scaling
- The bias persists even after removing explicitly political content from training datasets
- Existing open-source reward models show similar left-leaning bias patterns
- Bias is strongest on topics like climate, energy, and labor unions, but weaker or reversed for taxes and the death penalty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing reward models for truthfulness on datasets designed to capture objective facts leads to left-leaning political bias.
- Mechanism: The datasets used to represent truthfulness contain subtle associations between factual statements and left-leaning political positions. When reward models learn to distinguish true from false statements, they inadvertently learn these associations, resulting in higher rewards for left-leaning statements.
- Core assumption: Truthfulness datasets contain implicit political associations that are not immediately obvious but become embedded in reward model behavior.
- Evidence anchors:
  - [abstract] "Our findings reveal that optimizing reward models for truthfulness on these datasets tends to result in a left-leaning political bias."
  - [section 5] "Our results suggest that even training on supposedly objective datasets can lead to unforeseen bias."
  - [corpus] Found 25 related papers. Top related title: "On the Inevitability of Left-Leaning Political Bias in Aligned Language Models" (weak corpus support for this specific mechanism).
- Break condition: If explicit political content is completely removed from truthfulness datasets and the bias persists, this mechanism would be insufficient to explain the phenomenon.

### Mechanism 2
- Claim: Larger models show stronger political bias due to their ability to capture more subtle patterns in the training data.
- Mechanism: As model size increases, the model's capacity to detect and encode subtle correlations between factual content and political orientation increases, leading to stronger bias expression.
- Core assumption: Model scale directly correlates with the ability to detect and encode subtle statistical patterns that smaller models miss.
- Evidence anchors:
  - [abstract] "We also find that existing open-source reward models (i.e., those trained on standard human preference datasets) already show a similar bias and that the bias is larger for larger models."
  - [section 4] "Notably, larger models also show greater bias, an example of inverse scaling."
  - [corpus] Found 25 related papers. Top related title: "Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models" (weak corpus support for scale-specific bias mechanisms).
- Break condition: If smaller models consistently show the same or greater bias than larger models on the same datasets, this mechanism would be invalid.

### Mechanism 3
- Claim: Stylistic artifacts in truthfulness datasets create spurious correlations between factual content and political orientation.
- Mechanism: Certain linguistic patterns (negation words, sentence structures, etc.) are more common in both false statements and right-leaning political positions. Reward models learn these patterns and apply them broadly.
- Core assumption: Stylistic features in language data are correlated with both truth status and political orientation in ways that create unintended bias.
- Evidence anchors:
  - [section 5.2] "We test this hypothesis with the n-gram baseline... Results on the other three datasets, however, are quite different, without a clear relationship to the direction or magnitude of the bias shown by the neural models."
  - [section 5.2] "Overall, stylistic artifacts do not seem to explain most of the political bias we observe."
  - [corpus] Found 25 related papers. Top related title: "AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents" (weak corpus support for stylistic artifact mechanisms).
- Break condition: If controlled experiments show that removing stylistic features eliminates the bias, this mechanism would be confirmed.

## Foundational Learning

- Concept: Reward model training and evaluation
  - Why needed here: The paper's core methodology involves training and evaluating reward models on truthfulness datasets and measuring their political bias.
  - Quick check question: How does a reward model differ from a standard language model, and what training objective does it optimize?

- Concept: Political bias measurement in language models
  - Why needed here: The paper introduces a novel dataset (TwinViews-13k) and methodology for measuring political bias in reward models.
  - Quick check question: What are the challenges in creating balanced datasets for measuring political bias in language models?

- Concept: Inverse scaling in machine learning
  - Why needed here: The paper observes that larger models show stronger bias, which is an example of inverse scaling.
  - Quick check question: What is inverse scaling, and how does it differ from the typical scaling behavior observed in large language models?

## Architecture Onboarding

- Component map: Base language model → Reward model fine-tuning → Political bias evaluation pipeline
- Critical path: Dataset selection → Reward model training → TwinViews-13k evaluation → Bias analysis
- Design tradeoffs: Using generated datasets for political statements provides scale but may introduce artifacts; using existing truthfulness datasets provides objectivity but may contain implicit biases.
- Failure signatures: If reward models show no bias, the evaluation methodology may be flawed; if bias is uniform across all topics, the measurement may be too coarse.
- First 3 experiments:
  1. Train reward models on cleaned truthfulness datasets (explicit political content removed) and evaluate on TwinViews-13k
  2. Train n-gram baseline models on the same datasets to test for stylistic artifact bias
  3. Train reward models on datasets with varying levels of political content to measure correlation between explicit political content and measured bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of the alignment process (e.g., human preference data, RLHF objectives) are primarily responsible for introducing political bias into reward models?
- Basis in paper: Explicit - The authors note that vanilla reward models show left-leaning bias and that the alignment process may influence models' political bias, but don't isolate which specific aspects are most culpable.
- Why unresolved: The paper compares vanilla reward models trained on different datasets but doesn't conduct controlled experiments to isolate the impact of specific alignment components like RLHF vs. DPO, or different types of human preference data.
- What evidence would resolve it: Controlled experiments training reward models on systematically varied alignment objectives and datasets (e.g., helpfulness-only vs. truthfulness-only vs. full RLHF) while measuring resulting political bias.

### Open Question 2
- Question: Do truthfulness reward models show different patterns of political bias across different model sizes, and if so, what causes this scaling behavior?
- Basis in paper: Explicit - The authors observe inverse scaling where larger models show greater political bias, but don't investigate the underlying causes of this phenomenon.
- Why unresolved: The paper demonstrates the scaling pattern exists but doesn't explore whether it's due to increased capacity capturing more subtle correlations, scaling of pretraining data biases, or other factors.
- What evidence would resolve it: Systematic scaling experiments with controlled pretraining data and architectural variations, combined with interpretability analysis of what larger models learn about political content.

### Open Question 3
- Question: Can the political bias in truthfulness datasets be fully eliminated through careful data curation, or are these biases inherent to how political knowledge is represented in LLMs?
- Basis in paper: Inferred - The authors attempt to remove explicitly political content but still observe bias, suggesting deeper issues with how political knowledge is encoded.
- Why unresolved: The paper audits for explicit political content but doesn't investigate whether political associations are embedded in seemingly apolitical facts or whether certain factual domains inherently contain political valence.
- What evidence would resolve it: Comprehensive auditing of factual datasets for implicit political associations, plus experiments training models on datasets constructed to be politically neutral in multiple dimensions (topic selection, phrasing, etc.).

## Limitations

- The paper does not isolate which specific aspects of the alignment process (e.g., RLHF vs. DPO, different human preference data) are primarily responsible for introducing political bias
- The mechanism explaining why larger models show stronger bias remains speculative and lacks theoretical grounding
- The study focuses on English-language models and datasets, limiting generalizability to other languages and cultural contexts

## Confidence

- **High** for the core empirical finding that reward models trained on truthfulness datasets show consistent left-leaning bias
- **Medium** for the claim that this is an inherent property of truthfulness alignment rather than a dataset-specific artifact
- **Medium-Low** for the proposed explanations of why model scale amplifies bias

## Next Checks

1. **Dataset Decomposition Analysis**: Systematically vary the proportion of explicitly political content in training datasets and measure the resulting bias to determine whether the bias correlates with political content density.

2. **Cross-Lingual Validation**: Train reward models on truthfulness datasets in multiple languages and evaluate political bias to determine whether the left-leaning bias is language-specific or universal across linguistic contexts.

3. **Adversarial Dataset Construction**: Create controlled datasets where factual statements are explicitly matched to political positions (e.g., "The Earth is warming" paired with both left-leaning and right-leaning conclusions) to test whether reward models learn to associate factual accuracy with specific political orientations.