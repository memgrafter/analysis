---
ver: rpa2
title: Towards Understanding Epoch-wise Double descent in Two-layer Linear Neural
  Networks
arxiv_id: '2407.09845'
source_url: https://arxiv.org/abs/2407.09845
tags:
- error
- descent
- double
- point
- curve
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates epoch-wise double descent in two-layer linear
  neural networks, extending previous work on single-layer models. The authors derive
  gradient flow dynamics for the two-layer case, which bridge the learning dynamics
  of standard linear regression and decoupled two-layer networks.
---

# Towards Understanding Epoch-wise Double descent in Two-layer Linear Neural Networks

## Quick Facts
- arXiv ID: 2407.09845
- Source URL: https://arxiv.org/abs/2407.09845
- Authors: Amanda Olmin; Fredrik Lindsten
- Reference count: 40
- Key outcome: Double descent in two-layer linear networks requires both input variance eigenvalues and input-output covariance singular values, with at least one inflection point between weight convergence times.

## Executive Summary
This paper extends the analysis of epoch-wise double descent from single-layer to two-layer linear neural networks. The authors derive gradient flow dynamics for the two-layer case, showing that the generalization error can be decomposed into a sum of individual error curves for each weight. Unlike single-layer models where only input variance eigenvalues matter, in two-layer models both input variance eigenvalues and input-output covariance singular values jointly control learning speed and inflection points. The paper provides necessary conditions for double descent to occur, revealing an additional factor not present in single-layer models.

## Method Summary
The authors analyze epoch-wise double descent in two-layer linear networks by deriving gradient flow dynamics that bridge standard linear regression and decoupled two-layer networks. They assume the input and input-output covariance matrices share singular vectors, enabling decoupled weight evolution. The generalization error is decomposed into individual error curves for each active weight, with the total error being their sum. The analysis focuses on spectral initialization and tracks how singular values of both covariance matrices influence learning trajectories and inflection points that enable double descent.

## Key Results
- Generalization error in two-layer linear networks decomposes into a sum of individual error curves for each weight
- Both input variance eigenvalues and input-output covariance singular values jointly control learning speed and inflection points
- Double descent requires at least one inflection point of an individual error curve to lie between convergence times of different weights
- The decoupled dynamics assumption enables analytical tractability but may not hold in fully trained networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Epoch-wise double descent arises from superposition of individual error curves tied to each weight's learning trajectory.
- Mechanism: Total generalization error is sum over U-shaped, monotonic increasing, or monotonic decreasing individual error curves for each active weight.
- Core assumption: Weights evolve independently under decoupled dynamics (Z remains diagonal).
- Evidence anchors:
  - [abstract] "generalization error can be decomposed into a sum of individual error curves"
  - [section 3.2] Proposition 2 derives this decomposition explicitly
- Break condition: If weights become coupled (non-diagonal Z), superposition fails and new interaction effects may emerge.

### Mechanism 2
- Claim: Singular values of both input covariance (λᵢ) and input-output covariance (λᵢ^(yx)^1/2) jointly control learning speed and inflection points.
- Mechanism: Each weight's trajectory depends on both singular values; larger values accelerate convergence and shift inflection points.
- Core assumption: Initial weights are sufficiently small and learning rates are balanced so both singular values affect dynamics.
- Evidence anchors:
  - [abstract] "singular values of input-output covariance matrix play an important role"
  - [section 3.1] Proposition 1 shows convergence depends on both singular values
  - [section 3.2] Lemma 2 shows inflection points depend on both λᵢ and λᵢ^(yx)^1/2
- Break condition: If one singular value dominates, one error curve dominates and double descent is suppressed.

### Mechanism 3
- Claim: Double descent occurs only if at least one inflection point lies strictly between convergence times of slowest and fastest active weights.
- Mechanism: Necessary condition requires inflection point t̂ satisfying min{t(1−ρᵢ)z*_i} < t̂ < max{t(1−ρᵢ)z*_i}.
- Core assumption: At least two active weights with different convergence times exist.
- Evidence anchors:
  - [abstract] "double descent requires at least one inflection point...to lie between convergence times of different weights"
  - [section 3.2] Corollary 1 gives the simplified two-weight case
- Break condition: If all weights converge at similar times or no inflection points fall in the required interval, double descent does not occur.

## Foundational Learning

- Concept: Gradient flow dynamics in decoupled two-layer networks
  - Why needed here: Entire analysis relies on decoupled dynamics; without it, weight evolution and error curve shapes cannot be computed.
  - Quick check question: What conserved quantity ensures that decoupled weights remain decoupled during training?

- Concept: Singular value decomposition (SVD) and its role in linear regression
  - Why needed here: Assumes input and input-output covariance matrices share singular vectors, enabling decoupled analysis.
  - Quick check question: How does the assumption that X^⊤X and y^⊤X share right singular vectors simplify the dynamics?

- Concept: Bias-variance decomposition in generalization error
  - Why needed here: Generalization error is expressed as sum over bias-variance curves for each weight.
  - Quick check question: In the one-layer case, which matrix's eigenvalues determine the bias-variance trade-off?

## Architecture Onboarding

- Component map:
  - Data matrices: X (inputs), y (targets)
  - Model parameters: W^(1), W^(2) (layer weights)
  - Synaptic weights: Z = Z^(2)Z^(1) (decoupled representation)
  - Learning dynamics: decoupled gradient flow eq. (6)
  - Error decomposition: sum over individual weight error curves eq. (10)

- Critical path:
  1. Compute SVD of X and y^⊤X
  2. Initialize Z diagonal with small values
  3. Evolve each diagonal element via eq. (6)
  4. Evaluate individual error curves eq. (11)
  5. Sum to get total generalization error eq. (10)
  6. Check for inflection points and convergence times to detect double descent

- Design tradeoffs:
  - Decoupling weights simplifies analysis but may miss coupling effects seen in fully trained networks
  - Assuming shared singular vectors is restrictive but enables closed-form solutions
  - Focusing on linear networks sacrifices realism for analytical tractability

- Failure signatures:
  - No double descent: All weights converge at similar rates, or one error curve dominates
  - Oversimplified dynamics: If actual training shows strong coupling, model predictions diverge
  - Poor initialization: If weights are initialized too large or at zero with γ=0, some may not activate

- First 3 experiments:
  1. Simulate two-layer linear network with controlled singular values (λᵢ, λᵢ^(yx)^1/2) and track individual error curves to verify superposition
  2. Vary initialization and learning rates to observe changes in inflection points and double descent occurrence
  3. Compare one-layer vs two-layer dynamics under same data to isolate effect of second singular value

## Open Questions the Paper Calls Out

- Question: How does coupling between weights in the two-layer model affect epoch-wise double descent compared to the decoupled case?
  - Basis in paper: [explicit] "The effect of coupling... While not observed for the decoupled two-layer dynamics, it remains to be investigated if such a behaviour could also be observed in the (coupled) linear two-layer neural network."
  - Why unresolved: The paper explicitly states this remains to be investigated, noting that coupling could give rise to new types of epoch-wise double descent behaviors not seen in the decoupled case.
  - What evidence would resolve it: Analytical or empirical studies comparing double descent patterns in coupled vs decoupled two-layer linear networks.

- Question: What role does incremental learning (plateau formation in training loss) play in predicting or causing epoch-wise double descent?
  - Basis in paper: [explicit] "It remains to be investigated if patterns in the training loss, such as the previously observed incremental learning pattern... could be indicators of epoch-wise double descent."
  - Why unresolved: While the paper notes incremental learning has been observed in two-layer networks, it hasn't been connected to double descent phenomena.
  - What evidence would resolve it: Empirical studies tracking both incremental learning patterns and double descent curves across multiple training runs.

- Question: How does the number of layers affect epoch-wise double descent beyond the two-layer case, and what new factors emerge in deeper models?
  - Basis in paper: [inferred] The paper derives approximate dynamics for L-layer networks and notes "the error curve for the multi-layer dynamics might show more complex behavior."
  - Why unresolved: While the paper extends analysis to multi-layer cases, it relies on simplifications and notes that more complex behaviors might emerge.
  - What evidence would resolve it: Analytical work extending the approximate dynamics without simplifications, or empirical studies systematically varying layer count.

## Limitations
- Decoupled dynamics assumption may not hold in practice when training two-layer networks with standard backpropagation
- Assumes shared singular vectors between input and input-output covariance matrices, a strong structural constraint
- Necessary conditions for double descent are mathematically derived but sufficiency is not established
- Limited empirical validation to theoretical predictions rather than full numerical experiments
- Focus on linear networks limits direct applicability to deep nonlinear networks

## Confidence

- **High**: The gradient flow derivation for decoupled two-layer networks and the error decomposition formula (Proposition 2) are mathematically sound.
- **Medium**: The claim that singular values of both input and input-output covariance matrices control double descent is well-supported theoretically but needs empirical verification in more general settings.
- **Low**: The necessary conditions for double descent are rigorously derived but may be overly restrictive when actual training dynamics deviate from the idealized decoupled model.

## Next Checks

1. **Numerical verification of decoupled dynamics**: Simulate two-layer linear networks with varying singular values and compare actual training trajectories against the decoupled prediction to quantify approximation error.

2. **Generalization to non-shared singular vectors**: Test whether the double descent mechanism persists when X^T X and y^T X do not share right singular vectors, relaxing the key structural assumption.

3. **Non-linear activation extension**: Introduce simple non-linearities (e.g., ReLU) and assess whether the dual-singular-value mechanism for double descent generalizes beyond linear networks.