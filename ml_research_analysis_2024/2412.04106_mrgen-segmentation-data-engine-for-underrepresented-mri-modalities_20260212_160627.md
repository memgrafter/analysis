---
ver: rpa2
title: 'MRGen: Segmentation Data Engine for Underrepresented MRI Modalities'
arxiv_id: '2412.04106'
source_url: https://arxiv.org/abs/2412.04106
tags:
- data
- segmentation
- modalities
- mask
- mrgen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training medical image segmentation
  models for underrepresented MRI modalities, which are clinically important but lack
  annotated data. The authors propose MRGen, a diffusion-based data engine for controllable
  medical image synthesis.
---

# MRGen: Segmentation Data Engine for Underrepresented MRI Modalities

## Quick Facts
- arXiv ID: 2412.04106
- Source URL: https://arxiv.org/abs/2412.04106
- Authors: Haoning Wu; Ziheng Zhao; Ya Zhang; Yanfeng Wang; Weidi Xie
- Reference count: 40
- Primary result: Diffusion-based data engine achieves FID 82.18 and DSC 44.71 for segmentation of underrepresented MRI modalities

## Executive Summary
MRGen addresses the challenge of training segmentation models for underrepresented MRI modalities by introducing a diffusion-based data engine that generates realistic medical images conditioned on text prompts and segmentation masks. The method leverages a large-scale radiology image-text dataset with rich metadata and employs a two-stage training strategy combining text-guided pretraining with mask-conditioned finetuning. Extensive experiments demonstrate that MRGen significantly improves segmentation performance on unannotated MRI modalities by providing high-quality synthetic data, outperforming existing methods in both generation and segmentation tasks.

## Method Summary
MRGen is a diffusion-based data engine that generates realistic MRI images for underrepresented modalities lacking annotated data. The method involves curating a large-scale radiology image-text dataset (MRGen-DB) with modality labels, attributes, regions, and organ information, with a subset featuring pixel-wise mask annotations. MRGen employs a two-stage training strategy: text-guided pretraining on diverse image-text pairs to learn modality-specific generation, followed by mask-conditioned finetuning on annotated data to align generated images with organ masks. Generated samples are filtered using SAM2 for quality control before being used to train segmentation models.

## Key Results
- MRGen achieves FID scores of 82.18, demonstrating high-quality image generation
- Dice Similarity Coefficient reaches 44.71 on unannotated MRI modalities
- Method outperforms existing approaches in both generation quality and segmentation performance
- Controllable generation via text prompts successfully produces images matching specified conditions

## Why This Works (Mechanism)

### Mechanism 1
Diffusion-based synthesis conditioned on both text and mask improves segmentation performance on unannotated MRI modalities. MRGen employs a two-stage training strategy where text-guided pretraining on diverse image-text pairs learns modality-specific generation, followed by mask-conditioned finetuning that teaches the model to align generated images with organ masks. This combination enables controllable synthesis even for modalities without mask annotations by leveraging learned latent representations and mask condition controllers.

### Mechanism 2
Controllable generation via text prompts improves synthetic data quality for segmentation. MRGen uses templated text prompts including modality labels, attributes, regions, and organ information, encoded using BiomedCLIP and integrated via cross-attention into the diffusion UNet. This structured conditioning ensures generated images match clinical characteristics of target modalities while maintaining realism through the diffusion process.

### Mechanism 3
Automatic filtering using SAM2 ensures high-quality synthetic data for segmentation training. After generating candidate images, MRGen uses SAM2 to predict segmentation maps and calculate confidence and IoU scores against input masks. Samples meeting predefined thresholds are selected, ensuring synthetic data aligns well with mask conditions and is of high quality, reducing noise in training data.

## Foundational Learning

- **Concept: Diffusion Models**
  - Why needed here: Diffusion models provide the foundation for controllable image synthesis, allowing MRGen to generate realistic MRI images conditioned on text and masks
  - Quick check question: What are the two main processes in a diffusion model, and how do they work together to generate images?

- **Concept: Variational Autoencoders (VAEs)**
  - Why needed here: VAEs compress high-resolution medical images into lower-dimensional latent space, making generation more efficient
  - Quick check question: How does the autoencoder in MRGen contribute to the overall generation process, and why is latent space compression important for medical images?

- **Concept: Cross-Attention Mechanisms**
  - Why needed here: Cross-attention integrates text information (encoded by BiomedCLIP) into image generation, enabling controllable synthesis
  - Quick check question: How does cross-attention work in MRGen, and why is it crucial for integrating text prompts into generation?

## Architecture Onboarding

- **Component map:**
  Latent Encoding -> Text-Guided Generation -> Mask-Conditioned Generation -> Automatic Filtering

- **Critical path:**
  1. Preprocess and compress input images using autoencoder
  2. Generate images using diffusion UNet conditioned on text prompts and masks
  3. Filter generated images using SAM2 to ensure quality and mask alignment
  4. Use filtered synthetic data to train segmentation models

- **Design tradeoffs:**
  - Latent space compression vs. image fidelity: Higher compression reduces computational cost but may lose fine details
  - Text prompt complexity vs. generation control: More detailed prompts improve control but increase conditioning complexity
  - Filtering thresholds vs. data quantity: Stricter thresholds ensure higher quality but reduce available training data

- **Failure signatures:**
  - Poor image quality: Check autoencoder reconstruction quality and diffusion UNet training
  - Mismatched masks: Verify mask conditioning and SAM2 filtering process
  - Mode collapse: Investigate diffusion training stability and training data diversity

- **First 3 experiments:**
  1. Reconstruct a sample from the autoencoder to verify latent space compression quality
  2. Generate a simple image (e.g., single organ) using only text conditioning to test text-guided generation
  3. Apply mask condition to generated image and check if mask is respected using SAM2

## Open Questions the Paper Calls Out

### Open Question 1
The paper identifies that MRGen struggles with extremely small organ masks due to variability and imbalance in their distribution across 3D volumes. This remains unresolved as the paper suggests more annotated data could help but doesn't provide specific solutions. Testing with datasets containing more small organ masks or experimenting with architectural changes specifically designed for small objects would provide evidence.

### Open Question 2
MRGen occasionally produces false-negative samples, such as synthesizing kidneys when not included in the mask condition, leading to false negatives during segmentation training. While the paper suggests designing a more robust data filtering pipeline, it doesn't provide implementation details or evaluate effectiveness. Implementing and evaluating different filtering strategies would resolve this issue.

### Open Question 3
MRGen is currently validated on 2D slices, with only preliminary results on 256 × 256 × 16 volumes mentioned for potential 3D extension. The paper doesn't fully explore computational requirements or potential benefits of 3D generation compared to current 2D approach. Comprehensive experiments on larger 3D volumes would provide clarity on feasibility and benefits.

## Limitations
- Performance heavily depends on quality and diversity of pretraining data, with limited information about coverage of rare MRI modalities
- Two-stage training strategy may not fully generalize to modalities significantly different from pretraining set
- Automatic filtering pipeline using SAM2 could introduce bias by discarding potentially useful samples
- Study focuses primarily on 2D MRI slices, limiting applicability to volumetric medical imaging tasks

## Confidence
- **High confidence**: Core mechanism of diffusion-based image synthesis with dual conditioning (text + mask)
- **Medium confidence**: Generalization capabilities across diverse MRI modalities
- **Medium confidence**: Effectiveness of two-stage training strategy
- **Low confidence**: Robustness of automatic filtering pipeline without extensive parameter tuning

## Next Checks
1. Test MRGen's performance on MRI modalities underrepresented or absent in pretraining data to assess true generalization capabilities
2. Conduct ablation studies on automatic filtering pipeline by varying confidence thresholds to understand impact on segmentation performance
3. Evaluate model's performance on 3D volumetric data or compare 2D slice-based generation with full-volume approaches to assess current implementation limitations