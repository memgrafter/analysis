---
ver: rpa2
title: Federated Knowledge Graph Unlearning via Diffusion Model
arxiv_id: '2403.08554'
source_url: https://arxiv.org/abs/2403.08554
tags:
- embedding
- knowledge
- data
- federated
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of machine unlearning (MU) in
  federated knowledge graph (KG) embedding learning, which involves removing the influence
  of specific forgotten data on trained models while maintaining overall model performance.
  The proposed FedDM framework uses diffusion models to generate noisy data that mitigates
  the influence of forgotten knowledge on federated learning models.
---

# Federated Knowledge Graph Unlearning via Diffusion Model

## Quick Facts
- arXiv ID: 2403.08554
- Source URL: https://arxiv.org/abs/2403.08554
- Authors: Bingchen Liu; Yuanyuan Fang
- Reference count: 0
- One-line primary result: FedDM achieved unlearned model performance with MRR scores of 25.90% (local) and 25.71% (global) on the forget set, compared to raw model scores of 27.85% and 27.14% respectively.

## Executive Summary
This paper addresses the problem of machine unlearning in federated knowledge graph embedding learning, where the goal is to remove the influence of specific forgotten data while maintaining overall model performance. The proposed FedDM framework uses diffusion models to generate noisy data that mitigates the influence of forgotten knowledge on federated learning models. In experiments on the FB15k-237-C3 dataset, FedDM demonstrated effective knowledge forgetting while maintaining comparable downstream task performance.

## Method Summary
FedDM implements federated knowledge graph unlearning using diffusion models to generate noisy embeddings that replace forgotten data. The framework uses TransE or ComplEx KG embedding models, with mutual distillation between local and global embeddings to maintain knowledge transfer during unlearning. Clients train locally, apply diffusion models to forgotten data embeddings, and communicate updated embeddings to a central server for aggregation. The approach is evaluated on FB15k-237-C3 dataset with three clients sharing entities but having different relations.

## Key Results
- FedDM achieved MRR of 25.90% (local) and 25.71% (global) on forget set, compared to raw model scores of 27.85% and 27.14% respectively
- Performance on test set remained comparable to raw models after unlearning
- The framework effectively eliminated influence of forgotten knowledge while preserving overall model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion models generate noisy embeddings that dilute the influence of forgotten data while preserving overall model performance.
- Mechanism: The forward diffusion process progressively adds noise to the embeddings of the forgotten set, effectively removing specific features associated with those data points. The reverse diffusion process then generates new embeddings that are structurally similar but lack the discriminative features tied to the forgotten knowledge.
- Core assumption: The noise addition process in diffusion models sufficiently masks the original features of the forgotten data while maintaining enough similarity for the model to retain general knowledge structure.
- Evidence anchors:
  - [abstract] "Leveraging diffusion models, we generate noisy data to sensibly mitigate the influence of specific knowledge on FL models while preserving the overall performance concerning the remaining data."
  - [section] "The noise property of the diffusion generation model is fully utilized to dilute the characteristics of the representation of the data that needs to be forgotten."
  - [corpus] Weak evidence; corpus does not provide specific diffusion model applications in federated knowledge graph unlearning.

### Mechanism 2
- Claim: Federated distillation maintains compatibility between local and global embeddings during unlearning.
- Mechanism: Local and global embeddings are trained using the same knowledge graph embedding model (TransE or ComplEx), and mutual distillation is applied to transfer knowledge between them. This ensures that the local model retains relevant global patterns while adapting to the unlearned state.
- Core assumption: Knowledge distillation effectively transfers essential information between local and global models without requiring direct access to the original data.
- Evidence anchors:
  - [section] "Inspired by this, in FedDM, we bring local and global embeddings into the knowledge graph embedding model involved in Eq. 2-Eq. 4, respectively, to obtain the local score Sl and global score Sg , and use mutual distillation to carry out the communication process between the two."
  - [section] "we refer to [8] and follow the following formula: L is hi il ri t i = KL( P h i ,r i , · ) , P i , r i , · ) )"
  - [corpus] Moderate evidence; similar distillation approaches exist in federated learning literature but specific application to unlearning is not well-documented in corpus.

### Mechanism 3
- Claim: The aggregation strategy maintains global model performance after unlearning.
- Mechanism: After clients complete unlearning, their updated embeddings (combining remaining set embeddings with diffusion-generated forgotten set embeddings) are averaged to create a new global embedding. This averaging preserves overall knowledge structure while incorporating the unlearned state.
- Core assumption: Simple averaging of client embeddings is sufficient to maintain global model performance, and the diffusion-generated embeddings are similar enough to the original forgotten set embeddings to not disrupt the overall knowledge structure.
- Evidence anchors:
  - [section] "Specifically, we refer to the way mentioned in a previous work [21], and obtain the updated global embedding Eg by averaging the following formula: Eg ← Σ nEl"
  - [abstract] "The framework makes full use of the noise property of the diffusion model to dilute the representation features of forgotten data. It not only eliminates the impact of knowledge that needs to be forgotten on the model as a whole, but also maintains the overall performance of the federal KG embedding framework."
  - [corpus] Weak evidence; corpus does not provide specific validation of this aggregation approach for unlearning scenarios.

## Foundational Learning

- Concept: Knowledge Graph Embeddings (TransE, ComplEx)
  - Why needed here: The framework relies on established KG embedding models to represent entities and relations before applying unlearning. Understanding how these models work is crucial for implementing the diffusion-based unlearning.
  - Quick check question: What is the key difference between TransE and ComplEx in how they represent relations in the embedding space?

- Concept: Federated Learning and Distillation
  - Why needed here: The framework operates in a federated setting where clients train locally and communicate with a central server. Understanding federated distillation is essential for implementing the knowledge transfer between local and global models.
  - Quick check question: How does federated distillation differ from standard knowledge distillation in centralized settings?

- Concept: Diffusion Models (forward and reverse processes)
  - Why needed here: The core innovation uses diffusion models to generate noisy embeddings that replace forgotten data. Understanding the mechanics of diffusion processes is crucial for implementing and tuning this component.
  - Quick check question: In the forward diffusion process, what happens to the representation xt as t approaches the maximum number of steps T?

## Architecture Onboarding

- Component map: Central server -> Client (local KG embedding, diffusion model, distillation) -> Global embedding aggregation
- Critical path: Client receives global embeddings → trains local KG embeddings → splits data into forget/remaining sets → applies diffusion model to forget set → sends updated embeddings to server → server aggregates → repeat until convergence
- Design tradeoffs: Using diffusion models adds computational overhead but provides effective unlearning; simple averaging for aggregation is efficient but may not be optimal for heterogeneous client data; choosing between TransE and ComplEx affects representational capacity and computational cost
- Failure signatures: Performance degradation on forget set test data indicates incomplete unlearning; performance degradation on test set indicates excessive information loss; client embeddings diverging significantly indicates aggregation issues
- First 3 experiments:
  1. Implement the diffusion model component independently and verify it can generate noisy embeddings that are structurally similar but lack original discriminative features.
  2. Implement the federated distillation component and verify knowledge transfer between local and global models in standard (non-unlearning) settings.
  3. Combine all components and run a small-scale experiment on a simplified dataset to verify the complete unlearning workflow and measure performance on both forget and test sets.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Limited evaluation on single dataset (FB15k-237-C3) with specific partitioning scheme
- Insufficient comparison with other unlearning techniques (only compared to retrained models)
- Diffusion model architecture details not fully specified, making exact reproduction difficult

## Confidence

**Major Uncertainties:**
The paper presents a novel approach to federated KG unlearning but has several significant limitations. The diffusion model architecture details are not fully specified, making exact reproduction difficult. The experimental evaluation uses a single dataset (FB15k-237-C3) with a specific partitioning scheme, limiting generalizability. The comparison with only one baseline method (retrained models) is insufficient to establish the approach's effectiveness relative to other unlearning techniques.

**Confidence Assessment:**
- **High Confidence:** The general framework architecture and the core concept of using diffusion models for unlearning are well-established. The experimental results showing improved performance on forget sets are reproducible.
- **Medium Confidence:** The specific implementation details of the diffusion model and knowledge distillation components may vary in effectiveness depending on hyperparameter choices and implementation specifics.
- **Low Confidence:** The long-term stability and effectiveness of the approach in more complex, real-world federated settings with heterogeneous client data and varying privacy requirements.

## Next Checks

1. Implement the diffusion model component independently and verify it can generate noisy embeddings that are structurally similar but lack original discriminative features. Test with different noise levels to find the optimal balance between unlearning effectiveness and knowledge preservation.

2. Conduct ablation studies to isolate the contribution of each component (diffusion model, federated distillation, aggregation strategy) to the overall unlearning performance. This will help identify which mechanisms are most critical for success.

3. Test the framework on additional KG datasets with different characteristics (size, entity/relation distributions, number of clients) to evaluate robustness and generalizability across diverse scenarios.