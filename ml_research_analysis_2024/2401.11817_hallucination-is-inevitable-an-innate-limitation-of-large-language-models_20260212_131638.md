---
ver: rpa2
title: 'Hallucination is Inevitable: An Innate Limitation of Large Language Models'
arxiv_id: '2401.11817'
source_url: https://arxiv.org/abs/2401.11817
tags:
- llms
- hallucination
- computable
- training
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formally defines and proves that hallucination is inevitable
  in large language models (LLMs) by showing they cannot learn all computable functions.
  Using a formal world framework, it proves that any computable LLM will inevitably
  produce inconsistent outputs for some computable ground truth functions, regardless
  of training data or architecture.
---

# Hallucination is Inevitable: An Innate Limitation of Large Language Models

## Quick Facts
- arXiv ID: 2401.11817
- Source URL: https://arxiv.org/abs/2401.11817
- Reference count: 40
- Primary result: Hallucination is inevitable in LLMs as they cannot learn all computable functions, proven using diagonalization argument

## Executive Summary
This paper proves that hallucination is an inherent limitation of large language models (LLMs) by demonstrating they cannot learn all computable functions. Using a formal theoretical framework based on computability theory and learning theory, the authors show that for any computable LLM, there exists a computable ground truth function that the model will inevitably fail to learn exactly. The study identifies specific problem classes prone to hallucination (combinatorial enumeration, Presburger arithmetic, SAT problems) and provides empirical validation showing even state-of-the-art LLMs fail on tasks requiring exhaustive enumeration or complex reasoning. While acknowledging LLMs' practical utility, the paper concludes that complete elimination of hallucination is impossible, emphasizing the need for external safeguards and human oversight in safety-critical applications.

## Method Summary
The authors employ a diagonalization argument from learning theory to prove that LLMs cannot learn all computable functions. They define a formal world where hallucination occurs when a computable LLM produces inconsistent outputs compared to a computable ground truth function. The proof shows that for any computably enumerable set of LLMs, there exists a computable function that no LLM in the set can learn exactly. This theoretical framework is then generalized to show that all computable LLMs will hallucinate on some inputs. The study identifies hallucination-prone problem classes based on computational complexity requirements and validates these findings empirically with experiments on combinatorial list tasks and arithmetic reasoning.

## Key Results
- Formal proof that hallucination is inevitable in any computable LLM regardless of training data or architecture
- Identification of specific hallucination-prone problem classes including combinatorial enumeration, Presburger arithmetic, and SAT problems
- Empirical validation showing LLMs fail on tasks requiring exponential time or exhaustive enumeration
- Conclusion that while external knowledge aids can reduce hallucination severity, complete elimination is impossible

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hallucination is inevitable for any computable LLM regardless of training data or architecture.
- Mechanism: Uses diagonalization argument to show that for any computable LLM and any training procedure, there exists a computable ground truth function that the LLM cannot learn exactly.
- Core assumption: The formal world of computable functions is a subset of the real world; therefore, if hallucination is inevitable in the formal world, it is also inevitable in the real world.
- Evidence anchors:
  - [abstract]: "By employing results from learning theory, we show that LLMs cannot learn all the computable functions and will therefore inevitably hallucinate if used as general problem solvers."
  - [section 3.2]: Theorem 3 proves that for all computable LLMs, there exists a computable ground truth function such that each LLM state will hallucinate with respect to that function.
  - [corpus]: Weak evidence - related papers discuss inevitability but focus on different theoretical angles.
- Break condition: If the assumption that the formal world is a subset of the real world is violated, or if the diagonalization argument is flawed.

### Mechanism 2
- Claim: Even with external knowledge aids, hallucination cannot be completely eliminated for all problems.
- Mechanism: Knowledge-enhanced LLMs and guardrails provide information beyond training samples, but the formal world framework shows that some computable functions are unlearnable by any computable LLM, regardless of information sources.
- Core assumption: External aids change how information is accessed but don't change the fundamental computational limitations of LLMs.
- Evidence anchors:
  - [section 4.2]: "Knowledge-Enhanced LLMs...explicitly controls the LLM workflow by changing how information is recalled through retrieval from knowledge database...Therefore, Theorem 3 is inapplicable herein. This is potentially an effective mitigator of hallucination in the formal world."
  - [section 3]: Theorem 3 applies to all computable LLMs regardless of training procedure or information sources.
  - [corpus]: Limited evidence - related works discuss external knowledge but don't prove complete elimination of hallucination.
- Break condition: If external aids fundamentally change what computable functions an LLM can learn, or if the formal world framework is incomplete.

### Mechanism 3
- Claim: Certain problem classes are inherently hallucination-prone for LLMs due to computational complexity.
- Mechanism: Identifies specific problem classes (combinatorial lists, Presburger arithmetic, SAT, etc.) that require computational resources beyond polynomial time, making them unlearnable by polynomial-time LLMs.
- Core assumption: Computational complexity directly determines learnability; problems requiring exponential time cannot be learned by polynomial-time LLMs.
- Evidence anchors:
  - [section 4.1]: Table 3 lists hallucination-prone problems with their computational requirements.
  - [section C]: Empirical study shows LLMs fail on combinatorial list tasks requiring exponential time.
  - [corpus]: Weak evidence - related works discuss complexity but don't establish direct link to hallucination.
- Break condition: If computational complexity doesn't determine learnability, or if LLMs can overcome complexity barriers through architectural innovations.

## Foundational Learning

- Concept: Diagonalization argument
  - Why needed here: Core proof technique used to show that no computable LLM can learn all computable functions.
  - Quick check question: Can you explain how the diagonalization argument proves that there exists a computable function that no computable LLM can learn exactly?

- Concept: Computable functions and sets
  - Why needed here: Formal framework defining what LLMs can and cannot learn.
  - Quick check question: What distinguishes a computable set from a non-computable set, and why does this matter for LLM learnability?

- Concept: Computational complexity classes
  - Why needed here: Explains why certain problem classes (NP-complete, exponential time) are hallucination-prone.
  - Quick check question: How does the difference between polynomial-time and exponential-time problems relate to LLM hallucination?

## Architecture Onboarding

- Component map: Formal world framework → Computable function space → LLM computational limits → Hallucination-prone problem classes → Empirical validation → Practical implications
- Critical path: Define formal world → Apply learning theory → Identify hallucination-prone problems → Validate empirically → Discuss mitigations → Draw practical conclusions
- Design tradeoffs: Theoretical completeness vs. practical applicability; formal rigor vs. empirical validation
- Failure signatures: LLM succeeds on some tasks but fails on tasks with exponential complexity; theoretical framework doesn't align with empirical observations
- First 3 experiments:
  1. Implement diagonalization argument to construct a function that no given LLM can learn exactly
  2. Test LLM on combinatorial list tasks with varying input sizes to observe performance degradation
  3. Compare LLM performance on polynomial-time vs. exponential-time problems to identify computational barriers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific classes of computable functions can LLMs provably learn without hallucination?
- Basis in paper: [explicit] Theorem E3 states that for any computably enumerable set C of total computable functions, there exists an LLM that can learn all functions in C without hallucination.
- Why unresolved: The paper doesn't specify which real-world problems fall into such computable function classes or how to identify them.
- What evidence would resolve it: A formal characterization of problem classes that are both computable and contained in computably enumerable sets of functions.

### Open Question 2
- Question: Can hallucination severity be reduced below a practical threshold for safety-critical applications?
- Basis in paper: [inferred] While the paper proves hallucination cannot be completely eliminated, it suggests hallucination severity might be controllable for many applications.
- Why unresolved: The paper doesn't provide quantitative bounds on hallucination severity or define what constitutes an acceptable threshold for different applications.
- What evidence would resolve it: Empirical studies showing hallucination rates below critical thresholds for specific safety-critical domains.

### Open Question 3
- Question: How can LLMs automatically identify and retrieve appropriate external tools for solving specific problems?
- Basis in paper: [explicit] Section 4.2 discusses knowledge-enhanced LLMs using external knowledge bases but notes scalability challenges in real-world tasks.
- Why unresolved: The paper doesn't address the mechanism for LLMs to determine which external tools are appropriate for different problem types.
- What evidence would resolve it: A framework showing how LLMs can match problem characteristics to appropriate external reasoning tools with measurable accuracy.

### Open Question 4
- Question: What is the theoretical computational limit of LLM capabilities compared to their current practical limitations?
- Basis in paper: [explicit] Section E notes that while LLMs have theoretical capabilities, current models show significant gaps (e.g., failing at R(m,5) for m≥256).
- Why unresolved: The paper identifies a gap between theory and practice but doesn't explain why current architectures underperform their theoretical limits.
- What evidence would resolve it: Comparative analysis showing specific architectural or training limitations that prevent current LLMs from reaching their theoretical computational ceiling.

## Limitations

- Theoretical framework's applicability to real-world LLMs is limited by abstraction of specific architectures and training details
- Empirical validation provides weak evidence for theoretical claims, testing specific problem classes without conclusively proving fundamental computational barriers
- Treatment of external knowledge aids and guardrails is incomplete, not fully analyzing whether they could fundamentally change computational landscape

## Confidence

**High Confidence**: The core mathematical claim that computable functions cannot be completely enumerated by computable functions (diagonalization argument). This is a well-established result in computability theory.

**Medium Confidence**: The mapping between theoretical computational limitations and practical LLM hallucination behaviors. While the theoretical framework is sound, the connection to real-world LLM performance requires more empirical validation.

**Low Confidence**: The claim that no future architectural innovations could overcome these limitations. The paper assumes current computational paradigms but doesn't account for potential breakthroughs in neural network architectures or learning paradigms.

## Next Checks

1. **Rigorous empirical test of computational complexity barriers**: Design experiments comparing LLM performance on problems with varying computational complexity (P vs NP-complete vs exponential time) using identical architectures but different problem instances. Measure performance degradation patterns to validate whether computational complexity directly correlates with hallucination likelihood.

2. **Formal analysis of knowledge-augmented LLMs**: Extend the theoretical framework to explicitly model how external knowledge retrieval changes the computable function space. Prove whether knowledge-enhanced systems can learn additional computable functions beyond standard LLMs, potentially breaking the diagonalization argument.

3. **Architecture-specific validation**: Test the theoretical claims against multiple LLM architectures (transformer variants, recurrent models, hybrid systems) to determine if certain architectures can avoid the identified limitations or if the computational barriers apply universally across all computable architectures.