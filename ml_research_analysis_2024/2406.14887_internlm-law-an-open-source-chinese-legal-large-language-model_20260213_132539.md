---
ver: rpa2
title: 'InternLM-Law: An Open Source Chinese Legal Large Language Model'
arxiv_id: '2406.14887'
source_url: https://arxiv.org/abs/2406.14887
tags:
- legal
- data
- tasks
- chinese
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'InternLM-Law is a Chinese legal large language model that outperforms
  state-of-the-art models including GPT-4 on legal benchmarks. The model is trained
  on a dataset of over 1 million legal queries using a two-stage supervised fine-tuning
  process: first on both legal and general data, then on high-quality legal data.'
---

# InternLM-Law: An Open Source Chinese Legal Large Language Model

## Quick Facts
- arXiv ID: 2406.14887
- Source URL: https://arxiv.org/abs/2406.14887
- Reference count: 40
- Outperforms GPT-4 on 13 out of 20 LawBench subtasks

## Executive Summary
InternLM-Law is a Chinese legal large language model that achieves state-of-the-art performance on legal benchmarks through a novel two-stage supervised fine-tuning approach. The model is built on InternLM2-Chat and fine-tuned on a carefully constructed dataset of over 1 million legal queries, demonstrating superior performance on legal knowledge memorization, understanding, and application tasks. The authors make both the model and dataset publicly available, enabling further research in the field.

## Method Summary
The method employs a two-stage supervised fine-tuning process. First, the model is fine-tuned on a mixture of legal and general-purpose content to establish broad knowledge foundations. Second, the model undergoes exclusive fine-tuning on high-quality legal data to enhance structured output generation and factual accuracy. The training uses 64 A100-80GB GPUs for 8 hours per stage with a sequence length of 32k and learning rate of 1e-5.

## Key Results
- Achieves highest average performance on LawBench benchmark
- Outperforms GPT-4 on 13 out of 20 subtasks
- Maintains strong general language capabilities while excelling at specialized legal tasks

## Why This Works (Mechanism)

### Mechanism 1
Two-stage supervised fine-tuning enables better transfer of general language capabilities to specialized legal tasks. The first stage fine-tunes on both legal and general data, allowing the model to learn how to apply general reasoning skills to legal contexts. The second stage refines these capabilities with high-quality legal data, enhancing both accuracy and response style.

### Mechanism 2
Large-scale, diverse legal dataset construction with careful filtering and processing improves model performance on specialized legal tasks. The paper constructs a dataset of over 1 million queries covering legal NLP tasks, legal consultations, and Chinese laws/regulations, with filtering pipelines using rule-based methods and LLM evaluation to ensure quality and diversity.

### Mechanism 3
Specialized fine-tuning on high-quality legal data improves factual accuracy and response style for legal queries. The second stage of fine-tuning focuses exclusively on high-quality legal data, enhancing the model's ability to generate accurate legal responses with proper structure.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT is used to adapt the pre-trained LLM to the specialized legal domain by training on labeled examples of legal queries and responses.
  - Quick check question: What is the difference between pre-training and supervised fine-tuning in the context of LLMs?

- Concept: Instruction Following
  - Why needed here: The model needs to understand and follow legal instructions, which often require precise reasoning and structured output formats.
  - Quick check question: How does instruction following capability affect the performance of a legal LLM on structured legal tasks?

- Concept: Data Filtering and Quality Assurance
  - Why needed here: Legal data often contains noise, irrelevant information, and varying quality. Proper filtering ensures the model is trained on reliable data.
  - Quick check question: What are the key criteria for filtering legal consultation data to ensure quality?

## Architecture Onboarding

- Component map: Pre-trained InternLM2-Chat -> Stage 1 Fine-tuning (legal + general) -> Stage 2 Fine-tuning (high-quality legal) -> Evaluation
- Critical path: Data collection → data processing and filtering → stage 1 fine-tuning → stage 2 fine-tuning → evaluation
- Design tradeoffs: 7B model size balances performance with computational efficiency; two-stage approach trades additional training complexity for better performance; 32k sequence length enables handling complex legal documents but increases computational cost
- Failure signatures: Poor legal benchmark performance may indicate data quality issues; inability to follow instructions suggests stage 1 problems; hallucinations indicate insufficient stage 2 legal data
- First 3 experiments:
  1. Test different ratios of general to legal data in the first stage of fine-tuning
  2. Evaluate the effect of different filtering thresholds on legal consultation data quality
  3. Compare performance of models trained with and without the second stage of high-quality legal fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between general domain data and legal domain data in the two-stage fine-tuning process?
- Basis in paper: [explicit] The authors test different ratios of general to legal data (10% and 20%) and observe effects on both general task performance and legal task performance
- Why unresolved: The paper only tests two specific ratios (10% and 20% general data) and doesn't explore the full spectrum of possible ratios or determine the optimal balance
- What evidence would resolve it: Systematic testing of different ratios (e.g., 5%, 15%, 25%, 50%) across both general and legal tasks to identify the ratio that maximizes performance on both types of tasks

### Open Question 2
- Question: How does the two-stage training approach compare to other training strategies like joint training of all data or progressive training with more than two stages?
- Basis in paper: [explicit] The authors compare two-stage training to one-stage training and merged training, but don't explore alternative training strategies
- Why unresolved: The paper only evaluates three specific training strategies and doesn't explore whether other approaches (like multi-stage progressive training or joint training) might yield better results
- What evidence would resolve it: Comprehensive comparison of multiple training strategies including joint training, three-stage training, and other progressive training approaches

### Open Question 3
- Question: What is the impact of the 32k sequence length limit on the model's performance with very long legal documents?
- Basis in paper: [explicit] The authors set the training sequence length to 32k to accommodate most legal text inputs, but don't test performance on documents longer than this
- Why unresolved: The paper doesn't test model performance on legal documents exceeding 32k characters or explore whether longer context windows would improve performance
- What evidence would resolve it: Evaluation of model performance on legal documents of varying lengths (e.g., 40k, 50k, 60k characters) to determine if the 32k limit is optimal or if longer contexts would be beneficial

## Limitations
- Evaluation primarily focuses on Chinese legal tasks, leaving performance on other languages and legal systems unexplored
- Direct comparisons with other state-of-the-art models beyond those mentioned would strengthen claims
- Potential biases in training data and their effects on legal reasoning are not extensively discussed

## Confidence
- **High Confidence**: Strong performance on LawBench, outperforming GPT-4 on 13 out of 20 subtasks
- **Medium Confidence**: Effectiveness of the two-stage fine-tuning approach
- **Medium Confidence**: Maintaining general language capabilities while excelling at legal tasks
- **Low Confidence**: Effective application to real-world legal scenarios without additional practical validation

## Next Checks
1. Cross-Lingual and Cross-Jurisdictional Testing: Evaluate the model's performance on legal tasks in other languages and legal systems to assess generalizability beyond Chinese law.
2. Bias and Fairness Analysis: Conduct a comprehensive analysis of potential biases in the model's legal reasoning, particularly focusing on different demographic groups and types of legal cases.
3. Real-World Application Testing: Test the model in simulated or real legal consultation scenarios to validate its practical utility and identify any limitations not captured by benchmark tests.