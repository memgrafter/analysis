---
ver: rpa2
title: A second-order-like optimizer with adaptive gradient scaling for deep learning
arxiv_id: '2410.05871'
source_url: https://arxiv.org/abs/2410.05871
tags:
- innaprop
- learning
- training
- adamw
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: INNAprop is a new optimizer for deep learning that combines the
  DIN method with RMSprop's adaptive gradient scaling. It leverages second-order information
  and rescaling while keeping memory requirements similar to standard methods like
  AdamW or SGD with momentum.
---

# A second-order-like optimizer with adaptive gradient scaling for deep learning

## Quick Facts
- arXiv ID: 2410.05871
- Source URL: https://arxiv.org/abs/2410.05871
- Reference count: 40
- INNAprop consistently matches or outperforms AdamW in training speed and accuracy with minimal hyperparameter tuning

## Executive Summary
INNAprop is a novel optimizer that combines second-order inertial dynamics with RMSprop-style adaptive scaling to achieve faster convergence and improved generalization in deep learning. The method discretizes a second-order dynamical system while maintaining memory efficiency comparable to standard optimizers like AdamW. Through careful coordinate transformations, INNAprop achieves second-order information benefits without the computational overhead of full Hessian computation.

## Method Summary
INNAprop discretizes a second-order dynamical system (DIN) that approximates Newton steps using gradient differences, while incorporating RMSprop's adaptive gradient scaling through accumulated squared gradients. The algorithm maintains three state vectors (vk, ψk, θk) through coordinate transformation, achieving memory efficiency comparable to AdamW. Key hyperparameters α and β control the trade-off between convergence speed and generalization, with fixed values (0.1, 0.9) for fast training and (2.0, 2.0) for better final accuracy.

## Key Results
- Matches or outperforms AdamW on CIFAR-10, Food101, ImageNet with ResNets, VGG, DenseNet, and ViT
- Shows improved training speed and accuracy on GPT-2 (OpenWebText) both from scratch and with LoRA fine-tuning
- Maintains memory requirements similar to AdamW while providing second-order optimization benefits
- Requires minimal hyperparameter tuning across large-scale settings

## Why This Works (Mechanism)

### Mechanism 1
INNAprop achieves faster convergence by combining second-order inertial dynamics with RMSprop-style adaptive scaling, effectively approximating Newton steps without full Hessian computation. The optimizer uses a discretized version of a second-order dynamical system (DIN) that captures curvature information through the derivative of the gradient, while RMSprop accumulates squared gradients to adaptively scale updates per parameter. This hybrid approach smooths the trajectory in flat regions and accelerates through steep ones.

### Mechanism 2
Memory efficiency is preserved by carefully managing state variables—only three full-dimension vectors are stored per iteration, matching AdamW's footprint. Through variable substitution and careful derivation, the algorithm avoids storing separate momentum and auxiliary variables by expressing updates in a reduced coordinate system. This keeps memory usage at three vectors (vk, ψk, θk) rather than six.

### Mechanism 3
Hyperparameter α and β control the trade-off between fast convergence and generalization, with (α, β) = (0.1, 0.9) accelerating early training and (2.0, 2.0) improving final accuracy. α modulates the inertial term's influence on parameter updates, while β controls the strength of the Newtonian damping effect. Lower values prioritize rapid descent, higher values encourage smoother trajectories that generalize better.

## Foundational Learning

- **Second-order optimization methods and their approximations**: Understanding how INNAprop approximates second-order information without full Hessian computation is key to grasping its efficiency gains. *Quick check*: How does the discretized dynamical system in INNAprop approximate second-order information using only gradients?
- **Adaptive gradient methods and their memory implications**: RMSprop's adaptive scaling is central to INNAprop's performance, and understanding its memory footprint helps explain the algorithm's efficiency. *Quick check*: What are the memory requirements of standard adaptive methods like AdamW, and how does INNAprop achieve similar performance with fewer stored variables?
- **Hyperparameter tuning strategies in deep learning optimization**: INNAprop's performance depends on choosing appropriate α and β values, and understanding the tuning process is crucial for practical deployment. *Quick check*: What grid search strategy was used to select α and β for CIFAR-10, and how were these values validated on larger datasets?

## Architecture Onboarding

- **Component map**: vk (RMSprop accumulator) -> ψk (auxiliary variable) -> θk (parameters)
- **Critical path**: 1. Compute gradient gk 2. Update RMSprop accumulator vk+1 3. Update auxiliary variable ψk+1 4. Update parameters θk+1 5. Apply weight decay and learning rate scaling
- **Design tradeoffs**: Memory vs. accuracy: Reduced state representation saves memory but may lose some gradient history; Speed vs. generalization: Lower α, β values accelerate training but may hurt final accuracy; Computational overhead: Second-order approximation adds computation but avoids Hessian computation
- **Failure signatures**: Training loss plateaus early: May indicate poor (α, β) choice or insufficient learning rate; Parameter explosion: Could signal numerical instability in the second-order approximation; Slow convergence: Might indicate the RMSprop scaling is too conservative
- **First 3 experiments**: 1. Reproduce CIFAR-10 results with VGG11 using (α, β) = (0.1, 0.9) to verify fast convergence 2. Test (α, β) = (2.0, 2.0) on the same setup to confirm better final accuracy 3. Validate memory usage by comparing INNAprop's state vectors against AdamW's on a small model

## Open Questions the Paper Calls Out

### Open Question 1
How does INNAprop perform compared to other second-order-like optimizers such as Sophia or Adafactor on large-scale language models? The paper demonstrates INNAprop's effectiveness against AdamW but does not compare it to other second-order-like optimizers. Direct experimental comparisons of INNAprop with Sophia, Adafactor, and other second-order-like optimizers on large-scale language models like GPT-2 or LLaMA would resolve this.

### Open Question 2
What is the optimal scheduling strategy for the hyperparameters α and β in INNAprop for different training durations and architectures? The authors mention that developing scheduling strategies for damping parameters (α, β) could be beneficial but were unable to explore this due to resource limitations. Experimental results showing the performance of INNAprop with various scheduling strategies for α and β across different training durations and architectures would resolve this.

### Open Question 3
How does INNAprop's performance scale with model size and dataset complexity beyond those tested in the paper? The paper tests INNAprop on CIFAR-10, Food101, ImageNet, and GPT-2 but does not explore its performance on larger models or more complex datasets. Experimental results of INNAprop on larger models like GPT-3, PaLM, or more complex datasets like JFT-300M would resolve this.

### Open Question 4
How sensitive is INNAprop to the choice of RMSprop parameters (σ and ϵ) compared to AdamW? The authors state that for INNAprop, the default settings for the RMSprop component align with those of AdamW, but they do not explore the sensitivity of INNAprop to these parameters. Experimental results showing INNAprop's performance with different values of σ and ϵ compared to AdamW's performance with the same parameter variations would resolve this.

## Limitations

- Evaluation primarily focuses on computer vision and transformer models, with limited testing on recurrent architectures or reinforcement learning scenarios
- Memory efficiency claim relies on theoretical state count but lacks empirical measurements of GPU memory consumption under various batch sizes
- Second-order approximation's accuracy in highly non-convex loss landscapes remains unverified, particularly for tasks with sharp minima or saddle points

## Confidence

- **High confidence**: Memory efficiency claims and state reduction mechanism, supported by explicit derivation and clear state variable count
- **Medium confidence**: Convergence speed and accuracy improvements, based on reported results but limited to specific model-dataset pairs
- **Medium confidence**: Hyperparameter generalization across tasks, as (α, β) selection was based on grid search on CIFAR-10 but applied to larger datasets without per-task tuning

## Next Checks

1. **Memory profiling**: Measure actual GPU memory consumption of INNAprop vs AdamW during training across different batch sizes and model scales
2. **Architecture generalization**: Test INNAprop on recurrent networks (LSTM, GRU) and reinforcement learning environments to verify broader applicability
3. **Loss landscape analysis**: Compare INNAprop's trajectory against AdamW in highly non-convex optimization scenarios to quantify the accuracy of the second-order approximation