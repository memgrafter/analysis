---
ver: rpa2
title: Online Pseudo-Zeroth-Order Training of Neuromorphic Spiking Neural Networks
arxiv_id: '2407.12516'
source_url: https://arxiv.org/abs/2407.12516
tags:
- training
- neural
- networks
- learning
- opzo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training spiking neural networks
  (SNNs) in a biologically plausible and neuromorphic-hardware-friendly manner. The
  authors propose a novel online pseudo-zeroth-order (OPZO) training method that overcomes
  limitations of traditional backpropagation techniques.
---

# Online Pseudo-Zeroth-Order Training of Neuromorphic Spiking Neural Networks

## Quick Facts
- arXiv ID: 2407.12516
- Source URL: https://arxiv.org/abs/2407.12516
- Reference count: 40
- This paper proposes OPZO, an online pseudo-zeroth-order training method that achieves similar or superior performance to spatial backpropagation on spiking neural networks while maintaining biological plausibility and neuromorphic-hardware friendliness.

## Executive Summary
This paper addresses the challenge of training spiking neural networks (SNNs) in a biologically plausible and neuromorphic-hardware-friendly manner. The authors propose a novel online pseudo-zeroth-order (OPZO) training method that overcomes limitations of traditional backpropagation techniques. OPZO uses a single forward propagation with noise injection and direct top-down feedback signals for spatial credit assignment, avoiding the need for symmetric weights and separate forward-backward propagation phases. The key innovation is a pseudo-zeroth-order formulation that leverages the available first-order property of the loss function while maintaining the zeroth-order formulation for the model, significantly reducing the large variance typically associated with zeroth-order methods.

## Method Summary
The proposed OPZO method introduces a pseudo-zeroth-order formulation that leverages the available first-order property of the loss function while maintaining the zeroth-order formulation for the model. This approach significantly reduces the large variance typically associated with zeroth-order methods. Momentum feedback connections are introduced to propagate error signals directly to hidden layers, with connections updated based on zeroth-order estimation of the Jacobian's expectation. The method combines with online training for temporal credit assignment and has a similar form to three-factor Hebbian learning based on direct top-down modulations. Experiments on neuromorphic and static datasets with fully connected and convolutional networks demonstrate OPZO's effectiveness, achieving similar or superior performance compared to spatial backpropagation with accuracy improvements of 0.3-6.2% on various datasets.

## Key Results
- OPZO achieves similar or superior performance compared to spatial backpropagation on various datasets
- Accuracy improvements of 0.3-6.2% on neuromorphic and static datasets
- Demonstrates robustness under different noise injection settings
- Estimated lower computational costs than backpropagation on potential neuromorphic hardware

## Why This Works (Mechanism)
OPZO works by leveraging a pseudo-zeroth-order formulation that exploits the first-order properties of the loss function while maintaining zeroth-order formulation for the model. This combination significantly reduces gradient estimation variance. The momentum feedback connections enable direct top-down propagation of error signals to hidden layers without requiring symmetric weights. The noise injection mechanism allows for efficient gradient estimation in a single forward pass, eliminating the need for separate forward and backward propagation phases. This approach aligns with biological plausibility constraints while maintaining computational efficiency.

## Foundational Learning
1. **Zeroth-order optimization** - Needed because traditional gradient-based methods require backward passes incompatible with biological plausibility; quick check: verify gradient estimation variance reduction.
2. **Neuromorphic computing constraints** - Needed to understand why traditional backpropagation is unsuitable for SNN training on neuromorphic hardware; quick check: confirm compatibility with spiking neuron dynamics.
3. **Spatial credit assignment** - Needed to propagate error signals through network layers; quick check: validate effective error signal propagation to early layers.
4. **Three-factor Hebbian learning** - Needed for biologically plausible weight updates; quick check: verify weight update rules match biological observations.

## Architecture Onboarding
**Component map:** Input -> SNN layers -> Noise injection -> Feedback connections -> Output with loss calculation
**Critical path:** Forward propagation with noise injection → Error calculation → Feedback connection updates → Weight updates
**Design tradeoffs:** Biological plausibility vs. training efficiency, variance reduction vs. computational overhead, online vs. batch training
**Failure signatures:** High gradient variance leading to unstable training, poor credit assignment to early layers, excessive computational overhead
**First experiments:** 1) Test basic OPZO implementation on simple MNIST classification, 2) Compare variance reduction with standard zeroth-order methods, 3) Evaluate computational efficiency on simulated neuromorphic hardware

## Open Questions the Paper Calls Out
None

## Limitations
- Unknown implementation details for momentum feedback connections and noise injection mechanisms
- Lack of specific hyperparameter settings for different datasets and model architectures
- Limited validation on actual neuromorphic hardware platforms

## Confidence
- High confidence in the theoretical framework and biological plausibility of OPZO
- Medium confidence in reported accuracy improvements across different datasets
- Low confidence in the claimed computational efficiency benefits without detailed hardware implementation specifications

## Next Checks
1. Implement the momentum feedback connections with varying noise injection parameters to verify the variance reduction claims and identify optimal settings
2. Conduct ablation studies removing the pseudo-zeroth-order formulation to isolate its contribution to performance improvements
3. Measure actual energy consumption and latency on neuromorphic hardware platforms (e.g., Intel Loihi) to validate computational efficiency claims