---
ver: rpa2
title: 'TaskComplexity: A Dataset for Task Complexity Classification with In-Context
  Learning, FLAN-T5 and GPT-4o Benchmarks'
arxiv_id: '2409.20189'
source_url: https://arxiv.org/abs/2409.20189
tags:
- dataset
- learning
- https
- flan-t5
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a dataset of 4,112 programming tasks collected
  via web scraping for the purpose of classifying task complexity. Two machine learning
  approaches were evaluated: fine-tuning FLAN-T5 and in-context learning with GPT-4o-mini.'
---

# TaskComplexity: A Dataset for Task Complexity Classification with In-Context Learning, FLAN-T5 and GPT-4o Benchmarks

## Quick Facts
- **arXiv ID:** 2409.20189
- **Source URL:** https://arxiv.org/abs/2409.20189
- **Reference count:** 40
- **Primary result:** Task complexity classification dataset of 4,112 programming tasks; GPT-4o-mini in-context learning (57% accuracy) outperforms FLAN-T5 fine-tuning (52.24% accuracy)

## Executive Summary
This paper introduces TaskComplexity, a dataset of 4,112 programming tasks collected via web scraping for task complexity classification. The authors evaluate two machine learning approaches: fine-tuning FLAN-T5-small and in-context learning with GPT-4o-mini. Despite GPT-4o-mini achieving better performance, both methods show limited effectiveness due to high variability in task structures within each complexity class, suggesting inherent challenges in this classification task.

## Method Summary
The authors created TaskComplexity by scraping programming problems from platforms like Kattis, LeetCode, HackerRank, and Topcoder. They evaluated two approaches: (1) Fine-tuning FLAN-T5-small with an 80-20 train-test split and preprocessing to add a prompt prefix, and (2) In-context learning with GPT-4o-mini using few-shot examples. Both models were evaluated using standard metrics: accuracy, F1-score, precision, and recall.

## Key Results
- GPT-4o-mini in-context learning achieved 57% accuracy, 53.99% F1-score, 57% recall, and 56.29% precision
- FLAN-T5 fine-tuning achieved 52.24% accuracy, 47.17% F1-score, 47.23% recall, and 49.02% precision
- Both approaches showed limited effectiveness due to variability in task structures within each complexity class

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GPT-4o-mini's in-context learning outperforms FLAN-T5 fine-tuning because it dynamically adapts to structural variability without retraining
- **Mechanism:** ICL provides task-relevant examples in the prompt, allowing the model to infer classification rules from context rather than memorizing fixed patterns. This is especially effective when samples in the same complexity class have diverse structures
- **Core assumption:** The variability in task structures is too high for fine-tuned models to learn consistent classification patterns, but ICL can generalize from few examples
- **Evidence anchors:** [abstract] "in-context learning with GPT-4o-mini outperformed the FLAN-T5 model"; [section] "Each sample, even if it pertains to the same class, exhibits different structures and meanings"
- **Break condition:** If task variability decreases significantly, fine-tuning could catch up or surpass ICL performance

### Mechanism 2
- **Claim:** FLAN-T5's lower performance is due to the small version's limited parameter capacity and high variability of task structures
- **Mechanism:** With only 77 million parameters, FLAN-T5-small struggles to capture diverse structural patterns across programming tasks. Fine-tuning requires stable features, but variability prevents this
- **Core assumption:** The dataset's structural diversity exceeds what a small model can effectively learn through fine-tuning
- **Evidence anchors:** [abstract] "in-context learning with GPT-4o-mini outperformed the FLAN-T5 model"; [section] "Due to the lack of access to computational resources, the small version of the Flan-T5 was used"
- **Break condition:** If a larger FLAN-T5 variant is used, or if the dataset becomes more homogeneous, fine-tuning could achieve comparable or better performance

### Mechanism 3
- **Claim:** The task complexity classification problem is inherently challenging due to lack of consistent linguistic or structural markers within each complexity class
- **Mechanism:** Unlike traditional classification tasks where samples share common keywords, programming tasks vary widely in description, input/output format, and examples, even within the same complexity class
- **Core assumption:** The absence of stable, class-specific patterns is the primary reason for low performance across both methods
- **Evidence anchors:** [abstract] "both methods showed limited effectiveness due to the variability in task structures"; [section] "Each sample, even if it pertains to the same class, exhibits different structures and meanings"
- **Break condition:** If the dataset is expanded or curated to include more homogeneous task structures, or if a more powerful model is used, performance could improve significantly

## Foundational Learning

- **Concept:** In-context learning (ICL)
  - **Why needed here:** ICL allows the model to adapt to task variability without retraining, which is crucial given the structural diversity of programming tasks in the dataset
  - **Quick check question:** What are the three types of in-context learning mentioned in the paper, and how do they differ?
- **Concept:** Fine-tuning
  - **Why needed here:** Fine-tuning adapts a pre-trained model to a specific task by updating its weights, but it requires stable patterns in the training data to be effective
  - **Quick check question:** Why did the authors choose the FLAN-T5-small version instead of a larger variant?
- **Concept:** Task complexity classification
  - **Why needed here:** Understanding the challenge of classifying programming tasks by complexity is key to interpreting why both ICL and fine-tuning struggled in this study
  - **Quick check question:** What is the main reason given in the paper for the limited effectiveness of both classification approaches?

## Architecture Onboarding

- **Component map:** Dataset collection -> Task preprocessing -> Model training/evaluation -> Performance metrics
- **Critical path:** 1) Collect and preprocess 4,112 programming tasks from various websites; 2) Split dataset into training (80%) and testing (20%) sets; 3) Fine-tune FLAN-T5-small on training set; 4) Apply ICL with GPT-4o-mini on test set; 5) Evaluate both models using standard metrics
- **Design tradeoffs:** Model size vs. computational resources (FLAN-T5-small chosen due to resource constraints); Data variability vs. classification accuracy (high variability limits model performance); Open-source vs. proprietary models (FLAN-T5 vs. GPT-4o-mini)
- **Failure signatures:** Low accuracy (<60%) and F1-score (<54%) for both models; Inconsistent performance across complexity classes; High variability in task structures within each class
- **First 3 experiments:** 1) Fine-tune FLAN-T5-small with current dataset and evaluate performance; 2) Apply ICL with GPT-4o-mini using few-shot examples and evaluate performance; 3) Compare performance of FLAN-T5-small and GPT-4o-mini on subset of tasks with more homogeneous structures

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific features of task structure and complexity contribute most to the difficulty of classification models?
- **Basis in paper:** [explicit] The paper notes that variability in task structures within each complexity class hinders model performance
- **Why unresolved:** The paper identifies this as a key challenge but does not analyze which specific structural features are most predictive of difficulty
- **What evidence would resolve it:** A detailed feature analysis showing which task characteristics correlate most strongly with classification errors across models

### Open Question 2
- **Question:** How would different LLM architectures (beyond FLAN-T5 and GPT-4o-mini) perform on this task complexity classification task?
- **Basis in paper:** [explicit] The authors suggest exploring other LLMs like LLaMA-3 and ChatGPT-4 in future work
- **Why unresolved:** Only two approaches were tested, leaving open whether more advanced or differently-architected models would show substantially different results
- **What evidence would resolve it:** Comparative experiments with multiple LLM architectures using the same dataset and evaluation metrics

### Open Question 3
- **Question:** Does increasing dataset size beyond 4,112 tasks significantly improve classification performance?
- **Basis in paper:** [inferred] The authors plan to collect more samples to create a larger dataset in future work
- **Why unresolved:** The current dataset size may be insufficient for learning complex patterns, but the relationship between dataset size and performance remains untested
- **What evidence would resolve it:** Performance benchmarks showing accuracy improvements (or lack thereof) as dataset size increases through controlled experiments

## Limitations
- Low performance (best at 57% accuracy) due to high structural variability within complexity classes
- FLAN-T5-small's limited parameter capacity (77M) constrains its ability to capture diverse task patterns
- Small dataset size (4,112 tasks) may provide insufficient training signal for fine-tuning

## Confidence

- **High Confidence:** Dataset creation methodology and baseline results are clearly documented and reproducible; Comparative performance between FLAN-T5 and GPT-4o-mini is reliable
- **Medium Confidence:** Explanation for why ICL outperforms fine-tuning (adaptability to structural variability) is plausible but not empirically validated beyond current results
- **Low Confidence:** Claim that task complexity classification is inherently challenging due to lack of consistent markers lacks strong supporting evidence from related work

## Next Checks
1. **Model Scaling Validation:** Re-run experiment with FLAN-T5-base or larger variants to determine if increased parameter capacity improves performance on structurally diverse dataset
2. **Dataset Homogeneity Analysis:** Create subset of dataset with more standardized structures and evaluate whether performance improves significantly compared to full dataset
3. **Cross-Platform Generalization Test:** Train on tasks from one programming platform and test on tasks from another to measure how structural differences between platforms affect classification accuracy