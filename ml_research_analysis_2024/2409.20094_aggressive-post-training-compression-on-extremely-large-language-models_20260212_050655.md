---
ver: rpa2
title: Aggressive Post-Training Compression on Extremely Large Language Models
arxiv_id: '2409.20094'
source_url: https://arxiv.org/abs/2409.20094
tags:
- sparsity
- pruning
- weight
- llms
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of aggressively compressing extremely
  large language models (LLMs) while minimizing accuracy loss. The proposed method
  combines sparse pruning with quantization, achieving over 0.7 sparsity and less
  than 8-bit quantization.
---

# Aggressive Post-Training Compression on Extremely Large Language Models

## Quick Facts
- arXiv ID: 2409.20094
- Source URL: https://arxiv.org/abs/2409.20094
- Authors: Zining Zhang; Yao Chen; Bingsheng He; Zhenjie Zhang
- Reference count: 3
- Key result: Achieves >0.7 sparsity and <8-bit quantization on BLOOM-176B and OPT models with significantly improved perplexity over baseline methods

## Executive Summary
This paper presents a novel post-training compression method for extremely large language models that combines sparse pruning with quantization to achieve aggressive compression rates. The approach addresses the challenge of maintaining model accuracy while pushing compression to levels that enable deployment on resource-constrained devices. By introducing a layer-wise sparsity scheduler based on inverse Hessian matrix estimates, the method intelligently distributes sparsity across model layers to minimize accuracy degradation. Experiments demonstrate substantial improvements over existing techniques, achieving both high sparsity (>0.7) and aggressive quantization (<8-bit) while maintaining significantly better perplexity scores.

## Method Summary
The proposed method integrates sparse pruning with quantization in a post-training compression pipeline for LLMs. The core innovation is a layer-wise sparsity scheduler that estimates pruning errors using the inverse Hessian matrix, then distributes sparsity based on log-level clustering of these estimates. This scheduler exploits insights about weight update directions under selective pruning to maintain accuracy while enabling aggressive compression. The method processes the model layer by layer, computing Hessian-based error estimates for each layer, clustering these estimates to determine sparsity targets, and then applying both pruning and quantization. The approach adds minimal scheduling time to the compression process while enabling practical deployment of compressed LLMs on personal computers and mobile devices.

## Key Results
- Achieves >0.7 sparsity with <8-bit quantization on BLOOM-176B and OPT models
- Demonstrates significantly lower perplexity scores compared to baseline compression methods
- Maintains model accuracy while enabling practical deployment on resource-constrained devices

## Why This Works (Mechanism)
The method works by intelligently distributing sparsity across different layers based on their sensitivity to pruning. The inverse Hessian matrix provides information about how changes in weights affect the loss function, allowing the scheduler to identify which layers can tolerate more aggressive pruning without significant accuracy loss. By clustering these error estimates and assigning sparsity targets accordingly, the method preserves critical weights in sensitive layers while aggressively pruning less critical ones. The combination with quantization further reduces model size, and the layer-wise approach ensures that compression is optimized for the specific architecture rather than applying uniform compression across all layers.

## Foundational Learning

**Inverse Hessian Matrix Estimation**
- Why needed: Provides curvature information about the loss landscape to estimate sensitivity to weight changes
- Quick check: Verify that Hessian estimation scales efficiently for extremely large models

**Layer-wise Sparsity Scheduling**
- Why needed: Different layers have varying sensitivity to pruning; uniform sparsity leads to suboptimal results
- Quick check: Confirm that layer-wise approach consistently outperforms uniform sparsity across model architectures

**Log-level Clustering of Error Estimates**
- Why needed: Enables efficient grouping of layers with similar pruning sensitivity for systematic sparsity distribution
- Quick check: Validate that clustering method identifies meaningful groups that correlate with actual performance impact

## Architecture Onboarding

**Component Map**
Model -> Hessian Estimation Module -> Clustering Module -> Sparsity Assignment Module -> Pruning + Quantization Module -> Compressed Model

**Critical Path**
The critical path flows through the Hessian estimation for each layer, followed by clustering and sparsity assignment, then pruning and quantization. The layer-wise processing means each layer must be processed sequentially in the scheduling phase, though parallel processing may be possible during actual pruning and quantization.

**Design Tradeoffs**
- Accuracy vs. compression ratio: More aggressive compression yields greater size reduction but risks accuracy degradation
- Computational overhead vs. compression quality: Hessian-based estimation provides better results but adds scheduling time
- Uniform vs. layer-wise sparsity: Uniform compression is simpler but layer-wise optimization provides better accuracy preservation

**Failure Signatures**
- Degradation in perplexity scores beyond acceptable thresholds
- Layer collapse where critical weights are pruned in sensitive layers
- Quantization errors becoming dominant at extreme compression levels

**First Experiments**
1. Apply uniform sparsity and quantization to establish baseline performance
2. Implement layer-wise sparsity without Hessian estimation to isolate its contribution
3. Test compression on a smaller model variant before scaling to full model size

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to only two model architectures (BLOOM-176B and OPT)
- Primary metric is perplexity, which may not capture downstream task performance degradation
- Claims about generalizability to other model families and extreme sparsity levels (>0.8) remain unverified

## Confidence
- High confidence: The theoretical framework for layer-wise sparsity scheduling based on Hessian estimates
- Medium confidence: Empirical results on BLOOM-176B and OPT models showing improved perplexity
- Low confidence: Claims about generalizability to arbitrary LLM architectures and extreme sparsity levels (>0.8)

## Next Checks
1. Evaluate the method on additional LLM architectures (e.g., LLaMA, GPT-2) and benchmark downstream task performance beyond perplexity
2. Conduct ablation studies isolating the contribution of Hessian-based error estimation versus other components of the pipeline
3. Test the compression method at extreme sparsity levels (>0.8) to identify potential non-linear degradation thresholds