---
ver: rpa2
title: 'LIONs: An Empirically Optimized Approach to Align Language Models'
arxiv_id: '2407.06542'
source_url: https://arxiv.org/abs/2407.06542
tags:
- training
- data
- dataset
- arxiv
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a comprehensive analysis of modern language
  model alignment pipelines, examining the impact of various design choices across
  supervised fine-tuning, offline preference learning, and online preference learning
  stages. The study identifies key techniques including sequence packing with loss
  masking in SFT, scaling offline preference datasets and training steps in DPO, and
  online preference learning for chat benchmarks.
---

# LIONs: An Empirically Optimized Approach to Align Language Models

## Quick Facts
- arXiv ID: 2407.06542
- Source URL: https://arxiv.org/abs/2407.06542
- Reference count: 40
- Key outcome: LION models consistently outperform official instruct models (Gemma-2b-it and LLaMA-3-8b-it) on multiple benchmarks using only open-source resources.

## Executive Summary
This paper presents a comprehensive empirical study of language model alignment pipelines, identifying key techniques that significantly improve performance across supervised fine-tuning, offline preference learning, and online preference learning stages. The authors train the LION series of models from Gemma-2b-base and LLaMA-3-8b-base using publicly available datasets and open-source algorithms. Their findings show that sequence packing with loss masking in SFT, scaling offline preference datasets and training steps in DPO, and online preference learning for chat benchmarks collectively enable LION models to consistently outperform official instruct models on multiple benchmarks including Arena-Hard-Auto, AlpacaEval-2, MT-Bench, and OpenLLM.

## Method Summary
The LION alignment pipeline consists of three stages: (1) Supervised Fine-Tuning with sequence packing and loss masking, (2) Direct Preference Optimization (DPO) with tuned KL-divergence β and 2048 sequence length, and (3) Online Preference Learning using a reward model judge. The method uses Gemma-2b-base and LLaMA-3-8b-base models trained on open-source datasets including OpenHermes-2.5, SlimOrca, MetaMathQA, UltraChat for SFT, HH-RLHF, TLDR-Preference, UltraFeedback for offline DPO, and UltraFeedback prompts for online DPO. The training procedure involves 3 epochs for SFT with batch size 32 and learning rate 2e-5, DPO with batch size 128 and learning rate 5e-7, followed by online DPO with 60k sampled responses.

## Key Results
- LION models consistently outperform official instruct models (Gemma-2b-it and LLaMA-3-8b-it) on Arena-Hard-Auto, AlpacaEval-2, MT-Bench, and OpenLLM benchmarks
- Sequence packing with loss masking in SFT prevents overfitting to chat templates and improves benchmark performance
- Scaling offline preference datasets and training steps in DPO shows clear performance improvements following scaling laws
- Online preference learning significantly benefits chat benchmarks but shows limited impact on core capability/knowledge benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequence packing with loss masking significantly improves SFT performance.
- Mechanism: Packing groups sequences of varying lengths into a single long sequence, while loss masking ignores tokens that are not output tokens like user instructions. This prevents the model from learning irrelevant information and alleviates catastrophic forgetting.
- Core assumption: The model can effectively learn from packed sequences without being confused by the varying lengths and that ignoring loss on non-output tokens does not hinder learning.
- Evidence anchors:
  - [abstract] "...using techniques like sequence packing, loss masking in SFT, increasing the preference dataset size in DPO, and online DPO training can significantly improve the performance of language models."
  - [section] "We find that combining packing with loss masking consistently yields the best performance across both dataset scales. We believe this is because other strategies may overfit chat templates: the starting tokens in each batch remain unchanged, leading to poor adaptation to unseen templates used in benchmarks such as OpenLLM."
- Break condition: If the model starts generating irrelevant or incorrect information, or if the performance on benchmarks significantly drops.

### Mechanism 2
- Claim: Scaling offline preference datasets and training steps in DPO improves overall performance.
- Mechanism: Increasing the size of the preference dataset provides more diverse examples for the model to learn from, while increasing training steps allows the model to better optimize the preference objective.
- Core assumption: The additional data and training steps provide meaningful signal for the model to learn and do not lead to overfitting or over-optimization.
- Evidence anchors:
  - [abstract] "...scaling offline preference datasets improves overall performance..."
  - [section] "We find that increasing dataset size raises the point of saturation. This indicates that similar to SFT, scaling law exists in offline preference learning so that scaling both dataset size and training steps can improve performance."
- Break condition: If increasing dataset size or training steps leads to diminishing returns or a decrease in performance.

### Mechanism 3
- Claim: Online preference learning significantly benefits chat benchmarks.
- Mechanism: Online preference learning allows the model to iteratively refine its responses based on feedback from a reward model, leading to improved performance on chat-related tasks.
- Core assumption: The reward model provides accurate and informative feedback, and the model can effectively incorporate this feedback to improve its responses.
- Evidence anchors:
  - [abstract] "...and 3) online preference learning for chat benchmarks."
  - [section] "In Table 5, we first find that online training mainly benefits chat benchmarks (Arena Hard Auto*) but not core capability/knowledge benchmarks (OpenLLM). We believe this is because online preference pairs are derived from πθ itself, making it unlikely for πθ to acquire new knowledge or skills."
- Break condition: If the reward model's feedback becomes unreliable or if the model starts to overfit to the specific prompts used in online learning.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT is the initial step in aligning language models, where the model is trained to follow instructions using high-quality human-written demonstrations.
  - Quick check question: What is the main objective of SFT, and how does it differ from pre-training?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is an offline preference learning algorithm that directly optimizes the language model to align with human preferences, without the need for a separate reward model.
  - Quick check question: How does DPO differ from traditional RLHF methods, and what are its key advantages?

- Concept: Online Preference Learning
  - Why needed here: Online preference learning allows the model to iteratively refine its responses based on feedback from a reward model, leading to improved performance on chat-related tasks.
  - Quick check question: What is the main difference between online and offline preference learning, and why is online learning particularly beneficial for chat benchmarks?

## Architecture Onboarding

- Component map:
  - SFT stage: Gemma-2b-base or LLaMA-3-8b-base → Sequence packing + loss masking → πSFT
  - Offline DPO stage: πSFT → DPO with KL-divergence β and sequence length 2048 → πDPO
  - Online DPO stage: πDPO → Online preference learning with Pair-RM judge → πODPO

- Critical path: SFT → Offline DPO → Online DPO

- Design tradeoffs:
  - SFT: Packing vs. padding; with or without loss masking
  - DPO: Choice of KL-divergence β, sequence length, and reference model
  - Online DPO: Number of online training samples and the choice of reward model

- Failure signatures:
  - Overfitting to chat templates during SFT
  - Over-optimization or over-training during DPO
  - Unreliable or biased feedback from the reward model during online DPO

- First 3 experiments:
  1. Compare the performance of SFT with and without packing and loss masking on a small dataset
  2. Investigate the effect of different KL-divergence β values on the performance of DPO
  3. Evaluate the impact of online preference learning on chat benchmarks compared to offline DPO alone

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the parabolic pattern in probability margin changes indicate a universal characteristic of well-aligned models, or is it specific to the training methodology used in this study?
- Basis in paper: [explicit] The authors observe a parabolic shape in the probability margin changes for well-trained models, but do not explore whether this pattern is universal or methodology-specific.
- Why unresolved: The study only examines one training methodology and does not compare the probability margin changes across different alignment techniques.
- What evidence would resolve it: Comparative analysis of probability margin changes across multiple alignment methodologies (e.g., PPO, KTO, CTO) using the same model architecture and datasets.

### Open Question 2
- Question: How do the performance gains from scaling offline preference datasets and training steps in DPO compare to similar scaling in other preference optimization algorithms like KTO or CTO?
- Basis in paper: [explicit] The authors find that scaling offline preference datasets and training steps improves DPO performance, but do not compare this to other preference optimization algorithms.
- Why unresolved: The study focuses on DPO and does not investigate the scaling effects in other preference optimization algorithms.
- What evidence would resolve it: Systematic scaling experiments of dataset size and training steps across multiple preference optimization algorithms (DPO, KTO, CTO) using the same model architecture and datasets.

### Open Question 3
- Question: Is there a threshold dataset size or diversity beyond which further increases in dataset size no longer improve model performance, and how does this threshold vary across different model architectures?
- Basis in paper: [explicit] The authors observe performance saturation with increasing dataset size, but do not investigate if there is a specific threshold or how it varies across model architectures.
- Why unresolved: The study uses a fixed model architecture (Gemma-2b) and does not explore the dataset size threshold or its variation across different architectures.
- What evidence would resolve it: Scaling experiments of dataset size and diversity across multiple model architectures (e.g., Gemma-2b, LLaMA-3-8b, Mistral-7b) to identify performance thresholds and their variation.

## Limitations

- The sequence packing with loss masking technique lacks theoretical justification for why it prevents overfitting better than other padding strategies
- The KL-divergence hyperparameter β tuning procedure is not fully specified, with only a "small subset" mentioned
- Online preference learning improvements are limited to chat benchmarks and may not transfer to general capabilities

## Confidence

**High Confidence**: The effectiveness of sequence packing with loss masking in SFT, supported by consistent improvements across multiple dataset scales and direct comparisons to baseline methods. The scaling laws observed in offline DPO with larger datasets and training steps are also well-supported by systematic ablation studies.

**Medium Confidence**: The online preference learning benefits are supported by experimental results but are limited to chat benchmarks, with the mechanism for why this doesn't transfer to core capabilities remaining speculative. The KL-divergence hyperparameter tuning procedure shows effectiveness but lacks detailed methodological transparency.

**Low Confidence**: The claim that LION models consistently outperform official instruct models across all benchmarks, as this comparison may be influenced by differences in training data composition and evaluation protocols that are not fully controlled for in the study.

## Next Checks

1. **Cross-dataset validation**: Train LION-style models using completely different open-source datasets to verify whether the sequence packing and loss masking improvements are dataset-dependent or represent a general technique.

2. **Scaling analysis**: Systematically vary the online preference learning sample size beyond 60k to identify the point of diminishing returns and establish optimal computational tradeoffs for different model sizes.

3. **Transfer capability assessment**: Design experiments that specifically test whether online preference learning improvements transfer to non-chat capabilities by creating mixed benchmarks that combine chat-style prompts with factual knowledge questions.