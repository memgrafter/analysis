---
ver: rpa2
title: 'Deep Unlearn: Benchmarking Machine Unlearning for Image Classification'
arxiv_id: '2410.01276'
source_url: https://arxiv.org/abs/2410.01276
tags:
- unlearning
- methods
- data
- machine
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive benchmark of 18 machine unlearning
  methods across 5 datasets and 2 neural network architectures, evaluating their performance
  using accuracy retention, privacy metrics, and computational efficiency. The study
  finds that with proper hyperparameter tuning, Masked Small Gradients (MSG) and Convolution
  Transpose (CT) consistently outperform other methods across different models, datasets,
  and initializations, as measured by population-based membership inference attacks
  and per-sample unlearning likelihood ratio attacks.
---

# Deep Unlearn: Benchmarking Machine Unlearning for Image Classification

## Quick Facts
- arXiv ID: 2410.01276
- Source URL: https://arxiv.org/abs/2410.01276
- Reference count: 40
- This paper benchmarks 18 machine unlearning methods across 5 datasets and 2 neural network architectures, finding that Masked Small Gradients (MSG) and Convolution Transpose (CT) consistently outperform other methods with proper hyperparameter tuning.

## Executive Summary
This comprehensive benchmark evaluates 18 machine unlearning methods for image classification, testing them across five datasets (MNIST, FashionMNIST, CIFAR-10, CIFAR-100, UTKFace) and two neural network architectures (ResNet18, TinyViT). The study reveals that with proper hyperparameter tuning, Masked Small Gradients (MSG) and Convolution Transpose (CT) consistently achieve superior performance in terms of accuracy retention, privacy protection against membership inference attacks, and computational efficiency. The research demonstrates that commonly used baselines like Gradient Ascent perform poorly and should be replaced by methods like Negative Gradient Plus, highlighting the need for comprehensive evaluation frameworks in machine unlearning research.

## Method Summary
The study trains original models on five image datasets using two architectures, then applies 18 different unlearning methods to remove the influence of 10% randomly selected forget set data points. Each method undergoes a hyperparameter sweep with 100 trials to optimize for accuracy retention, privacy protection, and efficiency. The methods are evaluated using multiple metrics including population-based membership inference attacks (MIA), per-sample unlearning likelihood ratio attacks (U-LiRA), accuracy metrics, and runtime efficiency. Results are aggregated across 10 random seeds per configuration to ensure reliability.

## Key Results
- MSG and CT consistently outperform other methods across datasets, models, and initializations when properly tuned
- Commonly used baselines like Gradient Ascent perform poorly and should be replaced by methods like Negative Gradient Plus
- A method's reliability cannot be judged solely by performance against commonly used baselines
- The benchmark requires significant computational resources (50,220 models total) but provides reliable comparisons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: With proper hyperparameter tuning, Masked Small Gradients (MSG) and Convolution Transpose (CT) consistently outperform other methods across datasets, models, and initializations.
- Mechanism: MSG identifies and updates weights in convolutional layers most relevant to the forget set while dampening subsequent updates on the retain set. CT transposes convolutional layer weights to remove forget set influence.
- Core assumption: The forget set's influence can be localized to specific weights that can be selectively modified without severely degrading retain set performance.
- Evidence anchors:
  - [abstract] "Masked Small Gradients (MSG) and Convolution Transpose (CT), consistently perform better in terms of model accuracy and run-time efficiency across different models, datasets, and initializations"
  - [section] "MSG obtains a first rank in both Performance Retention Deviation and Indiscernibility. Its unique approach identifies the parameters in the Convolutional layers that most contribute to the information to be forgotten."
- Break condition: If forget set influence is distributed across many weights rather than localized, or if the model architecture doesn't use convolutional layers.

### Mechanism 2
- Claim: Comparing MU methods only with commonly used baselines like Gradient Ascent (GA) and Successive Random Relabeling (SRL) is inadequate.
- Mechanism: These baselines have fundamental limitations - GA only performs gradient ascent on the forget set without retain set correction, and SRL randomizes forget set labels but doesn't actively unlearn.
- Core assumption: Better baselines like Negative Gradient Plus (NG+) exist that address these limitations by combining gradient ascent on forget set with gradient descent on retain set.
- Evidence anchors:
  - [abstract] "our benchmark highlights the fact that comparing a MU method only with commonly used baselines, such as Gradient Ascent (GA) or Successive Random Relabeling (SRL), is inadequate, and we need better baselines like Negative Gradient Plus (NG+)"
  - [section] "GA which performs gradient ascent on the forget set, performs poorly across both metrics. Its more recent variation NG+, which uses an additional retain set correction, ranks fifth for both metrics, making it a more suitable baseline."
- Break condition: If NG+ doesn't consistently outperform GA across different datasets and model architectures.

### Mechanism 3
- Claim: A method's reliability cannot be judged solely by its performance against commonly used baselines.
- Mechanism: The benchmark uses comprehensive evaluation including population-based membership inference attacks (MIA) and per-sample unlearning likelihood ratio attacks (U-LiRA) across multiple datasets and architectures.
- Core assumption: Comprehensive evaluation frameworks are necessary because MU methods can perform differently depending on dataset characteristics, model architecture, and initialization.
- Evidence anchors:
  - [abstract] "our benchmark highlights the fact that comparing a MU method only with commonly used baselines... is inadequate"
  - [section] "The performance of the MU methods can change across datasets, model configurations, and model initializations; a reliable MU method remains consistent across these changes."
- Break condition: If a method shows consistent performance across all evaluation metrics regardless of dataset or architecture.

## Foundational Learning

- Concept: Membership Inference Attacks (MIA)
  - Why needed here: MIA is a primary evaluation metric for unlearning effectiveness, measuring whether the unlearned model can still distinguish forget set data from retain set data.
  - Quick check question: What does an indiscernibility score of 1 indicate about a model's resistance to MIA?

- Concept: Neural Network Architectures (ResNet vs TinyViT)
  - Why needed here: The benchmark evaluates methods across different architectures to assess generalizability, as some methods may work better for CNNs than transformers.
  - Quick check question: Why might methods like CT that are designed for CNNs still perform competitively on Vision Transformers?

- Concept: Hyperparameter Optimization
  - Why needed here: The study performs comprehensive hyperparameter sweeps to find optimal configurations, revealing that method performance heavily depends on proper hyperparameter selection.
  - Quick check question: How many hyperparameter trials were used per method to ensure fair comparison?

## Architecture Onboarding

- Component map: Original trained models -> Unlearning methods (18 types) -> Evaluation metrics (MIA, U-LiRA, accuracy metrics) -> Computational efficiency measurements -> Aggregation and ranking

- Critical path: 1) Train original models on full dataset, 2) Split data into forget/retain sets, 3) Apply each unlearning method with optimized hyperparameters, 4) Evaluate using multiple metrics across all initializations, 5) Aggregate results and rank methods

- Design tradeoffs: The comprehensive evaluation requires significant computational resources (50,220 models total) but provides reliable comparisons. Some methods may be architecture-specific but are still evaluated on all architectures to test generalizability.

- Failure signatures: Methods failing to produce usable models across all initializations are assigned to the Failed group. Poor performance on U-LiRA attacks indicates vulnerability to stronger privacy attacks despite passing weaker MIA tests.

- First 3 experiments:
  1. Verify baseline performance: Run Fine-tuning (FT) and Gradient Ascent (GA) on a small dataset (MNIST) with ResNet18 to confirm they serve as reasonable/unreasonable baselines respectively.
  2. Test architecture generalization: Apply MSG and CT to both ResNet18 and TinyViT on CIFAR-10 to verify they maintain top performance across architectures.
  3. Evaluate computational efficiency: Measure runtime of CT vs MSG vs FT on UTKFace to confirm speed claims (CT should be fastest, MSG should provide 5x+ speedup over retraining).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do machine unlearning methods perform across different types of data (e.g., medical images, time series, audio) compared to natural images?
- Basis in paper: [inferred] The paper states "we put our focus on natural image data, however, machine unlearning is relevant to other data types such as medical images or other modalities such as time series, audio and speech, or language data."
- Why unresolved: The study only evaluated on natural image datasets (MNIST, FashionMNIST, CIFAR-10, CIFAR-100, UTKFace) and did not explore other data types.
- What evidence would resolve it: Experiments applying the same 18 MU methods to non-image datasets and comparing their performance across different data modalities.

### Open Question 2
- Question: How do machine unlearning methods perform on different learning tasks beyond image classification?
- Basis in paper: [inferred] The paper mentions "we focus on the classification task, however, other learning tasks would greatly benefit from machine unlearning too. For instance removing concepts from generative models for images or poisoned data in language models."
- Why unresolved: The study only evaluated on image classification tasks and did not test other tasks like object detection, segmentation, generative modeling, or language modeling.
- What evidence would resolve it: Comprehensive benchmarking of MU methods across multiple learning tasks and model architectures.

### Open Question 3
- Question: What is the theoretical relationship between machine unlearning and differential privacy, and how can this inform the development of more effective MU methods?
- Basis in paper: [explicit] The paper discusses "Machine Unlearning and Differential Privacy" and states "Despite enabling provable error guarantees for Unlearning methods, Differential Privacy requires strong model and algorithmic assumptions, making MU, derived from it, potentially less effective against practical adversaries."
- Why unresolved: The paper does not provide a theoretical analysis of the relationship between MU and DP, and how DP assumptions affect practical MU effectiveness.
- What evidence would resolve it: Theoretical framework connecting MU and DP guarantees, with empirical validation showing how DP assumptions impact MU performance against real-world attacks.

## Limitations

- The study focuses only on image classification tasks, limiting generalizability to other domains like NLP or tabular data
- Evaluation relies on specific attack methodologies that may not capture all aspects of privacy leakage
- Computational intensity of comprehensive hyperparameter sweeps may limit practical applicability for researchers with constrained resources

## Confidence

- **High Confidence**: Claims about MSG and CT outperforming other methods across multiple datasets and architectures, supported by extensive experimental validation across 10 random seeds and comprehensive metrics
- **Medium Confidence**: Claims about commonly used baselines being inadequate, as these are based on relative performance comparisons that could shift with different datasets or attack methodologies
- **Medium Confidence**: Claims about computational efficiency advantages, as these depend on specific hardware implementations and optimization levels

## Next Checks

1. **Architecture Generalization Test**: Apply the top 5 methods (MSG, CT, NG+, CT-E, FT-E) to Vision Transformer architectures on ImageNet-1K to verify performance consistency beyond ResNet and TinyViT

2. **Domain Transfer Validation**: Evaluate the same 18 methods on a non-image dataset (e.g., IMDB movie reviews) to test whether findings about method reliability and efficiency transfer across data modalities

3. **Attack Method Robustness Check**: Implement and evaluate against newer attack methodologies like white-box membership inference attacks or differential privacy-based auditing to verify that current top performers maintain their rankings under different threat models