---
ver: rpa2
title: Multi-modal Cross-domain Self-supervised Pre-training for fMRI and EEG Fusion
arxiv_id: '2409.19130'
source_url: https://arxiv.org/abs/2409.19130
tags:
- fmri
- data
- domains
- brain
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel Multi-modal Cross-domain Self-supervised
  Pre-training Model (MCSP) to integrate fMRI and EEG data across spatial, temporal,
  and spectral domains. MCSP employs cross-domain and cross-modal self-supervised
  losses to leverage complementary information between modalities and domains through
  contrastive learning and knowledge distillation.
---

# Multi-modal Cross-domain Self-supervised Pre-training for fMRI and EEG Fusion

## Quick Facts
- arXiv ID: 2409.19130
- Source URL: https://arxiv.org/abs/2409.19130
- Authors: Xinxu Wei; Kanhao Zhao; Yong Jiao; Nancy B. Carlisle; Hua Xie; Gregory A. Fonzo; Yu Zhang
- Reference count: 40
- One-line primary result: MCSP achieves up to 11.9% AUROC improvement over state-of-the-art methods for multi-modal neuroimaging classification

## Executive Summary
This study introduces a novel Multi-modal Cross-domain Self-supervised Pre-training Model (MCSP) to integrate fMRI and EEG data across spatial, temporal, and spectral domains. The model employs cross-domain and cross-modal self-supervised losses to leverage complementary information between modalities and domains through contrastive learning and knowledge distillation. Experimental results demonstrate that MCSP significantly outperforms existing methods on ADHD, ASD, MDD, and sex classification tasks, achieving AUROC improvements of up to 11.9% compared to state-of-the-art approaches.

## Method Summary
The MCSP model integrates fMRI and EEG data by constructing spatial, temporal, and frequency domains from each modality. Domain-specific encoders (Graph Transformer for spatial, Transformer for temporal and frequency) process each domain separately. Cross-domain self-supervised loss employs domain-specific augmentations (edge removal for spatial, time-point noise for temporal, frequency transformation for frequency domain) combined with contrastive learning. Cross-modal self-supervised loss uses knowledge distillation and consistency regularization to transfer complementary information between modalities, with different transfer directions depending on domain-specific information density. The model is pretrained on large-scale datasets combining multiple neuroimaging sources before fine-tuning for specific classification tasks.

## Key Results
- MCSP achieves up to 11.9% AUROC improvement over state-of-the-art methods
- Spatial domain provides the most discriminative information for fMRI-based classification
- Temporal and frequency domains contribute complementary information for EEG-based classification
- Cross-modal pretraining significantly improves performance on downstream classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-domain self-supervised loss (CD-SSL) enables effective integration of spatial, temporal, and frequency domains by enforcing domain-specific augmentation and contrastive learning.
- Mechanism: Domain-specific data augmentation (edge removal, time-point noise, frequency transformation) generates paired contrastive views. Intra-domain contrastive loss maximizes similarity between original and augmented views while minimizing similarity to negatives, promoting domain-invariant feature extraction.
- Core assumption: The semantic content of brain signals remains invariant under domain-specific augmentations while the domain-specific structure varies.
- Evidence anchors:
  - [abstract]: "Our model employs cross-domain self-supervised loss that bridges domain differences by implementing domain-specific data augmentation and contrastive loss, enhancing feature discrimination."
  - [section]: "For the spatial domain, we leverage graph augmentation to generate spatially augmented views while keep semantic invariance of the brain network graph. For the temporal domain, to obtain temporal augmented views, we introduce random noise to the time series of both fMRI and EEG."
  - [corpus]: Weak - corpus lacks direct evidence about contrastive learning with domain-specific augmentation, but shows similar approaches exist in time series domains.
- Break condition: If domain-specific augmentations alter semantic content rather than just structural representation, contrastive learning would force alignment of semantically different features.

### Mechanism 2
- Claim: Cross-modal self-supervised loss (CM-SSL) effectively leverages complementary information between fMRI and EEG by knowledge distillation and consistency regularization.
- Mechanism: Intra-domain cross-modal distillation transfers knowledge from the modality with richer information in each domain to the other (fMRI→EEG for spatial, EEG→fMRI for temporal/frequency). Cross-modal consistency loss ensures distribution alignment across modality-fused representations.
- Core assumption: Different modalities contain complementary information in different domains (fMRI richer in spatial, EEG richer in temporal/frequency), enabling effective knowledge transfer.
- Evidence anchors:
  - [abstract]: "Furthermore, MCSP introduces cross-modal self-supervised loss to capitalize on the complementary information of fMRI and EEG, facilitating knowledge distillation within domains and maximizing cross-modal feature convergence."
  - [section]: "Assumption I, the similarity between embeddings of different modalities within the same domain should be as close as possible since they share similar semantics... Assumption II, because the information of fMRI and EEG signals varies across three domains..."
  - [corpus]: Missing - corpus doesn't contain direct evidence of cross-modal distillation between fMRI and EEG, though similar concepts appear in other neuroimaging fusion work.
- Break condition: If the complementary information assumption is incorrect (e.g., modalities don't have systematically different strengths across domains), distillation would transfer noise rather than useful information.

### Mechanism 3
- Claim: Cross-model distillation across domains enables knowledge transfer from a comprehensive multi-domain teacher model to specialized single-domain student models.
- Mechanism: A teacher model trained on spatial+temporal+frequency fusion provides rich representations. Student models for each domain learn through soft/hard target distillation, inheriting cross-domain knowledge while specializing.
- Core assumption: Multi-domain fusion captures generalizable patterns that benefit single-domain specialization when properly distilled.
- Evidence anchors:
  - [section]: "Another potential pre-training and fine-tuning paradigm is cross-model distillation across domains, showing the ability to distill the knowledge from a large teacher model, fused with knowledge from multiple domains, into a smaller student model focusing on a single domain."
  - [corpus]: Weak - corpus lacks direct evidence of cross-model distillation across domains, though similar concepts appear in model compression literature.
- Break condition: If the teacher model's multi-domain representations are too domain-specific, distillation would transfer domain-specific artifacts rather than generalizable knowledge.

## Foundational Learning

- Concept: Contrastive learning with domain-specific augmentation
  - Why needed here: Enables learning domain-invariant representations while preserving domain-specific discriminative features for neuroimaging data
  - Quick check question: How does domain-specific augmentation differ from generic data augmentation in preserving semantic content while varying structural properties?

- Concept: Knowledge distillation across modalities and domains
  - Why needed here: Leverages complementary information strengths of different modalities across different domains without requiring paired labels
  - Quick check question: Why does the direction of knowledge transfer (fMRI→EEG vs EEG→fMRI) change depending on the domain?

- Concept: Graph neural networks for brain connectivity
  - Why needed here: fMRI data is naturally represented as brain networks where ROIs are nodes and connectivity strengths are edges
  - Quick check question: How does graph augmentation (edge removal/perturbation) preserve semantic invariance while creating useful contrastive pairs?

## Architecture Onboarding

- Component map:
  Data preprocessing → Domain construction (spatial graphs, temporal/frequency sequences) → Domain-specific encoders (Graph Transformer for spatial, Transformer for temporal/frequency) → Modality-aware projectors (align sequences, project to contrastive space) → Cross-domain SSL (CD-SSL with augmentation + contrastive loss) → Cross-modal SSL (CM-SSL with distillation + consistency loss) → Modality fusion layers → Task-specific MLP classifier

- Critical path: Data construction → Encoding → Projection → Cross-domain/contrastive loss → Cross-modal loss → Fusion → Classification

- Design tradeoffs:
  Parameter efficiency vs. performance: Three separate domain-specific models vs. single unified model
  Augmentation strength vs. semantic preservation: Stronger augmentations create better contrastive pairs but risk semantic drift
  Distillation direction vs. information flow: Choosing correct modality-domain transfer directions based on information density

- Failure signatures:
  Performance collapse during pre-training: Likely indicates contrastive loss optimization issues or augmentation semantic drift
  Poor cross-modal transfer: Suggests incorrect assumptions about modality complementarity or insufficient alignment
  Domain-specific degradation: May indicate overfitting to specific domain characteristics or insufficient cross-domain regularization

- First 3 experiments:
  1. Ablation study: Train with only spatial domain vs. all three domains to verify cross-domain benefits
  2. Modality swap: Pre-train on fMRI, fine-tune on EEG and vice versa to test modality-agnostic learning
  3. Augmentation sensitivity: Vary augmentation strength (edge removal percentage, noise level) to find optimal contrastive signal

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between spatial, temporal, and frequency domain representations for different mental disorders when using MCSP?
- Basis in paper: [inferred] The authors show that different domains have varying performance across disorders (e.g., spatial domain outperformed temporal/frequency for fMRI by ~4% in AUROC/Accuracy), but don't systematically explore optimal domain weighting strategies.
- Why unresolved: The current ablation study shows relative performance differences but doesn't investigate whether adaptive weighting or domain-specific attention mechanisms could improve results for specific disorders.
- What evidence would resolve it: Systematic experiments varying domain importance weights or implementing domain-specific attention mechanisms, with results showing performance gains for different disorder types.

### Open Question 2
- Question: How does the cross-matching strategy for generating fMRI-EEG pairs affect the quality and generalizability of pre-training?
- Basis in paper: [explicit] The authors acknowledge that cross-matching strategy (generating up to 22 pairs per subject) was used to increase pre-training sample size due to limited paired subjects, but don't evaluate potential artifacts or biases this introduces.
- Why unresolved: The authors note this strategy may "potentially compromise the effectiveness of pre-training" but don't investigate whether the artificial pairings affect model performance or introduce domain-specific artifacts.
- What evidence would resolve it: Comparative experiments using only naturally paired data versus cross-matched data, with analysis of performance differences and potential artifacts introduced by the pairing strategy.

### Open Question 3
- Question: What is the theoretical basis for the domain-specific knowledge distillation directions (fMRI→EEG in spatial domain, EEG→fMRI in temporal/frequency domains)?
- Basis in paper: [explicit] The authors state that "fMRI has higher spatial resolution but lower temporal and frequency resolution, while EEG has higher temporal and frequency resolution but lower spatial resolution," which justifies their distillation directions, but don't provide empirical validation.
- Why unresolved: While the authors provide intuitive reasoning for their distillation direction choices, they don't empirically test whether reversing these directions would affect performance or whether the directionality is optimal.
- What evidence would resolve it: Ablation studies comparing different distillation direction configurations, with performance metrics showing whether the proposed directionality is optimal or task-dependent.

## Limitations
- Cross-domain augmentation assumes semantic invariance under structural transformations, requiring empirical validation
- Performance gains need replication on independent datasets to confirm generalizability
- Cross-matching strategy for generating fMRI-EEG pairs may introduce artifacts or biases

## Confidence
**High Confidence**: Core architectural innovations are technically sound and well-motivated by neuroimaging principles
**Medium Confidence**: Performance improvements are substantial within reported experiments but may vary with dataset characteristics
**Low Confidence**: Generalization to clinical settings and different acquisition protocols remains uncertain

## Next Checks
1. Ablation study on domain-specific augmentations: Systematically vary augmentation strength to identify optimal balance between informative contrastive pairs and semantic preservation
2. Modality transferability test: Pre-train on fMRI, fine-tune on EEG (and vice versa) to evaluate modality-agnostic learning
3. Clinical generalization assessment: Apply pretrained model to independent clinical dataset with different protocols and populations