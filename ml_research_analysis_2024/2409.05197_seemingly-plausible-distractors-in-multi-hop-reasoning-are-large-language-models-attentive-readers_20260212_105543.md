---
ver: rpa2
title: 'Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language
  Models Attentive Readers?'
arxiv_id: '2409.05197'
source_url: https://arxiv.org/abs/2409.05197
tags:
- reasoning
- paragraphs
- question
- answer
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether state-of-the-art Large Language
  Models (LLMs) can perform multi-hop reasoning when presented with seemingly plausible
  but ultimately incorrect reasoning paths. Previous research showed that fine-tuned
  models often circumvent reasoning requirements by exploiting simple textual cues.
---

# Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?

## Quick Facts
- arXiv ID: 2409.05197
- Source URL: https://arxiv.org/abs/2409.05197
- Authors: Neeladri Bhuiya; Viktor Schlegel; Stefan Winkler
- Reference count: 27
- Key outcome: LLMs show up to 45% relative decrease in F1 score when presented with seemingly plausible but incorrect reasoning paths, revealing limitations in their multi-hop reasoning capabilities.

## Executive Summary
This paper investigates whether state-of-the-art Large Language Models (LLMs) can perform multi-hop reasoning when presented with seemingly plausible but ultimately incorrect reasoning paths. Previous research showed that fine-tuned models often circumvent reasoning requirements by exploiting simple textual cues. The authors propose a method to generate "plausible distractors" - alternative reasoning chains that appear valid but lead to incorrect answers. They evaluate multiple open and proprietary LLMs, including GPT-4, and find that model performance drops significantly when presented with these distractors. Analysis reveals that LLMs tend to ignore misleading lexical cues but struggle with misleading reasoning paths.

## Method Summary
The authors propose a challenging multi-hop reasoning benchmark by generating seemingly plausible multi-hop reasoning chains that ultimately lead to incorrect answers. They decompose questions into sub-questions, extract main entities and modifiable details, and use GPT-4 to create plausible but wrong reasoning chains. These distractor paragraphs are then presented alongside correct information to evaluate various LLMs' ability to perform multi-hop reasoning. The method goes beyond simple lexical similarity by ensuring continuity in reasoning paths and creating distractors that require attention to small but important details.

## Key Results
- LLMs show up to 45% relative decrease in F1 score when presented with plausible distractors
- Even GPT-4, the strongest model tested, showed a 14-point F1 score drop under the strongest adversarial attack setting
- Analysis reveals that LLMs tend to ignore misleading lexical cues but struggle with misleading reasoning paths
- Existing adversarial attacks are inadequate for evaluating LLMs as they rely on simple lexical overlap rather than complex reasoning patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can be distracted by seemingly plausible reasoning paths that ultimately lead to incorrect answers.
- **Mechanism:** The proposed method generates "distractor" paragraphs that create alternative reasoning chains. These chains appear valid but lead to incorrect answers. When LLMs are presented with these distractors alongside correct information, they struggle to maintain performance because the alternative paths exploit the model's tendency to over-reason or be misled by superficially coherent but ultimately wrong narratives.
- **Core assumption:** LLMs rely on the coherence and plausibility of reasoning chains rather than just lexical cues when performing multi-hop reasoning.
- **Evidence anchors:**
  - [abstract] "We propose a challenging multi-hop reasoning benchmark, by generating seemingly plausible multi-hop reasoning chains, which ultimately lead to incorrect answers. We evaluate multiple open and proprietary state-of-the-art LLMs, and find that their performance to perform multi-hop reasoning is affected, as indicated by up to 45% relative decrease in F1 score when presented with such seemingly plausible alternatives."
  - [section] "We find evidence that they indeed circumvent the requirement to perform multi-hop reasoning, but they do so in more subtle ways than what was reported about their fine-tuned pre-trained language model (PLM) predecessors."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.43, average citations=0.0. Top related titles: Robust Knowledge Editing via Explicit Reasoning Chains for Distractor-Resilient Multi-Hop QA, Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering, Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction.

### Mechanism 2
- **Claim:** Existing adversarial attacks are inadequate for evaluating LLMs' multi-hop reasoning capabilities.
- **Mechanism:** Previous adversarial methods focused on fine-tuned models and relied on simple lexical overlap or word matching to create distracting paragraphs. LLMs, due to their size and emergent capabilities, circumvent multi-hop reasoning through more subtle textual cues that these existing methods don't capture. The proposed method goes beyond simple lexical similarity by ensuring continuity in reasoning paths and creating distractors that require attention to small but important details.
- **Core assumption:** LLMs have evolved beyond simple pattern matching and require more sophisticated adversarial examples that test their ability to follow complex reasoning paths.
- **Evidence anchors:**
  - [abstract] "Analysis reveals that LLMs tend to ignore misleading lexical cues but struggle with misleading reasoning paths."
  - [section] "By design, these methods bear only negative predictive power (Gardner et al., 2020): failing to see a performance drop does not imply that the model performs the evaluated capability well, but rather that the methodology might have limited suitability to evaluate the investigated phenomenon, i.e., multi-hop reasoning."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.43, average citations=0.0. Top related titles: Robust Knowledge Editing via Explicit Reasoning Chains for Distractor-Resilient Multi-Hop QA, Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering, Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction.

### Mechanism 3
- **Claim:** LLMs exhibit "sycophancy" and "over-reasoning" behaviors that make them vulnerable to seemingly plausible but incorrect reasoning paths.
- **Mechanism:** Due to sycophancy (tendency to generate the presumably preferred answer over the correct one) and over-reasoning (generating complicated reasoning where none is required), LLMs are prone to following seemingly plausible reasoning chains even when they lead to incorrect answers. The proposed method exploits these tendencies by creating alternative reasoning paths that appear valid but contain subtle errors.
- **Core assumption:** LLMs have emergent behaviors like sycophancy and over-reasoning that weren't present in fine-tuned models and create new vulnerabilities in reasoning tasks.
- **Evidence anchors:**
  - [abstract] "Another line of research suggests that LLMs tend to 'over-reason' (Chiang and Lee, 2024), perhaps due to 'sycophancy' (Perez et al., 2023), i.e., the tendency to generate the presumably preferred answer over the correct one, leading to complicated reasoning where none is required."
  - [section] "We show that existing methods—calibrated to evaluate pre-LLM architectures—are inadequate to evaluate LLMs, and that LLM reasoning failures are indeed distinct from their fine-tuned PLM predecessors."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.43, average citations=0.0. Top related titles: Robust Knowledge Editing via Explicit Reasoning Chains for Distractor-Resilient Multi-Hop QA, Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering, Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction.

## Foundational Learning

- **Concept:** Multi-hop reasoning
  - **Why needed here:** The paper focuses on evaluating LLMs' ability to integrate information from multiple textual sources to answer questions. Understanding multi-hop reasoning is crucial because the proposed method specifically targets this capability by creating alternative reasoning chains.
  - **Quick check question:** What distinguishes multi-hop reasoning from single-hop reasoning in question answering tasks?

- **Concept:** Adversarial examples in NLP
  - **Why needed here:** The paper introduces a new method for creating adversarial examples specifically designed to test LLMs' reasoning capabilities. Understanding how adversarial examples work in NLP is essential for grasping why existing methods are inadequate and how the proposed method improves upon them.
  - **Quick check question:** How do traditional adversarial attacks in NLP differ from the proposed method for creating "plausible distractors"?

- **Concept:** Language model evaluation methodologies
  - **Why needed here:** The paper discusses the limitations of current evaluation methods for LLMs and proposes a new benchmark. Understanding evaluation methodologies is crucial for appreciating the significance of the proposed approach and its implications for future research.
  - **Quick check question:** What are the key differences between evaluating fine-tuned models versus zero/few-shot LLMs?

## Architecture Onboarding

- **Component map:** Question decomposition module -> Entity extraction module -> Distractor generation module -> Evaluation framework -> Analysis tools
- **Critical path:** 1. Decompose question into sub-questions 2. Extract main entities and modifiable details 3. Generate fake entities and modify questions 4. Create distractor paragraphs using GPT-4 5. Evaluate LLMs on original vs. adversarial datasets 6. Analyze performance differences and parameter effects
- **Design tradeoffs:** Using GPT-4 for distractor generation ensures high-quality plausible distractors but adds dependency on a specific model; manual verification of generated examples ensures quality but limits scalability; the method requires question decomposition, which may not work for all question types; the approach focuses on HotpotQA-style questions, which may not generalize to all multi-hop reasoning tasks
- **Failure signatures:** If generated distractors contain contradictions with gold paragraphs; if performance drop is due to increased complexity rather than reasoning difficulty; if the method fails to create sufficiently plausible alternative reasoning chains; if LLMs can easily distinguish between real and fake paragraphs based on subtle cues
- **First 3 experiments:** 1. Baseline comparison: Test Llama-2-13B on original HotpotQA vs. existing adversarial methods (AddDoc) to show inadequacy of current approaches 2. Proposed method evaluation: Test multiple LLMs (Llama-2-13B, Mixtral-8x7B, Llama-2-70B, GPT-3.5, GPT-4) on original vs. proposed adversarial dataset to measure performance drops 3. Parameter analysis: Vary number of distractor paragraphs, relatedness, and modification types to understand what makes reasoning harder for LLMs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do more advanced prompting techniques like tree-of-thought reasoning or least-to-most prompting improve LLMs' resilience to plausible distractors in multi-hop reasoning?
- **Basis in paper:** [inferred] The paper tested self-consistency and instructed chain-of-thought prompting, finding limited improvements (4.2 F1 points for self-consistency), but did not explore more sophisticated reasoning methods.
- **Why unresolved:** The paper only tested basic prompting enhancements. More complex reasoning methods could potentially provide better robustness against plausible distractors.
- **What evidence would resolve it:** Experiments testing tree-of-thought, least-to-most prompting, or other advanced reasoning methods on the same adversarial multi-hop reasoning benchmark, comparing F1 score drops to the baseline methods tested.

### Open Question 2
- **Question:** What is the theoretical limit of LLM performance on multi-hop reasoning when presented with plausible distractors, and how does this compare to human performance on similar tasks?
- **Basis in paper:** [explicit] The paper found that even GPT-4 (the strongest model tested) showed a 14-point F1 score drop under the strongest adversarial attack setting, suggesting a fundamental limitation.
- **Why unresolved:** The paper only tested a subset of models and did not include human baselines for comparison. The gap between current LLM capabilities and theoretical limits remains unclear.
- **What evidence would resolve it:** Human evaluation studies using the same plausible distractor methodology, along with testing of additional LLM architectures and scales to map the performance ceiling.

### Open Question 3
- **Question:** How do different types of reasoning shortcuts (lexical, syntactic, semantic) contribute to LLM failures on multi-hop reasoning tasks, and can these be disentangled?
- **Basis in paper:** [explicit] The paper found that existing adversarial methods targeting lexical shortcuts (AddDoc) were ineffective against LLMs, suggesting they exploit more subtle cues. The authors also distinguished between distractors affecting lexical cues versus reasoning paths.
- **Why unresolved:** While the paper identified that LLMs use more subtle shortcuts than fine-tuned models, it did not systematically categorize or disentangle the different types of reasoning shortcuts that LLMs might exploit.
- **What evidence would resolve it:** Ablation studies where different types of shortcuts are systematically introduced or removed, combined with interpretability analysis to identify which model components are responsible for each type of failure.

## Limitations
- The method relies heavily on GPT-4 for generating adversarial examples, creating dependency on a specific model
- Evaluation focuses primarily on HotpotQA-style questions, limiting generalizability to other multi-hop reasoning tasks
- The paper doesn't fully explain why certain models (like GPT-4) perform better than others beyond reasoning capability

## Confidence
- **High confidence:** The core finding that LLMs' performance drops significantly (up to 45% relative decrease in F1 score) when presented with plausible distractors is well-supported by the experimental results and analysis
- **Medium confidence:** The claim that existing adversarial methods are inadequate for evaluating LLMs' reasoning capabilities is supported, but the paper doesn't fully explore whether newer adversarial techniques might address these limitations
- **Medium confidence:** The explanation of why LLMs struggle with misleading reasoning paths (sycophancy and over-reasoning) is plausible but not definitively proven, as alternative explanations could also account for the observed behavior

## Next Checks
1. **Cross-dataset validation:** Test the proposed adversarial method on other multi-hop reasoning datasets (e.g., QASC, StrategyQA) to verify whether the observed performance drops generalize beyond HotpotQA

2. **Alternative distractor generation:** Implement distractor generation using different LLMs or rule-based methods to assess whether the performance drops are consistent across different ways of creating plausible distractors

3. **Fine-tuning resistance:** Fine-tune selected LLMs on datasets containing the proposed adversarial examples to determine whether models can learn to distinguish between valid and invalid reasoning paths, providing insights into the nature of the reasoning failures