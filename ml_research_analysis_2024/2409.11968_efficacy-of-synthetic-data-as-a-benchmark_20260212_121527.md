---
ver: rpa2
title: Efficacy of Synthetic Data as a Benchmark
arxiv_id: '2409.11968'
source_url: https://arxiv.org/abs/2409.11968
tags:
- data
- synthetic
- performance
- llms
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper evaluates whether synthetic data generated by LLMs can\
  \ serve as reliable benchmarks for NLP tasks. It compares absolute and relative\
  \ performance between synthetic and real datasets, using mean squared performance\
  \ difference (MSPD) and Spearman\u2019s rank correlation (SRCC)."
---

# Efficacy of Synthetic Data as a Benchmark
## Quick Facts
- **arXiv ID**: 2409.11968
- **Source URL**: https://arxiv.org/abs/2409.11968
- **Reference count**: 18
- **Primary result**: Synthetic data accurately reflects relative performance for simpler tasks like intent detection but less so for complex tasks like NER; smaller models exhibit bias toward their own generated data

## Executive Summary
This paper investigates whether synthetic data generated by large language models can serve as reliable benchmarks for NLP tasks. The authors compare absolute and relative performance between synthetic and real datasets using mean squared performance difference (MSPD) and Spearman's rank correlation (SRCC). Their findings reveal that synthetic data effectively captures relative performance for simpler tasks such as intent detection, while showing reduced fidelity for more complex tasks like named entity recognition (NER). The study introduces a novel bias factor metric demonstrating that smaller LLMs exhibit systematic bias toward their own synthetic data, while larger models do not.

The research provides important insights for practitioners considering synthetic data for benchmarking. For simpler classification tasks, synthetic data offers a cost-effective alternative to real data, accurately reflecting both absolute and relative model performance. However, for complex tasks requiring nuanced understanding, the reliability diminishes. The bias factor analysis reveals a critical size-dependent limitation: smaller models systematically overestimate their own performance on self-generated data. The authors recommend using synthetic data from multiple larger models to ensure robust benchmarking, particularly when evaluating complex NLP tasks.

## Method Summary
The authors evaluate synthetic data as a benchmark by comparing model performance on synthetic versus real datasets across multiple NLP tasks. They employ two primary metrics: mean squared performance difference (MSPD) to measure absolute performance deviation, and Spearman's rank correlation coefficient (SRCC) to assess relative performance ordering. The study focuses on intent detection and named entity recognition tasks, using various LLM sizes to generate synthetic data. A novel bias factor metric is introduced to quantify systematic bias in model performance when evaluating self-generated data. The experimental design includes controlled comparisons between absolute and relative performance metrics to determine the reliability of synthetic benchmarks.

## Key Results
- Synthetic data accurately reflects relative performance for simpler tasks like intent detection (high SRCC values)
- Performance correlation degrades substantially for complex tasks like named entity recognition
- Smaller language models show systematic bias toward their own synthetic data, while larger models do not
- MSPD and SRCC metrics provide quantitative measures for evaluating synthetic benchmark reliability

## Why This Works (Mechanism)
Synthetic data from larger models captures broader linguistic patterns and task representations, making it more representative of real-world performance. The mechanism relies on the capacity of larger models to generate diverse, high-quality examples that span the task distribution more completely. For simpler tasks, the decision boundaries are more straightforward, allowing synthetic data to accurately represent performance differences. The bias factor mechanism reveals that smaller models have limited capacity to generate data that adequately challenges their own capabilities, leading to overestimation when self-evaluating. This size-dependent bias suggests that model capacity directly influences the quality and representativeness of synthetic benchmarks.

## Foundational Learning
- **Mean Squared Performance Difference (MSPD)**: Measures absolute deviation between synthetic and real data performance - needed to quantify how closely synthetic benchmarks match real-world results; quick check: calculate MSPD between any two performance vectors
- **Spearman's Rank Correlation Coefficient (SRCC)**: Assesses whether relative performance ordering is preserved - needed to determine if synthetic data maintains model ranking accuracy; quick check: compute SRCC between two ranking lists
- **Bias Factor Metric**: Quantifies systematic bias when models evaluate their own synthetic data - needed to identify size-dependent reliability issues; quick check: compare model performance on self-generated vs. external synthetic data
- **Synthetic Data Generation**: Process of creating task-specific examples using LLMs - needed as the core methodology for creating benchmarks; quick check: verify generated examples meet task requirements
- **Task Complexity Spectrum**: Classification from simple (intent detection) to complex (NER) - needed to understand when synthetic data remains reliable; quick check: map tasks to complexity levels based on required reasoning
- **Model Size Correlation**: Relationship between model capacity and benchmark quality - needed to explain why larger models produce better synthetic benchmarks; quick check: correlate model parameters with bias factor scores

## Architecture Onboarding
Component map: Synthetic Data Generator -> Performance Evaluator -> Metric Calculator -> Bias Analyzer
Critical path: Synthetic data generation → model evaluation on synthetic data → performance comparison with real data → bias factor calculation → reliability assessment
Design tradeoffs: Simpler tasks favor synthetic data use (cost-effective, scalable) vs. complex tasks require real data (accuracy critical, resource intensive)
Failure signatures: High MSPD values indicate absolute performance mismatch; low SRCC values suggest ranking distortion; positive bias factors reveal self-evaluation inflation
First experiments: 1) Generate synthetic intent detection data and compare MSPD vs real data performance 2) Calculate SRCC for NER task rankings between synthetic and real data 3) Measure bias factor across model sizes for self-generated NER data

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Experimental scope limited to two task types (intent detection and NER)
- Model diversity restricted to a narrow range of LLM sizes
- Analysis focuses on correlation rather than causation for bias factors
- Does not address domain-specific nuances or subtle linguistic phenomena

## Confidence
High: Synthetic data effectively benchmarks relative performance for simple tasks like intent detection
Medium: Synthetic data less reliable for complex tasks like NER; bias in smaller models is a real concern
Medium: Using multiple larger models for synthetic data generation improves robustness

## Next Checks
1. Replicate the study across additional NLP tasks (e.g., question answering, sentiment analysis) to test generalizability
2. Compare synthetic vs. real data performance using models trained on different data distributions to assess domain transfer
3. Investigate whether ensemble methods combining multiple synthetic datasets reduce bias more effectively than using individual larger models