---
ver: rpa2
title: Graph Learning for Numeric Planning
arxiv_id: '2410.24080'
source_url: https://arxiv.org/abs/2410.24080
tags:
- planning
- numeric
- learning
- graph
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes learning approaches for numeric planning tasks,\
  \ extending symbolic planning to include numeric variables. The authors introduce\
  \ a new graph representation (\u03BDILG) for numeric planning tasks and develop\
  \ the CC WL algorithm to handle graphs with both continuous and categorical attributes."
---

# Graph Learning for Numeric Planning

## Quick Facts
- arXiv ID: 2410.24080
- Source URL: https://arxiv.org/abs/2410.24080
- Reference count: 40
- Key outcome: CC WL graph kernels achieve competitive coverage on numeric planning tasks with faster heuristic evaluation than GNNs

## Executive Summary
This paper extends graph learning approaches from symbolic to numeric planning tasks by introducing a new graph representation (νILG) and algorithms capable of handling continuous attributes. The authors develop the CC WL algorithm, which extends the Weisfeiler-Lehman algorithm to handle graphs with both continuous and categorical attributes, and propose new optimization methods for learning heuristic functions, including a ranking formulation. Experiments demonstrate that their graph kernels are more efficient and generalize better than graph neural networks for numeric planning, achieving competitive coverage performance compared to domain-independent numeric planners.

## Method Summary
The authors propose learning heuristic functions for numeric planning tasks using graph learning methods. They introduce the νILG graph representation that encodes numeric planning tasks as graphs with nodes representing objects, propositional/numeric variables, and goal conditions, and edges capturing their relationships. Two main approaches are explored: (1) CC WL kernel with linear models (SVR or ranking LP optimization), and (2) GNNs on transformed νILG graphs with MSE or ranking loss. The learned heuristics are used with Greedy Best First Search (GBFS) to solve planning problems. The method is evaluated on 8 numeric planning domains converted from IPC-LT benchmarks.

## Key Results
- CC WL models achieve coverage comparable to domain-independent numeric planners (2-29% higher than LPG-td) while being significantly faster to evaluate
- Ranking formulations outperform cost-to-go estimation, with ranking models achieving 100% coverage on Childsnack and 91% on Rovers
- GNN models are at least an order of magnitude slower than CC WL models for heuristic evaluation due to intensive matrix operations
- Models trained with ranking formulations tend to generalize better than cost-to-go models across most domains

## Why This Works (Mechanism)

### Mechanism 1
Graph learning is naturally suited for numeric planning because it can exploit relational structures and handle arbitrary numbers of objects. The νILG graph representation encodes numeric planning tasks as graphs where nodes represent objects, propositional/numeric variables, and goal conditions, while edges capture their relationships. This representation allows graph kernels and neural networks to operate directly on planning instances without requiring explicit grounding of all possible variables.

### Mechanism 2
The CC WL algorithm can handle both continuous and categorical attributes in a meaningful way for numeric planning. It extends the Weisfeiler-Lehman algorithm by hashing both categorical and continuous node features, using edge labels in the hashing function, and pooling continuous attributes across nodes with the same color. This creates features that capture both the symbolic and numeric aspects of planning tasks.

### Mechanism 3
Ranking formulations are more effective than cost-to-go estimation for learning heuristic functions in numeric planning. The ranking formulation trains heuristics to correctly order states rather than predict exact cost values. This uses more training data by considering successor states and has a larger solution space since exact values aren't required.

## Foundational Learning

- **Concept**: Graph Neural Networks (GNNs) and their limitations
  - **Why needed here**: Understanding GNN limitations explains why the CC WL kernel approach was developed and why GNNs serve as a baseline comparison
  - **Quick check question**: What is the fundamental limitation of GNNs that makes them less effective for arbitrary planning domains?

- **Concept**: Heuristic search and its role in planning
  - **Why needed here**: The paper builds heuristic functions for use with Greedy Best First Search (GBFS), so understanding how heuristics guide search is essential
  - **Quick check question**: How does a heuristic function guide GBFS differently than an admissible heuristic in A*?

- **Concept**: Weisfeiler-Lehman (WL) algorithm and graph kernels
  - **Why needed here**: The CC WL algorithm is an extension of the WL algorithm, so understanding the original WL algorithm is crucial for grasping the extensions made
  - **Quick check question**: What is the key operation in the WL algorithm that creates increasingly refined node features?

## Architecture Onboarding

- **Component map**: νILG graph encoder -> CC WL/GNN feature generator -> Linear model (SVR/LP) -> GBFS search engine

- **Critical path**: 
  1. Convert planning task to νILG graph
  2. Generate features using CC WL or GNN
  3. Train linear model using SVR or LP formulation
  4. Use learned heuristic in GBFS search

- **Design tradeoffs**:
  - CC WL vs GNN: CC WL is faster and more interpretable but less expressive; GNNs are slower but can learn more complex patterns
  - Cost-to-go vs ranking: Cost-to-go requires exact predictions but uses less training data; ranking is more flexible but may not provide useful cost estimates
  - Number of CC WL iterations: More iterations increase expressivity but also computation time and risk overfitting

- **Failure signatures**:
  - CC WL models: Poor coverage on domains requiring complex numeric reasoning (Rovers, Satellite, Transport)
  - GNN models: Significantly slower heuristic evaluation, similar or worse coverage than CC WL
  - Ranking models: May not provide useful cost estimates even when coverage is good
  - Cost-to-go models: May overfit to training data and fail to generalize

- **First 3 experiments**:
  1. Run CC WL with L=0 and L=1 on Blocksworld to verify basic functionality and observe performance difference
  2. Compare heuristic evaluation times between CC WL and GNN models on a small problem to confirm the speed difference
  3. Test the ranking formulation on Childsnack to verify perfect coverage achievement and understand why it succeeds where cost-to-go fails

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CC WL models scale with increasing number of iterations L, and is there an optimal value of L for different numeric planning domains? The paper mentions that increasing L improves model expressivity but comes at the cost of heuristic evaluation time and increased risk of overfitting. It also notes that surprisingly, L=0 or L=1 provides the best coverage for most domains, but does not provide a detailed analysis of how CC WL performance scales with L across different domains.

### Open Question 2
Can the CC WL algorithm be extended to directly refine continuous attributes during the refinement process, and how would this affect performance and computational complexity? The paper suggests this could be done by introducing aggregation functions, but notes that this would increase the size of the continuous feature vector exponentially with the number of layers. The paper does not implement or test this extension.

### Open Question 3
How do GNN models perform on numeric planning tasks when evaluated on GPUs instead of CPUs, and does this change their relative performance compared to CC WL models? The paper mentions that GNN models are evaluated on CPUs and could be sped up with access to GPUs, but does not provide GPU evaluation results for GNN models.

## Limitations

- CC WL algorithm's effectiveness for complex numeric domains (Rovers, Satellite, Transport) remains limited, suggesting the hashing approach may not capture sufficient numeric semantics
- Experimental validation relies on synthetic training data (small problems with known optimal solutions), raising questions about real-world applicability
- Comparative analysis focuses primarily on coverage, with limited discussion of solution quality (plan length) and computational overhead

## Confidence

- **High**: CC WL algorithm improves runtime efficiency over GNNs while maintaining comparable coverage
- **Medium**: Ranking formulations outperform cost-to-go estimation for learning heuristic functions
- **Medium**: Graph learning is well-suited for numeric planning due to its ability to handle relational structures

## Next Checks

1. Test CC WL with varying numbers of continuous features (L parameter) on domains with increasing numeric complexity to identify the breaking point where hashing loses semantic information

2. Evaluate the learned heuristics on domains with partially observable states or noisy sensor data to assess robustness to real-world conditions

3. Compare solution quality (plan length) and search efficiency (node expansions) between graph-learned heuristics and classical heuristics on a subset of problems where both succeed