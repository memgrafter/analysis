---
ver: rpa2
title: Optimizing the Deployment of Tiny Transformers on Low-Power MCUs
arxiv_id: '2404.02945'
source_url: https://arxiv.org/abs/2404.02945
tags:
- mhsa
- memory
- attention
- data
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a comprehensive framework to enable and optimize
  the deployment of encoder Tiny Transformers on commercial MCUs. The framework provides
  an optimized library of kernels to maximize data reuse and avoid unnecessary data
  marshaling operations into the crucial attention block.
---

# Optimizing the Deployment of Tiny Transformers on Low-Power MCUs

## Quick Facts
- **arXiv ID**: 2404.02945
- **Source URL**: https://arxiv.org/abs/2404.02945
- **Reference count**: 40
- **Primary result**: 4.79x and 2.0x latency reduction vs SotA libraries CMSIS-NN and PULP-NN respectively

## Executive Summary
This work introduces a comprehensive framework for optimizing Tiny Transformer models on low-power MCUs, focusing on encoder architectures for edge applications. The framework combines a novel Fused-Weight Self-Attention (FWSA) optimization, Depth-First Tiling (DFT) for memory efficiency, and an optimized kernel library tailored for ARM Cortex-M and RISC-V architectures. Results demonstrate significant performance improvements over state-of-the-art libraries while reducing memory footprint, enabling transformer deployment on resource-constrained devices for biomedical sensing and gesture recognition tasks.

## Method Summary
The framework comprises three key innovations: FWSA fuses query and key weight matrices offline to reduce operations and parameters; DFT processes Multi-Head Self-Attention layers with aggressive tiling to minimize memory peak by avoiding full attention matrix materialization; and an optimized kernel library maximizes data reuse and eliminates unnecessary marshaling through fused operations. The approach targets three MCU platforms (STM32H7, STM32L4, GAP9) using ARM and RISC-V ISAs, with evaluation on three Tiny Transformer variants (EEGFormer, ECGFormer, TR-Former) for seizure detection, arrhythmia classification, and hand gesture recognition.

## Key Results
- 4.79x and 2.0x lower latency compared to CMSIS-NN (ARM) and PULP-NN (RISC-V) libraries respectively
- Memory peak reduced by up to 6.19x using Depth-First Tiling for MHSA
- Runtime reduced by 1.53x and parameter count decreased by 25% using Fused-Weight Attention
- Framework validated across three different MCU classes (STM32H7, STM32L4, GAP9)

## Why This Works (Mechanism)

### Mechanism 1: Fused-Weight Self-Attention
- **Claim**: FWSA reduces computational operations and parameter count by fusing query and key weight matrices offline
- **Mechanism**: Merges query and key projections into a single matrix multiplication by computing fused weight matrix $W^* = W_{query} \cdot W_{key}^T$ offline, reducing multiplications from three to two
- **Core assumption**: Parameter reduction doesn't significantly impact accuracy, maximized when embedding dimension $E$ is small relative to projection dimension $P$
- **Evidence anchors**: Abstract reports 1.53x runtime reduction and 25% parameter reduction; equations show fused weight computation; "The devil in linear transformer" discusses linear transformer challenges
- **Break condition**: Becomes less beneficial or detrimental when $E$ approaches or exceeds $P$, potentially increasing computational overhead

### Mechanism 2: Depth-First Tiling for MHSA
- **Claim**: DFT significantly reduces memory peak by tiling multiple layers together and avoiding full attention matrix materialization
- **Mechanism**: Tiles two GEMMs of MHSA stage and softmax operator together, processing depth-first to avoid storing full attention matrix $A$ (size $H \times S \times S$)
- **Core assumption**: Memory savings from avoiding full attention matrix outweigh computational overhead from complex tiling; MCU has sufficient memory for tiled intermediate results
- **Evidence anchors**: Abstract reports up to 6.19x memory peak reduction; section describes tiling approach avoiding full matrix materialization; "DeFiNES" discusses depth-first scheduling basis
- **Break condition**: Less effective for small sequence lengths $S$ where memory savings are minimal, or when MCU has ample memory making complexity unnecessary

### Mechanism 3: Optimized Kernel Library
- **Claim**: Optimized kernel library maximizes data reuse and minimizes data marshaling operations for significant performance improvements
- **Mechanism**: Uses optimized data layouts and loop reordering to maximize register file data reuse and minimize expensive memory accesses; fuses data marshaling operations into kernels themselves
- **Core assumption**: Optimized layouts effective on both ARM and RISC-V architectures; fused marshaling operations provide significant speedup over separate operations
- **Evidence anchors**: Abstract mentions optimized kernels maximizing data reuse and avoiding unnecessary marshaling; section describes library embedding re-quantization and 8-bit tensor processing; "PULP-NN" discusses RISC-V kernel optimizations
- **Break condition**: Less effective with large cache hierarchies that automatically optimize data reuse, or when model dimensions don't benefit from optimized layouts

## Foundational Learning

- **Concept**: Multi-Head Self-Attention (MHSA) and its computational complexity
  - **Why needed here**: MHSA is the core operation of transformer models and primary bottleneck for MCU deployment; the paper focuses on optimizing this operation
  - **Quick check question**: What are the four main parameters that determine tensor dimensions in MHSA, and how do they relate to computational complexity?

- **Concept**: Quantization and its impact on model accuracy and performance
  - **Why needed here**: Quantization reduces memory footprint and computational requirements for MCU deployment; understanding trade-offs is essential for evaluation
  - **Quick check question**: What are key differences between Post-Training Quantization (PTQ) and Quantization Aware Training (QAT), and when is each typically used?

- **Concept**: Memory hierarchy and data movement in MCUs
  - **Why needed here**: MCUs have limited memory and no hardware-managed caches, making data movement critical for performance; optimizations focus on minimizing data movement and maximizing reuse
  - **Quick check question**: What are main differences between software-managed and hardware-managed memory hierarchies, and how do these impact DNN kernel design for MCUs?

## Architecture Onboarding

- **Component map**: PyTorch Transformer model -> Optional FWSA transformation -> QuantLib (quantization and graph transformation) -> Deployment framework (DumpO for ARM, DORY for RISC-V) -> Optimized kernel library (MHSA and FWSA kernels) -> DFT scheme (optional, for memory-constrained scenarios)

- **Critical path**: Model quantization (QuantLib) -> Kernel selection (optimized library) -> Tiling strategy (DFT or LWT) -> Code generation (DumpO/DORY) -> MCU execution

- **Design tradeoffs**: FWSA vs. MHSA (parameter reduction vs. potential computational overhead); DFT vs. LWT (memory reduction vs. computational complexity); Quantization bit-width (memory/computation reduction vs. accuracy impact)

- **Failure signatures**: Excessive memory usage (model dimensions too large even with optimizations); poor performance (optimized kernels ineffective for specific dimensions or architecture); accuracy degradation (quantization or FWSA significantly impacts model accuracy)

- **First 3 experiments**:
  1. Benchmark kernel performance: Measure throughput of optimized MHSA and FWSA kernels on target MCU for various input dimensions, comparing against SotA libraries (PULP-NN and CMSIS-NN)
  2. Evaluate FWSA impact: Quantify reduction in parameters and operations for specific transformer model, measure impact on latency and accuracy
  3. Assess DFT memory savings: Measure memory peak achieved by DFT compared to LWT for MHSA operation, evaluate impact on overall latency

## Open Questions the Paper Calls Out

- **Open Question 1**: How does DFT perform on Transformer models with larger sequence lengths (>512 tokens) compared to evaluated models?
  - **Basis**: Paper evaluates DFT on models with sequence lengths up to 81 and 198 tokens, but doesn't explore performance on larger sequences common in other applications
  - **Why unresolved**: Only provides results for small sequence lengths typical of TinyML applications, leaving performance characterization for larger sequences unknown
  - **What evidence would resolve it**: Benchmarking DFT on Transformer models with sequence lengths >512 tokens across various MCU platforms, measuring memory footprint reduction and latency impact vs Layer-Wise Tiling

- **Open Question 2**: How does FWSA optimization perform on Transformer models with varying embedding dimensions relative to projection dimension?
  - **Basis**: Paper identifies FWSA beneficial when E < P, but only provides specific analysis for E < 52 without exploring full range of possible E and P relationships
  - **Why unresolved**: Only analyzes FWSA performance for specific small-scale models without characterizing scaling across different E/P ratios
  - **What evidence would resolve it**: Comprehensive benchmarking of FWSA across wide range of E/P ratios on various MCU platforms, measuring performance gains and parameter reduction

- **Open Question 3**: What is performance impact of proposed kernel library on decoder-only Transformer architectures (like GPT-style models)?
  - **Basis**: Paper focuses exclusively on encoder-only architectures for tasks like gesture recognition and seizure detection, not addressing decoder architectures
  - **Why unresolved**: Kernel optimizations designed specifically for encoder architecture structure, leaving applicability and performance benefits for decoder architectures unexplored
  - **What evidence would resolve it**: Implementing and benchmarking kernel library on decoder-only Transformer models for edge applications like on-device language models, comparing against existing solutions

## Limitations

- Accuracy impact of quantization and FWSA transformation not empirically measured on target datasets despite 25% parameter reduction and 1.53x speedup claims
- Memory peak claims lack baseline comparison and per-platform breakdown; 90MB memory budget claim appears inconsistent with STM32H7's 1MB SRAM specification
- Optimizations demonstrated only on three Tiny Transformer variants for biomedical/sensor tasks, limiting generalization to other transformer architectures and domains

## Confidence

- **High confidence**: Kernel-level optimizations for data reuse and marshaling avoidance are technically sound and consistent with established MCU optimization principles; FWSA mechanism (offline weight fusion) is mathematically valid
- **Medium confidence**: Quantitative claims (4.79x vs CMSIS-NN, 2.0x vs PULP-NN) based on reported methodology but lack sufficient detail for independent verification; memory peak reduction is plausible given algorithmic changes but needs empirical validation
- **Low confidence**: Claims about FWSA's effectiveness across different embedding/projection dimension ratios lack systematic ablation studies; accuracy impact of quantization + FWSA on each specific task not reported

## Next Checks

1. Measure top-1 accuracy of each Tiny Transformer model (EEGFormer, ECGFormer, TR-Former) after quantization and FWSA transformation on respective validation datasets, comparing against floating-point baselines

2. Instrument MHSA kernel to record actual memory allocation during execution on STM32H7, measuring peak usage with both DFT and LWT tiling to verify 6.19x reduction claim

3. Calculate exact percentage of parameters eliminated by FWSA for each model variant and verify 25% average reduction claim with reported model dimensions