---
ver: rpa2
title: 'SNP: Structured Neuron-level Pruning to Preserve Attention Scores'
arxiv_id: '2404.11630'
source_url: https://arxiv.org/abs/2404.11630
tags:
- pruning
- layers
- attention
- scores
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SNP introduces a graph-aware neuron-level pruning approach for\
  \ Vision Transformers that preserves attention scores by pruning query-key filter\
  \ pairs with the least correlation to informative components and removes inter-head\
  \ redundancy in value layers. Unlike conventional head pruning, SNP achieves significant\
  \ model compression (up to 80% parameter reduction) while maintaining accuracy and\
  \ accelerating inference on diverse hardware (3.85\xD7 on RTX3090, 4.93\xD7 on Jetson\
  \ Nano)."
---

# SNP: Structured Neuron-level Pruning to Preserve Attention Scores

## Quick Facts
- arXiv ID: 2404.11630
- Source URL: https://arxiv.org/abs/2404.11630
- Reference count: 40
- Primary result: Achieves up to 80% parameter reduction with minimal accuracy loss on Vision Transformers

## Executive Summary
SNP introduces a graph-aware neuron-level pruning approach for Vision Transformers that preserves attention scores by pruning query-key filter pairs with the least correlation to informative components and removes inter-head redundancy in value layers. Unlike conventional head pruning, SNP achieves significant model compression (up to 80% parameter reduction) while maintaining accuracy and accelerating inference on diverse hardware (3.85× on RTX3090, 4.93× on Jetson Nano). The method outperforms existing pruning techniques, achieving higher compression rates with minimal performance degradation, and demonstrates robustness across multiple Transformer architectures including DeiT and EfficientFormer.

## Method Summary
SNP is a structured neuron-level pruning method that operates on Multi-Head Self-Attention (MSA) modules in Vision Transformers. The approach uses Singular Value Decomposition (SVD) to identify and preserve the most informative components of attention scores while pruning query-key filter pairs with the least correlation to these components. Additionally, SNP removes inter-head redundancy in value layers by computing cosine similarity between all value filters across heads and pruning the most redundant ones. The method ensures consistent filter index removal across graphically connected layers, enabling hardware-agnostic acceleration without requiring sparse matrix libraries.

## Key Results
- Achieves up to 80% parameter reduction while maintaining competitive accuracy
- Provides 3.85× inference acceleration on RTX3090 and 4.93× on Jetson Nano
- Outperforms existing pruning techniques in both compression ratio and accuracy preservation
- Demonstrates robustness across multiple Transformer architectures (DeiT, EfficientFormer)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-aware neuron-level pruning preserves attention scores by removing query-key filter pairs with the least correlation to informative components of attention.
- Mechanism: Uses SVD to decompose attention scores and computes cosine similarity between each filter's attention scores and the top singular components. Filters with lowest similarity are pruned, preserving overall attention map structure.
- Core assumption: Attention scores can be decomposed into informative and uninformative components, and removing low-correlation components won't significantly affect model performance.
- Evidence anchors:
  - [abstract]: "prunes graphically connected query and key layers having the least informative attention scores while preserving the overall attention scores"
  - [section]: "We maintain the graphically connected query-key filter pair (Qh i and K h i ), constituting a filter-by-attention score (Qh i ·(K h i )T ∈ RN ×N ), that retains the most significant aspects of the overall attention scores"
  - [corpus]: Weak evidence - no direct citations, but related work exists on SVD-based pruning in transformers
- Break condition: If r (rank hyperparameter) is too low, important components of attention scores can be removed, leading to significant performance degradation

### Mechanism 2
- Claim: Inter-head redundancy removal prunes value layers to eliminate redundant information across different attention heads.
- Mechanism: Computes cosine similarity between all value filters across heads and removes filters with highest similarity (redundancy) first.
- Core assumption: Different attention heads contain redundant information that can be removed without significant performance loss.
- Evidence anchors:
  - [abstract]: "removes inter-head redundancy in value layers"
  - [section]: "we propose to measure the distance between all the value layers of MSA module, irrespective of the heads"
  - [corpus]: Weak evidence - general concept of attention head redundancy is known from literature but specific implementation details not well-supported
- Break condition: If pruning removes too many unique value filters across heads, model performance will degrade due to loss of diverse attention patterns

### Mechanism 3
- Claim: Consistent filter index removal across graphically connected layers enables hardware-agnostic acceleration without sparse matrix libraries.
- Mechanism: When pruning query and key layers, identical filter indices are removed across all layers connected by residual connections, maintaining computational graph compatibility.
- Core assumption: Residual connections require identical filter indices to be pruned across connected layers for proper operation.
- Evidence anchors:
  - [abstract]: "removing identical filter indices across all graphically connected layers, SNP can accelerate various Transformer models on various devices without additional libraries"
  - [section]: "all residual connections are linked by the last single residual connection 'add 24'. Consequently, the importance scores of all filter indices are summed up to represent the importance of each filter index"
  - [corpus]: Weak evidence - no direct citations, but general understanding of residual connections supports this
- Break condition: If inconsistent pruning occurs across residual-connected layers, model will fail to execute properly or lose significant performance

## Foundational Learning

- Concept: SVD (Singular Value Decomposition) for matrix decomposition
  - Why needed here: Used to identify informative components of attention scores for pruning decisions
  - Quick check question: How does SVD help identify which attention components are most informative?

- Concept: Cosine similarity for measuring vector correlation
  - Why needed here: Used to measure correlation between filter attention scores and informative components
  - Quick check question: Why is cosine similarity appropriate for measuring correlation between attention score matrices?

- Concept: Graph connectivity in neural networks
  - Why needed here: Understanding how layers are connected via residual connections affects pruning strategy
  - Quick check question: How do residual connections constrain which filters can be pruned independently?

## Architecture Onboarding

- Component map: Input tokens -> Patch embedding -> Linear projection -> MSA module (query-key-value computation with attention) -> Residual connection + FFN -> Classification head

- Critical path:
  1. Input tokens → Patch embedding → Linear projection
  2. MSA module (query-key-value computation with attention)
  3. Residual connection + FFN
  4. Repeat for multiple layers
  5. Classification head

- Design tradeoffs:
  - Parallel vs sequential head computation affects pruning strategy
  - Reshape operations enable parallelism but constrain pruning patterns
  - Hardware acceleration potential vs. accuracy preservation

- Failure signatures:
  - Inconsistent filter removal across residual-connected layers
  - Excessive pruning of informative attention components
  - Breaking parallel computation assumptions

- First 3 experiments:
  1. Apply SNP to single-head MSA module and verify attention preservation
  2. Test residual connection pruning with consistent vs inconsistent filter removal
  3. Measure acceleration on CPU vs GPU with different pruning ratios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SNP perform on Transformer models with different attention mechanisms, such as those using convolutional attention or other non-standard attention formulations?
- Basis in paper: [inferred] The paper primarily focuses on standard MSA modules and demonstrates robustness on EfficientFormer. The authors mention applying SNP to other vision tasks but don't test alternative attention mechanisms.
- Why unresolved: The current evaluation is limited to standard MSA architectures, and the paper doesn't explore how SNP's pruning criteria might need to be adapted for different attention formulations.
- What evidence would resolve it: Experiments showing SNP's effectiveness on various attention mechanisms like convolutional attention, sparse attention, or other novel attention formulations in Transformer architectures.

### Open Question 2
- Question: What is the optimal pruning ratio for each layer in SNP when considering different target hardware constraints and accuracy requirements?
- Basis in paper: [explicit] The paper provides layer-wise pruning ratios for DeiT models but mentions that global pruning isn't endorsed and that the optimal value may vary according to model domain task or trained datasets.
- Why unresolved: While the paper demonstrates effective pruning ratios for specific DeiT models, it doesn't provide a systematic method for determining optimal layer-wise pruning ratios based on hardware constraints or accuracy requirements.
- What evidence would resolve it: A comprehensive study mapping different pruning ratios to specific hardware platforms and accuracy targets, potentially with an adaptive pruning strategy that considers these factors.

### Open Question 3
- Question: How does SNP perform on Transformer models in domains outside of computer vision, such as natural language processing or audio processing?
- Basis in paper: [inferred] The paper focuses on Vision Transformers and mentions potential applications to other vision tasks but doesn't explore other domains where Transformers are used.
- Why unresolved: The paper's evaluation is limited to computer vision tasks, and the authors don't provide evidence of SNP's effectiveness on Transformers in other domains where different types of input data and attention patterns might exist.
- What evidence would resolve it: Experiments demonstrating SNP's performance on Transformer models for NLP tasks (like BERT, GPT) or audio processing tasks, showing whether the pruning criteria need to be adapted for different input modalities and attention patterns.

## Limitations
- Weak evidence supporting SVD-based attention preservation and cosine-similarity-based redundancy removal
- Unspecified implementation details including rank parameter r and complete layer-wise pruning ratios
- Claims about hardware-agnostic acceleration assume consistent filter removal across residual-connected layers

## Confidence
- **High Confidence**: Hardware acceleration results (3.85× on RTX3090, 4.93× on Jetson Nano) are empirically measured and reproducible
- **Medium Confidence**: The general pruning approach (removing low-correlation query-key pairs and redundant value filters) is sound, though specific implementation details need clarification
- **Low Confidence**: Claims about SVD-based importance scoring and the exact threshold for determining "informative components" lack sufficient supporting evidence

## Next Checks
1. **Layer-wise Pruning Verification**: Systematically vary the rank parameter r in the SVD decomposition across different layers and document the resulting accuracy-Compression tradeoff curves to identify optimal settings

2. **Residual Connection Consistency Test**: Implement both consistent and inconsistent pruning patterns across residual-connected layers and measure the impact on both model accuracy and inference speed to validate the importance of this constraint

3. **Cross-Architecture Generalization**: Apply SNP to additional Transformer architectures beyond DeiT and EfficientFormer (such as Swin Transformer or PVT) to verify the claimed robustness across diverse architectural designs