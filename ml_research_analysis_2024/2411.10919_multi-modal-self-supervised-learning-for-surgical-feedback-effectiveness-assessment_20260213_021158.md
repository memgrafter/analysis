---
ver: rpa2
title: Multi-Modal Self-Supervised Learning for Surgical Feedback Effectiveness Assessment
arxiv_id: '2411.10919'
source_url: https://arxiv.org/abs/2411.10919
tags:
- feedback
- surgical
- video
- trainee
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of predicting the effectiveness
  of real-time surgical feedback during training, specifically whether it leads to
  trainee behavior change. The authors propose a multi-modal approach that combines
  transcribed verbal feedback and corresponding surgical video to predict feedback
  effectiveness.
---

# Multi-Modal Self-Supervised Learning for Surgical Feedback Effectiveness Assessment

## Quick Facts
- arXiv ID: 2411.10919
- Source URL: https://arxiv.org/abs/2411.10919
- Authors: Arushi Gupta; Rafal Kocielnik; Jiayun Wang; Firdavs Nasriddinov; Cherine Yang; Elyssa Wong; Anima Anandkumar; Andrew Hung
- Reference count: 18
- One-line primary result: Multi-modal approach combining transcribed feedback and surgical video achieves AUROC of 0.70±0.02 for predicting feedback effectiveness, improving accuracy by up to 6.6% over text-only methods

## Executive Summary
This paper addresses the challenge of predicting whether real-time surgical feedback during training leads to trainee behavior change. The authors propose a multi-modal approach that combines transcribed verbal feedback with corresponding surgical video to predict feedback effectiveness. By employing self-supervised fine-tuning of VideoMAE on surgical video data, they enhance representation learning and achieve improved prediction performance. Their approach demonstrates that both transcribed feedback and surgical video are individually predictive of trainee behavior changes, with the combination of modalities yielding the best results.

## Method Summary
The method combines videoMAE (with self-supervised fine-tuning on surgical video data) and SBERT embeddings from transcribed feedback to predict feedback effectiveness. The approach extracts 10-second video clips centered on feedback timestamps, downsamples to 320x250 resolution, and samples 16 frames uniformly. Text features are extracted using SBERT from automated transcriptions, while video features are extracted using VideoMAE pre-trained on Kinetics-400 and fine-tuned with self-supervised learning on surgical video data. The multi-modal model combines video features (768-dim vector reduced to 256) and text features (384-dim vector reduced to 64) through concatenation and classification layers.

## Key Results
- Multi-modal approach achieves AUROC of 0.70±0.02 for predicting feedback effectiveness
- Task-relevant self-supervised fine-tuning achieves comparable accuracy with 14.8% of the data compared to domain-relevant fine-tuning
- Both transcribed feedback and surgical video are individually predictive of trainee behavior changes
- SSL fine-tuning enhances prediction performance, improving AUROC by 6.16-6.55% compared to no SSL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal learning improves feedback effectiveness prediction by combining semantic meaning from transcribed feedback and visual context from surgical video.
- Mechanism: The model fuses embeddings from Sentence Transformers (capturing semantic meaning of feedback) with visual features from VideoMAE (capturing surgical context and trainee behavior), allowing the model to leverage complementary information from both modalities.
- Core assumption: The semantic content of feedback and the corresponding visual surgical context contain complementary predictive signals for trainee behavior change.
- Evidence anchors:
  - [abstract] "both transcribed feedback and surgical video are individually predictive of trainee behavior changes, and their combination achieves an AUROC of 0.70±0.02"
  - [section 3.6] "Features extracted from VideoMAE were averaged across the temporal and spatial dimension resulting in a 768-dimensional vector. Two fully connected layers with ReLU activations were applied, reducing the dimensions to 512 and then 256. Features extracted from SBERT, initially a 384-dimensional vector, were also processed through two fully connected layers with ReLU activations, reducing their dimensions to 128 and then 64. The resulting vectors were concatenated into a 320-dimensional vector"

### Mechanism 2
- Claim: Self-supervised fine-tuning on surgical video data enhances representation learning for feedback effectiveness prediction.
- Mechanism: SSL fine-tuning allows the model to learn domain-specific visual features from unlabeled surgical video data before supervised fine-tuning on the specific task, improving the model's ability to extract relevant visual information.
- Core assumption: Surgical video contains domain-specific visual patterns that are relevant to understanding feedback effectiveness, and SSL can learn these patterns without labeled data.
- Evidence anchors:
  - [abstract] "we introduce self-supervised fine-tuning as a strategy for enhancing surgical video representation learning, which is scalable and further enhances prediction performance"
  - [section 4.3] "With task-relevant and domain-relevant SSL pre-training, the AUROC improved to 0.70, reflecting gains of 6.16% and 6.55%, respectively"

### Mechanism 3
- Claim: Task-relevant SSL fine-tuning achieves comparable performance to domain-relevant SSL fine-tuning with significantly less data.
- Mechanism: Task-relevant SSL fine-tuning focuses the model's learning on feedback-specific visual patterns, making it more data-efficient while still capturing the essential visual features needed for the prediction task.
- Core assumption: The visual patterns relevant to feedback effectiveness are primarily present in feedback-specific video segments, making task-relevant fine-tuning sufficient.
- Evidence anchors:
  - [section 4.3] "Both SSL fine-tuning approaches performed similarly, suggesting that using additional surgical video data did not yield significant benefits"
  - [abstract] "task-relevant SSL achieves comparable accuracy with 14.8% of the data"

## Foundational Learning

- Concept: Self-supervised learning
  - Why needed here: Labeled surgical feedback data is scarce and expensive to obtain, while unlabeled surgical video data is more abundant
  - Quick check question: What is the key difference between self-supervised learning and supervised learning in terms of data requirements?

- Concept: Multi-modal representation learning
  - Why needed here: Feedback effectiveness depends on both the semantic content of verbal feedback and the visual context of surgical actions
  - Quick check question: Why might combining text and video features provide better predictions than using either modality alone?

- Concept: Masked autoencoders for video
  - Why needed here: VideoMAE can learn meaningful video representations by reconstructing masked portions of video, which is particularly useful for surgical video analysis
  - Quick check question: How does masking portions of video input help the model learn more robust visual representations?

## Architecture Onboarding

- Component map: VideoMAE (video feature extraction) -> SBERT (text feature extraction) -> Multi-modal fusion layer (concatenation and classification)
- Critical path: ASR transcription -> SBERT embedding -> VideoMAE feature extraction -> Multi-modal fusion -> Behavior change prediction
- Design tradeoffs: Using frozen pre-trained models (SBERT, VideoMAE) for efficiency vs. fine-tuning them for potentially better performance; task-relevant vs. domain-relevant SSL fine-tuning for data efficiency
- Failure signatures: Poor performance on either modality alone suggests issues with feature extraction; similar performance to unimodal baselines suggests fusion layer issues
- First 3 experiments:
  1. Train text-only model with SBERT features to establish baseline performance
  2. Train video-only model with VideoMAE features to assess visual modality contribution
  3. Train multi-modal model without SSL fine-tuning to measure benefit of modality fusion alone

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the task-relevant self-supervised fine-tuning consistently outperform domain-relevant fine-tuning as the size of the training dataset increases?
- Basis in paper: [inferred] The paper shows that task-relevant and domain-relevant SSL fine-tuning perform comparably, with task-relevant using 14.8% of the data. The authors suggest this might be because non-feedback video clips are less likely to capture active surgical work that typically triggers feedback.
- Why unresolved: The current study only uses a fixed dataset size. The relative performance of the two fine-tuning strategies might change as more data becomes available.
- What evidence would resolve it: A controlled experiment comparing the two fine-tuning strategies across different dataset sizes, showing whether the performance gap widens or narrows with more data.

### Open Question 2
- Question: How does the performance of the multi-modal model change when incorporating additional structured visual information such as instrument kinematics or surgical gesture classification?
- Basis in paper: [explicit] The authors state that "incorporating visual features alongside auto-transcribed text features provides limited additional benefit" and suggest that "incorporating more structured visual information such as instrument kinematics and surgical gesture classification could further improve prediction performance."
- Why unresolved: The current study only uses raw video frames without additional structured visual features. The potential benefit of such features remains untested.
- What evidence would resolve it: An experiment comparing the current model with variants that incorporate instrument kinematics or surgical gesture classification features, showing whether these additions improve prediction accuracy.

### Open Question 3
- Question: Would a contrastive learning approach that explicitly models pre- and post-feedback video segments improve prediction accuracy compared to the current approach?
- Basis in paper: [explicit] The authors propose that "contrastive learning could be applied to distinguish video segments pre- and post-feedback by creating positive pairs (related to the same feedback) and negative pairs (unrelated segments)" as a future direction.
- Why unresolved: The current study uses VideoMAE with SSL fine-tuning but does not employ contrastive learning specifically designed to capture temporal changes around feedback delivery.
- What evidence would resolve it: A head-to-head comparison between the current model and a contrastive learning-based model trained to explicitly distinguish pre- and post-feedback segments, measuring prediction accuracy on the same task.

## Limitations
- The study relies on a specific dataset (JIGSAWS) with limited diversity in surgical procedures and trainee skill levels, which may limit generalizability to broader surgical training contexts
- The evaluation focuses solely on binary prediction of feedback effectiveness without deeper analysis of which feedback characteristics (tone, specificity, timing) are most predictive
- Self-supervised learning benefits are demonstrated through improved AUROC but lack ablation studies isolating the specific contributions of different SSL strategies

## Confidence
- High Confidence: Multi-modal approach outperforms unimodal baselines (AUROC 0.70 vs. 0.64 for text-only)
- Medium Confidence: SSL fine-tuning enhances representation learning, as evidenced by consistent performance improvements across both task-relevant and domain-relevant approaches
- Medium Confidence: Task-relevant SSL achieves comparable performance with 14.8% of the data, though the exact data reduction calculation requires verification

## Next Checks
1. Conduct ablation studies on SSL fine-tuning strategies to determine which specific components (masking ratio, reconstruction loss) contribute most to performance gains
2. Test model generalization on external surgical datasets with different procedural contexts and feedback delivery methods
3. Perform interpretability analysis to identify which visual and textual features most strongly predict behavior change, validating the model's decision-making process