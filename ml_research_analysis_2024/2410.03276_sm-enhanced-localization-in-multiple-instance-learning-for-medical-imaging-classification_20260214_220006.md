---
ver: rpa2
title: 'Sm: enhanced localization in Multiple Instance Learning for medical imaging
  classification'
arxiv_id: '2410.03276'
source_url: https://arxiv.org/abs/2410.03276
tags:
- attention
- instance
- instances
- learning
- camelyon16
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Sm, a novel mechanism for modeling local dependencies
  in Multiple Instance Learning (MIL) for medical imaging classification. The core
  idea is to encourage smoothness in attention values, which are used as proxies for
  instance labels, by minimizing the Dirichlet energy.
---

# Sm: enhanced localization in Multiple Instance Learning for medical imaging classification

## Quick Facts
- **arXiv ID**: 2410.03276
- **Source URL**: https://arxiv.org/abs/2410.03276
- **Reference count**: 40
- **Primary result**: Sm achieves state-of-the-art localization in MIL for medical imaging while maintaining competitive classification performance

## Executive Summary
This paper introduces Sm, a novel mechanism for modeling local dependencies in Multiple Instance Learning (MIL) for medical imaging classification. The method encourages smoothness in attention values used as proxies for instance labels by minimizing Dirichlet energy. Sm can be used alone or combined with transformers to account for global dependencies. Extensive experiments on three medical imaging datasets demonstrate that Sm achieves state-of-the-art localization performance while being competitive or superior in classification tasks.

## Method Summary
The proposed method applies a smooth operator Sm to instance embeddings or attention values in the MIL pipeline. Sm is derived from minimizing Dirichlet energy, which measures the variability of attention values across neighboring instances. The method can be applied at different stages (early, mid, or late) and can be combined with transformer encoders to capture both local and global dependencies. The smooth operator iteratively reduces the Dirichlet energy of attention values while maintaining fidelity to the original signal.

## Key Results
- Sm achieves state-of-the-art localization performance on RSNA, PANDA, and CAMELYON16 datasets
- The method ranks first in eight out of twelve dataset-score pairs when compared to eight state-of-the-art MIL methods
- Sm demonstrates improved instance-level performance while maintaining competitive bag-level classification results
- The approach is robust to different placement configurations within the MIL pipeline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The smooth operator reduces the Dirichlet energy of attention values, leading to more localized predictions.
- Mechanism: The smooth operator Sm is derived from minimizing a Dirichlet energy functional that measures the variability of attention values across neighboring instances in a bag. By iteratively applying Sm, the attention values become smoother, reducing their variability while maintaining fidelity to the original signal.
- Core assumption: Neighboring instances in medical imaging bags are likely to have the same label.
- Evidence anchors:
  - [abstract] "Motivated by a simple observation – that neighboring instances are likely to have the same label – we propose a novel, principled, and flexible mechanism to model local dependencies."
  - [section 4.1] "Our goal now turns into finding an operatorSm : RN ×D → RN ×D that, given a bag graph multivariate function U ∈ RN ×D, returns another bag graph multivariate function Sm (U) ∈ RN ×D such that its Dirichlet energy is lower without losing the information present in the original U."
  - [corpus] Weak evidence: The corpus contains related work on multiple instance learning but lacks direct discussion of Dirichlet energy minimization or smoothness operators.
- Break condition: If neighboring instances have dissimilar labels, applying Sm would smooth over important distinctions and hurt localization performance.

### Mechanism 2
- Claim: The smooth operator can be integrated at different stages of the MIL pipeline (early, mid, or late) with minimal impact on performance.
- Mechanism: Sm can be applied to instance embeddings (early), attention logits (mid), or attention values (late) before aggregation. The theoretical bound on Dirichlet energy decrease holds regardless of where Sm is applied, making the method robust to placement choices.
- Core assumption: The attention-based pooling mechanism can accommodate smoothed inputs at various stages without breaking the bag-level classification.
- Evidence anchors:
  - [section 4.3] "Based on the idea that smoothness can be imposed at previous locations, recall Eq. 4, we propose to also apply Sm to the transformer output... Notice that SmTAP uses Sm in two places: the first after the transformer encoder and the second in the aggregator."
  - [section 5.3.1] "We observe that the proposed method is robust to different placement configurations, which is consistent with the theoretical guarantees presented in Sec. 4.1. However, none of the variants consistently outperforms the others."
  - [corpus] Weak evidence: The corpus discusses multiple instance learning variants but doesn't specifically address operator placement flexibility or robustness.
- Break condition: If Sm is applied too early (before meaningful instance representations are learned), it may smooth out important discriminative features.

### Mechanism 3
- Claim: Combining Sm with transformer encoders provides complementary global and local modeling capabilities.
- Mechanism: SmTAP applies Sm both after the transformer encoder (to smooth the global interactions) and within the attention pooling (to smooth local dependencies). This dual application captures both long-range and spatial relationships among instances.
- Core assumption: Global dependencies learned by transformers and local dependencies modeled by Sm are complementary rather than redundant.
- Evidence anchors:
  - [abstract] "It can be used alone or combined with any mechanism to model global dependencies (e.g., transformers)."
  - [section 4.3] "SmTAP: Smooth Transformer Attention Pooling... The only difference with SmAP is that the neural network acting independently on the instance embeddings is replaced by a transformer encoder to account for global dependencies."
  - [section 5.1] "Notably, SmTAP significantly outperforms SETMIL, GTP, and CAMIL, which also model local dependencies. Contrary to our method, their design is focused on bag-level performance and it does not translate into meaningful instance-level properties."
  - [corpus] Weak evidence: The corpus contains related transformer-based MIL approaches but lacks direct comparison of dual smoothing strategies.

## Foundational Learning

- Concept: Dirichlet energy and graph Laplacians
  - Why needed here: The smooth operator is derived from minimizing Dirichlet energy, which requires understanding graph Laplacians and their spectral properties.
  - Quick check question: What is the relationship between the graph Laplacian eigenvalues and the rate at which Dirichlet energy decreases when applying the smooth operator?

- Concept: Attention-based pooling in MIL
  - Why needed here: The proposed method builds on attention-based pooling, modifying it to incorporate smoothness. Understanding the original mechanism is crucial.
  - Quick check question: How does attention-based pooling aggregate instance embeddings to produce bag-level predictions, and where do attention values come from?

- Concept: Transformer self-attention mechanism
  - Why needed here: The SmTAP variant uses transformers to model global dependencies, requiring understanding of how self-attention works in the MIL context.
  - Quick check question: How does the transformer encoder in MIL treat instances as tokens and learn global interactions through self-attention?

## Architecture Onboarding

- Component map:
  Feature extractor (ResNet/ViT) -> Instance embeddings
  MLP layers (with spectral normalization) -> Attention logits/values
  Smooth operator (Sm) -> Smoothed attention values/embeddings
  Attention pooling -> Bag embedding
  Classifier -> Bag prediction
  (Optional) Transformer encoder -> Global instance interactions

- Critical path:
  Feature extraction -> Instance encoding -> (Optional: Transformer) -> Smoothing -> Attention pooling -> Classification

- Design tradeoffs:
  - Early vs. late application of Sm: Early smoothing may lose discriminative features; late smoothing may not propagate smoothness effectively
  - Number of approximation steps T: More steps better approximate the exact solution but increase computation
  - α parameter: Higher α enforces more smoothness but may oversmooth; lower α preserves details but less smoothing

- Failure signatures:
  - Poor localization with good classification: Sm oversmoothing important instance-level details
  - Poor classification with good localization: Sm undersmoothing, not leveraging local consistency
  - No improvement over baseline: Incorrect placement of Sm or insufficient smoothing strength

- First 3 experiments:
  1. Implement SmAP-early on a small dataset (e.g., RSNA) and compare attention maps with ABMIL baseline
  2. Vary α parameter (0.1, 0.5, 0.9) on the same dataset to observe smoothness-localization tradeoff
  3. Add transformer encoder to create SmTAP and compare against SmAP to verify complementary benefits

## Open Questions the Paper Calls Out
None

## Limitations
- The core assumption that neighboring instances share labels may not hold in all medical imaging scenarios, particularly at tumor boundaries or heterogeneous tissue regions
- The theoretical claims about Dirichlet energy minimization lack rigorous mathematical proofs and direct empirical validation
- The method shows robustness to placement configurations but doesn't provide clear guidance on optimal selection for different dataset characteristics

## Confidence
**High confidence**: The overall classification performance improvements are well-supported by extensive experiments across three diverse medical imaging datasets. The SmTAP variant consistently ranking first in eight out of twelve dataset-score pairs provides strong empirical validation of the core approach.

**Medium confidence**: The localization improvements and instance-level performance gains are demonstrated but rely more heavily on the specific assumption about local label consistency. The ablation studies showing placement flexibility are supportive but don't establish optimal configurations.

**Low confidence**: The theoretical claims about Dirichlet energy minimization and the precise mechanism by which smoothness translates to better localization lack rigorous mathematical proofs and direct empirical validation.

## Next Checks
1. **Spectral analysis validation**: Conduct eigenvalue decomposition of the graph Laplacian for each dataset to empirically verify that attention values do indeed have lower Dirichlet energy after applying Sm, and correlate this reduction with localization performance.

2. **Boundary case evaluation**: Test Sm on datasets where neighboring instances may have dissimilar labels (e.g., tumor margins, heterogeneous tissue) to determine when the local consistency assumption breaks down and measure the resulting impact on both classification and localization.

3. **Alternative smoothness metrics**: Replace the Dirichlet energy minimization with other smoothness-inducing regularization terms (e.g., total variation, Laplacian regularization with different formulations) to assess whether the specific choice of smoothness operator is critical to the observed improvements.