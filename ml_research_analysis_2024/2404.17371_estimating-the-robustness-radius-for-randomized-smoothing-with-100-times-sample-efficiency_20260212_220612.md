---
ver: rpa2
title: Estimating the Robustness Radius for Randomized Smoothing with 100$\times$
  Sample Efficiency
arxiv_id: '2404.17371'
source_url: https://arxiv.org/abs/2404.17371
tags:
- robustness
- radius
- samples
- where
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of randomized
  smoothing (RS) in computing point-wise robustness certificates for deep neural networks
  (DNNs). The authors demonstrate that reducing the number of samples by one to two
  orders of magnitude can still enable the computation of a slightly smaller robustness
  radius (typically around 20% reduction) with the same confidence level.
---

# Estimating the Robustness Radius for Randomized Smoothing with 100$\times$ Sample Efficiency

## Quick Facts
- arXiv ID: 2404.17371
- Source URL: https://arxiv.org/abs/2404.17371
- Reference count: 35
- One-line primary result: Reduces randomized smoothing sample requirements by 100x with only ~20% radius reduction

## Executive Summary
This paper addresses the computational inefficiency of randomized smoothing (RS) in computing point-wise robustness certificates for deep neural networks. The authors demonstrate that reducing the number of samples by one to two orders of magnitude can still enable the computation of a slightly smaller robustness radius (typically around 20% reduction) with the same confidence level. The key method idea is based on applying the Central Limit Theorem (CLT) to approximate the binomial distribution used in Clopper-Pearson interval calculation, and integrating Shore's numerical approximation for the inverse cumulative function of the normal distribution.

## Method Summary
The paper develops a theoretical framework for estimating robustness radii with significantly fewer samples than traditional randomized smoothing approaches. The method uses CLT approximations to reduce computational requirements while maintaining reasonable accuracy in radius estimation. The approach provides an early stopping criterion for sample selection and has potential applications in AI safety for efficient robustness profiling of classifiers.

## Key Results
- Reducing samples from 10,000 to 1,000 results in only about 20% decrease in average robustness radius
- Reducing samples from 10,000 to 100 results in about 50% decrease in average robustness radius
- Theoretical bound shows average robustness radius drop is approximately 1 - Θ√(α/n)
- Extensive experiments on CIFAR-10 and ImageNet confirm theoretical findings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing the number of samples by one to two orders of magnitude still enables computation of a slightly smaller robustness radius (commonly ~20% radius reduction) with the same confidence level.
- Mechanism: The Central Limit Theorem (CLT) approximation to the binomial distribution allows for accurate estimation of the lower bound probability with fewer samples, and the inverse cumulative function of the normal distribution can be numerically approximated to maintain reasonable accuracy in radius estimation.
- Core assumption: The distribution of point-wise probabilities follows a pattern where most data points are far from the decision boundary, concentrating probability mass in a small interval near 1.
- Evidence anchors:
  - [abstract] "reducing the number of samples by one or two orders of magnitude can still enable the computation of a slightly smaller robustness radius (commonly ≈ 20% radius reduction) with the same confidence"
  - [section 4.2] "Theorem 4.3... Rα,n σ(pA) ≈ σΦ−1(pA − tα,n)... where tα,n = zα q pA(1−pA) n"
  - [corpus] Weak evidence - no direct corpus support found for this specific CLT-based mechanism
- Break condition: When the distribution of point-wise probabilities is uniform across all probabilities (not concentrated near 1), the approximation breaks down.

### Mechanism 2
- Claim: The average robustness radius drop is approximately 1 - Θ√(α/n), where Θ is between 1.64 and 2 depending on the distribution of point-wise probabilities.
- Mechanism: By integrating the relationship between sample size and robustness radius across the probability distribution of all data points, the average radius reduction can be characterized as a function of sample reduction.
- Core assumption: The probability distribution of point-wise probabilities follows a piecewise uniform distribution with most mass concentrated in a small interval near 1.
- Evidence anchors:
  - [section 4.3] "Theorem 4.5... rσ(α, n) := Rσ(α, n) Rσ(0, ∞) ≈ 1 − Θ zα√n... where Θ def:= ( 1.64 if β ∈ [0.8, 1) 2 if β = 0.5 )"
  - [section 4.5] "Analyzing the measurements across all datasets and models, the results well-support the theoretical bound characterized by Thm. 4.5"
  - [corpus] Weak evidence - no direct corpus support found for this specific distribution-based mechanism
- Break condition: When the distribution of point-wise probabilities is uniform across all probabilities (not concentrated near 1), the Θ value changes significantly.

### Mechanism 3
- Claim: The certified accuracy drop is bounded by zα/√n, providing a predictable relationship between sample size reduction and accuracy loss.
- Mechanism: The relationship between the number of samples and the confidence interval for probability estimation directly translates to bounds on certified accuracy drop.
- Core assumption: The probability distribution of point-wise probabilities is uniform in the interval [0.5, 1).
- Evidence anchors:
  - [section 4.4] "Theorem 4.6... ∆accR0(α, n) ≤ zα√n"
  - [section 5] "we observe that the distance between the curves (e.g., the robustness radius drop) is roughly constant, until a curve drops to zero, in accordance with Eq. (31)"
  - [corpus] Weak evidence - no direct corpus support found for this specific accuracy-bound mechanism
- Break condition: When the probability distribution of point-wise probabilities deviates significantly from uniformity in [0.5, 1), the bound becomes less tight.

## Foundational Learning

- Concept: Central Limit Theorem (CLT)
  - Why needed here: CLT provides the mathematical foundation for approximating the binomial distribution used in Clopper-Pearson interval calculation, enabling the reduction in sample requirements
  - Quick check question: What is the minimum sample size typically required for CLT to provide a reasonable approximation to a binomial distribution?

- Concept: Clopper-Pearson interval calculation
  - Why needed here: Understanding the exact method being approximated helps engineers grasp why the CLT approximation is valid and what precision is being traded
  - Quick check question: How does the Clopper-Pearson method calculate confidence intervals for binomial proportions without assuming normality?

- Concept: Normal cumulative distribution function (CDF) and its inverse
  - Why needed here: The robustness radius calculation depends on the inverse CDF of the normal distribution, and understanding this relationship is crucial for grasping how radius estimates change with sample size
  - Quick check question: How does the inverse normal CDF transform a probability value into a radius measurement in the context of randomized smoothing?

## Architecture Onboarding

- Component map: Input -> Noisy samples -> Classifier predictions -> Majority vote -> Confidence interval -> Robustness radius
- Critical path: Data point → Noisy samples → Classifier predictions → Majority vote → Confidence interval → Robustness radius
- Design tradeoffs:
  - Sample size vs. radius accuracy: Larger n provides more accurate radius but higher computational cost
  - CLT approximation vs. exact Clopper-Pearson: CLT enables faster computation but introduces small approximation error
  - Early stopping threshold: Must balance computational savings against acceptable radius reduction
- Failure signatures:
  - Unexpectedly large radius reduction: May indicate non-concentrated probability distribution or invalid CLT assumptions
  - Inconsistent radius estimates: Could suggest insufficient samples or classifier instability
  - Excessive computational cost: May indicate suboptimal sample size selection or missing early stopping criteria
- First 3 experiments:
  1. Measure radius reduction when reducing samples from 10,000 to 1,000 on CIFAR-10 with α=0.001
  2. Test early stopping criterion by monitoring radius improvement as samples increase beyond 1,000
  3. Verify the independence of radius reduction from noise level σ by testing multiple σ values with the same sample reduction

## Open Questions the Paper Calls Out
- How does the relationship between sample size and robustness radius reduction vary for different types of noise distributions beyond Gaussian (e.g., uniform, Laplacian)?
- Can the early stopping criterion for sample selection be optimized further to balance computational efficiency and robustness certificate quality?
- How does the reduction in sample size affect the robustness certification of models against other types of adversarial attacks, such as those targeting semantic or physical world vulnerabilities?

## Limitations
- Theoretical framework relies on asymptotic approximations that may break down for extremely small sample sizes (< 100)
- Results are primarily validated on CIFAR-10 and ImageNet, with limited testing on other domains or data distributions
- Analysis assumes Gaussian noise, which may not capture all real-world perturbation scenarios

## Confidence
- Mechanism 1 (CLT-based approximation): **High** - Well-established mathematical foundation with extensive empirical validation
- Mechanism 2 (average radius characterization): **Medium** - Theoretical derivation is sound but relies on distributional assumptions that need broader validation
- Mechanism 3 (certified accuracy bounds): **Medium** - Theoretical bound is derived but empirical validation shows some deviation from predictions

## Next Checks
1. **Distribution sensitivity test**: Evaluate radius reduction performance across different probability distributions (uniform, bimodal, skewed) to verify the robustness of the Θ approximation

2. **Cross-domain generalization**: Apply the methodology to non-vision datasets (e.g., speech, text) to test whether the 20% radius reduction threshold holds across domains

3. **Extreme sample size validation**: Test the methodology with sample sizes below 50 to identify the practical lower bound where the CLT approximation becomes unreliable