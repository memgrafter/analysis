---
ver: rpa2
title: Focal Modulation Networks for Interpretable Sound Classification
arxiv_id: '2402.02754'
source_url: https://arxiv.org/abs/2402.02754
tags:
- interpretability
- audio
- focal
- interpretation
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies focal modulation networks (FocalNets) to environmental
  sound classification for the first time, evaluating their interpretability properties
  on the ESC-50 dataset. FocalNets, which are interpretable by-design, outperform
  a similarly sized vision transformer in both accuracy and interpretability metrics
  (fidelity-to-input and faithfulness).
---

# Focal Modulation Networks for Interpretable Sound Classification

## Quick Facts
- arXiv ID: 2402.02754
- Source URL: https://arxiv.org/abs/2402.02754
- Authors: Luca Della Libera; Cem Subakan; Mirco Ravanelli
- Reference count: 0
- Primary result: FocalNets achieve 77.4% accuracy and 0.305 fidelity-to-input on ESC-50 test set, outperforming similarly sized ViT in both accuracy and interpretability metrics.

## Executive Summary
This paper applies focal modulation networks (FocalNets) to environmental sound classification for the first time, evaluating their interpretability properties on the ESC-50 dataset. FocalNets, which are interpretable by-design, outperform a similarly sized vision transformer in both accuracy and interpretability metrics (fidelity-to-input and faithfulness). The proposed method also shows competitive performance against PIQ, a state-of-the-art post-hoc interpretation method specifically designed for audio. The authors introduce a simple strategy to generate interpretations from FocalNet classifiers, which can be extended to transformers.

## Method Summary
The paper uses FocalNets, attention-free networks designed to capture contextual information through hierarchical depth-wise convolutions and modulation gating. The models are trained on log-spectrogram representations of audio signals from the ESC-50 dataset, using the Adam optimizer with a cyclical learning rate schedule, gradient clipping, and data augmentation techniques. Interpretations are generated using the modulation map's L2 norm across channels, thresholded at a chosen quantile to produce interpretable masks highlighting critical spectrogram regions.

## Key Results
- FocalNets achieve 77.4% accuracy and 0.305 fidelity-to-input on ESC-50 test set
- Outperform similarly sized ViT in both accuracy and interpretability metrics
- Show competitive performance against PIQ, a state-of-the-art post-hoc interpretation method for audio

## Why This Works (Mechanism)

### Mechanism 1
FocalNets achieve interpretability by design through focal modulation that aggregates global context first, then applies spatially-aware gating, avoiding spurious correlations that plague self-attention. The modulator computes a context vector via hierarchical depth-wise convolutions, then gates each token's output via element-wise multiplication with this global modulator. This ensures that feature activations depend on the input's global context rather than local attention artifacts.

### Mechanism 2
The modulation map's L2 norm across channels acts as a saliency measure, naturally highlighting input regions critical for the classifier's decision without additional supervision. By thresholding the modulation map at a chosen quantile, the method retains only the most influential spectrogram regions, producing an interpretable mask that aligns with the classifier's focus.

### Mechanism 3
Pretraining on ImageNet and using log-spectrogram stacking allows FocalNets to transfer visual interpretability mechanisms to the audio domain without architectural changes. The 3-channel log-spectrogram mimics an RGB image; FocalNet's learned visual features adapt to audio patterns, preserving interpretability while achieving competitive accuracy.

## Foundational Learning

- **Hierarchical depth-wise convolutions for context aggregation**: Captures long-to-short range dependencies in audio spectrograms, forming the basis for meaningful modulation. Quick check: What is the kernel size progression in the focal levels, and why does it matter for capturing context?

- **Quantile-based thresholding for interpretation masks**: Balances fidelity-to-input and faithfulness by controlling how much of the spectrogram is highlighted. Quick check: How does increasing the quantile order affect the trade-off between fidelity-to-input and faithfulness?

- **Log-spectrogram as image-like input**: Enables reuse of pretrained visual models without architectural modification, critical for transfer learning. Quick check: Why stack three identical log-spectrogram channels instead of using a single channel?

## Architecture Onboarding

- **Component map**: Log-spectrogram → FocalNet backbone → modulation map → interpretation mask → fidelity/faithfulness evaluation
- **Critical path**: Log-spectrogram → FocalNet backbone → modulation map → interpretation mask → fidelity/faithfulness evaluation
- **Design tradeoffs**: Using pretrained ImageNet weights trades domain specificity for faster convergence and interpretability benefits. Quantile-based thresholding is simple but requires tuning per model and dataset. Log-spectrogram stacking enables transfer but may lose fine-grained audio structure.
- **Failure signatures**: Low fidelity-to-input despite high accuracy (modulation map not capturing true signal importance). High faithfulness but low fidelity-to-input (mask too focused, removing necessary context). Poor accuracy (visual pretraining not aligning with audio semantics).
- **First 3 experiments**: 1) Train FocalNet and ViT on ESC-50; compare modulation vs attention maps for interpretability. 2) Sweep quantile orders (0.5–0.95) and plot fidelity-to-input vs faithfulness trade-off. 3) Replace log-spectrogram stacking with a learned 1D→2D projection; compare interpretability metrics.

## Open Questions the Paper Calls Out
- How well do FocalNets perform on audio datasets beyond ESC-50?
- How do subjective user evaluations compare to the quantitative metrics used in the paper?
- Are FocalNet interpretations more robust to input perturbations compared to PIQ?
- How does FocalNet's performance scale with larger audio inputs or longer durations?
- What architectural modifications to FocalNets could further improve audio interpretability?

## Limitations
- Lack of detailed architectural specifications for the FocalNet and ViT models used
- Improvement over PIQ is described as "competitive" rather than definitively superior
- Cross-modal transfer from visual pretraining to audio requires further validation

## Confidence
- **High Confidence**: Basic methodology of using log-spectrogram preprocessing and FocalNet architecture for audio classification is reproducible
- **Medium Confidence**: Interpretability claims and comparisons with ViT and PIQ are supported by quantitative metrics
- **Low Confidence**: Assumption that visual pretraining transfers effectively to audio interpretability requires more rigorous validation

## Next Checks
1. Obtain and verify the exact FocalNet and ViT architectures used through direct correspondence with authors or implementation from open-source FocalNet repositories
2. Conduct ablation studies comparing FocalNets trained from scratch on audio spectrograms versus those using visual pretraining
3. Perform controlled experiments varying input signal-to-noise ratios and audio durations to test focal modulation's robustness under degraded conditions