---
ver: rpa2
title: Calibrating Long-form Generations from Large Language Models
arxiv_id: '2402.06544'
source_url: https://arxiv.org/abs/2402.06544
tags:
- answer
- confidence
- calibration
- score
- correctness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel calibration framework for large language
  models that treats both answer correctness and confidence as distributions, addressing
  the limitations of traditional binary evaluation methods for long-form generation
  tasks. The framework includes three calibration metrics measuring correlation, Wasserstein
  similarity, and selective F1, along with two confidence elicitation methods based
  on self-consistency and self-evaluation.
---

# Calibrating Long-form Generations from Large Language Models

## Quick Facts
- **arXiv ID**: 2402.06544
- **Source URL**: https://arxiv.org/abs/2402.06544
- **Reference count**: 40
- **One-line primary result**: Larger models don't necessarily achieve better calibration, and calibration performance is metric-dependent

## Executive Summary
This paper introduces a novel calibration framework for large language models that treats both answer correctness and confidence as distributions, addressing the limitations of traditional binary evaluation methods for long-form generation tasks. The framework includes three calibration metrics measuring correlation, Wasserstein similarity, and selective F1, along with two confidence elicitation methods based on self-consistency and self-evaluation. Experiments across multiple datasets show that larger models do not necessarily achieve better calibration, that calibration performance is metric-dependent, and that self-consistency methods excel in factoid datasets. The study also demonstrates practical calibration enhancement strategies including fine-tuning, temperature scaling, and document integration, culminating in a cost-effective cascading strategy that optimizes model selection within API budget constraints.

## Method Summary
The framework treats correctness and confidence as distributions over [0,1] rather than binary values, enabling nuanced evaluation of long-form generation. It employs two confidence elicitation methods: self-evaluation (where models assess their own answers) and self-consistency (where multiple sampled responses are compared for agreement). Three calibration metrics are introduced: correlation between confidence and correctness distributions, Wasserstein distance between the distributions, and selective F1 measuring the model's ability to identify when it should answer. The framework is tested across multiple datasets (ASQA, ELI5, QAMPARI, CNNDM) using various LLM sizes and enhancement strategies like temperature scaling and document integration.

## Key Results
- Larger models don't necessarily achieve better calibration, with non-monotonic relationships observed across different datasets
- Calibration performance is metric-dependent, with different models excelling on different metrics
- Self-consistency methods significantly outperform self-evaluation on factoid datasets but show mixed results on open-ended tasks
- Temperature scaling and document integration serve as effective calibration enhancement strategies
- A cascading model selection strategy optimizes cost-effectiveness within API budget constraints

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The framework treats both correctness and confidence as distributions rather than binary values, allowing for better representation of subjective or partial correctness in long-form generation.
- **Mechanism:** By modeling correctness as a distribution over [0,1] and eliciting confidence as a similar distribution, the framework can capture the nuanced quality of long-form answers that may be partially correct.
- **Core assumption:** Long-form answers inherently have degrees of correctness rather than being strictly right or wrong.
- **Evidence anchors:**
  - [abstract]: "both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores"
  - [section]: "we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores"
  - [corpus]: Weak - the corpus doesn't provide direct evidence for this mechanism, though related papers on calibration suggest this is a novel approach
- **Break condition:** This mechanism breaks down when tasks have clear objective ground truth with no subjectivity.

### Mechanism 2
- **Claim:** Self-consistency methods work better than self-evaluation on factoid datasets because they measure agreement across multiple sampled responses.
- **Mechanism:** For factoid questions with specific answers, multiple sampled responses from the model will tend to converge on the correct information, and the consistency between these responses serves as a proxy for confidence.
- **Core assumption:** Factoid answers have definitive correct information that models will consistently produce when sampled multiple times.
- **Evidence anchors:**
  - [abstract]: "self-consistency methods excel in factoid datasets"
  - [section]: "In factoid datasets (ASQA and QAMPARI), self-consistency confidence elicitation outperforms self-evaluation for all models"
  - [corpus]: Weak - corpus doesn't directly support this mechanism, though it mentions related calibration work
- **Break condition:** This mechanism breaks down on open-ended or subjective tasks where multiple valid answers exist.

### Mechanism 3
- **Claim:** Larger models don't necessarily guarantee better calibration, and calibration is metric-dependent.
- **Mechanism:** The relationship between model size and calibration quality is non-monotonic because larger models may have different biases (like overconfidence from RL fine-tuning) and different calibration behaviors across different metrics.
- **Core assumption:** Model size doesn't directly correlate with calibration quality due to various confounding factors.
- **Evidence anchors:**
  - [abstract]: "larger models don't necessarily guarantee better calibration, that calibration performance is found to be metric-dependent"
  - [section]: "Larger models are not necessarily better calibrated. Table 1 reveals a surprising trend: Vicuna-13b, though performing poorly on ASQA, ELI5 and QAMPARI, has the highest correlation with self-consistency confidence on the same datasets"
  - [corpus]: Weak - corpus mentions calibration work but doesn't directly address model size effects
- **Break condition:** This mechanism breaks down when comparing models of vastly different scales or when using metrics that correlate with model capability.

## Foundational Learning

- **Probability distributions and their properties**
  - Why needed here: The framework relies on treating correctness and confidence as probability distributions over [0,1], requiring understanding of how distributions work
  - Quick check question: If a model gives confidence scores of 0.8, 0.9, and 0.7 for an answer, what is the mean confidence and what does it represent?

- **Calibration and evaluation metrics**
  - Why needed here: The framework introduces three specific calibration metrics (correlation, Wasserstein similarity, selective F1) that require understanding of what calibration means
  - Quick check question: What's the difference between measuring correlation between confidence and correctness versus measuring Wasserstein distance between their distributions?

- **Self-consistency and self-evaluation techniques**
  - Why needed here: The framework uses two methods to elicit confidence distributions, requiring understanding of how these techniques work
  - Quick check question: How does asking a model to evaluate its own answer 10 times differ from asking it to generate 10 different answers to the same question?

## Architecture Onboarding

- **Component map:** Question → Generation → Correctness Evaluation + Confidence Elicitation → Calibration Measurement
- **Critical path:** Question → Generation → Correctness Evaluation + Confidence Elicitation → Calibration Measurement
- **Design tradeoffs:**
  - Using GPT-4 for evaluation provides better correlation with human judgment but adds cost and latency
  - Self-consistency requires multiple generations, increasing computational cost but potentially providing better confidence estimates
  - Distribution-based approach captures nuance but requires more complex metrics than binary evaluation
- **Failure signatures:**
  - Low correlation between confidence and correctness distributions indicates poor calibration
  - High Wasserstein distance suggests the confidence distribution doesn't match the shape of the correctness distribution
  - Low selective F1 means the model isn't good at identifying when it should answer
- **First 3 experiments:**
  1. Run the full pipeline on a small dataset (like 10 examples) to verify all components work together
  2. Compare calibration scores using self-evaluation vs self-consistency on a factoid dataset to observe the claimed difference
  3. Test temperature scaling effects by running the same question with different temperatures and observing calibration changes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the calibration framework perform when applied to other types of long-form generation tasks, such as creative writing or code generation?
- **Basis in paper:** [inferred]
- **Why unresolved:** The paper focuses on long-form QA and summarization tasks, but does not explore the framework's effectiveness in other domains.
- **What evidence would resolve it:** Conduct experiments using the calibration framework on datasets from different domains, such as creative writing prompts or code generation tasks, and compare the results to those presented in the paper.

### Open Question 2
- **Question:** How do the calibration metrics (Correlation, Wasserstein Similarity, and Selective F1) compare in terms of their effectiveness at measuring calibration for different types of long-form generation tasks?
- **Basis in paper:** [explicit]
- **Why unresolved:** The paper demonstrates that calibration performance is metric-dependent, but does not provide a comprehensive analysis of how each metric performs across various tasks.
- **What evidence would resolve it:** Perform a detailed analysis of the three calibration metrics on multiple datasets, examining their strengths and weaknesses for different types of long-form generation tasks.

### Open Question 3
- **Question:** How does the calibration framework handle ambiguous or subjective questions, where the ground truth correctness distribution may be highly variable?
- **Basis in paper:** [inferred]
- **Why unresolved:** The paper does not explicitly address how the framework deals with questions that have multiple valid interpretations or subjective criteria for correctness.
- **What evidence would resolve it:** Test the calibration framework on datasets containing ambiguous or subjective questions, and analyze how the framework's performance is affected by the variability in ground truth correctness distributions.

## Limitations

- The relationship between model size and calibration quality is complex and non-monotonic, requiring further validation across diverse model architectures
- The metric-dependent nature of calibration performance suggests no universal approach works across all tasks
- The cascading strategy for cost-effective model selection needs validation in real-world production environments

## Confidence

- **High confidence**: The framework's ability to treat correctness and confidence as distributions represents a meaningful advancement over binary evaluation methods. The experimental evidence across multiple datasets and model sizes supports this core contribution.
- **Medium confidence**: The specific superiority of self-consistency methods on factoid datasets is well-supported, but the generalizability to other task types requires more extensive testing. The cascading strategy for cost-effective model selection shows promise but needs validation in production environments.
- **Medium confidence**: The effectiveness of temperature scaling and document integration as calibration enhancement strategies is demonstrated, but optimal parameter tuning across different contexts remains an open question.

## Next Checks

1. **Cross-dataset generalization test**: Apply the calibration framework to a new long-form generation dataset not used in the original experiments to verify the metric-dependent findings hold across diverse domains.
2. **Production deployment simulation**: Implement the cascading strategy in a simulated API budget-constrained environment with real-time evaluation to measure practical utility and cost-effectiveness.
3. **Alternative confidence elicitation comparison**: Compare the self-consistency and self-evaluation methods against other confidence estimation techniques (such as uncertainty quantification methods) on both factoid and open-ended datasets to better understand when each approach excels.