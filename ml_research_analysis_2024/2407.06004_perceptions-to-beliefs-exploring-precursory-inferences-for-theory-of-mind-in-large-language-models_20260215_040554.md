---
ver: rpa2
title: 'Perceptions to Beliefs: Exploring Precursory Inferences for Theory of Mind
  in Large Language Models'
arxiv_id: '2407.06004'
source_url: https://arxiv.org/abs/2407.06004
tags:
- inference
- perception
- llms
- belief
- ella
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores large language models'' (LLMs) capabilities
  for theory of mind (ToM) reasoning by examining two precursory inference abilities:
  perception inference and perception-to-belief inference. The authors create new
  perception-augmented ToM benchmarks (Percept-ToMi and Percept-FANToM) by annotating
  character perceptions on existing ToM datasets.'
---

# Perceptions to Beliefs: Exploring Precursory Inferences for Theory of Mind in Large Language Models

## Quick Facts
- arXiv ID: 2407.06004
- Source URL: https://arxiv.org/abs/2407.06004
- Authors: Chani Jung; Dongkwan Kim; Jiho Jin; Jiseon Kim; Yeon Seonwoo; Yejin Choi; Alice Oh; Hyunwoo Kim
- Reference count: 19
- Key outcome: LLMs show strong perception inference but struggle with perception-to-belief inference due to weak inhibitory control, which can be improved using the PercepToM framework.

## Executive Summary
This paper investigates large language models' theory of mind (ToM) reasoning capabilities by examining two precursory inference abilities: perception inference and perception-to-belief inference. The authors create perception-augmented ToM benchmarks (Percept-ToMi and Percept-FANToM) and evaluate eight state-of-the-art LLMs on these tasks. They find that while models perform well on perception inference, they struggle with perception-to-belief inference due to their inability to suppress irrelevant information. Based on these findings, the authors propose PercepToM, a framework that leverages LLMs' strong perception inference to improve their ToM reasoning by isolating relevant context. Experiments show that PercepToM significantly improves LLM performance, especially in false belief scenarios.

## Method Summary
The authors create perception-augmented ToM benchmarks by annotating character perceptions on existing ToMi and FANToM datasets. They evaluate eight state-of-the-art LLMs on two tasks: perception inference (identifying which characters perceived each information unit) and perception-to-belief inference (inferring beliefs from perceptions with ground truth perception information provided). The PercepToM framework is then proposed, which uses LLMs' perception inference capability to extract perspective context (isolating information perceived by the target character) before answering ToM questions. The framework is evaluated on both the original perception-augmented benchmarks and a PercepToM+Oracle variant where ground truth perception information is provided.

## Key Results
- LLMs achieve high accuracy on perception inference tasks but struggle with perception-to-belief inference, especially lacking inhibitory control to suppress irrelevant information.
- PercepToM significantly improves LLM performance on both ToMi and FANToM datasets, with GPT-4 Turbo achieving perfect scores on false belief scenarios in ToMi.
- The correlation between perception-to-belief inference and ToM performance suggests LLMs aren't fully leveraging provided perception information.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs struggle with perception-to-belief inference due to weak inhibitory control.
- Mechanism: When provided with complete context and ground truth perception information, LLMs still underperform because they cannot effectively suppress irrelevant information (non-target character perceptions and non-perceived context).
- Core assumption: The inclusion of irrelevant information significantly impairs LLMs' ability to infer beliefs from perceptions.
- Evidence anchors:
  - [abstract] "exhibiting limited capability in perception-to-belief inference (e.g., lack of inhibitory control)"
  - [section 5.4] "we can see LLMs struggle to effectively suppress irrelevant information"
- Break condition: If LLMs develop better attention mechanisms that can selectively focus on relevant information, this mechanism would fail.

### Mechanism 2
- Claim: PercepToM improves ToM performance by leveraging LLMs' strong perception inference to isolate relevant context.
- Mechanism: By extracting only the context perceived by the target character, PercepToM transforms false belief scenarios into true belief scenarios where information symmetry exists, allowing LLMs to answer correctly.
- Core assumption: LLMs perform better when information symmetry exists between the model and the target character.
- Evidence anchors:
  - [abstract] "PercepToM significantly improves LLM performance, especially in false belief scenarios"
  - [section 3] "When given this isolated context along with the ToM question, the scenario becomes a simple true belief scenario"
- Break condition: If LLMs develop better reasoning capabilities that don't require information symmetry, this mechanism would fail.

### Mechanism 3
- Claim: The correlation between perception-to-belief inference and ToM performance suggests LLMs aren't fully leveraging provided perception information.
- Mechanism: Despite being given ground truth perception information, LLMs show similar performance in perception-to-belief inference and direct ToM tasks, indicating they're not effectively using the hints.
- Core assumption: LLMs should show significant improvement when given ground truth perception information but don't, suggesting suboptimal utilization.
- Evidence anchors:
  - [section 5.2] "LLMs hardly benefit from the additional character perception information, which should serve as significant hints"
  - [section 5.2] "we can see that they are not fully leveraging the ground truth perception information"
- Break condition: If LLMs are trained to better utilize explicit perception information, this mechanism would fail.

## Foundational Learning

- Concept: Theory of Mind (ToM)
  - Why needed here: Understanding ToM is crucial for grasping why perception inference and perception-to-belief inference are important precursors
  - Quick check question: What is the difference between true belief and false belief scenarios in ToM?

- Concept: Inhibitory control
  - Why needed here: Understanding inhibitory control explains why LLMs struggle with perception-to-belief inference despite having ground truth perception information
  - Quick check question: How does inhibitory control relate to the ability to suppress irrelevant information in cognitive tasks?

- Concept: Information symmetry
  - Why needed here: Understanding information symmetry is key to grasping why PercepToM transforms false belief scenarios into true belief scenarios
  - Quick check question: Why would information symmetry between a model and target character improve ToM reasoning performance?

## Architecture Onboarding

- Component map: Context with character perceptions → Perception Inference → Perspective Context Extraction → Response Generation → ToM answer

- Critical path: Perception Inference → Perspective Context Extraction → Response Generation

- Design tradeoffs:
  - Simple string-matching vs. complex belief state tracking (SymbolicToM)
  - Reliance on LLM's perception inference accuracy vs. manual annotation
  - Generalizability across input formats vs. tailored solutions

- Failure signatures:
  - Low perception inference accuracy leading to incorrect context isolation
  - LLM failing to utilize the isolated context effectively
  - String-matching algorithm incorrectly extracting context

- First 3 experiments:
  1. Test perception inference accuracy on Percept-ToMi and Percept-FANToM datasets
  2. Compare ToM performance with and without PercepToM framework
  3. Analyze the impact of irrelevant information by comparing perception-to-belief inference with PercepToM+Oracle

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs' ToM capabilities evolve across their training phases (pre-training, instruction tuning, and preference tuning)?
- Basis in paper: [inferred] The paper mentions that analyzing precursory inferences across training phases would help understand when social reasoning abilities emerge in LLMs.
- Why unresolved: The study focuses on state-of-the-art LLMs without examining their developmental trajectory through different training stages.
- What evidence would resolve it: Experiments tracking ToM performance metrics across different training phases of the same model architecture.

### Open Question 2
- Question: Do LLMs exhibit different performance patterns on first-order versus second-order ToM questions?
- Basis in paper: [explicit] The paper mentions analyzing differences between true/false belief scenarios but does not explore different orders of ToM questions.
- Why unresolved: The study focuses on true/false belief scenarios without differentiating between orders of ToM questions.
- What evidence would resolve it: Comparative analysis of LLM performance on first-order and second-order ToM questions using the same models and datasets.

### Open Question 3
- Question: How would multimodal ToM evaluation (incorporating visual stimuli) compare to text-only ToM benchmarks for LLMs?
- Basis in paper: [inferred] The paper discusses limitations of text-only evaluation and mentions considering visual ToM and multimodal evaluations.
- Why unresolved: The study exclusively uses text-based ToM datasets, leaving multimodal capabilities unexplored.
- What evidence would resolve it: Experiments comparing LLM performance on visual ToM tasks versus text-only ToM tasks using the same models.

## Limitations
- The paper doesn't directly measure whether LLMs are actually utilizing provided ground truth perception information versus relying on their own perception inference capabilities.
- The effectiveness of PercepToM depends heavily on the accuracy of the LLM's perception inference, but the paper doesn't thoroughly analyze failure modes where incorrect perception inference leads to wrong context isolation.
- The study uses text-only ToM datasets, leaving multimodal ToM capabilities unexplored.

## Confidence

- **High confidence**: Claims about LLMs' general performance on perception inference are well-supported by direct experimental results across multiple models and datasets.
- **Medium confidence**: The claim that PercepToM significantly improves false belief scenario performance is supported by results but depends on the assumption that information symmetry is the primary bottleneck.
- **Medium confidence**: The correlation between perception-to-belief inference and ToM performance suggesting suboptimal utilization of provided perception information is plausible but not definitively proven.

## Next Checks

1. Conduct an ablation study comparing LLM performance with and without the ground truth perception information to directly measure whether models are actually utilizing the provided hints versus relying on their own perception inference capabilities.

2. Analyze LLM attention patterns or internal representations when processing contexts with ground truth perception information versus without, to determine if models are actually "looking at" the perception information during reasoning.

3. Test PercepToM's robustness by intentionally introducing perception inference errors and measuring how these errors propagate through the context isolation step to impact final ToM performance, establishing the framework's error tolerance boundaries.