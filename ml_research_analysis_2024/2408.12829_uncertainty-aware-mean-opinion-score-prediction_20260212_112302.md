---
ver: rpa2
title: Uncertainty-Aware Mean Opinion Score Prediction
arxiv_id: '2408.12829'
source_url: https://arxiv.org/abs/2408.12829
tags:
- uncertainty
- prediction
- epistemic
- speech
- aleatoric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an uncertainty-aware MOS prediction system
  that models aleatoric and epistemic uncertainty using heteroscedastic regression
  and Monte Carlo dropout, respectively. The method calibrates aleatoric uncertainty
  and leverages epistemic uncertainty for out-of-domain detection.
---

# Uncertainty-Aware Mean Opinion Score Prediction

## Quick Facts
- arXiv ID: 2408.12829
- Source URL: https://arxiv.org/abs/2408.12829
- Reference count: 0
- Primary result: Calibrated aleatoric uncertainty (UCE: 0.0338, NLL: 0.632) with improved OOD detection (AUC up to 0.890)

## Executive Summary
This paper proposes an uncertainty-aware MOS prediction system for speech quality assessment that models both aleatoric (data) and epistemic (model) uncertainty. The method uses heteroscedastic regression to predict MOS scores with associated aleatoric uncertainty, and Monte Carlo dropout to estimate epistemic uncertainty during inference. A simple scalar calibration step further improves aleatoric uncertainty reliability. Experimental results demonstrate effective uncertainty capture, enabling selective prediction and out-of-domain detection, thus enhancing practical utility in diverse real-world environments.

## Method Summary
The proposed system uses a Wav2Vec 2.0 base model with two task-specific heads: one for MOS prediction and another for aleatoric uncertainty (log-variance). Heteroscedastic regression is trained with NLL loss to predict both the MOS score and its uncertainty. Monte Carlo dropout (p=0.5, T=25) estimates epistemic uncertainty by computing the variance across stochastic forward passes. Aleatoric uncertainty is calibrated using a simple scalar parameter r optimized on a validation set. The system is trained on the BVCC dataset and evaluated on multiple out-of-domain datasets for OOD detection.

## Key Results
- Calibrated model achieves UCE of 0.0338 and NLL of 0.632
- Improved selective prediction performance with calibrated uncertainties
- Enhanced OOD detection capability (AUC up to 0.890) using epistemic uncertainty

## Why This Works (Mechanism)

### Mechanism 1
Heteroscedastic regression enables data-dependent uncertainty estimation that captures aleatoric uncertainty arising from inherent data randomness. The model learns to predict both the MOS score and the log-variance of a Gaussian distribution over the target. During training, it maximizes the Negative Gaussian Log-Likelihood (NLL) loss, which explicitly optimizes the predicted variance to match the observed data dispersion. Core assumption: The target MOS scores can be reasonably modeled as samples from a Gaussian distribution with a data-dependent variance.

### Mechanism 2
Monte Carlo dropout captures epistemic uncertainty arising from the model's lack of knowledge, particularly for out-of-domain samples. Dropout is enabled during inference, and the model performs T stochastic forward passes. The variance of the predictions across these passes estimates the epistemic uncertainty, which is high when the model is uncertain about its predictions. Core assumption: Dropout acts as a Bayesian approximation, and the variance across stochastic forward passes reflects the model's uncertainty.

### Mechanism 3
Uncertainty calibration improves the reliability of aleatoric uncertainty estimates by scaling them to better match the actual prediction errors. After initial heteroscedastic regression, a simple scaling factor r is optimized on a calibration set to minimize the NLL loss, effectively aligning the predicted uncertainty with observed errors. Core assumption: A simple multiplicative scaling of the heteroscedastic variance is sufficient to calibrate the uncertainty to match observed errors.

## Foundational Learning

- Concept: Gaussian distribution and its parameters (mean and variance).
  - Why needed here: The model assumes MOS scores follow a Gaussian distribution, and heteroscedastic regression learns to predict both the mean (MOS score) and variance (uncertainty).
  - Quick check question: What are the two parameters that define a Gaussian distribution?

- Concept: Negative Log-Likelihood (NLL) loss.
  - Why needed here: NLL loss is used to train the heteroscedastic regression model, optimizing it to predict both accurate MOS scores and calibrated uncertainties.
  - Quick check question: How does NLL loss encourage the model to predict accurate uncertainties?

- Concept: Monte Carlo methods and dropout as a Bayesian approximation.
  - Why needed here: MC dropout uses multiple stochastic forward passes to approximate the posterior distribution, and the variance across these passes estimates the epistemic uncertainty.
  - Quick check question: Why does enabling dropout during inference help estimate epistemic uncertainty?

## Architecture Onboarding

- Component map: Wav2Vec 2.0 base model -> Linear layer (768â†’256) -> Dropout -> MOS prediction head and Aleatoric uncertainty head
- Critical path: Input audio -> Wav2Vec 2.0 -> Linear layer -> Dropout -> MOS prediction head and Aleatoric uncertainty head
- Design tradeoffs:
  - Using a pretrained Wav2Vec 2.0 model provides strong audio feature extraction but may limit flexibility
  - Enabling dropout during inference for epistemic uncertainty estimation increases computation but provides valuable uncertainty information
  - Simple scaling calibration is computationally efficient but may not capture complex relationships between predicted and actual errors
- Failure signatures:
  - Poor NLL or UCE scores: Indicates issues with aleatoric uncertainty estimation or calibration
  - High epistemic uncertainty on in-domain samples: Suggests the model is not confident in its predictions even for familiar data
  - Low epistemic uncertainty on out-of-domain samples: Indicates the model may not be detecting unfamiliarity correctly
- First 3 experiments:
  1. Train the model with only MSE loss (no uncertainty estimation) and compare its performance to the heteroscedastic regression model on in-domain data
  2. Enable MC dropout during inference and visualize the epistemic uncertainty on a held-out in-domain test set
  3. Apply the uncertainty calibration step and evaluate its impact on UCE and NLL metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of dropout rate p in MC dropout affect the balance between epistemic prediction uncertainty and epistemic distributional uncertainty?
- Basis in paper: The paper mentions using a dropout rate p of 0.5 for estimating epistemic uncertainty but does not explore how different rates might impact the two types of epistemic uncertainty.
- Why unresolved: The paper does not investigate the sensitivity of epistemic uncertainty to different dropout rates, which could influence the model's performance in OOD detection and selective prediction.
- What evidence would resolve it: Experiments comparing epistemic uncertainty and model performance across a range of dropout rates would clarify the optimal setting for different uncertainty types.

### Open Question 2
- Question: Can the uncertainty-aware MOS prediction framework be extended to other subjective quality assessment tasks beyond speech, such as image or video quality assessment?
- Basis in paper: The paper focuses on MOS prediction for speech quality but discusses uncertainty modeling in a general context that could apply to other domains.
- Why unresolved: The paper does not explore the applicability of the uncertainty-aware framework to other subjective quality assessment tasks, leaving its generalizability untested.
- What evidence would resolve it: Applying the framework to image or video quality assessment tasks and comparing its performance to existing methods would demonstrate its broader applicability.

### Open Question 3
- Question: How does the uncertainty-aware MOS prediction system perform in real-world deployment scenarios with continuously evolving data distributions?
- Basis in paper: The paper discusses the system's potential for selective prediction and OOD detection but does not address its performance in dynamic, real-world environments.
- Why unresolved: The paper's experiments are limited to static datasets, and it does not explore how the system adapts to changes in data distribution over time.
- What evidence would resolve it: Longitudinal studies deploying the system in real-world settings with evolving data would reveal its robustness and adaptability in practice.

## Limitations

- Critical implementation details (dropout layer placement, batch size, learning rate, training duration) are unspecified, hindering exact reproduction
- Heteroscedastic regression assumes Gaussian distribution of MOS scores, which may not hold for highly subjective ratings
- MC dropout requires 25 forward passes during inference, potentially limiting real-time deployment

## Confidence

- High Confidence: The fundamental mechanisms of heteroscedastic regression and MC dropout for uncertainty estimation are well-established in the literature and the paper provides sufficient detail on their implementation.
- Medium Confidence: The uncertainty calibration approach using simple scalar scaling is straightforward but may not generalize well to all scenarios. The OOD detection performance relies on the assumption that epistemic uncertainty effectively captures domain shift.
- Low Confidence: Without complete architectural specifications and hyperparameter details, the exact reproducibility of the reported results (UCE of 0.0338, NLL of 0.632, AUC up to 0.890) cannot be guaranteed.

## Next Checks

1. **Distribution Validation**: Test the Gaussian assumption by analyzing the distribution of MOS scores in the BVCC dataset. If the distribution significantly deviates from Gaussian, explore alternative distributions (e.g., Student's t-distribution) for heteroscedastic regression.

2. **Calibration Method Comparison**: Compare the simple scalar calibration approach against more sophisticated methods like temperature scaling or Platt scaling. Evaluate whether these methods provide better uncertainty calibration, particularly for out-of-domain data.

3. **Computational Efficiency Analysis**: Profile the inference time with MC dropout (T=25) and investigate whether fewer forward passes (e.g., T=5 or T=10) maintain acceptable uncertainty estimation quality. Explore variance reduction techniques to improve computational efficiency.