---
ver: rpa2
title: Optimizing the Training Schedule of Multilingual NMT using Reinforcement Learning
arxiv_id: '2410.06118'
source_url: https://arxiv.org/abs/2410.06118
tags:
- training
- language
- learning
- translation
- lrls
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper applies reinforcement learning to optimize the training
  schedule for multilingual neural machine translation (NMT), specifically for translating
  low-resource languages (LRLs) paired with high-resource languages (HRLs). The authors
  propose two algorithms: Teacher-Student Curriculum Learning (TSCL) and Deep Q Network
  (DQN).'
---

# Optimizing the Training Schedule of Multilingual NMT using Reinforcement Learning

## Quick Facts
- arXiv ID: 2410.06118
- Source URL: https://arxiv.org/abs/2410.06118
- Reference count: 40
- One-line primary result: DQN outperforms baselines in BLEU and COMET scores for low-resource language translation by optimizing training batch scheduling

## Executive Summary
This paper applies reinforcement learning to optimize the training schedule for multilingual neural machine translation (NMT), specifically targeting translation quality for low-resource languages (LRLs) paired with high-resource languages (HRLs). The authors propose two RL algorithms: Teacher-Student Curriculum Learning (TSCL) and Deep Q Network (DQN). Both methods learn to adjust the proportion of LRL versus HRL batches during training based on estimated model competence, improving performance particularly for LRLs while maintaining overall translation quality.

## Method Summary
The paper introduces RL-based training schedule optimization for multilingual NMT. Two algorithms are proposed: TSCL uses exponentially smoothed estimates of returns based on development set losses, while DQN employs an auxiliary neural network to estimate rewards from training history. Both methods operate on monolingual batches, selecting the source language at each training step based on learned estimates of model competence. The approach includes a warm-up period on HRLs before RL-based scheduling begins, and uses cross-entropy loss values on development batches as the state representation for the RL agent.

## Key Results
- DQN outperforms baseline training schedules in both BLEU and COMET scores for low-resource language translation
- The RL approach achieves better performance for LRLs while maintaining overall translation quality
- DQN demonstrates robustness to hyperparameter changes and improves convergence speed compared to standard training schedules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DQN learns to optimize the balance between HRL and LRL batch presentation by estimating future rewards from current model states.
- Mechanism: The Q-network approximates expected rewards for selecting each language as the next training batch, based on cross-entropy loss patterns across all languages in a prototype batch.
- Core assumption: Cross-entropy loss values on development batches provide sufficient signal about model competence in each language to guide training schedule optimization.
- Evidence anchors:
  - [abstract] "DQN estimates rewards using an additional neural network trained from the history of actions selected in different states of the system, together with the rewards received"
  - [section] "we compute the current state of the model as the vector of cross-entropy loss values obtained from the NMT system over a development batch of sentences"
  - [corpus] Weak evidence - related papers focus on LRL/HRL performance gaps but not RL-based training schedule optimization
- Break condition: If loss patterns become too noisy or if the state representation fails to capture meaningful competence differences between languages.

### Mechanism 2
- Claim: TSCL improves learning efficiency by exponentially smoothing returns based on observed loss decreases.
- Mechanism: The algorithm tracks the exponentially weighted moving average of reward signals (loss decreases) for each language action, enabling better exploitation of languages that consistently yield positive returns.
- Core assumption: Loss decreases on development batches are reliable indicators of learning progress and can be smoothed to reduce variance in training decisions.
- Evidence anchors:
  - [abstract] "TSCL uses an exponentially smoothed estimate of the returns of each action based on the loss on monolingual or multilingual development subsets"
  - [section] "The expected return Qt of the action is the exponentially weighted moving average of the rewards for the respective source language"
  - [corpus] Weak evidence - related work on curriculum learning exists but not specifically on exponentially smoothed return estimation
- Break condition: If the smoothing coefficient is set too high (over-smoothing) or too low (insufficient noise reduction), or if loss signals become unreliable during training.

### Mechanism 3
- Claim: The warm-up period on HRLs enables cross-lingual transfer benefits before LRL optimization begins.
- Mechanism: Initial training exclusively on HRLs builds foundational representations that transfer to related LRLs, improving overall model performance when LRLs are later introduced.
- Core assumption: HRLs share sufficient linguistic features with related LRLs to enable meaningful transfer learning during early training stages.
- Evidence anchors:
  - [section] "we use an ϵ-greedy policy with a fixed value of ϵ... Rewards of the RL agent only start to play a role after the warm-up phase"
  - [section] "Unlike TSCL, we follow Kumar et al. (2019) and start with ϵ = 1 during warm-up, then gradually decrease this value at the end of warm-up"
  - [corpus] Weak evidence - related papers discuss multilingual training but not specifically warm-up period design
- Break condition: If the warm-up period is too long (preventing LRL optimization) or too short (insufficient transfer learning), or if HRL-LRL pairs share too few linguistic features.

## Foundational Learning

- Concept: Reinforcement Learning - specifically Q-learning and policy optimization
  - Why needed here: The training schedule optimization problem maps naturally to RL, where actions (language selection) yield rewards (loss improvements) in states (model competence)
  - Quick check question: What is the difference between on-policy and off-policy RL, and which approach does DQN use?

- Concept: Curriculum Learning and Self-Paced Learning
  - Why needed here: The paper builds on curriculum learning principles where training material is presented in an order that matches model competence, but uses RL instead of fixed difficulty metrics
  - Quick check question: How does TSCL differ from traditional self-paced learning in terms of how competence is estimated?

- Concept: Neural Machine Translation architecture and training dynamics
  - Why needed here: Understanding how multilingual NMT models handle multiple languages and how training schedules affect convergence and generalization is crucial for designing effective RL agents
  - Quick check question: Why might uniform batch sampling lead to overfitting on LRLs while proportional sampling leads to under-representation?

## Architecture Onboarding

- Component map: NMT model (OpenNMT Transformer) -> RL agent (TSCL or DQN) -> State representation (cross-entropy loss vectors) -> Reward function (loss decrease between time steps) -> Action space (language selection) -> Experience replay buffer (DQN only) -> Target network (DQN only)

- Critical path:
  1. Initialize NMT model and RL agent
  2. Warm-up phase on HRLs
  3. For each training step:
     - Select action (language) using RL policy
     - Sample batch and train NMT model
     - Compute reward (loss decrease)
     - Update RL agent (experience replay + Q-network update)
     - Update target network (DQN only)

- Design tradeoffs:
  - State representation complexity vs. computational cost
  - Exploration vs. exploitation balance (ϵ-greedy policy)
  - Batch composition (monolingual vs. multilingual)
  - Warm-up duration vs. LRL optimization time
  - Replay buffer size vs. memory constraints

- Failure signatures:
  - RL agent gets stuck selecting only HRLs (insufficient exploration)
  - NMT model shows catastrophic forgetting of earlier languages
  - Reward signals become too noisy to provide useful gradients
  - Training becomes unstable due to high variance in loss estimates

- First 3 experiments:
  1. Run baseline with uniform sampling to establish reference performance
  2. Implement TSCL with default hyperparameters and compare convergence speed
  3. Implement DQN with default architecture and compare macro-average BLEU scores across all languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the DQN algorithm's performance compare when using multilingual batches instead of monolingual batches?
- Basis in paper: [explicit] The paper compares DQN to a baseline using shuffled multilingual batches but does not test DQN itself with multilingual batches.
- Why unresolved: The current implementation only allows for monolingual batch selection, so multilingual batch optimization remains untested.
- What evidence would resolve it: Running DQN with multilingual batches and comparing its performance metrics (BLEU/COMET) to both monolingual DQN and the shuffled multilingual baseline.

### Open Question 2
- What is the optimal warm-up period duration for different dataset sizes and language family compositions?
- Basis in paper: [explicit] The paper uses a fixed 16k step warm-up period and notes that convergence speed is affected by warm-up presence/absence, but does not explore optimal warm-up duration.
- Why unresolved: The 16k step warm-up was chosen based on prior work rather than being optimized for this specific task, and the paper only tests with/without warm-up.
- What evidence would resolve it: Systematic experiments varying warm-up duration (e.g., 8k, 16k, 32k steps) across different dataset configurations and measuring convergence speed and final performance.

### Open Question 3
- How does the DQN algorithm perform on translation tasks with non-Latin scripts or distant language families?
- Basis in paper: [explicit] The experiments only include Latin-script languages from Romance, Slavic, and Turkic families, with the authors noting this limitation.
- Why unresolved: The current study's language selection limits generalizability to scripts and language families with different transfer patterns or orthographic features.
- What evidence would resolve it: Testing DQN on datasets containing non-Latin scripts (e.g., Arabic, Chinese, Devanagari) or distant language families (e.g., Uralic, Semitic) and comparing performance to the current results.

### Open Question 4
- What is the relationship between the DQN algorithm's learned training schedule and the actual difficulty progression of the NMT model across languages?
- Basis in paper: [explicit] The paper analyzes the Q network's behavior when probing with artificially difficult languages but does not systematically correlate learned schedules with model difficulty progression.
- Why unresolved: While the paper shows that DQN adjusts LRL/HRL proportions, it doesn't establish whether these adjustments align with actual learning difficulty curves.
- What evidence would resolve it: Tracking per-language loss trajectories during training and correlating them with the proportion of batches allocated by DQN at each training stage.

## Limitations

- The paper only tests on Latin-script languages from Romance, Slavic, and Turkic families, limiting generalizability to scripts with different orthographic features
- The state representation using cross-entropy loss vectors may not capture all relevant information about model competence across languages
- The optimal warm-up period duration remains unexplored, with the current 16k step duration chosen based on prior work rather than task-specific optimization

## Confidence

- High confidence: DQN outperforms TSCL and baseline training schedules on macro-averaged BLEU scores
- Medium confidence: The warm-up period on HRLs enables meaningful cross-lingual transfer benefits
- Low confidence: The specific state representation using cross-entropy loss vectors captures all relevant information about model competence across languages

## Next Checks

1. Conduct ablation studies varying the state representation in DQN - compare performance using only loss values vs. including additional features like batch diversity or previous action history

2. Test the robustness of TSCL's exponential smoothing parameter α across different language pairs and data distributions to determine if the current value of 0.1 is universally optimal

3. Implement a control experiment where the RL agent selects languages randomly during the exploitation phase (after warm-up) to isolate whether learned policies provide meaningful improvements over exploration alone