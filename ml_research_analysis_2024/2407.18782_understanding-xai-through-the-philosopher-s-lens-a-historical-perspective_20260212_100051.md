---
ver: rpa2
title: 'Understanding XAI Through the Philosopher''s Lens: A Historical Perspective'
arxiv_id: '2407.18782'
source_url: https://arxiv.org/abs/2407.18782
tags:
- explanation
- explanations
- scientific
- science
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the historical development of the concept of
  explanation in both the philosophy of science and AI, identifying key parallels
  and potential insights for the field of explainable AI (XAI). The authors trace
  the evolution of scientific explanation from pre-Hempelian era to modern statistical
  and counterfactual models, and draw analogies with the progression of XAI from early
  rule-based systems to current black-box models with surrogate explanations.
---

# Understanding XAI Through the Philosopher's Lens: A Historical Perspective

## Quick Facts
- arXiv ID: 2407.18782
- Source URL: https://arxiv.org/abs/2407.18782
- Reference count: 40
- One-line primary result: The paper maps the evolution of scientific explanation models onto XAI methods, identifying shared epistemological themes and potential frameworks for evaluating XAI.

## Executive Summary
This paper explores the historical development of the concept of explanation in both the philosophy of science and AI, identifying key parallels and potential insights for the field of explainable AI (XAI). The authors trace the evolution of scientific explanation from pre-Hempelian era to modern statistical and counterfactual models, and draw analogies with the progression of XAI from early rule-based systems to current black-box models with surrogate explanations. They identify common themes such as the shift from deductive to statistical models, the relationship between explanation and understanding, and the search for bona fide explanations. The study aims to provide an epistemological framework for understanding XAI and suggests that insights from the philosophy of science can inform the development of more rigorous and effective XAI approaches.

## Method Summary
The paper provides a conceptual and historical comparison between the philosophical discourse on scientific explanation and the modern field of explainable AI (XAI). Rather than presenting an empirical study, the authors conduct a theoretical review, mapping the progression of explanation models in both domains and discussing related epistemological themes. The method involves identifying and analyzing parallels in the progression of both fields, focusing on the shift from deductive to statistical models, the emergence of concepts like explanation vs. understanding, the role of pragmatic factors, and the criteria for bona fide explanations.

## Key Results
- The development of scientific explanation models and XAI methods followed a similar progression from deductive to statistical approaches
- Shared themes between the two domains include the explanation-understanding relationship and the role of pragmatic factors
- Philosophical criteria and concepts can provide a more rigorous framework for evaluating XAI methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper works by mapping the evolution of explanation concepts in science onto the evolution of XAI methods, creating a historical parallel that gives XAI a shared epistemological foundation.
- Mechanism: It identifies that both fields independently moved from deterministic, rule-based explanations to probabilistic, statistical ones. By aligning the timeline of philosophical explanation models (D-N, I-S, S-R, etc.) with the development of AI explainability methods (MYCIN, LIME, SHAP, etc.), it constructs a bridge between the two domains.
- Core assumption: The trajectory of explanation in science is structurally analogous to the trajectory of explanation in AI, so insights from one can be transferred to the other.
- Evidence anchors:
  - [abstract] "we show that a gradual progression has independently occurred in both domains from logical-deductive to statistical models of explanation"
  - [section V] "the development of the debates followed a general common progression, specifically from deductive to statistical explanations"
- Break condition: If the historical parallels are found to be superficial or if the underlying epistemological goals of science and AI are fundamentally different, the analogy collapses.

### Mechanism 2
- Claim: The paper works by introducing philosophical terminology and criteria (e.g., explanandum, explanans, bona fide explanations) to bring rigor and clarity to the XAI discourse.
- Mechanism: It imports concepts like "explanandum" and "explanans" from philosophy of science to frame what needs explaining and what serves as the explanation in XAI. It also adopts criteria for good explanations (similarity, exactness, fruitfulness, simplicity) from Carnap to evaluate XAI methods.
- Core assumption: Philosophical concepts and criteria are directly applicable to evaluating and designing XAI explanations.
- Evidence anchors:
  - [section III.C] "the majority of the aforementioned questions remain partially unresolved, to the extent that the precise definition of 'explanation' remains to some degree obscure"
  - [section V.C] "researchers in both epistemology and the XAI domains have sought to identify the characteristics that distinguish bona fide explanations"
- Break condition: If the philosophical criteria do not map well onto the practical needs of XAI users, or if the imported terminology introduces more confusion than clarity, the mechanism fails.

### Mechanism 3
- Claim: The paper works by highlighting shared themes between the two domains, such as the explanation-understanding relationship and the role of pragmatic factors, to show that XAI is part of a broader epistemological inquiry.
- Mechanism: It identifies that both fields have independently recognized the importance of context, user needs, and the distinction between explaining and understanding. This positions XAI as part of a larger philosophical tradition rather than an isolated technical challenge.
- Core assumption: The shared themes are deep enough to warrant a unified epistemological framework for XAI.
- Evidence anchors:
  - [section V.A] "an explanation is not decontextualized but pertains to the situation in which questions and answers are posed"
  - [section V.A] "explanations involve understanding how the world works"
- Break condition: If the shared themes are coincidental rather than fundamental, or if XAI's unique technical constraints make it incomparable to scientific explanation, the mechanism fails.

## Foundational Learning

- Concept: Deductive-Nomological (D-N) model of explanation
  - Why needed here: To understand the earliest philosophical model of explanation that the paper uses as a starting point for comparison with early rule-based AI systems.
  - Quick check question: What are the two main components of the D-N model, and how do they relate to each other?

- Concept: Statistical relevance and probabilistic causation
  - Why needed here: To grasp how later philosophical models moved away from strict deduction to handle uncertainty, paralleling the shift in XAI from interpretable rules to statistical approximations.
  - Quick check question: How does the Statistical Relevance model differ from the D-N model in terms of handling uncertainty?

- Concept: Explanandum and explanans
  - Why needed here: To use the precise philosophical terminology introduced in the paper to frame what needs explaining (explanandum) and what serves as the explanation (explanans) in XAI.
  - Quick check question: In the context of XAI, what would be an example of an explanandum and an explanans?

## Architecture Onboarding

- Component map:
  - Literature Review: Pre-Hempelian era, Received View (D-N, I-S models), Post-Hempelian era (S-R, Pragmatics, Unificationist, Abductive, Neo-mechanistic, Counterfactual)
  - XAI Development Timeline: Early rule-based systems (MYCIN), ML black-box era, Modern XAI methods (LIME, SHAP, counterfactuals)
  - Comparative Analysis: Identifying parallels and shared themes between the two timelines
  - Implications for XAI: Using philosophical criteria and concepts to evaluate and guide XAI development

- Critical path:
  1. Understand the philosophical models of explanation
  2. Map them to the corresponding XAI methods
  3. Identify shared themes and concepts
  4. Apply philosophical criteria to evaluate XAI
  5. Draw implications for future XAI research

- Design tradeoffs:
  - Depth vs. Breadth: Covering the full history of both fields vs. focusing on the most relevant parallels
  - Technical vs. Philosophical: Balancing the technical details of XAI methods with the philosophical concepts of explanation
  - Analogy vs. Reality: Ensuring the parallels drawn are substantive and not superficial

- Failure signatures:
  - If the historical parallels are forced or do not hold up to scrutiny
  - If the philosophical concepts are misapplied or do not add value to the XAI discourse
  - If the shared themes are coincidental rather than fundamental

- First 3 experiments:
  1. Create a detailed timeline mapping of philosophical explanation models to XAI methods, ensuring each pairing is justified.
  2. Apply Carnap's criteria for good explanations (similarity, exactness, fruitfulness, simplicity) to evaluate a set of XAI methods.
  3. Conduct a user study to test whether explanations designed with philosophical criteria in mind are more effective for XAI users.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the principles of scientific explanation from the philosophy of science provide a more rigorous framework for evaluating XAI methods?
- Basis in paper: [explicit] The authors suggest that epistemological principles may serve as a source of inspiration in evaluating XAI, helping in the assessment of theoretical guidelines for evaluating XAI derived from the philosophy of science.
- Why unresolved: While the paper establishes parallels between scientific explanation and XAI, it does not provide a concrete framework or specific criteria for applying these philosophical principles to XAI evaluation.
- What evidence would resolve it: A study demonstrating how specific principles from the philosophy of science can be applied to evaluate and improve XAI methods, with measurable outcomes.

### Open Question 2
- Question: How can the concept of counterfactual explanations be further developed to balance the need for causal understanding with the practical limitations of data availability and computational complexity?
- Basis in paper: [inferred] The paper discusses the growing popularity of counterfactual explanations in both scientific explanation and XAI, but does not address the practical challenges of implementing them.
- Why unresolved: While counterfactual explanations are promising, their implementation faces challenges related to data requirements, computational cost, and ensuring the validity of the generated counterfactuals.
- What evidence would resolve it: Research demonstrating efficient algorithms for generating counterfactual explanations that are both causally valid and computationally feasible, along with empirical studies showing their effectiveness in improving user understanding.

### Open Question 3
- Question: Can the distinction between global and local explanations in XAI be further refined to account for the varying needs of different stakeholders and contexts?
- Basis in paper: [explicit] The authors note the distinction between global and local explanations in XAI, and draw a parallel to top-down and bottom-up accounts in scientific explanation.
- Why unresolved: The paper acknowledges the existence of this distinction but does not explore how it might be further refined or adapted to address the diverse needs of different users and contexts in XAI applications.
- What evidence would resolve it: A framework for tailoring explanations to specific user groups and contexts, along with empirical studies demonstrating improved user understanding and trust in AI systems when using such tailored explanations.

## Limitations
- The parallels drawn between philosophical explanation models and XAI methods may oversimplify the unique technical and practical constraints of AI systems
- The application of philosophical criteria to evaluate XAI methods is largely theoretical and lacks empirical validation with end-users
- The historical mapping relies on the authors' interpretation of both philosophical and AI development trajectories, which may introduce subjective bias

## Confidence
- High confidence: The historical accuracy of key philosophical models (D-N, I-S, S-R, etc.) and their core characteristics
- Medium confidence: The identification of shared themes between scientific explanation and XAI, as these are conceptual parallels that may be debated
- Medium confidence: The mapping of philosophical criteria to XAI evaluation, pending empirical validation

## Next Checks
1. Conduct a user study to empirically test whether explanations designed with philosophical criteria (similarity, exactness, fruitfulness, simplicity) are more effective for XAI users compared to standard approaches
2. Create a detailed timeline mapping of philosophical explanation models to XAI methods, ensuring each pairing is justified by the underlying epistemological goals and mechanisms
3. Analyze specific XAI methods (e.g., LIME, SHAP) against the criteria for bona fide explanations identified in the philosophical literature to assess their explanatory power and limitations