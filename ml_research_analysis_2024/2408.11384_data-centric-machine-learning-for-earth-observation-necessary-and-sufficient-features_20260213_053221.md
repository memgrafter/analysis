---
ver: rpa2
title: 'Data-Centric Machine Learning for Earth Observation: Necessary and Sufficient
  Features'
arxiv_id: '2408.11384'
source_url: https://arxiv.org/abs/2408.11384
tags:
- features
- performance
- data
- feature
- important
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a data-centric machine learning approach for
  Earth observation using explainable AI to identify necessary and sufficient features
  in temporal multimodal geospatial data. The method uses incremental deletion guided
  by feature attribution scores (Shapley Value Sampling and Guided Backprop) to determine
  which bands and time-steps are critical for model performance.
---

# Data-Centric Machine Learning for Earth Observation: Necessary and Sufficient Features

## Quick Facts
- arXiv ID: 2408.11384
- Source URL: https://arxiv.org/abs/2408.11384
- Reference count: 20
- One-line primary result: Data-centric approach using explainable AI identifies minimal feature subsets that maintain baseline performance in Earth observation tasks

## Executive Summary
This paper presents a data-centric machine learning framework for Earth observation that identifies necessary and sufficient features in temporal multimodal geospatial data. The method uses incremental deletion guided by feature attribution scores from Shapley Value Sampling (SVS) and Guided Backprop (GB) to systematically remove non-essential features while maintaining model performance. Applied to three datasets (CropHarvest, CropYield, and China PM2.5), the approach demonstrates that optimal accuracy can be achieved with less than 20% of temporal instances in some cases, and that SVS attributions are more faithful than GB. The framework provides a systematic way to reduce computational costs and environmental impact while maintaining model accuracy.

## Method Summary
The method employs a data-centric approach using incremental deletion guided by feature attribution scores to identify minimal feature subsets that maintain baseline performance. It combines temporal multimodal geospatial data with explainable AI techniques, specifically Shapley Value Sampling (SVS) and Guided Backprop (GB), to estimate feature importance. The framework uses feature grouping strategies to analyze temporal and spectral components collectively, then iteratively removes features ranked by attribution scores while retraining and evaluating model performance. Ensemble variants (SG-SQ, VAR) are also tested to improve attribution correctness in specific cases.

## Key Results
- SVS attributions are more faithful than GB, with ensemble methods improving correctness in specific cases
- Some datasets can reach optimal accuracy with less than 20% of temporal instances
- Others require only a single band from one modality to maintain baseline performance
- The approach successfully maintains baseline performance while reducing feature set size

## Why This Works (Mechanism)

### Mechanism 1
Incremental deletion guided by Shapley Value Sampling (SVS) attributions can identify minimal feature subsets that maintain baseline performance. The method iteratively removes features ranked by attribution scores, retrains the model, and observes performance changes. Features whose removal doesn't significantly degrade performance are deemed non-essential. This assumes attribution scores accurately reflect feature importance for the specific model and task.

### Mechanism 2
Temporal and spectral feature grouping strategies allow efficient identification of important time-steps and bands. Features are grouped by time-step or by band before attribution estimation, revealing the importance of entire temporal or spectral components. This assumes grouping features for collective perturbation accurately captures their combined importance to model predictions.

### Mechanism 3
Ensemble variants of attribution methods (SG-SQ, VAR) can improve the correctness of feature importance estimates in some cases. These methods add noise or variance to input samples when estimating attributions, creating an ensemble of estimates that can filter out noise and improve robustness. This assumes the noise/variance helps identify truly important features rather than spurious correlations.

## Foundational Learning

- Concept: Shapley Value Sampling (SVS)
  - Why needed here: SVS provides a model-agnostic way to estimate feature importance by considering all possible feature coalitions, making it suitable for comparing different model architectures.
  - Quick check question: How does SVS differ from traditional Shapley value computation in terms of computational efficiency?

- Concept: Incremental deletion framework
  - Why needed here: This framework allows systematic evaluation of feature importance by observing performance changes as features are removed, rather than relying solely on attribution scores.
  - Quick check question: What is the key difference between this approach and traditional feature selection methods like Recursive Feature Elimination?

- Concept: Feature grouping strategies
  - Why needed here: Grouping allows the method to identify important time-steps or bands rather than individual features, which is more relevant for temporal and spectral data analysis.
  - Quick check question: How might the results differ if features were not grouped before attribution estimation?

## Architecture Onboarding

- Component map: Data preparation -> Model selection -> Attribution estimation -> Incremental deletion -> Analysis
- Critical path: Model selection → Attribution estimation → Incremental deletion → Performance analysis
- Design tradeoffs:
  - SVS vs. GB: SVS is more faithful but computationally expensive; GB is faster but less reliable
  - Single vs. ensemble methods: Ensembles may improve correctness but add complexity
  - Feature grouping: Provides higher-level insights but may miss important individual features
- Failure signatures:
  - Performance doesn't degrade when removing features: May indicate over-regularization or attribution scores not reflecting true importance
  - Performance drops immediately: May suggest critical features are being removed too early
  - Inconsistent results across datasets: May indicate method is not generalizable
- First 3 experiments:
  1. Implement SVS attribution estimation on a small subset of the CropHarvest dataset and verify it produces reasonable feature rankings.
  2. Apply incremental deletion with SVS on the CropHarvest dataset, removing least important bands first, and observe if performance is maintained.
  3. Compare SVS and GB attribution results on the CropYield dataset to verify the claim about SVS faithfulness.

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal combination of feature attribution methods and model architectures that maximizes both faithfulness and performance in identifying minimal feature subsets? The paper states that "comparing these results across various model architectures might help identify the features necessary and sufficient for predicting the target regardless of the model employed." This remains unresolved as the paper only tested TempCNN with a limited set of attribution methods and did not explore combinations across different architectures.

### Open Question 2
How does the incremental deletion framework perform on datasets with longer temporal spans or higher frequency sampling (e.g., daily or hourly data)? The paper tested datasets with monthly, 5-day, and hourly sampling but did not explore the impact of temporal resolution on feature importance. The framework's scalability to different sampling frequencies is untested.

### Open Question 3
What is the computational trade-off between using ensemble-based attribution methods versus single estimators in terms of accuracy and resource efficiency? The paper notes that "ensemble-based variants only improved the faithfulness of GB estimates in a few cases" and questions whether the additional computational cost is justified. This trade-off was not quantified in the paper.

## Limitations
- Computational expense of SVS scales exponentially with feature count, limiting practical application
- Effectiveness depends heavily on assumption that attribution scores accurately reflect true feature importance
- Ensemble variants show inconsistent improvements across datasets, suggesting limited generalizability

## Confidence

- Claim: SVS attributions are more faithful than GB
  - Confidence: Low
  - Reason: Lacks external validation from broader ML literature

- Claim: Minimal feature subsets can maintain baseline performance
  - Confidence: Medium
  - Reason: Promising results but need replication across more diverse datasets

- Claim: Incremental deletion framework systematically identifies necessary features
  - Confidence: Medium
  - Reason: Well-established concepts but specific implementation needs validation

## Next Checks

1. Compare SVS attribution results with established feature importance methods (e.g., permutation importance) on the same datasets to validate faithfulness claims.

2. Test the incremental deletion framework on a fourth, independent geospatial dataset with different characteristics to assess generalizability.

3. Implement a scaled-down version of the method on a dataset with known ground-truth feature importance to verify that the approach correctly identifies critical features.