---
ver: rpa2
title: 'LLaMA Beyond English: An Empirical Study on Language Capability Transfer'
arxiv_id: '2401.01055'
source_url: https://arxiv.org/abs/2401.01055
tags:
- language
- llama
- chinese
- languages
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to effectively transfer the language
  generation and instruction-following capabilities of large language models (LLMs)
  to non-English languages. Through extensive empirical experiments based on LLaMA,
  the authors analyze the impact of key factors such as vocabulary extension, further
  pretraining, and instruction tuning on transfer.
---

# LLaMA Beyond English: An Empirical Study on Language Capability Transfer

## Quick Facts
- **arXiv ID**: 2401.01055
- **Source URL**: https://arxiv.org/abs/2401.01055
- **Reference count**: 10
- **Primary result**: Comparable performance to state-of-the-art transfer models can be achieved with less than 1% of the pretraining data

## Executive Summary
This paper investigates how to effectively transfer the language generation and instruction-following capabilities of large language models (LLMs) to non-English languages. Through extensive empirical experiments based on LLaMA, the authors analyze the impact of key factors such as vocabulary extension, further pretraining, and instruction tuning on transfer. They find that comparable performance to state-of-the-art transfer models can be achieved with less than 1% of the pretraining data. Additionally, they observe instances of code-switching during transfer training, suggesting cross-lingual alignment might be internalized within the model. Similar results are observed across 13 low-resource languages.

## Method Summary
The study investigates vocabulary extension, further pretraining, and instruction tuning to transfer LLaMA's capabilities to non-English languages. Experiments use Chinese as the primary target language, with extension to 13 low-resource languages. The approach involves extending LLaMA's vocabulary to include target language tokens, further pretraining on limited target language data (up to 100 billion tokens), and instruction tuning using datasets like BELLE and Bactrain-X. Performance is evaluated across multiple benchmarks including LLM-Eval, C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench.

## Key Results
- Vocabulary extension is not necessary for small-scale transfer training (tens of billions of tokens)
- Code-switching in model outputs indicates internalized cross-lingual alignment
- Small-scale instruction tuning (hundreds of thousands of examples) significantly improves response quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vocabulary extension is not necessary for small-scale transfer training (tens of billions of tokens).
- Mechanism: When pretraining data is limited to tens of billions of tokens, extending the vocabulary to include target language tokens does not provide significant performance gains compared to training on the original vocabulary.
- Core assumption: The cross-lingual semantic alignment within the model allows it to handle target language tokens even without explicit vocabulary extension.
- Evidence anchors:
  - [abstract] "We find that further pretraining with 0.5 billion Chinese tokens on the original vocabulary significantly outperforms performance on the extended vocabulary, even though the latter has been further pretrained on over 30 billion tokens."
  - [section] "This suggests that vocabulary extension might not be a suitable choice for small-scale incremental pretraining in the order of tens of billions."
- Break condition: If pretraining scales to trillions of tokens, vocabulary extension might become beneficial as the model has more capacity to learn explicit cross-lingual representations.

### Mechanism 2
- Claim: Code-switching in model outputs indicates internalized cross-lingual alignment.
- Mechanism: During transfer training, the model generates outputs containing tokens from multiple languages (code-switching), suggesting it has learned to map concepts across languages internally.
- Core assumption: The model's pretraining on diverse multilingual data allows it to develop semantic abstractions that bridge languages, which is then activated during transfer training.
- Evidence anchors:
  - [section] "We observed a certain proportion of code-switching samples... These samples' model responses consist of tokens from multiple languages and are semantically coherent."
  - [section] "We have observed that code-switching occurs not only in the transfer process when Chinese is the target language, but also when other 13 low-resource languages are target languages."
- Break condition: If transfer training is conducted exclusively on monolingual data without any code-switching examples, the model might not exhibit this behavior.

### Mechanism 3
- Claim: Small-scale instruction tuning can significantly improve response quality without extensive pretraining.
- Mechanism: Fine-tuning on a relatively small amount of instruction data (hundreds of thousands of examples) improves the model's ability to follow instructions in the target language, even without large-scale further pretraining.
- Core assumption: The model's existing language generation capabilities can be adapted to follow instructions through instruction tuning, independent of knowledge level improvements from further pretraining.
- Evidence anchors:
  - [abstract] "enhancing LLaMA's response quality (i.e., language generation capability), requires only hundreds of thousands of instruction data rather than a large-scale further pretraining."
  - [section] "We find that further Chinese pretraining with 100 billion tokens or fewer is insufficient to significantly improve LLaMA's knowledge level. However, enhancing LLaMA's response quality... requires only hundreds of thousands of instruction data rather than a large-scale further pretraining."
- Break condition: If the instruction tuning data is too limited or not diverse enough, it may not effectively transfer instruction-following capabilities.

## Foundational Learning

- **Cross-lingual semantic alignment**: Understanding how multilingual models can transfer capabilities across languages without explicit vocabulary extension.
  - Why needed here: Explains how models handle multiple languages without vocabulary extension
  - Quick check question: How does the model's ability to handle tokens from multiple languages without vocabulary extension demonstrate cross-lingual semantic alignment?

- **Instruction tuning vs. further pretraining**: Distinguishing between improving response quality (instruction tuning) and knowledge level (further pretraining) in transfer scenarios.
  - Why needed here: Clarifies when to use instruction tuning versus additional pretraining
  - Quick check question: Why does instruction tuning improve response quality without necessarily increasing the model's knowledge level?

- **Code-switching as evidence of language transfer**: Recognizing code-switching in model outputs as an indicator of successful cross-lingual capability transfer.
  - Why needed here: Provides a diagnostic for successful language transfer
  - Quick check question: What does the presence of code-switching in model outputs suggest about the model's internal representation of language concepts?

## Architecture Onboarding

- **Component map**: LLaMA -> Vocabulary Extension (optional) -> Further Pretraining -> Instruction Tuning
- **Critical path**: Instruction tuning on target language instruction data provides the most significant improvement in response quality
- **Design tradeoffs**: Vocabulary extension vs. training on original vocabulary - vocabulary extension may not be necessary for small-scale transfer but could be beneficial for larger-scale pretraining. Multilingual joint training vs. monolingual transfer training - joint training preserves original language capabilities better.
- **Failure signatures**: Poor transfer performance may indicate insufficient instruction tuning data, inadequate target language pretraining, or the need for vocabulary extension in large-scale scenarios.
- **First 3 experiments**:
  1. Compare transfer performance with and without vocabulary extension on the original vocabulary with limited pretraining (0.5 billion tokens)
  2. Evaluate the impact of different scales of instruction tuning (1K, 5K, 950K examples) on response quality
  3. Assess code-switching rates in model outputs during transfer training across multiple target languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of language models change when extending vocabulary and further pretraining with scales beyond tens of billions of tokens, such as trillions of tokens?
- Basis in paper: [explicit] The paper mentions that vocabulary extension might not be suitable for small-scale incremental pretraining in the order of tens of billions of tokens, but does not negate its efficacy in settings involving larger-scale pretraining.
- Why unresolved: The paper's experiments focus on pretraining scales up to 100 billion tokens, and do not explore the impact of larger scales.
- What evidence would resolve it: Conducting experiments with vocabulary extension and further pretraining using scales in the trillions of tokens, and comparing the performance to models without vocabulary extension.

### Open Question 2
- Question: What is the impact of cross-lingual alignment on the performance of language models in code-switching scenarios, and how does it affect their ability to generate coherent and contextually appropriate responses?
- Basis in paper: [explicit] The paper observes instances of code-switching during transfer training, suggesting that cross-lingual alignment might be internalized within the model.
- Why unresolved: The paper does not investigate the impact of cross-lingual alignment on code-switching performance or the quality of generated responses.
- What evidence would resolve it: Conducting experiments to evaluate the performance of language models in code-switching scenarios, and analyzing the impact of cross-lingual alignment on the quality of generated responses.

### Open Question 3
- Question: How do language models' performance and capabilities change when fine-tuned on instruction-following tasks in multiple languages simultaneously, compared to fine-tuning on individual languages?
- Basis in paper: [inferred] The paper extends experiments to 13 low-resource languages and observes similar trends in performance improvement, suggesting that multilingual fine-tuning might have an impact on model capabilities.
- Why unresolved: The paper does not compare the performance of language models when fine-tuned on multiple languages simultaneously versus fine-tuning on individual languages.
- What evidence would resolve it: Conducting experiments to compare the performance of language models when fine-tuned on multiple languages simultaneously versus fine-tuning on individual languages, and analyzing the impact on their capabilities.

## Limitations
- Limited empirical validation across all 13 low-resource languages mentioned
- Qualitative rather than quantitative analysis of code-switching behavior
- Does not address potential catastrophic forgetting of original English capabilities

## Confidence

**High Confidence**: The finding that vocabulary extension is not necessary for small-scale transfer training (tens of billions of tokens) is well-supported by direct comparisons showing the original vocabulary outperforming extended vocabulary even with significantly less training data.

**Medium Confidence**: The observation of code-switching as evidence of cross-lingual alignment is supported by examples but lacks systematic analysis. The claim about instruction tuning requiring only hundreds of thousands of examples is supported but not thoroughly validated across different instruction complexities.

**Low Confidence**: The generalizability of these findings across all 13 mentioned low-resource languages is asserted but not empirically demonstrated. The specific thresholds where vocabulary extension becomes beneficial are not established.

## Next Checks

1. **Cross-linguistic validation**: Conduct systematic experiments across all 13 low-resource languages mentioned, measuring not just overall performance but specific aspects like knowledge retention, instruction following, and code-switching frequency to verify the claimed patterns hold universally.

2. **Quantitative code-switching analysis**: Implement automated detection of code-switching in model outputs, measuring its frequency, distribution across different instruction types, and correlation with downstream task performance to establish whether it's merely a curiosity or a reliable indicator of successful transfer.

3. **Catastrophic forgetting assessment**: Design controlled experiments comparing English capability retention between joint multilingual training versus sequential transfer training, with comprehensive evaluation on English benchmarks before and after transfer to quantify the trade-offs involved.