---
ver: rpa2
title: 'FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision'
arxiv_id: '2407.08608'
source_url: https://arxiv.org/abs/2407.08608
tags:
- attention
- arxiv
- speed
- forward
- wgmma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FlashAttention-3 introduces asynchronous GPU programming and FP8
  low-precision to achieve up to 2.0x speedup and 75% H100 utilization for attention
  layers. It uses warp-specialization to overlap memory movement and computation,
  interlaces softmax with block-wise GEMMs via a two-stage pipeline, and leverages
  FP8 Tensor Cores with block quantization and incoherent processing for both speed
  and accuracy.
---

# FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision

## Quick Facts
- arXiv ID: 2407.08608
- Source URL: https://arxiv.org/abs/2407.08608
- Reference count: 40
- Primary result: Achieves 1.5-2.0x speedup over FlashAttention-2 with 740 TFLOPs/s in FP16 and 1.2 PFLOPs/s in FP8

## Executive Summary
FlashAttention-3 introduces asynchronous GPU programming and FP8 low-precision to achieve significant speedups for attention layers. By exploiting the asynchrony of Tensor Cores and TMA, the method overlaps computation and data movement through warp-specialization and two-stage pipelining. The approach reaches 75% H100 utilization in FP16 and nearly 1.2 PFLOPs/s in FP8, with improved numerical accuracy through block quantization and incoherent processing.

## Method Summary
FlashAttention-3 implements warp-specialization where producer warps handle only TMA loads for K and V tiles while consumer warps execute WGMMA matmuls and softmax operations. A pingpong scheduling mechanism overlaps softmax computation with GEMM operations across warpgroups, while a two-stage pipeline further overlaps softmax within a single warpgroup with successive GEMM operations. For FP8 precision, block quantization and incoherent processing techniques reduce quantization error while maintaining high throughput.

## Key Results
- 1.5-2.0x speedup over FlashAttention-2 in FP16 with 740 TFLOPs/s (75% H100 utilization)
- Nearly 1.2 PFLOPs/s in FP8 with 2.6x lower numerical error than baseline FP8 attention
- Surpasses vendor libraries for long sequences (512 to 16k)
- Achieves 75% H100 utilization, approaching hardware limits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Warp-specialization with asynchronous producer-consumer roles hides memory and instruction latencies.
- Mechanism: The CTA splits into producer warps (only issuing TMA loads) and consumer warps (only issuing WGMMA and softmax). Producer warps perform preallocations and loads of K and V tiles, notifying consumers when data is ready. Consumers then execute the matmul and softmax operations on these tiles, releasing the buffer stage for reuse.
- Core assumption: TMA and WGMMA are truly asynchronous relative to the CUDA cores doing logic and computation.
- Evidence anchors: [abstract] "exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization" [section 3.1] "TMA for loads of Qi and {K_j, V_j} and WGMMA to execute the GEMMs" [corpus] Weak evidence; no direct citations to hardware specs for Hopper TMA/WGMMA asynchrony in related work.
- Break condition: If TMA or WGMMA become synchronous, the producer-consumer split offers no latency hiding.

### Mechanism 2
- Claim: Pingpong scheduling overlaps softmax and GEMM operations across warpgroups.
- Mechanism: Two warpgroups are scheduled such that when one group executes GEMM (WGMMA), the other executes softmax. Barrier synchronization (bar.sync) forces this order, ensuring that the low-throughput softmax (exp) is computed while the Tensor Cores are busy.
- Core assumption: The softmax computation is much slower than GEMM and can be overlapped.
- Evidence anchors: [section 3.1] "overlap the softmax computation of one warpgroup with the GEMM of another warpgroup" [section 2.2] "the exponential has 256x lower throughput, so exponential can take 50% of the cycle compared to matmul" [corpus] No direct evidence; inference based on FLOPS comparison in the paper.
- Break condition: If softmax becomes faster relative to GEMM (e.g., via hardware support), pingpong gains diminish.

### Mechanism 3
- Claim: 2-stage pipelining overlaps softmax within a single warpgroup with successive GEMM operations.
- Mechanism: The consumer warpgroup maintains two stages: while the first WGMMA of iteration j+1 is issued, the softmax of iteration j is being computed. Intermediate results are buffered in registers to break sequential dependencies.
- Core assumption: Register pressure can be managed to hold S_next and other intermediates.
- Evidence anchors: [section 3.2] "overlap some instructions in the softmax with some instructions in the GEMMs" [section 3.2] "We use WGMMA as a metonym for asynchronous GEMM. Within the mainloop... the second WGMMA operation of iteration j (line 11) is overlapped with softmax operations from iteration j+1" [corpus] No direct citations; described as novel contribution in the paper.
- Break condition: If register pressure forces spilling, performance degrades.

## Foundational Learning

- Concept: GPU memory hierarchy (HBM, L2, SMEM, registers)
  - Why needed here: FlashAttention-3 reduces HBM traffic by keeping intermediate tensors in SMEM and registers
  - Quick check question: Why does minimizing HBM reads/writes improve performance?

- Concept: Tensor Memory Accelerator (TMA) and Tensor Cores (WGMMA) asynchrony
  - Why needed here: The algorithm relies on TMA/WGMMA being asynchronous to overlap loads with computation
  - Quick check question: What would happen if TMA or WGMMA were synchronous?

- Concept: Low-precision number formats (FP8) and quantization error
  - Why needed here: FP8 enables higher throughput but introduces quantization error that must be managed
  - Quick check question: Why does block quantization help reduce quantization error?

## Architecture Onboarding

- Component map: Producer warpgroup -> TMA loads (Q, K, V tiles) -> Buffer (SMEM) -> Consumer warpgroup -> WGMMA matmuls -> softmax -> second WGMMA -> rescale
- Critical path: TMA load → WGMMA matmul → softmax → second WGMMA → rescale
- Design tradeoffs:
  - Larger block sizes reduce HBM traffic but increase register pressure
  - More warpgroups improve overlap but may increase scheduling overhead
  - FP8 increases throughput but requires careful quantization (block quantization, incoherent processing)
- Failure signatures:
  - Low TFLOPS/s → likely register spilling or poor occupancy
  - Numerical instability → softmax scaling or quantization errors
  - OOM → block sizes too large for SMEM
- First 3 experiments:
  1. Profile with small block sizes to confirm register pressure is not an issue
  2. Measure TMA/WGMMA latency to confirm asynchrony assumptions
  3. Compare FP16 vs FP8 throughput to validate low-precision gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FlashAttention-3 scale on GPUs with different architectures (e.g., Blackwell, Ada Lovelace) that may have different Tensor Core and TMA capabilities?
- Basis in paper: [inferred] The paper focuses on Hopper GPUs but mentions that techniques may apply to other architectures
- Why unresolved: The paper only benchmarks on H100 SXM5, so comparative results on other architectures are not provided
- What evidence would resolve it: Benchmarking FlashAttention-3 on different GPU architectures and comparing performance metrics like TFLOPs/s and speedup ratios

### Open Question 2
- Question: What is the impact of FlashAttention-3 on training stability and convergence for large language models, especially with FP8 precision?
- Basis in paper: [inferred] The paper mentions limitations including understanding the effects of low-precision attention in large-scale training
- Why unresolved: The paper focuses on inference speed and accuracy but does not explore training dynamics
- What evidence would resolve it: Training large language models using FlashAttention-3 with FP8 precision and comparing convergence rates, final accuracy, and stability metrics against baseline training methods

### Open Question 3
- Question: How does the choice of block size and warp specialization parameters affect the performance and memory usage of FlashAttention-3?
- Basis in paper: [explicit] The paper mentions trade-offs between tile size and pipeline depth, and register pressure affecting block size choices
- Why unresolved: The paper does not provide an exhaustive exploration of parameter space or guidelines for optimal parameter selection
- What evidence would resolve it: Systematic experiments varying block sizes and warp specialization parameters, measuring performance, memory usage, and sensitivity analysis to identify optimal configurations for different use cases

## Limitations
- The true hardware asynchrony of TMA and WGMMA operations on Hopper GPUs remains unverified
- Numerical accuracy claims for FP8 require further validation across diverse attention scenarios
- Implementation complexity of incoherent processing and block quantization may limit reproducibility

## Confidence
- High confidence: FP16 performance claims (740 TFLOPs/s, 75% H100 utilization) are well-supported by systematic benchmarking
- Medium confidence: FP8 speedups and accuracy improvements, as they depend on architectural assumptions and novel quantization techniques
- Low confidence: Exact register allocation strategy and incoherent processing implementation details, which are not fully specified

## Next Checks
1. Profile TMA and WGMMA latencies on Hopper hardware to confirm asynchrony assumptions
2. Implement and test incoherent processing for FP8 attention across diverse sequence lengths and value distributions
3. Validate numerical accuracy of FP8 attention on long sequences with high dynamic range values compared to FP16 baselines