---
ver: rpa2
title: 'DebateQA: Evaluating Question Answering on Debatable Knowledge'
arxiv_id: '2408.01419'
source_url: https://arxiv.org/abs/2408.01419
tags:
- answer
- question
- answers
- questions
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces DebateQA, a dataset of 2,941 debatable questions\
  \ with multiple human-annotated partial answers. It develops two metrics: Perspective\
  \ Diversity, which evaluates answer comprehensiveness by measuring the perplexity\
  \ of generating partial answers, and Dispute Awareness, which checks if the model\
  \ acknowledges the question\u2019s debatable nature."
---

# DebateQA: Evaluating Question Answering on Debatable Knowledge

## Quick Facts
- arXiv ID: 2408.01419
- Source URL: https://arxiv.org/abs/2408.01419
- Reference count: 40
- Introduces DebateQA dataset with 2,941 debatable questions and develops metrics for evaluating comprehensive, multi-perspective answers

## Executive Summary
This paper introduces DebateQA, a novel dataset of 2,941 debatable questions designed to evaluate question answering systems on their ability to provide comprehensive answers that acknowledge multiple perspectives. The dataset includes human-annotated partial answers for each question, capturing the nuanced nature of debatable topics. The authors develop two evaluation metrics - Perspective Diversity and Dispute Awareness - to measure how well models can generate comprehensive answers and recognize the debatable nature of questions, respectively.

## Method Summary
The authors constructed DebateQA by first collecting 2,941 debatable questions from diverse sources. For each question, they obtained multiple human-annotated partial answers representing different perspectives on the topic. Two evaluation metrics were developed: Perspective Diversity, which measures the perplexity of generating diverse partial answers to assess comprehensiveness, and Dispute Awareness, which evaluates whether models can recognize and acknowledge the debatable nature of questions. The metrics were validated against human preferences through controlled experiments.

## Key Results
- Both Perspective Diversity and Dispute Awareness metrics align with human preferences and show stability across different models
- Evaluation of 12 LLMs revealed varying capabilities in providing comprehensive answers with diverse perspectives, despite generally good performance in recognizing debate
- The metrics successfully capture the multi-faceted nature of debatable questions and can differentiate between models based on their ability to handle nuanced topics

## Why This Works (Mechanism)
The approach works by explicitly modeling the multi-perspective nature of debatable questions through human-annotated partial answers, creating a ground truth for what constitutes comprehensive coverage. The perplexity-based metric for Perspective Diversity effectively captures the diversity of generated responses by measuring how well models can reproduce the variety of human perspectives. The Dispute Awareness metric leverages the inherent tension in debatable questions to test whether models can appropriately signal uncertainty and acknowledge multiple valid viewpoints.

## Foundational Learning
- Perplexity measurement: why needed - quantifies language model uncertainty; quick check - lower perplexity indicates better model confidence
- Partial answer annotation: why needed - captures multiple valid perspectives; quick check - at least 3 distinct viewpoints per question
- Debatable question identification: why needed - distinguishes from factual questions; quick check - questions have no single correct answer

## Architecture Onboarding

**Component Map:**
Dataset Collection -> Partial Answer Annotation -> Metric Development -> Model Evaluation -> Human Validation

**Critical Path:**
Question collection → Partial answer generation → Metric formulation → LLM evaluation → Human preference validation

**Design Tradeoffs:**
- Dataset size vs. annotation quality: 2,941 questions with multiple high-quality human annotations
- Metric complexity vs. interpretability: perplexity-based measures are theoretically sound but require careful implementation
- Domain coverage vs. depth: focused on social sciences and humanities for meaningful debate

**Failure Signatures:**
- Metrics fail if partial answers are not sufficiently diverse
- Performance degrades if questions lack genuine debatable aspects
- Results may not generalize to factual question answering domains

**First 3 Experiments:**
1. Evaluate metric alignment with human preferences on subset of questions
2. Test metric stability across different LLM architectures
3. Compare metric sensitivity to model performance changes

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Metric validation depends heavily on specific LLM implementations (LLaMA-3-8B and GPT-4) that may not generalize
- Human evaluation sample size (48 questions) may not capture full diversity of debatable questions
- Perplexity-based metric may capture superficial lexical variation rather than genuine semantic diversity

## Confidence
- Core metric claims: Medium-High
- Dataset quality: Medium-High
- Generalization across domains: Low-Medium

## Next Checks
1. Replicate the metric validation across at least five different LLM families (including non-transformer architectures) to test generalization
2. Conduct human evaluations on a separate validation set of 100+ questions from diverse domains to verify metric alignment
3. Compare the metrics' sensitivity to model performance changes against established QA benchmarks under controlled conditions