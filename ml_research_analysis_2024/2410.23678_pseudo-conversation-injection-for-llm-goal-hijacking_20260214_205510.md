---
ver: rpa2
title: Pseudo-Conversation Injection for LLM Goal Hijacking
arxiv_id: '2410.23678'
source_url: https://arxiv.org/abs/2410.23678
tags:
- hijacking
- pseudo-conversation
- goal
- prompt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses goal hijacking in Large Language Models (LLMs),
  where malicious inputs can manipulate models into generating unintended outputs.
  It introduces Pseudo-Conversation Injection, a method that exploits LLMs' weak role
  identification in dialogues by fabricating conversations to deceive models into
  executing malicious instructions.
---

# Pseudo-Conversation Injection for LLM Goal Hijacking

## Quick Facts
- **arXiv ID**: 2410.23678
- **Source URL**: https://arxiv.org/abs/2410.23678
- **Authors**: Zheng Chen; Buhui Yao
- **Reference count**: 20
- **Primary result**: Novel method exploits LLM role confusion through fabricated conversations, achieving 6.3-15.2% improvement in goal hijacking success rates over baselines

## Executive Summary
This study addresses the critical security vulnerability of goal hijacking in Large Language Models, where malicious inputs can manipulate models into executing unintended actions. The researchers introduce Pseudo-Conversation Injection, a method that exploits LLMs' weak role identification capabilities by fabricating artificial conversations to deceive models into executing malicious instructions. The approach demonstrates significant effectiveness across multiple platforms, revealing fundamental weaknesses in how LLMs process conversational context and maintain role awareness during interactions.

## Method Summary
The researchers developed Pseudo-Conversation Injection as a targeted attack methodology that constructs artificial conversational contexts to manipulate LLM behavior. Three construction strategies were proposed: Targeted Pseudo-Conversations (optimized for specific attack goals), Universal Pseudo-Conversations (broadly applicable across domains), and Robust Pseudo-Conversations (designed to maintain effectiveness under varying conditions). These methods were evaluated on major LLM platforms including ChatGPT and Qwen, with success measured by the model's compliance with injected malicious instructions. The approach leverages the observation that LLMs often fail to properly distinguish between genuine and fabricated conversational roles, allowing attackers to craft inputs that bypass normal safety mechanisms.

## Key Results
- Success rates increased by 6.3% to 15.2% compared to baseline attack methods across tested platforms
- Attack effectiveness varies significantly between platforms, with Qwen showing higher vulnerability than ChatGPT
- All three construction strategies (Targeted, Universal, Robust) demonstrated measurable improvements over baseline approaches
- The method successfully exploits fundamental weaknesses in LLM role identification during conversational processing

## Why This Works (Mechanism)
The attack succeeds by exploiting LLMs' inability to reliably distinguish between authentic and fabricated conversational roles. When presented with a constructed dialogue context, models often process the entire conversation as legitimate context, failing to recognize that certain participants or statements are artificial constructs. This role confusion allows attackers to embed malicious instructions within fabricated conversational turns that the model subsequently executes as if they were part of normal dialogue flow.

## Foundational Learning
- **Role Identification in LLMs**: Understanding how models track and differentiate between conversational participants - critical because this is the primary attack vector being exploited
- **Conversational Context Processing**: How LLMs integrate and prioritize dialogue history - essential for understanding how fabricated conversations can influence model behavior
- **Goal Hijacking Attacks**: General framework for manipulating LLM outputs through adversarial inputs - provides context for the specific contribution of pseudo-conversation techniques
- **Safety Alignment Mechanisms**: How models are trained to recognize and resist malicious instructions - important for understanding why role confusion represents an effective bypass
- **Platform-Specific Model Architectures**: Differences in how ChatGPT vs Qwen process conversational context - explains the observed variance in attack effectiveness
- **Evaluation Metrics for Adversarial Attacks**: Methods for quantifying attack success in LLM security research - necessary for interpreting the reported improvements

## Architecture Onboarding

**Component Map**: Input Prompt -> Conversation Construction Module -> LLM Platform -> Output Generation -> Success Evaluation

**Critical Path**: The attack flow follows: malicious instruction design → pseudo-conversation construction → injection into prompt → LLM processing → compliance measurement. The conversation construction module is the critical component, as its quality directly determines attack success rates.

**Design Tradeoffs**: Targeted approaches offer higher precision but require model-specific tuning, while universal approaches sacrifice some effectiveness for broader applicability. Robust variants add defensive mechanisms against detection but increase construction complexity.

**Failure Signatures**: Attack failures manifest as the model rejecting the injected instruction, identifying the conversation as artificial, or maintaining the original user intent despite the fabricated context. Low success rates indicate strong role identification capabilities.

**First Experiments**:
1. Test baseline success rates without conversation injection to establish control measurements
2. Evaluate individual construction strategies (Targeted, Universal, Robust) separately to identify optimal approach
3. Cross-platform validation on additional LLM architectures beyond ChatGPT and Qwen

## Open Questions the Paper Calls Out
None

## Limitations
- Attack effectiveness varies significantly across platforms, suggesting model-specific defenses may limit generalizability
- Study focuses on success rates without comprehensive analysis of false positives or benign conversation disruption
- Does not address potential mitigation strategies or defensive mechanisms against this attack vector
- Evaluation limited to specific platforms without broader validation across diverse LLM architectures

## Confidence

**High confidence**: Core observation that LLMs exhibit weak role identification in conversational contexts is well-supported by experimental results across multiple platforms.

**Medium confidence**: Comparative effectiveness of the three construction strategies is demonstrated, though universal approach's broad applicability needs further validation.

**Medium confidence**: Claim that this represents a critical security vulnerability is reasonable given results, but real-world exploitability and countermeasures remain unexplored.

## Next Checks
1. Test attack methodology across additional LLM architectures (Claude, Gemini, LLaMA) to establish generalizability beyond ChatGPT and Qwen
2. Evaluate whether role identification weaknesses persist in models with enhanced safety fine-tuning or alignment protocols
3. Conduct user studies to determine whether injected conversations maintain coherence and believability to human observers, affecting practical attack feasibility