---
ver: rpa2
title: Large Language Models As Evolution Strategies
arxiv_id: '2402.18381'
source_url: https://arxiv.org/abs/2402.18381
tags:
- arxiv
- context
- evollm
- prompt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can be prompted to act as evolution strategies
  for black-box optimization. The method, EvoLLM, uses integer-discretized solution
  representations and fitness-sorted context sequences to induce the LLM to propose
  distribution mean updates.
---

# Large Language Models As Evolution Strategies

## Quick Facts
- **arXiv ID**: 2402.18381
- **Source URL**: https://arxiv.org/abs/2402.18381
- **Reference count**: 40
- **Primary result**: LLMs can implement evolution strategies through in-context learning, with smaller models often outperforming larger ones on black-box optimization tasks

## Executive Summary
This paper introduces EvoLLM, a novel approach that uses large language models as evolution strategies for black-box optimization. The method leverages the in-context learning capabilities of LLMs by constructing prompts with fitness-sorted, discretized solution candidates and querying the LLM to propose improvements to the distribution mean. The approach is demonstrated on benchmark functions and small neuroevolution tasks, showing competitive performance against traditional optimization algorithms while highlighting interesting phenomena such as the inverse relationship between model size and optimization performance.

## Method Summary
EvoLLM uses LLMs to implement evolution strategies through a prompting strategy that involves discretizing solution candidates into integers, maintaining a context buffer of fitness-sorted generations, and querying the LLM to propose mean updates for the next generation. The approach uses least-to-most sorting of discretized population members and constructs prompts asking the LLM to improve the mean statistic. The method can be extended through instruction fine-tuning on teacher optimization trajectories, and includes batching strategies to handle larger search spaces. Experiments demonstrate effectiveness on BBOB benchmark functions and neuroevolution control tasks, with performance comparisons to random search, Gaussian Hill Climbing, and SNES baselines.

## Key Results
- EvoLLM outperforms random search and Gaussian Hill Climbing on BBOB benchmark functions (Sphere, Rosenbrock, Discus, Schwefel)
- Smaller LLM models (PaLM-XS, Llama2-7B) generally outperform larger ones (PaLM-L, Llama2-70B) for this task
- Instruction fine-tuning on teacher optimization trajectories provides small but robust performance improvements
- EvoLLM shows competitive performance on neuroevolution control tasks (CartPole-v1, Acrobot-v1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can implement recombination operations for evolution strategies through in-context learning
- Mechanism: The LLM processes fitness-sorted sequences of discretized solutions and proposes an improved mean statistic for the next generation
- Core assumption: The self-attention mechanism in Transformers can effectively model set operations needed for evolutionary recombination
- Evidence anchors: [abstract] "we introduce a novel prompting strategy, consisting of least-to-most sorting of discretized population members and querying the LLM to propose an improvement to the mean statistic"; [section 4] "LLMs are capable of implementing a plethora of so-called in-context learning algorithms"
- Break condition: If context length becomes too long or discretization resolution is too coarse/fine, the LLM may fail to infer meaningful improvements

### Mechanism 2
- Claim: Smaller LLM models generally outperform larger ones for this task
- Mechanism: The in-context learning capability may be more efficient in smaller models due to reduced computational overhead or different optimization of the training objective
- Core assumption: There is an inverse relationship between model size and in-context optimization performance for this specific task
- Evidence anchors: [section 5.1] "We observe that the LLM model size inversely affects the downstream performance of EvoLLM. Larger models (PaLM-L, Llama2-70B) tend to perform worse than smaller models (PaLM-XS, Llama2-7B)"
- Break condition: If task complexity increases significantly or requires more complex reasoning, larger models may eventually outperform smaller ones

### Mechanism 3
- Claim: Instruction fine-tuning on teacher optimization trajectories can improve EvoLLM performance
- Mechanism: Fine-tuning the LLM on sequences generated by established optimization algorithms provides additional guidance for the in-context learning process
- Core assumption: The LLM can effectively learn from and generalize optimization patterns demonstrated by teacher algorithms
- Evidence anchors: [section 7] "we use two BBOB functions (Sphere and Rosenbrock) and collect BBO rollouts using a simple Gaussian Hill Climbing algorithm"; [section 7] "For all cases, we observe a small but robust performance increase after instruction-based fine-tuning"
- Break condition: If fine-tuning data is insufficient or teacher algorithm is too dissimilar from target optimization tasks, improvement may not materialize

## Foundational Learning

- **Concept: In-context learning in LLMs**
  - Why needed here: The entire EvoLLM approach relies on the LLM's ability to learn and improve from context without weight updates
  - Quick check question: Can you explain the difference between in-context learning and traditional fine-tuning in LLMs?

- **Concept: Evolution strategies and black-box optimization**
  - Why needed here: Understanding fundamental concepts of ES and BBO is crucial for designing prompt strategy and interpreting results
  - Quick check question: What is the key difference between evolution strategies and genetic algorithms in terms of solution representation?

- **Concept: Transformer architecture and self-attention**
  - Why needed here: The self-attention mechanism is the core component that enables the LLM to process and improve upon the optimization context
  - Quick check question: How does the permutation invariance property of self-attention relate to its suitability for set operations in evolutionary algorithms?

## Architecture Onboarding

- **Component map**: Base LLM model -> Discretization module -> Context buffer -> Prompt construction -> LLM query -> Sampling and evaluation

- **Critical path**: 
  1. Warm-up with random search to fill context buffer
  2. Discretize and sort context generations and candidates
  3. Construct prompt with fitness improvement query
  4. Query LLM for mean update
  5. Parse LLM output and sample new candidates
  6. Evaluate candidates and update context buffer

- **Design tradeoffs**:
  - Model size vs. performance: Smaller models often outperform larger ones
  - Discretization resolution: Must balance between precision and tokenization issues
  - Context length vs. scalability: Batching dimensions to handle larger search spaces
  - Selection and sorting strategies: Different approaches can significantly impact performance

- **Failure signatures**:
  - LLM output fails to parse into a valid mean update
  - Performance plateaus or degrades after initial improvements
  - Inconsistent results across different runs or problem instances
  - Context length grows too large, causing LLM to output non-informative information

- **First 3 experiments**:
  1. Run EvoLLM on a simple 2D Sphere function with default settings to verify basic functionality
  2. Compare performance of different LLM base models (e.g., PaLM-XS vs. Llama2-7B) on the same problem
  3. Test impact of different discretization resolutions on optimization performance for a moderately conditioned function like Rosenbrock

## Open Questions the Paper Calls Out

- **Open Question 1**: How do different pretraining and fine-tuning protocols of LLMs affect their performance as evolution strategies?
  - Basis in paper: [explicit] The paper mentions that "We expect EvoLLM's performance to differ based on pretraining & fine-tuning protocols" and that "Understanding how these details affect BBO performance is a key open challenge"
  - Why unresolved: The paper does not investigate or compare different pretraining or fine-tuning approaches for the LLMs used in EvoLLM
  - What evidence would resolve it: Systematic experiments comparing EvoLLM performance using LLMs with different pretraining objectives, architectures, or fine-tuning methods

- **Open Question 2**: Can EvoLLM be extended to non-isotropic evolution strategies that adapt both the mean and covariance matrix of the search distribution?
  - Basis in paper: [inferred] The paper mentions preliminary experiments with non-isotropic ES where the LLM had to output updates to the diagonal covariance, but this did not yield significant improvements
  - Why unresolved: The paper does not provide detailed results or analysis of these preliminary experiments with non-isotropic ES
  - What evidence would resolve it: Detailed experiments showing EvoLLM's performance on non-isotropic ES tasks, comparing it to standard isotropic ES and other covariance matrix adaptation methods

- **Open Question 3**: What is the optimal tokenization strategy for numerical representations in EvoLLM, and how does it impact performance?
  - Basis in paper: [explicit] The paper states that "One promising future direction may want to explore tokenization techniques tailored for numerical representations"
  - Why unresolved: The paper uses a simple integer discretization approach but does not explore more sophisticated tokenization methods for numerical values
  - What evidence would resolve it: Experiments comparing EvoLLM performance using different tokenization strategies for numerical values and analysis of how tokenization affects the LLM's ability to learn improvement patterns

## Limitations

- **Tokenization and Discretization Sensitivity**: The approach relies heavily on precise integer discretization of floating-point solutions, but the paper provides minimal detail on how this mapping is performed
- **Limited Task Complexity**: Experiments are conducted on relatively simple benchmark functions and basic control tasks, not demonstrating effectiveness on problems requiring more than 100 dimensions
- **Context Length and Batching Scalability**: The fundamental limitation of context length remains unaddressed, with proposed batching solutions described but not empirically validated for high-dimensional problems

## Confidence

- **High Confidence**: The core claim that LLMs can implement evolution strategies through in-context learning is well-supported
- **Medium Confidence**: The observation that smaller models generally outperform larger ones is consistently observed but underlying reasons remain speculative
- **Low Confidence**: The scalability claims beyond tested dimensions and effectiveness of instruction fine-tuning are not well-established

## Next Checks

1. **Tokenization Analysis**: Conduct controlled experiments varying discretization resolution systematically (e.g., 50, 100, 1000, 10000 bins) on the same problems to isolate impact of representation quality from model capability

2. **Scaling Challenge**: Test EvoLLM on higher-dimensional problems (500-1000 dimensions) from the BBOB suite, particularly those with complex conditioning, to evaluate whether batching strategies adequately address context length limitations

3. **Teacher Algorithm Ablation**: Perform systematic ablation studies on the instruction fine-tuning component by testing different teacher algorithms and varying the amount of fine-tuning data to determine which components are most impactful