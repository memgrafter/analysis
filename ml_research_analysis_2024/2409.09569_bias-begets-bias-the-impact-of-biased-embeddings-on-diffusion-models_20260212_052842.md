---
ver: rpa2
title: 'Bias Begets Bias: The Impact of Biased Embeddings on Diffusion Models'
arxiv_id: '2409.09569'
source_url: https://arxiv.org/abs/2409.09569
tags:
- images
- bias
- prompt
- diffusion
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the impact of embedding bias on diffusion-based
  text-to-image models. It proposes new statistical fairness criteria for generative
  models and proves that biased prompt embeddings lead to representationally imbalanced
  generations, both theoretically and empirically.
---

# Bias Begets Bias: The Impact of Biased Embeddings on Diffusion Models

## Quick Facts
- arXiv ID: 2409.09569
- Source URL: https://arxiv.org/abs/2409.09569
- Reference count: 40
- This paper studies the impact of embedding bias on diffusion-based text-to-image models, proposing fairness criteria and bias mitigation methods.

## Executive Summary
This paper investigates how biased text embeddings propagate through diffusion models to create representationally imbalanced image generations. The authors develop a theoretical framework proving that biased prompt embeddings lead to biased generations, and empirically demonstrate gender bias in CLIP embeddings. They introduce new fairness metrics for generative models including representational balance, multi-accuracy, and multi-calibration. The study also examines how embedding bias affects prompt-image alignment evaluation and proposes mitigation strategies like subclass-score and average-then-score methods.

## Method Summary
The paper combines theoretical analysis with empirical experiments to study bias propagation in diffusion models. The theoretical component uses diffusion model properties and Lipschitz assumptions to prove that biased embeddings lead to biased generations. Empirically, the authors collect CLIP embedding data for occupations, calculate gender bias ratios using cosine similarity, and implement bias mitigation methods. They train a custom diffusion model on synthetic images to validate their theoretical findings and evaluate the effectiveness of proposed fairness interventions on alignment auditing functions.

## Key Results
- Biased prompt embeddings lead to representationally imbalanced generations, proven theoretically and validated empirically
- CLIP embeddings show significant gender bias in both text-text and text-image similarities
- The subclass-score method significantly outperforms other approaches in alleviating gender bias in alignment scores
- Multi-accuracy and multi-calibration metrics provide effective tools for fair alignment auditing

## Why This Works (Mechanism)
The mechanism relies on the fundamental property that diffusion models operate by gradually denoising random noise based on text embeddings. When embeddings contain bias, this bias propagates through the denoising process, amplifying representational imbalances in the generated images. The theoretical framework shows that if embedding similarities between prompts and attributes are biased, the resulting generations will reflect these biases in their distributions. The mitigation methods work by adjusting how alignment scores are computed across different demographic groups, either by computing separate scores for each group (subclass-score) or by averaging embeddings before scoring (average-then-score).

## Foundational Learning
- **Diffusion Model Denoising Process**: Why needed - Understanding how text embeddings guide image generation through iterative denoising; Quick check - Verify that embeddings directly influence each denoising step
- **Representationally Balanced Generations**: Why needed - Framework for defining fairness in generative outputs; Quick check - Calculate TV distance between demographic group distributions in generated images
- **Multicalibration**: Why needed - Extending calibration concepts to multiple protected attributes in multimodal settings; Quick check - Verify that model predictions are calibrated across all attribute-group combinations
- **Lipschitz Continuity**: Why needed - Mathematical property ensuring smooth transitions in embedding space; Quick check - Confirm that embedding transformations satisfy Lipschitz conditions
- **Alignment Auditing Functions**: Why needed - Methods for evaluating how well text prompts match generated images; Quick check - Compare alignment scores across different demographic groups
- **Subclass Score Methods**: Why needed - Technique for computing fair alignment scores across demographic subgroups; Quick check - Implement subclass scoring and verify improved fairness metrics

## Architecture Onboarding

Component Map:
CLIP Embeddings -> Bias Analysis -> Diffusion Model Training -> Generation Validation -> Fairness Metrics

Critical Path:
1. Collect CLIP embeddings for prompts and attributes
2. Calculate bias metrics (cosine similarity ratios)
3. Train diffusion model on synthetic images
4. Generate images using biased and debiased embeddings
5. Evaluate representational balance and alignment fairness

Design Tradeoffs:
- Theoretical rigor vs. empirical practicality in fairness definitions
- Model complexity vs. interpretability in bias mitigation methods
- Generalizability vs. specificity in bias detection approaches

Failure Signatures:
- High TV distance values indicating strong bias in prompt embeddings
- Subclass-score not improving alignment fairness despite implementation
- Inconsistent results across different embedding models or datasets

First Experiments:
1. Reproduce theoretical analysis using Theorem 4.5 proof framework with Lipschitz assumptions
2. Implement CLIP embedding collection and gender bias calculation for occupations dataset
3. Evaluate subclass-score method on collected embeddings to verify bias mitigation

## Open Questions the Paper Calls Out
**Open Question 1**
How can we develop sufficient conditions for fairness in multimodal embeddings that go beyond the necessary conditions established in this paper?
Basis in paper: The paper discusses necessary conditions for fairness but acknowledges the need for sufficient conditions, referencing Dwork et al. (2011) as a potential direction.
Why unresolved: The paper focuses on establishing necessary conditions for fairness, leaving the development of sufficient conditions as an open question for future research.
What evidence would resolve it: A formal mathematical framework that defines and proves sufficient conditions for fairness in multimodal embeddings, potentially extending or adapting existing fairness definitions from other domains.

**Open Question 2**
What are the most effective methods for mitigating bias in alignment auditing functions when true alignment scores are not available?
Basis in paper: The paper proposes methods like subclass-score and average-then-score to mitigate bias in alignment auditing, but acknowledges limitations and the need for further research.
Why unresolved: The proposed methods show promise but are not perfect solutions, and the paper highlights the challenge of evaluating auditing functions without access to true alignment scores.
What evidence would resolve it: Empirical studies comparing the effectiveness of different bias mitigation methods across various datasets and alignment auditing functions, demonstrating improvements in fairness metrics.

**Open Question 3**
How can we address the epistemic challenge of identifying true attributes (e.g., race, gender) for artificial generations in fairness auditing?
Basis in paper: The paper acknowledges the fundamental challenge of identifying true labels for generated images and proposes group fairness criteria that bypass these labels, but recognizes the need for a "sound resolution" to this issue.
Why unresolved: The lack of ground truth for generated images makes it difficult to define and measure fairness in a consistent and meaningful way, as discussed in the paper's conclusion.
What evidence would resolve it: Development of a robust methodology for attribute identification in generated images that balances accuracy, fairness, and practicality, potentially involving human annotation, model-based approaches, or a combination of both.

## Limitations
- Theoretical framework assumes Lipschitz conditions and distinct category separation which may not hold for all real-world embedding spaces
- Empirical evaluation relies on CLIP embeddings and a custom-trained diffusion model without complete architectural details for full reproducibility
- Study focuses primarily on gender bias, potentially missing other forms of representational imbalance
- Proposed mitigation methods may not generalize well to other types of biases or embedding models

## Confidence
- **High confidence**: Theoretical proof of bias propagation in diffusion models (Theorem 4.5) and empirical demonstration of gender bias in CLIP embeddings
- **Medium confidence**: Effectiveness of subclass-score and average-then-score mitigation methods, as results are based on a single diffusion model and embedding type
- **Low confidence**: Generalization of fairness criteria and mitigation strategies to other forms of bias beyond gender, due to limited empirical validation

## Next Checks
1. Implement the complete custom diffusion model training pipeline using provided architectural details and validate against reported TV distance metrics
2. Test subclass-score and average-then-score methods on additional embedding models (e.g., ALIGN, OpenCLIP) to assess generalizability of bias mitigation
3. Extend fairness evaluation framework to include intersectional bias analysis (e.g., gender Ã— profession combinations) and validate with human perceptual studies