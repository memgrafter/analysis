---
ver: rpa2
title: 'Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation'
arxiv_id: '2404.07053'
source_url: https://arxiv.org/abs/2404.07053
tags:
- metaphor
- language
- metaphorical
- computational
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Meta4XNLI, the first parallel dataset for
  Natural Language Inference (NLI) newly annotated for metaphor detection and interpretation
  in English and Spanish. The authors developed the dataset by leveraging XNLI and
  esXNLI, then annotating metaphors at both token and sentence levels using MIPVU
  guidelines.
---

# Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation

## Quick Facts
- **arXiv ID**: 2404.07053
- **Source URL**: https://arxiv.org/abs/2404.07053
- **Reference count**: 40
- **Primary result**: First parallel dataset for metaphor detection and interpretation across English and Spanish, showing MLMs outperform LLMs for metaphor detection and that metaphorical language reduces NLI performance

## Executive Summary
This paper introduces Meta4XNLI, a parallel corpus of 13,320 sentences newly annotated for metaphor detection and interpretation in English and Spanish. The dataset extends XNLI and esXNLI by adding token-level metaphor annotations following MIPVU guidelines and premise-hypothesis pairs for NLI-based metaphor interpretation. The authors conduct comprehensive experiments comparing encoder-based models (MLMs) and decoder-based models (LLMs) across multiple training scenarios, finding that MLMs consistently outperform LLMs in metaphor detection while both model types struggle with metaphor interpretation. The work also analyzes how translation affects metaphor preservation and cross-lingual transfer capabilities.

## Method Summary
The Meta4XNLI dataset was created through a semi-automatic annotation process that leveraged XNLI and esXNLI. Spanish sentences were first annotated using MIPVU guidelines for metaphor detection at the token level. These annotations were then projected onto English sentences using EasyLabel-Projection and word alignments, followed by manual refinement. For metaphor interpretation, premise-hypothesis pairs were created and annotated for metaphor presence. The dataset was used to fine-tune both encoder-only models (mDeBERTa, XLM-RoBERTa) and decoder-only models (LLaMA-3.1-8B-Instruct, Qwen2.5-7B-Instruct, gemma-7B-it) for metaphor detection and NLI tasks. Experiments evaluated monolingual, multilingual, cross-domain, and zero-shot cross-lingual transfer performance.

## Key Results
- Fine-tuned MLMs achieve 0.83 F1 for metaphor detection in Spanish compared to 0.79 F1 for decoder-only LLMs
- NLI accuracy decreases when metaphorical expressions are present in premise-hypothesis pairs
- Cross-lingual transfer performance varies significantly, with translation direction and metaphor preservation affecting results
- Best metaphor detection results occur when models are trained on data from both languages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Parallel annotations enable direct cross-lingual transfer of metaphor detection and interpretation.
- **Mechanism**: The dataset is constructed by projecting metaphor labels from Spanish sentences onto their English translations using word alignments and data transfer, then manually refining these projections. This preserves metaphor presence and allows models trained on one language to generalize to the other.
- **Core assumption**: Metaphors in source and target sentences maintain the same token-level representations after translation.
- **Evidence anchors**:
  - [abstract] "translation plays an important role in the preservation or loss of metaphors across languages, introducing shifts that might impact metaphor occurrence and model performance."
  - [section] "The annotation process consists of four phases, as depicted in Figure 3... The first step involves the projection of ES labels onto EN sentences using EasyLabel-Projection... This technique is appropriate when the labeled entity is certainly appearing in both source and target sentences."
  - [corpus] "Cross-lingual Transfer... we found that one of the primary factors contributing to metaphor transference is the translation process... instances where metaphors present in the original version were expressed literally in the translation."
- **Break condition**: If translation alters the metaphorical meaning or removes metaphors entirely, cross-lingual transfer fails and models trained on one language will underperform on the other.

### Mechanism 2
- **Claim**: Fine-tuned MLMs outperform decoder-only LLMs for metaphor detection due to their architecture's ability to leverage bidirectional context.
- **Mechanism**: Masked Language Models (MLMs) like mDeBERTa and XLM-RoBERTa process input bidirectionally, allowing them to capture both preceding and following context for metaphor identification. Decoder-only LLMs process text autoregressively, limiting their contextual understanding to past tokens only.
- **Core assumption**: Bidirectional context is essential for accurate metaphor detection.
- **Evidence anchors**:
  - [abstract] "Our results show that fine-tuned encoders outperform decoders-only LLMs in metaphor detection."
  - [section] "Regarding metaphor detection, after the annotation process and experimental results, we can conclude the importance of establishing a unified criterion for annotation... Overall, encoder-only models outperform decoder-only models in this task."
  - [corpus] "Best results are obtained when Meta4XNLI in both languages is used for training. The augmentation of the training set size and the parallel annotations might boost this performance."
- **Break condition**: If metaphors can be reliably identified from unidirectional context alone, or if LLMs develop superior context modeling through other means, this performance gap may diminish.

### Mechanism 3
- **Claim**: The presence of metaphorical expressions negatively impacts model performance on NLI tasks.
- **Mechanism**: When models must process both the literal and figurative meanings of metaphors to determine inference relationships, their accuracy decreases compared to processing literal sentences. This suggests that understanding metaphorical language requires additional cognitive processing that current models struggle with.
- **Core assumption**: NLI performance is directly correlated with the model's ability to interpret metaphorical meaning.
- **Evidence anchors**:
  - [abstract] "Metaphor interpretation is evaluated via the NLI framework with comparable performance of masked and autoregressive models, which notably decreases when the inference is affected by metaphorical language."
  - [section] "From the reported results, we can observe a tendency of the models to perform lower with pairs that contain at least one metaphorical expression... This outcome replicates in both languages and experimental setups a) and b)."
  - [corpus] "With respect to results, the purpose of our experiments is not to outperform the state-of-the-art results, but to analyze LMs' capabilities in processing metaphorical language in a multilingual and crosslingual setting."
- **Break condition**: If models develop better mechanisms for integrating literal and figurative meanings, or if NLI tasks are reformulated to explicitly handle metaphorical content, this performance degradation may be reduced.

## Foundational Learning

- **Concept**: Natural Language Inference (NLI)
  - Why needed here: The paper uses NLI as the framework for evaluating metaphor interpretation, requiring understanding of entailment, contradiction, and neutral relationships.
  - Quick check question: What are the three possible inference relationships between premise and hypothesis in NLI tasks?

- **Concept**: Metaphor detection as sequence labeling
  - Why needed here: The paper approaches metaphor detection by labeling individual tokens as metaphorical or literal, following MIPVU guidelines.
  - Quick check question: How does the MIPVU methodology determine whether a token is used metaphorically?

- **Concept**: Cross-lingual transfer learning
  - Why needed here: The paper explores how models trained on one language can transfer metaphor detection capabilities to another language using parallel data.
  - Quick check question: What challenges arise when translating metaphorical expressions between languages?

## Architecture Onboarding

- **Component map**: XNLI/esXNLI collection -> Spanish annotation (automatic + manual) -> English projection (automatic + manual) -> final Meta4XNLI dataset -> Model training (MLMs and LLMs) -> Evaluation (cross-domain, monolingual, multilingual, zero-shot cross-lingual) -> Analysis (token-level error analysis and cross-lingual metaphor transfer study)

- **Critical path**:
  1. Annotate Spanish data using MIPVU guidelines
  2. Project Spanish annotations to English using EasyLabel-Projection
  3. Fine-tune MLMs on monolingual and multilingual data
  4. Evaluate model performance across all experimental setups
  5. Analyze error patterns and cross-lingual transfer

- **Design tradeoffs**:
  - Semi-automatic annotation vs. full manual annotation: Speed vs. quality control
  - Bidirectional vs. autoregressive models: Better context understanding vs. computational efficiency
  - Parallel vs. non-parallel data: Direct transfer capability vs. broader coverage

- **Failure signatures**:
  - High false positive rates in out-of-vocabulary evaluation indicate overgeneralization
  - Performance drop in cross-domain experiments suggests domain-specific metaphor usage
  - Lower scores for metaphorical pairs in NLI tasks reveal difficulty with figurative language

- **First 3 experiments**:
  1. Fine-tune mDeBERTa on Spanish Meta4XNLI and evaluate on English Meta4XNLI (zero-shot cross-lingual)
  2. Compare MLM vs. LLM performance on metaphor detection in monolingual Spanish setup
  3. Analyze cross-lingual metaphor transfer patterns between Spanish and English datasets

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does translation direction (EN→ES vs ES→EN) systematically affect metaphor preservation and interpretation accuracy?
- **Basis in paper**: [explicit] The paper notes that XNLI and esXNLI use opposite translation directions and discusses how translation affects metaphor preservation.
- **Why unresolved**: While the paper identifies translation as a factor, it doesn't provide systematic analysis of how translation direction specifically impacts metaphor preservation and model performance.
- **What evidence would resolve it**: Comparative analysis of metaphor preservation rates and interpretation accuracy across translation directions, controlling for other variables like domain and sentence complexity.

### Open Question 2
- **Question**: What is the relationship between in-vocabulary and out-of-vocabulary metaphor performance, and does this indicate models are memorizing rather than learning metaphor patterns?
- **Basis in paper**: [explicit] The paper reports that in-vocabulary performance often exceeds overall performance while out-of-vocabulary performance drops, raising questions about memorization versus learning.
- **Why unresolved**: The paper notes this discrepancy but doesn't investigate whether it reflects genuine learning or memorization, or what this means for metaphor detection generalization.
- **What evidence would resolve it**: Analysis of model behavior on novel metaphorical expressions, including studies of how models generalize to unseen metaphors versus literal uses of familiar words.

### Open Question 3
- **Question**: How do different metaphor annotation guidelines (MIPVU vs alternatives) impact cross-lingual metaphor detection and interpretation performance?
- **Basis in paper**: [explicit] The paper uses MIPVU guidelines and notes subjectivity in annotation, while also mentioning that other datasets use different criteria.
- **Why unresolved**: The paper acknowledges guideline subjectivity but doesn't compare performance across different annotation frameworks or examine how guideline choice affects cross-lingual transfer.
- **What evidence would resolve it**: Comparative experiments using multiple annotation guidelines on the same data, measuring impact on model performance and cross-lingual consistency.

## Limitations

- **Annotation consistency**: The semi-automatic approach combining automatic projection with manual refinement may introduce inconsistencies between English and Spanish annotations, affecting cross-lingual transfer reliability.

- **Dataset size constraints**: With approximately 13,320 sentences total, the dataset may be insufficient for training larger LLMs effectively, particularly for zero-shot cross-lingual experiments.

- **Translation effects**: The paper acknowledges that translation can alter or eliminate metaphorical expressions, but the full extent of this impact on cross-lingual transfer is not quantified.

## Confidence

**High Confidence**: The finding that fine-tuned MLMs outperform decoder-only LLMs for metaphor detection (0.83 F1 vs 0.79 F1 in Spanish) is well-supported by experimental results across multiple models and languages. The bidirectional context advantage for MLMs is a well-established architectural principle.

**Medium Confidence**: The claim that metaphorical expressions negatively impact NLI performance is supported by experimental data, but the effect size varies considerably across different experimental setups. The interpretation that this reflects models' difficulty with figurative language is plausible but not definitively proven.

**Low Confidence**: The cross-lingual transfer results, particularly zero-shot experiments, show inconsistent performance patterns that suggest the methodology may be sensitive to factors not fully controlled in the study. The small performance differences between training strategies make it difficult to draw firm conclusions about optimal approaches.

## Next Checks

1. **Annotation consistency validation**: Conduct inter-annotator agreement studies on a subset of the dataset to quantify annotation reliability and identify systematic differences between English and Spanish metaphor annotations. This would clarify whether cross-lingual transfer performance is limited by annotation quality.

2. **Translation preservation analysis**: Systematically analyze how often metaphors are preserved, transformed, or eliminated during translation between English and Spanish in the dataset. This would quantify the impact of translation on cross-lingual transfer and help explain performance variations.

3. **Scaling experiment**: Test the same experimental setup with progressively larger subsets of the dataset (e.g., 25%, 50%, 100%) to determine whether the observed performance patterns are due to dataset size limitations or represent genuine architectural differences between MLMs and LLMs.