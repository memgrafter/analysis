---
ver: rpa2
title: 'GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language
  Model'
arxiv_id: '2402.02408'
source_url: https://arxiv.org/abs/2402.02408
tags:
- prompt
- evaluation
- prompts
- optimization
- self-consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a gold label-agnostic prompt evaluation (GLaPE)
  method for optimizing prompts in large language models (LLMs). The method leverages
  self-consistency evaluation and mutual-consistency refinement to assess prompts
  without relying on manually annotated gold labels.
---

# GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language Model

## Quick Facts
- arXiv ID: 2402.02408
- Source URL: https://arxiv.org/abs/2402.02408
- Reference count: 9
- Introduces gold label-agnostic prompt evaluation method for LLM optimization

## Executive Summary
This paper presents GLaPE, a method for optimizing prompts in large language models without requiring manually annotated gold labels. The approach leverages self-consistency evaluation and mutual-consistency refinement to assess prompt quality. Experimental results on six reasoning tasks demonstrate that GLaPE provides reliable evaluations that correlate with accuracy, enabling effective prompt optimization comparable to traditional accuracy-based approaches.

## Method Summary
GLaPE uses self-consistency of generated answers as an initial evaluation metric, then refines scores through mutual-consistency across prompts producing identical answers. The method operates within an OPRO framework, using the GLaPE score to guide prompt generation without access to gold labels. Prompts are evaluated by generating multiple outputs, identifying the most frequent answer, and measuring consistency. The mutual-consistency refinement step adjusts scores for prompts that produce the same answer but show inconsistent self-consistency values.

## Key Results
- GLaPE provides reliable evaluations that correlate with accuracy across six reasoning tasks
- Optimized prompts achieve comparable performance to accuracy-based optimization methods
- The approach eliminates dependence on gold labels, which are often difficult to obtain in real-world tasks

## Why This Works (Mechanism)

### Mechanism 1
Self-consistency of generated answers is a strong indicator of prompt quality. When a prompt is good, the model's multiple decoding iterations produce the same answer more often, which correlates with correctness. The core assumption is that the model's self-consistency reflects the quality of the reasoning path triggered by the prompt. Evidence shows that self-consistency alone may not always yield accurate evaluation, leading to the refinement step. This mechanism breaks when the model's internal reasoning is inconsistent for correct answers due to ambiguity.

### Mechanism 2
Mutual-consistency refinement corrects self-consistency bias by penalizing inconsistent scores among prompts that produce the same answer. Prompts that generate the same answer but have very different self-consistency scores are likely inconsistent in their reasoning, so the lower-scoring prompt is penalized. The core assumption is that when multiple prompts produce the same wrong answer, their self-consistency scores should be similar and low. This mechanism breaks when the dataset contains many questions where all prompts fail to produce correct answers.

### Mechanism 3
Gold label-agnostic evaluation can achieve comparable performance to accuracy-based optimization when the correlation between self-consistency and accuracy is strong. By optimizing prompts to maximize the combined self-consistency and mutual-consistency score, the method indirectly steers toward prompts that yield correct answers. The core assumption is that the optimization landscape guided by the GLaPE score has peaks near the accuracy peaks. This mechanism breaks if the self-consistency/accuracy correlation is weak for a specific task.

## Foundational Learning

- **Concept**: Chain-of-thought prompting
  - Why needed here: The method relies on the model generating reasoning steps; self-consistency is measured over these reasoning traces
  - Quick check question: What is the difference between direct answer generation and chain-of-thought prompting in terms of output diversity?

- **Concept**: Prompt optimization via gradient-free methods
  - Why needed here: The work uses an LLM as an optimizer without access to gradients, so it must rely on evaluation scores to guide prompt generation
  - Quick check question: How does a meta-prompt guide the LLM to generate new prompts based on evaluation scores?

- **Concept**: Spearman correlation as a measure of monotonic relationship
  - Why needed here: The authors use Spearman correlation to quantify the relationship between their evaluation score and accuracy
  - Quick check question: Why is Spearman correlation preferred over Pearson correlation when assessing the relationship between GLaPE scores and accuracy?

## Architecture Onboarding

- **Component map**: Input -> Prompt pool -> LLM backend -> Evaluation module -> Optimizer -> Optimized prompt
- **Critical path**: 
  1. Generate 10 outputs per prompt using chain-of-thought with temperature 0.7
  2. Compute self-consistency for each prompt
  3. Group prompts by their most frequent answer
  4. Apply mutual-consistency refinement within each group
  5. Sum refined scores to get final GLaPE score
  6. Use scores in meta-prompt to generate new prompts
  7. Repeat until convergence or budget exhausted
- **Design tradeoffs**: Self-consistency vs. accuracy (cheaper but less reliable); more decoding iterations improve estimation but increase cost; temperature affects diversity vs. consistency signal
- **Failure signatures**: Low Spearman correlation between GLaPE and accuracy; optimization trajectory shows no improvement; uniformly high or low GLaPE scores across all prompts
- **First 3 experiments**:
  1. Run self-consistency evaluation on held-out prompts to verify correlation with accuracy
  2. Apply mutual-consistency refinement and measure change in Spearman correlation
  3. Run full optimization loop on AddSub and compare best GLaPE prompt accuracy to baseline

## Open Questions the Paper Calls Out

### Open Question 1
How can we improve GLaPE's performance on challenging questions that exceed the intrinsic capabilities of LLMs? The paper discusses limitations in accurately assessing prompts for complex questions, which stem from inherent LLM limitations. This remains unresolved as the paper acknowledges the challenge but doesn't provide a concrete solution.

### Open Question 2
Can we develop a more granular evaluation method for prompts that provides comprehensive information beyond a single numerical score? The current methodology uses a singular digital score that fails to furnish comprehensive information regarding the prompts. The paper doesn't explore alternative evaluation methods that could provide more detailed assessments.

### Open Question 3
How can we aggregate assessments on individual questions to mitigate the adverse effects of challenging questions on the overall dataset evaluation? The paper discusses the impact of challenging questions and mentions the need to explore innovative approaches to aggregate assessments, but doesn't provide a concrete approach for this aggregation.

## Limitations
- Effectiveness depends critically on the correlation between self-consistency and accuracy being strong enough for optimization to work
- The method's performance on complex reasoning tasks like AQuA is limited, suggesting constraints when questions exceed LLM capabilities
- The exact mathematical formulation of mutual-consistency refinement is unclear from the paper

## Confidence

- **High Confidence**: Self-consistency is a useful but imperfect indicator of prompt quality that requires refinement
- **Medium Confidence**: Mutual-consistency refinement improves the correlation between evaluation scores and accuracy
- **Medium Confidence**: Gold label-agnostic optimization can achieve comparable performance to accuracy-based methods on tested reasoning tasks

## Next Checks

1. **Correlation validation**: Run self-consistency evaluation on a held-out set of prompts to verify the correlation with accuracy before applying mutual-consistency refinement. Plot SC-Accuracy graphs to identify inconsistent prompts and measure baseline Spearman correlation.

2. **Refinement impact**: Apply mutual-consistency refinement to the same set and measure the change in Spearman correlation. Compare the distribution of scores before and after refinement to verify that the method penalizes inconsistent scores across prompts with identical answers.

3. **Optimization trajectory**: Run a full optimization loop on a small dataset (e.g., AddSub) and track GLaPE scores across iterations. Compare the best GLaPE prompt's accuracy to the baseline "Let's think step by step" and visualize the optimization path to ensure it converges toward high-quality prompts.