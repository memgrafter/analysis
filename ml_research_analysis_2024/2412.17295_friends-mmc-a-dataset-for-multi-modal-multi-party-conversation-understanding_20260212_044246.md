---
ver: rpa2
title: 'Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding'
arxiv_id: '2412.17295'
source_url: https://arxiv.org/abs/2412.17295
tags:
- speaker
- conversation
- visual
- context
- face
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Friends-MMC, a multi-modal multi-party conversation
  dataset collected from the TV series Friends. It addresses the gap in existing datasets
  by providing conversations with multiple speakers situated in the visual context,
  rather than discussing given images/videos as bystanders.
---

# Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding

## Quick Facts
- arXiv ID: 2412.17295
- Source URL: https://arxiv.org/abs/2412.17295
- Reference count: 22
- Primary result: New dataset for multi-modal multi-party conversation understanding from Friends TV series

## Executive Summary
This paper introduces Friends-MMC, a novel multi-modal multi-party conversation dataset collected from the TV series Friends. The dataset addresses limitations in existing conversation datasets by providing naturally occurring multi-party dialogues situated within visual contexts, rather than discussions about external images or videos. With over 24,000 utterances paired with video context, speaker annotations, and face bounding boxes, Friends-MMC enables research on speaker identification and response prediction in multi-party conversational settings.

The paper establishes two primary tasks: conversation speaker identification and conversation response prediction. For speaker identification, the authors propose a modular baseline method combining visual and textual models with quadratic optimization, demonstrating significantly better performance than pre-trained models. The response prediction task shows that incorporating speaker information improves model performance compared to random or shuffled speaker assignments. The core contribution is establishing a new research direction for multi-modal multi-party conversations and providing benchmark datasets with task-specific baselines to advance the field.

## Method Summary
The dataset was constructed by collecting dialogue and visual context from Friends TV episodes, with detailed annotations including speaker identities, face bounding boxes, and conversation turn boundaries. For the speaker identification task, the authors developed a modular baseline approach that combines visual features (extracted from face regions) with textual features (from utterances) using a quadratic optimization solver to determine speaker attribution. The response prediction task involves predicting the next utterance in a conversation, with models trained with speaker information showing improved performance over baselines that ignore speaker context. The evaluation framework includes both quantitative metrics and qualitative analysis of model performance across different conversation scenarios.

## Key Results
- Achieved significantly better speaker identification performance using the proposed modular baseline compared to pre-trained models
- Demonstrated that incorporating speaker information improves conversation response prediction accuracy
- Established benchmark results for two fundamental tasks in multi-modal multi-party conversation understanding
- Provided a dataset with over 24,000 utterances paired with video context and detailed annotations

## Why This Works (Mechanism)
The dataset's effectiveness stems from capturing naturally occurring multi-party conversations within rich visual contexts, providing authentic scenarios for training and evaluating multi-modal models. The visual context from the TV show scenes provides additional cues about speaker identity, emotional state, and conversational dynamics that text-only datasets lack. The face bounding boxes and speaker annotations enable models to learn associations between visual appearance and conversational behavior. The modular approach for speaker identification works by separately processing visual and textual modalities before combining them through optimization, allowing each modality to contribute complementary information for speaker attribution.

## Foundational Learning

**Multi-modal conversation understanding**: Understanding conversations requires processing both linguistic content and visual context - essential because real-world conversations occur within visual environments that provide crucial contextual cues.

**Speaker identification in multi-party settings**: Determining who is speaking in group conversations - necessary because conversation models must track multiple participants to maintain coherent dialogue understanding.

**Face detection and tracking**: Locating and identifying speakers in video frames - important because visual speaker information provides strong cues for attribution and helps disambiguate overlapping speech.

**Quadratic optimization for speaker attribution**: Using mathematical optimization to assign speakers to utterances - critical because this approach can incorporate constraints and preferences from multiple modalities simultaneously.

## Architecture Onboarding

**Component map**: Video frames -> Face detection module -> Visual feature extractor -> Speaker identification module <- Textual feature extractor <- Utterance processing module -> Quadratic optimization solver -> Speaker attribution

**Critical path**: Utterance + Visual context → Feature extraction → Quadratic optimization → Speaker identification → Response prediction

**Design tradeoffs**: The modular approach allows separate optimization of visual and textual components but may miss cross-modal interactions that end-to-end models could capture. The dataset focuses on scripted conversations which may limit generalization to spontaneous dialogue.

**Failure signatures**: Poor performance on speakers with similar appearance or speaking style, difficulty with overlapping speech or rapid turn-taking, reduced accuracy when visual context is degraded or occluded.

**First experiments**: 1) Ablation study removing visual features to measure modality contribution, 2) Cross-episode evaluation to test generalization across different Friends episodes, 3) Comparison with end-to-end multi-modal models on both tasks.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset construction methodology may introduce bias from scripted TV show patterns and character dynamics
- Evaluation methodology lacks comprehensive baseline comparisons to other multi-modal conversation models
- Visual context integration methodology lacks detailed error analysis for failure cases
- Claims about advancing the field lack sufficient empirical validation across diverse conversation scenarios

## Confidence
- High confidence: Dataset construction process and basic statistics are well-documented and reproducible
- Medium confidence: The claim that Friends-MMC represents a novel research direction is reasonable but could have more systematic comparisons
- Low confidence: The assertion that the dataset "advances the field" significantly lacks sufficient empirical validation

## Next Checks
1. **Cross-dataset generalization**: Test speaker identification and response prediction models on at least two other multi-modal conversation datasets to evaluate generalization beyond the Friends domain

2. **Ablation studies**: Conduct comprehensive ablation studies removing visual context, speaker information, or face bounding boxes to quantify each modality's contribution to model performance

3. **Longitudinal conversation analysis**: Analyze model performance across different conversation lengths and speaker turn patterns to identify specific scenarios where proposed methods succeed or fail