---
ver: rpa2
title: Can Language Models Use Forecasting Strategies?
arxiv_id: '2406.04446'
source_url: https://arxiv.org/abs/2406.04446
tags:
- event
- events
- human
- predictions
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether large language models (LLMs) can forecast
  real-world events as well as humans. Using a novel dataset from a Google prediction
  market (GleanGen), the authors compare human predictions to several LLM-based forecasting
  strategies.
---

# Can Language Models Use Forecasting Strategies?

## Quick Facts
- arXiv ID: 2406.04446
- Source URL: https://arxiv.org/abs/2406.04446
- Reference count: 40
- Primary result: Basic LLM baseline outperformed humans due to bias toward predicting low probabilities on negatively-skewed dataset

## Executive Summary
This paper evaluates whether large language models can forecast real-world events as well as humans using various forecasting strategies. Using a novel dataset from a Google prediction market (GleanGen), the authors compare human predictions to several LLM-based forecasting strategies. Surprisingly, a simple LLM baseline outperformed human forecasters. However, further analysis revealed this was due to a model bias toward predicting low probabilities—combined with the dataset's skew toward negative outcomes. When controlling for this bias using a weighted Brier score, human performance was superior.

## Method Summary
The study used a Google prediction market dataset (GleanGen) containing 692 events across four categories with associated human predictions and outcomes. The authors employed PaLM 2 (pre-training ended July 2022) to forecast events active on August 1, 2022. Six forecasting strategies were tested: Basic, Forecaster, Breakdown, Base Rates, Both Sides, and Crowd, plus a News API module. Eight predictions per event were generated using low temperature sampling and averaged. Performance was evaluated using Brier Score and a newly introduced Weighted Brier Score that separately calculates Brier Scores for events resolving positively and negatively.

## Key Results
- A simple LLM baseline outperformed human forecasters on the standard Brier Score metric
- This performance gap disappeared when using the Weighted Brier Score that controls for dataset skew
- Prompting the model to produce rationales increased predicted probabilities, leading to worse Brier scores when most events resolve negatively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The basic LLM baseline outperformed human forecasters because of a model bias toward predicting low probabilities combined with the dataset's skew toward negative outcomes.
- Mechanism: The LLM tends to predict lower probabilities for events occurring. Since most events in the dataset resolve negatively (condition not met), this bias leads to better Brier scores by default.
- Core assumption: The dataset contains significantly more negatively resolving events than positively resolving ones.
- Evidence anchors:
  - [abstract] "However, further analysis revealed this was due to a model bias toward predicting low probabilities—combined with the dataset's skew toward negative outcomes."
  - [section] "The large majority of events resolved negatively. This means that biases that push the models/humans to predict lower values often lead to a higher Brier Score."
  - [corpus] No direct evidence in corpus papers about this specific bias mechanism.
- Break condition: If the dataset were balanced between positive and negative outcomes, or if the LLM bias were toward predicting high probabilities, the basic baseline would not outperform human forecasters.

### Mechanism 2
- Claim: The introduction of the Weighted Brier Score reveals that the basic LLM's superior performance was misleading due to the underlying probability bias.
- Mechanism: The Weighted Brier Score separately calculates Brier Scores for events that resolved positively and negatively, then averages them. This post-hoc balancing reveals that the basic LLM actually performs worse on positively resolving events.
- Core assumption: The original Brier Score was unfairly favoring models that predicted low probabilities due to the dataset's skew.
- Evidence anchors:
  - [abstract] "When controlling for this bias using a weighted Brier score, human performance was superior."
  - [section] "We introduce the Weighted Brier Score, a metric designed to counteract the inherently uneven nature of the collected dataset."
  - [corpus] No direct evidence in corpus papers about weighted Brier Score methodology.
- Break condition: If the dataset were evenly distributed between positive and negative outcomes, the standard Brier Score would accurately reflect model performance without needing weighting.

### Mechanism 3
- Claim: Prompting the model to produce rationales increases predicted probabilities, which leads to worse Brier scores when most events resolve negatively.
- Mechanism: When instructed to provide a rationale before answering, the model predicts higher probabilities for events. Since most events in the dataset resolve negatively, these higher predictions lead to worse Brier scores.
- Core assumption: The model's internal probability distribution shifts upward when required to justify its predictions.
- Evidence anchors:
  - [abstract] "Our follow-up experiments indicate this is likely due to models' tendency to guess that most events are unlikely to occur"
  - [section] "When prompted to produce a rationale, the model predicts a consistently higher probability."
  - [corpus] No direct evidence in corpus papers about rationale-prompting effects on probability distributions.
- Break condition: If the dataset had more positively resolving events, or if the model's probability distribution shifted downward when producing rationales, this mechanism would not explain the performance difference.

## Foundational Learning

- Concept: Brier Score calculation
  - Why needed here: The evaluation metric used to compare forecasting accuracy between models and humans
  - Quick check question: If an event occurs and a model predicts 0.3 probability, what is the Brier Score component for that prediction?

- Concept: Dataset skew and its impact on evaluation metrics
  - Why needed here: Understanding why the basic LLM performed well requires recognizing that most events resolved negatively
  - Quick check question: If 80% of events resolve negatively, what prediction strategy would yield a better Brier Score by default?

- Concept: Prompt engineering for LLMs
  - Why needed here: Different forecasting strategies were implemented through various prompt designs
  - Quick check question: How might prompting an LLM to "explain your reasoning" versus "just answer" affect its output?

## Architecture Onboarding

- Component map: Data ingestion -> Preprocessing -> Model layer -> Evaluation -> Analysis
- Critical path:
  1. Load and clean GleanGen data
  2. Filter to resolved events in appropriate categories
  3. Split into validation and test sets
  4. Generate LLM predictions using various strategies
  5. Calculate Brier Scores for comparison
  6. Perform weighted analysis to check for bias effects
- Design tradeoffs:
  - Using slightly outdated models (PaLM 2) to ensure events are in model's "future" versus using more capable but potentially contaminated models
  - Single prediction date approach versus time-series prediction approach for evaluation
  - Simple probability extraction versus more complex calibration methods
- Failure signatures:
  - Basic LLM outperforming all sophisticated strategies suggests dataset or evaluation bias
  - Weighted Brier Score revealing different performance patterns than standard Brier Score
  - Consistent bias toward low probabilities across different prompting strategies
- First 3 experiments:
  1. Run basic LLM predictions and calculate standard Brier Score
  2. Calculate Weighted Brier Score to check for skew effects
  3. Generate predictions with and without rationale prompting to test probability distribution effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Weighted Brier Score change if we use a more sophisticated balancing method beyond simple averaging of positive and negative event scores?
- Basis in paper: [explicit] The paper introduces a Weighted Brier Score that separately calculates Brier Scores for events resolving positively and negatively, then averages them.
- Why unresolved: The authors only implemented a basic averaging approach to balance the dataset skew. More complex re-weighting methods might yield different insights about model performance.
- What evidence would resolve it: Results showing Brier Scores calculated using alternative balancing methods (e.g., inverse frequency weighting, SMOTE-like oversampling of minority class events) compared to the current Weighted Brier Score.

### Open Question 2
- Question: Would fine-tuning PaLM 2 on the GleanGen dataset improve its forecasting performance compared to the zero-shot strategies tested in this paper?
- Basis in paper: [inferred] The authors note that concurrent studies found fine-tuned models approached human accuracy, and they acknowledge that using more capable models (like PaLM 2) slightly behind SOTA is a limitation.
- Why unresolved: The paper only tests zero-shot strategies with PaLM 2. Fine-tuning on the prediction market data could potentially improve performance.
- What evidence would resolve it: Comparative results showing forecasting accuracy of a PaLM 2 model fine-tuned on GleanGen data versus the zero-shot strategies tested in this paper.

### Open Question 3
- Question: How does the performance of LLM forecasting strategies change when evaluated on a dataset with a more balanced distribution of positive and negative outcomes?
- Basis in paper: [explicit] The authors hypothesize that the strong performance of the Basic forecaster is due to a model bias toward predicting low probabilities combined with the dataset's skew toward negative outcomes.
- Why unresolved: All experiments were conducted on the GleanGen dataset, which has a predominance of negative outcomes. The impact of outcome distribution on strategy performance remains unclear.
- What evidence would resolve it: Results showing Brier Scores and Weighted Brier Scores for each forecasting strategy when evaluated on a balanced dataset with equal numbers of positive and negative outcomes.

## Limitations
- The GleanGen dataset is not publicly available, limiting independent verification
- Results may be specific to the PaLM 2 model and may not generalize to other LLMs
- The analysis assumes bias is inherent to the LLM rather than a consequence of the specific prompting strategy

## Confidence
- Medium confidence in core findings due to clear methodology but lack of data availability for independent verification
- High confidence that dataset skew affects evaluation metrics
- Low confidence that findings generalize to balanced datasets or other LLM architectures

## Next Checks
1. Test whether the basic LLM baseline maintains its performance advantage on a balanced dataset where positive and negative outcomes are equally represented
2. Compare different LLM architectures (beyond PaLM 2) to determine if the low-probability bias is consistent across models
3. Implement and test additional debiasing techniques beyond the Weighted Brier Score to verify whether the bias is systematic or prompt-dependent