---
ver: rpa2
title: Towards Few-shot Self-explaining Graph Neural Networks
arxiv_id: '2408.07340'
source_url: https://arxiv.org/abs/2408.07340
tags:
- graph
- mse-gnn
- explanation
- classification
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of self-explaining graph neural
  networks (GNNs) in few-shot scenarios. Existing self-explaining GNN models require
  large amounts of training data, limiting their applicability in situations with
  limited data, such as new drug discovery.
---

# Towards Few-shot Self-explaining Graph Neural Networks

## Quick Facts
- arXiv ID: 2408.07340
- Source URL: https://arxiv.org/abs/2408.07340
- Authors: Jingyu Peng; Qi Liu; Linan Yue; Zaixi Zhang; Kai Zhang; Yunhao Sha
- Reference count: 40
- Primary result: MSE-GNN achieves superior few-shot graph classification accuracy while generating high-quality explanations compared to existing methods

## Executive Summary
This paper addresses the challenge of self-explaining graph neural networks in few-shot scenarios, where existing models require large amounts of training data. The authors propose a Meta-learned Self-Explaining GNN (MSE-GNN) that separates explanation selection from prediction through a two-stage architecture. The model incorporates task information via prototypes and uses a meta-training framework based on MAML to enable effective adaptation to new tasks. Extensive experiments on four datasets demonstrate MSE-GNN's superior performance on both prediction tasks and explanation generation.

## Method Summary
MSE-GNN is a two-stage self-explaining GNN that generates explanations to support predictions in few-shot settings. The model consists of an explainer and a predictor, where the explainer first selects an explanation subgraph by imitating human attention mechanisms, and then the predictor makes predictions based on the generated explanation. The model uses a meta-training process based on MAML with a mechanism that exploits task information through prototype embeddings. The training involves global updates to learn encoder, explainer, and predictor initialization parameters, and local updates to adapt the predictor to new tasks.

## Key Results
- MSE-GNN achieves superior classification accuracy on few-shot graph classification tasks compared to baseline methods
- The model generates high-quality explanations with better AUC-ROC scores on datasets with ground-truth explanations
- MSE-GNN demonstrates effective adaptation to new few-shot tasks through its meta-learning framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The explainer-predictor two-stage structure enables few-shot learning by separating explanation selection from prediction
- Mechanism: The explainer uses a GNN encoder and MLP to generate a soft mask vector indicating node importance, while the predictor makes predictions solely based on the selected explanation subgraph
- Core assumption: The most important nodes for classification in one task will be similarly important in related few-shot tasks
- Evidence anchors: [abstract], [section 3.1]

### Mechanism 2
- Claim: Task information via prototypes improves both explanation quality and prediction accuracy in few-shot settings
- Mechanism: For each class in a task, the model creates a prototype embedding by aggregating graph representations from the support set
- Core assumption: Prototype embeddings capture the essential characteristics of each class and can guide explanation selection and prediction in new tasks
- Evidence anchors: [abstract], [section 3.1]

### Mechanism 3
- Claim: The meta-training framework based on MAML enables effective adaptation to new few-shot tasks
- Mechanism: MSE-GNN uses a two-level optimization process where slow parameters are updated globally across many tasks, while fast parameters are locally updated within each task
- Core assumption: Learning to optimize the predictor on support sets across many tasks will produce initialization parameters that enable rapid adaptation to new tasks
- Evidence anchors: [abstract], [section 3.3]

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: MSE-GNN builds on GNNs for both explanation selection and prediction
  - Quick check question: How does a standard GNN layer update node representations using neighbor information?

- Concept: Few-shot learning and meta-learning
  - Why needed here: The paper addresses few-shot graph classification and uses meta-learning (specifically MAML) for training
  - Quick check question: In N-way K-shot learning, how many total support examples are available for training within a single task?

- Concept: Prototype learning
  - Why needed here: MSE-GNN uses prototype embeddings to represent each class in a task
  - Quick check question: How are prototype embeddings typically computed from support examples in few-shot learning?

## Architecture Onboarding

- Component map: Input graph -> Graph encoder f -> Explainer g -> Node mask -> Rationale subgraph -> Predictor p -> Prediction

- Critical path: Input graph → Graph encoder f → Explainer g → Node mask → Rationale subgraph → Predictor p → Prediction

- Design tradeoffs:
  - Using a separate GNN encoder in the explainer versus sharing with the main encoder
  - The size of the explanation subgraph (controlled by γ) versus explanation quality
  - Number of local update steps T versus adaptation speed and overfitting risk

- Failure signatures:
  - Poor classification accuracy with high-quality explanations suggests the predictor isn't effectively using the explanations
  - High classification accuracy with poor explanations suggests the explainer is not identifying truly important nodes
  - Degraded performance on both tasks suggests meta-training issues or overfitting

- First 3 experiments:
  1. Train MSE-GNN on Synthetic dataset with 2-way 5-shot setting, evaluate classification accuracy and AUC-ROC for explanations
  2. Test MSE-GNN on held-out tasks from Synthetic dataset, measuring few-shot adaptation performance
  3. Compare MSE-GNN against baseline methods (ProtoNet, GREA_Meta, CAL_Meta) on MNIST-sp dataset, visualizing explanation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of the explanation subgraph (controlled by γ) affect the model's performance on different types of graph data?
- Basis in paper: [explicit] The paper mentions a sensitivity analysis on γ but notes that the impact is less pronounced on the OGBG-Molsider dataset compared to the Synthetic dataset
- Why unresolved: The paper does not provide a detailed analysis of why the impact of γ varies across different datasets
- What evidence would resolve it: Conducting experiments on a wider range of graph datasets with varying characteristics and analyzing the relationship between explanation subgraph size and model performance

### Open Question 2
- Question: How does the choice of GNN backbone affect the model's ability to generate high-quality explanations and achieve good classification performance in few-shot scenarios?
- Basis in paper: [explicit] The paper mentions that they use GIN and GraphSAGE as GNN backbones but does not provide a detailed comparison of their performance
- Why unresolved: The paper does not explore the impact of the GNN backbone on the model's performance in few-shot scenarios
- What evidence would resolve it: Conducting experiments with different GNN backbones and comparing their performance on few-shot graph classification tasks

### Open Question 3
- Question: How does the model's performance on few-shot graph classification tasks with explanation generation compare to state-of-the-art methods that focus solely on classification without explanation generation?
- Basis in paper: [explicit] The paper compares the model's performance to baseline methods on few-shot graph classification tasks but does not explicitly compare it to state-of-the-art methods that do not generate explanations
- Why unresolved: The paper does not provide a comprehensive comparison of the model's performance to state-of-the-art methods that focus solely on classification without explanation generation
- What evidence would resolve it: Conducting experiments comparing the model's performance to state-of-the-art methods that focus solely on classification without explanation generation

## Limitations

- The model assumes that explanation quality directly correlates with prediction performance, but this relationship is not empirically validated across all datasets
- The meta-training process relies heavily on the quality and diversity of the training tasks, with limited discussion of how to ensure task distribution alignment with target applications
- The explainer's node selection mechanism may oversimplify complex graph structures by reducing them to soft mask vectors

## Confidence

- High: The two-stage architecture (explainer-predictor) improves few-shot performance compared to end-to-end approaches
- Medium: Task information via prototypes effectively guides both explanation selection and prediction
- Medium: The meta-training framework based on MAML enables effective adaptation to new few-shot tasks

## Next Checks

1. Conduct ablation studies removing the explainer component to quantify the exact contribution of self-explanations to prediction performance
2. Test model robustness by evaluating performance when training and testing tasks have domain shifts or structural differences
3. Measure computational efficiency during both training and inference phases, particularly the overhead introduced by the two-stage architecture