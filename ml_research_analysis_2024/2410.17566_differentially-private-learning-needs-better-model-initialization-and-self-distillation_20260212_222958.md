---
ver: rpa2
title: Differentially Private Learning Needs Better Model Initialization and Self-Distillation
arxiv_id: '2410.17566'
source_url: https://arxiv.org/abs/2410.17566
tags:
- dpsgd
- data
- dprefine
- private
- non-private
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DPRefine, a three-phase method that combines
  data synthesis, differentially private fine-tuning, and self-distillation to improve
  the linguistic quality of privacy-preserving language models. By generating high-quality
  synthetic data for initialization, applying DPSGD on private data, and refining
  outputs through self-distillation, DPRefine significantly outperforms vanilla DPSGD.
---

# Differentially Private Learning Needs Better Model Initialization and Self-Distillation

## Quick Facts
- arXiv ID: 2410.17566
- Source URL: https://arxiv.org/abs/2410.17566
- Authors: Ivoline C. Ngong; Joseph P. Near; Niloofar Mireshghallah
- Reference count: 24
- Primary result: DPRefine achieves 78.4% AlpacaEval preference over vanilla DPSGD while reducing linguistic errors by 84%

## Executive Summary
This paper introduces DPRefine, a three-phase method that combines data synthesis, differentially private fine-tuning, and self-distillation to improve the linguistic quality of privacy-preserving language models. By generating high-quality synthetic data for initialization, applying DPSGD on private data, and refining outputs through self-distillation, DPRefine significantly outperforms vanilla DPSGD. AlpacaEval shows DPRefine is preferred in 78.4% of cases across datasets, while manual analysis reveals an 84.0% reduction in linguistic errors and better handling of hallucinations and inconsistencies. The approach demonstrates that small pre-trained models can effectively initialize and refine domain-specific models under privacy constraints.

## Method Summary
DPRefine employs a three-phase approach: (1) data synthesis using small pre-trained models like GPT-2 or BioGPT to generate synthetic data with rigorous filtering, (2) DP fine-tuning using DPSGD on private data with privacy budget ε=8, and (3) self-distillation refinement where the DP model generates new training data from its own outputs. The method uses entailment filtering, length filtering, diversity filtering, grammar filtering, numerical consistency filtering, and redundancy filtering to ensure high-quality training data. The approach is evaluated on XSum summarization, PubMed biomedical summarization, and MRPC paraphrasing tasks using T5-large models.

## Key Results
- DPRefine achieves 78.4% preference rate in AlpacaEval evaluations over vanilla DPSGD
- Manual analysis shows 84.0% reduction in linguistic errors across datasets
- The method significantly improves handling of hallucinations and inconsistencies in model outputs
- DPRefine demonstrates strong performance while maintaining strict privacy guarantees (ε=8)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data initialization improves model performance under privacy constraints.
- Mechanism: High-quality synthetic data generated by small pre-trained models provides a strong starting point for fine-tuning, enabling the model to learn domain-specific patterns before applying DPSGD.
- Core assumption: Small pre-trained models can generate diverse and meaningful synthetic data that captures essential domain features.
- Evidence anchors: [abstract] "generating high-quality synthetic data using a small pre-trained language model (e.g., GPT-2) with rigorous filtering"; [section 3.1] "We begin by generating a context c based on a domain-specific prefix... Multiple sentence completions {a, b, c, ...} are then generated based on the context c"

### Mechanism 2
- Claim: Self-distillation refines outputs and mitigates privacy-induced errors.
- Mechanism: The differentially private model generates new training data from its own outputs, which is then filtered and used to fine-tune the model, improving linguistic quality and reducing inconsistencies.
- Core assumption: The model's own outputs can be leveraged to create valuable training examples that correct privacy-induced errors.
- Evidence anchors: [abstract] "performs self-distillation to refine outputs"; [section 3.3] "the final phase of DPRefine involves using self-distillation to further refine the model... Mprivate generates new outputs based on input contexts and self-corrects using its own predictions"

### Mechanism 3
- Claim: Rigorous filtering improves data quality across all phases.
- Mechanism: Custom filters (entailment, length, diversity, grammar, numerical consistency, redundancy) are applied to synthetic and self-generated data to ensure high-quality training examples.
- Core assumption: Filtering can effectively remove low-quality or inconsistent data, improving model performance.
- Evidence anchors: [section 3.1] "We apply the following filters: 1. Entailment Filtering... 2. Length Filtering... 3. Diversity Filtering... 4. Grammar Filtering... 5. Numerical Consistency Filtering... 6. Redundancy Filtering"; [abstract] "with rigorous filtering"

## Foundational Learning

- Concept: Differentially Private Stochastic Gradient Descent (DPSGD)
  - Why needed here: DPSGD is the core technique used to ensure privacy during fine-tuning on private data.
  - Quick check question: How does DPSGD balance privacy and utility in model training?

- Concept: Knowledge Distillation
  - Why needed here: Knowledge distillation is used in both data synthesis and self-distillation phases to transfer knowledge from larger or pre-trained models to smaller models.
  - Quick check question: What are the benefits of using knowledge distillation in privacy-preserving model training?

- Concept: Data Synthesis and Filtering
  - Why needed here: Generating synthetic data and applying rigorous filtering are essential steps to create high-quality training examples without accessing private data.
  - Quick check question: How do entailment and diversity filters contribute to the quality of synthetic data?

## Architecture Onboarding

- Component map:
  - Data Synthesis Phase: Small pre-trained model (GPT-2/BioGPT) → Context generation → Multiple completions → Input-output pair formation → Filtering → Fine-tuning base model
  - DP Fine-tuning Phase: Base model → DPSGD on private data → Differentially private model
  - Self-distillation Phase: DP model → Output generation → New input-output pairs → Filtering → Final fine-tuning

- Critical path:
  1. Generate synthetic data with small pre-trained model
  2. Apply filtering to ensure quality
  3. Fine-tune base model on synthetic data
  4. Apply DPSGD to fine-tune on private data
  5. Generate new data via self-distillation
  6. Apply filtering and final fine-tuning

- Design tradeoffs:
  - Computational cost vs. data quality: More synthetic data and filtering improve quality but increase computation.
  - Privacy budget (ε) vs. model performance: Higher ε allows better performance but reduces privacy.
  - Model size vs. efficiency: Smaller models are faster but may generate less diverse data.

- Failure signatures:
  - Poor synthetic data quality: Low diversity or relevance in generated examples.
  - Overfitting to synthetic data: Model performs well on synthetic data but poorly on private data.
  - Ineffective self-distillation: Self-generated data does not improve model performance.
  - Filtering issues: Loss of valuable data or retention of low-quality examples.

- First 3 experiments:
  1. Generate synthetic data with GPT-2/BioGPT and evaluate diversity and quality.
  2. Apply filtering and fine-tune base model; compare performance with and without filtering.
  3. Implement DPSGD on private data and assess privacy-utility tradeoff at different ε values.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DPRefine's performance scale with different model sizes (e.g., T5-small, T5-base) and does the synthetic data generation phase need to be adjusted for smaller models?
- Basis in paper: [explicit] The paper focuses on T5-large but mentions that small models like GPT-2 can be effective for initialization and distillation
- Why unresolved: The experiments only evaluate DPRefine using T5-large, leaving questions about its effectiveness with smaller models that might be more computationally efficient for deployment
- What evidence would resolve it: Systematic experiments comparing DPRefine's performance across different model sizes (T5-small, T5-base, T5-large) while measuring computational costs and output quality metrics

### Open Question 2
- Question: Can DPRefine's data synthesis phase be extended to generate synthetic labeled data for multiple NLP tasks simultaneously, and how would this multi-task initialization affect downstream performance?
- Basis in paper: [inferred] The paper demonstrates success with summarization and paraphrasing tasks but doesn't explore multi-task synthetic data generation
- Why unresolved: The current implementation treats each task independently, missing potential synergies from multi-task learning during the initialization phase
- What evidence would resolve it: Experiments generating synthetic data covering multiple tasks (e.g., summarization, paraphrasing, question answering) and evaluating whether this multi-task initialization improves performance across all tasks

### Open Question 3
- Question: What is the optimal trade-off between the quality and quantity of synthetic data generated in Phase 1, and how does this affect the final model's performance under different privacy budgets?
- Basis in paper: [explicit] The paper mentions using filtering criteria but doesn't systematically study the impact of varying the amount of synthetic data
- Why unresolved: The paper uses a fixed amount of synthetic data without exploring how different volumes might interact with privacy budgets to affect final performance
- What evidence would resolve it: Experiments varying the amount of synthetic data (e.g., 50%, 100%, 200% of current amount) while measuring performance at different privacy budgets to identify optimal trade-offs

## Limitations

- The paper lacks ablation studies showing the individual contribution of each phase (synthetic data initialization, DP fine-tuning, self-distillation), making it difficult to assess which components are most critical to performance gains
- Limited evaluation on a narrow set of datasets (primarily summarization and paraphrasing) raises questions about generalizability to other NLP tasks
- The filtering pipeline is extensive but not fully specified, particularly the entailment filtering thresholds and graph-based redundancy detection, which could significantly impact reproducibility

## Confidence

- High confidence: The overall three-phase methodology is well-structured and the reported AlpacaEval preference scores (78.4%) are specific and reproducible
- Medium confidence: The privacy-utility tradeoff claims are supported by DPSGD implementation details, though the lack of ablation studies limits mechanistic understanding
- Low confidence: The effectiveness of self-distillation refinement is supported by results but lacks detailed analysis of why this phase specifically improves linguistic quality

## Next Checks

1. Conduct ablation studies removing each phase (synthetic data initialization, DP fine-tuning, self-distillation) to quantify individual contributions to the 78.4% AlpacaEval preference score
2. Test the DPRefine approach on additional NLP tasks beyond summarization and paraphrasing, particularly generation tasks with different characteristics (e.g., dialogue, question answering)
3. Perform detailed error analysis comparing linguistic quality before and after self-distillation to understand which types of errors are most effectively corrected through this refinement phase