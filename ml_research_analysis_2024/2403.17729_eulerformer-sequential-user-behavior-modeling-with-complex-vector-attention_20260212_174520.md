---
ver: rpa2
title: 'EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention'
arxiv_id: '2403.17729'
source_url: https://arxiv.org/abs/2403.17729
tags:
- positional
- eulerformer
- complex
- difference
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of modeling sequential user behavior
  in recommendation systems by improving the transformer architecture's ability to
  capture both semantic and positional differences. The authors propose EulerFormer,
  a novel transformer variant with complex vector attention that unifies semantic
  and positional difference modeling using Euler's formula to transform token embeddings
  into polar-form complex vectors.
---

# EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention

## Quick Facts
- **arXiv ID:** 2403.17729
- **Source URL:** https://arxiv.org/abs/2403.17729
- **Reference count:** 40
- **Primary result:** Achieves up to 13% relative improvement in NDCG@10 compared to SASRec on sequential recommendation tasks

## Executive Summary
This paper addresses the problem of modeling sequential user behavior in recommendation systems by improving the transformer architecture's ability to capture both semantic and positional differences. The authors propose EulerFormer, a novel transformer variant with complex vector attention that unifies semantic and positional difference modeling using Euler's formula to transform token embeddings into polar-form complex vectors. A key innovation is the differential rotation mechanism with an adaptation function that can flexibly integrate semantic and positional differences according to context. Experiments on four public datasets show that EulerFormer consistently outperforms strong baselines, achieving up to 13% relative improvement in NDCG@10 compared to SASRec. The method is efficient with linear complexity and has good properties for handling long sequences.

## Method Summary
EulerFormer introduces a transformer architecture with complex vector attention that transforms token embeddings into polar-form complex vectors using Euler's formula. The core innovation is unifying semantic difference (Œîùë∫) and positional difference (Œîùë∑) modeling through complex vector rotation, where both differences are expressed as rotation angles in complex space. An adaptation function adjusts semantic rotation angles with layer-specific parameters, enabling flexible integration of semantic and positional information. The model also incorporates phase contrastive learning to improve representation isotropy by focusing on phase components during training. The approach maintains linear complexity while achieving superior performance on sequential recommendation tasks.

## Key Results
- Achieves up to 13% relative improvement in NDCG@10 compared to SASRec baseline
- Demonstrates consistent performance gains across four public datasets (MovieLens-1M, MovieLens-20M, Yelp2022, Amazon_Books)
- Shows effectiveness for long-sequence modeling with improved handling of positional information
- Maintains linear complexity while outperforming more complex transformer variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EulerFormer unifies semantic and positional difference modeling through complex vector rotation, achieving more expressive sequence modeling than prior methods that treat these differences separately.
- Mechanism: The core innovation is transforming token embeddings into polar-form complex vectors using Euler's formula, where semantic difference (Œîùë∫) and positional difference (Œîùë∑) are both expressed as rotation angles in complex space. This enables unified modeling of both differences in the form "exp[ùëñ (Œîùë∫ + Œîùë∑)]".
- Core assumption: Semantic and positional differences can be meaningfully represented as rotation angles in complex space, and their additive combination in the complex exponent preserves their distinct interpretability while enabling unified computation.
- Evidence anchors:
  - [abstract] "provides a unified theoretical framework to formulate both semantic difference and positional difference"
  - [section 3.2.1] "casts the dot-product attention into a complex vector rotation, enabling the unified modeling of both semantic and positional difference in a rotation form"
  - [corpus] Weak corpus support for the specific mathematical mechanism; neighboring papers discuss complex vector attention but not the specific Euler-based unification described here.
- Break condition: If semantic differences are not appropriately scaled to be comparable to positional differences, the unified rotation approach may become ineffective or lead to gradient issues.

### Mechanism 2
- Claim: The differential rotation mechanism with adaptive function allows flexible integration of semantic and positional differences according to context.
- Mechanism: An adaptation function Adapt(¬∑) adjusts the semantic rotation angles based on layer-specific parameters (scale factor ùúπ and bias ùíÉ), enabling the model to dynamically balance semantic and positional contributions in each layer.
- Core assumption: Different layers in the transformer benefit from different relative weights of semantic versus positional information, and learnable adaptation parameters can discover these optimal weights.
- Evidence anchors:
  - [abstract] "develops a differential rotation mechanism, where the semantic rotation angles can be controlled by an adaptation function"
  - [section 3.2.2] "enables the model to better capture the sequential dependencies" through "generalized adaptive rotation"
  - [corpus] No direct corpus evidence for this specific adaptive rotation mechanism; neighboring papers mention complex attention but not adaptive integration of semantic and positional differences.
- Break condition: If the adaptation function overfits to specific contexts or if semantic differences dominate positional differences too strongly, the positional information may be effectively ignored.

### Mechanism 3
- Claim: Phase contrastive learning enhances isotropy of contextual representations, improving the effectiveness of positional encoding.
- Mechanism: By focusing only on the phase component of transformed embeddings and using contrastive learning between augmented (masked) and original phases, the method encourages diverse orientations of item representations while maintaining semantic consistency.
- Core assumption: Anisotropic representations (where items cluster in representation space) hinder effective positional encoding and sequence modeling, and encouraging diverse orientations improves model performance.
- Evidence anchors:
  - [abstract] "proposes a phase contrastive learning task to improve the isotropy of contextual representations in EulerFormer"
  - [section 3.3] "can effectively enhance the discriminability of different items, while not compromising the other semantics"
  - [corpus] No corpus evidence for this specific phase contrastive learning approach; neighboring papers discuss contrastive learning but not phase-specific methods for isotropy.
- Break condition: If the contrastive learning objective conflicts with the primary recommendation task or if phase augmentation destroys meaningful semantic relationships, performance may degrade.

## Foundational Learning

- Concept: Complex vector space representation using Euler's formula
  - Why needed here: Provides the mathematical foundation for unifying semantic and positional differences through rotation angles
  - Quick check question: Can you express a 2D vector as a complex number and compute its polar form (modulus and phase)?

- Concept: Transformer self-attention mechanism and positional encoding
  - Why needed here: EulerFormer builds upon and extends the standard transformer architecture, requiring understanding of how self-attention works and why positional encoding is necessary
  - Quick check question: Why can't vanilla transformer self-attention capture sequence order, and how do positional encodings solve this?

- Concept: Contrastive learning and representation isotropy
  - Why needed here: Phase contrastive learning is a key component for improving representation quality, requiring understanding of contrastive objectives and their effects on embedding distributions
  - Quick check question: What is the difference between instance discrimination and feature-level contrastive learning, and how does each affect representation space?

## Architecture Onboarding

- Component map: Input embeddings -> Euler Transformation -> Adaptive Rotation -> Differential Rotation -> Attention computation -> Phase Contrastive Learning -> Output representations

- Critical path:
  1. Transform input embeddings to complex space
  2. Apply adaptive rotation to queries and keys
  3. Compute attention scores using unified semantic+positional rotation
  4. Backpropagate through both recommendation loss and phase contrastive loss
  5. Apply inverse transformation to obtain final representations

- Design tradeoffs:
  - The unified rotation approach trades mathematical simplicity for potential expressiveness gains
  - Phase contrastive learning adds training complexity but improves representation quality
  - Complex number operations require careful implementation to maintain efficiency

- Failure signatures:
  - Vanishing gradients in early layers if semantic differences dominate too strongly
  - Mode collapse in contrastive learning if temperature is too high
  - Performance degradation on short sequences where positional information is less critical

- First 3 experiments:
  1. Ablation study: Remove adaptive rotation (fix ùúπ=1, ùíÉ=0) to verify its contribution
  2. Ablation study: Remove phase contrastive learning to measure its impact on isotropy
  3. Sequence length analysis: Test performance on varying sequence lengths to verify long-sequence benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EulerFormer perform when scaling to extremely long sequences beyond typical recommendation scenarios (e.g., thousands of items)?
- Basis in paper: [explicit] The authors claim "one advantage of adaptive positional encoding in EulerFormer lies in its capacity for handling the long-sequence data" and show improvements on relatively long sequences, but don't test with sequences of thousands of items.
- Why unresolved: The experiments only test sequences up to length 250, which is considered "long" for recommendations but not for other sequence modeling tasks. The theoretical properties for handling long-term decay don't directly translate to empirical performance on much longer sequences.
- What evidence would resolve it: Testing EulerFormer on datasets with much longer sequences (e.g., web browsing logs, chat conversations) and comparing its performance and efficiency to other transformers on sequences of 1000+ items.

### Open Question 2
- Question: Can the phase contrastive learning component be effectively applied to other transformer architectures beyond EulerFormer?
- Basis in paper: [inferred] The authors introduce phase contrastive learning specifically for EulerFormer and show it improves isotropy, but the mechanism of using phase information for contrastive learning could potentially benefit other models.
- Why unresolved: The paper only evaluates phase contrastive learning within the EulerFormer framework. It's unclear whether the benefits come from the specific complex vector representation or if the contrastive approach could be generalized to other architectures.
- What evidence would resolve it: Implementing phase contrastive learning on standard transformer models (like BERT or GPT) and comparing performance with and without this component across various sequence modeling tasks.

### Open Question 3
- Question: What is the optimal balance between semantic difference and positional difference for different recommendation domains (e.g., e-commerce vs. music streaming)?
- Basis in paper: [explicit] The authors note that "users freely interact with interested items, the generated interaction sequence can be more complex than formal natural language text, which makes the semantic differences among items highly varied across different layers or sequences" and propose an adaptive mechanism, but don't explore domain-specific optimization.
- Why unresolved: While the adaptation function provides flexibility, the paper uses a single set of hyperparameters across all datasets without exploring whether different recommendation domains benefit from different balances between semantic and positional information.
- What evidence would resolve it: Domain-specific hyperparameter tuning experiments across diverse recommendation scenarios (e-commerce, music, video, social media) to identify optimal semantic-to-positional ratios for each domain.

## Limitations

- The paper relies heavily on the assumption that semantic and positional differences can be meaningfully unified through complex vector rotation, with limited empirical validation of this specific mathematical framework
- Phase contrastive learning lacks ablation studies showing its relative importance compared to other methods of addressing anisotropic representations
- The adaptation function parameters (ùúπ, ùíÉ) are treated as hyperparameters without clear justification for their initialization or sensitivity analysis

## Confidence

- **High confidence** in the mathematical framework and complex vector transformation approach
- **Medium confidence** in the adaptive rotation mechanism's ability to flexibly balance semantic and positional information across layers
- **Medium confidence** in phase contrastive learning's contribution to representation quality

## Next Checks

1. **Ablation Study**: Remove the adaptation function (fix ùúπ=1, ùíÉ=0) and compare performance to verify whether the flexible integration of semantic and positional differences is essential or if simple summation suffices.

2. **Parameter Sensitivity**: Conduct a systematic study of the adaptation function parameters (ùúπ, ùíÉ) across different initialization ranges and learning rates to understand their impact on model performance and convergence.

3. **Isotropy Analysis**: Measure representation isotropy (e.g., using average pairwise cosine similarity) with and without phase contrastive learning to quantify its effect on embedding space structure and verify whether improved isotropy correlates with better recommendation performance.