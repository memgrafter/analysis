---
ver: rpa2
title: Understanding Epistemic Language with a Language-augmented Bayesian Theory
  of Mind
arxiv_id: '2408.12022'
source_url: https://arxiv.org/abs/2408.12022
tags:
- player
- language
- epistemic
- beliefs
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LaBToM, a computational model that interprets
  epistemic language by grounding it in inferences about others' beliefs using Bayesian
  theory-of-mind. The model translates natural language into an epistemic language-of-thought
  (ELoT) via grammar-constrained LLM decoding, then evaluates these translations against
  BToM inferences.
---

# Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind

## Quick Facts
- arXiv ID: 2408.12022
- Source URL: https://arxiv.org/abs/2408.12022
- Reference count: 40
- Key outcome: LaBToM achieves r=0.76 correlation with human epistemic language judgments, outperforming GPT-4o (r=0.52) and Gemini (r=0.23)

## Executive Summary
This paper introduces LaBToM, a computational model that interprets natural language statements about beliefs by grounding them in Bayesian theory-of-mind inferences. The model translates natural language into an epistemic language-of-thought (ELoT) using grammar-constrained LLM decoding, then evaluates these translations against inferences from a generative model of rational action and perception. Tested on human judgments of statements about a player's beliefs in a maze puzzle, LaBToM significantly outperforms multimodal LLM baselines and demonstrates the importance of compositional representation for capturing graded plausibility judgments of epistemic claims.

## Method Summary
LaBToM combines language models with Bayesian theory-of-mind to interpret epistemic language. The model uses grammar-constrained sequential Monte Carlo (SMC) decoding to translate natural language statements into an epistemic language-of-thought (ELoT) formalism, which explicitly represents epistemic operators as probability thresholds. These ELoT formulas are then evaluated against belief distributions inferred by inverting a generative model of rational action and perception (BToM). The BToM component models agents as approximately rational decision-makers in partially observable Markov decision processes, inferring beliefs from observations and actions. The model's predictions are validated against human plausibility judgments of statements about beliefs in a grid-world maze navigation task.

## Key Results
- LaBToM achieves r=0.76 correlation with human ratings of epistemic statements, significantly outperforming GPT-4o (r=0.52) and Gemini (r=0.23)
- ELoT representation provides 2.4× more accurate translations than a lowered form alternative, with 81-91% equivalence accuracy
- The model successfully captures how human evaluations change with agent behavior and distinguishes in-context from out-of-context statement evaluation with 70% accuracy
- Grammar-constrained SMC decoding improves translation accuracy from 34-42% to 81-91% compared to unconstrained methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The ELoT representation enables compositional grounding of epistemic language into probabilistic mental state inference.
- **Mechanism:** Natural language sentences about beliefs are parsed into ELoT formulas that explicitly encode epistemic operators as probability thresholds. These formulas are then evaluated against inferred distributions over agent beliefs from BToM.
- **Core assumption:** Epistemic language can be represented at a level of granularity that mirrors natural language structure while mapping cleanly to probabilistic mental state representations.
- **Evidence anchors:**
  - [abstract] "By translating natural language into an epistemic 'language-of-thought' with grammar-constrained LLM decoding, then evaluating these translations against the inferences produced by inverting a generative model of rational action and perception, LaBToM captures graded plausibility judgments of epistemic claims."
  - [section 3.1.1] "A key aspect of ELoT expressions is that they represent epistemic concepts at a similar level of granularity as our tested language (English), simplifying the mapping to ELoT propositions."

### Mechanism 2
- **Claim:** Grammar-constrained SMC decoding produces more accurate semantic parsing than unconstrained LLM generation.
- **Mechanism:** SMC sampling constrained by the ELoT grammar samples completions from the posterior distribution over syntactically valid ELoT formulas, avoiding syntax errors and improving translation accuracy from 34-42% to 81-91%.
- **Core assumption:** Posterior sampling with syntactic constraints is more effective than beam search or greedy decoding for mapping diverse natural language to formal representations.
- **Evidence anchors:**
  - [section 4.4] "ELoT also increases the benefit of grammar-constrained SMC decoding, leading to an improvement of 0.20 in equivalence accuracy for LLaMa 3.1 8B."
  - [section 3.1.2] "Specifically, we use sequential Monte Carlo (SMC)-based grammar-constrained sampling (Loula et al., 2025), since it avoids the failure modes of beam search and greedy token-masking (Lew et al., 2023b)."

### Mechanism 3
- **Claim:** BToM inference over rational action and perception provides the computational basis for grounding epistemic language in observable behavior.
- **Mechanism:** Observers infer agent beliefs by inverting a generative model of how agents update beliefs from observations and act toward goals. The inferred belief distributions are then used to evaluate the probability of epistemic formulas.
- **Core assumption:** Human theory-of-mind involves approximate Bayesian inference over mental state representations that guide rational action and perception.
- **Evidence anchors:**
  - [abstract] "By translating natural language into an epistemic 'language-of-thought' with grammar-constrained LLM decoding, then evaluating these translations against the inferences produced by inverting a generative model of rational action and perception..."
  - [section 3.2.1] "Following the structure of Partially Observable Markov Decision Processes (POMDPs) (Kaelbling et al., 1998), this theory of approximately rational agency can be formalized as a probabilistic generative model."

## Foundational Learning

- **Concept:** Bayesian inference over generative models
  - Why needed here: The core of LaBToM is inverting a generative model of rational agency to infer beliefs from actions and observations
  - Quick check question: How would you compute P(beliefs | actions, observations) using Bayes' rule given a generative model P(actions, observations | beliefs)?

- **Concept:** Probabilistic programming and SMC sampling
  - Why needed here: The grammar-constrained SMC decoder requires understanding of sequential Monte Carlo for sampling from constrained distributions
  - Quick check question: What is the key difference between SMC sampling and beam search when decoding constrained sequences?

- **Concept:** Epistemic modal logic and probability semantics
  - Why needed here: ELoT builds on formal semantics of epistemic modality using probability thresholds rather than truth conditions
  - Quick check question: How does "believes(A, ϕ)" differ from "knows(A, ϕ)" in the ELoT formalism?

## Architecture Onboarding

- **Component map:** Natural language input → Grammar-constrained SMC decoder → ELoT formula → BToM inference engine → Probability evaluation → Human judgment comparison
- **Critical path:**
  1. Parse natural language to ELoT using grammar-constrained SMC
  2. Run BToM inference to obtain belief distributions
  3. Evaluate ELoT formulas against inferred beliefs
  4. Compare with human ratings

- **Design tradeoffs:**
  - ELoT vs. lowered form: ELoT provides better alignment with natural language structure but requires more complex parsing
  - Exact vs. approximate inference: Exact inference is tractable for small problems but scales poorly; SMC approximation would be needed for larger domains
  - Grammar constraints vs. flexibility: Strict grammar ensures valid outputs but may miss some valid paraphrases

- **Failure signatures:**
  - Low correlation with human ratings: Could indicate parsing errors, threshold miscalibration, or BToM inference issues
  - Syntax errors in ELoT output: Suggests SMC sampling not properly constrained by grammar
  - Mode collapse in sampling: May indicate LLM fine-tuning issues affecting sample diversity

- **First 3 experiments:**
  1. Test ELoT parsing accuracy on a small set of gold-translated sentences with different decoding methods
  2. Validate BToM inference produces reasonable belief distributions for simple scenarios
  3. Evaluate full pipeline on a single scenario with known ground truth to debug integration issues

## Open Questions the Paper Calls Out
None

## Limitations
- Scaling to richer epistemic vocabulary: ELoT was specifically designed for experimental stimuli and may not generalize to the full richness of natural language epistemic expressions
- Robustness across contexts: Performance on different scenarios beyond maze navigation is unknown
- Threshold calibration: Probability thresholds for epistemic operators were set based on linguistic intuition rather than empirical calibration

## Confidence

**High confidence:** The core architectural claim that combining grammar-constrained SMC decoding with BToM inference outperforms standard LLM baselines is well-supported by experimental results with 2.4× improvement in translation accuracy.

**Medium confidence:** The claim that ELoT provides the right level of granularity for epistemic language representation is supported by improved performance but limited to a single domain.

**Low confidence:** The assertion that this approach captures "graded plausibility judgments" as humans make them is based on correlation with human ratings in a specific experimental setup.

## Next Checks
1. Evaluate LaBToM on at least two additional domains with different action structures to assess generalization beyond maze navigation
2. Systematically vary the probability thresholds used to interpret epistemic operators and measure impact on human judgment correlation
3. Create test cases with subtle epistemic distinctions to probe whether ELoT and BToM can capture fine-grained epistemic meaning differences