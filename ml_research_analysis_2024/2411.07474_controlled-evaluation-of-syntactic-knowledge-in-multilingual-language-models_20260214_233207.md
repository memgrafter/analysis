---
ver: rpa2
title: Controlled Evaluation of Syntactic Knowledge in Multilingual Language Models
arxiv_id: '2411.07474'
source_url: https://arxiv.org/abs/2411.07474
tags:
- test
- language
- accuracy
- linguistics
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper develops and applies targeted syntactic evaluation\
  \ tests to assess how well open-access multilingual Transformer language models\
  \ (LMs) capture syntactic knowledge in low-resource languages. The authors created\
  \ evaluation materials for three low-resource languages\u2014Basque, Hindi, and\
  \ Swahili\u2014focusing on distinctive morphosyntactic phenomena: auxiliary verb\
  \ agreement in Basque, split ergativity in Hindi, and noun class agreement in Swahili."
---

# Controlled Evaluation of Syntactic Knowledge in Multilingual Language Models

## Quick Facts
- arXiv ID: 2411.07474
- Source URL: https://arxiv.org/abs/2411.07474
- Reference count: 16
- Key outcome: Multilingual Transformers perform well on Basque auxiliary and Hindi ergativity but struggle with Swahili noun classes; model size generally improves accuracy except for XGLM4.5B

## Executive Summary
This paper develops and applies targeted syntactic evaluation tests to assess how well open-access multilingual Transformer language models capture syntactic knowledge in low-resource languages. The authors created evaluation materials for three low-resource languages—Basque, Hindi, and Swahili—focusing on distinctive morphosyntactic phenomena: auxiliary verb agreement in Basque, split ergativity in Hindi, and noun class agreement in Swahili. They evaluated five families of multilingual Transformers across their various model sizes.

The results show that LMs generally perform well on Basque auxiliary agreement and Hindi split ergativity, with notable exceptions including multilingual BERT's bias toward the habitual aspect in Hindi and difficulties with indirect objects in Basque. Performance on Swahili noun class agreement proved more challenging, with models often performing near random guessing. The study found a positive relationship between model size and accuracy across most test suites, though XGLM4.5B systematically underperformed similar-sized models, possibly due to lack of low-resource language upsampling and the "curse of multilinguality." Syntactic complexity affected performance differently across languages, with intervening demonstrative phrases significantly degrading Swahili performance but not Hindi performance.

## Method Summary
The authors developed targeted syntactic evaluation materials for three low-resource languages, focusing on language-specific morphosyntactic phenomena. For Basque, they tested auxiliary verb agreement with different argument structures; for Hindi, they tested split ergativity patterns; and for Swahili, they tested noun class agreement. They created synthetic sentences with controlled linguistic properties, varying factors like syntactic complexity and animacy. Five multilingual Transformer families were evaluated (mGPT, BLOOM, XGLM, multilingual BERT, and XLM-RoBERTa) across multiple model sizes. Models were tested by comparing log-likelihoods of grammatical versus ungrammatical completions, with additional controls for animacy effects.

## Key Results
- Multilingual Transformers generally perform well on Basque auxiliary agreement and Hindi split ergativity tasks
- Performance on Swahili noun class agreement was notably poor, often near random guessing
- Larger models show better accuracy across most test suites, except XGLM4.5B which underperformed despite its size
- Syntactic complexity (intervening phrases) significantly impacted Swahili performance but not Hindi performance
- Multilingual BERT showed bias toward habitual aspect in Hindi split ergativity tests

## Why This Works (Mechanism)
Unknown. The paper does not explicitly detail the underlying mechanisms that explain why certain models perform better on specific syntactic tasks.

## Foundational Learning
- Morphosyntactic agreement: Why needed - Understanding how grammatical features like person, number, and gender are marked across sentence elements; Quick check - Identify subject-verb agreement patterns in a sample sentence
- Split ergativity: Why needed - Understanding how case marking and agreement patterns change based on tense/aspect in some languages; Quick check - Distinguish ergative vs. nominative case patterns in sample sentences
- Noun classes: Why needed - Understanding how languages categorize nouns into classes that affect agreement patterns; Quick check - Identify noun class markers in sample Swahili sentences
- Cross-linguistic syntactic variation: Why needed - Understanding how different languages implement similar syntactic concepts differently; Quick check - Compare argument structure patterns across Basque, Hindi, and Swahili
- Transformer language model evaluation: Why needed - Understanding how to assess syntactic knowledge in neural models; Quick check - Calculate log-likelihood differences for grammatical vs. ungrammatical completions

## Architecture Onboarding
Component map: Synthetic sentence generation -> Model embedding extraction -> Log-likelihood calculation -> Accuracy scoring
Critical path: Test sentence creation → Model embedding selection → Grammaticality judgment → Accuracy measurement
Design tradeoffs: Synthetic vs. natural sentences (control vs. ecological validity), perplexity thresholding (precision vs. coverage)
Failure signatures: Poor performance on syntactically complex sentences, bias toward certain grammatical patterns, size-related performance plateaus
First experiments: 1) Test on additional low-resource languages, 2) Vary embedding selection strategy, 3) Compare synthetic vs. natural sentence performance

## Open Questions the Paper Calls Out
Assumption: The paper does not explicitly call out open questions, though several are implicit in the limitations and next checks sections.

## Limitations
- Synthetic test sentences may not fully capture natural language complexity and variability
- Limited evaluation to only three low-resource languages constrains generalizability
- Use of inanimate subjects/objects limits applicability to naturalistic contexts
- Perplexity threshold selection may have excluded useful model representations

## Confidence
- High Confidence: Performance differences between model families, general relationship between model size and accuracy, multilingual BERT's habitual aspect bias
- Medium Confidence: Language-specific difficulty claims, impact of syntactic complexity across languages
- Low Confidence: Explanations for XGLM4.5B's underperformance, specific mechanisms behind curse of multilinguality

## Next Checks
1. Replicate key findings using naturally occurring sentences from corpus data for each language, particularly for Swahili noun class agreement
2. Extend the evaluation methodology to additional low-resource languages with different typological features (e.g., case systems, polysynthetic languages)
3. Compare performance across different embedding selection strategies (e.g., using all layers vs. perplexity-thresholded embeddings)