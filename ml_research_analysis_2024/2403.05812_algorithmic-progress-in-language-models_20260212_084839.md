---
ver: rpa2
title: Algorithmic progress in language models
arxiv_id: '2403.05812'
source_url: https://arxiv.org/abs/2403.05812
tags:
- data
- compute
- const
- algorithmic
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates algorithmic progress in pre-training language
  models by analyzing a dataset of over 200 language model evaluations on Wikitext
  and Penn Treebank spanning 2012-2023. The authors find that the compute required
  to reach a set performance threshold has halved approximately every 8 months, substantially
  faster than hardware gains per Moore's Law.
---

# Algorithmic progress in language models

## Quick Facts
- arXiv ID: 2403.05812
- Source URL: https://arxiv.org/abs/2403.05812
- Reference count: 7
- Key outcome: Compute required to reach a performance threshold halves every 8 months, with 60-95% of gains from scaling compute rather than algorithmic innovation

## Executive Summary
This paper investigates algorithmic progress in pre-training language models by analyzing a dataset of over 200 model evaluations spanning 2012-2023. The authors find that the compute required to reach a set performance threshold has halved approximately every 8 months, substantially faster than hardware gains per Moore's Law. Using augmented scaling laws, they quantify algorithmic progress and determine the relative contributions of scaling models versus innovations in training algorithms. Despite the rapid pace of algorithmic progress and the development of new architectures such as the transformer, the analysis reveals that the increase in compute made an even larger contribution to overall performance improvements over this time period.

## Method Summary
The authors analyze language model evaluations on Wikitext and Penn Treebank benchmarks from 2012-2023 to quantify algorithmic progress. They fit augmented scaling laws to model the relationship between loss, parameters, and dataset size, then use these models to estimate effective compute doubling times. The contribution of compute scaling versus algorithmic improvements is decomposed using Shapley values, and the transformer's compute-equivalent gain is estimated by comparing transformer and non-transformer models within the same scaling law framework.

## Key Results
- Compute required to reach a fixed performance threshold has halved approximately every 8 months
- 60-95% of performance gains stem from compute scaling, while algorithms contribute only 5-40%
- The transformer architecture provided between 3x and 46x in compute-equivalent gain, representing about 20% of total algorithmic gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Algorithmic progress in language models is measurable via compute efficiency improvements.
- Mechanism: Language model capabilities improve over time both by increasing compute and by making better use of existing compute. The authors quantify this by fitting scaling laws to performance data, isolating how much of the gain is from "effective compute" (i.e., better algorithms) versus raw compute scaling.
- Core assumption: The scaling law form used (cross-entropy loss vs. compute and dataset size) accurately models both the effect of scaling and algorithmic improvements.
- Evidence anchors:
  - [abstract] "the compute required to reach a set performance threshold has halved approximately every 8 months"
  - [section] "We quantify algorithmic progress in terms of reductions of the resources (N and D) required to achieve the same level of performance over time."
  - [corpus] Corpus evidence is weak or missing; related work exists but does not directly validate this specific mechanism.
- Break condition: If the underlying scaling law does not accurately describe the data (e.g., for very small or very large compute regimes), the computed efficiency gains may be inaccurate.

### Mechanism 2
- Claim: Most recent language model performance gains come from scaling compute rather than algorithmic innovation.
- Mechanism: By decomposing performance improvements into contributions from scaling compute and data versus improvements in algorithmic efficiency (using Shapley values), the authors find that scaling compute and data accounts for the majority of recent gains.
- Core assumption: The decomposition method (Shapley values) correctly attributes marginal improvements to each factor.
- Evidence anchors:
  - [abstract] "our analysis reveals that the increase in compute made an even larger contribution to overall performance improvements over this time period."
  - [section] "60-95% of the performance gains stem from compute scaling, while algorithms contribute only 5-40%."
  - [corpus] Related work in other domains (e.g., linear programming, SAT solvers) shows similar patterns where scaling dominates recent improvements.
- Break condition: If algorithmic innovations become much more efficient at larger scales, the attribution could shift over time.

### Mechanism 3
- Claim: The transformer architecture provided a substantial compute-equivalent gain.
- Mechanism: By modifying the scaling law to include a term for transformer vs. non-transformer architectures, the authors estimate the transformer's compute-equivalent gain as 3x-46x, accounting for about 20% of total algorithmic gains.
- Core assumption: The transformer's efficiency gain can be captured as a constant multiplicative factor on the reducible loss term.
- Evidence anchors:
  - [abstract] "The introduction of the transformer architecture in 2017 was a major algorithmic advance, representing between 3x and 46x in compute-equivalent gain"
  - [section] "The transformer architecture typically lowers reducible loss proportionally by 4.6% [95% CI: 3.0%, 7.0%]."
  - [corpus] The transformer's significance is well-established in the literature, though direct comparison to other architectures is limited.
- Break condition: If other architectures improve significantly or if the transformer's advantage diminishes at very large scales, this estimate may overstate its long-term contribution.

## Foundational Learning

- Concept: Neural scaling laws
  - Why needed here: The paper's entire analysis rests on the idea that model performance scales predictably with compute, data, and model size. Understanding the form and limitations of these laws is essential to interpreting the results.
  - Quick check question: What are the two main components of neural scaling laws as applied in this paper?

- Concept: Shapley value decomposition
  - Why needed here: The authors use Shapley values to attribute performance improvements to different factors (compute scaling, data scaling, algorithmic progress). Understanding this method is key to interpreting the "relative contributions" claims.
  - Quick check question: How does the Shapley value method avoid double-counting when attributing performance gains?

- Concept: Compute-optimal training
  - Why needed here: The paper discusses both compute scaling and algorithmic progress in the context of achieving optimal performance for a given compute budget. Understanding what "compute-optimal" means is important for interpreting the efficiency gains.
  - Quick check question: What is the relationship between model size and dataset size under compute-optimal scaling?

## Architecture Onboarding

- Component map: Data collection -> Scaling law fitting -> Decomposition -> Interpretation of results
- Critical path: Data collection → Scaling law fitting → Decomposition → Interpretation of results
- Design tradeoffs:
  - Using perplexity as a performance metric provides a common basis for comparison but may not capture all aspects of model capability
  - Focusing on established benchmarks ensures comparability but may miss innovations evaluated on different tasks
  - The chosen scaling law form simplifies analysis but may not capture all relevant factors (e.g., data quality, fine-tuning)
- Failure signatures:
  - Poor fit of scaling laws to the data (e.g., high residuals, systematic biases) suggests the model form is misspecified
  - Unstable or implausible estimates of algorithmic progress rates (e.g., negative doubling times) indicate issues with the decomposition method or data quality
  - Large discrepancies between different model specifications or robustness checks suggest high uncertainty in the estimates
- First 3 experiments:
  1. Replicate the core analysis on a subset of the data (e.g., only transformer models) to check sensitivity to architecture
  2. Test alternative performance metrics (e.g., fine-tuned task performance) to see if results generalize beyond perplexity
  3. Fit the scaling laws to data from a different domain (e.g., computer vision) to check if the methodology is broadly applicable

## Open Questions the Paper Calls Out
None

## Limitations
- Potential for double-counting when attributing performance gains to both compute scaling and algorithmic progress due to ambiguity in how innovations affect model performance
- Reliance on perplexity as a proxy for model capability may not capture all aspects of language model performance, particularly for downstream tasks
- Data coverage issues, particularly for early years (pre-2017), may lead to unreliable estimates of algorithmic progress rates

## Confidence
- High Confidence: The overall finding that algorithmic progress has been substantial and faster than hardware improvements (compute halving every 8 months) is well-supported by the data
- Medium Confidence: The attribution of performance gains to compute scaling versus algorithmic progress (60-95% from compute) is more sensitive to model specification and data quality
- Low Confidence: The absolute rates of algorithmic progress and the precise contributions of different innovations are highly uncertain due to model specification choices and data coverage issues

## Next Checks
1. Perform sensitivity analysis on model specification by re-running the analysis using alternative scaling law forms and comparing the resulting estimates of algorithmic progress
2. Collect and analyze data on model performance on downstream tasks beyond perplexity to determine whether pre-training efficiency trends generalize to practical applications
3. Conduct structured interviews with language model researchers to elicit expert estimates of innovation contributions and compare with quantitative estimates from the scaling law analysis