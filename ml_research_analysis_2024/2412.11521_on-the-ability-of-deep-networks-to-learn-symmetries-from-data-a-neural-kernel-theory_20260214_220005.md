---
ver: rpa2
title: 'On the Ability of Deep Networks to Learn Symmetries from Data: A Neural Kernel
  Theory'
arxiv_id: '2412.11521'
source_url: https://arxiv.org/abs/2412.11521
tags:
- kernel
- latexit
- neural
- error
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether conventional deep networks can
  learn symmetries from data without explicit architectural constraints. The authors
  propose a "neural kernel theory" that analyzes generalization in the infinite-width
  limit using kernel methods.
---

# On the Ability of Deep Networks to Learn Symmetries from Data: A Neural Kernel Theory
## Quick Facts
- **arXiv ID**: 2412.11521
- **Source URL**: https://arxiv.org/abs/2412.11521
- **Reference count**: 40
- **Primary result**: Conventional deep networks cannot learn symmetries not explicitly embedded in their architecture, as shown by a neural kernel theory in the infinite-width limit.

## Executive Summary
This paper investigates whether standard deep networks can learn symmetries from data without explicit architectural constraints. The authors propose a neural kernel theory that analyzes generalization in the infinite-width limit using kernel methods. They construct classification problems where some classes include all transformations of a cyclic group while others include only subsets. Using Gaussian process regression with neural tangent kernels, they derive a spectral error formula that predicts generalization performance based on the ratio of inverse kernel frequency powers. Their theory reveals that successful generalization requires either good class separation in kernel space or dense class orbits. Empirical results on rotated-MNIST datasets show that standard architectures (MLP, CNN, ViT) fail to generalize symmetries to partially observed classes, confirming the theoretical predictions. The work concludes that conventional deep networks lack mechanisms to learn symmetries not explicitly embedded in their architecture.

## Method Summary
The authors analyze symmetry learning using Neural Tangent Kernel (NTK) theory in the infinite-width limit. They construct classification tasks with cyclic group symmetries where some classes contain complete orbits and others contain partial orbits. The kernel matrix over these symmetric datasets is circulant, allowing Fourier analysis. They derive a spectral error formula εs = λ⁻¹_N / ⟨λ⁻¹⟩ that predicts generalization performance based on the ratio of the highest frequency eigenvalue to the average of inverse eigenvalues. The framework is validated empirically on rotated-MNIST datasets using standard architectures (MLP, CNN, ViT) and equivariant architectures, comparing theoretical predictions with actual generalization performance.

## Key Results
- Standard architectures (MLP, CNN, ViT) fail to generalize symmetries to partially observed classes on rotated-MNIST datasets
- The spectral error formula εs = λ⁻¹_N / ⟨λ⁻¹⟩ accurately predicts generalization behavior based on class separation and orbit density
- Equivariant architectures succeed only when their encoded symmetry matches the dataset's inherent symmetry
- The frozen kernel property in infinite-width networks prevents learning of symmetries not explicitly built into the architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conventional deep networks cannot learn symmetries not explicitly embedded in their architecture due to the frozen kernel property in the infinite-width limit.
- Mechanism: In the infinite-width limit, neural networks behave as kernel machines with a fixed kernel determined by the architecture. This kernel cannot adapt to learn new symmetries present in the data that weren't built into the architecture design.
- Core assumption: The equivalence between infinitely wide neural networks and kernel machines holds (Neural Tangent Kernel theory).
- Evidence anchors:
  - [abstract] "In the infinite-width limit, where kernel analogies apply, we derive a neural kernel theory of symmetry learning"
  - [section] "Typical neural kernels (MLP, CNN) greatly simplify when computed over a dataset generated by a cyclic group action"
  - [corpus] Weak - no direct evidence in related papers about frozen kernel limitations
- Break condition: The frozen kernel assumption breaks when networks are finite-width and can adapt their kernel during training.

### Mechanism 2
- Claim: Generalization success depends on the ratio of inverse kernel frequency powers in the Fourier domain.
- Mechanism: The spectral error formula shows that successful generalization requires either good class separation in kernel space or dense class orbits. This is captured by the ratio of the highest frequency eigenvalue to the average of inverse eigenvalues.
- Core assumption: The kernel matrix over cyclic group-generated datasets is circulant, allowing Fourier analysis.
- Evidence anchors:
  - [section] "We find that the generalization behavior of networks is predicted by a simple ratio of inverse kernel frequency powers"
  - [section] "The presence of the aforementioned high frequency component in the kernel is manifested in the low value of λ⁻¹ₙ"
  - [corpus] Weak - no direct evidence in related papers about spectral error formulas
- Break condition: When the circulant structure assumption fails (e.g., non-cyclic groups or complex symmetries).

### Mechanism 3
- Claim: Equivariant architectures succeed when their encoded symmetry matches the dataset's inherent symmetry.
- Mechanism: When architecture and data symmetry match, the kernel matrix becomes rank-deficient or constant, causing perfect generalization. When they mismatch, the kernel remains circulant but not rank-deficient, leading to generalization failure.
- Core assumption: The kernel matrix properties (constant, circulant, rank-deficient) directly determine generalization behavior.
- Evidence anchors:
  - [section] "Our framework also applies to equivariant architectures (e.g., CNNs), and recovers their success in the special case where the architecture matches the inherent symmetry of the data"
  - [section] "The kernel matrix of a global average pooling convolutional network over a translation orbit is constant"
  - [corpus] Weak - no direct evidence in related papers about equivariant architecture success conditions
- Break condition: When the architecture's equivariance doesn't match the data symmetry or when the kernel matrix doesn't exhibit the expected properties.

## Foundational Learning

- Concept: Kernel methods and Fourier analysis of circulant matrices
  - Why needed here: The entire theoretical framework relies on analyzing neural kernels in the Fourier domain when the kernel matrix is circulant due to cyclic group structure.
  - Quick check question: What property of a matrix makes it diagonal in the Fourier basis, and why is this important for analyzing neural networks on symmetric datasets?

- Concept: Neural Tangent Kernel (NTK) theory and infinite-width limit
  - Why needed here: The theory characterizes generalization behavior in the infinite-width limit where neural networks become equivalent to kernel machines.
  - Quick check question: What happens to the neural network's ability to learn from data when the width goes to infinity, and how does this relate to kernel methods?

- Concept: Group theory and representation theory basics
  - Why needed here: The datasets are constructed using cyclic group actions, and understanding how group representations affect the kernel structure is crucial.
  - Quick check question: How does a group action on data affect the structure of the kernel matrix, and why does this matter for symmetry learning?

## Architecture Onboarding

- Component map: Neural network architecture (defining kernel) -> Dataset structure with cyclic group symmetries -> Generalization task on partially observed symmetries -> Performance determined by spectral error ratio
- Critical path: 1) Compute the neural kernel over the symmetric dataset, 2) Check if the kernel matrix is circulant, 3) Analyze the spectrum in Fourier domain, 4) Compute the spectral error ratio, 5) Determine if local structure dominates over symmetric structure
- Design tradeoffs: Fully connected networks vs. equivariant architectures - fully connected networks cannot learn symmetries but can be simpler, while equivariant networks succeed only when their symmetry matches the data but require architectural constraints upfront
- Failure signatures: Poor generalization on partially observed classes despite good performance on fully observed classes, spectral error ratio indicating dominance of symmetric structure over local structure, kernel matrix not being rank-deficient when architecture and data symmetry match
- First 3 experiments:
  1. Test a standard MLP on rotated-MNIST with one digit class missing some rotations - expect poor generalization on missing rotations
  2. Test a CNN with global average pooling on translated-MNIST - expect perfect generalization due to matching symmetry
  3. Test a CNN without global pooling on rotated-MNIST - expect poor generalization as with MLP, confirming architecture mismatch

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the spectral kernel theory extend to continuous high-dimensional symmetry groups like SO(3) for real-world applications?
- Basis in paper: [explicit] Authors state in Discussion that they focused on discrete one-dimensional cyclic groups and ask whether the theory could be extended to continuous, high-dimensional, and more complicated groups like SO(3).
- Why unresolved: The current framework relies on circulant kernel matrices which are specific to cyclic groups. Extending to continuous groups requires different mathematical machinery and assumptions about kernel structure.
- What evidence would resolve it: Successful application of the spectral error formula to datasets with continuous symmetries, showing similar predictive power for generalization behavior on leave-out classes.

### Open Question 2
- Question: How does the kernel theory apply to symmetry learning in latent spaces rather than native dataset spaces?
- Basis in paper: [explicit] Discussion mentions that group actions may more realistically exist in a latent space affecting the dataset indirectly, and notes that architectures cannot easily be designed to be equivariant to symmetries not directly acting in dataset space.
- Why unresolved: The current analysis assumes symmetries are "native" to the dataset space (like image rotations), but real-world symmetries often operate in complex latent representations.
- What evidence would resolve it: Empirical validation showing the spectral error formula predicts generalization behavior when symmetries operate through latent transformations, possibly using autoencoder-based architectures.

### Open Question 3
- Question: What training mechanisms beyond standard supervision could enable deep networks to learn symmetries from data?
- Basis in paper: [explicit] Discussion suggests that networks cannot learn symmetries because their kernel is defined by architecture and cannot adapt to dataset symmetries, and asks whether meta-learning, feature learning, or dynamic architecture choices could help.
- Why unresolved: While the paper demonstrates the limitations of conventional architectures, it does not provide or test specific alternative training approaches that might overcome these limitations.
- What evidence would resolve it: Demonstration of a training method that achieves symmetry generalization on partially observed classes where standard architectures fail, with theoretical analysis showing how the method modifies the effective kernel.

## Limitations

- The theory is limited to cyclic groups and may not generalize to more complex symmetry groups like SO(3)
- The infinite-width limit assumption may not accurately reflect the behavior of practical finite-width networks
- The framework assumes symmetries are "native" to the dataset space rather than operating through latent representations

## Confidence

- Frozen kernel property and NTK theory: High
- Spectral error formula derivation: Medium
- Empirical validation on rotated-MNIST: Medium
- Generalizability to other symmetry types: Low

## Next Checks

1. Test the spectral error formula prediction against finite-width networks of varying widths to assess the infinite-width assumption's practical validity
2. Evaluate the theory on non-cyclic symmetries (e.g., dihedral groups, permutation groups) to test generalizability beyond rotations
3. Compare the proposed spectral error ratio against alternative metrics like margin-based measures in predicting generalization performance