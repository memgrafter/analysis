---
ver: rpa2
title: Investigating Large Language Models for Complex Word Identification in Multilingual
  and Multidomain Setups
arxiv_id: '2411.01706'
source_url: https://arxiv.org/abs/2411.01706
tags:
- complexity
- probability
- complex
- prediction
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigated the use of large language models (LLMs)
  for complex word identification (CWI) and lexical complexity prediction (LCP) across
  multilingual and multidomain settings. We evaluated zero-shot, few-shot, and fine-tuning
  approaches on open-source models (Llama 2, Llama 3, Vicuna) and closed-source models
  (ChatGPT-3.5-turbo, GPT-4o) using datasets in English, German, and Spanish.
---

# Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups

## Quick Facts
- arXiv ID: 2411.01706
- Source URL: https://arxiv.org/abs/2411.01706
- Authors: Răzvan-Alexandru Smădu; David-Gabriel Ion; Dumitru-Clementin Cercel; Florin Pop; Mihaela-Claudia Cercel
- Reference count: 40
- Primary result: LLMs struggle with CWI/LCP tasks, showing task hallucination and lower performance than specialized baselines, even after fine-tuning.

## Executive Summary
This work systematically evaluates large language models (LLMs) for complex word identification (CWI) and lexical complexity prediction (LCP) across multilingual and multidomain settings. The study compares zero-shot, few-shot, and fine-tuning approaches using both open-source (Llama 2, Llama 3, Vicuna) and closed-source (ChatGPT-3.5-turbo, GPT-4o) models on English, German, and Spanish datasets. Results reveal that LLMs consistently underperform compared to specialized baseline models, with particular difficulties in identifying very difficult words and tendency toward task hallucination. Fine-tuned LLMs show modest improvements but still fall short of existing methods.

## Method Summary
The study evaluates LLMs on CWI 2018 and CompLex LCP 2021 datasets across three languages using multiple prompting strategies (zero-shot, few-shot, chain-of-thought) and fine-tuning approaches. Open-source models (Llama 2/3, Vicuna) and closed-source models (ChatGPT-3.5-turbo, GPT-4o) are assessed for their ability to identify complex words and predict lexical complexity. Performance is measured using F1-score, accuracy, Pearson correlation, and mean average error. The research also investigates task hallucination effects and prediction distribution patterns across different model architectures and prompting strategies.

## Key Results
- LLMs show consistent underperformance compared to specialized baseline models in both CWI and LCP tasks
- Task hallucination significantly impacts model performance, particularly in zero-shot settings
- Models tend to overestimate word simplicity, struggling specifically with identifying very difficult words
- Fine-tuning provides modest improvements but does not close the performance gap with specialized approaches
- Meta-learning with prompt tuning yields comparable results to zero-shot inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning LLMs on task-specific data improves performance on CWI and LCP tasks compared to zero-shot and few-shot settings.
- Mechanism: Fine-tuning allows the model to learn latent instructions and adapt to the specific task distribution, reducing task hallucination.
- Core assumption: The model can effectively learn from the provided task-specific data to improve its performance.
- Evidence anchors:
  - [abstract]: "Fine-tuned LLMs matched or slightly outperformed zero- and few-shot settings but still fell behind existing methods, particularly in identifying very difficult words."
  - [section 5.1]: "When fine-tuning, we notice that open-source models achieve competitive results with ChatGPT-3.5-tubo-ft."
  - [corpus]: Weak evidence. Related papers focus on machine-generated text detection, not CWI or LCP.
- Break condition: If the fine-tuning data is insufficient, noisy, or not representative of the task distribution, the model's performance may not improve significantly.

### Mechanism 2
- Claim: LLMs struggle with task hallucination, particularly in zero-shot and few-shot settings.
- Mechanism: The model fails to correctly reproduce the task instructions and target words from the input query, leading to incorrect predictions.
- Core assumption: The model's ability to understand and follow task instructions is crucial for accurate predictions.
- Evidence anchors:
  - [abstract]: "Results showed that LLMs struggled with task hallucination and achieved lower performance compared to smaller, specialized baseline models."
  - [section 6.1]: "We investigate the hallucination effects of LLMs reproducing the task firsthand before outputting the prediction."
  - [corpus]: Weak evidence. Related papers focus on machine-generated text detection, not task hallucination in CWI or LCP.
- Break condition: If the task instructions are clear, concise, and the model has sufficient capacity to understand them, the hallucination effect may be reduced.

### Mechanism 3
- Claim: LLMs tend to overestimate the ease of words, particularly for very difficult words.
- Mechanism: The model's prediction distribution is skewed towards easier labels, leading to misclassification of difficult words as easier ones.
- Core assumption: The model's internal representation of word difficulty is biased towards easier labels.
- Evidence anchors:
  - [abstract]: "However, in limited instances, they can achieve similar results with other, more lightweight approaches."
  - [section 6.4]: "We notice that the models struggle to identify the very difficult label, regardless of whether the model was fine-tuned or not."
  - [corpus]: Weak evidence. Related papers focus on machine-generated text detection, not word difficulty prediction.
- Break condition: If the model is explicitly trained to recognize and correctly label very difficult words, the overestimation bias may be reduced.

## Foundational Learning

- Concept: Prompt engineering and its impact on LLM performance.
  - Why needed here: The study investigates various prompting strategies (zero-shot, few-shot, chain-of-thought) and their influence on LLM performance in CWI and LCP tasks.
  - Quick check question: How does the choice of prompt template and example selection affect the model's ability to understand and solve the given task?

- Concept: Fine-tuning LLMs for task-specific performance.
  - Why needed here: The study explores the effectiveness of fine-tuning LLMs on CWI and LCP datasets compared to zero-shot and few-shot approaches.
  - Quick check question: What are the key considerations when preparing a fine-tuning dataset for LLMs in CWI and LCP tasks, and how do they impact the model's performance?

- Concept: Meta-learning and its application in few-shot learning scenarios.
  - Why needed here: The study investigates the use of meta-learning combined with prompt learning to improve the model's ability to adapt to new tasks with limited data.
  - Quick check question: How does the choice of meta-learning algorithm (e.g., FOMAML, Reptile) and the selection of pre-training tasks influence the model's performance in few-shot CWI and LCP settings?

## Architecture Onboarding

- Component map: Data preparation (CWI 2018, CompLex LCP 2021) -> LLM selection (Llama 2/3, Vicuna, ChatGPT, GPT-4o) -> Prompting strategies (zero-shot, few-shot, chain-of-thought) -> Fine-tuning (if applicable) -> Evaluation (F1, accuracy, Pearson, MAE) -> Analysis (hallucination, distribution, confusion matrices)

- Critical path: Data preparation → LLM selection → Prompting strategies → Fine-tuning (if applicable) → Evaluation → Analysis

- Design tradeoffs:
  - Model size vs. computational cost: Larger models may offer better performance but require more resources for training and inference
  - Prompt engineering vs. fine-tuning: Prompt engineering is less resource-intensive but may not always yield optimal results compared to fine-tuning
  - Multilingual vs. monolingual models: Multilingual models can handle multiple languages but may sacrifice performance on individual languages compared to monolingual models

- Failure signatures:
  - Task hallucination: Model fails to correctly reproduce task instructions and target words from the input query
  - Overestimation bias: Model tends to label difficult words as easier than they actually are
  - Language-specific performance: Model may struggle with certain languages or domains compared to others

- First 3 experiments:
  1. Evaluate zero-shot performance of open-source and closed-source LLMs on the CWI 2018 English dataset using the provided prompt templates
  2. Fine-tune a selected LLM (e.g., Llama 2 7B) on the CWI 2018 English dataset and compare its performance to the zero-shot setting
  3. Investigate the impact of few-shot examples and chain-of-thought prompting on the model's performance in the CWI task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can prompt design be optimized to reduce task hallucination in LLMs for CWI and LCP tasks?
- Basis in paper: [inferred] The paper discusses that LLMs struggle with task hallucination, particularly in zero-shot settings, and mentions that specific instructions can help reduce this issue.
- Why unresolved: The paper identifies task hallucination as a limitation but does not provide a detailed solution or framework for optimizing prompts to minimize this effect.
- What evidence would resolve it: Empirical studies comparing different prompt structures, including JSON formatting and guidance libraries, across multiple LLM architectures and tasks, with quantitative measures of hallucination reduction.

### Open Question 2
- Question: What is the optimal number of inference steps for achieving stable predictions in the LCP task across different LLM architectures?
- Basis in paper: [explicit] The paper investigates the effect of varying the number of inference steps (K) on prediction stability and concludes that at least 10-15 runs are required before scores stabilize.
- Why unresolved: While the paper provides insights into the number of inference steps needed for some models, it does not explore the optimal number across different architectures or its impact on other tasks.
- What evidence would resolve it: Comprehensive analysis of inference step stability across various LLM architectures, tasks, and datasets, with clear guidelines on the minimum number of steps required for reliable predictions.

### Open Question 3
- Question: How do adversarial examples specifically affect the performance of LLMs in CWI and LCP tasks, and what strategies can mitigate these effects?
- Basis in paper: [inferred] The paper mentions that investigating adversarial examples affecting model capabilities is a future direction, indicating a gap in understanding how these examples impact performance.
- Why unresolved: The paper identifies the need to explore adversarial examples but does not provide specific examples or strategies to address their impact on model performance.
- What evidence would resolve it: Identification and categorization of adversarial examples in CWI and LCP tasks, along with experiments testing various mitigation strategies, such as data augmentation or model robustness techniques.

## Limitations
- LLMs consistently underperform compared to specialized baseline models, particularly for identifying very difficult words
- Task hallucination remains a significant challenge across all prompting strategies
- The study's findings are based on specific datasets and may not generalize to all CWI/LCP scenarios
- Computational resources required for large LLMs may limit practical deployment

## Confidence
- Medium confidence in major claims regarding LLM underperformance in CWI/LCP tasks
- Medium confidence in findings about task hallucination effects
- Medium confidence in conclusions about fine-tuning benefits being limited

## Next Checks
1. Evaluate the performance of LLMs on additional CWI and LCP datasets, including those with different languages and domains, to assess the generalizability of the findings
2. Investigate the impact of prompt engineering techniques, such as few-shot learning and chain-of-thought prompting, on the model's ability to accurately identify and label very difficult words
3. Explore the use of alternative fine-tuning strategies, such as curriculum learning or multi-task learning, to improve the model's performance on CWI and LCP tasks and reduce task hallucination and overestimation bias