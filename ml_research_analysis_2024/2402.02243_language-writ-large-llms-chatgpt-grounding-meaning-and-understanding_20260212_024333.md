---
ver: rpa2
title: 'Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding'
arxiv_id: '2402.02243'
source_url: https://arxiv.org/abs/2402.02243
tags:
- grounding
- language
- understanding
- human
- sensorimotor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study explores why LLMs like ChatGPT can perform so well without\
  \ sensorimotor grounding, suggesting that certain inherent linguistic biases\u2014\
  such as propositional iconicity at scale, syntactic regularities, and constraints\
  \ from Universal Grammar\u2014may compensate for their lack of direct grounding.\
  \ It distinguishes between the Symbol Grounding Problem (linking symbols to referents)\
  \ and the Hard Problem of Consciousness (explaining subjective experience), arguing\
  \ that current LLMs lack both."
---

# Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding

## Quick Facts
- arXiv ID: 2402.02243
- Source URL: https://arxiv.org/abs/2402.02243
- Authors: Stevan Harnad
- Reference count: 15
- Key outcome: Explores why LLMs can perform well without sensorimotor grounding, suggesting inherent linguistic biases compensate for lack of direct grounding

## Executive Summary
This paper investigates why Large Language Models like ChatGPT can generate coherent, contextually appropriate outputs despite lacking sensorimotor grounding. The author argues that certain inherent linguistic biases—particularly propositional iconicity at scale, syntactic regularities, and constraints from Universal Grammar—may compensate for LLMs' inability to ground symbols in direct sensorimotor experience. The analysis distinguishes between the Symbol Grounding Problem (linking symbols to referents) and the Hard Problem of Consciousness (explaining subjective experience), arguing that current LLMs lack both forms of grounding despite their impressive performance.

## Method Summary
The study employs a dialogue format between the author and GPT-4, exploring various hypotheses about why LLMs can perform so well without grounding. Through iterative questioning and analysis, the author investigates potential compensatory mechanisms such as propositional iconicity, categorical perception effects, and linguistic constraints. The method focuses on theoretical reasoning and analysis of LLM capabilities rather than empirical experimentation with model performance.

## Key Results
- LLMs can generate coherent outputs through inherent linguistic biases despite lacking direct sensorimotor grounding
- The distinction between direct sensorimotor grounding and indirect verbal grounding is crucial for understanding LLM limitations
- Computational categorical perception effects may influence how LLMs process and generate language
- Propositional iconicity at LLM scale might provide constraints that shape model outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs generate coherent outputs through inherent constraints in language itself
- Mechanism: Language structure and patterns at large scale provide implicit biases that guide LLM outputs
- Core assumption: Language contains regularities that can be learned without direct experience
- Evidence anchors: Abstract mentions propositional iconicity at LLM scale; section discusses how this might explain performance
- Break condition: Failure to generate coherent outputs in contexts deviating from training data

### Mechanism 2
- Claim: Humans ground language through direct sensorimotor experiences plus indirect verbal descriptions, while LLMs rely only on indirect verbal grounding
- Mechanism: Language learning requires direct sensorimotor foundation which LLMs lack
- Core assumption: Language cannot be grounded through words alone
- Evidence anchors: Abstract states ChatGPT lacks direct sensorimotor grounding; section emphasizes no bottom-up grounding path exists
- Break condition: If LLMs demonstrate true understanding of sensorimotor concepts

### Mechanism 3
- Claim: LLMs exhibit categorical perception that influences internal representations
- Mechanism: Neural networks undergo dimension reduction during category learning, separating and compressing categories
- Core assumption: Computational CP effect occurs in LLMs
- Evidence anchors: Abstract mentions computational counterparts of human CP; section discusses CP effect in GPT-4 processing
- Break condition: Failure to demonstrate consistent category separation patterns

## Foundational Learning

- Concept: Symbol Grounding Problem (SGP)
  - Why needed here: Crucial for understanding LLM limitations and importance of grounding in language understanding
  - Quick check question: How does the SGP relate to direct vs indirect grounding in language learning?

- Concept: Turing Test (T2 and T3)
  - Why needed here: Provides framework for evaluating AI systems and grounding problem
  - Quick check question: What are key differences between T2 and T3 Turing Tests and how do they relate to grounding?

- Concept: Categorical Perception (CP)
  - Why needed here: Explains how systems learn category distinctions and how this manifests in LLM outputs
  - Quick check question: How does CP effect manifest in human perception and what are implications for AI systems?

## Architecture Onboarding

- Component map: Training data -> Processing algorithms -> Language model (GPT-4) -> Output generation
- Critical path: 1) Ingest and process large text corpus, 2) Identify and learn language patterns, 3) Generate outputs based on learned patterns, 4) Refine outputs based on feedback and context
- Design tradeoffs: Size vs efficiency (larger models more capable but resource-intensive), Generalizability vs specificity (diverse data more versatile but less specialized), Interpretability vs performance (complex models harder to understand but potentially more effective)
- Failure signatures: Inconsistent or nonsensical outputs, failure to generalize to new contexts, overfitting to specific training patterns
- First 3 experiments: 1) Test LLM's ability to generate coherent outputs in domain-specific context, 2) Evaluate LLM's performance on categorization tasks requiring nuanced distinctions, 3) Assess LLM's ability to learn new concepts through indirect verbal descriptions alone

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs develop propositional iconicity at the propositional level that guides outputs, compensating for lack of sensorimotor grounding?
- Basis in paper: Explicit discussion of propositional iconicity at LLM scale
- Why unresolved: Current research hasn't empirically tested whether LLMs develop such propositional iconicity
- What evidence would resolve it: Experimental studies comparing LLM outputs with and without training on datasets testing propositional iconicity

### Open Question 2
- Question: How does integration of bottom-up sensorimotor grounding with top-down linguistic processing in hybrid T3/GPT models affect cognitive capabilities?
- Basis in paper: Discussion of potential for integrating sensorimotor grounding with LLM capabilities
- Why unresolved: Challenge in developing and testing hybrid models that effectively combine both grounding types
- What evidence would resolve it: Developing and testing hybrid models, comparing performance with purely bottom-up or top-down systems

### Open Question 3
- Question: To what extent can biases and constraints in LLM training data, such as those from Universal Grammar, influence models' ability to generate coherent language?
- Basis in paper: Discussion of role of biases and constraints in LLM training data
- Why unresolved: Requires deeper understanding of how these biases are internalized and impact language generation
- What evidence would resolve it: Analyzing LLM outputs in relation to training data and comparing with human language use

## Limitations

- Weak empirical evidence for propositional iconicity at LLM scale, with only theoretical plausibility
- No clear empirical resolution to philosophical questions about functional performance versus genuine understanding
- Limited testing of Universal Grammar as explanation for LLM success against alternative theories

## Confidence

**High confidence**: The distinction between Symbol Grounding Problem and Hard Problem of Consciousness; argument that LLMs lack both forms of grounding; identification of gap between direct sensorimotor grounding and indirect verbal grounding as fundamental limitation.

**Medium confidence**: Claim that inherent linguistic biases compensate for lack of grounding at LLM scale; mechanism by which computational categorical perception might influence LLM outputs.

**Low confidence**: Specific claim that LLM-scale propositional iconicity is primary driver of performance; assertion that no bottom-up grounding path exists for any aspect of LLM understanding.

## Next Checks

1. Design experiment testing whether LLMs can learn novel sensorimotor concepts through indirect verbal descriptions alone, measuring transfer to novel contexts versus direct experience
2. Conduct systematic analysis of LLM outputs across different scales to identify whether propositional iconicity emerges as detectable pattern using established linguistic metrics
3. Compare LLM performance on categorization tasks before and after training on datasets specifically designed to either enhance or disrupt putative computational CP effects