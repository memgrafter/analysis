---
ver: rpa2
title: Prompting open-source and commercial language models for grammatical error
  correction of English learner text
arxiv_id: '2401.07702'
source_url: https://arxiv.org/abs/2401.07702
tags:
- prompt
- sentence
- tool
- prompts
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares the grammatical error correction (GEC) performance
  of ten large language models (LLMs), including seven open-source and three commercial
  models, on four established English GEC benchmarks. The models were evaluated using
  zero-shot and few-shot prompting with a variety of prompt templates designed to
  elicit minimal edit corrections.
---

# Prompting open-source and commercial language models for grammatical error correction of English learner text

## Quick Facts
- **arXiv ID**: 2401.07702
- **Source URL**: https://arxiv.org/abs/2401.07702
- **Reference count**: 40
- **Primary result**: Open-source models can outperform commercial ones on minimal edit GEC benchmarks

## Executive Summary
This paper evaluates the grammatical error correction (GEC) performance of ten large language models, including seven open-source and three commercial models, on four established English GEC benchmarks. The study uses zero-shot and few-shot prompting with various templates designed to elicit minimal edit corrections. Results show that while commercial LLMs excel on fluency-annotated benchmarks like JFLEG, open-source models often match or exceed commercial ones on minimal edit tasks (FCE, CoNLL-14, W&I+LOCNESS). The study finds that zero-shot prompting can be as effective as few-shot prompting in some settings, and supervised GEC models remain superior for minimal edit corrections.

## Method Summary
The study evaluates ten LLMs on four GEC benchmarks using zero-shot and few-shot prompting with minimal edit-focused templates. Models are tested on CoNLL 2014, FCE, JFLEG, and W&I+LOCNESS datasets. Evaluation uses F0.5 for FCE and W&I+LOCNESS, GLEU for JFLEG, and M2 scorer for CoNLL-2014, with error type analysis via ERRANT. Few-shot examples are fixed at four per prompt. The study compares open-source (BLOOMZ, FLAN-T5, InstructPalmyra, OPT-IML, Falcon-40B-Instruct, Stable Beluga 2, Llama-2-70B-chat) and commercial (Cohere Command, GPT-3.5 Turbo, GPT-4) models.

## Key Results
- Commercial LLMs outperform others on JFLEG due to its fluency-style annotations
- Several open-source models match or exceed commercial models on minimal edit benchmarks
- Zero-shot prompting is competitive with few-shot prompting in some settings
- Supervised GEC models remain superior for minimal edit corrections

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs perform best on JFLEG because it contains fluency-style corrections rather than minimal edits.
- Mechanism: JFLEG was annotated by crowdworkers allowed to make fluency rewrites, which aligns with LLM behavior to produce fluent outputs. The study attempted to elicit minimal edits, but LLMs still exhibit fluency bias.
- Core assumption: LLMs default to fluency rewrites unless constrained otherwise.
- Evidence anchors:
  - [abstract]: "LLMs do not always outperform supervised English GEC models except in specific contexts â€“ namely commercial LLMs on benchmarks annotated with fluency corrections as opposed to minimal edits."
  - [section]: "supervised GEC systems, trained on each corpus, are best for minimal edit style corrections, whereas LLMs generate SOTA fluency corrections more similar to the style found in JFLEG."
- Break condition: If prompts forcing minimal edits match supervised models, this mechanism breaks.

### Mechanism 2
- Claim: Open-source models can outperform commercial LLMs on minimal edit tasks.
- Mechanism: The study found several open-source models (e.g., FLAN-T5, Falcon-40B-Instruct, Stable Beluga 2) matched or exceeded commercial models like GPT-3.5 Turbo on minimal edit datasets.
- Core assumption: Open-source models are capable for GEC when appropriately prompted, despite lacking proprietary fine-tuning.
- Evidence anchors:
  - [abstract]: "several open-source models outperform commercial ones on minimal edit benchmarks."
  - [section]: "the LLMs perform poorly on the FCE and CoNLL-14 test sets, lagging far behind SOTA in both cases. For these datasets, open-source models outperform or compete with the commercial models."
- Break condition: If larger open-source models fail to match commercial ones on minimal edit benchmarks, this mechanism breaks.

### Mechanism 3
- Claim: Few-shot prompting does not consistently improve performance over zero-shot prompting.
- Mechanism: The study tested seven zero-shot and twelve few-shot prompt variants, finding that for some models and datasets, zero-shot prompts achieved comparable or better performance than few-shot prompts.
- Core assumption: Quality and selection of few-shot examples matter more than simply increasing examples; dynamic sampling could be more effective.
- Evidence anchors:
  - [abstract]: "in some settings zero-shot prompting is just as competitive as few-shot prompting."
  - [section]: "adding few-shot examples to the three zero-shot prompts does not always lead to an improvement in performance."
- Break condition: If dynamically sampled few-shot examples consistently outperform zero-shot prompts across all models and datasets, this mechanism breaks.

## Foundational Learning

- **Prompt engineering**: Why needed - The study heavily relies on crafting prompts to elicit minimal edit corrections from LLMs, with effectiveness varying based on prompt wording and structure. Quick check - What is the difference between zero-shot and few-shot prompting, and why might one be preferred over the other in GEC tasks?

- **Error type classification**: Why needed - The study uses ERRANT for automatic error annotation and evaluates models using F0.5 and GLEU, requiring understanding of how grammatical errors are categorized and scored. Quick check - What are the most common error types in learner English, and how does ERRANT classify them?

- **Open-source vs commercial LLM deployment**: Why needed - The study compares open-source and commercial models, highlighting differences in accessibility, performance, and suitability for research. Quick check - What are the key trade-offs between using open-source and commercial LLMs for research purposes?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Model evaluation -> Prompt management -> Post-processing -> Evaluation
- **Critical path**: 
  1. Preprocess benchmark datasets (detokenize, standardize)
  2. Select and prepare prompts (zero-shot and few-shot)
  3. Run LLMs on preprocessed data with chosen prompts
  4. Post-process model outputs to extract corrected sentences
  5. Evaluate outputs using ERRANT and calculate performance metrics
  6. Analyze results by error type and dataset
- **Design tradeoffs**: 
  - Prompt complexity vs model performance: More detailed prompts may improve results but increase computational cost
  - Open-source vs commercial models: Open-source offer transparency but may lag in performance; commercial are easier to use but less transparent
  - Few-shot vs zero-shot: Few-shot can improve performance but requires careful example selection; zero-shot is simpler but may be less effective for some models
- **Failure signatures**: 
  - Poor performance on minimal edit datasets but strong on JFLEG: Indicates fluency rewrite bias
  - High variance across different prompts: Suggests sensitivity to prompt wording
  - Low precision or recall in error type analysis: Indicates difficulty with specific error categories
- **First 3 experiments**:
  1. Run baseline evaluation using zero-shot prompts on FCE development set sample
  2. Test impact of different few-shot example sets on model performance
  3. Compare open-source and commercial models on JFLEG dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does document-level GEC performance compare to sentence-level GEC when using large language models?
- Basis in paper: The paper mentions document-level GEC has been proposed and recommended in previous work, and that exploratory work showed ChatGPT could not perform document-level GEC well. It notes that the paper worked with sentence-level GEC, which deviates from the greater amount of essay context given to annotators.
- Why unresolved: The paper does not provide any experimental results or comparisons between document-level and sentence-level GEC performance with LLMs.
- What evidence would resolve it: Empirical comparison of LLM GEC performance on document-level versus sentence-level tasks using the same models and prompts.

### Open Question 2
- Question: Does the proficiency level of the learner text impact the performance of large language models on grammatical error correction?
- Basis in paper: The paper includes an analysis showing that the majority of models perform relatively well on A-level learner text (beginners), followed by intermediate B-level text, native speaker text, and finally advanced learner C-level text. It notes this finding and suggests further investigation is needed.
- Why unresolved: While the paper observes differences in performance across proficiency levels, it does not investigate the reasons behind these differences or explore methods to improve performance on higher-level texts.
- What evidence would resolve it: Detailed analysis of error types and model performance across proficiency levels, and experiments with targeted prompts or fine-tuning to improve performance on advanced learner texts.

### Open Question 3
- Question: How do minimal edit corrections compare to fluency rewrites in terms of learning benefits for language learners?
- Basis in paper: The paper states that initial human evaluation studies suggest a preference for LLM-generated corrections over reference corrections, but notes that minimal edit corrections may be more helpful for language learning since they are more faithful to the original intended meaning. It suggests investigating learning benefits from receiving minimal edit grammatical feedback versus fluency rewrites.
- Why unresolved: The paper does not conduct any longitudinal studies or experiments to measure actual learning outcomes from different feedback styles.
- What evidence would resolve it: Longitudinal study comparing learning outcomes for students receiving minimal edit corrections versus fluency rewrites, with pre- and post-tests to measure improvement in grammatical accuracy and overall language proficiency.

## Limitations

- Benchmark generalizability may be limited as findings are based on four specific GEC benchmarks that may not represent all error types or learner populations
- Prompt template specificity is constrained by the fixed set used in the study, which may not represent optimal strategies for all model architectures
- Evaluation methodology relies on automatic metrics that may not fully capture correction quality, particularly for fluency-oriented outputs

## Confidence

- **High confidence**: Commercial LLMs outperforming open-source models on fluency-annotated benchmarks (JFLEG) is well-supported by consistent results across multiple evaluation metrics and models
- **Medium confidence**: Several open-source models outperforming commercial ones on minimal edit tasks is supported but requires careful interpretation due to small performance margins
- **Medium confidence**: Few-shot prompting not consistently improving over zero-shot prompting is supported, but the study's fixed few-shot examples may not represent optimal strategies

## Next Checks

1. **Error type analysis replication**: Replicate the error type analysis using ERRANT on a subset of outputs to verify reported patterns of model performance across different error categories

2. **Prompt template ablation study**: Conduct controlled experiment varying individual prompt template components to identify which elements most strongly influence model performance on minimal edit versus fluency corrections

3. **Cross-dataset generalization test**: Evaluate best-performing models on an additional GEC benchmark (such as BEA-2019 or Lang-8) to assess whether observed patterns hold across different learner error distributions and annotation styles