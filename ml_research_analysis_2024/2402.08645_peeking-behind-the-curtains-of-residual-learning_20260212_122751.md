---
ver: rpa2
title: Peeking Behind the Curtains of Residual Learning
arxiv_id: '2402.08645'
source_url: https://arxiv.org/abs/2402.08645
tags:
- plain
- residual
- pnnh
- neural
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper uncovers the "dissipating inputs" phenomenon in plain
  neural networks: ReLU non-linearities progressively eliminate negative neuron responses,
  causing loss of input information and eventual convergence failure. The authors
  provide theoretical bounds showing residual connections mitigate this by maintaining
  higher lower bounds on surviving neurons.'
---

# Peeking Behind the Curtains of Residual Learning

## Quick Facts
- arXiv ID: 2402.08645
- Source URL: https://arxiv.org/abs/2402.08645
- Reference count: 21
- This paper proposes Plain Neural Net Hypothesis (PNNH) to enable training deep plain neural networks without residual connections by introducing a weight-sharing autoencoder path within each layer.

## Executive Summary
This paper investigates why residual connections are essential for training deep neural networks. The authors identify a "dissipating inputs" phenomenon where ReLU non-linearities progressively eliminate negative neuron responses, causing loss of input information and eventual convergence failure in plain networks. They provide theoretical bounds showing residual connections mitigate this by maintaining higher lower bounds on surviving neurons. Based on these insights, they propose PNNH, which introduces an internal autoencoder path within each layer to preserve input information, enabling training of deep plain networks without explicit residual connections.

## Method Summary
The authors propose the Plain Neural Net Hypothesis (PNNH) paradigm that splits each neural network layer into two components: a learner that performs standard feature learning, and a coder that is an autoencoder with shared weights across layers. The coder preserves input information through an internal path, mitigating the dissipating input phenomenon. This enables training of deep plain networks without residual connections. The method uses rectified normal distribution for the autoencoder's self-supervised task and is evaluated on CIFAR-10/100 and ImageNet using both CNNs and Transformers.

## Key Results
- PNNH-enabled plain CNNs/Transformers achieve on-par accuracy with ResNets on CIFAR-10/100 and ImageNet
- Up to 0.3x faster training speed compared to ResNets
- Up to 2x more parameter efficient than ResNets
- Theoretical analysis demonstrates how plain neural nets degenerate input to random noise while residual connections maintain better lower bounds of surviving neurons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReLU non-linearities progressively eliminate negative neuron responses, causing loss of input information and eventual convergence failure in plain neural networks.
- Mechanism: The ReLU activation function outputs zero for negative inputs, effectively removing information contained in those neurons. As depth increases, this cumulative loss of information leads to inputs being "dissipated" into random noise, preventing proper learning.
- Core assumption: The input information lost through zeroed negative activations cannot be restored by subsequent layers, as they are initialized independently of the data.
- Evidence anchors:
  - [abstract] "ReLU non-linearities progressively eliminate negative neuron responses, causing loss of input information and eventual convergence failure."
  - [section 3.1] "ReLU non-linearity drops information in neural nets" and "Information loss induced by negative activations in 'dissipating inputs' cannot be restored by the following layers initialized independent of data."
- Break condition: If an activation function preserves information from negative values (e.g., Leaky ReLU), the dissipating input phenomenon is mitigated.

### Mechanism 2
- Claim: Residual connections maintain a higher lower bound on surviving neurons by centering the neuron distribution away from zero.
- Mechanism: By adding the input to the output of a layer (identity mapping), residual connections shift the mean of neuron responses away from zero. This reduces the probability that neurons will be zeroed by ReLU, preserving more input information through depth.
- Core assumption: The variance of neuron responses remains comparable between plain and residual architectures, making the mean shift the dominant factor in determining survival probability.
- Evidence anchors:
  - [section 3.3] "The lower bounds demonstrate that a residual connection pushes up the mean of neuron responses and yields a higher portion of non-zero activations surviving the ReLU non-linearity."
  - [abstract] "We theoretically demonstrate how plain neural nets degenerate the input to random noise and emphasize the significance of a residual connection that maintains a better lower bound of surviving neurons as a solution."
- Break condition: If the variance of neuron responses in residual networks becomes significantly larger than in plain networks, the advantage of mean shifting may be diminished.

### Mechanism 3
- Claim: The Plain Neural Net Hypothesis (PNNH) introduces an internal path via a weight-sharing autoencoder within each layer to preserve input information, enabling training of deep plain networks without explicit residual connections.
- Mechanism: PNNH splits each layer into a learner (for data-dependent feature learning) and a coder (an autoencoder that preserves input information). The coder uses weight sharing across layers to efficiently maintain an internal path for input information, mitigating the dissipating input phenomenon.
- Core assumption: The autoencoder can effectively preserve input information without interfering with the learner's ability to perform supervised learning.
- Evidence anchors:
  - [abstract] "we propose 'The Plain Neural Net Hypothesis' (PNNH) that identifies the internal path across non-linear layers as the most critical part in residual learning, and establishes a paradigm to support the training of deep plain neural nets devoid of residual connections."
  - [section 4.1] "Within the coder design, we establish an internal path using an autoencoder (Vincent et al., 2010) as the coder model to maintain efficient coding of input structures."
- Break condition: If the autoencoder's preservation of input information significantly conflicts with the learner's optimization, the overall network performance may degrade.

## Foundational Learning

- Concept: He-normal initialization
  - Why needed here: The analysis of the dissipating input phenomenon and the effectiveness of residual connections is based on He-normal initialization, which preserves variance through layers.
  - Quick check question: What is the mean and variance of weights initialized using He-normal initialization for a layer with input channels C_in and kernel size K?

- Concept: Chebyshev-Cantelli inequality
  - Why needed here: Used to derive the lower bounds on the probability of neurons surviving ReLU activation in both plain and residual layers.
  - Quick check question: How does the Chebyshev-Cantelli inequality provide a bound on the probability that a random variable is greater than a certain value, given its mean and variance?

- Concept: Autoencoder
  - Why needed here: The coder component in PNNH uses an autoencoder structure to preserve input information across layers without explicit residual connections.
  - Quick check question: What is the objective function minimized by an autoencoder during training, and how does weight sharing across layers contribute to parameter efficiency?

## Architecture Onboarding

- Component map:
  - Input -> Learner (convolutional/linear layers) -> Coder (autoencoder encoder) -> Combine -> Coder (autoencoder decoder) -> Output

- Critical path:
  1. Input passes through the learner to extract features.
  2. Input is also processed by the coder's encoder to create a compressed representation.
  3. The learner's output and the coder's compressed representation are combined (e.g., added or concatenated).
  4. The combined output is processed by the coder's decoder to reconstruct the input information.
  5. The final output is the combination of the learner's processed features and the preserved input information.

- Design tradeoffs:
  - Parameter efficiency vs. representation power: Weight sharing in the coder improves parameter efficiency but may limit the coder's ability to adapt to layer-specific characteristics.
  - Training complexity: Adding an autoencoder within each layer increases the model's complexity and may require careful tuning of the learning rates for the learner and coder.
  - Memory usage: The coder adds additional parameters and computations, increasing memory requirements during training and inference.

- Failure signatures:
  - Convergence failure: If the coder fails to preserve input information effectively, the network may suffer from the same dissipating input phenomenon as plain networks, leading to poor convergence.
  - Degraded performance: If the coder's preservation of input information conflicts with the learner's optimization, the overall network performance may be worse than a standard residual network.
  - Increased training time: The additional computations introduced by the coder may lead to longer training times compared to standard residual networks.

- First 3 experiments:
  1. Implement a simple PNNH-enabled layer with a learner (e.g., a single convolutional layer) and a coder (e.g., a small autoencoder) and train it on a toy dataset (e.g., MNIST) to verify that the coder can preserve input information.
  2. Replace the residual connections in a small ResNet (e.g., ResNet-18) with PNNH-enabled layers and compare its performance on CIFAR-10 to the original ResNet and a plain network without PNNH.
  3. Analyze the learned representations of the coder in a PNNH-enabled network by visualizing the reconstructed inputs and comparing them to the original inputs to assess the effectiveness of the input preservation mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the "dissipating inputs" phenomenon scale with different activation functions beyond ReLU, such as leaky ReLU or parametric ReLU?
- Basis in paper: [inferred] The paper mentions that "leaky ReLU preserves more information than ReLU and thus encourages convergence in very deep plain networks" and that "information loss induced by negative activations in 'dissipating inputs' cannot be restored by the following layers initialized independent of data."
- Why unresolved: The theoretical analysis focuses on ReLU non-linearity, and while leaky ReLU is mentioned as a comparison, no formal analysis or experiments are provided for other activation functions.
- What evidence would resolve it: Formal theoretical bounds for various activation functions showing their impact on the lower bound of surviving neurons, along with empirical comparisons across different activation functions in plain networks.

### Open Question 2
- Question: What is the optimal design for the coder architecture within the PNNH paradigm to maximize information preservation while minimizing parameter overhead?
- Basis in paper: [explicit] The paper describes the coder as using a weight-sharing autoencoder with configurations for different CNN blocks, but notes that "although residual connection (or RM operation) is an optimal solution in the theories of linear auto-encoder... the nonlinearity activation ReLU within the auto-encoder structure no longer guarantees the optimality of the above solution."
- Why unresolved: The paper uses a simplified coder design (CB = C/2) for all CNN blocks without exploring the design space or proving optimality.
- What evidence would resolve it: Systematic exploration of coder architectures with different compression ratios, encoder/decoder depths, and initialization schemes, demonstrating their impact on accuracy and parameter efficiency.

### Open Question 3
- Question: How does the PNNH paradigm perform when applied to architectures with non-local operations, such as attention mechanisms in transformers, beyond the residual MLP blocks?
- Basis in paper: [explicit] The paper mentions that "MLP is a special case of the residual block containing 2 3Ã—3 convolution blocks, with a kernel size of 1 for each convolution" when discussing transformers, but doesn't explore other transformer components.
- Why unresolved: The evaluation is limited to the residual MLP blocks in transformers, and there's no analysis of how the "dissipating inputs" phenomenon affects other transformer components.
- What evidence would resolve it: Analysis of information preservation in transformer self-attention and cross-attention mechanisms when using plain architectures, along with empirical results showing performance with PNNH applied to these components.

## Limitations
- Theoretical analysis relies on idealized assumptions about weight distributions and activation patterns
- Empirical validation is limited to standard vision datasets and architectures
- The specific implementation details and hyperparameter choices may significantly impact results

## Confidence
- High Confidence: The core dissipating inputs phenomenon and its relationship to ReLU activations is well-supported by both theory and experiments
- Medium Confidence: The PNNH framework's effectiveness is supported by experiments, but implementation details may significantly impact results
- Medium Confidence: The theoretical bounds provided are mathematically sound but rely on idealized assumptions

## Next Checks
1. Ablation study on coder architecture: Systematically vary the autoencoder architecture within PNNH (depth, width, activation functions) to identify the most critical design choices for maintaining performance.
2. Cross-domain validation: Evaluate PNNH on non-vision tasks (e.g., NLP, speech) to assess generalizability beyond image classification.
3. Failure mode analysis: Intentionally create scenarios where input dissipation would be severe (e.g., extreme depth, specific initialization schemes) to test the robustness limits of PNNH compared to standard residual connections.