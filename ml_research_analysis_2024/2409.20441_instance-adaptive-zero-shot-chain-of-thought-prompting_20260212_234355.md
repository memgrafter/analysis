---
ver: rpa2
title: Instance-adaptive Zero-shot Chain-of-Thought Prompting
arxiv_id: '2409.20441'
source_url: https://arxiv.org/abs/2409.20441
tags:
- reasoning
- prompt
- question
- arxiv
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of uniform task-level prompts
  in zero-shot Chain-of-Thought (CoT) reasoning by proposing an instance-adaptive
  prompting strategy (IAP). The authors analyze the information flow during zero-shot
  CoT reasoning using saliency scores, discovering that successful reasoning requires
  the prompt to gather semantic information from the question first, followed by the
  rationale aggregating information from both the question and the prompt.
---

# Instance-adaptive Zero-shot Chain-of-Thought Prompting

## Quick Facts
- arXiv ID: 2409.20441
- Source URL: https://arxiv.org/abs/2409.20441
- Authors: Xiaosong Yuan; Chen Shen; Shaotian Yan; Xiaofeng Zhang; Liang Xie; Wenxiao Wang; Renchu Guan; Ying Wang; Jieping Ye
- Reference count: 40
- One-line primary result: Instance-adaptive prompting improves zero-shot CoT reasoning by 2%-4% accuracy through matching prompts to instance-level information flow patterns.

## Executive Summary
This paper addresses the limitation of uniform task-level prompts in zero-shot Chain-of-Thought (CoT) reasoning by proposing an instance-adaptive prompting strategy (IAP). The authors analyze the information flow during zero-shot CoT reasoning using saliency scores, discovering that successful reasoning requires the prompt to gather semantic information from the question first, followed by the rationale aggregating information from both the question and the prompt. Based on this insight, they propose IAP, which adaptively selects appropriate prompts for each instance from a set of candidates. Experiments with LLaMA-2, LLaMA-3, and Qwen models on math, logic, and commonsense reasoning tasks show consistent improvements of 2%-4% accuracy compared to task-level optimal prompts.

## Method Summary
The authors propose an instance-adaptive zero-shot Chain-of-Thought prompting strategy that uses saliency score analysis to select appropriate prompts for each reasoning instance. They implement two approaches: Sequential Substitution (IAP-ss), which greedily selects the first prompt exceeding threshold saliency scores, and Majority Vote (IAP-mv), which aggregates multiple prompt predictions. The method computes saliency scores from attention matrices and gradients to measure information flow between question, prompt, and rationale tokens, then uses these scores to identify effective prompts for each instance.

## Key Results
- Instance-adaptive prompting improves zero-shot CoT reasoning accuracy by 2%-4% across multiple LLM architectures (LLaMA-2, LLaMA-3, Qwen)
- Both IAP-ss and IAP-mv strategies consistently outperform task-level optimal prompts on math, logic, and commonsense reasoning tasks
- Information flow analysis reveals that successful reasoning requires prompt tokens to effectively aggregate semantic information from the question

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instance-adaptive prompting improves CoT reasoning by matching prompts to instance-level information flow patterns.
- Mechanism: The model uses saliency scores to measure how well semantic information flows from question to prompt and from question+prompt to rationale. For each instance, it selects prompts that maximize these saliency scores, which correlates with correct reasoning.
- Core assumption: Higher saliency scores in question-to-prompt and question-to-rationale flows indicate better reasoning quality.
- Evidence anchors:
  - [abstract] "we discover that information flows from question to prompt and question to rationale jointly influence the reasoning results most"
  - [section 2.1] "good reasonings have higher mean values on the question-to-prompt, question-to-rationale, and prompt-to-rationale than those bad"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: If saliency scores don't correlate with reasoning quality, or if instance-level patterns are too noisy to identify reliably.

### Mechanism 2
- Claim: Multi-head attention in shallow layers is critical for aggregating question semantics into prompts.
- Mechanism: Specific attention heads in early layers concentrate question information into prompt tokens. When these heads effectively aggregate information, the reasoning process succeeds.
- Core assumption: Certain attention heads in shallow layers specialize in aggregating question information into prompts.
- Evidence anchors:
  - [section 2.3] "attention heads at the front of the middle and end positions effectively concentrate question semantics and aid their embedding into the prompt context"
  - [section 2.2] "there is a pronounced peak in shallow layers of the LLM, demonstrating a substantial transfer of semantic content from the question to prompt"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: If attention heads don't show consistent patterns across instances, or if deeper layers are more important than shallow layers.

### Mechanism 3
- Claim: Sequential substitution (IAP-ss) works by greedily selecting the first prompt that exceeds threshold saliency scores.
- Mechanism: For each instance, IAP-ss tests prompts in order until finding one that produces saliency scores above predefined thresholds, then uses that prompt for reasoning.
- Core assumption: Thresholds can reliably distinguish good from bad reasoning prompts.
- Evidence anchors:
  - [section 3] "we believe that a prompt with saliency scores surpassing the corresponding threshold is considered a good prompt for a given question"
  - [section 4.3] "we obtain threshold values w.r.t distinct LLMs on different datasets"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism
- Break condition: If thresholds are too strict (missing good prompts) or too loose (including bad prompts), or if instance-level variations make fixed thresholds ineffective.

## Foundational Learning

- Concept: Attention mechanism in transformer models
  - Why needed here: Understanding how information flows between question, prompt, and rationale tokens is central to the paper's analysis
  - Quick check question: How does multi-head attention allow a transformer to process the same sequence from different perspectives?

- Concept: Chain-of-thought prompting
  - Why needed here: The paper builds on and extends CoT prompting methodology for zero-shot reasoning
  - Quick check question: What distinguishes zero-shot CoT from few-shot CoT in terms of prompt requirements?

- Concept: Saliency score computation
  - Why needed here: The core analysis method uses saliency scores to measure information flow during reasoning
  - Quick check question: How is the saliency matrix computed from attention matrices and their gradients?

## Architecture Onboarding

- Component map: Question -> Prompt selection (via saliency analysis) -> Rationale generation -> Answer
- Critical path: Question → Prompt selection (via saliency analysis) → Rationale generation → Answer
- Design tradeoffs:
  - IAP-ss vs IAP-mv: Efficiency vs. robustness
  - Fixed vs. adaptive thresholds: Simplicity vs. instance-specific optimization
  - Number of prompt candidates: Coverage vs. computational cost
- Failure signatures:
  - Low saliency scores in question-to-prompt flow
  - Inconsistent attention head patterns across instances
  - Thresholds that don't generalize across datasets
- First 3 experiments:
  1. Replicate saliency score analysis on a simple GSM8K instance with LLaMA-2
  2. Implement IAP-ss with 3-5 prompt candidates on a small dataset
  3. Compare IAP-mv performance against AMV baseline on MMLU subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the information flow analysis change when applied to other types of reasoning tasks beyond math, logic, and commonsense reasoning?
- Basis in paper: [explicit] The paper mentions that the information flow analysis was conducted on math, logic, and commonsense reasoning tasks.
- Why unresolved: The paper only presents analysis results for a limited set of task types, leaving open how the findings generalize to other reasoning domains.
- What evidence would resolve it: Applying the saliency score analysis to other reasoning tasks like spatial reasoning, causal reasoning, or multi-modal reasoning tasks would reveal if the observed information flow patterns hold across different domains.

### Open Question 2
- Question: What is the impact of different prompt styles on the information flow patterns observed in the saliency score analysis?
- Basis in paper: [explicit] The paper uses a set of predefined prompts for the analysis but does not explore the impact of different prompt styles.
- Why unresolved: The analysis only considers a fixed set of prompts, limiting understanding of how prompt style affects information flow.
- What evidence would resolve it: Analyzing information flow patterns using a wider variety of prompt styles (e.g., open-ended vs. closed-ended, different levels of specificity) would reveal how prompt characteristics influence the reasoning process.

### Open Question 3
- Question: How does the effectiveness of the instance-adaptive prompting strategy (IAP) vary with the number and diversity of candidate prompts?
- Basis in paper: [inferred] The paper uses a fixed set of 9 candidate prompts for the IAP and shows consistent improvements, but does not explore the impact of prompt set size or diversity.
- Why unresolved: The optimal number and diversity of candidate prompts for effective IAP is unknown.
- What evidence would resolve it: Systematically varying the number and diversity of candidate prompts in the IAP experiments would reveal the relationship between prompt set characteristics and IAP performance.

## Limitations

- The effectiveness of saliency-based prompt selection depends heavily on having an appropriate set of candidate prompts, but the optimal number and diversity of prompts for different task domains remains unclear.
- The method requires dataset-specific threshold tuning for the IAP-ss strategy, which may limit its generalizability across different reasoning tasks and LLM architectures.
- The paper's analysis is limited to math, logic, and commonsense reasoning tasks, leaving open questions about how the findings generalize to other reasoning domains.

## Confidence

**High Confidence**: The observation that information flow patterns differ between successful and unsuccessful reasoning attempts is well-supported by the empirical analysis. The finding that certain attention heads in shallow layers play a crucial role in aggregating question semantics is consistent with established understanding of transformer attention mechanisms.

**Medium Confidence**: The claim of 2%-4% accuracy improvements with IAP strategies is supported by experimental results, but the magnitude of improvement may be task-dependent and could vary with different candidate prompt sets or threshold tuning strategies.

**Low Confidence**: The assertion that saliency scores are reliable indicators of reasoning quality for prompt selection requires more validation. The paper provides correlation evidence but doesn't establish causation or test whether alternative selection criteria might perform equally well.

## Next Checks

1. **Cross-task generalization test**: Evaluate the IAP strategies on reasoning tasks not included in the original experiments (e.g., mathematical proof generation or scientific reasoning) to verify if the 2%-4% improvement holds across domains.

2. **Ablation study on prompt diversity**: Systematically vary the number and types of candidate prompts (from 3 to 15) to determine the optimal prompt set size and composition for different reasoning tasks.

3. **Alternative selection criterion comparison**: Implement a baseline selection method using simple heuristics (e.g., prompt length, keyword matching) to compare against the saliency-based selection and determine if the additional complexity of saliency analysis provides meaningful benefits.