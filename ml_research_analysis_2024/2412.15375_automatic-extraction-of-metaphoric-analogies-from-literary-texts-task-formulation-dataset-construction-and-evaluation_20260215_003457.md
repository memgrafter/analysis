---
ver: rpa2
title: 'Automatic Extraction of Metaphoric Analogies from Literary Texts: Task Formulation,
  Dataset Construction, and Evaluation'
arxiv_id: '2412.15375'
source_url: https://arxiv.org/abs/2412.15375
tags:
- metaphors
- concepts
- analogies
- metaphor
- implicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of extracting metaphoric analogies
  from literary texts by formulating a new task that requires identifying concept
  pairs from source and target domains within a text. A novel dataset of 204 instances
  is constructed with expert annotations, featuring proportional analogies with explicit
  and implicit terms.
---

# Automatic Extraction of Metaphoric Analogies from Literary Texts: Task Formulation, Dataset Construction, and Evaluation

## Quick Facts
- arXiv ID: 2412.15375
- Source URL: https://arxiv.org/abs/2412.15375
- Reference count: 12
- Primary result: GPT-4 achieves 77% accuracy in extracting four-term metaphor quadruples from literary texts

## Executive Summary
This paper introduces a novel task for extracting metaphoric analogies from literary texts by identifying concept pairs from source and target domains. The authors construct a dataset of 204 expert-annotated instances of proportional analogies, featuring both explicit and implicit terms. Experiments evaluate large language models on this task using few-shot prompting and in-context learning, with GPT-4 and Mixtral models achieving competitive accuracy up to 77% for correct quadruple extraction. The work demonstrates LLMs' potential for structured metaphor extraction and opens avenues for large-scale knowledge base construction from literary texts.

## Method Summary
The authors formulate a task requiring extraction of four-term analogies (T1:T2::S1:S2) from literary metaphors, identifying source and target domains while generating implicit terms when necessary. They construct a dataset of 204 instances from literary sources with expert annotations, using a strict definition of proportional analogies. Models are evaluated through few-shot in-context learning with 7 examples per batch, using exact and lemmatized head-noun matching metrics. Manual evaluation by expert annotators rates the relevance of generated implicit terms on a 0-2 scale.

## Key Results
- GPT-4 achieves 77% accuracy for correct quadruple extraction
- Mixtral 8x22B scores 75% accuracy on explicit term extraction
- Manual relevance scoring for generated implicit terms: 1.21 out of 2
- Performance is robust across varying sentence lengths and implicit term counts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The models perform well because metaphors and analogies appear frequently in the pretraining corpus, providing rich semantic and structural patterns for the models to learn.
- Mechanism: During pretraining, models are exposed to a vast amount of text containing analogies and metaphors. This exposure allows them to learn the semantic relationships and structural patterns that define these linguistic constructs. When prompted with a metaphor, the model can leverage these learned patterns to identify the source and target domains and extract the relevant concepts.
- Core assumption: The pretraining corpus contains a sufficient number of diverse metaphors and analogies for the model to learn meaningful patterns.
- Evidence anchors:
  - [abstract]: "Extracting metaphors and analogies from free text requires high-level reasoning abilities such as abstraction and language understanding."
  - [section 1]: "Large pretrained language models have some analogical abilities, they can be prompted successfully for analogical reasoning (Yasunaga et al., 2024), and perform zero-shot analogical reasoning in visual task after converting them into language (Hu et al., 2023)."
- Break condition: If the pretraining corpus lacks diversity in metaphors and analogies, or if the models are not exposed to enough examples, their performance will degrade.

### Mechanism 2
- Claim: The task design, which involves extracting four-term analogies, is well-suited for the models' capabilities in understanding and generating structured outputs.
- Mechanism: The task requires the models to identify and extract specific concepts from the text, forming a structured analogy. This aligns with the models' ability to generate structured outputs and understand complex relationships between concepts. The use of prompts with examples helps guide the model's output and ensures it follows the desired format.
- Core assumption: The models can effectively understand and generate structured outputs when provided with clear instructions and examples.
- Evidence anchors:
  - [section 1]: "We design a task that involves (1) the extraction of the explicit elements that form an analogy via two pairs of concepts, (2) the identification of the source and target domain of the metaphor, and (3) the generation of relevant concepts to fill the missing frames of a mapping when they are implicit in the original text."
  - [section 5.1]: "We prompt the models in a few-shot setting, providing several batches of seven examples randomly sampled from our dataset."
- Break condition: If the prompts are unclear or the examples are not representative of the task, the models may struggle to generate accurate outputs.

### Mechanism 3
- Claim: The evaluation metrics, which focus on lemmatized head nouns, allow for flexibility in the models' output while still ensuring the core concepts are correctly identified.
- Mechanism: By using lemmatized head nouns as the evaluation metric, the models are given some leeway in how they express the extracted concepts. This allows them to generate outputs that are semantically equivalent to the ground truth, even if the exact wording differs. This flexibility helps mitigate the impact of minor variations in phrasing and focuses on the core semantic meaning.
- Core assumption: The lemmatized head noun metric is a reliable way to assess the semantic equivalence of the models' outputs to the ground truth.
- Evidence anchors:
  - [section 5.2]: "We test four simple term-matching functions between the manually tagged element and the expressions extracted by the models. After lower-casing the text, we compute the exact match, overlap (if one string is included in the other), lemmatized expressions and lemmatized head noun matching metrics."
  - [section 6]: "Using the lemmatized head-noun match based metric, and considering each field T1, T2, S1 or S2 separately (in opposition to the accuracy computed on the entire quadruples Q), the mean accuracy obtained using GPT 3.5 over 10 batches of experiments with different input examples is 0.62 (standard deviation= 0.03)."
- Break condition: If the lemmatized head noun metric is too lenient or too strict, it may not accurately reflect the models' performance.

## Foundational Learning

- Concept: Understanding the difference between source and target domains in metaphors.
  - Why needed here: The task requires identifying which concepts belong to the source domain (used metaphorically) and which belong to the target domain (the topic being discussed).
  - Quick check question: In the metaphor "My head is an apple without a core," which concepts belong to the source domain and which belong to the target domain?

- Concept: Recognizing proportional analogies.
  - Why needed here: The task focuses on extracting four-term analogies that follow the structure T1:T2::S1:S2.
  - Quick check question: In the analogy "Books are like imprisoned souls till someone takes them down from a shelf and frees them," identify the four concepts and their relationships.

- Concept: Handling implicit concepts in metaphors.
  - Why needed here: Some metaphors involve implicit concepts that are not explicitly stated in the text but need to be inferred.
  - Quick check question: In the metaphor "My head is an apple without a core," what is the implicit concept that completes the analogy?

## Architecture Onboarding

- Component map: Data (metaphor dataset) -> Models (GPT-3.5, GPT-4, Llama-3, Mixtral) -> Evaluation (metrics + manual scoring)
- Critical path: Prompt models with examples and input text -> Extract four concepts forming analogy -> Evaluate extracted concepts using exact match, lemmatized expressions, and lemmatized head noun matching
- Design tradeoffs:
  - Using a smaller dataset (204 instances) vs. a larger one for training and evaluation
  - Relying on lemmatized head nouns for evaluation vs. more strict or lenient metrics
  - Using in-context learning with examples vs. fine-tuning the models on the task
- Failure signatures:
  - Low accuracy in extracting the correct concepts
  - Inability to handle implicit concepts in metaphors
  - Generating outputs that do not follow the desired format
- First 3 experiments:
  1. Evaluate the models' performance on a subset of the dataset with fully explicit metaphors
  2. Test the models' ability to handle metaphors with one implicit concept
  3. Assess the models' performance on metaphors with two implicit concepts

## Open Questions the Paper Calls Out

1. **Generalization to Other Analogy Structures**: 
   - Question: How can the framework be extended to handle different structure mappings beyond proportional analogies?
   - Basis in paper: The paper mentions the possibility of extending the framework to different structure mappings in the conclusion.
   - Why unresolved: The current framework is designed specifically for proportional analogies, and the paper does not explore how it would adapt to other types of analogies.
   - What evidence would resolve it: Experiments demonstrating successful extraction of different analogy structures (e.g., linear, non-proportional) would resolve this.

2. **Performance on Unstructured Texts**:
   - Question: How does the performance of the models change when applied to open text, which is likely to be more complex and contain metaphors that are not as clearly structured?
   - Basis in paper: The paper suggests that applying the framework to open text would be more complicated, as it would require metaphor identification in addition to analogy extraction.
   - Why unresolved: The current experiments are conducted on a curated dataset of clearly structured metaphors, and the performance on unstructured text is unknown.
   - What evidence would resolve it: Experiments evaluating the models on a dataset of unstructured texts containing metaphors would resolve this.

3. **Manual Evaluation of Analogy Frame Extraction**:
   - Question: How does the accuracy of the models for analogy frame extraction compare to human performance, and what types of errors do the models make?
   - Basis in paper: The paper acknowledges that the current evaluation method using lemmatized head-noun matching is imperfect and does not capture all possible reformulations.
   - Why unresolved: The paper does not provide a manual evaluation of the extracted analogy frames, so the true accuracy of the models is unknown.
   - What evidence would resolve it: A manual evaluation of the extracted frames by human annotators would provide a more accurate measure of the models' performance and identify the types of errors they make.

## Limitations

- Small dataset (204 instances) with limited budget for extensive manual evaluation
- Expert annotators only, no large-scale crowd annotation
- Evaluation metrics may underestimate true performance due to conservative matching

## Confidence

- High: Models can extract explicit metaphor components with 75-77% accuracy using current prompt engineering
- Medium: Generated implicit terms provide meaningful semantic contributions despite not matching ground truth exactly
- Medium: Performance is robust across varying sentence lengths and implicit term counts

## Next Checks

1. Test model performance on metaphors from non-literary domains (scientific texts, news articles) to assess domain generalization
2. Conduct large-scale crowd annotation of generated implicit terms to validate expert manual evaluation results
3. Implement step-by-step extraction pipeline (separate term identification from frame assignment) to improve accuracy and interpretability of model outputs