---
ver: rpa2
title: 'The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale'
arxiv_id: '2406.17557'
source_url: https://arxiv.org/abs/2406.17557
tags:
- uni0065
- uni0020
- uni0069
- uni0061
- uni006e
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces FineWeb, a 15-trillion token dataset derived
  from 96 Common Crawl snapshots, and FineWeb-Edu, a 1.3-trillion token subset filtered
  for educational content. The dataset curation process involved systematic text extraction,
  base filtering, individual per-snapshot MinHash deduplication, and custom heuristic
  filters.
---

# The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale

## Quick Facts
- arXiv ID: 2406.17557
- Source URL: https://arxiv.org/abs/2406.17557
- Reference count: 40
- Primary result: FineWeb-Edu achieves 33.6% MMLU and 57% ARC accuracy, surpassing other open datasets

## Executive Summary
This work introduces FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots, and FineWeb-Edu, a 1.3-trillion token subset filtered for educational content. The dataset curation process involved systematic text extraction, base filtering, individual per-snapshot MinHash deduplication, and custom heuristic filters. Models trained on FineWeb outperformed other public web-based datasets, while those trained on FineWeb-Edu achieved significant gains on knowledge- and reasoning-intensive benchmarks, such as MMLU (33.6% accuracy) and ARC (57% accuracy), surpassing all other open datasets. Both datasets are publicly released under a permissive license.

## Method Summary
The FineWeb dataset was created through a multi-stage curation pipeline starting with 96 Common Crawl snapshots. The process involved text extraction using trafilatura, base filtering with URL filtering, fastText language filtering, and MassiveText quality filters. Per-snapshot MinHash deduplication was applied to remove near-duplicates while preserving unique regional or temporal content. Custom heuristic filters targeting low punctuation density, high duplicate line ratios, and many short lines were then applied to remove low-quality boilerplate and spam. An educational quality classifier trained on synthetic LLM annotations was used to create FineWeb-Edu, a filtered subset optimized for knowledge and reasoning tasks.

## Key Results
- FineWeb contains 15 trillion tokens from 96 Common Crawl snapshots
- FineWeb-Edu contains 1.3 trillion tokens of high-quality educational content
- FineWeb-Edu achieves 33.6% MMLU and 57% ARC accuracy, surpassing other open datasets
- Per-snapshot MinHash deduplication outperforms global deduplication while preserving content
- Custom heuristic filters significantly improve benchmark performance compared to standard C4 filters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-snapshot MinHash deduplication preserves high-quality content while removing global duplicates
- Mechanism: By deduplicating within each snapshot rather than globally, the pipeline avoids removing unique regional or temporal content that may appear in older crawls but not in newer ones. This reduces bias toward recent content while still removing near-duplicates within the same crawl.
- Core assumption: Duplicates are more likely to appear within a single crawl due to re-scraping the same URLs, rather than across independent crawls.
- Evidence anchors:
  - "When applied to the oldest snapshots, this process removed as much as 90% of the original base filtered data, as they were deduplicated against a large number of other snapshots."
  - "We therefore tried an alternative approach: individually deduplicating each snapshot (independently from the others), using the same parameters as before. This resulted in 20 trillion tokens of data. When training on a random sample from this dataset (with data sampled from all snapshots) it matched RefinedWeb's performance."

### Mechanism 2
- Claim: Heuristic filters based on line-level quality metrics effectively remove low-quality boilerplate and keyword-stuffed content without over-filtering.
- Mechanism: Filters target documents with low punctuation density, high duplicate line ratios, and many short lines, which correlate with spam, ads, and machine-generated text. This improves the signal-to-noise ratio in the pretraining data.
- Core assumption: Low punctuation, high duplicate line ratio, and short line density are reliable indicators of low-quality text in web corpora.
- Evidence anchors:
  - "We can see that the higher quality dataset has in general higher document density for larger values of our metric, and, in particular, the lower quality dataset has a much higher density of documents for values < 0.12."
  - "Out of all those runs, we identified three filters (see their ablations runs in Fig. 7) that demonstrated the most significant improvements on the aggregate benchmark score."

### Mechanism 3
- Claim: Educational classifier trained on synthetic LLM annotations effectively enriches dataset for knowledge- and reasoning-intensive benchmarks.
- Mechanism: Synthetic annotations from Llama-3-70B-Instruct provide fine-grained quality labels (0-5) for educational content. A regression model trained on these labels can then filter the full dataset to retain only high-quality educational text, improving performance on benchmarks like MMLU and ARC.
- Core assumption: LLM-generated annotations correlate strongly with human judgments of educational value and transfer to diverse web text.
- Evidence anchors:
  - "With a threshold of 3, the model achieved an F1 score of 82% on the validation set, indicating strong performance in distinguishing high-quality educational content."
  - "Specifically, MMLU score increases from 33% to 37%, a relative improvement of approximately 12%, and ARC score goes from 46% to 57%, an improvement of about 24%."

## Foundational Learning

- Concept: MinHash for approximate similarity detection
  - Why needed here: Enables scalable deduplication across billions of documents without pairwise comparison.
  - Quick check question: What parameter controls the minimum similarity threshold for matching documents in MinHash?
- Concept: TF-IDF for measuring term specificity
  - Why needed here: Used to quantify bias in the dataset by measuring how disproportionately certain terms co-occur with sensitive subgroup terms.
  - Quick check question: In TF-IDF, what happens to the weight of a term that appears in almost every document?
- Concept: Synthetic data for classifier training
  - Why needed here: Allows creation of high-quality training labels without manual annotation at scale.
  - Quick check question: Why might synthetic annotations from an LLM be preferable to crowd-sourced labels for this use case?

## Architecture Onboarding

- Component map: WARC -> trafilatura extraction -> base filtering (URL, language, quality) -> per-snapshot MinHash deduplication -> C4 filters -> custom heuristic filters -> PII anonymization -> dataset release
- Critical path: Extraction -> Deduplication -> Filtering -> Release
- Design tradeoffs:
  - Per-snapshot vs. global deduplication: balances duplicate removal vs. content preservation
  - Custom vs. standard filters: improves quality at risk of over-filtering
  - Synthetic vs. manual labels: scales annotation at risk of bias
- Failure signatures:
  - Over-removal: sudden drop in dataset size with minimal benchmark improvement
  - Under-removal: benchmark plateau despite deduplication/filtering steps
  - Bias amplification: skewed topic or subgroup term distributions in final dataset
- First 3 experiments:
  1. Train baseline model on raw WARC vs. trafilatura-extracted text to validate extraction quality.
  2. Compare per-snapshot MinHash deduplication vs. global deduplication on a single snapshot subset.
  3. Test individual heuristic filters (punctuation ratio, duplicate line ratio, short line ratio) on a 28B token ablation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance gains of FineWeb and FineWeb-Edu scale with larger model sizes (e.g., 7B, 70B parameters) and more training tokens?
- Basis in paper: [explicit] The paper trained data ablation models with 1.71B parameters on 28-350 billion tokens and compared them to other datasets at similar scales. However, it explicitly notes that "most of the experiments we ran were at a smaller scale due to computational constraints" and that "Designing datasets at more realistic scales could provide more reliable guidance."
- Why unresolved: The current results are based on relatively small-scale models. The paper acknowledges that scaling to larger models is a potential path for improvement but does not provide data on this.
- What evidence would resolve it: Training and evaluating models with 7B, 34B, and 70B+ parameters on FineWeb and FineWeb-Edu, then comparing their performance to similarly sized models trained on other datasets.

### Open Question 2
- Question: What is the impact of including or excluding code content in the FineWeb dataset on model performance for code-related tasks?
- Basis in paper: [inferred] The paper notes that "it is likely that code content is not prevalent in our dataset" due to the filtering steps applied. It also suggests that users "consider complementing FineWeb with other code datasets" for tasks involving programming.
- Why unresolved: The paper does not evaluate the impact of code content on model performance. It only mentions the potential lack of code in the dataset.
- What evidence would resolve it: Training models on FineWeb with and without code content, then evaluating their performance on code-related benchmarks like HumanEval or MBPP.

### Open Question 3
- Question: How does the performance of FineWeb-Edu compare to models trained on a combination of FineWeb and specialized educational datasets (e.g., textbooks, academic papers)?
- Basis in paper: [explicit] The paper introduces FineWeb-Edu as a filtered subset of FineWeb focused on educational content. It shows that FineWeb-Edu outperforms other open web datasets on knowledge and reasoning benchmarks. However, it does not compare FineWeb-Edu to models trained on a mixture of FineWeb and other educational sources.
- Why unresolved: The paper only evaluates FineWeb-Edu in isolation and does not explore the potential benefits of combining it with other educational datasets.
- What evidence would resolve it: Training models on FineWeb-Edu alone, FineWeb alone, and a mixture of FineWeb and other educational datasets (e.g., textbooks, academic papers), then comparing their performance on knowledge and reasoning benchmarks.

## Limitations
- Dataset curation relies heavily on heuristic filtering and synthetic annotations, which may introduce unknown biases
- Per-snapshot MinHash deduplication assumes duplicates are primarily intra-snapshot rather than global
- Custom heuristic filters lack precise threshold specifications for exact reproduction
- Educational classifier performance depends on quality and representativeness of synthetic annotations

## Confidence
- High Confidence: Dataset size claims (15 trillion tokens for FineWeb, 1.3 trillion for FineWeb-Edu) and general superiority of FineWeb-trained models over other public web datasets on benchmark evaluations
- Medium Confidence: Effectiveness of per-snapshot MinHash deduplication in preserving high-quality content while removing duplicates
- Medium Confidence: Educational classifier's ability to enrich for knowledge- and reasoning-intensive benchmarks
- Low Confidence: Universal applicability of the three custom heuristic filters across all web domains

## Next Checks
1. Replicate the ablation study: Train models using subsets of the FineWeb dataset with individual custom heuristic filters removed to verify their independent contributions to benchmark performance.
2. Cross-domain duplicate analysis: Analyze a random sample of documents removed by per-snapshot MinHash deduplication to determine whether they are indeed intra-snapshot duplicates versus globally unique content that happens to appear in multiple snapshots.
3. Educational classifier generalization test: Evaluate the FineWeb-Edu classifier on manually annotated educational content outside the training distribution to assess its robustness and identify potential bias patterns in the synthetic annotation approach.