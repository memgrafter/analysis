---
ver: rpa2
title: Improving the Natural Language Inference robustness to hard dataset by data
  augmentation and preprocessing
arxiv_id: '2412.07108'
source_url: https://arxiv.org/abs/2412.07108
tags:
- dataset
- data
- anli
- premise
- snli
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the weak robustness of Natural Language Inference
  (NLI) models, which often rely on spurious correlations rather than true inference.
  The authors propose data augmentation and preprocessing methods to improve model
  performance on out-of-distribution and hard datasets.
---

# Improving the Natural Language Inference robustness to hard dataset by data augmentation and preprocessing

## Quick Facts
- arXiv ID: 2412.07108
- Source URL: https://arxiv.org/abs/2412.07108
- Authors: Zijiang Yang
- Reference count: 3
- Primary result: Data augmentation and preprocessing methods improve NLI model robustness on out-of-distribution datasets, achieving >12% gains on HANS and 6-9% on ANLI

## Executive Summary
This paper addresses the weak robustness of Natural Language Inference (NLI) models that rely on spurious correlations rather than true inference. The authors propose data augmentation and preprocessing methods to improve model performance on out-of-distribution and hard datasets. They focus on three key issues: word overlap, numerical reasoning, and length mismatch. To tackle these, they generate additional training data that includes counter-heuristic examples for word overlap, simple numerical reasoning pairs, and a split algorithm for handling long premises. Experimental results show significant improvements: over 12% on the HANS dataset and 6-9% on the ANLI dataset. These gains are achieved by augmenting the original training set with only 1000 additional samples, demonstrating the effectiveness and efficiency of the proposed methods.

## Method Summary
The method combines data augmentation with preprocessing techniques to improve NLI model robustness. For word overlap, the authors generate counter-heuristic examples by creating premise-hypothesis pairs with high lexical overlap but different labels. For numerical reasoning, they augment the training data with simple year comparison examples. To handle length mismatch, they implement a Split algorithm that breaks long premises into individual sentences and evaluates each separately. The approach uses ELECTRA-small as the base model and combines 1000 augmented samples with the original SNLI training data. During inference, the Split algorithm applies a probability threshold to determine when to process premises at the sentence level.

## Key Results
- Over 12% improvement on HANS dataset when using 1000 augmented samples
- 6-9% improvement on ANLI dataset with same augmentation
- Baseline ELECTRA-small achieves 0.882 on SNLI but only 0.306 on ANLI
- Data augmentation alone achieves similar results to augmentation plus Split algorithm

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data augmentation with counter-heuristic examples reduces reliance on spurious correlations like word overlap.
- Mechanism: By generating training pairs that have high word overlap but different labels, the model learns to look beyond lexical overlap and focus on semantic relationships.
- Core assumption: The model can generalize from these carefully constructed counter-examples to unseen test cases.
- Evidence anchors:
  - [abstract] "We propose the data augmentation and preprocessing methods to solve the word overlap, numerical reasoning and length mismatch problems."
  - [section 3.2.1] "We construct the additional data via the following steps: ... Construct the additional dataset by generating 3 premises ... And the hypothesis will be the The n1 v1 the n2."
  - [corpus] Weak - no direct mention of word overlap mitigation in corpus neighbors.
- Break condition: If the augmented examples do not sufficiently vary in semantic content, the model may still rely on shallow heuristics.

### Mechanism 2
- Claim: Numerical reasoning augmentation improves handling of chronological and comparative reasoning.
- Mechanism: Generating premise-hypothesis pairs that require year comparison forces the model to learn arithmetic and temporal reasoning rather than memorizing patterns.
- Core assumption: Simple year-based comparisons are representative of broader numerical reasoning capabilities.
- Evidence anchors:
  - [abstract] "These methods are general methods that do not rely on the distribution of the testing data and they help improve the robustness of the models."
  - [section 3.2.2] "We solve it by augmenting simple premise-hypothesis pairs that ask for comparing the years."
  - [corpus] Weak - no direct evidence of numerical reasoning in corpus neighbors.
- Break condition: If the model encounters more complex numerical reasoning (e.g., multi-step arithmetic) not covered by the augmentation, performance may degrade.

### Mechanism 3
- Claim: The Split algorithm improves handling of long premises by breaking them into sentences.
- Mechanism: By evaluating each sentence individually, the model avoids losing critical information through truncation and focuses on the most relevant part of the premise.
- Core assumption: The key inference signal is contained within a single sentence of the premise.
- Evidence anchors:
  - [abstract] "These methods are general methods that do not rely on the distribution of the testing data and they help improve the robustness of the models."
  - [section 3.2.3] "We advocate that for the paragraph input, it is advised to split the paragraph into sentences and then test the hypothesis for each sentence."
  - [corpus] Weak - no mention of length mismatch or sentence splitting in corpus neighbors.
- Break condition: If the hypothesis depends on information spread across multiple sentences, the Split algorithm may fail to capture the full context.

## Foundational Learning

- Concept: Understanding of spurious correlations in NLI
  - Why needed here: The paper's core contribution is addressing models' reliance on heuristics rather than true inference.
  - Quick check question: What is a spurious correlation in the context of NLI, and why is it problematic?

- Concept: Data augmentation techniques
  - Why needed here: The proposed improvements rely on generating synthetic training examples to teach the model better reasoning.
  - Quick check question: How does generating counter-heuristic examples help a model learn to ignore word overlap?

- Concept: Sentence-level vs. paragraph-level processing
  - Why needed here: The Split algorithm changes how long premises are handled, requiring understanding of tokenization and context windows.
  - Quick check question: Why might truncating a long premise lead to incorrect NLI predictions?

## Architecture Onboarding

- Component map:
  - Data augmentation generator (word overlap, numerical reasoning) -> Base NLI model (ELECTRA-small) -> Split algorithm preprocessor -> Evaluation pipeline (SNLI, HANS, ANLI datasets)

- Critical path:
  1. Generate augmented data
  2. Combine with original training data
  3. Train base model
  4. Apply Split algorithm during inference if needed
  5. Evaluate on test datasets

- Design tradeoffs:
  - More augmentation data improves robustness but increases training time
  - Split algorithm adds inference complexity but handles long premises better
  - Simple heuristics in augmentation may not cover all edge cases

- Failure signatures:
  - High accuracy on SNLI but low on HANS/ANLI indicates reliance on spurious correlations
  - Performance drops on long premises suggest truncation issues
  - Poor numerical reasoning scores indicate need for more diverse augmentation

- First 3 experiments:
  1. Train base ELECTRA-small on SNLI only, evaluate on HANS to confirm baseline weakness
  2. Add word overlap augmentation, retrain, and measure HANS improvement
  3. Apply Split algorithm to long premises, evaluate on ANLI to confirm length mismatch handling

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Reliance on synthetic data augmentation without sufficient validation that generated examples capture full complexity of real-world NLI challenges
- Split algorithm assumes key inference signals are contained within single sentences, which may fail for cross-sentence reasoning
- Evaluation only considers ELECTRA-small as base model, limiting understanding of method transferability to other architectures

## Confidence
- High Confidence: Empirical results showing improvements on HANS and ANLI datasets are well-supported by reported numbers (12% and 6-9% gains)
- Medium Confidence: Effectiveness of Split algorithm and its contribution to overall improvements has some uncertainty
- Low Confidence: Generalizability of augmentation methods to other NLI models and datasets is uncertain

## Next Checks
1. **Cross-Architecture Validation**: Test whether the same augmentation and preprocessing methods improve robustness when applied to different base models (e.g., BERT, RoBERTa, or larger ELECTRA variants)
2. **Ablation Study on Augmentation Types**: Conduct experiments that isolate the contribution of each augmentation type (word overlap, numerical reasoning) and the Split algorithm
3. **Generalization to New Heuristic Types**: Evaluate the model's performance on additional challenging datasets that target different spurious correlations not explicitly addressed in the augmentation (e.g., negation, lexical inference, or semantic similarity)