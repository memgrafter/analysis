---
ver: rpa2
title: How Useful is Continued Pre-Training for Generative Unsupervised Domain Adaptation?
arxiv_id: '2401.17514'
source_url: https://arxiv.org/abs/2401.17514
tags:
- domain
- target
- language
- source
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the effectiveness of Continued Pre-Training
  (CPT) for Generative Unsupervised Domain Adaptation (UDA), where the goal is to
  adapt a generative language model to an unlabeled target domain using knowledge
  from a labeled source domain. The authors propose using CPT in a two-phase training
  pipeline: first, continued pre-training via masked or causal language modeling on
  both source and target domains, followed by supervised instruction tuning on the
  source domain.'
---

# How Useful is Continued Pre-Training for Generative Unsupervised Domain Adaptation?

## Quick Facts
- arXiv ID: 2401.17514
- Source URL: https://arxiv.org/abs/2401.17514
- Reference count: 40
- Primary result: CPT achieves 83.3% target-domain accuracy vs. 81.3% for UDAPTER on average across 40 domain pairs

## Executive Summary
This paper evaluates Continued Pre-Training (CPT) as a method for generative unsupervised domain adaptation (UDA), where the goal is to adapt a generative language model to an unlabeled target domain using knowledge from a labeled source domain. The authors propose a two-phase training pipeline: first continued pre-training on both source and target domains using masked or causal language modeling, followed by supervised instruction tuning on the source domain. They compare CPT against a state-of-the-art domain-invariance method (UDAPTER) and a supervised baseline across 40 domain pairs from MNLI and Amazon Reviews datasets.

Results show CPT is competitive with and more stable than UDAPTER, achieving target-domain accuracy of 83.3% versus 81.3% on average. CPT generalizes well across model architectures (T5, T0, GPT-2), tuning methods (full fine-tuning, adapters, (IA)3), and low-data regimes (as low as 32 shots). Analysis reveals that target-domain exposure is crucial, as performance degrades with high masking rates, and that the model implicitly learns task-relevant features by predicting informative masked words. The study highlights CPT as a robust and stable approach for generative UDA.

## Method Summary
The authors propose a two-phase training pipeline for generative unsupervised domain adaptation. In Phase 1, continued pre-training (CPT) is performed on both source and target domains using either masked language modeling (MLM) or causal language modeling (CLM). This phase leverages unlabeled target domain data to adapt the model's representations. In Phase 2, supervised instruction tuning is applied using only the labeled source domain data, where the model learns to perform the specific task (e.g., classification, sentiment analysis). The authors compare CPT against UDAPTER (a domain-invariance method) and a supervised baseline across 40 domain pairs from MNLI and Amazon Reviews datasets, using various model architectures and tuning methods.

## Key Results
- CPT achieves 83.3% target-domain accuracy versus 81.3% for UDAPTER on average across 40 domain pairs
- CPT demonstrates greater stability with lower variance than UDAPTER across different experimental conditions
- Performance generalizes across model architectures (T5, T0, GPT-2), tuning methods (full fine-tuning, adapters, (IA)3), and low-data regimes (down to 32 shots)
- Target-domain exposure is crucial - performance degrades significantly when masking rates exceed 15%

## Why This Works (Mechanism)
The paper provides empirical evidence but limited theoretical explanation for why CPT works. The mechanism appears to involve the model learning general domain-invariant representations during continued pre-training that transfer to the supervised task during fine-tuning. By predicting masked words in both source and target domains, the model implicitly learns task-relevant features and linguistic patterns specific to the target domain while maintaining task capability from the source domain. The stability advantage over UDAPTER suggests that learning domain-invariant features through generative pre-training may be more robust than explicit domain-invariance constraints.

## Foundational Learning
- **Generative Unsupervised Domain Adaptation**: Adapting generative models to new domains without labeled target data - needed because collecting labeled data for every domain is expensive and impractical
- **Continued Pre-Training**: Extending pre-training on new data after initial training - needed to adapt model representations to target domain without forgetting source task capabilities
- **Masked Language Modeling**: Predicting masked tokens in text - needed as pre-training objective to learn bidirectional context and rich representations
- **Causal Language Modeling**: Predicting next token sequentially - needed for autoregressive models like GPT-2 to learn directional context
- **Domain Invariance**: Learning representations that generalize across domains - needed to transfer knowledge from source to target domains effectively
- **Instruction Tuning**: Fine-tuning models on task instructions - needed to adapt pre-trained models to specific downstream tasks

## Architecture Onboarding

**Component Map**: Source Domain Data -> MLM/CLM Pre-training -> Target Domain Data -> MLM/CLM Pre-training -> Source Domain Labeled Data -> Instruction Tuning -> Adapted Model

**Critical Path**: The core workflow is a two-phase process: Phase 1 (Continued Pre-Training) uses both source and target domain data for masked/causal language modeling, then Phase 2 (Instruction Tuning) uses only source domain labeled data to learn the specific task.

**Design Tradeoffs**: 
- Using both source and target data in pre-training versus only target data (balances task preservation vs. domain adaptation)
- MLM versus CLM as pre-training objective (affects model architecture compatibility)
- Amount of pre-training versus direct fine-tuning (impacts computational cost vs. adaptation quality)

**Failure Signatures**: 
- Poor target-domain performance when source and target domains have minimal overlap
- Catastrophic forgetting of source task if too much pre-training on target domain
- Overfitting to source domain during instruction tuning if target domain exposure is insufficient

**First Experiments**:
1. Compare CPT against supervised baseline on a single domain pair to verify basic functionality
2. Test different masking rates (0%, 15%, 30%, 50%) to identify optimal target domain exposure
3. Evaluate CPT across different model architectures (T5, GPT-2) on the same task to verify generalization

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- Evaluation focuses primarily on classification tasks, leaving unclear whether CPT's advantages transfer to other generative tasks like summarization or dialogue generation
- Analysis of why CPT works remains largely empirical without deeper theoretical grounding, particularly regarding mechanisms by which masked word prediction transfers to supervised task performance
- Does not investigate potential negative transfer when source and target domains have minimal overlap, nor explore trade-offs between continued pre-training time and downstream performance

## Confidence
- **High confidence**: CPT's effectiveness demonstrated through consistent performance across multiple datasets, architectures, and tuning methods; stability advantage over UDAPTER well-supported through variance measurements
- **Medium confidence**: Finding that target-domain exposure is crucial - while masking experiments show clear degradation, analysis of why certain words matter more for adaptation could be more rigorous
- **Medium confidence**: Generalization across low-data regimes (down to 32 shots) demonstrated but with limited exploration of extreme low-resource settings

## Next Checks
1. Test CPT on diverse generative tasks beyond classification (e.g., summarization, story generation) to verify generalizability
2. Conduct ablation studies varying source-target domain similarity to quantify negative transfer risks
3. Measure computational overhead of CPT versus direct fine-tuning across different model sizes to assess practical efficiency trade-offs