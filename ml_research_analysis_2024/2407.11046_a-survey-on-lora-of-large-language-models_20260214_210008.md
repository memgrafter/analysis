---
ver: rpa2
title: A Survey on LoRA of Large Language Models
arxiv_id: '2407.11046'
source_url: https://arxiv.org/abs/2407.11046
tags:
- lora
- arxiv
- preprint
- fine-tuning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively reviews LoRA, a parameter-efficient
  fine-tuning method for large language models. It categorizes LoRA variants into
  five areas: (1) improving downstream adaptation performance, (2) achieving cross-task
  generalization, (3) boosting computation efficiency, (4) using LoRA in federated
  learning, and (5) applications.'
---

# A Survey on LoRA of Large Language Models

## Quick Facts
- arXiv ID: 2407.11046
- Source URL: https://arxiv.org/abs/2407.11046
- Reference count: 40
- One-line primary result: Comprehensive survey categorizing LoRA variants into five areas: improving downstream adaptation, cross-task generalization, computation efficiency, federated learning, and applications.

## Executive Summary
This survey comprehensively reviews LoRA (Low-Rank Adaptation), a parameter-efficient fine-tuning method for large language models. LoRA updates dense neural network layers with pluggable low-rank matrices, significantly reducing memory and computational requirements while maintaining comparable or better performance to full fine-tuning. The survey categorizes LoRA variants across five key areas and highlights its effectiveness in various applications including NLP, code, vision, and multimodal tasks.

## Method Summary
The survey conducts a comprehensive literature review of LoRA variants and applications, synthesizing findings from 40+ papers to create a taxonomy of LoRA progress. The methodology involves analyzing foundational LoRA papers, surveying variants across five categorization areas, and discussing theoretical analyses and practical applications. The approach focuses on understanding LoRA's mechanism, efficiency gains, and versatility across different domains while identifying future research directions and challenges.

## Key Results
- LoRA achieves parameter efficiency by decomposing weight updates into low-rank matrices (BA), reducing parameters from d×k to r×(d+k)
- LoRA's pluggable nature enables cross-task generalization through adapter mixing and composition
- LoRA significantly reduces memory usage (e.g., from ~60GB to ~23GB for fine-tuning) while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA achieves parameter efficiency by decomposing weight updates into low-rank matrices (BA), drastically reducing the number of trainable parameters while preserving performance.
- Mechanism: The original dense layer weights W0 are kept frozen. A low-rank update matrix ΔW is decomposed as BA, where B ∈ ℝ^{d×r} and A ∈ ℝ^{r×k} with r ≪ min(d,k). This reduces parameters from d×k to r×(d+k).
- Core assumption: Over-parameterized models lie on a low intrinsic dimension, so low-rank updates capture most of the useful adaptation signal.
- Evidence anchors:
  - [abstract] "updates the dense neural network layers with pluggable low-rank matrices, is one of the best performed parameter efficient fine-tuning paradigms"
  - [section 2.1] "LoRA decomposes ΔW into two small matrices B ∈ ℝ^{d×r} and A ∈ ℝ^{r×k}, i.e., W = W0 + αBA where r ≪ min{d, k}"
  - [corpus] Weak—no direct citations, but the LoRA original paper is heavily referenced.
- Break condition: If r is too small, the low-rank approximation loses expressiveness; if too large, efficiency gains diminish.

### Mechanism 2
- Claim: LoRA's pluggable nature allows reuse and composition of adapters across tasks, enabling cross-task generalization without retraining from scratch.
- Mechanism: LoRA matrices are stored separately from the base model. Multiple adapters can be loaded, mixed, or combined (via weighted averaging or learned gating) to support multi-task or continual learning scenarios.
- Core assumption: LoRA adapters capture orthogonal task-specific directions, so mixing them yields meaningful cross-task behavior.
- Evidence anchors:
  - [abstract] "Furthermore, it has significant advantages in cross-task generalization and privacy-preserving"
  - [section 4] "LoRA's pluggable nature enables users to accumulate LoRA plugins for different tasks... Mixing multiple LoRA plugins together... has been widely applied in areas requiring cross-task generalization"
  - [corpus] Weak—related works like LoRAHub and mixture-of-experts are mentioned, but no corpus citations provided.
- Break condition: If adapters interfere (non-orthogonal task directions), mixing degrades performance; also requires careful weight assignment.

### Mechanism 3
- Claim: LoRA reduces memory and compute during fine-tuning by freezing most model parameters and only updating a small subset, enabling large model adaptation on limited hardware.
- Mechanism: By freezing W0 and only updating BA, memory usage drops significantly: fewer gradients, fewer optimizer states, and smaller activation memory during backprop.
- Core assumption: The base model has already learned useful representations, so only small adjustments are needed for task adaptation.
- Evidence anchors:
  - [abstract] "significantly reducing memory and computational requirements while maintaining comparable or better performance to full fine-tuning"
  - [section 2.3] "full fine-tuning requires approximately 60GB of memory... LoRA fine-tuning only needs about 23GB of memory... LoRA significantly reduces memory usage"
  - [corpus] Weak—only general efficiency claims, no direct citation.
- Break condition: If task requires large parameter changes, LoRA's small update budget may be insufficient, leading to underfitting.

## Foundational Learning

- Concept: Low-rank matrix decomposition (SVD, rank-r approximation)
  - Why needed here: LoRA's core idea is to approximate weight updates with low-rank matrices; understanding SVD/rank helps grasp why this works.
  - Quick check question: If a weight matrix has dimensions 1000×500, how many parameters does a rank-10 LoRA decomposition use versus full fine-tuning?

- Concept: Transformer architecture (attention and FFN layers)
  - Why needed here: LoRA is typically applied to the projection matrices in attention and FFN sublayers; knowing these structures is essential to implement LoRA correctly.
  - Quick check question: In a standard Transformer, which two types of weight matrices are commonly adapted with LoRA?

- Concept: Federated learning and parameter-efficient fine-tuning
  - Why needed here: LoRA is highlighted as a solution for federated fine-tuning due to reduced communication and computation; understanding FL basics helps contextualize its advantages.
  - Quick check question: In federated learning, why does reducing the number of trainable parameters help with communication cost?

## Architecture Onboarding

- Component map:
  Base LLM (frozen weights) -> LoRA adapter matrices (B and A, one per adapted layer) -> Optional: gating/mixing networks for multi-task adapters -> Training loop: only backprop through adapter parameters

- Critical path:
  1. Load base model + LoRA adapters
  2. Forward pass: compute adapter contributions + base output
  3. Compute loss
  4. Backprop only through adapter parameters
  5. Update adapters

- Design tradeoffs:
  - Rank selection: higher rank → better performance but less efficiency
  - Adapter placement: attention vs FFN layers; often both used
  - Mixed precision: quantizing base model + higher-precision adapters (e.g., QLoRA)

- Failure signatures:
  - Underfitting: low rank, insufficient adaptation capacity
  - Overfitting: too high rank, overfitting to small dataset
  - Performance gap: LoRA underperforms full fine-tuning; may need larger rank or additional tuning

- First 3 experiments:
  1. Implement LoRA on a single attention layer of a small BERT; compare full fine-tuning vs LoRA in terms of parameters and accuracy.
  2. Test adapter mixing: train two adapters for different tasks, then mix them with equal weights and evaluate cross-task performance.
  3. Measure memory usage: profile GPU memory during full fine-tuning vs LoRA fine-tuning on the same model/dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LoRA's parameter efficiency compare to other parameter-efficient fine-tuning methods like adapters and prompt tuning in terms of both performance and computational resources?
- Basis in paper: [explicit] The paper discusses LoRA's parameter efficiency and compares it to full fine-tuning, but does not provide a direct comparison to other PEFT methods like adapters and prompt tuning.
- Why unresolved: The paper focuses on LoRA and its variants, so a direct comparison to other PEFT methods is not provided.
- What evidence would resolve it: A comprehensive study comparing LoRA to adapters and prompt tuning in terms of performance and computational resources across various tasks and model sizes.

### Open Question 2
- Question: What are the limitations of LoRA in terms of its low-rank assumption, and how can these limitations be addressed to improve its performance on knowledge-intensive tasks?
- Basis in paper: [explicit] The paper mentions that LoRA's low-rank updates restrict its ability to memorize downstream knowledge and generalize on downstream tasks, especially in knowledge- and skill-intensive domains.
- Why unresolved: The paper discusses some methods to break the low-rank bottleneck, but the fundamental limitations of the low-rank assumption are not fully explored.
- What evidence would resolve it: Further theoretical analysis and empirical studies to understand the limitations of the low-rank assumption and develop more effective methods to address them.

### Open Question 3
- Question: How can LoRA be effectively used in federated learning to handle data heterogeneity, device heterogeneity, and model heterogeneity while preserving parameter privacy?
- Basis in paper: [explicit] The paper discusses the challenges of federated learning and how LoRA can be used to address them, but the effectiveness of LoRA in handling these challenges is not fully explored.
- Why unresolved: The paper provides an overview of LoRA's potential in federated learning, but more research is needed to understand its effectiveness in real-world scenarios.
- What evidence would resolve it: Extensive empirical studies evaluating the performance of LoRA in federated learning across different datasets, model architectures, and device settings.

## Limitations

- Survey primarily based on literature review without extensive experimental validation of claimed benefits
- Many performance claims rely on cited papers rather than direct reproduction or analysis
- May miss emerging variants or applications published after the survey's completion

## Confidence

- High: Basic LoRA mechanism (low-rank decomposition of weight updates) - well-established in original LoRA paper and widely reproduced
- Medium: Cross-task generalization benefits - supported by surveyed papers but limited direct experimental evidence in this survey
- Low: Federated learning specific advantages - discussed conceptually but lacks detailed experimental validation

## Next Checks

1. **Memory Efficiency Validation**: Implement LoRA fine-tuning on a medium-sized model (e.g., LLaMA2-7B) and measure actual GPU memory usage compared to full fine-tuning across different batch sizes and ranks.

2. **Cross-Task Generalization Experiment**: Train LoRA adapters for two distinct tasks, then test the performance of adapter mixing versus individual adapters to validate the cross-task generalization claims.

3. **Rank Sensitivity Analysis**: Systematically vary the rank parameter across a range of values on a standard benchmark task to quantify the tradeoff between parameter efficiency and performance, validating the survey's claims about rank selection.