---
ver: rpa2
title: Large Language Models are Biased Reinforcement Learners
arxiv_id: '2405.11422'
source_url: https://arxiv.org/abs/2405.11422
tags:
- relative
- value
- prompt
- task
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) were tested in bandit tasks to examine
  whether they exhibit value encoding biases similar to humans. The study found that
  LLMs learned to maximize rewards in training contexts but often failed to generalize
  learned values to new contexts, choosing options with better relative value at rates
  exceeding those of an ideal, reward-maximizing agent.
---

# Large Language Models are Biased Reinforcement Learners

## Quick Facts
- arXiv ID: 2405.11422
- Source URL: https://arxiv.org/abs/2405.11422
- Reference count: 40
- Large language models exhibit context-dependent value encoding similar to human biases in reinforcement learning tasks

## Executive Summary
This study demonstrates that large language models (LLMs) display reinforcement learning biases similar to those observed in human decision-making. When tested in bandit tasks, LLMs learned to maximize rewards during training but failed to generalize these learned values to new contexts. Instead of absolute reward maximization, the models showed a systematic preference for options with better relative value - choosing options that were better than alternatives even when suboptimal in absolute terms. This "relative value bias" was enhanced when prompts included explicit outcome comparisons and was detected even in raw, pretrained models through hidden layer activations. Computational modeling revealed that LLM behavior aligns with reinforcement learning algorithms that encode outcomes in a context-dependent manner, suggesting these biases emerge from the fundamental learning mechanisms rather than from fine-tuning.

## Method Summary
The researchers designed a series of bandit tasks to test whether LLMs exhibit value encoding biases similar to humans. Models were trained on specific reward contexts and then tested on their ability to generalize learned values to new contexts. The study compared LLM behavior against an ideal reward-maximizing agent and examined how explicit outcome comparisons in prompts affected decision-making. Computational cognitive modeling was used to determine whether LLM behavior could be explained by standard reinforcement learning algorithms that incorporate relative value encoding at the outcome stage.

## Key Results
- LLMs consistently chose options with better relative value at rates exceeding ideal reward-maximizing agents
- The relative value bias was enhanced when explicit outcome comparisons were included in prompts
- Computational modeling showed LLM behavior aligns with RL algorithms incorporating context-dependent value encoding
- The bias was detected in hidden layer activations of raw, pretrained models, not just fine-tuned ones

## Why This Works (Mechanism)
The relative value bias emerges from the fundamental reinforcement learning mechanisms embedded in LLMs. During pretraining and fine-tuning, these models learn to encode outcomes in a context-dependent manner, where the value of an option is influenced by the values of other available options. This encoding happens at the outcome stage of reinforcement learning, meaning the model doesn't just learn absolute reward values but also learns to compare outcomes relative to alternatives. When faced with new contexts, this relative encoding persists, leading to systematic deviations from optimal absolute reward maximization.

## Foundational Learning

**Reinforcement Learning**: Why needed - Core framework for understanding how models learn from rewards; Quick check - Can the model learn to maximize cumulative reward over time?

**Bandit Tasks**: Why needed - Simple decision-making paradigm to test value encoding; Quick check - Does the model learn to prefer higher-reward options in training?

**Context-Dependent Value Encoding**: Why needed - Explains why learned values don't generalize perfectly; Quick check - Does model performance change when option values are presented in different contexts?

**Computational Cognitive Modeling**: Why needed - Links observed behavior to underlying algorithmic mechanisms; Quick check - Can a simple RL model with relative value encoding reproduce LLM behavior?

## Architecture Onboarding

Component map: Input prompt -> Token embedding -> Hidden layers -> Value encoding -> Decision output

Critical path: The value encoding stage is critical, as it determines how rewards are represented and influences all subsequent decision-making. The relative value bias manifests specifically at this encoding stage.

Design tradeoffs: Models must balance learning absolute reward values with context-dependent relative comparisons. This tradeoff enables flexible adaptation to different contexts but introduces systematic biases when generalization is required.

Failure signatures: Models consistently choose options that are better relative to alternatives rather than optimal in absolute terms, especially when prompts explicitly compare outcomes.

First experiments:
1. Test whether fine-tuning on value-consistent data can eliminate the relative value bias
2. Examine how different reward structures affect the magnitude of the bias
3. Compare the bias across different LLM architectures and pretraining objectives

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments focus on simple two-armed bandit tasks that may not reflect complex real-world decision-making
- The relative value bias could be an artifact of specific reward structures and prompt designs used
- The exact mechanism by which relative value encoding emerges during pretraining remains unclear

## Confidence

High: Context-dependent value encoding exists in LLMs as demonstrated by bandit task experiments and computational modeling

Medium: The generalizability of this bias across different task types and real-world applications

## Next Checks

1. Test the relative value bias in more complex sequential decision-making tasks with longer time horizons and multiple reward dimensions

2. Examine whether fine-tuning on value-consistent data can mitigate or eliminate the relative value bias

3. Investigate the relationship between the magnitude of relative value bias and specific architectural features or pretraining objectives across different LLM families