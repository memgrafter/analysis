---
ver: rpa2
title: Revisiting MoE and Dense Speed-Accuracy Comparisons for LLM Training
arxiv_id: '2405.15052'
source_url: https://arxiv.org/abs/2405.15052
tags:
- dense
- step
- expert
- time
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits the comparison between sparse mixture-of-experts
  (MoE) and dense language models (LLMs), arguing that prior comparisons have been
  unfair to dense models due to inaccurate complexity measurement. The authors propose
  using step time (which accounts for communication overhead) and the Chinchilla compute-optimal
  token-to-parameter ratio to determine training budgets.
---

# Revisiting MoE and Dense Speed-Accuracy Comparisons for LLM Training

## Quick Facts
- **arXiv ID**: 2405.15052
- **Source URL**: https://arxiv.org/abs/2405.15052
- **Reference count**: 9
- **Key outcome**: MoE models consistently outperform dense models on speed-accuracy trade-off curve with 3D sharding strategy keeping dense-to-MoE step time increases under 20%

## Executive Summary
This paper challenges prior unfair comparisons between sparse mixture-of-experts (MoE) and dense language models by introducing step time as a more accurate complexity measure and using the Chinchilla compute-optimal token-to-parameter ratio. The authors implement an efficient 3D sharding strategy for MoE layers that maintains dense-to-MoE step time increases under 20%. Across 6.4B, 12.6B, and 29.6B scales, experiments show MoEs consistently outperform dense models, with a 1.6B/256E MoE achieving +1.75% better accuracy than a 6.4B dense model while being 2× faster.

## Method Summary
The paper compares MoE and dense LLMs across three model scales (6.4B, 12.6B, 29.6B) using AdamW optimizer with cosine learning rate schedule, weight decay 0.1, and gradient clipping 1.0. Training follows the Chinchilla compute-optimal token-to-parameter ratio (20:1) on a corpus similar to LLaMA2. The MoE implementation uses Top-2 routing with 2.0 expert capacity and a novel 3D sharding strategy (Data, Expert, Model axes) that minimizes communication overhead. Dense models follow LLaMA2 architecture with pre-normalization, SwiGLU activation, RoPE positional embedding, and GQA attention. Performance is evaluated on 9 CoreEN tasks (ARC, HellaSwag, WinoGrande, PIQA, SciQ, LAMBADA, TriviaQA, WebQS), MMLU 5-shot, and GSM8K 8-shot.

## Key Results
- MoE models consistently outperform dense models on speed-accuracy trade-off curve across all scales
- A 1.6B/256E MoE achieves +1.75% better accuracy than a 6.4B dense model while being 2× faster
- 3D sharding strategy keeps dense-to-MoE step time increases under 20% even at 256 experts
- MoE advantages extend to instruction fine-tuning tasks beyond base model training

## Why This Works (Mechanism)
MoE models achieve superior speed-accuracy trade-offs by activating only a subset of experts per token while maintaining the same overall parameter count as dense models. The 3D sharding strategy effectively distributes the MoE computation across multiple axes, reducing communication overhead that typically penalizes sparse models. By using step time rather than FLOPs as the complexity measure, the comparison accounts for real-world training bottlenecks including data movement and communication costs.

## Foundational Learning
**Compute-optimal scaling**: The relationship between model size and training tokens that minimizes perplexity per FLOP, established by Chinchilla. *Why needed*: Provides a fair comparison baseline between MoE and dense models. *Quick check*: Verify token-to-parameter ratio follows Chinchilla's 20:1 guideline.

**3D sharding strategy**: Partitioning tensors across Data, Expert, and Model axes to optimize communication and computation in MoE training. *Why needed*: Enables efficient training of large MoE models without prohibitive communication overhead. *Quick check*: Monitor communication-to-computation ratio during training.

**Top-2 routing**: Selecting the two most relevant experts for each token based on router output scores. *Why needed*: Balances load distribution and computational efficiency in MoE architectures. *Quick check*: Track expert utilization variance across training steps.

**Router z-loss**: A regularization term that encourages balanced expert usage by penalizing variance in expert activations. *Why needed*: Prevents expert collapse and ensures all experts contribute meaningfully to model performance. *Quick check*: Monitor expert activation distribution for uniformity.

## Architecture Onboarding

**Component map**: Dense backbone -> MoE layers (with 3D sharded experts) -> Router -> Output projection

**Critical path**: Token embedding -> MoE routing decision -> Expert activation -> Gating mechanism -> Model output

**Design tradeoffs**: 
- Expert capacity vs. model quality: Higher capacity (2.0) improves performance but increases computation
- Number of experts vs. step time: More experts improve accuracy but increase communication overhead
- Router precision vs. efficiency: Higher precision improves routing accuracy but increases computational cost

**Failure signatures**:
- Expert collapse: Single expert dominates routing, reducing MoE benefits
- Communication bottleneck: Step time increases disproportionately with expert count
- Load imbalance: Some experts remain idle while others are overloaded

**First experiments**:
1. Train 6.4B dense model with LLaMA2 architecture to establish baseline performance
2. Implement 1.6B/64E MoE model with 3D sharding to verify step time claims
3. Compare expert utilization patterns between MoE models designed from smaller vs. same-scale dense backbones

## Open Questions the Paper Calls Out

**Open Question 1**: What is the optimal token-to-parameter ratio for MoE models when using step time as the complexity measure instead of FLOPs? The paper assumes MoEs should follow the same compute-optimal frontier as dense models without empirical validation that this holds true for sparse architectures.

**Open Question 2**: How does expert load balancing behavior differ between MoE models designed from smaller versus same-scale dense backbones? The paper observes performance differences but doesn't analyze the underlying routing mechanism distributions.

**Open Question 3**: What is the compute-optimal scaling relationship between number of experts and dense model size in MoE architectures? The paper scales experts up to 256 but doesn't systematically explore optimal scaling laws specific to MoE architectures.

## Limitations

- Hardware dependency: Performance gains are tightly coupled with specific hardware configurations (2048 GPUs) and may not generalize to different setups
- Training corpus specificity: Lack of precise data mixing ratios and quality control measures could impact model performance and training efficiency
- Scaling assumptions: Uniform application of Chinchilla-optimal token-to-parameter ratio may not account for non-linear scaling behaviors in MoE architectures

## Confidence

**High Confidence**: Technical feasibility of 3D sharding strategy (detailed implementation described, reasonable step time improvements); accuracy improvements of MoE over dense models (directly measured and consistent)

**Medium Confidence**: General speed-accuracy trade-off advantages of MoE (dependent on specific hardware and training conditions); effectiveness of router z-loss (empirically demonstrated but lacks theoretical guarantees)

**Low Confidence**: Absolute step time comparisons across scales (sensitive to implementation details); long-term stability of architecture (beyond reported training duration)

## Next Checks

1. **Hardware-agnostic performance validation**: Reproduce step time and accuracy comparisons on a smaller GPU cluster (64-128 GPUs) to verify scalability and practical applicability of 3D sharding strategy.

2. **Corpus composition sensitivity analysis**: Systematically vary training data mixing ratios and evaluate impact on both MoE and dense model performance to identify potential data-dependent biases in conclusions.

3. **Extended training stability test**: Train 29.6B MoE model for twice the original token count to assess long-term stability and whether initial performance advantages persist over extended training periods.