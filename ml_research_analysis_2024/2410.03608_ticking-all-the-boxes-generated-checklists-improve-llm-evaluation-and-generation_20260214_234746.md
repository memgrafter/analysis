---
ver: rpa2
title: 'TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation'
arxiv_id: '2410.03608'
source_url: https://arxiv.org/abs/2410.03608
tags:
- response
- checklist
- checklists
- instruction
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "TICK ING ALL THE BOXES: G ENERATED CHECKLISTS IMPROVE LLM EVALUATION\
  \ AND GENERATION Jonathan Cook, Tim Rockt\xE4schel, Jakob Foerster, Dennis Aumiller,\
  \ Alex Wang LLM evaluations often rely on preference judgments that obscure multi-faceted\
  \ reasoning and require costly human annotation. TICK generates instruction-specific\
  \ checklists of YES/NO questions to decompose complex instructions into targeted\
  \ evaluation criteria."
---

# TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation

## Quick Facts
- arXiv ID: 2410.03608
- Source URL: https://arxiv.org/abs/2410.03608
- Reference count: 40
- TICK increases exact agreement with human preferences by 5.8% using generated checklists

## Executive Summary
LLM evaluations often rely on preference judgments that obscure multi-faceted reasoning and require costly human annotation. TICK generates instruction-specific checklists of YES/NO questions to decompose complex instructions into targeted evaluation criteria. This structured approach increases exact agreement with human preferences by 5.8% compared to direct scoring and enables interpretable, fully automated evaluation. STICK (Self-TICK) uses these checklists for in-context self-improvement, achieving +7.8% on LiveBench reasoning tasks and +6.3% on WildBench via Best-of-N selection. Providing checklists to human evaluators also improves inter-annotator agreement from 0.194 to 0.256.

## Method Summary
TICK generates instruction-specific checklists using LLMs, which are then used to evaluate responses through YES/NO questions. The method involves generating checklists tailored to each instruction, evaluating responses against these checklists, and aggregating the results into overall scores or preferences. STICK extends this by using checklist-based feedback for self-refinement, where models iteratively improve their outputs based on targeted feedback. The approach is validated across multiple benchmarks including WildBench, LiveBench, and InFoBench, comparing performance against direct scoring and vanilla self-refinement methods.

## Key Results
- TICK increases exact agreement with human preferences by 5.8% compared to direct scoring
- STICK achieves +7.8% on LiveBench reasoning tasks and +6.3% on WildBench via Best-of-N selection
- Providing checklists to human evaluators improves inter-annotator agreement from 0.194 to 0.256

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TICK improves agreement with human preferences by decomposing complex instructions into checklist questions.
- Mechanism: Breaking down multi-faceted instructions into YES/NO checklist questions simplifies the evaluation task for LLMs, reducing cognitive load and ambiguity.
- Core assumption: LLMs can generate high-quality, instruction-specific checklists that align with human-written ones.
- Evidence anchors:
  - [abstract] TICK structures evaluations with LLM-generated, instruction-specific checklists and increases exact agreement with human preferences by 5.8%.
  - [section 3.2.2] GPT-4o achieves 0.826 accuracy when answering checklist questions compared to human annotators.
  - [corpus] Found 25 related papers; average neighbor FMR=0.444, suggesting moderate similarity in the literature.
- Break condition: If LLMs generate low-quality checklists that miss critical criteria or include irrelevant questions, the decomposed evaluation will not align with human preferences.

### Mechanism 2
- Claim: STICK enables self-improvement by providing targeted, interpretable feedback via checklists.
- Mechanism: Checklist evaluations identify specific failures in responses, allowing LLMs to iteratively refine their outputs based on precise feedback rather than unstructured critiques.
- Core assumption: Checklist-based feedback is more actionable for self-refinement than unstructured critiques.
- Evidence anchors:
  - [abstract] STICK achieves +7.8% on LiveBench reasoning tasks and +6.3% on WildBench via Best-of-N selection.
  - [section 4.1] STICK improves Command-R+ by +6.5% on InFoBench and +7.1% on WildBench, outperforming vanilla Self-Refine.
  - [corpus] Weak evidence; no direct citations in neighbors about self-refinement using checklists.
- Break condition: If checklist feedback becomes too generic or fails to highlight actionable improvements, self-refinement will stall or degrade performance.

### Mechanism 3
- Claim: LLM-generated checklists improve human inter-annotator agreement by providing structured guidance.
- Mechanism: Checklists decompose the evaluation task into specific criteria, reducing cognitive load and ensuring evaluators consider all relevant aspects of the instruction.
- Core assumption: Human evaluators will use checklist answers to inform but not limit their holistic scoring.
- Evidence anchors:
  - [abstract] Providing checklists to human evaluators increases inter-annotator agreement from 0.194 to 0.256.
  - [section 5.2] Annotators report checklists made scoring easier 78.5% of the time.
  - [corpus] Weak evidence; no direct citations in neighbors about checklists assisting human evaluation.
- Break condition: If human evaluators treat checklist answers as rigid constraints rather than guidance, scoring variance may increase or agreement may decrease.

## Foundational Learning

- Concept: Instruction decomposition
  - Why needed here: Complex instructions often contain multiple, interrelated requirements that are difficult to evaluate holistically.
  - Quick check question: Can you break down "Write a 300-word summary of the novel, focusing on the protagonist's journey and using a formal tone" into specific checklist criteria?

- Concept: YES/NO evaluation framing
  - Why needed here: Binary questions simplify the evaluation task for both LLMs and humans, reducing ambiguity in judgment.
  - Quick check question: How would you phrase "Does the response use subheadings?" to ensure a YES answer indicates correct formatting?

- Concept: Preference aggregation
  - Why needed here: Converting multiple checklist answers into a single preference or score requires a consistent aggregation method.
  - Quick check question: If a response passes 7 out of 10 checklist questions, what score would you assign on a 1-5 scale and why?

## Architecture Onboarding

- Component map:
  - Instruction → Checklist Generator (LLM) → Checklist
  - Response + Checklist → Checklist Evaluator (LLM) → Individual scores
  - Individual scores → Aggregator → Overall score/preference
  - Overall score + Instruction + Response → Refinement Prompt → New Response (for STICK)
  - Multiple Responses → Self-Evaluator (LLM) → Best-of-N Selection

- Critical path:
  1. Generate checklist for instruction
  2. Evaluate response against each checklist question
  3. Aggregate checklist answers into overall score
  4. (Optional) Use scores for self-refinement or Best-of-N selection

- Design tradeoffs:
  - Checklist length vs. evaluation speed: Longer checklists provide more granularity but increase inference cost.
  - Checklist specificity vs. generality: Highly specific checklists may not generalize across similar instructions.
  - Self-evaluation vs. external evaluation: Using the same LLM for generation and evaluation risks bias but reduces cost.

- Failure signatures:
  - Checklist questions that are impossible to answer YES/NO → Rework checklist generation prompt
  - Checklist answers that consistently disagree with human judgment → Reassess checklist quality or evaluator settings
  - Self-refinement that degrades performance → Check if checklist feedback is actionable or if refinement prompt needs adjustment

- First 3 experiments:
  1. Generate checklists for 10 diverse instructions and compare against human-written checklists using BLEU/ROUGE scores.
  2. Evaluate 50 response pairs using TICK and direct scoring, measuring agreement with human preferences via WPLD.
  3. Run STICK self-refinement on 20 LiveBench math problems, comparing performance against vanilla Self-Refine.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the checklist-based evaluation approach compare to learned or discovered evaluation structures?
- Basis in paper: Inferred from the limitations section where the authors acknowledge that checklists are only one heuristic for structuring evaluation.
- Why unresolved: The paper focuses on demonstrating the effectiveness of checklists but doesn't explore alternative evaluation structures or compare them directly to checklists.
- What evidence would resolve it: Experimental results comparing TICK against alternative evaluation structures such as learned evaluation functions or other forms of structured evaluation would resolve this question.

### Open Question 2
- Question: What is the optimal number of checklist questions to balance evaluation quality with computational cost?
- Basis in paper: Inferred from the observation that checklist generation and evaluation adds computational overhead compared to direct scoring.
- Why unresolved: The paper uses checklists of varying lengths but doesn't systematically investigate how checklist length affects evaluation quality or computational efficiency.
- What evidence would resolve it: Experiments varying the number of checklist questions per instruction and measuring both evaluation quality and computational cost would resolve this question.

### Open Question 3
- Question: How does the quality of checklist evaluations scale with increasing model size and capability?
- Basis in paper: Inferred from the experiments showing that checklist evaluations improve with larger models and more compute (majority voting, CoT).
- Why unresolved: The paper tests specific model sizes but doesn't investigate how evaluation quality changes as models continue to scale.
- What evidence would resolve it: Experiments running TICK with increasingly larger models (e.g., frontier models) and measuring agreement with human preferences would resolve this question.

### Open Question 4
- Question: How robust is the checklist generation process to instruction ambiguity or poorly-specified prompts?
- Basis in paper: Inferred from the observation that human evaluators still had low inter-annotator agreement even with checklists (0.194 → 0.256).
- Why unresolved: The paper tests on well-formed instructions but doesn't investigate how checklist quality degrades with ambiguous or poorly-specified prompts.
- What evidence would resolve it: Experiments testing TICK on intentionally ambiguous or poorly-specified instructions and measuring both checklist quality and evaluation accuracy would resolve this question.

## Limitations

- Evaluation relies heavily on proprietary benchmarks whose exact composition and difficulty distribution remain unclear
- Self-refinement mechanism shows promise but lacks ablation studies demonstrating superiority over alternative structured feedback formats
- The claim about improving human inter-annotator agreement lacks sufficient contextual information about baseline conditions and evaluator expertise levels

## Confidence

- **High confidence**: The core claim that checklist-based evaluation improves agreement with human preferences (5.8% gain) is well-supported by multiple experiments and consistent across different benchmarks.
- **Medium confidence**: The self-refinement mechanism's effectiveness (+7.8% on LiveBench) is demonstrated but may be sensitive to prompt engineering and checklist quality, which varies across instruction types.
- **Low confidence**: The claim that checklists significantly improve human inter-annotator agreement (from 0.194 to 0.256) lacks sufficient contextual information about baseline conditions and evaluator expertise levels.

## Next Checks

1. Conduct a reproducibility study using open-source benchmarks to verify the 5.8% improvement claim across different domains and instruction types.
2. Perform an ablation study comparing checklist-based feedback against alternative structured feedback mechanisms (e.g., rubric-based scoring, step-by-step guidance) in the STICK self-refinement pipeline.
3. Evaluate checklist robustness by testing whether generated checklists maintain effectiveness when instructions contain implicit requirements or domain-specific terminology not present in the training data.