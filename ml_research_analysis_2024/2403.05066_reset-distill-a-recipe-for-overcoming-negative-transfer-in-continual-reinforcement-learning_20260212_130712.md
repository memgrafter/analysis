---
ver: rpa2
title: 'Reset & Distill: A Recipe for Overcoming Negative Transfer in Continual Reinforcement
  Learning'
arxiv_id: '2403.05066'
source_url: https://arxiv.org/abs/2403.05066
tags:
- transfer
- task
- learning
- negative
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and addresses the negative transfer problem
  in continual reinforcement learning (CRL), where learning a new task can be impaired
  by previously learned tasks. Through extensive experiments across Meta World, DeepMind
  Control Suite, and Atari-100k environments, the authors demonstrate that negative
  transfer frequently occurs and cannot be mitigated by recent methods addressing
  plasticity loss or promoting positive transfer.
---

# Reset & Distill: A Recipe for Overcoming Negative Transfer in Continual Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2403.05066
- **Source URL**: https://arxiv.org/abs/2403.05066
- **Reference count**: 40
- **Key outcome**: Reset & Distill (R&D) method significantly outperforms state-of-the-art CRL baselines by addressing negative transfer through periodic network resets and knowledge distillation

## Executive Summary
This paper identifies negative transfer as a critical challenge in continual reinforcement learning (CRL), where learning new tasks can be impaired by previously learned tasks. Through extensive experiments across Meta World, DeepMind Control Suite, and Atari-100k environments, the authors demonstrate that negative transfer frequently occurs and cannot be mitigated by existing methods addressing plasticity loss or positive transfer. They propose Reset & Distill (R&D), a simple yet effective method that periodically resets online actor-critic networks and distills knowledge to an offline actor network, significantly outperforming state-of-the-art baselines across various task sequences.

## Method Summary
The Reset & Distill (R&D) method addresses negative transfer in CRL by periodically resetting online actor-critic networks before learning new tasks, combined with knowledge distillation to an offline actor network. The method uses behavior cloning on previous tasks stored in an expert buffer to prevent catastrophic forgetting. The offline actor learns all tasks sequentially through this distillation and cloning process, while the online learner focuses solely on the current task without interference from previous knowledge. This approach eliminates the negative influence of prior tasks while preserving useful information through supervised learning.

## Key Results
- R&D significantly outperforms fine-tuning, EWC, P&C, ClonEx, CReLU, and InFeR baselines on Meta World, DeepMind Control Suite, and Atari-100k
- Negative transfer occurs frequently in CRL and cannot be effectively addressed by recent methods targeting plasticity loss or positive transfer
- Reset & Distill achieves higher success rates across various task sequences while preventing catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Negative transfer occurs when new tasks are dissimilar to previous tasks, causing interference that impairs learning.
- **Mechanism**: Task dissimilarity leads to conflicting knowledge representations that reduce learning efficiency or cause complete failure, even with fine-tuning without stability constraints.
- **Core assumption**: Plasticity degradation in CRL stems from both stability-plasticity tradeoff and task similarity between consecutive tasks.
- **Evidence anchors**: Experiments show recent plasticity loss methods (Nikishin et al., 2022; Lewandowski et al., 2023) fail to mitigate negative transfer across multiple benchmark suites.

### Mechanism 2
- **Claim**: Resetting online actor-critic networks eliminates negative transfer by removing interference from previous tasks.
- **Mechanism**: Periodic parameter resets create a clean slate for learning new tasks, while offline distillation preserves useful knowledge without suffering from transfer interference.
- **Core assumption**: Negative transfer primarily results from online learner parameters being influenced by previous tasks.
- **Evidence anchors**: R&D combines network resets with offline distillation, showing superior performance compared to methods that maintain continuous parameter updates.

### Mechanism 3
- **Claim**: Knowledge distillation and behavior cloning prevent catastrophic forgetting while allowing online learners to focus on current tasks.
- **Mechanism**: Offline actors learn through supervised distillation from online learners (current task) and behavior cloning from expert buffers (previous tasks), avoiding online learning's transfer issues.
- **Core assumption**: Offline actors can effectively learn new tasks through supervised methods without experiencing negative transfer.
- **Evidence anchors**: The method stores state-action pairs in expert buffers and uses KL divergence for distillation, demonstrating stable performance across sequential tasks.

## Foundational Learning

- **Concept: Markov Decision Process (MDP)**
  - Why needed here: CRL involves learning multiple MDPs sequentially, requiring understanding of state, action, reward, and transition dynamics.
  - Quick check question: What are the components of an MDP, and how do they relate to the problem of continual reinforcement learning?

- **Concept: Actor-Critic Methods**
  - Why needed here: The paper uses actor-critic methods like SAC and PPO, requiring understanding of policy and value function networks.
  - Quick check question: How do actor-critic methods combine value-based and policy-based approaches in reinforcement learning?

- **Concept: Catastrophic Forgetting**
  - Why needed here: CRL aims to learn multiple tasks without forgetting previous ones, making catastrophic forgetting a central challenge.
  - Quick check question: What is catastrophic forgetting, and why is it a problem in continual learning?

## Architecture Onboarding

- **Component map**: Online actor → Online critic → Current task learning → Replay buffer → Offline actor ← Knowledge distillation ← Expert buffer ← Previous tasks

- **Critical path**:
  1. Initialize online and offline actor networks
  2. For each task: reset online networks, learn current task with SAC/PPO, generate replay buffer, distill to offline actor, store in expert buffer
  3. Final policy is offline actor with knowledge of all tasks

- **Design tradeoffs**:
  - Reset frequency vs. learning efficiency: Too frequent resets hinder learning, too infrequent allow negative transfer
  - Expert buffer size vs. memory efficiency: Larger buffers improve performance but require more memory
  - Knowledge distillation vs. behavior cloning: Balancing these components is crucial for preventing forgetting while allowing focused online learning

- **Failure signatures**:
  - Negative transfer: Agent fails to learn new tasks or performs worse than learning from scratch
  - Catastrophic forgetting: Agent forgets previously learned tasks
  - Inefficient learning: Agent takes too long to learn tasks or requires excessive computational resources

- **First 3 experiments**:
  1. Replicate 3-task experiment from Section 3.1 to observe negative transfer in simple setting
  2. Apply R&D to 3-task experiment and compare against baseline methods
  3. Evaluate on longer sequences (Easy, Hard, Random from Section 5.2) to assess scalability

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the analysis several natural questions emerge:

### Open Question 1
- Question: How does the magnitude of negative transfer vary across different types of task transitions in CRL?
- Basis in paper: The paper demonstrates that negative transfer occurs frequently but doesn't systematically quantify severity across task types
- Why unresolved: Paper identifies negative transfer but doesn't characterize its dependency on specific task characteristics or similarity measures
- What evidence would resolve it: Comprehensive study mapping task transition pairs to negative transfer severity using task embedding distances

### Open Question 2
- Question: Can Reset & Distill be extended to incorporate positive transfer when beneficial?
- Basis in paper: R&D focuses solely on negative transfer mitigation without promoting positive transfer
- Why unresolved: Paper demonstrates R&D's effectiveness against negative transfer but doesn't explore hybrid approaches for selective positive transfer
- What evidence would resolve it: Empirical comparisons of R&D variants with conditional knowledge transfer based on task similarity metrics

### Open Question 3
- Question: How does choice of offline learning algorithm affect R&D's performance?
- Basis in paper: R&D uses behavior cloning but mentions ClonEx uses best-reward exploration technique
- Why unresolved: Paper uses behavior cloning without exploring alternative offline learning methods or their impact
- What evidence would resolve it: Systematic comparison of different offline learning algorithms within R&D framework across various task sequences

## Limitations
- Experimental scope focuses on success rates and returns without deeper analysis of when and why negative transfer occurs
- Analysis of R&D mechanism remains somewhat surface-level without exploring sensitivity to hyperparameters
- Relationship between reset frequency and performance is not systematically explored

## Confidence
- **High confidence**: Negative transfer occurs frequently in CRL and is not solved by existing methods addressing plasticity loss or positive transfer
- **Medium confidence**: Reset & Distill mechanism effectively addresses negative transfer through periodic resets and knowledge distillation
- **Medium confidence**: Reset & Distill outperforms state-of-the-art baselines across multiple benchmark suites

## Next Checks
1. Conduct ablation studies varying reset frequency and distillation weight to understand R&D's sensitivity to hyperparameters
2. Analyze learning curves and failure cases to identify specific conditions where negative transfer occurs most severely
3. Test R&D on longer task sequences (10+ tasks) to evaluate scalability and long-term performance stability