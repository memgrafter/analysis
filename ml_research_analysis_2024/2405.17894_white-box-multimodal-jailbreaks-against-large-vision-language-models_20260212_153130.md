---
ver: rpa2
title: White-box Multimodal Jailbreaks Against Large Vision-Language Models
arxiv_id: '2405.17894'
source_url: https://arxiv.org/abs/2405.17894
tags:
- attack
- adversarial
- text
- image
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the adversarial robustness of Large Vision-Language
  Models (VLMs) by introducing a novel text-image multimodal jailbreak attack strategy.
  Unlike existing unimodal approaches that only attack either images or text, the
  proposed method jointly optimizes both modalities to exploit a broader spectrum
  of vulnerabilities within VLMs.
---

# White-box Multimodal Jailbreaks Against Large Vision-Language Models

## Quick Facts
- arXiv ID: 2405.17894
- Source URL: https://arxiv.org/abs/2405.17894
- Reference count: 33
- Primary result: Novel text-image multimodal jailbreak attack achieving 96% success rate on MiniGPT-4

## Executive Summary
This paper introduces a novel text-image multimodal jailbreak attack strategy against Large Vision-Language Models (VLMs) that jointly optimizes both visual and textual modalities. Unlike existing unimodal approaches, the proposed method embeds toxic semantics into an adversarial image prefix through optimization without text input, then jointly optimizes this image prefix with an adversarial text suffix to maximize affirmative responses to harmful instructions. The resulting Universal Master Key (UMK) can effectively circumvent alignment defenses of VLMs, achieving a 96% attack success rate on MiniGPT-4 compared to significantly lower rates from unimodal approaches.

## Method Summary
The attack employs a dual optimization objective strategy. First, an adversarial image prefix is optimized from random noise to maximize the generation probability of harmful sentences without any text input, imbuing the image with toxic semantics. Then, an adversarial text suffix is introduced and co-optimized with the image prefix using Projected Gradient Descent (PGD) for the image and Greedy Coordinate Gradient (GCG) for the text, maximizing the probability of eliciting affirmative responses to harmful instructions. This multimodal approach leverages the shared feature space between image embeddings and text token embeddings to update both components simultaneously, exploiting a broader spectrum of vulnerabilities within VLMs.

## Key Results
- Achieves 96% attack success rate on MiniGPT-4, significantly outperforming existing unimodal approaches
- Demonstrates effectiveness of joint text-image optimization compared to unimodal attacks (GCG and VAJM)
- Shows the dual optimization objective strategy successfully addresses insufficient toxicity and inadequate instruction following issues
- Proposes a Universal Master Key (UMK) comprising both adversarial image prefix and text suffix components

## Why This Works (Mechanism)

### Mechanism 1
- Embedding toxic semantics into the adversarial image prefix makes the model more likely to generate harmful content by optimizing the image to maximize harmful sentence generation probability without text input
- Core assumption: The model's text generation is influenced by visual input even without textual context
- Evidence anchors: Abstract description of optimizing adversarial image prefix to generate diverse harmful responses without text input; corpus analysis showing related work on VLM adversarial attacks

### Mechanism 2
- Jointly optimizing adversarial image prefix and text suffix maximizes affirmative response probability by co-optimizing both components to elicit affirmative responses to harmful instructions
- Core assumption: Encouraging affirmative responses increases likelihood of generating harmful content
- Evidence anchors: Abstract description of integrating adversarial text suffix to maximize affirmative response probability; section on dual optimization objectives aimed at generating highly toxic affirmative responses

### Mechanism 3
- Text-image multimodal attack exploits broader spectrum of vulnerabilities compared to unimodal attacks by jointly attacking both modalities and leveraging shared feature space
- Core assumption: VLMs are vulnerable to attacks exploiting both visual and textual modalities
- Evidence anchors: Abstract statement about jointly attacking both text and image modalities; section describing use of PGD and GCG to update adversarial components

## Foundational Learning

- Concept: Adversarial attacks on machine learning models
  - Why needed here: Understanding how to manipulate model inputs to produce desired (often harmful) outputs is crucial for both developing and defending against attacks
  - Quick check question: What is the difference between white-box and black-box adversarial attacks?

- Concept: Optimization techniques (e.g., Projected Gradient Descent, Greedy Coordinate Gradient)
  - Why needed here: These techniques are used to find adversarial examples that maximize the probability of generating harmful or affirmative responses
  - Quick check question: How does Projected Gradient Descent differ from standard gradient descent in the context of adversarial attacks?

- Concept: Vision-Language Models (VLMs) and their architecture
  - Why needed here: Understanding how VLMs integrate visual and textual information is essential for developing effective attacks and defenses
  - Quick check question: How do VLMs typically align visual and textual features?

## Architecture Onboarding

- Component map: Adversarial image prefix (ùëã ùëù ùëéùëëùë£) -> Adversarial text suffix (ùëã ùë† ùëéùëëùë£) -> Universal Master Key (UMK) -> VLM model (ùëìùúÉ)
- Critical path:
  1. Initialize adversarial image prefix with random noise
  2. Optimize adversarial image prefix to maximize harmful content generation without text input
  3. Introduce adversarial text suffix
  4. Jointly optimize adversarial image prefix and text suffix to maximize affirmative response probability
  5. Use UMK to jailbreak VLMs
- Design tradeoffs: Balancing toxicity and instruction following in optimization objectives; choosing adversarial text suffix length; selecting harmful sentences corpus
- Failure signatures: Low attack success rate on held-out test sets; model generating benign content despite adversarial inputs; poor transferability of UMK across different VLMs
- First 3 experiments:
  1. Reproduce baseline unimodal attacks (GCG and VAJM) on MiniGPT-4 to establish comparison metrics
  2. Implement first stage of proposed attack (optimizing adversarial image prefix without text input) and evaluate effectiveness in generating harmful content
  3. Implement full proposed attack (jointly optimizing image prefix and text suffix) and compare performance to baseline attacks on both train and test sets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the transferability of Universal Master Keys (UMKs) be improved across different Large Vision-Language Models (VLMs) with varying architectures and tokenizers?
- Basis in paper: The authors acknowledge that UMKs have constrained transferability due to different model architectures, parameters, and tokenizers among VLMs
- Why unresolved: Improving transferability would require developing methods that account for semantic variations of UMKs across different models
- What evidence would resolve it: Successful jailbreak attempts using the same UMK across multiple VLMs with different architectures

### Open Question 2
- Question: Can the dual optimization objective strategy be effectively applied to other types of adversarial attacks beyond jailbreak attacks?
- Basis in paper: The paper discusses effectiveness of dual optimization objective in addressing insufficient toxicity and inadequate instruction following in jailbreak attacks
- Why unresolved: Unclear whether this strategy would be as effective in other adversarial attack scenarios like evasion or poisoning attacks
- What evidence would resolve it: Successful application of dual optimization objective strategy to other types of adversarial attacks with improved attack performance

### Open Question 3
- Question: How does the proposed text-image multimodal attack strategy compare to other multimodal attack strategies in terms of effectiveness and efficiency?
- Basis in paper: The paper introduces novel text-image multimodal attack strategy and demonstrates effectiveness against existing unimodal attacks
- Why unresolved: The paper does not provide comprehensive comparison with other multimodal attack strategies
- What evidence would resolve it: Thorough comparison of proposed strategy with other multimodal attack strategies considering attack success rate, computational efficiency, and transferability

## Limitations

- The discovered UMK may not transfer effectively to VLMs with different architectures or training paradigms beyond the tested MiniGPT-4 model
- Manual toxicity assessment introduces subjectivity and potential bias in measuring attack success rates
- The paper does not address computational cost or practical feasibility of implementing such optimization-based attacks in real-world scenarios

## Confidence

**High Confidence**: The technical methodology for dual optimization objective strategy is well-defined and follows established principles from adversarial machine learning. The claim about embedding toxic semantics into adversarial image prefix influencing model behavior is supported by the optimization framework.

**Medium Confidence**: The 96% attack success rate on MiniGPT-4 is supported by experimental results but requires independent verification due to limited testing scope. The assertion that text-image multimodal approach outperforms unimodal attacks is reasonable but depends heavily on implementation details.

**Low Confidence**: Claims about UMK being truly "universal" and effective against broad range of VLMs are not substantiated with sufficient empirical evidence. The paper does not demonstrate transferability across different model architectures.

## Next Checks

1. **Transferability Assessment**: Test the discovered UMK on at least three different VLM architectures (e.g., BLIP, Flamingo, LLaVA) to evaluate cross-model effectiveness and identify architecture-specific limitations

2. **Human Evaluation Protocol**: Implement a blind, multi-rater human evaluation protocol with clear toxicity scoring guidelines to reduce subjectivity in measuring attack success and provide more robust validation

3. **Computational Feasibility Analysis**: Measure computational resources required for optimization process (GPU hours, memory usage) and evaluate whether attack remains practical when scaled to larger VLMs or under resource constraints