---
ver: rpa2
title: 'Advancing DRL Agents in Commercial Fighting Games: Training, Integration,
  and Agent-Human Alignment'
arxiv_id: '2406.01103'
source_url: https://arxiv.org/abs/2406.01103
tags:
- agent
- agents
- training
- games
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Shukai is a DRL agent system for commercial fighting games that
  addresses challenges of scalability and human alignment. It introduces Heterogeneous
  League Training (HELT) combining agents with different network structures (FIS,
  QS, FQS) to balance competence and generalization across large character pools.
---

# Advancing DRL Agents in Commercial Fighting Games: Training, Integration, and Agent-Human Alignment

## Quick Facts
- arXiv ID: 2406.01103
- Source URL: https://arxiv.org/abs/2406.01103
- Reference count: 40
- Primary result: 22% improvement in sample efficiency through Heterogeneous League Training for commercial fighting games

## Executive Summary
Shukai is a DRL agent system designed for commercial fighting games that addresses scalability and human alignment challenges. The system introduces Heterogeneous League Training (HELT), which combines agents with different network structures to balance competence and generalization across large character pools. Deployed in Naruto Mobile, Shukai achieved consistent performance across all characters despite training on only 13% of them, with players showing over 50% next-day retention and 5% growth rate in engagement.

## Method Summary
The method combines three network architectures (FIS for competence, QS and FQS for generalization) in a heterogeneous league training framework with PPO optimization. The system trains on a subset of characters using quantized state representations to improve generalization, then deploys agents with three reward functions (balanced, cautious, aggressive) to align with human player archetypes. The architecture includes a behavior evaluation system based on expert insights and action constraints to maintain human-like gameplay.

## Key Results
- 22% improvement in sample efficiency compared to homogeneous league training
- Consistent performance across all 400+ characters despite training on only 50 (13%)
- Player skill improvement with beginner and intermediate agents maintaining appropriate challenge levels
- Over 50% next-day retention and 5% growth rate in player engagement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HELT improves training efficiency by 22% through heterogeneous agent composition
- Mechanism: Combining QS/FQS agents as main agents with FIS agents as league exploiters expands policy space diversity while maintaining generalization
- Core assumption: Structural diversity between league agents leads to complementary exploration rather than overlapping exploration
- Evidence anchors: [abstract] 22% improvement, [section 3.2] policy space diversity through structural differences, [corpus] weak evidence

### Mechanism 2
- Claim: Quantifying states enhances generalization to untrained characters
- Mechanism: Reducing ID information in state representation forces network to learn game patterns rather than character-specific responses
- Core assumption: Over-reliance on ID information causes overfitting to training subset
- Evidence anchors: [section 4.2] 36% competence drop for FIS on unfamiliar characters, [section 3.1] incomplete information for representation, [corpus] weak evidence

### Mechanism 3
- Claim: Multi-style reward functions align agent behavior with human player expectations
- Mechanism: Designing reward functions for balanced, cautious, and aggressive archetypes induces corresponding behavioral patterns
- Core assumption: Human players cluster into these three archetypes and respond positively to agents matching their play style
- Evidence anchors: [section 3.3] three archetypes discovered, [section 5.2] CDF comparison shows behavioral patterns, [corpus] weak evidence

## Foundational Learning

- Concept: Markov Decision Process formulation for fighting games
  - Why needed here: The paper models Naruto Mobile as an MDP with state space S, action space A, reward function R, etc.
  - Quick check question: What are the six elements of the MDP tuple used for Naruto Mobile?

- Concept: Heterogeneous league training structure
  - Why needed here: HELT uses three types of agents (main, main exploiter, league exploiter) with different network structures
  - Quick check question: How do the three league agents differ in their opponent selection and reset mechanisms?

- Concept: Policy gradient methods with importance sampling
  - Why needed here: The PPO algorithm uses importance sampling weights ρt(θ) = πθ(st|at)/πold(st|at) for stable updates
  - Quick check question: What is the role of the clip function in PPO's objective?

## Architecture Onboarding

- Component map: State quantization → HELT training → reward alignment → deployment evaluation
- Critical path: Quantized states → heterogeneous league training → reward function application → player engagement measurement
- Design tradeoffs: FIS gives competence but poor generalization vs QS/FQS give generalization but lower competence
- Failure signatures: 
  - Competence collapse on familiar characters
  - No improvement over baseline homogeneous training
  - Agents fail to match human behavioral patterns
- First 3 experiments:
  1. Train FIS vs QS/FQS agents on training subset and measure competence vs generalization
  2. Compare HELT vs homogeneous league training on same character set
  3. Evaluate different reward functions against human player data using the behavior scoring system

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HELT perform when applied to fighting games with character pools significantly larger than Naruto Mobile's 400 characters?
- Basis in paper: [explicit] Discusses HELT's effectiveness but notes scalability challenges for extensive character pools
- Why unresolved: Experimental validation limited to Naruto Mobile's character pool size
- What evidence would resolve it: Comparative experiments across games with varying character pool sizes, particularly those exceeding 1000 characters

### Open Question 2
- Question: What specific mechanisms or modifications to HELT would be necessary to maintain human alignment in fighting games with asymmetric character abilities?
- Basis in paper: [explicit] Discusses agent-human alignment but doesn't address games with vastly different power levels
- Why unresolved: Paper doesn't explore HELT performance with extreme character asymmetries
- What evidence would resolve it: Experimental results showing HELT's ability to maintain balanced human-agent interactions across highly asymmetric character designs

### Open Question 3
- Question: How does the computational cost of HELT scale with the number of characters in the training subset?
- Basis in paper: [inferred] Mentions computational demands but lacks detailed scaling analysis
- Why unresolved: Paper demonstrates effectiveness without exploring upper bounds of practical implementation
- What evidence would resolve it: Detailed computational complexity analysis showing GPU/CPU requirements as a function of training subset size

## Limitations

- Generalization claims rely on testing with only 50 trained characters out of 400+ total, leaving uncertainty about performance on intermediate subset sizes
- Player engagement metrics come from a single commercial deployment without controlled A/B testing against alternative approaches
- The three-player archetype model is asserted based on qualitative analysis rather than quantitative clustering validation

## Confidence

**High confidence**: HELT framework architecture and implementation details are well-specified; basic claim that heterogeneous agents train more efficiently is supported by data; commercial deployment metrics are concrete and measurable.

**Medium confidence**: Generalization claims are reasonably supported by controlled experiments showing different network types' performance on unfamiliar characters, though sample size and testing methodology could be more rigorous.

**Low confidence**: Three-player archetype model lacks quantitative validation; behavioral scoring system design appears somewhat subjective; assumed archetypes covering "primary" player behaviors lack empirical support.

## Next Checks

1. **Generalization stress test**: Systematically vary training subset size (10%, 25%, 50% of characters) and measure performance degradation curve for each network type to identify optimal tradeoff point.

2. **Human preference validation**: Conduct blinded player testing where participants play against different agent types without knowing which reward function was used, then measure subjective preferences to validate the three-archetype model.

3. **Cross-game transferability**: Implement HELT framework in a different fighting game with different mechanics and character designs to test whether heterogeneous training advantages transfer beyond Naruto Mobile context.