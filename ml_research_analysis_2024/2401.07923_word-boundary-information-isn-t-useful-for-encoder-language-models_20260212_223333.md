---
ver: rpa2
title: Word Boundary Information Isn't Useful for Encoder Language Models
arxiv_id: '2401.07923'
source_url: https://arxiv.org/abs/2401.07923
tags:
- wordpiece
- word
- high
- across
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether word boundary information is useful
  for encoder language models. The authors train transformer encoders across four
  training scales and evaluate alternative approaches to including word boundary information
  on a range of tasks.
---

# Word Boundary Information Isn't Useful for Encoder Language Models

## Quick Facts
- **arXiv ID**: 2401.07923
- **Source URL**: https://arxiv.org/abs/2401.07923
- **Reference count**: 40
- **Primary result**: Removing word boundary information from tokenizers improves morphological validity and task performance without harming general domain tasks

## Executive Summary
This paper investigates whether word boundary information is useful for encoder language models by comparing tokenizers with and without explicit boundary markers across multiple training scales and tasks. The authors train transformer encoders using WordPiece and a modified WordPiece′ tokenizer (with word boundary markers removed) and evaluate performance on a range of English and Finnish downstream tasks. They find that WordPiece′ produces more morphologically valid tokenizations and maintains or improves performance across tasks, while various approaches to including word boundary information (embeddings, tokens, modified MLM objectives) do not yield consistent improvements.

## Method Summary
The authors train transformer encoders using masked language modeling (MLM) across four training scales (V Low to V High) with varying model sizes, batch sizes, and steps. They compare WordPiece and WordPiece′ tokenizers on English and Finnish Wikipedia data, evaluating morphological validity on established benchmarks (LADEC, MorphoLex, MorphyNet, DagoBERT, Finnish MorphyNet). Models are finetuned on downstream tasks including GLUE, NER, Superbizarre, FLOTA (English) and FiNER, Eduskunta, FinnSentiment (Finnish). Alternative approaches to including word boundary information are tested through explicit binary index embeddings, explicit boundary tokens, and implicit boundary prediction via an additional MLM head.

## Key Results
- WordPiece′ produces more morphologically valid tokenizations (improved sequence length, precision, F1) compared to WordPiece
- WordPiece′ maintains or improves performance across downstream tasks in both English and Finnish
- None of the alternative approaches to including word boundary information (embeddings, tokens, modified MLM) consistently improve performance over WordPiece′

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Word boundary information is not useful for encoder language models because morphemes are the primary linguistic units that carry meaning
- Mechanism: When training encoder models with MLM, the model learns to predict masked tokens based on surrounding context. Since word boundaries don't carry semantic information beyond what's already encoded in the morphemes themselves, explicitly marking word boundaries doesn't improve the model's ability to learn meaningful representations
- Core assumption: The MLM task provides sufficient contextual information for the model to learn word boundaries implicitly, making explicit boundary markers redundant
- Evidence anchors:
  - [abstract] "modifying tokenisers to remove word boundary information isn't leading to a loss of useful information"
  - [section 5] "incorporating word boundary information in transformer encoders, either explicitly or implicitly, does not lead to substantial performance improvements"
  - [corpus] Weak evidence - related papers focus on boundary detection and symbol handling, not encoder performance without boundaries
- Break condition: If the model needs to generate text where word boundaries must be explicitly marked (e.g., for downstream decoding tasks), removing boundary information could become problematic

### Mechanism 2
- Claim: Word boundary embeddings and tokens introduce unnecessary complexity without improving performance
- Mechanism: Adding word boundary information through embeddings or tokens increases the model's parameter count and computational complexity. However, the model can already learn word boundary patterns from the sequential nature of text and whitespace patterns, making these additions redundant
- Core assumption: The model's ability to learn sequential patterns is sufficient for understanding word structure without explicit boundary markers
- Evidence anchors:
  - [section 4.2] "including the WB embeddings during finetuning decreases training stability"
  - [section 5] "none of these models consistently improve over WordPiece′"
  - [corpus] No direct evidence - related papers don't address embedding complexity trade-offs
- Break condition: In morphologically rich languages with complex compounding rules, explicit boundary markers might help disambiguate word boundaries that aren't clear from context alone

### Mechanism 3
- Claim: Modified MLM objectives that include word boundary prediction don't improve performance because they don't align with the primary learning objective
- Mechanism: The standard MLM task focuses on predicting tokens based on context. Adding an auxiliary task of predicting word boundaries creates a secondary objective that doesn't directly contribute to the primary goal of learning meaningful token representations. The model already learns boundary patterns as a byproduct of learning token sequences
- Core assumption: The standard MLM objective is sufficient for learning all necessary linguistic patterns, including word boundaries
- Evidence anchors:
  - [section 5] "adding the extra loss term gives mixed results across the four tasks and training scales"
  - [section 5] "the additional MLM head increases the total loss"
  - [corpus] Weak evidence - related papers focus on modified pretraining objectives but don't specifically address boundary prediction
- Break condition: If the downstream task specifically requires explicit word boundary knowledge (e.g., word segmentation tasks), the modified objective might become beneficial

## Foundational Learning

- Concept: Subword tokenization and WordPiece algorithm
  - Why needed here: Understanding how WordPiece works and how it encodes word boundaries is fundamental to grasping why removing boundary information doesn't hurt performance
  - Quick check question: How does WordPiece decide where to split words, and what role do the "##" and "_" prefixes play in this process?

- Concept: Masked Language Modeling (MLM) pretraining
  - Why needed here: The MLM task is the core learning mechanism being evaluated. Understanding how it works helps explain why explicit boundary information isn't necessary
  - Quick check question: What percentage of tokens are masked during MLM training, and how does this affect the model's ability to learn word boundaries?

- Concept: Morphologically complex words and their tokenization
  - Why needed here: The paper specifically evaluates performance on tasks involving complex words, making it crucial to understand how different tokenizations affect representation learning
  - Quick check question: How does the tokenization of a word like "unbeatable" differ between WordPiece and WordPiece′, and why might one be more effective for downstream tasks?

## Architecture Onboarding

- Component map:
  - Tokenizers (WordPiece vs WordPiece′) → Input embeddings → Transformer encoder → MLM heads (standard + optional boundary prediction) → Pretraining objective → Finetuning on downstream tasks
  - Key components: Embedding layer with position embeddings, transformer layers, MLM heads, and any boundary-specific components (embeddings, tokens, or additional loss terms)

- Critical path:
  1. Tokenization of input text
  2. Embedding lookup with positional information
  3. Transformer encoding
  4. MLM prediction (primary task)
  5. Optional boundary prediction (secondary task)
  6. Loss calculation and backpropagation

- Design tradeoffs:
  - Removing boundary markers reduces vocabulary size and redundancy but may lose explicit boundary information
  - Adding boundary embeddings increases parameter count and complexity without clear performance benefits
  - Modified MLM objectives add training complexity but don't improve primary task performance

- Failure signatures:
  - Decreased performance on tasks requiring explicit boundary knowledge (word segmentation)
  - Increased training instability when adding boundary components
  - No improvement or degradation in performance on standard NLP tasks despite added complexity

- First 3 experiments:
  1. Train WordPiece and WordPiece′ on a small corpus, compare vocabulary size and tokenization of morphologically complex words
  2. Implement the explicit binary index boundary embedding model and train on a small scale, compare MLM loss and downstream performance on a simple task
  3. Implement the implicit model with additional MLM head for boundary prediction, train on a small scale, and compare the two MLM head accuracies to understand if the boundary prediction task is being learned

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the morphological validity of tokenisation impact downstream performance for different languages?
- Basis in paper: The paper mentions that removing word boundary information improves morphological validity of tokenisations and suggests that morphemes are the most important subunits, but it only tests this on English and Finnish.
- Why unresolved: The paper only tests on two languages, so it's unclear if the findings would generalize to other languages with different morphological structures.
- What evidence would resolve it: Testing the models on a wider range of languages with different morphological structures, such as agglutinative languages like Turkish or polysynthetic languages like Inuktitut, would help determine if the findings are language-specific or generalizable.

### Open Question 2
- Question: How does the size of the vocabulary impact the performance of models with and without word boundary information?
- Basis in paper: The paper mentions that they did not alter the vocabulary size of their tokenisers, but it's unclear how this might impact the results.
- Why unresolved: The paper only tests on a fixed vocabulary size, so it's unclear how the results might change with different vocabulary sizes.
- What evidence would resolve it: Testing the models with different vocabulary sizes, both larger and smaller than the current size, would help determine if the results are dependent on the vocabulary size.

### Open Question 3
- Question: How do different architectures for including word boundary information impact downstream performance?
- Basis in paper: The paper tests several architectures for including word boundary information, but it's possible that there are alternative approaches that would perform better.
- Why unresolved: The paper only tests a limited number of architectures, so it's unclear if there are other approaches that would lead to better performance.
- What evidence would resolve it: Testing alternative architectures, such as using a separate encoder for word boundary information or using a different type of embedding for word boundaries, would help determine if there are better approaches than those tested in the paper.

## Limitations

- The evaluation focuses exclusively on English and Finnish, which may not generalize to languages with vastly different morphological structures
- The "no performance degradation" finding could be partly attributed to the relatively simple nature of the evaluated tasks - more complex linguistic tasks might reveal different patterns
- The tokenizers are evaluated primarily on morphological validity metrics, which may not capture all aspects of token quality that could affect downstream performance

## Confidence

**High Confidence (80-100%)**
- WordPiece′ produces more morphologically valid tokenizations than WordPiece, as measured by sequence length, precision, and F1 metrics on established benchmarks
- WordPiece′ maintains comparable or superior performance to WordPiece across downstream tasks in both English and Finnish
- Adding word boundary embeddings or tokens doesn't improve performance and may decrease training stability

**Medium Confidence (50-80%)**
- Word boundary information isn't useful for encoder language models in general domain tasks
- Morphemes are more important subunits than word boundaries for encoder models
- The modified MLM objective with boundary prediction doesn't improve primary task performance

**Low Confidence (0-50%)**
- These findings generalize to languages beyond English and Finnish
- Encoder-decoder architectures would show the same patterns as encoder-only models
- The results would hold for more complex downstream tasks involving explicit word boundary detection

## Next Checks

1. **Cross-linguistic validation**: Train and evaluate the same models on morphologically rich languages like Turkish or Hungarian, where word boundaries are less clear-cut than in English and Finnish. This would test whether the "morphemes are the primary units" hypothesis holds across typologically diverse languages.

2. **Encoder-decoder comparison**: Implement parallel experiments using encoder-decoder architectures (like BART or T5) to determine if the lack of word boundary utility is specific to encoder-only models or generalizes to sequence-to-sequence models that require explicit boundary knowledge for generation.

3. **Complex task evaluation**: Design and evaluate on tasks specifically requiring explicit word boundary knowledge, such as word segmentation in languages without clear orthographic boundaries, or tasks involving compound word decomposition where boundary information might be more critical than the paper's evaluated tasks suggest.