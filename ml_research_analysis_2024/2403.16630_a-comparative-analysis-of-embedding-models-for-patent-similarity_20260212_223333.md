---
ver: rpa2
title: A comparative analysis of embedding models for patent similarity
arxiv_id: '2403.16630'
source_url: https://arxiv.org/abs/2403.16630
tags:
- patent
- which
- similarity
- patents
- claims
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares static and contextual embedding models for
  patent similarity, using patent interference data as ground-truth. The proposed
  Patent SBERT-adapt-ub, a domain-adapted Sentence Transformer, outperforms existing
  state-of-the-art models.
---

# A comparative analysis of embedding models for patent similarity

## Quick Facts
- arXiv ID: 2403.16630
- Source URL: https://arxiv.org/abs/2403.16630
- Authors: Grazia Sveva Ascione; Valerio Sterzi
- Reference count: 8
- Primary result: Domain-adapted Patent SBERT outperforms existing models; static embeddings match contextual performance when trained on large datasets

## Executive Summary
This study compares static and contextual embedding models for patent similarity using patent interference data as ground-truth. The proposed Patent SBERT-adapt-ub, a domain-adapted Sentence Transformer, outperforms current state-of-the-art models. Notably, static embeddings (Word2vec TF-IDF) achieve comparable performance to contextual models when trained on large datasets, suggesting that training data scale matters as much as architecture. Among SBERT variants, domain adaptation leads to superior results, confirming the value of task-specific fine-tuning.

## Method Summary
The study compares static (Word2vec TF-IDF, Doc2vec) and contextual (SBERT variants) embedding models for patent similarity. Models are trained on USPTO patent data (abstracts, claims, CPC codes) from PatentsView, with SBERT variants fine-tuned using triplet loss on anchor-positive-negative patent pairs. Evaluation uses patent interference data from Ganguli et al. (2020) as ground-truth, measuring cosine similarity performance. The proposed Patent SBERT-adapt-ub is a domain-adapted version of pretrained SBERT architecture.

## Key Results
- Patent SBERT-adapt-ub outperforms current state-of-the-art in patent similarity
- Static embeddings achieve comparable performance to contextual models when trained on extensive data
- Domain adaptation of SBERT models leads to superior results compared to generic SBERT

## Why This Works (Mechanism)

### Mechanism 1
Domain adaptation of SBERT models leads to better patent similarity performance compared to generic SBERT. Fine-tuning a pretrained SBERT model on a task-specific dataset (patent triplets) creates embeddings that better capture the semantic nuances of patent text, which contains technical and legal jargon not well-represented in general text corpora. Core assumption: Patent text has domain-specific characteristics that require specialized training to capture effectively.

### Mechanism 2
Static embeddings can achieve performance comparable to contextual embeddings when trained on sufficiently large datasets. With enough training data, static embeddings can learn robust representations that capture semantic relationships effectively, compensating for their lack of context-awareness in the embedding itself. Core assumption: The size and quality of training data can offset architectural limitations of static embeddings.

### Mechanism 3
Patent interference data provides valid ground-truth for evaluating patent similarity models. Patent interferences represent cases where patent examiners have determined that two patent claims are overlapping, making them a natural proxy for maximum similarity between patent texts. Core assumption: Patent interference decisions are reliable indicators of textual similarity and can serve as ground-truth labels.

## Foundational Learning

- **Concept**: Understanding of different embedding architectures (static vs contextual)
  - Why needed here: The paper directly compares these two approaches and claims that training data scale can offset architectural differences
  - Quick check question: What is the fundamental difference between static and contextual embeddings in terms of how they represent word meaning?

- **Concept**: Knowledge of SBERT and its variants
  - Why needed here: The paper proposes two original SBERT models and compares them with existing ones
  - Quick check question: How does SBERT differ from vanilla BERT in its approach to sentence embeddings?

- **Concept**: Familiarity with patent structure and terminology
  - Why needed here: The domain-specific nature of patent text is a key factor in the model performance claims
  - Quick check question: What are the main textual components of a patent document, and which ones are typically used for similarity calculations?

## Architecture Onboarding

- **Component map**: Data preprocessing (Patents View extraction, triplet creation) -> Model training (fine-tuning SBERT variants on patent triplets) -> Evaluation (cosine similarity calculation and comparison against interference ground-truth) -> Analysis (performance comparison across different model types and training approaches)

- **Critical path**: 
  1. Data collection and preprocessing (Patents View extraction, triplet creation)
  2. Model training (fine-tuning SBERT variants on patent triplets)
  3. Evaluation setup (interference dataset preparation)
  4. Performance comparison and analysis

- **Design tradeoffs**: 
  - Static vs contextual embeddings: Computational cost vs. context-awareness
  - Domain adaptation vs. general models: Specialization vs. generalization
  - Training data size vs. model complexity: More data can compensate for simpler architectures

- **Failure signatures**: 
  - Poor performance across all models: Likely indicates issues with data preprocessing or evaluation methodology
  - Domain-adapted model underperforming generic model: Suggests inadequate domain-specific training data or poor fine-tuning strategy
  - Static model outperforming contextual model: May indicate contextual model overfitting or insufficient training data

- **First 3 experiments**:
  1. Compare cosine similarity scores of all models on a small subset of the interference dataset to verify basic functionality
  2. Test the impact of different pooling strategies (mean, max, CLS token) on SBERT model performance
  3. Evaluate model performance when trained on varying amounts of patent triplet data to determine the data threshold for static model competitiveness

## Open Questions the Paper Calls Out

### Open Question 1
Does the superiority of contextual embeddings over static embeddings in patent similarity tasks depend on the specific task architecture or on the training methodology? The authors note that "large static models performances are still comparable to contextual ones when trained on extensive data" and suggest that "the superiority in the performance of contextual embeddings may not be related to the actual architecture but rather to the way the training phase is performed." This remains unresolved as the study does not systematically vary training methodologies to isolate their effects from architectural differences.

### Open Question 2
How does the Patent SBERT-adapt-ub model perform on patent similarity tasks when trained on different types of patent text, such as claims versus abstracts? The authors suggest exploring other text types but no such comparison is provided. Training and evaluating the model on different patent text types would reveal how input text affects performance.

### Open Question 3
Is the domain adaptation of SBERT models universally beneficial for all types of patent similarity tasks, or is its effectiveness task-specific? While domain adaptation shows promise for patent similarity, its applicability to other patent tasks (e.g., classification, retrieval) is not explored. Evaluating the domain-adapted SBERT model across various patent tasks would determine if its benefits are task-specific or generalizable.

## Limitations

- Patent interference data may not provide valid ground-truth for similarity as interference decisions involve legal and strategic factors beyond textual overlap
- Comparison between static and contextual models conflates model architecture with training data scale, lacking controlled experiments to isolate these factors
- Domain adaptation mechanism relies on triplet training, but the size and diversity of the patent triplet dataset is not reported

## Confidence

**High Confidence**: Domain-adapted SBERT outperforms generic SBERT on patent similarity tasks, well-supported by experimental results and established literature on domain adaptation benefits.

**Medium Confidence**: Static embeddings can match contextual performance with sufficient data, but comparison lacks rigorous controlled experiments isolating data size effect from architectural differences.

**Low Confidence**: Patent interference data provides valid ground-truth for similarity evaluation, with weakest support and no external validation of this foundational assumption.

## Next Checks

1. Test all models on an independent patent similarity dataset with human-annotated relevance judgments (not interference-based) to verify whether relative performance rankings hold across different ground-truth sources.

2. Train both static and contextual models on identically-sized, progressively larger subsets of patent data to isolate the effect of training data volume from architectural differences.

3. Analyze the textual overlap between interfering patents and randomly selected non-interfering patents with similar CPC codes to quantify how much interference decisions reflect textual similarity versus other legal factors.