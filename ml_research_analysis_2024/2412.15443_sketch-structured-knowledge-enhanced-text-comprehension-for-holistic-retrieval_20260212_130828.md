---
ver: rpa2
title: 'SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic Retrieval'
arxiv_id: '2412.15443'
source_url: https://arxiv.org/abs/2412.15443
tags:
- sketch
- retrieval
- context
- text
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SKETCH improves retrieval-augmented generation by combining semantic
  chunking with knowledge graphs. It addresses context loss and limited comprehension
  in traditional RAG systems by merging structured and unstructured data for holistic
  understanding.
---

# SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic Retrieval

## Quick Facts
- arXiv ID: 2412.15443
- Source URL: https://arxiv.org/abs/2412.15443
- Authors: Aakash Mahalingam; Vinesh Kumar Gande; Aman Chadha; Vinija Jain; Divya Chaudhary
- Reference count: 40
- Primary result: SKETCH consistently outperforms baseline RAG methods across four diverse datasets, achieving high answer relevancy and context precision scores.

## Executive Summary
SKETCH is a novel retrieval-augmented generation system that combines semantic chunking with knowledge graphs to address context loss and limited comprehension in traditional RAG systems. By merging structured and unstructured data through hybrid retrieval, SKETCH achieves holistic understanding of queries. The system was evaluated on four datasets—Italian Cuisine, QuALITY, QASPER, and NarrativeQA—demonstrating consistent improvements over baseline methods across multiple RAGAS metrics including answer relevancy and context precision.

## Method Summary
SKETCH improves retrieval-augmented generation by integrating semantic text retrieval with knowledge graphs. The method uses semantic chunking to preserve thematic coherence of text segments, then combines these with structured knowledge extracted from text using GPT-4 NER. A hybrid retrieval approach merges results from both structured (knowledge graph) and unstructured (semantic chunks) sources, with tokens appearing in both contexts treated as confirmation signals for relevance. The system was evaluated using RAGAS metrics with GPT-3.5-turbo-16k as the evaluation judge across four diverse datasets.

## Key Results
- On Italian Cuisine dataset: answer relevancy of 0.94 and context precision of 0.99
- On QuALITY dataset: answer relevancy of 0.73
- On QASPER dataset: answer relevancy of 0.56
- On NarrativeQA dataset: answer relevancy of 0.50

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic chunking preserves thematic coherence of text segments, improving embedding quality and retrieval accuracy.
- Mechanism: Text is split into sentences, then merged into chunks based on cosine distance thresholds between sequential window embeddings, ensuring each chunk represents a semantically coherent unit.
- Core assumption: Thematic continuity within chunks leads to more accurate semantic embeddings, which improves matching with query embeddings.
- Evidence anchors:
  - [abstract] "integrating semantic text retrieval with knowledge graphs, thereby merging structured and unstructured data for a more holistic comprehension."
  - [section] "The process begins by splitting the text into individual sentences, which are then grouped with neighboring sentences based on a window size k... Vector embeddings are calculated for each window, and the cosine distance between sequential windows is evaluated."
  - [corpus] Weak - corpus neighbors do not discuss chunking methods in detail.
- Break condition: If threshold T is too high, chunks become too large and lose coherence; if too low, chunks become fragmented and lose context.

### Mechanism 2
- Claim: Knowledge graphs provide structured context about entity relationships, enabling multi-hop reasoning across the corpus.
- Mechanism: Entities are extracted from text using GPT-4 NER, structured into nodes and edges, then queried via cypher to retrieve relevant entities and their relationships based on user queries.
- Core assumption: Graph traversal can retrieve contextually relevant information that may be missed by semantic similarity alone, especially for multi-context questions.
- Evidence anchors:
  - [abstract] "merging structured and unstructured data for a more holistic comprehension."
  - [section] "Using the graph documents, a comprehensive Knowledge Graph (KG) is constructed. The KG captures the intricate relationships and connections between various entities..."
  - [corpus] Weak - corpus neighbors do not discuss knowledge graph construction or multi-hop reasoning.
- Break condition: If the knowledge graph is sparse or incomplete, multi-hop reasoning fails; if entity extraction is inaccurate, wrong nodes are retrieved.

### Mechanism 3
- Claim: Hybrid retrieval combining semantic chunks and knowledge graph results produces more accurate and contextually relevant answers than either approach alone.
- Mechanism: Results from structured (KG) and unstructured (semantic chunks) retrievers are merged, with tokens appearing in both contexts treated as confirmation signals for relevance.
- Core assumption: Information appearing in both structured and unstructured contexts is more likely to be truly relevant to the query.
- Evidence anchors:
  - [abstract] "integrating semantic text retrieval with knowledge graphs, thereby merging structured and unstructured data for a more holistic comprehension."
  - [section] "The retrieved results from these two components are then combined, forming a unified context that is subsequently fed to the Large Language Model (LLM)."
  - [corpus] Weak - corpus neighbors do not discuss hybrid retrieval mechanisms in detail.
- Break condition: If one retriever consistently outperforms the other, the hybrid approach may add unnecessary computational overhead without benefit.

## Foundational Learning

- Concept: Cosine similarity in high-dimensional embedding space
  - Why needed here: Used to measure semantic similarity between text chunks and queries for retrieval
  - Quick check question: If two text chunks have cosine similarity of 0.8, are they more or less semantically similar than chunks with similarity 0.3?

- Concept: Named Entity Recognition (NER)
  - Why needed here: Used to extract entities from queries to construct knowledge graph cypher queries
  - Quick check question: If a query is "What is the capital of France?", which entities would NER extract?

- Concept: Knowledge graph traversal and multi-hop reasoning
  - Why needed here: Enables retrieval of information connected through multiple relationships, important for complex queries
  - Quick check question: If entity A is related to B, and B is related to C, how many hops are needed to connect A to C?

## Architecture Onboarding

- Component map:
  Document Loader → Semantic Splitter → Recursive Splitter → FAISS Vector Store (unstructured)
  Document Loader → GPT-4 NER → Knowledge Graph Builder → Cypher Query Engine (structured)
  Hybrid Retriever → LLM

- Critical path: Query → NER → KG Query + Semantic Similarity → Result Merge → LLM → Answer

- Design tradeoffs:
  - Semantic chunking vs. fixed-length chunking: semantic preserves context but is computationally heavier
  - KG construction cost vs. retrieval quality: more comprehensive KGs improve retrieval but require more resources
  - Hybrid vs. single retriever: hybrid provides better accuracy but doubles retrieval cost

- Failure signatures:
  - Poor answer relevancy: Check embedding quality and threshold T
  - Low faithfulness: Check KG entity extraction accuracy
  - High computational cost: Check if both retrievers are needed for specific query types

- First 3 experiments:
  1. Compare semantic chunking with fixed-length chunking on a small dataset, measuring answer relevancy
  2. Test KG retrieval alone vs. semantic retrieval alone on multi-context questions
  3. Vary the threshold T for semantic chunking and measure impact on retrieval precision and recall

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SKETCH's performance scale with increasing dataset size, and what are the computational costs associated with maintaining large-scale knowledge graphs?
- Basis in paper: [inferred] The paper mentions limitations related to scalability and cost, noting that constructing large-scale knowledge graphs is labor-intensive and relying on paid LLMs like GPT-4 increases expenses.
- Why unresolved: The paper does not provide experimental results or analysis on SKETCH's performance with datasets of varying sizes or the computational costs involved.
- What evidence would resolve it: Experimental results comparing SKETCH's performance and computational costs across datasets of different sizes, including both small and large-scale datasets.

### Open Question 2
- Question: How does SKETCH handle the variability and potential errors introduced by using GPT models for query parsing and RAGAS evaluation?
- Basis in paper: [explicit] The paper states that SKETCH's dependence on GPT models for query parsing and RAGAS evaluation can introduce errors and variance due to sampling randomness, prompt sensitivity, and occasional hallucinations.
- Why unresolved: The paper acknowledges these potential issues but does not provide strategies or results showing how SKETCH mitigates these effects or how they impact overall performance.
- What evidence would resolve it: Analysis of SKETCH's performance with different GPT models, prompt variations, and sampling strategies, including error rates and their impact on retrieval accuracy.

### Open Question 3
- Question: Can SKETCH's knowledge graph construction be optimized to reduce labor intensity and improve efficiency?
- Basis in paper: [inferred] The paper mentions that constructing large-scale knowledge graphs is labor-intensive, suggesting a need for optimization.
- Why unresolved: The paper does not explore methods to optimize knowledge graph construction or compare the efficiency of SKETCH's approach with alternative methods.
- What evidence would resolve it: Comparative studies of knowledge graph construction methods, including time and resource requirements, and analysis of how different construction techniques affect SKETCH's performance.

## Limitations

- The paper lacks specific configuration parameters for semantic chunking algorithm (window size k, threshold T for merging) which significantly affect retrieval quality
- Evaluation focuses primarily on answer relevancy and context precision metrics without reporting computational efficiency comparisons
- Quality of retrieval depends heavily on comprehensiveness of knowledge graph, with no discussion of performance degradation when entity extraction fails or knowledge graph is sparse

## Confidence

- **High confidence**: The hybrid retrieval mechanism combining semantic chunks and knowledge graph results is well-founded and the reported performance improvements over baseline methods are statistically significant based on the provided RAGAS metrics.
- **Medium confidence**: The semantic chunking approach improves contextual coherence, though the optimal threshold parameters may vary across different domains and require tuning.
- **Low confidence**: The scalability and computational efficiency of SKETCH for large-scale production systems, as the paper does not provide performance benchmarks or resource utilization data.

## Next Checks

1. **Parameter sensitivity analysis**: Systematically vary the semantic chunking threshold T and window size k across different datasets to determine optimal parameter ranges and assess robustness to parameter changes.

2. **Knowledge graph quality assessment**: Evaluate retrieval performance when the knowledge graph has varying levels of completeness and entity extraction accuracy to understand failure modes and limitations of the KG component.

3. **Computational overhead measurement**: Compare the runtime and resource utilization of SKETCH against baseline methods on identical hardware to quantify the practical costs of the hybrid retrieval approach.