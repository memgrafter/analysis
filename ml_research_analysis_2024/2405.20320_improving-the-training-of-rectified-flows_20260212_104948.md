---
ver: rpa2
title: Improving the Training of Rectified Flows
arxiv_id: '2405.20320'
source_url: https://arxiv.org/abs/2405.20320
tags:
- rectified
- flow
- training
- arxiv
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes improved training techniques for rectified
  flows, a simulation-free generative model. The key insight is that under practical
  settings, a single iteration of the Reflow algorithm is sufficient to learn nearly
  straight ODE trajectories, contrary to prior work which used multiple Reflow iterations.
---

# Improving the Training of Rectified Flows

## Quick Facts
- arXiv ID: 2405.20320
- Source URL: https://arxiv.org/abs/2405.20320
- Reference count: 40
- Key outcome: Single-iteration Reflow training achieves 3.07 FID on CIFAR-10 in one step, outperforming state-of-the-art distillation methods

## Executive Summary
This paper introduces improved training techniques for rectified flows that achieve state-of-the-art performance in the low NFE (number of function evaluations) regime. The key insight is that under practical settings, a single iteration of the Reflow algorithm is sufficient to learn nearly straight ODE trajectories, contrary to prior work which used multiple Reflow iterations. Based on this observation, the authors propose several training improvements including a U-shaped timestep distribution, an LPIPS-Huber loss, and EDM initialization. These techniques significantly improve 2-rectified flow performance, achieving 3.07 FID on CIFAR-10 in one step and surpassing consistency distillation and progressive distillation on ImageNet while matching improved consistency training.

## Method Summary
The method trains 2-rectified flows using a single Reflow iteration with improved techniques. First, a 1-rectified flow is pre-trained using EDM initialization and a U-shaped timestep distribution. Synthetic data-noise pairs are then generated from this pre-trained model. The 2-rectified flow is trained on these synthetic pairs using the U-shaped timestep distribution and LPIPS-Huber loss. The LPIPS-Huber loss combines LPIPS perceptual similarity with Pseudo-Huber loss, weighted by (1-t) to focus more on challenging timesteps. The method supports efficient few-step inversion for applications like image interpolation and translation.

## Key Results
- Achieves 3.07 FID on CIFAR-10 in one step, outperforming state-of-the-art distillation methods
- On ImageNet 64×64, surpasses consistency distillation and progressive distillation in both one-step and two-step settings
- Matches performance of improved consistency training while supporting efficient few-step inversion
- Demonstrates 28% improvement from U-shaped timestep distribution and 35% improvement from LPIPS-Huber loss on CIFAR-10

## Why This Works (Mechanism)

### Mechanism 1
Under practical settings, a single iteration of the Reflow algorithm is sufficient to learn nearly straight ODE trajectories. The linear interpolation trajectories of the pre-trained rectified flow rarely intersect, meaning the optimal 2-rectified flow has near-zero curvature. This occurs because z'' = z' + (x' - x'') is not a common noise realization under realistic data distributions and sufficient training.

### Mechanism 2
The U-shaped timestep distribution improves training by focusing computational resources on more challenging timesteps. The training loss of 2-rectified flow is large at each end of the interval t ∈ [0, 1] and small in the middle, so a U-shaped distribution puts more emphasis on these challenging regions where the lower bound term in the training error decomposition is nearly zero.

### Mechanism 3
LPIPS-Huber loss improves few-step generative performance by focusing on perceptual similarity. LPIPS forces the model to reduce perceptual distance between generated data and ground truth, which is more important than pixel-wise accuracy for human perception. The Pseudo-Huber loss with weighting (1-t) relies more on LPIPS when t is close to 1 where the task is more challenging.

## Foundational Learning

- Concept: Ordinary Differential Equations (ODEs) and their numerical integration
  - Why needed here: Rectified flows are neural ODEs that need to be solved efficiently for fast sampling
  - Quick check question: What is the difference between Euler and Heun solvers in terms of accuracy and computational cost?

- Concept: Diffusion models and probability flow ODEs
  - Why needed here: Rectified flows are a generalization of diffusion models, and understanding their relationship is crucial for initialization and training
  - Quick check question: How does the perturbation kernel N(x, t²I) relate to the probability flow ODE?

- Concept: Knowledge distillation and consistency training
  - Why needed here: The paper compares rectified flows to distillation-based methods, so understanding their differences is important
  - Quick check question: What is the key difference between consistency distillation and progressive distillation in terms of training procedure?

## Architecture Onboarding

- Component map: Data loader -> Model (velocity field vθ) -> Loss function (LPIPS-Huber) -> Optimizer (Adam) -> Sampler (Euler/Heun) -> Initialization (EDM)

- Critical path: 1) Generate synthetic data-noise pairs using pre-trained diffusion model 2) Train 1-rectified flow using chosen loss and timestep distribution 3) Generate new synthetic pairs from 1-rectified flow 4) Train 2-rectified flow using improved techniques 5) Evaluate FID and other metrics

- Design tradeoffs: Batch size vs. training stability (larger batches improve performance but require more memory); NFE vs. sample quality (higher NFEs give better quality but slower sampling); LPIPS weight vs. perceptual quality (higher weight improves perceptual similarity but may harm pixel-wise accuracy)

- Failure signatures: High FID but low reconstruction error (model not capturing right distribution); Low FID but poor inversion quality (model not preserving invertibility); Training instability (learning rate too high or batch size too small)

- First 3 experiments: 1) Train 2-rectified flow with uniform timestep distribution and squared ℓ2 loss as baseline 2) Switch to U-shaped timestep distribution and measure FID improvement 3) Replace squared ℓ2 with LPIPS-Huber loss and measure perceptual quality improvement

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed training techniques be applied to other generative models beyond rectified flows? The paper only applies these techniques to rectified flows and does not explore their applicability to other generative models such as diffusion models or GANs.

### Open Question 2
What is the theoretical limit of the proposed techniques in terms of improving the performance of rectified flows? The paper shows significant improvements but does not provide a theoretical analysis of their limitations.

### Open Question 3
How do the proposed techniques affect the inversion process of rectified flows? The paper mentions that rectified flows naturally support inversion from data to noise but does not explore the effects of the proposed techniques on this aspect.

## Limitations
- The single-iteration Reflow assumption relies on distributional assumptions that need broader validation across diverse datasets beyond natural images
- The LPIPS-Huber loss effectiveness assumes perceptual metrics correlate with quantitative evaluation metrics, which may not generalize to all domains
- The U-shaped timestep distribution effectiveness depends on specific training error decomposition that may vary with model architecture or dataset characteristics

## Confidence
- **High Confidence**: The empirical improvements on CIFAR-10 and ImageNet benchmarks (3.07 FID for 1-step generation) are well-supported by quantitative results
- **Medium Confidence**: The theoretical justification for single-iteration Reflow relies on distributional assumptions that need broader validation across more datasets
- **Medium Confidence**: The LPIPS-Huber loss effectiveness assumes perceptual metrics correlate with quantitative evaluation metrics, which may not generalize to all domains

## Next Checks
1. Cross-dataset validation: Test the single-iteration Reflow assumption and U-shaped timestep distribution on diverse datasets (medical imaging, satellite imagery, text-to-image) to verify generalizability beyond natural images.

2. Ablation on LPIPS-Huber components: Systematically vary the LPIPS weighting parameter and conduct human perceptual studies to validate whether the perceptual improvements translate to human judgment across different image types.

3. Robustness to initialization quality: Evaluate performance when the pre-trained 1-rectified flow is imperfectly trained (varying training epochs, architectures) to test the sensitivity of the proposed techniques to initialization quality.