---
ver: rpa2
title: 'Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly
  fashion'
arxiv_id: '2406.19185'
source_url: https://arxiv.org/abs/2406.19185
tags:
- reward
- copg
- policy
- gradient
- generations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Contrastive Policy Gradient (CoPG), a reinforcement
  learning method for fine-tuning large language models on sequence-level rewards.
  CoPG addresses the limitation of existing direct alignment methods by optimizing
  arbitrary reward functions while remaining as simple and computationally lightweight.
---

# Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion

## Quick Facts
- arXiv ID: 2406.19185
- Source URL: https://arxiv.org/abs/2406.19185
- Reference count: 24
- Key outcome: Contrastive Policy Gradient (CoPG) is a reinforcement learning method that fine-tunes large language models on sequence-level rewards using a contrastive baseline, achieving higher rewards and lower KL divergence compared to direct alignment baselines on a summarization task.

## Executive Summary
This paper introduces Contrastive Policy Gradient (CoPG), a reinforcement learning method for fine-tuning large language models on sequence-level rewards. CoPG addresses the limitation of existing direct alignment methods by optimizing arbitrary reward functions while remaining as simple and computationally lightweight. The method achieves this by using a contrastive baseline in the policy gradient, enabling off-policy learning without importance sampling or additional value networks. Theoretical analysis shows CoPG generalizes both policy gradient and direct alignment methods. Experiments on a summarization task demonstrate that CoPG successfully optimizes a learned reward function, achieving higher rewards and lower KL divergence compared to direct alignment baselines.

## Method Summary
CoPG is a policy gradient method that uses a contrastive baseline to enable off-policy learning without importance sampling. The method takes as input a reference policy (πref), a reward model (R), and pairs of generations with identical prompts. It optimizes a policy (πθ) by minimizing a contrastive loss that compares pairs of generations weighted by the difference in their regularized rewards. The contrastive baseline is the expected reward under the other distribution (µ1 or µ2), which cancels out the bias from off-policy sampling while maintaining gradient direction toward the optimal policy. CoPG generalizes both direct alignment methods like IPO and classic policy gradient, unifying them under a single framework.

## Key Results
- CoPG successfully optimizes a learned reward function on the TL;DR summarization task, achieving higher rewards than direct alignment baselines
- The method maintains lower KL divergence between the optimized policy and reference model compared to baselines
- Theoretical analysis proves CoPG generalizes both policy gradient and direct alignment methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoPG achieves off-policy policy gradient optimization without importance sampling by using a contrastive baseline term.
- Mechanism: The contrastive baseline is the expected reward under the other distribution (µ1 or µ2), which cancels out the bias from off-policy sampling while maintaining gradient direction toward the optimal policy.
- Core assumption: The two sampling distributions (µ1 and µ2) share the same support as the reference policy and the optimized policy.
- Evidence anchors:
  - [abstract]: "It can be seen as an off-policy policy gradient approach that does not rely on important sampling techniques and highlights the importance of using (the right) state baseline."
  - [section]: "We obtain a sum of two policy-like gradients, however with striking differences. First, the expectation is not according to the learnt policy π, but according to either µ1 or µ2, meaning that it can be understood as a sound off-policy policy gradient. Second, there is a baseline, the contrastive term, which is the expected reward but according to the other distribution."
- Break condition: If the support assumption is violated (distributions don't share prompts or generations), the contrastive baseline loses its corrective power and the gradient becomes biased.

### Mechanism 2
- Claim: CoPG generalizes both direct alignment methods and classic policy gradient, unifying them under a single framework.
- Mechanism: By setting the reward to binary preference signals, CoPG recovers the IPO gradient; by using on-policy sampling with the contrastive baseline, it recovers standard policy gradient.
- Core assumption: The reward function can be transformed to match the preference structure without changing the optimal policy.
- Evidence anchors:
  - [abstract]: "We show this approach to generalize the direct alignment method IPO (identity preference optimization) and classic policy gradient."
  - [section]: "Property 3 (CoPG and IPO). For a pair of generations (y, y′), assume without loss of generality that y is preferred to y′ according to the reward model, and redefine R(y) = −R(y′) = 1/4, then we have ∇ℓCoPG(y, y′; π) = −1/2β(1/2 − β(ln π(y)/πref(y) − ln π(y′)/πref(y′)))², where the term on the right-hand side is the gradient of the sample-based IPO loss to be minimized."
- Break condition: If the reward function is not transformable to binary preferences (e.g., continuous rewards with no clear preference ordering), the IPO generalization breaks.

### Mechanism 3
- Claim: The contrastive objective creates a supervised-friendly loss that directly optimizes for the optimal KL-regularized policy.
- Mechanism: The loss compares pairs of generations and weights their log-likelihoods by the difference in rewards, creating a contrastive signal that drives the policy toward higher-reward sequences.
- Core assumption: The reward model accurately reflects the true utility of sequences, and the preference pairs in the dataset are correctly labeled.
- Evidence anchors:
  - [abstract]: "The method achieves this by using a contrastive baseline in the policy gradient, enabling off-policy learning without importance sampling or additional value networks."
  - [section]: "The proposed objective function (6), alongside with the strong result of Thm. 1, thanks to the specific form of the gradient (7), tells us that policy gradient can be safely applied to off-policy data, without the introduction of a correcting importance sampling term, if we use the right baseline, that is the contrastive term depicted above."
- Break condition: If the reward model is poorly calibrated or the preference pairs are noisy, the contrastive signal becomes misleading and optimization degrades.

## Foundational Learning

- Concept: Policy Gradient and the REINFORCE algorithm
  - Why needed here: CoPG is fundamentally a variant of policy gradient, so understanding the baseline role and variance reduction is essential
  - Quick check question: What is the purpose of subtracting a baseline in policy gradient, and why doesn't it change the expected gradient?

- Concept: Contrastive Learning and its application to RL
  - Why needed here: CoPG borrows the contrastive principle from self-supervised learning to create its unique baseline
  - Quick check question: How does the contrastive baseline in CoPG differ from the value baseline commonly used in PPO?

- Concept: KL-regularized RL and the optimal policy derivation
  - Why needed here: The optimal policy for the regularized objective is central to understanding why CoPG works
  - Quick check question: What is the form of the optimal policy for the KL-regularized reward objective, and how is it derived?

## Architecture Onboarding

- Component map:
  - Reference policy (πref) -> Sampling distributions (µ1, µ2) -> Reward model (R) -> Policy network (πθ)

- Critical path:
  1. Load reference policy and reward model
  2. Load dataset of generation pairs (y, y') with identical prompts
  3. For each batch, compute regularized rewards for both generations
  4. Compute contrastive loss and its gradient
  5. Update policy parameters via gradient ascent
  6. Periodically evaluate on validation prompts using reward model

- Design tradeoffs:
  - On-policy vs. off-policy: CoPG allows pure offline learning but may benefit from fresh generations
  - Single vs. heterogeneous distributions: Using the same distribution for µ1 and µ2 simplifies implementation but may limit exploration
  - Temperature (β) sensitivity: Too low causes instability, too high limits reward optimization

- Failure signatures:
  - KL divergence increasing rapidly while reward plateaus: Temperature too low
  - Reward barely increasing: Temperature too high or reward model poorly calibrated
  - Training becoming unstable: Batch size too small or learning rate too high

- First 3 experiments:
  1. Verify gradient correctness: Compute ∇ℓCoPG analytically and compare with finite differences on a simple toy reward
  2. Test bandit convergence: Apply CoPG to the 3-arm bandit from the paper and verify it reaches the optimal policy
  3. Validate reward optimization: Train on TL;DR with a fixed reward model and confirm generations achieve higher rewards over training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the exact conditions under which CoPG becomes unstable when the temperature β is too low, and can these be mitigated through alternative baseline choices or normalization techniques?
- Basis in paper: [explicit] The paper mentions CoPG becomes unstable when β is too low during LLM experiments
- Why unresolved: The paper identifies the instability but doesn't investigate the root cause or propose specific solutions
- What evidence would resolve it: Experiments testing different baseline formulations, normalization methods, or adaptive temperature schedules that could prevent instability

### Open Question 2
- Question: How does CoPG perform in online settings compared to offline, and what modifications (e.g., replay buffer strategies, exploration mechanisms) would be most effective for online CoPG?
- Basis in paper: [inferred] The paper mentions online settings as a key perspective and discusses replay buffers in Section 3.2
- Why unresolved: The paper only validates CoPG in offline settings and hasn't tested online implementations
- What evidence would resolve it: Comparative experiments between online CoPG with different data collection strategies versus offline CoPG and other online RL methods

### Open Question 3
- Question: Can CoPG be effectively extended to multi-objective RL settings, and what would be the optimal way to combine multiple reward signals within the contrastive framework?
- Basis in paper: [explicit] The paper explicitly identifies multi-objective extension as an open question in the Discussion section
- Why unresolved: The current formulation only handles single reward functions, and the paper doesn't propose a multi-objective variant
- What evidence would resolve it: Experiments demonstrating CoPG with multiple rewards, comparing different aggregation methods (weighted sum, Pareto optimization) against single-objective baselines

## Limitations

- The method only demonstrates effectiveness on a single task (summarization) with one reward model, limiting generalizability claims
- CoPG becomes unstable when the temperature parameter β is too low, though the paper doesn't fully investigate the root cause
- The theoretical analysis assumes the two sampling distributions share the same support as reference and optimized policies, which may not hold in practice

## Confidence

**High Confidence**: CoPG is a valid policy gradient method with a contrastive baseline that enables off-policy learning without importance sampling. The mathematical derivation is rigorous and the gradient formula is correct.

**Medium Confidence**: CoPG generalizes both policy gradient and direct alignment methods. While the mathematical proofs are sound, the practical implications and ease of use in real applications require more validation.

**Low Confidence**: CoPG significantly outperforms existing alignment methods in practice. The paper provides only one experimental domain, and the results, while positive, are not comprehensive enough to establish broad superiority.

## Next Checks

1. **Reward Model Sensitivity Analysis**: Test CoPG's performance across multiple reward models with varying quality levels (including noisy or poorly calibrated ones) to verify the robustness claims about contrastive baselines handling imperfect reward signals better than direct alignment methods.

2. **Multi-Task Generalization**: Apply CoPG to at least two additional tasks beyond summarization (e.g., dialogue response generation and code generation) with different reward models to assess whether the theoretical advantages translate to diverse practical settings.

3. **Temperature Sensitivity Grid Search**: Systematically vary the temperature parameter β across multiple orders of magnitude and document the precise failure modes (KL explosion, reward plateau, instability) to provide clearer guidance on hyperparameter selection and validate the claims about β's critical role.