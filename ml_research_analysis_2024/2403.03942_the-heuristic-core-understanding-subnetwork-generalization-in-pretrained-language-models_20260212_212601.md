---
ver: rpa2
title: 'The Heuristic Core: Understanding Subnetwork Generalization in Pretrained
  Language Models'
arxiv_id: '2403.03942'
source_url: https://arxiv.org/abs/2403.03942
tags:
- subnetworks
- heads
- head
- layer
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors analyze how pretrained language models generalize on
  syntactic tasks. They find that pruning a fine-tuned model with different random
  seeds yields subnetworks that perform similarly in-domain but generalize differently
  to out-of-domain tests.
---

# The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models

## Quick Facts
- arXiv ID: 2403.03942
- Source URL: https://arxiv.org/abs/2403.03942
- Reference count: 40
- Key outcome: Pruning fine-tuned models yields subnetworks with shared "heuristic core" heads that compute shallow features; generalization arises from additional heads building on these core components rather than competition between distinct subnetworks.

## Executive Summary
This paper investigates how pretrained language models generalize on syntactic tasks by analyzing subnetworks discovered through structured pruning. Contrary to the competing subnetworks hypothesis, the authors find that all subnetworks—regardless of their generalization ability—share a common set of nine attention heads called the "heuristic core." These heads compute simple, non-generalizing features, and the model generalizes by incorporating additional heads that depend on the heuristic core to compute higher-level representations. This suggests generalization arises from composing simple components rather than selecting between distinct subnetworks.

## Method Summary
The authors fine-tune BERT, RoBERTa, and GPT-2 on MNLI and QQP tasks, then apply structured pruning with multiple random seeds to identify subnetworks that preserve in-domain performance. They evaluate these subnetworks on both in-domain and out-of-domain adversarial datasets (HANS for MNLI, PAWS-QQP for QQP) to measure generalization. The analysis tracks effective size (smallest subnetwork matching model performance within a threshold) over training checkpoints and identifies frequent attention heads across subnetworks to locate the "heuristic core." Finally, they ablate these core heads to understand their role in generalization.

## Key Results
- Different subnetworks at the same sparsity level generalize differently to out-of-domain tests while maintaining similar in-domain accuracy
- All subnetworks share a common set of nine attention heads (the heuristic core) that compute shallow, non-generalizing features
- Generalization is accompanied by a sharp increase in effective size, contrasting with the decrease seen in grokking
- At intermediate sparsity levels, subnetworks contain different "counter-heuristic" components leading to varying degrees of partial generalization to different subcases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generalization in pretrained LMs occurs through incorporation of additional heads that depend on the outputs of a common "heuristic core" rather than competition between disjoint subnetworks.
- Mechanism: A fixed set of attention heads (the heuristic core) computes simple, non-generalizing features early in training. Generalization arises when the model adds more heads that build upon these features to compute higher-level representations.
- Core assumption: The heuristic core heads are necessary but not sufficient for generalization; they must be combined with additional specialized heads.
- Evidence anchors:
  - [abstract] "Instead of finding competing subnetworks, we find that all subnetworks—whether they generalize or not—share a set of attention heads, which we refer to as the heuristic core."
  - [section 4] "We find a small set of nine attention heads that occur in all subnetworks—even the ones that do not generalize at all."
- Break condition: If removing the heuristic core heads does not reduce OOD accuracy or if the model generalizes without these heads.

### Mechanism 2
- Claim: The effective size of the model increases during training as it generalizes, contrary to the decrease seen in grokking.
- Mechanism: Generalization is achieved by adding more components to the network rather than pruning away non-generalizing parts. This results in a sparse network with many specialized heads rather than a compact, general one.
- Core assumption: The model does not converge to a single subnetwork but instead maintains and builds upon multiple components.
- Evidence anchors:
  - [section 4] "On the other hand, we find that generalization in our case is accompanied by a sharp increase in effective size."
  - [abstract] "The model learns to generalize by incorporating additional attention heads, which depend on the outputs of the 'heuristic' heads to compute higher-level features."
- Break condition: If effective size decreases during generalization or if sparser subnetworks achieve equal or better generalization.

### Mechanism 3
- Claim: Different subnetworks at the same sparsity level generalize differently because they contain different "counter-heuristic" components that handle specific subcases.
- Mechanism: The model learns to handle different adversarial cases by incorporating different combinations of specialized heads beyond the heuristic core. Each subnetwork has a unique set of these additional heads.
- Core assumption: The heuristic core is necessary for all generalization, but the specific counter-heuristic heads determine which subcases are handled well.
- Evidence anchors:
  - [section 3] "Different subnetworks generalize differently... Subnetwork #8 achieves 41.4% at SO-Swap (Lexical Overlap) but only 18.0% at Embed-If (Constituent)."
  - [abstract] "At intermediate sparsity levels, subnetworks contain different 'counter-heuristic' components, leading to different degrees of partial generalization to different subcases."
- Break condition: If all subnetworks generalize equally well to all subcases or if removing counter-heuristic heads does not affect OOD accuracy.

## Foundational Learning

- Concept: Structured pruning and subnetwork identification
  - Why needed here: The study relies on isolating subsets of attention heads and MLP layers that approximate the full model's behavior to analyze generalization mechanisms.
  - Quick check question: What is the difference between structured and unstructured pruning, and why did the authors choose structured pruning?

- Concept: Effective size and faithfulness
  - Why needed here: The paper uses effective size (smallest subnetwork matching model performance within a threshold) to track how generalization emerges during training.
  - Quick check question: How is effective size computed, and what does it tell us about the model's generalization process?

- Concept: Heuristics and out-of-domain evaluation
  - Why needed here: The study focuses on syntactic generalization where models can exploit shallow heuristics that fail on adversarial test cases.
  - Quick check question: What are the HANS and PAWS datasets, and why are they used to test for heuristic exploitation?

## Architecture Onboarding

- Component map: BERT architecture with attention heads and MLP layers → pruning masks that select subsets of these components → evaluation on in-domain and out-of-domain datasets
- Critical path: Fine-tune BERT → Prune with multiple random seeds → Evaluate subnetworks → Identify heuristic core → Analyze attention patterns → Ablate heuristic core → Interpret results
- Design tradeoffs: Structured pruning preserves semantic meaning of components (attention heads) but may miss interactions between heads; unstructured pruning could find more expressive subnetworks but harder to interpret
- Failure signatures: If pruning fails to find subnetworks matching full model performance, if heuristic core cannot be identified consistently, or if ablation experiments do not show expected effects on OOD accuracy
- First 3 experiments:
  1. Prune a fine-tuned BERT model with 12 random seeds at 50% sparsity and evaluate in-domain vs OOD accuracy to confirm subnetwork generalization differences
  2. Compute head frequencies across subnetworks to identify the heuristic core and verify it appears in all subnetworks
  3. Ablate each heuristic core head from the full model and measure changes in in-domain and OOD accuracy to understand their role

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions in the text provided.

## Limitations
- The analysis relies on specific syntactic generalization datasets (HANS, PAWS) that may not capture the full breadth of generalization challenges
- The study focuses on attention heads while potentially missing important contributions from MLP layers
- The stability of the heuristic core identification across different task domains remains unclear

## Confidence
**High Confidence:**
- Different subnetworks at the same sparsity level exhibit varying out-of-domain generalization performance
- A common set of attention heads (heuristic core) exists across all subnetworks, including those that don't generalize
- The heuristic core computes shallow, non-generalizing features as evidenced by attention pattern analysis

**Medium Confidence:**
- Generalization occurs through incorporation of additional heads that depend on the heuristic core
- Effective size increases during generalization rather than decreases (contrasting with grokking)
- Counter-heuristic components in different subnetworks explain varying degrees of partial generalization

**Low Confidence:**
- The heuristic core is necessary but not sufficient for generalization (based on ablation results showing minimal performance drops)
- The mechanism generalizes beyond BERT to RoBERTa and GPT-2 models
- The nine identified heuristic core heads represent the complete set across all settings

## Next Checks
1. **Ablation Validation**: Systematically ablate each of the nine heuristic core heads individually and in combinations from the full model, measuring both in-domain and out-of-domain performance across multiple random seeds to verify their non-essential role.

2. **Cross-Domain Stability**: Apply the same analysis pipeline to a different task domain (e.g., syntactic parsing or semantic role labeling) to test whether the heuristic core identification method and mechanism hold across diverse NLP tasks.

3. **Temporal Analysis Extension**: Track the emergence of the heuristic core and counter-heuristic components throughout training by analyzing model checkpoints at finer intervals, particularly around the point where generalization first appears, to better understand the dynamics of subnetwork formation.