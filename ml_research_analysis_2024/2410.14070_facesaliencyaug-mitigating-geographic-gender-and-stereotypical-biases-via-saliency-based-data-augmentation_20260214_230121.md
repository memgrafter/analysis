---
ver: rpa2
title: 'FaceSaliencyAug: Mitigating Geographic, Gender and Stereotypical Biases via
  Saliency-Based Data Augmentation'
arxiv_id: '2410.14070'
source_url: https://arxiv.org/abs/2410.14070
tags: []
core_contribution: This paper addresses geographical, gender, and stereotypical biases
  in computer vision models by introducing FaceSaliencyAug, a data augmentation approach
  that leverages salient face regions to improve dataset diversity and mitigate biases.
  The method randomly applies masking strategies to the salient region of face images
  and restores the original image, enhancing data diversity and fairness.
---

# FaceSaliencyAug: Mitigating Geographic, Gender and Stereotypical Biases via Saliency-Based Data Augmentation

## Quick Facts
- arXiv ID: 2410.14070
- Source URL: https://arxiv.org/abs/2410.14070
- Reference count: 40
- Primary result: FaceSaliencyAug improves dataset diversity (ISS-intra from 0.9644 to 0.9699, ISS-inter from 0.9846 to 0.9871) and reduces gender bias (IIAS scores decrease by up to 53 times) while maintaining competitive accuracy on CIFAR-10 and CIFAR-100

## Executive Summary
FaceSaliencyAug is a data augmentation approach that leverages salient face regions to improve dataset diversity and mitigate biases in computer vision models. The method randomly applies masking strategies to the salient region of face images and restores the original image, enhancing data diversity and fairness. Experiments across five datasets show superior diversity metrics and significant reduction in gender bias for both CNNs and ViTs, while achieving competitive accuracy on standard benchmarks.

## Method Summary
FaceSaliencyAug detects salient facial regions and applies random partial erasure (row, column, or block masking) to these regions before restoring the original image. This forces models to learn from incomplete, spatially varied facial cues rather than relying on correlated attributes that perpetuate bias. The approach uses six erasure strategies (RSE, CSE, RCSE, PSE, HHSE, VHSE) to ensure no single bias pattern is reinforced. The augmented data increases diversity (measured by ISS-intra) while the restoration step maintains realism, reducing gender bias by breaking reliance on gender-correlated visual cues.

## Key Results
- ISS-intra scores improved from 0.9644 to 0.9699 and ISS-inter scores from 0.9846 to 0.9871
- Gender bias reduction with IIAS scores decreasing by up to 53 times in unmasked scenarios
- ResNet56 accuracy improved from 94.69% to 95.14% on CIFAR-10 while maintaining competitive performance on CIFAR-100

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Saliency-based masking disrupts biased facial feature co-occurrence patterns while preserving identity-relevant structure
- Mechanism: The method detects salient facial regions and applies random partial erasure to these regions before restoring the original image, forcing the model to learn from incomplete, spatially varied facial cues
- Core assumption: Salient facial regions contain both identity-critical and bias-correlated features; disrupting their consistency without removing them entirely reduces shortcut learning
- Evidence anchors: [abstract] Leveraging salient regions mitigates geographical and stereotypical biases; [section 3] FaceSaliencyAug randomly selects masks from a predefined search space and applies them to salient regions
- Break condition: If salient region detection fails to exclude biased background context or if mask patterns become predictable

### Mechanism 2
- Claim: Random masking increases intra-dataset variance without introducing synthetic artifacts, improving fairness metrics
- Mechanism: By randomly selecting from six erasure strategies, the approach ensures that no single bias pattern is reinforced while the augmented data increases diversity
- Core assumption: Diversity in masked patterns correlates with reduced bias in learned representations
- Evidence anchors: [abstract] The proposed augmentation strategy enhances data diversity, thereby improving model performance and debiasing effects; [section 4.2] ISS scores improved significantly
- Break condition: If masking erases too much contextual information, model performance may degrade despite bias reduction

### Mechanism 3
- Claim: The method reduces gender bias in both CNNs and ViTs by breaking reliance on gender-correlated visual cues
- Mechanism: IIAS scores measure gender bias via cosine similarity between gender attributes and occupation images; masking salient facial regions prevents easy association of gender with occupation stereotypes
- Core assumption: Gender bias in vision models stems partly from over-reliance on visible facial cues rather than contextual or behavioral indicators
- Evidence anchors: [abstract] Experiments reveal reduction in gender bias for both CNNs and ViTs; [section 4.2] IIAS scores decreased by up to 53 times
- Break condition: If the model learns to infer masked gender cues from other features, bias may persist

## Foundational Learning

- Concept: Saliency detection in facial images
  - Why needed here: The entire augmentation strategy depends on accurately identifying which facial regions are most informative for identity and potentially biased
  - Quick check question: What are the two most common algorithms for facial saliency detection used in fairness-focused vision research?

- Concept: Image Similarity Score (ISS) metrics
  - Why needed here: ISS-intra and ISS-inter are the primary quantitative measures of dataset diversity improvements claimed by the paper
  - Quick check question: How does ISS-intra differ from ISS-inter in terms of what aspect of diversity they capture?

- Concept: Image-Image Association Score (IIAS) for bias measurement
  - Why needed here: IIAS is used to quantify gender bias reduction in CNNs and ViTs; understanding its computation is critical to interpreting results
  - Quick check question: What does a positive IIAS score indicate about the model's associations between gender and occupation?

## Architecture Onboarding

- Component map: Salient Region Detector → Mask Generator (6 strategies) → Masked Image → Restoration Layer → Output → Downstream evaluation
- Critical path: Saliency detection → Mask application → Image restoration → Diversity/bias evaluation
- Design tradeoffs: Masking intensity vs. information preservation; mask strategy randomness vs. pattern consistency
- Failure signatures: If ISS-intra does not improve, mask strategies may not be sufficiently diverse; if IIAS reduction is minimal, model may infer gender from non-facial cues
- First 3 experiments: 1) Verify salient region detection across all datasets, 2) Apply each mask strategy individually to measure ISS-intra change, 3) Train CNN on unmasked vs. masked data and compare IIAS scores

## Open Questions the Paper Calls Out
No specific open questions are explicitly called out in the paper.

## Limitations
- The effectiveness depends heavily on the quality of saliency detection, which is not specified in detail
- The mechanism by which saliency-based masking specifically disrupts gender bias patterns is not empirically validated and relies on theoretical assumptions
- ISS-intra and ISS-inter metrics are central to diversity claims, but their exact implementation details are not provided, creating a potential reproduction barrier

## Confidence
- High confidence: ISS score improvements (0.9644 → 0.9699 and 0.9846 → 0.9871) are specific, measurable, and directly reported
- Medium confidence: IIAS reduction claims (53x decrease) are supported by experimental data but rely on proper saliency detection and metric implementation
- Low confidence: The mechanism by which saliency-based masking specifically disrupts gender bias patterns is not empirically validated and relies on theoretical assumptions

## Next Checks
1. Verify that the saliency detection method correctly identifies facial regions across all five datasets and that these regions correspond to areas where bias typically manifests
2. Implement the ISS-intra and ISS-inter algorithms independently to confirm that the reported diversity improvements can be reproduced with the augmentation method
3. Test the method on a held-out dataset not used in training to verify that the bias reduction generalizes beyond the specific occupation datasets