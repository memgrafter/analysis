---
ver: rpa2
title: 'Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence'
arxiv_id: '2404.05892'
source_url: https://arxiv.org/abs/2404.05892
tags:
- eagle
- finch
- language
- rwkv
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Eagle (RWKV-5) and Finch (RWKV-6), two RNN-based
  architectures that improve upon the RWKV (RWKV-4) architecture. The key innovations
  are multi-headed matrix-valued states and a dynamic recurrence mechanism that improve
  expressivity while maintaining the inference efficiency characteristics of RNNs.
---

# Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence

## Quick Facts
- arXiv ID: 2404.05892
- Source URL: https://arxiv.org/abs/2404.05892
- Reference count: 40
- Introduces Eagle (RWKV-5) and Finch (RWKV-6) with matrix-valued states and dynamic recurrence

## Executive Summary
Eagle and Finch are RNN-based language models that improve upon the RWKV-4 architecture through multi-headed matrix-valued states and a dynamic recurrence mechanism. The models are trained on a new multilingual corpus of 1.12 trillion tokens and evaluated across various benchmarks, showing competitive performance. Four Eagle models (0.46 to 7.5 billion parameters) and two Finch models (1.6 and 3.1 billion parameters) are released under the Apache 2.0 license.

## Method Summary
The paper introduces Eagle and Finch, RNN-based architectures that build on RWKV-4 with two key innovations: multi-headed matrix-valued states and dynamic recurrence. Matrix-valued states allow per-channel attention decay rates, increasing expressive power over vector-valued states. Finch adds dynamic recurrence with data-dependent decay schedules using LoRA augmentation. The models are trained on a 1.12 trillion token multilingual corpus using time-parallel training on GPUs, with evaluation on multilingual and English benchmarks.

## Key Results
- Four Eagle models (0.46B-7.5B params) and two Finch models (1.6B-3.1B params) show competitive performance across multilingual benchmarks
- Eagle-7.5B achieves strong results on LAMBADA, XNLI, and other multilingual tasks
- Finch-3.1B demonstrates improved expressivity through dynamic recurrence mechanism
- Models maintain RNN efficiency characteristics while improving long-range dependency handling

## Why This Works (Mechanism)

### Mechanism 1
Matrix-valued states allow per-channel attention decay rates, increasing expressive power over RWKV-4's vector-valued states. Each channel has its own 64×64 matrix storing richer context information and applying independent decay schedules. Core assumption: added complexity is worth parameter increase and computational cost.

### Mechanism 2
Dynamic recurrence with data-dependent decay schedules (Finch) adapts attention weighting based on input context. Decay rate becomes a function of current and previous tokens through LoRA augmentation, learning when to prioritize recent vs. older information. Core assumption: input-dependent decay is more effective than static learned decay for long-range dependencies.

### Mechanism 3
Token-shift with linear interpolation learns how much new vs. old information to use per channel and timestep. The model learns per-channel interpolation weights between current and previous tokens, creating "induction heads" within single layers. Core assumption: learned interpolation weights capture meaningful patterns about when to use new vs. old information.

## Foundational Learning

- **Linear attention mechanism**: Needed for O(1) complexity per token instead of O(N²). Quick check: How does linear attention achieve O(1) complexity compared to standard attention's O(N²)?

- **Matrix operations and diagonal matrices**: Core mechanism uses diag(w) · kT i · vi where w is learned per-channel decay vector. Quick check: Why is diag(w) used instead of just w in attention computation?

- **Low-Rank Adaptation (LoRA)**: Finch uses LoRA to augment static decay rates with data-dependent offsets. Quick check: How does LoRA allow for efficient parameter-efficient adaptation of learned vectors?

## Architecture Onboarding

- **Component map**: Token → Token Shift → Time Mixing (with WKV state) → Channel Mixing → Output
- **Critical path**: Token through token shift module, time mixing with WKV state, channel mixing, to output
- **Design tradeoffs**: Matrix-valued states vs vector states (more expressive but higher parameter count); static decay vs dynamic decay (simpler but potentially less adaptive); linear attention vs standard attention (efficient but potentially less powerful)
- **Failure signatures**: Poor performance on long sequences (issues with decay rate learning); instability during training (improper matrix initialization); slow inference (inefficient matrix operations implementation)
- **First 3 experiments**: 1) Ablation study: remove token shift and measure impact; 2) Parameter sweep: vary matrix dimension size and measure trade-off; 3) Long context test: evaluate on sequences longer than training context

## Open Questions the Paper Calls Out

- **Open Question 1**: How does dynamic recurrence in Finch affect long-term memory compared to Eagle's static decay rates? The paper contrasts Finch's "data-dependent" decay with Eagle's "static learned vector" but doesn't provide quantitative comparisons of long-term memory capacity.

- **Open Question 2**: What is the optimal LoRA rank for token-shift and decay parameters in Finch, and how does it scale with model size? The paper mentions future models may "double or more" the weight matrix size but doesn't provide specific recommendations.

- **Open Question 3**: How does performance scale when trained on datasets larger than 1.12 trillion tokens? The paper states their corpus is "much smaller than training data sizes for contemporary models" and plans expansion, but only reports results from 1.12 trillion tokens.

## Limitations

- Limited ablation studies without comprehensive detail on individual component contributions
- Evaluation primarily on multilingual benchmarks with limited testing on code generation or mathematical reasoning
- No detailed computational complexity analysis of matrix-valued states overhead
- Reproducibility concerns due to incomplete implementation details

## Confidence

- **High Confidence**: Architectural modifications are correctly described; training methodology follows standard practices; models demonstrate competitive benchmark performance
- **Medium Confidence**: Specific performance improvements over RWKV-4 are accurately reported; efficiency claims are valid; multilingual corpus construction is sound
- **Low Confidence**: Relative contribution of each innovation to overall performance; generalization to untested domains; claimed advantages over transformers in practical deployment

## Next Checks

1. **Ablation Study Replication**: Implement and train Eagle models with individual components removed to quantify each mechanism's contribution and compare performance degradation patterns.

2. **Efficiency Benchmarking**: Measure actual inference latency and memory usage across different sequence lengths for Eagle/Finch versus RWKV-4 and transformer baselines, including both theoretical analysis and empirical measurements.

3. **Long Context Extrapolation Test**: Evaluate Eagle and Finch models on sequences significantly longer than training context (e.g., 8192 tokens when trained on 4096) to test genuine long-range dependency benefits versus fitting training distribution.