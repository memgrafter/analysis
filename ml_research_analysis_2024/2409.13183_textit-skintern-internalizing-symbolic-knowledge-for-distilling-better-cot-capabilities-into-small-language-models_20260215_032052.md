---
ver: rpa2
title: '$\textit{SKIntern}$: Internalizing Symbolic Knowledge for Distilling Better
  CoT Capabilities into Small Language Models'
arxiv_id: '2409.13183'
source_url: https://arxiv.org/abs/2409.13183
tags:
- knowledge
- skintern
- learning
- reasoning
- symbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SKIntern is a novel method for distilling reasoning capabilities
  into small language models (SLMs) by gradually internalizing symbolic knowledge
  and few-shot examples. It uses a progressive fine-tuning pipeline where knowledge
  is compressed and examples are pruned over time, allowing the SLM to internalize
  information into its parameters.
---

# $\textit{SKIntern}$: Internalizing Symbolic Knowledge for Distilling Better CoT Capabilities into Small Language Models

## Quick Facts
- arXiv ID: 2409.13183
- Source URL: https://arxiv.org/abs/2409.13183
- Reference count: 34
- SKIntern achieves over 5% improvement on both in-domain and out-of-domain tasks while reducing inference costs by up to 4×

## Executive Summary
SKIntern is a novel method for distilling reasoning capabilities into small language models (SLMs) by gradually internalizing symbolic knowledge and few-shot examples. The approach uses a progressive fine-tuning pipeline where knowledge is compressed and examples are pruned over time, allowing the SLM to internalize information into its parameters. During inference, only the question is processed, eliminating the need for external knowledge or examples. The method outperforms state-of-the-art baselines across various SLM sizes while significantly reducing computational overhead.

## Method Summary
SKIntern employs a teacher-student framework where a large language model (LLM) generates rationales and symbolic knowledge for training examples. This knowledge undergoes progressive compression through token-level pruning and example-level pruning using a linear decay schedule. The SLM is then fine-tuned progressively on increasingly compressed representations, internalizing the reasoning capabilities into its parameters. During inference, the model processes only the question without requiring external knowledge or examples.

## Key Results
- Outperforms state-of-the-art baselines by over 5% on both in-domain and out-of-domain tasks
- Reduces inference costs by up to 4× compared to traditional CoT approaches
- Demonstrates effectiveness across various SLM sizes (1B-7B parameters)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive internalization through token-level symbolic knowledge compression and instance-level example pruning gradually transfers knowledge from explicit prompt context into model parameters.
- Mechanism: During each fine-tuning schedule step, SKIntern prunes tokens and examples according to a linear decay schedule, forcing the model to internalize compressed knowledge rather than rely on explicit context.
- Core assumption: Models can effectively learn from progressively compressed representations without losing critical information.
- Evidence anchors:
  - [abstract]: "gradually internalizing symbolic knowledge and few-shot examples into the model's parameters"
  - [section 3.2]: "we gradually perform token-level symbolic knowledge compression and instance-level example pruning based on a predefined linear decay schedule"
  - [corpus]: Weak - no direct citations supporting progressive compression effectiveness
- Break condition: If compressed representations lose too much semantic information before internalization completes, or if the decay schedule is too aggressive for the model to adapt.

### Mechanism 2
- Claim: Symbolic knowledge generation from LLMs provides structured reasoning patterns and supplementary context that improves SLM's ability to generalize beyond training data.
- Mechanism: Teacher LLM generates both rationales and symbolic knowledge (learning summaries and supplementary knowledge) for each training instance, which are then progressively compressed and internalized.
- Core assumption: Structured symbolic knowledge contains generalizable reasoning patterns that can be distilled into smaller models.
- Evidence anchors:
  - [abstract]: "gradually internalizing symbolic knowledge and few-shot examples into the model's parameters"
  - [section 3.1]: "We use prompt pk which is in the Appendix D.2 to enable teacher LLM to generate learning summaries km that incorporate thinking processes and supplemental knowledge kp"
  - [corpus]: Weak - related papers focus on CoT distillation but don't specifically address symbolic knowledge generation
- Break condition: If generated symbolic knowledge is too task-specific or noisy, preventing effective generalization.

### Mechanism 3
- Claim: The linear decay schedule provides optimal balance between knowledge retention and internalization efficiency compared to other scheduling patterns.
- Mechanism: Analysis shows linear decay outperforms exponential and inverse exponential schedules in maintaining performance while reducing computational overhead.
- Core assumption: Gradual, steady reduction in knowledge examples is more effective than aggressive or slow decay patterns.
- Evidence anchors:
  - [section 4.6]: "the linear decay consistently delivers the highest performance, showcasing superior parsing efficiency and language understanding"
  - [table 2]: Direct comparison showing linear decay outperforms exp and exp^-1 patterns
  - [corpus]: Weak - no citations supporting linear decay superiority for knowledge internalization
- Break condition: If task complexity requires different scheduling patterns, or if model architecture interacts differently with decay schedules.

## Foundational Learning

- Concept: Curriculum Learning
  - Why needed here: SKIntern's progressive fine-tuning follows curriculum learning principles by gradually increasing task difficulty through knowledge compression
  - Quick check question: What is the difference between standard fine-tuning and curriculum learning in the context of model training?

- Concept: Knowledge Distillation
  - Why needed here: The core approach transfers reasoning capabilities from large teacher models to smaller student models
  - Quick check question: How does CoT distillation differ from standard knowledge distillation in terms of what knowledge is being transferred?

- Concept: Parameter-Efficient Fine-Tuning
  - Why needed here: SKIntern uses LoRA for fine-tuning, which is crucial for efficient knowledge internalization in smaller models
  - Quick check question: What are the advantages of LoRA compared to full fine-tuning when working with small language models?

## Architecture Onboarding

- Component map: Teacher LLM (GPT-3.5-turbo) -> Symbolic Knowledge Generation -> LLMLingua2 Compression -> Progressive Fine-tuning -> SLM with LoRA -> VLLM Inference

- Critical path: Knowledge Generation → Symbolic Knowledge Compression → Example Pruning → Progressive Fine-tuning → Inference

- Design tradeoffs:
  - Progressive compression vs. immediate compression: Gradual approach preserves more information but requires more training steps
  - Knowledge quality vs. quantity: Higher quality symbolic knowledge improves internalization but may require more teacher LLM queries
  - Model size vs. performance: Larger SLMs can internalize more knowledge but increase computational costs

- Failure signatures:
  - Performance degradation when schedule step T is too small (overly aggressive compression)
  - Insufficient knowledge internalization when schedule step T is too large (insufficient compression)
  - Increased inference costs if knowledge is not properly internalized

- First 3 experiments:
  1. Test different schedule patterns (linear, exponential, inverse exponential) on a small dataset to identify optimal decay pattern
  2. Evaluate knowledge retention at different compression levels to find optimal balance point
  3. Compare inference efficiency with and without progressive internalization on benchmark tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SKIntern vary when using different teacher models (e.g., GPT-4 instead of GPT-3.5-Turbo) across different reasoning tasks?
- Basis in paper: [inferred] The paper mentions that they used GPT-3.5-Turbo as the teacher model due to budget constraints, and suggests that using GPT-4 could better verify SKIntern's effectiveness.
- Why unresolved: The paper did not conduct experiments with different teacher models due to computational and financial limitations.
- What evidence would resolve it: Conducting experiments comparing SKIntern's performance using GPT-3.5-Turbo versus GPT-4 across various reasoning tasks and model sizes would provide empirical evidence.

### Open Question 2
- Question: What is the optimal schedule step T for different types of reasoning tasks (factual, mathematical, general reasoning)?
- Basis in paper: [explicit] The paper found that T=4 offered the best performance in their experiments, but this was tested across mixed tasks. They note that selecting an appropriate schedule step is crucial.
- Why unresolved: The experiments used a fixed T=4 across all task types, and the paper only briefly mentions that different schedules (exponential, inverse exponential, linear) were tested.
- What evidence would resolve it: Systematic experiments varying T for each task type individually would reveal if different optimal values exist for factual, mathematical, and general reasoning tasks.

### Open Question 3
- Question: How does SKIntern perform on tasks outside the tested domains, such as coding exercises or extended text tasks?
- Basis in paper: [explicit] The paper states that their current tests encompass factual knowledge, mathematics, and complex reasoning, but acknowledges that efficacy for different tasks requires further analysis.
- Why unresolved: The experiments were limited to factual, mathematical, and general reasoning tasks, and the paper explicitly calls for further investigation into other task types.
- What evidence would resolve it: Evaluating SKIntern on coding benchmarks and extended text generation tasks would demonstrate its generalizability to these domains.

## Limitations
- Scalability concerns for larger models or different architectures beyond tested 1B-7B parameter range
- Heavy reliance on LLM quality for symbolic knowledge generation without quantification of sensitivity
- Limited evaluation to mathematical/scientific reasoning tasks, with unclear generalizability to other reasoning types

## Confidence

**High confidence** in the core mechanism: The progressive fine-tuning approach with knowledge compression is well-defined and reproducible.

**Medium confidence** in performance claims: While benchmark results show consistent improvements, evaluation is limited to specific datasets and configurations.

**Low confidence** in computational efficiency claims: The paper demonstrates reduced inference costs but lacks complete cost analysis including initial LLM queries for knowledge generation.

## Next Checks

1. **Schedule sensitivity analysis**: Systematically test SKIntern with different decay schedules (exponential, logarithmic, adaptive) on the same benchmarks to quantify how sensitive performance is to the scheduling pattern, particularly for different task complexities.

2. **Knowledge quality ablation**: Compare model performance when using different quality levels of symbolic knowledge (generated with different LLM prompts, temperatures, or model sizes) to determine the minimum knowledge quality threshold required for effective internalization.

3. **Cross-task generalization**: Evaluate SKIntern on reasoning tasks outside the mathematical/scientific domain (such as commonsense reasoning, creative writing, or multi-modal tasks) to assess whether the progressive internalization approach transfers to fundamentally different reasoning patterns.