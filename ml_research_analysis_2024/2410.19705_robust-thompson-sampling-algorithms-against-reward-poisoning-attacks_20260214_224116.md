---
ver: rpa2
title: Robust Thompson Sampling Algorithms Against Reward Poisoning Attacks
arxiv_id: '2410.19705'
source_url: https://arxiv.org/abs/2410.19705
tags:
- algorithms
- regret
- thompson
- sampling
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes robust Thompson sampling algorithms for stochastic
  and contextual linear bandit settings under adversarial reward poisoning attacks.
  The key idea is to compute optimistic pseudo-posteriors that are less susceptible
  to manipulation by the attacker.
---

# Robust Thompson Sampling Algorithms Against Reward Poisoning Attacks

## Quick Facts
- arXiv ID: 2410.19705
- Source URL: https://arxiv.org/abs/2410.19705
- Authors: Yinglun Xu; Zhiwei Wang; Gagandeep Singh
- Reference count: 40
- Primary result: Near-optimal regret bounds for robust Thompson sampling under reward poisoning attacks

## Executive Summary
This paper proposes robust Thompson sampling algorithms for stochastic and contextual linear bandit settings under adversarial reward poisoning attacks. The key innovation is computing optimistic pseudo-posteriors that are less susceptible to manipulation by the attacker. For stochastic bandits, this involves adding a robustness parameter to the posterior mean, while for contextual bandits, a weighted ridge regression estimator reduces attack influence. The algorithms achieve near-optimal regret bounds under attacks and demonstrate significantly improved robustness compared to standard Thompson sampling and UCB methods across various attack scenarios and corruption levels.

## Method Summary
The paper introduces two main algorithms: a robust Thompson sampling algorithm for stochastic bandits that computes optimistic posteriors with a compensation term C/(k_i(t)+1) to handle known corruption budgets, and a robust Thompson sampling algorithm for contextual linear bandits that uses weighted ridge regression with weights w_k = min(1, γ/||x_i(t)||_{B(t)^{-1}}) to limit attack influence. When the corruption budget is unknown, the algorithms employ an estimation procedure to determine C or γ. The methods maintain the exploration benefits of Thompson sampling while providing theoretical guarantees against reward poisoning attacks.

## Key Results
- Near-optimal regret bounds under reward poisoning attacks for both stochastic and contextual bandit settings
- Significant improvement in robustness compared to standard Thompson sampling and UCB methods
- Weighted ridge regression estimator effectively reduces attack influence on posterior parameter estimation
- Algorithms maintain exploration benefits of Thompson sampling while providing protection against manipulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimistic posteriors prevent underestimation of arm performance under reward poisoning.
- Mechanism: The algorithm computes pseudo-posteriors that include a robustness parameter (C/(k_i(t)+1)) to compensate for the maximum possible bias from the attacker, ensuring the posterior mean never falls below the true mean plus a safety margin.
- Core assumption: The attacker's total corruption budget C is either known or can be estimated.
- Evidence anchors:
  - [abstract]: "We solve this problem by computing pseudo-posteriors that are less likely to be manipulated by the attack."
  - [section]: "Instead of computing the posteriors as if the data are uncorrupted, the algorithm computes the optimistic posteriors with respect to corruption for each of the arms."
  - [corpus]: Weak - no direct evidence about optimistic posterior mechanisms in related work.

### Mechanism 2
- Claim: Weighted ridge regression estimator reduces attack influence on posterior parameter estimation.
- Mechanism: By assigning less weight to data points with large contexts (using w_t = min(1, γ/||x_i(t)||_{B(t)^{-1}})), the estimator limits the attacker's ability to manipulate the posterior through targeted corruption of high-influence data points.
- Core assumption: The attacker cannot arbitrarily scale the context vectors to bypass the weighting scheme.
- Evidence anchors:
  - [abstract]: "In the linear contextual MAB setting, we show that with such an estimator, the influence of the attack on the estimation of the posteriors is limited."
  - [section]: "The key is that it assigns less weight to the data with a 'large' context so that the attacker can apply less influence on the estimator by corrupting such data."
  - [corpus]: Weak - no direct evidence about weighted ridge regression in related poisoning work.

### Mechanism 3
- Claim: Thompson sampling's stochastic exploration maintains sufficient exploration even under attack.
- Mechanism: The random sampling from posterior distributions ensures that even if the attacker manipulates the posterior means, the variance in the posterior still allows for exploration of suboptimal arms with non-zero probability, preventing premature convergence to suboptimal policies.
- Core assumption: The posterior variance remains sufficiently large to maintain exploration.
- Evidence anchors:
  - [abstract]: "Thompson sampling has several advantages: • Utilizing prior information: By design, Thompson sampling algorithms utilize and benefit from the prior information about the arms."
  - [section]: "Thompson sampling is a random exploration strategy, which could be more resilient under some bandit settings."
  - [corpus]: Weak - no direct evidence about Thompson sampling's resilience to poisoning in related work.

## Foundational Learning

- Concept: Multi-armed bandit problem formulation
  - Why needed here: The entire paper builds on the MAB framework where an agent sequentially selects arms to maximize cumulative reward while learning arm values.
  - Quick check question: What is the difference between stochastic and contextual bandit settings?

- Concept: Posterior distributions in Bayesian inference
  - Why needed here: Thompson sampling relies on maintaining and updating posterior distributions over arm rewards, which becomes the target of attack.
  - Quick check question: How does a Gaussian posterior update when new reward data is observed?

- Concept: Adversarial machine learning threat models
  - Why needed here: The paper addresses reward poisoning attacks, requiring understanding of how attackers can manipulate learning algorithms through data corruption.
  - Quick check question: What is the difference between strong and weak attack models in adversarial bandit settings?

## Architecture Onboarding

- Component map:
  Optimistic posterior computation -> Thompson sampling sampling -> Corruption budget estimation -> Weighted ridge regression estimator

- Critical path:
  1. Receive arm/context information
  2. Compute optimistic posterior or weighted estimate
  3. Sample from posterior distribution
  4. Select arm based on sampled values
  5. Observe corrupted reward
  6. Update posterior/estimator with corrupted data
  7. Repeat

- Design tradeoffs:
  - Conservative vs. aggressive robustness parameter selection
  - Computational cost of weighted ridge regression vs. standard regression
  - Exploration-exploitation balance under attack conditions
  - Known vs. unknown corruption budget handling

- Failure signatures:
  - Rapid increase in cumulative regret during training
  - Convergence to suboptimal arm despite sufficient exploration
  - Posterior variance collapsing to near-zero
  - Systematic underestimation of optimal arm's true reward

- First 3 experiments:
  1. Run standard Thompson sampling vs. robust version on stochastic bandit with known corruption level C=25, measure regret growth over time.
  2. Test weighted ridge regression estimator vs. standard estimator in contextual bandit with Garcelon's attack at C=200, compare posterior accuracy.
  3. Evaluate unknown corruption budget case by varying C from 0 to T, measure regret threshold behavior and compare to theoretical predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the robustness parameter C be automatically tuned in real-time without prior knowledge of the attack budget?
- Basis in paper: [inferred] The paper discusses cases where C is known vs unknown, but does not explore adaptive tuning mechanisms.
- Why unresolved: The paper focuses on theoretical bounds with fixed C values and does not investigate online adaptation strategies.
- What evidence would resolve it: Experimental results showing regret performance with dynamically adjusted C values under varying attack strengths.

### Open Question 2
- Question: How do the proposed robust algorithms perform in multi-agent or cooperative bandit settings under reward poisoning attacks?
- Basis in paper: [inferred] The paper focuses on single-agent settings, though such extensions could be valuable for practical applications.
- Why unresolved: The theoretical framework and experiments are designed for single-agent scenarios without considering interactions between multiple learners.
- What evidence would resolve it: Empirical studies comparing single-agent vs multi-agent performance under coordinated or independent attacks.

### Open Question 3
- Question: Can the weighted ridge regression estimator be extended to non-linear contextual bandit settings?
- Basis in paper: [explicit] The paper uses weighted ridge regression specifically for linear contextual bandits and does not explore non-linear extensions.
- Why unresolved: The theoretical guarantees rely on linear payoff functions, and the paper does not discuss kernel methods or neural network-based extensions.
- What evidence would resolve it: Empirical results showing regret bounds and robustness in non-linear settings using similar weighted estimation techniques.

## Limitations
- Reliance on accurate corruption budget estimation when C is unknown
- Computational overhead introduced by weighted ridge regression estimator
- Theoretical guarantees assume specific attack models that may not capture adaptive attackers

## Confidence

**High Confidence**: The mechanism of using optimistic posteriors to prevent underestimation of arm performance under known corruption budgets. This follows directly from the mathematical formulation and is supported by both theoretical analysis and empirical results showing consistent performance improvements across multiple attack scenarios.

**Medium Confidence**: The effectiveness of the weighted ridge regression estimator in reducing attack influence on posterior parameter estimation. While the theoretical framework is sound, the empirical validation is limited to specific attack types, and the performance against more sophisticated adaptive attacks remains unclear.

**Low Confidence**: The claim that Thompson sampling's stochastic exploration maintains sufficient exploration even under attack. This relies on the assumption that posterior variance remains sufficiently large, but the paper provides limited empirical evidence about how variance dynamics evolve under different attack strategies and corruption levels.

## Next Checks

1. **Adaptive Attacker Test**: Implement an adaptive attacker that modifies its strategy based on the algorithm's observed behavior, and test whether the robust algorithms maintain their performance guarantees under this more sophisticated threat model.

2. **Unknown C Estimation Accuracy**: Conduct extensive experiments to measure the accuracy of the corruption budget estimation procedure across different bandit environments and attack types, quantifying the impact of estimation errors on regret performance.

3. **Computational Efficiency Analysis**: Perform a detailed analysis of the computational overhead introduced by the weighted ridge regression estimator, measuring both wall-clock time and memory requirements compared to standard Thompson sampling baselines across varying problem sizes.