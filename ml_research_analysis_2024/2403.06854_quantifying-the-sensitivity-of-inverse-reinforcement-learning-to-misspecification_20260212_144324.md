---
ver: rpa2
title: Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification
arxiv_id: '2403.06854'
source_url: https://arxiv.org/abs/2403.06854
tags:
- reward
- function
- such
- then
- misspecification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper quantifies how robust inverse reinforcement learning
  (IRL) is to misspecification of the behavioural model. It introduces necessary and
  sufficient conditions that completely characterise the types of misspecification
  various behavioural models will tolerate.
---

# Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification

## Quick Facts
- arXiv ID: 2403.06854
- Source URL: https://arxiv.org/abs/2403.06854
- Reference count: 40
- Inverse reinforcement learning (IRL) is highly sensitive to misspecification of behavioral models, with even small perturbations leading to large errors in recovered reward functions.

## Executive Summary
This paper provides a rigorous theoretical analysis of how sensitive inverse reinforcement learning is to misspecification of the behavioral model used to infer demonstrator preferences. The authors introduce necessary and sufficient conditions that completely characterize when different behavioral models can tolerate misspecification without incurring large errors. Their results show that widely-used behavioral models like Boltzmann-rational and maximum causal entropy are not robust to arbitrarily small perturbations in the observed policy or to misspecification of discount factors and environment dynamics. The analysis uses STARC-metrics to quantify reward function differences and provides theoretical guarantees for these metrics.

## Method Summary
The paper takes a theoretical approach, providing mathematical proofs of robustness conditions for inverse reinforcement learning under various forms of misspecification. The method involves characterizing when one behavioral model can approximate another within a given error threshold using STARC-metrics, which measure the distance between reward functions based on their ability to distinguish between policies. The analysis covers three main types of misspecification: arbitrary transformations of the behavioral model, small perturbations in the observed policy, and misspecification of discount factors or environment dynamics. The proofs establish necessary and sufficient conditions for robustness under each scenario.

## Key Results
- A wide class of behavioral models including optimal policy, Boltzmann-rational, and maximum causal entropy models are not robust to arbitrarily small perturbations in observed policy
- These models are also not robust to arbitrarily small misspecification of discount factors γ or transition dynamics τ
- Continuous behavioral models map nearby reward functions (in ℓ²) to nearby policies, but ℓ²-close rewards can have large STARC distances, leading to policy closeness despite reward divergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IRL using Boltzmann-rational or MCE models can tolerate misspecification only if the misspecified model is a small transformation of the correct model.
- Mechanism: The paper proves that f is ϵ-robust to misspecification with g iff g = f ∘ t for some t with bounded STARC-distance. This bounds allowable deviations.
- Core assumption: Behavioral models f and g must be functions; the true reward lies in the restricted space ˆR; STARC-metric correctly captures policy-order equivalence.
- Evidence anchors:
  - [abstract] "provides necessary and sufficient conditions that completely characterise how the observed data may differ from the assumed behavioural model without incurring an error above a given threshold."
  - [section 3.1] "Theorem 1... f is ϵ-robust to misspecification with g (as defined by dR) if and only if g = f ◦ t for some t : ˆR → ˆR such that dR(R, t(R)) ≤ ϵ for all R ∈ ˆR."
  - [corpus] Weak. Neighbor papers discuss partial identifiability but not this precise characterization.
- Break condition: If the true behavioral model is not a small transformation of the assumed model under STARC, or if the reward space is unrestricted, robustness fails.

### Mechanism 2
- Claim: Continuous behavioral models are not robust to small perturbations in the observed policy under any metric close to ℓ².
- Mechanism: Continuous models map nearby reward functions (in ℓ²) to nearby policies. But ℓ²-close rewards can have large STARC distances, leading to policy closeness despite reward divergence.
- Core assumption: The policy metric dΠ satisfies a continuity-like property; the reward metric is dSTARC; behavioral models are continuous.
- Evidence anchors:
  - [abstract] "very mild misspecification can lead to very large errors in the inferred reward function."
  - [section 3.2] "Theorem 3... f is not ϵ/δ-separating for any ϵ < 1 or δ > 0."
  - [corpus] Weak. No direct neighbor evidence; this is an original theoretical finding.
- Break condition: If the behavioral model is discontinuous or the policy metric does not satisfy the continuity assumption, perturbation robustness may hold.

### Mechanism 3
- Claim: Behavioral models invariant to potential shaping or S′-redistribution are not robust to misspecification of discount factor γ or transition dynamics τ.
- Mechanism: For γ misspecification, potential shaping preserves policy under one γ but not another, enabling large reward errors. For τ misspecification, S′-redistribution preserves policy under one τ but not another, enabling large errors.
- Core assumption: Behavioral model is invariant to potential shaping/S′-redistribution; transition function is non-trivial; discount factors differ; γ or τ misspecification is small.
- Evidence anchors:
  - [abstract] "shows that a wide class of behavioural models... are not robust to arbitrarily small perturbations of the observed policy or to misspecification of the discount factor γ or environment dynamics τ."
  - [section 3.3] "Theorem 4... f is not ϵ-robust to misspecification with fγ2 under dSTARC for any non-trivial τ, any γ3, and any ϵ < 0.5."
  - [corpus] Weak. Neighbor papers discuss misspecification but not these specific parameter sensitivities.
- Break condition: If the behavioral model is not invariant to potential shaping/S′-redistribution, or if γ and τ are correctly specified, robustness may hold.

## Foundational Learning

- Concept: Markov Decision Process (MDP) tuple and policy evaluation.
  - Why needed here: IRL assumes the demonstrator follows some policy in an MDP; understanding MDP components is essential to define reward functions and behavioral models.
  - Quick check question: What are the five components of an MDP tuple and how does a policy map states to actions?

- Concept: Potential shaping and S′-redistribution invariance.
  - Why needed here: Many IRL behavioral models (optimal, Boltzmann, MCE) are invariant to these transformations, which is central to the robustness analysis.
  - Quick check question: Why do potential shaping and S′-redistribution leave the optimal policy unchanged?

- Concept: STARC-metric and its soundness/completeness.
  - Why needed here: STARC is the reward distance metric used to quantify misspecification; its properties determine robustness bounds.
  - Quick check question: What does it mean for dSTARC to be sound and complete, and why is this desirable?

## Architecture Onboarding

- Component map: Reward space ˆR -> Behavioral model f -> Policy space Π -> Demonstrator policy π -> Recovered reward Rh
- Critical path: 1) Define MDP and reward space ˆR. 2) Choose behavioral model f. 3) Observe policy π = g(R⋆) from demonstrator. 4) Solve for Rh satisfying f(Rh) = π. 5) Evaluate robustness via STARC-distance to true R⋆.
- Design tradeoffs: Using dSTARC provides tight regret bounds but requires computing potential shaping equivalence classes. Simpler metrics like ℓ² are easier but less meaningful. Restricting ˆR can improve identifiability but risks excluding true R⋆.
- Failure signatures: If f(R1) = f(R2) but dSTARC(R1,R2) > ϵ, robustness fails. If f is continuous but policy metric is too coarse, perturbation robustness fails. If model is invariant to shaping/redistribution, parameter misspecification causes large errors.
- First 3 experiments:
  1. Implement a simple gridworld MDP, define R⋆, generate π via optimal policy, apply f = optimal policy model, recover Rh, compute dSTARC(Rh,R⋆).
  2. Perturb π slightly, repeat inference, measure dSTARC; verify failure of perturbation robustness.
  3. Change discount factor in f vs g, repeat; measure dSTARC to confirm parameter misspecification sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we find a pseudometric on the space of reward functions that induces a lower bound on worst-case regret while also being invariant to S′-redistribution?
- Basis in paper: [explicit] The paper discusses the limitations of EPIC pseudometric, which is sensitive to S′-redistribution, and proposes STARC-metrics as an alternative. However, STARC-metrics are not explicitly tested against the requirement of inducing a lower bound on worst-case regret while being invariant to S′-redistribution.
- Why unresolved: The paper does not provide a comprehensive comparison of different pseudometrics against these two criteria. It focuses on STARC-metrics as a solution but does not explore whether other metrics might satisfy both requirements.
- What evidence would resolve it: A rigorous mathematical proof or empirical demonstration showing that a specific pseudometric (different from STARC or EPIC) satisfies both conditions would resolve this question.

### Open Question 2
- Question: How can we modify Definition 1 of misspecification robustness to make it more probabilistic, allowing for a certain degree of uncertainty in the true reward function or the observed policy?
- Basis in paper: [inferred] The paper acknowledges that Definition 1 is a worst-case analysis and suggests that a more probabilistic approach could be beneficial in certain cases, particularly when there is prior information about the distribution of the true reward function.
- Why unresolved: The paper does not provide a concrete alternative definition of misspecification robustness that incorporates probabilistic elements. It only hints at the possibility of extending the framework in this direction.
- What evidence would resolve it: A formal definition of a probabilistic variant of misspecification robustness, along with theoretical results or empirical examples demonstrating its advantages over the worst-case definition, would resolve this question.

### Open Question 3
- Question: Can we identify specific classes of reward functions for which IRL algorithms are more robust to misspecification, and develop algorithms that leverage this knowledge to improve performance?
- Basis in paper: [inferred] The paper mentions that restricting the space of reward functions (ˆR) might help mitigate some of the negative results, but it does not provide a systematic approach for identifying such classes or developing algorithms that exploit them.
- Why unresolved: The paper does not explore the potential benefits of restricting the reward function space in detail. It focuses on general results that apply to any choice of ˆR, but does not investigate how specific restrictions might improve robustness.
- What evidence would resolve it: A thorough analysis of different reward function classes, along with experimental results showing that IRL algorithms perform better on restricted spaces, would resolve this question. Additionally, developing new algorithms specifically designed for these classes would provide further evidence.

## Limitations
- The analysis assumes perfect knowledge of the behavioral model class and restricted reward space
- The STARC-metric's practical computational complexity is not addressed
- Real-world deviations from idealized assumptions (e.g., stochastic dynamics, approximate optimality) are not quantified

## Confidence
- High confidence in the mathematical proofs and theoretical characterizations (Theorems 1-5)
- Medium confidence in the practical implications, as real-world factors could modify the sensitivity
- Low confidence in the exact error bounds without empirical validation

## Next Checks
1. Implement STARC-metric computation and verify its soundness/completeness properties on simple MDPs
2. Test perturbation robustness empirically: recover rewards from slightly perturbed policies and measure error growth
3. Validate parameter sensitivity: systematically vary γ and τ in recovery experiments to quantify error amplification