---
ver: rpa2
title: 'Telco-DPR: A Hybrid Dataset for Evaluating Retrieval Models of 3GPP Technical
  Specifications'
arxiv_id: '2410.19790'
source_url: https://arxiv.org/abs/2410.19790
tags:
- passages
- system
- questions
- dataset
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of developing a Question-Answering
  (QA) system for the telecom domain using 3GPP technical documents, which are highly
  technical and often include structured elements like tables. The proposed method
  leverages a Retriever-Augmented Generation (RAG) framework with a hybrid dataset,
  Telco-DPR, combining text and tables.
---

# Telco-DPR: A Hybrid Dataset for Evaluating Retrieval Models of 3GPP Technical Specifications

## Quick Facts
- **arXiv ID**: 2410.19790
- **Source URL**: https://arxiv.org/abs/2410.19790
- **Reference count**: 40
- **Primary result**: DHR model fine-tuned at document and passage levels achieves 86.2% Top-10 accuracy and 0.68 MRR on Telco-DPR dataset

## Executive Summary
This paper introduces Telco-DPR, a hybrid dataset for evaluating retrieval models on 3GPP technical specifications. The dataset combines text passages and tables from 57 3GPP documents, totaling 1,741 question-answer pairs. The authors propose a Retriever-Augmented Generation (RAG) framework using Dense Hierarchical Retrieval (DHR) that outperforms traditional methods, achieving significant improvements in retrieval accuracy and QA system performance. The work addresses the challenge of retrieving and answering questions from highly technical telecom documents where information is distributed across both text and structured table elements.

## Method Summary
The method extracts 3GPP technical specifications using automated tools to tag text elements and create 512-token passages while separately indexing tables with captions and LLM-generated summaries. Synthetic question-answer pairs are generated using GPT-3.5 with carefully crafted prompts. The DHR model implements a two-stage hierarchical approach: first retrieving relevant documents using metadata (title, abstract, headings), then retrieving passages within those documents. This is compared against BM25, DPR, and DPR-FT baselines. The best retriever is integrated with GPT-4 in a RAG framework to generate final answers.

## Key Results
- DHR achieves 86.2% Top-10 accuracy compared to 78.5% for DPR-FT and 70.3% for BM25
- DHR achieves MRR of 0.68, outperforming other methods significantly
- The QA system using DHR + GPT-4 achieves 14% improvement in answer accuracy over previous benchmarks
- Table integration improves performance on table-dependent questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dense Hierarchical Retrieval (DHR) outperforms standard dense retrieval by combining document-level and passage-level semantic matching.
- Mechanism: DHR uses two stages: first, a document-level retriever matches the query against document metadata (title, abstract, section headings), filtering to a small set of candidate documents; second, a passage-level retriever searches within those candidates for semantically relevant passages.
- Core assumption: The 3GPP documents have a well-defined hierarchical structure (documents → sections → passages) that can be leveraged for more precise retrieval.
- Evidence anchors:
  - [abstract] "DHR, a retriever model utilising hierarchical passage selection through fine-tuning at both the document and passage levels, outperforms traditional methods in retrieving relevant technical information"
  - [section] "DHR utilizes a two-stage hierarchical approach: in the first stage, Dense Document-level Retrieval (DHR-D) assesses the similarity between the question and the document's metadata. In the second stage, Dense Passage-level Retrieval (DHR-P) focuses the search on the passages within the most relevant documents."
  - [corpus] No direct corpus evidence of hierarchical effectiveness, but the corpus contains well-structured 3GPP documents with sections and subsections.
- Break condition: If the documents lack consistent metadata (titles, abstracts) or if passages are too short/long relative to token limits, the hierarchical filtering may degrade rather than improve retrieval.

### Mechanism 2
- Claim: Incorporating tables as hybrid data improves answer accuracy because many questions are table-dependent.
- Mechanism: Tables are indexed separately with both caption and LLM-generated summary, then retrieved alongside text passages. The reader model must understand table structure to extract correct answers.
- Core assumption: In 3GPP TSs, tables contain critical information not fully captured in surrounding text.
- Evidence anchors:
  - [abstract] "the dataset includes a set of synthetic question/answer pairs designed to evaluate the retrieval performance of QA systems on this type of data" and "Telco-DPR, which consists of a curated 3GPP corpus in a hybrid format, combining text and tables"
  - [section] "However, they face difficulties when applied to domain-specific contexts such as MWNs. LLMs, typically trained on general-purpose corpora, struggle with the specialised language, technical jargon, and complex protocols that define technical standards like those established by the 3GPP"
  - [corpus] Corpus statistics show 2,316 tables across 57 documents, averaging 40.63 tables per document, indicating heavy reliance on tables.
- Break condition: If the LLM summarizer fails to capture table semantics accurately, or if the reader cannot parse table structure, hybrid data may introduce noise rather than signal.

### Mechanism 3
- Claim: Fine-tuning dense retrieval models on domain-specific data improves retrieval relevance compared to off-the-shelf embeddings.
- Mechanism: SBERT embeddings are fine-tuned using "Multiple Negatives Ranking Loss" on the Telco-DPR QA pairs, learning to align questions with correct passages more precisely.
- Core assumption: The semantic space of 3GPP technical language differs significantly from general text, requiring adaptation.
- Evidence anchors:
  - [section] "SBERT has proven to be an effective alternative to LLM-based word embeddings, offering efficiency when computational resources are limited" and "A key advantage of dense retrieval methods is that the embedding models, based on neural networks, can be fine-tuned to domain-specific data"
  - [corpus] The Telco-DPR dataset contains 1,741 question-answer pairs from 3GPP TSs, providing a substantial domain-specific fine-tuning corpus.
- Break condition: If the fine-tuning dataset is too small or biased, the model may overfit to narrow patterns and fail to generalize to unseen questions.

## Foundational Learning

- Concept: Dense Passage Retrieval (DPR)
  - Why needed here: DPR uses neural embeddings to capture semantic similarity, overcoming the keyword-matching limitations of sparse methods like BM25 in technical domains.
  - Quick check question: What is the main advantage of DPR over BM25 in the context of 3GPP TS retrieval?
- Concept: Hierarchical Retrieval
  - Why needed here: 3GPP documents have clear hierarchical structure; exploiting it reduces the search space and improves precision.
  - Quick check question: How does DHR use document metadata in its first retrieval stage?
- Concept: Hybrid Data (Text + Tables)
  - Why needed here: Tables in 3GPP TSs often contain essential information; ignoring them leads to missed answers.
  - Quick check question: What two components are used to index tables in the Telco-DPR dataset?

## Architecture Onboarding

- Component map: Corpus Builder -> Retriever (DHR) -> Reader (GPT-4) -> Answer
- Critical path: Corpus → Retriever (DHR) → Reader (GPT-4) → Answer
- Design tradeoffs:
  - Table summarization vs. raw table storage: summarization reduces noise but may lose detail.
  - Token limit balancing: too short passages lose context; too long cause truncation.
  - Retriever choice: DPR-FT improves accuracy but adds fine-tuning cost; DHR gives best accuracy but needs structured metadata.
- Failure signatures:
  - Low Top-10 accuracy indicates retriever is not matching queries well.
  - High answer accuracy but low passage accuracy suggests LLM is inferring from memory rather than context.
  - Tables not retrieved when answers are table-based.
- First 3 experiments:
  1. Compare BM25 vs. DPR vs. DPR-FT on the test set, measuring Top-10 accuracy.
  2. Run DHR with only document-level retrieval vs. both levels, to quantify hierarchy benefit.
  3. Evaluate reader accuracy with and without table passages in the context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DHR compare to other state-of-the-art retrieval methods on domain-specific datasets outside of the telecom industry?
- Basis in paper: [inferred] The paper demonstrates that DHR outperforms traditional methods in retrieving relevant technical information for the telecom domain, suggesting its potential applicability to other domain-specific contexts.
- Why unresolved: The paper only evaluates DHR on the Telco-DPR dataset, which is specific to the telecom domain. There is no comparison with other state-of-the-art retrieval methods on datasets from different domains.
- What evidence would resolve it: Conducting experiments to compare the performance of DHR with other state-of-the-art retrieval methods on domain-specific datasets from various industries, such as healthcare, finance, or legal.

### Open Question 2
- Question: What are the limitations of using synthetic question-answer pairs generated by LLMs for evaluating retrieval models, and how can these limitations be addressed?
- Basis in paper: [explicit] The paper mentions that the MCQ dataset used for evaluation may have limitations in accurately assessing the true quality of RAG systems and suggests that open-ended questions could be more effective.
- Why unresolved: The paper does not provide a detailed analysis of the limitations of using synthetic question-answer pairs or propose specific methods to address these limitations.
- What evidence would resolve it: Conducting a comprehensive study to identify the limitations of synthetic question-answer pairs and developing strategies to mitigate these limitations, such as incorporating human-generated questions or using a combination of synthetic and real-world data.

### Open Question 3
- Question: How can the QA system be further improved to handle open-ended questions and integrate visual elements from technical documents?
- Basis in paper: [explicit] The paper mentions that future work will focus on developing QA systems capable of handling open-ended questions and integrating a multi-modal approach to include visual elements found in technical documents.
- Why unresolved: The paper does not provide any details on how these improvements can be achieved or the specific challenges involved in handling open-ended questions and visual elements.
- What evidence would resolve it: Conducting research to develop techniques for handling open-ended questions, such as question reformulation or answer generation, and exploring methods to extract and process visual information from technical documents, such as diagrams or charts.

## Limitations

- The synthetic question-answer pairs may not fully capture the complexity of real-world questions and could introduce bias
- The model's performance on open-ended questions and integration of visual elements remains unexplored
- The generalizability of DHR to other technical domains beyond telecommunications is not evaluated

## Confidence

- **High**: DHR model effectiveness in improving retrieval accuracy on Telco-DPR dataset
- **Medium**: Impact of fine-tuning on retrieval performance in domain-specific context
- **Low**: Generalizability of results to other technical domains and real-world scenarios

## Next Checks

1. Evaluate the reader model's performance on questions that require understanding table structure
2. Test the DHR model on a dataset with a different document structure to assess its adaptability
3. Compare the synthetic question-answer pairs with human-annotated pairs to validate the quality of the generated data