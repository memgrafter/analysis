---
ver: rpa2
title: An adaptive approach to Bayesian Optimization with switching costs
arxiv_id: '2405.08973'
source_url: https://arxiv.org/abs/2405.08973
tags:
- cost
- switching
- setup
- optimization
- costly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends Bayesian optimization (BO) to a resource-constrained
  setting where changes to certain design variables incur switching costs. The problem
  is formulated as an expensive black-box optimization with a mixed-cost domain, where
  some dimensions are "cheap" (no switching cost) and others are "costly" (incur switching
  cost when changed).
---

# An adaptive approach to Bayesian Optimization with switching costs

## Quick Facts
- arXiv ID: 2405.08973
- Source URL: https://arxiv.org/abs/2405.08973
- Reference count: 32
- Key outcome: The cost-aware Expected Improvement per Unit Cost (EIPU) algorithm performs comparably to tuned algorithms across all settings, with performance improving as switching costs increase

## Executive Summary
This paper extends Bayesian optimization (BO) to a resource-constrained setting where changes to certain design variables incur switching costs. The problem is formulated as an expensive black-box optimization with a mixed-cost domain, where some dimensions are "cheap" (no switching cost) and others are "costly" (incur switching cost when changed). The authors adapt two batch-constrained BO algorithms to this sequential setting and propose two new methods: a cost-aware Expected Improvement per Unit Cost (EIPU) algorithm and a probabilistic re-use BO (pReuseBO). Experiments on seven scalable test functions across different dimensionalities and switching cost settings show that the cost-aware EIPU algorithm performs comparably to tuned algorithms in all settings, with performance improving as switching costs increase.

## Method Summary
The paper proposes extending Bayesian Optimization to handle switching costs in sequential settings. Four algorithms are developed: pReuseBO (probability-based reuse), Periodic Switching BO Nested (PSBO Nested), Periodic Switching BO (PSBO), and Expected Improvement per Unit Cost (EIPU). All methods use Gaussian Processes with Matérn 5/2 kernel and Automatic Relevance Determination as surrogates. The EIPU algorithm introduces a cost-aware acquisition function that discounts expected improvement by unit cost using a cooling strategy, while pReuseBO probabilistically reuses expensive dimensions. The methods are evaluated on seven scalable test functions across varying dimensionalities and switching costs, with performance measured using the GAP metric.

## Key Results
- The cost-aware EIPU algorithm performs comparably to tuned algorithms across all switching cost settings
- EIPU performance improves with increasing switching costs, showing robustness to varying landscape features
- pReuseBO requires tuning of probability parameter p, with optimal values varying based on problem dimensionality and costly dimensionality
- PSBO Nested achieves performance close to pReuseBO without requiring tuning, but may plateau after initial progress

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The EIPU algorithm achieves cost-aware optimization by evaluating two candidate points per iteration - one with switching cost and one without - then selecting based on discounted expected improvement.
- Mechanism: For each iteration, the algorithm performs two acquisition optimizations: one assuming a setup switch (costly) and one assuming no switch (cheap). The expected improvement (EI) of each candidate is discounted by its respective cost using a cost-cooling strategy (EI-cool(x) = EI(x)/(c(x)^γ)), where γ = (B - Bt)/B. The point with higher discounted EI is chosen.
- Core assumption: The cost function is discontinuous (switching vs non-switching) and can be modeled as a binary decision at each step.
- Evidence anchors:
  - [abstract] "We propose two new algorithms: one cost-aware and one cost-ignorant" and "cost-aware Expected Improvement per Unit Cost (EIPU)"
  - [section] "We propose an adaptation of the EI per unit cost (EIPU) to the pathological case where the cost function is discontinuous"
  - [corpus] Weak evidence - no direct mention of EIPU or similar switching cost BO methods
- Break condition: If the switching cost is uniform (cswitch = 1), the algorithm degenerates to standard BO since cost-cooling becomes irrelevant.

### Mechanism 2
- Claim: pReuseBO achieves robustness by probabilistically reusing expensive dimensions, balancing exploration and exploitation based on switching cost.
- Mechanism: At each iteration, the algorithm keeps expensive decision variables fixed with probability p, effectively creating a trade-off between cheap evaluations (no switching cost) and costly evaluations (with switching cost). Higher p values lead to more frequent reuse and fewer switches.
- Core assumption: The optimal probability p* varies with problem dimensionality and switching cost, and can be tuned to achieve near-optimal performance.
- Evidence anchors:
  - [abstract] "We propose two new methods — one cost-aware and one cost-ignorant" with pReuseBO being the cost-ignorant method
  - [section] "This algorithm keeps the same expensive decision variables independently with probability p ∈ [0, 1] every time step"
  - [corpus] No direct mention of probabilistic reuse BO, though related to bandit problems with switching costs
- Break condition: If p is set too high in low switching cost scenarios, the algorithm will perform poorly by not exploring the costly dimensions enough.

### Mechanism 3
- Claim: Periodic Switching BO Nested achieves cost-efficiency by fixing expensive dimensions for k evaluations before switching, creating a batch-like behavior in sequential setting.
- Mechanism: The algorithm maintains the same setup for k-1 consecutive evaluations, then switches to a new setup and repeats. This creates a periodic switching pattern that balances the trade-off between evaluation cost and setup switching cost.
- Core assumption: The optimal periodicity k can be determined and is inversely related to the optimal probability p (k = 1/(1-p)).
- Evidence anchors:
  - [section] "Another possible approach is to maintain the value of the costly dimensions for k evaluations, then change them (and pay the switch cost) and perform another k evaluations only changing the cheap dimensions"
  - [section] "k is a simpler planning strategy to the above p representation, where the setup is changed after every k evaluations"
  - [corpus] No direct mention of periodic switching in the context of BO with switching costs
- Break condition: If k is set too high relative to switching cost, the algorithm will explore insufficiently; if too low, it will incur excessive switching costs.

## Foundational Learning

- Concept: Gaussian Process (GP) regression with Matérn 5/2 kernel and Automatic Relevance Determination (ARD)
  - Why needed here: The paper uses GP surrogates for the objective function in all Bayesian optimization algorithms
  - Quick check question: What is the main advantage of using ARD in the GP kernel for this problem?

- Concept: Expected Improvement (EI) acquisition function
  - Why needed here: EI is the acquisition function used across all algorithms, including the cost-aware EIPU variant
  - Quick check question: How does the cost-cooling strategy modify the standard EI calculation in the EIPU algorithm?

- Concept: Switching cost formulation and cost-aware optimization
  - Why needed here: The entire problem formulation revolves around optimizing with different costs for switching vs non-switching setups
  - Quick check question: What is the mathematical expression for the cost function c(xt, xt-1) and what does cswitch represent?

## Architecture Onboarding

- Component map: GP surrogate model -> Acquisition function optimizer (L-BFGS-B) -> Cost function evaluation -> Switching logic -> Next evaluation point
- Critical path: For each iteration: 1) Train/update GP model with current data, 2) Compute candidate points (2 for EIPU, 1 for others), 3) Apply cost discounting if applicable, 4) Select next point based on acquisition value, 5) Evaluate function and update dataset.
- Design tradeoffs: EIPU is hyperparameter-free but requires two optimization runs per iteration; pReuseBO is simple but requires tuning p; Periodic Switching requires tuning k but has simpler per-iteration logic. The tradeoff is between computational cost per iteration and tuning complexity.
- Failure signatures: If GAP plateaus early, it may indicate insufficient exploration of costly dimensions; if performance degrades with increasing switching cost, the algorithm may be switching too frequently; if performance doesn't improve with more iterations, the GP model may not be capturing the function structure well.
- First 3 experiments:
  1. Run EIPU on Ackley function (2D, cswitch=2) to verify basic functionality and compare to standard BO
  2. Run pReuseBO with varying p values (0.1, 0.5, 0.9) on Griewank function (2D, cswitch=8) to observe switching behavior
  3. Run Periodic Switching BO Nested with k=2, k=5, k=10 on Rosenbrock function (2D, cswitch=16) to find optimal periodicity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal p* value range for different problem dimensionalities and costly dimensionalities across varying switching costs?
- Basis in paper: [explicit] The paper identifies that optimal p* decreases with increasing costly dimensionality and increases with increasing switching cost, but does not provide a general formula or comprehensive mapping
- Why unresolved: The paper shows these relationships exist but only provides specific examples rather than a generalizable relationship
- What evidence would resolve it: Empirical data showing p* values across a wide range of dimensionalities, costly dimensionalities, and switching costs, potentially leading to a mathematical relationship

### Open Question 2
- Question: How does the performance of cost-aware sequential BO compare to batch-constrained BO in scenarios where both could be applied?
- Basis in paper: [inferred] The paper extends batch-constrained BO to sequential settings but doesn't directly compare the two approaches
- Why unresolved: The paper focuses on sequential settings without benchmarking against batch-constrained methods
- What evidence would resolve it: Head-to-head comparison of sequential vs. batch-constrained BO algorithms on the same problems

### Open Question 3
- Question: What lookahead-based heuristics would improve the cost-value trade-off in switching costs scenarios?
- Basis in paper: [explicit] The conclusion suggests analyzing performance lookahead-based heuristics as a potential extension
- Why unresolved: The paper only mentions this as a future direction without exploring it
- What evidence would resolve it: Implementation and testing of lookahead-based heuristics compared to current methods

### Open Question 4
- Question: How sensitive are the proposed algorithms to the initial random sample selection?
- Basis in paper: [inferred] The paper mentions 20 independent runs for robustness but doesn't analyze sensitivity to initialization
- Why unresolved: Initial conditions could significantly affect convergence in resource-constrained settings
- What evidence would resolve it: Systematic analysis of algorithm performance with different initialization strategies

## Limitations

- The algorithms are tested only on synthetic benchmark functions, and real-world applicability to complex engineering design problems remains unverified
- The cost-cooling strategy (γ = (B - Bt)/B) in EIPU is presented without rigorous justification for why this specific form achieves optimal exploration-exploitation balance
- The sensitivity analysis for hyperparameters like p in pReuseBO and k in Periodic Switching algorithms is limited, with only coarse grid searches reported rather than systematic sensitivity analysis

## Confidence

High confidence: The basic algorithmic framework (GP-based BO with switching cost modeling) is well-established and the experimental methodology (GAP metric, repeated runs) follows standard practice.

Medium confidence: The comparative performance claims between algorithms are reasonable but could be affected by implementation details not fully specified (initial sampling strategy, exact cost function implementation).

Low confidence: The claims about robustness across all landscape features and cost trade-offs are based on a limited set of test functions that may not span the full range of possible optimization landscapes.

## Next Checks

1. Implement and test the algorithms on a real-world engineering optimization problem with actual switching costs (e.g., CNC machine setup changes or lab experiment configurations) to verify practical applicability.

2. Conduct a systematic sensitivity analysis for the probability parameter p in pReuseBO across the full range [0,1] with finer granularity to identify optimal settings for different problem classes.

3. Compare the cost-cooling strategy γ = (B - Bt)/B against alternative cooling schedules (linear, exponential, adaptive) to determine if the proposed form is optimal or if performance can be improved with different cooling functions.