---
ver: rpa2
title: Self-Supervised Learning of Iterative Solvers for Constrained Optimization
arxiv_id: '2409.08066'
source_url: https://arxiv.org/abs/2409.08066
tags:
- solver
- training
- problem
- network
- predictor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LISCO, a neural network-based iterative solver
  for constrained parametric optimization problems, designed to provide high-accuracy
  solutions under tight real-time constraints. The approach combines a predictor network
  that generates initial primal-dual solution estimates with a learned iterative solver
  that refines these estimates until convergence.
---

# Self-Supervised Learning of Iterative Solvers for Constrained Optimization

## Quick Facts
- arXiv ID: 2409.08066
- Source URL: https://arxiv.org/abs/2409.08066
- Authors: Lukas Lüken; Sergio Lucia
- Reference count: 40
- Primary result: Neural network-based iterative solver achieves up to 10x speedup over IPOPT while maintaining high accuracy for constrained parametric optimization

## Executive Summary
This paper introduces LISCO, a neural network-based iterative solver for constrained parametric optimization problems designed to provide high-accuracy solutions under tight real-time constraints. The approach combines a predictor network that generates initial primal-dual solution estimates with a learned iterative solver that refines these estimates until convergence. A novel self-supervised training loss function based on Karush-Kuhn-Tucker (KKT) conditions enables training without pre-sampled optimizer solutions, with theoretical guarantees ensuring minima occur only at KKT points. A convexification procedure extends the method to nonconvex problems while preserving these guarantees.

The method is evaluated on two nonconvex case studies: nonlinear MPC for a double integrator system and a large parametric optimization problem with 50 parameters. Results demonstrate speedups of up to one order of magnitude compared to IPOPT while achieving orders of magnitude higher accuracy than competing learning-based approaches. The predictor-based initialization reduces solver iterations significantly, and the method achieves high accuracy with no constraint violations after convergence.

## Method Summary
LISCO uses a two-stage approach where a predictor network maps problem parameters to initial primal-dual variable estimates, followed by a solver network that iteratively refines these estimates. The key innovation is a self-supervised loss function based on KKT residuals that enables training without requiring pre-computed optimal solutions. For nonconvex problems, a convexification procedure approximates the problem locally with a convex QP around each predicted point. The method trains on the convexified problem while maintaining theoretical guarantees that minima occur only at KKT points. During inference, the solver operates on the original nonconvex problem using the learned update rules.

## Key Results
- Achieves up to 10x speedup compared to IPOPT while maintaining higher accuracy than competing learning-based approaches
- Orders of magnitude higher accuracy than state-of-the-art learning-based methods, with no constraint violations after convergence
- Predictor-based initialization reduces solver iterations by 60-70% in tested cases
- Successfully handles nonconvex problems through convexification while preserving theoretical guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The KKT-based loss function ensures that minima of the loss correspond exactly to optimal solutions of the optimization problem.
- Mechanism: The loss function uses the squared 2-norm of KKT residuals, and through theoretical analysis (Lemma 1), it's proven that this loss only has minima where the KKT conditions are satisfied. The proof relies on showing that the KKT matrix is non-singular under given assumptions.
- Core assumption: The optimization problem is feasible, constraints are twice continuously differentiable, and the Hessian of the Lagrangian is positive definite (or can be made positive definite through convexification).
- Evidence anchors:
  - [abstract]: "Theoretical guarantees ensure that the training loss function attains minima exclusively at KKT points."
  - [section IV]: Detailed proof of Lemma 1 showing that ∇zl(ˆzi,p i) = 0⇔rϕ(ˆzi,p i) = 0
  - [corpus]: No direct corpus evidence for this specific theoretical guarantee
- Break condition: If the Hessian approximation is not positive definite or the KKT matrix becomes singular, the guarantee fails and the loss may have minima at non-KKT points.

### Mechanism 2
- Claim: The convexification procedure enables training on nonconvex problems while preserving theoretical guarantees.
- Mechanism: By locally approximating the nonlinear problem with a convex quadratic program around each predicted point, the method ensures the Hessian is positive definite. The KKT residuals for the convexified problem are identical to the original problem at the prediction point, so training still targets correct solutions.
- Core assumption: The nonconvexity originates from nonlinear dynamics/objectives that can be locally approximated, and the approximation error is small near the solution.
- Evidence anchors:
  - [abstract]: "A convexification procedure enables application to nonconvex problems while preserving these guarantees."
  - [section V.B]: Description of replacing the Hessian with a positive definite approximation and linearizing constraints
  - [corpus]: No direct corpus evidence for this specific convexification approach
- Break condition: If the convex approximation is poor (e.g., far from the solution or highly nonlinear), the solver may converge to incorrect local minima.

### Mechanism 3
- Claim: The predictor network initialization significantly reduces solver iterations and overall runtime.
- Mechanism: The predictor provides a good initial guess for the primal-dual variables, reducing the distance to the optimal solution. This means fewer solver iterations are needed to reach the desired tolerance, as shown in the convergence plots.
- Core assumption: The predictor can learn a reasonable approximation of the solution mapping from parameters to optimal primal-dual variables.
- Evidence anchors:
  - [abstract]: "The predictor-based initialization reduces solver iterations significantly"
  - [section VI.A]: Figure 3 shows convergence curves, and Table II shows accuracy improvement with predictor initialization
  - [corpus]: No direct corpus evidence for predictor effectiveness on this specific architecture
- Break condition: If the predictor is poorly trained or the parameter space is too complex, initialization may not provide significant benefit or could even mislead the solver.

## Foundational Learning

- Concept: Karush-Kuhn-Tucker (KKT) conditions
  - Why needed here: The entire method is built on KKT conditions being the foundation for both the loss function and the solver architecture
  - Quick check question: What are the four main components of the KKT conditions for constrained optimization?

- Concept: Self-supervised learning
  - Why needed here: The method eliminates the need for pre-sampled optimal solutions by using problem structure (KKT conditions) as supervision
  - Quick check question: How does self-supervised learning differ from supervised learning in the context of optimization?

- Concept: Neural network architecture for iterative refinement
  - Why needed here: The solver network must predict update steps that iteratively improve the solution, requiring understanding of how neural networks can learn iterative algorithms
  - Quick check question: What architectural choices enable a neural network to predict meaningful update steps in an iterative optimization process?

## Architecture Onboarding

- Component map: Parameters → Predictor → Initial Primal-Dual → Solver (iterative) → Final Primal-Dual → KKT Residual Check

- Critical path: Parameters → Predictor → Initial Primal-Dual → Solver (iterative) → Final Primal-Dual → KKT Residual Check

- Design tradeoffs:
  - Predictor accuracy vs. network complexity: Better predictors reduce solver iterations but increase model size
  - Solver network capacity vs. generalization: More capacity can handle complex problems but may overfit
  - Convexification accuracy vs. computational cost: Better approximations improve training but increase computation

- Failure signatures:
  - Slow convergence: Predictor not providing good initializations
  - Divergence: Solver steps too large or network not learning correct update direction
  - Constraint violations: KKT residual computation errors or improper handling of inequality constraints
  - Poor generalization: Training data not representative of test distribution

- First 3 experiments:
  1. Train predictor alone on a simple convex QP problem, verify it learns reasonable initializations
  2. Train solver with perfect predictor initialization on same problem, verify convergence
  3. Train end-to-end on nonconvex problem with convexification, compare to IPOPT solver

## Open Questions the Paper Calls Out
The paper explicitly states that "Future work will consider the derivation of rigorous convergence guarantees for the learned iterative solver." While the paper provides theoretical guarantees that the loss function has minima exclusively at KKT points, it does not establish convergence guarantees for the iterative refinement process itself. The convergence behavior observed in experiments is empirical rather than theoretically proven.

## Limitations
- The convexification approach for nonconvex problems relies on the assumption that local quadratic approximations will be sufficiently accurate near optimal solutions, which may break down for problems with high nonlinearity or poor scaling.
- The self-supervised sampling algorithm's effectiveness depends on the dynamic adjustment of primal-dual estimate distributions, but specific implementation details are not provided in the paper.
- Performance scaling with problem dimension and constraint density has not been systematically explored, limiting understanding of practical limitations for large-scale problems.

## Confidence

- KKT-based loss function guarantees: High
- Speedup claims: Medium
- Predictor effectiveness: Medium
- Convexification procedure: Low-Medium

## Next Checks

1. **KKT Residual Analysis**: For a known convex problem with analytical solution, verify that the learned solver consistently produces solutions with near-zero KKT residuals across different parameter values.

2. **Generalization Test**: Train the model on a simpler version of a nonconvex problem and evaluate performance on a more complex variant with different constraint structures to assess robustness.

3. **Solver Ablation**: Compare convergence behavior and final accuracy when using: (a) random initialization, (b) predictor initialization, and (c) true optimal initialization to quantify the predictor's contribution.