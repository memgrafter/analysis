---
ver: rpa2
title: Mechanistic Interpretability of Binary and Ternary Transformers
arxiv_id: '2405.17703'
source_url: https://arxiv.org/abs/2405.17703
tags:
- binary
- ternary
- networks
- transformer
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work applies mechanistic interpretability to analyze binary
  and ternary transformer networks on the modular addition task, comparing them to
  full-precision transformers. The key finding is that despite the discrete nature
  of binarized/ternarized weights, binary and ternary transformers learn similar Fourier-based
  "clock" algorithms to full-precision models, rather than simpler or more interpretable
  discrete algorithms.
---

# Mechanistic Interpretability of Binary and Ternary Transformers

## Quick Facts
- arXiv ID: 2405.17703
- Source URL: https://arxiv.org/abs/2405.17703
- Authors: Jason Li
- Reference count: 6
- This work applies mechanistic interpretability to analyze binary and ternary transformer networks on the modular addition task, comparing them to full-precision transformers.

## Executive Summary
This study investigates whether binarized and ternarized transformer networks learn fundamentally different or more interpretable algorithms compared to full-precision transformers. Using mechanistic interpretability techniques on the modular addition task, the research finds that despite the discrete nature of binarized/ternarized weights, these networks learn similar Fourier-based "clock" algorithms to their full-precision counterparts. The study demonstrates that binary and ternary transformers exhibit grokking behavior under weight decay regularization and show comparable periodicity patterns in their learned representations, though with increased noise levels.

## Method Summary
The research applies BitNet binarization and ternarization to transformer networks on the modular addition task (prime 113). The methodology involves training 1-layer transformers with specified architecture (P=113, d=128 residual dimension, 4 attention heads, MLP hidden dimension 512) using AdamW optimizer with weight decay regularization. The study employs mechanistic interpretability techniques including Fourier component analysis of embedding matrices and logits to examine the learned algorithms and periodicity patterns. Binary/ternary transformers are compared against full-precision baselines to assess differences in algorithm learning, grokking behavior, and noise characteristics.

## Key Results
- Binary and ternary transformers learn similar Fourier-based "clock" algorithms to full-precision models on modular addition
- Binary and ternary networks exhibit grokking behavior under weight decay regularization
- Binary and ternary networks show more noise in learned representations compared to full-precision models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Binary and ternary transformers learn similar Fourier-based "clock" algorithms to full-precision models on modular addition
- Mechanism: The discrete weight constraints do not fundamentally alter the optimization landscape enough to force different algorithmic solutions; instead, the models still converge to the same Fourier-based representations for solving modular arithmetic
- Core assumption: The modular addition task has a strong basin of attraction toward Fourier-based solutions regardless of weight precision
- Evidence anchors:
  - [abstract] "binary and ternary transformers learn similar algorithms as full precision networks"
  - [section] "we observe periodicity in the embedding matrix just as in (Nanda et al., 2023)"
  - [corpus] Weak evidence - neighboring papers focus on graph transformers and time series, not binary/ternary transformers specifically

### Mechanism 2
- Claim: Binary and ternary networks exhibit grokking behavior under weight decay regularization
- Mechanism: The regularization helps the discrete-weight models escape local minima and converge to generalizable solutions, similar to full-precision networks
- Core assumption: Weight decay regularization is necessary for both discrete and continuous networks to achieve generalization
- Evidence anchors:
  - [section] "we find that under weight decay regularization, both binary and ternary networks consistently exhibit grokking"
  - [section] "without weight decay, binary and ternary networks do not exhibit grokking"
  - [corpus] Weak evidence - neighboring papers don't discuss grokking in binary/ternary networks

### Mechanism 3
- Claim: Binary and ternary networks show more noise in learned representations compared to full-precision
- Mechanism: The discrete weight constraints introduce quantization noise that affects the smoothness of learned representations, particularly in the projection to Fourier basis
- Core assumption: Weight quantization introduces noise that propagates through the network's learned representations
- Evidence anchors:
  - [section] "we observe significant noise in the projection to fourier basis"
  - [section] "fraction of variance explained by degree-2 polynomials" shows lower values for binary models
  - [corpus] Weak evidence - no neighboring papers discuss noise in binary/ternary transformers

## Foundational Learning

- Concept: Modular arithmetic and Fourier transforms
  - Why needed here: The "clock" algorithm relies on representing modular addition in the Fourier domain
  - Quick check question: Can you explain why Fourier transforms are useful for modular addition problems?

- Concept: Mechanistic interpretability techniques
  - Why needed here: The paper uses techniques like analyzing Fourier components and neuron activation patterns
  - Quick check question: What are the key steps in reverse-engineering a neural network algorithm?

- Concept: Weight quantization and straight-through estimator
  - Why needed here: Binary/ternary transformers use these techniques to handle discrete weights during training
  - Quick check question: How does the straight-through estimator work and why is it necessary for binary networks?

## Architecture Onboarding

- Component map:
  Input embedding layer -> Attention mechanism with binarized/ternarized weights -> MLP layer with binarized/ternarized weights -> RMSNorm layer -> Output unembedding layer -> Logits output

- Critical path:
  1. Input tokens â†’ embedding
  2. Self-attention with binarized weights
  3. Feed-forward with binarized weights
  4. RMSNorm
  5. Unembedding
  6. Logits output

- Design tradeoffs:
  - Memory efficiency vs. representation capacity
  - Training stability vs. inference speed
  - Binarization of all layers vs. keeping some full-precision

- Failure signatures:
  - No grokking behavior (likely missing weight decay)
  - High noise in Fourier components (weight quantization issues)
  - Poor accuracy despite proper training (insufficient model capacity)

- First 3 experiments:
  1. Train a full-precision model on modular addition and verify the "clock" algorithm
  2. Train a binary model with weight decay and check for grokking
  3. Compare Fourier component analysis between binary and full-precision models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can binary/ternary transformers learn fundamentally different discrete algorithms for modular addition that are more interpretable than the Fourier-based "clock" algorithm?
- Basis in paper: [explicit] The authors hypothesized that discrete-weight transformers might learn alternative discrete algorithms which are more interpretable, but found they learned similar Fourier-based algorithms to full-precision models.
- Why unresolved: The paper only tested one specific problem (modular addition) and found similar algorithms. The authors noted they didn't have conclusive evidence about the exact algorithms learned, and other discrete algorithms may exist for this or other problems.
- What evidence would resolve it: Testing binary/ternary transformers on multiple discrete problems (modular addition, sorting, logic operations) to see if they consistently learn Fourier-based algorithms or occasionally discover simpler discrete algorithms. Comparing the interpretability and simplicity of any alternative algorithms discovered.

### Open Question 2
- Question: Does binarizing/ternarizing all layers including embeddings/unembeddings affect the learned algorithms and interpretability?
- Basis in paper: [explicit] The authors noted their implementation did not binarize embed/unembed layers because they couldn't get optimization to work with fully binarized networks, suggesting this as future work.
- Why unresolved: The study only partially binarized transformers (excluding embed/unembed layers), so the impact of full binarization on algorithm learning is unknown.
- What evidence would resolve it: Implementing successful optimization techniques for fully binarized/ternarized transformers and comparing their learned algorithms and interpretability to both the partially binarized models in this study and full-precision models.

### Open Question 3
- Question: Can advanced binarization/ternarization techniques (beyond straight-through estimation) lead to more interpretable algorithms?
- Basis in paper: [explicit] The authors noted they only explored basic STE techniques and mentioned other optimization tricks exist, suggesting this as future work.
- Why unresolved: The study used only basic STE optimization, which may constrain the learning process in ways that prevent discovery of simpler algorithms.
- What evidence would resolve it: Applying advanced binarization/ternarization optimization techniques (gradient scaling, loss-aware binarization, etc.) to the same problems and determining if these enable discovery of more interpretable discrete algorithms compared to both basic STE and full-precision models.

## Limitations

- The study focuses on a single synthetic task (modular addition) with strong algorithmic structure, limiting generalizability to complex real-world problems
- The analysis relies heavily on Fourier-based interpretability techniques, which may not capture all relevant algorithmic differences
- The noise characterization lacks quantitative comparisons across different quantization schemes or training durations

## Confidence

- **High Confidence**: The observation that binary/ternary transformers learn similar "clock" algorithms to full-precision models on modular addition. This is directly observable through Fourier component analysis and supported by multiple experiments.
- **Medium Confidence**: The claim that discrete-weight transformers are unlikely to offer interpretability advantages. This extrapolation from a single task requires more diverse experimental validation.
- **Low Confidence**: The mechanism explanation for why weight quantization doesn't fundamentally change the learned algorithm. The paper suggests the modular addition task has a strong basin of attraction toward Fourier solutions, but doesn't provide theoretical justification for this claim.

## Next Checks

1. **Cross-task validation**: Apply the same mechanistic interpretability analysis to binary/ternary transformers on other algorithmic tasks (e.g., XOR, sorting) to determine if the Fourier-based algorithm pattern holds across different problem types.

2. **Noise characterization**: Quantitatively compare noise levels in learned representations across different quantization bit-widths (2-bit, 3-bit, 4-bit) and training durations to establish whether noise decreases with increased precision or training time.

3. **Alternative algorithmic solutions**: Design experiments to force binary/ternary transformers to learn non-Fourier algorithms (e.g., by modifying the loss function or architecture constraints) to test whether the "clock" algorithm is truly the only viable solution under discrete weight constraints.