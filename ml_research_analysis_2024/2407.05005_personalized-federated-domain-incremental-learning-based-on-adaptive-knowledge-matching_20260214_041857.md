---
ver: rpa2
title: Personalized Federated Domain-Incremental Learning based on Adaptive Knowledge
  Matching
arxiv_id: '2407.05005'
source_url: https://arxiv.org/abs/2407.05005
tags:
- learning
- tasks
- knowledge
- each
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of catastrophic forgetting in
  federated domain-incremental learning, where each client must learn new tasks with
  different domains while preserving performance on previous tasks. The proposed pFedDIL
  method employs adaptive knowledge matching through auxiliary classifiers to calculate
  task similarity, enabling intelligent selection of learning strategies (either reusing
  similar models or initializing new ones) and knowledge migration from related tasks.
---

# Personalized Federated Domain-Incremental Learning based on Adaptive Knowledge Matching

## Quick Facts
- **arXiv ID**: 2407.05005
- **Source URL**: https://arxiv.org/abs/2407.05005
- **Reference count**: 40
- **Primary result**: pFedDIL achieves up to 14.35% improvement in average accuracy compared to state-of-the-art methods in federated domain-incremental learning

## Executive Summary
This paper addresses catastrophic forgetting in federated domain-incremental learning, where clients must learn new tasks with different domains while preserving performance on previous tasks. The proposed pFedDIL method employs adaptive knowledge matching through auxiliary classifiers to calculate task similarity, enabling intelligent selection of learning strategies (either reusing similar models or initializing new ones) and knowledge migration from related tasks. To reduce model size, partial parameter sharing is implemented between the auxiliary classifier and target model. Extensive experiments across three datasets demonstrate pFedDIL achieves up to 14.35% improvement in average accuracy compared to state-of-the-art methods, while requiring fewer communication rounds and maintaining model efficiency through parameter sharing.

## Method Summary
The pFedDIL method operates in a federated learning setting where each client learns incremental tasks with domain shifts. The core approach uses auxiliary binary classifiers to calculate task similarity, then selects between reusing similar models or initializing new ones based on these similarity scores. Knowledge migration from similar tasks is applied with weights proportional to similarity, and partial parameter sharing between auxiliary classifiers and target models reduces overall model size. The method employs weighted ensemble inference during testing to combine predictions from all personalized models. Training involves 180 communication rounds per task with E=20 local epochs, batch size 32, and learning rate 0.001.

## Key Results
- pFedDIL achieves up to 14.35% improvement in average accuracy compared to state-of-the-art methods
- The method requires fewer communication rounds than existing approaches
- Parameter sharing between auxiliary classifiers and target models maintains model efficiency while improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive knowledge matching via auxiliary classifiers identifies task similarity to guide learning strategy selection
- Mechanism: Each client trains auxiliary binary classifiers to distinguish task boundaries, then uses their outputs as similarity scores between new and previous tasks
- Core assumption: Feature representations extracted by shared encoder layers contain sufficient discriminative information to assess task similarity
- Evidence anchors:
  - [abstract] "each client first calculates its local correlations with previous tasks"
  - [section] "we define the auxiliary classifier, a binary classification deep neural networkθ for the corresponding local taskT , which is trained by the discrimination of the sample belonging to the task T"
  - [corpus] Weak - no direct corpus support found
- Break condition: If tasks share overlapping feature distributions without clear boundaries, auxiliary classifiers cannot reliably distinguish them

### Mechanism 2
- Claim: Knowledge migration from similar tasks prevents catastrophic forgetting while accelerating new task learning
- Mechanism: When training on new tasks, clients apply weighted knowledge migration from previous personalized models, with weights based on similarity scores
- Core assumption: Knowledge from similar tasks is complementary and can be effectively transferred without interference
- Evidence anchors:
  - [abstract] "migrate knowledge from related tasks based on these correlations"
  - [section] "each client can migrate knowledge from other not matched but similar personalized models in a weighted manner, where the weights are the correlations obtained previously"
  - [corpus] Weak - no direct corpus support found
- Break condition: If previous tasks have conflicting knowledge or feature distributions, migration may introduce noise and degrade performance

### Mechanism 3
- Claim: Partial parameter sharing between auxiliary classifiers and target models reduces model size while maintaining discrimination capability
- Mechanism: Front layers of target model are shared with auxiliary classifier, reducing total parameters while maintaining feature extraction capability
- Core assumption: Shared layers contain general feature representations useful for both classification and task discrimination
- Evidence anchors:
  - [abstract] "we propose sharing partial parameters between the auxiliary classifier and target classification model to condense model parameters"
  - [section] "we propose sharing the front model layers between the two models which are used to extract features"
  - [corpus] Weak - no direct corpus support found
- Break condition: If task discrimination requires specialized features not present in shared layers, performance will degrade

## Foundational Learning

- Concept: Federated Learning fundamentals
  - Why needed here: The entire system operates in a distributed client-server architecture where clients never share raw data
  - Quick check question: Can you explain the difference between FedAvg and FedProx in handling non-IID data?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The core problem being solved is preventing models from forgetting previous tasks when learning new ones
  - Quick check question: What is the key difference between rehearsal-based and regularization-based approaches to catastrophic forgetting?

- Concept: Domain adaptation and domain shift
  - Why needed here: The paper focuses on domain-incremental learning where tasks have same classes but different feature distributions
  - Quick check question: How does domain shift differ from class-incremental learning in terms of feature space overlap?

## Architecture Onboarding

- Component map: Client-side: Personalized model collection, auxiliary classifiers, knowledge matching module, migration controller → Server-side: Model aggregation, client selection → Communication: Local model parameters, similarity scores

- Critical path: Task arrival → similarity calculation → strategy selection → model training with migration → aggregation

- Design tradeoffs:
  - Model size vs. performance: More personalized models provide better specialization but increase storage requirements
  - Migration strength vs. stability: Stronger migration helps faster learning but risks interference
  - Communication frequency vs. convergence: More frequent updates help but increase overhead

- Failure signatures:
  - High variance in task similarity scores suggests poor feature separation
  - Degradation in previous task performance indicates excessive migration
  - Slow convergence suggests insufficient migration or poor similarity estimation

- First 3 experiments:
  1. Baseline test: Run without knowledge migration to establish baseline performance
  2. Similarity calibration: Test auxiliary classifier accuracy on distinguishing task boundaries
  3. Migration sensitivity: Vary migration strength to find optimal balance between learning speed and stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of pFedDIL compare to traditional federated learning methods when the tasks have minimal domain shift versus significant domain shift?
- Basis in paper: [explicit] The paper mentions that pFedDIL outperforms state-of-the-art methods by up to 14.35% in terms of average accuracy on all tasks, but it doesn't specify the performance difference across varying levels of domain shift.
- Why unresolved: The paper does not provide a detailed analysis of the performance of pFedDIL under different levels of domain shift, which is crucial for understanding its robustness and adaptability.
- What evidence would resolve it: Conducting experiments that systematically vary the degree of domain shift and comparing the performance of pFedDIL against traditional federated learning methods would provide the necessary evidence.

### Open Question 2
- Question: What is the impact of the partial parameter sharing between the auxiliary classifier and the target classification model on the overall model efficiency and performance?
- Basis in paper: [explicit] The paper mentions that partial parameter sharing is proposed to condense model parameters, but it does not provide a detailed analysis of its impact on model efficiency and performance.
- Why unresolved: While the paper states the intention behind partial parameter sharing, it lacks empirical evidence on how this affects the model's efficiency and performance.
- What evidence would resolve it: Experiments that compare the performance and efficiency of pFedDIL with and without partial parameter sharing would clarify its impact.

### Open Question 3
- Question: How does the choice of the hyper-parameter λ affect the balance between accuracy and storage in pFedDIL, and what is the optimal strategy for selecting this parameter?
- Basis in paper: [explicit] The paper discusses the sensitivity of pFedDIL to the hyper-parameter λ and its effect on average accuracy and model size, but it does not provide a definitive strategy for selecting the optimal value of λ.
- Why unresolved: The paper provides some insights into the effects of λ but lacks a comprehensive strategy for its selection, which is crucial for practical implementation.
- What evidence would resolve it: A detailed study that explores the trade-offs between accuracy and storage for different values of λ and proposes a strategy for its selection would resolve this question.

## Limitations

- The paper's claims about adaptive knowledge matching effectiveness rely heavily on the auxiliary classifier's ability to accurately assess task similarity, but limited empirical validation of classifier performance on this discrimination task is provided
- The partial parameter sharing mechanism is described but lacks detailed analysis of which layers benefit most from sharing and which should remain task-specific
- The claimed 14.35% improvement over state-of-the-art methods lacks detailed statistical analysis and comparison conditions

## Confidence

- **High confidence**: The core federated learning framework and overall methodology structure are sound and well-motivated
- **Medium confidence**: The adaptive strategy selection mechanism appears reasonable but requires empirical validation of the similarity scoring effectiveness
- **Low confidence**: The claimed 14.35% improvement over state-of-the-art methods lacks detailed statistical analysis and comparison conditions

## Next Checks

1. **Similarity scoring validation**: Conduct ablation studies isolating the auxiliary classifier's accuracy in distinguishing task boundaries, with quantitative metrics on classification performance for the discrimination task itself

2. **Migration impact analysis**: Perform controlled experiments varying the knowledge migration strength parameter to demonstrate the trade-off between learning speed and catastrophic forgetting, with per-task accuracy tracking over time

3. **Parameter sharing efficiency**: Analyze which specific layers benefit most from parameter sharing versus task-specific adaptation, potentially through layer-wise ablation studies comparing full vs. partial sharing scenarios