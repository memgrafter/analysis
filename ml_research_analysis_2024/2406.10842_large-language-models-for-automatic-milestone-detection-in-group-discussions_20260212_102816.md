---
ver: rpa2
title: Large Language Models for Automatic Milestone Detection in Group Discussions
arxiv_id: '2406.10842'
source_url: https://arxiv.org/abs/2406.10842
tags:
- milestone
- group
- which
- sentence
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the use of large language models (LLMs)
  to automatically detect milestones in group discussions, a task traditionally requiring
  manual annotation. A puzzle-based experiment with 20 groups was designed, featuring
  five milestones that could be achieved in any order.
---

# Large Language Models for Automatic Milestone Detection in Group Discussions

## Quick Facts
- arXiv ID: 2406.10842
- Source URL: https://arxiv.org/abs/2406.10842
- Reference count: 7
- Primary result: GPT-4 iterative prompting achieves 94.5-97.5% accuracy for milestone detection in controlled puzzle discussions

## Executive Summary
This study explores the use of large language models for automatically detecting milestones in group discussions, a task traditionally requiring manual annotation. The researchers developed a puzzle-based experiment with 20 groups and five milestones that could be achieved in any order. They compared a baseline BERT-based semantic similarity approach with an iterative prompting method using GPT-4. The iterative GPT-4 approach significantly outperformed the baseline, demonstrating high accuracy in detecting milestone achievements and identifying the specific utterances marking milestone completion. However, challenges including non-determinism, formatting issues, and occasional hallucinations were observed, highlighting the need for careful prompt engineering and result validation.

## Method Summary
The researchers designed a controlled experiment where 20 groups solved a puzzle with five milestones. They implemented two approaches: a baseline using BERT-based sentence embeddings to detect semantic similarity between group utterances and milestone descriptions, and an iterative prompting approach using GPT-4. The iterative method involved prompting GPT-4 to analyze discussion transcripts, identify milestone completions, and return the specific utterances marking these achievements. Both methods were evaluated against ground truth milestone annotations, with the iterative GPT-4 approach demonstrating superior performance in accuracy and precision.

## Key Results
- GPT-4 iterative prompting achieved 94.5% accuracy for detecting the "octopus" milestone
- GPT-4 iterative prompting achieved 97.5% accuracy for detecting the "solution" milestone
- GPT-4 successfully identified specific utterances marking milestone completion, outperforming the BERT baseline

## Why This Works (Mechanism)
The iterative prompting approach works by leveraging GPT-4's advanced language understanding capabilities to analyze discussion context holistically. Unlike the BERT baseline that relied solely on semantic similarity between isolated utterances and milestone descriptions, GPT-4 can maintain conversation context, understand implicit milestone achievements, and reason about the progression of group discussions. The iterative nature allows for multiple passes of analysis, enabling the model to refine its understanding and improve detection accuracy. GPT-4's ability to understand nuanced language, sarcasm, and indirect references to milestones gives it a significant advantage over traditional semantic similarity approaches.

## Foundational Learning
- Semantic similarity metrics - needed to understand baseline BERT approach; quick check: can measure cosine similarity between embeddings
- Prompt engineering - essential for effective LLM interaction; quick check: can craft prompts that elicit desired responses consistently
- Iterative reasoning - important for complex multi-step tasks; quick check: can design prompts that ask model to refine previous answers
- Conversation context modeling - crucial for understanding group dynamics; quick check: can track topic progression across multiple utterances

## Architecture Onboarding

Component map: Discussion transcript -> GPT-4 analysis -> Milestone detection -> Utterance identification

Critical path: Transcript ingestion → Context window processing → Milestone matching → Response generation

Design tradeoffs: The iterative approach trades computational cost for accuracy, while the non-deterministic nature of GPT-4 introduces variability that must be managed through validation mechanisms.

Failure signatures: Hallucinations producing false milestone detections, formatting inconsistencies in outputs, context window limitations causing missed milestones in long discussions.

First experiments:
1. Test with a single milestone and simple transcript to verify basic functionality
2. Compare single-pass vs. multi-pass prompting strategies
3. Evaluate performance with increasing transcript lengths to identify context window limitations

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental design limited to 20 groups and five specific milestones, reducing generalizability
- Non-deterministic GPT-4 outputs introduce variability affecting real-world deployment
- Observed hallucinations and formatting issues require robust validation mechanisms

## Confidence
High: Comparative performance of GPT-4 vs BERT within experimental scope
Medium: Generalizability across different discussion types and milestone structures

## Next Checks
1. Test the iterative prompting approach across multiple discussion domains (business meetings, educational settings) to assess domain transfer
2. Evaluate performance with larger group sizes and longer discussion durations to identify scalability limits
3. Implement a human-in-the-loop validation framework to quantify the trade-off between automation efficiency and accuracy when hallucinations occur