---
ver: rpa2
title: A Centralized-Distributed Transfer Model for Cross-Domain Recommendation Based
  on Multi-Source Heterogeneous Transfer Learning
arxiv_id: '2411.09286'
source_url: https://arxiv.org/abs/2411.09286
tags:
- domain
- transfer
- domains
- target
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a centralized-distributed transfer model (CDTM)
  for cross-domain recommendation (CDR) that addresses feature dimensional heterogeneity
  and latent space heterogeneity among domains. The model uses a dual embedding structure
  consisting of domain-specific embeddings (DSE) and global shared embeddings (GSE),
  combined through an attention mechanism and transfer matrix.
---

# A Centralized-Distributed Transfer Model for Cross-Domain Recommendation Based on Multi-Source Heterogeneous Transfer Learning

## Quick Facts
- arXiv ID: 2411.09286
- Source URL: https://arxiv.org/abs/2411.09286
- Reference count: 31
- Primary result: 5.1% CTR and 6.6% eCPM improvement in online A/B testing

## Executive Summary
This paper addresses the challenge of cross-domain recommendation (CDR) in heterogeneous scenarios where source and target domains have different feature dimensions and latent space distributions. The authors propose a Centralized-Distributed Transfer Model (CDTM) that uses a dual embedding structure to handle feature dimensional heterogeneity and a transfer matrix combined with attention mechanism to address latent space heterogeneity. Tested on real-world commercial data from NetEase Cloud Music's advertising system, CDTM achieved significant improvements over baseline models in both offline AUC metrics and online CTR/eCPM performance.

## Method Summary
CDTM employs a dual embedding structure consisting of Domain-Specific Embeddings (DSE) and Global Shared Embeddings (GSE) to handle heterogeneous feature dimensions across domains. The model uses a transfer matrix to map GSE into DSE's latent space, addressing latent space heterogeneity, and an attention mechanism to adaptively combine DSE and mapped GSE based on feature importance. The model is trained centrally across all domains with a combined loss function, allowing knowledge transfer from source to target domains while maintaining domain-specific characteristics.

## Key Results
- 3.62% AUC improvement over baseline in single-source transfer tasks
- 4.57% AUC improvement in multi-source transfer tasks
- 5.1% improvement in click-through rate (CTR) and 6.6% improvement in effective cost per mille (eCPM) in online A/B testing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual embedding structure with DSE and GSE enables cross-domain transfer despite feature dimensional heterogeneity.
- Mechanism: Separates transferable and non-transferable features, allowing each domain to maintain unique characteristics via DSE while capturing global patterns via GSE.
- Core assumption: Transferable features exist and can be identified between domains.
- Evidence anchors: [abstract] dual embedding structure; [section III-B] lookup tables for DSE and GSE.

### Mechanism 2
- Claim: Transfer matrix mapping between DSE and GSE latent spaces addresses latent space heterogeneity.
- Mechanism: Linear transformation aligns latent spaces by minimizing Euclidean distance between Ec and T⊗Gc.
- Core assumption: A linear transformation is sufficient to align latent spaces.
- Evidence anchors: [abstract] transfer matrix maps and combines embeddings; [section III-C1] proposal of transfer matrix.

### Mechanism 3
- Claim: Combination attention mechanism weights DSE and mapped GSE contributions based on feature importance in different domains.
- Mechanism: Computes attention weights A to determine reliance on DSE versus GSE for each transferable feature.
- Core assumption: Same feature has different importance across domains that can be learned through attention.
- Evidence anchors: [abstract] attention mechanism used to combine embeddings; [section III-C2] attention formula A = σ(V1h0 + b1).

## Foundational Learning

- Concept: Transfer learning and cross-domain recommendation
  - Why needed here: Addresses transferring knowledge from source domains to improve target domain performance when data is sparse.
  - Quick check question: What is the key difference between traditional single-domain recommendation and cross-domain recommendation?

- Concept: Embedding techniques and feature representation learning
  - Why needed here: Model relies heavily on embedding layers to represent both domain-specific and global features in shared vector space.
  - Quick check question: How do embeddings help capture complex feature interactions that raw features cannot represent directly?

- Concept: Attention mechanisms in neural networks
  - Why needed here: Combination layer uses attention to weight contribution of DSE versus GSE for each transferable feature.
  - Quick check question: What is the purpose of using attention mechanisms in neural network architectures?

## Architecture Onboarding

- Component map: Embedding Layer (DSE + GSE) → Combination Layer (transfer matrix + attention) → Deep Layer (fully connected layers) → Output Layer (sigmoid)
- Critical path: Embedding Layer → Combination Layer → Deep Layer → Output Layer, with joint training across all domains
- Design tradeoffs: Dual embedding structure increases complexity and parameters but enables more effective heterogeneous transfer; tradeoff between transfer effectiveness and computational cost
- Failure signatures: Negative transfer, unstable training due to conflicting gradients, poor generalization from overfitting to specific domains
- First 3 experiments:
  1. Single-source transfer task (Task1): Train CDTM using one source domain and measure AUC improvement on each target domain
  2. Multi-source transfer comparison (Task2): Compare CDTM using both source domains versus single-source variants
  3. Ablation study (Task3): Remove combination attention to measure its contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CDTM model perform in scenarios with more than two source domains?
- Basis in paper: [inferred] Paper mentions CDTM can be "extended to multi-domain recommendation settings" but only tested with two source domains (H and J).
- Why unresolved: Only tested with two source domains, leaving open whether performance would scale or degrade with more source domains.
- What evidence would resolve it: Experiments testing CDTM with three or more source domains to compare performance gains against two-source case.

### Open Question 2
- Question: What is the optimal number of transferable features for maximizing CDTM's performance?
- Basis in paper: [inferred] Paper mentions "feature dimensional heterogeneity" and different transferable features across domains but doesn't investigate how quantity affects performance.
- Why unresolved: While acknowledging feature dimensional heterogeneity, doesn't systematically explore how varying number of transferable features impacts performance.
- What evidence would resolve it: Experiments varying number of transferable features across domains and measuring CDTM's performance to identify optimal threshold.

### Open Question 3
- Question: How does CDTM handle scenarios where source and target domains have completely disjoint feature sets?
- Basis in paper: [explicit] Paper mentions "feature dimensional heterogeneity" and discusses transferable vs non-transferable features but doesn't address completely disjoint features.
- Why unresolved: Assumes some overlap in transferable features but doesn't explore what happens when source and target domains share no common features.
- What evidence would resolve it: Experiments testing CDTM when source and target domains have no overlapping features.

## Limitations
- Results based on proprietary NetEase Cloud Music data that cannot be independently verified
- Specific feature fields, distributions, and domain characteristics remain undisclosed
- Performance improvements may be context-specific and not generalize to other domains

## Confidence
- High Confidence: Theoretical framework for handling feature dimensional heterogeneity and latent space heterogeneity is well-established
- Medium Confidence: Performance improvements are plausible but proprietary dataset prevents independent verification
- Low Confidence: Generalization capability to vastly different domains or behavior under negative transfer scenarios remains unclear

## Next Checks
1. Implement CDTM on publicly available multi-domain datasets (Amazon product reviews, MovieLens) to verify cross-domain transfer effectiveness
2. Conduct extensive ablation studies removing transfer matrix, attention mechanism, or GSE to quantify each component's contribution
3. Test model's robustness to negative transfer by introducing dissimilar source domains and measuring degradation in target domain performance