---
ver: rpa2
title: 'MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation'
arxiv_id: '2412.07147'
source_url: https://arxiv.org/abs/2412.07147
tags:
- text
- translation
- image
- images
- mit-10m
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MIT-10M, a large-scale multilingual image
  translation dataset with 10 million image-text pairs and 840K high-resolution images
  across 14 languages. It addresses the lack of large-scale, diverse, and high-quality
  datasets for training and evaluating image translation models.
---

# MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation

## Quick Facts
- arXiv ID: 2412.07147
- Source URL: https://arxiv.org/abs/2412.07147
- Authors: Bo Li; Shaolin Zhu; Lijie Wen
- Reference count: 40
- Key outcome: Models fine-tuned on MIT-10M achieve 230% BLEU, 88% chrF++, and 130% METEOR score improvements

## Executive Summary
MIT-10M is a large-scale multilingual image translation dataset containing 10 million image-text pairs across 14 languages, addressing the critical need for diverse, high-quality training data in this domain. The dataset was constructed through a pipeline involving web crawling, text recognition, and multilingual translation validation, resulting in 840K high-resolution images with three resolution variants. Experimental results demonstrate that models fine-tuned on MIT-10M significantly outperform baseline models across multiple evaluation metrics.

## Method Summary
The MIT-10M dataset was constructed through a systematic pipeline: web crawling to collect multilingual images, OCR for text extraction, multilingual translation generation, and quality validation. The dataset provides images in three resolutions (original, 768px width, 500px width) and categorizes text difficulty into easy, medium, and hard levels based on bounding box count and token length. The primary evaluation involved fine-tuning Qwen2-VL models on different subsets (10%, 30%, 50%, 70%, 100%) of the training data and measuring BLEU, chrF++, and METEOR scores.

## Key Results
- Fine-tuned models show 230% BLEU, 88% chrF++, and 130% METEOR score improvements over baselines
- Larger image resolutions consistently improve performance across all metrics
- Progressive scaling of training data from 10% to 100% yields monotonic performance improvements

## Why This Works (Mechanism)

### Mechanism 1: High-resolution images improve OCR accuracy and feature extraction
Larger images provide richer visual context for text detection and translation, enabling better OCR accuracy and feature extraction for multimodal models. The visual encoder can effectively utilize higher resolution information for improved text localization and translation.

### Mechanism 2: Multi-dimensional difficulty categorization creates effective training gradients
Different difficulty levels expose models to varying complexity scenarios, from simple single-line text to complex multi-line text with multiple bounding boxes, enabling progressive capability building. Models can learn transferable features across difficulty levels.

### Mechanism 3: Large-scale parallel corpus enables multilingual transfer learning
Massive multilingual data exposure allows models to learn cross-lingual visual-text correlations and transfer knowledge between language pairs. Multimodal models can effectively learn shared visual-text representations across diverse languages.

## Foundational Learning

- Concept: Multimodal model architecture (visual encoder + text decoder)
  - Why needed here: Understanding how models process image features and generate translations is crucial for working with MIT-10M
  - Quick check question: What are the two main components of end-to-end image translation models and how do they interact?

- Concept: OCR and text recognition fundamentals
  - Why needed here: MIT-10M uses OCR for text extraction and evaluation; understanding accuracy limitations is important
  - Quick check question: How does OCR accuracy typically vary with image resolution and text complexity?

- Concept: Machine translation evaluation metrics (BLEU, chrF++, METEOR)
  - Why needed here: These metrics are used throughout the paper to evaluate model performance on MIT-10M
  - Quick check question: What are the key differences between BLEU, chrF++, and METEOR metrics and when would each be most appropriate?

## Architecture Onboarding

- Component map: Dataset ingestion → Image preprocessing (resize to three resolutions) → Text extraction (OCR) → Translation generation → Evaluation (BLEU/chrF++/METEOR) → Model fine-tuning pipeline
- Critical path: Data loading → preprocessing → model inference/evaluation → metrics computation
- Design tradeoffs: Resolution vs. computational cost, dataset size vs. quality filtering, multilingual coverage vs. depth per language
- Failure signatures: Low OCR accuracy on small text, translation errors on complex layouts, metric scores not correlating with human judgment
- First 3 experiments:
  1. Load a small batch of MIT-10M images and verify three resolution variants are correctly generated
  2. Run a baseline model (e.g., EasyOCR + NLLB) on the test set and compute all three metrics
  3. Fine-tune a pre-trained multimodal model on 10% of the training data and compare performance to baseline

## Open Questions the Paper Calls Out

### Open Question 1: Language diversity scaling
How does the performance of multilingual image translation models scale with increasing language diversity beyond the 14 languages currently supported in MIT-10M?

### Open Question 2: Synthetic vs. real-world data balance
What is the optimal balance between synthetic and real-world data for training robust multilingual image translation models?

### Open Question 3: Long-term resolution effects
How do different image resolution sizes affect the long-term performance and generalization of multilingual image translation models across diverse real-world scenarios?

## Limitations

- Dataset representation gap: May over-represent certain scenarios while underrepresenting others across text types and image domains
- Model architecture specificity: Performance gains may be specific to Qwen2-VL rather than model-agnostic
- Generalization boundary: Limited validation of cross-dataset generalization, potential domain-specific overfitting

## Confidence

**High Confidence (8-10/10)**:
- MIT-10M is the largest real-world image translation dataset with 10M pairs across 14 languages
- Models fine-tuned on MIT-10M show significant performance improvements (230% BLEU increase)
- The dataset's difficulty categorization effectively differentiates model capabilities

**Medium Confidence (5-7/10)**:
- MIT-10M provides superior adaptability for challenging image translation tasks compared to existing datasets
- The three-image resolution strategy meaningfully improves model robustness
- Performance gains translate to real-world multilingual image translation scenarios

**Low Confidence (1-4/10)**:
- MIT-10M enables truly generalizable multilingual image translation capabilities
- The dataset comprehensively covers the diversity of real-world multilingual image translation scenarios
- Performance improvements are primarily due to dataset quality rather than model architecture specifics

## Next Checks

1. **Cross-Dataset Generalization Test**: Evaluate models fine-tuned on MIT-10M against established image translation benchmarks (e.g., TextOCR, COCO-Text) to assess whether performance gains are dataset-specific or represent genuine capability improvements.

2. **Architecture-Agnostic Validation**: Repeat the fine-tuning experiments using different multimodal model architectures (e.g., BLIP-2, LLaVA, or other vision-language models) to determine if the performance benefits are specific to Qwen2-VL or generalize across architectures.

3. **Domain Diversity Analysis**: Conduct systematic analysis of MIT-10M's domain coverage by categorizing images into semantic categories (documents, signage, products, etc.) and evaluating whether model performance varies significantly across these domains, identifying potential coverage gaps.