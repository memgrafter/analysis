---
ver: rpa2
title: 'DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying
  Synthetic Query Generation'
arxiv_id: '2404.02489'
source_url: https://arxiv.org/abs/2404.02489
tags:
- domain
- training
- zero-shot
- queries
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DUQGen addresses unsupervised domain adaptation for neural rankers
  by generating synthetic training data. The core method clusters target documents,
  samples diverse documents from each cluster, and generates queries via LLM prompting.
---

# DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation

## Quick Facts
- arXiv ID: 2404.02489
- Source URL: https://arxiv.org/abs/2404.02489
- Authors: Ramraj Chandradevan; Kaustubh D. Dhole; Eugene Agichtein
- Reference count: 16
- Primary result: DUQGen outperforms zero-shot baselines on 16 out of 18 datasets in BEIR benchmark

## Executive Summary
DUQGen addresses unsupervised domain adaptation for neural rankers by generating synthetic training data. The method clusters target documents, samples diverse documents from each cluster, and generates queries via LLM prompting. Extensive experiments on BEIR show DUQGen consistently outperforms all zero-shot baselines and substantially outperforms SOTA baselines on 16 out of 18 datasets, achieving an average 4% relative improvement across all datasets.

## Method Summary
DUQGen performs unsupervised domain adaptation by first encoding target documents using Contriever, clustering them into K groups with K-Means, and sampling documents from each cluster proportionally to cluster size using a softmax-based probabilistic approach. The sampled documents are then used as few-shot examples to prompt Llama2-7B-Chat for query generation. The synthetic query-document pairs are used to fine-tune neural rankers (MonoT5, ColBERT), with hard negative mining to improve training quality.

## Key Results
- Outperforms zero-shot baselines on 16 out of 18 BEIR datasets
- Achieves average 4% relative improvement in nDCG@10 across all datasets
- Particularly effective on datasets with limited in-domain training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering target documents creates representative domain subsets that enable focused adaptation.
- Mechanism: Documents are encoded, clustered into K groups, then each cluster is sampled proportionally to its size to ensure coverage of diverse topical regions within the domain.
- Core assumption: Cluster centroids adequately represent the semantic space of their members; larger clusters contain more representative domain content.
- Evidence anchors:
  - [abstract] "DUQGen produces a more effective representation of the target domain by identifying clusters of similar documents"
  - [section] "We propose to divide the collection into portions, and then sample documents within each portion. We use a clustering approach for the collection representation."
  - [corpus] Weak: No quantitative clustering quality metrics reported in corpus; only cluster counts (K=1000) and qualitative cluster topic examples shown.
- Break condition: If clusters are too large/small, sampling may miss rare but important domain signals, or overfit to cluster centroids.

### Mechanism 2
- Claim: Probabilistic sampling weighted by document-to-centroid similarity increases training data diversity and relevance.
- Mechanism: Each document's selection probability is proportional to its cosine similarity to its cluster centroid, scaled by a softmax temperature, ensuring both representativeness and variation.
- Core assumption: Similarity to cluster centroid is a good proxy for document's domain relevance and diversity contribution.
- Evidence anchors:
  - [section] "We intend to sample N number of synthetic training examples from K number of clusters... the probability of selecting Di from clusterk is proportional to ck ∀Di ∈ clusterk"
  - [section] "di = cosine(vDi, 1/ck ckX j=1 vDj )"
  - [corpus] Weak: No ablation of sampling strategy; performance gain attributed to clustering+sampling without isolating sampling effect.
- Break condition: If temperature is too low, sampling becomes deterministic and loses diversity; too high, and irrelevant documents are included.

### Mechanism 3
- Claim: In-domain few-shot prompting of LLM generates queries that better reflect the target domain than out-of-domain MS-MARCO examples.
- Mechanism: LLM is prompted with 3 in-domain (query, document) pairs and asked to generate queries for new documents, leveraging in-context learning to adapt style and terminology.
- Core assumption: Few-shot examples sufficiently bias the LLM toward the target domain's language patterns without overfitting to idiosyncrasies.
- Evidence anchors:
  - [section] "Our contribution lies in showing that the in-domain few-shot examples (query-document pairs) help to achieve high-quality of queries compared to out-of-domain generic MS-MARCO examples."
  - [section] "On each domain, we create a handful (e.g., 3) human generated queries for the few-shot example documents"
  - [corpus] Weak: No ablation of prompt source (MS-MARCO vs in-domain) shown in corpus; claim based on author observation.
- Break condition: If in-domain examples are unrepresentative or too few, the LLM may revert to generic patterns or produce poor-quality queries.

## Foundational Learning

- Concept: Clustering as unsupervised representation learning
  - Why needed here: To partition the document space into coherent topical groups before sampling, avoiding random sampling bias
  - Quick check question: If K=1 (no clustering), how does the sampling strategy change and what impact might that have on domain coverage?

- Concept: Softmax-based probabilistic sampling
  - Why needed here: To balance between selecting highly representative documents and maintaining diversity across clusters
  - Quick check question: What happens to the distribution of selected documents if temperature T→0 or T→∞?

- Concept: In-context learning with few-shot prompts
  - Why needed here: To steer LLM generation toward the target domain's query style without full fine-tuning
  - Quick check question: If you replace 3 in-domain examples with 3 MS-MARCO examples, what qualitative changes would you expect in generated queries?

## Architecture Onboarding

- Component map: Document encoder (Contriever) → Vector store → K-Means clustering → Cluster sampling (softmax+MMR) → Query generator (LLaMA-2-7B) → Hard negative mining → Fine-tuner (MonoT5/ColBERT)
- Critical path: Encoding → Clustering → Sampling → Query generation → Training
- Design tradeoffs:
  - Cluster count K: Higher K → more granular topical coverage but more clusters to sample from; lower K → broader clusters but risk of mixing unrelated topics
  - Sample size N: Larger N → more training data but higher cost and risk of overfitting; smaller N → cheaper but may miss rare patterns
  - Prompt strategy: More in-domain examples → better domain fit but higher labeling cost; fewer examples → cheaper but potentially lower quality
- Failure signatures:
  - Low nDCG gains: Check if clusters are too coarse, sampling is too narrow, or LLM prompts are ineffective
  - Overfitting: Too many synthetic examples relative to N; insufficient diversity in sampling
  - Degraded zero-shot performance: Too aggressive fine-tuning; catastrophic forgetting
- First 3 experiments:
  1. Vary K (e.g., 100, 500, 1000, 2000) and measure impact on downstream nDCG; expect optimal K around 1000
  2. Compare in-domain vs MS-MARCO few-shot prompts for query generation; expect in-domain to outperform
  3. Test sampling without MMR diversity step; expect slight performance drop, confirming MMR's role in robustness

## Open Questions the Paper Calls Out

What are the limitations of DUQGen in terms of clustering and query generation?

- Question:
- Basis in paper: [inferred]
- Why unresolved: The paper discusses the use of clustering and query generation in DUQGen, but it does not explicitly address the limitations of these components.
- What evidence would resolve it: A detailed analysis of the limitations of clustering and query generation in DUQGen, including potential issues such as scalability, robustness, and sensitivity to hyperparameters.

How does DUQGen compare to other unsupervised domain adaptation methods in terms of performance and efficiency?

- Question:
- Basis in paper: [inferred]
- Why unresolved: The paper compares DUQGen to several baseline methods, but it does not provide a comprehensive comparison of DUQGen to all existing unsupervised domain adaptation methods.
- What evidence would resolve it: A systematic evaluation of DUQGen against a wide range of unsupervised domain adaptation methods, including both neural and non-neural approaches.

What are the potential ethical considerations and risks associated with using DUQGen for unsupervised domain adaptation?

- Question:
- Basis in paper: [explicit]
- Why unresolved: The paper mentions ethical considerations and risks, but it does not provide a detailed discussion of the potential implications of using DUQGen in real-world applications.
- What evidence would resolve it: A thorough analysis of the ethical considerations and risks associated with using DUQGen, including potential issues such as bias, fairness, and transparency.

## Limitations
- No quantitative clustering quality metrics reported to validate that clusters are meaningful representations of the domain
- Lacks ablation study comparing in-domain vs out-of-domain few-shot prompts to isolate the impact of prompt source
- Only compared against zero-shot and two other UDA methods, missing comparison to semi-supervised or in-domain fine-tuning approaches

## Confidence
- **High Confidence**: The claim that DUQGen outperforms zero-shot baselines on 16 out of 18 datasets is well-supported by the experimental results.
- **Medium Confidence**: The assertion that in-domain few-shot prompting is the key differentiator from out-of-domain MS-MARCO examples lacks direct ablation in the corpus.
- **Medium Confidence**: The effectiveness of the clustering and sampling strategy is demonstrated, but without isolated ablations of each component, the relative contribution of clustering vs sampling is uncertain.

## Next Checks
1. **Ablation of Clustering Quality**: Measure and report clustering coherence metrics (e.g., average intra-cluster cosine similarity) to validate that clusters are meaningful representations of the domain.
2. **Prompt Source Comparison**: Conduct an ablation study comparing in-domain few-shot prompts against out-of-domain MS-MARCO prompts to isolate the impact of prompt source on query quality.
3. **Scaling Study**: Test the method with varying cluster counts (K) and sample sizes (N) to identify optimal configurations and assess robustness to parameter changes.