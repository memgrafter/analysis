---
ver: rpa2
title: 'FairTargetSim: An Interactive Simulator for Understanding and Explaining the
  Fairness Effects of Target Variable Definition'
arxiv_id: '2403.06031'
source_url: https://arxiv.org/abs/2403.06031
tags:
- target
- variable
- fairness
- definition
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FairTargetSim addresses the fairness implications of target variable
  definition in machine learning by providing an interactive simulator that allows
  users to define and compare different target variables using real-world psychometric
  test data. The simulator trains two models based on user-defined target variables
  and provides visualizations of how these definitions impact fairness metrics and
  overall performance across different demographic groups.
---

# FairTargetSim: An Interactive Simulator for Understanding and Explaining the Fairness Effects of Target Variable Definition

## Quick Facts
- arXiv ID: 2403.06031
- Source URL: https://arxiv.org/abs/2403.06031
- Authors: Dalia Gala; Milo Phillips-Brown; Naman Goel; Carinal Prunkl; Laura Alvarez Jubete; medb corcoran; Ray Eitel-Porter
- Reference count: 8
- Primary result: Interactive simulator demonstrating how target variable definition in ML hiring algorithms affects fairness outcomes across demographic groups

## Executive Summary
FairTargetSim is an interactive web-based simulator that allows users to define and compare different target variables for machine learning hiring algorithms. Using real psychometric test data, the simulator trains two models based on user-defined weightings of cognitive traits and visualizes how these definitions impact fairness metrics across demographic groups. The tool aims to bridge the gap between technical and non-technical stakeholders in understanding how target variable definition is a value-laden process with significant fairness implications.

## Method Summary
The simulator uses real-world psychometric test data containing cognitive trait scores (memory, reasoning, attention, processing speed, planning) for 1,000 individuals with demographic information. Users define two target variables through sliders that weight the importance of each cognitive trait in determining "good employees." The system calculates weighted average scores, applies a percentile threshold (top 50%) to create binary labels, and trains SVM models for each target definition. The simulator then computes and visualizes fairness metrics (demographic parity, equal opportunity, predictive value parity) and selection rates across demographic groups, displaying results through interactive charts and comparisons.

## Key Results
- Different target variable definitions lead to significant variations in fairness metrics across demographic groups
- Interactive visualization reveals intersectional impacts of target definition on selection rates for different demographic categories
- The simulator successfully demonstrates that target variable definition is a value-laden process with profound implications for algorithmic fairness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: User-defined target variables directly control model behavior and fairness outcomes
- Mechanism: The simulator trains two models based on different weighted combinations of cognitive traits (memory, reasoning, attention, etc.), where the weights are set by user sliders. These weights determine which applicants are classified as "good employees" through a percentile-based selection process
- Core assumption: Cognitive trait weightings can be meaningfully mapped to real-world hiring target definitions through linear combinations and percentile thresholds
- Evidence anchors:
  - [abstract] "the simulator trains two models based on user-defined target variables and provides visualizations of how these definitions impact fairness metrics"
  - [section] "FTS invites the user to imagine that they are building a hiring algorithm... The user defines two target variables, using real-world psychometric test data"
  - [corpus] No direct corpus evidence; weak signal (0.517 average FMR)

### Mechanism 2
- Claim: Interactive visualization reveals intersectional fairness impacts across demographic groups
- Mechanism: The simulator displays multiple visualizations comparing demographic distributions (gender, education, age, nationality) between the two models, showing how different target definitions lead to different selection rates and fairness metric outcomes
- Core assumption: Demographic group labels in the dataset are accurate and complete enough to reveal meaningful fairness patterns
- Evidence anchors:
  - [abstract] "different target variable definitions can lead to significant differences in fairness outcomes"
  - [section] "charts as in Figure 2 show how models A and B differ in, e.g. the proportions of selected applicants across demographic groups"
  - [corpus] No direct corpus evidence; weak signal (0.517 average FMR)

### Mechanism 3
- Claim: The simulator bridges technical and non-technical understanding of algorithmic fairness
- Mechanism: By using a concrete hiring scenario with real psychometric data and intuitive sliders, the tool makes abstract fairness concepts tangible for non-technical stakeholders while providing technical depth for developers
- Core assumption: The hiring scenario is universally understandable and the cognitive test data is sufficiently interpretable
- Evidence anchors:
  - [abstract] "can be used by algorithm developers, non-technical stakeholders, researchers, and educators"
  - [section] "the user defines two target variables, using sliders depicted in Figure 1, how important the five cognitive traits are to what makes for a 'good employee'"
  - [corpus] No direct corpus evidence; weak signal (0.517 average FMR)

## Foundational Learning

- Concept: Target variable definition in machine learning
  - Why needed here: The entire simulator is built around how defining "good employee" numerically affects fairness outcomes
  - Quick check question: What is the difference between a target variable and a feature in a machine learning model?

- Concept: Fairness metrics in algorithmic systems
  - Why needed here: The simulator visualizes multiple fairness metrics (true/false positive rates, predictive values) across demographic groups
  - Quick check question: What is the difference between demographic parity and equal opportunity in fairness definitions?

- Concept: Support Vector Machine (SVM) classification
  - Why needed here: The simulator uses SVM models to classify applicants based on weighted cognitive trait scores
  - Quick check question: How does an SVM find the decision boundary between positive and negative classes?

## Architecture Onboarding

- Component map: Frontend interface (HTML/CSS/JavaScript) with slider controls -> Backend Python processing -> SVM model training with scikit-learn -> Visualization generation (matplotlib/seaborn) -> Static site deployment
- Critical path: User defines target variables -> Slider values processed -> Weighted average scores calculated -> Percentile-based labeling -> SVM training -> Fairness metrics computed -> Visualizations generated -> Results displayed
- Design tradeoffs: Interactive sliders vs. complex real-world definitions; simplified percentile method vs. more nuanced scoring; limited demographic categories vs. comprehensive intersectional analysis
- Failure signatures: Sliders not responding -> JavaScript error; no visualizations appearing -> Python backend failure; demographic charts showing zero values -> Data loading issue; SVM training taking too long -> Model complexity too high
- First 3 experiments:
  1. Set all cognitive trait sliders to equal weight (1.0) for both models and verify similar demographic distributions
  2. Maximize reasoning and attention for Model A, maximize memory and processing speed for Model B, then compare fairness metric differences
  3. Create extreme weightings (one trait at 10.0, others at 0.1) to observe how single-trait focus affects fairness outcomes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the long-term impacts of using FairTargetSim on developers' and stakeholders' decision-making processes in real-world algorithmic hiring systems?
- Basis in paper: [inferred] The paper mentions that FTS could be used for human-centered research and user studies to test how it affects how stakeholders think about, develop, and use algorithms.
- Why unresolved: The paper only describes the potential for future research through user studies, but does not present any empirical data on how FTS actually influences decision-making behaviors or outcomes over time.
- What evidence would resolve it: Longitudinal studies tracking how users who interact with FTS subsequently approach target variable definition in their actual algorithmic development work, including whether they demonstrate more careful consideration of fairness implications.

### Open Question 2
- Question: How do different weighting schemes for combining multiple stakeholders' judgments (e.g., various managers' opinions on "good" employees) affect the resulting fairness metrics and model performance?
- Basis in paper: [explicit] The paper discusses how FTS could be extended to real-world settings where employers identify current employees as "good," and mentions that these judgments can be weighted in different ways resulting in different target variables.
- Why unresolved: While the paper suggests this as a potential application, it does not investigate how different weighting schemes for stakeholder input would quantitatively affect fairness outcomes or model performance.
- What evidence would resolve it: Empirical comparison of multiple weighting schemes applied to the same stakeholder judgments, measuring the resulting differences in fairness metrics across demographic groups and overall model performance.

### Open Question 3
- Question: How does the performance of FTS-generated models compare to real-world hiring algorithms that use psychometric test data when evaluated on the same fairness metrics?
- Basis in paper: [inferred] The paper states that FTS's models are similar to real-world systems in using support vector machine models and psychometric test data, but differ in how target variables are defined.
- Why unresolved: The paper does not provide a direct comparison between FTS-generated models and actual deployed hiring algorithms, leaving uncertainty about how well FTS's simulations reflect real-world algorithmic behavior.
- What evidence would resolve it: Side-by-side evaluation of FTS-generated models and real-world hiring algorithms on the same datasets, comparing their performance across identical fairness metrics and demographic groups.

## Limitations

- The mapping between abstract cognitive trait weightings and meaningful real-world hiring definitions remains uncertain and may not reflect actual job performance assessment
- Demographic group labels may not capture full intersectional complexity, potentially limiting the tool's ability to reveal nuanced fairness impacts
- The simulator's reliance on a single dataset and SVM classification model restricts generalizability to other contexts and model types

## Confidence

- High Confidence: The demonstration that target variable definition affects fairness metrics and selection rates across demographic groups
- Medium Confidence: The claim that target variable definition is a value-laden process with profound implications for fairness
- Low Confidence: The assertion that the simulator successfully bridges technical and non-technical understanding

## Next Checks

1. **Dataset Validation Study**: Conduct a comprehensive analysis of the psychometric test dataset to verify demographic group completeness, assess missing data patterns, and evaluate whether the cognitive trait measurements are job-relevant and unbiased.

2. **Real-World Mapping Experiment**: Recruit HR professionals to map their actual hiring criteria to the cognitive trait weightings, then compare their target definitions with the simulator's outputs to assess ecological validity.

3. **User Study Implementation**: Design and execute a controlled experiment with both technical and non-technical participants to measure learning outcomes, usability, and whether the simulator actually improves understanding of target variable fairness implications.