---
ver: rpa2
title: Training Hamiltonian neural networks without backpropagation
arxiv_id: '2411.17511'
source_url: https://arxiv.org/abs/2411.17511
tags:
- hamiltonian
- networks
- neural
- swim
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a backpropagation-free method for training Hamiltonian
  neural networks using data-agnostic and data-driven sampling schemes. The method
  avoids iterative gradient-based optimization by sampling network parameters directly
  and solving a linear least-squares problem for the final layer.
---

# Training Hamiltonian neural networks without backpropagation

## Quick Facts
- arXiv ID: 2411.17511
- Source URL: https://arxiv.org/abs/2411.17511
- Authors: Atamert Rahma; Chinmay Datar; Felix Dietrich
- Reference count: 40
- Primary result: Backpropagation-free training of Hamiltonian neural networks achieves 100x speedup and up to 10,000x better accuracy on chaotic systems using data-driven parameter sampling

## Executive Summary
This work introduces a backpropagation-free method for training Hamiltonian neural networks by combining Extreme Learning Machine (ELM) with data-driven sampling schemes. The approach eliminates iterative gradient-based optimization by sampling network parameters directly and solving a linear least-squares problem for the final layer. Three sampling strategies are presented: data-agnostic ELM, data-driven SWIM, and approximate-SWIM for unsupervised settings. The method demonstrates significant computational speedups (100x faster on CPU) while achieving superior accuracy, particularly for systems with steep gradients or large input domains.

## Method Summary
The method constructs neural networks for approximating Hamiltonian systems without backpropagation by first sampling hidden layer parameters using either data-agnostic (ELM) or data-driven (SWIM) approaches, then solving a linear least-squares problem to determine the final layer weights. The Hamiltonian constraint is enforced by matching the network's gradient to the symplectic matrix times the time derivative, which translates to a single linear system solve. For unsupervised settings, approximate-SWIM uses an initial approximation to drive resampling without requiring true function values.

## Key Results
- 100x faster training time compared to traditional gradient-based methods on CPU
- Up to 10,000x better accuracy on Hénon-Heiles chaotic system
- SWIM outperforms ELM for functions with steep gradients or wide input domains
- Approximate-SWIM achieves competitive accuracy to true SWIM using only approximate function values
- Computational cost scales linearly with number of data points

## Why This Works (Mechanism)

### Mechanism 1
Data-driven parameter sampling (SWIM) outperforms data-agnostic sampling (ELM) when the target Hamiltonian has steep gradients or large input domains. SWIM constructs network weights and biases from input pairs sampled with density proportional to the gradient magnitude of the target function, placing more basis functions where the function changes rapidly. This results in better approximation of high-curvature regions without requiring iterative optimization.

### Mechanism 2
Solving a linear least-squares problem for the final layer eliminates the need for backpropagation through the integrator while still enforcing Hamiltonian dynamics. After sampling hidden layer parameters, the gradient of the network output with respect to input is analytically computed. This gradient is then matched to the symplectic matrix times the time derivative via a linear system, which is solved exactly once.

### Mechanism 3
Approximate-SWIM (A-SWIM) can achieve near-SWIM accuracy using only approximate Hamiltonian values for resampling, without requiring true function values. A-SWIM first uses uniform sampling (U-SWIM) to get an initial approximation, then uses the approximate function values from this initial model to drive the SWIM resampling. This allows data-driven placement of basis functions in the unsupervised setting.

## Foundational Learning

- Concept: Symplectic structure of Hamiltonian systems
  - Why needed here: The method enforces Hamiltonian dynamics by matching the network gradient to J^(-1)v, which relies on the symplectic structure (J matrix) to preserve energy-like quantities
  - Quick check question: What matrix structure ensures that Hamiltonian flow preserves phase space volume?

- Concept: Extreme Learning Machine (ELM) random feature approximation
  - Why needed here: ELM provides the baseline data-agnostic sampling method, showing how randomly sampled parameters can still approximate functions, but highlighting the need for data-driven sampling in challenging regimes
  - Quick check question: How does ELM ensure universal approximation capability with random hidden layer parameters?

- Concept: Linear least-squares optimization
  - Why needed here: The final layer parameters are determined by solving a linear system that enforces the Hamiltonian constraint, which is computationally efficient and avoids iterative optimization
  - Quick check question: What conditions make the least-squares problem well-posed when solving for the final layer?

## Architecture Onboarding

- Component map: Input layer (q, p) -> Hidden layers (SAMPLED) -> Output layer (linear) -> Sampling module -> Linear solver
- Critical path:
  1. Sample hidden layer parameters (data-agnostic or data-driven)
  2. Compute analytical gradients of hidden layer outputs
  3. Set up and solve linear least-squares system for final layer
  4. (For A-SWIM) Resample using approximate function values and repeat steps 2-3
- Design tradeoffs:
  - Sampling quality vs. computational cost: SWIM is more expensive than ELM but more accurate for challenging functions
  - Single vs. multiple resampling passes: A-SWIM trades additional computation for improved accuracy without requiring true function values
  - Network width vs. approximation accuracy: Wider networks can compensate for poor sampling but increase computational cost
- Failure signatures:
  - Poor gradient matching in test set: Indicates ill-conditioned linear system or inadequate sampling
  - Large error in regions of high gradient: Suggests insufficient basis functions in those regions (ELM limitation)
  - Degraded performance with A-SWIM: May indicate initial approximation too poor to guide resampling
- First 3 experiments:
  1. Implement ELM sampling with single pendulum Hamiltonian on domain [-2π, 2π] × [-1, 1] and verify accuracy vs. network width
  2. Add U-SWIM sampling to same experiment and compare accuracy, especially in high-gradient regions
  3. Implement A-SWIM and verify it can match SWIM accuracy using only approximate function values for resampling

## Open Questions the Paper Calls Out

### Open Question 1
Can the SWIM sampling method be extended to higher-dimensional Hamiltonian systems beyond the two-dimensional cases studied in this paper? The authors note that their approach requires solving a large linear system and suggest that "in higher dimensional examples that require a lot of computational requirements, one can rely on HPC resources and iterative solvers." This remains unresolved as the paper only demonstrates the method on two-dimensional systems (single pendulum, Lotka-Volterra, double pendulum, Hénon-Heiles).

### Open Question 2
How does the method perform when trained on noisy trajectory data rather than exact Hamiltonian values? The authors state "Another important direction is to extend our algorithm to handle noisy data." All experiments in the paper use exact data without noise, leaving questions about robustness to measurement errors or discretization noise.

### Open Question 3
Can the approximate-SWIM (A-SWIM) method be improved to match the accuracy of true-SWIM without requiring double the training time? The authors note that "if the initial approximation with (U-SWIM) is sufficiently accurate, A-SWIM can closely match the SWIM method's performance" but requires "approximately double the training time." The paper does not explore alternative initialization strategies or optimization techniques to reduce this computational overhead.

## Limitations

- Scalability to higher dimensions remains untested, with computational complexity concerns for large systems
- Sensitivity to noise in training data has not been evaluated, limiting real-world applicability
- Approximate-SWIM requires approximately double the training time compared to direct SWIM methods

## Confidence

- Mechanism 1 (SWIM advantage): Medium - Strong empirical support for tested systems, but limited theoretical analysis of why sampling outperforms gradient descent in all cases
- Mechanism 2 (Linear solver): High - Well-established linear algebra, directly implemented from equations
- Mechanism 3 (A-SWIM): Medium - Demonstrated in experiments but with limited ablation studies on initialization quality requirements

## Next Checks

1. Test A-SWIM with deliberately poor initializations to quantify the minimum accuracy threshold needed for effective resampling
2. Evaluate performance on a 10+ dimensional Hamiltonian system to assess scalability limits
3. Compare computational cost breakdown (sampling vs. linear solve) across different network widths to identify crossover points with gradient-based training