---
ver: rpa2
title: 'Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities
  Using Situational Context'
arxiv_id: '2412.16359'
source_url: https://arxiv.org/abs/2412.16359
tags:
- adversarial
- attack
- step
- human-readable
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that human-readable adversarial prompts
  exploiting situational context can effectively bypass safety mechanisms in large
  language models. The authors developed a novel attack method using movie scenarios
  combined with human-readable adversarial insertions that appear natural to humans
  but trigger harmful responses from models like GPT-3.5-Turbo-0125 and Gemma-7b.
---

# Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context

## Quick Facts
- arXiv ID: 2412.16359
- Source URL: https://arxiv.org/abs/2412.16359
- Reference count: 33
- Human-readable adversarial prompts exploiting situational context can effectively bypass LLM safety mechanisms

## Executive Summary
This paper demonstrates that human-readable adversarial prompts exploiting situational context can effectively bypass safety mechanisms in large language models. The authors developed a novel attack method using movie scenarios combined with human-readable adversarial insertions that appear natural to humans but trigger harmful responses from models like GPT-3.5-Turbo-0125 and Gemma-7b. Their approach transforms nonsensical adversarial suffixes into innocuous-looking text using AdvPrompter with p-nucleus sampling, achieving significantly higher attack success rates than previous methods. Experiments show Gemma-7b generated 65 harmful responses (score 5/5) and GPT-3.5-Turbo-0125 showed particular vulnerability to war-themed prompts, with attacks succeeding after multiple attempts.

## Method Summary
The research developed a novel adversarial attack methodology that transforms nonsensical adversarial suffixes into human-readable text using AdvPrompter with p-nucleus sampling. The method combines malicious prompts with situational context from movie scripts and human-readable adversarial insertions to create full-prompts that appear natural but trigger harmful responses. The framework employs few-shot chain-of-thought (FS-CoT) prompting to transfer successful attacks across different LLM architectures. Evaluation uses GPT-4o-mini Judge to score harmfulness of LLM responses on a 1-5 scale, comparing results against traditional nonsensical adversarial attacks.

## Key Results
- Gemma-7b generated 65 harmful responses (score 5/5) using human-readable adversarial prompts
- GPT-3.5-Turbo-0125 showed particular vulnerability to war-themed prompts, with attacks succeeding after multiple attempts
- Human evaluation confirmed GPT-4o-mini tended to overrate harmfulness compared to human judges
- AdvPrompter with p-nucleus sampling achieved significantly higher attack success rates than traditional nonsensical adversarial suffixes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-readable adversarial insertions exploit the semantic understanding capabilities of LLMs to bypass safety mechanisms.
- Mechanism: The research transforms nonsensical adversarial suffixes into natural language text using AdvPrompter with p-nucleus sampling. This creates prompts that appear benign to human reviewers but trigger harmful responses when processed by LLMs.
- Core assumption: LLMs prioritize semantic understanding over syntactic correctness, making them vulnerable to attacks that appear natural but contain hidden adversarial elements.
- Evidence anchors:
  - [abstract] "Unlike obvious nonsensical text strings that safety systems can easily catch, our work reveals that human-readable situation-driven adversarial full-prompts that leverage situational context are effective but much harder to detect."
  - [section] "Our research overcomes these limitations with an innovative optimization-free strategy that significantly simplifies the creation of deceptive adversarial prompts."
- Break condition: The attack fails when safety mechanisms evolve to detect subtle semantic manipulation patterns or when models are trained to recognize contextually inappropriate responses within seemingly benign prompts.

### Mechanism 2
- Claim: Situational context from movie scripts provides a framework that makes malicious prompts appear contextually appropriate.
- Mechanism: The research uses movie overviews as situational contexts, combining them with human-readable adversarial insertions and malicious prompts. This creates full-prompts that appear as normal movie-related queries but contain harmful instructions.
- Core assumption: LLMs have extensive training data including movie content, making them comfortable engaging with familiar cultural touchpoints that can be manipulated.
- Evidence anchors:
  - [abstract] "We developed attacks that use movie scripts as situational contextual frameworks, creating natural-looking full-prompts that trick LLMs into generating harmful content."
  - [section] "First, LLMs have been trained on countless movie references, making them comfortable engaging with these familiar cultural touchpoints."
- Break condition: The attack fails when models develop stronger context-awareness that can distinguish between genuine movie-related queries and malicious ones exploiting situational context.

### Mechanism 3
- Claim: P-nucleus sampling in AdvPrompter generates diverse human-readable adversarial text that maintains adversarial properties while appearing more natural.
- Mechanism: P-nucleus sampling dynamically adjusts the sampling scope based on the model's confidence level for each position, selecting from probable tokens rather than always choosing the most likely ones. This creates varied, coherent text that preserves adversarial properties.
- Core assumption: More natural-sounding text is harder for both automated systems and human reviewers to detect as adversarial content.
- Evidence anchors:
  - [section] "P-nucleus sampling (also known as nucleus sampling or top-p sampling), as mentioned earlier, is integrated into AdvPrompter to enhance output diversity by intelligently selecting from a range of probable tokens instead of always choosing the most likely ones."
  - [section] "This approach enables more natural and varied human-readable adversarial insertions, making them more effective at triggering vulnerabilities in target LLMs."
- Break condition: The attack fails when safety mechanisms can detect the subtle patterns introduced by nucleus sampling or when models become resistant to attacks using diverse token selection strategies.

## Foundational Learning

- Concept: Adversarial attack methodology in NLP
  - Why needed here: The paper builds on existing adversarial attack frameworks but extends them to human-readable formats, requiring understanding of both traditional attack approaches and their limitations.
  - Quick check question: What distinguishes human-readable adversarial attacks from traditional nonsensical adversarial attacks in terms of detectability and effectiveness?

- Concept: Few-shot chain-of-thought (FS-CoT) prompting
  - Why needed here: The research uses FS-CoT to transfer successful attacks across different LLM architectures, leveraging in-context learning capabilities.
  - Quick check question: How does few-shot prompting enhance the transferability of adversarial attacks across different LLM architectures?

- Concept: P-nucleus (top-p) sampling
  - Why needed here: The enhanced AdvPrompter framework uses p-nucleus sampling to generate diverse adversarial text that appears more natural while maintaining adversarial properties.
  - Quick check question: What is the key difference between greedy sampling and p-nucleus sampling in terms of token selection strategy?

## Architecture Onboarding

- Component map: Movie scenarios (crime, horror, war genres) -> Model-specific nonsensical adversarial suffixes -> AdvPrompter with p-nucleus sampling -> Human-readable adversarial insertions -> Full-prompt construction -> LLM response generation -> GPT-4o-mini Judge evaluation

- Critical path: Adversarial insertion generation (AdvPrompter with p-nucleus sampling) -> Full-prompt construction (combining malicious prompt, adversarial insertion, situational context) -> LLM response generation -> Harmfulness evaluation (GPT-4o-mini Judge)

- Design tradeoffs: The research balances attack effectiveness against detectability. More natural adversarial insertions are harder to detect but may be less effective at triggering harmful responses. The p-nucleus sampling approach attempts to optimize this tradeoff by generating diverse, coherent text.

- Failure signatures: Common failure modes include: (1) LLMs refusing to respond to prompts regardless of adversarial insertion, (2) Generated responses receiving low harmfulness scores, (3) Adversarial insertions failing to transfer across model architectures, and (4) Safety mechanisms detecting contextual inconsistencies.

- First 3 experiments:
  1. Test the basic full-prompt structure with a single human-readable adversarial insertion on a quantized Llama-2 7B model to verify the fundamental attack concept works.
  2. Implement FS-CoT with varying numbers of demonstrations to determine the minimum effective demonstrations needed for successful attacks on different model architectures.
  3. Compare AdvPrompter with and without p-nucleus sampling on the same set of adversarial insertions to quantify the performance improvement from diverse sampling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the success rate of human-readable adversarial attacks compare between open-source and proprietary LLMs when using the same prompt structures?
- Basis in paper: [explicit] The paper tested 10 diverse LLMs including both open-source (Llama, Gemma, Phi) and proprietary (GPT-3.5-Turbo-0125, GPT-4) models, finding that Gemma-7b and GPT-3.5-Turbo-0125 were particularly vulnerable while GPT-4-0125-preview and Mistral-7B-v0.1 were most robust.
- Why unresolved: The paper shows differential vulnerability but doesn't establish whether this is due to architectural differences, training approaches (like RLHF), or specific safety mechanisms. The comparative analysis focuses on specific models rather than systematically comparing open-source vs proprietary approaches.
- What evidence would resolve it: A controlled experiment varying only the access type (open-source vs proprietary) while keeping model size and architecture constant, or a comprehensive study of safety mechanisms across both categories.

### Open Question 2
- Question: What is the minimum number of adversarial insertions required to achieve reliable attack success across different LLM families?
- Basis in paper: [explicit] The paper tested varying numbers of demonstrations for few-shot prompting attacks, finding that different models required different minimum numbers (GPT-3.5-Turbo-0125 and Gemma-7b needed 1 demonstration, Phi-1.5 and Llama-3-8B needed 2, GPT-4 needed 10).
- Why unresolved: The paper only tested up to 10 demonstrations and focused on specific models. It's unclear whether the required number scales with model size, whether there's a diminishing returns threshold, or if certain model families are inherently more resistant to few-shot attacks.
- What evidence would resolve it: Systematic testing of demonstration counts from 1 to 50+ across a broader range of model families and sizes to identify patterns in attack success rates.

### Open Question 3
- Question: How effective are human-readable adversarial insertions when used as prefixes or suffixes rather than as insertions within the prompt template?
- Basis in paper: [inferred] The paper acknowledges that their adversarial insertion approach is "rigid because of the defined full-prompt-template where it sits as an insertion" and explicitly states they "leave the use of this human-readable adversarial expression as a prefix or suffix to future experiments."
- Why unresolved: The paper only tested adversarial insertions within a specific template structure. Prefix and suffix positions might interact differently with model context windows and attention mechanisms, potentially offering different attack vectors.
- What evidence would resolve it: Direct comparison experiments testing the same adversarial insertions in prefix, suffix, and insertion positions across multiple model families to measure attack success rate differences.

## Limitations

- Model-specific vulnerability patterns vary significantly across architectures without clear explanation of root causes
- Judge reliability concerns arise from systematic differences between GPT-4o-mini automated evaluation and human assessment
- Transferability boundaries for adversarial insertions across model families are not well-defined

## Confidence

**High Confidence**: The core finding that human-readable adversarial prompts exploiting situational context can effectively bypass LLM safety mechanisms is well-supported by experimental results.

**Medium Confidence**: The effectiveness of p-nucleus sampling in AdvPrompter for generating diverse, natural-sounding adversarial text is supported by results but could benefit from more rigorous ablation studies.

**Low Confidence**: Claims about why certain model architectures show different vulnerability patterns lack sufficient empirical support.

## Next Checks

1. Conduct comprehensive human evaluation of a stratified sample of 100 adversarial prompts across all tested model architectures, comparing human harmfulness ratings against GPT-4o-mini Judge scores to establish ground truth and calibrate the automated evaluation system.

2. Systematically test the same set of human-readable adversarial insertions across a broader range of model architectures (including open-weight models of varying sizes) while collecting detailed model-specific metadata (training data characteristics, safety fine-tuning approaches) to identify patterns in vulnerability factors.

3. Create a systematic grid of adversarial insertions tested across all model pairs to map the precise boundaries of successful transfer, identifying which prompt characteristics enable cross-model effectiveness and developing predictive models for transferability success.