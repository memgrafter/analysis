---
ver: rpa2
title: 'AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free Information
  Retrieval'
arxiv_id: '2404.06004'
source_url: https://arxiv.org/abs/2404.06004
tags:
- search
- diskann
- node
- aisaq
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AiSAQ addresses the high memory usage of DiskANN for billion-scale
  ANNS by offloading compressed vectors to SSD storage instead of keeping them in
  DRAM. The method stores product-quantized vectors directly in node chunks on storage,
  reducing memory usage to ~10 MB regardless of dataset size.
---

# AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free Information Retrieval

## Quick Facts
- arXiv ID: 2404.06004
- Source URL: https://arxiv.org/abs/2404.06004
- Reference count: 14
- Primary result: Achieves >95% recall@1 with millisecond latency while using 2-3 orders of magnitude less memory than DiskANN

## Executive Summary
AiSAQ introduces a DRAM-free approach to billion-scale approximate nearest neighbor search (ANNS) by storing product-quantized vectors directly on SSD storage instead of keeping them in DRAM. The method reduces memory usage to approximately 10 MB regardless of dataset size while maintaining high recall and millisecond-order latency. This enables flexible deployment of multiple large-scale indices for retrieval-augmented generation applications without the memory constraints of traditional approaches like DiskANN.

## Method Summary
AiSAQ stores product-quantized vectors in node chunks directly on SSD storage, eliminating the need to keep full-precision vectors in DRAM. The approach leverages the sequential read bandwidth of SSDs (up to 1TB/s) to decompress and access compressed vectors during search operations. This DRAM-free design allows for rapid index switching between multiple billion-scale datasets and reduces index load time from seconds to milliseconds. The method maintains the graph topology and high recall characteristics of DiskANN while dramatically reducing memory requirements.

## Key Results
- Achieves >95% recall@1 with millisecond-order latency
- Uses 2-3 orders of magnitude less memory than DiskANN (approximately 10 MB)
- Reduces index load time from seconds to milliseconds
- Maintains graph topology and high recall while enabling DRAM-free operation

## Why This Works (Mechanism)
AiSAQ exploits the high sequential read bandwidth of modern SSDs to overcome the bottleneck of accessing compressed vectors stored on disk. By using product quantization to compress vectors and storing them directly in node chunks, the system can decompress vectors during search operations without keeping full-precision vectors in DRAM. The SSD's sequential read capability (up to 1TB/s) is leveraged to maintain search performance despite the additional decompression overhead.

## Foundational Learning
- **Product Quantization**: Why needed - reduces vector storage size by 16-64x; Quick check - verify compression ratio matches expected values for dataset dimensionality
- **Disk-based ANNS**: Why needed - enables billion-scale indexing without DRAM constraints; Quick check - confirm SSD sequential read bandwidth meets requirements
- **Graph-based search topology**: Why needed - maintains high recall while enabling fast search; Quick check - validate graph connectivity and search quality
- **Sequential vs random SSD access**: Why needed - sequential reads provide 100x better bandwidth; Quick check - measure actual SSD sequential read performance
- **Vector decompression overhead**: Why needed - determines search latency; Quick check - benchmark decompression time per vector

## Architecture Onboarding

**Component Map:**
SSD storage -> Product quantizer -> Graph nodes -> Search algorithm -> DRAM buffer

**Critical Path:**
1. SSD sequential read of compressed node chunks
2. Decompression of product-quantized vectors
3. Graph traversal using decompressed vectors
4. Result aggregation and ranking

**Design Tradeoffs:**
- Memory vs latency: DRAM-free operation increases search latency due to SSD access and decompression
- Compression ratio vs recall: Higher compression reduces memory but may impact search quality
- SSD sequential bandwidth vs random access: Sequential reads are essential for maintaining performance

**Failure Signatures:**
- Low recall indicates insufficient SSD sequential bandwidth or excessive compression
- High latency suggests SSD bottlenecks or inefficient decompression
- Memory usage spikes indicate failure to properly implement DRAM-free storage

**First 3 Experiments:**
1. Measure SSD sequential read bandwidth with typical node chunk sizes
2. Benchmark product quantization compression ratio and decompression speed
3. Test recall@1 and latency with varying levels of vector compression

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Performance heavily dependent on SSD sequential read bandwidth (1TB/s demonstrated)
- Real-world SSD performance varies significantly based on drive type and workload
- Evaluation focuses on billion-scale datasets; scalability to larger datasets uncertain
- Millisecond latency claims require sufficient SSD sequential read bandwidth

## Confidence
- High confidence: DRAM-free operation and ~10MB memory usage claims
- Medium confidence: Latency claims due to dependency on specific SSD characteristics
- Medium confidence: Recall@1 claims as they are demonstrated but may vary with hardware configurations

## Next Checks
1. Test AiSAQ performance across different SSD types (QLC vs TLC vs SLC) to verify latency claims under varying sequential read bandwidths
2. Evaluate recall@1 and latency scaling behavior with datasets significantly larger than 1 billion vectors (e.g., 10-100 billion)
3. Measure impact of different vector dimensionalities on compression ratios and search quality to establish generalizability beyond the 96D case studied