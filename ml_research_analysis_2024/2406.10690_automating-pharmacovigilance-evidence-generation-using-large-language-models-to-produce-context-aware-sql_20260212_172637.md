---
ver: rpa2
title: 'Automating Pharmacovigilance Evidence Generation: Using Large Language Models
  to Produce Context-Aware SQL'
arxiv_id: '2406.10690'
source_url: https://arxiv.org/abs/2406.10690
tags:
- queries
- document
- business
- context
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating accurate SQL queries
  from natural language in complex pharmacovigilance databases. It proposes using
  a Large Language Model (LLM) enhanced with a business context document within a
  retrieval-augmented generation (RAG) framework.
---

# Automating Pharmacovigilance Evidence Generation: Using Large Language Models to Produce Context-Aware SQL

## Quick Facts
- arXiv ID: 2406.10690
- Source URL: https://arxiv.org/abs/2406.10690
- Reference count: 23
- Primary result: RAG framework with business context document increases NLQ-to-SQL accuracy from 8.3% to 78.3%

## Executive Summary
This paper addresses the challenge of generating accurate SQL queries from natural language in complex pharmacovigilance databases. The authors propose using a Large Language Model (LLM) enhanced with a business context document within a retrieval-augmented generation (RAG) framework. This approach significantly improves the accuracy of converting natural language queries into SQL, demonstrating the critical value of domain-specific contextual knowledge. The integration of contextual knowledge proves especially valuable for complex queries, showing that business context documents can bridge the gap between technical database structures and real-world business requirements.

## Method Summary
The authors employed OpenAI's GPT-4 model within a RAG framework to convert natural language queries (NLQs) into SQL. The system used both a comprehensive database schema (290-page PDF) and a business context document containing 36 key table definitions with domain-specific knowledge. A vector-based retrieval strategy using text-embedding-ada-002 embeddings was employed to retrieve relevant context from the business document. The approach was tested on 60 NLQs from historical logs, comparing performance with schema-only versus schema-plus-business-context. A complexity scoring algorithm objectively measured SQL query difficulty based on factors like table count, joins, and clauses.

## Key Results
- NLQ-to-SQL accuracy increased from 8.3% with database schema alone to 78.3% with business context document
- The business context document significantly improved performance across all query complexity levels
- Context-aware generation proved particularly valuable for complex queries requiring domain knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Business context document significantly improves LLM's ability to generate accurate SQL queries by providing domain-specific knowledge
- Mechanism: The document bridges the gap between technical database structures and business requirements by translating complex safety rules into plain language
- Core assumption: Domain-specific contextual knowledge is more valuable than raw schema information for query generation accuracy
- Evidence anchors:
  - [abstract]: "This approach significantly improved NLQ-to-SQL accuracy, increasing from 8.3% with the database schema alone to 78.3% with the business context document."
  - [section]: "This document summarized key data structures in plain language, providing contextual insights into the relevance of the database elements."
  - [corpus]: Weak corpus evidence - no directly related papers found on business context documents for SQL generation
- Break condition: If the business context document becomes outdated or misaligned with database schema changes, accuracy would degrade

### Mechanism 2
- Claim: Retrieval-augmented generation (RAG) framework enhances LLM performance by providing relevant context during query generation
- Mechanism: Vector embeddings of the business context document are retrieved and integrated with user queries to guide SQL generation
- Core assumption: The RAG framework can effectively retrieve and utilize relevant contextual information during generation
- Evidence anchors:
  - [abstract]: "We utilized OpenAI's GPT-4 model within a retrieval-augmented generation (RAG) framework, enriched with a business context document"
  - [section]: "A vector-based retrieval strategy, utilizing embeddings from the text-embedding-ada-002 model, was employed."
  - [corpus]: Moderate corpus evidence - related papers on RAG systems for document understanding and context-aware generation
- Break condition: If retrieval quality degrades (e.g., poor embeddings, irrelevant chunks), SQL generation accuracy would suffer

### Mechanism 3
- Claim: Complexity scoring algorithm objectively measures SQL query difficulty and correlates with LLM performance
- Mechanism: The algorithm quantifies complexity based on measurable factors like table count, joins, and clauses, enabling systematic evaluation
- Core assumption: Complexity can be objectively measured through quantifiable metrics rather than subjective assessment
- Evidence anchors:
  - [section]: "We developed a scoring algorithm to objectively measure SQL query complexity, which considers factors such as the number of tables, joins, and clauses"
  - [section]: "This method offers a reproducible way to evaluate SQL complexity"
  - [corpus]: Weak corpus evidence - no directly related papers found on SQL complexity scoring algorithms
- Break condition: If the algorithm doesn't capture all aspects of query complexity (e.g., semantic difficulty, domain knowledge requirements), it may not accurately predict performance

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG enables the LLM to access and utilize external contextual knowledge during query generation, addressing the limitation of LLMs relying solely on pre-trained knowledge
  - Quick check question: How does RAG differ from simply fine-tuning an LLM on domain-specific data?

- Concept: Vector embeddings and semantic search
  - Why needed here: Vector embeddings enable efficient retrieval of relevant context from the business document based on semantic similarity to user queries
  - Quick check question: What embedding model was used in this study and why was it chosen?

- Concept: SQL query complexity measurement
  - Why needed here: Objective complexity measurement enables systematic evaluation of LLM performance across different query difficulty levels
  - Quick check question: What factors does the complexity scoring algorithm consider?

## Architecture Onboarding

- Component map: User Interface -> Embedding Service -> Vector Database -> LLM Service -> SQL Output
- Critical path: User Query → Embedding → Context Retrieval → Prompt Construction → LLM Generation → SQL Output
- Design tradeoffs:
  - Business context document vs. database schema: Contextual knowledge vs. structural completeness
  - Chunk size and overlap: Balance between retrieval granularity and context coherence
  - LLM choice: GPT-4 vs. open-source alternatives (tradeoff between performance and cost/control)
- Failure signatures:
  - High failure rate with complex queries: May indicate insufficient context or model limitations
  - Degradation over time: Could signal outdated business context document
  - Inconsistent performance: Might indicate retrieval quality issues
- First 3 experiments:
  1. Baseline test: NLQ to SQL without any context document
  2. Context integration test: Compare performance with and without business context document
  3. Schema optimization test: Test performance with essential tables only vs. full schema

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the business context document approach scale when applied to databases with different schemas and complexity levels?
- Basis in paper: [explicit] The paper acknowledges that the efficacy of the business context document was assessed with a relatively small set of NLQs within a single enterprise database, and the scalability and generalizability of the findings to other databases remain to be validated
- Why unresolved: The study was conducted using a single enterprise database with specific characteristics. The approach's effectiveness on databases with different schemas, sizes, and complexity levels is unknown
- What evidence would resolve it: Conducting similar experiments on multiple databases with varying schemas, sizes, and complexity levels, and comparing the performance of the business context document approach to baseline methods

### Open Question 2
- Question: How can the system handle ambiguous user intents and improve the accuracy of query generation for complex queries?
- Basis in paper: [explicit] The paper mentions that concerns related to scalability and implementation within large, dynamic enterprise environments highlight the need for future investigations to enhance the robustness and applicability of the methodology, particularly in handling complex queries and resolving ambiguous user intents
- Why unresolved: The study demonstrated the effectiveness of the business context document approach for various query complexities, but the system's ability to handle ambiguous user intents and improve accuracy for complex queries is not fully explored
- What evidence would resolve it: Developing and testing techniques to disambiguate user intents, such as asking clarifying questions or using additional context, and evaluating the impact on query generation accuracy for complex queries

### Open Question 3
- Question: How can the business context document be automatically generated or updated to keep pace with changes in the database schema and business rules?
- Basis in paper: [inferred] The paper mentions that updates to database schemas could impact the document's utility, and reliability should be monitored over time. The business context document was developed iteratively with input from domain experts
- Why unresolved: The study relied on a manually created business context document, which may not be feasible for large-scale or frequently changing databases. Automating the generation and updating of the document would improve its practicality and maintainability
- What evidence would resolve it: Developing and evaluating methods for automatically generating or updating the business context document based on database schema changes, business rule updates, or user feedback, and assessing the impact on query generation accuracy

## Limitations

- The business context document's content and methodology for creation remain unspecified, limiting reproducibility
- The study's scope is limited to a single pharmacovigilance database, raising questions about generalizability
- The complexity scoring algorithm may not capture all aspects of query difficulty, particularly semantic complexity

## Confidence

- High Confidence: The RAG framework with business context document significantly improves SQL generation accuracy compared to using schema alone
- Medium Confidence: The complexity scoring algorithm provides an objective measure of query difficulty
- Medium Confidence: The approach generalizes to other pharmacovigilance databases

## Next Checks

1. Apply the same RAG approach to a different pharmacovigilance database with distinct schema and business rules to validate generalizability beyond the original system

2. Simulate schema changes and outdated business context by deliberately introducing discrepancies between the context document and database schema, then measure degradation in performance to establish robustness limits

3. Conduct expert review comparing the algorithmic complexity scores against human assessments of query difficulty across a diverse set of SQL queries to validate the scoring methodology's accuracy and completeness