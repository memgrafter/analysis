---
ver: rpa2
title: Towards Deep Active Learning in Avian Bioacoustics
arxiv_id: '2406.18621'
source_url: https://arxiv.org/abs/2406.18621
tags:
- learning
- deep
- avian
- bird
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces deep active learning (AL) to avian bioacoustics,\
  \ addressing the challenge of adapting deep learning models to diverse PAM environments\
  \ with limited annotated data. The authors propose a framework leveraging pre-trained\
  \ models (e.g., Google\u2019s Perch) and fine-tuning on soundscape recordings through\
  \ AL cycles that query the most informative instances for labeling."
---

# Towards Deep Active Learning in Avian Bioacoustics

## Quick Facts
- arXiv ID: 2406.18621
- Source URL: https://arxiv.org/abs/2406.18621
- Reference count: 20
- Primary result: Deep active learning improves avian soundscape classification with up to 15% better accuracy than random sampling

## Executive Summary
This paper introduces deep active learning to avian bioacoustics for adapting pre-trained models to diverse passive acoustic monitoring (PAM) environments with limited annotations. The authors propose a framework that fine-tunes a pre-trained feature extractor (Google's Perch) on soundscape recordings through iterative AL cycles that query the most informative instances for labeling. Experiments on the BirdSet dataset demonstrate that uncertainty-based (Entropy), diversity-based (Typiclust), and hybrid (Badge) query strategies consistently outperform random sampling, with Entropy achieving up to 15% better accuracy in class-based mean average precision and T1-Accuracy metrics.

## Method Summary
The method employs pool-based active learning with a pre-trained feature extractor (Google's Perch) and a classification head. The AL cycle iteratively fine-tunes the model on soundscapes, using query strategies to select batches that maximize performance gains per annotation cost. Experiments use the BirdSet dataset (HSN soundscape segments: 5,280 for training pool, 6,720 for test) with an initial 10 labeled instances, running 50 iterations with batch size 10. The last DNN layer is trained using Rectified Adam optimizer with binary cross-entropy loss and cosine annealing scheduler.

## Key Results
- Entropy query strategy achieves up to 15% better accuracy than random sampling in class-based mean average precision (cmAP)
- T1-Accuracy improves consistently across all query strategies compared to random sampling
- Typiclust performs best early in the AL cycle, while Entropy excels in later iterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep active learning accelerates model adaptation from focal recordings to soundscape recordings by querying the most informative instances in the target domain.
- Mechanism: The AL cycle iteratively fine-tunes a pre-trained feature extractor on soundscapes, using query strategies to select batches that maximize performance gains per annotation cost.
- Core assumption: The initial pre-trained model provides a strong feature embedding that can be effectively adapted to the target distribution via fine-tuning.
- Evidence anchors:
  - [abstract] "Active learning (AL) reduces annotation cost and speed ups adaption to diverse scenarios by querying the most informative instances for labeling."
  - [section] "Our approach... we equip a model with a pre-trained feature extractor... and a classification head... During each cycle iteration ð‘¡, the query strategy compiles the most informative instances into a batch..."
  - [corpus] Weak evidence. The corpus contains general bioacoustics papers but none specifically validating the AL mechanism in avian soundscapes.
- Break condition: If the feature extractor's pre-trained embeddings are too domain-specific to focal recordings, fine-tuning may fail to bridge the distribution shift to soundscapes.

### Mechanism 2
- Claim: Uncertainty-based query strategies (e.g., Entropy) consistently improve classification performance in imbalanced, multi-label PAM scenarios.
- Mechanism: Entropy measures prediction uncertainty across all classes; high-entropy samples are queried for annotation, focusing the model on ambiguous instances that are most likely to improve cmAP and T1-Accuracy.
- Core assumption: In multi-label soundscapes, instances with high prediction entropy contain the most valuable information for reducing model uncertainty.
- Evidence anchors:
  - [abstract] "Experiments... using uncertainty-based (Entropy)... query strategies show consistent performance improvements over random sampling, with Entropy achieving up to 15% better accuracy..."
  - [section] "As an uncertainty-based strategy, we employ the mean Entropy of all binary predictions."
  - [corpus] Weak evidence. Corpus neighbors focus on bioacoustics and PAM but do not provide direct support for uncertainty-based AL in avian soundscapes.
- Break condition: If the model's initial predictions are already highly confident (low entropy), the query strategy may select uninformative samples, stalling performance gains.

### Mechanism 3
- Claim: Diversity-based query strategies (e.g., Typiclust) are most effective early in the AL cycle when the labeled pool is small.
- Mechanism: Typiclust clusters instances and selects diverse representatives, ensuring broad coverage of the input space and reducing redundancy in the labeled set during early iterations.
- Core assumption: Early in the AL cycle, model uncertainty is high across many regions of the feature space, so diversity in the labeled pool accelerates overall learning.
- Evidence anchors:
  - [section] "Typiclust displays strong performance across all metrics at the start of the deep AL cycle, supporting the findings of [16] that a diverse selection is beneficial at the cycle's onset."
  - [abstract] "Experiments... using... diversity-based (Typiclust)... query strategies show consistent performance improvements..."
  - [corpus] No direct evidence. Corpus neighbors do not discuss diversity-based AL strategies.
- Break condition: If the feature space is highly redundant or the model quickly overfits to diverse samples, Typiclust's early advantage may not translate to sustained performance.

## Foundational Learning

- Concept: Domain shift between focal recordings and soundscapes
  - Why needed here: Models trained on focal recordings (isolated bird sounds) must adapt to soundscapes (mixed, overlapping sounds), requiring domain adaptation techniques like AL.
  - Quick check question: Why can't a model trained on focal recordings be directly deployed on soundscapes without adaptation?

- Concept: Multi-label classification vs. multi-class classification
  - Why needed here: Soundscapes require predicting multiple bird species simultaneously (multi-label), unlike focal recordings which isolate a single species (multi-class), affecting loss functions and evaluation metrics.
  - Quick check question: How does the binary cross-entropy loss differ from categorical cross-entropy in this context?

- Concept: Feature embeddings and transfer learning
  - Why needed here: Pre-trained models (e.g., Google's Perch) provide rich embeddings that can be fine-tuned on small labeled datasets, crucial when annotations are scarce in PAM.
  - Quick check question: What is the dimensionality of the feature embeddings used in this study, and why is it important?

## Architecture Onboarding

- Component map:
  - Pre-trained feature extractor (Google's Perch, 1280-dim embeddings) -> Classification head (sigmoid-activated DNN layer) -> Query strategy module (Entropy, Typiclust, Badge) -> AL cycle controller (batch selection, model retraining) -> Test evaluation

- Critical path: Pre-trained model â†’ feature extraction â†’ classification head â†’ uncertainty/diversity scoring â†’ batch selection â†’ model update â†’ test evaluation

- Design tradeoffs:
  - Early diversity (Typiclust) vs. late uncertainty focus (Entropy): Typiclust ensures broad coverage initially but may plateau; Entropy refines the model later but risks redundancy.
  - Batch size vs. annotation budget: Larger batches reduce annotation rounds but may include less informative samples.

- Failure signatures:
  - No performance improvement over random sampling: Likely due to poor feature extractor choice or ineffective query strategy.
  - Early overfitting: May indicate insufficient diversity in initial labeled pool or too aggressive fine-tuning.

- First 3 experiments:
  1. Random baseline: Run AL cycle with random sampling to establish performance floor.
  2. Single strategy test: Implement and evaluate only Entropy to isolate uncertainty-based gains.
  3. Strategy comparison: Run all three strategies (Entropy, Typiclust, Badge) with identical settings to benchmark relative performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different deep active learning strategies perform across diverse PAM environments beyond the HSN dataset?
- Basis in paper: [explicit] "For future work, we aim to expand the implementation of deep AL in avian bioacoustics utilizing all datasets from the BirdSet dataset collection to provide more robust performance insights"
- Why unresolved: Current experiments are limited to one dataset (HSN), which may not capture the full variability of PAM scenarios across different regions and habitats.
- What evidence would resolve it: Comparative experiments across all BirdSet datasets showing strategy performance consistency or variation across different acoustic environments.

### Open Question 2
- Question: What is the optimal batch size and budget allocation for deep active learning in avian bioacoustics?
- Basis in paper: [inferred] The current study uses fixed parameters (batch size 10, budget 510) without exploring how different allocations affect performance or efficiency.
- Why unresolved: The paper states hyperparameters were "empirically determined" but doesn't explore the sensitivity of results to these choices or compare different allocation strategies.
- What evidence would resolve it: Systematic experiments varying batch sizes and budget allocations to identify optimal configurations for different PAM scenarios.

### Open Question 3
- Question: How does active learning compare to semi-supervised learning approaches in avian bioacoustics with limited annotations?
- Basis in paper: [inferred] The paper focuses exclusively on active learning without comparing to other low-annotation approaches like semi-supervised learning or self-training.
- Why unresolved: The authors highlight the challenge of annotation scarcity but don't benchmark AL against alternative methods that could also address this limitation.
- What evidence would resolve it: Direct comparison of active learning with semi-supervised learning approaches on the same datasets and annotation budgets.

## Limitations

- Experimental scope limited to one BirdSet dataset (HSN), limiting generalizability to diverse PAM environments
- Exact implementation details of Typiclust and Badge query strategies not specified, affecting reproducibility
- Assumption that pre-trained Perch embeddings effectively bridge domain shift from focal recordings to soundscapes remains untested

## Confidence

- **High Confidence**: The framework's basic AL cycle implementation and the use of established metrics (T1-Accuracy, cmAP, AUROC) are well-defined and reproducible.
- **Medium Confidence**: The relative performance rankings of query strategies (Entropy > Typiclust > Badge) are supported by the experiments but may not generalize to other datasets or scenarios.
- **Low Confidence**: The assumption that pre-trained Perch embeddings effectively bridge the domain shift to soundscapes is not empirically validated in this study.

## Next Checks

1. **Dataset Generalization**: Test the AL framework on additional BirdSet datasets (e.g., BirdVox, NIPS4B) to assess robustness across different acoustic environments.
2. **Feature Space Analysis**: Visualize and quantify the distribution shift between focal recordings and soundscapes using t-SNE or UMAP to validate the pre-trained model's adaptability.
3. **Query Strategy Robustness**: Compare the performance of Entropy, Typiclust, and Badge under varying initial pool sizes (e.g., 5, 20, 50 instances) to determine their sensitivity to annotation budget.