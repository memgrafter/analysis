---
ver: rpa2
title: A distributional simplicity bias in the learning dynamics of transformers
arxiv_id: '2410.19637'
source_url: https://arxiv.org/abs/2410.19637
tags:
- factored
- interactions
- data
- layers
- clones
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce a method to study how transformer models
  learn higher-order interactions between tokens in natural language data. They create
  "clones" of datasets by training generative models with factored attention layers,
  where the depth of the model controls the maximum order of interactions captured.
---

# A distributional simplicity bias in the learning dynamics of transformers

## Quick Facts
- arXiv ID: 2410.19637
- Source URL: https://arxiv.org/abs/2410.19637
- Authors: Riccardo Rende; Federica Gerace; Alessandro Laio; Sebastian Goldt
- Reference count: 40
- Primary result: Transformers learn increasingly higher-order token interactions sequentially during training, with a simplicity bias favoring low-order interactions.

## Executive Summary
This paper investigates how transformers learn higher-order interactions between tokens in natural language data. The authors introduce a novel method using "clones" - datasets generated by factored-attention models with controlled maximum interaction orders. By training standard transformers on natural language data and testing them on these clones during training, they demonstrate that transformers exhibit a simplicity bias: they first learn low-order interactions (unigrams, bigrams) and only later learn higher-order interactions. This sequential learning pattern is observed consistently across both BERT models trained with masked language modeling and GPT models trained with next-token prediction, suggesting a general phenomenon in transformer learning dynamics.

## Method Summary
The authors create controlled datasets ("clones") using factored-attention models with quadratic activation functions, where the depth of the model determines the maximum order of token interactions that can be captured. These clones are then used to probe the learning progress of standard transformers during training. By measuring test loss on clones with different maximum interaction orders, the researchers can track which interaction orders the transformer has learned at different training stages. This approach allows them to demonstrate that transformers learn interactions in a sequential, ordered manner rather than learning all interaction orders simultaneously.

## Key Results
- Transformers show a clear simplicity bias, learning low-order interactions (unigrams, bigrams) before high-order interactions during training
- Test loss on low-order clones plateaus while continuing to improve on high-order clones, demonstrating sequential learning
- This phenomenon is observed consistently across both BERT (bidirectional) and GPT (autoregressive) architectures
- The sequential learning pattern emerges naturally from the training dynamics without explicit architectural modifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers learn increasingly higher-order token interactions during training.
- Mechanism: The model starts by fitting low-degree interactions (unigrams, bigrams), then progressively learns higher-degree interactions as training proceeds.
- Core assumption: The data distribution contains higher-order interactions, and the transformer architecture can represent them.
- Evidence anchors:
  - [abstract] "transformers, trained on natural language data, also display a simplicity bias. Specifically, they sequentially learn many-body interactions among input tokens, reaching a saturation point in the prediction error for low-degree interactions while continuing to learn high-degree interactions."
  - [section 4] "The test losses of the transformer on the clones are close in the beginning of training up to ≈ 20 epochs. Beyond this threshold, BERT exhibits a different generalisation behaviour on clones that capture different orders of interaction."
  - [corpus] Weak - neighbors discuss simplicity bias in general, but not the specific mechanism of sequential higher-order learning.

### Mechanism 2
- Claim: Factored attention with x² activation enables controlled learning of interactions up to a specific degree.
- Mechanism: Stacking n layers of factored attention with quadratic activation allows the model to capture interactions up to order 2n-1 + 1.
- Core assumption: The quadratic non-linearity in the activation function is what enables the sequential learning of higher-order interactions.
- Evidence anchors:
  - [section 2] "by stacking n layers of factored attention with x2 activation function we can infer interactions up to order 2n-1 + 1."
  - [section 3.1] Analytical gradient flow analysis showing how different layers learn different orders of interactions.
  - [corpus] Weak - neighbors discuss transformer architectures but not this specific mechanism of factored attention.

### Mechanism 3
- Claim: Clones created from factored attention models serve as probes to detect which interaction orders the transformer has learned.
- Mechanism: By testing a trained transformer on clones that capture only up to a specific order of interactions, we can determine which orders the transformer has successfully learned.
- Core assumption: The clones accurately represent the truncated interaction orders they claim to capture.
- Evidence anchors:
  - [section 4] "We use these clones to investigate how a standard transformer encoder trained on MLM learns the different degrees of interactions among input words."
  - [section 3] "we show how, in a controlled setup, the multi-layer factored attention model introduced in section 2 sequentially learns increasingly higher-order interactions among tokens during training."
  - [corpus] Weak - neighbors discuss model probing but not this specific clone-based approach.

## Foundational Learning

- Concept: Many-body interactions in probability distributions
  - Why needed here: Understanding that tokens can interact in higher-order ways (not just pairwise) is fundamental to grasping what the paper is measuring
  - Quick check question: If tokens interact in a 3-body way, does this mean the probability of one token depends on the values of two other tokens simultaneously?

- Concept: Masked Language Modeling (MLM) objective
  - Why needed here: The training task (predicting masked tokens) is how transformers learn these interactions
  - Quick check question: In MLM, if you mask token i, what exactly is the model trying to predict - the identity of the token or something else?

- Concept: Gradient descent optimization dynamics
  - Why needed here: The sequential learning pattern emerges from how SGD updates the model weights over time
  - Quick check question: If a model shows a plateau in loss on simple tasks while continuing to improve on complex tasks, what does this suggest about what the model has learned?

## Architecture Onboarding

- Component map:
  - Factored attention layers: Linear transformations with input-independent attention weights
  - Quadratic activation (x²): Enables higher-order interaction learning
  - Layer stacking: Controls maximum interaction order (n layers → 2n-1+1 order)
  - Skip connections: Allow lower layers to contribute to predictions
  - Layer normalization: Stabilizes training

- Critical path: Data → Factored attention model training → Clone generation via sampling → Standard transformer training → Test on clones → Analyze sequential learning pattern

- Design tradeoffs:
  - More factored attention layers → higher-order interactions but more parameters and training cost
  - Sampling quality → affects clone fidelity and thus experimental validity
  - Vocabulary size → impacts computational cost and interaction complexity

- Failure signatures:
  - If test loss on all clones improves uniformly → no sequential learning detected
  - If clones don't capture the claimed interaction order → misleading results
  - If sampling doesn't converge → clones don't represent the intended distributions

- First 3 experiments:
  1. Train a 2-layer factored attention model on TinyStories, generate 3-body clones, verify the model captures 3-body interactions by testing on original data
  2. Train a standard BERT model on TinyStories, test on clones from 2-layer and 4-layer factored models, observe if there's a sequential learning pattern
  3. Vary the number of factored attention layers (2, 4, 6) and observe how the maximum interaction order changes, then test if BERT shows progressive learning of these orders

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the sample complexities required for transformers to learn different orders of interactions in natural language data?
- Basis in paper: [inferred] The authors note that estimating sample complexities is an active research topic even for simpler models, and adapting these techniques to attention-based neural networks remains an intriguing challenge.
- Why unresolved: The paper provides empirical evidence of sequential learning but does not quantify how many training examples are needed to learn specific interaction orders. This is particularly challenging for attention-based models compared to simpler architectures.
- What evidence would resolve it: Controlled experiments varying training set sizes while measuring learning curves for different interaction orders, combined with theoretical analysis of the sample complexity bounds for factored attention architectures.

### Open Question 2
- Question: How does the sequential learning of interactions differ between bidirectional (BERT) and autoregressive (GPT) transformer architectures?
- Basis in paper: [explicit] The authors show sequential learning occurs in both BERT and GPT models but note that bidirectional architectures are more expressive yet challenging to sample.
- Why unresolved: While the paper demonstrates sequential learning in both architectures, the underlying mechanisms and efficiency of learning different interaction orders may differ significantly between bidirectional and autoregressive approaches.
- What evidence would resolve it: Detailed comparative analysis of learning dynamics, including speed of convergence for different interaction orders and the role of architecture-specific features like attention patterns and position encoding.

### Open Question 3
- Question: Can the factored attention architecture be extended to capture interactions beyond the current limitations imposed by the depth of the network?
- Basis in paper: [explicit] The authors show that stacking n layers of factored attention with x2 activation function can infer interactions up to order 2n-1 + 1, suggesting a fundamental limitation based on network depth.
- Why unresolved: The current approach has a clear ceiling on the maximum interaction order that can be captured, and the paper does not explore potential architectural modifications to overcome this limitation.
- What evidence would resolve it: Experimental validation of alternative architectures or activation functions that can capture higher-order interactions with fewer layers, or theoretical analysis of the fundamental limitations of the factored attention approach.

## Limitations

- The experimental setup relies on synthetic datasets (TinyStories) that may not fully represent the complexity of real-world language data
- The quality and fidelity of the clone datasets depends heavily on the sampling procedure, which introduces potential uncertainty
- The findings are based on two specific transformer variants (BERT and GPT) and may not generalize to other architectures or domains

## Confidence

**High Confidence**: The observation that transformers show different generalization behaviors on clones with different interaction orders is well-supported by the experimental results.

**Medium Confidence**: The theoretical framework connecting factored attention depth to maximum interaction order is internally consistent, but practical implementation details introduce uncertainty.

**Low Confidence**: The generalizability of these findings to other architectures beyond BERT and GPT, and to other domains beyond language modeling, remains an open question.

## Next Checks

1. **Clone Fidelity Validation**: Train a standard transformer model on each clone dataset and compare its performance to the original factored attention model. If the test losses are similar, this provides stronger evidence that the clones accurately represent the intended interaction orders.

2. **Cross-Architecture Generalization**: Apply the same experimental protocol to other transformer variants (e.g., decoder-only models like GPT-3, encoder-decoder models like T5) and to non-transformer architectures (e.g., RNNs, CNNs).

3. **Real-World Data Extension**: Repeat the experiments on larger, more diverse language datasets (e.g., C4, BooksCorpus) and on non-language data (e.g., images, audio) to test the robustness of the simplicity bias phenomenon.