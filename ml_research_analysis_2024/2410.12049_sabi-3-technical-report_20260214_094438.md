---
ver: rpa2
title: "Sabi\xE1-3 Technical Report"
arxiv_id: '2410.12049'
source_url: https://arxiv.org/abs/2410.12049
tags:
- sabi
- benchmark
- zhang
- gpt-4o
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This technical report introduces Sabi\xE1-3 and Sabiazinho-3,\
  \ Brazilian Portuguese language models trained on Brazil-centric data to improve\
  \ performance on Portuguese and Brazil-related tasks. The models use a continual\
  \ learning approach, first pre-training on specialized data then fine-tuning with\
  \ instruction and preference data."
---

# Sabiá-3 Technical Report

## Quick Facts
- **arXiv ID**: 2410.12049
- **Source URL**: https://arxiv.org/abs/2410.12049
- **Reference count**: 23
- **Primary result**: Sabiá-3 achieves competitive accuracy with frontier models like GPT-4o on Brazilian academic exams while being 3-4x more cost-effective per token

## Executive Summary
This technical report introduces Sabiá-3 and Sabiazinho-3, Brazilian Portuguese language models trained on Brazil-centric data to improve performance on Portuguese and Brazil-related tasks. The models use a continual learning approach, first pre-training on specialized data then fine-tuning with instruction and preference data. Evaluations show Sabiá-3 achieves competitive accuracy with frontier models like GPT-4o on Brazilian academic exams, while being 3-4x more cost-effective per token. It also shows strong performance in conversation, long-context, and function-calling tasks, and excels in web-grounded agentic tasks. The work demonstrates the benefits of domain specialization for improving cost-quality trade-offs in LLMs.

## Method Summary
The development of Sabiá-3 followed a two-phase approach: first, pre-training a base model on a large Brazilian-centric corpus of Portuguese documents, then fine-tuning with instruction and preference data. The models were trained using TPU v5 accelerators with Jax, employing data and model parallelism. The pre-training corpus was filtered using heuristic and model-based methods to ensure high quality. Post-training involved both human-annotated and synthetically generated instruction data to teach the model to follow instructions and align with human preferences. The final models support up to 32,000 token context length.

## Key Results
- Sabiá-3 achieves competitive accuracy with frontier models like GPT-4o on Brazilian academic exams
- The model is 3-4x more cost-effective per token compared to frontier models
- Strong performance across conversation, long-context, and function-calling tasks
- Excels in web-grounded agentic tasks according to AgentBench evaluations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Domain specialization improves cost-quality trade-offs in LLMs.
- **Mechanism**: By training on a large Brazilian-centric corpus, the model gains deeper contextual understanding of Brazilian culture, history, and language nuances, allowing it to perform better on Brazil-related tasks while requiring fewer tokens to achieve competitive accuracy.
- **Core assumption**: The specialized corpus contains high-quality, diverse Brazilian data that captures relevant cultural and linguistic patterns not well represented in general-purpose training data.
- **Evidence anchors**:
  - [abstract]: "Evaluations across diverse professional and academic benchmarks show a strong performance on Portuguese and Brazil-related tasks."
  - [section]: "Compared to our previous release, Sabiá-2 [5], we have collected a significantly larger volume of data for pre-training. In addition to the scale, we also improved the quality of our pre-training data by using a mixture of heuristic and model-based methods to filter out low-quality data."
  - [corpus]: Weak evidence - The corpus search returned papers with average FMR 0.458, suggesting moderate relevance but not strong direct evidence for this specific mechanism.
- **Break condition**: If the specialized data quality degrades or if Brazilian-specific knowledge becomes less relevant for target tasks, the cost-quality advantage diminishes.

### Mechanism 2
- **Claim**: Continual learning approach improves model performance.
- **Mechanism**: Starting with a pre-trained generalist model and further training it on specialized data allows the model to retain general language understanding while acquiring domain-specific knowledge, leading to better performance than training from scratch on specialized data alone.
- **Core assumption**: The base model has already acquired sufficient language understanding and reasoning abilities that can be effectively transferred to the Brazilian domain through further training.
- **Evidence anchors**:
  - [abstract]: "We applied an approach of continual learning by leveraging a 'generalist' model that already acquired some level of language understanding and reasoning abilities, and then further trained it on our corpus of high-quality data relevant to the Brazilian context."
  - [section]: "The development consisted of two main phases: (1) the pre-training phase, in which we further train a pre-trained model on specialized data following a self-supervised learning strategy optimizing for the next token prediction objective, and (2) the post-training phase where the model is tuned to follow instructions and align to human preferences."
  - [corpus]: No direct evidence found in corpus - this mechanism relies on the technical report's own description.
- **Break condition**: If the base model's general knowledge conflicts significantly with Brazilian-specific knowledge, or if the base model is too dissimilar from the target domain, transfer learning benefits may be limited.

### Mechanism 3
- **Claim**: Post-training with instruction and preference data improves instruction-following capabilities.
- **Mechanism**: After pre-training on specialized data, the model is fine-tuned with human-annotated and synthetically generated instruction data, teaching it to follow instructions and align with human preferences, which improves its practical usability.
- **Core assumption**: The combination of human-annotated and synthetic instruction data provides diverse, high-quality examples that effectively teach the model instruction-following behavior.
- **Evidence anchors**:
  - [abstract]: "After pre-training, we employed a mix of human-annotated and synthetically generated data to teach our base model to follow instructions [1, 2, 15, 16, 13, 8]."
  - [section]: "The instruction tuning data is composed of both human-annotated prompts and synthetically generated instructions targeting specific capabilities. In addition, we also include synthetic examples to induct self-awareness abilities in the model [6]."
  - [corpus]: No direct evidence found in corpus - this mechanism is described in the technical report itself.
- **Break condition**: If the instruction data quality is poor or if the synthetic data generation process introduces biases, the instruction-following capabilities may not improve as expected.

## Foundational Learning

- **Concept**: Continual learning in LLMs
  - **Why needed here**: The approach of starting with a pre-trained generalist model and further training on specialized data is central to Sabiá-3's development strategy.
  - **Quick check question**: What are the two main phases of development described in the report, and what is the purpose of each phase?

- **Concept**: Domain specialization in LLMs
  - **Why needed here**: Understanding how domain-specific training data can improve performance on targeted tasks is crucial for grasping Sabiá-3's advantages.
  - **Quick check question**: According to the report, what specific types of Brazilian content were the models trained on to achieve specialization?

- **Concept**: Post-training alignment techniques
  - **Why needed here**: The instruction tuning and preference alignment phase is critical for making the model practically useful beyond raw language understanding.
  - **Quick check question**: What two types of data were combined in the post-training phase to teach the model to follow instructions and align to human preferences?

## Architecture Onboarding

- **Component map**: Base pre-trained model -> Specialized pre-training on Brazilian-centric data -> Post-training instruction and preference alignment -> Final model with 32K context length
- **Critical path**: The most critical path for achieving the reported performance is the combination of specialized pre-training data quality and the post-training instruction tuning. Without high-quality Brazilian-centric data, the specialization benefits disappear. Without effective instruction tuning, the model cannot follow instructions well despite strong language understanding.
- **Design tradeoffs**: The report chose a continual learning approach over training from scratch, trading the potential for perfectly clean domain-specific initialization for faster development and leveraging existing general capabilities. The choice of synthetic data generation for instruction tuning trades human annotation costs for potential quality control challenges.
- **Failure signatures**: If the model performs well on knowledge-intensive tasks but poorly on instruction-following, this indicates issues with the post-training phase. If performance on Brazilian tasks is no better than general models, this suggests problems with the specialized pre-training data or its integration with the base model.
- **First 3 experiments**:
  1. **Data quality validation**: Test the specialized pre-training data by training a small model and evaluating its performance on a few key Brazilian benchmarks to ensure the data captures the intended domain knowledge.
  2. **Base model compatibility test**: Evaluate how well a small version of the base model transfers to Brazilian tasks before committing to full-scale training, to verify the continual learning assumption.
  3. **Instruction tuning effectiveness**: Test the instruction following capabilities on a small set of human-annotated instructions versus synthetic instructions to determine the optimal data mix ratio.

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- The specialized corpus composition and quality remain largely unspecified, making it difficult to assess whether performance gains are truly attributable to effective domain specialization.
- Evaluation focuses heavily on Brazilian academic exams and benchmarks, which may not fully represent real-world performance across diverse use cases.
- Comparison with frontier models is based on cost-effectiveness per token rather than direct performance parity, leaving questions about absolute capability gaps.

## Confidence

- **High Confidence**: The models achieve competitive accuracy on Brazilian academic exams and benchmarks, as evidenced by specific numerical results presented in the report.
- **Medium Confidence**: The claim that Sabiá-3 is 3-4x more cost-effective than frontier models, as this comparison is based on reported token costs but lacks detailed breakdown of operational costs.
- **Medium Confidence**: The assertion that continual learning approach improves performance, as this is supported by the development methodology description but lacks ablation studies comparing against training from scratch.
- **Low Confidence**: The general claim that domain specialization always improves cost-quality trade-offs for LLMs, as this is an extrapolation beyond the specific case of Brazilian Portuguese and requires broader validation.

## Next Checks

1. **Cross-cultural generalization test**: Evaluate Sabiá-3's performance on non-Brazilian Portuguese tasks and general language understanding benchmarks to quantify the specialization trade-off - how much general capability is sacrificed for domain expertise.

2. **Independent benchmark validation**: Have the model evaluated on the same benchmarks by an independent third party to verify the reported performance metrics and ensure no data contamination or benchmark-specific overfitting occurred.

3. **Cost-effectiveness real-world trial**: Conduct a practical deployment test comparing Sabiá-3 against GPT-4o on actual Brazilian enterprise use cases, measuring both performance quality and total operational costs including fine-tuning, hosting, and maintenance expenses.