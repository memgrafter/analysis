---
ver: rpa2
title: 'Sparse-ProxSkip: Accelerated Sparse-to-Sparse Training in Federated Learning'
arxiv_id: '2405.20623'
source_url: https://arxiv.org/abs/2405.20623
tags:
- sparse
- training
- learning
- page
- cited
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently training large
  models in Federated Learning (FL) under client resource constraints and communication
  costs. The authors propose Sparse-ProxSkip, a method that combines sparse training
  and acceleration in FL.
---

# Sparse-ProxSkip: Accelerated Sparse-to-Sarse Training in Federated Learning

## Quick Facts
- arXiv ID: 2405.20623
- Source URL: https://arxiv.org/abs/2405.20623
- Reference count: 30
- Primary result: Combines sparse training and acceleration in FL with client-side pruning, achieving 10-20× better communication efficiency than baselines

## Executive Summary
This paper addresses the challenge of efficiently training large models in Federated Learning under client resource constraints and communication costs. The authors propose Sparse-ProxSkip, which integrates sparse training with acceleration techniques like ProxSkip. A key insight is that pruning must be done at the clients rather than the server to maintain a crucial zero-sum control variates invariant. The method achieves state-of-the-art communication efficiency while maintaining high model accuracy, outperforming baselines by significant margins on linear regression, logistic regression, and deep learning tasks.

## Method Summary
Sparse-ProxSkip combines ProxSkip's control variate mechanism with local sparse training using TopK pruning at the client level. The algorithm uses Straight-Through Estimator to approximate gradients through the non-differentiable pruning operation, maintaining a sparse model throughout training. Clients perform multiple local gradient steps before communicating, pruning their models locally before sending updates to the server. The server aggregates models and returns the average, while clients maintain control variates to ensure convergence. This approach provably achieves optimal accelerated communication complexity while reducing the number of prox evaluations (communication rounds).

## Key Results
- On FEMNIST with 99% sparsity, Sparse-ProxSkip achieves 72.8% accuracy versus 25.5% for best baseline
- Achieves 10-20× better communication efficiency than baselines
- Maintains zero-sum control variates invariant crucial for convergence
- Demonstrates effectiveness across linear regression, logistic regression, and deep learning tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pruning must occur at clients, not server, to maintain zero-sum control variates invariant
- **Mechanism:** The algorithm relies on the invariant that the sum of control variates across all clients equals zero. When pruning is applied at the server (after aggregation), this invariant is violated because the pruning operation is applied to the averaged model, which breaks the relationship between the control variates and the local gradients.
- **Core assumption:** Control variates are computed and maintained at the client level before any aggregation occurs
- **Evidence anchors:**
  - [abstract] "we show that pruning should be done at the clients instead"
  - [section] "it is crucial thatP i hi = 0 always holds" and "we can show that the algorithm diverges"
  - [corpus] weak - no direct mention of this specific invariant violation in related work
- **Break condition:** If the sum of control variates across clients deviates from zero, the algorithm diverges instead of converging to the optimum

### Mechanism 2
- **Claim:** Sparse-ProxSkip achieves accelerated communication complexity through local training and sparse model updates
- **Mechanism:** The algorithm combines ProxSkip's control variate mechanism with local sparse training. By allowing clients to take multiple local gradient steps before communicating, and pruning the model locally before sending updates, the communication cost is reduced while maintaining convergence guarantees.
- **Core assumption:** Local gradient steps with control variates can provably accelerate convergence in FL
- **Evidence anchors:**
  - [abstract] "local training can provably achieve the optimal accelerated communication complexity"
  - [section] "one prox evaluation corresponds exactly to one communication round" and "reducing the number of prox evaluations is crucial"
  - [corpus] weak - related work mentions sparse training but not the specific acceleration mechanism
- **Break condition:** If the probability parameter p is set too low, the algorithm may not achieve the desired acceleration; if too high, convergence may slow

### Mechanism 3
- **Claim:** Straight-Through Estimator pruning enables efficient sparse-to-sparse training in FL
- **Mechanism:** The algorithm uses the Straight-Through Estimator (STE) to approximate gradients through the non-differentiable TopK pruning operation. This allows the sparse model to be maintained throughout training while still enabling gradient-based optimization.
- **Core assumption:** The STE approximation is sufficiently accurate for the pruning operation to maintain model performance
- **Evidence anchors:**
  - [abstract] "we provide theoretical and empirical explanations of this phenomenon"
  - [section] "inspired byRandProx... which provably combines sparse training and acceleration"
  - [corpus] weak - related work mentions STE but not specifically in the FL context
- **Break condition:** If the sparsity level is set too high, the STE approximation may become inaccurate and degrade model performance

## Foundational Learning

- **Concept:** Federated Learning and its communication constraints
  - **Why needed here:** The entire paper addresses how to train large models efficiently in FL where communication costs are the main bottleneck
  - **Quick check question:** What is the primary challenge that federated learning addresses compared to centralized training?

- **Concept:** Acceleration through local training and control variates
  - **Why needed here:** The paper builds on ProxSkip and RandProx which use control variates to enable local training steps without losing convergence guarantees
  - **Quick check question:** How do control variates help mitigate the client drift problem in local training?

- **Concept:** Sparse-to-sparse training and pruning techniques
  - **Why needed here:** The method maintains sparsity throughout training rather than starting dense and pruning later, which is crucial for resource-constrained clients
  - **Quick check question:** What is the difference between dense-to-sparse and sparse-to-sparse training in terms of computational efficiency?

## Architecture Onboarding

- **Component map:** Clients -> Local gradient computation -> STE-based TopK pruning -> Model update -> Server aggregation -> Model average -> Client control variate update

- **Critical path:**
  1. Initialize models and control variates on all clients
  2. Clients compute local gradients and apply pruning using STE
  3. With probability p, clients send pruned models to server for aggregation
  4. Server averages models and sends back to clients
  5. Clients update control variates based on the difference between aggregated and local models
  6. Repeat until convergence

- **Design tradeoffs:**
  - Sparsity level vs model accuracy: Higher sparsity reduces communication but may impact performance
  - Probability p vs convergence speed: Higher p means more frequent communication but faster convergence
  - Local steps vs client drift: More local steps reduce communication but may increase drift

- **Failure signatures:**
  - Divergence when sum of control variates ≠ 0 (server pruning variant)
  - Slow convergence when p is set too low
  - Degraded accuracy when sparsity is too aggressive
  - NaN errors in deep learning experiments without gradient clipping

- **First 3 experiments:**
  1. Linear regression on BlogFeedback dataset with 90% sparsity to verify basic functionality
  2. Logistic regression on FEMNIST with 99% sparsity to test scalability and communication efficiency
  3. Deep learning on CIFAR-10 with ResNet18 at 90% sparsity to validate in complex, nonconvex setting

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas remain unresolved based on the analysis of the work:

### Open Question 1
- **Question:** Does the failure of accelerated server pruning in Federated Learning generalize to other acceleration techniques beyond ProxSkip, such as FedNova or SCAFFOLD?
- **Basis in paper:** [explicit] The paper demonstrates that accelerated server pruning fails with ProxSkip and provides a theoretical explanation related to the violation of the zero-sum property of control variates. This raises the question of whether this issue is specific to ProxSkip or a more general problem with combining acceleration and server-side pruning in FL.
- **Why unresolved:** The paper only tests accelerated server pruning with ProxSkip. Other acceleration techniques might have different mechanisms for handling control variates or local updates, which could potentially make them more compatible with server-side pruning.
- **What evidence would resolve it:** Experiments testing accelerated server pruning with other acceleration techniques like FedNova or SCAFFOLD would be needed. Additionally, a theoretical analysis of how these techniques handle control variates and their compatibility with server-side pruning would be valuable.

### Open Question 2
- **Question:** How does the performance of Sparse-ProxSkip scale with the number of clients in the federated learning setting, especially in scenarios with a very large number of clients?
- **Basis in paper:** [inferred] The paper's experiments use a relatively small number of clients (100 for FEMNIST, 10 for CIFAR-10). The paper mentions that ProxSkip requires modifications to support partial client participation, but does not explore this aspect. This raises the question of how Sparse-ProxSkip would perform in large-scale federated learning scenarios.
- **Why unresolved:** The paper does not investigate the performance of Sparse-ProxSkip with a large number of clients. The impact of client heterogeneity and the frequency of communication rounds on the algorithm's effectiveness in large-scale settings is unknown.
- **What evidence would resolve it:** Experiments testing Sparse-ProxSkip with a varying number of clients, including scenarios with a very large number of clients, would be needed. Additionally, theoretical analysis of how the algorithm's performance scales with the number of clients would be valuable.

### Open Question 3
- **Question:** Can the principles of Sparse-ProxSkip be extended to other types of sparse training techniques beyond magnitude-based pruning, such as structured pruning or pruning based on other criteria?
- **Basis in paper:** [inferred] The paper focuses on magnitude-based pruning using the TopK operator. While this is a common and effective pruning technique, other methods exist that might offer different trade-offs in terms of sparsity, accuracy, and computational efficiency. The paper's theoretical insights about the importance of client-side pruning and the zero-sum property of control variates could potentially be applied to other pruning techniques.
- **Why unresolved:** The paper only explores magnitude-based pruning. The compatibility of other pruning techniques with the principles of Sparse-ProxSkip and their potential benefits or drawbacks in the federated learning setting are unknown.
- **What evidence would resolve it:** Experiments testing Sparse-ProxSkip with other pruning techniques, such as structured pruning or pruning based on other criteria, would be needed. Additionally, theoretical analysis of how these techniques interact with the principles of Sparse-ProxSkip would be valuable.

## Limitations

- The theoretical analysis is limited to specific problem classes and may not generalize to all federated learning scenarios
- Experiments are conducted on relatively small-scale federated learning setups (100 clients maximum)
- The paper does not explore the impact of highly heterogeneous client data distributions on algorithm performance
- Limited comparison with the most recent state-of-the-art federated learning methods

## Confidence

- **High**: The fundamental mechanism of client-side pruning maintaining the zero-sum control variate invariant
- **Medium**: The empirical performance gains on FEMNIST and CIFAR-10, which depend on specific hyperparameter choices and random seeds
- **Medium**: The communication efficiency improvements, as they depend on the specific network conditions and client participation rates

## Next Checks

1. Test the zero-sum control variate invariant empirically across different datasets and sparsity levels to verify it holds under various conditions
2. Evaluate Sparse-ProxSkip with different neural network architectures (e.g., transformers, LSTMs) to assess generalizability beyond ResNet18
3. Conduct ablation studies varying the probability parameter p and local steps to map the full convergence landscape