---
ver: rpa2
title: 'In-Memory Learning: A Declarative Learning Framework for Large Language Models'
arxiv_id: '2403.02757'
source_url: https://arxiv.org/abs/2403.02757
tags:
- agents
- learning
- notes
- process
- creature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel learning framework called In-Memory
  Learning (IML) for large language models, inspired by declarative memory in intelligent
  organisms. The framework enables agents to self-improve without relying on human-labeled
  data by summarizing past experiences, refining and updating existing notes within
  memory components.
---

# In-Memory Learning: A Declarative Learning Framework for Large Language Models

## Quick Facts
- arXiv ID: 2403.02757
- Source URL: https://arxiv.org/abs/2403.02757
- Authors: Bo Wang; Tianxiang Sun; Hang Yan; Siyin Wang; Qingyuan Cheng; Xipeng Qiu
- Reference count: 12
- Primary result: In-Memory Learning enables large language models to self-improve through iterative refinement of natural language notes, with llama2-70b-chat achieving 58.67% accuracy on a proposed classification task.

## Executive Summary
This paper introduces In-Memory Learning (IML), a novel declarative learning framework for large language models that enables self-improvement without human-labeled data. Inspired by declarative memory in intelligent organisms, IML allows agents to distill insights from past experiences, refining and updating natural language notes to enhance performance. The framework operates through three phases: inference (collecting trajectories), induction (summarizing common features into batch notes), and revision (merging batch notes into existing notes). Systematic experiments demonstrate that models like llama2-70b-chat and GPT-3.5-turbo exhibit continuous improvement over learning steps, with momentum and accumulation parameters playing crucial roles in stabilizing the learning process.

## Method Summary
In-Memory Learning operates through a three-phase cycle. During inference, the agent selects actions based on current observations and records trajectories. In the induction phase, the agent processes collected trajectories to produce batch notes that summarize common patterns. During revision, these batch notes are merged into existing notes, with optional momentum constraints that require starting new notes with initial words from previous ones. The framework was evaluated on a proposed classification benchmark consisting of 3200 shuffled samples describing creatures in 10 dimensions, with relationships between features and labels clearly defined. Experiments systematically varied parameters like momentum and accumulation steps to study their effects on learning stability and accuracy.

## Key Results
- Models demonstrate continuous improvement over learning steps, with accuracy increasing from baseline performance
- Llama2-70b-chat achieved 58.67% accuracy on the proposed classification task after iterative learning
- Momentum constraints significantly stabilize the learning process, with no-momentum settings showing greater instability
- Accumulation steps improve stability by providing more representative samples for rule induction, though smaller accumulation leads to greater instability

## Why This Works (Mechanism)

### Mechanism 1
The model improves accuracy by iteratively refining natural language "notes" that encode learned rules from experience. During inference, trajectories are collected and summarized in the induction phase into batch notes capturing common features. These are merged into existing notes during revision, which are then used in subsequent inference steps. This process mimics gradient descent but operates over discrete natural language representations. The core assumption is that summarization of common patterns across experiences reliably produces generalizable rules. Evidence shows accuracy improvements across learning steps, though the corpus does not directly confirm rule-distillation effectiveness.

### Mechanism 2
Momentum-like constraints in the revision phase stabilize learning by forcing the model to begin new notes with initial words from previous notes. This reduces the model's freedom to change drastically, analogous to momentum in gradient-based learning, preventing destabilizing changes. The core assumption is that constraining output format reduces variance in revision steps, making updates more incremental and stable. Evidence shows that full momentum settings yield the most stable performance, while no momentum leads to instability. However, the corpus does not provide evidence about momentum-like constraints in natural language revision.

### Mechanism 3
Accumulation over multiple minibatches improves learning stability by providing a more reliable statistical basis for rule induction. Because context window limits single batch size, the model iteratively accumulates results from multiple minibatches before revising notes, smoothing out noise from individual batches. The core assumption is that larger accumulated samples provide more representative basis for summarizing rules than individual minibatches. Evidence shows smaller accumulation steps lead to greater instability, though the corpus does not directly support the accumulation claim.

## Foundational Learning

- **Concept:** Natural language summarization as rule extraction
  - **Why needed here:** The framework relies on converting experience into discrete natural language "notes" that can be iteratively refined. Without this capability, the model cannot represent learned rules in modifiable form.
  - **Quick check question:** Can the model reliably summarize a set of examples into a concise rule description without losing critical distinguishing features?

- **Concept:** Discrete optimization via iterative refinement
  - **Why needed here:** Unlike gradient descent over continuous parameters, this framework must optimize over discrete natural language representations. Understanding how iterative refinement can approximate gradient-based learning is essential.
  - **Quick check question:** Does accuracy improve monotonically with more refinement steps, or does it plateau or degrade due to noise?

- **Concept:** Context window limitations and minibatch processing
  - **Why needed here:** The framework must work within constraints of the model's context window, requiring careful batching and accumulation strategies.
  - **Quick check question:** What is the maximum number of experiences that can be processed in a single induction step given the context window size?

## Architecture Onboarding

- **Component map:** Inference phase -> Induction phase -> Revision phase -> Inference phase
- **Critical path:** Trajectory collection → Induction (summarization) → Revision (merging) → Inference (application) → Evaluation
- **Design tradeoffs:**
  - Note granularity vs. context window size: More detailed notes improve inference but consume more context
  - Momentum strength vs. adaptability: Stronger constraints stabilize learning but may prevent correction of errors
  - Accumulation window size vs. responsiveness: Larger windows improve stability but slow adaptation to new patterns
- **Failure signatures:**
  - Accuracy plateaus or declines after initial improvement → likely summarization failure or over-constraining
  - High variance in accuracy across steps → insufficient accumulation or unstable revision process
  - Model fails to correct known errors → momentum constraints too strong
- **First 3 experiments:**
  1. **Baseline accuracy test:** Run inference with no prior notes to establish baseline performance
  2. **Single-step refinement test:** Apply one complete inference → induction → revision cycle and measure accuracy change
  3. **Momentum comparison test:** Compare accuracy trajectories with and without momentum constraints to verify stabilization effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the In-Memory Learning framework scale with larger datasets and more complex tasks?
- Basis in paper: The paper mentions the constraint of the context window size, which limits the batch size for the induction phase.
- Why unresolved: The experiments were conducted on a relatively small dataset and a simple classification task. The scalability of the framework to larger datasets and more complex tasks remains unexplored.
- What evidence would resolve it: Conducting experiments on larger datasets and more complex tasks, such as multi-hop reasoning or question answering, would provide insights into the scalability of the framework.

### Open Question 2
- Question: How does the In-Memory Learning framework compare to other learning frameworks, such as reinforcement learning or meta-learning?
- Basis in paper: The paper focuses on comparing the framework to in-context learning, but does not provide a comprehensive comparison with other learning frameworks.
- Why unresolved: The paper does not provide a thorough comparison with other learning frameworks, leaving the question of its relative performance unanswered.
- What evidence would resolve it: Conducting experiments to compare the performance of the In-Memory Learning framework with other learning frameworks, such as reinforcement learning or meta-learning, on various tasks would provide insights into its relative strengths and weaknesses.

### Open Question 3
- Question: How does the In-Memory Learning framework handle uncertainty and incomplete information?
- Basis in paper: The paper mentions that the framework is designed for self-improvement in the absence of human-labeled data, which implies that it needs to handle uncertainty and incomplete information.
- Why unresolved: The paper does not provide a detailed discussion of how the framework handles uncertainty and incomplete information, leaving the question of its robustness unanswered.
- What evidence would resolve it: Conducting experiments to evaluate the performance of the framework in scenarios with uncertainty and incomplete information, such as noisy labels or missing data, would provide insights into its robustness and ability to handle real-world challenges.

## Limitations

- The proposed classification benchmark is synthetic and may not reflect the complexity of real-world tasks
- The framework's reliance on natural language summarization for rule extraction lacks direct validation of summary quality and generalizability
- Evaluation focuses primarily on accuracy metrics without examining the interpretability or stability of learned notes themselves

## Confidence

- **High confidence:** The framework's architectural design and phase structure are clearly specified. The experimental setup (using llama2-70b-chat and GPT-3.5-turbo on the proposed benchmark) is well-documented.
- **Medium confidence:** The effectiveness of momentum and accumulation parameters in stabilizing learning is demonstrated, but the mechanisms behind these effects could benefit from deeper analysis.
- **Low confidence:** The claim that models can reliably extract generalizable rules through natural language summarization is supported by accuracy improvements but lacks direct validation of the quality of the extracted rules.

## Next Checks

1. **Rule quality analysis:** Extract and manually evaluate the notes produced by the model at different learning stages to assess whether they capture meaningful, generalizable patterns rather than memorizing specific examples.

2. **Transfer task evaluation:** Test whether notes learned on the classification benchmark can be effectively applied to related but distinct classification tasks, measuring zero-shot or few-shot transfer performance.

3. **Noise robustness test:** Systematically introduce noise into the experience trajectories and measure how the model's accuracy and note quality degrade, establishing the framework's robustness to imperfect or contradictory experiences.