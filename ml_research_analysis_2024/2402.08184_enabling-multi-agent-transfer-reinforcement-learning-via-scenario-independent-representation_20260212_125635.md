---
ver: rpa2
title: Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent
  Representation
arxiv_id: '2402.08184'
source_url: https://arxiv.org/abs/2402.08184
tags:
- learning
- agents
- transfer
- scenarios
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a transfer learning framework for multi-agent
  reinforcement learning that unifies state spaces into fixed-size inputs, enabling
  a single deep learning policy to work across different scenarios in multi-agent
  systems. The approach uses spatial and feature encoding with Influence Maps to create
  scenario-independent representations, combining local observations and abstracted
  global information.
---

# Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation

## Quick Facts
- arXiv ID: 2402.08184
- Source URL: https://arxiv.org/abs/2402.08184
- Reference count: 32
- One-line primary result: Unified scenario-independent state representation via Influence Maps and Curriculum Transfer Learning achieves significant performance improvements in multi-agent StarCraft II micromanagement across diverse scenarios.

## Executive Summary
This study introduces a transfer learning framework for multi-agent reinforcement learning that addresses the challenge of applying policies across scenarios with varying agent counts and types. The approach unifies state spaces into fixed-size inputs using Influence Maps (IMs) for local observations and Multi-Agent Influence Maps (MAIM) for global abstractions, enabling a single neural network policy to work across different SMAC scenarios. By combining local observations with abstracted global information and implementing Curriculum Transfer Learning (CTL), the method achieves enhanced learning efficiency and robustness, with empirical results showing significant performance gains compared to learning from scratch.

## Method Summary
The framework transforms heterogeneous state spaces into fixed-size representations using Influence Maps for local observations and MAIM for global abstraction. Local observations are converted into 37×37 IMs based on agent sight range, while global information is abstracted through MAIM. The method employs a unified neural network policy trained using Advantage Actor-Critic (A2C) with ε-soft action selection. Curriculum Transfer Learning is implemented by first training on simpler homogeneous scenarios (3m, 8m), then progressively transferring knowledge to more complex scenarios (25m, 2s3z). The approach generalizes attack actions by selecting the closest enemy, enabling consistent action spaces across scenarios with different unit types.

## Key Results
- Influence Map resolution of 37×37 provides optimal balance between information preservation and computational efficiency across tested scenarios
- Curriculum Transfer Learning (3m → 8m → 2s3z) achieves superior performance compared to learning from scratch in heterogeneous scenarios
- The unified state representation enables consistent policy application across scenarios with different agent counts and types, reducing the need for scenario-specific models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scenario-independent state representation via spatial and feature encoding enables transfer learning across different agent counts and types.
- Mechanism: Local observations are converted into fixed-size Influence Maps (IMs) and combined with abstracted global information from Multi-Agent Influence Maps (MAIM). This removes dependency on scenario-specific agent counts, enabling a single neural network to handle diverse SMAC scenarios.
- Core assumption: Key spatial and feature information for decision-making is preserved when transforming local observations into fixed-size IMs, regardless of the number of agents.
- Evidence anchors:
  - [abstract] "unifying various state spaces into fixed-size inputs that allow one unified deep-learning policy viable in different scenarios"
  - [section] "We extended the use of IM from global information abstraction to local observation aggregation... The input dimension is determined by the agent's sight range in all four directions, resulting in a unified two-dimensional scaled matrix"
  - [corpus] Weak - no direct evidence of fixed-size IMs enabling transfer in other MARL works.
- Break condition: If crucial tactical information is lost during IM transformation (e.g., specific unit counts or distances), the policy may fail to make optimal decisions.

### Mechanism 2
- Claim: Curriculum Transfer Learning (CTL) enables progressive knowledge acquisition from simpler to more complex scenarios, improving final performance.
- Mechanism: The model first learns from the simplest scenario (3m), then retrains on a medium difficulty scenario (8m), and finally applies combined knowledge to the most complex heterogeneous scenario (2s3z). This staged approach allows the model to build foundational skills before tackling more challenging tasks.
- Core assumption: Knowledge gained from homogeneous scenarios (all marines) can be effectively transferred to heterogeneous scenarios (mix of stalkers and zealots).
- Evidence anchors:
  - [abstract] "Curriculum Transfer Learning (CTL), enabling our deep learning policy to progressively acquire knowledge and skills across pre-designed homogeneous learning scenarios organized by difficulty levels"
  - [section] "We scrutinized the outcome of CTL in which the 3m

## Foundational Learning

**Influence Maps (IM)**: Spatial abstraction technique that converts local observations into fixed-size grids for consistent policy input.
*Why needed*: Eliminates scenario-specific state space variations caused by different agent counts.
*Quick check*: Verify that key tactical information (unit positions, health, cooldowns) is preserved after IM transformation.

**Multi-Agent Influence Maps (MAIM)**: Global abstraction layer that provides scene-level context beyond individual agent observations.
*Why needed*: Enables agents to understand broader tactical situations while maintaining fixed input dimensions.
*Why needed*: Complements local IMs with strategic awareness for coordinated decision-making.
*Quick check*: Confirm that global feature extraction captures team composition and enemy positioning effectively.

**Curriculum Transfer Learning (CTL)**: Sequential training approach that builds skills progressively from simple to complex scenarios.
*Why needed*: Prevents catastrophic failure when applying policies directly to complex heterogeneous scenarios.
*Quick check*: Monitor performance improvements at each curriculum stage to ensure effective knowledge transfer.

## Architecture Onboarding

**Component Map**: Local Observations → Influence Maps (37×37) → MAIM (64×64) → Neural Network Policy → Action Selection → Environment Feedback → Reward Signal

**Critical Path**: State Representation → Neural Network Training → Curriculum Transfer → Performance Evaluation

**Design Tradeoffs**: Fixed-size IM resolution (37×37) balances information preservation against computational efficiency; the choice impacts both transfer effectiveness and training speed.

**Failure Signatures**: 
- Poor transfer performance indicates loss of critical information during IM transformation
- Unstable learning during curriculum transitions suggests difficulty in reconciling different unit dynamics
- Suboptimal IM resolutions lead to either information loss (too small) or computational inefficiency (too large)

**First Experiments**:
1. Test Influence Map transformations with different resolutions (19×19, 37×37, 55×55) to identify optimal balance
2. Train homogeneous scenarios individually to establish baseline performance and identify best transfer seeds
3. Implement curriculum learning progression (3m → 8m → 2s3z) and compare against direct training on target scenario

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Influence Map (IM) resolution (19×19, 37×37, 55×55) affect transfer learning performance across different SMAC scenarios?
- Basis in paper: [explicit] The paper evaluated different IM resolutions and selected 37×37 based on overall stability of performance across scenarios.
- Why unresolved: While the paper identified the best-performing resolution, it did not systematically investigate why this particular resolution worked best or whether there's an optimal resolution for specific scenario types.
- What evidence would resolve it: Controlled experiments varying IM resolution across multiple scenario types with statistical analysis of performance differences, including computational efficiency trade-offs.

### Open Question 2
- Question: How transferable are learned policies across heterogeneous unit types beyond the specific Stalkers and Zealots tested in 2s3z?
- Basis in paper: [inferred] The paper demonstrated successful transfer learning for Stalkers and Zealots in 2s3z but only tested one heterogeneous scenario.
- Why unresolved: The paper's curriculum learning success was demonstrated on a single heterogeneous scenario, leaving open questions about generalizability to other unit combinations and scenario types.
- What evidence would resolve it: Systematic testing of curriculum transfer learning across multiple heterogeneous scenarios with different unit combinations, measuring transfer effectiveness and identifying limitations.

### Open Question 3
- Question: What is the relationship between scenario difficulty, distance, and knowledge transfer effectiveness in curriculum learning?
- Basis in paper: [inferred] The paper mentions that "the effectiveness of knowledge transfer is influenced by domain difficulty and distance" from related work, but doesn't empirically investigate this relationship.
- Why unresolved: While curriculum learning was implemented successfully, the paper didn't analyze how the ordering or difficulty progression of training scenarios affects transfer learning outcomes.
- What evidence would resolve it: Experiments varying the sequence and difficulty progression of training scenarios in curriculum learning, measuring how different orderings affect final performance in target scenarios.

## Limitations
- The effectiveness of Influence Map transformations is assumed but not rigorously validated through ablation studies showing what tactical information might be lost
- Curriculum Transfer Learning is only tested on a single progression path (3m → 8m → 2s3z), leaving questions about generalizability to other scenario sequences
- The paper doesn't address potential catastrophic forgetting when transferring knowledge between heterogeneous scenarios with fundamentally different unit dynamics

## Confidence
**High Confidence**: The core methodology of using fixed-size Influence Maps for state representation is technically sound and well-implemented. The experimental setup using SMAC benchmarks is appropriate and the statistical evaluation across 31 random seeds is rigorous.

**Medium Confidence**: The claim that this approach significantly improves learning efficiency over learning from scratch is supported by the results, but the absolute performance metrics could be better contextualized against state-of-the-art MARL methods that weren't included in the comparison.

**Low Confidence**: The assertion that the method demonstrates "robustness" across scenarios is not fully substantiated - the results show successful transfer in tested cases but don't prove the approach won't fail catastrophically in scenarios with more complex unit interactions or larger agent counts.

## Next Checks
1. **Ablation Study on Influence Map Resolution**: Systematically test different IM grid sizes (19×19, 37×37, 55×55) to determine the minimum resolution that preserves transfer effectiveness, establishing whether information loss occurs at smaller resolutions.

2. **Transfer Path Sensitivity Analysis**: Evaluate Curriculum Transfer Learning with alternative progression paths (e.g., 8m → 3m → 2s3z, or 25m → 2s3z directly) to determine whether the specific order tested is critical or if the approach is robust to different learning sequences.

3. **Catastrophic Forgetting Evaluation**: After training on heterogeneous scenarios, test whether the model completely loses the ability to perform well on the original homogeneous scenarios it was trained on, measuring performance degradation to assess knowledge retention.