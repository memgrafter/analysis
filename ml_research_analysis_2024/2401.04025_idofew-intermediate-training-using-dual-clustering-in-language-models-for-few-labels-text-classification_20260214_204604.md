---
ver: rpa2
title: 'IDoFew: Intermediate Training Using Dual-Clustering in Language Models for
  Few Labels Text Classification'
arxiv_id: '2401.04025'
source_url: https://arxiv.org/abs/2401.04025
tags:
- text
- classification
- language
- clustering
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of few-label text classification,
  where limited labeled data leads to poor performance in pre-trained language models.
  To tackle this, the authors propose IDoFew, a novel two-stage clustering approach
  that improves pseudo-label generation for intermediate training.
---

# IDoFew: Intermediate Training Using Dual-Clustering in Language Models for Few Labels Text Classification

## Quick Facts
- arXiv ID: 2401.04025
- Source URL: https://arxiv.org/abs/2401.04025
- Reference count: 40
- Primary result: Achieved accuracy gains up to 45.87% on DBpedia, 20.16% on AG News, 12.54% on SMS Spam, and 9.98% on ISEAR using BERT as base model

## Executive Summary
The paper addresses the challenge of few-label text classification by proposing IDoFew, a novel two-stage clustering approach for improving pseudo-label generation during intermediate training of language models. The method combines Spectral Information Bottleneck (SIB) clustering on the full dataset with KMeans clustering on a small fraction of the data to correct errors from the first stage. This dual-clustering strategy significantly outperforms state-of-the-art models across seven benchmark datasets, demonstrating substantial accuracy improvements when limited labeled data is available.

## Method Summary
IDoFew introduces a two-stage clustering framework for intermediate training of language models in few-label scenarios. The first stage applies SIB clustering to the entire dataset to generate initial pseudo-labels, capturing complex semantic relationships. The second stage applies KMeans clustering to a small fraction (1%) of the data to refine and correct errors from the first stage. This corrected pseudo-labeling is then used for intermediate training of pre-trained models like BERT and RoBERTa. The approach leverages the strengths of both clustering methods: SIB's ability to handle high-dimensional text representations and KMeans' simplicity for fine-tuning corrections.

## Key Results
- Achieved 45.87% accuracy gain on DBpedia dataset compared to standard fine-tuning
- Demonstrated 20.16% improvement on AG News benchmark
- Showed 12.54% performance increase on SMS Spam classification
- Obtained 9.98% accuracy boost on ISEAR emotion dataset

## Why This Works (Mechanism)
The dual-clustering approach works by first capturing broad semantic patterns through SIB clustering, which can identify complex relationships in text data that single clustering methods might miss. The subsequent KMeans stage on a small labeled subset acts as a quality control mechanism, correcting misclassifications and refining the pseudo-labels. This hierarchical correction process creates higher-quality pseudo-labels that better represent the true data distribution, leading to more effective intermediate training. The method essentially combines unsupervised discovery of semantic structure with supervised refinement, bridging the gap between limited labeled data and the need for large-scale training signals.

## Foundational Learning

**Spectral Information Bottleneck (SIB) Clustering**: A clustering method that maximizes mutual information between data points and cluster assignments while minimizing information loss. Why needed: To capture complex semantic relationships in high-dimensional text embeddings. Quick check: Verify that SIB produces coherent clusters by examining representative samples from each cluster.

**KMeans Clustering**: A partitioning method that groups data points based on Euclidean distance to cluster centroids. Why needed: To provide a simple, interpretable refinement mechanism for correcting SIB clustering errors. Quick check: Ensure KMeans converges and produces stable cluster assignments across multiple runs.

**Pseudo-label Generation**: The process of assigning labels to unlabeled data using model predictions or clustering results. Why needed: To create synthetic training signals when true labels are scarce. Quick check: Measure pseudo-label quality by comparing against available validation labels.

**Intermediate Training**: Pre-training language models on task-specific data before fine-tuning on limited labeled examples. Why needed: To bridge the gap between general pre-training and specific task requirements. Quick check: Compare performance with and without intermediate training on the same dataset.

## Architecture Onboarding

**Component Map**: Raw Text -> Embedding Model -> SIB Clustering -> Initial Pseudo-labels -> KMeans Correction (1% data) -> Final Pseudo-labels -> Intermediate Training -> Fine-tuning

**Critical Path**: The dual-clustering process (SIB followed by KMeans) represents the critical innovation, as the quality of pseudo-labels directly determines downstream performance. The 1% labeled subset used for KMeans correction is crucial for achieving the reported accuracy gains.

**Design Tradeoffs**: The method trades increased computational complexity (two clustering stages) for improved label quality and final accuracy. Using only 1% labeled data balances the need for correction with the few-label constraint. The choice of SIB over simpler clustering methods prioritizes semantic coherence over computational efficiency.

**Failure Signatures**: Poor performance may manifest when: clustering algorithms fail to converge, the 1% labeled subset is not representative of the full data distribution, or the initial SIB clustering produces highly imbalanced pseudo-labels. Additionally, the method may underperform on datasets where semantic relationships are not well-captured by the embedding space.

**First Experiments**: 
1. Apply SIB clustering to a small dataset and visualize cluster distributions to verify semantic coherence
2. Run the complete dual-clustering pipeline on a single dataset class to observe correction patterns
3. Compare pseudo-label quality metrics (purity, normalized mutual information) between SIB-only and dual-clustering approaches

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Limited evaluation to BERT and RoBERTa base models without exploring more recent architectures
- Heavy reliance on benchmark datasets that may not reflect real-world few-label scenarios
- Computational overhead of dual-clustering approach not thoroughly analyzed against simpler alternatives

## Confidence
- High confidence in reported accuracy improvements on benchmark datasets
- Medium confidence in generalizability to diverse real-world scenarios
- Medium confidence in computational efficiency claims due to lack of thorough analysis

## Next Checks
1. Evaluate IDoFew on diverse dataset types including longer documents and multi-domain scenarios to test robustness across text characteristics
2. Compare computational efficiency against baseline methods while maintaining comparable accuracy levels to assess practical deployment viability
3. Conduct extensive ablation studies isolating the contributions of SIB clustering versus KMeans correction to understand which components drive performance improvements