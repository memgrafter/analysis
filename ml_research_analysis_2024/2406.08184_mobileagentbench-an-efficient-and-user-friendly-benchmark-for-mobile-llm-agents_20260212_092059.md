---
ver: rpa2
title: 'MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM
  Agents'
arxiv_id: '2406.08184'
source_url: https://arxiv.org/abs/2406.08184
tags:
- task
- agent
- agents
- arxiv
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MobileAgentBench, a new benchmark for evaluating
  large language model (LLM)-based mobile agents on Android devices. The key innovation
  is using app event signals captured via Android Accessibility Service to reliably
  determine task completion status, addressing the challenge of judging success in
  dynamic GUI navigation tasks.
---

# MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents

## Quick Facts
- arXiv ID: 2406.08184
- Source URL: https://arxiv.org/abs/2406.08184
- Reference count: 29
- Primary result: Mobile agents achieved 8-40% success rates on 100 tasks across 10 Android apps using Accessibility Service-based verification

## Executive Summary
MobileAgentBench introduces a new benchmark for evaluating large language model-based mobile agents on Android devices. The benchmark leverages Android Accessibility Service to capture app event signals, providing reliable task completion verification for dynamic GUI navigation tasks. With 100 tasks across 10 apps of varying difficulty, the benchmark is designed for easy integration requiring fewer than 10 lines of additional code. The evaluation of five state-of-the-art mobile agents revealed success rates ranging from 8% to 40%, with AppAgent achieving the highest performance.

## Method Summary
The benchmark uses Android Accessibility Service to monitor app event signals and determine task completion status, addressing the challenge of judging success in dynamic GUI navigation tasks. It includes 100 tasks across 10 Android apps with varying difficulty levels. The evaluation framework measures success rate, step-wise efficiency, latency, token usage, and false positive/negative rates. The benchmark is designed to be easily integrated with minimal code modifications, requiring fewer than 10 lines of additional code for implementation.

## Key Results
- Success rates ranged from 8% to 40% across five evaluated agents
- AppAgent achieved the highest success rate of 40%
- AutoDroid and CogAgent showed very high false negative rates, indicating frequent premature termination
- The benchmark successfully measured step-wise efficiency, latency, and token usage across all evaluated agents

## Why This Works (Mechanism)
The benchmark leverages Android Accessibility Service to capture real-time app event signals, providing objective verification of task completion status. This approach addresses the fundamental challenge of determining when GUI navigation tasks are successfully completed in dynamic mobile environments. By monitoring system-level events rather than relying on surface-level observations, the verification process becomes more reliable and consistent across different agent implementations.

## Foundational Learning

**Android Accessibility Service**
*Why needed:* Provides system-level access to app events and UI changes
*Quick check:* Can capture and process accessibility events from target applications

**Task Completion Verification**
*Why needed:* Determines when agents successfully complete navigation tasks
*Quick check:* Correctly identifies task success/failure states in test scenarios

**Mobile GUI Navigation**
*Why needed:* Foundation for understanding how agents interact with app interfaces
*Quick check:* Agent can navigate through multi-step tasks across different app types

## Architecture Onboarding

**Component Map**
MobileAgentBench -> Accessibility Service -> Task Verifier -> Performance Metrics -> Agent Evaluation

**Critical Path**
Agent executes task -> Accessibility Service captures events -> Task Verifier checks completion -> Performance metrics recorded -> Results aggregated

**Design Tradeoffs**
Accessibility Service provides reliable verification but requires additional permissions and system access. The benchmark balances comprehensiveness with ease of integration, though this may limit generalizability to non-Android platforms.

**Failure Signatures**
- High false negative rates indicate agents stopping prematurely
- Low success rates suggest navigation or decision-making deficiencies
- High latency may indicate inefficient task execution strategies

**First Experiments**
1. Run benchmark with a simple scripted agent to verify basic functionality
2. Test single-task execution to validate Accessibility Service integration
3. Compare results between two different agents on identical tasks to confirm measurement consistency

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Success rates based on only five evaluated agents and 100 tasks may not represent full real-world difficulty
- No quantitative comparison provided against prior verification methods to validate claimed improvements
- Benchmark assumes English language tasks only, limiting multilingual applicability
- Ease-of-integration claim lacks independent verification across different LLM frameworks

## Confidence
High confidence in methodological framework and task diversity
Medium confidence in success rate comparisons between agents
Low confidence in claimed ease-of-integration benefit without independent validation

## Next Checks
1. Independent replication with at least two additional mobile agents to verify success rate ranges and performance rankings
2. Quantitative comparison study between Accessibility Service approach and prior verification methods
3. Integration test demonstrating "fewer than 10 lines of code" setup across three different LLM agent frameworks