---
ver: rpa2
title: 'BLINK: Multimodal Large Language Models Can See but Not Perceive'
arxiv_id: '2404.12390'
source_url: https://arxiv.org/abs/2404.12390
tags:
- image
- visual
- multimodal
- point
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Blink, a new benchmark for multimodal large
  language models (LLMs) that focuses on core visual perception abilities not found
  in other evaluations. Most of the Blink tasks can be solved by humans "within a
  blink" (e.g., relative depth estimation, visual correspondence, forensics detection,
  and multi-view reasoning).
---

# BLINK: Multimodal Large Language Models Can See but Not Perceive

## Quick Facts
- **arXiv ID**: 2404.12390
- **Source URL**: https://arxiv.org/abs/2404.12390
- **Reference count**: 40
- **Primary result**: Current multimodal LLMs achieve only 45.72-51.26% accuracy on Blink perception tasks, compared to human 95.70% accuracy

## Executive Summary
This paper introduces Blink, a benchmark testing core visual perception abilities in multimodal large language models. The benchmark reformats 14 classic computer vision tasks into 3,807 multiple-choice questions that humans can solve "within a blink" - such as relative depth estimation, visual correspondence, forensics detection, and multi-view reasoning. Despite their impressive capabilities, current multimodal LLMs including GPT-4V and Gemini perform only marginally better than random guessing on these tasks, achieving 45.72% and 51.26% accuracy respectively. The study reveals a significant gap between human-level visual perception and what current multimodal LLMs can achieve, suggesting that perception abilities have not yet emerged in these models.

## Method Summary
The Blink benchmark transforms traditional computer vision tasks into multiple-choice question format, pairing each question with single or multiple images and visual prompting. The benchmark covers 14 classic computer vision tasks including relative depth estimation, visual correspondence, forensics detection, and multi-view reasoning. Human performance was established through testing, with an average accuracy of 95.70% across all tasks. The benchmark was then evaluated on several state-of-the-art multimodal LLMs including GPT-4V and Gemini, with specialist computer vision models also tested for comparison. The evaluation measures accuracy on the multiple-choice questions to assess the models' core visual perception capabilities.

## Key Results
- Humans achieve 95.70% average accuracy on Blink tasks, solving them "within a blink"
- GPT-4V achieves only 51.26% accuracy, just 13.17% above random guessing
- Gemini achieves only 45.72% accuracy, just 7.63% above random guessing
- Specialist CV models significantly outperform multimodal LLMs on perception tasks

## Why This Works (Mechanism)
The Blink benchmark works by isolating core visual perception abilities that resist mediation through natural language. These tasks require immediate visual understanding that current multimodal LLMs struggle to process, even with their sophisticated language understanding capabilities. The benchmark reveals that while LLMs can process and reason about visual information when it can be described through language, they fail at fundamental perception tasks that humans solve intuitively. This suggests that current multimodal architectures lack the necessary visual processing pathways for rapid, intuitive visual understanding.

## Foundational Learning
**Visual Perception Fundamentals**: Understanding of core visual tasks like depth estimation, correspondence matching, and forensic detection - needed to appreciate what Blink tests; quick check: can you explain how humans judge relative depth in images?

**Multimodal Model Architecture**: Knowledge of how current multimodal LLMs integrate visual and language processing - needed to understand architectural limitations; quick check: what are the key differences between vision-language models and pure CV models?

**Computer Vision vs. Multimodal Approaches**: Understanding of specialist CV model capabilities versus multimodal LLM approaches - needed to interpret comparative results; quick check: how do traditional CV models approach perception tasks differently from multimodal LLMs?

## Architecture Onboarding
**Component Map**: Image Input -> Visual Encoder -> Language Model Integration -> Natural Language Reasoning -> Multiple Choice Output

**Critical Path**: Visual perception tasks require rapid feature extraction and spatial reasoning, but current multimodal LLMs must translate visual information into language tokens before processing, creating a bottleneck for perception tasks.

**Design Tradeoffs**: Multimodal LLMs prioritize language understanding and reasoning over specialized visual processing, sacrificing perception speed and accuracy for versatility in language tasks.

**Failure Signatures**: Models fail on tasks requiring immediate visual judgment, struggle with spatial reasoning, and cannot leverage visual context as efficiently as humans or specialist CV models.

**First Experiments**:
1. Test Blink tasks with open-ended responses instead of multiple-choice
2. Compare performance using different visual encoding methods
3. Evaluate specialist CV model integration with multimodal LLMs

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies on multiple-choice format which may constrain model performance
- 3,807 questions represent a curated subset that may not capture all visual perception capabilities
- Assumes "within a blink" human performance is consistent across all task types
- Comparison doesn't fully account for different training objectives between CV and multimodal models

## Confidence

**High Confidence**: Empirical findings showing multimodal LLMs significantly underperform humans on Blink tasks; robust comparative analysis between GPT-4V, Gemini, and human performance.

**Medium Confidence**: Assertion that perception tasks resist mediation through natural language; recommendation for specialist CV model integration as improvement path.

**Low Confidence**: Claim that perception abilities have not "emerged" in multimodal LLMs; assumes emergence is expected development pathway.

## Next Checks
1. Test Blink tasks with open-ended response formats rather than multiple-choice to determine if format constraints artificially limit LLM performance.

2. Conduct cross-cultural validation studies to assess whether the benchmark's perception tasks are universally human-solvable or culturally dependent.

3. Implement ablation studies testing different visual encoding methods and prompt engineering strategies to identify specific bottlenecks in LLM visual perception capabilities.