---
ver: rpa2
title: Guardrail Baselines for Unlearning in LLMs
arxiv_id: '2403.03329'
source_url: https://arxiv.org/abs/2403.03329
tags:
- unlearning
- arxiv
- finetuning
- preprint
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors evaluate prompt-based and filtering-based \u201Cguardrail\u201D\
  \ approaches for unlearning in LLMs, comparing them to finetuning baselines on three\
  \ benchmarks. They find that simple prompting can achieve competitive or superior\
  \ results to finetuning for broad topic unlearning (e.g., Harry Potter), while output\
  \ filtering is effective for small forget sets."
---

# Guardrail Baselines for Unlearning in LLMs

## Quick Facts
- arXiv ID: 2403.03329
- Source URL: https://arxiv.org/abs/2403.03329
- Reference count: 24
- Key outcome: Simple prompting can achieve competitive or superior results to finetuning for broad topic unlearning, while output filtering is effective for small forget sets

## Executive Summary
This paper evaluates prompt-based and filtering-based "guardrail" approaches for unlearning in large language models, comparing them to finetuning baselines on three benchmarks. The authors find that lightweight guardrails can provide practical unlearning capabilities without the expense of updating model weights. Simple prompting shows competitive or superior results to finetuning for broad topic unlearning (e.g., Harry Potter), while output filtering performs well for small forget sets. The study also reveals weaknesses in benchmark design and metrics, highlighting the need for better evaluation methods that distinguish guardrails from finetuning.

## Method Summary
The paper evaluates guardrail approaches (prompting and filtering) as lightweight baselines for unlearning in LLMs, comparing them to finetuning baselines. Three benchmarks are used: "Who's Harry Potter?" (300 Harry Potter-related questions), TOFU (synthetic dataset of 4000 questions about fictional authors), and WMDP (multiple-choice expert-level questions about biology, cybersecurity, and chemistry). Guardrail approaches include prompt prefix (input preprocessing) and filtering (input and output postprocessing). The authors evaluate these on LLaMA-2-7b-chat-hf, LLaMA-2-13b, and GPT-4, measuring forget accuracy, retain performance, and conversational fluency.

## Key Results
- Prompting achieved comparable or superior results to finetuning for broad topic unlearning (e.g., Harry Potter)
- Output filtering was highly effective on TOFU tasks with small forget sets
- Input filtering performed well for multiple-choice settings in WMDP
- Guardrails exposed weaknesses in benchmark design and metrics, highlighting evaluation challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simple prompting can effectively suppress knowledge about specific topics without model weight updates
- Mechanism: Prefix the query with instructions to forget the target topic, leveraging the LLM's ability to follow instructions in-context
- Core assumption: The LLM's attention mechanism can be steered by prompt prefixes to alter output distribution
- Evidence anchors:
  - [abstract] "we show that simple guardrail-based approaches such as prompting and filtering can achieve unlearning results comparable to finetuning"
  - [section 4.1] "We first replicate the benchmark and metric from Eldan and Russinovich (2023) using prompting. Each prompt is augmented with the prefix: 'You are an AI Assistant who is supposed to unlearn about the book series Harry Potter...'"
  - [corpus] Weak - corpus contains no direct evidence for prompting mechanism effectiveness
- Break condition: When the model's instruction-following capability is insufficient for the complexity of the unlearn request, or when adversarial queries bypass the prompt constraints

### Mechanism 2
- Claim: Output filtering can achieve high forget accuracy for small forget sets by refusing to answer questions about forgotten content
- Mechanism: Post-process model outputs through a filter model that classifies whether the input contains forgotten content, and either blocks or modifies the response
- Core assumption: A separate model can accurately classify whether a query relates to forgotten content without requiring knowledge of the forgotten data itself
- Evidence anchors:
  - [section 4.2] "Instead, we evaluate another very simple strategy for TOFU: implementing a postprocessing filter on the output of the finetuned LLaMA-2 model"
  - [section 4.2] "We found that output filtering could be highly effective on the TOFU tasks"
  - [corpus] Weak - corpus contains guardrail papers but no direct evidence for output filtering on unlearning
- Break condition: When the forget set grows large enough that the filter model cannot generalize effectively, or when queries are formulated to evade detection by the filter

### Mechanism 3
- Claim: Input filtering can effectively block queries about forgotten topics by modifying or rejecting the input before it reaches the base model
- Mechanism: Pre-process the input query with a classifier that determines if the query relates to forgotten content, and either reject it or modify it with a prefix to generate incorrect answers
- Core assumption: The filter model can reliably distinguish between queries about forgotten vs. retained content based on topic features
- Evidence anchors:
  - [section 4.3] "To mitigate this, we instead apply a filter directly to the input (question)"
  - [section 4.3] "One complication is that this trivial filter does not take into account questions that have nothing to do with biology or cybersecurity"
  - [corpus] Weak - corpus contains no direct evidence for input filtering effectiveness on unlearning
- Break condition: When the filter cannot generalize to novel query formulations, or when the distinction between forget/retain sets is not based on easily identifiable features

## Foundational Learning

- Concept: Instruction-following in LLMs
  - Why needed here: The prompting approach relies on the model's ability to understand and execute instructions embedded in the prompt prefix
  - Quick check question: How would you test whether an LLM reliably follows a given instruction prefix across different query types?

- Concept: Classification for content filtering
  - Why needed here: Both input and output filtering strategies depend on a classifier that can distinguish between queries about forgotten vs. retained content
  - Quick check question: What features would you extract from a query to train a binary classifier for filtering unlearned content?

- Concept: Prompt engineering principles
  - Why needed here: The effectiveness of prompting approaches depends on crafting prefixes that clearly communicate the unlearn objective without degrading performance on unrelated tasks
  - Quick check question: How would you systematically test different prompt formulations to optimize for both forget accuracy and retain performance?

## Architecture Onboarding

- Component map:
  Base LLM -> Prompt module -> Input filter -> Output filter -> Evaluation harness

- Critical path:
  1. Query arrives at the system
  2. Input filter (if present) classifies and potentially modifies the query
  3. Prompt module (if present) prepends instruction prefix
  4. Base LLM generates response
  5. Output filter (if present) classifies and potentially modifies the response
  6. Final response is returned

- Design tradeoffs:
  - Prompting vs. filtering: Prompting is simpler but may require careful engineering; filtering adds complexity but can be more robust
  - Single vs. multi-stage filtering: Single filters are simpler but may miss edge cases; multi-stage can handle more complex scenarios but increases latency
  - Filter model selection: Using the same model as base vs. a different model involves tradeoffs between consistency and capability

- Failure signatures:
  - High forget accuracy but poor retain performance: Filter is too aggressive or prompt is degrading general capability
  - Low forget accuracy: Filter is not specific enough or prompt is not being followed
  - Inconsistent results across similar queries: Classifier is not robust to query variations

- First 3 experiments:
  1. Test prompting approach on a simple benchmark (like WHP) with a basic prefix to establish baseline effectiveness
  2. Implement and evaluate a simple keyword-based output filter on a small forget set to test filtering viability
  3. Compare input vs. output filtering on a multiple-choice benchmark to determine which format works better for different query types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can prompting and filtering guardrails scale effectively to larger unlearning tasks with hundreds or thousands of concepts?
- Basis in paper: Explicit - The authors note that "as the number of topics or items to be deleted increases, the guardrail may become less effective and less efficient to evaluate" and observe degradation in performance on larger forget sets in TOFU
- Why unresolved: The paper only tests guardrails on relatively small unlearning tasks. It's unclear how well these methods would perform when trying to unlearn many concepts simultaneously
- What evidence would resolve it: Experiments evaluating guardrails on benchmarks with larger and more numerous forget sets, comparing performance and efficiency to finetuning baselines

### Open Question 2
- Question: How robust are prompting and filtering guardrails to adversarial attacks designed to bypass the unlearning restrictions?
- Basis in paper: Explicit - The authors state they did not explore "explicitly adversarial inputs that attempt to break the prompt prefix" and note that prompt-based approaches "may be more susceptible to these attacks"
- Why unresolved: The paper only considers an "honest but curious" adversary model. It's unclear how vulnerable guardrails are to more sophisticated attacks
- What evidence would resolve it: Experiments systematically testing guardrails against a range of adversarial attacks, measuring success rates and comparing robustness to finetuning approaches

### Open Question 3
- Question: What are the best practices for designing effective and efficient guardrails for unlearning in LLMs?
- Basis in paper: Explicit - The authors note that "prompts required some trial and error to develop" and that "tailoring a prompt for each model may be a more fair comparison and produce better results"
- Why unresolved: The paper uses relatively simple and generic guardrails. It's unclear what factors make guardrails more or less effective, and how to optimize them for different unlearning scenarios
- What evidence would resolve it: Systematic experiments varying guardrail design choices (e.g., prompt wording, filter architectures) and measuring impact on unlearning performance, to identify best practices and design principles

## Limitations

- The findings are based on three specific benchmarks that may not generalize to all unlearning scenarios
- The evaluation relies heavily on GPT-4 as both a base model and evaluator, potentially introducing bias
- Filtering approaches may degrade as forget sets grow larger, though this scaling behavior is not thoroughly explored
- Prompting effectiveness depends on the LLM's instruction-following capabilities, which vary significantly across models

## Confidence

- High confidence: Guardrail approaches are viable baselines that should be considered alongside finetuning for unlearning tasks
- Medium confidence: Prompting is particularly effective for broad topic unlearning like "Who's Harry Potter?"
- Medium confidence: Output filtering shows strong results for small forget sets in synthetic benchmarks
- Low confidence: Input filtering's effectiveness on expert-level multiple-choice questions due to limited testing scope

## Next Checks

1. Test the guardrail approaches on a larger and more diverse set of benchmarks, including real-world unlearning scenarios with varying forget set sizes and topic complexities.

2. Evaluate the robustness of the prompting and filtering approaches across different base model architectures (e.g., GPT-3.5, Claude, open-source alternatives) to assess generalizability.

3. Conduct a systematic analysis of how guardrail effectiveness scales with forget set size, measuring performance degradation points for both prompting and filtering approaches.