---
ver: rpa2
title: 'GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via VLM'
arxiv_id: '2407.10870'
source_url: https://arxiv.org/abs/2407.10870
tags:
- shot
- ultrasound
- gpt-4o
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large vision-language models (LVLMs)
  such as GPT-4o can classify hand gestures from forearm ultrasound images without
  fine-tuning, and performance improves with few-shot in-context learning. Using ultrasound
  data from 3 subjects performing 5 hand gestures, the authors show that GPT-4o achieves
  over 70% accuracy for cross-session gesture classification using just 2-3 in-context
  examples per class.
---

# GPT Sonograpy: Hand Gesture Decoding from Forearm Ultrasound Images via VLM

## Quick Facts
- arXiv ID: 2407.10870
- Source URL: https://arxiv.org/abs/2407.10870
- Authors: Keshav Bimbraw; Ye Wang; Jing Liu; Toshiaki Koike-Akino
- Reference count: 27
- Primary result: GPT-4o achieves over 70% accuracy for cross-session hand gesture classification from forearm ultrasound using 2-3 in-context examples per class

## Executive Summary
This paper demonstrates that large vision-language models (LVLMs) such as GPT-4o can classify hand gestures from forearm ultrasound images without fine-tuning, and performance improves with few-shot in-context learning. Using ultrasound data from 3 subjects performing 5 hand gestures, the authors show that GPT-4o achieves over 70% accuracy for cross-session gesture classification using just 2-3 in-context examples per class. This approach offers a promising alternative to computationally expensive fine-tuning of large models for specialized medical imaging tasks, potentially enabling powerful AI assistance tools in healthcare settings.

## Method Summary
The authors collected ultrasound imaging data from the forearm of subjects performing various hand gestures. They then prompted GPT-4o with these images along with a few in-context examples to classify the gestures. The key innovation was leveraging the model's existing capabilities through prompting rather than fine-tuning, demonstrating that LVLMs can effectively process medical ultrasound imagery when provided with appropriate contextual examples.

## Key Results
- GPT-4o achieved over 70% accuracy for cross-session gesture classification using only 2-3 in-context examples per class
- Performance improved with additional in-context learning examples
- The approach worked without any model fine-tuning, demonstrating zero-shot capabilities

## Why This Works (Mechanism)
The paper leverages the multimodal capabilities of GPT-4o to process ultrasound imagery through natural language prompting. The model's pre-trained understanding of visual patterns and spatial relationships allows it to interpret ultrasound images without domain-specific fine-tuning. In-context learning enables the model to adapt to the specific task through few examples, effectively bridging the gap between general visual understanding and specialized medical image interpretation.

## Foundational Learning
- **Ultrasound imaging physics**: Understanding how ultrasound waves interact with tissue and create image patterns is crucial for interpreting why certain anatomical features appear in the images. Quick check: Verify the fundamental principles of A-mode and B-mode ultrasound imaging.
- **Forearm anatomy**: Knowledge of muscle, tendon, and bone structures in the forearm helps explain why different hand gestures produce distinct ultrasound signatures. Quick check: Map the major anatomical structures visible in forearm ultrasound imaging.
- **In-context learning**: The mechanism by which LLMs adapt to new tasks through prompt examples without parameter updates. Quick check: Demonstrate how prompt engineering affects model performance on unseen tasks.
- **Multimodal model architecture**: Understanding how vision-language models process and integrate visual and textual information. Quick check: Trace the information flow from image input to text output in multimodal models.
- **Medical image classification**: The standard approaches for training models to recognize patterns in medical imagery. Quick check: Compare traditional supervised learning approaches with few-shot prompting methods.
- **Cross-session consistency**: The challenges of maintaining model performance across different imaging sessions due to physiological variations. Quick check: Identify factors that affect ultrasound image consistency across time.

## Architecture Onboarding
**Component map**: Ultrasound image acquisition -> Image preprocessing -> Prompt construction -> GPT-4o model -> Text output -> Classification result

**Critical path**: The core workflow involves capturing ultrasound images, formatting them with appropriate prompts containing few-shot examples, and processing through GPT-4o to obtain gesture classifications.

**Design tradeoffs**: The approach trades computational efficiency (no fine-tuning) for potentially lower accuracy compared to specialized models. The few-shot method requires minimal data but may not capture subtle nuances that extensive training would reveal.

**Failure signatures**: Poor performance may manifest as inconsistent classifications across similar images, confusion between anatomically similar gestures, or degradation in cross-session scenarios where anatomical appearance changes.

**Three first experiments**:
1. Test classification accuracy with varying numbers of in-context examples (1, 2, 3, 5 examples per class)
2. Evaluate performance across different ultrasound imaging parameters (frequency, depth, gain settings)
3. Assess model robustness to image quality variations (different operators, equipment, positioning)

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (3 subjects) limits generalizability across diverse populations
- Limited gesture vocabulary (5 gestures) raises concerns about scalability to more complex gesture sets
- Cross-session performance, while promising at over 70%, still represents a substantial error rate for clinical applications

## Confidence
- Claim about GPT-4o's ability to classify gestures without fine-tuning: **High** confidence
- Assertion that this approach offers a "promising alternative" to fine-tuning: **Medium** confidence
- Suggestion about enabling "powerful AI assistance tools in healthcare settings": **Low** confidence

## Next Checks
1. Evaluate the approach with a larger, more diverse participant pool (minimum 20 subjects) to assess generalizability across different demographics and forearm anatomies.
2. Test the model on an expanded gesture vocabulary (at least 15-20 distinct gestures) to evaluate scalability and identify performance degradation patterns.
3. Conduct longitudinal studies with multiple sessions per subject to quantify model robustness to physiological changes over time and validate the cross-session claims under real-world conditions.