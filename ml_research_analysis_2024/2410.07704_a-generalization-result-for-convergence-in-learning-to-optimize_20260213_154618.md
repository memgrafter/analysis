---
ver: rpa2
title: A Generalization Result for Convergence in Learning-to-Optimize
arxiv_id: '2410.07704'
source_url: https://arxiv.org/abs/2410.07704
tags:
- algorithm
- learning
- which
- convergence
- measurable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of proving convergence for learned
  optimization algorithms, which lack theoretical guarantees despite empirical success.
  The authors develop a probabilistic framework that generalizes classical optimization
  convergence results to learning-to-optimize.
---

# A Generalization Result for Convergence in Learning-to-Optimize

## Quick Facts
- arXiv ID: 2410.07704
- Source URL: https://arxiv.org/abs/2410.07704
- Reference count: 19
- Primary result: A PAC-Bayesian framework proving learned optimization algorithms converge to critical points with high probability on unseen problems

## Executive Summary
This paper addresses the challenge of proving convergence for learned optimization algorithms, which lack theoretical guarantees despite empirical success. The authors develop a probabilistic framework that generalizes classical optimization convergence results to learning-to-optimize. Their core idea is to show that properties of the algorithm's trajectory that ensure convergence to critical points generalize to unseen problems using PAC-Bayesian generalization theory. The main result is a theorem that lower bounds the probability of convergence to a critical point for parametric classes of non-smooth, non-convex loss functions. The approach is demonstrated through experiments on quadratic problems and training a neural network, showing the learned algorithm converges faster than classical baselines and achieves high probability of convergence as predicted by the theory.

## Method Summary
The paper develops a probabilistic framework for proving convergence of learned optimization algorithms using PAC-Bayesian generalization theory. The approach models the algorithm as a Markov process with hyperparameters H, problem parameters P, and randomness ξ. It identifies measurable sets encoding sufficient descent, relative error, and boundedness conditions that imply convergence to critical points via Kurdyka-Łojasiewicz theory. The PAC-Bayesian bound then relates the empirical probability of these conditions holding on training data to the true probability on unseen problems, providing a lower bound on convergence probability. The training procedure uses progressive probabilistic constraining to find hyperparameters satisfying the required conditions.

## Key Results
- Proves convergence of learned optimization algorithms to critical points with high probability using PAC-Bayesian generalization
- Shows learned algorithm converges faster than classical baselines (HBF) on quadratic problems
- Demonstrates high probability of convergence on neural network training matching theoretical predictions
- First theoretical framework connecting learning-to-optimize with classical non-convex optimization convergence theory

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Properties sufficient for convergence to critical points in non-smooth non-convex optimization generalize from training to unseen problems with high probability.
- Mechanism: The authors establish measurable sets encoding sufficient descent, relative error, and boundedness conditions. They show these conditions imply convergence via Attouch et al. (2013), then use PAC-Bayesian generalization to bound the probability these conditions hold on unseen problems.
- Core assumption: The loss function ℓ(·,p) is a Kurdyka-Łojasiewicz function for every p, and the subdifferential map is outer semi-continuous.
- Evidence anchors:
  - [abstract]: "our main theorem is a generalization result for parametric classes of potentially non-smooth, non-convex loss functions and establishes the convergence of learned optimization algorithms to critical points with high probability."
  - [section 7.2]: "Suppose that Assumptions 6.1, 6.2, and 6.7 hold. Further, assume that ℓ(·, p) is a Kurdyka– Lojasiewicz function for every p ∈ P."
  - [corpus]: Weak evidence - related works discuss PAC-Bayesian bounds and generalization but don't directly address this specific combination of KL functions and convergence.
- Break condition: If ℓ(·,p) is not a KL function, or if the sufficient descent/relative error conditions fail to hold on unseen problems, the generalization guarantee breaks down.

### Mechanism 2
- Claim: The convergence analysis does not require restricting the learned algorithm's update step, unlike traditional safeguarding approaches.
- Mechanism: By working probabilistically rather than deterministically, the authors avoid needing geometric constraints on updates. They only need to bound the probability that conditions hold, not guarantee they always hold.
- Core assumption: The algorithm's update can be modeled as a Markov process satisfying measurability conditions.
- Evidence anchors:
  - [abstract]: "This effectively generalizes the results of a worst-case analysis into a probabilistic framework, and frees the design of the learned algorithm from using safeguards."
  - [section 2]: "Another approach, pioneered by Gregor and LeCun (2010), is unrolling, which limits the number of iterations, yet can be applied to any iterative algorithm."
  - [corpus]: Strong evidence - related works discuss constrained methods and their limitations, supporting the claim that this approach avoids those restrictions.
- Break condition: If the Markovian structure assumption fails (e.g., the algorithm has long-term memory dependencies), the probabilistic framework may not apply.

### Mechanism 3
- Claim: The approach provides quantitative bounds on convergence probability that improve with more training data.
- Mechanism: The PAC-Bayesian bound provides a lower bound on P(P,ξ)|H {Aconv} that depends on empirical estimates from N training problems, with tighter bounds as N increases.
- Core assumption: The prior distribution PH over hyperparameters and the data distribution PS are well-behaved.
- Evidence anchors:
  - [abstract]: "The main result is a theorem that lower bounds the probability of convergence to a critical point for parametric classes of non-smooth, non-convex loss functions."
  - [section 7.2]: "For larger N, we have more confidence in our estimate, so we can choose a larger λ which dampens the last term and tightens the lower bound for P(P,ξ)|H {Aconv}."
  - [corpus]: Moderate evidence - PAC-Bayesian bounds are known to improve with data, but specific application to optimization convergence is novel.
- Break condition: If the empirical estimates are biased or the prior is poorly chosen, the bounds may not improve meaningfully with more data.

## Foundational Learning

- Concept: Kurdyka-Łojasiewicz (KL) functions and their role in non-convex optimization
  - Why needed here: KL functions ensure convergence to critical points under sufficient descent and relative error conditions. The theory relies on this property to guarantee that sequences satisfying certain conditions actually converge.
  - Quick check question: Why do KL functions help prove convergence in non-convex optimization where traditional convex analysis fails?

- Concept: PAC-Bayesian generalization theory and change of measure inequalities
  - Why needed here: Provides the probabilistic framework to transfer convergence guarantees from training data to unseen problems. The Kullback-Leibler divergence bounds how much the learned posterior can deviate from the prior.
  - Quick check question: How does the PAC-Bayesian framework differ from traditional uniform convergence bounds in statistical learning?

- Concept: Measurability of sets in product spaces and outer semi-continuity of set-valued maps
  - Why needed here: Ensures the convergence and sufficient condition sets are measurable, which is necessary for applying the PAC-Bayesian theorem. The subdifferential's outer semi-continuity guarantees measurable selections.
  - Quick check question: Why can't we directly observe convergence events in infinite sequences, and how does measurability help circumvent this?

## Architecture Onboarding

- Component map: PH -> P(P,ξ)|H{A} -> PAC-Bayesian bound -> P(P,ξ)|H{Aconv}

- Critical path:
  1. Train algorithm A on N problem instances
  2. Verify sufficient descent and relative error conditions on training data
  3. Construct prior PH over hyperparameters
  4. Compute empirical estimates of P(Pn,ξn)|H,Pn {A}
  5. Apply PAC-Bayesian bound to get lower bound on P(P,ξ)|H {Aconv}

- Design tradeoffs:
  - More training data N improves bound tightness but increases computational cost
  - Choice of prior PH affects generalization - too restrictive limits performance, too loose weakens bounds
  - KL function assumption restricts applicable problem classes but enables convergence guarantees

- Failure signatures:
  - PAC-bound remains loose (empirical estimate << bound) suggests conditions don't generalize well
  - Algorithm fails on test problems despite good training performance indicates overfitting or violated assumptions
  - Convergence probability estimates drop significantly from training to test suggests distribution shift

- First 3 experiments:
  1. Quadratic optimization with varying strong convexity and smoothness - verify faster convergence than HBF baseline
  2. Neural network training on synthetic regression data - compare against Adam and check convergence probability
  3. Ablation study on KL function assumption - test on functions that violate KL property to see bound breakdown

## Open Questions the Paper Calls Out

- Can the theoretical framework be extended to stochastic optimization settings where the sufficient-descent condition may not hold?
  - Basis in paper: [explicit] The authors explicitly state that "due to the sufficient-descent condition, Theorem 6.5 is not well-suited for stochastic optimization" and note this as a direction for future work.
  - Why unresolved: The current convergence theorem relies on deterministic sufficient-descent and relative-error conditions that require full access to the function and subgradients at each iteration, which are not available in stochastic settings.
  - What evidence would resolve it: A proof showing that convergence to critical points can be guaranteed with high probability under stochastic oracle access, or a counterexample demonstrating fundamental limitations of such guarantees.

- How tight are the PAC-Bayesian bounds for convergence probability compared to empirical observations in practice?
  - Basis in paper: [explicit] The authors observe "there is a substantial gap P(P,ξ)|H {Aconv \ A}" between the convergence probability and the bound based on observable conditions, but note they "do not know the tightness of this bound."
  - Why unresolved: While the theoretical framework provides a lower bound on convergence probability, the actual gap between this bound and the true convergence probability remains unknown and could significantly impact practical applications.
  - What evidence would resolve it: A comprehensive empirical study comparing PAC-Bayesian bounds with actual convergence frequencies across diverse problem classes, potentially revealing systematic patterns in the tightness of the bounds.

- What is the computational complexity and sample complexity of the progressive constraining procedure needed to ensure the sufficient-descent and relative-error conditions hold with high probability?
  - Basis in paper: [inferred] The authors describe the progressive constraining procedure as "challenging and, unfortunately, not guaranteed to work" and note it is "key to get a meaningful guarantee," suggesting significant practical difficulties.
  - Why unresolved: While the theoretical framework is sound, the paper shows that ensuring the algorithm satisfies the required conditions during training is difficult and time-consuming, raising questions about scalability and practical feasibility.
  - What evidence would resolve it: Empirical analysis of the number of training iterations and problem instances required to achieve satisfactory convergence guarantees across different problem classes and algorithm architectures.

## Limitations
- The KL function assumption may be too restrictive for practical optimization problems
- The Markovian structure assumption may not hold for more complex learned algorithms with long-term dependencies
- Empirical validation is limited to specific problem classes (quadratics, neural network training), leaving open questions about generalizability

## Confidence
- **High**: The PAC-Bayesian generalization framework is well-established; the measurability conditions are standard in stochastic optimization theory
- **Medium**: The combination of KL functions with PAC-Bayesian bounds for optimization convergence is novel but relies on established techniques
- **Low**: The practical implications of the bounds and their tightness in realistic scenarios remains unclear from the limited experiments

## Next Checks
1. Test the bounds on a wider variety of optimization problems, including non-KL functions, to understand the practical limitations
2. Implement more complex learned algorithms with memory to test the Markovian assumption's necessity
3. Conduct systematic experiments varying N (training data size) to empirically verify the bound tightness improvements predicted by theory