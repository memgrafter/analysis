---
ver: rpa2
title: 'EmbodiedCity: A Benchmark Platform for Embodied Agent in Real-world City Environment'
arxiv_id: '2410.09604'
source_url: https://arxiv.org/abs/2410.09604
tags:
- embodied
- environment
- task
- tasks
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EmbodiedCity, a novel benchmark platform
  for evaluating embodied artificial intelligence (AI) in real-world city environments.
  The platform addresses the limitations of existing benchmarks that focus primarily
  on indoor scenarios by constructing a highly realistic 3D simulation environment
  based on a real commercial district in Beijing.
---

# EmbodiedCity: A Benchmark Platform for Embodied Agent in Real-world City Environment

## Quick Facts
- **arXiv ID**: 2410.09604
- **Source URL**: https://arxiv.org/abs/2410.09604
- **Reference count**: 40
- **Primary result**: Introduces EmbodiedCity, a benchmark platform for evaluating embodied AI in realistic urban environments.

## Executive Summary
This paper presents EmbodiedCity, a novel benchmark platform for evaluating multimodal large language models (MLLMs) in real-world city environments. The platform addresses the limitations of existing indoor-focused benchmarks by constructing a highly realistic 3D simulation environment based on a real commercial district in Beijing. It includes five essential embodied tasks: first-view scene understanding, question answering, dialogue, visual-language navigation, and task planning. The platform provides comprehensive input and output interfaces for embodied agents, enabling them to interact with the environment and receive performance evaluations. The authors evaluate popular MLLMs on this platform, demonstrating its effectiveness in assessing embodied intelligence capabilities.

## Method Summary
The EmbodiedCity platform constructs a 1:1 scale 3D model of a real Beijing commercial district in Unreal Engine 5.3, populated with dynamic pedestrian and vehicle flows. Agents interact via AirSim-based APIs to receive multimodal observations (RGB, depth, semantic, GPS, IMU, LiDAR) and execute actions. The benchmark comprises five embodied tasks evaluated using metrics like BLEU, ROUGE, METEOR, CIDEr for text generation, and success rate, SPL, and navigation error for visual-language navigation. Ground truth data is generated through a hybrid approach of LLM generation and human refinement.

## Key Results
- The platform successfully evaluates MLLMs on embodied tasks in a realistic urban environment, covering perception, reasoning, and decision-making.
- Embodied scene understanding task achieved BLEU-1 score of 0.8697 and CIDEr score of 1.7618.
- Embodied question answering task achieved 73.7% accuracy for count questions and 89.1% for position questions.
- Embodied VLN task achieved a success rate of 45.0% and SPL of 0.424.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embodied intelligence capabilities can be effectively evaluated by integrating high-fidelity 3D urban simulation with multimodal LLM agents.
- Mechanism: The platform constructs a 1:1 scale 3D model of a real Beijing commercial district in Unreal Engine, populated with dynamic pedestrian and vehicle flows. This provides continuous first-person RGB, depth, and semantic observations, along with GPS, IMU, and LiDAR sensor data. Agents interact via AirSim-based APIs to receive observations and execute actions, enabling the assessment of perception, reasoning, and decision-making in realistic urban environments.
- Core assumption: High visual and behavioral fidelity of the simulated environment translates to meaningful performance signals for real-world embodied intelligence.
- Evidence anchors:
  - [abstract] states the platform "constructs a highly realistic 3D simulation environment based on the real buildings, roads, and other elements in a real city."
  - [section] notes the environment "is based on the real streets, buildings, city elements, pedestrians, and traffic in one commercial district from one of China's largest cities, Beijing."
  - [corpus] includes "TongSIM: A General Platform for Simulating Intelligent Machines" and "MFE-ETP: A Comprehensive Evaluation Benchmark for Multi-modal Foundation Models on Embodied Task Planning" with similar FMR scores, suggesting related simulation research.
- Break condition: If the fidelity gap between simulation and reality is too large, agents may overfit to simulator artifacts, leading to poor generalization in real-world deployment.

### Mechanism 2
- Claim: Multimodal LLM agents can perform embodied tasks in urban environments by interpreting visual observations and language instructions to produce actionable plans or answers.
- Mechanism: Agents receive concatenated RGB, depth, and semantic observations plus natural language prompts (e.g., "Describe your surroundings" or "Navigate to the billboard"). The LLM processes these multimodal inputs and generates textual outputsâ€”either descriptive captions, answers to spatial questions, dialogue responses, navigation action sequences, or task plans. These outputs are then mapped to agent actions or evaluated against ground truth.
- Core assumption: LLMs pretrained on large multimodal corpora can generalize to novel embodied scenarios when given in-context observations and instructions.
- Evidence anchors:
  - [abstract] states "we evaluate some popular large language models for embodied intelligence capabilities of different dimensions and difficulties."
  - [section] details tasks like "embodied first-view scene understanding," "embodied question answering," and "embodied VLN," showing how LLMs are tested on perception, reasoning, and navigation.
  - [corpus] includes "Survey of Vision-Language-Action Models for Embodied Manipulation," suggesting research on multimodal agents in embodied tasks.
- Break condition: If the LLM's multimodal understanding is insufficient (e.g., poor spatial reasoning or visual grounding), it will fail to produce accurate outputs, especially in complex or long-horizon tasks.

### Mechanism 3
- Claim: Human refinement combined with LLM generation yields high-quality ground truth data for embodied AI benchmarks.
- Mechanism: The platform uses LLMs to generate initial responses for tasks like scene description and QA. Human annotators then review and correct these responses, filtering out errors such as wrong object counts, incorrect spatial relations, or unnecessary content. This hybrid approach balances cost and accuracy, producing reliable evaluation sets.
- Core assumption: LLM-generated content, when filtered by human experts, can efficiently produce high-quality annotations at scale.
- Evidence anchors:
  - [section] explicitly states "human refinement plays an important role" and describes filtering "low-quality responses or revise incorrect answers provided by LLMs."
  - [section] notes the dataset includes "87.1k cases" with "plenty of human efforts in data labeling," implying large-scale annotation.
  - [corpus] does not directly address human-in-the-loop annotation but includes related works like "CityEQA," suggesting embodied QA benchmarking.
- Break condition: If human annotators are inconsistent or biased, or if LLM errors are too frequent, the ground truth quality may degrade, undermining benchmark validity.

## Foundational Learning

- **Concept**: Multimodal large language models
  - Why needed here: The benchmark evaluates LLM agents on tasks requiring integration of visual, textual, and sometimes spatial information (e.g., "Which building is closer?" or "Navigate to the billboard"). Understanding how LLMs process and fuse these modalities is critical to interpreting results.
  - Quick check question: How does a multimodal LLM like GPT-4 Turbo combine image and text inputs to produce a navigation plan?

- **Concept**: Embodied AI and simulation environments
  - Why needed here: The platform simulates real-world city dynamics to test agents' ability to perceive, reason, and act in continuous 3D spaces. Knowing how simulators like Unreal Engine and AirSim model physics, sensors, and agent control is essential for debugging and extending the system.
  - Quick check question: What sensor modalities are available to agents in this platform, and how do they map to real-world equivalents?

- **Concept**: Evaluation metrics for embodied tasks
  - Why needed here: The benchmark uses BLEU, ROUGE, METEOR, CIDEr for text tasks, and success rate, SPL, and navigation error for VLN. Understanding these metrics helps assess model performance and identify failure modes.
  - Quick check question: Why might BLEU-1 and BLEU-4 scores diverge for the same task, and what does that indicate about the model's output?

## Architecture Onboarding

- **Component map**: Unreal Engine 5.3 city simulator -> AirSim plugin -> Python proxy server -> Python SDK -> LLM backends -> Human annotation pipeline -> Online platform

- **Critical path**: 1. Agent requests observation via SDK -> proxy server -> AirSim -> Unreal Engine 2. Unreal Engine returns multimodal sensor data (RGB, depth, semantic, GPS, IMU, LiDAR) 3. Agent processes data with LLM -> generates response or action plan 4. Agent sends action to proxy server -> AirSim -> Unreal Engine 5. Ground truth data (human-verified) used for evaluation

- **Design tradeoffs**: 
  - Unreal Engine vs. lightweight engines: Higher fidelity but heavier compute vs. faster iteration
  - Continuous vs. discrete actions: More realistic but harder to control and evaluate
  - Human-in-the-loop vs. fully automated annotation: Higher quality but slower and costlier
  - Multimodal input complexity: Richer context but increased latency and model size requirements

- **Failure signatures**:
  - High navigation error (NE) but high success rate (SR): Agent reaches goal but takes inefficient paths
  - Low BLEU-4 but high BLEU-1: Model generates short, generic descriptions lacking detail
  - Zero scores on counting/position QA: Model fails to detect or localize objects in images
  - LLM timeouts or API errors: Agent interface or multimodal processing pipeline issues

- **First 3 experiments**:
  1. Deploy a simple drone agent to fly to a fixed waypoint and report its surroundings using GPT-4 Turbo; verify observation and action APIs work
  2. Test embodied QA: Place agent at a known location, ask "How many traffic lights?" and compare LLM answer to ground truth
  3. Run embodied scene understanding: Capture eight-direction views at a landmark, generate description with Claude 3, and evaluate against human-verified captions using BLEU and CIDEr

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the EmbodiedCity platform be extended to support multi-agent collaboration tasks in urban environments?
- Basis in paper: [inferred] The paper mentions the potential for extending the benchmark to include multi-agent collaboration tasks, where coordination and communication between multiple agents are required to achieve common goals.
- Why unresolved: The current benchmark focuses on individual agent tasks and does not explore scenarios involving multiple agents working together. Implementing and evaluating multi-agent collaboration would require significant changes to the platform's architecture and task design.
- What evidence would resolve it: Evidence of successful multi-agent collaboration tasks in urban environments, demonstrating improved performance or new capabilities compared to single-agent approaches.

### Open Question 2
- Question: How can the EmbodiedCity platform be adapted to test human-agent interaction scenarios in real-world urban settings?
- Basis in paper: [inferred] The paper suggests creating scenarios where human users interact with agents, necessitating a more sophisticated understanding of human behavior and natural language. However, it does not provide details on how to implement or evaluate such interactions.
- Why unresolved: Human-agent interaction in real-world urban settings is complex and requires addressing challenges such as natural language understanding, social cues, and safety considerations. The current platform focuses on simulated environments and does not account for real-world human factors.
- What evidence would resolve it: Evidence of successful human-agent interaction scenarios in urban environments, demonstrating the platform's ability to support realistic and safe interactions between humans and agents.

### Open Question 3
- Question: How can the EmbodiedCity platform be used to evaluate an agent's adaptability and learning capabilities in dynamic urban environments?
- Basis in paper: [inferred] The paper mentions the potential for implementing tasks that test an agent's ability to learn from its environment and adapt to new and unforeseen scenarios. However, it does not provide details on how to design or evaluate such tasks.
- Why unresolved: Evaluating adaptability and learning in dynamic urban environments requires creating scenarios that change over time and measuring an agent's ability to adjust its behavior accordingly. The current platform focuses on static tasks and does not account for dynamic changes in the environment.
- What evidence would resolve it: Evidence of agents successfully adapting to dynamic changes in urban environments, demonstrating improved performance or new capabilities compared to static task approaches.

## Limitations
- **Unreal Engine dependency**: The platform's reliance on Unreal Engine 5.3 and AirSim creates a heavy computational and setup burden. Users must have compatible hardware and familiarity with these tools, limiting accessibility.
- **Limited physical realism**: While visually detailed, the platform does not simulate full physical dynamics (e.g., rigid body physics, friction, material properties). This limits its applicability to tasks requiring fine-grained physical interaction.
- **Annotation scalability**: The human-in-the-loop refinement process, while improving quality, is labor-intensive and may not scale efficiently to larger or more diverse urban datasets.

## Confidence
- **High confidence**: The core mechanism of using multimodal LLM agents to perform embodied tasks in a simulated urban environment is well-supported by the platform's design and preliminary evaluations.
- **Medium confidence**: The effectiveness of human refinement for ground truth generation is plausible but not quantitatively validated in the paper.
- **Low confidence**: The claim that this platform significantly expands the boundaries of embodied AI research is somewhat overstated without comparative studies against existing benchmarks.

## Next Checks
1. **Fidelity assessment**: Conduct a user study to measure the gap between simulator perception and real-world visual understanding. Have human annotators label real urban images and compare against simulator labels to quantify fidelity loss.
2. **Generalization test**: Deploy the same LLM agents on both EmbodiedCity and a simpler indoor benchmark (e.g., AI2-THOR). Compare performance drops to assess whether urban complexity adds meaningful challenge or just overfitting risk.
3. **Annotation efficiency analysis**: Measure the time and cost of human refinement per task type. Compare LLM-only vs. human-only annotation quality and cost to quantify the efficiency gains claimed by the hybrid approach.