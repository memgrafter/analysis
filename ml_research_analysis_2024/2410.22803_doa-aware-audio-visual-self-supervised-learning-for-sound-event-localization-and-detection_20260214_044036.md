---
ver: rpa2
title: DOA-Aware Audio-Visual Self-Supervised Learning for Sound Event Localization
  and Detection
arxiv_id: '2410.22803'
source_url: https://arxiv.org/abs/2410.22803
tags:
- audio
- data
- visual
- sound
- seld
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving sound event localization
  and detection (SELD) for spatial audio recordings captured by first-order ambisonics
  (FOA) microphones. The proposed method involves pretraining the feature extraction
  part of a deep neural network (DNN) in a self-supervised manner using spatial audio-visual
  recordings abundantly available as virtual reality contents.
---

# DOA-Aware Audio-Visual Self-Supervised Learning for Sound Event Localization and Detection

## Quick Facts
- arXiv ID: 2410.22803
- Source URL: https://arxiv.org/abs/2410.22803
- Reference count: 25
- Key outcome: Pretraining with 100 hours of non-annotated audio-visual recordings reduced SELD error score from 36.4 pts to 34.9 pts on DCASE2022 Task 3

## Executive Summary
This paper addresses the challenge of improving sound event localization and detection (SELD) for spatial audio recordings captured by first-order ambisonics (FOA) microphones. The proposed method involves pretraining the feature extraction part of a deep neural network (DNN) in a self-supervised manner using spatial audio-visual recordings abundantly available as virtual reality contents. The core idea is to jointly train audio and visual encoders with contrastive learning such that the audio and visual embeddings of the same recording and direction of arrival (DOA) are made close. A key feature of the method is that the DOA-wise audio embeddings are jointly extracted from the raw audio data, while the DOA-wise visual embeddings are separately extracted from the local visual crops centered on the corresponding DOA. The experiment using the DCASE2022 Task 3 dataset of 20 hours shows that non-annotated audio-visual recordings of 100 hours reduced the error score of SELD from 36.4 pts to 34.9 pts.

## Method Summary
The method pretrains an audio feature extractor using DOA-aware audio-visual contrastive learning with two variants: DOA-wise and recording-wise. The audio encoder (ResNet-Conformer) processes FOA spectrograms into frame-wise latent features, which are then transformed into DOA-wise embeddings via a projection head. A visual encoder (R(2+1)D) processes DOA-wise visual crops from 360° equirectangular data. The contrastive learning objective aligns embeddings from the same recording and DOA while separating those from different pairs. For recording-wise learning, visual data is randomly rotated to generate negative samples, forcing the audio encoder to learn DOA-specific features. After pretraining, the model is fine-tuned for SELD using an ACCDOA representation on annotated datasets.

## Key Results
- SELD error score improved from 36.4 pts to 34.9 pts on DCASE2022 Task 3 using 100 hours of non-annotated audio-visual pretraining data
- The method effectively learns DOA-specific features through audio-visual contrastive learning, improving both localization and detection accuracy
- Performance gains are particularly notable for datasets with limited annotated data, though effectiveness varies with annotation availability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining the audio feature extractor using DOA-aware audio-visual contrastive learning enables the model to learn features that encode both sound event classes and their spatial locations.
- Mechanism: By jointly extracting DOA-wise audio embeddings from raw FOA data and aligning them with DOA-wise visual embeddings, the audio encoder is forced to represent spatial information alongside semantic content.
- Core assumption: Sound events in spatial audio-visual recordings are spatially aligned with their corresponding visual sources, enabling cross-modal learning of spatial features.
- Evidence anchors:
  - [abstract] "Assuming that sound objects are concurrently observed by the FOA microphones and the omni-directional camera, we jointly train audio and visual encoders with contrastive learning such that the audio and visual embeddings of the same recording and DOA are made close."
  - [section] "This encourages the latent features of the audio encoder to represent both the classes and DOAs of sound events."
  - [corpus] Weak evidence: related papers focus on general audio-visual alignment but lack detailed discussion of DOA-aware pretraining for SELD.
- Break condition: If the spatial alignment assumption fails (e.g., background music without visible source), the learned features may not encode accurate DOA information.

### Mechanism 2
- Claim: Recording-wise contrastive learning improves generalization by capturing global audio-visual similarity across all DOAs, reducing sensitivity to local spatial misalignments.
- Mechanism: By averaging DOA-wise similarities over all directions and maximizing similarity for audio-visual pairs from the same recording, the model learns robust global embeddings that are less affected by individual DOA misalignments.
- Core assumption: The global audio-visual similarity across a recording is more stable than individual DOA alignments, even when some local alignments are incorrect.
- Evidence anchors:
  - [section] "We also propose global contrastive learning based on recording-wise audio-visual similarity obtained by averaging the DOA-wise similarities over all the DOAs."
  - [section] "To encourage the audio encoder to extract DOA information as the latent features, we introduce a data augmentation technique that randomly spatially rotates only the equirectangular visual data to generate negative samples from the same recording."
  - [corpus] No direct evidence: related work does not discuss recording-wise contrastive learning for SELD.
- Break condition: If the audio-visual co-occurrence is weak across the entire recording (e.g., many silent visual regions), global similarity maximization may not provide useful supervisory signal.

### Mechanism 3
- Claim: Data augmentation through random spatial rotation of visual data generates effective negative samples for recording-wise contrastive learning, forcing the audio encoder to learn DOA-specific features.
- Mechanism: By rotating the visual data while keeping audio unchanged, pairs from the same recording but different DOAs become negative samples, encouraging the audio encoder to encode spatial information that distinguishes DOAs.
- Core assumption: Spatial rotation of visual data creates realistic negative samples that differ meaningfully from positive DOA-aligned pairs.
- Evidence anchors:
  - [section] "To encourage the audio encoder to extract DOA information as the latent features, we introduce a data augmentation technique that randomly spatially rotates only the equirectangular visual data to generate negative samples from the same recording."
  - [section] "Specifically, one can synthesize multichannel audio data by convolving source signals of various classes with room impulse responses (RIRs) that simulate various acoustic environments and DOA conditions."
  - [corpus] No direct evidence: related work does not discuss spatial rotation augmentation for contrastive learning in SELD.
- Break condition: If the visual rotation does not create sufficiently distinct DOA alignments (e.g., symmetric scenes), the negative samples may not effectively train DOA-specific features.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: The method relies on making embeddings of aligned audio-visual pairs similar and unaligned pairs dissimilar, which is the core principle of contrastive learning.
  - Quick check question: What is the role of the temperature hyperparameter τ in the InfoNCE loss formulation?

- Concept: First-order ambisonics (FOA)
  - Why needed here: FOA format allows extraction of audio signals from arbitrary directions, which is essential for DOA-wise feature extraction in this method.
  - Quick check question: How many channels does first-order ambisonics use, and what is their spatial arrangement?

- Concept: Directional audio encoding
  - Why needed here: The method requires encoding audio features that capture both sound class and direction, which necessitates understanding directional audio representation techniques.
  - Quick check question: What is the difference between encoding audio features at the frame level versus the DOA level?

## Architecture Onboarding

- Component map:
  - Raw FOA audio → Audio feature extractor → Projection head → DOA-wise embeddings → SELD head → Output
  - 360° visual data → Visual encoder → DOA-wise visual crops → Visual embeddings → Contrastive loss

- Critical path: Raw FOA audio → Audio feature extractor → Projection head → DOA-wise embeddings → SELD head → Output

- Design tradeoffs:
  - DOA resolution vs. computational cost: Higher K (number of DOAs) improves spatial resolution but increases computation
  - Contrastive learning variant: DOA-wise provides fine-grained spatial learning but may be sensitive to misalignments; recording-wise provides robustness but may lose spatial precision
  - Pretraining dataset domain: Broader domain coverage improves generalization but may introduce domain mismatch

- Failure signatures:
  - Poor localization performance despite good classification suggests the audio encoder failed to learn DOA features
  - Degraded performance on classes absent from pretraining dataset indicates domain mismatch
  - Performance degradation when using limited labeled data suggests overfitting to pretraining domain

- First 3 experiments:
  1. Compare SELD performance with and without DOA-aware pretraining using the same downstream architecture
  2. Ablation study: Evaluate DOA-wise vs. recording-wise contrastive learning variants
  3. Test robustness by evaluating on a dataset with different domain characteristics than the pretraining data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of DOA-aware contrastive learning vary with the amount and quality of annotated data available for fine-tuning?
- Basis in paper: [explicit] The paper shows that pretraining with non-annotated audio-visual recordings of 100 hours reduced the error score of SELD from 36.4 pts to 34.9 pts using the DCASE2022 Task 3 dataset of 20 hours. However, the paper notes that the proposed A V-SSLs degraded the SELD scores more than A VC when only a few hours of imbalanced data were used for training (STARSS22).
- Why unresolved: The paper only evaluates the impact of pretraining on SELD performance using two specific datasets (STARSS22+Synth1 and STARSS22). It does not explore how the effectiveness of DOA-aware contrastive learning varies with different amounts and qualities of annotated data.
- What evidence would resolve it: Conducting experiments with various amounts and qualities of annotated data to evaluate the impact on SELD performance after pretraining with DOA-aware contrastive learning.

### Open Question 2
- Question: How does the proposed method perform on datasets with different acoustic environments and sound event classes compared to the current evaluation datasets?
- Basis in paper: [inferred] The paper evaluates the proposed method on the STARSS22 and Synth1 datasets, which include 13 sound event classes. However, it does not explore the performance on datasets with different acoustic environments and sound event classes.
- Why unresolved: The paper only evaluates the proposed method on two specific datasets, limiting the generalizability of the results to other acoustic environments and sound event classes.
- What evidence would resolve it: Conducting experiments on diverse datasets with various acoustic environments and sound event classes to evaluate the performance of the proposed method.

### Open Question 3
- Question: How does the proposed method handle sound events that are not spatially aligned with their corresponding visual data, such as background music?
- Basis in paper: [explicit] The paper notes that background music was considered to have a negative impact on the localization of sound events related to music because it did not spatially correspond to the paired visual data.
- Why unresolved: The paper only mentions the negative impact of background music on the localization of sound events related to music but does not explore how the proposed method handles other sound events that are not spatially aligned with their corresponding visual data.
- What evidence would resolve it: Conducting experiments with various sound events that are not spatially aligned with their corresponding visual data to evaluate the performance of the proposed method.

## Limitations

- The assumption of spatial alignment between audio and visual sources may not hold for all sound events, particularly background music or sounds without visible sources
- The effectiveness of the method on datasets with different acoustic environments and sound event classes remains unexplored
- The impact of limited annotated data on pretraining effectiveness varies significantly, with potential degradation when fine-tuning data is scarce

## Confidence

- **High**: The general framework of using audio-visual contrastive learning for pretraining SELD models is sound and aligns with established self-supervised learning principles
- **Medium**: The specific implementation details (DOA resolution, augmentation strategy, contrastive learning variants) are well-specified and likely to be effective
- **Low**: The claim that this approach will generalize across diverse acoustic environments and visual domains is not yet substantiated by broad experimental validation

## Next Checks

1. **Ablation study on contrastive learning variants**: Compare DOA-wise and recording-wise contrastive learning variants on multiple datasets to determine which provides better localization accuracy versus detection robustness.

2. **Domain generalization test**: Evaluate the pretrained model on datasets with different acoustic characteristics (e.g., indoor vs. outdoor, anechoic vs. reverberant) to assess cross-domain performance and identify domain-specific failure modes.

3. **Limited annotation experiment**: Measure SELD performance as a function of available annotated training data to quantify the practical benefit of pretraining when fine-tuning data is scarce.