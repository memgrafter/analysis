---
ver: rpa2
title: LLM Agent for Fire Dynamics Simulations
arxiv_id: '2412.17146'
source_url: https://arxiv.org/abs/2412.17146
tags:
- case
- code
- agent
- simulation
- firefoam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'An LLM agent called FoamPilot was developed to support fire dynamics
  simulations using FireFOAM. The agent provides three core functionalities: code
  insight for navigating source code, case configuration for modifying simulation
  setups, and job execution for managing simulations on HPC systems.'
---

# LLM Agent for Fire Dynamics Simulations

## Quick Facts
- arXiv ID: 2412.17146
- Source URL: https://arxiv.org/abs/2412.17146
- Authors: Leidong Xu; Danyal Mohaddes; Yi Wang
- Reference count: 5
- FoamPilot achieved 4/5 success rate for serial job execution and 5/5 for code insight and case configuration tasks

## Executive Summary
This paper presents FoamPilot, an LLM agent designed to support fire dynamics simulations using FireFOAM. The agent provides three core functionalities: code insight for navigating source code, case configuration for modifying simulation setups, and job execution for managing simulations on HPC systems. Tested on a combination of simple and complex tasks, the agent showed promising results for basic operations but struggled with more sophisticated multi-step workflows. The work highlights both the potential and current limitations of using LLM agents in scientific computing workflows.

## Method Summary
The FoamPilot agent was implemented using the LangChain/LangGraph framework with GPT-4o as the underlying LLM. It integrates three tools: Shell Command Tool for system operations, Python Interpreter Tool for case configuration, and RAG Tool for code navigation. The agent uses a graph structure with nodes for user, LLM, and tools, allowing dynamic tool selection based on task requirements. RAG-based code insight combines header and source files for comprehensive context, while detailed prompt engineering guides job execution. Experiments were conducted on AWS infrastructure with FireFOAM installed, testing five repetitions for each task type.

## Key Results
- Simple tasks showed high success rates: 5/5 for code insight, 5/5 for case configuration, and 4/5 for serial job execution
- Complex multi-functionality tasks performed poorly: 2/5 for combined code insight and case configuration, 1/5 for HPC job submission
- Performance limitations attributed to general-purpose LLM nature and FireFOAM codebase complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RAG-based code insight allows efficient navigation of FireFOAM's complex C++ codebase.
- Mechanism: Combining header and source files before embedding ensures that both declarations and implementations are retrieved together, providing comprehensive context to the LLM for code understanding.
- Core assumption: The embedding model can effectively capture semantic relationships between combined code files, and the retrieved context fits within the LLM's context window.
- Evidence anchors:
  - [abstract]: "Code insight is an alternative to traditional keyword searching leveraging retrieval-augmented generation (RAG) and aims to enable efficient navigation and summarization of the FireFOAM source code"
  - [section]: "To improve the usefulness of the retrieved content, we combine the header and source files into a single document before passing them to an embedding model"
  - [corpus]: Weak evidence - the corpus neighbors discuss general agent frameworks but don't specifically address RAG implementation for scientific codebases
- Break condition: If the combined code files exceed the embedding model's input size limit, or if the semantic relationships are too complex for the embedding model to capture effectively

### Mechanism 2
- Claim: The agent's iterative tool usage pattern allows it to handle multi-step tasks by chaining different functionalities.
- Mechanism: The agent uses a graph structure with nodes for user, LLM, and tools, where the LLM dynamically decides which tools to invoke based on the task requirements, allowing it to combine code insight, case configuration, and job execution as needed.
- Core assumption: The LLM can accurately determine which tools to use at each step and maintain sufficient context across multiple tool calls within its context window.
- Evidence anchors:
  - [abstract]: "The integration of these functionalities into a single LLM agent is a step aimed at accelerating the simulation workflow"
  - [section]: "The agent structure follows a graph consisting of three nodes, as shown in Fig. 2: user, LLM, and tools, with edges connecting them to facilitate message transfer"
  - [corpus]: Moderate evidence - the corpus neighbors show similar multi-agent frameworks for different domains, suggesting the general approach is viable
- Break condition: If the task complexity exceeds the LLM's ability to maintain context across multiple tool calls, or if the tool selection logic fails to identify the correct sequence of operations

### Mechanism 3
- Claim: Specialized prompt engineering with detailed instructions enables the agent to execute complex simulation tasks.
- Mechanism: By providing detailed, step-by-step instructions in the prompts for both serial and HPC job execution, the agent can correctly invoke the necessary shell commands and python interpreter tools to run simulations and analyze results.
- Core assumption: The LLM can correctly parse and follow detailed prompt instructions to execute the correct sequence of commands without hallucination or deviation.
- Evidence anchors:
  - [section]: "The Job Execution functionality was achieved by providing detailed instructions through prompting, which cause the agent to use the Shell Tool to fulfill the request"
  - [section]: "When running a FireFOAM simulation locally or on the head node, the agent must prepare the mesh and execute the simulation directly on the command line"
  - [corpus]: Weak evidence - the corpus neighbors discuss general agent capabilities but don't specifically address prompt engineering for scientific simulation execution
- Break condition: If the LLM fails to correctly parse the detailed instructions, or if it hallucinates file paths or commands not present in the actual system

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) and vector embeddings
  - Why needed here: RAG allows the agent to search through the FireFOAM codebase semantically rather than just by keywords, which is crucial for finding relevant code sections in a large, complex codebase
  - Quick check question: How does RAG differ from traditional keyword search, and why is this difference particularly important for navigating scientific codebases?

- Concept: Large Language Model (LLM) context windows and token management
  - Why needed here: The agent needs to fit both the user query, tool outputs, and relevant code snippets within the LLM's context window while maintaining coherence across multiple tool calls
  - Quick check question: What strategies does the FoamPilot agent use to manage token usage while ensuring sufficient context for complex tasks?

- Concept: Agent-based frameworks and tool orchestration
  - Why needed here: The agent needs to dynamically choose and sequence different tools (Shell, Python, RAG) based on the task requirements, which requires understanding how agent frameworks manage tool calls and context
  - Quick check question: How does the LangChain/LangGraph framework enable the FoamPilot agent to make dynamic tool choices while maintaining task state?

## Architecture Onboarding

- Component map: User Interface -> LLM Core (GPT-4o) -> Tool Node (Shell, Python, RAG) -> Vector Store (FAISS) -> Graph Structure (LangChain/LangGraph)
- Critical path: 1. User query processed by LLM with tool information 2. LLM decides tool and generates structured output 3. Tool executes and returns result 4. LLM processes result and determines next action 5. Process repeats until completion or failure
- Design tradeoffs: Using cloud-hosted LLM vs. local model (better performance but external dependency), combining header and source files for RAG (better context but increased embedding size), static conditional edges vs. dynamic graph traversal (simpler implementation but less flexible)
- Failure signatures: Incorrect tool selection, context window overflow during multi-step tasks, hallucination of non-existent files or commands, inability to recover from incorrect RAG retrievals
- First 3 experiments: 1. Simple code insight task: Find and summarize energy equation from solid phase source code 2. Basic case configuration: Modify burner size in poolFireMcCaffrey tutorial case 3. Serial job execution: Run modified poolFireMcCaffrey case and verify output files

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal embedding model and chunking strategy for efficiently retrieving relevant FireFOAM source code while minimizing token usage?
- Basis in paper: [explicit] The paper discusses testing multiple embedding models and chunking strategies, noting that some models had poor performance due to limited input sizes, while others worked well with the 8192-token limit.
- Why unresolved: The paper mentions testing several models but doesn't specify which performed best or provide detailed comparison metrics between different chunking approaches.
- What evidence would resolve it: Systematic comparison of retrieval accuracy, token usage, and processing time across different embedding models and chunking strategies on a standardized set of code queries.

### Open Question 2
- Question: How does FoamPilot's performance scale with increasing task complexity when combining multiple functionalities (Code Insight, Case Configuration, and Job Execution)?
- Basis in paper: [explicit] The paper shows success rates drop from 4/5 or 5/5 for simple tasks to 2/5 for combined code insight and case configuration tasks, but doesn't explore intermediate levels of complexity.
- Why unresolved: The paper only presents results for simple individual tasks and one complex multi-functionality task, without examining the performance gradient between these extremes.
- What evidence would resolve it: Systematic testing of tasks requiring 2, 3, 4, and 5 tool interactions, measuring success rates and failure modes at each complexity level.

### Open Question 3
- Question: Would specialized LLMs trained on FireFOAM/OpenFOAM code outperform general-purpose models like GPT-4o on complex multi-step tasks?
- Basis in paper: [inferred] The paper suggests that "limited domain-specific knowledge of general-purpose LLMs reduces FoamPilot's ability to handle complex tasks" and mentions that continued pre-training on the codebase "may potentially allow accurate zero-shot prompting."
- Why unresolved: The paper only tests GPT-4o and doesn't compare against any specialized or fine-tuned models on the same tasks.
- What evidence would resolve it: Head-to-head comparison of FoamPilot using GPT-4o versus the same agent architecture using a model fine-tuned on FireFOAM/OpenFOAM code, tested on identical complex multi-functionality tasks.

## Limitations
- Performance degrades significantly for complex multi-step tasks (2/5 for combined tasks, 1/5 for HPC jobs)
- Heavy dependency on GPT-4o introduces cost and availability constraints
- Lack of specialized training on FireFOAM code limits effectiveness for complex operations

## Confidence
- **High confidence**: Simple serial job execution and basic code insight tasks (5/5 success rates)
- **Medium confidence**: Combined code insight and case configuration tasks (2/5 success rate)
- **Low confidence**: HPC job submission (1/5 success rate)

## Next Checks
1. **Context window stress test**: Systematically measure how the agent's performance degrades as task complexity increases and context window usage approaches maximum capacity, using tasks of increasing complexity that require maintaining state across multiple tool calls.

2. **RAG retrieval quality analysis**: Evaluate the precision and recall of the RAG system by comparing retrieved code snippets against ground truth relevant sections for a set of benchmark queries, identifying patterns in retrieval failures.

3. **Specialized model comparison**: Implement a version of the agent using a smaller, FireFOAM-specific model fine-tuned on the codebase, and compare its performance on the same task suite to quantify the benefits of domain adaptation versus general-purpose LLMs.