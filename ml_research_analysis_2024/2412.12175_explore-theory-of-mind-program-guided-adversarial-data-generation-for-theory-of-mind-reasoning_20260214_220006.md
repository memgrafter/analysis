---
ver: rpa2
title: 'Explore Theory of Mind: Program-guided adversarial data generation for theory
  of mind reasoning'
arxiv_id: '2412.12175'
source_url: https://arxiv.org/abs/2412.12175
tags:
- story
- theory
- mind
- data
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ExploreToM is a framework for generating challenging, diverse theory
  of mind datasets to evaluate and train large language models. It uses an A search
  over a domain-specific language to create complex story structures with varying
  character beliefs and mental states, then optionally infills these with natural-sounding
  text.
---

# Explore Theory of Mind: Program-guided adversarial data generation for theory of mind reasoning

## Quick Facts
- arXiv ID: 2412.12175
- Source URL: https://arxiv.org/abs/2412.12175
- Reference count: 40
- Key outcome: LLM accuracy drops to 0-9% on adversarial ToM data; fine-tuning yields 27-point improvement

## Executive Summary
ExploreToM introduces a program-guided framework for generating challenging theory of mind datasets to evaluate and train large language models. The approach uses an A* search over a domain-specific language to create complex story structures with varying character beliefs and mental states, then optionally infills these with natural-sounding text. The framework addresses limitations of existing benchmarks by generating more diverse scenarios and stress-testing models adversarially. Evaluations show that state-of-the-art models like GPT-4o and Llama-3.1-70B achieve very low accuracy on ExploreToM-generated data, highlighting the need for more robust theory of mind evaluation.

## Method Summary
ExploreToM generates theory of mind datasets through program-guided adversarial data generation. The framework employs an A* search algorithm over a domain-specific language to construct story structures with complex character beliefs and mental states. These programmatic structures can then be optionally infilled with natural language text. The approach creates more diverse and challenging scenarios than existing benchmarks, allowing systematic stress-testing of language models' theory of mind capabilities. The generated data is used both for evaluation and fine-tuning, with the latter showing significant performance improvements on established ToM benchmarks.

## Key Results
- State-of-the-art models (GPT-4o, Llama-3.1-70B) achieve 0-9% accuracy on ExploreToM-generated data
- Fine-tuning Llama-3.1-8B on ExploreToM data yields 27-point accuracy improvement on ToMi benchmark
- Models maintain general reasoning capabilities while improving on theory of mind tasks

## Why This Works (Mechanism)
The program-guided approach systematically generates complex belief structures that challenge language models' ability to track multiple mental states across characters and time. By using A* search over a domain-specific language, the framework can create scenarios that are both diverse and deliberately adversarial, exposing weaknesses in current models' theory of mind reasoning that simpler datasets might miss.

## Foundational Learning
- Theory of Mind: The ability to attribute mental states to oneself and others - needed for understanding characters' beliefs and intentions in stories
- Adversarial Data Generation: Creating challenging examples to expose model weaknesses - quick check: generate examples that fool current models
- A* Search Algorithm: Heuristic search for optimal solutions - quick check: verify search finds increasingly complex belief structures
- Domain-Specific Languages: Specialized languages for expressing domain concepts - quick check: ensure DSL primitives cover essential ToM scenarios

## Architecture Onboarding
**Component Map**: DSL Definition -> A* Search -> Program Generation -> Text Infilling -> Dataset Output
**Critical Path**: The A* search over DSL programs is the core mechanism that determines the difficulty and diversity of generated examples
**Design Tradeoffs**: Programmatic generation vs. natural language corpus - programmatic offers control and diversity but may miss naturalistic patterns
**Failure Signatures**: Low accuracy on generated data suggests models struggle with complex belief tracking; failure to maintain general reasoning after fine-tuning indicates overfitting
**First Experiments**: (1) Run A* search with varying depth limits to assess complexity scaling, (2) Compare human vs. model performance on generated examples, (3) Test ablation of text infilling to measure contribution to model performance

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- The A* search domain language may not capture all real-world ToM scenarios
- The relationship between program complexity and human-interpretable difficulty is not empirically validated
- Fine-tuning benefits are only demonstrated on the ToMi benchmark, with limited validation of maintained general reasoning capabilities

## Confidence
- Core methodology: Medium-High - program-guided approach is well-defined and transparent
- Performance gap claims: Medium - dependent on specific generated examples and potential overfitting
- Fine-tuning benefits: Medium - promising results but limited benchmark diversity and lack of systematic ablation studies

## Next Checks
1. Evaluate ExploreToM-generated examples with human annotators to confirm they represent genuinely challenging ToM scenarios beyond program complexity
2. Test whether models trained on ExploreToM data show improved performance on independently developed ToM benchmarks, not just ToMi
3. Perform ablation studies varying the domain language primitives to assess sensitivity of generated difficulty to language design choices