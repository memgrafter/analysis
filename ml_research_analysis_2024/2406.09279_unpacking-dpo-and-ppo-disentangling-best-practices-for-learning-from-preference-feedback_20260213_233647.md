---
ver: rpa2
title: 'Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference
  Feedback'
arxiv_id: '2406.09279'
source_url: https://arxiv.org/abs/2406.09279
tags:
- training
- reward
- performance
- prompts
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically studies the key components of learning
  from preference feedback for language models. The authors examine the impact of
  preference data, learning algorithm (PPO vs DPO), reward model, and policy training
  prompts on model performance across 11 diverse benchmarks.
---

# Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback

## Quick Facts
- arXiv ID: 2406.09279
- Source URL: https://arxiv.org/abs/2406.09279
- Authors: Hamish Ivison; Yizhong Wang; Jiacheng Liu; Zeqiu Wu; Valentina Pyatkin; Nathan Lambert; Noah A. Smith; Yejin Choi; Hannaneh Hajishirzi
- Reference count: 40
- One-line primary result: PPO outperforms DPO by up to 2.5% in math and 1.2% in general domains when using synthetic, diverse data with per-aspect annotations

## Executive Summary
This paper systematically examines the key components of learning from preference feedback for language models, focusing on preference data, learning algorithms (PPO vs DPO), reward models, and policy training prompts. Through extensive experiments across 11 diverse benchmarks, the authors identify optimal configurations for each component and their interactions. The findings provide concrete recommendations for practitioners building aligned language models, showing that PPO generally outperforms DPO, synthetic diverse data with per-aspect annotations works best, and larger reward models significantly improve math performance while having marginal effects on other capabilities.

## Method Summary
The authors conduct a comprehensive ablation study comparing PPO and DPO algorithms using the TüLU 2 13B model as a base. They test various combinations of preference datasets (including synthetic data with per-aspect annotations), reward model sizes (13B vs 70B), and policy training prompts. The study evaluates performance across 11 benchmarks covering math (GSM8k), general capabilities (MMLU, Big Bench Hard), instruction following (AlpacaEval 1 & 2, IFEval), coding (HumanEval+, MBPP+), safety (ToxiGen), and reasoning (XSTest). Both algorithms use the same base model and data, with KL penalty coefficient tuning to optimize performance.

## Key Results
- PPO outperforms DPO by up to 2.5% in math tasks and 1.2% in general domains
- Synthetic, diverse data with per-aspect annotations provides the best preference learning signals
- Larger reward models significantly improve math performance but have marginal effects on other capabilities
- Targeted prompts help when focusing on specific domains but don't improve overall performance
- KL penalty coefficient tuning is crucial, with optimal values varying by reward model and task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PPO outperforms DPO because it uses online sampling that better explores the policy space during training.
- Mechanism: PPO samples responses from the current policy during training, allowing the reward model to adapt to the evolving distribution of model outputs. This dynamic adaptation helps the policy escape local optima and better match the target distribution.
- Core assumption: The reward model can effectively evaluate online-generated responses and provide useful gradients for policy improvement.
- Evidence anchors:
  - [abstract] "PPO outperforms DPO by up to 2.5% in math and 1.2% in general domains"
  - [section] "PPO trains on online data generated by the current policy, while DPO trains on static, pre-generated offline data. This may limit exploration in DPO and thus hurt the training quality."
  - [corpus] Weak - related work focuses on theoretical comparisons but lacks empirical evidence about exploration benefits
- Break condition: If the reward model becomes unreliable or overfits to the current policy distribution, online sampling could lead to training collapse or reward hacking.

### Mechanism 2
- Claim: Synthetic preference data with per-aspect annotations provides better alignment signals than human-annotated data.
- Mechanism: Per-aspect annotations capture multiple dimensions of quality (helpfulness, harmlessness, etc.) that can be combined into a more nuanced preference signal. Synthetic data allows for controlled generation of diverse examples that cover edge cases.
- Core assumption: GPT-4 or similar models can reliably annotate preferences across multiple aspects and that these annotations transfer to human preferences.
- Evidence anchors:
  - [abstract] "synthetic, diverse data annotated with per-aspect preferences works best for learning from preferences"
  - [section] "using datasets collected by first getting per-aspect annotations...tend to outperform datasets that rely only on overall judgements"
  - [corpus] Weak - related work discusses data quality but doesn't specifically address per-aspect annotation benefits
- Break condition: If the synthetic data distribution becomes too different from real user interactions, the model may learn to optimize for synthetic preferences that don't generalize.

### Mechanism 3
- Claim: Larger reward models provide more accurate reward signals that improve policy performance, particularly for math tasks.
- Mechanism: Larger models can capture more nuanced preferences and provide more reliable scores, especially for complex reasoning tasks. The improved reward signal helps PPO make better policy updates.
- Core assumption: The computational cost and potential overfitting risks of larger reward models are outweighed by their improved accuracy.
- Evidence anchors:
  - [abstract] "High-quality preference data leads to improvements of up to 8% in instruction following and truthfulness"
  - [section] "Increasing reward model size or dataset size used to train the reward model results in improved reward model performance on benchmarks directly testing reward model performance"
  - [corpus] Moderate - related work shows mixed results on reward model scaling benefits
- Break condition: If the policy model becomes over-optimized to the reward model's specific scoring patterns, it may degrade on real-world tasks that don't match the reward model's training distribution.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding the RLHF pipeline is essential for grasping how preference data flows through reward model training to policy optimization
  - Quick check question: What are the three main stages of the RLHF pipeline and how do they connect?

- Concept: Preference optimization algorithms (PPO vs DPO)
  - Why needed here: The paper directly compares these two approaches, so understanding their core differences is crucial
  - Quick check question: What is the key architectural difference between how PPO and DPO use preference data?

- Concept: Reward model training and evaluation
  - Why needed here: The paper extensively studies how reward model quality affects downstream performance
  - Quick check question: How do you evaluate a reward model's performance independently of the policy it's used to train?

## Architecture Onboarding

- Component map: Data pipeline: preference datasets → reward model training → policy training prompts; Model pipeline: base SFT model → reward model → policy model (PPO) or directly trained model (DPO); Evaluation pipeline: trained models → diverse benchmark suite

- Critical path: The most important sequence is data quality → reward model → policy training. Each stage depends critically on the previous one.

- Design tradeoffs:
  - PPO vs DPO: PPO offers better exploration but requires more compute; DPO is simpler but may get stuck
  - Reward model size: Larger models provide better signals but increase computational cost and risk overfitting
  - Data type: Synthetic data offers diversity and control but may not match real user preferences

- Failure signatures:
  - Poor reward model performance → policy optimization fails regardless of algorithm choice
  - Overfitting to synthetic preferences → degraded real-world performance
  - Insufficient exploration → policy gets stuck in local optima

- First 3 experiments:
  1. Run DPO vs PPO comparison on a small subset of UltraFeedback to verify the core algorithmic difference
  2. Train reward models of different sizes on the same data to establish the scaling relationship
  3. Test policy performance with different prompt distributions to understand the importance of prompt matching

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the findings from this paper about DPO vs PPO performance transfer to multilingual settings or greatly differing base models?
- Basis in paper: [inferred] The paper notes that results are largely based on an in-depth examination over a single model suite (TÜLU 2) using two base models (Llama 2 and 3). It states that examining how results carry to multilingual settings or greatly differing base models is interesting future work.
- Why unresolved: The study was computationally limited and focused on a specific model suite and base models. Multilingual performance was not tested.
- What evidence would resolve it: Systematic experiments comparing DPO vs PPO performance across a diverse set of multilingual datasets and varying base model architectures (e.g. different model families, scales, training approaches).

### Open Question 2
- Question: What is the optimal balance between reward model size and reward model dataset size for downstream policy model performance?
- Basis in paper: [explicit] The paper found that increasing reward model size or dataset size improves direct reward model performance, but these improvements have marginal effects on downstream policy model performance. It notes that improving reward model evaluations may not necessarily translate to improved downstream performance.
- Why unresolved: The study only tested a limited set of reward model sizes (13B vs 70B) and dataset mixtures. The interaction between these factors was not fully explored.
- What evidence would resolve it: Comprehensive ablation studies varying both reward model size and dataset size independently, measuring both direct reward model performance and downstream policy model performance across diverse benchmarks.

### Open Question 3
- Question: How does the choice of KL penalty coefficient interact with reward model quality and task-specificity of policy training prompts?
- Basis in paper: [explicit] The paper found that optimal KL penalty coefficient values change with the reward model used, and that AlpacaEval 2 performance grows with reduced KL coefficient values. It notes that AlpacaEval 2 performance (which matches PPO training setup) improves with lower β, but this comes at the cost of other evaluations.
- Why unresolved: The study only tested a limited range of KL penalty coefficient values (0.01 to 0.05) and did not systematically explore interactions with reward model quality or prompt specificity.
- What evidence would resolve it: Systematic experiments varying KL penalty coefficient across different reward model scales and prompt sets (generic vs task-specific), measuring performance on both matching and non-matching evaluation tasks.

## Limitations

- The findings are based on a single model architecture (TüLU 2 13B) and may not generalize to larger or smaller models
- The synthetic data generation process using GPT-4 introduces an additional layer of complexity that may not be replicable with different base models
- The study focuses primarily on math and general capabilities, leaving open questions about performance in specialized domains
- The computational cost of PPO with online sampling may limit practical applicability for some use cases

## Confidence

- **High Confidence**: The core finding that PPO outperforms DPO (2.5% in math, 1.2% in general domains) is well-supported by systematic ablation studies and consistent across multiple benchmarks
- **Medium Confidence**: The impact of reward model size shows clear trends but exhibits some variability across different task types
- **Low Confidence**: The optimal balance between reward model quality and computational cost is not fully explored

## Next Checks

1. **Algorithm Generalization Test**: Replicate the PPO vs DPO comparison using a different base model architecture (e.g., Llama 2 or Mistral) to verify that the algorithmic differences persist across model families

2. **Real-World Preference Validation**: Conduct A/B testing with actual user interactions to measure how well the synthetic preference data correlates with human preferences in production settings

3. **Reward Model Scaling Analysis**: Systematically vary reward model size and training data quantity to identify the point of diminishing returns and optimal cost-benefit tradeoff for different task domains