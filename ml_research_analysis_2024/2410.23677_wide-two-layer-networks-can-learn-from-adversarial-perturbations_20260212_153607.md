---
ver: rpa2
title: Wide Two-Layer Networks can Learn from Adversarial Perturbations
arxiv_id: '2410.23677'
source_url: https://arxiv.org/abs/2410.23677
tags:
- perturbation
- training
- learning
- adversarial
- assumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides theoretical justification for the feature hypothesis
  and perturbation learning in the context of adversarial examples. The authors prove
  that adversarial perturbations contain sufficient class-specific features for two-layer
  neural networks to generalize from them.
---

# Wide Two-Layer Networks can Learn from Adversarial Perturbations

## Quick Facts
- arXiv ID: 2410.23677
- Source URL: https://arxiv.org/abs/2410.23677
- Reference count: 40
- Primary result: Proves that classifiers trained on adversarial perturbations can generalize correctly despite mislabeled training data

## Executive Summary
This paper provides theoretical justification for the feature hypothesis in adversarial examples by showing that two-layer neural networks can learn from adversarial perturbations despite their mislabeled nature. The authors prove that under mild conditions, a classifier trained on adversarial perturbations achieves predictions consistent with a classifier trained on correctly labeled clean samples. The key insight is that adversarial perturbations contain sufficient class-specific features and are parallel to the weighted sum of training samples, enabling generalization even with incorrect labels.

## Method Summary
The paper analyzes wide two-layer neural networks trained via gradient flow in the lazy training regime. Two classifiers are trained: one on clean labeled data and another on adversarial perturbations generated from the first classifier. The theoretical analysis uses the lazy training framework and Neural Tangent Kernel to show that under three mild conditions (functional margin conditions and agreement condition), the predictions of both classifiers coincide. The method involves generating adversarial perturbations using gradient-based methods and training both classifiers with the same differentiable non-decreasing loss function.

## Key Results
- Adversarial perturbations are parallel to the weighted sum of training samples in wide two-layer networks
- Classifiers trained on mislabeled adversarial examples achieve predictions consistent with classifiers trained on correctly labeled clean samples
- Three mild conditions (functional margin conditions and agreement condition) ensure prediction agreement
- Theoretical results hold for any data distribution and are supported by synthetic dataset experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial perturbations are parallel to the weighted sum of training samples
- Mechanism: In wide two-layer networks under lazy training, the gradient of the loss with respect to inputs becomes proportional to the weighted sum of all training samples because hidden layer activations remain nearly constant
- Core assumption: Wide two-layer neural networks with lazy training regime
- Break condition: If the network is not sufficiently wide, lazy training breaks down

### Mechanism 2
- Claim: Class-specific features in perturbations enable generalization despite mislabeled nature
- Mechanism: The adversarial perturbations contain sufficient class-specific features that allow classifiers to extract meaningful representations, enabling correct predictions even with incorrect labels
- Core assumption: The feature hypothesis - adversarial perturbations contain class-specific features
- Break condition: If perturbations lose their class-specific features

### Mechanism 3
- Claim: Three mild conditions ensure prediction agreement between classifiers
- Mechanism: Functional margin condition 1 (clean classifier margin), functional margin condition 2 (perturbation-based classifier margin), and agreement condition (sign agreement) together ensure prediction consistency
- Core assumption: Classifier predictions can be approximated by specific functions capturing decision boundaries
- Break condition: If any of the three conditions fail

## Foundational Learning

- Concept: Lazy training in wide neural networks
  - Why needed here: Simplifies learning dynamics and allows precise mathematical analysis
  - Quick check question: What happens to the hidden layer derivatives during lazy training in wide networks?

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: Provides theoretical framework for understanding how wide networks learn and generalize
  - Quick check question: How does the NTK behave during lazy training compared to feature learning regime?

- Concept: Gradient flow optimization
  - Why needed here: Assumes gradient flow rather than practical gradient descent for theoretical results
  - Quick check question: What is the key difference between gradient flow and practical gradient descent with finite learning rates?

## Architecture Onboarding

- Component map: Input layer (d-dimensional) -> Hidden layer (m neurons with ReLU/Leaky-ReLU) -> Output layer (single neuron)
- Critical path:
  1. Initialize network with Gaussian weights
  2. Train first classifier on clean labeled data using gradient flow
  3. Generate adversarial perturbations using gradient of loss
  4. Train second classifier on mislabeled perturbations using gradient flow
  5. Verify prediction agreement between both classifiers

- Design tradeoffs:
  - Width vs. theoretical guarantees: Wider networks provide better guarantees but are computationally expensive
  - Perturbation size: Larger perturbations may contain more features but risk losing class-specific information
  - Training time: Longer training improves margin conditions but increases computational cost

- Failure signatures:
  - Prediction disagreement between classifiers indicates failure of one or more conditions
  - Large deviation from lazy training regime suggests insufficient network width
  - Vanishing or exploding gradients during training indicate numerical instability

- First 3 experiments:
  1. Synthetic Gaussian dataset with varying dimensions to test theoretical predictions
  2. Comparison of prediction agreement across different perturbation sizes
  3. Ablation study on network width to verify lazy training regime requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do adversarial perturbations behave in deeper neural networks compared to two-layer networks?
- Basis in paper: The authors acknowledge that their analysis is confined to two-layer networks and note that deeper networks typically capture high-level features from images
- Why unresolved: The theoretical framework and results are specifically derived for two-layer networks
- What evidence would resolve it: Experimental and theoretical results demonstrating how adversarial perturbations scale and behave in deeper neural networks

### Open Question 2
- Question: What are the exact training times required for perturbation learning to succeed under various data distributions and loss functions?
- Basis in paper: The authors note that the relationship between network width and training times depends on the training set and loss function
- Why unresolved: The paper provides theoretical bounds but does not specify exact training times needed for perturbation learning
- What evidence would resolve it: Empirical studies measuring the minimum training times required for perturbation learning across various datasets

### Open Question 3
- Question: How does the agreement condition behave under different data distributions and network architectures?
- Basis in paper: The authors acknowledge that the agreement condition is difficult to interpret and provide a sufficient condition
- Why unresolved: The paper provides theoretical insights but does not fully characterize how the agreement condition varies
- What evidence would resolve it: Experimental results showing how the agreement condition varies across different datasets and network architectures

## Limitations

- Analysis limited to two-layer networks and binary classification
- Requires very wide networks and infinitesimal learning rates (lazy training regime)
- Three conditions for prediction agreement may be difficult to verify empirically
- Practical applicability depends on whether theoretical conditions can be satisfied in real-world settings

## Confidence

- Mechanism 1 (Adversarial perturbations parallel to weighted sum): Medium
- Mechanism 2 (Class-specific features in perturbations): Medium
- Mechanism 3 (Three conditions for prediction agreement): High

## Next Checks

1. **Empirical validation of lazy training**: Verify that the network width requirements are met in practice and that hidden layer derivatives remain approximately constant during training.

2. **Perturbation size sensitivity analysis**: Systematically vary the perturbation size around the theoretical scaling to determine practical bounds where prediction agreement holds.

3. **Finite-width regime behavior**: Test the prediction agreement in networks with realistic widths to understand how quickly theoretical guarantees degrade as width decreases.