---
ver: rpa2
title: The Role of Higher-Order Cognitive Models in Active Learning
arxiv_id: '2401.04397'
source_url: https://arxiv.org/abs/2401.04397
tags:
- learning
- human
- level
- active
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for active learning with human
  feedback that incorporates higher-order cognitive models to improve human-AI collaboration.
  The authors propose a taxonomy of agent levels (1-5) based on increasing sophistication
  of modeling human teachers' beliefs and intentions.
---

# The Role of Higher-Order Cognitive Models in Active Learning

## Quick Facts
- arXiv ID: 2401.04397
- Source URL: https://arxiv.org/abs/2401.04397
- Reference count: 9
- Key outcome: Framework for active learning with human feedback incorporating higher-order cognitive models (agent levels 1-5) to improve human-AI collaboration through belief attribution and strategic teaching

## Executive Summary
This paper presents a theoretical framework for active learning that treats human teachers as agents with agency rather than passive feedback providers. The authors propose a taxonomy of agent levels (1-5) based on increasing sophistication of modeling human beliefs and intentions. A level-3 agent can infer what a level-2 active learning system believes about the human, enabling strategic teaching behaviors. The framework leverages Theory of Mind (ToM) reasoning to attribute beliefs to AI agents based on their queries, potentially improving sample efficiency and communication quality in human-AI interactions.

## Method Summary
The method introduces a taxonomy of agent levels from 1 to 5, where higher levels demonstrate increasingly sophisticated cognitive modeling of human teachers. Level 2 agents maximize information gain through mutual information optimization when generating preference queries. Level 3 humans perform ToM inference to attribute beliefs to the agent based on observed queries, detecting false beliefs through strategic teaching. The framework uses Bayesian inference for belief updating and information theory for query optimization. A computational case study demonstrates how a level-3 agent can infer the belief of a level-2 active learning system from preference queries and use this inference for strategic teaching.

## Key Results
- A level-3 agent can detect both unimodal and bimodal beliefs in a level-2 active learning system through preference queries
- Belief attribution through ToM enables strategic teaching where humans can correct false beliefs in the learning system
- Pragmatic questioning emerges from level-4 agents, creating bidirectional information flow that improves communication efficiency
- Increasing levels of agency result in qualitatively different forms of rational communication between active learning systems and teachers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Higher-order cognitive models improve sample efficiency by allowing the AI agent to infer the human's beliefs about the agent itself.
- Mechanism: When the agent models the human as a level 3 agent, it can infer the human's understanding of its own uncertainty and knowledge state. This enables generation of questions that either extract more information (level 2 behavior) or convey information strategically (level 4 behavior).
- Core assumption: Humans naturally model AI agents recursively and can be modeled as such computationally.
- Evidence anchors: [abstract] empirical evidence for higher-order cognition in human behavior; [section] increasing level of agency results in qualitatively different forms of rational communication
- Break condition: If humans do not actually model AI agents recursively, or if computational complexity outweighs sample efficiency gains.

### Mechanism 2
- Claim: Belief attribution through Theory of Mind enables strategic teaching where humans can correct false beliefs in the learning system.
- Mechanism: Level 3 humans observing agent queries can infer what the agent believes about human preferences. If these beliefs are incorrect, humans strategically select examples that demonstrate true preferences while highlighting the agent's misconceptions.
- Core assumption: Humans can recognize when an AI system has false beliefs and can strategically correct them through responses.
- Evidence anchors: [abstract] level 3 agent can infer belief of level-2 system from preference queries; [section] model of level 3 human can detect false belief through preference queries
- Break condition: If humans cannot detect false beliefs in AI systems, or if strategic teaching is not more effective than other feedback forms.

### Mechanism 3
- Claim: Pragmatic questioning emerges from level 4 agents, creating bidirectional information flow that improves communication efficiency.
- Mechanism: Level 4 agents generate questions serving dual purposes - extracting information about human preferences while conveying information about the agent's own state or highlighting relevant features.
- Core assumption: Questions can be interpreted both literally (information extraction) and rhetorically (information conveyance), and humans can perform pragmatic inference to distinguish between these intentions.
- Evidence anchors: [abstract] increasing level of agency results in qualitatively different forms of rational communication; [section] weighted combination of level 2 and 4 utilities results in objective optimizing bidirectional information flow
- Break condition: If humans cannot perform pragmatic inference on AI-generated questions, or if bidirectional communication does not yield meaningful efficiency gains.

## Foundational Learning

- Concept: Computational rationality and recursive modeling
  - Why needed here: The framework is built on computational rationality theory, which models behavior as optimal given preferences and cognitive constraints. Understanding recursive modeling (level-k models) is essential to grasp how different levels of agency work.
  - Quick check question: Can you explain the difference between a level 2 agent and a level 4 agent in terms of their modeling assumptions about the human teacher?

- Concept: Bayesian inference and belief updating
  - Why needed here: The framework relies heavily on Bayesian updating of beliefs about human preferences and the human's beliefs about the agent. This is fundamental to how both the agent and human reason about each other.
  - Quick check question: How does the agent update its belief about human preferences after receiving an answer to a preference query?

- Concept: Information theory and entropy
  - Why needed here: The framework uses information gain (mutual information) as a utility function for active learning. Understanding entropy and information gain is crucial for grasping how the agent selects queries and how the human infers the agent's beliefs.
  - Quick check question: What is the relationship between entropy and information gain in the context of active learning from preference queries?

## Architecture Onboarding

- Component map:
  Human model component (encodes levels 1-5) -> Agent belief component -> Query generation component -> Inference component -> Strategic teaching component -> Utility function component

- Critical path:
  1. Agent generates query based on current belief and utility function
  2. Human observes query and performs ToM inference to attribute belief to agent
  3. Human responds strategically based on inferred agent belief
  4. Agent updates its belief about human preferences using the response
  5. Repeat until convergence or sufficient sample efficiency is achieved

- Design tradeoffs:
  - Computational complexity vs. modeling accuracy: Higher-order models (level 3+) are more accurate but computationally expensive
  - Transparency vs. efficiency: More sophisticated communication (level 4+) may be more efficient but less interpretable
  - Model assumptions vs. generality: The framework assumes specific cognitive models that may not generalize to all human-AI interactions

- Failure signatures:
  - Computational intractability: Higher-order models may become too complex to compute in real-time
  - Misattribution of beliefs: The agent may incorrectly infer human beliefs about its state
  - Strategic misalignment: The human's strategic teaching may not effectively correct the agent's false beliefs
  - Communication breakdown: Pragmatic questioning may fail if humans cannot perform the required inference

- First 3 experiments:
  1. Implement a level 2 agent with standard information gain maximization and measure sample efficiency on a simple preference learning task
  2. Implement a level 3 human model that performs ToM inference and test its ability to detect the agent's belief from queries in both unimodal and bimodal settings
  3. Implement a level 4 agent with pragmatic questioning and evaluate whether it achieves better sample efficiency than level 2 through bidirectional communication

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the identifiability of the active learning system's belief change when queries are generated with different level 2 objectives beyond mutual information maximization?
- Basis in paper: [explicit] The authors note this as a potential direction for characterizing the effect of modeling assumptions, stating "how does the identifiablity of the active learning system's belief change when queries are generated with different level 2 objectives?"
- Why unresolved: The case study only explores one specific utility function (information gain) for generating queries, leaving open how different objectives might affect a level 3 human's ability to infer the system's beliefs.
- What evidence would resolve it: Computational studies comparing belief attribution accuracy across multiple query generation objectives under varying levels of belief complexity.

### Open Question 2
- Question: What types of approximations maintain the qualitative behaviors predicted by computationally rational higher-order models when implementing level 4 and level 5 agents?
- Basis in paper: [explicit] The authors state "at some stage, it is anticipated that approximations to optimal communication will become necessary" and suggest this should be a focus of future research.
- Why unresolved: The paper presents theoretical frameworks for higher-order cognition but doesn't address practical computational limitations or how to approximate optimal behavior while preserving key characteristics.
- What evidence would resolve it: Empirical validation showing which approximation strategies preserve the bidirectional information flow and pragmatic communication behaviors while remaining computationally tractable.

### Open Question 3
- Question: How does strategic teaching behavior vary when humans attribute different false beliefs to the active learning system?
- Basis in paper: [inferred] The case study demonstrates that belief attribution affects strategic teaching behavior, showing one example where the human corrects a false belief, but doesn't explore the full range of possible false beliefs.
- Why unresolved: The paper provides only one example of strategic teaching behavior in response to a specific false belief scenario, leaving open how humans might respond to different types or magnitudes of false beliefs.
- What evidence would resolve it: User studies with varying false belief scenarios that measure the diversity and effectiveness of strategic teaching responses, comparing them to predictions from computational models.

## Limitations
- Empirical validation remains incomplete with no direct experimental evidence demonstrating improved sample efficiency or communication quality in human-AI interactions
- Computational study described appears theoretical rather than empirical, lacking user studies with actual human participants
- Scalability to real-world applications with complex state spaces and multiple humans remains unclear
- Assumption that humans naturally engage in recursive modeling of AI agents may not hold across different user populations or task domains

## Confidence
- **Medium** for the theoretical framework and taxonomy of agent levels - conceptual structure is well-defined and grounded in established cognitive science literature, but practical implementation details are not fully specified
- **Low** for empirical claims about improved sample efficiency and communication - paper lacks direct experimental validation with human participants demonstrating measurable improvements
- **Medium** for the computational study showing belief detection capabilities - methodology appears sound but absence of real human data limits confidence in generalizability

## Next Checks
1. **Human Participant Study**: Conduct a controlled experiment where human participants interact with level 2, 3, and 4 agents on a preference learning task. Measure sample efficiency (number of queries to reach target accuracy) and participant satisfaction/understanding of the interaction. Compare performance against a baseline active learning system without higher-order modeling.

2. **Scalability Assessment**: Implement the framework on a real-world dataset with higher dimensionality (e.g., image or text preferences) and evaluate computational performance. Measure inference time for ToM reasoning and query generation at each agent level, identifying the point where computational costs outweigh potential benefits.

3. **Belief Attribution Validation**: Design an experiment where humans are explicitly asked to predict what the AI agent believes about their preferences based on observed queries. Compare human predictions against the model's inferred beliefs to validate the ToM inference mechanism. Test both successful belief detection cases and cases where the agent has false beliefs.