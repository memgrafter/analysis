---
ver: rpa2
title: Measuring User Understanding in Dialogue-based XAI Systems
arxiv_id: '2408.06960'
source_url: https://arxiv.org/abs/2408.06960
tags:
- understanding
- explanations
- users
- user
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a controlled experimental framework to objectively
  measure user understanding in dialogue-based XAI systems. The framework uses a three-phase
  simulation task to assess understanding before and after interaction with either
  static or interactive explanations.
---

# Measuring User Understanding in Dialogue-based XAI Systems

## Quick Facts
- arXiv ID: 2408.06960
- Source URL: https://arxiv.org/abs/2408.06960
- Reference count: 40
- Dialogue-based XAI significantly improves objective model understanding (p < 0.004) compared to static explanations

## Executive Summary
This study introduces a controlled experimental framework to objectively measure user understanding in dialogue-based XAI systems. Through a three-phase simulation task, researchers assessed understanding before and after interaction with either static or interactive explanations. The results demonstrate that dialogue-based XAI significantly improves objective model understanding compared to static explanations, with users who showed the highest understanding gains asking more feature-specific questions and exploring explanations in greater depth.

## Method Summary
The research employed a three-phase experiment framework with 200 lay users recruited through Prolific. Participants first completed an initial test without explanations, then engaged with either static written reports or interactive dialogue-based explanations, and finally completed a post-test. The study used a Random Forest Classifier on the Adult dataset and measured understanding through prediction tasks scored using a one-parameter Item Response Theory (IRT) model. Explanations were generated using LIME, DICE, Anchors, and Ceteris Paribus methods.

## Key Results
- Dialogue-based XAI significantly improves objective model understanding (p < 0.004) compared to static explanations
- Users with the highest understanding gains engaged more deeply with feature-specific questions and explored individual attributes thoroughly
- The structured three-phase experiment design effectively isolates the impact of interactive explanations on model understanding by controlling for prior intuition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interactive dialogue-based XAI systems lead to significantly higher objective model understanding than static explanations
- Mechanism: The dialogue format requires users to actively select questions, promoting deeper cognitive engagement with the model's decision-making process. This active selection allows users to construct personalized explanation paths tailored to their specific needs
- Core assumption: Active user engagement through question selection is more effective for understanding complex AI models than passive consumption of static explanations
- Evidence anchors:
  - Results show that dialogue-based XAI significantly improves objective model understanding (p < 0.004) compared to static explanations
  - Interactive explanations significantly increase objective model understanding, with no substantial change in subjective understanding reported by users
- Break condition: If users are overwhelmed by too many choices or lack sufficient domain knowledge to ask relevant questions, the interactive approach may not yield better understanding

### Mechanism 2
- Claim: Users who show the highest understanding gains engage more deeply with feature-specific questions and explore individual attributes thoroughly
- Mechanism: By asking feature-specific questions like "What happens if the value of Investment Outcome is changed," users gain a granular understanding of how individual features influence model predictions
- Core assumption: Understanding the impact of individual features is crucial for grasping overall model behavior, especially for non-expert users
- Evidence anchors:
  - Participants who achieved the highest understanding gains explored individual attributes more thoroughly by asking feature-specific questions and selected more questions overall
  - The former focused more on feature-specific questions and did not finish the dialogues after general questions as often as the latter group
- Break condition: If the feature space is too large or features are too complex, users may not be able to effectively explore individual attributes, limiting understanding gains

### Mechanism 3
- Claim: The structured three-phase experiment design effectively isolates the impact of interactive explanations on model understanding by controlling for prior intuition and measuring understanding over time
- Mechanism: By assessing user intuition before any explanations, then measuring understanding during and after the learning phase, the study can attribute changes in understanding specifically to the explanation format
- Core assumption: Controlling for prior knowledge and using standardized scoring methods allows for accurate measurement of explanation effectiveness
- Evidence anchors:
  - The study employed a one-parameter Item Response Theory (IRT) model to assign performance scores, and intuitive performance across groups was not significantly different
  - The three-phase experiment framework effectively measures objective user understanding through prediction tasks comparing different interface designs
- Break condition: If the task is too simple or too complex, or if the IRT model doesn't accurately capture difficulty variations, measurements may not reflect true understanding differences

## Foundational Learning

- Concept: Item Response Theory (IRT)
  - Why needed here: IRT is used to adjust for varying difficulty across prediction instances, ensuring fair and accurate measurement of model understanding
  - Quick check question: How does IRT account for differences in item difficulty when comparing participants' scores?
- Concept: Cognitive forcing
  - Why needed here: Requiring users to make initial predictions before seeing explanations promotes active engagement and reflection, enhancing understanding
  - Quick check question: Why might delaying the display of explanations until after an initial prediction improve learning outcomes?
- Concept: Feature attribution methods (LIME, Anchors, Ceteris Paribus)
  - Why needed here: These methods provide different types of explanations (global importance, necessary conditions, individual feature impact) that users can explore interactively
  - Quick check question: What is the difference between LIME's local feature importance and Anchors' necessary conditions for a prediction?

## Architecture Onboarding

- Component map: Web application with three main panels - instance attributes (upper left), question selection (right), chatbot history (center). Static condition replaces chatbot with a written report. Backend handles question-answer mapping and IRT scoring
- Critical path: User loads instance → makes prediction → selects question → receives explanation → updates understanding → repeats until satisfied → final assessment
- Design tradeoffs: Predefined questions ensure clarity but limit flexibility; IRT scoring adds complexity but improves measurement accuracy; three-phase design controls for prior knowledge but increases experiment length
- Failure signatures: Users not engaging with questions (low click-through), misunderstanding explanations (incorrect predictions despite explanations), or dropping out early (experiment too long or complex)
- First 3 experiments:
  1. Test the predefined question set with a small user group to ensure clarity and coverage of common user needs
  2. Compare understanding scores between interactive and static conditions with a pilot study to validate the experimental design
  3. Analyze question selection patterns in the interactive condition to identify which explanations are most frequently used and correlate with understanding gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of interactive explanations—such as freedom to explore, depth of engagement, or real-time feedback—most effectively enhance user understanding compared to static explanations?
- Basis in paper: The paper discusses that interactive explanations significantly improve objective model understanding and that participants with higher understanding gains engaged more with feature-specific questions and explored explanations more thoroughly
- Why unresolved: The study identifies that interactive explanations are more effective but does not isolate which specific interactive features contribute most to improved understanding
- What evidence would resolve it: A controlled experiment varying individual interactive features (e.g., question freedom, depth of explanation, real-time feedback) while keeping other factors constant could identify which aspects are most critical for enhancing understanding

### Open Question 2
- Question: Should the sequence of explanations in dialogue-based XAI systems be structured and guided (starting from general to specific) or driven by the explainee's inquiries?
- Basis in paper: The paper notes that participants with the highest understanding improvements typically started with general questions about important features before exploring specific details, while those with lower improvements often started directly with detailed explanations
- Why unresolved: The study observes patterns in question sequences but does not determine whether a structured approach or user-driven inquiry is more effective overall
- What evidence would resolve it: Comparative studies testing structured versus user-driven explanation sequences on objective understanding outcomes would clarify which approach is more effective

### Open Question 3
- Question: How does the complexity of the AI task and the domain influence the effectiveness of interactive versus static explanations in enhancing user understanding?
- Basis in paper: The study uses a simplified income prediction task with intuitive, mostly categorical features, suggesting that task complexity might affect the impact of interactive explanations
- Why unresolved: The research focuses on a specific task and dataset, leaving open how results might vary with more complex tasks or domains
- What evidence would resolve it: Replicating the study across diverse AI tasks and domains with varying complexity would reveal how task characteristics influence the effectiveness of interactive explanations

## Limitations
- Sample characteristics are limited to Prolific participants with specific demographic filters (UK/US residents, native English speakers)
- Results are based on a single classification task (income prediction from the Adult dataset) and may not generalize to other ML tasks or domains
- The study only compares dialogue-based explanations to static text, not to other interactive formats like direct manipulation or visual interfaces

## Confidence
- High confidence in the finding that dialogue-based XAI significantly improves objective model understanding (p < 0.004), given the controlled experimental design and rigorous statistical analysis
- Medium confidence in the mechanism that deeper engagement with feature-specific questions drives understanding gains, as this is supported by correlation but not definitively proven causation
- Medium confidence in the generalizability of results across different XAI tasks, due to the single-task nature of the study

## Next Checks
1. Replicate the experiment with a more diverse participant pool across different age groups, education levels, and cultural backgrounds to assess generalizability
2. Conduct the same study using a different ML task (e.g., medical diagnosis or loan approval) to verify that the dialogue advantage holds across domains
3. Test the understanding framework with alternative explanation types (e.g., visual vs. textual) to determine if the dialogue format's advantage is specific to text-based explanations or extends to multimodal explanations