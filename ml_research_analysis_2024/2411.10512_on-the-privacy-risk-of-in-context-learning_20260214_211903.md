---
ver: rpa2
title: On the Privacy Risk of In-context Learning
arxiv_id: '2411.10512'
source_url: https://arxiv.org/abs/2411.10512
tags:
- prompted
- data
- rate
- privacy
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that data used in few-shot prompts for large language
  models is highly vulnerable to membership inference attacks (MIAs). The authors
  find that prompted models leak private information more severely than fine-tuned
  models at similar utility levels, with AUC scores around 0.72-0.86 depending on
  the dataset.
---

# On the Privacy Risk of In-context Learning

## Quick Facts
- arXiv ID: 2411.10512
- Source URL: https://arxiv.org/abs/2411.10512
- Authors: Haonan Duan; Adam Dziedzic; Mohammad Yaghini; Nicolas Papernot; Franziska Boenisch
- Reference count: 40
- Key outcome: Prompted models leak more private information than fine-tuned models at similar utility levels, with AUC scores around 0.72-0.86; ensembling reduces this to ~0.50

## Executive Summary
This paper demonstrates that in-context learning (prompting) in large language models is significantly more vulnerable to membership inference attacks (MIAs) than fine-tuning, even at equivalent utility levels. The authors find that prompted models achieve AUC scores of 0.72-0.86 across multiple datasets, substantially higher than the random guessing baseline of 0.50. They attribute this vulnerability to higher prediction confidence on prompted data and propose an ensembling defense that reduces MIA success rates to near-random levels by aggregating predictions from multiple independently prompted models.

## Method Summary
The study evaluates privacy risks of in-context learning using GPT2-xl and GPT2-base models on four classification datasets (agnews, cb, sst2, trec). The methodology involves generating 1000 random 4-shot prompts per dataset, selecting the top 50 performing prompts with disjoint training data, and performing MIAs using the model's output probability at the correct target class. For comparison, fine-tuned models matching the prompted models' utility are also evaluated. The defense mechanism involves ensembling 50 prompted models using either probability averaging (Avg-Ens) or majority voting (Vote-Ens), with MIA success measured by AUC and TPR at low FPRs.

## Key Results
- Prompted models show significantly higher MIA vulnerability than fine-tuned models at equivalent utility (AUC: 0.72-0.86 vs baseline 0.50)
- Larger models (GPT2-xl) leak less private information than smaller ones (GPT2-base)
- Ensembling 50 prompted models reduces MIA success rates to near-random levels (AUC ≈ 0.50)
- Prediction confidence differences between member and non-member data drive privacy leakage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompted models leak more private information than fine-tuned models at the same utility levels.
- Mechanism: Higher prediction confidence on prompted data increases vulnerability to membership inference attacks.
- Core assumption: The model's prediction distribution differs significantly between member and non-member data.
- Evidence anchors:
  - [abstract] "We also observe that the privacy risk of prompted models exceeds fine-tuned models at the same utility levels."
  - [section] "Figure 3 shows for the sst2 dataset that the prediction outputs for non-members are overall lower than for members."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.423, average citations=0.0.
- Break condition: If prediction confidence becomes similar between member and non-member data.

### Mechanism 2
- Claim: Ensembling multiple prompted models mitigates privacy leakage.
- Mechanism: Averaging predictions over multiple models reduces the difference in output distribution between member and non-member data.
- Core assumption: Each prompted model in the ensemble uses disjoint training data.
- Evidence anchors:
  - [abstract] "By aggregating over multiple different versions of a prompted model, membership inference risk can be decreased."
  - [section] "Figure 11 in Appendix B highlights that through ensembling, the distributions for member and non-member probabilities become much more similar."
  - [corpus] "Membership Inference Attacks Against In-Context Learning" (weak evidence - no direct support for ensembling mitigation).
- Break condition: If models in ensemble are highly correlated or use overlapping data.

### Mechanism 3
- Claim: Larger models leak less private information about their prompts.
- Mechanism: Better generalization capacity of larger models leads to smaller differences in output distribution between member and non-member data.
- Core assumption: Model size correlates with generalization ability.
- Evidence anchors:
  - [section] "Figure 5 depicts the membership risk of prompted models of different sizes... We find that GPT2-base consistently yields higher TPRs (i.e., higher membership risk) than GPT2-xl across different datasets."
  - [corpus] Weak evidence - no direct support in corpus for this specific mechanism.
- Break condition: If larger models are overfitted or have other vulnerabilities.

## Foundational Learning

- Concept: Membership Inference Attack (MIA)
  - Why needed here: The paper's main threat model and evaluation metric
  - Quick check question: What does an adversary need to perform a successful MIA?

- Concept: In-context Learning (Prompting)
  - Why needed here: The paper's focus on privacy risks in prompting vs fine-tuning
  - Quick check question: How does prompting differ from fine-tuning in terms of parameter updates?

- Concept: Ensemble Methods
  - Why needed here: The proposed defense mechanism against MIAs
  - Quick check question: What are the two main ensemble approaches evaluated in the paper?

## Architecture Onboarding

- Component map:
  - Prompted models (Lprompt) → Probability output (y) → Membership inference attack → AUC/TPR metrics
  - Ensembling layer → Aggregation function (Avg-Ens or Vote-Ens) → Defense against MIAs

- Critical path:
  1. Generate prompts with disjoint training data
  2. Query prompted models to get probability outputs
  3. Perform MIA using probability outputs
  4. Evaluate privacy risk (AUC, TPR)
  5. Apply ensembling defense
  6. Re-evaluate privacy risk

- Design tradeoffs:
  - Privacy vs Utility: Ensembling reduces privacy risk but may slightly impact utility
  - Inference time vs Privacy: Larger ensembles provide better privacy but increase inference time
  - Model size vs Privacy: Larger models provide better privacy but are more resource-intensive

- Failure signatures:
  - High AUC scores in MIA despite ensembling
  - Large standard deviation in TPR across prompted models
  - Significant drop in utility after applying ensembling defense

- First 3 experiments:
  1. Replicate the basic MIA on prompted models using the sst2 dataset
  2. Compare MIA success rates between GPT2-base and GPT2-xl
  3. Evaluate the effectiveness of Avg-Ens on reducing MIA risk

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ensembling also reduce privacy risks when using soft prompts instead of discrete prompts?
- Basis in paper: [inferred] The paper mentions soft prompts as a future research direction and notes that their study is limited to discrete prompts.
- Why unresolved: The paper explicitly states that privacy leakage of soft prompts and potential defenses are addressed in future work, indicating this question remains unanswered.
- What evidence would resolve it: Experimental evaluation of MIA on ensembled soft-prompted models compared to discrete-prompted models would demonstrate whether ensembling provides similar privacy benefits.

### Open Question 2
- Question: Can the ensemble size required for privacy protection be minimized while maintaining utility?
- Basis in paper: [explicit] The paper shows that increasing ensemble size reduces privacy risk but acknowledges a trade-off between inference time and privacy.
- Why unresolved: While the paper demonstrates the general trend that larger ensembles provide better privacy, it doesn't optimize the ensemble size or explore the precise utility-privacy trade-off curve.
- What evidence would resolve it: Systematic experiments varying ensemble size while measuring both MIA success rates and downstream task accuracy would identify the minimal ensemble size achieving desired privacy-utility balance.

### Open Question 3
- Question: Do larger language models require larger ensembles for privacy protection?
- Basis in paper: [explicit] The paper shows that larger models have lower privacy risk but doesn't investigate whether ensemble requirements scale with model size.
- Why unresolved: The paper only compares GPT2-base and GPT2-xl for privacy risk analysis, not for ensemble effectiveness, leaving the relationship between model size and ensemble requirements unexplored.
- What evidence would resolve it: Experiments testing ensemble effectiveness on progressively larger models (e.g., GPT2-xl, GPT-Neo, GPT-3) would reveal whether larger models need proportionally larger ensembles for privacy protection.

## Limitations

- The results may not generalize to real-world prompting scenarios where prompts are less carefully curated
- The 50-model ensemble requires significant computational overhead and resources
- The study focuses on classification tasks and may not extend to other NLP tasks
- The paper doesn't explore adaptive adversaries who might design prompts to evade ensemble defenses

## Confidence

- **High confidence**: The observation that prompted models leak more private information than fine-tuned models at similar utility levels is well-supported by experimental results across multiple datasets.
- **Medium confidence**: The effectiveness of ensembling as a defense mechanism is demonstrated but may be sensitive to implementation details like prompt diversity and ensemble size.
- **Medium confidence**: The claim that larger models leak less private information is supported by experiments but may depend on other factors not controlled for in the study.

## Next Checks

1. Test the ensembling defense against adaptive adversaries who craft prompts specifically designed to maximize correlation between ensemble members, potentially defeating the defense.
2. Evaluate the framework on non-classification tasks (e.g., summarization, question answering) to assess generalizability beyond the datasets used.
3. Measure the actual computational overhead and resource requirements of implementing the 50-model ensemble in production environments, including inference latency and storage costs.