---
ver: rpa2
title: 'Balancing Speciality and Versatility: A Coarse to Fine Framework for Mitigating
  Catastrophic Forgetting in Large Language Models'
arxiv_id: '2404.10306'
source_url: https://arxiv.org/abs/2404.10306
tags:
- versatility
- speciality
- layer
- cofitune
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting (CF) in large language
  models (LLMs) when fine-tuning for specialized tasks. The authors propose CoFiTune,
  a coarse-to-fine framework that balances specialty and versatility by first identifying
  key modules in specific layer ranges for fine-tuning using an empirical tree-search
  algorithm, then applying a fine-grained soft-masking mechanism to regulate gradient
  updates based on unit importance for versatility.
---

# Balancing Speciality and Versatility: A Coarse to Fine Framework for Mitigating Catastrophic Forgetting in Large Language Models

## Quick Facts
- arXiv ID: 2404.10306
- Source URL: https://arxiv.org/abs/2404.10306
- Reference count: 40
- Key outcome: CoFiTune framework achieves over 95% versatility and 90% specialty by fine-tuning only FFN modules in mid-to-lower layers with Fine-SoftMask gradient regulation

## Executive Summary
This paper addresses catastrophic forgetting (CF) in large language models (LLMs) when fine-tuning for specialized tasks. The authors propose CoFiTune, a coarse-to-fine framework that balances specialty and versatility by first identifying key modules in specific layer ranges for fine-tuning using an empirical tree-search algorithm, then applying a fine-grained soft-masking mechanism to regulate gradient updates based on unit importance for versatility. CoFiTune outperforms baseline methods across diverse tasks and model scales, achieving over 95% versatility and 90% specialty compared to original and full SFT models.

## Method Summary
CoFiTune employs a two-stage approach to mitigate catastrophic forgetting during LLM fine-tuning. First, an empirical tree-search algorithm identifies the optimal layer range and module (FFN in mid-to-lower layers) for fine-tuning by evaluating different configurations. Second, the Fine-SoftMask mechanism regulates backward gradient flow based on estimated unit importance for versatility preservation, using KL-divergence loss as a proxy. The framework is evaluated across multiple tasks (Finance, Law, Math, CAG) and model scales (1.8B, 7B), demonstrating consistent improvements in balancing specialty and versatility compared to baseline methods.

## Key Results
- CoFiTune achieves over 95% versatility and 90% specialty compared to original and full SFT models
- Fine-tuning only FFN modules in mid-to-lower layers provides optimal balance between specialty and versatility
- Fine-SoftMask effectively mitigates CF in LLM's versatility without harming specialty performance
- Consistent improvements over baseline methods across diverse tasks and model scales

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-tuning only FFN modules in mid-to-lower layers achieves the best balance between specialty and versatility.
- **Mechanism**: The mid-to-lower layers are responsible for processing and extracting relevant knowledge, while the bottom layers predominantly store versatility-related information. Fine-tuning FFN modules in the mid-to-lower layers allows for gaining specialty without significantly impacting versatility.
- **Core assumption**: The information forwarding process in LLMs involves knowledge extraction and processing in the mid-to-lower layers, with the bottom layers storing versatility information.
- **Evidence anchors**:
  - [abstract] "We find that only tuning the FFN module in the mid-to-lower layer range achieves satisfactory speciality without significantly affecting versatility."
  - [section] "In step 3, the Uni. score for '(10, 20] - FFN' surpasses all other configurations. Specially, the Spec. score of '(10, 20] - FFN' is comparable to '(10, 20] - MHA & FFN' while its Vers. score is superior."
  - [corpus] Weak or missing - the corpus neighbors do not directly address this specific mechanism.
- **Break condition**: If the information forwarding process in LLMs is significantly different from the assumed mechanism, or if the role of FFN modules and layer ranges differs from what is stated in the paper.

### Mechanism 2
- **Claim**: Fine-SoftMask effectively mitigates catastrophic forgetting in LLM's versatility without harming specialty performance.
- **Mechanism**: Fine-SoftMask regulates the backward gradient flow based on the importance values of units for versatility, allowing for more targeted updates that preserve versatility while still gaining specialty.
- **Core assumption**: Not all units (attention heads or neurons) in a module contribute equally to LLM versatility, and their importance can be estimated using a proxy based on robustness (KL-divergence loss).
- **Evidence anchors**:
  - [abstract] "Subsequently, we utilize a fine-grained soft-masking (Fine-SoftMask) mechanism to control the backward gradient flow based on their importance values for versatility, aiming to further mitigate the CF issue."
  - [section] "When applying Fine-SoftMask, we observe nearly identical Spec. scores in the Finance and Law tasks compared to not applying it, and even a slight improvement in CAG and Math tasks."
  - [corpus] Weak or missing - the corpus neighbors do not directly address this specific mechanism.
- **Break condition**: If the estimated importance values for versatility do not accurately reflect the true importance of units, or if the Fine-SoftMask mechanism does not effectively regulate the gradient flow as intended.

### Mechanism 3
- **Claim**: The CoFiTune framework consistently outperforms baseline methods across diverse tasks and model scales.
- **Mechanism**: By employing an empirical tree-search algorithm to identify the optimal layer range and module for fine-tuning, and applying the Fine-SoftMask mechanism, CoFiTune effectively balances specialty and versatility gains.
- **Core assumption**: The empirical tree-search algorithm can identify the best layer range and module for fine-tuning, and the Fine-SoftMask mechanism can effectively mitigate catastrophic forgetting.
- **Evidence anchors**:
  - [abstract] "In an overall evaluation of both speciality and versatility, CoFiTune consistently outperforms baseline methods across diverse tasks and model scales."
  - [section] "CoFiTune consistently outperforms all baseline methods. Specifically, in the Finance task of 7B model, it exhibits improvements in Uni. scores of 3.7%, 4.3%, and 4.5% compared to L1, LoRA, and V-SoftMask respectively."
  - [corpus] Weak or missing - the corpus neighbors do not directly address this specific mechanism.
- **Break condition**: If the empirical tree-search algorithm fails to identify the optimal layer range and module, or if the Fine-SoftMask mechanism does not effectively mitigate catastrophic forgetting, leading to subpar performance compared to baseline methods.

## Foundational Learning

- **Concept**: Catastrophic forgetting (CF) in machine learning.
  - **Why needed here**: Understanding CF is crucial for grasping the motivation behind CoFiTune and its approach to mitigating this issue.
  - **Quick check question**: What is catastrophic forgetting, and why is it a significant challenge in continual learning and fine-tuning of machine learning models?

- **Concept**: Large Language Models (LLMs) and their architecture.
  - **Why needed here**: CoFiTune is designed specifically for LLMs, and understanding their architecture (e.g., transformer layers, attention mechanisms, feed-forward networks) is essential for comprehending the proposed framework.
  - **Quick check question**: What are the key components of a typical LLM architecture, and how do they contribute to the model's performance on various tasks?

- **Concept**: Fine-tuning and transfer learning in machine learning.
  - **Why needed here**: CoFiTune involves fine-tuning LLMs for specific tasks, and understanding the principles of fine-tuning and transfer learning is necessary to appreciate the proposed approach.
  - **Quick check question**: What is fine-tuning, and how does it differ from other forms of transfer learning in machine learning?

## Architecture Onboarding

- **Component map**: CoFiTune consists of empirical tree-search algorithm for identifying optimal layer range/module -> Fine-SoftMask mechanism for gradient regulation -> Fine-tuning process for specialty gains
- **Critical path**: Run empirical tree-search to identify best layer range and module, then apply Fine-SoftMask during fine-tuning to mitigate catastrophic forgetting
- **Design tradeoffs**: CoFiTune trades off some degree of specialty for improved versatility by fine-tuning only specific modules in mid-to-lower layers, rather than the entire model
- **Failure signatures**: Poor identification of optimal layer range/module by tree-search, ineffective CF mitigation by Fine-SoftMask, suboptimal performance compared to baselines
- **First 3 experiments**:
  1. Implement empirical tree-search algorithm to identify best layer range/module on small-scale LLM and task
  2. Apply Fine-SoftMask during fine-tuning and evaluate impact on CF mitigation
  3. Compare CoFiTune performance with baseline methods on same task and LLM, focusing on specialty-versatility balance

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on empirical observations about layer-specific functionality without theoretical grounding
- Effectiveness of Fine-SoftMask depends on proxy importance scores that may not accurately reflect true unit importance
- Empirical tree-search optimality claims based on specific search space that may not generalize across different task types

## Confidence
- **Core claims validation**: Medium - experimental results show consistent improvements but lack theoretical justification and comprehensive ablation studies
- **Generalizability**: Low - limited testing across diverse task distributions and no long-term continual learning analysis
- **Mechanism reliability**: Medium - Fine-SoftMask shows effectiveness but depends on potentially unreliable importance proxies

## Next Checks
1. Conduct ablation studies removing either the empirical tree-search component or the Fine-SoftMask mechanism to quantify their individual contributions to performance gains
2. Test the approach on a more diverse set of task pairs including non-specialized tasks (creative writing, general reasoning) to evaluate robustness across task types
3. Implement a long-term continual learning scenario with sequential task additions to measure degradation over time and compare against methods specifically designed for continual learning settings