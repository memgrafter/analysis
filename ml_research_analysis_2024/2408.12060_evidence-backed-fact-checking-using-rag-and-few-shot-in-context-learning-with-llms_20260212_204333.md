---
ver: rpa2
title: Evidence-backed Fact Checking using RAG and Few-Shot In-Context Learning with
  LLMs
arxiv_id: '2408.12060'
source_url: https://arxiv.org/abs/2408.12060
tags:
- evidence
- claim
- system
- fact
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces an automated fact-checking system for online
  claims that combines retrieval-augmented generation (RAG) with few-shot in-context
  learning (ICL) using large language models. The system retrieves relevant documents,
  extracts evidence via question-answer generation, and classifies claims into one
  of four categories: Supported, Refuted, Conflicting Evidence, or Not Enough Evidence.'
---

# Evidence-backed Fact Checking using RAG and Few-Shot In-Context Learning with LLMs

## Quick Facts
- arXiv ID: 2408.12060
- Source URL: https://arxiv.org/abs/2408.12060
- Reference count: 11
- Primary result: Achieves Averitec score of 0.33, representing 22% absolute improvement over baseline

## Executive Summary
This paper presents an automated fact-checking system that combines retrieval-augmented generation (RAG) with few-shot in-context learning (ICL) using large language models. The system addresses the challenge of verifying online claims by retrieving relevant documents, extracting evidence through question-answer generation, and classifying claims into four categories: Supported, Refuted, Conflicting Evidence, or Not Enough Evidence. The approach requires minimal training data while providing evidence-backed predictions.

## Method Summary
The system architecture employs a multi-stage pipeline: first retrieving relevant documents based on the claim, then generating questions and answers from these documents to extract evidence, and finally using few-shot ICL with LLMs to classify the claim. The few-shot approach allows the model to learn from minimal examples while maintaining strong performance. The system operates in a zero-training or few-shot setup, making it adaptable to different domains without extensive retraining requirements.

## Key Results
- Achieves Averitec score of 0.33 on the Averitec dataset
- Demonstrates 22% absolute improvement over baseline methods
- Shows effectiveness of combining RAG with few-shot ICL for fact-checking tasks

## Why This Works (Mechanism)
The system leverages the complementary strengths of retrieval and generation. RAG ensures access to relevant external knowledge, while few-shot ICL allows the model to adapt to fact-checking tasks without extensive fine-tuning. The question-answer generation step acts as an evidence extraction mechanism, converting retrieved documents into structured evidence that can be more easily processed by the classification model.

## Foundational Learning
- Retrieval-augmented generation (RAG): Why needed - provides access to external knowledge; Quick check - evaluate retrieval recall on benchmark datasets
- Few-shot in-context learning (ICL): Why needed - enables adaptation without fine-tuning; Quick check - measure performance across different few-shot sample sizes
- Question-answer generation for evidence extraction: Why needed - structures unstructured documents into verifiable evidence; Quick check - assess quality of generated questions and answers

## Architecture Onboarding

**Component Map:** Document Retrieval -> Question-Answer Generation -> Few-Shot ICL Classification

**Critical Path:** The system's performance is bottlenecked by document retrieval quality. Poor retrieval results directly impact the evidence extraction quality and ultimately the classification accuracy. The few-shot ICL component must be carefully designed to handle the evidence format generated by the Q&A module.

**Design Tradeoffs:** The system trades computational efficiency for accuracy by using multiple retrieval and generation steps. The few-shot approach requires careful prompt engineering but eliminates the need for task-specific fine-tuning. The evidence extraction step adds complexity but provides interpretability to the predictions.

**Failure Signatures:** 
- Low retrieval recall manifests as "Not Enough Evidence" classifications even for verifiable claims
- Poor question generation results in incomplete or irrelevant evidence
- Few-shot ICL may struggle with ambiguous evidence leading to "Conflicting Evidence" classifications

**First Experiments:**
1. Measure retrieval recall@10 on held-out documents to establish baseline retrieval quality
2. Evaluate question-answer generation quality using standard metrics (ROUGE, BLEU)
3. Test few-shot ICL performance with varying numbers of examples (1-shot, 5-shot, 10-shot)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond noting the challenges with "Conflicting Evidence" and "Not Enough Evidence" categories.

## Limitations
- Evaluation limited to single Averitec dataset, limiting generalizability
- Performance challenges on "Conflicting Evidence" and "Not Enough Evidence" categories
- Quality of predictions directly depends on retrieval effectiveness, with no discussion of failure handling

## Confidence
- High confidence: System architecture combining RAG with few-shot ICL is technically sound
- Medium confidence: 22% improvement claim valid within Averitec dataset context
- Low confidence: Generalizability to other fact-checking datasets and real-world deployment

## Next Checks
1. Evaluate the system on additional fact-checking benchmarks (FEVER, SciFact, or domain-specific datasets) to assess generalizability across different claim types and domains
2. Conduct ablation studies to quantify the individual contributions of RAG retrieval quality, question-answer generation, and few-shot ICL components to overall performance
3. Perform robustness testing by deliberately introducing retrieval failures or conflicting evidence scenarios to evaluate system behavior under adverse conditions