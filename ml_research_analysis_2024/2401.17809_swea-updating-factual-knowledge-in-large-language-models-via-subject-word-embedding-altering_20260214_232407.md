---
ver: rpa2
title: 'SWEA: Updating Factual Knowledge in Large Language Models via Subject Word
  Embedding Altering'
arxiv_id: '2401.17809'
source_url: https://arxiv.org/abs/2401.17809
tags:
- embedding
- knowledge
- editing
- word
- subject
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel method for updating factual knowledge
  in large language models (LLMs) called Subject Word Embedding Altering (SWEA). The
  key idea is to modify the word embedding of subject tokens in the input to control
  the factual knowledge triggered by that subject.
---

# SWEA: Updating Factual Knowledge in Large Language Models via Subject Word Embedding Altering

## Quick Facts
- arXiv ID: 2401.17809
- Source URL: https://arxiv.org/abs/2401.17809
- Reference count: 20
- Authors: Xiaopeng Li, Shasha Li, Shezheng Song, Huijun Liu, Bin Ji, Xi Wang, Jun Ma, Jie Yu, Xiaodong Liu, Jing Wang, Weimin Zhang

## Executive Summary
The paper introduces Subject Word Embedding Altering (SWEA), a framework for updating factual knowledge in large language models by modifying subject token embeddings at the input layer. The method uses precise token-level matching to identify relevant subject tokens and fuses them with learnable embeddings optimized for target knowledge. An "optimizing then suppressing" fusion approach addresses conflicts between original and edited embeddings. Experiments on COUNTER FACT, zsRE, and RIPPLE EDITS benchmarks demonstrate state-of-the-art performance, particularly in reasoning about edited knowledge while preserving original model weights.

## Method Summary
SWEAOS modifies the word embedding of subject tokens in the input to control factual knowledge retrieval without altering model weights. The framework uses precise token-level matching to find relevant subject tokens and fuses them with learnable embeddings optimized for target knowledge. An additional "optimizing then suppressing" step identifies and suppresses Knowledge Embedding Dimensions (KEDs) to avoid conflicts between original and updated embeddings. The method is implemented as a hook in the input processing layer with a fused embedding cache indexed by token IDs.

## Key Results
- SWEAOS achieves superior efficacy and specificity metrics compared to ROME/MEMIT on COUNTER FACT dataset
- State-of-the-art performance on zsRE and RIPPLE EDITS benchmarks, especially in reasoning about edited knowledge
- Demonstrates effectiveness in updating factual knowledge while preserving original model weights
- Shows better generalization and logical reasoning capabilities than competing methods

## Why This Works (Mechanism)

### Mechanism 1
Modifying subject word embeddings in the input layer can trigger changes in factual knowledge retrieval without altering model weights. The framework modifies the word embedding of subject tokens before they enter Transformer layers, changing the knowledge trigger while preserving original parameters. Word embeddings contain task-specific information where certain dimensions correspond to specific factual knowledge about the subject.

### Mechanism 2
The "optimizing then suppressing" fusion method resolves conflicts between original and edited embeddings by selectively suppressing knowledge-relevant dimensions. First optimizes a learnable embedding vector for target knowledge, then uses knowledge attribution methods to identify and suppress dimensions (KEDs) corresponding to original knowledge.

### Mechanism 3
Precise token-level matching in SWEA provides more reliable knowledge editing than vector-level matching approaches. Uses exact token ID matching to identify subject tokens and apply fused embeddings, avoiding fuzzy matching issues of vector-level approaches.

## Foundational Learning

- **Word embeddings and dimensional interpretability**: Word embedding dimensions carry interpretable semantic information that can be manipulated to control factual knowledge retrieval. [Quick check: What research demonstrates that different dimensions of word embeddings correspond to specific semantic concepts or knowledge?]

- **Knowledge attribution and influence methods**: Knowledge attribution techniques identify which dimensions of embeddings are responsible for specific factual associations. [Quick check: How does knowledge attribution (Dai et al., 2021) identify which embedding dimensions contribute to specific factual associations?]

- **Model editing paradigms and tradeoffs**: Understanding the landscape of model editing methods (global vs local optimization, parameter editing vs module addition) is crucial for positioning SWEAOS's approach. [Quick check: What are the key differences between global optimization, local editing, and module addition approaches in model editing?]

## Architecture Onboarding

- **Component map**: Input processing layer with SWEA fusion hook -> Fused embedding cache indexed by token IDs -> Optimization module for computing fused embeddings -> Knowledge attribution module for KED identification -> Inference pipeline with longest continuous match algorithm

- **Critical path**: 1. Tokenization and embedding generation 2. Token ID matching against fused embedding cache 3. Embedding fusion when matches are found 4. Forward pass through Transformer layers 5. Output generation

- **Design tradeoffs**: Precision vs. flexibility (token-level matching is more precise but requires subject to appear in text), inference overhead (minimal for SWEA itself but requires maintaining fused embedding cache), knowledge preservation (suppressing step protects original knowledge but may reduce fluency)

- **Failure signatures**: No subject token in input text → no editing occurs, poor optimization → conflicting knowledge representations, cache misses → edited knowledge not applied, excessive suppression → loss of fluency or coherence

- **First 3 experiments**: 1. Baseline verification: Run COUNTER FACT dataset with GPT-J(6B) to confirm SWEAOS achieves superior efficacy and specificity metrics compared to ROME/MEMIT 2. Subject presence test: Create test cases with and without subject tokens in input to verify editing only occurs when subject is present 3. Cache performance: Measure cache hit rate and inference latency impact when applying SWEA to batch inputs with multiple edited subjects

## Open Questions the Paper Calls Out

### Open Question 1
How do different dimensions of word embeddings correspond to specific factual knowledge about subjects in large language models? The paper mentions that different dimensions of word embeddings contain task-specific information and that certain dimensions may correspond to specific factual knowledge about subjects. Current research does not fully explain word embeddings, making it difficult to directly change specific dimensions corresponding to factual knowledge. A comprehensive analysis mapping each dimension of word embeddings to specific types of factual knowledge would provide insights into this correspondence.

### Open Question 2
How does the SWEA framework handle subjects that appear in different contexts or with different aliases? The paper discusses using token IDs as keys to cache fused embeddings, but it does not explicitly address how the framework deals with subject aliases or contextual variations. The paper does not provide details on how the framework manages variations in subject representation across different contexts or aliases. Experiments demonstrating the framework's ability to correctly identify and update knowledge for subjects with multiple aliases or in varying contexts would clarify this aspect.

### Open Question 3
What is the impact of the optimizing then suppressing fusion method on the overall performance and generalization of large language models? The paper mentions that the optimizing then suppressing fusion method is used to obtain fused embeddings and discusses its role in mitigating conflicts between optimized and original word vectors. The paper does not provide a detailed analysis of how this fusion method affects the model's performance and generalization beyond the specific task of knowledge editing. A comprehensive evaluation of the model's performance on various tasks before and after applying the fusion method would provide insights into its broader impact.

### Open Question 4
How can the SWEA framework be extended to handle subjects that do not appear explicitly in the input text? The paper acknowledges a limitation where the SWEA framework cannot edit knowledge for subjects not present in the input sentence. The paper suggests that adjusting the output embedding layer might solve this issue but does not provide a concrete solution or experimental validation. Developing and testing a method to extend the SWEA framework to handle implicit subjects would address this limitation and demonstrate its broader applicability.

## Limitations
- Dependency on subject token presence in input text creates hard constraint limiting real-world applicability
- Knowledge attribution method effectiveness for identifying KEDs not thoroughly validated across different model architectures
- Suppression mechanism introduces uncertainty about optimal suppression levels and potential degradation of model fluency

## Confidence

**High Confidence**: The core mechanism of modifying subject word embeddings at the input layer is technically sound and the experimental methodology is rigorous. The results showing SWEAOS outperforming baselines on the COUNTER FACT dataset are well-documented and reproducible.

**Medium Confidence**: The "optimizing then suppressing" fusion method shows promise but requires more extensive ablation studies to confirm that suppression is necessary and optimal. The knowledge attribution approach for identifying KEDs needs broader validation across different knowledge types.

**Low Confidence**: The assumption that token-level matching provides reliable subject identification across diverse contexts has not been thoroughly stress-tested. The long-term stability of edited embeddings and their interaction with fine-tuning or continued pretraining remains unexplored.

## Next Checks

1. **Subject Token Dependency Test**: Create a controlled experiment varying subject mention frequency and phrasing in input texts to quantify the impact of token presence requirements on editing success rates across different knowledge types.

2. **Suppression Parameter Sensitivity**: Conduct a systematic ablation study varying the suppression strength parameter across multiple datasets to identify optimal suppression levels and test whether the "optimizing then suppressing" approach consistently outperforms simpler fusion methods.

3. **Cross-Context Generalization**: Test the edited knowledge retrieval when subjects appear in semantically similar but lexically different contexts to evaluate whether the precise token matching creates brittleness in knowledge application across varied input phrasings.