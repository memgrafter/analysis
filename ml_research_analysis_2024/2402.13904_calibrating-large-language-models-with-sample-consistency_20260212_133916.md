---
ver: rpa2
title: Calibrating Large Language Models with Sample Consistency
arxiv_id: '2402.13904'
source_url: https://arxiv.org/abs/2402.13904
tags:
- consistency
- calibration
- confidence
- answer
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates confidence calibration for Large Language
  Models (LLMs) using sample consistency metrics. The authors propose three approaches
  to measure consistency: agreement-based, entropy-based, and first-second-distance-based
  (FSD).'
---

# Calibrating Large Language Models with Sample Consistency

## Quick Facts
- arXiv ID: 2402.13904
- Source URL: https://arxiv.org/abs/2402.13904
- Authors: Qing Lyu; Kumar Shridhar; Chaitanya Malaviya; Li Zhang; Yanai Elazar; Niket Tandon; Marianna Apidianaki; Mrinmaya Sachan; Chris Callison-Burch
- Reference count: 35
- Key outcome: Consistency-based calibration significantly outperforms traditional methods like raw logits and verbalized confidence, with agreement-based consistency working best for open-source models and Codex, while FSD and entropy perform better for GPT models.

## Executive Summary
This paper investigates confidence calibration for Large Language Models (LLMs) using sample consistency metrics. The authors propose three approaches to measure consistency: agreement-based, entropy-based, and first-second-distance-based (FSD). These methods derive confidence scores from the distribution of multiple model generations without requiring additional calibration data or model access. Extensive experiments across nine reasoning datasets and various open/closed-source models show that consistency-based calibration significantly outperforms existing post-hoc methods like raw logits and verbalized confidence.

## Method Summary
The authors explore three consistency metrics (agreement-based, entropy-based, and FSD-based) applied to multiple model generations. They sample 40 outputs per input with temperature 0.4, extract final answers, and compute consistency scores. These scores are then mapped to confidence values and evaluated using Brier Score and Expected Calibration Error (ECE) against nine reasoning datasets covering math word problems, multi-hop QA, planning, and relational reasoning tasks.

## Key Results
- Consistency-based calibration significantly outperforms raw logits and verbalized confidence methods
- Agreement-based consistency works best for open-source models and Codex
- FSD and entropy-based metrics perform better for GPT models
- Explanation-based prompting strategies improve calibration performance
- Model scaling enhances calibration while instruction-tuning has a negative effect

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple generations allow estimation of output distribution consistency, which correlates with correctness likelihood.
- Mechanism: The model samples multiple outputs for the same input, parses the final answers, and computes consistency metrics (agreement, entropy, FSD) from the distribution. Higher consistency suggests the model is more confident in the answer.
- Core assumption: When a model is uncertain, its outputs will be more diverse; when confident, outputs will converge.
- Evidence anchors:
  - [abstract] "We explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency."
  - [section 3.1] "In this work, we investigate the research question: How can we best elicit a model's confidence from the consistency of multiple generations?"

### Mechanism 2
- Claim: Agreement-based consistency can be overly confident when all answers are equally frequent.
- Mechanism: Agreement only considers the most-voted answer, so if all answers appear with equal frequency, it may assign high confidence despite no clear consensus.
- Core assumption: Agreement-based consistency ignores the full distribution of answers.
- Evidence anchors:
  - [abstract] "Figure 1 (right) shows an example of how agreement can potentially lead to overconfidence only based on one most popular answer, when all the answers are equally frequent."
  - [section 3.1] "Agreement-based consistency,Agree(¯a) is defined as: Agree(¯a) = 1/n ∑ 1 (ˆai = ¯a)"

### Mechanism 3
- Claim: Entropy-based and FSD-based metrics capture richer information from the answer distribution than agreement.
- Mechanism: Entropy measures the spread of the answer distribution, while FSD considers the gap between the top two answers. Both avoid overconfidence in cases where agreement fails.
- Core assumption: These metrics provide a more nuanced view of consistency than agreement alone.
- Evidence anchors:
  - [abstract] "However, existing studies have only focused on agreement-based measures of consistency, resulting in potential overconfidence. This necessitates a systematic study on how to best elicit confidence from consistency."
  - [section 3.1] "Entropy-based consistency,Ent(a) as: Ent(a) = 1 − (− 1/log(|¯a|) ∑ pi log(pi))"

## Foundational Learning

- Concept: Probability and entropy
  - Why needed here: Understanding how entropy measures uncertainty in a probability distribution is crucial for interpreting entropy-based consistency.
  - Quick check question: If a model generates three answers with probabilities 0.5, 0.3, and 0.2, what is the entropy of this distribution?

- Concept: Statistical significance testing
  - Why needed here: The paper uses p-values to compare the effectiveness of different consistency metrics. Understanding how to interpret these results is important.
  - Quick check question: If two methods have Brier Scores of 0.15 and 0.18 with p=0.03, what does this mean?

- Concept: Calibration metrics (Brier Score, ECE)
  - Why needed here: These metrics measure how well the model's confidence aligns with its actual accuracy. Understanding them is essential for evaluating calibration performance.
  - Quick check question: If a model predicts with 80% confidence and is correct 60% of the time, is it well-calibrated?

## Architecture Onboarding

- Component map: Query -> Multiple generations -> Answer extraction -> Consistency computation -> Confidence calibration -> Evaluation
- Critical path: Query → Multiple generations → Answer extraction → Consistency computation → Confidence calibration → Evaluation
- Design tradeoffs:
  - Sampling cost vs. calibration accuracy (more samples = better calibration but higher cost)
  - Choice of consistency metric based on model type (agreement for open-source/Codex, FSD/entropy for GPT)
  - Prompting strategy impact (explanations improve calibration)
- Failure signatures:
  - High calibration error despite high consistency (possible overconfidence)
  - Low calibration error but poor accuracy (model is uncertain but often correct)
  - Inconsistent results across different metrics (need to investigate model characteristics)
- First 3 experiments:
  1. Generate 3-5 outputs for a simple query and compute all three consistency metrics manually
  2. Compare agreement-based vs. entropy-based calibration on a small dataset
  3. Test how temperature affects consistency metrics by varying T from 0.1 to 1.0

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of consistency-based calibration methods vary across different reasoning tasks and domains?
- Basis in paper: [explicit] The paper mentions that the effectiveness of consistency metrics can vary across different tasks and domains, with agreement-based consistency working best for open-source models and Codex, while FSD and entropy perform better for GPT models.
- Why unresolved: While the paper provides some insights into the performance of different consistency metrics across tasks, it does not provide a comprehensive analysis of how these methods perform across a wide range of reasoning tasks and domains.
- What evidence would resolve it: Conducting experiments with a larger and more diverse set of reasoning tasks and domains, and analyzing the performance of consistency-based calibration methods across these tasks.

### Open Question 2
- Question: How does the performance of consistency-based calibration methods compare to other calibration techniques, such as ensemble-based or density-based approaches?
- Basis in paper: [explicit] The paper focuses on consistency-based calibration methods and compares them to post-hoc baselines, but does not provide a direct comparison to other calibration techniques like ensemble-based or density-based approaches.
- Why unresolved: The paper does not explore how consistency-based methods compare to other established calibration techniques, which could provide valuable insights into their relative strengths and weaknesses.
- What evidence would resolve it: Conducting experiments comparing consistency-based calibration methods to ensemble-based or density-based approaches on the same datasets and tasks, and analyzing their performance in terms of calibration accuracy and other relevant metrics.

### Open Question 3
- Question: How does the choice of temperature parameter affect the performance of consistency-based calibration methods?
- Basis in paper: [explicit] The paper mentions that the temperature parameter is set to 0.4 in their experiments, but does not explore how different temperature values impact the effectiveness of consistency-based calibration methods.
- Why unresolved: The paper does not provide an analysis of how the temperature parameter influences the performance of consistency-based methods, which could be important for understanding their robustness and generalizability.
- What evidence would resolve it: Conducting experiments with different temperature values and analyzing the impact on the performance of consistency-based calibration methods, both in terms of calibration accuracy and other relevant metrics.

## Limitations

- Generalizability to non-reasoning tasks is unknown, as the study focuses exclusively on reasoning tasks
- Optimal sample size requirements are unclear, with the paper using 40 samples but suggesting 3-5 may be sufficient
- Model-specific behavior patterns are observed but not fully explained, leaving uncertainty about why different metrics work better for different model types

## Confidence

- **High Confidence**: Consistency-based calibration outperforms traditional methods (raw logits, verbalized confidence)
- **Medium Confidence**: Agreement-based consistency works best for open-source models, while FSD/entropy work better for GPT models
- **Low Confidence**: Explanation-based prompting universally improves calibration

## Next Checks

**Validation Check 1**: Test the three consistency metrics on non-reasoning tasks (e.g., text classification or summarization) to assess generalizability. Generate 3-5 samples per input and compare calibration performance across task types.

**Validation Check 2**: Conduct a systematic analysis of sample size requirements by varying the number of generations from 1 to 40 in increments of 5, measuring calibration accuracy and computational cost trade-offs for each consistency metric.

**Validation Check 3**: Implement an ablation study isolating the effects of temperature, model architecture, and training procedure on consistency metric performance to better understand why different metrics work better for different model types.