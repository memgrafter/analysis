---
ver: rpa2
title: 'SKIM: Any-bit Quantization Pushing The Limits of Post-Training Quantization'
arxiv_id: '2412.04180'
source_url: https://arxiv.org/abs/2412.04180
tags:
- quantization
- skim
- scaling
- arxiv
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of quantizing large language
  models (LLMs) for efficient deployment. Standard quantization techniques experience
  significant performance drops at low bit levels.
---

# SKIM: Any-bit Quantization Pushing The Limits of Post-Training Quantization

## Quick Facts
- arXiv ID: 2412.04180
- Source URL: https://arxiv.org/abs/2412.04180
- Authors: Runsheng Bai; Bo Liu; Qiang Liu
- Reference count: 31
- Primary result: Narrowed gap between 3-bit quantized LLaMA models and full precision by 16.3% on average in perplexity

## Executive Summary
This paper addresses the challenge of quantizing large language models (LLMs) for efficient deployment. Standard quantization techniques experience significant performance drops at low bit levels. The authors propose SKIM (Scaled K-means clustering wIth Mixed precision), which combines two novel techniques: 1) a greedy algorithm for approximately optimal bit allocation across weight channels, and 2) a trainable scaling vector for non-differentiable K-means clustering. SKIM can adapt to any given bit level, including non-integer values. The method narrows the gap between 3-bit quantized LLaMA models and their full precision counterparts by 16.3% on average in terms of model perplexity. Experiments on LLaMA and OPT models show consistent improvements over state-of-the-art methods across various bit levels and tasks.

## Method Summary
SKIM is a post-training quantization method that addresses low-bit quantization challenges through two key innovations. First, it employs a greedy algorithm to allocate bits across weight channels based on their quantization error characteristics, assigning more bits to high-error channels and fewer to low-error channels. Second, it introduces a trainable scaling vector that adjusts weight distributions before K-means clustering, enabling better quantization quality for non-differentiable clustering operations. The method operates through an iterative optimization process that alternates between fixed grouping and scaling vector updates. SKIM supports any bit level including non-integer values and demonstrates effectiveness across various model families including LLaMA and OPT architectures.

## Key Results
- Narrowed the gap between 3-bit quantized LLaMA models and full precision counterparts by 16.3% on average in perplexity
- Achieved consistent improvements over state-of-the-art methods (SqueezeLLM, OmniQuant) across multiple bit levels (2-8 bits)
- Demonstrated effectiveness on both WikiText2 and C4 datasets with LLaMA and OPT model families

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Channel-wise mixed precision allocation reduces quantization error more effectively than uniform bit allocation
- Mechanism: Different weight channels exhibit varying data distributions and quantization errors. The greedy algorithm assigns more bits to high-error channels and fewer bits to low-error channels, optimizing overall reconstruction error under a bit budget constraint
- Core assumption: Quantization error reduction per additional bit is non-uniform across channels and can be predicted via pre-computation
- Evidence anchors:
  - [abstract] "A greedy algorithm to solve approximately optimal bit allocation across weight channels"
  - [section] "We observe a significant disparity in data distribution across channels and propose a greedy algorithm for approximately optimal bit allocation"
  - [corpus] Weak - no direct citations about greedy algorithms in quantization, but related works mention channel-wise allocation
- Break condition: If channel quantization errors are uniformly distributed or the greedy approximation becomes too far from optimal

### Mechanism 2
- Claim: Trainable scaling vector regularizes column-wise variations and improves quantization quality for non-differentiable K-means clustering
- Mechanism: A learnable scaling vector multiplies weight columns before quantization, adjusting data spread. Iterative optimization alternates between fixed grouping (K-means) and scaling vector updates using gradient descent on quantization error
- Core assumption: Scaling vector can compensate for channel-specific variations without affecting quantization label assignment
- Evidence anchors:
  - [abstract] "A trainable scaling vector for non-differentiable K-means clustering"
  - [section] "we incorporate a trainable scaling vector based on our novel iterative optimization strategy"
  - [corpus] Weak - related works mention scaling but primarily in uniform quantization contexts
- Break condition: If scaling vector optimization diverges or fails to improve perplexity after initial iterations

### Mechanism 3
- Claim: Unified reformulation of layer-wise and sensitivity-based objectives enables cross-objective optimization within SKIM framework
- Mechanism: Both objectives can be expressed in full or diagonal forms, sharing similar structures. This allows selecting different objectives for different optimization steps (e.g., L-full for scaling, S-diag for clustering) without interference
- Core assumption: Different objective forms maintain synchronization toward final quantization quality despite structural differences
- Evidence anchors:
  - [section] "the above transformation highlights the similarities and core differences between layer-wise and sensitivity-based quantization"
  - [section] "Although the layer-wise and sensitivity-based objectives differ, they can ultimately be transformed into either the full or diag forms"
  - [corpus] Weak - no direct citations about unified objective reformulation in quantization literature
- Break condition: If cross-objective optimization leads to instability or reduced performance

## Foundational Learning

- Concept: K-means clustering with weighted centroids
  - Why needed here: Core quantization mechanism groups weight values into discrete centroids with error minimization
  - Quick check question: How does the weighted K-means objective differ from standard K-means in this context?

- Concept: Second-order Taylor expansion for sensitivity analysis
  - Why needed here: Sensitivity-based quantization uses Hessian/Fisher information to weight quantization errors by their impact on final loss
  - Quick check question: What approximation makes the Fisher information matrix computationally tractable for large models?

- Concept: Greedy approximation algorithms for knapsack problems
  - Why needed here: Bit allocation across channels maps to a constrained optimization problem solvable approximately via greedy selection
  - Quick check question: Why is dynamic programming impractical for this bit allocation problem?

## Architecture Onboarding

- Component map: Weight matrix → Bit allocation (greedy) → Channel-wise K-means clustering → Scaling vector training (iterative) → Dequantized weights
- Critical path: Pre-compute errors → Allocate bits → Cluster weights → Optimize scaling → Output quantized model
- Design tradeoffs: Memory vs accuracy (higher precision centroids improve reconstruction but increase memory), computation vs optimality (greedy vs DP for bit allocation)
- Failure signatures: Perplexity degradation after scaling optimization, inconsistent quantization errors across channels, memory overflow during pre-computation
- First 3 experiments:
  1. Validate bit allocation effectiveness by comparing uniform vs greedy allocation on single layer
  2. Test scaling vector convergence by monitoring perplexity during iterative optimization
  3. Benchmark memory usage vs accuracy tradeoff at different bit levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SKIM's performance scale when applied to extremely large models beyond 30B parameters?
- Basis in paper: [inferred] The paper demonstrates effectiveness on LLaMA models up to 30B parameters and mentions generalizability across model families, but does not test on larger models like GPT-4 or future trillion-parameter models
- Why unresolved: The paper focuses on models up to 30B parameters, leaving the performance characteristics on larger models unexplored
- What evidence would resolve it: Experimental results showing perplexity and MMLU scores on models with 100B+ parameters, along with memory usage comparisons

### Open Question 2
- Question: What is the impact of SKIM on models with different architectural designs, such as those using mixture-of-experts or recurrent neural networks?
- Basis in paper: [explicit] The paper states that SKIM is evaluated across various model families and sizes, but does not specifically test on architectures other than standard transformers
- Why unresolved: The experiments focus exclusively on transformer-based models, leaving the method's effectiveness on alternative architectures unknown
- What evidence would resolve it: Application of SKIM to mixture-of-experts models and recurrent architectures, with comparative perplexity and task performance metrics

### Open Question 3
- Question: How sensitive is SKIM to the choice of calibration dataset size and composition?
- Basis in paper: [explicit] The paper mentions using 100 samples from the C4 dataset for calibration but does not explore how varying the size or distribution of this dataset affects quantization quality
- Why unresolved: Only a single calibration setup is tested, without examining the trade-offs between calibration dataset size and quantization accuracy
- What evidence would resolve it: Systematic experiments varying calibration dataset size from 10 to 10,000 samples and testing with different data distributions, measuring the resulting impact on perplexity and task performance

## Limitations

- The greedy algorithm for bit allocation is approximate and may not achieve globally optimal solutions, particularly for models with highly heterogeneous weight distributions
- The iterative optimization for the scaling vector assumes convergence within a fixed number of iterations, but no theoretical guarantees are provided for convergence rates or stability
- The method's effectiveness is primarily demonstrated on LLaMA and OPT architectures, with limited validation across diverse model families and tasks

## Confidence

**High Confidence:** The core mechanism of combining channel-wise mixed precision with K-means clustering is technically sound and aligns with established quantization principles. The empirical improvements in perplexity metrics are directly measurable and reproducible.

**Medium Confidence:** The theoretical justification for the unified objective reformulation and its practical benefits in cross-objective optimization. While the mathematical transformations are correct, the practical advantages over simpler sequential approaches need further validation.

**Low Confidence:** The scalability claims for very low bit levels (particularly sub-2-bit quantization) and the generalization to models significantly larger than those tested. The paper does not address potential degradation in performance or stability at extreme quantization levels.

## Next Checks

1. **Convergence Analysis:** Systematically evaluate the scaling vector optimization across different model layers and bit levels to establish convergence patterns and identify potential divergence conditions.

2. **Cross-Architecture Generalization:** Apply SKIM to diverse model architectures (CNNs, vision transformers, smaller language models) to validate the method's broader applicability beyond LLaMA and OPT.

3. **Extreme Quantization Testing:** Rigorously test SKIM at sub-2-bit levels to identify the practical limits of the method and determine whether performance degradation follows predictable patterns or exhibits unexpected behaviors.