---
ver: rpa2
title: Function Gradient Approximation with Random Shallow ReLU Networks with Control
  Applications
arxiv_id: '2410.05071'
source_url: https://arxiv.org/abs/2410.05071
tags:
- function
- approximation
- bound
- control
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of function and gradient approximation\
  \ using shallow ReLU neural networks with random input parameters, motivated by\
  \ control applications such as policy evaluation in reinforcement learning. The\
  \ key contribution is a theoretical guarantee showing that when input parameters\
  \ (weights and biases) are randomly generated according to a specified probability\
  \ distribution, and output parameters are trained via least squares, both the function\
  \ approximation error and gradient approximation error decrease at a rate of O(\u221A\
  log(m)/m), where m is the number of neurons."
---

# Function Gradient Approximation with Random Shallow ReLU Networks with Control Applications

## Quick Facts
- **arXiv ID**: 2410.05071
- **Source URL**: https://arxiv.org/abs/2410.05071
- **Reference count**: 16
- **Primary result**: Random shallow ReLU networks with trained output parameters achieve O(√(log(m)/m)) gradient approximation error and O(1/√m) function approximation error

## Executive Summary
This paper addresses function and gradient approximation using shallow ReLU neural networks with random input parameters, motivated by control applications like policy evaluation in reinforcement learning. The key contribution is a theoretical guarantee showing that when input parameters are randomly generated and output parameters are trained via least squares, both function and gradient approximation errors decrease at controlled rates. The work improves upon previous research by providing bounds for simultaneous function and gradient approximation with better constants. A numerical example demonstrates the theoretical bounds, though the constants are large enough that many neurons are needed for practical accuracy.

## Method Summary
The method uses shallow ReLU networks with randomly generated input parameters (weights and biases) drawn from a specified probability distribution, while output parameters are trained via least squares regression. The function approximation is constructed using Fourier analysis and the Radon transform to express the target function as a linear combination of ReLU activations. The gradient approximation requires additional analysis using VC-dimension arguments due to the discontinuous nature of ReLU derivatives. The approach provides simultaneous bounds for both function and gradient approximation errors with high probability.

## Key Results
- Function approximation error satisfies ||fN - f||_∞ ≤ O(1/√m)
- Gradient approximation error satisfies ||∇fN - ∇f||₂,∞ ≤ O(√log(m)/m)
- Both bounds hold with probability at least 1 - δ
- The results improve upon previous work by providing simultaneous function and gradient bounds with better constants
- Numerical example confirms theoretical bounds but shows constants are too large for practical use without many neurons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random shallow ReLU networks with fixed input parameters and trained output parameters can approximate both a smooth function and its gradient with controlled error rates.
- Mechanism: The function is decomposed into an integral representation using Fourier analysis and the Radon transform, allowing expression as a linear combination of ReLU activations weighted by randomly sampled coefficients. Output layer coefficients are trained via least squares, providing a sparse representation that converges as O(√(log(m)/m)) for gradient and O(1/√m) for function.
- Core assumption: Target function satisfies Assumption 1 (smoothness condition with bounded Fourier transform).
- Evidence anchors: [abstract] "both the function approximation error and gradient approximation error decrease at a rate of O(√log(m)/m)", [section] "Assumption 1: There is an integer k ≥ n+3 and a number ρ > 0 such that sup_ω |ˆf(ω)|(1 + ||ω||^k) ≤ ρ"
- Break condition: If function doesn't satisfy smoothness condition (bounded Fourier transform with sufficient decay), error bounds don't hold.

### Mechanism 2
- Claim: Importance sampling with randomly generated input parameters yields function approximation that converges with high probability.
- Mechanism: Integral representation of function is approximated using Monte Carlo sampling over sphere and real line. Random samples provide probabilistic cover of function space, and Hoeffding inequality ensures concentration of approximation error around its mean.
- Core assumption: Input parameter density P has positive lower bound Pmin > 0.
- Evidence anchors: [section] "Let P : S^(n-1) × [-R, R] → R be a probability density function. Assumption 2: The density, P, has a positive lower bound: P_min : = inf_(α,t)∈S^(n-1)×[-R,R] P(α, t) > 0.", [section] "Theorem 1: Let Assumptions 1 and 2 hold... with probability at least 1 - δ there exist coefficients c1,...,cm such that..."
- Break condition: If Pmin approaches zero or sampling becomes biased, concentration inequalities fail and error bounds degrade.

### Mechanism 3
- Claim: Gradient approximation error can be bounded using VC-dimension arguments and Rademacher complexity.
- Mechanism: Discontinuous nature of ReLU gradient requires different analysis approach than function approximation. VC-dimension of function class is bounded by n+1, which controls Rademacher complexity and provides uniform convergence bound for gradient approximation.
- Core assumption: Function class has finite VC-dimension, which holds for ReLU networks with fixed input parameters.
- Evidence anchors: [section] "The class of functions, L, is said to shatter a collection of points... Since L is a vector space of dimension n+1, Proposition 4.20 of [16] shows that VC(L) ≤ n+1.", [section] "Lemma 3: Assume that P_min > 0 and let f satisfy Assumption 1... the following bounds hold with probability at least 1 - δ"
- Break condition: If function class becomes too complex (VC-dimension grows), uniform convergence bounds fail and gradient error increases.

## Foundational Learning

- Concept: Fourier analysis and integral representations of functions
  - Why needed here: Proof relies on representing smooth functions as integrals over sphere using their Fourier transforms, enabling connection to ReLU networks
  - Quick check question: What condition on the Fourier transform of a function ensures it can be represented as an integral over the sphere with ReLU activations?

- Concept: Concentration inequalities and uniform convergence
  - Why needed here: Error bounds are probabilistic and require showing approximation error concentrates around its mean uniformly over domain
  - Quick check question: How does the VC-dimension of a function class relate to the Rademacher complexity and uniform convergence bounds?

- Concept: Importance sampling and Monte Carlo methods
  - Why needed here: Function approximation constructed using random samples from probability distribution, requiring understanding of how Monte Carlo integration error scales with sample size
  - Quick check question: What is the relationship between variance of importance weights and convergence rate of Monte Carlo integration?

## Architecture Onboarding

- Component map: Input parameters (random weights/biases) -> Fixed ReLU activations -> Trainable output coefficients -> Least squares optimization
- Critical path: 1. Sample input parameters (αi, ti) from P 2. Construct integral representation of target function 3. Compute importance weights and form approximation 4. Train output coefficients via least squares 5. Evaluate function and gradient approximation errors
- Design tradeoffs: Number of neurons m vs. approximation accuracy, Choice of sampling distribution P vs. error constants, Smoothness assumptions vs. generality of function class, Computational cost of training vs. closed-form solution availability
- Failure signatures: Large gradient approximation error despite small function error, Error bounds not improving with increased m, Sensitivity to choice of sampling distribution P, Numerical instability in computing output coefficients
- First 3 experiments: 1. Test approximation of simple polynomial function with known Fourier transform to verify integral representation approach 2. Vary number of neurons m and measure both function and gradient approximation errors to confirm O(1/√m) and O(√(log(m)/m)) rates 3. Compare different sampling distributions P (uniform vs. non-uniform) to understand impact on error constants and convergence behavior

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions but identifies key limitations and areas for future work, particularly around improving the large constant factors in the approximation error bounds.

## Limitations
- Theoretical bounds have constants that are too large for practical use in many control applications
- Smoothness assumptions are restrictive and may not hold for functions encountered in practice
- Gradient approximation bounds are looser than function approximation due to ReLU gradient discontinuity
- Numerical example demonstrates need for prohibitively large number of neurons for reasonable accuracy

## Confidence
- Function approximation bounds (||fN - f||_∞ ≤ O(1/√m)): Medium-High confidence
- Gradient approximation bounds (||∇fN - ∇f||₂,∞ ≤ O(√log(m)/m)): Medium confidence
- Practical applicability for control systems: Low-Medium confidence

## Next Checks
1. Empirical validation on benchmark control problems: Test the approximation framework on standard control tasks (e.g., inverted pendulum, robotic arm control) to measure actual performance versus theoretical bounds and identify practical limitations.

2. Sensitivity analysis of smoothness parameters: Systematically vary the smoothness coefficient k and measure how approximation quality degrades as functions approach the boundary of Assumption 1 to better understand the practical implications of the theoretical constraints.

3. Comparison with alternative approximation methods: Benchmark the random shallow ReLU approach against other function approximation techniques (e.g., kernel methods, polynomial approximation) in terms of both accuracy and computational efficiency for gradient-based control algorithms.