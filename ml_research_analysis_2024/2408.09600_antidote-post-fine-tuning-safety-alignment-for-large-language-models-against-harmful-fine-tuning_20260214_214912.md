---
ver: rpa2
title: 'Antidote: Post-fine-tuning Safety Alignment for Large Language Models against
  Harmful Fine-tuning'
arxiv_id: '2408.09600'
source_url: https://arxiv.org/abs/2408.09600
tags:
- harmful
- fine-tuning
- stage
- alignment
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of harmful fine-tuning attacks
  on safety-aligned LLMs, where malicious data mixed into fine-tuning datasets can
  break the model's safety alignment. Existing defenses fail when high learning rates
  or many training epochs are used during fine-tuning, which are often necessary for
  good downstream task performance.
---

# Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning

## Quick Facts
- arXiv ID: 2408.09600
- Source URL: https://arxiv.org/abs/2408.09600
- Authors: Tiansheng Huang; Gautam Bhattacharya; Pratik Joshi; Josh Kimball; Ling Liu
- Reference count: 9
- Primary result: Reduces harmful score by 11.56% with 1.45% accuracy loss

## Executive Summary
The paper addresses harmful fine-tuning attacks on safety-aligned LLMs, where malicious data mixed into fine-tuning datasets can break the model's safety alignment. Existing defenses fail when high learning rates or many training epochs are used during fine-tuning, which are often necessary for good downstream task performance. The authors propose Antidote, a post-fine-tuning solution that remains agnostic to fine-tuning hyperparameters by identifying and removing harmful parameters responsible for generating harmful content after fine-tuning completes.

## Method Summary
Antidote uses a three-stage pipeline: (1) safety alignment using SFT on alignment dataset, (2) harmful fine-tuning on mixed dataset (p% harmful, (1-p)% task data), and (3) one-shot pruning using Wanda score on re-alignment dataset to remove harmful parameters. The Wanda score measures parameter importance as |weight| × ||activation||², identifying parameters most important for harmful content generation. The method is evaluated across multiple models (Llama2-7B, Mistral-7B, Gemma-7B) and tasks (SST2, AGNEWS, GSM8K, AlpacaEval), showing significant harmful score reduction while maintaining downstream task accuracy.

## Key Results
- Reduces harmful score by 11.56% compared to unprotected SFT with only 1.45% accuracy loss
- Robust to different learning rates and training epochs, unlike existing defenses
- Generalizes well to different models and tasks with minimal computational overhead
- Outperforms baseline defenses (Vaccine, RepNoise, Lisa, LDIFS) across all evaluated scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-fine-tuning pruning removes harmful parameters regardless of fine-tuning hyperparameters
- Mechanism: The Wanda score identifies parameters most important for harmful content generation on a realignment dataset, and pruning these parameters eliminates harmful behavior while preserving task performance
- Core assumption: Harmful parameters can be reliably identified by their importance on harmful data, and removing them doesn't break task capabilities
- Evidence anchors:
  - [abstract] "The core idea is to identify and remove harmful parameters responsible for generating harmful content after fine-tuning completes"
  - [section] "Antidote relies on the philosophy that by removing the harmful parameters, the harmful model can be recovered from the harmful behaviors, regardless of how those harmful parameters are formed in the fine-tuning stage"
- Break condition: If harmful parameters are not uniquely identifiable or if they're essential for both harmful and task-related generation

### Mechanism 2
- Claim: Wanda score effectively measures parameter importance for harmful content generation
- Mechanism: The Wanda score calculates importance as |weight| × ||activation||², identifying parameters that have large weights and respond strongly to harmful inputs
- Core assumption: Parameters with high Wanda scores on harmful data are indeed responsible for generating harmful content
- Evidence anchors:
  - [section] "The Wanda score (Sun et al. 2023) measures the importance score of parameters given a dataset D, as follows: [h(w, D)]j = 1/|D| Σ X∈D |wj| · ||Xj||²"
- Break condition: If the Wanda score metric doesn't correlate with actual harmful generation capability

### Mechanism 3
- Claim: Post-fine-tuning timing makes Antidote hyperparameter-agnostic
- Mechanism: By applying pruning after fine-tuning completes, Antidote avoids the need to modify training procedures that are sensitive to learning rates and epochs
- Core assumption: The harmful parameters formed during fine-tuning can be identified and removed regardless of how they were formed
- Evidence anchors:
  - [abstract] "The core idea is to identify and remove harmful parameters responsible for generating harmful content after fine-tuning completes"
  - [section] "Antidote, a post-fine-tuning stage solution, which remains agnostic to the training hyper-parameters in the fine-tuning stage"
- Break condition: If the harmful parameter formation process depends critically on fine-tuning hyperparameters in ways that affect identification

## Foundational Learning

- Concept: Parameter importance scoring in neural networks
  - Why needed here: Antidote relies on Wanda score to identify which parameters are most important for harmful content generation
  - Quick check question: What are the two components of the Wanda score formula and what do they represent?

- Concept: Model pruning and sparsification
  - Why needed here: The core operation in Antidote is pruning identified harmful parameters from the model
  - Quick check question: How does one-shot pruning differ from iterative pruning approaches in terms of computational efficiency?

- Concept: Fine-tuning attack vectors and safety alignment
  - Why needed here: Understanding how harmful fine-tuning works is essential to appreciate why Antidote's approach is effective
  - Quick check question: What is the difference between alignment-stage and fine-tuning-stage defenses against harmful attacks?

## Architecture Onboarding

- Component map: Safety alignment (SFT) -> Harmful fine-tuning (SFT) -> Wanda score calculation -> Parameter pruning -> Deployed model
- Critical path: The pruning operation in Stage III is the critical path - all computation builds toward identifying and removing harmful parameters
- Design tradeoffs:
  - Mask ratio α: Higher values remove more harmful parameters but risk task performance loss
  - Realignment dataset size: Larger datasets improve Wanda score accuracy but increase computation
  - Parameter selection granularity: Layer-wise vs. global pruning affects both effectiveness and computational overhead
- Failure signatures:
  - High harmful score despite pruning: Wanda score may not be identifying correct parameters
  - Significant task accuracy loss: Mask ratio may be too high or pruning may be removing task-critical parameters
  - No improvement over baseline: Realignment dataset may be too small or not representative
- First 3 experiments:
  1. Baseline test: Run SFT without defense to establish harmful score and task accuracy
  2. Wanda score validation: Verify that parameters with high Wanda scores on harmful data actually correlate with harmful output
  3. Mask ratio sweep: Test different α values to find the optimal tradeoff between harmful score reduction and task accuracy preservation

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but the analysis reveals several critical uncertainties that require further investigation. The Wanda score methodology appears central to the approach yet has minimal direct evidence in the corpus about its effectiveness for harmful parameter identification specifically. The post-fine-tuning timing claim has Medium confidence - while theoretically sound, there's no empirical evidence showing why pre-fine-tuning defenses fail with different hyperparameters while post-fine-tuning approaches succeed. The results show promising average improvements, but the analysis doesn't reveal the variance across different tasks or models.

## Limitations

- The Wanda score methodology relies on a single citation without extensive validation for safety alignment contexts
- Limited evaluation to 7B parameter models, leaving scalability to larger models unknown
- No theoretical justification for why harmful data is necessary for effective parameter identification
- Results show average improvements but don't reveal variance across different tasks or models

## Confidence

- Mechanism 1 (Post-fine-tuning pruning): Medium
- Mechanism 2 (Wanda score effectiveness): Low
- Mechanism 3 (Hyperparameter-agnostic timing): Medium

## Next Checks

1. **Wanda score validation**: Run controlled experiments comparing Wanda-identified parameters against random parameter removal to verify that the scoring method actually identifies harmful parameters rather than just high-magnitude parameters

2. **Hyperparameter sensitivity analysis**: Systematically test Antidote across extreme learning rates and epoch counts to verify the claimed agnosticism, including scenarios where the baseline methods completely fail

3. **Parameter dependency analysis**: Analyze whether pruned parameters show any correlation with task-critical parameters across different downstream tasks to assess the risk of breaking capabilities while removing harm