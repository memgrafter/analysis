---
ver: rpa2
title: Phrase-Level Adversarial Training for Mitigating Bias in Neural Network-based
  Automatic Essay Scoring
arxiv_id: '2409.04795'
source_url: https://arxiv.org/abs/2409.04795
tags:
- attack
- adversarial
- data
- essay
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses bias and robustness issues in Automatic Essay
  Scoring (AES) systems, which are vulnerable to adversarial attacks and biased towards
  overrepresented data samples. The authors propose a phrase-level adversarial training
  approach that generates adversarial essay sets and augments the training data to
  improve model robustness.
---

# Phrase-Level Adversarial Training for Mitigating Bias in Neural Network-based Automatic Essay Scoring

## Quick Facts
- **arXiv ID:** 2409.04795
- **Source URL:** https://arxiv.org/abs/2409.04795
- **Reference count:** 18
- **Primary result:** Phrase-level adversarial training improves AES model robustness and accuracy, with QWK improvements of 0.083 to 0.194

## Executive Summary
This paper addresses bias and robustness issues in Automatic Essay Scoring (AES) systems by proposing a phrase-level adversarial training approach. AES models are vulnerable to adversarial attacks and often biased towards overrepresented data samples. The authors develop a method that generates adversarial essay sets and augments training data to improve model robustness. Their approach uses sentence extraction, phrase extraction, blank-infilling perturbation, and label-preserving filtering to create adversarial examples. Experiments on four AES datasets demonstrate that models trained with augmented data significantly outperform those trained only on original data.

## Method Summary
The authors propose a phrase-level adversarial training approach for AES systems that consists of four main steps: sentence extraction to identify relevant text segments, phrase extraction to isolate key phrases, blank-infilling perturbation to create adversarial examples by masking and replacing phrases, and label-preserving filtering to ensure generated examples maintain the same quality score. The adversarial examples are then used to augment the training dataset. The method is evaluated using four AES datasets, comparing models trained with original data alone versus those trained with original plus adversarial examples.

## Key Results
- Models trained with augmented data showed QWK score improvements ranging from 0.083 to 0.194 across four datasets
- AES models demonstrated significant sensitivity to adversarial attacks, with accuracy dropping by up to 0.267 when tested on adversarially generated examples
- Phrase-level adversarial training effectively improves model robustness against both bias and adversarial attacks

## Why This Works (Mechanism)
The approach works by exposing the model to perturbed versions of training data during the learning process, which forces the model to learn more robust feature representations that are less sensitive to specific phrase-level variations. By systematically masking and replacing key phrases while preserving labels, the model learns to focus on more general essay quality indicators rather than memorizing specific phrasings. This augmentation strategy helps mitigate both bias toward overrepresented samples and vulnerability to adversarial manipulation.

## Foundational Learning
- **Quadratic Weighted Kappa (QWK)**: Why needed - Standard evaluation metric for AES that accounts for partial credit between adjacent score categories; Quick check - Verify that QWK calculations properly handle score distributions and class imbalances
- **Adversarial training**: Why needed - Technique to improve model robustness by training on perturbed examples; Quick check - Confirm that perturbations are meaningful but don't change core meaning or labels
- **Sentence and phrase extraction**: Why needed - Identifies relevant text units for targeted perturbation; Quick check - Ensure extraction captures meaningful semantic units rather than arbitrary text segments
- **Label-preserving filtering**: Why needed - Maintains dataset integrity by only including valid adversarial examples; Quick check - Verify that perturbed examples truly maintain their original quality scores
- **Blank-infilling perturbation**: Why needed - Creates controlled variations while maintaining grammatical structure; Quick check - Assess whether generated text remains coherent and relevant to the original context
- **Automatic Essay Scoring metrics**: Why needed - Multiple evaluation criteria needed beyond accuracy to assess fairness and robustness; Quick check - Confirm comprehensive evaluation across different performance dimensions

## Architecture Onboarding

**Component map:** Data preprocessing -> Sentence extraction -> Phrase extraction -> Blank-infilling perturbation -> Label-preserving filtering -> Training data augmentation -> Model training -> Evaluation

**Critical path:** The most critical components are the blank-infilling perturbation and label-preserving filtering steps, as these directly determine the quality and validity of the adversarial examples used for training. Poor perturbation strategies or incorrect label preservation will compromise the entire augmentation process.

**Design tradeoffs:** The approach balances between creating meaningful adversarial examples and maintaining label integrity. More aggressive perturbations might create better adversarial examples but risk label corruption, while conservative perturbations might not sufficiently challenge the model.

**Failure signatures:** Key failure modes include: adversarial examples that change the fundamental meaning of essays, perturbations that introduce artifacts the model can exploit, label corruption during the augmentation process, and overfitting to the specific perturbation patterns rather than learning robust features.

**First experiments:**
1. Test the impact of different perturbation intensities on both adversarial example quality and label preservation
2. Compare model performance when trained with varying ratios of original to adversarial examples
3. Evaluate whether the approach generalizes across different AES model architectures (e.g., transformer vs. LSTM-based models)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas warrant further investigation including the long-term effects on model calibration, fairness across demographic groups, and generalizability to other NLP tasks beyond AES.

## Limitations
- Generalizability beyond the four AES datasets tested remains uncertain
- Potential for adversarial examples to introduce noise or artifacts affecting real-world performance
- Limited analysis of generated adversarial example quality and diversity
- No comprehensive analysis of fairness across different demographic subgroups

## Confidence
- **High confidence**: Effectiveness of phrase-level adversarial training in improving AES model robustness and accuracy (evidenced by significant QWK improvements across multiple datasets)
- **High confidence**: AES models' sensitivity to adversarial attacks (demonstrated by accuracy drops of up to 0.267)
- **Medium confidence**: Claim that the method mitigates bias (lacks comprehensive fairness analysis across subgroups)

## Next Checks
1. Conduct thorough analysis of generated adversarial examples to assess quality, diversity, and potential impact on real-world model performance
2. Evaluate long-term effects of phrase-level adversarial training on model calibration, fairness across demographic groups, and performance on unseen data
3. Extend the study to additional AES datasets and other NLP tasks to assess generalizability and broader applicability of the approach