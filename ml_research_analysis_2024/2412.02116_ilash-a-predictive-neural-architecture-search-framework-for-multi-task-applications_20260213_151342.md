---
ver: rpa2
title: 'ILASH: A Predictive Neural Architecture Search Framework for Multi-Task Applications'
arxiv_id: '2412.02116'
source_url: https://arxiv.org/abs/2412.02116
tags:
- ilash
- neural
- search
- architecture
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ILASH, a neural architecture search (NAS)
  framework designed for multi-task learning on edge devices. The core idea is to
  create layer-shared architectures that minimize redundant computations by reusing
  layers across tasks, thereby reducing energy consumption, inference time, and model
  size.
---

# ILASH: A Predictive Neural Architecture Search Framework for Multi-Task Applications

## Quick Facts
- arXiv ID: 2412.02116
- Source URL: https://arxiv.org/abs/2412.02116
- Authors: Md Hafizur Rahman; Md Mashfiq Rizvee; Sumaiya Shomaji; Prabuddha Chakraborty
- Reference count: 40
- Primary result: Up to 16x reduction in energy utilization and CO2 emissions during NAS compared to AutoKeras

## Executive Summary
ILASH is a neural architecture search framework designed for multi-task learning on edge devices. It introduces layer-sharing architectures that minimize redundant computations by reusing layers across tasks, significantly reducing energy consumption, inference time, and model size. The framework employs two search strategies: ILASH-Heu (heuristic-based) and ILASH-Pred (predictive ML-based), and was evaluated on four datasets across various edge devices, demonstrating substantial efficiency improvements.

## Method Summary
ILASH uses a hybrid layer-sharing approach combining "No Sharing" and "Hard Layer Sharing" architectures. The framework employs two NAS strategies: ILASH-Heu uses heuristic branching with goodness evaluation to search for optimal architectures, while ILASH-Pred uses an ML model (Auto-ILASH) to predict goodness metrics and accelerate the search process. The goodness metric (GN) balances accuracy and efficiency by incorporating layer position information. MobileNet serves as the base model for classification/regression tasks, with custom encoder-decoder for 2D semantic tasks. The Auto-ILASH model is trained on data from ILASH-Heu runs to predict branching goodness, enabling faster search in ILASH-Pred.

## Key Results
- Up to 16x reduction in energy utilization and CO2 emissions during NAS compared to AutoKeras
- Up to 3x reduction in inference energy and CO2 emissions
- ILASH-Pred significantly speeds up the search process while maintaining model accuracy
- Demonstrated effectiveness across four datasets (UTKFace, MTFL, CelebA, Taskonomy) on multiple edge devices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer sharing reduces redundant computations by reusing intermediate outputs across tasks.
- Mechanism: Instead of training separate models for each task, ILASH reuses feature maps from earlier layers, sharing computations where possible and branching only when tasks diverge.
- Core assumption: Shared intermediate layers produce sufficiently rich representations for multiple tasks without significant accuracy loss.
- Evidence anchors:
  - [abstract] "minimize redundant computations by reusing layers across tasks"
  - [section III-A] "ILASH architecture is a hybrid between 'No Sharing' and 'Hard Layer Sharing'"
  - [corpus] Weak - no direct citations about layer sharing performance, only general NAS context.
- Break condition: If branching leads to degraded accuracy or if layer reuse introduces significant domain shift between tasks.

### Mechanism 2
- Claim: Predictive ML model accelerates NAS by avoiding full training of each candidate architecture.
- Mechanism: Auto-ILASH predicts the goodness metric (GN) for potential branching points, allowing the search to skip expensive training runs and focus on promising architectures.
- Core assumption: Goodness metric predictions correlate strongly with actual model performance and can be learned from prior search data.
- Evidence anchors:
  - [abstract] "ILASH-Pred, a predictive approach using machine learning to accelerate the search process"
  - [section III-C] "predicts the goodness (GN) metric for a given branching choice during the ILASH model search"
  - [corpus] Weak - no direct citations about predictive NAS performance, only general ML context.
- Break condition: If prediction accuracy degrades significantly across datasets or if the learned model overfits to specific task distributions.

### Mechanism 3
- Claim: Goodness metric balances accuracy and efficiency by incorporating layer position information.
- Mechanism: GN combines validation accuracy with a term that rewards earlier branching, creating a multi-objective optimization that favors efficient architectures.
- Core assumption: The relative position of branching layers directly correlates with computational efficiency and energy consumption.
- Evidence anchors:
  - [section III-B] "We propose a metric called Goodness (GN). This metric is calculated using the following equation"
  - [abstract] "The proposed AI-Guided NAS (ILASH-NAS) makes building these ILASH models highly energy efficient"
  - [corpus] Weak - no direct citations about GN metric design or validation.
- Break condition: If the weighting between accuracy and efficiency leads to suboptimal models for specific task combinations.

## Foundational Learning

- Concept: Neural Architecture Search fundamentals
  - Why needed here: Understanding how ILASH-NAS differs from traditional NAS approaches is crucial for proper implementation and extension.
  - Quick check question: What are the key differences between cell-based, transformation-based, and heuristic-based NAS approaches?

- Concept: Multi-task learning architecture design
  - Why needed here: The layer-sharing concept requires understanding how tasks can share representations without interfering with each other.
  - Quick check question: How does hard layer sharing differ from soft parameter sharing in multi-task learning?

- Concept: Energy consumption modeling for edge devices
  - Why needed here: The framework optimizes for both accuracy and energy efficiency, requiring understanding of how different architectural choices affect power consumption.
  - Quick check question: What factors contribute most significantly to inference-time energy consumption on edge devices?

## Architecture Onboarding

- Component map:
  - Dataset preprocessing and task encoding -> Base model selection (MobileNet/Custom encoder-decoder) -> ILASH-Heu search algorithm -> Auto-ILASH model training -> ILASH-Pred search algorithm -> Edge device evaluation pipeline

- Critical path:
  1. Load dataset and initialize base model
  2. Run ILASH-Heu to generate initial dataset for Auto-ILASH
  3. Train Auto-ILASH decision tree model
  4. Execute ILASH-Pred search using Auto-ILASH predictions
  5. Evaluate final model on target edge devices

- Design tradeoffs:
  - Branch position vs. accuracy: Earlier branching reduces computation but may limit task-specific optimization
  - Search time vs. model quality: Heuristics are thorough but slow; predictions are fast but may miss optimal solutions
  - Model complexity vs. energy efficiency: More shared layers reduce energy but may compromise task performance

- Failure signatures:
  - High variance in Auto-ILASH predictions across similar branching points
  - Degradation in accuracy when increasing layer sharing
  - Energy consumption measurements that don't correlate with architectural complexity

- First 3 experiments:
  1. Implement ILASH-Heu on UTKFace with MobileNet base, measure energy vs. accuracy tradeoffs
  2. Train Auto-ILASH on ILASH-Heu output from UTKFace, evaluate prediction accuracy
  3. Run ILASH-Pred on MTFL using previously trained Auto-ILASH, compare search time and energy consumption against ILASH-Heu

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ILASH-NAS scale with increasing numbers of tasks and layers, and what are the computational limits of the layer-sharing approach?
- Basis in paper: [inferred] The paper demonstrates ILASH's effectiveness on 3-4 tasks but does not explore scalability to larger multi-task scenarios or deeper networks.
- Why unresolved: The evaluation focuses on datasets with 3-5 tasks and relatively shallow architectures, leaving the framework's behavior in more complex scenarios untested.
- What evidence would resolve it: Experiments testing ILASH-NAS on datasets with 10+ tasks and deeper architectures, along with analysis of search time and memory scaling.

### Open Question 2
- Question: How does the goodness metric (GN) balance accuracy and efficiency across different types of tasks (e.g., classification vs. regression vs. 2D semantic tasks), and is it optimal for all scenarios?
- Basis in paper: [explicit] The GN metric is introduced to balance accuracy and efficiency, but the paper does not thoroughly evaluate its effectiveness across diverse task types or compare it to alternative metrics.
- Why unresolved: The paper only briefly mentions the GN metric's role in the search process without providing detailed analysis of its impact on different task categories or exploring alternative formulations.
- What evidence would resolve it: Comparative studies of ILASH-NAS using different goodness metrics across diverse task types, along with ablation studies on the Gth parameter's influence.

### Open Question 3
- Question: What is the impact of task ordering on the final ILASH model performance, and can the search process be made robust to task sequence variations?
- Basis in paper: [explicit] The paper acknowledges that task ordering affects performance and reports results for different task orders, but does not investigate why this occurs or propose solutions to mitigate ordering effects.
- Why unresolved: The paper demonstrates that task ordering influences results but lacks analysis of the underlying mechanisms or methods to make the search process ordering-independent.
- What evidence would resolve it: Analysis of how task ordering affects branching decisions and final model architecture, along with modifications to the search algorithm to reduce ordering sensitivity.

## Limitations
- Architectural Detail Gaps: The branching point encoding mechanism in Algorithm 2 is not explicitly specified, and goodness metric threshold values (Gth) are undefined.
- Evidence Strength: Foundational mechanisms rely on weak empirical support with no direct citations validating layer sharing performance or predictive NAS effectiveness.
- Dataset Generalization: Transferability of Auto-ILASH predictions across different task types and domain distributions remains uncertain.

## Confidence
**High Confidence**: The basic premise that layer sharing can reduce redundant computations in multi-task learning is well-established. The general framework structure is clearly described.

**Medium Confidence**: The energy efficiency claims are supported by measurements but lack comparison against state-of-the-art multi-task NAS approaches.

**Low Confidence**: The goodness metric design and its effectiveness in balancing accuracy vs. efficiency is not empirically validated.

## Next Checks
1. **Auto-ILASH Prediction Quality**: Measure the MAE/MSE/RMSE of the Auto-ILASH decision tree on a held-out validation set of branching point encodings to verify prediction accuracy before trusting ILASH-Pred results.

2. **Layer Sharing Impact Analysis**: Systematically vary the number of shared layers in ILASH-Heu on UTKFace and measure the degradation in task-specific accuracy to establish the fundamental tradeoff curve.

3. **Cross-Dataset Transferability**: Train Auto-ILASH on UTKFace and evaluate its predictions on MTFL to quantify performance degradation and establish dataset generalization limits.