---
ver: rpa2
title: Weighted-Reward Preference Optimization for Implicit Model Fusion
arxiv_id: '2412.03187'
source_url: https://arxiv.org/abs/2412.03187
tags:
- llms
- source
- preference
- wrpo
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces WRPO (Weighted-Reward Preference Optimization),
  a novel implicit model fusion method that transfers capabilities from multiple heterogeneous
  source LLMs to a target LLM through preference optimization. Unlike explicit fusion
  methods that require complex vocabulary alignment and distribution merging, WRPO
  addresses distributional deviations between source and target models using a progressive
  adaptation strategy with weighted internal rewards.
---

# Weighted-Reward Preference Optimization for Implicit Model Fusion

## Quick Facts
- arXiv ID: 2412.03187
- Source URL: https://arxiv.org/abs/2412.03187
- Reference count: 22
- The paper introduces WRPO (Weighted-Reward Preference Optimization), a novel implicit model fusion method that transfers capabilities from multiple heterogeneous source LLMs to a target LLM through preference optimization.

## Executive Summary
This paper presents WRPO, a novel implicit model fusion method that transfers capabilities from multiple heterogeneous source LLMs to a target LLM through preference optimization. Unlike explicit fusion methods that require complex vocabulary alignment and distribution merging, WRPO addresses distributional deviations between source and target models using a progressive adaptation strategy with weighted internal rewards. Experiments show that WRPO significantly outperforms existing knowledge fusion methods and fine-tuning baselines. When applied to LLaMA3-8B-Instruct, it achieves a length-controlled win rate of 55.9% against GPT-4-Preview-1106 on AlpacaEval-2 and 46.2% against GPT-4-0314 on Arena-Hard.

## Method Summary
WRPO operates by sampling N responses from each source model, scoring them with a reward model, and selecting the top source response (yws). The method then samples responses from the target model (ywt and yl) to form quadruples (x, yws, ywt, yl). Training begins with supervised fine-tuning (SFT) on yws for one-third of the dataset, followed by preference optimization on the remaining data using WRPO loss with a dynamic α schedule. The progressive adaptation strategy gradually shifts from on-policy sampling (α=0) to hybrid-policy sampling (α up to 0.1), while the weighted-reward mechanism dynamically balances contributions from both target and source models in the preference pair.

## Key Results
- WRPO achieves 55.9% length-controlled win rate against GPT-4-Preview-1106 on AlpacaEval-2
- WRPO achieves 46.2% win rate against GPT-4-0314 on Arena-Hard
- WRPO outperforms knowledge fusion baselines including EFB, ModelScope, and SFT on multiple benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The progressive adaptation strategy mitigates distributional deviation between source and target LLM outputs by starting with target-only preference pairs and gradually shifting toward hybrid-policy preference pairs.
- Mechanism: During training, WRPO begins with α = 0 (on-policy only, ywt and yl from target) and linearly increases α to a small positive value (e.g., 0.1). This smooths the transition from target-model distribution to the heterogeneous source-model distribution, reducing the risk of divergence or poor reward estimation.
- Core assumption: Gradual shift in α reduces distribution shift impact; the internal reward margin remains stable throughout the transition.
- Evidence anchors:
  - [abstract] "To address distributional deviations between the source and target LLMs, WRPO introduces a progressive adaptation strategy that gradually shifts reliance on preferred examples from the target LLM to the source LLMs."
  - [section 3.3] "This smoothing process facilitates the integration of strengths from the source models into the target model while mitigating the distributional discrepancies."
- Break condition: If α is increased too quickly, the distributional gap between source and target outputs may be too large, causing instability or collapse in reward estimation.

### Mechanism 2
- Claim: The weighted-reward mechanism dynamically balances the contributions of preferred responses from both target and source models, improving the stability of preference optimization.
- Mechanism: WRPO replaces the single preferred response yw with a weighted combination yw = α·yws + (1-α)·ywt, where yws is the best source response and ywt is the best target response. This weighted reward in the Bradley-Terry model ensures both responses influence the gradient update proportionally.
- Core assumption: Both yws and ywt contain useful preference signals; the weighted combination better reflects the true preference distribution than using either alone.
- Evidence anchors:
  - [section 3.3] "This approach enables the target LLM to transition smoothly from its distribution to align with that of the source LLMs."
  - [section 4.4] "Figure 3(c) showcases our proposed weighted-reward mechanism, which synthesizes both on-policy and hybrid-policy reward margins through dynamic weighting."
- Break condition: If the weight α is set too high too early, the training may be dominated by source responses that have high distributional shift, leading to poor optimization.

### Mechanism 3
- Claim: The hybrid-policy sampling during training exposes the target model to a richer set of preference signals than on-policy sampling alone, improving final capability.
- Mechanism: By incorporating yws from source models into the preference pair, the target model is trained not only to improve its own outputs (ywt vs yl) but also to match or exceed the quality of strong source outputs (yws vs yl). This broadens the optimization objective beyond self-improvement.
- Core assumption: Source models provide responses that are measurably better than target responses in some prompts, so their inclusion as yws adds value.
- Evidence anchors:
  - [abstract] "WRPO eliminates the need for vocabulary alignment and matrix fusion and can be efficiently scaled to accommodate various LLMs."
  - [section 4.4] "Notably, the exclusion of source model responses leads to performance declines... highlighting the important role of yws in providing valuable preference signals."
- Break condition: If source models do not provide better responses than the target in any meaningful proportion, their inclusion may add noise rather than signal.

## Foundational Learning

- Concept: Distribution shift in preference optimization
  - Why needed here: WRPO operates in a hybrid-policy setting where the preferred response yws comes from a different distribution (source model) than the dispreferred yl (target model). Without understanding this shift, the training objective can fail or be unstable.
  - Quick check question: What happens to the Bradley-Terry probability model when the two responses come from very different distributions?

- Concept: Bradley-Terry preference model
  - Why needed here: WRPO is built directly on the Bradley-Terry model with a weighted reward, so a solid grasp of its formulation is required to understand how the optimization objective is derived.
  - Quick check question: How does the Bradley-Terry model convert reward differences into preference probabilities?

- Concept: Reward estimation and internal reward margins
  - Why needed here: WRPO relies on internal reward functions to compare responses; understanding how these margins drive gradient updates is essential for debugging and tuning.
  - Quick check question: In the gradient formula for WRPO, how does the σ term influence the direction and magnitude of updates?

## Architecture Onboarding

- Component map: Sample N responses from each source model → score with reward model → pick top source (yws) → sample top/bottom from target (ywt, yl) → form quadruples (x, yws, ywt, yl) → apply WRPO loss with dynamic α schedule
- Critical path:
  1. Offline data collection (source sampling + reward scoring)
  2. SFT stage on yws (⅓ of data)
  3. Preference optimization stage on quadruples with WRPO
- Design tradeoffs:
  - α schedule: too slow → slow convergence; too fast → instability
  - Number of source models: more sources → richer signal but more offline compute
  - Choice of reward model: ArmoRM-LLaMA3-8B used here; a stronger model could improve yws selection but adds cost
- Failure signatures:
  - Target-SFT collapses on some benchmarks → catastrophic forgetting
  - Training loss diverges → α schedule or reward model mismatch
  - Final model underperforms on MMLU/GSM8K → domain shift in training data
- First 3 experiments:
  1. Verify that the offline data pipeline correctly selects yws and ywt/yl for a small subset of prompts.
  2. Train a Target-SFT baseline and check that it improves on the held-out SFT portion but does not overfit.
  3. Run WRPO with α=0 (pure on-policy) vs α=0.1 to confirm the impact of the weighted reward on a validation set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does WRPO's performance scale with the number of source LLMs beyond the tested range?
- Basis in paper: [explicit] The paper demonstrates performance improvements as the number of source LLMs increases from 1 to 10, but does not explore beyond this range.
- Why unresolved: The experiments only tested up to 10 source LLMs, leaving the scalability to larger numbers uncertain.
- What evidence would resolve it: Experiments with WRPO using more than 10 source LLMs, comparing performance gains and computational costs.

### Open Question 2
- Question: What is the optimal target value for the fusion coefficient α in WRPO?
- Basis in paper: [explicit] The paper uses a greedy search over [0.1, 0.3, 0.5, 0.7, 0.9] to determine the target value, but does not claim to have found the optimal value.
- Why unresolved: The search was limited to a discrete set of values, and the optimal value may lie between these points or outside the tested range.
- What evidence would resolve it: A more comprehensive search over a continuous range of α values, potentially using optimization techniques to find the optimal value.

### Open Question 3
- Question: How does WRPO perform on tasks outside the instruction-following domain?
- Basis in paper: [inferred] The paper mentions that WRPO improves performance on instruction-following benchmarks but does not extensively test other domains like mathematics or general knowledge.
- Why unresolved: The training dataset used (UltraFeedback) is focused on instruction-following tasks, limiting the evaluation to similar domains.
- What evidence would resolve it: Experiments evaluating WRPO on diverse tasks such as MMLU, mathematics, or coding challenges to assess its generalization capabilities.

## Limitations

- The paper lacks ablation studies on how different reward model choices affect final performance
- Evaluation is limited to English-centric benchmarks without testing multilingual robustness
- The paper does not present comparative experiments with explicit fusion methods to validate the claim that WRPO "eliminates the need for vocabulary alignment and matrix fusion"

## Confidence

- High confidence: The core mechanism of weighted-reward preference optimization and its formulation based on the Bradley-Terry model
- Medium confidence: The claim that progressive adaptation strategy mitigates distributional deviation, as the evidence is indirect and based on ablation performance rather than explicit distributional analysis
- Low confidence: The assertion that WRPO "eliminates the need for vocabulary alignment and matrix fusion" since no comparative experiments with explicit fusion methods are presented

## Next Checks

1. Test WRPO with different reward models (e.g., GPT-4 based rewarders) to verify that performance gains are not specific to the chosen ArmoRM model
2. Conduct an ablation study varying the α schedule (linear vs. exponential vs. step-wise) to quantify its impact on final performance and stability
3. Evaluate the trained model on out-of-distribution prompts and multilingual datasets to assess robustness beyond the training distribution