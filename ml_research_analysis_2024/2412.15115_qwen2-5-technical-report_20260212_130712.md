---
ver: rpa2
title: Qwen2.5 Technical Report
arxiv_id: '2412.15115'
source_url: https://arxiv.org/abs/2412.15115
tags:
- qwen2
- tasks
- zhang
- yang
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Qwen2.5 introduces a series of large language models (LLMs) featuring
  enhanced pre-training with 18 trillion tokens and advanced post-training techniques,
  including supervised fine-tuning and multi-stage reinforcement learning. These improvements
  boost human preference alignment, long text generation, and structural data analysis.
---

# Qwen2.5 Technical Report

## Quick Facts
- arXiv ID: 2412.15115
- Source URL: https://arxiv.org/abs/2412.15115
- Reference count: 13
- Key outcome: Qwen2.5 models achieve competitive performance with Llama-3-405B-Instruct using 5x fewer parameters through enhanced pre-training (18T tokens) and multi-stage RL

## Executive Summary
Qwen2.5 introduces a family of large language models built on enhanced pre-training with 18 trillion tokens and advanced post-training techniques including supervised fine-tuning and multi-stage reinforcement learning. The models demonstrate improved human preference alignment, long text generation capabilities, and structural data analysis while maintaining strong performance on general tasks. Available in sizes from 0.5B to 72B parameters with both open-weight and proprietary MoE variants, Qwen2.5 achieves state-of-the-art performance in several benchmarks while offering competitive cost-effectiveness.

## Method Summary
Qwen2.5 employs dense Transformer architectures with grouped query attention, SwiGLU activations, rotary positional embeddings, and RMSNorm. The models undergo staged pre-training on 18T tokens with progressive context length scaling (4K→32K→up to 1M for MoE variants), followed by supervised fine-tuning on 1M+ samples covering general instructions, long generation, mathematics, and coding. Post-training uses a two-stage RL approach: offline DPO for developing capabilities difficult for reward models to evaluate, followed by online GRPO for refining nuanced quality aspects. Long-context fine-tuning employs YARN and DCA techniques for sparse attention.

## Key Results
- Qwen2.5-72B-Instruct performs competitively with Llama-3-405B-Instruct despite being 5x smaller
- Qwen2.5-Turbo and Qwen2.5-Plus offer strong cost-effectiveness with enhanced long-context capabilities
- Models serve as foundation for specialized variants including Qwen2.5-Math and Qwen2.5-Coder
- Achieved state-of-the-art performance on multiple benchmarks including MMLU, MATH, and coding tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling pre-training data from 7T to 18T tokens directly improves model performance across general and specialized tasks.
- Mechanism: Larger data volume provides richer exposure to diverse knowledge, code, and mathematical reasoning patterns, critical for generalization.
- Core assumption: Data quality remains consistent during scaling.
- Evidence anchors: [abstract] scaling to 18T tokens; [section] strategic down-sampling/up-sampling of domains; [corpus] weak external validation.
- Break condition: Performance degrades if increased volume includes proportionally more low-quality samples.

### Mechanism 2
- Claim: Multi-stage RL (offline DPO + online GRPO) improves human preference alignment more effectively than single-stage RL.
- Mechanism: Offline RL develops hard-to-evaluate capabilities first, then online RL refines nuanced quality aspects using reward models.
- Core assumption: Reward models can accurately differentiate subtle quality differences.
- Evidence anchors: [abstract] multistage RL enhances human preference; [section] offline focuses on challenging capabilities, online leverages reward model nuances; [corpus] no external validation.
- Break condition: Reward models over-optimized on benchmarks (Goodhart's law), degrading real-world alignment.

### Mechanism 3
- Claim: Progressive context length extension during training enables effective long-context processing without degrading short-context performance.
- Mechanism: Training starts with shorter contexts and gradually increases length through stages, allowing smooth adaptation.
- Core assumption: Models can retain short-sequence capabilities while learning longer sequences.
- Evidence anchors: [abstract] staged transitions among mixtures; [section] progressive expansion through four stages; [corpus] no external validation.
- Break condition: Models overfit to long-context patterns, losing efficiency on short-context tasks.

## Foundational Learning

- Concept: Scaling laws for hyperparameters
  - Why needed here: Determines optimal learning rates and batch sizes for different model sizes, ensuring efficient convergence
  - Quick check question: What relationship did authors find between model size and optimal learning rate/batch size?

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Enables larger capacity with reduced computational cost by activating relevant expert pathways
  - Quick check question: How does MoE implementation differ from standard dense architectures?

- Concept: Supervised fine-tuning with large-scale datasets
  - Why needed here: Provides foundation for instruction-following before RL refinement
  - Quick check question: What specific data types were included beyond general instruction following?

## Architecture Onboarding

- Component map: Dense Transformer backbone → GQA + SwiGLU + RoPE + QKV bias + RMSNorm → MoE variants → Progressive context extension → Multi-stage RL pipeline
- Critical path: Pre-training (18T tokens) → SFT (1M samples) → Offline RL (DPO) → Online RL (GRPO) → Long-context fine-tuning → Evaluation
- Design tradeoffs: MoE provides cost-effectiveness but adds routing complexity; progressive context extension improves long-context handling but requires careful curation
- Failure signatures: Performance degradation on short-context after long-context training; reward model over-optimization leading to alignment issues; MoE expert capacity underutilization
- First 3 experiments:
  1. Benchmark Qwen2.5-72B-Instruct against Llama-3-405B-Instruct on MMLU and MATH to verify 5x parameter efficiency
  2. Test Qwen2.5-Turbo's 1M context length with passkey retrieval to validate long-context capabilities
  3. Compare single-stage vs multi-stage RL outcomes on instruction-following benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does optimal hyperparameter configuration vary across different model scales and architectures in MoE vs dense models?
- Basis in paper: [explicit] Developing scaling laws for hyperparameters based on experimentation with dense (44M to 14B) and MoE models (44M to 1B activated parameters)
- Why unresolved: Paper mentions developing scaling laws but doesn't provide specific formulas for hyperparameter adjustments
- What evidence would resolve it: Detailed scaling law equations showing relationships between optimal learning rate, batch size, model size, and data scale

### Open Question 2
- Question: What is the impact of different synthetic data generation methods on model performance across various domains?
- Basis in paper: [explicit] Using Qwen2-72B-Instruct and Qwen2-Math-72B-Instruct for synthetic data generation in math, code, and knowledge domains
- Why unresolved: Paper describes using synthetic data but lacks detailed analysis of how different generation methods impact domain-specific performance
- What evidence would resolve it: Comparative studies showing performance differences across domains using different synthetic data methods

### Open Question 3
- Question: How do reward model evaluation benchmarks correlate with actual RL model performance in real-world applications?
- Basis in paper: [explicit] Current reward model benchmarks don't accurately predict RL model performance; higher RM scores don't necessarily correlate with superior RL performance
- Why unresolved: Paper identifies limitation but provides no alternative evaluation methods or explanation of the disconnect
- What evidence would resolve it: Studies comparing RM benchmark scores with actual RL model performance across diverse real-world applications

## Limitations

- Limited external validation of the 18T token scaling claims and progressive context extension methodology
- No comparative studies validating multi-stage RL superiority over single-stage approaches
- Evaluation focuses on benchmarks where Qwen2.5 performs well, with limited discussion of real-world failure modes

## Confidence

**High Confidence Claims:**
- Model architecture specifications and basic training methodology
- Performance comparisons showing competitive results on standard benchmarks

**Medium Confidence Claims:**
- Specific performance gains from scaling pre-training to 18T tokens
- Effectiveness of multi-stage RL approach
- Practical long-context capabilities beyond benchmark optimization

**Low Confidence Claims:**
- Cost-effectiveness claims relative to competitors
- Generalizability of improvements across all task domains
- Real-world alignment quality beyond benchmark metrics

## Next Checks

1. **Independent benchmark replication**: Run Qwen2.5-72B-Instruct through independent third-party evaluation suites to verify the 5x parameter efficiency claim against Llama-3-405B-Instruct across diverse task categories.

2. **Long-context retrieval validation**: Design comprehensive long-context evaluation including real-world document analysis and passkey retrieval at 1M context length to verify practical benefits of progressive context extension.

3. **Ablation study on RL methodology**: Conduct controlled experiments comparing single-stage vs multi-stage reinforcement learning on identical checkpoints to determine if added complexity provides statistically significant improvements.