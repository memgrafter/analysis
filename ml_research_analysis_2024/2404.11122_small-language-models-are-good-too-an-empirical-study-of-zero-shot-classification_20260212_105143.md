---
ver: rpa2
title: 'Small Language Models are Good Too: An Empirical Study of Zero-Shot Classification'
arxiv_id: '2404.11122'
source_url: https://arxiv.org/abs/2404.11122
tags:
- datasets
- classification
- language
- performance
- impact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models are necessary
  for effective zero-shot text classification. The authors benchmark 72 language models
  ranging from 77M to 40B parameters across 15 diverse datasets using four different
  scoring functions.
---

# Small Language Models are Good Too: An Empirical Study of Zero-Shot Classification

## Quick Facts
- arXiv ID: 2404.11122
- Source URL: https://arxiv.org/abs/2404.11122
- Reference count: 0
- Small language models (77M-3B parameters) can perform zero-shot classification tasks comparably to larger models on many datasets

## Executive Summary
This paper investigates whether large language models are necessary for effective zero-shot text classification. The authors benchmark 72 language models ranging from 77M to 40B parameters across 15 diverse datasets using four different scoring functions. Their results show that small models (77M-3B parameters) perform comparably to larger models on many classification tasks, with 10 of 15 datasets showing no significant correlation between model size and performance. The choice of architecture (encoder-decoder vs decoder-only) and instruction-tuning significantly impacts performance on specific datasets, while the choice of scoring function has no marked effect. These findings suggest that resource-efficient small models can be viable solutions for text classification challenges, challenging the prevailing dominance of large language models.

## Method Summary
The study evaluates 72 language models (77M-40B parameters) across 15 diverse text classification datasets using zero-shot prompting with four scoring functions: Probability, DCPMI, PMI, and Similarity. Models are tested in both encoder-decoder and decoder-only architectures, with and without instruction-tuning. Performance is measured using accuracy for balanced datasets and F1 scores for imbalanced ones. Statistical analysis includes biweight midcorrelation and ANCOVA to assess relationships between model characteristics and performance outcomes.

## Key Results
- Small models (77M-3B parameters) perform comparably to larger models on 10 of 15 datasets tested
- Architectural choice (encoder-decoder vs decoder-only) significantly impacts performance on 7 of 15 datasets
- Scoring function choice (Probability, DCPMI, PMI, Similarity) shows no significant impact on classification performance
- Instruction-tuning improves performance, but its effectiveness is architecture-dependent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small language models (77M-3B parameters) can perform zero-shot classification tasks comparably to larger models on many datasets.
- Mechanism: The model's performance in zero-shot classification depends more on task-specific factors like dataset characteristics and architectural choices than on model size alone.
- Core assumption: Classification tasks can be effectively solved with smaller parameter counts when the dataset and task structure align well with the model's capabilities.
- Evidence anchors:
  - [abstract] "Our findings reveal that small models can effectively classify texts, getting on par with or surpassing their larger counterparts"
  - [section] "10 of 15 datasets show p-values exceeding 0.05, suggesting no significant link between Acc/F1 scores and model size"
  - [corpus] "Average neighbor FMR=0.569, average citations=0.1" - weak evidence of direct relevance to zero-shot classification performance
- Break condition: When dataset complexity or task requirements exceed the representational capacity of small models, leading to performance degradation.

### Mechanism 2
- Claim: The choice of architecture (encoder-decoder vs decoder-only) significantly impacts zero-shot classification performance on specific datasets.
- Mechanism: Different architectural designs provide varying strengths in processing and generating text, which affects their ability to understand prompts and generate appropriate classifications.
- Core assumption: Architectural differences create task-specific advantages that can outweigh the benefits of larger model sizes.
- Evidence anchors:
  - [abstract] "The choice of architecture (encoder-decoder vs decoder-only) and instruction-tuning significantly impacts performance on specific datasets"
  - [section] "7 out of 15 datasets... show p-values below 0.05, suggesting there the architecture has a significant impact"
  - [corpus] No direct evidence found regarding architectural impacts
- Break condition: When dataset characteristics align well with both architectures, minimizing the impact of architectural differences.

### Mechanism 3
- Claim: Instruction-tuning improves zero-shot classification performance, but its effectiveness is architecture-dependent.
- Mechanism: Fine-tuning on instruction-following datasets enhances the model's ability to interpret and respond to prompts, though this benefit varies based on the underlying architecture.
- Core assumption: Instruction-tuning provides task-general benefits that interact differently with encoder-decoder versus decoder-only architectures.
- Evidence anchors:
  - [abstract] "the choice of architecture (encoder-decoder vs decoder-only) and instruction-tuning significantly impacts performance on specific datasets"
  - [section] "instruction fine-tuning improves performances when compared to not fine-tuning... This is evident from the graphical representation and the significant p-values"
  - [corpus] No direct evidence found regarding instruction-tuning impacts
- Break condition: When the instruction-tuned knowledge conflicts with or is irrelevant to the specific classification task.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: Understanding how models can perform tasks without task-specific training is fundamental to this research
  - Quick check question: Can you explain the difference between zero-shot, few-shot, and fine-tuned learning?

- Concept: Prompt engineering
  - Why needed here: The study relies on carefully crafted prompts to elicit classification responses from models
  - Quick check question: How would you design a prompt to classify text into multiple categories without providing examples?

- Concept: Statistical correlation and significance testing
  - Why needed here: The analysis uses biweight midcorrelation and ANCOVA to determine relationships between variables
  - Quick check question: What does a p-value of 0.05 represent in hypothesis testing?

## Architecture Onboarding

- Component map:
  Model selection (77M-40B parameters, encoder-decoder vs decoder-only) -> Prompt design (task-specific instructions and verbalizers) -> Scoring functions (Probability, DCPMI, PMI, Similarity) -> Evaluation metrics (accuracy for balanced datasets, F1 for imbalanced) -> Statistical analysis (correlation coefficients, ANCOVA, Kruskal-Wallis test)

- Critical path:
  1. Select model architecture and size
  2. Design appropriate prompt for classification task
  3. Apply chosen scoring function
  4. Evaluate performance on test set
  5. Analyze results using statistical methods

- Design tradeoffs:
  - Model size vs. computational efficiency
  - Encoder-decoder vs. decoder-only architectures
  - Simple prompts vs. complex prompt engineering
  - Different scoring functions for final classification

- Failure signatures:
  - Low correlation between model size and performance across multiple datasets
  - Significant performance variance based on architectural choice
  - Inconsistent impact of instruction-tuning across different architectures

- First 3 experiments:
  1. Compare performance of a 77M parameter model vs. 40B parameter model on a balanced sentiment classification dataset using identical prompts
  2. Test encoder-decoder and decoder-only versions of the same model size on a relation classification task
  3. Evaluate the impact of instruction-tuning by comparing tuned vs. non-tuned versions on a topic classification dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does instruction-tuning provide consistent performance benefits across different model architectures, or are its effects architecture-dependent?
- Basis in paper: [explicit] The paper explicitly states "while instruction fine-tuning has the potential to enhance model performance on many datasets, its impact may vary depending on the specific dataset" and shows through ANCOVA that the impact of instruction-tuning is significant for encoder-decoder models but not for decoder-only models.
- Why unresolved: The paper only tested instruction-tuning on a limited set of architectures (encoder-decoder and decoder-only) and datasets. The underlying reasons for why instruction-tuning benefits one architecture but not another remain unclear.
- What evidence would resolve it: Systematic testing of instruction-tuning across a broader range of architectures (including encoder-only, non-causal decoders, and newer architectures like RWKV or Retentive Networks) would clarify whether the architecture-dependence is a general phenomenon or specific to the tested models.

### Open Question 2
- Question: What is the optimal balance between model size and task complexity for zero-shot classification performance?
- Basis in paper: [inferred] The paper finds that "model size isn't the sole determinant of performance" and that "10 of 15 datasets show p-values exceeding 0.05, suggesting no significant link between Acc/F1 scores and model size," yet also notes that for some datasets like cdr, ethos, and imdb, there are correlations.
- Why unresolved: The paper doesn't establish clear guidelines for when model size becomes a limiting factor versus when architectural choices or other factors dominate. The relationship between model size and task complexity remains unexplored.
- What evidence would resolve it: Comprehensive benchmarking across tasks with varying complexity levels (simple binary classification to multi-class problems with nuanced distinctions) while systematically varying model sizes would reveal whether there are tipping points where size becomes critical.

### Open Question 3
- Question: How do different scoring functions interact with model architecture and instruction-tuning to affect zero-shot classification performance?
- Basis in paper: [explicit] The paper tests four different scoring functions (Probability, DCPMI, PMI, Similarity) and finds "no significant impact on the choice of scoring functions" across architectures, yet acknowledges that "many valid sequences can represent the same concept" (surface form competition).
- Why unresolved: The paper tests scoring functions in isolation but doesn't explore how they might compound with other factors like architecture choice or instruction-tuning. The finding that scoring functions don't significantly impact performance may mask more nuanced interactions.
- What evidence would resolve it: A factorial experimental design testing all combinations of architecture, instruction-tuning status, and scoring functions across diverse datasets would reveal whether certain scoring functions are more effective for specific architecture-instruction combinations.

## Limitations
- Prompt specificity and design details are not fully specified, making it difficult to assess whether prompt quality could be driving performance differences
- Specific HuggingFace model versions are not identified, which could affect reproducibility since different versions may have varying performance characteristics
- Dataset preprocessing steps and tokenization methods are not fully specified, which could impact results

## Confidence

**High Confidence**: Zero-shot classification methodology - The paper clearly establishes a reproducible methodology using four scoring functions and standard evaluation metrics across diverse datasets.

**Medium Confidence**: Small model performance claims - The finding that small models perform comparably to large models is well-supported, though the specific conditions under which this holds true require further validation.

**Medium Confidence**: Architecture impact findings - The observed significance of architectural choices is supported by statistical tests, but the practical implications may vary based on implementation details not fully specified.

**Low Confidence**: Instruction-tuning impact - While the paper suggests instruction-tuning improves performance, the magnitude and consistency of this effect across different architectures and datasets needs further investigation.

## Next Checks

**Validation Check 1**: Implement and test the same small and large models on 2-3 datasets using the exact prompts described in the paper to verify if performance patterns match the reported findings.

**Validation Check 2**: Test encoder-decoder and decoder-only versions of the same model size (e.g., 1B parameters) on the same classification task to isolate architectural effects from other variables.

**Validation Check 3**: Reproduce the correlation and ANCOVA analyses using the published datasets to verify the statistical significance claims and assess whether different significance thresholds would change the conclusions.