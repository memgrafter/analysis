---
ver: rpa2
title: Are Large Language Models Good at Utility Judgments?
arxiv_id: '2403.19216'
source_url: https://arxiv.org/abs/2403.19216
tags:
- utility
- llms
- evidence
- judgments
- passages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether LLMs can distinguish between relevance
  and utility in retrieval-augmented generation. The authors conduct a comprehensive
  study using five representative LLMs across multiple datasets, introducing a benchmarking
  procedure with candidate passages of different characteristics.
---

# Are Large Language Models Good at Utility Judgments?

## Quick Facts
- **arXiv ID:** 2403.19216
- **Source URL:** https://arxiv.org/abs/2403.19216
- **Reference count:** 40
- **Primary result:** LLMs can distinguish between relevance and utility in retrieval-augmented generation, with utility judgments proving more effective for answer generation than relevance judgments

## Executive Summary
This paper investigates whether large language models (LLMs) can effectively distinguish between relevance and utility in retrieval-augmented generation tasks. Through comprehensive experiments across five representative LLMs and multiple datasets, the authors demonstrate that well-instructed LLMs can indeed make this distinction. The study reveals that utility judgments lead to better answer generation performance compared to relevance judgments, while also uncovering LLMs' sensitivity to counterfactual passages and the position of ground-truth evidence in input lists.

The authors propose a k-sampling, listwise approach to mitigate position dependency issues, showing improved answer generation performance. While the results demonstrate promising capabilities of LLMs in utility judgments, the study also identifies a significant performance gap compared to using ground-truth evidence. This work provides important insights into the potential and limitations of using LLMs for utility assessment in retrieval-augmented generation systems.

## Method Summary
The authors conduct a comprehensive study using five representative LLMs across multiple datasets to investigate utility judgment capabilities. They introduce a benchmarking procedure with candidate passages of different characteristics, including counterfactual passages and varying positions of ground-truth evidence. The experimental setup involves assessing both relevance and utility judgments through controlled scenarios where LLMs must evaluate passage utility for answer generation. A key innovation is the proposed k-sampling, listwise approach to address position dependency issues in utility judgments. The study employs standard evaluation metrics for answer generation quality while comparing utility judgments against both relevance judgments and ground-truth evidence baselines.

## Key Results
- Well-instructed LLMs can distinguish between relevance and utility judgments in retrieval-augmented generation
- Utility judgments prove more effective for answer generation than relevance judgments
- LLMs exhibit high sensitivity to counterfactual passages and the position of ground-truth evidence in input lists
- The proposed k-sampling, listwise approach mitigates position dependency and improves answer generation performance

## Why This Works (Mechanism)
The effectiveness of utility judgments stems from LLMs' ability to understand contextual relevance beyond surface-level matching. When properly instructed, LLMs can evaluate how passages contribute to answer generation by considering semantic coherence, information completeness, and the logical flow of reasoning. The k-sampling approach works by reducing position bias through multiple sampling strategies that present evidence in varied orders, allowing the model to focus on intrinsic utility rather than positional artifacts.

## Foundational Learning
1. **Retrieval-augmented generation (RAG)**: Why needed - Forms the core application domain for utility judgments; Quick check - Understanding how retrieved passages support answer generation
2. **Relevance vs utility distinction**: Why needed - Central to the paper's investigation of LLM capabilities; Quick check - Recognizing that relevant passages may not always be useful for generation
3. **Position bias in language models**: Why needed - Critical for understanding the proposed k-sampling solution; Quick check - Awareness of how input ordering affects model judgments
4. **Counterfactual evaluation**: Why needed - Essential for testing model robustness to misleading information; Quick check - Ability to identify and handle irrelevant or misleading passages
5. **Listwise ranking approaches**: Why needed - Foundation for understanding the proposed mitigation strategy; Quick check - Knowledge of how to evaluate and rank multiple candidates simultaneously
6. **LLM instruction tuning**: Why needed - Key to enabling utility judgment capabilities; Quick check - Understanding how specific instructions shape model behavior

## Architecture Onboarding

**Component Map:**
Retrieval System -> Candidate Passage Selection -> Utility Judgment Module -> Answer Generation Module -> Evaluation Metrics

**Critical Path:**
The critical path flows from retrieval through utility judgment to answer generation. The utility judgment module serves as the pivotal component, as its quality directly impacts answer generation performance. This module must effectively distinguish between relevance and utility while mitigating position dependency issues through the k-sampling approach.

**Design Tradeoffs:**
The primary tradeoff involves balancing instruction specificity with model flexibility. Highly specific instructions may improve utility judgment accuracy but could reduce the model's ability to handle diverse scenarios. The k-sampling approach trades computational efficiency for reduced position bias, requiring multiple model evaluations to achieve more reliable judgments.

**Failure Signatures:**
- Over-reliance on surface-level relevance rather than contextual utility
- Sensitivity to passage ordering leading to inconsistent judgments
- Inability to properly handle counterfactual or misleading passages
- Degradation in performance when ground-truth evidence appears in non-optimal positions

**First Experiments:**
1. Compare utility judgment quality across different temperature settings to identify optimal configuration
2. Test the k-sampling approach with varying k values to determine the sweet spot between performance and efficiency
3. Evaluate model performance on counterfactual passages to establish robustness baselines

## Open Questions the Paper Calls Out
None

## Limitations
- The controlled experimental setup may not fully capture real-world retrieval scenarios with open-ended document collections
- Focus on single-turn QA tasks and pre-filtered candidate passages creates artificial constraints
- Reliance on specific model-instruct variants and temperature settings raises questions about generalizability
- Benchmark datasets may not represent the full spectrum of retrieval-augmented generation challenges

## Confidence

**High confidence:**
- LLMs can distinguish between relevance and utility judgments (well-supported by experimental evidence)
- Utility judgments outperform relevance judgments in answer generation (strong empirical backing)

**Medium confidence:**
- Effectiveness of k-sampling, listwise approach (demonstrated but implementation-sensitive)
- LLMs' sensitivity to counterfactual passages (supported but needs broader testing)
- Comparison between utility judgments and ground-truth evidence (influenced by benchmark construction)

## Next Checks
1. Test the utility judgment framework across different model families (e.g., Claude, LLaMA, PaLM) and instruct variants to assess generalizability beyond the current model selection.
2. Evaluate the k-sampling approach in open retrieval settings with large-scale document collections to verify its effectiveness in more realistic scenarios.
3. Conduct ablation studies on the impact of different temperature settings and sampling strategies on utility judgment quality to establish optimal configuration parameters.