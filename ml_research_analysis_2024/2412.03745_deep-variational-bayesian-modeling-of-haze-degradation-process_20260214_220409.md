---
ver: rpa2
title: Deep Variational Bayesian Modeling of Haze Degradation Process
arxiv_id: '2412.03745'
source_url: https://arxiv.org/abs/2412.03745
tags:
- image
- dehazing
- transmission
- gcanet
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses single image dehazing by proposing a variational
  Bayesian framework that models uncertainties in transmission maps and haze-free
  images. The core idea is to treat both the clean image and transmission map as latent
  variables, with their posterior distributions parameterized by neural networks (dehazing
  and transmission networks).
---

# Deep Variational Bayesian Modeling of Haze Degradation Process

## Quick Facts
- arXiv ID: 2412.03745
- Source URL: https://arxiv.org/abs/2412.03745
- Reference count: 40
- Primary result: Model-agnostic variational Bayesian framework that jointly models transmission maps and haze-free images improves dehazing performance across benchmark datasets

## Executive Summary
This paper introduces a novel variational Bayesian framework for single image dehazing that models both the haze-free image and transmission map as latent variables with neural network-parameterized posteriors. The framework derives a new objective function based on the physical haze degradation model that encourages cooperation between the dehazing and transmission networks during joint training. By treating both latent variables as uncertain quantities with appropriate prior distributions (Laplace for haze-free images, Lognormal for transmission maps), the method captures uncertainty and improves performance. The approach is model-agnostic and can be integrated with existing dehazing networks without architectural modification, showing consistent improvements across synthetic and real-world datasets.

## Method Summary
The method treats dehazing as a variational inference problem where the haze-free image and transmission map are latent variables with posteriors parameterized by neural networks (D-Net and T-Net). The framework derives an objective function combining a likelihood term from the atmospheric scattering model with KL divergence terms that regularize the posteriors to match their priors. Joint training of both networks through this objective enables cooperation and mutual improvement. The model is designed to be architecture-agnostic, accepting any existing dehazing network as D-Net while using a transmission estimation network (GCANet) as T-Net. Training involves optimizing both networks simultaneously using the derived variational lower bound.

## Key Results
- Consistently improves dehazing performance across multiple benchmark datasets (RESIDE/SOTS, Haze4K, NH-Haze, Fattal) when integrated with various baseline networks
- Enhances visual quality of dehazed images and improves downstream object detection performance (mAP50 and mAP50-95 on KITTI Haze dataset)
- Ablation studies show that Laplace and Lognormal priors outperform Gaussian alternatives, validating the uncertainty modeling approach
- Model-agnostic design successfully integrates with GCANet, FFA-Net, and DehazeFormer-B, demonstrating flexibility

## Why This Works (Mechanism)

### Mechanism 1
Joint training of dehazing and transmission networks via variational lower bound encourages mutual improvement. The likelihood term directly links the dehazing network's output and the transmission network's output through the atmospheric scattering model, creating a cooperative optimization signal. Core assumption: Both latent variables are accurately parameterized by neural networks and their joint posterior can be approximated by independent posteriors. Break condition: If the atmospheric scattering model is significantly violated in real-world data, the cooperation signal becomes unreliable.

### Mechanism 2
Model-agnostic design allows integration with any existing dehazing network without architectural modification. The variational framework treats the dehazing network as a black box that estimates the posterior of the haze-free image, requiring only that it takes a hazy image as input and outputs an estimate of the clean image. Core assumption: The baseline dehazing network's output can serve as a sufficient statistic for the posterior distribution of the haze-free image. Break condition: If the baseline network's architecture makes it incompatible with the variational framework's training dynamics.

### Mechanism 3
Uncertainty modeling through Laplace and Lognormal priors improves dehazing performance compared to deterministic approaches. By modeling the haze-free image with Laplace distribution and transmission map with Lognormal distribution, the framework captures uncertainty in these estimates, leading to more robust training and better generalization. Core assumption: The chosen prior distributions accurately reflect the statistical properties of haze-free images and transmission maps in the training data. Break condition: If the training data does not follow the assumed statistical distributions, the uncertainty modeling becomes counterproductive.

## Foundational Learning

Concept: Atmospheric scattering model (I = J ⊙ t + A · (1 - t))
Why needed here: This physical model provides the mathematical foundation for relating the observed hazy image to the latent clean image and transmission map
Quick check question: What are the three components that make up a hazy image according to the atmospheric scattering model?

Concept: Variational inference and variational lower bound
Why needed here: These techniques allow approximation of the intractable posterior distribution of latent variables using neural networks
Quick check question: What is the relationship between the marginal log-likelihood and the variational lower bound?

Concept: Kullback-Leibler (KL) divergence
Why needed here: KL divergence terms in the objective function regularize the approximate posterior to be close to the prior distribution
Quick check question: What does the KL divergence term KL(q(z|y) ∥ p(z)) encourage in the training process?

## Architecture Onboarding

Component map:
Hazy image → D-Net → Estimated haze-free image; Hazy image → T-Net → Estimated transmission map; Both estimates used in likelihood term

Critical path: Hazy image → D-Net → Estimated haze-free image; Hazy image → T-Net → Estimated transmission map; Both estimates used in likelihood term

Design tradeoffs:
- Model-agnostic vs. specialized architecture: Allows flexibility but may miss architecture-specific optimizations
- Joint training vs. separate training: Enables cooperation but increases complexity
- Uncertainty modeling vs. deterministic approach: Captures variability but requires more parameters

Failure signatures:
- Poor dehazing performance despite good transmission estimates: Indicates D-Net not learning effectively from T-Net
- Transmission map estimates are noisy: Suggests T-Net not receiving sufficient signal from D-Net
- Training instability: May indicate improper weighting of likelihood vs. KL terms

First 3 experiments:
1. Replace D-Net with a simple CNN and verify framework still trains (test model-agnostic claim)
2. Remove the likelihood term and observe if performance degrades (test cooperation mechanism)
3. Replace Laplace and Lognormal priors with Gaussian and measure impact on SSIM (test uncertainty modeling contribution)

## Open Questions the Paper Calls Out

### Open Question 1
Question: How does the performance of the proposed framework change when using different prior distributions for the latent variables (e.g., Gaussian vs. Laplace for the haze-free image, or Gaussian vs. Log-normal for the transmission map)?
Basis in paper: The paper mentions an ablation study on prior distributions in section 4.4, stating that Laplace and Log-normal distributions tend to yield better performance than Gaussian distributions.
Why unresolved: The paper only provides a comparison of a few specific cases and does not explore the full space of possible prior distributions or their impact on the model's performance across different datasets and scenarios.
What evidence would resolve it: A comprehensive ablation study exploring a wider range of prior distributions and their impact on performance metrics (e.g., PSNR, SSIM) across multiple datasets and dehazing networks.

### Open Question 2
Question: What is the impact of the transmission network's (T-Net) capacity on the overall dehazing performance, and how does it interact with the dehazing network's (D-Net) architecture?
Basis in paper: The paper discusses the role of T-Net in section 4.5 and mentions that improving D-Net architecture tends to improve transmission accuracy and vice versa.
Why unresolved: The paper only provides a limited analysis of the relationship between T-Net and D-Net capacities and does not explore the optimal balance or the impact of different architectural choices for each network.
What evidence would resolve it: A detailed study investigating the impact of T-Net's capacity on dehazing performance across various D-Net architectures and datasets, including an analysis of the optimal balance between the two networks.

### Open Question 3
Question: How does the proposed framework handle real-world scenarios where the assumptions of the atmospheric scattering model are violated, such as non-homogeneous atmospheric conditions or varying atmospheric light?
Basis in paper: The paper mentions in section 4.5 that the assumptions of the atmospheric scattering model may not hold true in real-world hazy images and discusses a modification to mitigate the effects of these violations.
Why unresolved: The paper does not provide a comprehensive evaluation of the framework's performance on real-world datasets with varying atmospheric conditions or a detailed analysis of the limitations and potential improvements for handling such scenarios.
What evidence would resolve it: A thorough evaluation of the framework's performance on diverse real-world datasets with varying atmospheric conditions, including a comparison with other state-of-the-art methods and an analysis of the framework's limitations and potential improvements.

## Limitations

- The model-agnostic claim requires further validation as integration has only been demonstrated with a limited set of baseline networks
- The choice of Laplace and Lognormal priors, while showing improvement, lacks theoretical justification for why these specific distributions are optimal
- The framework's performance in real-world scenarios where atmospheric scattering model assumptions are violated has not been comprehensively evaluated

## Confidence

- High Confidence: The variational Bayesian framework's mathematical formulation and the derivation of the objective function from the atmospheric scattering model
- Medium Confidence: The performance improvements on benchmark datasets, as results are compared against specific baselines under controlled conditions
- Low Confidence: The model-agnostic claim, as integration with only a few baseline networks was demonstrated

## Next Checks

1. Test the framework with a wider range of dehazing architectures (e.g., transformer-based, GAN-based) to validate true model-agnostic behavior
2. Conduct experiments on real-world datasets with significantly different haze characteristics than the training data to test generalization
3. Replace the Laplace and Lognormal priors with data-driven learned priors to determine if performance gains are due to the specific distributions or the uncertainty modeling approach itself