---
ver: rpa2
title: 'AMPS: ASR with Multimodal Paraphrase Supervision'
arxiv_id: '2411.18368'
source_url: https://arxiv.org/abs/2411.18368
tags:
- speech
- amps
- words
- training
- paraphrase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces AMPS, a technique that augments multilingual
  multimodal ASR systems with paraphrase-based supervision to improve conversational
  ASR in low-resource languages. The method leverages paraphrases of reference transcriptions
  as additional training supervision, selectively activating this objective when ASR
  loss exceeds a threshold.
---

# AMPS: ASR with Multimodal Paraphrase Supervision

## Quick Facts
- arXiv ID: 2411.18368
- Source URL: https://arxiv.org/abs/2411.18368
- Reference count: 23
- One-line primary result: AMPS achieves up to 5% relative WER reduction in low-resource multilingual ASR using selective paraphrase supervision

## Executive Summary
This work introduces AMPS, a technique that augments multilingual multimodal ASR systems with paraphrase-based supervision to improve conversational ASR in low-resource languages. The method leverages paraphrases of reference transcriptions as additional training supervision, selectively activating this objective when ASR loss exceeds a threshold. Applied to the SeamlessM4T model across five languages (Hindi, Marathi, Malayalam, Kannada, Nyanja), AMPS achieves relative WER reductions of up to 5% compared to strong baselines. Human evaluation confirms consistent improvements in transcription quality. The approach is particularly effective for utterances where standard ASR performs poorly, and shows adaptability across diverse language families.

## Method Summary
AMPS improves low-resource ASR by using paraphrases of reference transcriptions as additional supervision, selectively activating this objective when ASR loss exceeds a threshold τ. The method uses round-trip translation to generate paraphrases that preserve word order, enabling easy alignment between speech and text representations. Parameter-efficient adapter layers are used to fine-tune the SeamlessM4T model, allowing adaptation with minimal trainable parameters. The approach combines standard ASR loss with paraphrase loss for utterances with high ASR loss, focusing training resources on challenging cases while maintaining the primary ASR objective.

## Key Results
- Relative WER reductions of up to 5% compared to strong baselines across five languages
- Consistent improvements confirmed through human evaluation on scale 0-5
- Particularly effective for utterances where standard ASR performs poorly
- Adaptable across diverse language families (Indic and Bantu languages)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: AMPS improves ASR by selectively using paraphrase supervision when ASR loss is high.
- **Mechanism**: The model computes ASR loss per utterance. If the loss exceeds a threshold τ, it activates the paraphrase objective alongside the ASR loss, effectively offering the model semantically similar alternatives when audio clarity is poor.
- **Core assumption**: High ASR loss correlates with poorly enunciated or noisy speech, and paraphrase supervision can guide the model toward more accurate semantic predictions.
- **Evidence anchors**:
  - [abstract] "selectively invoke this paraphrase objective for utterances with poor ASR performance"
  - [section 3] "AMPS τ : Loss Function Thresholding. We aim at improving the model's performance in noisy regions where the ASR loss is high by selectively triggering the paraphrase objective only when the ASR loss exceeds a predefined threshold τ."
  - [corpus] Weak: only general ASR/LLM neighbors; no direct AMPS paraphrase evidence.

### Mechanism 2
- **Claim**: Using paraphrases with minimal word-order change preserves the audio-text alignment in multimodal models.
- **Mechanism**: By generating paraphrases that retain the original sentence structure and word order, the model can more easily map speech encoder outputs to paraphrase decoder outputs without having to learn a new alignment mapping.
- **Core assumption**: The shared decoder in SeamlessM4T benefits from consistent positional patterns between speech and text inputs.
- **Evidence anchors**:
  - [abstract] "It is important that the paraphrases should not significantly differ in word order from the original transcripts, thus enabling the model to easily align representations of speech, text, and its paraphrase."
  - [section 3] "Paraphrasing. We translated the reference transcriptions into English using IndicTrans-2... before translating them back to their original languages."
  - [corpus] Weak: no explicit corpus mention of paraphrase alignment; only general ASR/MLM context.

### Mechanism 3
- **Claim**: Parameter-efficient finetuning (adapter layers) allows AMPS to scale to low-resource languages without overfitting.
- **Mechanism**: Adapters are inserted after each encoder/decoder layer to project features into a reduced space, apply GeLU, then project back, enabling task-specific adaptation with far fewer trainable parameters.
- **Core assumption**: The bulk of the multimodal knowledge in SeamlessM4T is preserved when only adapter weights are updated.
- **Evidence anchors**:
  - [section 4] "Given the limited amount of training data, we use parameter-efficient finetuning of adapter layers (Houlsby et al., 2019) in the speech encoder and text decoder layers of the SeamlessM4T model"
  - [appendix B.1] "With D2 set to half of D1, this setup introduced 100M trainable parameters while keeping the rest of the model frozen."
  - [corpus] Weak: no corpus evidence on adapter efficacy for AMPS; only general ASR finetuning.

## Foundational Learning

- **Concept**: Multilingual multimodal ASR pretraining (e.g., SeamlessM4T)
  - Why needed here: Provides a strong speech encoder and shared text decoder that can be adapted for both ASR and paraphrasing tasks.
  - Quick check question: Does the base model already have a shared text decoder that accepts both speech and text encoder outputs?

- **Concept**: Round-trip translation for paraphrase generation
  - Why needed here: Generates semantically equivalent text with minimal syntactic change, preserving word order alignment.
  - Quick check question: Does the round-trip translation preserve the original sentence structure and word order?

- **Concept**: Threshold-based selective supervision
  - Why needed here: Focuses paraphrase training on hard utterances where ASR loss is high, preventing dilution of the main ASR objective.
  - Quick check question: How is the threshold τ chosen, and is it validated on a held-out set?

## Architecture Onboarding

- **Component map**:
  Speech encoder (Conformer layers) -> ASR loss
  Text encoder (Transformer) + text decoder (Transformer) -> paraphrase loss
  Adapters on each layer (speech encoder & text decoder) -> trainable parameters
  Threshold τ -> gating paraphrase loss

- **Critical path**:
  1. Compute LASR for each batch.
  2. If LASR > τ, compute LPAR and sum with LASR; else use LASR alone.
  3. Backpropagate through adapters only.
  4. Update model for next batch.

- **Design tradeoffs**:
  - **Low-resource vs. full fine-tuning**: Adapters reduce overfitting risk but may limit capacity; full fine-tuning could improve performance if data is sufficient.
  - **Paraphrase diversity vs. alignment**: More diverse paraphrases improve robustness but may break alignment; round-trip translation keeps alignment but may produce less varied paraphrases.
  - **Threshold τ tuning**: High τ reduces paraphrase influence; low τ risks over-relying on paraphrases for easy cases.

- **Failure signatures**:
  - WER improvement stalls or degrades when τ is too low (over-paraphrasing easy utterances).
  - No improvement on "hard" subset indicates paraphrases are too divergent or ASR loss threshold mis-set.
  - Training loss divergence if paraphrase generation produces noisy or misaligned text.

- **First 3 experiments**:
  1. Train baseline ASR-only on 50h mixed speech, measure WER.
  2. Train AMPS τ with round-trip paraphrases, sweep τ values, compare WER and ∆Hard.
  3. Compare paraphrase generation methods (round-trip vs. LLM) on a small dev set to assess quality and alignment.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does AMPS performance vary when applied to languages with significantly different morphological complexity compared to the tested languages (Hindi, Marathi, Malayalam, Kannada, Nyanja)?
  - Basis in paper: [inferred] The paper demonstrates AMPS effectiveness across five diverse languages but doesn't explore languages with extreme morphological differences.
  - Why unresolved: The current evaluation focuses on Indic languages and one Bantu language, all with moderate morphological complexity. The model's ability to handle vastly different morphological systems remains untested.
  - What evidence would resolve it: Testing AMPS on languages with extreme morphological differences (e.g., Turkish, Finnish, Chinese, Vietnamese) and comparing performance against baseline ASR systems.

- **Open Question 2**: What is the optimal threshold τ for AMPS across different languages and how can it be learned dynamically rather than set manually?
  - Basis in paper: [explicit] "The threshold value τ is manually defined and not a dynamic value that is learned across languages."
  - Why unresolved: The paper uses manually determined thresholds for each language setting, which may not generalize well and requires manual tuning for each new language or dataset.
  - What evidence would resolve it: Developing a method to learn τ dynamically during training, possibly through reinforcement learning or meta-learning approaches, and evaluating its performance across multiple languages and data conditions.

- **Open Question 3**: How does AMPS perform on atypical speech patterns such as non-native accents, speech impairments, or code-switching beyond the planned investigation mentioned in the limitations?
  - Basis in paper: [explicit] "Future work will investigate how techniques like AMPS could be used to improve ASR for atypical speech."
  - Why unresolved: The current study focuses on spontaneous speech in native languages without exploring how AMPS handles non-native speakers, speech impairments, or code-switching between languages.
  - What evidence would resolve it: Systematic evaluation of AMPS on datasets containing non-native speech, disordered speech, and code-switched utterances, measuring both WER and semantic preservation metrics.

## Limitations

- The effectiveness of selective paraphrase supervision depends heavily on the assumed correlation between ASR loss and utterance quality, which is not empirically validated.
- Round-trip translation for paraphrase generation may not produce sufficiently diverse paraphrases to handle all types of speech recognition errors.
- Parameter-efficient adapter approach may limit the model's capacity to fully adapt to low-resource languages, especially for languages very different from pretraining languages.

## Confidence

**High Confidence**: The claim that AMPS achieves relative WER reductions of up to 5% compared to strong baselines is well-supported by the reported experimental results across five languages.

**Medium Confidence**: The mechanism claims about how AMPS works (selective activation based on ASR loss, word order preservation, and adapter efficiency) are plausible but would benefit from additional empirical validation.

**Low Confidence**: The claim about AMPS being "particularly effective for utterances where standard ASR performs poorly" lacks detailed quantitative evidence regarding performance distribution across different utterance difficulty levels.

## Next Checks

1. **ASR Loss Correlation Validation**: Conduct a detailed analysis correlating ASR loss values with actual utterance difficulty metrics (e.g., background noise levels, speaker clarity scores, or human-rated difficulty) to empirically validate whether high ASR loss reliably indicates utterances that would benefit from paraphrase supervision.

2. **Paraphrase Diversity Analysis**: Compare the performance of AMPS using round-trip translation paraphrases versus more diverse paraphrase generation methods (such as LLM-based paraphrasing with controlled diversity) to quantify the tradeoff between word order preservation and paraphrase variety.

3. **Adapter vs. Full Fine-tuning Comparison**: Perform controlled experiments comparing AMPS with adapter-based fine-tuning against the same approach with full fine-tuning on a subset of the data to quantify the performance gap and determine whether the parameter efficiency comes at a significant cost in low-resource scenarios.