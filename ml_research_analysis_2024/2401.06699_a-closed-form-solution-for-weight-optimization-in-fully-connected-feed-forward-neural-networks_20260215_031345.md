---
ver: rpa2
title: A Closed-form Solution for Weight Optimization in Fully-connected Feed-forward
  Neural Networks
arxiv_id: '2401.06699'
source_url: https://arxiv.org/abs/2401.06699
tags:
- proposed
- layer
- weights
- network
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel closed-form approach for optimizing
  weights in fully-connected feed-forward neural networks. Unlike existing iterative
  back-propagation methods, the proposed solution optimizes weights in a single iteration
  using least squares methodology.
---

# A Closed-form Solution for Weight Optimization in Fully-connected Feed-forward Neural Networks

## Quick Facts
- arXiv ID: 2401.06699
- Source URL: https://arxiv.org/abs/2401.06699
- Reference count: 18
- Primary result: Novel closed-form least squares approach optimizes neural network weights in single iteration for injective mappings, achieving >1000x speedup over iterative methods while maintaining competitive accuracy on MNIST/Fashion-MNIST.

## Executive Summary
This paper proposes a novel closed-form approach for optimizing weights in fully-connected feed-forward neural networks using least squares methodology. Unlike traditional iterative back-propagation methods, the proposed solution optimizes weights in a single iteration for injective mappings by back-propagating through layers and solving local least squares problems for each neuron independently. The approach enables parallel implementation across neurons in the same layer and provides deterministic running time with exact computation counts. Experimental results demonstrate competitive accuracy on linear/non-linear toy examples and real-world datasets while achieving significant speed improvements over state-of-the-art methods like Adam, SGD, NAG, and AdaGrad.

## Method Summary
The method implements weight optimization by randomly initializing weights U[-1,1], performing a forward pass to compute all activations, then back-propagating layer-by-layer to solve least squares problems for each neuron. For each layer l from output to input, the algorithm constructs matrix A_l from neuron values of layer l-1 (or inputs for l=1) and vector b_l from desired outputs or back-propagated desired neuron values, then solves w_l = (A_l A_l^T)^(-1) A_l b_l^T for each neuron. For non-injective mappings like classification, the method iterates until convergence using only misclassified samples. The approach is parallelizable per layer, with all neurons in a layer optimized simultaneously via matrix operations.

## Key Results
- Achieves >1000x speedup over iterative methods like Adam, SGD, NAG, and AdaGrad on MNIST and Fashion-MNIST datasets
- Maintains competitive classification accuracy while optimizing weights in single iteration for injective mappings
- Provides deterministic running time with exact computation counts, unlike convergence-dependent iterative methods
- Enables parallel implementation across neurons within each layer, reducing wall-clock time significantly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Closed-form weight optimization via least squares enables deterministic, single-iteration training for injective mappings.
- Mechanism: For each neuron, the network peels back layer by layer in a back-propagating fashion, solving a linear least squares problem at each step to directly compute optimal weights without iterative gradient updates.
- Core assumption: The input-to-output mapping is injective, meaning the system of linear equations derived from the forward pass has a unique solution.
- Evidence anchors:
  - [abstract] "the proposed approach offers the solution for weight optimization in closed-form by means of least squares (LS) methodology"
  - [section II] "the proposed solution requires a single iteration to optimize the weights of each neuron without resorting to derivatives and its solution is given in a closed-form"
  - [corpus] Weak evidence: No direct comparison of runtime between closed-form and gradient methods in related works cited.
- Break condition: If the mapping is non-injective (e.g., classification), the method falls back to iterative refinement, losing the single-iteration guarantee.

### Mechanism 2
- Claim: Parallelizable computation across neurons in the same layer drastically reduces wall-clock time.
- Mechanism: Since weight optimization for each neuron in a layer depends only on local inputs and outputs (not on other neurons' weights), all neurons in a layer can be optimized simultaneously using matrix operations.
- Core assumption: The network architecture allows decoupling of neuron computations within a layer, which is true for fully-connected layers.
- Evidence anchors:
  - [abstract] "An important advantage over the existing solutions is that these computations (for all neurons in a layer) are independent from each other; thus, they can be carried out in parallel"
  - [section II] "the proposed scheme achieves weight optimization in a single iteration (per layer) and in just a few iterations for problems with injective and non-injective input-to-output mapping, respectively"
  - [corpus] Weak evidence: No mention of parallelization strategies in related works; assumption is structural.
- Break condition: If layer dependencies exist (e.g., residual connections, shared weights), parallelization is not possible.

### Mechanism 3
- Claim: Deterministic running time allows exact prediction of computational cost.
- Mechanism: The total number of operations is fixed by network dimensions (Nl × (L + 1)), unlike iterative methods where convergence time is unknown.
- Core assumption: All matrix inversions and multiplications are performed exactly without early stopping or adaptive iteration counts.
- Evidence anchors:
  - [abstract] "its running time is deterministic in the sense that one can obtain the exact number of computations necessary to optimize the weights in all network layers"
  - [section II] "its running time is deterministic in the sense that one can obtain the exact number of computations necessary to optimize the weights in all network layers (per iteration, in the case of non-injective mapping)"
  - [corpus] No evidence in related works about deterministic complexity guarantees.
- Break condition: If numerical instability requires iterative refinement or if hardware limitations introduce variable execution times.

## Foundational Learning

- Concept: Least squares solution for overdetermined systems
  - Why needed here: The method solves for weights by minimizing the squared error between predicted and desired outputs at each neuron, which is a classic least squares problem.
  - Quick check question: Given A ∈ ℝ^{m×n} with m > n and b ∈ ℝ^m, what is the closed-form solution for minimizing ‖Ax - b‖²?

- Concept: Back-propagation as layer peeling
  - Why needed here: The method conceptually reverses the standard forward pass, using outputs to infer inputs layer by layer, enabling localized least squares solutions.
  - Quick check question: In a network with L hidden layers, if you know the desired output and the weights of the last layer, how do you compute the desired hidden activations of the previous layer?

- Concept: Matrix inversion and multiplication complexity
  - Why needed here: The core computation involves (A^T A)^(-1) A^T b for each neuron, so understanding the cost of these operations is critical for runtime analysis.
  - Quick check question: For an n × n matrix, what is the asymptotic complexity of computing its inverse using standard methods?

## Architecture Onboarding

- Component map: Random initialization -> Forward pass -> Layer-wise least squares optimization -> Parallel matrix operations -> (Optional) Iterative refinement for non-injective cases

- Critical path:
  1. Random initialization of all weights U[-1,1]
  2. Single forward pass to compute all activations
  3. For l = L+1 down to 1: Solve least squares for all neurons in layer l
  4. If non-injective: Iterate using misclassified samples until convergence

- Design tradeoffs:
  - Speed vs. accuracy: Closed-form solution is fast but may not generalize as well as iterative methods for complex, non-convex problems
  - Determinism vs. flexibility: Fixed computation count vs. adaptive convergence
  - Parallelism vs. memory: Simultaneous matrix operations require more memory but reduce wall-clock time

- Failure signatures:
  - Poor generalization on test data despite good training performance (overfitting)
  - Numerical instability when (A^T A) is ill-conditioned (near-singular)
  - Excessive iterations in non-injective case indicating the method is struggling with the problem structure

- First 3 experiments:
  1. Linear regression toy problem (injective): Verify single-iteration convergence matches analytical solution
  2. XOR classification (non-injective): Test iterative refinement with few epochs
  3. MNIST with softmax output: Benchmark against Adam/SGD in terms of accuracy and runtime

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the closed-form weight optimization approach be extended to neural networks with activation functions beyond the ones considered (e.g., ReLU, Leaky ReLU, etc.)?
- Basis in paper: [explicit] The paper mentions that the approach works for networks with injective activation functions but does not explore other types of activation functions.
- Why unresolved: The paper does not provide experimental results or theoretical analysis for activation functions other than sigmoid, hyperbolic tangent, exponential linear unit, softmax, softplus, and softminus.
- What evidence would resolve it: Experiments comparing the performance of the closed-form approach with various activation functions (e.g., ReLU, Leaky ReLU, etc.) on different datasets would provide evidence for the generalizability of the approach.

### Open Question 2
- Question: How does the proposed approach scale to very deep neural networks (e.g., networks with hundreds or thousands of layers)?
- Basis in paper: [inferred] The paper only considers a network with one hidden layer for the real-world experiments, and the running time analysis is only provided for a network with a few layers.
- Why unresolved: The paper does not provide any analysis or experiments on the scalability of the approach to very deep networks.
- What evidence would resolve it: Experiments comparing the performance and running time of the proposed approach with existing methods on very deep neural networks (e.g., ResNet, VGG, etc.) would provide evidence for the scalability of the approach.

### Open Question 3
- Question: Can the closed-form approach be adapted to handle other types of neural network architectures, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs)?
- Basis in paper: [inferred] The paper only considers fully-connected feed-forward neural networks and does not explore other types of architectures.
- Why unresolved: The paper does not provide any analysis or experiments on the applicability of the approach to other types of neural network architectures.
- What evidence would resolve it: Experiments comparing the performance of the proposed approach with existing methods on other types of neural network architectures (e.g., CNNs, RNNs) would provide evidence for the generalizability of the approach.

## Limitations
- The method's performance on deeper networks (beyond 2-3 layers) remains unclear due to potential numerical instability and precision issues with matrix inversions
- The claimed "1000x speedup" comparison may be misleading as it doesn't account for convergence checks and adaptive learning rates in iterative methods
- Iterative refinement for non-injective mappings may significantly reduce the theoretical speed advantage, though exact iteration counts are not provided

## Confidence

- **High confidence**: The mathematical framework for single-layer least squares optimization is sound and well-established. The parallelizability claim for fully-connected layers is structurally valid.
- **Medium confidence**: The deterministic running time calculation appears correct for fixed network architectures, but real-world performance depends on numerical stability and hardware considerations.
- **Low confidence**: The generalization performance on complex real-world datasets and the claimed speedup factor compared to optimized iterative methods need independent verification.

## Next Checks

1. Implement the method on a synthetic linear regression problem with known analytical solution to verify the closed-form weights match expected values exactly.
2. Test numerical stability by measuring condition numbers of A^T A matrices across layers and network depths, particularly for MNIST-scale problems.
3. Benchmark wall-clock time on a single layer optimization problem against standard least squares solvers to isolate the claimed efficiency gain from network effects.