---
ver: rpa2
title: 'Leveraging Lecture Content for Improved Feedback: Explorations with GPT-4
  and Retrieval Augmented Generation'
arxiv_id: '2405.06681'
source_url: https://arxiv.org/abs/2405.06681
tags:
- feedback
- lecture
- students
- information
- programming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the use of Retrieval Augmented Generation (RAG)
  to improve the feedback generated by Large Language Models for programming tasks.
  For this purpose, corresponding lecture recordings were transcribed and made available
  to the Large Language Model GPT-4 as external knowledge source together with timestamps
  as metainformation by using RAG.
---

# Leveraging Lecture Content for Improved Feedback: Explorations with GPT-4 and Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2405.06681
- Source URL: https://arxiv.org/abs/2405.06681
- Reference count: 29
- Primary result: Students preferred RAG-enhanced feedback in some situations, but slower generation speed made benefits context-dependent

## Executive Summary
This paper explores using Retrieval Augmented Generation (RAG) to improve Large Language Model feedback for programming tasks by incorporating lecture content. The authors developed a system that transcribes lecture videos, creates searchable chunks with timestamps, and uses these as external knowledge sources for GPT-4 when generating feedback. The system employs a two-run prompt chain where GPT-4 first identifies missing concepts and generates retrieval queries, then produces feedback with links to relevant lecture segments. Students could request feedback through an exercise platform and choose whether to include RAG augmentation. The evaluation with 10 workshop participants showed that RAG-enhanced feedback was preferred in some scenarios, particularly for linking to lecture content, though the slower generation time (18 seconds vs 1-2 seconds) made the benefits situationally dependent.

## Method Summary
The authors implemented a two-run GPT-4 prompt chain for generating programming feedback. In the first run, GPT-4 analyzes student code, compiler output, and unit test results to identify missing concepts and generate retrieval queries. These queries are used to retrieve relevant lecture chunks from a PostgreSQL database with pgvector indexing, where lecture videos have been transcribed using Whisper, chunked into 512-character segments with 64-character overlap, and embedded using text-embedding-ada-002. The second GPT-4 run generates feedback using the retrieved chunks, incorporating timestamp metadata as Markdown footnotes that link directly to specific video positions. Students could choose between feedback with or without RAG augmentation through an exercise platform, with their preferences and usage data collected for evaluation.

## Key Results
- Students preferred RAG-enhanced feedback when they wanted to review lecture content, but not when they needed quick responses
- Feedback generation with RAG took approximately 18 seconds versus 1-2 seconds without RAG
- The system successfully linked feedback to specific lecture video timestamps through Markdown footnotes
- Student preferences were context-dependent, with RAG being more valuable for conceptual understanding than for immediate debugging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-Augmented Generation reduces hallucination by grounding GPT-4 responses in actual lecture content
- Mechanism: RAG indexes transcribed lecture segments as vectors and retrieves the most relevant chunks based on similarity to automatically generated queries from student context
- Core assumption: Lecture content embeddings accurately capture the semantic meaning needed to answer programming feedback queries
- Evidence anchors:
  - [abstract] "The purpose of this is to prevent hallucinations and to enforce the use of the technical terms and phrases from the lecture"
  - [section] "The purpose of this is to prevent hallucinations and to enforce the use of the technical terms and phrases from the lecture"
  - [corpus] Weak - related papers discuss RAG in education but don't directly validate hallucination reduction

### Mechanism 2
- Claim: The two-run prompt chain effectively generates relevant retrieval queries from student context
- Mechanism: First run identifies missing concepts and formulates specific questions that serve as queries for retrieval; second run generates feedback using retrieved lecture chunks
- Core assumption: GPT-4 can accurately identify missing concepts from student code, compiler output, and unit test results
- Evidence anchors:
  - [abstract] "For each concept (e.g. recursion), the LLM formulates a simple question (e.g. 'How does recursion work in Python?')"
  - [section] "Similarly to the ReAct logic, the first run identifies X missing concepts for a correct solution based on the available student context information"
  - [corpus] Weak - related papers use different RAG approaches but don't validate this specific prompt chain mechanism

### Mechanism 3
- Claim: Timestamp-linked video segments enhance learning by providing immediate access to visual lecture content
- Mechanism: Retrieved chunks include timestamp metadata that generates Markdown footnotes linking directly to specific video positions
- Core assumption: Students benefit from accessing exact lecture moments rather than generic explanations
- Evidence anchors:
  - [abstract] "In this way, the corresponding lecture videos can be viewed immediately at the corresponding positions"
  - [section] "By linking the lecture recording to the corresponding timestamp, students are able to perceive the associated visual elements"
  - [corpus] Weak - related papers discuss video interaction but don't validate timestamp-specific linking for feedback

## Foundational Learning

- Concept: Vector embeddings and similarity search
  - Why needed here: The system uses embedding models to convert lecture chunks into vectors for similarity-based retrieval
  - Quick check question: What embedding model is used to create vector representations of lecture chunks?

- Concept: Prompt engineering with function calling
  - Why needed here: The system uses structured prompts with function calling to generate retrieval queries in the first run
  - Quick check question: What is the maximum number of queries generated per feedback in the first run?

- Concept: Markdown footnote generation
  - Why needed here: The system converts timestamp metadata into clickable links that appear as footnotes in feedback
  - Quick check question: What format is used to display video links in the generated feedback?

## Architecture Onboarding

- Component map:
  - Whisper speech recognition → .SRT transcription
  - Chunking module → 512-char segments with 64-char overlap
  - Embedding model (text-embedding-ada-002) → vector representations
  - PostgreSQL with pgvector → vector database
  - GPT-4 (two runs) → query generation and feedback synthesis
  - Markdown parser → footnote formatting
  - Video player modal → timestamp-linked content

- Critical path:
  1. Student submits code and context
  2. First GPT-4 run identifies concepts and generates queries
  3. RAG retrieves top 4 chunks per query (max 8 total)
  4. Second GPT-4 run generates feedback with footnote links
  5. Student receives feedback with clickable video timestamps

- Design tradeoffs:
  - Slower feedback (18 seconds vs 1-2 seconds) for lecture linkage vs. speed without RAG
  - Uniform chunking (512 chars) vs. semantic chunking strategies
  - Limited queries (max 2 per feedback) vs. more comprehensive coverage

- Failure signatures:
  - Empty or irrelevant retrieved chunks → feedback lacks lecture references
  - GPT-4 timeouts during either run → system hangs
  - Broken video links → footnote references don't open correct positions
  - Slow response times → students abandon feedback requests

- First 3 experiments:
  1. Test RAG retrieval with known concepts to verify chunk relevance
  2. Measure latency difference between feedback with and without RAG
  3. Validate footnote generation by checking Markdown parsing and video positioning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do students utilize the lecture video links provided in the feedback? Specifically, how much time do they spend watching each linked video segment?
- Basis in paper: [inferred] The authors mention that future studies should record how long each video was watched per feedback to investigate the extent to which the linked videos were used.
- Why unresolved: The current study did not track the time students spent watching the linked video segments.
- What evidence would resolve it: Data on the duration of video playback for each linked segment, aggregated across all students and feedback instances.

### Open Question 2
- Question: How does the effectiveness of RAG-enhanced feedback compare to feedback without RAG in terms of student learning outcomes and problem-solving skills?
- Basis in paper: [explicit] The authors note that knowledge and skill acquisition were not the subject of the current evaluation, and that the study only focused on student preferences for feedback types.
- Why unresolved: The study did not measure the impact of RAG-enhanced feedback on student learning or problem-solving abilities.
- What evidence would resolve it: Pre- and post-tests to assess knowledge gains, and analysis of student code quality and problem-solving approaches before and after using RAG-enhanced feedback.

### Open Question 3
- Question: How can the chunking strategy for lecture transcripts be improved to provide more relevant and concise information to the LLM for feedback generation?
- Basis in paper: [explicit] The authors mention that the current chunking strategy, based on character count, could be improved by semantic chunking strategies or the use of a knowledge graph.
- Why unresolved: The study used a simple character-based chunking approach, and the authors acknowledge that more sophisticated strategies could be beneficial.
- What evidence would resolve it: Comparative analysis of feedback quality and relevance using different chunking strategies, such as semantic chunking or knowledge graph-based approaches, against the current character-based method.

## Limitations
- Small sample size of 10 workshop participants limits generalizability of findings
- No objective measures of learning outcomes or problem-solving success rates
- Technical implementation details remain partially unspecified, particularly prompt templates
- Limited evaluation scope focused on student preferences rather than actual learning impact

## Confidence
- High Confidence: The basic technical feasibility of using RAG to retrieve and integrate lecture content into GPT-4 feedback generation
- Medium Confidence: The claim that RAG improves feedback quality in terms of reducing hallucinations and providing more relevant references
- Low Confidence: The assertion that timestamp-linked video segments significantly enhance learning outcomes

## Next Checks
1. Conduct a controlled study comparing student performance and problem-solving success rates when using feedback with and without RAG augmentation
2. Systematically evaluate the relevance and usefulness of retrieved lecture chunks across diverse programming problems
3. Test the two-run GPT-4 prompt chain with broader programming languages and problem types to verify effectiveness outside the specific Python context