---
ver: rpa2
title: A Chatbot for Asylum-Seeking Migrants in Europe
arxiv_id: '2407.09197'
source_url: https://arxiv.org/abs/2407.09197
tags:
- user
- https
- argumentation
- chatbot
- protection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ACME, a chatbot for asylum-seeking migrants
  in Europe that uses computational argumentation to help identify the highest level
  of protection an applicant can receive. The system addresses the challenge that
  migrants often struggle to navigate complex asylum regulations and may not know
  which protection type they qualify for.
---

# A Chatbot for Asylum-Seeking Migrants in Europe

## Quick Facts
- arXiv ID: 2407.09197
- Source URL: https://arxiv.org/abs/2407.09197
- Reference count: 27
- Primary result: ACME uses neuro-symbolic architecture with argumentation frameworks to help asylum seekers identify eligible protection levels while preserving privacy

## Executive Summary
This paper presents ACME, a chatbot designed to assist asylum-seeking migrants in Europe by helping them identify the highest level of protection they can receive. The system addresses the challenge that migrants often struggle to navigate complex asylum regulations and may not know which protection type they qualify for. ACME employs a neuro-symbolic architecture combining natural language processing with an argumentation framework to dynamically evaluate user-provided facts and determine consistent protection levels.

The chatbot interacts with users through a standard chat interface, using multiple NLU methods including sentence embeddings and a large language model to map user responses to knowledge base concepts. The approach aims to reduce burden on territorial commissions and courts while preserving privacy through its modular design. The paper demonstrates the system using a case study of Nigerian migrants seeking protection in Italy, considering refugee/subsidiary protection and special humanitarian protection categories.

## Method Summary
ACME uses a neuro-symbolic architecture that separates natural language understanding (neural module) from reasoning (symbolic module) to balance user privacy with transparent asylum decision reasoning. The system employs three parallel NLU approaches - direct yes/no matching, sentence embedding similarity, and LLM-based concept mapping - to identify knowledge base concepts from user input. An argumentation framework models the relationships between protection statuses, their requirements, and user facts, distinguishing between "consistent" and "potentially consistent" protection recommendations. The knowledge base encodes different protection statuses as reply arguments and their requirements as status arguments, with the system dynamically evaluating user-provided facts to determine defensible protection levels.

## Key Results
- ACME successfully demonstrates a neuro-symbolic chatbot architecture for asylum seekers
- The system distinguishes between consistent and potentially consistent protection replies
- Privacy is preserved through modular design separating NLU from reasoning components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The neuro-symbolic architecture allows the system to balance user privacy with transparency in asylum decision reasoning.
- Mechanism: The architecture separates natural language understanding (neural module) from reasoning (symbolic module), with the symbolic module only accessing KB concepts mapped from user input. This prevents irrelevant personal data from reaching the reasoning layer while maintaining interpretable argumentation-based explanations.
- Core assumption: The natural language module can accurately map user responses to KB concepts without losing critical information needed for protection determination.
- Evidence anchors:
  - [abstract] "The architecture design is focused on decoupling between the natural language module and the argumentation module"
  - [section] "Any other irrelevant information provided by the user is discarded, guaranteeing their privacy"
  - [corpus] Weak evidence - no corpus papers directly address this specific neuro-symbolic privacy-interpretation tradeoff

### Mechanism 2
- Claim: The argumentation framework ensures only defensible protection recommendations are provided to users.
- Mechanism: The system distinguishes between "consistent" and "potentially consistent" replies, where consistent replies cannot be defeated by future information and can be given immediately, while potentially consistent replies require further fact-checking before being recommended.
- Core assumption: The argumentation framework can accurately model the complex dependencies and incompatibilities between different protection statuses and their requirements.
- Evidence anchors:
  - [section] "Our system distinguishes between consistent and potentially consistent reply"
  - [section] "the system works to provide only robust replies, possibly delaying replies that need further fact-checking"
  - [corpus] No direct corpus evidence found for this specific argumentation-based reply consistency mechanism

### Mechanism 3
- Claim: Multiple NLU methods improve the accuracy of mapping user responses to knowledge base concepts.
- Mechanism: The system employs three parallel NLU approaches: direct yes/no matching, sentence embedding similarity, and LLM-based concept mapping, providing redundancy and coverage for different response types.
- Core assumption: Different response types (direct answers, descriptive answers, complex explanations) will be captured by at least one of the three NLU methods.
- Evidence anchors:
  - [section] "the matching of the user answer with KB nodes is performed in several ways"
  - [section] "If all three methods do not find any corresponding node, the chatbot asks the user for clarification"
  - [corpus] No corpus evidence found specifically addressing this three-method NLU approach for argumentation chatbots

## Foundational Learning

- Concept: Abstract Argumentation Frameworks
  - Why needed here: ACME uses an argumentation framework to model the relationships between protection statuses, their requirements, and the user's facts
  - Quick check question: What are the key components of an abstract argumentation framework and how do they relate to each other?

- Concept: Sentence Embeddings and Similarity
  - Why needed here: The system uses sentence transformers to compare user input with KB concept descriptions to find matches
  - Quick check question: How do sentence embeddings enable semantic similarity comparison between user responses and KB concepts?

- Concept: Privacy-Preserving System Design
  - Why needed here: The architecture must protect sensitive asylum seeker information while still providing accurate protection guidance
  - Quick check question: What are the key principles of privacy-preserving AI system design and how do they apply to ACME's neuro-symbolic approach?

## Architecture Onboarding

- Component map: User Interface → Natural Language Module → Argumentation Module ← Knowledge Base
- Critical path: User input → NLU mapping → KB concept activation → Argumentation reasoning → Reply selection → User output
- Design tradeoffs: The neuro-symbolic approach trades computational efficiency for privacy and interpretability; multiple NLU methods increase coverage but add complexity
- Failure signatures: System gets stuck asking for clarification repeatedly (NLU mapping failure), provides inconsistent protection recommendations (argumentation logic error), or exposes irrelevant user data (architecture breach)
- First 3 experiments:
  1. Test NLU mapping with controlled inputs across different protection scenarios to verify concept mapping accuracy
  2. Validate argumentation reasoning by providing complete fact sets and verifying correct protection determination
  3. Verify privacy boundary by attempting to access non-mapped user information through the system interface

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is ACME at accurately identifying the highest level of protection for asylum seekers compared to expert legal assessment?
- Basis in paper: [explicit] The paper describes ACME's architecture and case study but does not provide validation results against expert assessments or actual outcomes of asylum applications.
- Why unresolved: The authors state they plan to validate ACME with legal experts in the future, but no empirical validation data is presented in this paper.
- What evidence would resolve it: Comparative studies showing ACME's recommendations versus expert legal opinions on actual asylum cases, including accuracy rates and false positive/negative rates.

### Open Question 2
- Question: What is the optimal balance between using different NLU methods (sentence embeddings, LLM, direct matching) for mapping user responses to knowledge base concepts?
- Basis in paper: [explicit] The paper describes three different NLU methods but does not provide comparative performance data or explain how they complement each other.
- Why unresolved: The authors mention using multiple methods but don't discuss their relative effectiveness, error rates, or how the system decides which method to prioritize.
- What evidence would resolve it: Systematic evaluation comparing accuracy, response time, and resource usage of each NLU method individually and in combination, with user response scenarios.

### Open Question 3
- Question: How does ACME handle complex cases where applicants may be eligible for multiple protection types with conflicting requirements?
- Basis in paper: [inferred] The argumentation framework is described as handling attacks between status arguments, but the paper doesn't demonstrate how this works in practice for complex eligibility scenarios.
- Why unresolved: The case study focuses on a relatively straightforward scenario, and the paper doesn't address edge cases, conflicting information, or situations where multiple protection types might apply simultaneously.
- What evidence would resolve it: Detailed analysis of ACME's performance on complex, ambiguous cases with conflicting requirements, including how it resolves conflicts and presents recommendations to users.

## Limitations

- No empirical validation data demonstrating accuracy across diverse asylum cases
- Privacy-preservation mechanism lacks formal security analysis and error rate specifications
- Multiple NLU method approach introduces complexity without performance comparison data

## Confidence

**High Confidence**: The architectural design principles (neuro-symbolic separation, argumentation framework for reasoning, privacy-preserving modularity) are well-established in AI literature and logically sound for this application domain.

**Medium Confidence**: The mechanism by which the argumentation framework distinguishes between "consistent" and "potentially consistent" replies appears technically feasible, though the specific implementation details and legal reasoning accuracy are not demonstrated.

**Low Confidence**: The effectiveness of the three parallel NLU methods in practice, the accuracy of protection determination across diverse migrant cases, and the real-world impact on reducing territorial commission burden remain unverified claims.

## Next Checks

1. **NLU Mapping Accuracy Test**: Conduct controlled experiments with 50-100 diverse asylum seeker responses across different protection scenarios to measure the accuracy of each NLU method (yes/no matching, sentence embeddings, LLM mapping) in correctly identifying KB concepts.

2. **Argumentation Framework Validation**: Create a comprehensive test suite of complete asylum cases with known protection outcomes, then verify that the system correctly identifies consistent protection levels without requiring unnecessary fact-checking delays.

3. **Privacy Boundary Verification**: Perform systematic testing to confirm that the NLU module cannot inadvertently expose irrelevant user information through the argumentation module, including adversarial testing with sensitive but irrelevant data.