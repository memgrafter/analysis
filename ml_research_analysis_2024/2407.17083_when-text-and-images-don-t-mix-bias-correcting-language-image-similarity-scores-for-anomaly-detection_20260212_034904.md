---
ver: rpa2
title: 'When Text and Images Don''t Mix: Bias-Correcting Language-Image Similarity
  Scores for Anomaly Detection'
arxiv_id: '2407.17083'
source_url: https://arxiv.org/abs/2407.17083
tags:
- text
- class
- normal
- anomaly
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CLIP\u2019s image-text alignment assumption breaks down due to\
  \ a \u201Ctext clustering effect,\u201D where text embeddings are tightly grouped\
  \ regardless of semantic content, inducing a \u201Csimilarity bias\u201D in anomaly\
  \ scoring. BLISS corrects this bias by incorporating an external text score that\
  \ measures similarity to a general dictionary of text embeddings, alongside an internal\
  \ class score."
---

# When Text and Images Don't Mix: Bias-Correcting Language-Image Similarity Scores for Anomaly Detection

## Quick Facts
- arXiv ID: 2407.17083
- Source URL: https://arxiv.org/abs/2407.17083
- Authors: Adam Goodge; Bryan Hooi; Wee Siong Ng
- Reference count: 37
- Primary result: BLISS achieves up to 99.1 AUROC on CIFAR-10, outperforming baselines by up to 1.3% points

## Executive Summary
CLIP's image-text alignment assumption breaks down due to a "text clustering effect," where text embeddings are tightly grouped regardless of semantic content, inducing a "similarity bias" in anomaly scoring. BLISS corrects this bias by incorporating an external text score that measures similarity to a general dictionary of text embeddings, alongside an internal class score. The method uses only pre-trained CLIP and requires no training, making it efficient. On CIFAR-10, CIFAR-100, and TinyImageNet, BLISS achieves up to 99.1 AUROC (vs. ~98 for best baselines), with robust performance even with few normal samples and across different VLMs.

## Method Summary
BLISS addresses anomaly detection by correcting a "similarity bias" in CLIP-based scoring. It computes two scores for each test image: an internal class score (normalized similarity to normal class labels) and an external text score (normalized similarity to top-K dictionary entries). These scores are combined with a weighting parameter λ to produce the final anomaly score. The method requires no training, using only pre-trained CLIP and a dictionary of general text embeddings.

## Key Results
- Achieves 99.1 AUROC on CIFAR-10, 95.7 AUROC on CIFAR-100, and 95.6 AUROC on TinyImageNet
- Outperforms best baselines by up to 1.3% points on CIFAR-10
- Maintains performance even with few labeled normal samples (10 per class)
- Robust across different VLMs including CLIP and BLIP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP's text embeddings cluster tightly regardless of semantic content, violating the contrastive training objective.
- Mechanism: During training, CLIP aligns image-text pairs, but text embeddings converge to a narrow manifold in latent space, making them more similar to each other than to their paired images.
- Core assumption: The model's contrastive objective is sufficient to maintain semantic separation between text embeddings of unrelated concepts.
- Evidence anchors:
  - [abstract] "our empirical experiments show that the embeddings of text inputs unexpectedly tightly cluster together, far away from image embeddings"
  - [section] "We plot the t-SNE projections… All text embeddings are highly clustered together (they all visually overlap…)."

### Mechanism 2
- Claim: The text clustering effect induces similarity bias in anomaly detection by inflating similarity scores between normal class labels and unrelated text.
- Mechanism: Because class label text embeddings are similar to unrelated text embeddings, images that are similar to those unrelated text embeddings (including anomaly class labels) are incorrectly scored as normal.
- Evidence anchors:
  - [abstract] "this phenomenon induces a 'similarity bias' - in which false negative and false positive errors occur due to bias in the similarities between images and the normal label text embeddings"
  - [section] "false negative errors… are much more frequent amongst images with high average dictionary similarity… false positive errors… are much more frequent in images with low average dictionary similarity"

### Mechanism 3
- Claim: BLISS corrects similarity bias by combining an internal class score (normalized similarity to normal class labels) with an external text score (normalized similarity to general dictionary embeddings).
- Mechanism: The external text score measures how similar a test image is to general text embeddings, correcting for the learned notion of normality captured in µi(d*) and σi(d*). This reduces false positives/negatives caused by the text clustering effect.
- Evidence anchors:
  - [abstract] "BLISS corrects this bias by incorporating an external text score that measures similarity to a general dictionary of text embeddings"
  - [section] "The external text score corrects this by 'subtracting away' this bias, leaving behind a more meaningful view of the similarity between an image and the normal class labels"

## Foundational Learning

- Concept: Contrastive learning and its training objective
  - Why needed here: Understanding how CLIP's training objective should align image-text pairs but fails for text clustering.
  - Quick check question: What does the contrastive loss in CLIP maximize, and why does this fail to separate unrelated text embeddings?

- Concept: Cosine similarity and embedding normalization
  - Why needed here: BLISS relies on cosine similarity between normalized embeddings and requires understanding how normalization affects similarity distributions.
  - Quick check question: Why are CLIP embeddings normalized to the unit hypersphere, and how does this impact the interpretation of similarity scores?

- Concept: Statistical normalization (z-score) of similarity scores
  - Why needed here: Both internal class and external text scores are normalized using mean and standard deviation from labeled normal samples to account for learned normality.
  - Quick check question: How does z-score normalization of similarity scores help in distinguishing normal from anomalous samples?

## Architecture Onboarding

- Component map:
  CLIP image encoder (I) -> CLIP text encoder (T) -> Memory bank (Z^(train)) -> Internal class scorer -> External text scorer -> BLISS aggregator

- Critical path:
  1. Precompute normal image embeddings and store in memory bank
  2. For each test image: embed with I, find top-K dictionary matches, compute both scores
  3. Combine scores and take minimum across classes for final anomaly score

- Design tradeoffs:
  - Fixed CLIP backbone vs. fine-tuning: avoids expensive training but limits adaptation
  - Dictionary size vs. computation: larger dictionaries improve correction but increase cost
  - K value in top-K selection: balances specificity and robustness of external text score

- Failure signatures:
  - If normal and anomaly classes are semantically very close, BLISS may struggle
  - If the dictionary is too small or biased, external text score becomes uninformative
  - If λ is set too high/low, one component dominates and degrades performance

- First 3 experiments:
  1. Reproduce t-SNE plot of text vs. image embeddings to verify text clustering effect
  2. Compute similarity distributions of normal class labels to dictionary vs. images
  3. Vary λ in BLISS and measure impact on AUROC to find optimal weighting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the text clustering effect observed in CLIP unique to CLIP, or is it a common phenomenon in other vision-language models?
- Basis in paper: [explicit] The paper mentions that similar plots in the supplementary material with embeddings from another VLM, BLIP, show that the text clustering effect is not unique to CLIP but a common feature of vision-language modeling.
- Why unresolved: The paper only briefly mentions the observation with BLIP and does not provide a comprehensive analysis of the text clustering effect across different VLMs.
- What evidence would resolve it: A systematic study comparing the text clustering effect across multiple VLMs, including different architectures and training objectives, would provide a clearer understanding of the prevalence and causes of this phenomenon.

### Open Question 2
- Question: How does the choice of prompt template for text inputs affect the text clustering effect and subsequent anomaly detection performance?
- Basis in paper: [explicit] The paper mentions that they try different prompt formulations in the supplementary material and find little effect on performance.
- Why unresolved: The paper only briefly mentions the experiment with different prompt formulations and does not provide a detailed analysis of how the choice of prompt template influences the text clustering effect and anomaly detection results.
- What evidence would resolve it: A thorough investigation of the impact of different prompt templates on the text clustering effect and anomaly detection performance, potentially including a sensitivity analysis, would provide insights into the robustness of the methodology to prompt engineering.

### Open Question 3
- Question: Can the similarity bias be mitigated by modifying the training objective or architecture of CLIP, rather than relying on post-hoc correction methods like BLISS?
- Basis in paper: [inferred] The paper identifies the text clustering effect as a consequence of CLIP's contrastive training objective and proposes BLISS as a solution to address the induced similarity bias. This implies that modifying the training process could potentially prevent the text clustering effect from occurring in the first place.
- Why unresolved: The paper focuses on addressing the similarity bias through the proposed BLISS methodology and does not explore alternative approaches to prevent the text clustering effect during training.
- What evidence would resolve it: Developing and evaluating alternative training objectives or architectural modifications for CLIP that aim to prevent the text clustering effect would provide insights into whether the similarity bias can be mitigated at the source rather than through post-hoc correction.

## Limitations
- Exact prompt formulations used for class labels and dictionary entries are not fully specified
- Text clustering effect severity may vary across different CLIP model variants or training configurations
- Dictionary selection strategy (ImageNet classes) may not generalize optimally to all domains

## Confidence

- High confidence in the empirical observation of text clustering and its impact on similarity bias
- Medium confidence in the general mechanism of BLISS correction and its effectiveness across benchmarks
- Low confidence in optimal parameter choices (λ = 0.75, K selection) without systematic sensitivity analysis

## Next Checks

1. Test BLISS with alternative dictionary sources (e.g., WordNet, Wikipedia titles) to assess robustness to dictionary selection
2. Evaluate performance across different CLIP model sizes (ViT-L/14 vs ViT-B/16) to determine if text clustering severity varies with model scale
3. Conduct ablation studies systematically varying λ and K parameters to establish optimal configurations for different dataset characteristics