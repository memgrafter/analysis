---
ver: rpa2
title: Integrating Visual and Textual Inputs for Searching Large-Scale Map Collections
  with CLIP
arxiv_id: '2410.01190'
source_url: https://arxiv.org/abs/2410.01190
tags:
- search
- maps
- collections
- clip
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores the application of multimodal machine learning
  to search and discovery for large-scale map collections. Using 562,842 images of
  maps from the Library of Congress, the authors employ the CLIP model to generate
  embeddings and develop three search strategies: natural language, reverse image
  search, and multimodal search combining both text and image inputs.'
---

# Integrating Visual and Textual Inputs for Searching Large-Scale Map Collections with CLIP

## Quick Facts
- arXiv ID: 2410.01190
- Source URL: https://arxiv.org/abs/2410.01190
- Reference count: 40
- Primary result: CLIP-based multimodal search enables sub-second retrieval across 562,842 map images using text, image, or combined queries.

## Executive Summary
This paper explores the application of multimodal machine learning to search and discovery for large-scale map collections. Using 562,842 images of maps from the Library of Congress, the authors employ the CLIP model to generate embeddings and develop three search strategies: natural language, reverse image search, and multimodal search combining both text and image inputs. The implementation enables searching half a million images in under a second on a consumer GPU. The results demonstrate CLIP's effectiveness in retrieving relevant maps based on descriptive queries, with image-based searches outperforming text-based ones. The authors also introduce a dataset of 10,504 map-caption pairs and code for fine-tuning CLIP. This work advances the use of multimodal AI for cultural heritage collections, offering scalable, interactive exploration beyond traditional metadata-based search.

## Method Summary
The authors built a multimodal search system for 562,842 map images from the Library of Congress using CLIP embeddings. They generated embeddings for all images using a ViT base model with 2000px width and 32x32 patches, stored in a beto tensor for fast retrieval. Three search modes were implemented: text-only (query embedding vs image embeddings), image-only (image embedding vs image embeddings), and multimodal (weighted combination of text and image embeddings with scaling factor α). A fine-tuning dataset of 10,504 map-caption pairs was created from loc.gov metadata, and experiments tested various batch sizes and learning rates, though results were mixed. The system achieves sub-second search latency on a consumer GPU.

## Key Results
- CLIP embeddings enable sub-second retrieval across 562,842 map images
- Image-based searches outperform text-based searches in retrieval quality
- Multimodal search with weighted combination of text and image inputs allows interactive refinement
- Joint search with α=0 (text-only) and α=1 (image-only) modes validated as expected

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP's shared embedding space enables unified search across text and image inputs.
- Mechanism: CLIP maps both images and text into a common 512-dimensional embedding space using a vision transformer and a text transformer. This alignment allows cosine similarity to directly compare text queries and map images without modality-specific handling.
- Core assumption: The contrastive pre-training objective aligns visual and textual semantics sufficiently for retrieval in the cultural heritage domain.
- Evidence anchors:
  - [abstract] "CLIP and other multimodal approaches have seen increasing adoption in the computational humanities community, showing great promise for use with digital collections"
  - [section 3.2] "CLIP embeds images and text in a common embedding space"
- Break condition: If map content semantics are too domain-specific, CLIP's general pre-training may fail to capture fine-grained visual cues, leading to poor similarity scores.

### Mechanism 2
- Claim: Joint text-and-image search allows interactive refinement of search intent.
- Mechanism: The search system computes a weighted combination of text and image embeddings using a scaling factor α, creating a centroid in embedding space. This enables the user to shift emphasis between visual and textual guidance dynamically.
- Core assumption: Weighted combination of embeddings in the shared space preserves semantic coherence for retrieval.
- Evidence anchors:
  - [section 4.3] "the engine then accepts a scaling factor α that determines how much relative weight should be assigned to the text and image inputs"
  - [section 5.1.3] "This method could be particularly valuable when a user does not know the proper vocabulary, but this method offers the added affordance of enabling the user to specify natural language in order to tune the search"
- Break condition: If the scaling factor α is poorly chosen, the combined embedding may drift away from meaningful regions of the embedding space, reducing retrieval accuracy.

### Mechanism 3
- Claim: Metadata extraction from structured fields suffices for fine-tuning dataset generation without heavy NLP models.
- Mechanism: The pipeline extracts descriptive terms from existing loc.gov metadata fields, normalizes them, and concatenates them into a natural language caption, bypassing the need for large language models.
- Core assumption: Library catalog metadata already contains sufficient visual descriptors to train CLIP effectively.
- Evidence anchors:
  - [section 3.3] "We smooth the capitalization and punctuation through a few simple functions, and we use this metadata to generate a descriptive, natural language caption for each map"
  - [section 5.1.1] "Examples where both our search implementation and metadata-based search do not have high precision include 'maps with drawings of people' and 'hand drawn maps'"
- Break condition: If metadata is sparse, inconsistent, or missing visual descriptors, the generated captions will be poor training signals, hurting fine-tuning quality.

## Foundational Learning

- Concept: Contrastive learning objective
  - Why needed here: CLIP is trained to align image and text embeddings that correspond to the same content, making it suitable for multimodal retrieval.
  - Quick check question: Why does contrastive loss encourage the model to map matching image-text pairs close together in embedding space?

- Concept: Cosine similarity in high-dimensional spaces
  - Why needed here: Retrieval is performed by computing cosine similarity between embeddings; understanding its geometric meaning helps debug why certain queries retrieve unexpected results.
  - Quick check question: What does a cosine similarity of 0.9 versus 0.5 imply about the alignment of two embeddings?

- Concept: Embedding normalization
  - Why needed here: CLIP embeddings are normalized to unit length before comparison; without normalization, dot product would be biased by vector magnitude.
  - Quick check question: What happens to cosine similarity if you compare unnormalized embeddings?

## Architecture Onboarding

- Component map:
  - Embedding generation pipeline (multiprocessing, IIIF requests, normalization)
  - beto tensor storage (PyTorch tensor + index list for mapping)
  - Search engine (text-only, image-only, joint modes)
  - Fine-tuning dataset builder (metadata extraction + caption generation)
  - Jupyter notebooks for reproducibility

- Critical path:
  1. Download and embed all map images into beto tensor.
  2. Receive user query (text, image, or both).
  3. Generate query embedding(s) on the fly.
  4. Compute cosine similarities to all beto embeddings.
  5. Return top-k results with normalized scores.

- Design tradeoffs:
  - Multiprocessing speeds embedding generation but increases memory usage.
  - Storing all embeddings in a single tensor enables fast indexing but limits scalability beyond RAM.
  - Using only metadata-derived captions avoids NLP overhead but may miss nuanced visual features.

- Failure signatures:
  - Slow queries: tensor not loaded into GPU memory or similarity computation not vectorized.
  - Irrelevant results: CLIP embeddings not well aligned to map domain, or query embedding poorly constructed.
  - Out-of-memory: Too many high-resolution images processed without batching.

- First 3 experiments:
  1. Load beto tensor and run a simple text query to confirm embedding lookup works.
  2. Test image-input search with a known map URL to verify embedding generation pipeline.
  3. Run joint text+image search with α=0 to confirm weighted centroid computation and retrieval.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal batch size and learning rate configuration for fine-tuning CLIP on the map-caption dataset?
- Basis in paper: [explicit] The authors tested various batch sizes (8, 16, 32, 64) and learning rates but found little significant reduction in validation loss.
- Why unresolved: The paper mentions that smaller batch sizes showed more modest decline in validation loss, but does not specify an optimal configuration.
- What evidence would resolve it: Additional experiments with a wider range of batch sizes and learning rates, along with a detailed analysis of validation loss trends, would help identify the optimal configuration.

### Open Question 2
- Question: How does the performance of the fine-tuned CLIP model compare to the base model for specific search tasks within the map collection?
- Basis in paper: [explicit] The authors note that the fine-tuned model showed mixed results and that accuracy for search and discovery tasks suffered qualitatively compared to the base model.
- Why unresolved: The paper does not provide a detailed comparison of the performance of the fine-tuned model versus the base model for specific search tasks.
- What evidence would resolve it: Conducting a comprehensive evaluation of the fine-tuned model's performance on various search tasks, and comparing it to the base model, would provide insights into its effectiveness.

### Open Question 3
- Question: How can the multimodal search capabilities of CLIP be further enhanced to improve the relevance of search results for complex queries?
- Basis in paper: [inferred] The authors mention that the multimodal search approach allows users to specify feedback to refine searches, but they do not explore ways to enhance this capability.
- Why unresolved: The paper does not delve into potential improvements or extensions to the multimodal search approach.
- What evidence would resolve it: Experimenting with different methods of combining text and image inputs, as well as incorporating user feedback mechanisms, could lead to more relevant search results for complex queries.

## Limitations

- The paper lacks detailed evaluation metrics (e.g., precision@k, recall) to quantify retrieval performance across query types, making it difficult to assess CLIP's effectiveness in the cultural heritage domain.
- Fine-tuning experiments are described as having "mixed results" without specifying the exact hyperparameters, training duration, or comparative baselines, limiting reproducibility and interpretation of the findings.
- The metadata-derived caption generation process is not fully detailed, raising concerns about the quality and consistency of training data for fine-tuning CLIP.

## Confidence

- High: CLIP embeddings enable unified text-image search in shared embedding space.
- Medium: Joint text-and-image search allows interactive refinement of search intent.
- Medium: Metadata extraction suffices for fine-tuning dataset generation without heavy NLP models.

## Next Checks

1. Evaluate retrieval quality using standard IR metrics (e.g., precision@10, mean average precision) across diverse query types to quantify CLIP's effectiveness.
2. Perform ablation studies on metadata-derived captions to assess their impact on fine-tuning performance and identify potential gaps in visual descriptor coverage.
3. Test joint search with varying α values on a held-out query set to determine optimal weighting and validate the claimed interactive refinement benefit.