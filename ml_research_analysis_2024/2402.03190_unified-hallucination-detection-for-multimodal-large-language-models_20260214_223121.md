---
ver: rpa2
title: Unified Hallucination Detection for Multimodal Large Language Models
arxiv_id: '2402.03190'
source_url: https://arxiv.org/abs/2402.03190
tags:
- hallucination
- detection
- claim
- mllms
- unihd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a unified framework, UNIHD, for detecting\
  \ hallucinations in Multimodal Large Language Models (MLLMs) across image-to-text\
  \ and text-to-image generation tasks. The approach leverages a suite of external\
  \ tools\u2014including object detection, attribute analysis, scene-text recognition,\
  \ and fact verification\u2014to systematically gather evidence for validating claims\
  \ extracted from model outputs."
---

# Unified Hallucination Detection for Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2402.03190
- Source URL: https://arxiv.org/abs/2402.03190
- Reference count: 13
- Introduces UNIHD framework for multimodal hallucination detection across image-to-text and text-to-image tasks

## Executive Summary
This paper presents UNIHD, a unified framework for detecting hallucinations in Multimodal Large Language Models (MLLMs) by leveraging external tools such as object detection, attribute analysis, scene-text recognition, and fact verification. The approach extracts claims from MLLM outputs and validates them against multimodal evidence, achieving significant improvements over baseline detectors. A novel benchmark, MHaluBench, is introduced with fine-grained claim-level annotations to comprehensively evaluate hallucination detection performance.

## Method Summary
The UNIHD framework addresses hallucination detection in MLLMs by employing a suite of external tools to gather evidence for validating claims extracted from model outputs. The process begins with claim extraction, where object, attribute, scene-text, and fact-related claims are identified from MLLM-generated content. These claims are then validated through four specialized tools: an object detection tool verifies the presence of objects, an attribute analysis tool checks visual attributes, a scene-text recognition tool confirms text content, and a fact verification tool cross-references claims against external knowledge sources. The validated evidence is used to classify whether hallucinations are present, providing a systematic and interpretable approach to multimodal hallucination detection.

## Key Results
- UNIHD significantly outperforms baseline detectors in identifying object and scene-text hallucinations
- Fact-conflicting hallucination detection shows strong performance when supported by reliable external tools
- Attribute-level hallucination detection remains challenging and requires further investigation

## Why This Works (Mechanism)
The framework's effectiveness stems from decomposing hallucination detection into claim-level validation tasks, each handled by specialized tools that provide concrete evidence. By separating the detection process into modular components (object detection, attribute analysis, text recognition, and fact verification), UNIHD can systematically validate different aspects of multimodal outputs. This modular design allows for targeted improvements and better interpretability compared to monolithic approaches.

## Foundational Learning
- **Multimodal Hallucinations**: Why needed - Understanding hallucinations in MLLMs requires recognizing that errors can occur in both visual and textual modalities. Quick check - Can the model generate outputs that contradict the input image or introduce non-existent elements?
- **Claim Extraction**: Why needed - Identifying specific claims within MLLM outputs is crucial for targeted validation. Quick check - Are claims clearly identifiable and categorized by type (object, attribute, text, fact)?
- **External Tool Integration**: Why needed - Specialized tools provide more reliable evidence than the MLLM alone. Quick check - Do the tools provide consistent and accurate validation across diverse inputs?
- **Fine-grained Annotation**: Why needed - Detailed annotations enable precise evaluation of detection capabilities. Quick check - Does the benchmark cover all hallucination types with sufficient granularity?
- **Fact Verification**: Why needed - External knowledge validation is essential for detecting fact-conflicting hallucinations. Quick check - Are fact verification sources comprehensive and up-to-date?
- **GPT-4V as Detection Backbone**: Why needed - Large vision-language models can reason about complex multimodal relationships. Quick check - Does GPT-4V maintain performance across diverse hallucination scenarios?

## Architecture Onboarding
- **Component Map**: Image/Text Input -> Claim Extraction -> Object Detection Tool -> Attribute Analysis Tool -> Scene-Text Recognition Tool -> Fact Verification Tool -> Evidence Aggregation -> Hallucination Classification
- **Critical Path**: The claim extraction and validation pipeline forms the core flow, where extracted claims are processed by respective tools before final classification
- **Design Tradeoffs**: Relies heavily on GPT-4V for evidence aggregation, which may introduce cost and accessibility barriers; modular tool design allows for targeted improvements but requires tool reliability
- **Failure Signatures**: Tool failures (e.g., missed objects or incorrect text recognition) propagate to detection errors; attribute-level detection remains consistently weak across scenarios
- **First Experiments**: 1) Test claim extraction accuracy across diverse image-text pairs 2) Validate individual tool performance on controlled hallucination types 3) Evaluate evidence aggregation consistency with varying tool outputs

## Open Questions the Paper Calls Out
Major uncertainties remain regarding the scalability and robustness of the proposed external tool suite across diverse real-world scenarios. While GPT-4V demonstrates strong performance on the curated MHaluBench dataset, its effectiveness may degrade when applied to less structured or noisier inputs not represented in the benchmark. The paper acknowledges that attribute-level hallucination detection remains challenging, suggesting potential gaps in the tool's ability to capture subtle visual details. Additionally, the heavy reliance on GPT-4V as the detection backbone raises concerns about cost, accessibility, and generalization to other foundation models. The benchmark itself, while comprehensive, may not fully capture the breadth of hallucination types encountered in practical applications, particularly those involving complex reasoning or ambiguous visual contexts.

## Limitations
- Heavy reliance on GPT-4V as the detection backbone raises concerns about cost and accessibility
- Attribute-level hallucination detection remains challenging and requires further investigation
- Tool failures or noisy outputs can propagate errors through the detection pipeline

## Confidence
- High confidence in object and scene-text hallucination detection due to consistent performance gains over baselines
- Medium confidence in fact-conflicting hallucination detection, dependent on external fact verification tool reliability
- Low confidence in attribute-level hallucination detection, as explicitly noted by authors as an area requiring further investigation

## Next Checks
1. Test UNIHD's performance on out-of-distribution images and text prompts not represented in MHaluBench to assess generalization
2. Evaluate the impact of tool failures or noisy outputs on overall detection accuracy by systematically degrading tool performance
3. Conduct a human evaluation study to compare UNIHD's detections against expert judgments across diverse hallucination types and contexts