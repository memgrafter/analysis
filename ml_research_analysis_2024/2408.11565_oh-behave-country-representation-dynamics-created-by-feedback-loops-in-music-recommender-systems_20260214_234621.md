---
ver: rpa2
title: Oh, Behave! Country Representation Dynamics Created by Feedback Loops in Music
  Recommender Systems
arxiv_id: '2408.11565'
source_url: https://arxiv.org/abs/2408.11565
tags:
- music
- user
- country
- recommendations
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the long-term effects of feedback loops
  on country representation in music recommender systems. Through a simulation study
  using the LFM-2b dataset, the authors find that most recommendation models (BPR,
  MultiVAE, NeuMF, Pop) increase the proportion of US music while decreasing local
  music in recommendations and user profiles over time.
---

# Oh, Behave! Country Representation Dynamics Created by Feedback Loops in Music Recommender Systems

## Quick Facts
- arXiv ID: 2408.11565
- Source URL: https://arxiv.org/abs/2408.11565
- Reference count: 28
- Most recommendation models increase US music representation while decreasing local music over time

## Executive Summary
This study investigates how feedback loops in music recommender systems affect country representation over time. Using the LFM-2b dataset and simulating 100 iterations of recommendation-consumption cycles, the authors find that most recommendation models (BPR, MultiVAE, NeuMF, Pop) increase the proportion of US music in both recommendations and user profiles while decreasing local music. LightGCN showed the least country miscalibration but still altered country proportions. Surprisingly, the most popularity-calibrated model (ItemKNN) provided the least country-calibrated recommendations, suggesting these two types of calibration are independent.

## Method Summary
The study simulates feedback loops over 100 iterations using the LFM-2b dataset with country information from Musicbrainz. Six recommendation models (BPR, ItemKNN, LightGCN, MultiVAE, NeuMF, Pop) are trained on a 5-core filtered sample (~2.29M interactions, ~100K tracks, ~12K users). At each iteration, models generate recommendations, users "consume" one item based on an acceptance probability function, profiles are updated, and models are retrained. The study measures US and local proportions in recommendations and user profiles, Jensen-Shannon Divergence for country and popularity miscalibration, and NDCG@k for accuracy.

## Key Results
- Most recommendation models (BPR, MultiVAE, NeuMF, Pop) decrease local music representation while increasing US music over iterations
- LightGCN showed the least country miscalibration but still altered country proportions
- ItemKNN, despite being the most popularity-calibrated model, provided the least country-calibrated recommendations
- Users from less represented countries (e.g., Finland) were most affected by underrepresentation of their local music

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feedback loops amplify existing representation biases in music recommender systems, particularly favoring US music over local music.
- Mechanism: At each iteration, the recommender system suggests items to users, users "consume" one recommended item, and the system retrains on the updated profiles. This creates a self-reinforcing cycle where popular (often US) music is recommended more frequently, leading to more consumption and further training on biased data.
- Core assumption: User choice model accurately simulates real-world consumption patterns.
- Evidence anchors:
  - [abstract] "most of the investigated recommendation models decrease the proportion of music from local artists in their recommendations"
  - [section] "The results suggest that most of the investigated recommendation models decrease the proportion of music from local artists in their recommendations"
  - [corpus] Weak evidence - corpus focuses on different topics (language models, genre classification)
- Break condition: If user choice model doesn't reflect actual consumption patterns, the feedback loop amplification may not occur as described.

### Mechanism 2
- Claim: LightGCN preserves country proportions better than other models because it uses graph convolution that considers neighborhood embeddings.
- Mechanism: LightGCN's graph-based approach incorporates both user and item embeddings from the interaction graph, potentially preserving more diverse neighborhood information that helps maintain balanced country representation across iterations.
- Core assumption: Graph convolution inherently preserves more diverse information than other model architectures.
- Evidence anchors:
  - [section] "LightGCN showed the least country miscalibration but still altered country proportions"
  - [section] "the most popularity-calibrated model (ItemKNN) provides the least country-calibrated recommendations"
  - [corpus] Weak evidence - corpus doesn't discuss graph-based recommendation models
- Break condition: If the graph structure itself is biased, LightGCN won't solve country representation issues.

### Mechanism 3
- Claim: ItemKNN's country miscalibration despite popularity calibration occurs because popularity-based similarity doesn't account for country distribution.
- Mechanism: ItemKNN recommends items based on similarity to previously consumed items, which may cluster by popularity metrics but doesn't inherently consider country origin. This creates a mismatch where popular items are recommended without regard to country balance.
- Core assumption: Popularity calibration and country calibration are independent metrics.
- Evidence anchors:
  - [abstract] "surprisingly, find that the most popularity-calibrated model in our study (ItemKNN) provides the least country-calibrated recommendations"
  - [section] "ItemKNN causes the least popularity-miscalibrated user profiles on average, while the highest miscalibration in terms of country is caused by ItemKNN"
  - [corpus] Weak evidence - corpus doesn't discuss ItemKNN specifically
- Break condition: If popularity and country are inherently correlated in the dataset, this mechanism wouldn't explain the observed behavior.

## Foundational Learning

- Concept: Feedback loop simulation methodology
  - Why needed here: The study relies on simulating iterative recommendation-consumption cycles to observe long-term representation dynamics
  - Quick check question: How does the acceptance probability function influence which items users "consume" in each iteration?

- Concept: Jensen-Shannon Divergence for calibration measurement
  - Why needed here: Used to quantify how much user profiles deviate from their original state in terms of country and popularity distributions
  - Quick check question: Why is Jensen-Shannon Divergence preferred over other divergence measures for this study?

- Concept: 5-core filtering in dataset preparation
  - Why needed here: Ensures that every user and item in the dataset has at least 5 interactions, creating a more robust dataset for training recommender systems
  - Quick check question: What impact would removing 5-core filtering have on the study's results?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> Recommendation generation -> Simulated consumption -> Profile update -> Model retraining -> Evaluation
- Critical path: Data preprocessing → model training → recommendation generation → simulated consumption → profile update → model retraining → evaluation
- Design tradeoffs: Computational efficiency vs. simulation accuracy (100 iterations chosen as balance)
- Failure signatures:
  - Model performance degradation over iterations
  - Unexpected convergence or divergence of country proportions
  - Statistical significance failures in T-tests
- First 3 experiments:
  1. Verify that all models achieve reasonable NDCG scores on the initial split before running feedback loop
  2. Test acceptance probability function with different alpha values to understand sensitivity
  3. Run single iteration feedback loop to verify data flow and model retraining works correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different user choice models affect country representation dynamics in music recommender systems?
- Basis in paper: [explicit] The authors note that their simulation uses a specific choice model where one item per user is consumed per iteration with slight preference toward higher-ranked items, but acknowledge that real-world scenarios may differ significantly
- Why unresolved: The paper uses a simplified choice model for simulation purposes and explicitly states that real users may have different consumption patterns and choice models
- What evidence would resolve it: Conducting similar feedback loop simulations with various choice models (e.g., multiple items per iteration, random consumption, popularity-based choice) and comparing the resulting country representation dynamics

### Open Question 2
- Question: What are the causal factors behind the observed differences in country miscalibration across different countries?
- Basis in paper: [explicit] The authors observe that countries with similar characteristics (e.g., CA, NL, AU, NO with few tracks) experience lower miscalibration while others (e.g., UK with more tracks) experience higher miscalibration, but cannot definitively explain these differences
- Why unresolved: The paper acknowledges that multiple factors (country representation in training data, consumption habits, etc.) may interact in complex ways that haven't been isolated
- What evidence would resolve it: Conducting controlled ablation studies with balanced datasets across different countries and systematically varying representation levels and consumption patterns

### Open Question 3
- Question: How can recommendation systems be designed to simultaneously optimize for popularity calibration and country calibration?
- Basis in paper: [explicit] The authors find that ItemKNN, which provides the most popularity-calibrated recommendations, produces the least country-calibrated results, suggesting an inverse relationship between these two types of calibration
- Why unresolved: The paper identifies this trade-off but doesn't explore potential solutions or hybrid approaches that might balance both objectives
- What evidence would resolve it: Developing and testing new recommendation algorithms or hybrid approaches that explicitly optimize for both popularity and country calibration metrics, then evaluating their performance on both dimensions

## Limitations
- The simulation assumes a specific user choice model with acceptance probability that may not accurately reflect real-world consumption patterns
- Country attribution relies on Musicbrainz data, which may have incomplete or inaccurate country information for some tracks
- The study uses a specific 5-core filtered subset of LFM-2b, potentially limiting generalizability to the full dataset

## Confidence
- High confidence in the observed feedback loop effects on US/local music proportions
- Medium confidence in the comparative model performance
- Medium confidence in the Jensen-Shannon Divergence measurements as calibration metrics
- Low confidence in the generalizability to other music recommendation contexts or datasets

## Next Checks
1. Validate country attribution accuracy by cross-checking a sample of tracks against Musicbrainz and assessing the completeness of country information
2. Test sensitivity of results to different acceptance probability functions and alpha values in the user choice model
3. Compare findings using alternative calibration metrics (e.g., Wasserstein distance) to verify robustness of JSD-based conclusions