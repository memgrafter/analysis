---
ver: rpa2
title: 'Integrating Physician Diagnostic Logic into Large Language Models: Preference
  Learning from Process Feedback'
arxiv_id: '2401.05695'
source_url: https://arxiv.org/abs/2401.05695
tags:
- patient
- doctor
- your
- medical
- diagnosis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PLPF, a method that integrates physician diagnostic
  logic into large language models for medical dialogue generation. PLPF uses a flowchart-based
  rule system to guide multi-round conversations, avoiding logical inconsistencies
  in the diagnostic process.
---

# Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback

## Quick Facts
- arXiv ID: 2401.05695
- Source URL: https://arxiv.org/abs/2401.05695
- Authors: Chengfeng Dou; Zhi Jin; Wenpin Jiao; Haiyan Zhao; Yongqiang Zhao; Zhenwei Tao
- Reference count: 11
- Key outcome: PLPF improves diagnostic accuracy by 17.6% over baseline and outperforms traditional reinforcement learning from human feedback in medical dialogue generation

## Executive Summary
This paper presents PLPF (Preference Learning from Process Feedback), a method that integrates physician diagnostic logic into large language models for medical dialogue generation. PLPF uses a flowchart-based rule system to guide multi-round conversations, avoiding logical inconsistencies in the diagnostic process. The method involves rule modeling, preference data generation, and preference alignment using a rule evaluation model and direct preference optimization. Experimental results show PLPF significantly improves diagnostic accuracy and outperforms traditional approaches.

## Method Summary
PLPF integrates physician diagnostic logic into LLMs through a three-stage process: rule modeling using flowchart-based systems, preference data generation via trajectory prediction and REM scoring, and preference alignment using direct preference optimization. The method uses a Rule Evaluation Model to score dialogue compliance with predefined rules, generates preference pairs by ranking dialogue trajectories, and fine-tunes the base LLM to follow the diagnostic flowchart logic. Evaluation uses Standardized Patient Testing to assess performance in multi-round and single-round dialogue tasks.

## Key Results
- PLPF improves diagnostic accuracy by 17.6% over baseline model in medical conversations
- Outperforms traditional reinforcement learning from human feedback approaches
- Demonstrates enhanced performance in both multi-round and single-round dialogue tasks

## Why This Works (Mechanism)

### Mechanism 1
PLPF improves multi-round dialogue coherence by aligning responses to a predefined diagnostic flowchart rather than optimizing for fluency or comprehensiveness alone. The Rule Evaluation Model (REM) scores conversations against rule-based goals and constraints, creating contrastive pairs that train the model to follow flowchart logic. This assumes the diagnostic flowchart accurately reflects physician best practices and that REM can generalize from a small, manually annotated dataset.

### Mechanism 2
The trajectory prediction step using one-shot learning with ChatGPT generates more diverse and clinically realistic candidate responses than random sampling alone. ChatGPT is prompted with an arbitrary reference dialogue and the dialogue to be completed, constrained to match styles and clinical context. This yields multiple future-round responses that better reflect real-world dialogue flow, assuming ChatGPT's training data includes realistic medical dialogue patterns similar to MedDialogue.

### Mechanism 3
Dynamic weighting of rules based on goal completion order and constraint satisfaction improves multi-round dialogue performance more than flat rule summation. Each rule is assigned a weight computed from its predecessors' satisfaction scores and constraint dependencies, encouraging the model to satisfy earlier goals before later ones. This approach requires careful tuning to avoid underweighting critical constraints or overemphasizing early goals.

## Foundational Learning

- Concept: Rule modeling and flowchart-based dialogue design
  - Why needed here: PLPF relies on explicit representation of physician diagnostic logic to guide LLM responses; understanding flowcharts and rule mapping is essential to implement REM
  - Quick check question: What are the two categories of rules used in PLPF and what does each measure?

- Concept: Preference learning via contrastive ranking
  - Why needed here: PLPF uses REM to rank dialogue trajectories and generate preference pairs; knowledge of ranking loss functions and sampling strategies is required to train the model
  - Quick check question: How does PLPF generate preference pairs from REM scores?

- Concept: Multi-round dialogue coherence evaluation
  - Why needed here: Standardized Patient Testing measures symptom collection, diagnosis accuracy, and test recommendation coverage; understanding this evaluation setup is necessary to validate PLPF performance
  - Quick check question: What three metrics are used to assess model performance in SP Testing?

## Architecture Onboarding

- Component map: Rule Evaluation Model (REM) -> Preference Data Construction -> Human Preference Alignment -> Standardized Patient Simulator
- Critical path: 1) Build and annotate REM training data 2) Train REM on rule compliance task 3) Sample dialogue histories and generate trajectories 4) Rank trajectories with REM and create preference pairs 5) Fine-tune base LLM with DPO on preference pairs 6) Evaluate with SP Testing
- Design tradeoffs: REM complexity vs. generalization, trajectory length vs. computational cost, rule strictness vs. model flexibility
- Failure signatures: REM overfitting, preference data imbalance, SP Testing variance
- First 3 experiments: 1) Train REM on annotated rule compliance dataset; validate on held-out test set 2) Generate preference pairs using sampled and ChatGPT-generated trajectories; check REM ranking consistency 3) Fine-tune base LLM with DPO on preference pairs; compare BLEU/ROUGE vs. SP Testing metrics

## Open Questions the Paper Calls Out

### Open Question 1
How would the PLPF approach perform with different rule sets or flowchart designs for medical diagnosis? The study only tested one specific rule set and flowchart design, making it unclear how robust the PLPF method is to changes in the diagnostic process representation or rule definitions.

### Open Question 2
What is the long-term impact of PLPF-trained models on patient outcomes compared to traditional approaches? The paper focuses on diagnostic accuracy in controlled testing environments but does not address real-world patient outcomes or longitudinal studies.

### Open Question 3
How does the performance of PLPF change when scaling to more complex medical conditions or larger language models? The experimental scope was limited to specific conditions and model sizes, leaving the scalability of PLPF to more complex medical scenarios or larger foundation models unexplored.

## Limitations
- Evaluation relies on a single dataset (MedDialogue) and methodology (Standardized Patient Testing)
- The flowchart-based rule system's adequacy in capturing physician diagnostic logic is untested against actual clinical practice
- REM's ability to generalize from small manually annotated dataset to diverse medical dialogues remains uncertain

## Confidence
- High: PLPF improves diagnostic accuracy in controlled experiments using SP Testing
- Medium: The preference learning framework is implementable and shows promise
- Low: The generalizability of the rule system to real clinical practice and the REM's ability to handle unseen dialogue patterns

## Next Checks
1. Test PLPF on multiple medical dialogue datasets to assess generalizability beyond MedDialogue
2. Conduct ablation studies removing dynamic weighting and one-shot learning to quantify their individual contributions
3. Evaluate PLPF's performance on real physician-patient conversations or with physician feedback to validate clinical relevance