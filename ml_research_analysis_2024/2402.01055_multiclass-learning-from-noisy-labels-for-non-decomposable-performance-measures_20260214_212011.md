---
ver: rpa2
title: Multiclass Learning from Noisy Labels for Non-decomposable Performance Measures
arxiv_id: '2402.01055'
source_url: https://arxiv.org/abs/2402.01055
tags:
- performance
- noisy
- learning
- measures
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes noise-corrected algorithms for learning from
  noisy labels when optimizing non-decomposable performance measures. The key idea
  is to introduce noise corrections to both class probability estimation and confusion
  matrix estimation when applying Frank-Wolfe and bisection-based methods.
---

# Multiclass Learning from Noisy Labels for Non-decomposable Performance Measures

## Quick Facts
- arXiv ID: 2402.01055
- Source URL: https://arxiv.org/abs/2402.01055
- Authors: Mingyuan Zhang; Shivani Agarwal
- Reference count: 40
- Key outcome: Proposes noise-corrected algorithms (NCFW, NCBS) for learning from noisy labels under class-conditional noise models, proven to be Bayes consistent with regret bounds quantifying label noise effects.

## Executive Summary
This paper addresses the problem of learning from noisy labels when optimizing non-decomposable performance measures like F1, G-mean, and Q-mean in multiclass classification. The authors propose noise-corrected versions of Frank-Wolfe and bisection-based algorithms that incorporate noise corrections to both class probability estimation and confusion matrix estimation. Under the widely studied class-conditional noise (CCN) model, these algorithms are proven to be Bayes consistent, meaning they converge to the optimal classifier for clean data as sample size increases, with regret bounds that quantify the impact of label noise.

## Method Summary
The proposed method introduces noise corrections to existing algorithms for optimizing non-decomposable performance measures. For monotonic convex measures, the Frank-Wolfe based NCFW algorithm is used, while for ratio-of-linear measures, the bisection-based NCBS algorithm is employed. Both algorithms incorporate noise corrections to class probability estimation (OP1) and confusion matrix estimation (OP2) under the CCN model. The noise corrections involve using the inverse of the noise matrix T to transform both the loss matrix and confusion matrix estimates, ensuring that the algorithms optimize the correct objective with respect to clean data despite training on noisy data.

## Key Results
- NCFW and NCBS algorithms achieve Bayes consistency under class-conditional noise models
- Regret bounds show that sample complexity increases linearly with the inverse of the minimum eigenvalue of the noise matrix
- Experiments on synthetic and real data demonstrate improved performance over baseline methods when label noise is present
- The algorithms outperform their non-corrected counterparts, especially as noise levels increase

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Noise corrections in OP1 ensure that Bayes optimal classifiers for the corrected linear loss-based performance measure w.r.t. noisy data remain Bayes optimal for the original non-decomposable performance measure w.r.t. clean data.
- Mechanism: The noise-corrected OP1 transforms the linear approximation Lt into (Lt)′ = (T⊤)−1Lt, aligning the optimization landscape of the noisy data with that of the clean data.
- Core assumption: The class-conditional noise matrix T is known and invertible.
- Evidence anchors:
  - [abstract] "introduce noise corrections to both class probability estimation and confusion matrix estimation"
  - [section] "we propose to find an optimal classifier for a noise-corrected loss-based performance measure (Lt)′ = (T⊤)−1Lt w.r.t. eD"
  - [corpus] Weak evidence; no direct mention of noise correction mechanism.
- Break condition: If T is not invertible or T is unknown and poorly estimated, the noise correction fails and Bayes consistency is lost.

### Mechanism 2
- Claim: Noise corrections in OP2 ensure that empirical confusion matrices computed from noisy data converge to the true confusion matrices of the clean data.
- Mechanism: The noise-corrected OP2 estimates CD[bgt] by T−1bC eS[bgt], leveraging the relation C eD[h] = TCD[h] under the CCN model.
- Core assumption: The confusion matrix estimator from the noisy sample is consistent (converges to C eD[h] as sample size increases).
- Evidence anchors:
  - [abstract] "introduce noise corrections to both class probability estimation and confusion matrix estimation"
  - [section] "we propose to estimate CD[bgt] by T−1bC eS[bgt]"
  - [corpus] No direct evidence; requires inference from general label noise literature.
- Break condition: If the confusion matrix estimator is biased or inconsistent, the correction will not recover the clean confusion matrix, breaking Bayes consistency.

### Mechanism 3
- Claim: The iterative Frank-Wolfe (or Bisection) algorithm, when applied with noise-corrected operations, converges to the Bayes optimal classifier for the non-decomposable performance measure on clean data.
- Mechanism: The algorithm iteratively refines a convex combination of classifiers (Frank-Wolfe) or binary searches the optimal threshold (Bisection), each step using noise-corrected OP1 and OP2 to stay aligned with the clean data objective.
- Core assumption: The performance measure is monotonic convex (Frank-Wolfe) or ratio-of-linear (Bisection) and the algorithm hyperparameters (T, step sizes) are chosen appropriately.
- Evidence anchors:
  - [abstract] "develop noise-corrected versions of the algorithms under the widely studied family of class-conditional noise models"
  - [section] "We can now incorporate the noise-corrected OP1 and OP2 into the iterative algorithm based on Frank-Wolfe method"
  - [corpus] Weak evidence; related work mentions optimization but not specific algorithm convergence under noise.
- Break condition: If the algorithm hyperparameters are poorly chosen (e.g., too few iterations, bad step sizes), convergence to the Bayes optimal may fail even with correct noise corrections.

## Foundational Learning

- Concept: Class-conditional noise (CCN) model
  - Why needed here: The entire noise correction framework relies on the assumption that the noise process is CCN, allowing the noise matrix T to be defined and used for correction.
  - Quick check question: If a label y is flipped to ey with probability that depends only on y and not on x, is this CCN?

- Concept: Confusion matrix as a sufficient statistic for non-decomposable performance measures
  - Why needed here: All non-decomposable performance measures considered are functions of the confusion matrix; the algorithms optimize over feasible confusion matrices to find the Bayes optimal.
  - Quick check question: Can the Micro F1 be computed from a confusion matrix without access to individual predictions?

- Concept: Consistency of classifiers under noisy labels
  - Why needed here: The regret bounds show that as sample size increases, the noise-corrected algorithms converge to the Bayes optimal performance on clean data, which is the definition of consistency under noise.
  - Quick check question: If a classifier trained on noisy data has regret tending to zero as the sample size grows, is it consistent?

## Architecture Onboarding

- Component map: Clean data (X,Y) -> Noisy data (X,eY) via CCN matrix T -> CPE learner (beη on eS1) -> Algorithm core (NCFW/NCBS with noise-corrected OP1 and OP2) -> Evaluation on clean test data

- Critical path:
  1. Split noisy data into eS1 (for CPE) and eS2 (for confusion matrix estimation)
  2. Learn beη on eS1
  3. For each iteration t:
     - Compute noise-corrected loss matrix (Lt)′
     - Solve linear minimization to get bgt
     - Update current classifier and confusion matrix estimate using eS2
  4. Output final randomized classifier

- Design tradeoffs:
  - Known T vs. estimated bT: Known T gives tighter regret bounds; estimated bT introduces additional error term.
  - Frank-Wolfe (monotonic convex) vs. Bisection (ratio-of-linear): Different convergence rates and applicable performance measure classes.
  - Number of iterations T: Larger T gives better approximation but higher computational cost.

- Failure signatures:
  - High ∥T−1∥1: Indicates severe label noise; may require much larger sample sizes to achieve same regret.
  - Poor CPE consistency: If beη does not converge to eη, the noise correction in OP1 fails.
  - Non-invertible T: Breaks the noise correction framework entirely.

- First 3 experiments:
  1. Synthetic data with varying noise levels (σ = 0.1, 0.3, 0.6): Verify that as ∥T−1∥1 increases, more samples are needed to achieve same performance.
  2. Real data with known noise matrix: Compare NCFW vs. FW and NCBS vs. BS; expect NCFW/NCBS to outperform when noise is present.
  3. Real data with estimated noise matrix: Use one of the estimation methods (e.g., Li et al. 2021) and run NCFW/NCBS; check if performance degrades compared to known T case.

## Open Questions the Paper Calls Out
No specific open questions are called out in the provided content.

## Limitations
- The effectiveness of the approach depends on accurate estimation of the noise matrix T, which may be challenging in practice.
- The algorithms are designed specifically for class-conditional noise models and may not perform well under other noise patterns like instance-dependent noise.
- The theoretical guarantees assume exact solutions to subproblems, but in practice, approximate solutions may be used, potentially affecting regret bounds.

## Confidence
- **High confidence** in the theoretical framework and Bayes consistency proofs under the CCN model assumptions.
- **Medium confidence** in the practical effectiveness of noise corrections, as empirical results show improvements but are limited to specific datasets and noise levels.
- **Low confidence** in the scalability of the approach to very large datasets or complex noise patterns beyond CCN.

## Next Checks
1. Conduct experiments with varying noise matrix estimation quality (e.g., using different estimation methods) to quantify the impact on final performance.
2. Test the algorithms on datasets with different noise patterns (e.g., instance-dependent noise) to assess robustness beyond CCN assumptions.
3. Evaluate the sample complexity empirically by measuring performance across different sample sizes and noise levels to verify the theoretical regret bounds.