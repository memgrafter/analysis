---
ver: rpa2
title: Transforming and Combining Rewards for Aligning Large Language Models
arxiv_id: '2402.00742'
source_url: https://arxiv.org/abs/2402.00742
tags:
- reward
- rewards
- transformation
- helpfulness
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses two problems in aligning large language models
  to human preferences using reward models: (1) identifying an optimal monotone transformation
  of learned reward models, and (2) combining multiple reward models for multi-objective
  alignment. The authors propose a probabilistic interpretation where alignment aims
  to produce samples from a distribution conditional on outputs being "good" according
  to human preferences.'
---

# Transforming and Combining Rewards for Aligning Large Language Models

## Quick Facts
- arXiv ID: 2402.00742
- Source URL: https://arxiv.org/abs/2402.00742
- Reference count: 11
- Primary result: Optimal monotone transformation (LSC-transformation) and principled reward combination for multi-objective LLM alignment

## Executive Summary
This work addresses two fundamental challenges in aligning large language models to human preferences: identifying optimal transformations of learned reward models and combining multiple reward models for multi-objective alignment. The authors propose a probabilistic interpretation where alignment aims to produce samples from a distribution conditional on outputs being "good" according to human preferences. They derive that for Bradley-Terry preference models, the optimal transformation is log-sigmoid of centered rewards (LSC-transformation). This transformation mitigates reward hacking and underfitting by reducing marginal utility of high rewards while emphasizing improvements for poorly-performing outputs. It also enables principled reward aggregation where summation corresponds to logical AND across properties.

## Method Summary
The approach centers on transforming learned reward models through a log-sigmoid centering (LSC) transformation to produce an optimal alignment distribution. The transformation takes the form: r' = log(1/(1+e^(-r))), where r is the centered reward. This transformation is derived from a probabilistic interpretation where the goal is to maximize the probability of producing "good" outputs according to human preferences. For combining multiple reward models, the authors show that summing transformed rewards corresponds to requiring outputs to satisfy multiple properties simultaneously (logical AND). The method is evaluated by aligning models to be both helpful and harmless, comparing against baseline methods across various KL regularization levels.

## Key Results
- The LSC-transformation (log-sigmoid of centered rewards) is mathematically proven to be optimal for Bradley-Terry preference models
- Transformed approach consistently outperforms raw reward methods across various KL regularization levels
- Reward summation under transformation corresponds to logical AND across properties, enabling principled multi-objective alignment
- Substantial improvements observed in both helpfulness and harmlessness alignment compared to baseline methods

## Why This Works (Mechanism)
The LSC-transformation works by reshaping the reward landscape to reduce the marginal utility of high rewards while emphasizing improvements for poorly-performing outputs. This mitigates reward hacking (where models exploit reward boundaries) and underfitting (where models fail to explore the full reward space). The log-sigmoid function compresses extreme values while maintaining the ordering of rewards, creating a more balanced optimization landscape. When combining rewards through summation, the transformation ensures that each property must be satisfied to some degree, rather than allowing one property to dominate the optimization process.

## Foundational Learning

1. **Bradley-Terry preference model**: A probabilistic model for pairwise comparisons where the probability of preferring one item over another depends exponentially on their rewards. Why needed: Forms the theoretical foundation for deriving the optimal transformation. Quick check: Verify that the model satisfies the independence of irrelevant alternatives property.

2. **KL regularization in alignment**: Kullback-Leibler divergence regularization that constrains how much the aligned model can deviate from the original model. Why needed: Controls the trade-off between alignment quality and maintaining the base model's capabilities. Quick check: Ensure the KL penalty coefficient is properly tuned for each experiment.

3. **Reward hacking**: When models exploit loopholes or boundaries in reward functions to maximize reward without actually improving the desired behavior. Why needed: Understanding this phenomenon motivates the need for reward transformation. Quick check: Test models with adversarial prompts designed to exploit reward boundaries.

4. **Conditional sampling interpretation**: Viewing alignment as producing samples from a distribution conditional on outputs being "good" according to human preferences. Why needed: Provides the probabilistic framework for deriving optimal transformations. Quick check: Verify that the conditional distribution properly concentrates probability mass on high-quality outputs.

5. **Multi-objective optimization**: The challenge of satisfying multiple competing objectives simultaneously in model alignment. Why needed: Real-world alignment requires balancing multiple properties like helpfulness, harmlessness, and truthfulness. Quick check: Test whether the summation approach properly balances all specified properties.

## Architecture Onboarding

Component map: Reward Model -> LSC-Transformation -> Alignment Distribution -> Sampling Procedure

Critical path: The transformation of rewards is the critical component - without proper transformation, the alignment distribution will be suboptimal regardless of other factors. The LSC-transformation directly determines the quality of the conditional distribution used for sampling.

Design tradeoffs: The method trades off computational complexity (requiring transformation of rewards) for improved alignment quality and robustness. Alternative approaches might skip transformation but risk reward hacking or poor multi-objective balancing.

Failure signatures: If the Bradley-Terry assumption is violated, the LSC-transformation may not be optimal. Poor alignment performance with high KL regularization could indicate the transformation is too aggressive. If one property dominates despite summation, the transformation may need adjustment.

First experiments:
1. Compare alignment quality with and without LSC-transformation on a simple binary preference task
2. Test reward summation with different transformation functions to isolate the effect of log-sigmoid
3. Evaluate robustness to adversarial prompts that attempt to exploit reward boundaries

## Open Questions the Paper Calls Out
None

## Limitations
- The Bradley-Terry model assumption may not capture all patterns in real human preferences
- Empirical validation focuses primarily on helpfulness and harmlessness, limiting generalizability
- Claims about robustness to reward hacking could benefit from more systematic adversarial testing
- Experimental design is relatively limited with specific KL regularization levels

## Confidence

High confidence:
- Mathematical derivation of LSC-transformation under Bradley-Terry assumptions is rigorous
- Proof that log-sigmoid transformation maximizes probability of producing "good" outputs is sound

Medium confidence:
- Empirical results showing improved alignment are convincing but could be expanded
- Claim of consistent outperformance over baselines is supported but based on specific conditions

Low confidence:
- Broader claims about robustness to reward hacking and underfitting need more systematic stress-testing
- Generalization to other alignment objectives beyond helpfulness and harmlessness is uncertain

## Next Checks

1. Test LSC-transformation effectiveness across a wider range of alignment objectives beyond helpfulness and harmlessness, including truthfulness, coherence, and task-specific performance metrics.

2. Conduct adversarial evaluation where models attempt to exploit reward boundaries to assess claimed robustness to reward hacking under the transformed reward regime.

3. Compare the proposed transformation approach against alternative reward normalization techniques (e.g., z-score normalization, quantile transformation) in controlled experiments to isolate specific benefits of the log-sigmoid centering approach.