---
ver: rpa2
title: What does Kiki look like? Cross-modal associations between speech sounds and
  visual shapes in vision-and-language models
arxiv_id: '2407.17974'
source_url: https://arxiv.org/abs/2407.17974
tags:
- effect
- language
- human
- images
- syllables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We investigated whether vision-and-language models exhibit the
  bouba-kiki effect, a human cross-modal preference linking speech sounds to visual
  shapes. Using images from prior human studies and novel synthetic shapes, we probed
  four VLMs with single syllables, pseudowords, and probability scores.
---

# What does Kiki look like? Cross-modal associations between speech sounds and visual shapes in vision-and-language models

## Quick Facts
- arXiv ID: 2407.17974
- Source URL: https://arxiv.org/abs/2407.17974
- Authors: Tessa Verhoef; Kiana Shahrasbi; Tom Kouwenhoven
- Reference count: 30
- Key outcome: Vision-and-language models show limited evidence for the bouba-kiki effect, with only CLIP and GPT-4o showing modest alignment in select tests

## Executive Summary
This study investigates whether vision-and-language models (VLMs) exhibit the bouba-kiki effect, a human cross-modal preference linking speech sounds to visual shapes. The researchers tested four VLMs—CLIP, ViLT, BLIP2, and GPT-4o—using images from prior human studies and novel synthetic shapes. Results show only limited evidence for the effect, with CLIP and GPT-4o demonstrating modest alignment in select tests. Architecture, attention mechanisms, and training data size influenced results, but no model consistently matched human-like associations. These findings suggest VLMs do not robustly encode sound-symbolic mappings, informing future work on aligning model representations with human cognitive biases.

## Method Summary
The study probes four VLMs (CLIP, ViLT, BLIP2, and GPT-4o) by extracting image-to-text classification probabilities for pseudowords given specific shapes. Researchers used images from original Köhler experiments, Maurer et al. (2006), Westbury (2005), and newly generated curved/jagged shapes. Pseudowords included single syllables and two-syllable combinations using sonorant-rounded and plosive-non-rounded syllables. The method involves passing each image through each model with all possible pseudoword labels, collecting probability scores, and analyzing whether probability patterns align with expected human bouba-kiki associations.

## Key Results
- Only CLIP and GPT-4o showed modest evidence for the bouba-kiki effect in select tests
- The effect disappeared when examining full probability distributions rather than single best-label selection
- Model architecture and training data size influenced cross-modal association strength

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLMs trained on multimodal data can encode cross-modal associations even without explicit auditory input
- Mechanism: Vision-language models learn statistical regularities between visual and linguistic features, and since natural language contains non-arbitrary sound-meaning mappings, these patterns can be picked up from text-image co-occurrence alone
- Core assumption: Sound-symbolic patterns in language are detectable through distributional learning from text-image pairs
- Evidence anchors: CLIP and GPT-4o showing modest alignment in select tests; human language contains non-arbitrary regularities between speech sounds and meaning

### Mechanism 2
- Claim: Model architecture and training objective influence the strength of cross-modal associations
- Mechanism: Dual-stream architectures with modality-specific attention mechanisms are more effective at maintaining separate visual and linguistic representations, enabling better cross-modal alignment when compared to single-stream models
- Core assumption: The attention mechanism design determines how effectively the model bridges the visual-linguistic modality gap
- Evidence anchors: Only CLIP, which uses modality-specific attention mechanisms, displays some evidence of a bouba-kiki effect; the Q-Former in BLIP2 apparently does not promote sound-symbolic associations

### Mechanism 3
- Claim: The way the probing task is structured (syllable selection vs. probability scoring) changes whether the effect appears
- Mechanism: Single best-label selection amplifies small differences in cross-modal association strength, while full probability distributions reveal that these differences are not robust across all labels
- Core assumption: The probing method acts as a filter that can create or obscure apparent patterns in model behavior
- Evidence anchors: When asking the model to select one best-fitting syllable, CLIP and GPT-4o both display the effect in the expected direction. However, this pattern disappears when looking at a richer dataset of probability scores

## Foundational Learning

- Concept: Cross-modal association
  - Why needed here: Understanding the bouba-kiki effect requires recognizing how humans link sounds and shapes, which is the benchmark for model alignment
  - Quick check question: Can you explain why humans reliably associate "kiki" with jagged shapes and "bouba" with rounded shapes?

- Concept: Vision-language model architecture
  - Why needed here: Different architectures (dual-stream vs. single-stream) affect how visual and linguistic features are integrated
  - Quick check question: What is the main difference between dual-stream and single-stream vision-language models?

- Concept: Sound symbolism
  - Why needed here: Sound symbolism is the linguistic basis for the bouba-kiki effect, and understanding it helps explain why models might or might not encode it
  - Quick check question: Give an example of a non-arbitrary mapping between a speech sound and a meaning in any language

## Architecture Onboarding

- Component map: Image + text prompt -> Vision encoder + text encoder -> Modality-specific attention (dual-stream) or merged attention (single-stream) -> Probability distribution over candidate labels

- Critical path:
  1. Load image and tokenize text prompt
  2. Pass through respective encoders
  3. Fuse representations via attention
  4. Compute similarity scores between image and each candidate label
  5. Apply softmax to get probabilities
  6. Analyze highest-scoring or full distribution

- Design tradeoffs:
  - Dual-stream + modality-specific attention: Better alignment with human-like cross-modal associations but more parameters and complexity
  - Single-stream + merged attention: Simpler and more parameter-efficient but weaker cross-modal integration
  - Training data size: Larger datasets improve alignment but increase compute cost

- Failure signatures:
  - Uniform probability distribution across all labels (no preference)
  - Preference based on visual texture rather than shape (texture bias)
  - Artifacts in background regions dominating predictions (visual artifact issue)

- First 3 experiments:
  1. Probe CLIP with original bouba/kiki image pair and single syllables; record top label and full distribution
  2. Compare BLIP2 and ViLT on same task; analyze attention layer outputs for modality-specific vs. merged attention
  3. Generate synthetic curved/jagged shapes; test model consistency across varying image complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does model size consistently predict better alignment with human cross-modal associations like the bouba-kiki effect?
- Basis in paper: The authors note that CLIP, trained on the largest dataset, showed the most evidence for the bouba-kiki effect, while BLIP2, despite having more parameters, did not show the effect
- Why unresolved: The study only compared four models, and model size may interact with other factors like architecture or training objectives
- What evidence would resolve it: Systematic testing across a larger set of models varying only in training data size, while controlling for architecture and training objectives

### Open Question 2
- Question: Would using more naturalistic images instead of synthetic ones yield stronger evidence for the bouba-kiki effect in VLMs?
- Basis in paper: The authors acknowledge that their use of synthetic images may be out-of-domain for models predominantly trained on realistic images
- Why unresolved: The current study's use of synthetic images may have limited the models' ability to demonstrate cross-modal associations
- What evidence would resolve it: Replicating the experiments with more naturalistic images and comparing results to those obtained with synthetic images

### Open Question 3
- Question: How does tokenization affect the bouba-kiki effect in VLMs, and are there model-specific cross-modal associations at the token level?
- Basis in paper: The authors note that tokenization may split syllables or pseudowords into tokens that would not necessarily evoke the expected cross-modal associations in humans
- Why unresolved: The current study used a basic tokenization approach, but the impact of tokenization on cross-modal associations remains unclear
- What evidence would resolve it: Investigating the effect of different tokenization strategies on the bouba-kiki effect and analyzing token-level cross-modal associations in various VLMs

## Limitations

- VLMs show only modest alignment with human bouba-kiki preferences, suggesting cross-modal associations are not robustly encoded
- GPT-4o findings are uncertain due to lack of transparency about its architecture and training data
- Synthetic shapes may not fully capture the complexity of natural forms that humans encounter
- Study does not account for cultural variations in sound-shape associations

## Confidence

- Medium confidence: VLMs do not robustly encode sound-symbolic mappings, as evidenced by inconsistent results across models and tasks
- Low confidence: GPT-4o findings due to lack of transparency about its architecture and training data
- Medium confidence: Model architecture and training data size influence cross-modal associations, supported by comparative results between CLIP and other models

## Next Checks

1. Test the same models on culturally diverse sound-shape associations to determine if results generalize beyond the bouba-kiki effect
2. Create a controlled ablation study comparing models trained on different dataset sizes and compositions to isolate the effect of training data on cross-modal associations
3. Implement attention visualization for the dual-stream models to directly observe how visual and linguistic features interact during cross-modal processing