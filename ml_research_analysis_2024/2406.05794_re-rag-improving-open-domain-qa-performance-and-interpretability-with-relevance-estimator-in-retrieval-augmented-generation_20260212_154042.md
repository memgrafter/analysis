---
ver: rpa2
title: 'RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance
  Estimator in Retrieval-Augmented Generation'
arxiv_id: '2406.05794'
source_url: https://arxiv.org/abs/2406.05794
tags:
- context
- performance
- contexts
- answer
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of performance degradation in retrieval-augmented
  generation (RAG) systems when provided with irrelevant contexts. The authors propose
  a new framework called RE-RAG that incorporates a relevance estimator (RE) module
  to measure both the relative relevance between retrieved contexts and their confidence
  in being useful for answering a given question.
---

# RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2406.05794
- Source URL: https://arxiv.org/abs/2406.05794
- Authors: Kiseung Kim; Jay-Yoon Lee
- Reference count: 13
- Primary result: RE-RAG improves Exact Match scores by 7.7 and F1 by 6.5 on NQ compared to baseline RAG

## Executive Summary
This paper addresses performance degradation in retrieval-augmented generation (RAG) systems when provided with irrelevant contexts. The authors propose RE-RAG, a framework that incorporates a relevance estimator (RE) module to measure both the relative relevance between retrieved contexts and their confidence in being useful for answering a given question. RE-RAG demonstrates significant improvements over traditional RAG on open-domain question answering tasks like Natural Questions and TriviaQA, while also providing interpretability through confidence scores that can guide decoding strategies.

## Method Summary
The RE-RAG framework trains a relevance estimator (RE) module using weakly supervised learning with only question-answer data, without explicit labels for correct contexts. The RE generates "true" or "false" tokens indicating relevance, which are normalized to yield confidence scores. These scores are used to rerank contexts and weight their contribution in answer marginalization. The framework jointly trains RE with the generator using KL-divergence loss to align their relevance judgments. Notably, the RE can be detached and applied to large language models (LLMs), improving their performance without retraining.

## Key Results
- RE-RAG achieves 7.7 EM and 6.5 F1 improvements on NQ compared to baseline RAG
- The RE module can be detached and applied to LLMs, improving their retrieval-augmented performance
- Decoding strategies using RE confidence scores (unanswerable classification, parametric knowledge fallback) enhance model interpretability and performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RE module provides both relative relevance and confidence scores that improve context selection and answer generation.
- Mechanism: RE is trained to generate "true" or "false" tokens indicating relevance, then normalized to yield a confidence score. This score is used both to rerank contexts and to weight their contribution in answer marginalization.
- Core assumption: Confidence scores from the RE correlate with actual utility of contexts for answering the question.
- Evidence anchors: RE provides confidence, which can be used to classify whether given context is useful for answering the given question.

### Mechanism 2
- Claim: Joint training of RE and generator via KL-divergence loss aligns their relevance judgments.
- Mechanism: The generator's per-context log-likelihood is softmaxed to create a relevance distribution QG, which is then matched to the RE's relevance distribution PRE via KL loss. This trains RE to mimic the generator's implicit relevance scoring.
- Core assumption: The generator's likelihood under each context is a valid signal of that context's relevance.
- Evidence anchors: Lre = DKL(PRE(qi, cj)||QG(qi, cj)) represents relative relevance between qi and cj.

### Mechanism 3
- Claim: RE can be detached and applied to LLMs to improve their retrieval-augmented performance.
- Mechanism: RE is trained on a small generator but then used as a standalone reranker/confidence estimator for larger models, enabling better context selection without retraining large models.
- Core assumption: Relevance estimation is transferable across model sizes; the RE's learned relevance patterns generalize.
- Evidence anchors: RE trained on a small generator (sLM) can not only improve the sLM fine-tuned together with RE but also improve previously unreferenced large language models (LLMs).

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG) fundamentals
  - Why needed here: Understanding how RAG combines retrieved contexts with generation is critical to seeing where RE adds value.
  - Quick check question: What is the difference between RAG-sequence and RAG-token models in terms of answer generation?

- Concept: Relevance estimation and reranking in information retrieval
  - Why needed here: RE is essentially a learned reranker; knowing standard reranking metrics (Recall@k, MRR) helps evaluate its effectiveness.
  - Quick check question: How does a reranker typically improve upon a first-stage retriever?

- Concept: Confidence thresholding and decision rules in NLP
  - Why needed here: RE outputs confidence scores used to decide whether to answer, mark as unanswerable, or use parametric knowledge.
  - Quick check question: What is the impact of setting a high vs low confidence threshold in an unanswerable classification task?

## Architecture Onboarding

- Component map: Retriever (bi-encoder DPR-style) → Context set C_i → RE module (T5 seq2seq trained to output "true"/"false") → Generator (T5 or LLM) → Answer marginalization layer (weighted sum using RE scores)

- Critical path: 1. Query → Retriever → Top-k contexts 2. Query+contexts → RE → Relevance scores 3. Top-k contexts → Generator → Per-context answers 4. RE scores → Answer marginalization → Final answer

- Design tradeoffs: RE adds latency but improves accuracy; trade-off between speed and quality. Using RE scores vs. retriever scores: more accurate but potentially overconfident if RE is poorly calibrated. Joint training vs. separate: joint training aligns RE and generator but is more complex.

- Failure signatures: Low RE confidence but high generator confidence → possible hallucination. High RE confidence but low generator confidence → poor context selection. RE overfitting to training distribution → poor generalization to new datasets.

- First 3 experiments: 1. Train RE-RAG on NQ and measure EM on dev set; compare to baseline RAG. 2. Evaluate RE as standalone reranker (Recall@k) on top-100 contexts. 3. Test RE on unseen dataset (e.g., train on NQ, test rerank on TQA) to measure generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the RE-RAG framework perform on multi-hop question answering tasks compared to single-hop tasks?
- Basis in paper: The paper mentions that their research has primarily focused on improving answer performance in single-hop QA tasks and that they have not sufficiently verified the effectiveness of their framework in multi-hop QA tasks.
- Why unresolved: The paper explicitly states that they have not conducted sufficient experiments or analysis on multi-hop QA tasks.
- What evidence would resolve it: Conducting experiments and evaluations of the RE-RAG framework on multi-hop QA datasets and comparing its performance to other state-of-the-art methods on these tasks.

### Open Question 2
- Question: How effective is the RE-RAG framework in detecting truly unanswerable questions that cannot be answered even with the model's parametric knowledge?
- Basis in paper: The paper discusses the limitation of their current approach in detecting truly unanswerable questions and suggests that future research needs to be conducted to detect whether the parametric knowledge has information that can adequately answer the query.
- Why unresolved: The paper acknowledges that their current method of classifying low-confidence contexts as unanswerable is not sufficient for detecting truly unanswerable questions that require the model's parametric knowledge.
- What evidence would resolve it: Developing and evaluating methods to assess the model's parametric knowledge for its ability to answer a given question, and integrating this assessment into the RE-RAG framework's decision-making process for classifying unanswerable questions.

### Open Question 3
- Question: What is the impact of different training strategies and hyperparameters on the performance of the RE module in the RE-RAG framework?
- Basis in paper: The paper mentions that they used default values for hyperparameters and did not explore different training strategies due to time and limited computing resources.
- Why unresolved: The paper did not conduct extensive experiments or ablation studies to investigate the impact of different training strategies and hyperparameter choices on the RE module's performance.
- What evidence would resolve it: Conducting comprehensive ablation studies and hyperparameter tuning experiments to evaluate the effects of different training strategies, loss function weights, learning rates, and other hyperparameters on the RE module's performance in the RE-RAG framework.

## Limitations
- Lack of direct evidence for RE's confidence scores truly correlating with context usefulness beyond observed performance gains
- Cross-model size transfer claim (applying RE trained on small models to LLMs) is under-supported with only empirical performance differences shown
- Weak support for interpretability claims - provides confidence scores but doesn't demonstrate these scores meaningfully explain model decisions

## Confidence
- **High Confidence**: The core claim that RE-RAG improves Exact Match scores on NQ and TriviaQA compared to baseline RAG is well-supported by the experimental results (7.7 EM and 6.5 F1 improvements on NQ).
- **Medium Confidence**: The claim that RE can be detached and applied to improve LLMs is supported by performance numbers but lacks ablation studies showing the RE's specific contribution versus other factors.
- **Low Confidence**: The interpretability claim is the weakest, as the paper provides confidence scores but doesn't demonstrate these scores actually improve human understanding of model decisions or provide meaningful error analysis.

## Next Checks
1. **Confidence Calibration Analysis**: Perform a detailed analysis of RE's confidence score calibration by plotting predicted confidence against actual context utility across different datasets. This would validate whether RE scores truly reflect usefulness rather than just correlating with performance gains.

2. **Ablation Study on RE Transferability**: Create an ablation study isolating the RE's contribution when applied to LLMs by comparing: (a) LLMs with RE reranking vs. (b) LLMs with retriever reranking vs. (c) LLMs alone. This would quantify how much of the improvement comes specifically from the RE versus other factors.

3. **Interpretability Case Studies**: Conduct user studies or detailed error analysis showing how RE confidence scores help humans understand model decisions. Select cases where RE correctly identified irrelevant contexts and analyze whether the confidence scores provide meaningful explanations for these decisions.