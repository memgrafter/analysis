---
ver: rpa2
title: 1-2-3-Go! Policy Synthesis for Parameterized Markov Decision Processes via
  Decision-Tree Learning and Generalization
arxiv_id: '2410.18293'
source_url: https://arxiv.org/abs/2410.18293
tags:
- deadline
- policy
- state
- policies
- instances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of synthesizing policies for
  extremely large parameterized Markov decision processes (MDPs) that are beyond the
  reach of existing tools. The proposed solution generalizes optimal policies from
  small instances to larger ones using decision-tree learning, effectively bypassing
  the need for explicit state-space exploration of large models.
---

# 1-2-3-Go! Policy Synthesis for Parameterized Markov Decision Processes via Decision-Tree Learning and Generalization

## Quick Facts
- arXiv ID: 2410.18293
- Source URL: https://arxiv.org/abs/2410.18293
- Authors: Muqsit Azeem; Debraj Chakraborty; Sudeep Kanav; Jan Kretinsky; Mohammadsadegh Mohagheghi; Stefanie Mohr; Maximilian Weininger
- Reference count: 40
- Primary result: Synthesizes policies for extremely large parameterized MDPs by generalizing from small instances via decision-tree learning, achieving near-optimal values in 13/21 benchmark cases

## Executive Summary
This paper addresses the challenge of synthesizing policies for extremely large parameterized Markov decision processes (MDPs) that are beyond the reach of existing tools. The proposed solution generalizes optimal policies from small instances to larger ones using decision-tree learning, effectively bypassing the need for explicit state-space exploration of large models. The approach achieves near-optimal values for most benchmark cases while scaling constantly with parameter instantiation.

The core innovation is a three-phase approach that selects small base instances, collects optimal policies from these instances, and generalizes them via decision-tree learning. This creates a decision tree representation that can be evaluated for any instantiation of the parameterized MDP without explicitly constructing the large model. Experimental results show the method outperforms state-of-the-art statistical model checking in several cases while maintaining interpretability of the resulting policies.

## Method Summary
The 1-2-3-Go! approach addresses policy synthesis for large parameterized MDPs through three phases: (1) selecting small base instances to learn from, (2) collecting optimal policies from these instances using model checking, and (3) generalizing these policies via decision-tree learning. The method exploits regularities in state space structure, where states are tuples of state variables, and learns predicates over these variables that capture invariant decision logic. Decision trees partition the state space based on variable thresholds, creating policies applicable to any parameter instantiation. The approach bypasses state-space explosion by never explicitly constructing the full MDP for large parameter values, instead synthesizing policies from small base instances that can be solved exactly.

## Key Results
- Near-optimal policy values achieved for 13 out of 21 benchmark cases
- Outperforms state-of-the-art statistical model checking in 2 cases
- Scales constantly with parameter instantiation since it only analyzes a fixed number of small base instances
- Produces interpretable decision tree policies that generalize across different instantiations
- Effective on standard benchmarks including csma, firewire, pacman, and zeroconf protocols

## Why This Works (Mechanism)

### Mechanism 1
Decision-tree policies generalize across different instantiations of parameterized MDPs because optimal policies for smaller instances preserve structural decision logic even when state domains expand. The method exploits regularities in state space structure—states as tuples of state variables—and learns predicates over these variables that capture invariant decision logic. Decision trees partition the state space based on variable thresholds, creating policies applicable to any parameter instantiation. This works because the optimal policy depends primarily on relative state variable relationships rather than absolute parameter values or specific domain sizes.

### Mechanism 2
The approach bypasses state-space explosion by never explicitly constructing the full MDP for large parameter values, instead synthesizing policies from small base instances. Rather than solving the large MDP directly, the method solves multiple small instances where exact methods are feasible, then generalizes these solutions via decision-tree learning. This creates a policy representation that scales constantly with parameter instantiation because the computational cost of decision-tree learning and evaluation is negligible compared to explicit MDP solving, and the learned policy remains effective across parameter ranges.

### Mechanism 3
Decision-tree representation enables explainable and interpretable policies that generalize better than black-box approaches like neural networks. Decision trees use axis-aligned predicates (x > c) that create human-understandable decision logic. This interpretability allows the policy to capture essential decision patterns that remain valid across different instantiations, unlike opaque representations. Explainable policies are more generalizable because they capture fundamental decision principles rather than instance-specific optimizations, making them robust to parameter variations.

## Foundational Learning

- **Concept**: Markov Decision Processes (MDPs) and reachability objectives
  - Why needed here: The entire approach operates on MDPs with reachability objectives, requiring understanding of states, actions, policies, and value computation.
  - Quick check question: What is the difference between a policy and a strategy in MDPs, and how does this relate to the reachability objective?

- **Concept**: Decision tree learning and generalization
  - Why needed here: The core mechanism uses decision-tree learning to generalize policies from small to large instances, requiring understanding of how decision trees partition state spaces and handle multiple labels.
  - Quick check question: How does a decision tree handle cases where the same state has different optimal actions across different base instances?

- **Concept**: Parameterized systems and scalability
  - Why needed here: The approach specifically targets parameterized MDPs where parameter values affect state space size and structure, requiring understanding of how parameters relate to state variables and transitions.
  - Quick check question: What types of parameters (state space vs. transition dynamics) affect the generalizability of decision-tree policies?

## Architecture Onboarding

- **Component map**: Parameter selection module -> MDP solver integration -> Policy collection -> Decision tree learning engine -> Policy evaluation module -> SMC integration
- **Critical path**: Parameter selection → MDP solving → Policy collection → Decision tree learning → Policy evaluation
- **Design tradeoffs**:
  - Small vs. large base instances: Smaller instances are faster to solve but may provide less information for generalization
  - Single vs. multiple base instances: More instances may improve generalization but increase computational cost
  - Axis-aligned vs. complex predicates: Simpler predicates are more interpretable but may limit expressiveness
- **Failure signatures**:
  - Poor performance on large instances despite good performance on base instances
  - Decision tree becoming too large or complex to be useful
  - High variance in policy values across different parameter instantiations
- **First 3 experiments**:
  1. Implement the parameter selection module and verify it can generate valid base instances for a simple parameterized MDP
  2. Integrate with Storm to solve small instances and collect optimal policies, checking that policies are correctly extracted
  3. Implement basic decision tree learning and verify it can generalize from one small instance to slightly larger instances

## Open Questions the Paper Calls Out

### Open Question 1
What are the precise theoretical guarantees (if any) on the quality of policies produced by the 1-2-3-Go! approach for extremely large MDPs? The paper acknowledges that optimal values for such large MDPs are unknown by definition, making optimality unprovable. It states policies can only be "provably good enough" with statistical guarantees via SMC evaluation. This remains unresolved because the method relies on generalization from small instances, which lacks formal guarantees of proximity to optimum for large instances.

### Open Question 2
What specific properties of MDPs make them amenable to successful policy generalization using decision trees? The discussion contrasts successful cases (like csma+some_before) where the generalizing policy is independent of additional modules, with failures (like pacman+crash) where policies cannot be generalized. The paper suggests this relates to whether the optimal policy depends on newly added states/variables. This remains unresolved as the paper identifies this as an open direction for future work.

### Open Question 3
How does the choice of base instances affect the quality of the generalized policy, and what selection strategies minimize the number of instances needed? The paper mentions conducting experiments to observe the effect of different base instance sets, noting that "one or two instances are often already enough" but provides limited analysis of selection strategies. This remains unresolved as the paper only briefly mentions linear and exponential scaling approaches for parameter selection without systematic comparison or theoretical guidance.

## Limitations
- Generalizability depends critically on the choice of base instances and may fail when optimal policies for small instances don't capture essential decision logic for larger ones
- Assumes axis-aligned predicates are sufficient to represent optimal policies, which may not hold for all parameterized MDPs
- Only handles a subset of the QVBS benchmarks and may struggle with more complex parameterized structures

## Confidence

- **High confidence**: The core methodology of using decision-tree learning to generalize policies from small to large instances is sound and well-implemented
- **Medium confidence**: The experimental results showing near-optimal performance on most benchmarks, though limited to a subset of cases
- **Low confidence**: Claims about scalability advantages over existing tools, as the comparison is limited and doesn't fully explore the parameter space

## Next Checks

1. Test the decision tree generalization on a wider range of parameterized MDPs, including those with different types of parameters (state space vs. transition dynamics) to validate the claimed generalizability
2. Systematically vary the choice of base instances to determine how sensitive the approach is to the selection of small instances and identify optimal selection strategies
3. Compare the computational efficiency of the three-phase approach against direct solving of large instances for cases where both are feasible to validate the claimed scalability advantages