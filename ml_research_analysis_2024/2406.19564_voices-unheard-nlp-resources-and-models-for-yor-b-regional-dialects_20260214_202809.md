---
ver: rpa2
title: "Voices Unheard: NLP Resources and Models for Yor\xF9b\xE1 Regional Dialects"
arxiv_id: '2406.19564'
source_url: https://arxiv.org/abs/2406.19564
tags:
- dialects
- standard
- dialect
- translation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the lack of NLP resources for regional dialects\
  \ of Yor\xF9b\xE1, a language spoken by roughly 47 million people. The authors introduce\
  \ YOR\xD9LECT, the first high-quality parallel text and speech corpus across four\
  \ Yor\xF9b\xE1 dialects (Standard Yor\xF9b\xE1, If\xE8, \xCCj\xE8.b\xFA, and \xCC\
  l\xE0j e.) in three domains (religious, news, and Ted talks)."
---

# Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects

## Quick Facts
- arXiv ID: 2406.19564
- Source URL: https://arxiv.org/abs/2406.19564
- Reference count: 40
- Authors: Orevaoghene Ahia; Anuoluwapo Aremu; Diana Abagyan; Hila Gonen; David Ifeoluwa Adelani; Daud Abolade; Noah A. Smith; Yulia Tsvetkov
- Introduces YORÙLECT, the first high-quality parallel text and speech corpus across four Yorùbá dialects in three domains

## Executive Summary
This work addresses the critical gap in NLP resources for regional dialects of Yorùbá, a language spoken by approximately 47 million people. The authors introduce YORÙLECT, the first high-quality parallel text and speech corpus covering four Yorùbá dialects (Standard Yorùbá, Ifè, Ìjè.bú, and Ìlàj e.) across three domains (religious, news, and Ted talks). Through systematic evaluation and dialect-adaptive fine-tuning, the study reveals substantial performance disparities between standard Yorùbá and regional dialects across machine translation, automatic speech recognition, and speech-to-text translation tasks. The dialect-adaptive fine-tuning approach significantly narrows these gaps, demonstrating the potential for developing inclusive NLP technologies that serve all dialect speakers.

## Method Summary
The study employs a multi-stage approach: First, YORÙLECT dataset was collected through native speaker engagement across four Yorùbá dialects in three domains, containing 1506 parallel sentences and ~3 hours of audio per dialect. Second, zero-shot evaluation was conducted using existing models (M2M-100, NLLB, MENYO-20k for MT; Whisper, SeamlessM4T, MMS for ASR/S2TT) to establish baseline performance disparities. Third, dialect-adaptive fine-tuning was performed on the top-performing models using the training portion of YORÙLECT. For MT, both joint training (all dialects under Yorùbá language code) and individual dialect training approaches were explored. The evaluation metrics included BLEU for translation tasks, WER for ASR, and AfriCOMET for translation quality assessment.

## Key Results
- Zero-shot models show substantial performance disparities between Standard Yorùbá and regional dialects across all three tasks
- Dialect-adaptive fine-tuning narrows the performance gap with average increases of 14 BLEU points for MT and 5 BLEU points for S2TT
- ASR performance improves with a 20-point decrease in word-error-rate through fine-tuning
- Joint fine-tuning proves more effective than individual fine-tuning for machine translation tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dialect-adaptive fine-tuning with only 802 training instances per dialect can significantly improve model performance on regional dialects.
- **Mechanism:** The fine-tuning process allows the model to learn the specific linguistic patterns of each dialect, compensating for the lack of robustness in zero-shot settings.
- **Core assumption:** The dialectal differences, while substantial, are learnable with a relatively small amount of targeted data.
- **Evidence anchors:**
  - [abstract]: "dialect-adaptive fine-tuning, narrows this gap, with an average increase of 14 and 5 BLEU points for machine translation and speech-to-text translation respectively, and a 20-point decrease in word-error-rate for automatic speech recognition."
  - [section 4]: "With 802 training instances in each dialect, this approach leads to an average increase of 14 and 5 BLEU points for both MT and S2TT respectively, as well as a 20-point decrease in word-error-rate for ASR."
  - [corpus]: Weak - The paper does not explicitly show the performance on a held-out test set after fine-tuning, only reporting improvements.
- **Break condition:** If the dialectal differences are too complex or the data size is insufficient, the model may not be able to learn the patterns effectively.

### Mechanism 2
- **Claim:** Joint fine-tuning on all dialects is more effective than individual fine-tuning for machine translation tasks.
- **Mechanism:** By training on all dialects simultaneously, the model can leverage shared linguistic features across dialects, leading to better overall performance.
- **Core assumption:** The dialects share enough common linguistic features that joint training can benefit all dialects.
- **Evidence anchors:**
  - [section 7]: "In MT, Table 11 in the Appendix shows a huge drop in performance across all dialects when we finetune on each dialect individually. This suggests that by jointly finetuning, the model leverages shared features across dialects for mutual benefit."
  - [section 4]: "We experiment with training all dialects jointly under the Yoruba language code, and training the dialects separately by adding new language codes for each dialect and initializing them with the Yoruba embedding."
  - [corpus]: Weak - The paper does not provide detailed analysis of the shared features across dialects.
- **Break condition:** If the dialectal differences are too significant, joint training might lead to interference and degrade performance.

### Mechanism 3
- **Claim:** Edit distance is not a reliable predictor of model performance on dialect adaptation tasks.
- **Mechanism:** The paper finds that edit distance does not correlate well with the actual performance of the models on dialect adaptation tasks, suggesting that other factors are more important.
- **Core assumption:** The complexity of dialectal differences cannot be fully captured by simple character-level edit distance.
- **Evidence anchors:**
  - [section 7]: "Here again, we also see no correlations between edit distance and performance on dialect adaptation."
  - [section 4]: "We compute the average edit distance per dialect, d = 1/N sum(d(si, ti)), where N is the number of sentences in the test set of the dialect, s is the sentence in Standard Yoruba, t is the sentence in the corresponding dialect, and d(si, ti) is the edit distance between si and ti at the character-level."
  - [corpus]: Weak - The paper does not explore other potential metrics for dialect similarity.
- **Break condition:** If the dialectal differences are primarily at the character level, edit distance might become a more reliable predictor.

## Foundational Learning

- **Concept:** Tonal languages and diacritics
  - **Why needed here:** Yorùbá is a tonal language, and diacritics play a crucial role in disambiguating word meanings. Understanding this is essential for working with Yorùbá NLP tasks.
  - **Quick check question:** What is the role of diacritics in Yorùbá, and how do they affect the meaning of words?

- **Concept:** Dialect continuum and mutual intelligibility
  - **Why needed here:** Yorùbá encompasses a dialect continuum with several distinct regional dialects. Understanding the concept of a dialect continuum and mutual intelligibility is crucial for analyzing the performance of NLP models on different dialects.
  - **Quick check question:** What is a dialect continuum, and how does it relate to the mutual intelligibility of Yorùbá dialects?

- **Concept:** Low-resource language challenges
  - **Why needed here:** Yorùbá is a low-resource language, and developing NLP technologies for it presents unique challenges. Understanding these challenges is essential for designing effective solutions.
  - **Quick check question:** What are the main challenges in developing NLP technologies for low-resource languages like Yorùbá?

## Architecture Onboarding

- **Component map:** Data collection -> Model training -> Evaluation -> Fine-tuning
- **Critical path:** 1) Collect and curate high-quality parallel text and speech data across dialects 2) Evaluate zero-shot performance of existing models on the collected data 3) Identify performance gaps between standard and regional dialects 4) Fine-tune top-performing models on the collected data to improve performance on regional dialects 5) Evaluate the fine-tuned models and compare with zero-shot performance
- **Design tradeoffs:** Balancing the need for high-quality data with resource constraints; choosing between MT-specific models and LMs based on their strengths and weaknesses; deciding between joint and individual fine-tuning for different tasks
- **Failure signatures:** Poor zero-shot performance on regional dialects; large performance gaps between standard and regional dialects; inability to improve performance on regional dialects through fine-tuning
- **First 3 experiments:** 1) Evaluate zero-shot performance of M2M-100, NLLB, and MENYO-20k on the collected MT data 2) Fine-tune NLLB-600M on the collected MT data and evaluate the improvement in performance on regional dialects 3) Compare the performance of joint and individual fine-tuning strategies on the MT task

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the impact of increasing the size of the YORÙLECT corpus on model performance, particularly for the less-represented dialects like Ìjè.bú?
- **Basis in paper:** [inferred] The authors mention that scaling up the data could potentially bridge the performance gap between standard Yorùbá and the other dialects. They also acknowledge that the limited size of the corpus might not represent perfectly communities and speakers of the dialects.
- **Why unresolved:** The paper only uses a limited amount of data (802 training instances per dialect) and does not explore the effect of increasing the corpus size on model performance.
- **What evidence would resolve it:** Conducting experiments with larger versions of YORÙLECT and comparing model performance on all dialects, especially the less-represented ones, would provide insights into the impact of corpus size on performance.

### Open Question 2
- **Question:** How does the performance of dialect-specific fine-tuning compare to joint fine-tuning when using larger amounts of data for each dialect?
- **Basis in paper:** [inferred] The authors observe that joint training is beneficial for both MT and ASR tasks, but they do not explore the effect of using larger amounts of data for each dialect during individual fine-tuning.
- **Why unresolved:** The paper only uses a limited amount of data (802 training instances per dialect) and does not explore the effect of increasing the data size for individual fine-tuning.
- **What evidence would resolve it:** Conducting experiments with larger amounts of data for each dialect during individual fine-tuning and comparing the results with joint fine-tuning would provide insights into the effectiveness of individual fine-tuning with larger datasets.

### Open Question 3
- **Question:** How do the performance disparities between standard Yorùbá and the other dialects change when using model-based metrics that are specifically designed to handle dialectal variations?
- **Basis in paper:** [explicit] The authors acknowledge that model-based metrics like AfriCOMET could be biased towards standard dialects that their models have been trained on. They suggest that exploring model-based metrics that facilitate robust evaluations on dialectal tasks remains a challenge for future work.
- **Why unresolved:** The paper only uses standard evaluation metrics (BLEU, WER, AfriCOMET) that may not be well-suited for handling dialectal variations.
- **What evidence would resolve it:** Developing and evaluating new model-based metrics specifically designed to handle dialectal variations and comparing their results with standard metrics would provide insights into the impact of metric choice on performance disparities.

## Limitations
- The dialect-specific training data is relatively small (802 instances per dialect), raising questions about scalability
- The evaluation framework focuses primarily on quantitative metrics without deeper qualitative analysis
- The study does not explore linguistic factors driving performance differences between dialects
- Limited exploration of model-based metrics that could be biased toward standard dialects

## Confidence

- **High Confidence**: The existence of performance disparities between Standard Yorùbá and regional dialects (supported by multiple experiments across three different NLP tasks)
- **Medium Confidence**: The effectiveness of dialect-adaptive fine-tuning (improvements are well-documented but dependent on limited training data)
- **Medium Confidence**: The claim that joint fine-tuning is superior for MT tasks (supported by results but lacking detailed linguistic analysis of why this occurs)

## Next Checks

1. **Ablation Study on Training Data Size**: Systematically vary the amount of dialect-specific training data to determine the relationship between data volume and performance gains, helping establish whether the current results would generalize to larger datasets.

2. **Linguistic Feature Analysis**: Conduct a detailed linguistic analysis comparing Standard Yorùbá and regional dialects to identify specific phonological, morphological, or syntactic features that drive model performance differences, validating whether the edit distance metric is indeed inadequate.

3. **Cross-Dialect Transfer Learning**: Test whether models fine-tuned on one regional dialect can effectively transfer to other dialects without additional fine-tuning, providing insight into the relationships between dialects and the generalizability of the approach.