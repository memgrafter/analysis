---
ver: rpa2
title: 'CLMASP: Coupling Large Language Models with Answer Set Programming for Robotic
  Task Planning'
arxiv_id: '2406.03367'
source_url: https://arxiv.org/abs/2406.03367
tags:
- planning
- task
- plan
- action
- skeleton
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLMASP addresses the challenge of grounding LLM-generated plans
  for robotic task execution by coupling LLMs with Answer Set Programming (ASP). The
  approach uses LLMs to generate initial skeleton plans from natural language instructions,
  which are then refined by ASP using the robot's action model and environmental constraints.
---

# CLMASP: Coupling Large Language Models with Answer Set Programming for Robotic Task Planning

## Quick Facts
- arXiv ID: 2406.03367
- Source URL: https://arxiv.org/abs/2406.03367
- Reference count: 40
- Primary result: Achieves over 90% executability rate on VirtualHome tasks by coupling LLMs with ASP

## Executive Summary
CLMASP addresses the challenge of grounding LLM-generated plans for robotic task execution by coupling LLMs with Answer Set Programming (ASP). The approach uses LLMs to generate initial skeleton plans from natural language instructions, which are then refined by ASP using the robot's action model and environmental constraints. Experiments on the VirtualHome platform show that CLMASP achieves over 90% executability rate, significantly outperforming baseline LLM approaches with less than 2% executability. The system effectively combines the general planning capabilities of LLMs with the logical reasoning strengths of ASP to produce executable robot task plans.

## Method Summary
CLMASP employs a two-stage planning approach where an LLM generates abstract skeleton plans from natural language task instructions, followed by ASP refinement to produce executable action sequences. The system integrates a vector database for semantic referring-grounding of objects, mapping natural language object references to actual scene objects through cosine similarity search. The ASP solver encodes the robot's action model with causal rules and environmental constraints to ensure plan executability. The method uses Chain-of-Thought prompting and iterative self-refinement to improve the quality of LLM-generated plans.

## Key Results
- Achieves over 90% executability rate on VirtualHome tasks, compared to less than 2% for baseline LLM approaches
- Successfully handles complex task planning involving thousands of objects and relations in VirtualHome environment
- Demonstrates effective object grounding through vector database, correctly mapping natural language references to scene objects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLMASP achieves high executability by separating high-level planning from low-level constraint satisfaction
- Mechanism: LLM generates abstract skeleton plans (task decomposition, sequence ordering) while ASP refines these skeletons with environmental constraints and action models to produce executable plans
- Core assumption: The skeleton plan captures the essential task structure without needing to be executable, and ASP can reliably fill in missing details
- Evidence anchors:
  - [abstract] "CLMASP initiates with a LLM generating a basic skeleton plan, which is subsequently tailored to the specific scenario using a vector database. This plan is then refined by an ASP program with a robot's action knowledge"
  - [section 4.2] "We call this method CLMASP, a two-level planning approach provided by the LLM's framework and completed by ASP, for generating robot natural language tasks into atomic action sequences"
- Break condition: If the skeleton plan lacks critical task understanding or if the ASP action model is incomplete, the refinement process will fail

### Mechanism 2
- Claim: Vector database with semantic embedding enables accurate object grounding from natural language references
- Mechanism: LLM-generated plans often reference objects that don't exist in the specific environment; the vector database maps natural language object references to actual scene objects through cosine similarity search
- Core assumption: Pretrained embedding models capture meaningful semantic relationships between object names and categories
- Evidence anchors:
  - [section 4.2.3] "For accurately mapping expressions to the correct environmental objects, the RG process involves constructing a vector database for the scene with Milvus [40] and embedding all scene categories using the model text-embedding-ada-002"
  - [section 4.2.3] "The cosine similarity between two vectors ⃗ a(embedding of cq) and ⃗b (embedding of cm) is calculated as follows"
- Break condition: If the embedding model lacks sufficient semantic understanding or the vector database is poorly constructed, object references will be incorrectly grounded

### Mechanism 3
- Claim: ASP's non-monotonic reasoning handles complex constraints that LLMs cannot reliably process
- Mechanism: ASP encodes causal rules about action preconditions, effects, and environmental constraints that ensure generated plans are executable within the robot's action model
- Core assumption: The ASP program can efficiently encode and solve the planning problem with the given skeleton plan and action model
- Evidence anchors:
  - [section 3.2] "Answer set programming (ASP) programs for planning are preferred, as ASP is a non-monotonic logic programming formalism which can effectively handle the nature of non-monotonic causality in action reasoning"
  - [section 4.3] "We developed a series of Python scripts that convert action descriptions into logic programs with answer set semantics, enabling planning through ASP"
- Break condition: If the ASP encoding becomes too complex or the solver cannot handle the computational load, planning will fail

## Foundational Learning

- Concept: Answer Set Programming (ASP) and non-monotonic logic
  - Why needed here: ASP provides the formal reasoning framework that transforms abstract plans into executable sequences while handling complex constraints
  - Quick check question: What makes ASP particularly suitable for robotic task planning compared to classical planning approaches?

- Concept: Vector embeddings and semantic similarity
  - Why needed here: Enables mapping between natural language object references and actual scene objects when the LLM generates incorrect or non-existent object names
  - Quick check question: How does cosine similarity between embeddings help in grounding natural language object references to real scene objects?

- Concept: Chain-of-Thought prompting and self-refinement
  - Why needed here: Improves the quality of LLM-generated skeleton plans by iteratively correcting syntax errors and refining the plan structure
  - Quick check question: Why is iterative self-refinement more effective than single-shot prompting for generating executable plans?

## Architecture Onboarding

- Component map: Task Instruction → LLM Prompt → Skeleton Plan → Syntax Checker → Vector Database Search → ASP Encoding → ASP Solver → Executable Plan
- Critical path: Task Instruction → LLM Prompt → Skeleton Plan → Syntax Checker → Vector Database Search → ASP Encoding → ASP Solver → Executable Plan
- Design tradeoffs:
  - LLM flexibility vs ASP precision: LLMs provide general planning capability but lack constraint satisfaction; ASP provides precise constraint handling but needs well-formed input
  - Vector database accuracy vs computational cost: More accurate embeddings improve grounding but increase computational overhead
  - ASP encoding complexity vs solver performance: Richer action models enable more complex planning but increase computational load
- Failure signatures:
  - Low executability (< 50%): Indicates problems with skeleton plan generation or ASP encoding
  - Long planning time: Suggests ASP encoding is too complex or solver is overwhelmed
  - Incorrect object references: Indicates vector database or embedding model issues
  - Syntax errors in output: Suggests problems with syntax checker or LLM prompting
- First 3 experiments:
  1. Test skeleton plan generation with simple tasks to verify LLM understands task decomposition
  2. Test vector database grounding with controlled object name variations to verify semantic matching
  3. Test ASP refinement with known executable plans to verify solver correctly handles constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would CLMASP perform with open-source LLMs like Llama 3 compared to proprietary models like GPT-4?
- Basis in paper: [explicit] The authors mention they plan to explore open-source LLMs as an alternative to GPT series in future research, with preliminary trials on Llama3 showing encouraging outcomes.
- Why unresolved: The current experiments were conducted using OpenAI's API with GPT-3.5 and GPT-4, limiting direct comparison with open-source alternatives.
- What evidence would resolve it: Conducting the same experiments with Llama 3 and comparing performance metrics (executability rate, goal achievement rate) directly against GPT-4 results.

### Open Question 2
- Question: Can the ASP program generation be fully automated without expert knowledge, and what would be the performance impact?
- Basis in paper: [explicit] The authors state that translating causal rules into ASP programs still requires expert knowledge, constituting approximately 30% of ASP program content.
- Why unresolved: Current ASP generation requires human experts to extract causal models and translate them into ASP rules, limiting scalability.
- What evidence would resolve it: Implementing a system that automatically generates ASP rules from knowledge graphs using inductive logic techniques, and measuring the executability and goal achievement rates compared to expert-generated ASP programs.

### Open Question 3
- Question: What is the computational overhead of CLMASP compared to pure LLM approaches, and how does it scale with environment complexity?
- Basis in paper: [inferred] The authors mention that task planning problems in VirtualHome often involve thousands of objects and relations, requiring well-designed ASP encoding for efficiency, but don't provide direct computational cost comparisons.
- Why unresolved: The paper focuses on executability and goal achievement rates but doesn't analyze the computational resources required by CLMASP versus baseline methods.
- What evidence would resolve it: Benchmarking CLMASP's runtime and memory usage against pure LLM approaches across environments of varying complexity, measuring how performance scales with number of objects and constraints.

## Limitations

- The approach relies on VirtualHome's simplified action model which may not translate directly to real robotic systems with continuous state spaces
- Manual effort required for ASP encoding limits scalability to more complex robotic systems
- Vector database approach assumes sufficient semantic coverage in embedding models, which may fail with novel objects or complex scene configurations

## Confidence

- **High**: The core mechanism of using ASP to refine LLM-generated skeleton plans into executable sequences is well-supported by the experimental results and theoretical foundations
- **Medium**: The vector database approach for object grounding works effectively within VirtualHome's constrained environment, but may face challenges in more complex or open-world scenarios
- **Medium**: The overall system architecture and component integration are sound, but the manual effort required for ASP encoding and the dependence on VirtualHome's simplified model limit broader applicability

## Next Checks

1. **Cross-domain generalization**: Test CLMASP on a real-world robotic platform (e.g., Fetch robot) with continuous state spaces to evaluate how well the approach transfers from VirtualHome's discrete, simplified environment to physical robotics.

2. **Scaling analysis**: Systematically increase the complexity of action models and scene configurations to determine when the ASP solver becomes computationally intractable and identify breaking points in the approach.

3. **Alternative grounding methods**: Compare the vector database approach with other object grounding techniques (e.g., end-to-end neural approaches, rule-based grounding) to assess whether the current method provides sufficient accuracy and efficiency for real-world deployment.