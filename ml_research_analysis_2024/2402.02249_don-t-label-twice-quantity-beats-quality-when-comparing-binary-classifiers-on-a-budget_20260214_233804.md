---
ver: rpa2
title: 'Don''t Label Twice: Quantity Beats Quality when Comparing Binary Classifiers
  on a Budget'
arxiv_id: '2402.02249'
source_url: https://arxiv.org/abs/2402.02249
tags:
- label
- labels
- classifiers
- case
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the optimal allocation of a limited labeling
  budget for comparing the accuracy of two binary classifiers. It shows that, contrary
  to conventional wisdom, it is best to spend the budget on collecting a single label
  for more samples rather than aggregating multiple labels per sample.
---

# Don't Label Twice: Quantity Beats Quality when Comparing Binary Classifiers on a Budget

## Quick Facts
- arXiv ID: 2402.02249
- Source URL: https://arxiv.org/abs/2402.02249
- Reference count: 40
- Primary result: For comparing binary classifiers with limited labeling budget, collecting single labels for more samples is optimal rather than aggregating multiple labels per sample.

## Executive Summary
This paper challenges conventional wisdom about label aggregation in classifier comparison tasks. Through rigorous analysis using Cramér's theorem from large deviation theory, the authors prove that when comparing two binary classifiers with a fixed labeling budget, it is optimal to collect single labels for more samples rather than multiple labels per sample. This finding has significant implications for machine learning benchmark design, suggesting that expending labeling budget on quantity rather than quality leads to better classifier discrimination.

The theoretical results are supported by numerical verification across parameter spaces and demonstrate that the single-label approach provides exponentially better guarantees for benchmarking multiple classifiers than traditional multiple-labels-per-point approaches. The authors also provide sample size bounds superior to those derived from Hoeffding's inequality.

## Method Summary
The paper analyzes the optimal allocation of a limited labeling budget for comparing binary classifier accuracy. The method involves modeling the comparison task using gap indicators (ternary random variables indicating which classifier is correct and which label is correct), then applying Cramér's theorem to calculate error exponents. The authors compare two strategies: collecting m labels per data point with n data points (mn total labels), versus collecting single labels across mn data points. They prove that the latter strategy dominates asymptotically for all reasonable parameter values, and provide numerical verification for the conjecture extending to all finite sample sizes.

## Key Results
- Single labels for more samples is optimal for classifier comparison under a fixed labeling budget
- Cramér theorem analysis shows the single-label approach has superior error exponents compared to label aggregation
- The results provide sample size bounds superior to Hoeffding's inequality
- The approach allows benchmarking exponentially more classifiers while maintaining the same error probability bound

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating multiple labels per sample reduces the effective sample size available for estimating classifier accuracy differences, leading to worse performance in classifier comparison tasks.
- Mechanism: The Cramér theorem-based analysis shows that the error exponent for correctly identifying the better classifier is maximized when using single labels per data point. Multiple labels per point improve label quality but at the cost of reduced sample size, and the gains in label accuracy are outweighed by the loss of information about classifier disagreements.
- Core assumption: The better classifier has a consistent advantage over the worse classifier across all data points, and label noise is independent of classifier performance.
- Evidence anchors:
  - [abstract] "it's best to spend the budget on collecting a single label for more samples"
  - [section] "the gains in label accuracy from aggregation are outweighed by the loss in sample size and information about classifier disagreements"
  - [corpus] Weak evidence - corpus focuses on label quantity/quality trade-offs but not directly on classifier comparison methodology
- Break condition: If label noise systematically aligns with classifier errors (e.g., worse classifier is better at predicting noisy labels), or if the cost of unlabeled data is significant relative to labeling cost.

### Mechanism 2
- Claim: For large sample budgets, the single-label approach provides exponentially better guarantees for benchmarking multiple classifiers than the multiple-labels-per-point approach.
- Mechanism: Using Hoeffding's inequality and union bounds, the single-label strategy allows testing exponentially more classifiers while maintaining the same error probability bound. The Cramér-based bounds are tighter than Hoeffding bounds, showing the single-label approach can benchmark e^(d2-d1)nϵ² times more classifiers.
- Core assumption: Classifier accuracies are stable across test sets and the goal is to rank classifiers rather than estimate precise accuracies.
- Evidence anchors:
  - [abstract] "our results provide sample size bounds superior to what follows from Hoeffding's bound"
  - [section] "moving from an e^(-d1 n ϵ²) to an e^(-d2 n ϵ²) bound for d2 > d1 by not collecting multiple labels per data point allows us to benchmark e^((d2-d1) n ϵ²) times as many classifiers"
  - [corpus] Weak evidence - corpus papers focus on label aggregation but not on the specific exponential improvement in benchmarking capacity
- Break condition: If the primary goal is accurate risk estimation for individual classifiers rather than ranking, or if label noise is highly heterogeneous and systematically disadvantages the better classifier.

### Mechanism 3
- Claim: The ternary structure of the gap indicator G and its behavior under label aggregation uniquely favors the single-label approach for classifier comparison.
- Mechanism: The gap indicator G takes values in {-1, 0, 1} based on which classifier is correct and which label is correct. The Cramér rate calculation shows that for ternary variables, the error exponent is maximized when using single labels. The proof uses the concavity of the majority vote function and algebraic manipulation to show that the single-label approach dominates for all reasonable parameter values.
- Core assumption: The joint distribution of classifier errors and label errors can be parameterized in a way that captures realistic scenarios while maintaining analytical tractability.
- Evidence anchors:
  - [section] "We begin by considering the fully independent case... This assumption allows us to use the equivalence of a single labeler with accuracy M_m(q(x)) and m labelers with accuracies q(x) each"
  - [section] "Lemma 2. For X ternary with P(X = 1) = x, P(X = -1) = y, and P(X = 0) = z, -Λ*_X(0) = log(2√(xy) + z)"
  - [corpus] Weak evidence - corpus papers discuss ternary outcomes but not specifically in the context of label aggregation for classifier comparison
- Break condition: If the true distribution of classifier errors and label errors violates the independence or homogeneity assumptions, or if the aggregation method deviates significantly from simple majority voting.

## Foundational Learning

- Concept: Large deviation theory and Cramér's theorem
  - Why needed here: Provides tight bounds on tail probabilities for sums of random variables, which is essential for analyzing the probability of correctly identifying the better classifier
  - Quick check question: What is the key difference between Cramér's theorem and Hoeffding's inequality in terms of the type of bounds they provide?

- Concept: Ternary random variables and their moment generating functions
  - Why needed here: The gap indicator G is a ternary random variable, and understanding its properties is crucial for the Cramér rate calculation
  - Quick check question: How does the ternary structure of G simplify the calculation of the Cramér rate compared to a general random variable?

- Concept: Stochastic dominance and its application to heterogeneous label accuracy
  - Why needed here: Allows extending the homogeneous case results to scenarios where label accuracy varies across data points
  - Quick check question: Under what conditions does a heterogeneous label accuracy scenario stochastically dominate the homogeneous case in terms of classifier comparison performance?

## Architecture Onboarding

- Component map: Parameter space exploration -> Analytical proof framework -> Hoeffding bound comparison -> Web interface for probability calculator -> Benchmark extension module for multiple classifiers

- Critical path: 
  1. Define parameter space and generate grid points
  2. Calculate exact probabilities using convolution
  3. Apply Cramér theorem to derive asymptotic bounds
  4. Compare single-label vs multi-label performance
  5. Extend to correlated classifiers and heterogeneous scenarios
  6. Package results for web interface

- Design tradeoffs:
  - Numerical precision vs computational efficiency in probability calculations
  - Generality of assumptions vs analytical tractability in proofs
  - Tightness of bounds vs ease of computation and interpretation

- Failure signatures:
  - Numerical instability in probability calculations for extreme parameter values
  - Breakdown of independence assumptions in real-world data
  - Violation of stochastic dominance conditions in heterogeneous scenarios

- First 3 experiments:
  1. Verify the conjecture numerically for a small grid of parameters (n=1,2,3; m=3,5; q=0.6,0.7,0.8)
  2. Compare Hoeffding bounds vs Cramér bounds for a fixed parameter set (p=0.8, ϵ=0.01, k=1500)
  3. Test the effect of correlated classifiers on the single-label vs multi-label comparison

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the single-label strategy remain optimal for all sample sizes n ≥ 1, not just asymptotically?
- Basis in paper: [explicit] The paper states "While we prove our main theorem for sufficiently large sample sizes, we conjecture that the statement holds for all n ≥ 1."
- Why unresolved: The proof relies on Cramér's theorem which is asymptotic, and the authors acknowledge that different methods would be needed to prove the conjecture for small n.
- What evidence would resolve it: Numerical verification showing the single-label approach consistently outperforms m > 1 labels across all n values, or a formal proof extending beyond the asymptotic regime.

### Open Question 2
- Question: Can the assumptions of "No biased label accuracy" (qb ≥ qw) and "No biased heterogeneity" be relaxed while maintaining the optimality of single labels?
- Basis in paper: [explicit] "While we do not study the effects of aggregation for datasets that have already been constructed using multiple labels per instance, we would like to reiterate Denton et al. (2021)'s recommendation to 'Consider what valuable information might be lost through such aggregation'."
- Why unresolved: The current proof involves a series of non-tight inequalities that rely on these assumptions, and the authors suggest these could likely be relaxed at the cost of additional complexity.
- What evidence would resolve it: Identifying specific scenarios where the assumptions fail and demonstrating whether single labels still perform optimally, or developing a modified proof with weaker assumptions.

### Open Question 3
- Question: How does the single-label optimality extend to multi-class classification tasks?
- Basis in paper: [inferred] The paper focuses on binary classification but acknowledges "most notably reward modelling for Reinforcement Learning from Human Feedback (RLHF)" as an important multi-class application, stating "Extending our results to that setting is a challenging open problem."
- Why unresolved: Multi-class classification introduces complexities like class-conditional error probabilities and different aggregation procedures that aren't present in the binary case.
- What evidence would resolve it: Formal analysis of multi-class scenarios showing whether single labels remain optimal, or identifying specific multi-class settings where multiple labels per instance become beneficial.

## Limitations
- The analysis assumes strict independence between classifier errors and label noise, which may not hold in real-world scenarios
- The theoretical bounds are tight for the assumed model, but practical significance needs empirical validation
- The paper doesn't fully explore implications when independence assumptions break down

## Confidence

- High confidence: The main theoretical result showing single-label aggregation dominates for the independent case, supported by rigorous Cramér theorem application and numerical verification across parameter spaces.
- Medium confidence: The extension to heterogeneous label accuracy scenarios via stochastic dominance, as this relies on additional assumptions that may not always hold.
- Medium confidence: The Hoeffding bound comparison and its implications for benchmarking multiple classifiers, as this requires careful interpretation of the bounds' practical significance.

## Next Checks

1. **Empirical validation on real-world datasets**: Test the single-label vs multi-label comparison strategy on established ML benchmarks (ImageNet, CIFAR, etc.) with human-labeled data to verify if the theoretical advantages translate to practical improvements.

2. **Robustness to independence assumption violations**: Design synthetic experiments where classifier errors and label noise are correlated, then measure how quickly the single-label advantage degrades as the correlation increases.

3. **Cost-benefit analysis for different labeling budgets**: Vary the ratio of unlabeled data cost to labeling cost and determine the break-even point where multi-label aggregation becomes preferable, which would inform practical deployment decisions.