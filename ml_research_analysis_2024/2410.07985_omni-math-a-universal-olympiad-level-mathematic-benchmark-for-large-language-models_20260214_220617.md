---
ver: rpa2
title: 'Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language
  Models'
arxiv_id: '2410.07985'
source_url: https://arxiv.org/abs/2410.07985
tags: []
core_contribution: Omni-MATH is a comprehensive benchmark of 4,428 Olympiad-level
  mathematics problems, categorized into 33 sub-domains and 10+ difficulty levels,
  designed to rigorously evaluate large language models' mathematical reasoning. Unlike
  prior benchmarks, it focuses exclusively on mathematics with human-annotated solutions.
---

# Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models

## Quick Facts
- arXiv ID: 2410.07985
- Source URL: https://arxiv.org/abs/2410.07985
- Reference count: 30
- Primary result: Even the strongest models achieve only 52-60% accuracy on Olympiad-level mathematics problems

## Executive Summary
Omni-MATH is a comprehensive benchmark of 4,428 Olympiad-level mathematics problems designed to rigorously evaluate large language models' mathematical reasoning capabilities. Unlike prior benchmarks, it focuses exclusively on mathematics with human-annotated solutions and provides detailed categorization across 33 sub-domains and 10+ difficulty levels. Evaluations on 15 models show that even the strongest, OpenAI o1-mini and o1-preview, achieve only 60.54% and 52.55% accuracy respectively, indicating significant challenges in Olympiad-level reasoning. Open-source models score as low as 36.2%, and the benchmark reveals that models struggle particularly with discrete mathematics and logical reasoning.

## Method Summary
Omni-MATH was constructed by collecting problems from international mathematical competitions, AoPS Wiki, and filtered AoPS Forum discussions. The dataset underwent rigorous manual annotation by graduate and doctoral students to verify solutions and answers. Problems were categorized into 33 sub-domains and 10+ difficulty levels using a hierarchical classification system. Model evaluation was conducted using GPT-4o-based evaluation with consistency checks, and Omni-Judge—a 7B verifier trained on GPT-4o evaluation data—was developed to provide cost-effective assessment with over 90% consistency to GPT-4o judgments.

## Key Results
- OpenAI o1-mini achieved the highest accuracy at 60.54%, while o1-preview scored 52.55% on Omni-MATH
- Open-source models scored as low as 36.2% accuracy, with the average accuracy for all models being 48.15%
- Models show marginally greater aptitude for algebra but struggle significantly with discrete mathematics and logical reasoning problems
- Test-time scaling methods like Best-of-N proved ineffective for Olympiad-level mathematics problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Omni-MATH provides a more rigorous and differentiated evaluation of LLM mathematical reasoning capabilities compared to existing benchmarks.
- Mechanism: By curating 4,428 Olympiad-level problems categorized into 33 subdomains and 10+ difficulty levels with human-annotated solutions, Omni-MATH creates a hierarchical evaluation framework that captures nuanced performance differences across mathematical domains and difficulty levels.
- Core assumption: The classification of problems into subdomains and difficulty levels accurately reflects the true complexity and domain-specific challenges of mathematical reasoning.
- Evidence anchors:
  - [abstract] "Our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels"
  - [section] "We have established a hierarchical classification of mathematical domains, including 33 sub-domains and more than 10 distinct difficulty levels (see Figure 2), allowing for a nuanced analysis of model performance across various mathematical disciplines and levels of complexity"
  - [corpus] Weak evidence - no direct comparison of classification accuracy with human expert judgments

### Mechanism 2
- Claim: Omni-Judge provides a cost-effective and reliable alternative to GPT-4o for evaluating Olympiad-level mathematical solutions.
- Mechanism: By training a 7B verifier on GPT-4o evaluation data with cases in context, Omni-Judge achieves over 90% consistency with GPT-4o judgments while being significantly smaller and more efficient.
- Core assumption: The consistency between Omni-Judge and GPT-4o judgments indicates reliability, and the 7B size provides sufficient reasoning capacity for mathematical verification.
- Evidence anchors:
  - [abstract] "Omni-Judge—a 7B verifier—achieves over 90% consistency with GPT-4o judgments, providing a cost-effective solution for assessing model outputs"
  - [section] "Our Omni-Judge provides achieves over 91% consistency with GPT-4o and 86% consistency with human judgments, providing reliable feedback"
  - [corpus] Moderate evidence - consistency rates are reported but no comparison with other verifier architectures

### Mechanism 3
- Claim: The data collection methodology ensures high-quality, uncontaminated benchmark problems.
- Mechanism: By sourcing problems from official competition pages, AoPS Wiki (with verified solutions), and carefully filtered AoPS Forum discussions with manual annotation verification, the benchmark maintains solution correctness and appropriate difficulty levels.
- Core assumption: The manual annotation process by graduate and doctoral students effectively identifies and corrects errors, and the cross-validation approach ensures reliability.
- Evidence anchors:
  - [section] "we engaged a team of professional annotators, comprised of graduate and doctoral students to verify the solutions and the answers of the dataset manually"
  - [section] "We employ cross-validation to enhance the robustness of our findings, yielding an accuracy rate of 92.7%. After excluding inconsistent cases, we conducted a manual sampling of 150 entries, resulting in an improved accuracy rate of 97.3%"
  - [corpus] Moderate evidence - annotation accuracy is reported but no details on inter-annotator agreement or error analysis

## Foundational Learning

- Concept: Mathematical domain classification and hierarchical knowledge organization
  - Why needed here: Understanding how mathematical problems are categorized into subdomains (algebra, discrete math, geometry, etc.) and difficulty levels is essential for interpreting benchmark results and identifying model weaknesses
  - Quick check question: If a problem involves counting the number of ways to arrange objects with certain constraints, which subdomain would it belong to and why?

- Concept: Olympiad mathematics competition structure and problem selection
  - Why needed here: Knowledge of how mathematical olympiads select and categorize problems helps understand the benchmark's construction and why certain problems are included at specific difficulty levels
  - Quick check question: What distinguishes an International Mathematical Olympiad (IMO) problem from a national olympiad problem in terms of typical difficulty and mathematical sophistication?

- Concept: Test-time scaling methods and their limitations in mathematical reasoning
  - Why needed here: Understanding approaches like Best-of-N and their effectiveness (or lack thereof) for complex mathematical problems is crucial for interpreting why even advanced models struggle with Omni-MATH
  - Quick check question: Why might generating multiple solutions and selecting the best one work well for simple arithmetic problems but fail for complex olympiad-level proofs?

## Architecture Onboarding

- Component map: Web scraping from contest pages -> MathPix conversion to LaTeX -> Initial filtering -> Manual annotation -> Final dataset
- Critical path: Data collection -> Manual annotation verification -> Domain and difficulty classification -> Model evaluation -> Omni-Judge training -> Benchmark deployment
- Design tradeoffs: Comprehensive coverage (4,428 problems) vs. annotation quality control, model-based evaluation vs. rule-based approaches, open-source accessibility vs. proprietary evaluation methods
- Failure signatures: Low inter-annotator agreement during manual verification, poor correlation between difficulty ratings and model performance, Omni-Judge consistency dropping below acceptable thresholds
- First 3 experiments:
  1. Evaluate a small subset (100 problems) across multiple annotators to measure inter-annotator agreement and refine annotation guidelines
  2. Test GPT-4o evaluation consistency on a sample of 50 problems with human judgments to validate the evaluation methodology
  3. Train Omni-Judge on a subset of problems and evaluate its consistency with GPT-4o on a held-out validation set to optimize model selection and training parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can test-time scaling techniques like Best-of-N be effectively adapted for Olympiad-level mathematics problems?
- Basis in paper: Explicit - "The commonly used approach for test-time scaling, Best-of-N, has proven ineffective for Olympiad-level mathematics problems"
- Why unresolved: The paper identifies ineffectiveness but doesn't explore alternative test-time scaling approaches or explain why Best-of-N specifically fails
- What evidence would resolve it: Comparative analysis of different test-time scaling methods (e.g., self-consistency, verifiers, multiple choice fine-tuning) applied to Omni-MATH problems

### Open Question 2
- Question: What specific aspects of discrete mathematics and logical reasoning make them particularly challenging for LLMs compared to other mathematical domains?
- Basis in paper: Explicit - "LLMs show a marginally greater aptitude for solving algebra, while struggling significantly with discrete mathematics" and "LLMs frequently fail to solve discrete mathematics problems and often make logical missteps"
- Why unresolved: The paper observes these difficulties but doesn't analyze the underlying reasons for the domain-specific challenges
- What evidence would resolve it: Detailed error analysis comparing solution approaches across mathematical domains, identifying common failure patterns in discrete math/logic problems

### Open Question 3
- Question: How does the difficulty consistency of Omni-MATH problems correlate with human experts' perception of problem difficulty?
- Basis in paper: Explicit - "We observe that the consistency of all models is positive, indicating that as the difficulty increases, the overall accuracy of all models declines"
- Why unresolved: The paper validates internal difficulty consistency but doesn't compare with human difficulty assessments or explain potential discrepancies
- What evidence would resolve it: Human expert evaluation of Omni-MATH problems' difficulty levels compared with the automated difficulty classification system

## Limitations
- The manual annotation process lacks detailed reporting on inter-annotator agreement and error analysis, raising questions about consistency across the 4,428 problems
- The effectiveness of the 33-domain classification system is assumed rather than empirically validated, with no comparison to alternative categorization schemes
- The generalizability of the benchmark findings to other mathematical reasoning tasks remains to be fully established

## Confidence
- **High confidence**: The benchmark's core claim of providing a comprehensive, Olympiad-level evaluation dataset is well-supported by the detailed problem categorization and human-annotated solutions
- **Medium confidence**: The claim about Omni-Judge achieving over 90% consistency with GPT-4o judgments is supported by reported metrics, but lacks comparison with other verifier architectures
- **Low confidence**: The assumption that the hierarchical classification system accurately captures the true complexity and domain-specific challenges of mathematical reasoning is not empirically validated

## Next Checks
1. **Inter-annotator Agreement Analysis**: Conduct a systematic study where multiple annotators independently classify 200 randomly selected problems into subdomains and difficulty levels, then measure Cohen's kappa for agreement
2. **Cross-benchmark Validation**: Evaluate the same set of models on Omni-MATH and at least two other established mathematical reasoning benchmarks to assess whether performance patterns are consistent across different evaluation frameworks
3. **Omni-Judge Generalization Test**: Evaluate Omni-Judge on a held-out test set of problems from mathematical domains and difficulty levels not represented in its training data to assess its ability to generalize beyond its training distribution