---
ver: rpa2
title: 'MAIR: A Massive Benchmark for Evaluating Instructed Retrieval'
arxiv_id: '2410.10127'
source_url: https://arxiv.org/abs/2410.10127
tags:
- retrieval
- tasks
- question
- arxiv
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAIR, a massive benchmark designed to evaluate
  instruction-following capabilities in information retrieval (IR) models. Unlike
  existing benchmarks that focus on limited tasks, MAIR comprises 126 diverse IR tasks
  across 6 domains, including web, academic, code, legal, finance, and medical domains,
  with 805 manually annotated instructions.
---

# MAIR: A Massive Benchmark for Evaluating Instructed Retrieval

## Quick Facts
- arXiv ID: 2410.10127
- Source URL: https://arxiv.org/abs/2410.10127
- Authors: Weiwei Sun; Zhengliang Shi; Jiulong Wu; Lingyong Yan; Xinyu Ma; Yiding Liu; Min Cao; Dawei Yin; Zhaochun Ren
- Reference count: 18
- Primary result: Instruction-tuned models significantly outperform non-instruction-tuned models when provided with instructions, achieving an average nDCG@10 of 48.40

## Executive Summary
MAIR is a comprehensive benchmark designed to evaluate instruction-following capabilities in information retrieval models across diverse domains and tasks. The benchmark addresses the limitation of existing IR benchmarks that focus on limited task types by providing 126 diverse IR tasks across 6 domains with 805 manually annotated instructions. Through extensive evaluation of state-of-the-art instruction-tuned and non-instruction-tuned models, MAIR demonstrates that instruction tuning significantly improves retrieval performance when instructions are provided, while also revealing that current models still struggle with complex instruction-following tasks.

## Method Summary
MAIR collects 126 IR tasks from existing resources, benchmarks, TREC tracks, and LLM evaluation datasets across 6 domains (Web, Academic, Code, Medical, Legal, Finance). The benchmark employs K-means clustering for efficient query sampling and manually annotates 805 instructions following a standardized format. Evaluation is conducted using nDCG@10 as the primary metric, comparing various retrieval models (sparse, single-task, multi-task, instruction-tuned embeddings, re-rankers) under two settings: without instruction and with instruction input. The benchmark validates that sampled data maintains high correlation with full-scale testing while ensuring computational efficiency.

## Key Results
- Instruction-tuned models like GritLM-7B achieve an average nDCG@10 of 48.40, significantly outperforming non-instruction-tuned models when instructions are provided
- Non-instruction-tuned models experience performance drops when instructions are added, demonstrating the importance of instruction tuning
- Current instruction-tuned models still struggle with complex instruction-following tasks, particularly in IFEval subtasks
- The benchmark reveals domain-specific performance variations, with instruction-tuned models showing particular strength in Web and Academic domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MAIR improves instruction-following evaluation by covering diverse task types and domains.
- Mechanism: The benchmark aggregates 126 distinct IR tasks from 6 domains, ensuring that instruction-tuned models must generalize beyond narrow task scopes. The tasks are selected from varied sources (SIGIR tracks, BEIR, KILT, TREC, LLM datasets) to represent realistic instruction-following scenarios.
- Core assumption: Diversity in task types and domains leads to better generalization assessment.
- Evidence anchors:
  - [abstract] "MAIR comprises 126 diverse IR tasks across 6 domains, including web, academic, code, legal, finance, and medical domains"
  - [section 2.1] "Finally, we collect 126 tasks. There is a simple process for us to collect data: We first review the documentation of various conference tracks, merging tasks with identical corpus and settings to avoid duplication."
- Break condition: If the diversity in task types and domains does not correlate with improved generalization, the benchmark's effectiveness would be limited.

### Mechanism 2
- Claim: Instruction annotations enable more precise evaluation of instruction-following capabilities.
- Mechanism: MAIR provides 805 manually annotated instructions, each specifying the query, document, and relevance criteria. This detailed annotation allows models to be evaluated on their ability to understand and follow instructions in completing unseen tasks.
- Core assumption: Detailed instructions are necessary for evaluating instruction-following capabilities.
- Evidence anchors:
  - [abstract] "The benchmark also reveals that current instruction-tuned models still struggle with complex instruction-following tasks, such as those in the IFEval subtasks."
  - [section 2.3] "Following Asai et al. (2022), the basic format of the instruction describes the query, the passages, and the relevance criterion."
- Break condition: If models can perform well on the benchmark without utilizing the provided instructions, the value of the annotations would be diminished.

### Mechanism 3
- Claim: Data sampling maintains evaluation efficiency without compromising reliability.
- Mechanism: MAIR uses K-means clustering to sample diverse queries and randomly samples documents to balance the dataset. This approach ensures that the benchmark remains computationally efficient while preserving the diversity and representativeness of the tasks.
- Core assumption: Sampled data can represent the full dataset effectively.
- Evidence anchors:
  - [section 2.2] "We validate that the sampled benchmark produces results highly correlated with full-scale testing, achieving a balance between evaluation accuracy and cost."
  - [section 2.2] "Figure 3 illustrates the correlation of different test sets cut off. We can see that retaining more instances leads to a higher correlation, i.e., the evaluation is more robust."
- Break condition: If the correlation between sampled and full-scale results drops significantly, the sampling method would need revision.

## Foundational Learning

- Concept: Understanding the importance of task diversity in benchmarking.
  - Why needed here: To appreciate why MAIR's approach of aggregating tasks from various domains and sources is beneficial for evaluating instruction-following capabilities.
  - Quick check question: Why is it important to have a diverse set of tasks in a benchmark like MAIR?

- Concept: Grasping the role of instruction annotations in evaluating model performance.
  - Why needed here: To understand how the detailed instructions in MAIR enable a more precise assessment of a model's ability to follow instructions.
  - Quick check question: How do instruction annotations contribute to the evaluation of instruction-following capabilities in MAIR?

- Concept: Recognizing the significance of data sampling in maintaining benchmark efficiency.
  - Why needed here: To comprehend how MAIR's data sampling approach balances evaluation accuracy with computational efficiency.
  - Quick check question: How does data sampling help in maintaining the efficiency of a large-scale benchmark like MAIR?

## Architecture Onboarding

- Component map: Data Collection Module -> Data Sampling Module -> Instruction Annotation Module -> Evaluation Module
- Critical path: Collect diverse tasks from various sources → Sample data for efficiency using K-means clustering → Annotate 805 instructions following standardized format → Evaluate models using nDCG@10 metric under instruction and non-instruction settings
- Design tradeoffs: The tradeoff between task diversity and computational efficiency is managed through data sampling. While more diverse tasks would be ideal, sampling ensures the benchmark remains manageable.
- Failure signatures: If models perform similarly well with and without instructions, it indicates that the instructions are not effectively testing instruction-following capabilities. If evaluation results are not correlated with full-scale testing, the sampling method may need adjustment.
- First 3 experiments:
  1. Evaluate a non-instruction-tuned model on MAIR with and without instructions to observe performance changes.
  2. Compare the performance of instruction-tuned models on MAIR versus other benchmarks to assess generalization.
  3. Analyze the correlation between sampled and full-scale evaluation results to validate the sampling method's effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does instruction tuning impact retrieval performance across different task domains, and are there specific domains where it consistently underperforms?
- Basis in paper: The paper reports domain-specific results showing that instruction-tuned models like GritLM-7B outperform non-instruction-tuned models in most domains but still struggle in specific long-tail tasks. However, it does not analyze whether certain domains consistently show weaker performance.
- Why unresolved: The paper provides aggregate results across domains but does not investigate domain-specific trends or identify consistent weaknesses in instruction tuning.
- What evidence would resolve it: A detailed analysis of instruction-tuned models' performance across each domain, identifying domains where performance is consistently lower or where instruction tuning has minimal impact.

### Open Question 2
- Question: What is the optimal size of the instruction-tuning dataset for improving retrieval performance, and does performance plateau or degrade with larger datasets?
- Basis in paper: The paper evaluates models like GritLM-7B and NV-Embed-v1, which are instruction-tuned on large datasets, but does not explore the relationship between dataset size and performance.
- Why unresolved: The paper does not experimentally test how varying the size of the instruction-tuning dataset affects retrieval performance, leaving the optimal dataset size unknown.
- What evidence would resolve it: A controlled study varying the size of the instruction-tuning dataset and measuring its impact on retrieval performance across multiple tasks.

### Open Question 3
- Question: How sensitive are instruction-tuned retrieval models to the phrasing and structure of instructions, and can small changes in instructions significantly impact performance?
- Basis in paper: The paper mentions that non-instruction-tuned models often perform worse when instructions are added, suggesting sensitivity to instruction format. However, it does not investigate how specific phrasing or structure affects performance.
- Why unresolved: The paper does not conduct experiments to test how different instruction phrasings or structures influence retrieval performance.
- What evidence would resolve it: A systematic evaluation of instruction-tuned models using varied instruction phrasings and structures to measure performance changes.

### Open Question 4
- Question: Can instruction-tuned retrieval models generalize to unseen languages or multilingual retrieval tasks, and what are the limitations of current models in this regard?
- Basis in paper: The paper explicitly states that MAIR focuses only on English text retrieval and does not study multilingual settings, highlighting a limitation of the current benchmark.
- Why unresolved: The paper does not evaluate instruction-tuned models on multilingual tasks, leaving their generalization to other languages untested.
- What evidence would resolve it: Evaluating instruction-tuned retrieval models on multilingual datasets to assess their ability to generalize across languages.

## Limitations

- The benchmark focuses exclusively on English text retrieval, limiting generalizability to multilingual instruction-following scenarios
- Manual annotation of 805 instructions, while extensive, may contain biases or inconsistencies that could affect model evaluation
- The correlation between sampled and full-scale evaluation results, while reported as high, may not hold across all task types and domains
- The benchmark does not investigate how instruction phrasing and structure impact model performance

## Confidence

**High Confidence:**
- MAIR successfully demonstrates that instruction-tuned models outperform non-instruction-tuned models when instructions are provided
- The benchmark effectively reveals that current instruction-tuned models still struggle with complex instruction-following tasks

**Medium Confidence:**
- The data sampling approach maintains evaluation efficiency without compromising reliability across all task types
- MAIR provides a comprehensive testbed for evaluating generalization and instruction-following capabilities

**Low Confidence:**
- The diversity in task types and domains directly correlates with improved generalization assessment
- The benchmark's effectiveness extends equally well to all six domains covered

## Next Checks

1. **Correlation Validation**: Conduct additional experiments to verify the correlation between sampled and full-scale evaluation results across all 126 tasks, particularly for underrepresented domains like medical and legal.

2. **Cross-Domain Generalization**: Evaluate the same instruction-tuned models on MAIR and other benchmarks to quantify their ability to generalize instruction-following capabilities across different domains and task types.

3. **Instruction Utilization Analysis**: Perform ablation studies to determine whether models are truly leveraging the provided instructions or achieving high performance through other means, by comparing results with synthetically corrupted or missing instructions.