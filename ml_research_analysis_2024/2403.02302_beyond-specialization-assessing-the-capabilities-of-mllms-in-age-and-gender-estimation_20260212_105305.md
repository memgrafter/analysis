---
ver: rpa2
title: 'Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender
  Estimation'
arxiv_id: '2403.02302'
source_url: https://arxiv.org/abs/2403.02302
tags:
- gender
- estimation
- tasks
- face
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares the capabilities of multimodal large language
  models (MLLMs) to a specialized age and gender estimation model (MiVOLO) across
  several benchmarks. The authors evaluate several MLLMs, including ChatGPT-4V, LLaVA,
  and ShareGPT4V, and fine-tune ShareGPT4V on a large dataset for this task.
---

# Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation

## Quick Facts
- arXiv ID: 2403.02302
- Source URL: https://arxiv.org/abs/2403.02302
- Authors: Maksim Kuprashevich; Grigorii Alekseenko; Irina Tolstykh
- Reference count: 40
- Primary result: Fine-tuned ShareGPT4V outperforms specialized MiVOLO model on age estimation tasks

## Executive Summary
This paper investigates whether multimodal large language models (MLLMs) can match or exceed specialized age and gender estimation models without being explicitly designed for these tasks. The authors evaluate several MLLMs including ChatGPT-4V, LLaVA, and ShareGPT4V, comparing their performance against the specialized MiVOLO model across multiple benchmarks. They find that with fine-tuning on large datasets, general-purpose MLLMs can achieve state-of-the-art results in age estimation, while showing varying confidence levels in gender recognition tasks.

## Method Summary
The authors evaluate multiple MLLMs (ChatGPT-4V, LLaVA, ShareGPT4V) and fine-tune ShareGPT4V-7B on the LAGENDAext dataset with specific hyperparameters (2×10⁻⁶ learning rate, 32 batch size, 1 epoch, LoRA disabled). They use body and face crops from LAGENDA, IMDB-clean, Adience, and Wild104 datasets, evaluating performance using MAE, CS@5, accuracy for gender, and MAPE for age. The fine-tuning process involves specific prompt formats and temperature settings (0.0) during evaluation.

## Key Results
- Fine-tuned ShareGPT4V-7B achieves MAE of 4.92 on IMDB-clean, outperforming MiVOLO's 5.17
- ChatGPT-4O excels at age estimation but shows less confidence in gender recognition tasks
- MLLMs process both face and body crops simultaneously, providing more information than specialized models using only face crops
- ChatGPT API has 21% failure rate where it refuses to process images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning MLLMs on large target datasets enables them to outperform specialized models in age and gender estimation tasks.
- Mechanism: Training on large, diverse datasets with clear age and gender labels allows MLLMs to learn robust feature representations even without original design for these tasks.
- Core assumption: Fine-tuning dataset is sufficiently large and diverse to capture complexity of age and gender variations.
- Evidence anchors: Abstract mentions achieving state-of-the-art results through fine-tuning; section 4.3 identifies optimal hyperparameters for ShareGPT4V-7B.
- Break condition: If fine-tuning dataset is too small or not diverse enough, MLLM may not generalize well to unseen data.

### Mechanism 2
- Claim: MLLMs can leverage general knowledge and reasoning abilities to perform well on specialized tasks like age and gender estimation, even without fine-tuning.
- Mechanism: MLLMs trained on vast amounts of data develop strong world understanding that can be applied to specialized tasks.
- Core assumption: Pre-training data includes sufficient information related to age and gender estimation.
- Evidence anchors: Abstract notes MLLMs' strong general knowledge and reasoning abilities; section 1 highlights their high accuracy and generalization capability.
- Break condition: If pre-training data lacks sufficient information related to age and gender estimation, MLLM may not perform well without fine-tuning.

### Mechanism 3
- Claim: MLLMs can process both face and body crops simultaneously, providing more information for age and gender estimation compared to specialized models that typically use only face crops.
- Mechanism: Incorporating information from both face and body allows MLLMs to capture additional cues like body proportions, clothing style, and overall appearance.
- Core assumption: Additional information from body crops is relevant and helpful for age and gender estimation.
- Evidence anchors: Section 2 notes MiVOLO uses face and body crops; section 4.3 shows finest results with ShareGPT4V-7b trained with whole body crops.
- Break condition: If body crops don't provide relevant information or introduce noise/biases, MLLM performance may not improve or may degrade.

## Foundational Learning

- Concept: Fine-tuning
  - Why needed here: To adapt MLLM to specific task of age and gender estimation by training on large, labeled dataset
  - Quick check question: What is the purpose of fine-tuning an MLLM, and how does it differ from pre-training?

- Concept: Multimodal learning
  - Why needed here: To understand how MLLMs process and integrate information from both visual and textual modalities for tasks like age and gender estimation
  - Quick check question: What are key differences between unimodal and multimodal models, and how do they impact performance on tasks like age and gender estimation?

- Concept: Generalization
  - Why needed here: To understand how MLLMs apply learned knowledge to new, unseen data and tasks such as age and gender estimation
  - Quick check question: What factors contribute to generalization ability of MLLMs, and how can it be evaluated?

## Architecture Onboarding

- Component map: Input image -> Vision encoder -> Text encoder -> Cross-modal attention -> Prediction head

- Critical path:
  1. Input image processed by vision encoder to extract visual features
  2. Input prompt encoded by text encoder
  3. Visual and textual features aligned and integrated using cross-modal attention
  4. Integrated features passed through prediction head to generate age and gender predictions

- Design tradeoffs:
  - Model size: Larger models may have better performance but require more computational resources
  - Input resolution: Higher resolution inputs provide more detailed information but increase computational cost
  - Fine-tuning vs. zero-shot: Fine-tuning improves performance but requires labeled data, while zero-shot relies on model's general knowledge

- Failure signatures:
  - Poor performance on age and gender estimation tasks
  - Inconsistent predictions across similar inputs
  - High computational cost or memory usage
  - Difficulty processing certain types of images or prompts

- First 3 experiments:
  1. Evaluate MLLM performance on held-out test set to assess generalization ability
  2. Compare performance of MLLM with and without fine-tuning to understand impact of fine-tuning
  3. Analyze predictions on challenging cases to identify potential failure modes and areas for improvement

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- High refusal rate (21%) with ChatGPT API where model refuses to process images, creating practical deployment concerns
- Fine-tuning dataset (LAGENDAext) is not publicly available, making independent verification difficult
- Paper does not address potential biases in age and gender predictions across different demographic groups

## Confidence

**High confidence**: Fine-tuned ShareGPT4V-7B outperforms MiVOLO on age estimation (MAE 4.92 vs 5.17 on IMDB-clean) is well-supported by experimental results and consistent across multiple datasets.

**Medium confidence**: ChatGPT-4O excels at age estimation but shows less confidence in gender recognition, though high refusal rate (21%) introduces uncertainty about whether this reflects true model capability or API limitations.

**Low confidence**: MLLMs can leverage general knowledge and reasoning abilities for age and gender estimation without fine-tuning is weakly supported, as paper provides limited evidence about pre-training data's relevance to these tasks.

## Next Checks

1. Reproduce fine-tuning results with public data: Replicate fine-tuning experiments using publicly available dataset to verify ShareGPT4V-7B consistently outperforms MiVOLO across different training datasets.

2. Analyze bias across demographic groups: Evaluate MLLM predictions for systematic biases across age ranges, genders, and ethnicities to identify potential fairness issues affecting real-world deployment.

3. Test zero-shot performance on specialized data: Assess whether MLLMs can perform competitively on age and gender estimation tasks without fine-tuning when provided with detailed prompts, to better understand role of general knowledge versus task-specific training.