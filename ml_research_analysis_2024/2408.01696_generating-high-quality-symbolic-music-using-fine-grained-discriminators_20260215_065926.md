---
ver: rpa2
title: Generating High-quality Symbolic Music Using Fine-grained Discriminators
arxiv_id: '2408.01696'
source_url: https://arxiv.org/abs/2408.01696
tags:
- music
- melody
- discriminator
- rhythm
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating high-quality symbolic
  music, which involves creating MIDI-like sequences that represent musical compositions.
  Existing methods typically use a single discriminator to evaluate the global quality
  of generated music, but this approach may not effectively capture the distinct characteristics
  of melody and rhythm, two primary dimensions of music.
---

# Generating High-quality Symbolic Music Using Fine-grained Discriminators

## Quick Facts
- arXiv ID: 2408.01696
- Source URL: https://arxiv.org/abs/2408.01696
- Authors: Zhedong Zhang; Liang Li; Jiehua Zhang; Zhenghui Hu; Hongkui Wang; Chenggang Yan; Jian Yang; Yuankai Qi
- Reference count: 25
- Key outcome: Novel architecture with fine-grained discriminators for melody and rhythm achieves superior performance on POP909 benchmark for symbolic music generation

## Executive Summary
This paper addresses the challenge of generating high-quality symbolic music by proposing a novel architecture with fine-grained discriminators specifically designed for melody and rhythm. The authors decouple melody and rhythm information from input music sequences and use separate discriminators for each aspect, with the melody discriminator employing pitch augmentation and the rhythm discriminator using bar-level relative positional encoding. The method demonstrates superior performance compared to state-of-the-art methods on the POP909 benchmark dataset, achieving better objective metrics and higher subjective quality scores in listening tests.

## Method Summary
The method uses a seq2seq transformer generator with NLL loss combined with adversarial losses from two fine-grained discriminators. The melody discriminator employs pitch augmentation (uniformly raising or lowering absolute pitch) to improve generalization across voice parts, while the rhythm discriminator uses bar-level relative positional encoding to capture rhythmic patterns within bar structures. The generator and discriminators are pre-trained separately before joint adversarial training using hyperparameters ùõº=0.15, ùõΩ=0.15 for 100 epochs. Evaluation uses objective metrics (pitch class entropy, scale consistency, groove consistency, divergences, MIDI-BERT similarity) and subjective listening tests.

## Key Results
- Achieves better pitch class entropy, scale consistency, and pitch divergence than baseline models, indicating improved melody generation
- Shows better groove consistency and velocity divergence, reflecting enhanced rhythm control
- Demonstrates improved MIDI-BERT similarity and higher subjective scores in coherence, richness, and overall quality compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Decoupling melody and rhythm allows discriminators to focus on specific musical dimensions, providing more precise feedback to the generator
- The melody-rhythm decoupling module masks note velocity tokens to extract melody and pitch tokens to extract rhythm, enabling specialized discriminators to learn distinct features
- Core assumption: Melody and rhythm are separable musical properties that can be independently modeled without losing essential information about music's quality

### Mechanism 2
- Pitch augmentation strategy improves melody discriminator's ability to recognize melodies across different voice parts and absolute pitch variations
- The melody discriminator applies uniform pitch shifts during training to create augmented samples that simulate different voice parts
- Core assumption: Absolute pitch variations don't fundamentally change the melodic contour or quality that defines good melody

### Mechanism 3
- Bar-level relative positional encoding helps rhythm discriminator better capture rhythmic patterns by focusing on local structure within bars
- The rhythm discriminator uses relative positional encoding that accumulates from the beginning of each bar and resets at the next bar
- Core assumption: Rhythmic patterns are primarily organized at the bar level, and relative positions within bars are more informative for rhythm discrimination than absolute positions

## Foundational Learning

- Concept: Adversarial training with multiple discriminators
  - Why needed here: Single discriminator provides global feedback that may not capture distinct characteristics of melody and rhythm, limiting generator's ability to produce high-quality music in both dimensions
  - Quick check question: Why might a single discriminator be insufficient for symbolic music generation compared to text generation?

- Concept: Positional encoding in transformers
  - Why needed here: Music sequences require positional information to maintain temporal relationships between notes, and bar-level encoding specifically helps capture rhythmic patterns
  - Quick check question: How does bar-level relative positional encoding differ from standard sinusoidal positional encoding in transformers?

- Concept: Data augmentation through pitch shifting
  - Why needed here: Pitch augmentation helps melody discriminator generalize across different voice parts and absolute pitch variations, improving ability to evaluate melodies regardless of starting pitch
  - Quick check question: What musical property remains invariant under pitch shifting that makes this augmentation useful for melody discrimination?

## Architecture Onboarding

- Component map: Generator (seq2seq transformer with NLL loss + adversarial losses) ‚Üí Melody Discriminator (encoder-only transformer with pitch augmentation) ‚Üí Rhythm Discriminator (encoder-only transformer with bar-level relative positional encoding) ‚Üí Feedback loop for adversarial training

- Critical path: Generator output ‚Üí Decoupling module (masks velocity/pitch tokens) ‚Üí Fine-grained discriminators (melody and rhythm) ‚Üí Adversarial loss computation ‚Üí Generator parameter updates

- Design tradeoffs: Decoupling provides focused feedback but may lose some cross-modal interactions between melody and rhythm; pitch augmentation improves generalization but may obscure pitch-specific qualities; bar-level encoding captures local rhythm but may miss long-range rhythmic patterns

- Failure signatures: If generator produces music that sounds coherent but scores poorly on objective metrics, discriminators may be too strict or miss important musical qualities; if one discriminator dominates training, corresponding musical dimension may be over-optimized at expense of the other

- First 3 experiments:
  1. Train with only melody discriminator (wMo variant) and evaluate pitch-related metrics to verify melody-specific improvements
  2. Train with only rhythm discriminator (wRo variant) and evaluate rhythm-related metrics to verify rhythm-specific improvements
  3. Train with both discriminators and compare to single-discriminator baseline (WGAN) on overall MIDI-BERT similarity to verify combined benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method scale with larger and more diverse music datasets beyond POP909?
- Basis in paper: The authors evaluate their method on the POP909 benchmark dataset, but the scalability and generalizability to larger, more diverse datasets is not explored
- Why unresolved: The paper focuses on demonstrating the effectiveness of the fine-grained discriminators architecture on a single benchmark dataset
- What evidence would resolve it: Experiments evaluating the method's performance on larger and more diverse music datasets, such as MAESTRO or Lakh MIDI Dataset

### Open Question 2
- Question: How do the fine-grained discriminators impact the long-term structure and coherence of generated music compositions?
- Basis in paper: The authors mention that their method outperforms baseline models in terms of coherence in subjective listening tests, but the impact on long-term structure is not explicitly analyzed
- Why unresolved: While the fine-grained discriminators provide more precise feedback to the generator, their effect on the overall structure and coherence of longer music compositions is not thoroughly investigated
- What evidence would resolve it: Analyzing the generated music samples for long-term structure and coherence, possibly using metrics that capture global musical patterns and themes

### Open Question 3
- Question: Can the proposed method be extended to generate multi-track music compositions with distinct melodic and rhythmic patterns for each track?
- Basis in paper: The authors focus on generating single-track piano music, but the potential extension to multi-track compositions is not explored
- Why unresolved: While the fine-grained discriminators architecture effectively captures melody and rhythm in single-track music, its application to multi-track compositions with distinct melodic and rhythmic patterns for each track is not investigated
- What evidence would resolve it: Experiments generating multi-track music compositions using the proposed method, along with subjective evaluations of the generated music's quality and coherence across tracks

## Limitations
- The decoupling approach may miss important cross-modal interactions between melodic and rhythmic elements
- Pitch augmentation might obscure important pitch-specific qualities that are contextually important in music
- The bar-level positional encoding may not effectively capture complex or irregular rhythmic patterns that span across bar boundaries

## Confidence

- **High confidence** in empirical demonstration that fine-grained discriminators improve objective metrics compared to single-discriminator baselines
- **Medium confidence** in mechanism explanations - plausible reasoning provided but lacks deep exploration of alternative explanations
- **Low confidence** in generalization of results to other musical styles beyond the POP909 dataset

## Next Checks

1. Conduct a cross-modal ablation study by training a variant where melody and rhythm discriminators receive partially coupled information to quantify the trade-off between specialization and information loss

2. Replace the bar-level relative positional encoding with alternative rhythmic representations to determine whether the specific bar-level approach is optimal or if benefits come primarily from using any specialized rhythmic encoding

3. Apply the fine-grained discriminator architecture to a different musical corpus (e.g., classical piano or jazz) to evaluate whether the same improvements translate across musical styles or require style-specific adaptations