---
ver: rpa2
title: Efficient and Effective Retrieval of Dense-Sparse Hybrid Vectors using Graph-based
  Approximate Nearest Neighbor Search
arxiv_id: '2410.20381'
source_url: https://arxiv.org/abs/2410.20381
tags:
- sparse
- dense
- search
- hybrid
- vectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of efficient hybrid dense-sparse
  vector retrieval for text search. The authors propose a novel graph-based ANNS algorithm
  that tackles two main issues: the misalignment of dense and sparse vector distributions
  affecting accuracy, and the computational overhead of sparse vectors affecting efficiency.'
---

# Efficient and Effective Retrieval of Dense-Sarse Hybrid Vectors using Graph-based Approximate Nearest Neighbor Search

## Quick Facts
- arXiv ID: 2410.20381
- Source URL: https://arxiv.org/abs/2410.20381
- Authors: Haoyu Zhang; Jun Liu; Zhenhua Zhu; Shulin Zeng; Maojia Sheng; Tao Yang; Guohao Dai; Yu Wang
- Reference count: 30
- Primary result: 8.9× to 11.7× higher throughput with 1% to 9% higher recall accuracy compared to existing hybrid vector search methods

## Executive Summary
This paper addresses the challenge of efficient hybrid dense-sparse vector retrieval for text search. The authors propose a novel graph-based ANNS algorithm that tackles two main issues: the misalignment of dense and sparse vector distributions affecting accuracy, and the computational overhead of sparse vectors affecting efficiency. The core method introduces a distribution alignment technique that normalizes and statistically analyzes the distance distributions of dense and sparse vectors to determine optimal fusion weights. For efficiency, they design an adaptive two-stage computation strategy that initially uses only dense distances and later incorporates sparse distances, combined with sparse vector pruning to reduce computation. The algorithm is evaluated on mainstream text retrieval datasets and demonstrates significant improvements in both accuracy and efficiency.

## Method Summary
The paper proposes a graph-based ANNS algorithm for hybrid dense-sparse vector retrieval that addresses both accuracy and efficiency challenges. The method consists of two key components: distribution alignment and adaptive two-stage computation. The distribution alignment technique normalizes dense and sparse vector distances using statistical analysis to determine optimal fusion weights, ensuring proper integration of both vector types. The adaptive two-stage computation strategy initially uses only dense distances for candidate selection, then incorporates sparse distances in a second stage while pruning unnecessary sparse vector computations. This approach overcomes the limitations of existing hybrid search methods that either sacrifice accuracy for efficiency or vice versa, providing a unified solution that achieves both high accuracy and high throughput in hybrid vector retrieval.

## Key Results
- Achieved 8.9× to 11.7× higher throughput compared to existing hybrid vector search methods at equal accuracy levels
- Demonstrated 1% to 9% higher recall accuracy compared to baseline methods
- Successfully unified the trade-off between accuracy and efficiency in hybrid vector retrieval
- Validated on mainstream text retrieval datasets including MS MARCO and Natural Questions

## Why This Works (Mechanism)
The proposed method works by addressing the fundamental misalignment between dense and sparse vector distributions through statistical normalization and adaptive fusion. By analyzing the distance distributions of both vector types, the algorithm determines optimal weights that balance their contributions based on their inherent characteristics. The two-stage computation strategy exploits the fact that dense vectors alone can provide reasonable candidate sets, while sparse vectors refine these candidates for improved accuracy. This staged approach significantly reduces the computational burden of calculating sparse distances for all database vectors, as sparse computations are only performed on a reduced candidate set. The combination of distribution alignment and computational optimization enables both higher accuracy through better fusion and higher efficiency through reduced computation.

## Foundational Learning
- **Graph-based ANNS fundamentals**: Understanding how proximity graphs work for nearest neighbor search is essential for grasping the algorithm's search mechanism
- **Distance distribution analysis**: Statistical analysis of vector distances is needed to understand how the distribution alignment technique works
- **Hybrid vector fusion strategies**: Knowledge of how dense and sparse vectors complement each other in retrieval tasks is crucial for understanding the problem being solved
- **Two-stage computation optimization**: Understanding staged computation strategies helps explain how efficiency gains are achieved without sacrificing accuracy
- **Vector pruning techniques**: Knowledge of how and when to prune vector computations is important for understanding the efficiency improvements
- **Approximate nearest neighbor search trade-offs**: Understanding the balance between accuracy and efficiency in ANNS helps contextualize the algorithm's contributions

## Architecture Onboarding

**Component Map:**
Distribution Alignment Module -> Two-stage Computation Module -> Sparse Vector Pruning Module -> Graph-based ANNS Engine

**Critical Path:**
Query vector → Distribution alignment → Dense-only candidate selection → Sparse refinement → Final ranking → Results

**Design Tradeoffs:**
The algorithm trades off some additional preprocessing (distribution analysis) for significant runtime efficiency gains. The two-stage approach sacrifices the immediate use of sparse information for faster initial candidate generation, relying on the refinement stage to recover accuracy. Sparse vector pruning reduces memory and computation requirements but requires careful threshold tuning to avoid losing important information.

**Failure Signatures:**
- Poor distribution alignment leading to suboptimal fusion weights and degraded accuracy
- Overly aggressive sparse vector pruning causing significant recall degradation
- Ineffective candidate selection in the dense-only stage resulting in poor final results
- Computational overhead from distribution analysis outweighing efficiency gains in small datasets

**First 3 Experiments:**
1. Test distribution alignment on a simple dataset with known dense-sparse characteristics to verify weight calculation
2. Evaluate the two-stage computation strategy with varying candidate set sizes to find optimal parameters
3. Measure the impact of different sparse vector pruning thresholds on both efficiency and recall

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed distribution alignment method and sparse vector pruning affect hybrid retrieval accuracy when applied to different types of embedding models (e.g., contrastive learning-based vs. masked language model-based)?
- Basis in paper: [explicit] The paper mentions using various embedding models (BGE, GTE, SPLADE, BM25) but doesn't analyze the impact of the proposed method on different model types.
- Why unresolved: The paper only tests the proposed method on a limited set of models without exploring the generalizability across different model architectures and training objectives.
- What evidence would resolve it: Systematic experiments comparing the proposed method's performance across different embedding model families, particularly focusing on contrastive learning-based models vs. masked language model-based models.

### Open Question 2
- Question: What is the theoretical limit of the two-stage computation strategy in terms of sparse distance calculation reduction without compromising retrieval accuracy?
- Basis in paper: [inferred] The paper demonstrates practical effectiveness of the two-stage strategy but doesn't explore the theoretical boundaries of how much sparse computation can be eliminated.
- Why unresolved: The paper empirically shows the strategy works but doesn't provide theoretical analysis of the relationship between sparse distance contribution and search accuracy across different datasets and query distributions.
- What evidence would resolve it: Mathematical analysis establishing the relationship between sparse distance contribution thresholds and accuracy degradation, validated across diverse datasets.

### Open Question 3
- Question: How does the proposed hybrid vector search method perform in multi-lingual retrieval scenarios where sparse vectors may have different language-specific characteristics?
- Basis in paper: [inferred] The paper focuses on English datasets and doesn't address multi-lingual challenges that arise from language-specific sparse vector distributions.
- Why unresolved: The distribution alignment method may need adaptation for languages with different morphological structures, tokenization schemes, and term frequency characteristics that affect sparse vector generation.
- What evidence would resolve it: Experimental validation of the proposed method across multiple languages with varying linguistic characteristics, particularly focusing on languages with different writing systems and morphological complexity.

## Limitations
- The distribution alignment technique may face instability when dealing with heterogeneous or domain-shifted data distributions
- Scalability challenges may arise with extremely large sparse vectors or highly variable query patterns
- Sparse vector pruning could potentially impact recall in edge cases where important information resides in pruned dimensions
- The method's performance at scale (billions of vectors) remains unverified

## Confidence
- **High Confidence**: The empirical results demonstrating throughput improvements (8.9× to 11.7×) and recall accuracy gains (1% to 9%) are well-supported by the experimental data and methodology.
- **Medium Confidence**: The distribution alignment technique shows promise, but its robustness across diverse datasets and real-world scenarios needs further validation.
- **Medium Confidence**: The efficiency claims are convincing within the tested parameter ranges, but the performance characteristics at scale (billions of vectors) remain unverified.

## Next Checks
1. Test the algorithm's performance on domain-shifted datasets to evaluate the robustness of the distribution alignment technique under varying data distributions.
2. Conduct scalability experiments with extremely large sparse vectors (100K+ dimensions) to assess the adaptive two-stage computation strategy's effectiveness at scale.
3. Implement ablation studies to quantify the impact of sparse vector pruning on recall across different query types and document distributions.