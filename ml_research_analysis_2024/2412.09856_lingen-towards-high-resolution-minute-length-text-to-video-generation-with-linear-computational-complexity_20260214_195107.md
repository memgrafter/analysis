---
ver: rpa2
title: 'LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with
  Linear Computational Complexity'
arxiv_id: '2412.09856'
source_url: https://arxiv.org/abs/2412.09856
tags:
- video
- lingen
- videos
- arxiv
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LinGen, a text-to-video generation framework
  that scales linearly with video resolution and length, overcoming the quadratic
  complexity bottleneck of Diffusion Transformers (DiTs). The key innovation is replacing
  self-attention with a novel MATE block composed of an MA-branch (using bidirectional
  Mamba2 with Rotary-Major Scan and review tokens) and a TE-branch (TEmporal Swin
  Attention).
---

# LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity

## Quick Facts
- arXiv ID: 2412.09856
- Source URL: https://arxiv.org/abs/2412.09856
- Reference count: 40
- Achieves up to 15× speed-up over DiT baselines while maintaining comparable quality to commercial models

## Executive Summary
LinGen introduces a novel text-to-video generation framework that overcomes the quadratic computational complexity bottleneck of Diffusion Transformers (DiTs). By replacing self-attention with a linear-complexity MATE block composed of Mamba2 and Temporal Swin Attention, LinGen enables high-resolution minute-length video generation on a single GPU without compromising quality. The model achieves 50.5%-52.1% win rate against commercial baselines while offering significant computational efficiency improvements.

## Method Summary
LinGen is a Diffusion Transformer framework that replaces self-attention layers with MATE blocks consisting of an MA-branch (bidirectional Mamba2 with RMS and review tokens) and a TE-branch (Temporal Swin Attention). The model uses a Temporal AutoEncoder backbone and employs progressive training from 256p text-to-image to 512p text-to-video at increasing durations (17s → 34s → 68s). Training uses a hybrid approach mixing text-image and text-video pairs at a 1:100 ratio, followed by quality tuning on 3K curated high-quality videos.

## Key Results
- Achieves 15× speed-up over DiT baselines while maintaining quality
- Generates minute-length videos (68s at 16fps) on a single GPU
- Matches or exceeds quality of commercial models (50.5%-52.1% win rate vs Gen-3, LumaLabs, Kling)
- Maintains linear computational complexity through MATE block design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MATE block achieves linear complexity by replacing self-attention with a combination of Mamba2 and TESA.
- Mechanism: The MATE block replaces self-attention with a linear-complexity block composed of an MA-branch (bidirectional Mamba2 with RMS and review tokens) and a TE-branch (TESA). This design comprehensively handles short-, medium-, and long-range correlations while maintaining linear complexity.
- Core assumption: The linear complexity of Mamba2 and TESA can effectively replace the quadratic complexity of self-attention in video generation.
- Evidence anchors:
  - [abstract] "For the first time, LinGen enables high-resolution minute-length video generation on a single GPU without compromising quality. It replaces the computationally-dominant and quadratic-complexity block, self-attention, with a linear-complexity block called MATE, which consists of an MA-branch and a TE-branch."
  - [section] "LinGen replaces the self-attention layers in DiTs with our proposed linear-complexity MATE blocks."
- Break condition: If Mamba2 or TESA cannot effectively capture the necessary correlations for high-quality video generation, or if their linear complexity is insufficient to handle the computational demands of minute-length videos.

### Mechanism 2
- Claim: RMS and review tokens address the adjacency preservation issue of Mamba in vision tasks.
- Mechanism: RMS rearranges 3D token tensors in the latent space before they enter the bidirectional Mamba2 block, enhancing short-range correlations. Review tokens provide an overview of the processed token sequences to the hidden state of Mamba2 blocks at the start of sequence processing, calibrating long-range correlations.
- Core assumption: The adjacency preservation issue significantly hurts the quality of generated images and videos, and RMS and review tokens can effectively mitigate this issue.
- Evidence anchors:
  - [abstract] "The MA-branch targets short-to-long-range correlations, combining a bidirectional Mamba2 block with our token rearrangement method, Rotary Major Scan, and our review tokens developed for long video generation."
  - [section] "The MA-branch consists of a bidirectional Mamba2 block equipped with our RMS and review tokens."
- Break condition: If RMS and review tokens cannot effectively preserve adjacency in the rearranged 3D token tensors, or if they introduce significant computational overhead that negates their benefits.

### Mechanism 3
- Claim: TESA focuses on temporal correlations between adjacent tokens and medium-range tokens, improving video consistency.
- Mechanism: TESA divides the 3D video token tensor into small windows and calculates attention within each window. The windows are alternately shifted across layers to cross the boundaries of local windows, establishing connections among them and enlarging the receptive field.
- Core assumption: Focusing on temporal correlations between adjacent tokens and medium-range tokens is crucial for improving video consistency, and TESA can effectively capture these correlations.
- Evidence anchors:
  - [abstract] "The TE-branch is a novel TEmporal Swin Attention block that focuses on temporal correlations between adjacent tokens and medium-range tokens."
  - [section] "The TE-branch is a novel TEmporal Swin Attention (TESA) block. It computes correlations among short-range spatially adjacent and medium-range temporally adjacent tokens, focusing on addressing the adjacency preservation issue and improving video consistency."
- Break condition: If TESA cannot effectively capture the necessary temporal correlations, or if its small window size limits its ability to handle longer-range dependencies.

## Foundational Learning

- Concept: Linear complexity in sequence processing
  - Why needed here: The quadratic complexity of self-attention in video generation makes minute-length video generation extremely expensive. Replacing it with linear-complexity alternatives like Mamba2 and TESA enables high-resolution minute-length video generation on a single GPU.
  - Quick check question: What is the computational complexity of self-attention in video generation, and why is it a bottleneck for minute-length video generation?

- Concept: State Space Models (SSMs) and Mamba
  - Why needed here: Mamba and its variants (like Mamba2) are the foundation of the MA-branch in the MATE block. Understanding their properties and limitations is crucial for grasping how the MATE block addresses the adjacency preservation issue and achieves linear complexity.
  - Quick check question: What are the key differences between Mamba and Mamba2, and how do these differences impact their performance in video generation?

- Concept: Attention mechanisms in video processing
  - Why needed here: While self-attention is replaced in the MATE block, understanding how attention mechanisms work in video processing is essential for grasping the role of TESA and its contribution to video consistency.
  - Quick check question: How does the window-based attention in TESA differ from global self-attention, and what are the trade-offs in terms of computational complexity and video quality?

## Architecture Onboarding

- Component map: TAE backbone -> Denoising module with MATE blocks -> Text conditioning (UL2, ByT5, MetaCLIP, LLaMa-3.1) -> Video output
- Critical path: Text prompt encoding → Latent video token denoising using MATE blocks → Decoding to final video output
- Design tradeoffs: Computational complexity vs. video quality; linear complexity of MATE vs. comprehensive correlation capture of self-attention
- Failure signatures: Poor temporal consistency suggests TESA issues; quality degradation suggests MA-branch problems; inability to scale indicates linear complexity failures
- First 3 experiments:
  1. Compare LinGen with and without MA-branch to validate Mamba2, RMS, and review tokens effectiveness
  2. Compare LinGen with and without TE-branch to validate TESA's contribution to consistency
  3. Compare computational complexity and latency with standard DiT baseline to verify linear scaling

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but raises several implicit ones regarding scalability to hour-length videos, optimal hidden state sizes for Mamba2, and potential combinations with sampling distillation techniques.

## Limitations
- Limited evaluation of scalability beyond 68-second videos to truly minute-length or hour-length generation
- Proprietary training data (ShutterStock) limits reproducibility and generalizability
- Fixed hyperparameters for Mamba2 and TESA without exploration of optimal configurations

## Confidence

**High Confidence** (Experimental evidence strongly supports):
- The MATE block reduces computational complexity compared to standard DiTs
- LinGen can generate minute-length videos on a single GPU
- The model achieves measurable speed improvements (15×) over baseline DiTs

**Medium Confidence** (Plausible but with notable uncertainties):
- The 50.5%-52.1% win rate claim against commercial models
- The effectiveness of RMS and review tokens in addressing adjacency preservation
- The specific contributions of each MATE component to overall quality

**Low Confidence** (Claims with significant gaps in evidence):
- Generalization to unseen video content beyond training distribution
- Long-term video coherence beyond the evaluated durations
- The scalability of this approach to even longer videos (multi-minute) or higher resolutions

## Next Checks

1. **Ablation Study with Public Datasets**: Replicate the key ablation experiments (MA-branch, TE-branch, hybrid training) using publicly available datasets (WebVid, HD-Video) to verify that results are not dependent on proprietary ShutterStock data and can be reproduced independently.

2. **Controlled Quality Evaluation**: Conduct systematic quality evaluations using identical prompts across LinGen, commercial baselines, and open-source DiT implementations, measuring both technical metrics (FID, FVD) and human preference with proper statistical significance testing.

3. **Complexity Scaling Analysis**: Systematically measure computational complexity (FLOPs, memory usage, wall-clock time) across varying video resolutions, lengths, and sequence lengths to empirically verify the claimed linear scaling behavior and identify any hidden quadratic components.