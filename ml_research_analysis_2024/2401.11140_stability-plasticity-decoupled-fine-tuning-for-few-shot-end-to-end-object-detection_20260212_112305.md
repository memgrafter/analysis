---
ver: rpa2
title: Stability Plasticity Decoupled Fine-tuning For Few-shot end-to-end Object Detection
arxiv_id: '2401.11140'
source_url: https://arxiv.org/abs/2401.11140
tags:
- fine-tuning
- object
- r-cnn
- stage
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the stability-plasticity contradiction in
  few-shot object detection (FSOD) when fine-tuning end-to-end detectors like Sparse
  R-CNN. The key issue is that re-initialized classifiers need more plasticity to
  adapt to novel classes, while other modules require more stability to preserve pre-trained
  knowledge.
---

# Stability Plasticity Decoupled Fine-tuning For Few-shot end-to-end Object Detection

## Quick Facts
- arXiv ID: 2401.11140
- Source URL: https://arxiv.org/abs/2401.11140
- Authors: Yuantao Yin; Ping Yin
- Reference count: 0
- Primary result: 16.5% mAP on 10-shot setting vs 11.3% baseline

## Executive Summary
This paper addresses the stability-plasticity contradiction in few-shot object detection (FSOD) when fine-tuning end-to-end detectors like Sparse R-CNN. The core issue is that re-initialized classifiers need more plasticity to adapt to novel classes, while other modules require more stability to preserve pre-trained knowledge. The authors propose a three-stage fine-tuning procedure: base pre-training, plasticity classifier fine-tuning (PCF) stage, and stability regularization stage with multi-source ensemble (ME). Experiments on COCO benchmark show significant improvements over previous FSOD approaches.

## Method Summary
The proposed method introduces a three-stage fine-tuning procedure to address the stability-plasticity contradiction in FSOD. First, the detector is pre-trained on base classes. Second, during PCF stage, only classifiers are optimized while other modules are frozen to provide sufficient plasticity for classifier adaptation. Third, in the stability regularization stage, a multi-source ensemble technique aggregates predictions from diverse models using prototypes from an ImageNet pre-trained backbone to enhance stability. This decoupled approach allows classifiers to adapt to novel classes while preserving class-agnostic knowledge in other modules.

## Key Results
- Achieves 16.5% mAP on 10-shot setting compared to 11.3% baseline
- PCF contributes 3.8% mAP improvement through decoupled classifier optimization
- ME contributes 2.8% mAP improvement through stability regularization
- Demonstrates effectiveness in mitigating stability-plasticity contradiction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The stability-plasticity contradiction arises because classifiers (C) require more plasticity while other modules (E) require more stability during fine-tuning.
- Mechanism: Re-initialized classifiers need sufficient plasticity to adapt to novel classes, while pre-trained modules need stability to preserve class-agnostic knowledge. Coupling these conflicting demands during optimization leads to suboptimal performance.
- Core assumption: The stability-plasticity demand is inherently different between classifiers and other modules due to their distinct initialization states and knowledge requirements.
- Evidence anchors:
  - [abstract]: "Specifically, the random re-initialized classifiers need more plasticity to adapt to novel samples. The other modules inheriting pre-trained weights demand more stability to reserve their class-agnostic knowledge."
  - [section]: "During fine-tuning in regular two-stage procedure, re-initialized classifiers and other modules actually have contradictory stability-plasticity demand."
- Break condition: If the classifier initialization is not random or if the pre-trained modules don't need to preserve class-agnostic knowledge.

### Mechanism 2
- Claim: The plasticity classifier fine-tuning (PCF) stage addresses the contradiction by decoupling classifier optimization from other modules.
- Mechanism: By freezing all modules except classifiers during PCF, the method provides sufficient plasticity for classifier adaptation without risking over-fitting to other modules. This meets the plasticity demand of classifiers without compromising the stability of other modules.
- Core assumption: Freezing other modules during classifier optimization prevents the distortion of pre-trained knowledge while allowing sufficient classifier adaptation.
- Evidence anchors:
  - [section]: "In this stage, we freeze all modules except these re-initialized classifiers of these cascaded heads. And we set a sufficient long schedule to initialize and adapt these classifiers with novel samples."
- Break condition: If freezing other modules for extended periods causes catastrophic forgetting of class-agnostic knowledge.

### Mechanism 3
- Claim: The multi-source ensemble (ME) method enhances stability by aggregating predictions from diverse models.
- Mechanism: The method uses prototypes from an ImageNet pre-trained backbone to provide additional regularization during fine-tuning, helping to preserve the generalization stability of the detector.
- Core assumption: Prototypes from a different backbone model can provide useful regularization signals that help maintain the stability of the detector during fine-tuning.
- Evidence anchors:
  - [section]: "We propose an ensemble regularization method to further regularize the transfer. We introduce an array of prototypes using the offered ImageNet pre-trained backbone model BIm which is neither fine-tuned nor pre-trained in the base stage."
- Break condition: If the additional backbone model introduces conflicting regularization signals that harm rather than help stability.

## Foundational Learning

- Concept: Stability-plasticity dilemma
  - Why needed here: Understanding the trade-off between preserving pre-trained knowledge (stability) and adapting to novel classes (plasticity) is fundamental to the proposed method's design.
  - Quick check question: Why would forcing stability and plasticity during the same optimization phase lead to suboptimal performance?

- Concept: Fine-tuning procedures
  - Why needed here: The paper builds upon and modifies the classical two-stage fine-tuning procedure, so understanding its mechanics is essential.
  - Quick check question: What are the key differences between the classical two-stage and the proposed three-stage fine-tuning procedures?

- Concept: Object detection architectures
  - Why needed here: The method specifically targets Sparse R-CNN's cascaded architecture, so understanding how cascaded detectors differ from parallel architectures is important.
  - Quick check question: How does the cascaded architecture of Sparse R-CNN differ from Faster R-CNN's parallel architecture in terms of parameter count and gradient flow?

## Architecture Onboarding

- Component map: Base stage -> PCF stage -> Stability regularization stage -> Multi-source ensemble inference
- Critical path: Base stage pre-training -> PCF stage classifier fine-tuning -> Stability regularization stage fine-tuning -> Multi-source ensemble inference
- Design tradeoffs:
  - Freezing vs. fine-tuning modules: Too much freezing risks forgetting class-agnostic knowledge; too little risks over-fitting to novel samples
  - PCF schedule length: Longer schedules provide more plasticity but increase training time and risk over-fitting
  - Ensemble weight balancing (α, β): Must balance original detector confidence with ensemble prototype confidence
- Failure signatures:
  - Under-fitting classifiers: Low performance on novel classes despite long PCF schedules
  - Over-fitting other modules: Poor generalization to test data despite base stage pre-training
  - Ensemble conflicts: Degraded performance when prototype confidence contradicts detector confidence
- First 3 experiments:
  1. Implement baseline two-stage fine-tuning on Sparse R-CNN to establish performance floor
  2. Add PCF stage with various freezing configurations to find optimal plasticity balance
  3. Integrate multi-source ensemble with different weight balancing to optimize stability regularization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed three-stage fine-tuning procedure perform on end-to-end object detectors other than Sparse R-CNN, such as DETR or Deformable DETR?
- Basis in paper: [explicit] The paper states that the stability-plasticity contradiction is prominent in Sparse R-CNN due to its multi-classifier cascaded architecture, but does not explore other end-to-end detectors.
- Why unresolved: The authors focus on Sparse R-CNN as a case study but do not provide experimental results or analysis for other end-to-end detectors. This leaves the generalizability of the proposed method to other architectures unclear.
- What evidence would resolve it: Experimental results showing the effectiveness of the three-stage fine-tuning procedure on DETR, Deformable DETR, or other end-to-end detectors would provide strong evidence.

### Open Question 2
- Question: How sensitive is the proposed method to the choice of hyperparameters, such as the number of stages, the schedule for each stage, and the ensemble weights?
- Basis in paper: [inferred] The paper mentions that the stability-plasticity contradiction is addressed by decoupling the optimization schedule and setting a sufficient long schedule for the plasticity classifier fine-tuning stage, but does not provide a comprehensive analysis of the hyperparameter sensitivity.
- Why unresolved: The authors do not conduct a systematic hyperparameter study to determine the robustness of the proposed method to different choices of hyperparameters. This leaves questions about the practical applicability and generalizability of the method.
- What evidence would resolve it: A thorough hyperparameter sensitivity analysis, including ablation studies and parameter tuning experiments, would provide insights into the robustness and practical applicability of the proposed method.

### Open Question 3
- Question: How does the proposed method compare to other regularization techniques, such as knowledge distillation or weight regularization, in addressing the stability-plasticity contradiction?
- Basis in paper: [inferred] The paper focuses on decoupling the optimization schedule and using an ensemble method to regularize the detector, but does not compare the proposed method to other regularization techniques that could potentially address the same issue.
- Why unresolved: The authors do not provide a comparative analysis of the proposed method against other regularization techniques that could potentially address the stability-plasticity contradiction. This leaves questions about the relative effectiveness and efficiency of the proposed approach.
- What evidence would resolve it: Comparative experiments between the proposed method and other regularization techniques, such as knowledge distillation or weight regularization, would provide insights into the relative effectiveness and efficiency of the proposed approach.

## Limitations
- Limited cross-architectural validation: Effectiveness on other end-to-end detectors beyond Sparse R-CNN is not demonstrated
- Hyperparameter sensitivity unknown: Optimal configuration of PCF schedule length and ME weights remains untested
- Component interaction unclear: Whether PCF and ME improvements are additive or interactive is not examined

## Confidence
- High confidence: The stability-plasticity contradiction as a fundamental problem in FSOD fine-tuning is well-established and theoretically sound.
- Medium confidence: The proposed three-stage procedure and its components (PCF, ME) are conceptually valid and show performance improvements.
- Low confidence: The specific implementation details and optimal hyperparameter configurations for the proposed method are not fully specified.

## Next Checks
1. Component interaction analysis: Conduct experiments isolating PCF and ME effects across different base detector architectures to verify additive vs. interactive benefits.
2. Hyperparameter sensitivity testing: Systematically vary PCF schedule length and ME ensemble weights to identify robust configurations and potential overfitting risks.
3. Cross-dataset generalization: Evaluate the method on additional few-shot detection benchmarks (e.g., PASCAL VOC, LVIS) to assess generalizability beyond COCO.