---
ver: rpa2
title: Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised Sentence
  Embeddings
arxiv_id: '2402.15153'
source_url: https://arxiv.org/abs/2402.15153
tags:
- sarcse
- simcse
- sentence
- tokens
- reconstruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel self-adaptive reconstruction contrastive
  learning method for unsupervised sentence embeddings. The method addresses the issue
  of token bias in pretrained language models, which leads to poor performance in
  capturing fine-grained semantics in sentences.
---

# Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised Sentence Embeddings

## Quick Facts
- arXiv ID: 2402.15153
- Source URL: https://arxiv.org/abs/2402.15153
- Reference count: 30
- Achieves 78.09 average Spearman's correlation on 7 STS tasks, outperforming SimCSE's 76.57

## Executive Summary
This paper addresses token bias in pretrained language models for unsupervised sentence embeddings by introducing a self-adaptive reconstruction contrastive learning method. The approach combines contrastive learning with an AutoEncoder that reconstructs all tokens in sentences, forcing the model to preserve fine-grained semantics. A novel self-adaptive reconstruction loss based on token frequency reduces the influence of high-frequency tokens lacking determinative semantics. Experiments show the method significantly outperforms the strong SimCSE baseline on multiple STS benchmark tasks.

## Method Summary
The method uses RoBERTa as the base encoder, followed by a multi-scale TextCNN encoder with kernel sizes 3, 4, and 5 to capture context at different granularities. A CNN integrates these multi-scale representations into sentence embeddings. An AutoEncoder with transposed CNN and TextCNN layers reconstructs all tokens in the sentence. The training objective combines InfoNCE contrastive loss with self-adaptive reconstruction loss, where the reconstruction loss is weighted by token frequency to reduce bias toward frequent tokens. The model is trained on 1 million Wikipedia sentences with batch size 64 for 1 epoch.

## Key Results
- Achieves 78.09 average Spearman's correlation on 7 STS tasks
- Outperforms SimCSE baseline (76.57) by 1.52 percentage points
- Requires smaller batch size (64 vs 512) compared to SimCSE
- Shows consistent improvements across all evaluated STS datasets

## Why This Works (Mechanism)

### Mechanism 1: Token Reconstruction Bottleneck
Reconstruction forces the encoder to preserve fine-grained semantics by requiring the decoder to recover exact token representations. The AutoEncoder reconstructs all tokens in the sentence, creating a bottleneck that forces the sentence embedding to retain detailed token-level information that would otherwise be lost during CLS aggregation. The reconstruction task provides stronger supervision than contrastive learning alone for capturing token-level semantic differences.

### Mechanism 2: Self-Adaptive Frequency Weighting
Self-adaptive reconstruction loss reduces the influence of high-frequency tokens that lack determinative semantics. The loss function weights reconstruction errors by token frequency, giving less weight to common tokens (e.g., "and", "the") and more weight to rare, semantically important tokens. Token frequency correlates with semantic importance, so downweighting frequent tokens improves semantic representation quality.

### Mechanism 3: Multi-Scale Context Encoding
Multi-scale TextCNN encoding captures context at different granularities before reconstruction. Using convolution kernels of different sizes (3×d, 4×d, 5×d) creates representations that capture n-gram contexts of varying lengths, which are then integrated and reconstructed. Different kernel sizes capture complementary semantic information that benefits reconstruction.

## Foundational Learning

- Concept: Contrastive Learning with InfoNCE Loss
  - Why needed here: The method builds on SimCSE's contrastive framework but enhances it with reconstruction; understanding InfoNCE is crucial for grasping how positive/negative pairs work.
  - Quick check question: In InfoNCE loss, what mathematical relationship ensures that positive pairs are pulled together while negative pairs are pushed apart?

- Concept: AutoEncoder Architecture and Training
  - Why needed here: The reconstruction component is central to the method; understanding how encoders/decoders work together is essential.
  - Quick check question: Why does using transposed convolutions in the decoder help reconstruct the token sequence structure from the compressed sentence embedding?

- Concept: Token Frequency Distribution and Bias
  - Why needed here: The self-adaptive loss mechanism depends on understanding how token frequency affects model behavior in PLMs.
  - Quick check question: How does the non-uniform distribution of tokens with different frequencies in representation space create vulnerability for semantic understanding?

## Architecture Onboarding

- Component map: Input → RoBERTa tokenizer → RoBERTa encoder → Multi-scale TextCNN encoder → CNN integration → Sentence embedding → Transposed CNN decoder → Token reconstruction → Loss calculation (InfoNCE + self-adaptive MSE)
- Critical path: Token representations → Multi-scale encoding → Sentence embedding → Reconstruction → Loss computation → Parameter update
- Design tradeoffs: Reconstruction adds computational overhead but reduces batch size requirements; self-adaptive loss requires frequency statistics but improves semantic focus
- Failure signatures: Poor reconstruction accuracy → loss of fine-grained semantics; frequency-based weighting too aggressive → loss of common but important function words; multi-scale CNN parameters mismatched → redundant or missing context
- First 3 experiments:
  1. Train with reconstruction but fixed reconstruction loss (no frequency weighting) to isolate reconstruction benefit
  2. Train with frequency weighting but no reconstruction to isolate self-adaptive loss benefit
  3. Vary the θ parameter in frequency weighting to find optimal balance between high and low frequency token treatment

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of θ (minimum weight limit) impact the performance of SARCSE on different STS datasets and language pairs? The paper finds θ=0.1 optimal for English STS tasks but this may vary depending on dataset, language pair, and text characteristics. Experiments on diverse STS datasets in multiple languages would provide insights into generalizability.

### Open Question 2
How does the self-adaptive reconstruction loss perform when applied to other types of neural networks or tasks beyond sentence embeddings? The paper demonstrates effectiveness for unsupervised sentence embeddings but its performance on other tasks (text classification, machine translation) or architectures remains unknown.

### Open Question 3
How does the batch size affect the performance of SARCSE compared to SimCSE in terms of both computational efficiency and model quality? While the paper demonstrates effectiveness of smaller batch size (64 vs 512), it does not provide comprehensive analysis of trade-off between computational efficiency and model quality across range of batch sizes.

## Limitations

- The paper lacks ablation studies to isolate contributions of reconstruction vs. frequency weighting mechanisms
- Computational overhead of reconstruction approach is not discussed
- Assumes token frequency correlates with semantic importance, which may not hold across all domains or languages
- Limited evaluation to English STS tasks only

## Confidence

**High Confidence**: Core experimental results showing improvement over SimCSE on the 7 STS tasks. Methodology for calculating Spearman's correlation and overall training objective formulation are clearly specified.

**Medium Confidence**: Effectiveness of self-adaptive reconstruction loss mechanism. While paper provides theoretical justification and shows improved results, limited ablation analysis to isolate this component's specific contribution versus standard reconstruction.

**Low Confidence**: Claim that multi-scale TextCNN encoder significantly contributes to capturing complementary semantic information. Paper does not compare against single-scale TextCNN or other encoding approaches to demonstrate this benefit.

## Next Checks

1. **Ablation study design**: Run experiments with (a) reconstruction but fixed reconstruction loss, (b) frequency weighting but no reconstruction, and (c) standard SimCSE baseline to quantify individual contributions of each novel component.

2. **Cross-domain robustness test**: Evaluate the model on sentence embedding tasks from different domains (biomedical, legal, or conversational text) to verify that frequency-based weighting generalizes beyond Wikipedia-style text.

3. **Parameter sensitivity analysis**: Systematically vary the θ parameter in self-adaptive reconstruction loss (currently set to 0.1) and reconstruction loss coefficients β and γ (currently 2.5e-4) to identify optimal settings and understand sensitivity to hyperparameters.