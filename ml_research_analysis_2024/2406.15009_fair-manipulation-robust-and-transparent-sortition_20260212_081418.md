---
ver: rpa2
title: Fair, Manipulation-Robust, and Transparent Sortition
arxiv_id: '2406.15009'
source_url: https://arxiv.org/abs/2406.15009
tags:
- panel
- probability
- selection
- agents
- instance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing fair and manipulation-resistant
  algorithms for selecting participants in citizens' assemblies. The authors propose
  a new objective function, Goldilocks, that aims to simultaneously control both high
  and low selection probabilities to achieve fairness and manipulation robustness.
---

# Fair, Manipulation-Robust, and Transparent Sortition

## Quick Facts
- arXiv ID: 2406.15009
- Source URL: https://arxiv.org/abs/2406.15009
- Reference count: 40
- One-line primary result: Proposes Goldilocks algorithm achieving fairness and manipulation robustness in sortition while maintaining transparency.

## Executive Summary
This paper addresses the challenge of designing fair and manipulation-resistant algorithms for selecting participants in citizens' assemblies. The authors propose a new objective function, Goldilocks, that aims to simultaneously control both high and low selection probabilities to achieve fairness and manipulation robustness. They theoretically bound the performance of Goldilocks and find that it performs nearly instance-optimally in real-world datasets, achieving both near-optimal minimum and maximum selection probabilities simultaneously. The authors also show that Goldilocks can be made transparent with little-to-no cost to the maximum and minimum selection probabilities. Empirical results demonstrate that Goldilocks significantly outperforms previously studied algorithms in achieving fairness and manipulation robustness.

## Method Summary
The paper implements an algorithmic framework for optimizing convex equality objectives in sortition, adding the Goldilocks1 objective as a second-order cone program (SOCP) formulation with auxiliary constraints to enforce anonymity. The method preprocesses 9 anonymized real-world panel selection datasets with pool sizes from 70 to 1727, implementing feature dropping procedures to analyze algorithm performance as constraints loosen. Experiments compare all algorithms on maximum/minimum probabilities, fairness (Gini coefficient), manipulation robustness (MU strategy), and transparency (Pipage rounding with m=1000 panels).

## Key Results
- Goldilocks1 achieves nearly instance-optimal minimum and maximum selection probabilities simultaneously in most real instances
- Goldilocks1 guarantees Maximin fairness of at least order 1/(âˆšcn)
- Goldilocks can be made transparent with little-to-no cost to maximum and minimum selection probabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Goldilocks1 simultaneously bounds high and low selection probabilities, achieving fairness and manipulation robustness.
- Mechanism: The objective function Goldilocksð›¾ minimizes a weighted sum of the maximum selection probability and the inverse of the minimum selection probability, ensuring neither becomes too large or too small.
- Core assumption: There exist "good" solutions that control both high and low probabilities, even under manipulation.
- Evidence anchors:
  - [abstract] "We propose a new equality objective, Goldilocks, that aims to achieve these ideals simultaneously by ensuring that no potential participant receives too little or too much chance of selection."
  - [section] "Goldilocksð›¾ (ð…) := max(ð… ) avg(ð… ) + ð›¾ Â· avg(ð… ) min(ð… ) . In the style of multi-objective optimization, the first and second term respectively aim to control high and low selection probabilities, and ð›¾ âˆˆ [ 0, âˆž) is a scalar determining the extent to which the objective prioritizes controlling the minimum versus the maximum probability."
  - [corpus] Weak - the corpus papers focus on fairness in machine learning rather than sortition, so direct evidence is limited.
- Break condition: If manipulation can eliminate all solutions that control both high and low probabilities (as shown in Theorem 3), then no algorithm can achieve this trade-off.

### Mechanism 2
- Claim: Goldilocks1 achieves near-optimal fairness and manipulation robustness across real-world datasets.
- Mechanism: By bounding the maximum and minimum selection probabilities, Goldilocks1 ensures that no agent can gain too much probability through manipulation, while also ensuring no agent receives too little probability (fairness).
- Core assumption: The bounds on maximum and minimum probabilities translate to meaningful bounds on fairness and manipulation robustness.
- Evidence anchors:
  - [abstract] "Our empirical analysis of Goldilocks in real data is even more promising: we find that this objective achieves nearly instance-optimal minimum and maximum selection probabilities simultaneously in most real instances â€” an outcome not even guaranteed to be possible for any algorithm."
  - [section] "Regarding fairness, we find that Goldilocks1 guarantees Maximin fairness (i.e., minimum probability) of at least order 1/(âˆšð‘ð‘›). We then give a lower bound on the extent to which any algorithm can simultaneously guarantee fairness and manipulation robustness, revealing that our bounds are tight in a natural subset of regimes and nearly tight in the rest."
  - [corpus] Weak - the corpus papers focus on fairness in machine learning rather than sortition, so direct evidence is limited.
- Break condition: If the translation from probability bounds to fairness and manipulation robustness guarantees is invalid, then the claimed benefits of Goldilocks1 are not realized.

### Mechanism 3
- Claim: Goldilocks1 can be made transparent with minimal impact on fairness and manipulation robustness.
- Mechanism: The output of Goldilocks1 can be rounded to a uniform lottery over panels, allowing the public to verify the selection process while preserving the selection probabilities.
- Core assumption: Rounding the output of Goldilocks1 does not significantly alter the maximum and minimum selection probabilities.
- Evidence anchors:
  - [abstract] "The authors also show that Goldilocks can be made transparent with little-to-no cost to the maximum and minimum selection probabilities."
  - [section] "Finally, in Section 5 we bound the extent to which these results (approximately) hold after the output of Goldilocks1 is rounded to a uniform lottery, thereby establishing the extent to which fairness and manipulation robustness can be achieved alongside transparency."
  - [corpus] Weak - the corpus papers focus on fairness in machine learning rather than sortition, so direct evidence is limited.
- Break condition: If rounding significantly alters the maximum and minimum selection probabilities, then the benefits of transparency come at the cost of fairness and manipulation robustness.

## Foundational Learning

- Concept: Convex equality objectives
  - Why needed here: The algorithmic framework used in the paper requires convex equality objectives to compute maximally equal selection probability assignments.
  - Quick check question: What is the difference between a convex and a non-convex equality objective?

- Concept: Panel distributions and selection probabilities
  - Why needed here: The paper studies how to randomize over valid panels to achieve maximally equal selection probabilities for pool members.
  - Quick check question: How is a panel distribution related to the selection probabilities of pool members?

- Concept: Manipulation robustness
  - Why needed here: The paper evaluates algorithms based on how well they resist manipulation by agents misreporting their features to increase their selection probability.
  - Quick check question: What are the three types of manipulation incentives considered in the paper?

## Architecture Onboarding

- Component map: Goldilocks objective -> Algorithmic framework -> Rounding procedures -> Empirical evaluation
- Critical path: 1. Define equality objective (e.g., Goldilocks) 2. Compute maximally equal panel distribution using algorithmic framework 3. Round panel distribution to achieve transparency (if desired) 4. Evaluate fairness and manipulation robustness of resulting algorithm
- Design tradeoffs: Goldilocks vs. Leximin vs. Minimax: Goldilocks aims to control both high and low probabilities, while Leximin prioritizes low probabilities and Minimax prioritizes low probabilities. Transparency vs. fairness/manipulation robustness: Rounding to achieve transparency may slightly alter selection probabilities, potentially impacting fairness and manipulation robustness.
- Failure signatures: Poor performance on fairness or manipulation robustness: Indicates that the chosen equality objective is not well-suited to the instance. Significant change in selection probabilities after rounding: Indicates that the rounding procedure is not preserving the benefits of the chosen equality objective.
- First 3 experiments: 1. Implement Goldilocks objective and compare its performance to Leximin and Minimax on a small synthetic dataset. 2. Evaluate the impact of rounding on the selection probabilities achieved by Goldilocks. 3. Test the scalability of the algorithmic framework by computing maximally equal panel distributions for larger instances.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Goldilocks algorithm be further optimized to achieve tighter theoretical bounds on fairness and manipulation robustness, especially in cases where the pool size is not large?
- Basis in paper: [explicit] The paper discusses the current theoretical bounds on Goldilocks1's performance and mentions that these bounds depend on the pool size and the number of manipulators. It also notes that the current bounds are not tight in all cases, especially when the pool size is not large.
- Why unresolved: The paper provides bounds that are close to optimal in many cases, but there is room for improvement, especially in scenarios with smaller pool sizes or when the number of manipulators is significant.
- What evidence would resolve it: Empirical studies with smaller pool sizes or increased numbers of manipulators could provide data to test the tightness of the current bounds and potentially lead to improved algorithms or tighter theoretical bounds.

### Open Question 2
- Question: How does the performance of Goldilocks compare to other algorithms when the selection bias in the pool is extreme or when certain features are highly correlated with the likelihood of volunteering?
- Basis in paper: [explicit] The paper mentions that selection bias in the pool is a significant factor and that different algorithms may perform differently under varying levels of selection bias. It also notes that the performance of algorithms like Goldilocks, Leximin, and Minimax can vary depending on the structure of the pool and the quotas.
- Why unresolved: The paper provides some analysis of how algorithms perform under different conditions, but it does not deeply explore the impact of extreme selection bias or highly correlated features.
- What evidence would resolve it: Further empirical studies with datasets that have extreme selection bias or highly correlated features could provide insights into how different algorithms perform under these conditions.

### Open Question 3
- Question: What are the practical implications of using Goldilocks in real-world citizens' assemblies, particularly in terms of transparency and public trust?
- Basis in paper: [explicit] The paper discusses the importance of transparency in sortition and mentions that Goldilocks can be made transparent with little-to-no cost to fairness and manipulation robustness. It also notes that transparency is a key concern in the practical implementation of sortition algorithms.
- Why unresolved: While the paper provides theoretical guarantees on transparency, it does not delve into the practical aspects of implementing Goldilocks in real-world settings and how it might affect public trust and engagement.
- What evidence would resolve it: Case studies or pilot programs using Goldilocks in real citizens' assemblies could provide valuable data on its practical implementation and its impact on transparency and public trust.

## Limitations

- Theoretical bounds, while tight in specific regimes, may not fully capture all practical scenarios
- Claim of near-optimal fairness and manipulation robustness lacks rigorous proof for all dataset configurations
- Transparency mechanism through Pipage rounding introduces trade-offs that may not be negligible in all cases

## Confidence

- High Confidence: The empirical performance of Goldilocks1 on real-world datasets, showing superior fairness and manipulation robustness compared to other algorithms.
- Medium Confidence: The theoretical bounds on fairness and manipulation robustness, as they are tight in specific regimes but may not generalize to all instances.
- Low Confidence: The extent to which the transparency mechanism preserves fairness and manipulation robustness in all cases, as this is primarily supported by empirical results.

## Next Checks

1. Rigorously prove that Goldilocks1 achieves near-optimal fairness and manipulation robustness in all possible dataset configurations, not just empirically observed ones.
2. Conduct a comprehensive analysis of the impact of Pipage rounding on selection probabilities across a wider range of instances, including those with complex constraints.
3. Investigate the existence and effectiveness of manipulation strategies beyond the MU strategy, particularly those that exploit potential weaknesses in the rounding procedure.