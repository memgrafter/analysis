---
ver: rpa2
title: Differentially Private Adaptation of Diffusion Models via Noisy Aggregated
  Embeddings
arxiv_id: '2411.14639'
source_url: https://arxiv.org/abs/2411.14639
tags:
- privacy
- diffusion
- style
- images
- private
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of privately adapting diffusion
  models to small, sensitive datasets, where standard approaches like DP-SGD suffer
  from severe utility degradation. The authors propose a method called DPAgg-TI that
  leverages Textual Inversion to learn separate embeddings for each image, aggregates
  them into a noisy centroid with calibrated differential privacy, and uses the result
  for style adaptation.
---

# Differentially Private Adaptation of Diffusion Models via Noisy Aggregated Embeddings

## Quick Facts
- **arXiv ID**: 2411.14639
- **Source URL**: https://arxiv.org/abs/2411.14639
- **Reference count**: 40
- **Key outcome**: DPAgg-TI outperforms DP-SGD on small datasets, achieving high style fidelity with formal DP guarantees

## Executive Summary
This paper addresses the challenge of privately adapting diffusion models to small, sensitive datasets where standard approaches like DP-SGD suffer from severe utility degradation. The authors propose Differentially Private Aggregation via Textual Inversion (DPAgg-TI), which learns separate embeddings for each image, aggregates them with calibrated noise, and uses subsampling for privacy amplification. Experiments show that DPAgg-TI significantly outperforms DP-SGD in both utility and robustness under the same privacy budget, achieving results closely matching the non-private baseline on style adaptation tasks using private artwork and Paris 2024 Olympic pictograms.

## Method Summary
The paper proposes Differentially Private Aggregation via Textual Inversion (DPAgg-TI) for adapting diffusion models to small private datasets. The method learns separate Textual Inversion embeddings for each image in the dataset, normalizes and aggregates these embeddings with calibrated Gaussian noise to ensure differential privacy, and optionally uses style guidance for better style transfer. Subsampling is employed to reduce sensitivity and amplify privacy. The approach ensures formal DP guarantees while preserving high output fidelity, particularly effective for small datasets where DP-SGD struggles.

## Key Results
- DPAgg-TI achieves FID of 3.75 on artwork dataset vs 14.65 for DP-SGD
- With style guidance, DPAgg-TI achieves FID of 2.38 on artwork dataset
- DPAgg-TI maintains utility even at ε=0.5, while DP-SGD produces meaningless outputs at the same privacy budget

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Per-image embeddings preserve fine-grained stylistic information that gets lost when training a single embedding over the entire dataset.
- **Mechanism**: Training a separate TI embedding for each image creates a rich embedding space that captures subtle, individual image characteristics. Aggregating these embeddings with noise retains more style information than training one embedding over the whole dataset.
- **Core assumption**: The style of interest is not uniformly distributed across the dataset, so per-image embeddings can capture more nuanced features.
- **Evidence anchors**: [abstract] "Our approach, Differentially Private Aggregation via Textual Inversion (DPAgg-TI), adds calibrated noise to the aggregation of per-image embeddings to ensure formal DP guarantees while preserving high output fidelity." [section III] "Instead of training a single token embedding on the entire dataset as in regular TI, we train a separate embedding u(i) on each x(i)"

### Mechanism 2
- **Claim**: Subsampling reduces sensitivity to individual data points, allowing lower noise addition while maintaining the same privacy budget.
- **Mechanism**: By aggregating only a subset m of n embeddings, the ℓ2-sensitivity of the centroid reduces from 2/n to 2/m, enabling lower noise addition for the same ε.
- **Core assumption**: Subsampling without replacement maintains representativeness of the dataset while reducing sensitivity.
- **Evidence anchors**: [section III] "To reduce the amount of noise needed to provide the same level of DP, we employ subsampling: instead of computing the centroid over all n embedding vectors, we randomly sample m≤n embedding vectors without replacement" [section IV-B] "Reducing m also reduces the sensitivity of the generated image to ε... image generation can tolerate ε as low as 0.5 without significant changes in visual characteristics"

### Mechanism 3
- **Claim**: ℓ2-normalization of embeddings ensures bounded sensitivity, making the aggregation mechanism differentially private.
- **Mechanism**: Normalizing each embedding before aggregation bounds the ℓ2-sensitivity to 2/n, enabling calibrated noise addition via the Gaussian mechanism.
- **Core assumption**: The CLIP embeddings used are purely directional, so normalization preserves semantics while bounding sensitivity.
- **Evidence anchors**: [section III] "To ensure bounded sensitivity, we employ a purely directional token embedding (semantics depend only on direction), such as CLIP [16], and ℓ2-normalize each embedding vector prior to aggregation" [section III] "Under this normalization, the ℓ2-sensitivity of u*DP is ∆ = 2/n"

## Foundational Learning

- **Concept**: Differential Privacy (DP) and the Gaussian mechanism
  - Why needed here: The method adds calibrated Gaussian noise to aggregated embeddings to ensure formal DP guarantees.
  - Quick check question: What is the relationship between the Gaussian noise standard deviation σ and the privacy parameters ε and δ?

- **Concept**: Textual Inversion (TI) and embedding training
  - Why needed here: The method builds on TI by learning separate embeddings for each image rather than one embedding for the dataset.
  - Quick check question: How does the TI optimization objective differ when applied to individual images versus the entire dataset?

- **Concept**: Subsampling amplification in DP
- **Why needed here**: Subsampling reduces the sensitivity of the aggregation mechanism, allowing lower noise addition for the same privacy budget.
- **Quick check question**: How does the privacy budget ε change when subsampling m out of n data points?

## Architecture Onboarding

- **Component map**: Diffusion model (Stable Diffusion v1.5) -> CLIP text encoder -> CLIP image encoder -> TI training pipeline -> Aggregation mechanism -> Privacy accountant

- **Critical path**: 1. Encode each private image using CLIP image encoder 2. Train TI embeddings for each image separately 3. Normalize and aggregate embeddings with calibrated noise 4. Use aggregated embedding for style transfer during generation

- **Design tradeoffs**:
  - Per-image vs. dataset-level TI training: Per-image captures more nuance but requires more computation
  - Subsampling rate m/n: Higher rates improve fidelity but reduce privacy amplification
  - Guidance weight w: Higher weights improve style alignment but may reduce prompt fidelity

- **Failure signatures**:
  - Low KID scores with high variance: Style guidance may not be effective for out-of-distribution styles
  - Images indistinguishable from no-adaptation baseline at ε≈0: Token embedding has lost semantic meaning
  - DP-SGD on TI fails to converge: Method is designed for small datasets where DP-SGD struggles

- **First 3 experiments**:
  1. Train DPAgg-TI with m=n and varying ε to establish baseline performance without subsampling
  2. Compare DPAgg-TI with m=n/2 and m=n/4 at fixed ε to evaluate subsampling benefits
  3. Test style guidance variant on a dataset known to work well with style guidance (e.g., Van Gogh paintings)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed DPAgg-TI method perform on datasets significantly larger than 100 images, and what are the practical limitations of scaling this approach?
- **Basis in paper**: [explicit] The authors note that "DPAgg-TI is designed for the low-data, strong-privacy regime, where the number of private images is small (n≈100)" and acknowledge that "for large datasets with moderate subsampling, DP-SGD on the full model may become more efficient and could provide better utility."
- **Why unresolved**: The paper focuses on small datasets (n=158 for artwork and n=47 for pictograms) and does not explore performance on larger datasets. The computational cost analysis only considers sequential optimization and doesn't address how batching or parallelization would scale.
- **What evidence would resolve it**: Systematic experiments testing DPAgg-TI on datasets of varying sizes (e.g., 100, 500, 1000, 10000 images) while maintaining privacy budgets, comparing utility and computational costs against DP-SGD baselines, would clarify scalability limitations.

### Open Question 2
- **Question**: Can the privacy-utility tradeoff in DPAgg-TI be further improved by exploring alternative aggregation methods beyond simple centroid averaging, such as weighted averaging or robust statistics?
- **Basis in paper**: [inferred] The authors use simple centroid averaging with Gaussian noise for privacy, but acknowledge that "u ∗ DP may not fully solve the TI optimization problem" and that there's a trade-off between privacy and image quality. The discussion of NAF suggests that "asymmetric" approaches might offer better utility.
- **Why unresolved**: The paper only explores one aggregation method (centroid averaging with Gaussian noise) and doesn't investigate whether more sophisticated aggregation techniques could achieve better privacy-utility tradeoffs.
- **What evidence would resolve it**: Experiments comparing DPAgg-TI with various aggregation methods (weighted averaging based on image quality, median aggregation, robust statistics) while measuring both privacy guarantees and output quality would reveal if better aggregation methods exist.

### Open Question 3
- **Question**: What is the theoretical relationship between the differential privacy guarantees provided by DPAgg-TI and the NAF-style guarantees, and under what conditions do these frameworks diverge?
- **Basis in paper**: [explicit] The authors state that "Since DPAgg-TI satisfies ε-DP, it also satisfies ε-NAF under the leave-one-out-safe model" and provide detailed discussion of how NAF relates to DP concepts, but acknowledge that "this guarantee is meaningful only within NAF" and that "ε-DP offers protection under arbitrary post-processing and multiple outputs, whereas ε-DPG only guarantees privacy for single outputs."
- **Why unresolved**: While the paper establishes that DPAgg-TI satisfies both ε-DP and ε-NAF, it doesn't provide a formal analysis of when and why these guarantees might diverge, particularly for multi-output scenarios or under different divergence measures.
- **What evidence would resolve it**: A formal theoretical analysis proving the conditions under which DPAgg-TI's ε-DP guarantees translate to ε-NAF guarantees, and identifying scenarios where these guarantees might differ (e.g., multi-output generation, different divergence measures), would clarify the relationship between these privacy frameworks.

## Limitations
- Limited to extremely small datasets (n=47-158 images) - scalability to larger datasets unclear
- Computational cost of per-image TI optimization is high, especially without batching
- User study methodology lacks details on statistical significance and participant selection

## Confidence

- **High confidence**: The mechanism of using per-image embeddings with aggregation and subsampling to reduce sensitivity is theoretically sound and correctly implemented based on the described DP framework
- **Medium confidence**: The empirical superiority over DP-SGD is demonstrated but could benefit from additional datasets and ablation studies
- **Low confidence**: The claim about style guidance being effective across all artistic styles, as this is only validated on the specific artwork dataset without broader testing

## Next Checks

1. **Dataset generalization test**: Apply the same methodology to 2-3 additional small datasets (e.g., medical imaging, architectural styles) to verify the approach works beyond artistic adaptation tasks
2. **Subsampling sensitivity analysis**: Systematically vary the subsampling ratio m/n from 10% to 100% while keeping ε fixed to quantify the exact tradeoff between privacy amplification and style fidelity degradation
3. **DP-SGD baseline optimization**: Test whether tuning hyperparameters (learning rate, batch size, gradient clipping) for the specific small datasets could improve DP-SGD performance, providing a fairer comparison with the proposed method