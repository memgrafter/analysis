---
ver: rpa2
title: 'FovEx: Human-Inspired Explanations for Vision Transformers and Convolutional
  Neural Networks'
arxiv_id: '2408.02123'
source_url: https://arxiv.org/abs/2408.02123
tags:
- fovex
- maps
- attribution
- explanation
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FovEx addresses the need for model-agnostic explainability methods
  in deep learning by introducing a novel approach inspired by human foveated vision.
  The method uses differentiable foveation to create blurred versions of input images,
  then employs gradient-based attention mechanisms to iteratively explore the image
  and identify relevant regions.
---

# FovEx: Human-Inspired Explanations for Vision Transformers and Convolutional Neural Networks

## Quick Facts
- arXiv ID: 2408.02123
- Source URL: https://arxiv.org/abs/2408.02123
- Reference count: 40
- Key outcome: FovEx achieves state-of-the-art performance in model-agnostic explainability, outperforming existing methods in 4 out of 5 metrics for transformers and 3 out of 5 metrics for convolutional models while demonstrating better alignment with human gaze patterns.

## Executive Summary
FovEx introduces a novel model-agnostic explainability method inspired by human foveated vision, combining differentiable foveation with gradient-based attention mechanisms to generate attribution maps for both vision transformers and convolutional neural networks. The method iteratively creates foveated renderings of input images, uses gradient-based exploration to identify relevant regions, and combines these into final explanations that achieve superior performance across multiple evaluation metrics. FovEx demonstrates significant improvements in faithfulness to model predictions and alignment with human gaze patterns, achieving +14% in NSS compared to RISE and +203% in NSS compared to GradCAM.

## Method Summary
FovEx uses a differentiable foveation mechanism to create blurred peripheral vision while preserving sharp central vision, then employs gradient-based attention to iteratively explore and identify relevant image regions. The method generates a scanpath of fixations through optimization that minimizes classification loss with respect to fixation locations, then combines these fixation-based saliency maps into a final attribution map. FovEx is tested on pre-trained ResNet-50 and ViT-B/16 models using 5000 correctly classified ImageNet-1K validation images, with performance evaluated across five metrics including faithfulness (AVG % DROP, AVG % INCREASE, DELETE, INSERT), localization (Energy-Based Pointing Game), and human alignment (NSS, AUCJ).

## Key Results
- FovEx outperforms existing methods in 4 out of 5 faithfulness metrics for transformers and 3 out of 5 metrics for convolutional models
- The method demonstrates superior human-gaze alignment (+14% in NSS compared to RISE, +203% in NSS compared to GradCAM)
- FovEx achieves state-of-the-art performance across both vision transformers and convolutional neural networks without architecture-specific modifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FovEx uses biologically inspired foveation to create noise-robust, human-aligned attribution maps that outperform existing methods.
- Mechanism: The method applies differentiable foveation to create blurred peripheral vision while preserving sharp central vision, then uses gradient-based attention to iteratively explore and identify relevant image regions. These regions are combined into an attribution map that explains model predictions.
- Core assumption: The human visual system's foveated processing provides a superior basis for identifying relevant features compared to uniform processing.
- Evidence anchors:
  - [abstract]: "FovEx seamlessly integrates biologically inspired perturbations by iteratively creating foveated renderings of the image and combines them with gradient-based visual explorations to determine locations of interest efficiently."
  - [section]: "We draw inspiration from Schwinn et al. [26] to design a differentiable foveation mechanism... The foveation mechanism allows for sequential exploration of the given input image."
  - [corpus]: Weak evidence - corpus contains related work on XAI methods but no direct comparison to foveation-based approaches.
- Break condition: If the differentiable foveation introduces too much noise or if the gradient-based exploration fails to converge to meaningful locations, the attribution maps would lose both faithfulness and human alignment.

### Mechanism 2
- Claim: The gradient-based attention mechanism efficiently identifies relevant image regions by minimizing classification loss with respect to fixation locations.
- Mechanism: After each foveated transformation, the method computes gradients of the loss with respect to the current fixation point, then updates the fixation location to minimize loss. This creates a scanpath of fixations that captures the most informative regions.
- Core assumption: The model's classification loss gradients directly indicate which image regions are most relevant for the prediction.
- Evidence anchors:
  - [section]: "The next fixation locations are dynamically adjusted to minimize the loss function L with respect to the current fixation location ùëìùë°, i.e., ùëìùë°+1 = ùëìùë° ‚àí ùúÜ ùõøL/ùõø ùëìùë°"
  - [abstract]: "These locations are selected to maximize the performance of the model to be explained with respect to the downstream task"
  - [corpus]: Weak evidence - corpus contains gradient-based XAI methods but no direct evidence of gradient-based fixation optimization.
- Break condition: If the optimization gets stuck in local minima or if the gradients become unstable due to the foveated transformations, the method would fail to identify meaningful regions.

### Mechanism 3
- Claim: The combination of foveation-based perturbations and gradient-based exploration creates explanations that align with human gaze patterns while maintaining model faithfulness.
- Mechanism: The foveation introduces biologically plausible noise that mimics human visual processing, while the gradient-based exploration finds regions that matter to the model. The weighted combination of fixation-based saliency maps creates final explanations that satisfy both human and model criteria.
- Core assumption: Human gaze patterns and model-relevant regions overlap significantly, so a method that respects both will produce superior explanations.
- Evidence anchors:
  - [abstract]: "Furthermore, we show the alignment between the explanation map produced by FovEx and human gaze patterns (+14% in NSS compared to RISE, +203% in NSS compared to GradCAM)"
  - [section]: "These results together demonstrate how introducing biological constraints in artificial neural networks both increases alignment with the human counterpart and fosters model performances."
  - [corpus]: Weak evidence - corpus contains human-gaze correlation studies but no direct evidence of foveation-based alignment.
- Break condition: If human gaze patterns and model-relevant regions diverge significantly, the method would fail to simultaneously satisfy both criteria.

## Foundational Learning

- Concept: Differentiable image transformations
  - Why needed here: FovEx requires differentiable foveation to enable gradient-based optimization of fixation locations
  - Quick check question: Can you implement a Gaussian blur that is differentiable with respect to both the image and the blur parameters?

- Concept: Gradient-based optimization in high-dimensional spaces
  - Why needed here: The method optimizes fixation locations in 2D image space using gradients of classification loss
  - Quick check question: How would you handle cases where the gradient-based optimization gets stuck in local minima?

- Concept: Scanpath generation and fixation modeling
  - Why needed here: FovEx generates sequences of fixations that mimic human visual exploration patterns
  - Quick check question: What factors would you consider when deciding how many fixations to generate for a given image?

## Architecture Onboarding

- Component map:
  - Differentiable foveation module (Gaussian blurring with position-dependent weights)
  - Gradient-based attention optimizer (loss minimization with respect to fixation locations)
  - Scanpath generator (iterative fixation updates with random restarts)
  - Attribution map combiner (weighted linear combination of fixation-based saliency maps)

- Critical path:
  1. Input image ‚Üí Differentiable foveation ‚Üí Loss computation
  2. Loss gradients ‚Üí Fixation update ‚Üí New fixation location
  3. Multiple fixations ‚Üí Saliency maps ‚Üí Weighted combination ‚Üí Attribution map

- Design tradeoffs:
  - Foveation parameters (œÉf, œÉb) control the trade-off between localization precision and computational efficiency
  - Number of optimization steps (OS) vs. attribution map quality and computation time
  - Random restarts (RR) vs. risk of getting stuck in local minima

- Failure signatures:
  - Attribution maps that are too diffuse or too concentrated
  - Fixation locations that cluster in one area rather than exploring the image
  - Degradation in performance metrics when varying key hyperparameters

- First 3 experiments:
  1. Compare attribution maps generated with different foveation parameters on a simple image with clear objects
  2. Test the gradient-based optimization convergence on images with varying complexity
  3. Validate human-gaze alignment on a small dataset with known eye-tracking data

## Open Questions the Paper Calls Out
None

## Limitations
- The differentiable foveation mechanism's implementation details remain underspecified, potentially limiting reproducibility
- The method's computational complexity could be prohibitive for real-time applications
- The assumption that human gaze patterns align with model-relevant regions may not hold for all architectures or tasks

## Confidence
- High confidence: The method's technical feasibility and general approach to using differentiable foveation for XAI
- Medium confidence: Claims about superior performance across all metrics and architectures
- Low confidence: The extent of human-gaze alignment improvement and the method's robustness to different image types and model architectures

## Next Checks
1. Implement a controlled experiment comparing FovEx attribution maps with human eye-tracking data on a subset of MIT1003 to quantify the claimed +14% NSS improvement
2. Test FovEx's performance on out-of-distribution images and adversarial examples to assess robustness beyond the ImageNet validation set
3. Benchmark the method's computational efficiency against baselines, measuring both attribution map quality and generation time across different image resolutions