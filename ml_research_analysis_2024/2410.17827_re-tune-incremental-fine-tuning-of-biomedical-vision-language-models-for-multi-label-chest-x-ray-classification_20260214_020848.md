---
ver: rpa2
title: 'RE-tune: Incremental Fine Tuning of Biomedical Vision-Language Models for
  Multi-label Chest X-ray Classification'
arxiv_id: '2410.17827'
source_url: https://arxiv.org/abs/2410.17827
tags:
- text
- prompts
- image
- re-tune
- incremental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RE-tune is an exemplar-free, privacy-preserving approach for incremental
  fine-tuning of biomedical vision-language models for multi-label chest X-ray classification.
  The method freezes the image and text encoders of a pre-trained VLM and trains simple
  adaptor layers on top, while engineering positive and negative text prompts for
  each disease class.
---

# RE-tune: Incremental Fine Tuning of Biomedical Vision-Language Models for Multi-label Chest X-ray Classification

## Quick Facts
- arXiv ID: 2410.17827
- Source URL: https://arxiv.org/abs/2410.17827
- Reference count: 10
- Key outcome: Achieves strong performance (AUC 0.81-0.87) across incremental learning scenarios with only 40% of training data needed for data-incremental learning

## Executive Summary
RE-tune is a novel exemplar-free approach for incremental fine-tuning of biomedical vision-language models on multi-label chest X-ray classification tasks. The method freezes pre-trained image and text encoders while training lightweight adaptor layers on top, using engineered text prompts and contrastive loss to align visual embeddings with disease representations. Evaluated on CheXpert across three incremental learning scenarios, RE-tune demonstrates remarkable ability to prevent catastrophic forgetting while achieving strong classification performance.

## Method Summary
RE-tune freezes the image and text encoders of a pre-trained biomedical vision-language model and trains simple adaptor layers on top. For each disease class, positive and negative text prompts are engineered and used to create contrastive pairs. The adaptors learn to align visual embeddings with disease-related text prompts using a contrastive loss based on cosine similarity. The method is evaluated on CheXpert across class-incremental, label-incremental, and data-incremental scenarios, with template prompts showing the best performance (AUC 0.81-0.87).

## Key Results
- Template prompts achieve the best performance (AUC 0.81-0.87) across all incremental scenarios
- RE-tune prevents catastrophic forgetting, maintaining strong performance on previously seen classes
- Data-incremental learning requires only 40% of the training data while achieving comparable results
- The approach demonstrates computational efficiency through frozen backbones and lightweight adaptors

## Why This Works (Mechanism)

### Mechanism 1
Freezing the image and text encoders prevents catastrophic forgetting by preserving learned representations. By freezing the backbones (Eimg and Etxt), RE-tune maintains the integrity of the original feature spaces while only adapting the alignment between image and text embeddings through lightweight adaptor layers. The core assumption is that the pre-trained encoders already contain robust representations that generalize across incremental tasks.

### Mechanism 2
Contrastive loss using cosine similarity between image and text embeddings enables multi-label classification without exemplars. The adaptor layers learn to align visual embeddings with disease-related text prompts through a contrastive loss that maximizes positive cosine similarity (presence) and minimizes negative cosine similarity (absence) for each label. The core assumption is that the joint embedding space created by the encoders can be effectively steered by adaptor layers to discriminate between diseases.

### Mechanism 3
Semantic text prompts guide the training trajectory and prevent catastrophic forgetting. By engineering positive and negative text prompts for each disease class, RE-tune leverages the semantic understanding of the text encoder to create attractor patterns in the embedding space that preserve knowledge of previously learned classes. The core assumption is that the text encoder can generate meaningful representations for both disease presence and absence prompts that can guide the visual encoder.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: Enables learning without labels by pulling similar examples together and pushing dissimilar ones apart in embedding space
  - Quick check question: How does contrastive loss differ from standard classification loss in terms of what it optimizes?

- Concept: Catastrophic forgetting in incremental learning
  - Why needed here: Understanding why models lose previously learned information when trained on new tasks is crucial for designing prevention strategies
  - Quick check question: What are the two main categories of approaches to prevent catastrophic forgetting?

- Concept: Vision-language model architectures
  - Why needed here: Understanding how image and text encoders produce joint embeddings is fundamental to grasping how RE-tune works
  - Quick check question: What is the typical output dimensionality of vision-language joint embeddings in models like CLIP or BioViL?

## Architecture Onboarding

- Component map: BioViL model (frozen image and text encoders) → Image Adaptor (Aimg) → Text Adaptor (Atxt) → Cosine similarity computation → Binary Cross Entropy loss
- Critical path: Image → Eimg → Aimg → eimg → Cosine similarity → BCE loss (for presence) and Text → Etxt → Atxt → etxt → Cosine similarity → BCE loss (for absence)
- Design tradeoffs: Freezing encoders provides computational efficiency and prevents forgetting but limits adaptability; adaptor layers add minimal parameters but may be insufficient for domain shifts
- Failure signatures: Poor performance on previously learned classes indicates catastrophic forgetting; high variance across seeds suggests instability; degraded performance on unseen diseases indicates over-specialization
- First 3 experiments:
  1. Verify that freezing the encoders prevents their weights from updating during training
  2. Test different adaptor architectures (Dense vs MLP) on a small subset of data to find optimal configuration
  3. Evaluate zero-shot performance on CheXpert to establish baseline before any fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of text prompts affect the model's ability to prevent catastrophic forgetting across different incremental learning scenarios?
- Basis in paper: The paper states that "RE-tune with Template Prompts demonstrates remarkable class-incremental learning capabilities" and that "the quality of textual prompts plays a pivotal role in performance," but notes that "the best-performing prompts are still manually engineered ones."
- Why unresolved: While the paper shows that template prompts work well, it does not provide a systematic analysis of how different prompt qualities (e.g., semantic richness, specificity) quantitatively impact catastrophic forgetting prevention across various scenarios.
- What evidence would resolve it: A comprehensive ablation study varying prompt quality metrics (semantic similarity, specificity, etc.) across all incremental scenarios, measuring catastrophic forgetting rates and classification performance.

### Open Question 2
- Question: Can the adaptor architecture be optimized further to reduce noise and improve generalization across all incremental learning scenarios?
- Basis in paper: The paper notes that "some scenarios exhibit a higher degree of noise," indicating that while RE-tune is robust, there is room for improvement in generalization.
- Why unresolved: The paper evaluates different adaptor configurations but does not explore architectural optimizations or regularization techniques that could further reduce noise and enhance generalization.
- What evidence would resolve it: Experiments comparing RE-tune with alternative adaptor architectures (e.g., residual connections, attention mechanisms) and regularization strategies, measuring noise reduction and generalization performance across all scenarios.

### Open Question 3
- Question: How does RE-tune perform when fine-tuning on biomedical VLMs that were pre-trained on datasets with different disease distributions than CheXpert?
- Basis in paper: The paper states that "BioViL has not been pre-trained on CheXpert," implying that the model's performance may depend on the pre-training dataset's disease distribution.
- Why unresolved: The paper only evaluates RE-tune on BioViL, which was not pre-trained on CheXpert, leaving open the question of how well RE-tune generalizes to VLMs with different pre-training distributions.
- What evidence would resolve it: Fine-tuning experiments on multiple biomedical VLMs pre-trained on datasets with varying disease distributions, comparing RE-tune's performance and adaptability across these models.

## Limitations
- The long-term effectiveness of the approach for preventing catastrophic forgetting in real-world clinical deployment remains unproven
- The contrastive loss mechanism may have limitations in handling complex multi-label relationships where diseases co-occur frequently
- The prompt engineering approach relies heavily on the text encoder's understanding of medical terminology, which may not generalize well across all medical imaging domains

## Confidence

- **High**: The basic architecture of freezing encoders and adding adaptor layers is well-established in incremental learning literature
- **Medium**: The contrastive loss approach using cosine similarity is theoretically sound but requires empirical validation across diverse scenarios
- **Low**: The long-term effectiveness of the approach for preventing catastrophic forgetting in real-world clinical deployment remains unproven

## Next Checks

1. Test the approach on a larger set of chest X-ray diseases (beyond the five evaluated) to assess scalability and robustness
2. Evaluate performance when tasks are introduced in random order rather than the structured scenarios presented
3. Conduct ablation studies to quantify the individual contributions of prompt engineering, adaptor layers, and contrastive loss to overall performance