---
ver: rpa2
title: 'RealKIE: Five Novel Datasets for Enterprise Key Information Extraction'
arxiv_id: '2403.20101'
source_url: https://arxiv.org/abs/2403.20101
tags:
- base
- layoutlm-v3
- roberta
- deberta-v3
- longformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RealKIE introduces five novel datasets for key information extraction
  from enterprise documents, targeting challenges like poor OCR quality, sparse annotations,
  and complex layouts. The datasets include SEC S1 Filings, NDAs, UK Charity Reports,
  FCC Invoices, and Resource Contracts.
---

# RealKIE: Five Novel Datasets for Enterprise Key Information Extraction

## Quick Facts
- arXiv ID: 2403.20101
- Source URL: https://arxiv.org/abs/2403.20101
- Reference count: 40
- Primary result: Five novel datasets for key information extraction from enterprise documents, with DeBERTa-v3 achieving best overall performance (F1 45.5-83.7)

## Executive Summary
RealKIE introduces five novel datasets targeting key information extraction from enterprise documents, addressing challenges like poor OCR quality, sparse annotations, and complex layouts. The datasets span SEC S1 Filings, NDAs, UK Charity Reports, FCC Invoices, and Resource Contracts. Baseline models including RoBERTa, DeBERTa-v3, LayoutLM-v3, and Longformer were evaluated, with DeBERTa-v3 achieving the best overall performance. The datasets and code are publicly available under CC-BY-NC 4.0 license, providing a standardized benchmark for enterprise KIE research.

## Method Summary
The study evaluates four transformer-based models on five novel KIE datasets. Models were trained using token-classification with hyperparameter optimization via Hyperband Bayesian search. The evaluation uses span-level F1 scores, with undersampling of negative chunks to address class imbalance. Datasets include OCR outputs and PDF images, with text serialization varying by document type. The baseline comparison includes RoBERTa-base, DeBERTa-v3-base, LayoutLM-v3-base, and Longformer-base, with context sizes of 512 or 4096 tokens.

## Key Results
- DeBERTa-v3 achieved best overall performance (F1 45.5-83.7) across all five datasets
- LayoutLM-v3 excelled on paragraph-level fields (up to 19 F1 improvement) but showed inconsistent table performance
- Longformer benefited from longer context (4/5 tasks) compared to RoBERTa
- OCR quality varied significantly across datasets, with FCC Invoices and Resource Contracts having lowest confidence scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DeBERTa-v3 excels at paragraph-level field extraction due to its improved pre-training and embedding-sharing strategy
- Mechanism: DeBERTa-v3 uses ELECTRA-style pre-training with gradient-disentangled embedding sharing, enhancing its ability to model long-range dependencies and capture semantic nuances in longer spans typical of paragraph-level fields
- Core assumption: The pre-training approach provides DeBERTa-v3 with better representations for long text spans compared to RoBERTa and LayoutLM-v3
- Evidence anchors:
  - [abstract]: "LayoutLM-v3 excelled on paragraph-level fields (up to 19 F1 improvement)"
  - [section]: "The most consistent benefits of LayoutLM-v3 are found on the paragraph-level fields, where improvements as high as 19 F1 are observed"
  - [corpus]: "No direct evidence of ELECTRA-style pre-training advantage in corpus; assumption based on model description"
- Break condition: If the dataset lacks sufficient long paragraph fields or if text-only models achieve comparable performance on paragraph fields

### Mechanism 2
- Claim: Longformer's larger context size (4096 vs 512) improves performance on tasks requiring longer context
- Mechanism: Longformer uses sliding window attention mechanism that allows processing documents up to 4096 tokens, capturing information that shorter context models miss, particularly beneficial for sparse annotations and complex layouts
- Core assumption: The tasks in RealKIE benefit from longer context due to document length and sparse annotations
- Evidence anchors:
  - [abstract]: "Longformer benefited from longer context (4/5 tasks)"
  - [section]: "As Longformer is a RoBERTa derivative with a larger context size [42], comparing these two models hints at whether long-term context is useful for solving these information extraction tasks"
  - [corpus]: "Weak corpus support; comparison based on model architecture differences"
- Break condition: If documents are short enough to fit within 512 tokens or if the information needed for extraction is localized within shorter spans

### Mechanism 3
- Claim: LayoutLM-v3's 2D position embeddings provide marginal benefits for tabular field extraction despite the counter-intuitive result
- Mechanism: LayoutLM-v3 incorporates 2D position information from document layouts, which should theoretically help with table structure understanding, though the paper notes this doesn't consistently outperform text-only models on tables
- Core assumption: The 2D position information should improve table understanding, but practical benefits may be limited by OCR quality and table complexity
- Evidence anchors:
  - [abstract]: "LayoutLM-v3 excelled on paragraph-level fields (up to 19 F1 improvement), while Longformer benefited from longer context (4/5 tasks)"
  - [section]: "Margins are consistently small between DeBERTa-v3 and LayoutLM-v3 for tabular fields"
  - [corpus]: "No direct corpus evidence of 2D position benefit; assumption based on model architecture"
- Break condition: If table layouts are simple enough that 1D position information suffices, or if OCR quality is too poor for 2D position information to be reliable

## Foundational Learning

- Concept: Document layout complexity and its impact on information extraction
  - Why needed here: RealKIE datasets contain complex layouts including nested tables, varying text serialization, and poor OCR quality that affect model performance
  - Quick check question: What are the three main layout complexity measures reported in Table 1, and which dataset shows the lowest text linearity score?

- Concept: Class imbalance in long documents with sparse annotations
  - Why needed here: The datasets have high ratios of empty chunks to labeled chunks, creating significant class imbalance that affects model training and evaluation
  - Quick check question: What is the maximum class imbalance ratio (including background class) reported in Table 2, and which dataset exhibits this?

- Concept: OCR quality metrics and their relationship to information extraction performance
  - Why needed here: OCR confidence scores correlate with character recognition accuracy, affecting the quality of input text for models
  - Quick check question: Which two datasets have the lowest OCR confidence scores, and what does this indicate about their OCR quality?

## Architecture Onboarding

- Component map: OCR pipeline (OmniPage or Azure Read OCR) -> Text serialization -> Token classification model (RoBERTa, DeBERTa-v3, LayoutLM-v3, or Longformer) -> Span extraction -> Evaluation metrics
- Critical path: 1) Document preprocessing (OCR, image rendering, text serialization) 2) Data loading and chunking for model context size 3) Model training with hyperparameter optimization 4) Evaluation using span-level F1 metrics 5) Analysis of results by field type and dataset
- Design tradeoffs: Context length vs. computational efficiency (Longformer's 4096 vs others' 512), Layout information vs. text-only approaches (LayoutLM-v3 vs text-only models), Undersampling vs. class imbalance handling strategies, OCR quality vs. model robustness to text errors
- Failure signatures: Poor performance on paragraph-level fields despite layout models (indicates layout information not helpful for long text), Inconsistent performance across table and non-table fields (suggests layout information benefits are field-dependent), High variance in results across different runs (may indicate sensitivity to initialization or data ordering)
- First 3 experiments: 1) Compare baseline models on a single field type (e.g., dates) across all datasets to establish relative model strengths 2) Test different chunking strategies (fixed vs. sliding window) on a dataset with sparse annotations to measure impact on recall 3) Evaluate model performance with and without OCR confidence filtering to assess robustness to OCR quality variations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do OCR-free methods like DocParser or Donut compare to the token-classification baselines on these datasets, particularly for documents with poor OCR quality?
- Basis in paper: [inferred] The paper explicitly mentions that token-classification baselines omit OCR-free methods and notes this may impact fair comparison for approaches that re-OCR pages using different OCR providers
- Why unresolved: The paper chose to focus baselines on token-classification to standardize on span-based metrics, leaving OCR-free methods unexplored
- What evidence would resolve it: Direct evaluation of OCR-free models (DocParser, Donut) on the same RealKIE datasets with comparable metrics

### Open Question 2
- Question: What is the impact of document segmentation (page-level vs. document-level) on model performance, especially for long documents like S1 filings?
- Basis in paper: [explicit] The paper states S1 documents were segmented at page level due to practical challenges, but document-level versions exist
- Why unresolved: No comparison is provided between page-level and document-level processing performance
- What evidence would resolve it: Side-by-side evaluation of models trained on page-level vs. document-level S1 data with metrics reported

### Open Question 3
- Question: How does model performance vary with different OCR quality levels across the five datasets, and can models be made robust to OCR artifacts?
- Basis in paper: [explicit] The paper provides OCR confidence scores and notes that FCC Invoices and Resource Contracts have the lowest OCR confidence, indicating more frequent character-level OCR errors
- Why unresolved: While OCR quality is analyzed, the paper does not test model robustness to varying OCR quality or investigate mitigation techniques
- What evidence would resolve it: Experiments comparing model performance across datasets with different OCR quality, and testing techniques like data augmentation or error correction

### Open Question 4
- Question: What is the effect of including or excluding the background class on model training stability and performance for datasets with high class imbalance?
- Basis in paper: [explicit] The paper discusses class imbalance issues and mentions including an option to undersample negative chunks in the hyperparameter search
- Why unresolved: The paper does not report results comparing models trained with and without the background class
- What evidence would resolve it: Training and evaluating models with and without the background class on the same datasets, comparing metrics like F1, stability, and training dynamics

## Limitations

- Dataset size imbalance across the five datasets creates challenges for drawing definitive conclusions about model performance
- OCR quality varies substantially across datasets, with some having particularly poor performance that may artificially limit model performance
- Class imbalance ratios reaching up to 2000:1 create additional challenges for training robust models
- Evaluation relies solely on span-level F1 scores without considering semantic equivalence or partial matches

## Confidence

**High Confidence**: DeBERTa-v3 achieves best overall performance (F1 45.5-83.7) across all five datasets
**Medium Confidence**: LayoutLM-v3 excels on paragraph-level fields (up to 19 F1 improvement) while not consistently outperforming text-only models on tables
**Low Confidence**: Longformer's larger context size (4096 vs 512) provides consistent benefits across 4/5 tasks

## Next Checks

1. **Ablation Study on Context Length**: Conduct experiments systematically varying context window size (512, 1024, 2048, 4096 tokens) to quantify exact relationship between context length and performance on sparse annotation tasks

2. **OCR Quality Robustness Analysis**: Perform controlled experiments where models are trained and evaluated on documents with varying OCR quality thresholds (OCR confidence > 0.9, > 0.95, etc.)

3. **Layout Information Ablation**: Conduct experiments comparing LayoutLM-v3 with and without 2D position embeddings on the same datasets to directly measure contribution of layout information to paragraph-level field extraction