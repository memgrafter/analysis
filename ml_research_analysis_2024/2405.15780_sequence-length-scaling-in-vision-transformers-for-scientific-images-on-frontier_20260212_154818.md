---
ver: rpa2
title: Sequence Length Scaling in Vision Transformers for Scientific Images on Frontier
arxiv_id: '2405.15780'
source_url: https://arxiv.org/abs/2405.15780
tags:
- sequence
- length
- parallelism
- attention
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of scaling Vision Transformers
  (ViTs) to handle ultra-long sequence lengths, particularly for scientific imagery
  applications like climate modeling. The authors develop distributed sequence parallelism
  methods leveraging DeepSpeed-Ulysses and Long-Sequence-Segmentation, enabling ViTs
  to process up to 1 million tokens.
---

# Sequence Length Scaling in Vision Transformers for Scientific Images on Frontier

## Quick Facts
- arXiv ID: 2405.15780
- Source URL: https://arxiv.org/abs/2405.15780
- Reference count: 40
- One-line primary result: First training of transformer model on full-attention matrix over 188K sequence length, achieving 20% accuracy improvement in climate modeling

## Executive Summary
This work addresses the challenge of scaling Vision Transformers (ViTs) to handle ultra-long sequence lengths for scientific imagery applications, particularly climate modeling. The authors develop distributed sequence parallelism methods leveraging DeepSpeed-Ulysses and Long-Sequence-Segmentation, enabling ViTs to process up to 1 million tokens. By integrating model sharding and combining sequence parallelism with pipeline and tensor parallelism, they achieve a 94% batch scaling efficiency across 2,048 AMD MI250X GPUs. Their method significantly enhances climate modeling accuracy, improving temperature predictions by 20% compared to baseline models.

## Method Summary
The method employs distributed sequence parallelism using DeepSpeed-Ulysses and Long-Sequence-Segmentation to partition input sequences across GPUs, enabling processing of ultra-long sequences up to 1 million tokens. The system combines this with hybrid parallelism approaches including pipeline parallelism (PP) for model layer partitioning and tensor parallelism (TP) for tensor operation distribution. Flash Attention v2 is integrated to reduce memory footprint and enable longer sequences on the same hardware. The approach is validated on the ERA5 climate dataset with models up to 10B parameters, achieving full-attention matrix computation on 188K sequence length using 2,048 AMD MI250X GPUs on the Frontier supercomputer.

## Key Results
- First training of transformer model on full-attention matrix over 188K sequence length
- 20% improvement in temperature prediction accuracy for climate modeling compared to baseline models
- 94% batch scaling efficiency achieved across 2,048 AMD MI250X GPUs
- Successful processing of up to 1 million tokens using distributed sequence parallelism

## Why This Works (Mechanism)

### Mechanism 1
Distributed sequence parallelism enables Vision Transformers to process up to 1 million tokens by splitting the input sequence across GPUs using DeepSpeed-Ulysses. This partitions the attention computation across multiple GPUs while maintaining full attention matrix computation through coordinated all-to-all collective communication. The approach works by distributing sequence dimension computation, allowing each GPU to handle a subset of tokens while coordinating results. This breaks when communication overhead becomes prohibitive or GPU-GPU interconnect bandwidth is saturated.

### Mechanism 2
Combining sequence parallelism with pipeline and tensor parallelism enables scaling beyond single GPU memory limits for large models. The hybrid approach uses DeepSpeed-Ulysses for sequence distribution, Pipeline Parallelism for model layer partitioning, and Tensor Parallelism for tensor operation distribution. This orthogonal combination allows each method to address different scaling dimensions without creating deadlocks. The approach breaks when memory fragmentation or synchronization overhead from combining multiple parallelism strategies exceeds the benefits of distributed computation.

### Mechanism 3
Flash Attention v2 significantly reduces memory footprint by optimizing attention computation through tiling the attention matrix to reduce memory reads/writes between HBM and on-chip SRAM. This parallelization in the sequence length dimension allows fitting sequences 4-32 times larger than standard attention. The approach breaks when Flash Attention optimization cannot overcome fundamental memory bandwidth limitations or when kernel launch overhead becomes prohibitive for very large sequences.

## Foundational Learning

- Concept: Vision Transformer architecture and patch embedding
  - Why needed here: Understanding how ViTs convert images to token sequences is crucial for grasping why sequence length scaling is challenging
  - Quick check question: How does changing patch size from 16x16 to 4x4 affect the sequence length for a 224x224 image?

- Concept: Transformer self-attention mechanism and its computational complexity
  - Why needed here: The quadratic scaling of attention computation with sequence length is the fundamental bottleneck this work addresses
  - Quick check question: What is the computational complexity of standard self-attention in terms of sequence length L?

- Concept: Distributed training paradigms (data, pipeline, tensor, sequence parallelism)
  - Why needed here: The paper combines multiple parallelism strategies, requiring understanding of how each addresses different aspects of scaling
  - Quick check question: What is the primary dimension each parallelism type (DP, PP, TP, SP) partitions across?

## Architecture Onboarding

- Component map: Input sequence → DeepSpeed-Ulysses (sequence parallelism) → Pipeline Parallelism (model layer partitioning) → Tensor Parallelism (tensor operation distribution) → Flash Attention v2 (optimized attention) → DeepSpeed Zero Redundancy Optimizer (memory-efficient parameter distribution) → Output

- Critical path: Forward pass → Attention computation (distributed) → Feed-forward network → Backward pass → Gradient synchronization → Parameter update

- Design tradeoffs:
  - Memory vs. communication: More GPUs reduce memory per device but increase communication
  - Model size vs. sequence length: Larger models may require more GPUs, limiting available GPUs for sequence parallelism
  - Accuracy vs. efficiency: Full attention provides better accuracy but is computationally expensive compared to sparse alternatives

- Failure signatures:
  - Memory errors: Insufficient GPU memory despite distributed approach (indicates need for more aggressive model sharding or smaller local batch size)
  - Communication bottlenecks: Poor scaling efficiency with more GPUs (indicates interconnect saturation or suboptimal all-to-all patterns)
  - Compute underutilization: Low TFLOPs despite sufficient memory (indicates synchronization overhead or kernel launch inefficiencies)

- First 3 experiments:
  1. Single GPU baseline: Test standard ViT training on maximum sequence length that fits on one GPU to establish performance baseline
  2. DeepSpeed-Ulysses scaling: Test sequence parallelism with increasing GPUs (1→2→4→8) on fixed sequence length to measure scaling efficiency
  3. Hybrid approach validation: Combine DeepSpeed-Ulysses with Pipeline Parallelism on a model too large for single GPU to verify orthogonal scaling benefits

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of DeepSpeed-Ulysses compare to other sequence parallelism methods like Ring Attention in ultra-long sequence lengths beyond 1 million tokens? The paper focuses on DeepSpeed-Ulysses and LSS for sequences up to 1 million tokens, leaving a gap in understanding how these methods perform at extreme scales or against alternative approaches.

### Open Question 2
What is the optimal balance between model size and sequence length for maximizing accuracy in scientific imaging tasks? The paper highlights the importance of balancing these factors but lacks empirical data to determine the optimal ratio for different scientific imaging domains.

### Open Question 3
How does the accuracy of Vision Transformers with ultra-long sequences compare to traditional numerical methods in weather forecasting beyond 7-day predictions? The paper shows a 20% improvement in temperature predictions for up to 28-hour forecasts but doesn't explore longer-range forecasting where traditional methods might still excel.

## Limitations
- Limited generalizability beyond climate datasets to other scientific imaging domains
- Absolute throughput numbers and comparison to sparse attention alternatives remain unclear
- Practical performance advantages over sparse attention for real-world scientific applications not fully quantified

## Confidence

**High Confidence:**
- Technical feasibility of distributed sequence parallelism using DeepSpeed-Ulysses and Long-Sequence-Segmentation
- 20% improvement in temperature prediction accuracy compared to baseline models on tested climate dataset
- Ability to process up to 1 million tokens using proposed distributed approach

**Medium Confidence:**
- 94% batch scaling efficiency claim across 2,048 AMD MI250X GPUs (absolute performance metrics not provided)
- First training of transformer model on full-attention matrix over 188K sequence length
- Extension to handle 10B parameter models with full attention

**Low Confidence:**
- Generalizability of results to other scientific imaging domains beyond climate data
- Long-term scalability of hybrid parallelism approach as models continue to grow
- Practical performance advantages over sparse attention alternatives

## Next Checks
1. **Cross-domain validation**: Test sequence parallelism approach on at least two additional scientific imaging datasets (e.g., medical imaging and astronomical data) to verify generalizability beyond climate modeling, including comparison with sparse attention baselines.

2. **Scaling efficiency validation**: Conduct comprehensive scaling study measuring absolute TFLOPs, memory usage, and communication overhead across different GPU counts (1→2→4→8→16→32) for both DeepSpeed-Ulysses and LSS methods, comparing performance against theoretical limits.

3. **Alternative architecture comparison**: Implement and benchmark comparable ViT model using sparse attention mechanisms (e.g., Longformer, BigBird) on same climate prediction task to quantify trade-offs between full attention accuracy and computational efficiency.