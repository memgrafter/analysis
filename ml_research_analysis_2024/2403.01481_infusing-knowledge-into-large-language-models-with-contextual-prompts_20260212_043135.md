---
ver: rpa2
title: Infusing Knowledge into Large Language Models with Contextual Prompts
arxiv_id: '2403.01481'
source_url: https://arxiv.org/abs/2403.01481
tags:
- knowledge
- language
- prompts
- context
- triples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple yet generalisable approach for knowledge
  infusion by generating prompts from the context in the input text. The approach
  leverages relevant context retrieved from a domain-specific fine-tuning corpus to
  infuse knowledge into a pre-trained language model.
---

# Infusing Knowledge into Large Language Models with Contextual Prompts

## Quick Facts
- arXiv ID: 2403.01481
- Source URL: https://arxiv.org/abs/2403.01481
- Authors: Kinshuk Vasisht; Balaji Ganesan; Vikas Kumar; Vasudha Bhatnagar
- Reference count: 8
- Primary result: Proposed method achieves 46.4% improvement in Hits@1 score and 3.2x improvement in MRR on TACRED dataset compared to fine-tuning without context

## Executive Summary
This paper proposes a simple yet generalizable approach for knowledge infusion in large language models by generating prompts from context in input text. The method leverages relevant context retrieved from a domain-specific fine-tuning corpus to enhance the model's knowledge base. Experiments on relation prediction and question answering tasks demonstrate significant performance improvements compared to standard fine-tuning approaches.

The key innovation lies in using contextual text from documents rather than maintaining structured knowledge graphs, making the approach more efficient and applicable to entities not well-represented in existing knowledge bases. The method shows promise for low-resource settings where extensive compute resources and data sources are unavailable.

## Method Summary
The proposed approach involves generating contextual prompts by retrieving relevant sentences mentioning entities from a domain-specific corpus and appending them to the input prompt. During fine-tuning, the pre-trained language model (Flan-T5) is trained on these contextual prompts alongside the original task instructions. The method uses named entity recognition to identify entities in the input prompt, retrieves relevant context from the corpus, and generates augmented prompts for fine-tuning. This process allows the model to learn entity-specific knowledge representations while maintaining task-specific performance.

## Key Results
- Achieves 46.4% improvement in Hits@1 score on TACRED relation prediction task compared to fine-tuning without context
- Shows 3.2x improvement in MRR (Mean Reciprocal Rank) on TACRED dataset
- Demonstrates effectiveness across relation prediction and question answering tasks
- Proves more generalizable and efficient than structured knowledge graph approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextual prompts improve relation prediction by providing additional entity-specific knowledge not captured in the base model
- Mechanism: During fine-tuning, relevant sentences mentioning entities in the input prompt are retrieved from a domain-specific corpus and appended to the prompt, augmenting the model's knowledge about entities
- Core assumption: Retrieved context contains useful information that helps the model understand entities better and make more accurate predictions
- Evidence anchors:
  - [abstract] "Experiments on relation prediction and question answering tasks show that the proposed method significantly improves performance compared to fine-tuning without context"
  - [section] "Our experiments show the effectiveness of our approach which we evaluate by probing the fine-tuned LLMs"
  - [corpus] Weak evidence - related papers discuss knowledge infusion but don't directly support the specific mechanism of using contextual text from documents
- Break condition: If retrieved context is irrelevant or misleading, it may confuse the model and degrade performance

### Mechanism 2
- Claim: Fine-tuning with contextual prompts allows the model to learn entity-specific knowledge representations
- Mechanism: During fine-tuning, model parameters are updated based on task-specific loss, and contextual information helps learn representations capturing entity-specific knowledge
- Core assumption: Fine-tuning over contextual prompts enables the model to learn entity-specific knowledge representations useful for downstream tasks
- Evidence anchors:
  - [abstract] "Our experiments show the effectiveness of our approach which we evaluate by probing the fine-tuned LLMs"
  - [section] "Since we are doing full fine-tuning, the model parameters are updated, and the error is propagated based on the task"
  - [corpus] Weak evidence - related papers discuss fine-tuning and knowledge infusion but don't specifically address contextual prompts' role in learning entity-specific representations
- Break condition: If fine-tuning process is unstable or learning rate is not properly tuned, the model may not effectively learn from contextual prompts

### Mechanism 3
- Claim: Use of contextual prompts is more generalizable and efficient compared to structured knowledge graphs
- Mechanism: By leveraging relevant context from domain-specific corpora, the approach avoids maintaining structured knowledge graphs, making it more applicable to entities not well-represented in existing knowledge bases
- Core assumption: Domain-specific corpora can provide relevant and useful context for entities even if they're not present in structured knowledge graphs
- Evidence anchors:
  - [abstract] "Infusing knowledge directly from documents without having to create knowledge graphs is not only efficient, but also more general"
  - [section] "Using fine-tuning makes our approach simple, scalable, and employable in low-resource settings where excessive compute resources and data sources are unavailable"
  - [corpus] Weak evidence - related papers discuss knowledge graphs and their limitations but don't directly support contextual prompts as a more generalizable alternative
- Break condition: If domain-specific corpus is not well-curated or lacks relevant information about entities, the approach may not be effective

## Foundational Learning

- Concept: Fine-tuning
  - Why needed here: Fine-tuning adapts the pre-trained language model to specific downstream tasks by updating model parameters based on task-specific loss
  - Quick check question: What is the difference between fine-tuning and training a model from scratch?

- Concept: Knowledge infusion
  - Why needed here: Knowledge infusion augments the language model with additional knowledge from external sources (contextual text from domain-specific corpora) to improve performance on downstream tasks
  - Quick check question: How does knowledge infusion differ from simply training on a larger dataset?

- Concept: Retrieval of relevant context
  - Why needed here: Retrieving relevant context from domain-specific corpora provides the model with additional information about entities involved in the input prompt
  - Quick check question: What techniques can be used to retrieve relevant context from a large corpus of text?

## Architecture Onboarding

- Component map: Pre-trained language model (Flan-T5) -> Domain-specific fine-tuning corpus -> Named entity recognition (NER) system -> Context retrieval system -> Fine-tuning pipeline

- Critical path:
  1. Input prompt (task instance + instructions)
  2. NER to identify entities in the prompt
  3. Context retrieval from the fine-tuning corpus
  4. Generation of contextual prompts by appending retrieved context to the input prompt
  5. Fine-tuning of the language model on the contextual prompts
  6. Inference on new prompts using the fine-tuned model

- Design tradeoffs:
  - Using full fine-tuning vs. parameter-efficient methods (e.g., adapters)
  - Size of the pre-trained model (Flan-T5-small vs. Flan-T5-base)
  - Size of the context retrieved (number of sentences or tokens)
  - Choice of domain-specific corpus

- Failure signatures:
  - Poor performance on relation prediction or question answering tasks
  - Inconsistent or unstable fine-tuning results
  - Difficulty in retrieving relevant context from the corpus

- First 3 experiments:
  1. Fine-tuning on relation prediction task without contextual prompts (baseline)
  2. Fine-tuning on relation prediction task with contextual prompts
  3. Fine-tuning on question answering task with contextual prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method's performance scale with the size of the knowledge graph or corpus?
- Basis in paper: [inferred] The paper mentions experiments on different datasets and discusses the impact of context length, but does not explicitly explore the scalability of the method with respect to the size of the knowledge graph or corpus
- Why unresolved: The paper does not provide experiments or analysis on how the method's performance changes as the size of the knowledge graph or corpus increases
- What evidence would resolve it: Experiments demonstrating the method's performance on increasingly larger knowledge graphs or corpora, with analysis of the scalability and any potential limitations

### Open Question 2
- Question: How does the proposed method compare to other knowledge infusion techniques in terms of computational efficiency and resource requirements?
- Basis in paper: [explicit] The paper mentions that the method is simple and scalable, and can be employed in low-resource settings, but does not provide a direct comparison with other knowledge infusion techniques in terms of computational efficiency and resource requirements
- Why unresolved: The paper does not provide a detailed comparison of the proposed method with other knowledge infusion techniques in terms of computational efficiency and resource requirements
- What evidence would resolve it: Experiments comparing the computational efficiency and resource requirements of the proposed method with other knowledge infusion techniques, such as additional pre-training, adapters, and soft prompts

### Open Question 3
- Question: How does the proposed method handle out-of-vocabulary entities or entities not present in the fine-tuning corpus?
- Basis in paper: [explicit] The paper mentions that the method is useful for entities that are usually not found in any knowledge graph, but does not provide details on how the method handles out-of-vocabulary entities or entities not present in the fine-tuning corpus
- Why unresolved: The paper does not provide details on how the proposed method handles out-of-vocabulary entities or entities not present in the fine-tuning corpus, which is an important consideration for real-world applications
- What evidence would resolve it: Experiments or analysis demonstrating the method's performance on out-of-vocabulary entities or entities not present in the fine-tuning corpus, and any potential strategies or limitations for handling such cases

## Limitations

- Heavy reliance on the quality and coverage of the domain-specific fine-tuning corpus, with no detailed analysis of corpus coverage impact
- Assumes retrieved context is relevant and beneficial without adequately addressing scenarios where retrieval might introduce noise or irrelevant information
- Limited evaluation on diverse domains and datasets, raising questions about generalizability beyond TACRED

## Confidence

**High Confidence** (backed by strong experimental evidence):
- The proposed method improves performance on TACRED relation prediction and question answering tasks compared to fine-tuning without context
- The contextual prompts approach is more generalizable and efficient than maintaining structured knowledge graphs
- Fine-tuning with contextual prompts allows the model to learn entity-specific knowledge representations

**Medium Confidence** (supported by experimental results but with limitations):
- Contextual prompts provide additional entity-specific knowledge not captured in the base model
- The approach is effective in low-resource settings where extensive compute resources are unavailable

**Low Confidence** (limited evidence or unaddressed concerns):
- The method's effectiveness across diverse domains and entity types
- The impact of context retrieval quality on overall performance
- The approach's scalability to very large knowledge bases or corpora

## Next Checks

1. **Corpus Coverage Analysis**: Conduct systematic experiments evaluating how quality and coverage of the domain-specific corpus affect model performance across different entity types and domains, including analysis of entities with varying levels of representation in the corpus

2. **Retrieval Quality Impact Study**: Design experiments measuring the effect of irrelevant or noisy context on model performance, including deliberately introducing noise into retrieved context or using different retrieval strategies to assess robustness

3. **Cross-Domain Generalization Test**: Evaluate the approach on multiple datasets from different domains (beyond TACRED) to assess generalizability, including domains with varying levels of overlap with the fine-tuning corpus and entities with different characteristics