---
ver: rpa2
title: 'DELE: Deductive $\mathcal{EL}^{++}$ Embeddings for Knowledge Base Completion'
arxiv_id: '2411.01574'
source_url: https://arxiv.org/abs/2411.01574
tags:
- axioms
- ontology
- knowledge
- negative
- deductive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses limitations in geometric ontology embeddings
  for the Description Logic EL++ by improving negative sampling and incorporating
  deductive closure. The core method involves designing novel negative loss functions
  for all normal forms in EL++ and developing a fast approximate algorithm to compute
  the deductive closure.
---

# DELE: Deductive $\mathcal{EL}^{++}$ Embeddings for Knowledge Base Completion

## Quick Facts
- arXiv ID: 2411.01574
- Source URL: https://arxiv.org/abs/2411.01574
- Authors: Olga Mashkova; Fernando Zhapa-Camacho; Robert Hoehndorf
- Reference count: 40
- Primary result: DELE improves knowledge base completion for EL++ ontologies by incorporating deductive closure and novel negative losses for all normal forms

## Executive Summary
This paper addresses fundamental limitations in geometric ontology embeddings for the Description Logic EL++ by developing methods to properly handle deductive closure and negative sampling. The core insight is that standard embedding approaches fail to distinguish between statements that are unprovable and provably false, often using entailed statements as negatives during training. The authors propose DELE (Deductive EL++ embeddings), which filters out entailed axioms from negative samples using a fast approximate algorithm for computing the deductive closure of EL++ ontologies, and incorporates novel negative loss functions for all seven normal forms in EL++. The approach demonstrates improved performance on knowledge base completion tasks across multiple benchmark datasets.

## Method Summary
DELE improves EL++ ontology embeddings by addressing two key limitations: the use of entailed axioms as negative samples during training, and the lack of appropriate loss functions for all axiom types. The method computes the deductive closure of the ontology using a fast approximate algorithm, then filters this closure to remove entailed axioms from negative samples during training. Additionally, DELE implements novel negative loss functions for all seven normal forms in normalized EL++ (GCI0, GCI1, GCI2, GCI3, GCI0-BOT, GCI1-BOT, GCI3-BOT), not just the commonly used GCI2. The approach is evaluated using embedding methods ELBE and Box2EL on three benchmark datasets: Gene Ontology with STRING protein interactions (yeast), Food Ontology, and GALEN ontology, showing consistent improvements in prediction accuracy.

## Key Results
- Incorporating deductive closure filtering during training significantly improves prediction of both entailed and non-entailed axioms
- Adding negative losses for all seven normal forms in EL++ enhances model performance beyond using only GCI2 losses
- The fast approximate algorithm for deductive closure computation enables practical application to real-world ontologies while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negative sampling that ignores deductive closure can include entailed axioms, which weakens model training.
- Mechanism: Random negatives may sample axioms that are already entailed in the ontology, so the model treats true statements as false negatives, leading to incorrect gradient updates.
- Core assumption: Entailed axioms are more likely to be sampled as negatives than non-entailed ones in a randomly chosen axiom.
- Evidence anchors:
  - [abstract] "they do not distinguish between statements that are unprovable and provably false, and therefore they may use entailed statements as negatives"
  - [section] "the random sampling approach for negatives may lead to suboptimal learning since some of the axioms treated as negatives may be entailed"
  - [corpus] "Weak" - no direct mention of sampling bias in neighbor papers
- Break condition: If the ontology has very few entailed axioms compared to the space of possible axioms, the sampling bias becomes negligible.

### Mechanism 2
- Claim: Filtering negatives using the deductive closure improves model performance by removing true axioms from the negative set.
- Mechanism: Compute the deductive closure of the ontology and remove any sampled negative that appears in this closure before using it in training loss.
- Core assumption: The deductive closure can be efficiently computed for EL++ ontologies and contains all entailed axioms.
- Evidence anchors:
  - [abstract] "We evaluated a set of embedding methods for EL++ ontologies, incorporating several modifications that aim to make use of the ontology deductive closure"
  - [section] "We suggest to filter selected negatives during training based on the deductive closure of the knowledge base"
  - [corpus] "Weak" - no direct mention of deductive closure filtering in neighbor papers
- Break condition: If computing the deductive closure becomes computationally prohibitive for very large ontologies.

### Mechanism 3
- Claim: Adding negative losses for all normal forms in EL++ improves model ability to predict non-GCI2 axioms.
- Mechanism: Define loss functions for each normal form (GCI0, GCI1, GCI3, etc.) and include them in training to encourage correct geometric separation for all axiom types.
- Core assumption: The embedding space can represent all normal forms geometrically and the model can learn to distinguish them with appropriate losses.
- Evidence anchors:
  - [abstract] "We designed novel negative losses that account both for the deductive closure and different types of negatives"
  - [section] "we also sample negatives for other normal forms and add 'negative' losses for all other normal forms"
  - [corpus] "Weak" - no direct mention of normal form losses in neighbor papers
- Break condition: If the geometric representation becomes too complex to optimize effectively for all normal forms simultaneously.

## Foundational Learning

- Concept: Description Logic EL++
  - Why needed here: The entire method is designed specifically for EL++ ontologies, so understanding the syntax and semantics is fundamental
  - Quick check question: What are the seven normal forms in normalized EL++ and how do they differ from each other?
- Concept: Deductive closure
  - Why needed here: The method relies on computing and using the deductive closure to filter negatives and evaluate predictions
  - Quick check question: Why is the deductive closure finite for normalized EL++ but potentially infinite for non-normalized EL++?
- Concept: Geometric ontology embeddings
  - Why needed here: The method uses geometric representations (balls, boxes) to approximate models of EL++ theories
  - Quick check question: How does the geometric interpretation of GCI2 axioms (A ⊑ ∃r.B) differ from GCI0 axioms (A ⊑ B) in the embedding space?

## Architecture Onboarding

- Component map: EL++ reasoner (ELK) -> Deductive closure computation -> Embedding model (ELEmbeddings, ELBE, or Box2EL) -> Negative sampling with filtering -> Loss function module with all normal form losses -> Training loop with early stopping
- Critical path: Compute deductive closure → Train model with filtered negatives → Evaluate with deductive closure-aware metrics
- Design tradeoffs:
  - Compute deductive closure upfront vs. on-demand during training
  - Exact deductive closure vs. approximate inference rules
  - Full negative sampling vs. targeted sampling for specific axiom types
- Failure signatures:
  - Model performance doesn't improve with deductive closure filtering → Check if closure was computed correctly
  - Training becomes too slow → Check if too many negatives are being filtered out
  - Model overfits to entailed axioms → Check negative sampling ratio and loss balance
- First 3 experiments:
  1. Run the base embedding method without any modifications on a small EL++ ontology and record baseline metrics
  2. Add all negative losses but keep random negative sampling, compare performance to baseline
  3. Add deductive closure filtering to the random negatives from experiment 2, compare performance improvements

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Performance gains depend critically on correctness and efficiency of deductive closure computation, which becomes computationally challenging for large ontologies
- Reliance on specific EL++ reasoners limits generalizability to other description logics
- Lack of ablation studies for individual components (negative losses vs. closure filtering) prevents understanding which modifications drive the most benefit

## Confidence
- Mechanism 1 (Negative sampling bias): Medium - supported by theoretical reasoning but lacks empirical validation of sampling distributions
- Mechanism 2 (Deductive closure filtering): High - directly tested and shown to improve performance
- Mechanism 3 (All normal form losses): Low - no ablation study isolates the impact of individual normal form losses

## Next Checks
1. Conduct an ablation study removing deductive closure filtering to quantify its isolated contribution to performance gains
2. Test the method on a larger EL++ ontology (e.g., SNOMED CT subset) to evaluate scalability of closure computation
3. Compare sampling distributions with and without closure filtering to empirically verify the negative sampling bias claim