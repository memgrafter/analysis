---
ver: rpa2
title: 'Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations'
arxiv_id: '2402.12038'
source_url: https://arxiv.org/abs/2402.12038
tags:
- self-amplify
- rationales
- language
- post
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Self-AMPLIFY proposes a method to automatically improve the performance
  of small language models (SLMs) by generating rationales from post-hoc explanation
  methods applied to the model itself, without requiring human annotation or auxiliary
  models. The method consists of three steps: selecting promising samples based on
  the model''s predictions, generating rationales using post-hoc explanation methods
  (DeepLift, KernelShap, Selftopk, and Ph-CoT), and building a final prompt for in-context
  learning.'
---

# Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations

## Quick Facts
- arXiv ID: 2402.12038
- Source URL: https://arxiv.org/abs/2402.12038
- Reference count: 21
- Primary result: Self-AMPLIFY achieves strong accuracy improvements on SLMs using self-generated rationales without human annotation or auxiliary models

## Executive Summary
Self-AMPLIFY introduces a novel approach to automatically improve small language models by generating rationales through post-hoc explanation methods applied directly to the model itself. The method employs three key steps: selecting promising samples based on the model's own predictions, generating natural language rationales using various post-hoc explanation techniques (DeepLift, KernelShap, Self_topk, and Ph-CoT), and constructing an enriched prompt for in-context learning. Evaluated across four SLMs and five reasoning-intensive datasets, the approach demonstrates significant performance gains compared to baseline methods while maintaining flexibility and autonomy by eliminating the need for human annotation or auxiliary models.

## Method Summary
Self-AMPLIFY operates through a three-step framework that enhances small language models using their own explanations. First, it selects promising training samples based on the model's predictions using either success (correctly predicted) or error (misclassified) strategies. Second, it generates rationales using four post-hoc explanation methods: DeepLift and KernelShap for attribution-based explanations, Self_topk for top-k token selection, and Ph-CoT for free-text chain-of-thought rationales. Finally, it constructs an enriched in-context learning prompt by inserting the generated rationales between questions and answers. The method is evaluated on four small language models (Mistral-7B, Zephyr-7B, Gemma-7B, and Gemma-2B) across five reasoning-intensive datasets, comparing against baselines including standard input-output prompting, Auto-CoT, and AMPLIFY.

## Key Results
- Ph-CoT rationales yield the best average performance improvements across all evaluated SLMs and datasets
- Success selection strategy performs as well as error selection strategy on average
- Self-AMPLIFY achieves strong accuracy improvements compared to IO, Auto-CoT, and AMPLIFY baselines
- The approach works effectively across different model sizes from 2B to 7B parameters
- Post-hoc explanation methods can be directly applied to SLMs without auxiliary models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-hoc explanation methods can be directly applied to SLMs to generate rationales for self-improvement without requiring auxiliary models or human annotation.
- Mechanism: The framework uses the SLM's own predictions to select promising samples and applies post-hoc explanation methods (DeepLift, KernelShap, Self_topk, Ph-CoT) directly to the model to generate natural language rationales that enrich the prompt for in-context learning.
- Core assumption: Post-hoc explanation methods that work for large models can be computationally applied to smaller SLMs and generate meaningful rationales.
- Evidence anchors:
  - [abstract] "Self-AMPLIFY proposes a method to automatically improve the performance of small language models (SLMs) by generating rationales from post-hoc explanation methods applied to the model itself, without requiring human annotation or auxiliary models."
  - [section 3.3] "DeepLift decomposes the neural network prediction by backpropagating the contributions of all neurons in the network to each input feature" and "KernelSHAP samples instances in the neighborhood of x to approximate Shapley Values."
  - [corpus] Found 25 related papers but none directly validate this specific self-explanation mechanism for SLMs. Weak corpus evidence.
- Break condition: If the computational cost of post-hoc methods becomes prohibitive for even smaller SLMs, or if the generated rationales fail to meaningfully improve model performance.

### Mechanism 2
- Claim: Different types of post-hoc explanations (attribution methods, self_topk, Ph-CoT) can be used interchangeably in the Self-AMPLIFY framework, with Ph-CoT rationales yielding the best average results.
- Mechanism: The framework implements three types of post-hoc explanations: post-hoc attributions (DeepLift, KernelShap), self_topk explanations, and Ph-CoT rationales, allowing flexibility in rationale generation based on available model information and desired faithfulness.
- Core assumption: Different post-hoc explanation methods can generate equally valid rationales for self-improvement, with trade-offs between computational cost and faithfulness.
- Evidence anchors:
  - [abstract] "three types of post hoc explanations methods are implemented: post hoc attributions, self topk explanations and self free text rationales"
  - [section 3.3] "Self-AMPLIFY implements 3 types of post hoc explanations to generate natural language rationale: post hoc attributions (DeepLift and KernelShAP), post hoc Self_topk explanations and post hoc CoT (Ph-CoT) rationales"
  - [section 4.2] "Table 1 shows that Ph-CoT post hoc explanations give in average the best Self-AMPLIFY results as compared to DeepLift"
  - [corpus] Weak evidence - only mentions related work on self-explanations but no direct validation of this mechanism.
- Break condition: If one explanation type consistently underperforms across datasets, or if certain explanation types become computationally infeasible for specific model sizes.

### Mechanism 3
- Claim: The success selection strategy (selecting correctly predicted instances) performs as well as the error strategy (selecting misclassified instances) in improving SLM performance through rationale generation.
- Mechanism: Self-AMPLIFY employs two sample selection strategies - success (selecting instances the model predicts correctly) and error (selecting misclassified instances) - both of which can effectively improve model performance when rationales are generated and added to the prompt.
- Core assumption: Adding correctly classified examples with rationales can be as beneficial as adding misclassified examples for preventing future errors.
- Evidence anchors:
  - [abstract] "Self-AMPLIFY employs two simple yet efficient selecting strategies only based solely on f prediction, eliminating the need of an auxiliary model"
  - [section 3.2] "The success strategy relies on the idea that 'the higher the prediction certainty, the more relevant the explanation' (Bhan et al., 2023a). Conversely, the error strategy relies on the idea that adding misclassified examples may avoid similar misclassifications on the test set."
  - [section 4.2] "Table 1 highlights that the success selection strategy of Self-AMPLIFY gives good results overall, doing on average as well as the error one"
  - [corpus] No direct corpus evidence supporting this specific selection strategy mechanism.
- Break condition: If one selection strategy consistently outperforms the other across all datasets, or if neither strategy improves performance over baseline prompting.

## Foundational Learning

- Concept: Post-hoc explanation methods (attribution methods, LIME, SHAP, etc.)
  - Why needed here: These methods are used to generate rationales from the SLM itself without requiring human annotation or auxiliary models.
  - Quick check question: What is the difference between perturbation-based and gradient-based attribution methods, and why might gradient-based methods be preferred for SLMs?

- Concept: In-context learning (ICL) and prompting strategies
  - Why needed here: Self-AMPLIFY builds on ICL by enriching prompts with rationales to improve SLM performance on reasoning tasks.
  - Quick check question: How does adding rationales to an ICL prompt differ from standard input-output prompting, and what benefits might this provide?

- Concept: Selection strategies for sample inclusion in prompts
  - Why needed here: Self-AMPLIFY uses success and error selection strategies to choose which samples to include in the final prompt based on the SLM's own predictions.
  - Quick check question: Why might selecting correctly predicted instances (success strategy) be as effective as selecting misclassified instances (error strategy) for improving model performance?

## Architecture Onboarding

- Component map: n-shot Sample Selection → Rationale Generation → Prompt Design → Inference
- Critical path: Sample Selection → Rationale Generation → Prompt Design → Inference
- Design tradeoffs:
  - Post-hoc methods: DeepLift (requires model access, gradient-based) vs. KernelShap (perturbation-based, expensive) vs. Self_topk (only text generation) vs. Ph-CoT (free text, potentially less faithful)
  - Selection strategies: Success (correctly predicted) vs. Error (misclassified) - both effective but may have different failure patterns
  - Rationale format: Attribution-based (keyword lists) vs. free text (Ph-CoT) - trade-off between faithfulness and interpretability
- Failure signatures:
  - Rationale generation fails: Generated rationales are nonsensical or irrelevant to the input
  - Sample selection ineffective: Including samples doesn't improve or worsens performance
  - Prompt design issues: Overly long prompts or confusing rationale formats lead to poor inference
  - Computational constraints: Post-hoc methods become too expensive for smaller SLMs
- First 3 experiments:
  1. Run Self-AMPLIFY with DeepLift on a simple dataset (like ARC Challenge) using the success selection strategy to verify basic functionality
  2. Compare DeepLift vs. Ph-CoT rationales on the same dataset to observe performance differences
  3. Test both success and error selection strategies to confirm they perform similarly as claimed

## Open Questions the Paper Calls Out
- The impact of rationale faithfulness on Self-AMPLIFY's performance across different datasets and model sizes
- How the computational cost of different post hoc explanation methods affects the practical applicability of Self-AMPLIFY for real-world use cases
- The relationship between task complexity, selection strategy, and Self-AMPLIFY performance
- How the length of generated rationales (number of keywords or steps) affects the performance of Self-AMPLIFY

## Limitations
- Computational cost of DeepLift and KernelShap may become prohibitive for larger contexts or smaller SLMs
- Exact prompt formats for Ph-CoT and Self_topk rationale generation are not fully specified
- Limited evaluation to specific model sizes (7B and 2B parameters) and five reasoning datasets
- No systematic analysis of rationale faithfulness or how it impacts performance

## Confidence
- High confidence in the core finding that SLMs can generate self-explanations for performance improvement
- Medium confidence in the claim that different post-hoc methods yield comparable results, as Ph-CoT showed the best average performance but comparisons across all methods were limited
- Medium confidence in the selection strategy finding, as the success and error strategies performed similarly overall but may have dataset-specific variations

## Next Checks
1. Test Self-AMPLIFY on additional reasoning datasets beyond the five used (ARC Challenge, CommonsenseQA, Social IQa, Snarks, Causal Judgment) to assess generalizability across different reasoning domains
2. Evaluate the computational cost and runtime of DeepLift and KernelShap across different context sizes to determine scalability limits for smaller SLMs
3. Conduct ablation studies removing rationales from prompts to quantify their specific contribution to performance improvements versus the baseline ICL approach