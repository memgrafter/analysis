---
ver: rpa2
title: 'EthioLLM: Multilingual Large Language Models for Ethiopian Languages with
  Task Evaluation'
arxiv_id: '2403.13737'
source_url: https://arxiv.org/abs/2403.13737
tags:
- languages
- language
- dataset
- amharic
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the lack of large language models for Ethiopian
  languages, which exhibit remarkable linguistic diversity and cultural significance.
  The authors introduce EthioLLM, a multilingual large language model covering five
  Ethiopian languages (Amharic, Ge'ez, Afan Oromo, Somali, and Tigrinya) and English.
---

# EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation

## Quick Facts
- arXiv ID: 2403.13737
- Source URL: https://arxiv.org/abs/2403.13737
- Reference count: 0
- The paper introduces EthioLLM, multilingual language models for five Ethiopian languages (Amharic, Ge'ez, Afan Oromo, Somali, Tigrinya) plus English, achieving competitive performance on multiple NLP tasks.

## Executive Summary
This paper addresses the lack of large language models for Ethiopian languages by introducing EthioLLM, a family of multilingual models covering five Ethiopian languages and English. The authors create Ethiobenchmark, a new benchmark dataset for various downstream NLP tasks including news classification, machine translation, hate speech detection, named entity recognition, part-of-speech tagging, sentiment analysis, and question answering. The models demonstrate competitive performance compared to existing Afro-centric models and show promising zero-shot capabilities on unseen languages like Ge'ez.

## Method Summary
The authors collected monolingual datasets from diverse sources including news media, social media, Bible texts, and textbooks for five Ethiopian languages and English. They trained two tokenizers with different vocabulary sizes (70K for small models, 250K for base and large models) and used XLMR and mT5 architectures to create encoder-only (EthioLLM-small, EthioLLM-base, EthioLLM-large) and encoder-decoder (EthioMT5-small) models. The models were fine-tuned on downstream tasks and evaluated using existing benchmarks (MasakhaNEWS, MasakhaNER, AfriSenti) and newly created Ethiobenchmark datasets, with weighted F1-score for classification tasks and sacreBLEU for machine translation.

## Key Results
- EthioLLM models show competitive performance compared to state-of-the-art Afro-centric models across multiple tasks and languages
- Encoder-only models outperform encoder-decoder models on classification tasks
- Models demonstrate promising zero-shot performance on Ge'ez, a language not included in training
- Larger tokenizer vocabulary (250K) was used for base and large models to better handle morphologically rich Ethiopian languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual pretraining on combined Ethiopian language data improves cross-lingual transfer to low-resource languages.
- Mechanism: Training on multiple Ethiopian languages plus English exposes the model to shared linguistic features and scripts, enabling zero-shot performance on unseen languages like Ge'ez.
- Core assumption: The linguistic diversity of Ethiopian languages provides sufficient cross-lingual signal to bootstrap performance in zero-shot settings.
- Evidence anchors:
  - [abstract]: "Ethiopian languages exhibit remarkable linguistic diversity, encompassing a wide array of scripts..."
  - [section 3.1]: "We focused on training our language models with clean data and conducted further preprocessing and cleaning... Table 1 shows the selected languages and monolingual dataset used for LMs training."
  - [corpus]: Weak evidence. The corpus section lists available data but does not quantify cross-linguistic overlap or shared features.
- Break condition: If the model fails to show zero-shot performance on Ge'ez despite multilingual training, the assumption of sufficient cross-lingual signal is invalid.

### Mechanism 2
- Claim: Using a larger tokenizer vocabulary (250K) for base and large models improves performance on morphologically rich Ethiopian languages.
- Mechanism: A larger tokenizer captures more fine-grained subword units, which is critical for languages with complex morphology like Amharic and Ge'ez.
- Core assumption: Morphological richness in Ethiopian languages requires finer tokenization granularity to represent linguistic units effectively.
- Evidence anchors:
  - [section 3.2.1]: "We trained two new tokenizers, one with a 70K vocabulary size and the other one with a 250K vocabulary size. We used a tokenizer with a 70K vocabulary size to train EthioLLM-small and the other one for EthioLLM-base and EthioLLM-large."
  - [section 5.1]: "EthioLLM-base showed better performance for Amharic and Afan Oromo languages..."
  - [corpus]: Weak evidence. No direct evidence provided on the impact of tokenizer size on morphological accuracy.
- Break condition: If smaller tokenizer models (70K) match or outperform larger ones on morphologically complex tasks, the assumption is invalid.

### Mechanism 3
- Claim: Encoder-only architectures (XLMR, mT5) are more effective than encoder-decoder for classification tasks in Ethiopian languages.
- Mechanism: Encoder-only models capture richer contextual embeddings for classification, while encoder-decoder models are optimized for generation tasks.
- Core assumption: Classification tasks benefit more from deep contextual representations than from sequence-to-sequence architectures.
- Evidence anchors:
  - [section 5.1]: "Our encoder-only models (EthioLLM-small, EthioLLM-base, and EthioLLM-large) demonstrate competitive performance compared to the Afro-centric models... Seq2seq models (AfriTeVa-base and AfriMT5-base) performed less than all encoder-only models across all languages."
  - [section 5.2]: "For Amharic, XLMR-large, AfroLM-large, and EthioLLM-large exhibited similar results... For sequence-to-sequence models, Amharic results show the EthioMT5-small model outperformed the AfriMT5-base..."
  - [corpus]: No corpus evidence available for this mechanism.
- Break condition: If encoder-decoder models match or exceed encoder-only performance on classification tasks, the assumption is invalid.

## Foundational Learning

- Concept: Multilingual pretraining and fine-tuning
  - Why needed here: Ethiopian languages are low-resource, so pretraining on multiple languages provides shared linguistic knowledge and improves downstream task performance.
  - Quick check question: What is the key difference between pretraining and fine-tuning in the context of multilingual models?

- Concept: Zero-shot learning
  - Why needed here: The paper evaluates models on Ge'ez, a language not seen during training, requiring the model to generalize from other Ethiopian languages.
  - Quick check question: How does zero-shot performance indicate the model's ability to transfer knowledge across languages?

- Concept: Tokenization strategies for morphologically rich languages
  - Why needed here: Ethiopian languages like Amharic and Ge'ez have complex morphology, requiring careful tokenization to capture linguistic units effectively.
  - Quick check question: Why might a larger tokenizer vocabulary (250K) be beneficial for morphologically rich languages?

## Architecture Onboarding

- Component map:
  - Data pipeline: Web scraping (news, social media) -> manual corpus curation -> preprocessing (cleaning, normalization)
  - Model architecture: XLMR/mT5 encoder-only and encoder-decoder variants (small, base, large)
  - Tokenization: Two tokenizers (70K and 250K vocabulary)
  - Evaluation: MasakhaNEWS, MasakhaNER, AfriSenti, and custom Ethiobenchmark datasets

- Critical path:
  1. Collect and clean multilingual Ethiopian language data
  2. Train tokenizer and language model (XLMR/mT5)
  3. Fine-tune on downstream tasks (news classification, NER, etc.)
  4. Evaluate on benchmark datasets and zero-shot tasks

- Design tradeoffs:
  - Tokenizer size vs. model efficiency: Larger tokenizers capture more linguistic nuance but increase memory usage
  - Encoder-only vs. encoder-decoder: Encoder-only better for classification, encoder-decoder for generation
  - Pretraining data size vs. model performance: More data improves generalization but increases training cost

- Failure signatures:
  - Poor zero-shot performance on Ge'ez: Indicates insufficient cross-lingual transfer
  - Overfitting on downstream tasks: Suggests data leakage or insufficient regularization
  - High variance across tasks: May indicate inconsistent tokenization or data quality issues

- First 3 experiments:
  1. Evaluate zero-shot performance on Ge'ez using multilingual encoder-only models
  2. Compare encoder-only vs. encoder-decoder performance on classification tasks
  3. Test impact of tokenizer size (70K vs. 250K) on morphologically rich language tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do EthioLLM models perform on languages not included in the training corpus, such as Ge'ez?
- Basis in paper: [explicit] The paper mentions that the models show promising results in zero-shot evaluation for Ge'ez, suggesting potential for low-resource languages.
- Why unresolved: The evaluation on Ge'ez is limited, and the performance on other low-resource languages not included in the training data is unknown.
- What evidence would resolve it: Testing the models on a diverse set of low-resource languages and comparing their performance to languages included in the training corpus.

### Open Question 2
- Question: What is the impact of tokenizer vocabulary size on the performance of EthioLLM models?
- Basis in paper: [explicit] The paper mentions that different tokenizer sizes were tested, with 70K used for small models and 250K for base and large models.
- Why unresolved: The paper does not provide a detailed analysis of how different tokenizer sizes affect model performance across various tasks.
- What evidence would resolve it: Conducting experiments with different tokenizer sizes and evaluating their impact on model performance for each downstream task.

### Open Question 3
- Question: How do the encoder-decoder models (EthioMT5) perform compared to encoder-only models on sequence-to-sequence tasks other than machine translation?
- Basis in paper: [explicit] The paper states that encoder-decoder models were only tested on machine translation tasks.
- Why unresolved: The performance of encoder-decoder models on other sequence-to-sequence tasks, such as text summarization or dialogue generation, is unknown.
- What evidence would resolve it: Training and evaluating the encoder-decoder models on a variety of sequence-to-sequence tasks and comparing their performance to encoder-only models.

## Limitations

- Evaluation scope for zero-shot generalization is limited, with Ge'ez only tested on news classification tasks
- Insufficient detail on data quality controls for social media content across languages
- Limited comparative analysis with Afro-centric models, lacking breakdown of performance differences
- No systematic validation of the claimed benefits of larger tokenizers for morphologically rich languages

## Confidence

**High Confidence**: Claims about EthioLLM's competitive performance compared to Afro-centric models across multiple tasks are well-supported by the presented results in Tables 2-5, showing consistent F1-score and BLEU score improvements.

**Medium Confidence**: The assertion that multilingual pretraining enables effective zero-shot learning on Ge'ez is supported but limited by narrow task evaluation. The paper demonstrates this only for news classification.

**Low Confidence**: The mechanism claiming that 250K tokenizers specifically improve performance on morphologically rich languages lacks direct empirical validation. The paper shows different models used different tokenizers but doesn't systematically compare their impact on morphological tasks.

## Next Checks

1. **Zero-shot Task Expansion**: Evaluate EthioLLM's zero-shot performance on Ge'ez across all task types (NER, sentiment analysis, POS tagging) not just news classification to verify generalization claims.

2. **Tokenizer Impact Study**: Conduct controlled experiments comparing 70K vs 250K tokenizers on morphologically complex tasks for Amharic and Ge'ez to validate the claimed benefit of larger tokenizers.

3. **Cross-lingual Transfer Analysis**: Perform ablation studies by training models on individual Ethiopian languages versus the multilingual corpus to quantify the actual contribution of cross-lingual transfer to overall performance.