---
ver: rpa2
title: Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models
arxiv_id: '2403.01754'
source_url: https://arxiv.org/abs/2403.01754
tags:
- methods
- language
- optimization
- low-rank
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a gradient-free optimization approach for
  parameter-efficient tuning in large language models. The method prepends low-rank
  modules to each self-attention layer and employs two derivative-free optimization
  methods (CMA-ES and Fireworks Algorithm) to optimize these modules alternately.
---

# Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models

## Quick Facts
- **arXiv ID**: 2403.01754
- **Source URL**: https://arxiv.org/abs/2403.01754
- **Authors**: Feihu Jin; Yin Liu; Ying Tan
- **Reference count**: 18
- **Primary result**: Outperforms LoRA by 3.47 average points across 7 NLU datasets while using fewer parameters

## Executive Summary
This paper introduces a gradient-free optimization approach for parameter-efficient tuning in large language models. The method prepends low-rank modules to each self-attention layer and employs two derivative-free optimization methods (CMA-ES and Fireworks Algorithm) to optimize these modules alternately. Experiments on RoBERTa-large, GPT2-large, and GPT2-XL models demonstrate that the proposed method achieves substantial improvements over existing gradient-based and gradient-free methods in few-shot settings, with advantages in memory usage and convergence speed.

## Method Summary
The proposed method optimizes low-rank modules prepended to self-attention layers using derivative-free optimization (DFO) techniques. Specifically, it employs CMA-ES and Fireworks Algorithm alternately at each layer to optimize the low-rank matrices. The approach introduces significantly fewer parameters compared to full fine-tuning while achieving better performance than existing parameter-efficient methods like LoRA, particularly in few-shot learning scenarios.

## Key Results
- Achieves 3.47 average point improvement over LoRA across seven NLU datasets
- Outperforms existing gradient-based and gradient-free methods in few-shot settings
- Demonstrates better memory usage and convergence speed compared to existing DFO methods on larger models

## Why This Works (Mechanism)

### Mechanism 1
Derivative-free optimization methods like CMA-ES and Fireworks Algorithm can effectively optimize low-rank modules in large language models without requiring gradient computation. The method prepends low-rank modules to each self-attention layer and optimizes these modules alternately using DFO techniques, achieving substantial improvements over existing gradient-based and gradient-free methods.

## Foundational Learning

1. **Derivative-Free Optimization (DFO)**: Optimization methods that don't require gradient computation, crucial for handling non-differentiable objectives or when gradients are expensive to compute.
   - Why needed: Enables optimization of low-rank modules without backpropagation
   - Quick check: Verify understanding of CMA-ES and Fireworks Algorithm basics

2. **Low-Rank Adaptation (LoRA)**: Parameter-efficient fine-tuning method that injects low-rank matrices into transformer layers.
   - Why needed: Forms the baseline and comparison method for the proposed approach
   - Quick check: Confirm understanding of how LoRA modifies self-attention weights

3. **Self-Attention Mechanism**: Core component of transformer models where query, key, and value matrices process input representations.
   - Why needed: The proposed method specifically modifies the self-attention layers
   - Quick check: Verify understanding of how Q, K, and V matrices interact in attention

4. **CMA-ES (Covariance Matrix Adaptation Evolution Strategy)**: Evolutionary algorithm for non-linear non-convex optimization.
   - Why needed: One of the two DFO methods used to optimize low-rank modules
   - Quick check: Understand basic principles of evolutionary optimization

5. **Fireworks Algorithm**: Nature-inspired meta-heuristic optimization algorithm based on explosion dynamics.
   - Why needed: The second DFO method used in alternation with CMA-ES
   - Quick check: Know how the algorithm uses explosion and mutation operators

## Architecture Onboarding

### Component Map
Pre-trained LLM -> Self-Attention Layers -> Low-Rank Modules (A,B) -> DFO Optimizer (CMA-ES/FWA) -> Updated Parameters

### Critical Path
Input text -> Embedding -> Self-attention with modified Q,K weights -> Feed-forward -> Output prediction

### Design Tradeoffs
- **Fewer parameters vs. performance**: The method introduces significantly fewer parameters than full fine-tuning while achieving competitive results
- **Optimization complexity**: DFO methods are computationally expensive but avoid gradient computation
- **Layer-wise alternation**: Alternating between CMA-ES and FWA at each layer balances exploration and exploitation

### Failure Signatures
- Convergence issues on larger models (observed with CMA-ES on GPT2-XL)
- Performance degradation when initializing projection matrices randomly instead of using hidden state distributions
- Suboptimal results when optimizing K and V matrices instead of Q and K matrices

### First Experiments
1. Implement low-rank modules in self-attention layers and verify integration with pre-trained models
2. Test initialization strategies for projection matrices using hidden state distributions
3. Evaluate performance on a single NLU dataset (e.g., SST-2) to validate the basic approach

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of initialization strategy for the projection matrices (G) impact the performance and convergence speed of the proposed method?
- Basis in paper: The paper compares two initialization methods: random initialization with a normal distribution and initialization using the distribution of hidden states at each layer of the language model.
- Why unresolved: While the paper shows that initializing with the hidden states distribution leads to better performance, it does not explore the full spectrum of possible initialization strategies or analyze the underlying reasons for the observed difference.

### Open Question 2
How does the proposed method generalize to other NLP tasks beyond language understanding, such as text generation or summarization?
- Basis in paper: The paper primarily focuses on evaluating the method on language understanding tasks, and mentions that further exploration is needed for generation tasks.
- Why unresolved: The paper does not provide any experimental results or analysis of the method's performance on generation tasks, leaving its effectiveness in these areas uncertain.

### Open Question 3
What is the theoretical explanation for the observed preference of gradient-free optimization on the Q and K weight matrices in self-attention, as opposed to the K and V matrices in traditional LoRA?
- Basis in paper: The paper mentions that simultaneous application of derivative-free optimization on WQ and WK matrices yields the best performance, which differs from the conventional LoRA approach.
- Why unresolved: The paper attributes this observation to experimental results and acknowledges the lack of a comprehensive theoretical analysis.

## Limitations

- Experimental evaluation focuses on few-shot settings with a limited number of datasets
- GPT2-XL results show convergence issues with CMA-ES, indicating potential scalability problems
- No comparison with full fine-tuning provided, making it difficult to assess effectiveness relative to traditional approaches

## Confidence

**High Confidence**: The proposed method achieves substantial improvements over existing gradient-based and gradient-free methods in few-shot settings, as demonstrated by the experimental results on seven NLU datasets. The method also exhibits advantages in memory usage and convergence speed compared to existing derivative-free optimization methods on larger models.

**Medium Confidence**: The effectiveness of the method in scaling to larger models and datasets is not thoroughly investigated. The paper does not provide a detailed analysis of the computational complexity and scalability of the proposed approach.

**Low Confidence**: The comparison with full fine-tuning is not provided, making it difficult to assess the method's effectiveness relative to traditional approaches. The paper does not discuss the potential limitations and challenges of applying the method to other types of models or tasks.

## Next Checks

1. **Scalability Assessment**: Evaluate the method's performance on larger models (e.g., GPT-3, GPT-4) and datasets to assess its scalability and potential limitations.

2. **Comparison with Full Fine-tuning**: Conduct experiments to compare the proposed method with full fine-tuning in terms of performance, memory usage, and computational efficiency.

3. **Generalization to Other Tasks**: Apply the method to other types of tasks (e.g., generation, summarization) and models to assess its generalization capabilities and potential applications.