---
ver: rpa2
title: 'Beyond Content Relevance: Evaluating Instruction Following in Retrieval Models'
arxiv_id: '2410.23841'
source_url: https://arxiv.org/abs/2410.23841
tags:
- retrieval
- instructions
- query
- arxiv
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces InfoSearch, a novel benchmark for evaluating
  instruction-following capabilities in retrieval models across six document-level
  attributes: Audience, Keyword, Format, Language, Length, and Source. The authors
  develop two new metrics - Strict Instruction Compliance Ratio (SICR) and Weighted
  Instruction Sensitivity Evaluation (WISE) - to assess models'' responsiveness to
  instructions.'
---

# Beyond Content Relevance: Evaluating Instruction Following in Retrieval Models

## Quick Facts
- arXiv ID: 2410.23841
- Source URL: https://arxiv.org/abs/2410.23841
- Reference count: 32
- Top model: GPT-4o achieves highest instruction-following scores across all evaluated dimensions

## Executive Summary
This paper introduces InfoSearch, a benchmark designed to evaluate instruction-following capabilities in retrieval models beyond traditional content relevance metrics. The authors develop two new metrics - SICR and WISE - to assess models' responsiveness to six document-level attributes: Audience, Keyword, Format, Language, Length, and Source. Their comprehensive evaluation of 15 retrieval and reranking models reveals that instruction fine-tuning and larger model sizes improve performance, but most models still struggle with comprehensive instruction compliance. The study demonstrates that traditional IR metrics like nDCG are insufficient for capturing instruction-following abilities and highlights specific challenges with format and audience-specific instructions.

## Method Summary
The authors constructed the InfoSearch benchmark by annotating 1,000 documents from the MS MARCO dataset with six instruction-following attributes. They developed two novel evaluation metrics: Strict Instruction Compliance Ratio (SICR) measures exact compliance with instructions, while Weighted Instruction Sensitivity Evaluation (WISE) captures nuanced instruction-following performance. The evaluation framework tests models' ability to retrieve documents matching specific instructions across the six attributes, comparing retrieval models, reranking models, and large language models. The study uses both traditional IR metrics (nDCG, R-precision) and the newly proposed metrics to provide a comprehensive assessment of instruction-following capabilities.

## Key Results
- Reranking models outperform retrieval models in instruction-following tasks across all six attributes
- GPT-4o achieves the highest scores on both SICR and WISE metrics for all evaluated dimensions
- Traditional IR metrics (nDCG) show poor correlation with instruction-following performance
- Format and Audience attributes remain particularly challenging for most models despite instruction fine-tuning

## Why This Works (Mechanism)
The paper's approach works by explicitly evaluating models' ability to follow user instructions beyond content relevance. Traditional retrieval models optimize for semantic similarity and relevance scoring, but instruction-following requires understanding and executing specific document attributes. The InfoSearch benchmark creates a controlled environment where models must demonstrate both content understanding and instruction compliance simultaneously. The two proposed metrics capture different aspects of instruction-following: SICR measures strict adherence while WISE evaluates sensitivity to instruction variations, providing a more complete picture of model capabilities.

## Foundational Learning

**Document attribute annotation** - Understanding how human annotators label documents with audience, format, language, etc. is crucial for interpreting the benchmark's reliability and generalizability.

**Instruction-following evaluation** - The concept of measuring compliance beyond relevance requires understanding how instructions map to document attributes and how to quantify successful execution.

**Retrieval vs reranking architectures** - Knowledge of how these two retrieval approaches differ in processing documents and applying ranking functions is essential for interpreting performance differences.

**Traditional IR metrics vs instruction metrics** - Understanding the limitations of nDCG and R-precision in capturing instruction-following behavior explains why new metrics were necessary.

**Model fine-tuning paradigms** - Familiarity with instruction fine-tuning techniques helps explain performance variations between different model variants.

**Quick checks**: Verify understanding of each attribute's definition, test simple instruction-following scenarios, compare metric calculations, and examine fine-tuning differences between model variants.

## Architecture Onboarding

**Component map**: User Query -> Retrieval Model/ Reranker -> Document Set -> Instruction Attribute Check -> Score (SICR/WISE/nDCG)

**Critical path**: The evaluation pipeline processes queries through retrieval models to produce ranked document lists, then checks each document against instruction attributes to calculate compliance scores.

**Design tradeoffs**: The benchmark balances comprehensiveness (6 attributes) against annotation feasibility, and uses existing MS MARCO documents rather than creating new ones from scratch.

**Failure signatures**: Models fail when they prioritize content relevance over instruction compliance, when instruction interpretation is ambiguous, or when attribute annotations are inconsistent.

**3 first experiments**: 1) Compare retrieval vs reranking performance on simple attribute instructions 2) Test correlation between nDCG and SICR scores across models 3) Evaluate instruction-following on synthetically generated queries with clear attribute specifications.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided.

## Limitations

The benchmark relies on human annotation which may introduce subjectivity and inconsistency in attribute labeling. The study focuses exclusively on English-language documents, limiting generalizability to multilingual scenarios. The six evaluated attributes may not capture the full spectrum of instruction-following scenarios users encounter in practice.

## Confidence

- High confidence that traditional IR metrics poorly correlate with instruction-following capabilities
- Medium confidence in relative performance rankings between retrieval vs reranking models
- Medium confidence in the effectiveness of instruction fine-tuning based on limited model comparisons
- Low confidence in absolute performance gaps due to potential annotation noise

## Next Checks

1. Conduct inter-annotator reliability tests on the InfoSearch dataset to quantify annotation consistency
2. Test the instruction-following evaluation methodology on non-English documents for multilingual assessment
3. Perform controlled experiments with baseline models (no instruction tuning) to isolate the effect of instruction fine-tuning