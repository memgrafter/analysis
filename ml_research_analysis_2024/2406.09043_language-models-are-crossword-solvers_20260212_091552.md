---
ver: rpa2
title: Language Models are Crossword Solvers
arxiv_id: '2406.09043'
source_url: https://arxiv.org/abs/2406.09043
tags:
- llms
- crossword
- clue
- crosswords
- clues
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models show significantly improved ability to solve\
  \ crossword clues compared to prior methods, achieving 2-3\xD7 higher accuracy on\
  \ cryptic crossword datasets without fine-tuning. The study introduces a search\
  \ algorithm that uses LLM-generated candidates with graph-based pruning, successfully\
  \ solving 93% of NYT crossword puzzles."
---

# Language Models are Crossword Solvers

## Quick Facts
- arXiv ID: 2406.09043
- Source URL: https://arxiv.org/abs/2406.09043
- Authors: Soumadeep Saha; Sutanoya Chakraborty; Saptarshi Saha; Utpal Garain
- Reference count: 40
- Large language models solve 93% of NYT crosswords and outperform prior methods on cryptic clues by 2-3× without fine-tuning

## Executive Summary
This paper demonstrates that modern large language models (LLMs) show remarkable competence at solving crossword puzzles, achieving state-of-the-art performance on both cryptic and standard crosswords. The authors introduce SweepClip, a graph-based pruning algorithm that leverages LLM-generated candidates with grid constraints to solve full crossword grids. The approach successfully solves 93% of New York Times crossword puzzles and achieves 2-3× higher accuracy on cryptic crossword benchmarks compared to previous methods. The study reveals that while LLMs excel at deciphering wordplay and synonyms in cryptic clues, they struggle with length constraints due to tokenization artifacts, particularly for less frequent words.

## Method Summary
The authors develop SweepClip, a search algorithm that generates candidate answers for all clues using LLMs, then applies graph-based pruning to eliminate conflicting answers based on grid constraints. The algorithm iteratively refines answers using partially filled grids as character constraints. They test multiple LLM models including GPT-4-Turbo, Claude 3, Llama 3, and others on three datasets: Cryptonite, Init, and NYT crosswords. The method employs few-shot prompting, chain-of-thought prompting with self-consistency, and human evaluation of reasoning soundness. The approach requires no fine-tuning and demonstrates strong generalization to post-knowledge-cutoff puzzles.

## Key Results
- LLMs achieve 2-3× higher accuracy on cryptic crossword benchmarks compared to previous state-of-the-art methods
- SweepClip algorithm solves 93% of NYT crossword puzzles using out-of-the-box LLMs
- Chain-of-thought prompting with self-consistency provides measurable performance gains
- Models generalize well to cryptic clues published after their knowledge cutoff
- Human evaluation shows 74% of correct answers include sound reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can use contextual constraints to improve crossword clue solving accuracy.
- Mechanism: The models generate candidate answers for all clues first, then use a graph-based pruning algorithm to eliminate conflicting answers based on grid constraints. Partially filled grids provide character constraints that guide the LLM to refine answers in subsequent iterations.
- Core assumption: LLMs can successfully integrate character-level constraints from partially filled crossword grids to narrow down answer possibilities.
- Evidence anchors:
  - [abstract] "We also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with out-of-the-box LLMs for the very first time, achieving an accuracy of 93% on New York Times crossword puzzles."
  - [section 3.3] "We observe that, in all but one case, LLMs show improved performance with an increasing percentage of constraint information for both datasets."
  - [corpus] Weak evidence; the corpus contains studies on cryptic crossword solving but lacks direct evidence of grid constraint exploitation.
- Break condition: The pruning algorithm becomes too restrictive and discards correct answers, or the LLM fails to generate viable candidates that satisfy grid constraints.

### Mechanism 2
- Claim: LLMs demonstrate significantly improved cryptic crossword clue deciphering abilities compared to previous state-of-the-art methods.
- Mechanism: Modern LLMs leverage their pre-trained knowledge to recognize wordplay patterns, synonyms, and world knowledge embedded in cryptic clues. Chain-of-thought prompting and self-consistency further improve performance by encouraging step-by-step reasoning.
- Core assumption: The pre-training data includes sufficient examples of wordplay, synonyms, and world knowledge to enable LLMs to recognize patterns in cryptic clues.
- Evidence anchors:
  - [abstract] "We demonstrate that the current generation of language models shows significant competence at deciphering cryptic crossword clues and outperforms previously reported state-of-the-art (SoTA) results by a factor of 2-3 in relevant benchmarks."
  - [section 5.2] "We observed that SoTA LLMs have significantly improved cryptic crossword clue deciphering abilities. We also note that chain-of-thought prompting with self-consistency leads to further performance gains."
  - [corpus] Moderate evidence; related papers show LLMs can solve cryptic crosswords but with lower accuracy than this study reports.
- Break condition: The cryptic clues involve wordplay patterns or world knowledge not represented in the LLM's training data, or the chain-of-thought prompting fails to elicit correct reasoning.

### Mechanism 3
- Claim: LLMs struggle with length constraints due to limitations in sub-token counting, which affects their crossword solving performance.
- Mechanism: LLMs rely on tokenization methods like Byte-Pair Encoding that lose character-level information during embedding. They learn to count characters from training data artifacts containing length information, but this ability degrades for less frequent words.
- Core assumption: LLMs rely on memorized training instances rather than a generalizable ability to count characters within words or phrases.
- Evidence anchors:
  - [section 3.4] "We observed that even the best performing model, GPT-4-Turbo, produces answers of incorrect length on 26.2% of the Init dataset... This may be explained by the tokenization methods used in LLMs."
  - [section 3.4] "We observe that the accuracy of LLMs at the sub-token counting task declines with the frequency of the token for all LLMs tested."
  - [corpus] Weak evidence; the corpus does not directly address sub-token counting limitations in LLMs.
- Break condition: The LLM develops a generalizable ability to count characters regardless of word frequency, or the crossword clues consistently require answers of very common words.

## Foundational Learning

- Concept: Constraint Satisfaction Problems (CSPs)
  - Why needed here: Crossword solving is fundamentally a CSP where answers must satisfy both semantic clues and grid constraints simultaneously.
  - Quick check question: Can you explain how the graph-based pruning algorithm in SweepClip relates to CSP solving techniques?

- Concept: Chain-of-Thought Prompting
  - Why needed here: Eliciting step-by-step reasoning helps LLMs break down complex cryptic clues that involve multiple layers of wordplay and deduction.
  - Quick check question: How does chain-of-thought prompting differ from direct answer generation in terms of model performance on complex reasoning tasks?

- Concept: Tokenization and Subword Units
  - Why needed here: Understanding how LLMs process text at the token level explains why they struggle with character-level constraints like word length.
  - Quick check question: What is the difference between subword tokenization and character-level processing, and how might this affect length constraint adherence?

## Architecture Onboarding

- Component map:
  - Input: Crossword clues and grid structure
  - LLM Engine: Generates candidate answers
  - Graph Processor: Builds conflict graph and prunes candidates
  - Constraint Handler: Manages character constraints from filled grid positions
  - Output: Completed crossword grid

- Critical path:
  1. Generate initial candidate answers for all clues
  2. Build conflict graph based on grid constraints
  3. Prune conflicting candidates using largest connected component
  4. Generate new candidates for neighboring vertices with updated constraints
  5. Repeat until grid is complete or budget exhausted

- Design tradeoffs:
  - Computational cost vs. thoroughness: The pruning algorithm sacrifices completeness for efficiency by not exploring all possible answer combinations.
  - Model capability vs. task complexity: Simple LLMs require more iterations and budget to solve crosswords compared to larger models.
  - Length constraint handling vs. semantic accuracy: Strict adherence to length constraints may force semantically reasonable but incorrect answers.

- Failure signatures:
  - High pruning rate with low convergence: Indicates the algorithm is too restrictive or the LLM generates poor initial candidates
  - Consistent length constraint violations: Suggests sub-token counting limitations are the primary bottleneck
  - Poor performance on cryptic clues: May indicate insufficient training data for wordplay patterns or world knowledge

- First 3 experiments:
  1. Test sub-token counting ability on vocabulary vs. gibberish words to confirm frequency-dependent limitations
  2. Evaluate LLM performance with varying percentages of character constraints to quantify improvement from grid information
  3. Compare chain-of-thought prompting vs. direct answer generation on cryptic clue accuracy to measure reasoning benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs develop a generalizable ability to count sub-tokens through targeted training interventions rather than relying on memorization of training artifacts?
- Basis in paper: [explicit] The paper demonstrates that LLMs' sub-token counting performance degrades with word frequency and differs between vocabulary words and gibberish, suggesting reliance on memorization rather than generalization.
- Why unresolved: The paper hypothesizes that LLMs learn sub-token counting from length information in training data artifacts but lacks conclusive evidence. Testing this hypothesis would require intervening at the pre-training stage, which is computationally prohibitive.
- What evidence would resolve it: A controlled study intervening at pre-training to systematically vary length information in training data, then measuring sub-token counting performance on frequency-stratified test sets.

### Open Question 2
- Question: Would a more thorough search strategy with increased computational budget significantly improve LLM performance on cryptic crosswords beyond the current 12% success rate with 50% letter accuracy?
- Basis in paper: [inferred] The paper's algorithm successfully solves 93% of straight crosswords but only 12% of cryptic crosswords with current computational constraints, suggesting cryptic crosswords require more extensive search.
- Why unresolved: The paper's financial constraints limited exploration of improved algorithms for cryptic crosswords, despite recognizing the need for more thorough search strategies.
- What evidence would resolve it: Experiments with increased computational budget (e.g., 10-100x current allocation) using more sophisticated search algorithms like Monte Carlo Tree Search or best-first search on cryptic crossword datasets.

### Open Question 3
- Question: How do different tokenization methods affect LLM performance on constrained language generation tasks like crossword solving?
- Basis in paper: [explicit] The paper notes that tokenization methods like Byte-Pair Encoding cause loss of character-level information during word embedding, potentially contributing to sub-token counting difficulties.
- Why unresolved: The paper does not systematically investigate how alternative tokenization approaches (e.g., character-level, sentence-piece, or custom tokenizers for constrained tasks) might improve LLM performance on length-constrained generation.
- What evidence would resolve it: Comparative experiments training or fine-tuning LLMs with different tokenization methods on crossword datasets, measuring improvements in length constraint adherence and overall solving accuracy.

## Limitations

- The paper demonstrates strong performance on standard datasets but provides limited evidence about generalization to more diverse crossword styles or less frequent vocabulary.
- Human evaluation of reasoning soundness relies on subjective assessment criteria that may vary across evaluators.
- Major uncertainties remain about the robustness of sub-token counting ability across different tokenization schemes and whether the pruning algorithm consistently converges to correct solutions without local optima traps.

## Confidence

- High confidence: The core finding that modern LLMs significantly outperform previous state-of-the-art methods on cryptic crossword clues by 2-3×, and that the graph-based pruning approach successfully solves 93% of NYT crosswords, is well-supported by quantitative results across multiple models and datasets.
- Medium confidence: The claim that chain-of-thought prompting and self-consistency provide measurable performance gains relies on limited comparisons and may vary significantly with prompt engineering quality. The analysis of length constraint violations and sub-token counting limitations shows consistent patterns but may not generalize to all tokenization methods.
- Low confidence: The reasoning soundness evaluation (74% of correct answers) lacks detailed methodology description and may be sensitive to evaluator bias or interpretation differences.

## Next Checks

1. Test the same models and prompts on crosswords from different publications (e.g., The Guardian, The Times) to assess cross-dataset generalization
2. Implement a more exhaustive search variant of the pruning algorithm to compare against the proposed efficiency-focused approach and measure completeness trade-offs
3. Conduct a controlled experiment varying only the tokenization scheme (e.g., character-level vs. subword) to isolate the impact of tokenization on length constraint adherence