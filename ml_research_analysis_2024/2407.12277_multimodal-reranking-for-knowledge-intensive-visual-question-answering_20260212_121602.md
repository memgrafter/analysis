---
ver: rpa2
title: Multimodal Reranking for Knowledge-Intensive Visual Question Answering
arxiv_id: '2407.12277'
source_url: https://arxiv.org/abs/2407.12277
tags:
- answer
- question
- reranking
- knowledge
- candidates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-modal reranking component for knowledge-intensive
  visual question answering (KI-VQA). The reranker improves the relevance scores of
  retrieved knowledge candidates by using multi-modal question and candidate information
  to perform cross-item interaction.
---

# Multimodal Reranking for Knowledge-Intensive Visual Question Answering

## Quick Facts
- arXiv ID: 2407.12277
- Source URL: https://arxiv.org/abs/2407.12277
- Reference count: 32
- Key outcome: Multimodal reranking consistently improves answer generation accuracy compared to models without reranking.

## Executive Summary
This paper introduces a multimodal reranking component for knowledge-intensive visual question answering (KI-VQA) that improves the relevance scores of retrieved knowledge candidates. The reranker uses multi-modal question and candidate information to perform cross-item interaction, trained with distant supervision on OK-VQA and A-OKVQA datasets using pairwise ranking loss. Experiments show consistent improvements in answer generation accuracy, and the paper identifies a training-testing discrepancy where performance improves when training candidates are noisier than testing candidates.

## Method Summary
The approach uses image-text retrieval (ALIGN dual encoder) to retrieve top-20 knowledge candidates per image patch, which are then aggregated by keeping the highest score per candidate. A multimodal reranker (PaLI-based transformer) performs cross-item interaction between question and candidate representations using cross-attention. The reranker is trained with distant supervision using answer token presence in candidate text as proxy relevance scores. The answer generator (also PaLI-based) takes top candidates (reranked at test time) and the question to generate answers. The generator is trained on original retrieval candidates to avoid the training-testing discrepancy.

## Key Results
- Multimodal reranking consistently improves answer generation accuracy over models without reranking
- Performance improves when training knowledge candidates are similar to or noisier than testing candidates
- Oracle reranker provides a promising performance upper bound, indicating room for future improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-item interaction between question and knowledge candidates improves relevance score modeling over uni-modal retrieval
- Mechanism: The reranker takes multi-modal question and knowledge candidate inputs, encodes them separately, then fuses via cross-attention to capture joint context
- Core assumption: The interaction between question and candidate modalities contains information missed by independent encoding
- Evidence anchors: Abstract mentions cross-item interaction for better relevance score modeling; Section 3.1 describes concatenating image token representations with text embeddings for Transformer fusion
- Break condition: If candidate and question share no meaningful visual or textual overlap, cross-attention adds little signal

### Mechanism 2
- Claim: Distant supervision using answer candidate presence in knowledge text provides effective training signal for reranking
- Mechanism: For each question-candidate pair, count answer candidates appearing in candidate text to form relevance score, then use pairwise ranking loss
- Core assumption: Presence of answer tokens in candidate text is a reliable proxy for candidate usefulness
- Evidence anchors: Section 3.2 describes counting answer candidates in knowledge text to obtain relevance scores; Section 4.2 confirms distant supervision provides consistent improvement
- Break condition: If candidate text is too generic or answer candidates are common words, relevance proxy becomes noisy

### Mechanism 3
- Claim: Training-testing discrepancy occurs because answer generation trained on lower-quality candidates generalizes better to noisy test retrieval than when trained on higher-quality reranked candidates
- Mechanism: When reranking is applied only at test time, model trained on noisy data learns to be robust; when reranking is applied at both train and test, model overfits to high-quality signal and fails on noisy retrieval
- Core assumption: Training on noisier data induces robustness to retrieval noise
- Evidence anchors: Abstract mentions training-testing discrepancy; Section 3.3 explains that direct reranking at both training and testing hurts performance
- Break condition: If test retrieval noise distribution differs significantly from training, robustness gains vanish

## Foundational Learning

- Concept: Cross-modal attention and fusion in transformers
  - Why needed here: Reranker must combine image and text from both question and candidate to capture joint context
  - Quick check question: Can you explain how cross-attention differs from self-attention in a transformer layer?

- Concept: Distant supervision and weak supervision paradigms
  - Why needed here: No ground-truth relevance scores exist; reranker trained using answer token presence as proxy
  - Quick check question: What are the risks of using noisy labels in ranking model training, and how can they be mitigated?

- Concept: Pairwise ranking loss and optimization
  - Why needed here: Reranker learns to order candidates by relevance using logistic loss over pairs
  - Quick check question: How does pairwise loss differ from listwise or pointwise ranking losses in terms of gradients?

## Architecture Onboarding

- Component map: Image-text retrieval (ALIGN) → Aggregator → Reranker (PaLI) → Answer Generator (PaLI)
- Critical path: Question → Retrieval → Reranker (test only) → Answer Generator → Answer
- Design tradeoffs:
  - Reranker trained separately from generator to avoid overfitting to reranked data
  - Only top-20 candidates retained to limit memory, trading recall for efficiency
  - Distant supervision chosen over manual labeling due to scale
- Failure signatures:
  - Low reranker Hits@k but generator performance unaffected → reranker may not capture what generator needs
  - Generator performance drops when reranking applied at both train/test → training/testing discrepancy
  - High variance in reranker scores → candidate-text supervision too noisy
- First 3 experiments:
  1. Run retrieval only → measure baseline VQA accuracy
  2. Apply reranker at test only → measure gain over baseline
  3. Apply reranker at both train and test → confirm performance drop (training/testing discrepancy)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the training-testing discrepancy in knowledge candidate quality impact model robustness and generalization across different VQA datasets?
- Basis in paper: Explicit - The paper identifies a training-testing discrepancy where performance improves when training knowledge candidates are similar to or noisier than those used in testing
- Why unresolved: The paper demonstrates the phenomenon but does not provide a comprehensive analysis of its underlying causes or explore methods to mitigate it
- What evidence would resolve it: Systematic experiments comparing model performance across various training-testing candidate quality scenarios, along with analyses of model behavior under different noise levels and distribution shifts

### Open Question 2
- Question: Can the multi-modal reranking approach be effectively extended to other vision-language tasks beyond knowledge-intensive VQA?
- Basis in paper: Inferred - The paper focuses on applying reranking to KI-VQA but acknowledges that it's an open question whether this approach can help other vision-language tasks
- Why unresolved: The paper only explores reranking in the context of KI-VQA and does not investigate its applicability to other tasks
- What evidence would resolve it: Experiments applying the multi-modal reranking approach to a variety of vision-language tasks (e.g., image captioning, visual reasoning) and comparing performance improvements to baseline models

### Open Question 3
- Question: What is the optimal trade-off between the number of knowledge candidates used for reasoning and the computational resources required for multi-modal reranking?
- Basis in paper: Inferred - The paper mentions that to reduce memory usage, they use a smaller number of knowledge candidates compared to previous text-based approaches
- Why unresolved: The paper does not systematically explore the impact of varying the number of knowledge candidates on performance and computational efficiency
- What evidence would resolve it: Ablation studies varying the number of knowledge candidates while measuring both performance and computational requirements (e.g., memory usage, inference time) to identify the optimal balance

## Limitations
- Distant supervision approach relies on a potentially weak signal that may not correlate well with true relevance, especially for complex questions requiring multi-hop reasoning
- Training-testing discrepancy finding is observational rather than mechanistic - the paper identifies the phenomenon but doesn't establish causation or explore underlying reasons
- Reranker's effectiveness measured primarily through downstream VQA accuracy rather than direct ranking metrics, making it difficult to assess whether improvements come from better ranking or other factors

## Confidence
- High: Cross-modal attention mechanism for fusing question and candidate representations is sound and well-supported by the architecture description
- Medium: Distant supervision using answer token presence is a reasonable proxy but may be noisy, particularly for common words or generic candidate text
- Low: The training-testing discrepancy mechanism is hypothesized but not empirically validated - the paper identifies the phenomenon but doesn't provide definitive evidence for why it occurs

## Next Checks
1. Validate Distant Supervision Quality: Manually annotate a subset of question-candidate pairs with true relevance scores and compare them against the distant supervision labels to quantify noise levels and identify failure patterns

2. Isolate Reranker Impact: Evaluate the reranker directly using ranking metrics (NDCG, MRR) on held-out data, separate from the answer generation pipeline, to determine if improvements in VQA accuracy are primarily due to better ranking or other factors

3. Test Training-Testing Discrepancy Mechanism: Design an experiment that systematically varies the quality gap between training and testing candidates (rather than just binary presence/absence of reranking) to better understand the relationship between training data quality and model robustness to retrieval noise