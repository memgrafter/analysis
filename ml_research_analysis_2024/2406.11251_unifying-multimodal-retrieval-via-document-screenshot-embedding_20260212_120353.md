---
ver: rpa2
title: Unifying Multimodal Retrieval via Document Screenshot Embedding
arxiv_id: '2406.11251'
source_url: https://arxiv.org/abs/2406.11251
tags:
- retrieval
- text
- document
- information
- screenshot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Document Screenshot Embedding (DSE), a novel
  retrieval paradigm that treats document screenshots as a unified input format for
  retrieval, bypassing traditional content extraction preprocessing and preserving
  all information (text, image, layout). DSE leverages a large vision-language model
  to directly encode document screenshots into dense representations for retrieval.
---

# Unifying Multimodal Retrieval via Document Screenshot Embedding

## Quick Facts
- arXiv ID: 2406.11251
- Source URL: https://arxiv.org/abs/2406.11251
- Authors: Xueguang Ma; Sheng-Chieh Lin; Minghan Li; Wenhu Chen; Jimmy Lin
- Reference count: 18
- DSE outperforms BM25 by 17 points in top-1 accuracy on Wikipedia webpage retrieval and surpasses OCR-based text retrieval methods by over 15 points in nDCG@10 on slide retrieval

## Executive Summary
This paper introduces Document Screenshot Embedding (DSE), a novel retrieval paradigm that treats document screenshots as a unified input format for retrieval. DSE leverages a large vision-language model to directly encode document screenshots into dense representations, bypassing traditional content extraction preprocessing. The approach preserves all information including text, images, and layout while avoiding information loss from OCR and layout parsing errors.

## Method Summary
DSE uses a bi-encoder architecture where a vision-language model (Phi-3-vision) encodes document screenshots into dense vectors, and a text encoder processes queries. The model employs fine-grained patch encoding by cropping screenshots into multiple sub-images and encoding them as separate patch sequences. Training uses contrastive learning with InfoNCE loss to distinguish relevant documents from irrelevant ones. The system stores document embeddings in a Faiss index for efficient retrieval based on cosine similarity.

## Key Results
- DSE outperforms BM25 by 17 points in top-1 accuracy on Wikipedia webpage retrieval
- DSE surpasses OCR-based text retrieval methods by over 15 points in nDCG@10 on slide retrieval
- Fine-grained patch encoding improves top-10 retrieval accuracy from 62.0% to 73.7% when increasing crops from 1×1 to 4×4

## Why This Works (Mechanism)

### Mechanism 1
Bypassing traditional content extraction avoids information loss from OCR and layout parsing errors. Document screenshots preserve all visual elements (text, images, layout) in their original form, which are directly encoded by the vision-language model without intermediate text extraction steps. Core assumption: Vision-language models can effectively encode visual information from screenshots into meaningful representations for retrieval. Break condition: When screenshots are of insufficient quality (blurry, low-resolution) that the vision-language model cannot extract meaningful information.

### Mechanism 2
Fine-grained patch encoding captures more detailed information from document screenshots. The model crops screenshots into multiple sub-images and encodes them as separate patch sequences, allowing the model to focus on more detailed visual elements. Core assumption: Increasing the number of patches improves retrieval effectiveness by capturing more fine-grained information. Break condition: When the computational cost of processing more patches outweighs the marginal gains in retrieval effectiveness.

### Mechanism 3
Unified multimodal encoding outperforms separate processing of text and image modalities. The vision-language model jointly encodes text and visual elements from the same screenshot, learning cross-modal relationships that separate processing cannot capture. Core assumption: The model can learn meaningful associations between visual layout elements and their semantic content. Break condition: When the document layout is too complex or noisy for the vision-language model to learn meaningful cross-modal associations.

## Foundational Learning

- Concept: Dense vector representations for documents
  - Why needed here: DSE encodes documents into dense vectors that can be compared using cosine similarity for retrieval
  - Quick check question: What is the primary similarity metric used in dense retrieval systems like DSE?

- Concept: Vision-language model architecture
  - Why needed here: DSE uses a vision encoder to process screenshots and a language model to generate document embeddings
  - Quick check question: How does the vision-language model in DSE handle the transition from image patches to text embeddings?

- Concept: Contrastive learning for retrieval
  - Why needed here: DSE is trained using InfoNCE loss to distinguish relevant documents from irrelevant ones
  - Quick check question: What is the role of hard negative samples in training DSE?

## Architecture Onboarding

- Component map: Vision encoder (Phi-3-vision) -> Patch sequences -> Language model -> Document embedding -> Index -> Query -> Similarity computation -> Retrieval
- Critical path: Document screenshot → Vision encoder → Patch sequences → Language model → Document embedding → Index → Query → Similarity computation → Retrieval
- Design tradeoffs:
  - Fine-grained vs efficient encoding: More sub-image crops capture more detail but increase computational cost
  - Vision-language model capacity: Larger models may capture more information but require more resources
  - Screenshot quality: Higher resolution captures more detail but increases processing time
- Failure signatures:
  - Poor retrieval performance: May indicate vision encoder cannot extract meaningful features from screenshots
  - Slow encoding speed: May indicate too many sub-image crops or inefficient model configuration
  - Inconsistent results: May indicate sensitivity to screenshot quality or document layout variations
- First 3 experiments:
  1. Compare DSE retrieval effectiveness with different numbers of sub-image crops (Cx, Cy)
  2. Evaluate DSE on documents with varying levels of visual complexity (text-heavy vs image-heavy)
  3. Test DSE retrieval effectiveness with varying screenshot resolutions/quality

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of DSE change when using different types of document formats (e.g., PDFs, HTML files with complex structures) beyond Wikipedia pages and slides? The paper evaluates DSE on Wikipedia pages and slides but acknowledges the need for further exploration on more general-purpose document retrieval tasks.

### Open Question 2
What is the impact of pretraining DSE using contrastive learning on its retrieval effectiveness, compared to the current supervised fine-tuning approach? The paper mentions that while it relies on supervised fine-tuning, contrastive pretraining has shown significant improvements in text retrieval effectiveness.

### Open Question 3
How does the quality of document screenshots (e.g., resolution, clarity) affect DSE's retrieval effectiveness, and what is the optimal balance between image quality and computational efficiency? The paper acknowledges limitations regarding the reliance on visual data quality and the trade-off between image quality and computational efficiency.

## Limitations

- Primary Wikipedia retrieval experiment compares DSE against BM25 rather than modern dense retrieval baselines, creating weaker performance claims
- Slide retrieval experiment lacks comparisons against strong contemporary dense retrieval methods
- No end-to-end retrieval evaluation demonstrating performance with automatically generated queries

## Confidence

**High Confidence**: The paper successfully demonstrates that document screenshot embedding is a viable retrieval approach that preserves visual information better than traditional content extraction methods. The architectural design and training methodology are clearly described and reproducible.

**Medium Confidence**: The claim that DSE outperforms OCR-based methods on slide retrieval appears valid based on the experimental evidence, though the comparison lacks modern dense retrieval baselines for context.

**Low Confidence**: The paper's strongest claim - that DSE unifies multimodal retrieval and significantly outperforms existing methods - is not well-supported. The Wikipedia experiment uses an inappropriate baseline (BM25), and the lack of end-to-end evaluation and comparisons with modern dense retrieval methods undermines the validity of these claims.

## Next Checks

1. Re-run the Wikipedia screenshot retrieval experiments comparing DSE against modern dense retrieval baselines (DPR, ColBERT, or similar) to establish whether the 17-point improvement over BM25 translates to meaningful gains against state-of-the-art methods.

2. Implement a complete retrieval pipeline where NQ-style questions are automatically converted to queries and used to retrieve relevant Wikipedia screenshots, measuring the full system performance rather than manual query evaluation.

3. Test DSE's effectiveness on additional document types beyond Wikipedia pages and slides (e.g., PDFs, web articles, technical documentation) to validate the claim of unifying multimodal retrieval across diverse document formats.