---
ver: rpa2
title: 'HIGHT: Hierarchical Graph Tokenization for Molecule-Language Alignment'
arxiv_id: '2406.14021'
source_url: https://arxiv.org/abs/2406.14021
tags:
- graph
- molecular
- cited
- molecule
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of molecule-language alignment
  in large language models, where existing approaches using graph neural networks
  overlook the hierarchical structures in molecules, leading to subpar alignment and
  hallucination. The core method, HIGHT (Hierarchical Graph Tokenization), employs
  a hierarchical graph tokenizer to encode atom, motif, and molecular level tokens,
  enhancing molecular perception.
---

# HIGHT: Hierarchical Graph Tokenization for Molecule-Language Alignment

## Quick Facts
- arXiv ID: 2406.14021
- Source URL: https://arxiv.org/abs/2406.14021
- Reference count: 37
- One-line primary result: 40% reduction in hallucination on MotifHallu benchmark

## Executive Summary
HIGHT (Hierarchical Graph Tokenization) addresses the problem of molecule-language alignment in large language models, where existing approaches using graph neural networks overlook hierarchical structures in molecules, leading to subpar alignment and hallucination. The core method employs a hierarchical graph tokenizer to encode atom, motif, and molecular level tokens, enhancing molecular perception. Additionally, it uses an augmented instruction tuning dataset enriched with hierarchical graph information. The primary results show a 40% reduction in hallucination on the MotifHallu benchmark and significant improvements in various molecule-language downstream tasks.

## Method Summary
HIGHT implements a hierarchical graph tokenizer using VQ-VAE to encode atom, motif, and molecular level tokens. The method employs BRICS decomposition to detect motifs and functional groups, then augments the molecular graph with motif supernodes to create a hierarchical graph. Separate VQ-VAEs learn discrete token embeddings for atoms, motifs, and graphs, with Laplacian positional encodings preserving structural context. Separate adapters project different token types into the LLM embedding space. The model is trained in two stages: alignment pretraining with PubChem-295k dataset, followed by task-specific instruction tuning on datasets like MoleculeNet, ChEBI-20, and Mol-Instructions.

## Key Results
- 40% reduction in hallucination on the MotifHallu benchmark
- Significant improvements across 14 real-world benchmarks spanning property prediction, molecular description, and chemical reaction prediction tasks
- Enhanced molecular perception through explicit encoding of motif-level functional group information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HIGHT's hierarchical tokenization directly encodes motif-level functional group information into LLM tokens, bypassing the need for the LLM to learn motif composition from atom-level tokens alone.
- Mechanism: By augmenting the molecular graph with motif supernodes (via BRICS decomposition) and training separate VQ-VAEs for atom, motif, and graph-level tokens, HIGHT provides explicit motif tokens that capture functional group semantics. The LLM receives both atom and motif tokens simultaneously, along with Laplacian positional encodings to preserve structural context.
- Core assumption: Functional groups carry rich biochemical semantics that are crucial for accurate molecule-language alignment, and these semantics are lost when only atom-level tokens are used.
- Evidence anchors: [abstract] "The high-order structural information, such as motifs or functional groups, contains rich semantics of the biochemical functionalities of the molecules."

### Mechanism 2
- Claim: HIGHT reduces hallucination by providing explicit supervision about motif presence/absence through the HiPubChem instruction tuning dataset.
- Mechanism: HiPubChem augments the original molecule captions with explicit statements about the number and presence of each functional group. During instruction tuning, the LLM learns to associate motif tokens with these explicit descriptions, preventing it from guessing "Yes" for all functional groups.
- Core assumption: LLMs will align motif tokens with corresponding text descriptions during instruction tuning if such descriptions are present in the dataset.
- Evidence anchors: [abstract] "HIGHT also adopts an augmented instruction tuning dataset, enriched with the hierarchical graph information, to further enhance the graph-language alignment."

### Mechanism 3
- Claim: HIGHT's use of separate adapters for atom, motif, and graph-level tokens prevents feature distortion and allows the LLM to properly attend to each level of molecular structure.
- Mechanism: Instead of using a single adapter to project all graph tokens into the LLM embedding space, HIGHT uses three distinct adapters for atom, motif, and graph tokens respectively. This separation ensures that the semantic differences between these token types are preserved.
- Core assumption: The semantic meanings of atom, motif, and graph tokens are sufficiently different that a single adapter would distort or lose important information.
- Evidence anchors: [section] "Meanwhile, as motif (and graph) tokens pose different semantic meanings from atom tokens, we adopt separate adapters for different types of tokens."

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their limitations in capturing long-range dependencies and distinguishing isomorphic substructures.
  - Why needed here: HIGHT builds upon GNN-generated node embeddings as a starting point, but addresses their limitations by adding motif-level information. Understanding GNN mechanics is crucial for grasping why HIGHT's hierarchical approach is beneficial.
  - Quick check question: Why do standard GNNs struggle to distinguish between different functional groups that share common atoms (e.g., carboxylic acid vs. hydroperoxide)?

- Concept: Vector Quantized Variational Autoencoders (VQ-VAEs) and their role in learning discrete token representations.
  - Why needed here: HIGHT uses separate VQ-VAEs to learn discrete token embeddings for atoms, motifs, and graphs. Understanding VQ-VAE mechanics is essential for grasping how HIGHT generates meaningful tokens for each level of molecular structure.
  - Quick check question: How does the VQ-VAE training objective (reconstruction loss + codebook regularization) ensure that the learned tokens capture meaningful structural information?

- Concept: Large Language Model (LLM) tokenization and positional encoding.
  - Why needed here: HIGHT feeds hierarchical graph tokens into an LLM, which requires understanding how LLMs process discrete tokens and the role of positional encodings in preserving sequence/order information.
  - Quick check question: Why does HIGHT use Laplacian positional encodings instead of the standard sinusoidal positional encodings used in most LLMs?

## Architecture Onboarding

- Component map: Molecular graph -> BRICS decomposition -> Hierarchical graph -> VQ-VAE tokenization -> Laplacian positional encoding -> Separate adapter projection -> LLM processing -> Aligned output
- Critical path: Molecular graph → BRICS decomposition → Hierarchical graph → VQ-VAE tokenization → Positional encoding → Separate adapter projection → LLM processing → Aligned output
- Design tradeoffs: Using separate VQ-VAEs for each token level increases model complexity but allows for more specialized token learning. BRICS decomposition may miss some motifs or create overly large motifs, affecting the quality of motif tokens. Separate adapters increase the number of parameters but prevent feature distortion between token levels.
- Failure signatures: High hallucination rates on MotifHallu (node-centric baseline behavior). Poor performance on property prediction tasks (inability to capture functional group semantics). OOM errors during training (due to increased model complexity).
- First 3 experiments: 1) Train HIGHT on PubChem-295k and evaluate hallucination reduction on MotifHallu compared to the node-centric baseline. 2) Fine-tune HIGHT on MoleculeNet property prediction tasks and compare ROC-AUC/MAE to the node-centric baseline. 3) Evaluate HIGHT on molecular description generation (ChEBI-20) and compare BLEU/ROUGE scores to the node-centric baseline.

## Open Questions the Paper Calls Out

- Open Question 1: How does the hierarchical tokenization approach in HIGHT generalize to non-molecular graph data, such as social networks or knowledge graphs?
- Open Question 2: What is the optimal number and granularity of hierarchical levels for tokenization in different types of graphs?
- Open Question 3: How does the performance of HIGHT compare to foundation models specifically pre-trained on graph data (e.g., GraphGPT) when fine-tuned on molecule-language tasks?

## Limitations

- BRICS decomposition reliability: The paper relies heavily on BRICS decomposition to identify meaningful motifs and functional groups, but this rule-based fragmentation method may not capture all biologically relevant motifs.
- Separate adapter effectiveness: While the paper claims separate adapters prevent feature distortion, the empirical evidence for this is limited and the increased model complexity may not be justified.
- HiPubChem dataset quality: The augmented instruction tuning dataset is crucial for reducing hallucination, but the paper doesn't provide details on quality control measures for the motif descriptions.

## Confidence

- High Confidence: The core concept of using hierarchical graph tokenization to capture functional group information is well-founded and supported by existing literature.
- Medium Confidence: The specific implementation details of HIGHT, such as the exact architecture of the GNN backbone and training procedure, are not fully specified in the paper.
- Low Confidence: The paper claims a 40% reduction in hallucination on the MotifHallu benchmark, but specific details of this benchmark and evaluation methodology are not provided.

## Next Checks

1. Conduct an ablation study to assess the impact of BRICS decomposition quality on HIGHT's performance by comparing results using BRICS-identified motifs against alternative methods.
2. Implement a variant of HIGHT that uses a single adapter for all token types and compare its performance against the original HIGHT to determine if separate adapters are justified.
3. Manually inspect a random sample of the HiPubChem dataset to assess the accuracy and consistency of motif descriptions and identify potential systematic errors.