---
ver: rpa2
title: Towards a Holistic Evaluation of LLMs on Factual Knowledge Recall
arxiv_id: '2404.16164'
source_url: https://arxiv.org/abs/2404.16164
tags:
- knowledge
- answer
- question
- shot
- known
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FACT-BENCH, a comprehensive benchmark for
  evaluating large language models' ability to recall factual knowledge learned during
  pretraining. The benchmark consists of 20K question-answer pairs covering 20 domains,
  134 property types, 3 answer types, and different knowledge popularity levels.
---

# Towards a Holistic Evaluation of LLMs on Factual Knowledge Recall

## Quick Facts
- arXiv ID: 2404.16164
- Source URL: https://arxiv.org/abs/2404.16164
- Reference count: 21
- Key outcome: FACT-BENCH benchmark shows instruction-tuning degrades factual recall, larger models perform better, and fine-tuning on known knowledge improves performance

## Executive Summary
This paper introduces FACT-BENCH, a comprehensive benchmark for evaluating large language models' ability to recall factual knowledge learned during pretraining. The benchmark consists of 20K question-answer pairs covering 20 domains, 134 property types, 3 answer types, and different knowledge popularity levels. The authors benchmark 31 models across 10 model families and provide a holistic assessment of their strengths and weaknesses, revealing that instruction-tuning hurts knowledge recall, larger models generally perform better, and fine-tuning on known knowledge is beneficial.

## Method Summary
The authors construct FACT-BENCH from Wikidata triplets, generating questions for different knowledge popularity levels and validating the dataset. They benchmark 31 models across 10 model families using zero-shot and few-shot in-context learning, counterfactual demonstrations, and fine-tuning experiments on LLaMA-7B with known/unknown knowledge settings. Evaluation uses Exact Match, F1 score, and Contains metrics to assess knowledge recall accuracy across different domains and property types.

## Key Results
- Instruction-tuned models consistently underperform pretraining-only versions for all model families and metrics
- Larger models show better performance across all families, with GPT-4 achieving the best results but still showing significant gaps
- Fine-tuning on known knowledge improves performance, while fine-tuning on unknown knowledge leads to hallucinations
- Counterfactual exemplars significantly degrade factual recall for large models, especially when contradicting known knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuning degrades factual recall by imposing an alignment tax that interferes with parametric knowledge retrieval
- Mechanism: Pretraining-only models retain richer factual knowledge, while instruction-tuning aligns output format at the expense of precision
- Core assumption: Knowledge is primarily learned during pretraining; alignment only affects surface style
- Evidence anchors: Instruction-tuned models display inferior performance for all metrics; pretraining-only versions achieve best performance for each model family

### Mechanism 2
- Claim: Counterfactual in-context exemplars that contradict known knowledge cause significant degradation in factual recall, especially for large models
- Mechanism: Large models are more capable of recognizing contradictions between exemplars and their stored knowledge, leading to uncertainty
- Core assumption: Models encode knowledge in a way that allows them to detect contradictions
- Evidence anchors: LLaMA-65B drops from 52.45% EM to 26.60% with known-shuffle exemplars; smaller models show less significant drops

### Mechanism 3
- Claim: Fine-tuning on known knowledge improves factual recall, while fine-tuning on unknown knowledge leads to hallucinations
- Mechanism: Fine-tuning on known knowledge reinforces existing factual knowledge, while unknown knowledge teaches models to generate plausible but incorrect answers
- Core assumption: Models learn to associate fine-tuning with generating answers regardless of accuracy
- Evidence anchors: Training with known knowledge consistently outperforms mixed knowledge; unknown knowledge leads to worst performance

## Foundational Learning

- Concept: Closed-book question answering (QA)
  - Why needed here: The benchmark evaluates LLMs' ability to recall factual knowledge without external context
  - Quick check question: What is the difference between closed-book and open-book QA?

- Concept: Knowledge base (KB) construction and evaluation
  - Why needed here: The benchmark is constructed from Wikidata triplets, requiring understanding of KB evaluation
  - Quick check question: What are the key considerations when constructing a KB for evaluating factual recall in LLMs?

- Concept: In-context learning (ICL)
  - Why needed here: The benchmark uses ICL to evaluate learning from in-context exemplars
  - Quick check question: How does ICL differ from traditional fine-tuning, and what are its advantages and disadvantages?

## Architecture Onboarding

- Component map: Dataset construction -> Model evaluation -> Fine-tuning experiments
- Critical path: Download dataset → Select models → Run evaluation with prompts → Compare results across metrics
- Design tradeoffs: Trades diversity for simplicity and validity; questions are specific to ensure validity but may limit dataset diversity
- Failure signatures: Models may fail due to insufficient pretraining data, poor generalization, or sensitivity to in-context exemplars
- First 3 experiments:
  1. Run new model on evaluation set using provided prompts and compare to existing models
  2. Fine-tune new model on known/unknown/mixed knowledge and compare to LLaMA-7B experiments
  3. Conduct ICL experiment using counterfactual exemplars and compare to LLaMA experiments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the distribution of knowledge popularity in pretraining data affect LLMs' factual knowledge recall performance?
- Basis in paper: Knowledge popularity is a strong predictor of recall, with LLMs struggling on long-tail entities
- Why unresolved: Paper uses Wikipedia as pretraining proxy but doesn't analyze varying knowledge distributions
- What evidence would resolve it: Experiments comparing models pretrained on different corpora with varying knowledge distributions

### Open Question 2
- Question: What is the optimal number and selection strategy for in-context exemplars to maximize factual recall without introducing contradictions?
- Basis in paper: Counterfactual exemplars lead to degradation, especially when contradicting known knowledge
- Why unresolved: Paper identifies negative impact but doesn't provide guidance on optimal exemplar selection
- What evidence would resolve it: Systematic experiments varying exemplar number and selection criteria

### Open Question 3
- Question: How does fine-tuning on known versus unknown knowledge affect LLMs' ability to distinguish between factual and hallucinated information?
- Basis in paper: Fine-tuning on unknown knowledge teaches models to hallucinate
- Why unresolved: Experiments show performance differences but don't investigate underlying mechanisms
- What evidence would resolve it: Probing experiments analyzing model representations before and after fine-tuning

## Limitations
- Sampling bias in knowledge base due to popularity skew from Wikidata, despite stratified sampling attempts
- Knowledge state ambiguity across models trained at different times, creating confounding for time-sensitive facts
- Limited exploration of prompt engineering variations, potentially attributing performance differences to model knowledge rather than prompt format

## Confidence
- High confidence: Pretraining-only models outperforming instruction-tuned versions for factual recall
- Medium confidence: Counterfactual exemplar degradation effect, with variable magnitude across model sizes
- Low confidence: Fine-tuning results showing known knowledge benefits may be partially attributed to methodology

## Next Checks
1. Re-run benchmark with subset of models controlling for training cutoff dates to isolate temporal effects
2. Systematically vary prompt formats while holding model and knowledge constant to quantify prompt contribution
3. Conduct controlled fine-tuning experiments where same facts appear in both "known" and "unknown" contexts