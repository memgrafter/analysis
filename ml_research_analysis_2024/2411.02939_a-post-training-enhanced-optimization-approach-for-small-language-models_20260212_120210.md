---
ver: rpa2
title: A Post-Training Enhanced Optimization Approach for Small Language Models
arxiv_id: '2411.02939'
source_url: https://arxiv.org/abs/2411.02939
tags:
- data
- language
- post-training
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a continuous post-training optimization framework
  for small language models that constructs alignment data guided by large models
  to improve diversity and accuracy. The approach involves collecting and classifying
  instruction data, regenerating responses using both large and small language models,
  evaluating quality through multiple metrics including perplexity and safety scores,
  and constructing SFT and KTO datasets.
---

# A Post-Training Enhanced Optimization Approach for Small Language Models

## Quick Facts
- arXiv ID: 2411.02939
- Source URL: https://arxiv.org/abs/2411.02939
- Authors: Keke Zhai
- Reference count: 24
- Primary result: Continuous post-training optimization improves small language model performance across multiple benchmarks through large model-guided data construction, two-stage SFT-KTO training, and weight fusion

## Executive Summary
This paper presents a continuous post-training optimization framework for small language models that leverages large model guidance to construct high-quality alignment data. The approach addresses the challenge of improving small model performance by systematically collecting, classifying, and regenerating instruction-response pairs using both large and small language models. Through comprehensive experiments using Qwen2-0.5B-Instruct as the baseline, the method demonstrates significant performance improvements across multiple benchmarks including mmlu, cmmlu, ceval, HumanEval, and gsm8k. The framework combines supervised fine-tuning (SFT), knowledge transfer optimization (KTO), and model weight fusion to create a robust post-training pipeline that enhances both diversity and accuracy of small language models.

## Method Summary
The method involves a systematic pipeline for post-training small language models using large model guidance. It begins with collecting and classifying open-source instruction datasets, then employs large language models (Qwen2.5-7B-Instruct) to evaluate and regenerate responses while controlling for diversity. The approach constructs specialized SFT and KTO datasets based on quality metrics including perplexity, safety scores, and quality assessments. Training employs LLaMA-Factory with the Qwen2-0.5B-Instruct baseline model, using specific hyperparameters: SFT with learning rate 5e-8, batch size 4, gradient accumulation 250, warmup ratio 0.005, and Cosine Annealing Scheduler; KTO with learning rate 1e-8 and same batch settings. The framework implements a two-stage SFT-KTO training approach followed by model weight fusion to combine strengths from different optimization methods.

## Key Results
- Continuous post-training optimization significantly improves small language model performance across mmlu, cmmlu, ceval, HumanEval, and gsm8k benchmarks
- Two-stage SFT-KTO training provides additional performance gains compared to either continuous SFT or continuous KTO alone
- Model weight fusion of SFT-only, KTO-only, and two-stage models creates balanced performance improvements across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large model guidance improves diversity and accuracy of small model post-training data
- Mechanism: The approach uses Qwen2.5-7B-Instruct to classify, regenerate responses, and evaluate quality metrics (perplexity, safety, and quality scores) for the alignment data
- Core assumption: Large language models can provide reliable quality assessments and diverse responses that benefit small model training
- Evidence anchors:
  - [abstract] "The core of this method is based on the data guidance of large models, optimizing the diversity and accuracy of alignment data"
  - [section 3.5] "we choose to regenerate the replies, including using a large language model to generate replies, and a small language model to generate replies"
  - [corpus] Weak evidence - corpus contains related work on post-training quantization and alignment but no direct evidence of large model guidance for small model data construction
- Break condition: If large model quality evaluations are not correlated with actual small model performance improvements, the guidance becomes unreliable

### Mechanism 2
- Claim: Two-stage SFT-KTO training provides better performance than single-stage methods
- Mechanism: The approach first applies SFT training followed by KTO optimization, allowing the model to first learn general alignment then refine preference-based behaviors
- Core assumption: Sequential application of different optimization methods can capture complementary aspects of alignment that single methods miss
- Evidence anchors:
  - [section 4.4] "we designed a two-stage SFT-KTO experiment" with results showing "the model performance based on the two-stage experiment can be further improved compared to either continuous SFT or continuous KTO post-training alone"
  - [section 4.4] Table 4-3 showing consistent improvements across all benchmarks for the two-stage approach
  - [corpus] No direct evidence in corpus - related papers focus on other aspects of post-training but not specifically on two-stage approaches
- Break condition: If the KTO stage degrades the performance gains from SFT due to catastrophic forgetting or conflicting optimization objectives

### Mechanism 3
- Claim: Weight fusion of models trained with different methods provides balanced performance improvements
- Mechanism: The approach averages weights from SFT-only, KTO-only, and two-stage models to create a model that benefits from multiple training approaches
- Core assumption: Different training methods capture different aspects of model capability, and averaging their weights creates a more robust final model
- Evidence anchors:
  - [section 4.5] "we designed weight fusion experiments to verify the impact of weight fusion on model performance" with results showing "the overall performance of the model is further enhanced and relatively balanced on each indicator"
  - [section 4.5] Table 4-4 showing consistent improvements across all benchmarks after weight fusion
  - [corpus] No direct evidence in corpus - related work focuses on other optimization aspects rather than weight fusion strategies
- Break condition: If weight averaging dilutes the most effective aspects of each individual training method, resulting in performance worse than the best individual model

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT forms the foundation for the two-stage approach and provides initial alignment before preference optimization
  - Quick check question: What is the key difference between pre-training and fine-tuning in terms of data requirements and optimization objectives?

- Concept: Reinforcement Learning from Human Feedback (RLHF) and its variants (DPO, KTO)
  - Why needed here: These methods provide the preference-based optimization that complements SFT in the two-stage approach
  - Quick check question: How does KTO differ from DPO in terms of data requirements and optimization approach?

- Concept: Perplexity as a quality metric
  - Why needed here: Perplexity is used to evaluate and select high-quality responses during data construction
  - Quick check question: What does a lower perplexity score indicate about a language model's performance on a given text?

## Architecture Onboarding

- Component map: Data collection → Classification → Diversity control → Response regeneration → Quality evaluation → Dataset construction → SFT training → KTO training → Weight fusion → Evaluation
- Critical path: Data construction → SFT training → KTO training → Weight fusion → Evaluation
- Design tradeoffs: Computational cost vs. model performance, data quality vs. quantity, single-stage vs. two-stage training
- Failure signatures: Training instability with high learning rates, poor convergence on small batch sizes, evaluation metrics not improving despite training
- First 3 experiments:
  1. Single SFT training with varying learning rates to identify optimal hyperparameter
  2. Single KTO training to compare effectiveness with SFT
  3. Two-stage SFT-KTO training to validate the sequential approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of small language models trained with the proposed continuous post-training approach compare to larger models trained with traditional methods across diverse real-world applications?
- Basis in paper: [inferred] The paper demonstrates performance improvements in small language models using continuous post-training but does not compare these results to larger models in practical applications.
- Why unresolved: The study focuses on improving small models but does not benchmark against larger models in real-world scenarios.
- What evidence would resolve it: Empirical studies comparing the performance of small models trained with the proposed method against larger models in various real-world tasks and environments.

### Open Question 2
- Question: What are the long-term effects of continuous post-training on the generalization capabilities of small language models across different domains?
- Basis in paper: [explicit] The paper shows immediate performance improvements but does not explore long-term generalization effects.
- Why unresolved: The study evaluates short-term performance gains without assessing how models adapt over time or across new domains.
- What evidence would resolve it: Longitudinal studies tracking model performance across diverse domains and time periods to assess generalization and adaptability.

### Open Question 3
- Question: What are the computational and resource trade-offs of implementing continuous post-training for small language models in resource-constrained environments?
- Basis in paper: [explicit] The paper acknowledges resource constraints but does not provide a detailed analysis of computational trade-offs.
- Why unresolved: The study focuses on performance improvements without a thorough analysis of resource usage and efficiency.
- What evidence would resolve it: Detailed resource usage studies comparing computational costs and efficiency of continuous post-training against traditional methods in various environments.

## Limitations
- Large model guidance quality correlation remains unproven - the paper assumes perplexity and safety scores from large models predict small model performance, but this correlation is not directly validated
- Two-stage training potential conflicts are untested - while the SFT-KTO two-stage approach shows improvements, the paper doesn't test for potential catastrophic forgetting or objective conflicts between stages
- Weight fusion benefits beyond ensemble effects are unclear - the improvements from weight fusion could be due to simple ensemble averaging rather than capturing complementary strengths from different training methods

## Confidence
- **High Confidence**: The experimental methodology is sound, with clear hyperparameter settings, appropriate benchmark selection, and reproducible training procedures. The Qwen2-0.5B-Instruct baseline and evaluation datasets are well-specified.
- **Medium Confidence**: The core claim that continuous post-training improves small language models is supported by consistent benchmark improvements, but the mechanism by which large model guidance achieves this remains incompletely validated.
- **Low Confidence**: The specific advantages of the two-stage SFT-KTO approach over alternative training strategies and the effectiveness of weight fusion beyond simple ensemble averaging are not rigorously established.

## Next Checks
1. **Correlation Analysis**: Test whether perplexity and safety scores from large models actually predict small model performance improvements on held-out validation sets, rather than just appearing favorable during training.

2. **Stage Conflict Assessment**: Run ablations comparing SFT followed by KTO versus KTO followed by SFT, and test whether freezing earlier-stage parameters during later training prevents potential catastrophic forgetting or objective conflicts.

3. **Fusion Mechanism Investigation**: Compare weight fusion against other combination strategies (e.g., gating mechanisms, mixture-of-experts) and test whether the improvements persist when using random weight averaging versus averaging trained models, to distinguish true complementary learning from ensemble effects.