---
ver: rpa2
title: 'EntryPrune: Neural Network Feature Selection using First Impressions'
arxiv_id: '2410.02344'
source_url: https://arxiv.org/abs/2410.02344
tags:
- features
- entryprune
- feature
- selection
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EntryPrune is a novel feature selection method using a neural network
  with a dynamically sparse input layer. It employs entry-based pruning, comparing
  neurons based on the change they induce when entering the network, addressing the
  challenge of comparing regrown neurons with those active for varying durations.
---

# EntryPrune: Neural Network Feature Selection using First Impressions

## Quick Facts
- arXiv ID: 2410.02344
- Source URL: https://arxiv.org/abs/2410.02344
- Authors: Felix Zimmer; Patrik Okanovic; Torsten Hoefler
- Reference count: 40
- Primary result: Achieves 96.3% accuracy on MNIST using only 50 features, surpassing previous best of 94.3%

## Executive Summary
EntryPrune is a novel feature selection method using a neural network with a dynamically sparse input layer. It employs entry-based pruning, comparing neurons based on the change they induce when entering the network, addressing the challenge of comparing regrown neurons with those active for varying durations. Extensive experiments on 13 diverse datasets show that EntryPrune generally outperforms state-of-the-art methods, particularly improving average accuracy on low-dimensional datasets. The method also demonstrates lower runtime than competing approaches and can be extended with an adaptive input layer variant to adjust to dataset characteristics automatically.

## Method Summary
EntryPrune uses a dense neural network with a dynamic sparse input layer where only the first layer is sparse. The algorithm employs entry-based pruning, which levels the playing field by comparing features based on their initial short-term impact rather than accumulated gradients. Features are randomly regrown to enable better discovery of interaction features compared to gradient-based approaches. The method uses Adam optimizer with gradient accumulation for relative change scoring, and training stops when validation loss doesn't decrease for 100 rotations or feature set remains unchanged for 100 rotations. Hyperparameters include cratio (ratio of candidate features) and nmb (number of mini-batches for score calculation), with different settings for long versus wide datasets.

## Key Results
- Achieves 96.3% accuracy on MNIST using only 50 features, surpassing previous best of 94.3%
- Demonstrates lower runtime than competing approaches across all tested datasets
- Shows particularly strong performance on low-dimensional datasets, improving average accuracy over state-of-the-art methods
- Introduces adaptive input layer variant that automatically adjusts to dataset characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entry-based pruning compares features based on their initial short-term impact, enabling fairer competition among features with different active durations.
- Mechanism: When a feature enters the network, its initial gradient impact is recorded and used as its entry score. This prevents long-active features from accumulating advantage over newly added features.
- Core assumption: The initial gradient impact of a feature is a meaningful proxy for its long-term importance.
- Evidence anchors:
  - [abstract] "It employs entry-based pruning, a novel approach that compares neurons based on their relative change induced when they have entered the network."
  - [section] "Entry-based pruning addresses this issue by leveling the playing field, allowing regrown neurons to compete more effectively with established neurons."
- Break condition: If initial gradient impacts don't correlate with long-term importance, or if complex feature interactions make initial signals misleading.

### Mechanism 2
- Claim: Random regrowth enables better discovery of interaction features compared to gradient-based regrowth.
- Mechanism: By randomly selecting candidate features rather than using gradient signals, the algorithm avoids biasing toward features with strong linear correlations while allowing features with important interactions to emerge over time.
- Core assumption: Complex interactions between features cannot be captured by initial gradient signals alone.
- Evidence anchors:
  - [section] "Random Regrowth... is particularly promising because it allows features to incrementally prove their relevance: instead of relying on gradient signals for inclusion... candidates are randomly reselected and evaluated across multiple mini-batches."
  - [appendix] "Gradient-based regrowth, as used in Evci et al. (2020) and Atashgahi et al. (2023), selects candidates for regrowth based on the highest absolute gradient of adjacent weights. In this toy example, we examine how this metric behaves for interaction features."
- Break condition: If the feature space is dominated by linear relationships where gradient signals are sufficient, random regrowth may add unnecessary noise.

### Mechanism 3
- Claim: The dynamically sparse input layer with dense body leverages GPU optimization for dense operations while maintaining feature selection capability.
- Mechanism: Only the input layer is sparse, allowing the dense body to benefit from GPU-optimized dense matrix multiplication while still achieving feature selection through the sparse input.
- Core assumption: Modern GPUs achieve better performance with dense operations when sparsity is below 70%.
- Evidence anchors:
  - [section] "Prior work uses either fully sparse or fully dense networks. We propose a dynamically sparse input layer with a dense body, where only the input layer is sparse. Since modern GPUs are architected and optimized for dense matrix-matrix multiplication, existing sparse kernels surpass dense ones only when the sparsity level is high (>70%)"
- Break condition: If feature selection requires sparsity throughout the network, or if computational efficiency demands full sparsity.

## Foundational Learning

- Concept: Dynamic sparse training and neuron pruning in neural networks
  - Why needed here: EntryPrune builds on sparse neural network literature, adapting dynamic sparsity techniques specifically for feature selection in the input layer
  - Quick check question: What distinguishes EntryPrune's approach to sparsity from traditional sparse neural network training?

- Concept: Gradient-based importance metrics for neural network components
  - Why needed here: The method relies on calculating and comparing gradients to determine feature importance, requiring understanding of how gradients relate to feature utility
  - Quick check question: How does EntryPrune's entry score differ from traditional gradient-based pruning metrics?

- Concept: Feature selection as L0 regularization in neural networks
  - Why needed here: The paper frames feature selection as enforcing L0 regularization on first-layer weights, connecting the method to established regularization theory
  - Quick check question: Why is direct L0 regularization typically intractable, and how does EntryPrune approximate it?

## Architecture Onboarding

- Component map: Input features → Sparse input layer (dynamic) → Dense hidden layer (100 neurons, ReLU) → Output layer
- Critical path: Feature selection → gradient calculation → entry score update → feature rotation → weight optimization
- Design tradeoffs: Input layer size (cratio) balances exploration vs exploitation; random vs gradient-based regrowth affects interaction feature discovery; dense body vs sparse body impacts computational efficiency
- Failure signatures: Poor performance on wide datasets may indicate insufficient exploration; unstable feature selection suggests inappropriate input layer size; convergence issues could stem from hyperparameter mismatch
- First 3 experiments:
  1. Run EntryPrune on MNIST with K=50 and cratio=0.2, verify entry scores stabilize over time
  2. Compare random vs gradient-based regrowth on a toy dataset with interaction features
  3. Test different input layer sizes on ISOLET to observe exploration-exploitation tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does EntryPrune maintain performance advantages when extended to architectures with weight sharing (e.g., CNNs, Vision Transformers)?
- Basis in paper: [inferred] from the discussion in Appendix G noting that weight sharing makes the change-based scoring mechanism infeasible
- Why unresolved: The paper explicitly states this as a limitation but does not explore workarounds or alternative scoring mechanisms compatible with weight-sharing architectures
- What evidence would resolve it: Experimental results comparing EntryPrune against state-of-the-art methods on image classification tasks using CNN/Transformer architectures with adapted scoring mechanisms

### Open Question 2
- Question: How does EntryPrune perform under different data distributions, particularly with non-stationary or imbalanced datasets?
- Basis in paper: [inferred] from the lack of experiments on non-stationary data distributions, despite the algorithm's reliance on relative change scores computed over mini-batches
- Why unresolved: All experiments used static train-test splits with balanced datasets; the algorithm's behavior under distribution shift or class imbalance is unknown
- What evidence would resolve it: Experiments on datasets with temporal drift, class imbalance, or covariate shift showing feature selection stability and accuracy retention over time

### Open Question 3
- Question: What is the theoretical convergence behavior of EntryPrune given its bi-level optimization structure with non-stationary objectives?
- Basis in paper: [explicit] from the "Rationale" section acknowledging that standard SGD convergence analyses don't apply and that rigorous analysis requires new tools for non-convex, time-varying bi-level optimization
- Why unresolved: The authors explicitly state this analysis is left for future work and are not aware of existing theoretical frameworks that can handle this setting
- What evidence would resolve it: Mathematical proof of convergence rates under specific assumptions about feature rotation frequency and network dynamics, or empirical characterization of convergence patterns across diverse datasets

## Limitations

- The method's performance advantages may depend heavily on appropriate hyperparameter settings, particularly the ratio of candidate features and mini-batch count for score calculation
- Entry-based pruning may not be directly applicable to architectures with weight sharing (CNNs, Transformers) due to the fundamental reliance on per-feature gradient changes
- Theoretical convergence analysis remains incomplete, as the algorithm involves non-convex, time-varying bi-level optimization that doesn't fit existing frameworks

## Confidence

- **High Confidence:** The computational efficiency claims and basic architecture design are well-supported by methodology and implementation details
- **Medium Confidence:** The superiority claims over state-of-the-art methods are supported by extensive experiments across 13 datasets, though some results may depend on specific hyperparameter choices
- **Medium Confidence:** The mechanism explanations are logically coherent but rely on assumptions about gradient behavior and feature interactions that would benefit from additional theoretical grounding

## Next Checks

1. **Reproduce core results on MNIST:** Run EntryPrune with K=50 features on MNIST and verify the claimed 96.3% accuracy, comparing against baseline methods using the same evaluation protocol
2. **Test stopping criterion sensitivity:** Experiment with different rotation thresholds (e.g., 50 vs 100) and validation monitoring strategies to assess how sensitive feature selection quality is to stopping conditions
3. **Validate entry score stability:** Track entry scores across multiple runs on the same dataset to confirm they stabilize consistently, particularly examining whether features with initially high gradients maintain their importance over time