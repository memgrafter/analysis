---
ver: rpa2
title: How Lightweight Can A Vision Transformer Be
arxiv_id: '2407.17783'
source_url: https://arxiv.org/abs/2407.17783
tags:
- layer
- size
- vision
- learning
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a lightweight vision transformer architecture
  that uses Mixture-of-Experts (MoE) layers with shared parameters to reduce model
  size. The approach employs depth-wise scaling, progressively reducing hidden layer
  sizes and increasing the number of experts.
---

# How Lightweight Can A Vision Transformer Be

## Quick Facts
- arXiv ID: 2407.17783
- Source URL: https://arxiv.org/abs/2407.17783
- Authors: Jen Hong Tan
- Reference count: 0
- Primary result: Vision transformer achieves 79.15% CIFAR-100 accuracy with only 2.36M parameters using shared MoE layers

## Executive Summary
This paper explores the minimal parameter requirements for vision transformers by introducing a lightweight architecture that leverages Mixture-of-Experts (MoE) layers with shared parameters. The approach uses depth-wise scaling, progressively reducing hidden layer sizes while increasing the number of experts, and incorporates grouped query attention to further reduce parameters. The resulting model achieves competitive performance on small datasets, reaching 79.15% accuracy on CIFAR-100 with just 2.36M parameters. The architecture demonstrates that vision transformers can maintain strong performance even at extreme parameter counts (74.95% accuracy with 0.67M parameters), challenging the conventional wisdom that large models are necessary for good vision performance.

## Method Summary
The paper presents a streamlined vision transformer architecture that employs depth-wise scaling with progressively smaller hidden dimensions and increased expert counts in MoE layers. The model uses shared parameters across MoE experts to reduce memory footprint, and incorporates grouped query attention (GQA) to further decrease parameter count. The architecture is trained on small datasets (CIFAR-10, CIFAR-100, Tiny-ImageNet, Flowers-102) and demonstrates effective transfer learning capabilities when pre-trained with masked auto-encoders. The key innovation lies in balancing model capacity and parameter efficiency through careful architectural design rather than simply reducing model size indiscriminately.

## Key Results
- Achieves 79.15% accuracy on CIFAR-100 with only 2.36M parameters
- Maintains strong performance at extreme compression (74.95% accuracy with 0.67M parameters)
- Outperforms larger models when pre-trained with masked auto-encoders
- Demonstrates effective transfer learning capabilities on small datasets

## Why This Works (Mechanism)
The architecture works by strategically allocating parameters where they matter most. The shared MoE approach reduces redundancy while maintaining capacity, as experts can specialize on different input patterns without duplicating parameters. Depth-wise scaling ensures that early layers (which capture low-level features) have sufficient capacity while later layers can be more compact. Grouped query attention reduces computational complexity without sacrificing the ability to capture long-range dependencies. The progressive increase in experts compensates for the decreasing hidden dimensions, maintaining representational power throughout the network. This balanced approach allows the model to achieve competitive performance while minimizing parameter count.

## Foundational Learning
- **Mixture-of-Experts (MoE)**: A gating mechanism that routes inputs to specialized sub-networks (experts), activating only a subset per input. Needed to increase model capacity without proportional parameter growth. Quick check: Verify that gating probabilities sum to 1 and that only the top-k experts are activated.
- **Depth-wise Scaling**: Gradually reducing hidden dimensions through network layers. Needed to allocate parameters efficiently, giving more capacity to early layers. Quick check: Confirm hidden dimension reduction follows the specified schedule.
- **Grouped Query Attention (GQA)**: A variant of multi-head attention where query heads are grouped to share key-value pairs. Needed to reduce parameter count while maintaining attention capabilities. Quick check: Count total attention parameters and verify they're reduced compared to standard multi-head attention.
- **Masked Auto-Encoders (MAE)**: Self-supervised pre-training method that masks input patches and reconstructs them. Needed for effective transfer learning on small datasets. Quick check: Verify masking ratio and reconstruction loss implementation.
- **Parameter Sharing**: Reusing weights across different parts of the model. Needed to drastically reduce parameter count without losing functionality. Quick check: Confirm shared parameters are actually being reused across intended components.

## Architecture Onboarding

**Component Map**: Input -> Patch Embedding -> Shared MoE Layers (progressive expert increase) -> GQA -> MLP Blocks -> Classification Head

**Critical Path**: The model processes inputs through patch embeddings, then sequentially through shared MoE layers with increasing experts, GQA layers, and MLP blocks before classification. The MoE layers with shared parameters are the critical innovation that enables extreme parameter efficiency.

**Design Tradeoffs**: The architecture trades parameter count for potential inference overhead (MoE requires expert selection) and possible training instability (shared parameters may limit specialization). The depth-wise scaling reduces capacity in later layers, which could limit learning complex patterns but improves parameter efficiency. The progressive expert increase adds complexity to the architecture design.

**Failure Signatures**: Poor performance on CIFAR-100 (<70%) with extreme parameter reduction (0.67M) would indicate insufficient capacity. High training instability or failure to converge suggests issues with shared parameter implementation. Suboptimal transfer learning performance would indicate problems with the pre-training setup or architectural limitations for downstream tasks.

**First Experiments**:
1. Verify CIFAR-100 accuracy progression as parameters decrease from 2.36M to 0.67M to confirm the claimed performance curve
2. Test transfer learning from pre-trained weights on CIFAR-100 to Flowers-102 to validate transfer capabilities
3. Compare inference latency of the shared MoE architecture against a standard vision transformer with equivalent parameter count

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the optimal number of stages for increasing the number of experts in the MoE layers, and how does this affect model performance?
- Basis in paper: [explicit] The paper mentions increasing the number of experts from 3 to 5 after every 3, 4, or 5 layers, depending on the model size, but does not provide a definitive answer on the optimal number of stages.
- Why unresolved: The paper indicates that further experiments are required to confirm the effectiveness of the arrangement of MoE with respect to the number of stages.
- What evidence would resolve it: Conducting experiments with varying numbers of stages and comparing the performance of models with different configurations would provide insights into the optimal number of stages.

### Open Question 2
- Question: How does the performance of the model change with different pre-training durations, especially on small datasets?
- Basis in paper: [explicit] The paper notes that more epochs should have been allocated for mmLiT-XS and mmLiT-XXS, and mentions that the relationship between the loss of a model and its subsequent performance in downstream tasks is unclear, especially with small datasets.
- Why unresolved: The paper does not explore the effects of varying pre-training durations beyond the tested epochs, leaving uncertainty about the optimal pre-training time.
- What evidence would resolve it: Running pre-training for a wider range of epochs and evaluating the downstream task performance would clarify the impact of pre-training duration.

### Open Question 3
- Question: Can the streamlined MoE approach effectively replace convolutional layers in vision transformers for tasks requiring strong inductive bias?
- Basis in paper: [explicit] The paper discusses the potential of the streamlined MoE to alleviate the lack of inductive bias, but does not provide conclusive evidence on its effectiveness compared to convolutional layers.
- Why unresolved: The paper suggests that further investigation is needed to determine if the MoE approach can fully replace convolutional layers in tasks requiring strong inductive bias.
- What evidence would resolve it: Comparative experiments between models using streamlined MoE and those using convolutional layers on tasks requiring strong inductive bias would provide clarity on the effectiveness of the MoE approach.

## Limitations
- Evaluation limited to small-scale datasets (CIFAR, Tiny-ImageNet, Flowers-102), lacking validation on larger, more complex vision tasks
- Insufficient implementation details for hyperparameter settings, optimizer configuration, and regularization techniques, raising reproducibility concerns
- Limited comparative analysis with other lightweight vision transformer approaches beyond a few selected architectures

## Confidence
- **High Confidence**: The architectural design choices (depth-wise scaling, progressive expert increase, shared MoE parameters) are technically sound and the reported parameter counts are verifiable through the provided configurations.
- **Medium Confidence**: The stated accuracy figures on CIFAR-100 (79.15% at 2.36M parameters) are plausible given the architecture's design, but lack independent verification and sufficient methodological detail for precise replication.
- **Low Confidence**: Claims about transfer learning capabilities and superiority over larger models are weakly supported, as the paper provides minimal experimental evidence and lacks rigorous ablation studies on transfer performance across diverse downstream tasks.

## Next Checks
1. Replicate the CIFAR-100 results (79.15% accuracy at 2.36M parameters) using the exact architecture and training procedure specifications provided, including batch size, learning rate schedule, and optimizer settings.
2. Evaluate the proposed architecture on a larger-scale dataset (e.g., ImageNet-1K) to assess whether the lightweight design maintains competitive performance on more complex vision tasks.
3. Conduct controlled experiments comparing the model's inference latency and memory usage against both standard and other lightweight vision transformer architectures to validate "lightweight" claims beyond parameter count.