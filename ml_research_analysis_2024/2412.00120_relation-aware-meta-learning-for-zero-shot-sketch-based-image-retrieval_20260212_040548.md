---
ver: rpa2
title: Relation-Aware Meta-Learning for Zero-shot Sketch-Based Image Retrieval
arxiv_id: '2412.00120'
source_url: https://arxiv.org/abs/2412.00120
tags:
- loss
- image
- margin
- learning
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the zero-shot sketch-based image retrieval
  (ZS-SBIR) problem, where the goal is to retrieve natural photos based on hand-drawn
  sketches of categories not seen during training. The key challenge is bridging the
  domain gap between sketches and photos while maintaining strong generalization to
  unseen categories.
---

# Relation-Aware Meta-Learning for Zero-shot Sketch-Based Image Retrieval

## Quick Facts
- arXiv ID: 2412.00120
- Source URL: https://arxiv.org/abs/2412.00120
- Authors: Yang Liu; Jiale Du; Xinbo Gao; Jungong Han
- Reference count: 40
- One-line primary result: Achieves 8.0% and 17% improvements in mAP@all over previous best method ZSE on Sketchy Extended

## Executive Summary
This paper addresses zero-shot sketch-based image retrieval (ZS-SBIR), where the goal is to retrieve natural photos based on hand-drawn sketches of categories not seen during training. The key challenge is bridging the domain gap between sketches and photos while maintaining strong generalization to unseen categories. The authors propose a Relation-Aware Meta-Learning Network (RAMLN) that uses a relation-aware quadruplet loss to capture both inter-modal and intra-modal relationships, along with a meta-learning approach to learn optimal margin values.

## Method Summary
The authors propose a Relation-Aware Meta-Learning Network (RAMLN) that uses a relation-aware quadruplet loss with two negative samples from different modalities to capture inter-modal and intra-modal relationships. This prevents positive features from becoming disproportionately distant from one modality while remaining close to another. Additionally, a meta-learning approach is used to learn the optimal margin for the quadruplet loss through an external memory matrix, allowing category-specific margin values to be adaptively determined during testing. The method is evaluated on Sketchy Extended and TU-Berlin Extended datasets.

## Key Results
- On Sketchy Extended, RAMLN achieves 8.0% and 17% improvements in mAP@all over the previous best method ZSE
- On TU-Berlin Extended, RAMLN shows 9.2% and 8.0% improvements in mAP@all over IVT
- The method demonstrates strong performance with both ResNet and ViT backbones, showing adaptability across different architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The relation-aware quadruplet loss prevents positive features from becoming disproportionately distant from one modality while remaining close to another
- Mechanism: By incorporating two negative samples from different modalities, the loss function creates three contrastive pairs that jointly optimize the embedding space. The first negative pair provides a "strong push" to increase inter-class gaps, while the second negative pair provides a "weak push" that stabilizes the anchor position and prevents modality imbalance.
- Core assumption: Using two negative samples from different modalities is more effective than using only one negative sample for balancing cross-modal relationships
- Evidence anchors:
  - [abstract]: "By incorporating two negative samples from different modalities, the approach prevents positive features from becoming disproportionately distant from one modality while remaining close to another, thus enhancing inter-class separability."
  - [section 3.2]: "The second term assists in providing another gradient direction for updating... To make the quadratic loss comprehensive, we intuitively set the second negative image from a different modality than the negative image in the first negative term."
- Break condition: If the two negative samples are not sufficiently diverse or if they come from the same class, the balancing effect would be compromised.

### Mechanism 2
- Claim: The meta-learning approach learns optimal margin values that adapt to different categories and modalities
- Mechanism: An external memory matrix stores feature information from seen classes, which is used by a relation network to predict category-specific margin values for the quadruplet loss. This allows the model to automatically adjust the separation boundaries based on the distribution characteristics of each category.
- Core assumption: The optimal margin for zero-shot retrieval varies across categories and can be predicted from seen class data
- Evidence anchors:
  - [abstract]: "We also propose a Relation-Aware Meta-Learning Network (RAMLN) to obtain the margin, a hyper-parameter of cross-modal quadruplet loss, to improve the generalization ability of the model."
  - [section 3.3]: "We introduce a meta-learning process to learn this margin hyperparameter, allowing the optimal value of R(x) for each specific category to be adaptively determined during testing."
- Break condition: If the seen class data does not adequately represent the distribution of unseen classes, the predicted margins may not generalize well.

### Mechanism 3
- Claim: The combination of classification loss with metric learning prevents the model from getting stuck in bad local optima
- Mechanism: While the relation-aware quadruplet loss focuses on geometric relationships in the embedding space, the classification loss uses cross-entropy with softmax to ensure accurate class predictions. This combination provides both metric learning objectives and classification objectives, creating a more robust optimization landscape.
- Core assumption: Metric learning alone may converge to suboptimal solutions that don't preserve class boundaries well
- Evidence anchors:
  - [section 3.4]: "But we still use a linear layer to learn another similarity score from the output vectors and feed them into the classification loss to avoid getting trapped in bad local optimum."
  - [section 3.4]: "The cross-entropy loss only focuses on whether the prediction is correct or not, ignoring correlations between classes or modalities."
- Break condition: If the classification loss dominates too heavily, it may override the carefully learned metric relationships in the embedding space.

## Foundational Learning

- Concept: Metric learning and contrastive loss functions
  - Why needed here: The core approach relies on learning an embedding space where similar sketches and photos are close together while dissimilar ones are far apart
  - Quick check question: What is the difference between triplet loss and quadruplet loss in terms of the number of samples used and the type of constraints they impose?

- Concept: Zero-shot learning and domain adaptation
  - Why needed here: The method must generalize to unseen categories without any training examples from those categories
  - Quick check question: How does zero-shot learning differ from few-shot learning in terms of the number of examples available during training?

- Concept: Meta-learning and memory-augmented networks
  - Why needed here: The adaptive margin mechanism uses meta-learning to predict optimal hyperparameters based on seen class data
  - Quick check question: What is the key advantage of using meta-learning for hyperparameter optimization compared to grid search or random search?

## Architecture Onboarding

- Component map: Backbone (ResNet/CLIP) -> Embedding space -> Relation-aware quadruplet loss -> Classification loss -> Memory-augmented meta-learner -> Margin prediction
- Critical path: Image encoder -> Relation-aware quadruplet loss -> Gradient updates -> Embedding space optimization
- Design tradeoffs: Using two negative samples increases computational complexity but provides better modality balancing; meta-learning adds parameters but removes need for manual margin tuning
- Failure signatures: Poor retrieval performance on unseen classes suggests domain shift issues; mode collapse in embedding space indicates loss function imbalance
- First 3 experiments:
  1. Baseline test: Run with only classification loss to establish baseline performance
  2. Ablation test: Compare relation-aware quadruplet vs standard triplet loss
  3. Memory test: Evaluate impact of meta-learned margins vs fixed margins

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the relation-aware quadruplet loss perform when applied to datasets with more complex visual variations, such as sketches of highly stylized or abstract objects?
- Basis in paper: [inferred] The paper demonstrates effectiveness on Sketchy and TU-Berlin datasets but does not explore more abstract or stylized sketch variations.
- Why unresolved: The current evaluation focuses on object-level sketches, leaving uncertainty about performance with abstract or highly stylized sketches that may deviate significantly from their corresponding photos.
- What evidence would resolve it: Testing on datasets containing abstract or stylized sketches (e.g., QuickDraw with more varied drawing styles) and comparing retrieval performance with baseline methods.

### Open Question 2
- Question: What is the impact of using different memory update strategies in the meta-learning component, such as replacing the current weighted sum approach with attention-based memory reading?
- Basis in paper: [explicit] The paper mentions using weighted summation for memory updates but does not explore alternative memory reading mechanisms.
- Why unresolved: The effectiveness of different memory update strategies on margin optimization remains unexplored, potentially limiting the method's adaptability to diverse data distributions.
- What evidence would resolve it: Implementing and comparing alternative memory reading mechanisms (e.g., attention-based) and measuring their impact on retrieval performance across different datasets.

### Open Question 3
- Question: How does the proposed method scale to datasets with significantly larger numbers of classes or higher intra-class variability?
- Basis in paper: [inferred] The current experiments use datasets with 125-250 classes, but the scalability to larger-scale problems is not addressed.
- Why unresolved: The memory-based meta-learning approach may face challenges with computational complexity and memory requirements as the number of classes increases substantially.
- What evidence would resolve it: Evaluating performance on larger-scale sketch-photo datasets (e.g., expanded versions with thousands of classes) and analyzing computational requirements and retrieval accuracy trends.

## Limitations

- The evaluation focuses on only two datasets (Sketchy Extended and TU-Berlin Extended), which may not generalize to other sketch-photo retrieval scenarios
- The method's computational complexity increases with the use of two negative samples and meta-learning for margin optimization, potentially limiting scalability
- While the paper claims strong generalization to unseen categories, the evaluation protocol may not fully capture the challenges of real-world deployment where sketch-photo pairs can vary significantly in style and quality

## Confidence

**High Confidence**: The core contribution of using a relation-aware quadruplet loss with two negative samples from different modalities is well-supported by both theoretical reasoning and empirical results.

**Medium Confidence**: The meta-learning approach for margin optimization shows promise, but the paper could provide more detailed analysis of how the learned margins vary across categories and whether they consistently improve performance across different experimental settings.

**Medium Confidence**: The combination of classification loss with metric learning is a reasonable approach, but the paper doesn't fully explore the impact of different weighting schemes between these losses or provide detailed analysis of the optimization dynamics.

## Next Checks

1. **Generalization Test**: Evaluate the method on additional sketch-photo datasets beyond Sketchy and TU-Berlin to assess cross-dataset generalization.

2. **Efficiency Analysis**: Conduct a detailed computational complexity analysis comparing the relation-aware quadruplet loss with standard triplet loss, including training time, memory usage, and inference speed.

3. **Robustness Evaluation**: Test the method's performance across varying sketch qualities and styles, including highly abstract or poorly drawn sketches.