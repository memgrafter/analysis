---
ver: rpa2
title: 'Maybe you are looking for CroQS: Cross-modal Query Suggestion for Text-to-Image
  Retrieval'
arxiv_id: '2412.13834'
source_url: https://arxiv.org/abs/2412.13834
tags:
- query
- suggestion
- images
- cross-modal
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of cross-modal query suggestion,
  which aims to suggest minimal textual modifications to an initial query to explore
  visually consistent subsets of an image collection. The authors propose CroQS, a
  benchmark dataset with 50 initial queries and 295 semantic clusters of images, each
  associated with human-annotated query suggestions.
---

# Maybe you are looking for CroQS: Cross-modal Query Suggestion for Text-to-Image Retrieval

## Quick Facts
- arXiv ID: 2412.13834
- Source URL: https://arxiv.org/abs/2412.13834
- Authors: Giacomo Pacini; Fabio Carrara; Nicola Messina; Nicola Tonellotto; Giuseppe Amato; Fabrizio Falchi
- Reference count: 30
- Primary result: LLM-based and captioning-based methods significantly improve cluster specificity and representativeness in cross-modal query suggestion, with LLM-based approaches achieving better similarity to original queries

## Executive Summary
This paper introduces cross-modal query suggestion, a novel task for text-to-image retrieval that aims to suggest minimal textual modifications to an initial query to explore visually consistent subsets of an image collection. The authors present CroQS, a benchmark dataset with 50 initial queries and 295 semantic clusters, each associated with human-annotated query suggestions. Baseline methods adapted from image captioning and content summarization are evaluated on CroQS, demonstrating significant improvements in cluster specificity and representativeness compared to the initial query. The work establishes a foundation for further research in query refinement for image retrieval systems.

## Method Summary
The proposed method follows a pipeline approach: starting with an initial textual query, a cross-modal retrieval model retrieves a result set of images, which are then clustered into semantic groups based on their embeddings. For each cluster, adapted baseline methods generate suggested queries - either through prototype captioning (DeCap and ClipCap) or LLM-based summarization of image captions with query conditioning (GroupCap). The suggested queries are evaluated using three metrics: Cluster Specificity (measuring recall of cluster images), Representativeness (measuring MAP, NDCG, and recall), and Similarity to Original Query (measuring syntactic and semantic similarity to the initial query).

## Key Results
- Both LLM-based and captioning-based methods improve recall on cluster specificity by more than 115% and representativeness mAP by more than 52% compared to initial queries
- LLM-based approaches achieve better similarity to original queries while maintaining competitive performance on specificity and representativeness
- The CroQS benchmark enables standardized evaluation of cross-modal query suggestion methods
- The task effectively addresses the challenge of refining broad initial queries into more specific, semantically coherent queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal query suggestion improves retrieval performance by generating semantically refined queries that better match image clusters than the original broad query.
- Mechanism: The system uses cross-modal retrieval to find a result set, clusters this result set into semantically coherent groups, and generates queries specific to each cluster for more precise retrieval.
- Core assumption: Result sets contain distinct semantic groups that can be meaningfully clustered, and cluster-specific queries will improve retrieval metrics.
- Evidence anchors: Experiments show 115% improvement in recall on cluster specificity and 52% improvement in representativeness mAP.

### Mechanism 2
- Claim: The CroQS benchmark enables objective comparison of cross-modal query suggestion methods.
- Mechanism: By providing 50 initial queries, 295 human-annotated clusters, and suggested queries, the benchmark removes variability in clustering and human judgment.
- Core assumption: Human annotations represent valid semantic groupings and appropriate query refinements.
- Evidence anchors: The benchmark standardizes evaluation, though direct citations to CroQS are absent in neighbor papers.

### Mechanism 3
- Claim: Adapting image captioning and LLM methods leverages their ability to generate descriptive text from visual input while addressing query suggestion requirements.
- Mechanism: Image captioning models generate queries from cluster prototypes, while LLM-based methods summarize captions with initial query conditioning.
- Core assumption: Adapted models can generate text descriptive of images and appropriately refined from the initial query.
- Evidence anchors: Baseline methods show competitive results, with LLM-based approaches achieving better similarity to original queries.

## Foundational Learning

- Concept: Cross-modal retrieval (e.g., using models like CLIP)
  - Why needed here: The task relies on retrieving images based on textual queries and representing images and text in a shared embedding space for clustering and evaluation.
  - Quick check question: How does a cross-modal retrieval model like CLIP embed text and images into a shared space, and how is similarity between a query and an image computed?

- Concept: Clustering algorithms and evaluation metrics
  - Why needed here: The result set must be partitioned into semantic clusters, and cluster quality and suggested query quality must be evaluated.
  - Quick check question: What are some common clustering algorithms that could be used on image embeddings, and how do the proposed metrics differ from standard clustering evaluation metrics?

- Concept: Image captioning and large language models
  - Why needed here: Baseline methods for generating suggested queries are adapted from image captioning and LLM-based summarization techniques.
  - Quick check question: How do image captioning models generate descriptive text from an image, and how can large language models be prompted to summarize information or generate text based on a given context?

## Architecture Onboarding

- Component map: Initial Query -> Cross-modal Retrieval -> Clustering -> Query Suggestion Generation -> Evaluation
- Critical path: Initial Query → Cross-modal Retrieval → Clustering → Query Suggestion Generation → Evaluation
- Design tradeoffs:
  - Clustering granularity: Fine-grained clustering may lead to more specific suggestions but could fragment the result set unnecessarily.
  - Conditioning strength: Strong conditioning may lead to suggestions too similar to original, while weak conditioning may lead to irrelevant suggestions.
  - Model complexity: More complex models may generate better suggestions but are more computationally expensive.
- Failure signatures:
  - Low Cluster Specificity: Suggested queries not effectively distinguishing between clusters
  - Low Representativeness: Suggested queries not effectively retrieving images from their respective clusters
  - Low Similarity to Original Query: Suggested queries diverging too much from user's original intent
  - High variance in metrics across clusters: Model performing well on some query types but poorly on others
- First 3 experiments:
  1. Baseline Evaluation: Run adapted captioning and LLM models on CroQS benchmark and report three evaluation metrics.
  2. Ablation on Conditioning: Compare query suggestion models with and without conditioning on initial query.
  3. Cluster Granularity Study: Vary number of clusters and evaluate impact on query suggestion performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of clustering algorithm impact the quality of query suggestions in cross-modal retrieval?
- Basis in paper: [explicit] The authors mention using off-the-shelf clustering algorithms but do not explore how different algorithms affect results.
- Why unresolved: The paper focuses on generating query suggestions after clustering, not on the clustering process itself.
- What evidence would resolve it: Systematic comparison of multiple clustering algorithms (e.g., k-means, DBSCAN, hierarchical clustering) applied to the same dataset, measuring impact on final query suggestion quality metrics.

### Open Question 2
- Question: Can cross-modal query suggestion be effectively extended to video retrieval tasks?
- Basis in paper: [inferred] The authors discuss cross-modal retrieval and mention video browsing competitions, suggesting potential application to video domains.
- Why unresolved: The current work focuses on image collections. Video data introduces temporal dynamics that might require different approaches.
- What evidence would resolve it: Implementation and evaluation of cross-modal query suggestion methods on video datasets, comparing performance to image-based approaches.

### Open Question 3
- Question: How does the size of the image collection affect the effectiveness of cross-modal query suggestion?
- Basis in paper: [explicit] The authors mention "potentially large set of unannotated images" but do not study how collection size impacts results.
- Why unresolved: The benchmark uses COCO train split (118,000 images). Scaling to much larger collections might change effectiveness of different approaches.
- What evidence would resolve it: Experiments comparing query suggestion performance across datasets of varying sizes, from small collections to web-scale image datasets.

## Limitations
- The exact clustering methodology and parameters used to generate semantic clusters are not fully specified
- Implementation details such as specific LLM prompts for GroupCap and prototype selection strategy for captioning models require clarification
- Evaluation metrics may have limitations in capturing the full complexity of query suggestion quality

## Confidence
- High Confidence: The core task definition (cross-modal query suggestion) and three proposed evaluation metrics are clearly articulated and theoretically sound
- Medium Confidence: Baseline method adaptations from image captioning and LLM summarization are reasonable and follow established practices
- Medium Confidence: Claim that LLM-based methods outperform captioning-based methods is supported by results, but margin and significance could be further analyzed

## Next Checks
1. Implement and compare the baseline methods: Reproduce prototype captioning (DeCap and ClipCap) and LLM-based (GroupCap) approaches on CroQS benchmark to verify reported performance improvements.
2. Analyze the impact of clustering granularity: Systematically vary number of clusters formed from result sets and evaluate impact on quality of generated query suggestions.
3. Conduct a user study: Evaluate generated query suggestions with human users to assess practical utility in refining search results, complementing automated evaluation metrics.