---
ver: rpa2
title: A Thorough Comparison of Cross-Encoders and LLMs for Reranking SPLADE
arxiv_id: '2403.10407'
source_url: https://arxiv.org/abs/2403.10407
tags:
- u1d458
- gpt-4
- llms
- rerankers
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares cross-encoders and Large Language Models (LLMs)
  as rerankers for SPLADE retrievers, focusing on effectiveness and efficiency trade-offs.
  Experiments on TREC Deep Learning datasets and out-of-domain benchmarks like BEIR
  and LoTTE reveal that cross-encoders remain competitive with LLM-based rerankers,
  especially when efficiency is prioritized.
---

# A Thorough Comparison of Cross-Encoders and LLMs for Reranking SPLADE

## Quick Facts
- arXiv ID: 2403.10407
- Source URL: https://arxiv.org/abs/2403.10407
- Reference count: 40
- Primary result: Cross-encoders remain competitive with LLM-based rerankers for SPLADE retrieval, especially when efficiency is prioritized

## Executive Summary
This study compares cross-encoders and Large Language Models (LLMs) as rerankers for SPLADE retrievers, focusing on effectiveness and efficiency trade-offs. Experiments on TREC Deep Learning datasets and out-of-domain benchmarks like BEIR and LoTTE reveal that cross-encoders remain competitive with LLM-based rerankers, especially when efficiency is prioritized. GPT-4 demonstrates strong zero-shot performance but at high computational cost. Cross-encoders, particularly DeBERTa-v3, consistently improve retrieval effectiveness in out-of-domain settings. Increasing the number of reranked documents enhances performance, particularly for weaker models. Overall, LLM-based rerankers offer potential but require careful consideration of resource constraints and dataset characteristics.

## Method Summary
The study compares cross-encoders (DeBERTa-v3, ELECTRA) and LLMs (GPT-3.5, GPT-4) as rerankers for SPLADE-v3, SPLADE-v3-DistilBERT, and SPLADE-v3-Doc retrievers. Models are applied to TREC DL datasets and out-of-domain benchmarks (BEIR, LoTTE). Reranking is performed using top-k document sets (k=50, 100, 200) with sliding window and truncation strategies for LLMs. Effectiveness is measured using nDCG@10. The study evaluates zero-shot LLM reranking and fine-tuned cross-encoders, comparing their performance across different document lengths and retrieval strengths.

## Key Results
- Cross-encoders remain competitive with LLM-based rerankers, especially when efficiency is prioritized
- GPT-4 demonstrates strong zero-shot performance but at high computational cost
- Increasing the number of reranked documents enhances performance, particularly for weaker models
- DeBERTa-v3 consistently improves effectiveness over ELECTRA in out-of-domain settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Cross-encoders improve effectiveness when reranking results from strong SPLADE retrievers, especially in out-of-domain settings.
- **Mechanism**: Cross-encoders perform deep pairwise analysis of query-document pairs, allowing them to capture nuanced relevance signals that initial sparse or sparse-dense retrievers might miss.
- **Core assumption**: The reranker is applied to a manageable number of top-ranked documents (e.g., top 50-200), where deep analysis is computationally feasible.
- **Evidence anchors**:
  - [abstract] "cross-encoders remain competitive with LLM-based rerankers, especially when efficiency is prioritized"
  - [section 4.2.2] "In out-of-domain, we observe a clear 'winner': DeBERTa-v3 consistently improves over the ELECTRA-based model"
  - [corpus] Weak evidence - no direct comparison in cited papers
- **Break condition**: If the number of documents to rerank is too large, cross-encoders become computationally prohibitive.

### Mechanism 2
- **Claim**: GPT-4 demonstrates strong zero-shot performance as a listwise reranker but at high computational cost.
- **Mechanism**: GPT-4 uses its generative capabilities to directly output an ordered list of document IDs based on a ranked prompt, leveraging its large context window and reasoning abilities.
- **Core assumption**: The prompt can be constructed to include enough document context for the model to make informed ranking decisions.
- **Evidence anchors**:
  - [abstract] "GPT-4 demonstrates impressive (zero-shot) performance, we show that traditional cross-encoders remain very competitive"
  - [section 3.3.3] "The output of the model... is a list of document identifiers ordered by decreasing relevance"
  - [corpus] Weak evidence - no direct comparison in cited papers
- **Break condition**: If the cost per ranking is too high, or if the model fails to follow formatting instructions, performance degrades.

### Mechanism 3
- **Claim**: Increasing the number of documents to rerank improves effectiveness, particularly for weaker models.
- **Mechanism**: More documents in the reranking pool increase the likelihood of including highly relevant documents that were initially ranked lower by the retriever.
- **Core assumption**: The reranker is capable of accurately identifying relevant documents even when they appear lower in the initial ranking.
- **Evidence anchors**:
  - [abstract] "Increasing the number of reranked documents enhances performance, particularly for weaker models"
  - [section 4.1.3] "Using top_k = 50 as a reference... we observe that the smallest model (SPLADE-v3-DistilBERT) benefits the most from the increase of top_k"
  - [corpus] Weak evidence - no direct comparison in cited papers
- **Break condition**: If the reranker's capacity is exceeded (e.g., prompt length limits), effectiveness gains plateau or reverse.

## Foundational Learning

- **Concept**: Cross-encoders vs. other reranker architectures (e.g., pointwise, pairwise, listwise)
  - Why needed here: The paper compares cross-encoders to LLM-based listwise rerankers, requiring understanding of their differences.
  - Quick check question: What is the key architectural difference between a cross-encoder and a pointwise reranker?

- **Concept**: SPLADE retrieval model and its variants
  - Why needed here: The study uses SPLADE-v3, SPLADE-v3-DistilBERT, and SPLADE-v3-Doc as initial retrievers, so understanding their characteristics is crucial.
  - Quick check question: How does SPLADE-v3-DistilBERT differ from SPLADE-v3 in terms of model size and expected performance?

- **Concept**: Zero-shot vs. fine-tuned reranking
  - Why needed here: The paper evaluates GPT-4 as a zero-shot reranker and compares it to fine-tuned cross-encoders.
  - Quick check question: What is the main advantage of zero-shot reranking compared to fine-tuning?

## Architecture Onboarding

- **Component map**: SPLADE retriever (first-stage) → Cross-encoder or LLM reranker (second-stage) → Final ranked list
- **Critical path**: Retriever outputs top-k documents → Reranker processes query-document pairs or full document list → Reranker outputs reordered list
- **Design tradeoffs**:
  - Reranking depth (k): Higher k improves recall but increases computational cost
  - Reranker model size: Larger models may improve accuracy but reduce efficiency
  - Zero-shot vs. fine-tuning: Zero-shot offers flexibility but may underperform fine-tuned models on specific tasks
- **Failure signatures**:
  - Cross-encoders: Degradation when k is too large, inability to process long documents
  - LLM rerankers: High cost, prompt length limitations, formatting errors in output
- **First 3 experiments**:
  1. Rerank top 50 documents from SPLADE-v3 using DeBERTa-v3 cross-encoder; measure nDCG@10
  2. Rerank same documents using GPT-4 with sliding window; measure nDCG@10 and cost
  3. Increase k to 200 for both models; measure changes in effectiveness and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of LLM-based rerankers scale with increasing document length and complexity across different domains?
- Basis in paper: [inferred] The paper mentions that document truncation and title inclusion impact LLM performance differently than cross-encoders, but doesn't systematically explore scaling with document complexity.
- Why unresolved: The study focuses on MS MARCO passages and TREC-COVID, which have relatively short documents. The impact of longer, more complex documents on LLM reranking effectiveness remains unexplored.
- What evidence would resolve it: Experiments testing LLM rerankers on datasets with varying document lengths and complexity levels, measuring performance degradation and efficiency trade-offs.

### Open Question 2
- Question: What is the optimal number of documents to rerank for different first-stage retriever strengths and dataset characteristics?
- Basis in paper: [explicit] The paper notes that increasing the number of documents to rerank improves performance, especially for weaker retrievers, but this effect varies across datasets.
- Why unresolved: The study only tests a limited range of reranking depths (50, 100, 200) and doesn't provide a systematic method for determining the optimal number for different scenarios.
- What evidence would resolve it: Experiments varying reranking depths across diverse datasets and retriever strengths, identifying patterns or developing a model to predict optimal reranking depth.

### Open Question 3
- Question: How do different prompt engineering strategies impact the effectiveness and efficiency of LLM-based rerankers across various tasks?
- Basis in paper: [explicit] The paper uses a fixed prompt template inspired by RankGPT but doesn't explore alternative prompt strategies or their impact on performance.
- Why unresolved: The study only tests one prompt format, leaving the potential benefits of optimized prompts unexplored.
- What evidence would resolve it: Systematic comparison of different prompt engineering approaches (e.g., different instruction formats, examples, or few-shot learning) across multiple reranking tasks, measuring both effectiveness and computational efficiency.

## Limitations
- The study does not provide detailed cost-effectiveness analyses comparing different model sizes or reranking depths
- Evaluation focuses primarily on effectiveness metrics (nDCG@10) without extensive efficiency measurements or resource utilization comparisons
- Limited exploration of scaling behavior with document complexity and length

## Confidence

- **High confidence**: Cross-encoders remain competitive with LLMs for reranking, especially when efficiency is prioritized
- **Medium confidence**: GPT-4 shows strong zero-shot performance but at prohibitive computational cost
- **Medium confidence**: Increasing reranking depth improves effectiveness, particularly for weaker retrievers

## Next Checks
1. **Cost-effectiveness analysis**: Measure and compare computational costs (API calls, inference time, energy consumption) across different reranking configurations and model sizes to validate efficiency claims.

2. **Scaling behavior verification**: Test reranking performance across a broader range of top-k values (e.g., k=10, 50, 100, 200, 500) to confirm the diminishing returns pattern and identify optimal reranking depths.

3. **Generalization testing**: Evaluate the same reranking approaches on additional out-of-domain datasets beyond BEIR and LoTTE to verify the robustness of cross-encoder advantages in diverse retrieval scenarios.