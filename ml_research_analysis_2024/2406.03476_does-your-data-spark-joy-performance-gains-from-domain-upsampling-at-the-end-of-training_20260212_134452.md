---
ver: rpa2
title: Does your data spark joy? Performance gains from domain upsampling at the end
  of training
arxiv_id: '2406.03476'
source_url: https://arxiv.org/abs/2406.03476
tags:
- data
- domain
- datasets
- training
- upsampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a data intervention technique called domain
  upsampling, which involves upsampling domain-specific datasets relative to Common
  Crawl at the end of training to improve model performance on challenging benchmarks.
  The authors demonstrate that domain upsampling can boost model performance on difficult
  tasks like MMLU, GSM8K, and HumanEval by up to 6.90, 8.26, and 6.17 percentage points
  respectively, relative to the base data mix.
---

# Does your data spark joy? Performance gains from domain upsampling at the end of training

## Quick Facts
- arXiv ID: 2406.03476
- Source URL: https://arxiv.org/abs/2406.03476
- Reference count: 40
- One-line primary result: Domain upsampling at the end of training improves performance on challenging benchmarks by up to 6.90 percentage points on MMLU, 8.26 on GSM8K, and 6.17 on HumanEval.

## Executive Summary
This paper introduces domain upsampling, a technique that improves model performance on challenging benchmarks by increasing the relative frequency of domain-specific data during the final training phase. The authors demonstrate that by starting with a checkpoint at 80% of training and continuing with an upsampled domain-specific mix for the remaining 20%, models can achieve significant gains on difficult tasks like MMLU, GSM8K, and HumanEval. They also show that this technique can be used to characterize the impact of individual datasets on model capabilities by removing them during the upsampling phase, providing a cost-effective alternative to full pretraining runs.

## Method Summary
The method involves training a 7B parameter model for 1 trillion tokens using a baseline mix of publicly available datasets. After 80% of training (800B tokens), the data mix is modified to upsample domain-specific datasets relative to Common Crawl for the remaining 200B tokens. The optimal duration of this upsampling phase is ablated between 5% and 30% of training, with 10-20% found to be optimal. To characterize dataset impact, the authors remove specific dataset subsets during the upsampling phase and measure the resulting performance changes on targeted benchmarks.

## Key Results
- Domain upsampling improves MMLU performance by up to 6.90 percentage points relative to base data mix
- GSM8K performance increases by up to 8.26 percentage points with domain upsampling
- HumanEval coding benchmark improves by up to 6.17 percentage points
- Optimal upsampling duration is 10-20% of total training time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain upsampling boosts performance on targeted benchmarks by increasing the relative frequency of domain-specific tokens in the training distribution during the final training phase.
- Mechanism: During the last portion of training, the model is exposed to a higher proportion of domain-specific data relative to Common Crawl, allowing it to refine its representations for those domains without forgetting general capabilities built earlier.
- Core assumption: The model has already learned general language modeling capabilities in the first 80% of training, and can specialize further without catastrophic forgetting.
- Evidence anchors:
  - [abstract]: "upsampling them relative to CC at the end of training to drive performance improvements on difficult benchmarks"
  - [section]: "start with a checkpoint at 0.8T tokens of training, change the mixing proportions of our pretraining data mix, and continue training for the remaining 0.2T tokens"
- Break condition: If domain-specific data is too small or poorly filtered, the model may overfit to noise rather than gain useful specialization.

### Mechanism 2
- Claim: Domain upsampling is an efficient tool for characterizing dataset impact because it isolates the contribution of specific data subsets to downstream task performance.
- Mechanism: By removing certain datasets during the upsampling phase, researchers can measure the performance drop on targeted benchmarks, attributing that drop to the removed data's contribution.
- Core assumption: The upsampling phase is long enough to reveal measurable changes in emergent task performance but short enough to avoid full retraining costs.
- Evidence anchors:
  - [abstract]: "use domain upsampling to characterize at scale the utility of individual datasets for improving various benchmarks by removing them during this final phase of training"
  - [section]: "we repeat our experiment, applying domain upsampling for the last 10% of the training duration. We keep our dataset proportions identical to those in Table 4, but remove the math related subsets"
- Break condition: If the removed datasets are not critical for the targeted tasks, the performance difference may be negligible, making the experiment uninformative.

### Mechanism 3
- Claim: Domain upsampling allows navigation of the trade-off between general language modeling and domain-specific performance by varying the duration of the intervention.
- Mechanism: Short upsampling durations preserve general capabilities while still providing targeted gains; longer durations improve domain-specific tasks at the cost of general performance.
- Core assumption: The model's optimization trajectory can be steered by controlled exposure to different data distributions without destabilizing earlier learned representations.
- Evidence anchors:
  - [abstract]: "ablating the duration of domain upsampling from 5% to 30% of training reveals that 10% to 20% is optimal for balancing general language modeling capabilities and targeted benchmarks"
  - [section]: "we ablate the percentage of training that utilizes domain upsampling and show 10%-20% is optimal"
- Break condition: If the model is too far into training, even small data distribution changes may cause catastrophic forgetting of general capabilities.

## Foundational Learning

- Concept: Emergent task performance
  - Why needed here: The paper relies on the idea that certain benchmarks only show meaningful signal after sufficient FLOP scale, making smaller-scale experiments misleading.
  - Quick check question: Why might a 7B model trained for 100B tokens perform at random on MMLU, while the same model at 1T tokens shows clear improvement?

- Concept: Data distribution shift
  - Why needed here: Domain upsampling intentionally shifts the training data distribution in the final training phase; understanding how models handle such shifts is critical.
  - Quick check question: What could happen if the domain-specific data distribution is very different from the general pretraining data distribution?

- Concept: Catastrophic forgetting
  - Why needed here: The technique assumes that the model can specialize on domain data in the final phase without losing general language capabilities acquired earlier.
  - Quick check question: What training strategies can help mitigate catastrophic forgetting when introducing a new data distribution late in training?

## Architecture Onboarding

- Component map: Tokenizer (GPT-4 tiktoken) -> Model (MPT decoder-only transformer) -> Positional embedding (ALiBi) -> Optimizer (LionW) -> Learning rate schedule (inverse square root) -> Sequence length (4096) -> Batch size (960)

- Critical path: 1. Pretrain base model on mixed data for 80% of total tokens 2. Switch to domain upsampled mix for final 20% 3. Evaluate on targeted and general benchmarks

- Design tradeoffs: Upsampling duration vs. general capability retention, Domain-specific data quality vs. quantity, Computational cost of full pretraining vs. partial domain upsampling

- Failure signatures: General capabilities degrade significantly after upsampling, Targeted benchmarks show no improvement despite domain upsampling, Model overfits to small domain datasets and generalizes poorly

- First 3 experiments: 1. Replicate baseline training (no domain upsampling) to confirm scaling behavior 2. Apply domain upsampling for final 10% and measure performance changes 3. Remove a specific dataset subset during upsampling and measure its impact on targeted benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different proportions of domain-specific data versus Common Crawl data affect model performance on niche tasks?
- Basis in paper: [explicit] The paper discusses the impact of upsampling domain-specific datasets relative to Common Crawl at the end of training.
- Why unresolved: The paper suggests that domain upsampling can boost performance on challenging benchmarks, but it does not explore the optimal balance between domain-specific data and Common Crawl data for various niche tasks.
- What evidence would resolve it: Experiments comparing models trained with different proportions of domain-specific data versus Common Crawl data on a variety of niche tasks would provide insights into the optimal balance.

### Open Question 2
- Question: What is the long-term impact of domain upsampling on model generalization to unseen domains?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of domain upsampling in improving performance on specific benchmarks but does not address the potential trade-offs in generalization.
- Why unresolved: While domain upsampling improves performance on targeted tasks, it is unclear whether this improvement comes at the cost of the model's ability to generalize to new, unseen domains.
- What evidence would resolve it: Long-term studies comparing the generalization capabilities of models trained with and without domain upsampling on a wide range of unseen domains would clarify the impact on generalization.

### Open Question 3
- Question: How does the timing of domain upsampling (e.g., earlier vs. later in training) affect model performance and efficiency?
- Basis in paper: [explicit] The paper explores the effects of applying domain upsampling during the last 20% of training and ablates the duration of upsampling.
- Why unresolved: The paper focuses on the impact of domain upsampling at the end of training but does not investigate whether applying it earlier in the training process would yield different results or efficiencies.
- What evidence would resolve it: Comparative studies of models trained with domain upsampling applied at different stages of training (e.g., early, middle, and late) would provide insights into the optimal timing for upsampling.

## Limitations

- Data Composition and Quality: The effectiveness of domain upsampling may vary significantly depending on the quality and representativeness of the upsampled datasets, but the paper provides limited details about dataset curation and filtering.
- General Specialization Trade-off: The reported optimal duration (10-20%) may be model-scale and task-dependent, and the results may not generalize to larger models or different architectural choices.
- Benchmark Sensitivity: The technique's effectiveness on open-ended generation, few-shot learning, or non-academic benchmarks remains untested beyond the specific benchmarks evaluated.

## Confidence

**High Confidence**: The basic mechanism of domain upsampling is well-established and the observed performance improvements on tested benchmarks are statistically significant and reproducible.

**Medium Confidence**: The optimal duration range (10-20%) is supported by ablation studies but may vary depending on model size, data quality, and task difficulty.

**Low Confidence**: Claims about general applicability across different model scales and domains are speculative without broader validation.

## Next Checks

1. **Scale Transfer Experiment**: Apply the same domain upsampling methodology to a 70B parameter model and compare the optimal duration and performance gains against the 7B results to test whether the 10-20% recommendation holds across scales.

2. **Cross-Domain Generalization**: Implement domain upsampling using datasets from a completely different domain (e.g., biomedical or legal text) and evaluate performance on domain-specific benchmarks not mentioned in the original paper to validate the technique's broader applicability.

3. **Long-term Stability Analysis**: Train models with different upsampling durations and evaluate their performance at multiple time points (e.g., 1 week, 1 month, 3 months post-training) to assess whether the gains are stable or decay over time.