---
ver: rpa2
title: On the Robustness of Global Feature Effect Explanations
arxiv_id: '2406.09069'
source_url: https://arxiv.org/abs/2406.09069
tags:
- feature
- layer
- robustness
- effects
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the robustness of global feature effect explanations
  (partial dependence plots and accumulated local effects) to data and model perturbations.
  The authors derive theoretical bounds for the maximum change in explanations under
  perturbations, relating the distance between explanations to the distance between
  data distributions or models.
---

# On the Robustness of Global Feature Effect Explanations

## Quick Facts
- **arXiv ID:** 2406.09069
- **Source URL:** https://arxiv.org/abs/2406.09069
- **Reference count:** 40
- **Key outcome:** This paper analyzes the robustness of global feature effect explanations (partial dependence plots and accumulated local effects) to data and model perturbations.

## Executive Summary
This paper presents a theoretical and empirical analysis of the robustness of global feature effect explanations (partial dependence plots and accumulated local effects) to data and model perturbations. The authors derive theoretical bounds for the maximum change in explanations under perturbations, relating the distance between explanations to the distance between data distributions or models. Experiments on real-world datasets show that partial dependence is generally more robust to data perturbations than conditional dependence, and that accumulated local effects are significantly less affected by model perturbations compared to other feature effects. The work highlights the vulnerability of global explanations to adversarial attacks and emphasizes the need for more robust explanation methods.

## Method Summary
The authors derive theoretical bounds for the robustness of partial dependence plots and accumulated local effects to data and model perturbations. They use total variation distance to measure the change in feature effects due to data perturbations and norms of model differences or partial derivatives for model perturbations. The theoretical results are validated through experiments on real-world datasets, where the authors perturb the data using Gaussian noise and genetic algorithms, and perturb the models by randomizing their weights. The robustness of the explanations is quantified by computing the total variation distance between the original and perturbed feature effects.

## Key Results
- Partial dependence plots are generally more robust to data perturbations than conditional dependence.
- Accumulated local effects are significantly less affected by model perturbations compared to other feature effects.
- The robustness of global feature effects varies across different features and their importance to the model.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial data perturbations can significantly alter global feature effect explanations even when the model remains unchanged.
- **Mechanism:** The partial dependence plot (PDP) and accumulated local effects (ALE) are computed using the data distribution $p_X$. If an attacker manipulates the data used to estimate $p_X$, the resulting feature effects can deviate from the true effects. Theorems 2 and 4 provide theoretical bounds for this deviation in terms of the total variation distance between the original and perturbed distributions.
- **Core assumption:** The model predictions are bounded by a constant $B$ (for PDP and conditional dependence) or the model is Lipschitz continuous with constant $L$ (for ALE).
- **Evidence anchors:**
  - [abstract] "Experiments on real-world datasets show that partial dependence is generally more robust to data perturbations than conditional dependence, and that accumulated local effects are significantly less affected by model perturbations compared to other feature effects."
  - [section] "Theorem 2 gives an upper bound on the possible change of global feature effects in terms of distance between data distributions given model-specific constant $B$."
  - [corpus] Weak evidence. Corpus papers focus on related explainability topics but do not directly address adversarial robustness of global feature effects.
- **Break condition:** If the attacker cannot manipulate the data distribution used for estimating $p_X$, the bounds in Theorems 2 and 4 do not apply.

### Mechanism 2
- **Claim:** Model perturbations, such as fine-tuning or model randomization, can lead to changes in global feature effect explanations.
- **Mechanism:** The partial dependence plot and accumulated local effects are computed using the model $f$. If the model is perturbed to $f'$, the resulting feature effects can deviate from the original effects. Lemma 2 and Theorem 5 provide theoretical bounds for this deviation in terms of the norm between the original and perturbed models or their partial derivatives.
- **Core assumption:** The model perturbations are bounded by a norm (infinity norm for PDP and conditional dependence, or the norm between partial derivative functions for ALE).
- **Evidence anchors:**
  - [abstract] "The work highlights the vulnerability of global explanations to adversarial attacks and emphasizes the need for more robust explanation methods."
  - [section] "Theorem 5 extends these results to accumulated local effects."
  - [corpus] Weak evidence. Corpus papers focus on related explainability topics but do not directly address the robustness of global feature effects to model perturbations.
- **Break condition:** If the model perturbations are unbounded or the norms used in the bounds are not appropriate for the specific perturbation, the bounds in Lemma 2 and Theorem 5 do not apply.

### Mechanism 3
- **Claim:** The robustness of global feature effect explanations to data and model perturbations depends on the specific feature and its importance to the model.
- **Mechanism:** The experimental results in Section 5 show that the robustness of feature effects varies across different features and their quantile values. Features with higher importance (measured by the variance of feature effects) tend to be more robust to perturbations.
- **Core assumption:** The importance of features can be quantified using the variance of feature effects.
- **Evidence anchors:**
  - [abstract] "Experiments on real-world datasets show that partial dependence is generally more robust to data perturbations than conditional dependence, and that accumulated local effects are significantly less affected by model perturbations compared to other feature effects."
  - [section] "We explain three features $s$: the least, 'median' and most important to the model. We measure the importance of features with the variance of feature effects, i.e., higher variance means higher importance."
  - [corpus] Weak evidence. Corpus papers focus on related explainability topics but do not directly address the relationship between feature importance and robustness of global feature effects.
- **Break condition:** If the importance of features cannot be accurately quantified using the variance of feature effects, the experimental results may not hold for all features.

## Foundational Learning

- **Concept:** Total variation distance
  - **Why needed here:** The total variation distance is used in Theorems 2 and 4 to bound the deviation of global feature effects due to data perturbations.
  - **Quick check question:** What is the definition of the total variation distance between two probability distributions $p$ and $q$?
- **Concept:** Lipschitz continuity
  - **Why needed here:** The Lipschitz continuity of the model is assumed in Theorem 4 to bound the deviation of accumulated local effects due to data perturbations.
  - **Quick check question:** What is the definition of a Lipschitz continuous function $f$ with constant $L$?
- **Concept:** Partial derivatives
  - **Why needed here:** The partial derivatives of the model are used in Theorem 5 to bound the deviation of accumulated local effects due to model perturbations.
  - **Quick check question:** What is the definition of the partial derivative of a function $f(x_1, x_2, ..., x_p)$ with respect to the $i$-th variable?

## Architecture Onboarding

- **Component map:** Data -> Data Distribution Estimator -> $p_X$ -> Model $f$ -> Global Feature Effect Explainer -> Explanation
- **Critical path:** The critical path for computing global feature effects is: data $\rightarrow$ data distribution estimator $\rightarrow$ $p_X$ \rightarrow model $f$ $\rightarrow$ global feature effect explainer $\rightarrow$ explanation.
- **Design tradeoffs:** The choice of the data distribution estimator (e.g., kernel density estimation, Gaussian mixture models) and the model (e.g., neural networks, decision trees) can impact the accuracy and robustness of the global feature effects. Using more complex estimators or models may lead to more accurate explanations but may also be more sensitive to perturbations.
- **Failure signatures:** If the global feature effects are not robust to data or model perturbations, it may indicate that the data distribution estimator or the model is not appropriate for the given task. Additionally, if the bounds in Theorems 2, 4, and 5 are not tight, it may suggest that the assumptions made in the proofs are not valid for the specific scenario.
- **First 3 experiments:**
  1. Compute the global feature effects (PDP, conditional dependence, and ALE) for a simple synthetic dataset with known ground truth effects.
  2. Perturb the data distribution used for estimating $p_X$ and observe the change in the global feature effects. Compare the observed change to the bounds in Theorems 2 and 4.
  3. Perturb the model $f$ (e.g., by adding noise to its parameters) and observe the change in the global feature effects. Compare the observed change to the bounds in Lemma 2 and Theorem 5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does feature dependence (correlation and interactions) impact the robustness of global feature effects to model and data perturbation?
- Basis in paper: [inferred] The authors acknowledge this as an area for future work, stating that "Future theoretical and experimental work can analyze how feature dependence, e.g., correlation and interactions, impacts the robustness of global feature effects to model and data perturbation."
- Why unresolved: This is a complex relationship that requires both theoretical analysis and extensive experimentation across various datasets and model types.
- What evidence would resolve it: Theoretical bounds that incorporate feature correlation/interaction terms, and experimental results showing how different correlation structures affect the robustness bounds.

### Open Question 2
- Question: Can we improve the theoretical bounds for accumulated local effects to remove the assumption of Lipschitz continuity?
- Basis in paper: [explicit] The authors state in the limitations section that "Theorem 4 assumes model f is L-Lipschitz continuous and future work can improve the bound to remove this assumption."
- Why unresolved: The current bound relies on this assumption which doesn't hold for all model types (e.g., decision trees with infinite gradients).
- What evidence would resolve it: A new theoretical bound for accumulated local effects that doesn't require the Lipschitz continuity assumption, validated through experiments.

### Open Question 3
- Question: How does the choice of explanation estimator (e.g., marginal vs conditional vs accumulated) affect robustness to different types of perturbations?
- Basis in paper: [explicit] The experimental results show that "partial dependence (marginal) is more robust to data perturbation than conditional dependence" and that "accumulated local effects do not pass the model randomization test."
- Why unresolved: While the paper provides initial evidence, a comprehensive analysis across diverse datasets and perturbation types is needed to understand the general patterns.
- What evidence would resolve it: A systematic study comparing the robustness of different explanation estimators across various datasets, model types, and perturbation methods, potentially leading to guidelines for choosing the most robust estimator in different scenarios.

## Limitations
- The theoretical bounds rely on assumptions such as bounded predictions and Lipschitz continuity, which may not hold for all models and datasets.
- The experimental results are based on a limited number of datasets and model architectures, which may not generalize to other scenarios.
- The robustness of global feature effects is quantified using total variation distance, which may not capture all aspects of the change in explanations.

## Confidence
- **High confidence**: The general framework for analyzing the robustness of global feature effects to data and model perturbations, as presented in Theorems 2, 4, and 5.
- **Medium confidence**: The experimental results showing the relative robustness of partial dependence and accumulated local effects to data and model perturbations, as the results are based on a limited number of datasets and models.
- **Low confidence**: The relationship between feature importance and robustness of global feature effects, as the importance is quantified using the variance of feature effects, which may not be a comprehensive measure.

## Next Checks
1. **Expand the experimental evaluation**: Test the robustness of global feature effects on a wider range of datasets, model architectures, and perturbation methods to assess the generalizability of the results.
2. **Investigate the tightness of theoretical bounds**: Derive tighter bounds for the robustness of global feature effects by considering more specific assumptions about the model and data distributions, or by using alternative distance metrics.
3. **Explore alternative robustness measures**: Compare the total variation distance with other metrics, such as the Wasserstein distance or the maximum mean discrepancy, to assess the sensitivity of the results to the choice of distance measure.