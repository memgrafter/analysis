---
ver: rpa2
title: Why are Sensitive Functions Hard for Transformers?
arxiv_id: '2402.09963'
source_url: https://arxiv.org/abs/2402.09963
tags:
- blowup
- norm
- sensitivity
- functions
- parity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a theoretical explanation for why certain
  functions, like PARITY, are hard for transformers to learn, while others, like FIRST,
  are easy. The authors prove that transformers'' loss landscapes are constrained
  by the input-space sensitivity of the function being learned: highly sensitive functions
  (like PARITY) inhabit isolated points in parameter space, leading to very sharp
  minima.'
---

# Why are Sensitive Functions Hard for Transformers?

## Quick Facts
- arXiv ID: 2402.09963
- Source URL: https://arxiv.org/abs/2402.09963
- Authors: Michael Hahn; Mark Rofin
- Reference count: 40
- Key outcome: This paper provides a theoretical explanation for why certain functions, like PARITY, are hard for transformers to learn, while others, like FIRST, are easy.

## Executive Summary
This paper provides a theoretical explanation for why transformers struggle to learn sensitive functions like PARITY while easily learning low-sensitivity functions like FIRST. The authors prove that sensitive functions inhabit sharp minima in transformer parameter space, requiring large layer norm blowups that make them brittle to perturbations. This explains transformers' empirical difficulty with PARITY and their generalization bias towards low-sensitivity functions. The key result is that average sensitivity of learned functions is lower-bounded by a term involving layer norm blowup, making sensitive functions inherently harder to learn and less generalizable.

## Method Summary
The authors train transformer encoders on synthetic Boolean functions (PARITY, FIRST, MAJORITY, MEAN) at varying sequence lengths (4-500). They measure sharpness of minima using a perturbation-based metric, track parameter norms and layer norm blowup, and analyze the relationship between these quantities and function sensitivity. Training uses AdamW optimizer with MSE loss, and sharpness is computed by sampling perturbations from a Gaussian distribution.

## Key Results
- Transformers' loss landscapes are constrained by the input-space sensitivity of the function being learned
- Highly sensitive functions inhabit isolated points in parameter space, leading to very sharp minima
- The average sensitivity of a learned function is lower-bounded by a term involving average layer norm blowup
- Sensitive functions require large layer norm blowups or parameter norms, making them brittle to perturbations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sensitive functions (like PARITY) create very sharp minima in transformer parameter space.
- Mechanism: When a transformer computes a highly sensitive function, small perturbations to its parameters lead to large drops in sensitivity because sensitive functions require large layer norm blowups to be represented, and perturbations reduce this blowup, causing the function to become much less sensitive.
- Core assumption: The layer norm blowup τ(k)(x) is tightly coupled to the function's sensitivity.
- Evidence anchors: [abstract] "Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization." [section 6] "For PARITY, Lρ,n converges to 1 for large d, showing that arbitrarily small perturbations to a transformer computing PARITY will, in expectation, lead to a high loss for sufficiently long inputs."

### Mechanism 2
- Claim: There is a tradeoff between parameter norm and layer norm blowup for representing sensitive functions.
- Mechanism: Theorem 5 shows that at least one of parameter norm or squared layer norm blowup must be large to represent a sensitive function. As input length increases, both need to increase for highly sensitive functions like PARITY.
- Core assumption: Parameter norm and layer norm blowup contribute independently to expressive capacity.
- Evidence anchors: [section 4] "Our first theorem localizes the layer norm blowup to the Hamming neighborhoods of sensitive inputs... We then write Blowup(x) := ∏L k=1 τ(k)(x), an upper bound on the product of the successive layer-norm-induced blowups..." [section 7.2] "Tradeoff between Weight Norm and Layer Norm Blowup... By Theorem 5, the product of C and squared blowup is bounded from below with some value Bn(f)."

### Mechanism 3
- Claim: Transformers generalize with low sensitivity due to the low-sensitivity bias in the loss landscape.
- Mechanism: Theorem 6 shows that flat minima of the training loss generalize with bounded sensitivity. To the extent that gradient-based training tends to find flatter minima, this provides a theoretical justification for the empirical result that transformers' generalization behavior shows a strong bias towards low average sensitivity.
- Core assumption: Gradient-based training finds flatter minima.
- Evidence anchors: [section 6] "Another corollary is that, in expectation, the training loss landscape around an interpolating minimum places a constraint on a function's overall sensitivity... This means that, for long inputs, flat minima of the training loss generalize with bounded sensitivity." [section 7.2] "The Limits of Transformers' Generalization... Sharpness was indeed lower for the transformer fitting the extrapolated functions than for the transformer matching the original random functions f."

## Foundational Learning

- Concept: Average sensitivity of Boolean functions
  - Why needed here: It's the central metric used to characterize the difficulty of functions for transformers. Theorem 6 directly relates it to loss landscape sharpness.
  - Quick check question: What is the average sensitivity of the PARITY function on n-bit inputs?

- Concept: Layer norm blowup
  - Why needed here: It's the key mechanism by which transformers can represent sensitive functions, and its relationship to sensitivity is central to the theory.
  - Quick check question: How is the layer norm blowup τ(k)(x) defined for position i at layer k?

- Concept: Loss landscape sharpness
  - Why needed here: It's the quantity that Theorem 6 shows is lower-bounded by average sensitivity, explaining why sensitive functions are hard to learn.
  - Quick check question: How is the average direction sharpness Lρ,n defined in terms of parameter perturbations?

## Architecture Onboarding

- Component map: Token embeddings + positional encodings → Layer 0 → Multi-head attention + MLP + LayerNorm (repeated L times) → Hidden layers → Output projection → Final prediction
- Critical path: 1. Compute attention scores using key-query matrices 2. Apply softmax to get attention weights 3. Compute weighted sum of value vectors 4. Apply MLP with skip connection 5. Apply LayerNorm 6. Repeat for L layers 7. Project final activation to output
- Design tradeoffs: Deeper networks (larger L) can represent more complex functions but increase computational cost. Larger hidden dimensions (d) can reduce sharpness for sensitive functions but increase parameter count. Weight decay trades off parameter norm for LayerNorm blowup, affecting sharpness.
- Failure signatures: High loss that doesn't decrease during training. Very high sharpness values (>1) for sensitive functions. LayerNorm blowup that grows exponentially with input length. Parameter norms that don't decrease with weight decay.
- First 3 experiments: 1. Train a transformer on PARITY for n=10 and measure sharpness. Verify it's >1. 2. Train a transformer on FIRST for n=10 and measure sharpness. Verify it's close to 0. 3. Train a transformer on PARITY with and without weight decay. Verify the tradeoff between parameter norm and LayerNorm blowup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the tradeoff between parameter norm and LayerNorm blowup observed in the experiments for PARITY extend to all highly sensitive functions, or is it specific to PARITY?
- Basis in paper: [explicit] The paper states: "The tradeoff depends on the input length; blowup or parameter weights need to increase with the input length (in accordance with Corollary 5). This length dependency is not observed with low sensitivity functions."
- Why unresolved: While the paper demonstrates the tradeoff for PARITY and contrasts it with low-sensitivity functions, it does not explore other highly sensitive functions beyond PARITY.
- What evidence would resolve it: Experiments training transformers on other highly sensitive functions (e.g., MAJORITY for large n, or other functions with high average sensitivity) and measuring the relationship between parameter norm and LayerNorm blowup.

### Open Question 2
- Question: How does the sharpness of minima for sensitive functions change when using different training objectives or optimization algorithms beyond standard gradient descent?
- Basis in paper: [inferred] The paper focuses on the loss landscape and sharpness of minima for sensitive functions under the transformer architecture, but does not explore the impact of different training objectives or optimization algorithms.
- Why unresolved: The analysis in the paper is based on the assumption of standard gradient descent training. It is unclear how different training objectives (e.g., with regularization) or optimization algorithms (e.g., with momentum, adaptive learning rates) might affect the sharpness of minima for sensitive functions.
- What evidence would resolve it: Experiments training transformers on sensitive functions using different training objectives and optimization algorithms, and measuring the resulting sharpness of minima.

### Open Question 3
- Question: Can the inductive bias towards low sensitivity observed in transformers be attributed to other factors beyond the loss landscape sharpness, such as the initialization scheme or the architecture's inductive biases?
- Basis in paper: [explicit] The paper states: "We have proven that, under the transformer architecture, high sensitivity in input space can only be achieved in very sharp minima. Empirical results confirm the predictions of the theory."
- Why unresolved: While the paper provides a theoretical explanation for the low-sensitivity bias based on loss landscape sharpness, it does not rule out other potential contributing factors.
- What evidence would resolve it: Experiments manipulating the initialization scheme (e.g., using different distributions or schemes) or architectural choices (e.g., varying the number of layers, attention heads) and measuring their impact on the sensitivity of the learned functions.

## Limitations

- The theory's assumptions about layer norm blowup being tightly coupled to function sensitivity may not hold in all transformer variants
- Empirical validation is limited to synthetic Boolean functions, leaving open questions about real-world task applicability
- The sharpness metric assumes small perturbations and may not capture all aspects of loss landscape geometry

## Confidence

- **High confidence**: The mathematical proofs relating layer norm blowup to sensitivity are sound and the empirical observations of sharpness trends for PARITY vs FIRST are robust
- **Medium confidence**: The generalization to broader transformer architectures and real-world tasks is plausible but not directly tested
- **Low confidence**: The exact quantitative relationship between sharpness and generalization performance across diverse tasks remains to be validated

## Next Checks

1. **Architectural generalization test**: Verify the sensitivity-sharpness relationship holds for decoder transformers and transformers with different normalization schemes (e.g., RMSNorm instead of LayerNorm)

2. **Real task validation**: Test whether transformers learning sensitive functions on real-world datasets (e.g., text classification requiring attention to many positions) show the predicted high sharpness and layer norm blowup patterns

3. **Training dynamics analysis**: Track sharpness evolution during training across different optimizers and learning rate schedules to confirm the assumption that gradient descent finds flatter minima