---
ver: rpa2
title: 'PASTA: Controllable Part-Aware Shape Generation with Autoregressive Transformers'
arxiv_id: '2407.13677'
source_url: https://arxiv.org/abs/2407.13677
tags:
- part
- shape
- shapes
- object
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PASTA, a part-aware generative model for 3D
  shapes using autoregressive transformers. It addresses the problem of generating
  diverse and controllable 3D shapes by considering their underlying part-based structure.
---

# PASTA: Controllable Part-Aware Shape Generation with Autoregressive Transformers

## Quick Facts
- arXiv ID: 2407.13677
- Source URL: https://arxiv.org/abs/2407.13677
- Reference count: 40
- Primary result: PASTA outperforms existing methods in generating diverse, realistic 3D shapes while enabling controllable part-based editing

## Executive Summary
PASTA introduces a novel approach for generating 3D shapes using part-aware autoregressive transformers. The method generates objects as sequences of cuboidal primitives through an autoregressive transformer, then composes these into high-quality meshes using a blending network. The model is trained in two stages: first learning part arrangements from cuboid annotations, then refining geometry using explicit 3D mesh supervision. Results demonstrate superior performance over both part-based and non-part-based methods in generating more realistic and diverse 3D shapes while supporting versatile editing tasks like size-guided generation and part variations.

## Method Summary
PASTA consists of two main components: an autoregressive transformer that generates objects as sequences of labeled cuboids, and a blending network that composes these sequences into high-quality meshes. The model is trained in two stages - first training the generator using part annotations with scheduled sampling, then training the blending network using watertight meshes as 3D supervision. For conditional generation, the method uses CLIP embeddings to incorporate text or image guidance. The blending network employs a transformer decoder with cross-attention layers that allow each query point to attend to all part embeddings, generating implicit shapes without self-attention between points.

## Key Results
- Achieves MMD-CD scores of 0.63 on chairs and 0.74 on tables, outperforming state-of-the-art methods
- Maintains COV-CD scores above 0.8 across all categories, demonstrating high diversity
- User studies show 61% preference over baselines for shape quality and 68% for part-based editing capabilities
- Successfully enables controllable editing tasks including size-guided generation and part variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scheduled sampling improves generation by exposing the model to imperfect predictions during training
- Mechanism: During training, the model randomly uses its own predictions instead of always using ground-truth, making it more robust to errors during inference
- Core assumption: The model can learn to compensate for imperfect predictions if exposed to them during training
- Evidence anchors: [abstract] "we train our model using scheduled sampling [Bengio et al. 2015] and showcase that it significantly improves the generation performance of our model"

### Mechanism 2
- Claim: Two-stage training allows separate learning of part arrangements and geometry composition
- Mechanism: Stage 1 learns part distributions from cuboid annotations, Stage 2 refines geometry using high-quality meshes
- Core assumption: Learning part arrangements and composing them into high-quality geometry are separable tasks
- Evidence anchors: [abstract] "Our model is trained in two stages: First we train our autoregressive generative model using only annotated cuboidal parts as supervision and next, we train our blending network using explicit 3D supervision"

### Mechanism 3
- Claim: Transformer decoder with cross-attention effectively composes cuboids into implicit shapes
- Mechanism: Each query point attends to all part embeddings through cross-attention, understanding spatial relationships to generate coherent surfaces
- Core assumption: Cross-attention is sufficient for understanding spatial relationships without self-attention between points
- Evidence anchors: [section] "our blending network... implemented as a transformer decoder that combines the part sequences and synthesizes high quality implicit shapes"

## Foundational Learning

- Concept: Autoregressive modeling and teacher forcing
  - Why needed here: The object generator needs to learn sequential part generation, and teacher forcing is a standard technique for training autoregressive models
  - Quick check question: What is the difference between teacher forcing and scheduled sampling in training autoregressive models?

- Concept: Transformer architectures and attention mechanisms
  - Why needed here: Both components use transformer architectures relying on attention to process sequences and relationships
  - Quick check question: What is the difference between self-attention and cross-attention in transformer architectures?

- Concept: Implicit shape representations and occupancy networks
  - Why needed here: The blending network uses occupancy networks to represent shapes implicitly by predicting whether points are inside or outside surfaces
  - Quick check question: How does an occupancy network represent a 3D shape, and how is it different from explicit representations like meshes or voxels?

## Architecture Onboarding

- Component map: Object Generator -> Part Encoder -> Transformer Encoder -> Part Decoder -> Blending Network -> Output Mesh

- Critical path:
  1. Input: Bounding box and/or conditioning (text, image)
  2. Object Generator: Generates sequence of cuboids autoregressively
  3. Blending Network: Composes cuboids into implicit shape
  4. Output: High-quality 3D mesh

- Design tradeoffs:
  - Using cuboids for part representation: Simple and interpretable but may not capture fine geometric details
  - Two-stage training: Allows specialization but requires alignment between stages
  - Transformer without positional encoding: Models unordered sets of parts but might lose some spatial information

- Failure signatures:
  - Poor part arrangements: Object Generator might not be learning the part distribution correctly
  - Low-quality implicit shapes: Blending Network might not be composing the cuboids effectively
  - Inconsistent conditioning: The conditioning mechanism might not be properly integrated

- First 3 experiments:
  1. Generate objects from scratch with different bounding box sizes to test the Object Generator's ability to create diverse part arrangements
  2. Generate objects from partial inputs to test the Object Generator's ability to complete shapes
  3. Generate objects from text and image conditioning to test the conditioning mechanism and overall pipeline

## Open Questions the Paper Calls Out
1. How to extend PASTA to handle non-cuboidal parts, as cuboids may not capture fine geometric details and variations of part shapes.
2. How to extend PASTA to handle textures and materials, as the current method only focuses on geometry.
3. How to extend PASTA to handle more complex part hierarchies, as PartNet may not contain the full spectrum of fine-grained part hierarchies.

## Limitations
- Generalization to shape categories beyond chairs, tables, and lamps remains uncertain
- Reliance on cuboidal part representations may limit ability to capture fine geometric details
- Two-stage training introduces potential misalignment between part arrangements and geometry refinement

## Confidence
- **High Confidence**: Effectiveness of scheduled sampling in improving generation quality
- **High Confidence**: Superiority over existing methods in generating realistic and diverse shapes
- **Medium Confidence**: Transformer decoder's ability to compose cuboids through cross-attention
- **Medium Confidence**: Controllability through text and image conditioning

## Next Checks
1. Test PASTA on shape categories beyond chairs, tables, and lamps (e.g., vehicles, buildings) to assess generalization to different object structures
2. Replace cuboids with more complex part primitives (e.g., oriented bounding boxes) and evaluate impact on generation quality
3. Conduct in-depth analysis of failure cases, including shapes with complex part arrangements or fine geometric details, to identify limitations and potential improvements