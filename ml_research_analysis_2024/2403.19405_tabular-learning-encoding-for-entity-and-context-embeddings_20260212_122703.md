---
ver: rpa2
title: 'Tabular Learning: Encoding for Entity and Context Embeddings'
arxiv_id: '2403.19405'
source_url: https://arxiv.org/abs/2403.19405
tags:
- encoding
- data
- learning
- target
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the impact of different encoding techniques
  on entity and context embeddings in tabular learning. The study compares commonly
  used ordinal encoding with alternative methods, including one-hot, rarelabel, target,
  summary, and string similarity encodings.
---

# Tabular Learning: Encoding for Entity and Context Embeddings

## Quick Facts
- arXiv ID: 2403.19405
- Source URL: https://arxiv.org/abs/2403.19405
- Reference count: 40
- Primary result: String similarity encoding outperforms ordinal encoding in 9 out of 10 datasets, especially for multi-label classification

## Executive Summary
This paper investigates the impact of different encoding techniques on entity and context embeddings in tabular learning. The study systematically compares ordinal encoding with alternative methods including one-hot, rarelabel, target, summary, and string similarity encodings. Through experiments on 10 datasets, the research demonstrates that string similarity encoding consistently outperforms ordinal encoding, particularly in multi-label classification tasks. The transformer architecture further enhances performance for both ordinal and similarity encoding methods, though at the cost of increased computational complexity.

## Method Summary
The methodology involves discretizing continuous features using decision tree-based methods, then applying six different encoding techniques to predictors and targets. Two model architectures are trained: an entity model with embeddings and MLP blocks, and a context model that extends the entity model with a transformer encoder block. Models are trained for 10 epochs using binary cross-entropy loss and Adam optimizer, with evaluation based on F1-score and loss metrics. The study covers 10 datasets from the UCI Machine Learning Repository, including Adult, Mushroom, Bank, and others.

## Key Results
- String similarity encoding outperforms ordinal encoding in 9 out of 10 datasets for both entity and context models
- Transformer architecture improves performance for ordinal and similarity encoding in multi-label classification tasks
- String similarity encoding incurs significantly higher computational costs, particularly for high-cardinality predictors
- The encoding method choice has substantial impact on model performance, with string similarity showing the most consistent improvements

## Why This Works (Mechanism)

### Mechanism 1
String similarity encoding improves classification performance by preserving semantic relationships between categories, which ordinal encoding ignores. By computing a similarity matrix based on string comparisons (e.g., Jaro-Winkler distance), the encoding captures nuanced relationships between category labels that ordinal encoding cannot represent due to its forced numeric ordering. This mechanism assumes the semantic similarity between category labels correlates with their relationship to the target variable in the data.

### Mechanism 2
The transformer architecture enhances performance for both ordinal and similarity encoding in multi-label classification tasks through its multi-head attention mechanism, which can better capture complex interactions between features when they are encoded with richer representations. This assumes the additional representational capacity of transformers is beneficial when feature encodings contain meaningful structure.

### Mechanism 3
Discretization using decision trees preserves monotonic relationships between predictors and target, improving model learning by creating bins that maintain the relationship between continuous features and the target variable, whereas simple equal-width or equal-frequency methods do not. This mechanism assumes preserving the monotonic relationship between features and target during preprocessing leads to better model performance.

## Foundational Learning

- Concept: Categorical encoding techniques and their tradeoffs
  - Why needed here: The paper directly compares multiple encoding methods (ordinal, one-hot, rarelabel, target, summary, string similarity) and their impact on model performance
  - Quick check question: What is the key limitation of ordinal encoding that string similarity encoding addresses?

- Concept: Transformer architecture fundamentals
  - Why needed here: The context model extends the entity model with a transformer encoder block, requiring understanding of multi-head attention and positional encoding
  - Quick check question: How does the transformer architecture differ from the base MLP in terms of feature interaction modeling?

- Concept: Decision tree-based discretization
  - Why needed here: Continuous features are discretized using decision tree methods before encoding, which affects the quality of subsequent embeddings
  - Quick check question: What advantage does supervised discretization have over unsupervised methods in this context?

## Architecture Onboarding

- Component map: Input → Discretization (Decision Tree) → Encoding (6 methods) → Entity Model (Embeddings + MLP) OR Context Model (Embeddings + Transformer + MLP) → Output
- Critical path: Data preprocessing (discretization + encoding) → Embedding layer creation → Model training and evaluation
- Design tradeoffs: String similarity encoding provides better performance but at significantly higher computational cost, especially for high-cardinality features
- Failure signatures: Undefined results occur when there are zero true positive instances; poor performance may indicate encoding-method mismatch with data characteristics
- First 3 experiments:
  1. Implement and compare baseline ordinal encoding vs string similarity encoding on a small dataset (e.g., Mushroom)
  2. Test the impact of discretization method by comparing decision tree vs equal-width binning
  3. Evaluate the benefit of transformer architecture by comparing entity vs context models on a multi-label dataset

## Open Questions the Paper Calls Out

### Open Question 1
How do different encoding techniques affect neural network architectures that have separate inputs for continuous and discrete variables? The paper mentions that no comparison was made regarding neural networks with and without fully discretized datasets, and that the standard procedure consists of implementing two inputs, one for continuous and another for discrete variables. This remains unresolved because the authors did not conduct experiments to compare the effects of encoding techniques on architectures with separate inputs for continuous and discrete variables.

### Open Question 2
Why do neural networks seem to have more difficulty learning from target encoded data compared to other encoding techniques? The paper states that no further evaluation was made regarding the performance of a single encoding technique, and questions why it seems more difficult for neural networks to learn from target encoded data. This remains unresolved because the authors did not conduct further evaluation or analysis to understand the reasons behind the difficulty of neural networks in learning from target encoded data.

### Open Question 3
How do different encoding techniques affect the class structures of predictors, and how do these effects impact the entity and context embeddings? The paper mentions that One-Hot encoding destroys the class structure of the predictor by introducing a set of new binary features, and questions how the encoders affect such class structures by analysis of the entity and context embeddings. This remains unresolved because the authors did not conduct an analysis of how different encoding techniques affect the class structures of predictors and their impact on entity and context embeddings.

## Limitations
- The computational overhead of string similarity encoding is substantial, especially for high-cardinality features, limiting practical applicability
- The study focuses primarily on classification tasks, leaving open questions about performance on regression or ranking problems
- The evaluation relies on 10 datasets from the UCI repository, which may not fully represent the diversity of real-world tabular data

## Confidence

- High confidence: The superiority of string similarity encoding over ordinal encoding across 9 out of 10 datasets
- Medium confidence: The computational cost tradeoff, as this is a well-established characteristic of similarity-based methods
- Medium confidence: The transformer architecture improvements, as the evidence is primarily empirical without extensive ablation studies

## Next Checks

1. Conduct scalability experiments to quantify the computational cost of string similarity encoding across varying feature cardinalities and dataset sizes
2. Perform ablation studies to isolate the contribution of transformer architecture from the encoding method improvements
3. Test the generalizability of findings on additional datasets from diverse domains beyond the UCI repository