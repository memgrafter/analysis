---
ver: rpa2
title: 'The Fellowship of the LLMs: Multi-Model Workflows for Synthetic Preference
  Optimization Dataset Generation'
arxiv_id: '2408.08688'
source_url: https://arxiv.org/abs/2408.08688
tags:
- response
- evaluation
- datasets
- dataset
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes using multi-model LLM workflows to automate
  preference optimization dataset generation, replacing human annotators. It introduces
  a two-module pipeline: (1) response evaluation and (2) response generation.'
---

# The Fellowship of the LLMs: Multi-Model Workflows for Synthetic Preference Optimization Dataset Generation

## Quick Facts
- arXiv ID: 2408.08688
- Source URL: https://arxiv.org/abs/2408.08688
- Reference count: 15
- Primary result: Multi-model LLM workflows automate PO dataset generation with GPT-4o-as-a-Judge most consistent; Llama-3.1-8b + Gemma-2-9b achieves 71.8%/73.8% win rates over single models

## Executive Summary
This paper proposes automating preference optimization (PO) dataset generation through multi-model LLM workflows, replacing human annotators with a two-module pipeline combining response evaluation and generation. The approach uses GPT-4o-as-a-Judge for consistent evaluation across diverse datasets and an LLM Feedback Loop with Llama-3.1-8b as generator and Gemma-2-9b as reviewer. Experimental results show the multi-model approach maintains quality while significantly reducing annotation costs, though with higher computational overhead. The work addresses key challenges in data curation for alignment research by demonstrating that collaborative LLM systems can produce high-quality synthetic datasets.

## Method Summary
The method employs a two-module pipeline: (1) response evaluation using LLM-as-a-Judge, LLMs-as-a-Jury, or LLM Debate approaches with various prompting strategies, and (2) response generation using an LLM Feedback Loop framework where a generator LLM produces responses evaluated by a feedback LLM that provides improvement suggestions. The evaluation module compares different configurations across Alpaca Eval, FairEval, PandaLM-Eval, and MT-Bench datasets using Cohen's Kappa agreement with human annotations. The generation module tests various generator/reviewer pairings, iterating feedback N=3 times to create DPO and KTO datasets. The optimal configuration identified is GPT-4o-as-a-Judge with combined scoring strategy and Llama-3.1-8b as generator with Gemma-2-9b as reviewer.

## Key Results
- GPT-4o-as-a-Judge provides most consistent evaluation across all four datasets (Alpaca Eval, FairEval, PandaLM-Eval, MT-Bench) with second-place ranking in all cases
- Llama-3.1-8b as generator with Gemma-2-9b as reviewer achieves 71.8% and 73.8% win rates over single-model Llama and Gemma baselines respectively
- Multi-model feedback loops improve response quality through iterative refinement, though at higher computational costs
- The approach successfully automates PO dataset creation while maintaining quality, reducing dependence on human annotators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-model feedback loops improve response quality through iterative refinement
- Mechanism: One model generates a response, another model provides targeted feedback on instruction-following, helpfulness, relevance, accuracy, and creativity. The generator then revises based on this feedback, repeating for multiple iterations. This process leverages complementary strengths of different models to create more polished outputs.
- Core assumption: Different LLMs have complementary strengths that can be effectively combined in a feedback loop architecture
- Evidence anchors:
  - [abstract] "The multi-model approach uses the collaboration between multiple LLMs, where one model can provide suggestions for improvements, and the other can revise the response based on the feedback"
  - [section 3.3] "In this framework, a generator LLM produces a response, which is then evaluated by a feedback LLM that provides improvement suggestions"
  - [corpus] Weak evidence - no direct corpus citations supporting this specific mechanism
- Break condition: When computational costs outweigh quality improvements, or when models' strengths don't complement each other effectively

### Mechanism 2
- Claim: LLM-as-Judge with GPT-4o provides more consistent evaluation across diverse datasets than multi-model jury approaches
- Mechanism: GPT-4o evaluates responses using combined scoring strategy (providing all responses in one conversation context with scores out of 10), achieving higher Cohen's Kappa agreement with human annotations across Alpaca Eval, FairEval, PandaLM-Eval, and MT-Bench datasets
- Core assumption: A single, powerful LLM judge can provide more consistent evaluation than ensembles when candidate responses don't include outputs from the judge itself
- Evidence anchors:
  - [abstract] "Our evaluation shows that GPT-4o-as-a-Judge is more consistent across all datasets"
  - [section 4.1] "GPT-4o consistently ranks in second position across all three datasets" and "The performance of GPT-4o-as-a-Judge has been consistently high across various evaluations"
  - [corpus] Weak evidence - no direct corpus citations supporting this specific mechanism
- Break condition: When candidate responses include outputs from GPT-4o itself, creating evaluation bias

### Mechanism 3
- Claim: Using Llama-3.1-8b as generator and Gemma-2-9b as reviewer in feedback loop yields highest win rates against single models
- Mechanism: Llama's strengths in generating responses are enhanced by Gemma's ability to fine-tune and correct errors, leading to more polished outputs. This configuration achieved 71.8% win rate against Llama and 73.8% against Gemma in head-to-head comparisons
- Core assumption: Assigning appropriate roles based on specific strengths of each model maximizes performance in multi-model feedback loops
- Evidence anchors:
  - [abstract] "Experimenting with various configurations, we find that the LLM Feedback Loop, with Llama as the generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win rate over single-model Llama and Gemma, respectively"
  - [section 4.2] "We observe that using Llama as the generator improves the performance as compared to using Gemma as the generator because this configuration leads to a better win rate against all three baselines"
  - [corpus] Weak evidence - no direct corpus citations supporting this specific mechanism
- Break condition: When the computational overhead of multi-model loops exceeds the quality gains, or when models' roles could be more effectively reversed

## Foundational Learning

- Concept: Preference Optimization (PO) and Direct Preference Optimization (DPO)
  - Why needed here: The paper's core contribution is automating PO dataset generation, which requires understanding how preference optimization works and what data it requires
  - Quick check question: What are the two types of datasets generated in this work, and what distinguishes them?

- Concept: Multi-model evaluation frameworks (LLM-as-Judge, LLMs-as-a-Jury, LLM Debate)
  - Why needed here: The paper systematically compares these three approaches to determine the optimal evaluation strategy for PO dataset generation
  - Quick check question: Which evaluation approach was selected as the winner and why?

- Concept: Cohen's Kappa and win rate metrics
  - Why needed here: These metrics are used to evaluate both the evaluation module (Cohen's Kappa against human annotations) and the generation module (win rate against baseline models)
  - Quick check question: What metric was used to determine the best multi-model configuration for response generation?

## Architecture Onboarding

- Component map: Generator (Llama-3.1-8b) → Reviewer (Gemma-2-9b) → Revised Generator → Evaluator (GPT-4o-as-a-Judge) → Dataset (DPO/KTO)
- Critical path: Generator → Reviewer → Revised Generator → Evaluator → Dataset
- Design tradeoffs:
  - Single-model vs. multi-model evaluation: Single models (GPT-4o) offer consistency but risk bias; multi-model juries offer robustness but show high variance
  - Computational cost vs. quality: Multi-model approaches significantly increase resource requirements but improve output quality
  - Model selection: Different model pairings yield different performance characteristics; optimal pairing depends on specific use case
- Failure signatures:
  - Low Cohen's Kappa scores indicate poor evaluator performance
  - Win rates below baseline indicate ineffective generation configurations
  - High variance in evaluation scores across datasets suggests poor generalization
  - Computational timeouts or API failures in multi-model loops
- First 3 experiments:
  1. Implement the combined scoring strategy for LLM-as-Judge with GPT-4o on a small subset of Alpaca Eval to verify consistency claims
  2. Test the Llama-3.1-8b as generator with Gemma-2-9b as reviewer configuration on 100 prompts to measure win rate against baselines
  3. Run the full two-module pipeline on a small dataset to verify end-to-end functionality and dataset format correctness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific types of biases that LLMs-as-Jury and LLM Debate approaches may still exhibit despite their design to reduce bias, and how do these biases manifest in evaluation outcomes?
- Basis in paper: [inferred]
- Why unresolved: The paper mentions that LLMs-as-Jury and LLM Debate approaches can mitigate certain biases present in single-model evaluations, but it does not provide detailed analysis of what specific biases remain or how they affect results across different datasets and model configurations.
- What evidence would resolve it: Comparative analysis showing bias scores or agreement patterns for LLM-as-Jury and LLM Debate approaches across multiple datasets and model combinations, highlighting any persistent biases.

### Open Question 2
- Question: How does the performance of multi-model workflows scale with the number of feedback iterations in the LLM Feedback Loop, and is there an optimal number of iterations that balances quality improvement against computational costs?
- Basis in paper: [explicit]
- Why unresolved: The paper uses a fixed number of three feedback iterations for dataset generation but does not explore how varying this number affects the quality of generated responses or the overall efficiency of the process.
- What evidence would resolve it: Empirical results comparing win rates and response quality metrics across different iteration counts, along with computational cost analysis to identify the optimal iteration number.

### Open Question 3
- Question: What are the specific mechanisms by which Gemma-2-9b as a reviewer enhances Llama-3.1-8b's performance as a generator, and are these improvements transferable to other model combinations?
- Basis in paper: [inferred]
- Why unresolved: The paper demonstrates that the Llama-3.1-8b (generator) and Gemma-2-9b (reviewer) combination achieves the best win rates, but it does not analyze the specific feedback mechanisms or whether these benefits extend to other model pairings.
- What evidence would resolve it: Detailed analysis of reviewer feedback content and its impact on generator revisions, along with cross-model combination experiments to test generalizability of the improvement patterns.

## Limitations
- Computational cost implications are substantial - multi-model workflows require significantly more resources than single-model approaches, though the paper doesn't provide detailed cost-benefit analysis or runtime comparisons
- Corpus evidence is notably weak, with no direct citations supporting the proposed mechanisms or configurations, limiting ability to assess novelty
- Evaluation methodology has potential bias issues when GPT-4o evaluates responses it may have generated itself

## Confidence
- High Confidence: GPT-4o-as-a-Judge provides consistent evaluation across diverse datasets with second-place ranking in all cases
- Medium Confidence: Llama-3.1-8b as generator with Gemma-2-9b as reviewer achieves 71.8%/73.8% win rates over single models
- Low Confidence: Multi-model feedback loops consistently improve response quality through iterative refinement

## Next Checks
1. Reproduce the Cohen's Kappa evaluation using exact prompt configurations on Alpaca Eval dataset to verify GPT-4o's consistency claims, particularly examining cases where GPT-4o evaluates responses that might include its own outputs
2. Conduct cost-benefit analysis comparing computational resources required for multi-model workflows versus single-model approaches, including API costs, processing time, and quality trade-offs across different dataset sizes
3. Test generalizability by applying the Llama-3.1-8b generator with Gemma-2-9b reviewer configuration to a completely different domain (e.g., medical or legal responses) to assess whether performance gains extend beyond original task types