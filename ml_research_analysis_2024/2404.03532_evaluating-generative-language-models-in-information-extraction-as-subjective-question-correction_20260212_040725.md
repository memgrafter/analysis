---
ver: rpa2
title: Evaluating Generative Language Models in Information Extraction as Subjective
  Question Correction
arxiv_id: '2404.03532'
source_url: https://arxiv.org/abs/2404.03532
tags:
- answer
- information
- evaluation
- llms
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of accurately evaluating large
  language models (LLMs) on information extraction (IE) tasks like relation extraction
  and event extraction. Existing evaluation methods struggle with semantic consistency
  between model outputs and ground truth, and benchmarks are often incomplete due
  to restrictive human annotation schemas, leading to underestimated LLM performance.
---

# Evaluating Generative Language Models in Information Extraction as Subjective Question Correction

## Quick Facts
- arXiv ID: 2404.03532
- Source URL: https://arxiv.org/abs/2404.03532
- Reference count: 0
- Key outcome: Proposes SQC-Score, a new evaluation method for IE tasks that outperforms traditional metrics in human evaluation by addressing semantic consistency and benchmark incompleteness

## Executive Summary
This paper addresses the challenge of accurately evaluating large language models on information extraction tasks, where existing methods struggle with semantic consistency and incomplete benchmarks. The authors propose SQC-Score, which uses fine-tuned LLMs as matchers and NLI models as complementers to semantically align model outputs with golden labels while enriching benchmarks with missing answers. Experiments on relation extraction, event detection, and event argument extraction show SQC-Score is more preferred by human annotators compared to baseline metrics like BERTScore, BARTScore, GPT-3.5-Turbo, and F1-score.

## Method Summary
The authors propose SQC-Score, a new evaluation method for IE tasks that addresses semantic consistency and benchmark incompleteness. It uses two key components: a fine-tuned LLM matcher trained on subjective question correction data to semantically align model outputs with golden labels, and an NLI model complementer that enriches golden labels by identifying correct but omitted answers through entailment scoring. The method computes precision and recall by matching predicted information with both original and complemented golden information, then calculates the harmonic mean as the final SQC-Score.

## Key Results
- SQC-Score achieved 54.80% human preference vs 50.00% for F1-score on relation extraction
- LLMs can perform shallow IE tasks but struggle with structurally extracting information with well-defined schemas
- The approach successfully addresses semantic consistency issues between model outputs and ground truth

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SQC-Score improves semantic consistency evaluation between model outputs and ground truth in IE tasks
- Mechanism: Uses fine-tuned LLMs trained on subjective question correction data to semantically match predicted information with golden labels
- Core assumption: Subjective question correction data can effectively transfer to IE task matching because both involve aligning outputs based on semantic equivalence
- Evidence anchors:
  - [abstract] "This method innovatively utilizes LLMs, fine-tuned through subjective question correction data, to refine matching between model outputs and golden labels"
  - [section 2.2] "We collect triples{(Y, ˆY , s)} from mock tests of CCEE... We train 7B variant of LLaMA-2... for 2 epochs"
  - [corpus] Weak - corpus doesn't directly address semantic matching mechanism
- Break condition: If subjective question correction data doesn't capture semantic variations present in IE tasks

### Mechanism 2
- Claim: SQC-Score addresses benchmark incompleteness by identifying correct but missing answers
- Mechanism: Uses NLI models to evaluate whether predicted information is semantically entailed by input text
- Core assumption: NLI models can reliably determine semantic entailment between text and predicted information
- Evidence anchors:
  - [abstract] "by incorporating a Natural Language Inference (NLI) model, SQC-Score enriches golden labels, addressing benchmark incompleteness"
  - [section 2.3] "We use templates to generate the hypothesis hi for each unmatched predicted information... If the entailment scoresi is greater than a threshold τ, we believe the predicted information should be added"
  - [section C] "We propose to use NLI to remedy the drawbacks of the annotation of IE datasets... choose the NLI score of the top 40% gold labels as a threshold"
- Break condition: If NLI models have poor calibration on this specific task

### Mechanism 3
- Claim: SQC-Score provides evaluation scores that better align with human judgment than baseline metrics
- Mechanism: Combines precision and recall calculations that account for both semantic matching and NLI-complemented answers
- Core assumption: Human evaluators judge IE task correctness based on semantic understanding rather than exact match
- Evidence anchors:
  - [abstract] "Results on three information extraction tasks show that SQC-Score is more preferred by human annotators than the baseline metrics"
  - [section 3.1] "The results show that SQC-Score is more preferred by human annotators than the baseline metrics"
  - [section 3] Table 1 shows SQC-Score achieving 54.80% preference vs 50.00% for F1-score on RE
- Break condition: If human evaluators have inconsistent standards

## Foundational Learning

- Concept: Semantic equivalence vs syntactic equivalence in evaluation
  - Why needed here: The paper's core innovation relies on recognizing that IE tasks require semantic matching rather than exact string matching
  - Quick check question: Given golden answer "(Hong Kong, Country, China)" and model output "(Hong Kong, Place, China)", should this be considered a match? Why or why not?

- Concept: Natural Language Inference (NLI) and entailment scoring
  - Why needed here: The complementer component relies on NLI models to determine whether predicted information is semantically supported by the input text
  - Quick check question: If the input text states "Hong Kong is a territory of China" and the model predicts "(Hong Kong, Country, China)", what NLI score would you expect?

- Concept: Fine-tuning large language models for specialized tasks
  - Why needed here: The matcher component requires fine-tuning LLMs on subjective question correction data to adapt them for IE task matching
  - Quick check question: What are the key differences between zero-shot prompting and fine-tuning for adapting LLMs to new tasks?

## Architecture Onboarding

- Component map: Matcher -> Complementer -> Score Calculator -> Threshold Selector
- Critical path:
  1. Input text and model predictions processed by Matcher
  2. Unmatched predictions sent to Complementer for entailment scoring
  3. Scores computed using precision/recall formula incorporating both matched and complemented information
  4. Final SQC-Score returned as harmonic mean
- Design tradeoffs:
  - Using subjective question correction data vs IE-specific training data: Broader applicability vs task-specific performance
  - Fixed NLI threshold vs adaptive threshold: Consistency vs optimal performance per dataset
  - Harmonic mean vs weighted combination: Balanced vs task-specific importance of precision/recall
- Failure signatures:
  - Low precision despite high recall: Matcher over-generous in semantic matching
  - High precision but low recall: Matcher too strict, missing valid semantic matches
  - Threshold issues: Too many false positives (low threshold) or false negatives (high threshold) in complementer
- First 3 experiments:
  1. Test Matcher component alone on synthetic data with known semantic equivalences to verify matching accuracy
  2. Evaluate Complementer NLI scoring on a subset of predictions to tune threshold selection
  3. Compare SQC-Score against F1-score on a small dataset with human evaluation to validate preference claims

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following questions arise from the analysis:

### Open Question 1
- Question: How does the performance of SQC-Score compare to traditional metrics when evaluated on a broader range of information extraction tasks beyond relation extraction, event detection, and event argument extraction?
- Basis in paper: The paper presents experimental results on three specific IE tasks and discusses the potential of SQC-Score to improve evaluation across various IE tasks
- Why unresolved: The experiments conducted are limited to a few IE tasks, and the paper does not discuss or provide evidence on how SQC-Score would perform on other types of IE tasks
- What evidence would resolve it: Conducting experiments with SQC-Score on a diverse set of IE tasks would provide evidence of its generalizability

### Open Question 2
- Question: What are the specific challenges and limitations encountered when using LLMs as matchers in SQC-Score, and how do these challenges affect the accuracy and reliability of the evaluation?
- Basis in paper: The paper discusses the use of LLMs as matchers and mentions a discrepancy between subjective questions from CCEE and data in IE tasks
- Why unresolved: While the paper acknowledges the discrepancy, it does not delve into the specific challenges or limitations that may arise from using LLMs as matchers
- What evidence would resolve it: A detailed study analyzing the performance of LLMs as matchers across various IE tasks would provide insights into the challenges and limitations

### Open Question 3
- Question: How does the threshold τ for the entailment score in the NLI model complementer affect the precision and recall of SQC-Score, and what is the optimal value of τ for different types of IE tasks?
- Basis in paper: The paper mentions the use of a threshold τ but does not provide a detailed analysis of how different values of τ affect the performance of SQC-Score
- Why unresolved: The paper selects the threshold τ based on the 40th percentile but does not explore the impact of different threshold values on the evaluation results
- What evidence would resolve it: Conducting experiments with various threshold values and analyzing their effects on precision, recall, and overall performance would help determine the optimal threshold for each task type

## Limitations
- The approach's generalizability beyond Chinese language IE tasks remains unproven
- Key implementation details like prompt engineering and training procedures are underspecified
- The effectiveness of transferring subjective question correction data to IE task matching is assumed but not rigorously validated

## Confidence
- High Confidence: Core problem identification and general framework using semantic matching with NLI complementation
- Medium Confidence: Specific implementation details and hyperparameters for fine-tuning matcher models
- Low Confidence: Generalizability beyond evaluated IE tasks and Chinese language context

## Next Checks
1. Test matcher performance using different training data sources to quantify impact of data choice on semantic matching quality
2. Conduct ablation studies varying the NLI threshold τ across different datasets and IE task types to determine optimal threshold selection strategies
3. Evaluate SQC-Score on English IE datasets to assess cross-lingual generalization beyond Chinese language context used in the paper