---
ver: rpa2
title: Parallel Corpus Augmentation using Masked Language Models
arxiv_id: '2410.03194'
source_url: https://arxiv.org/abs/2410.03194
tags:
- data
- sentence
- language
- they
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to augment parallel corpora for low-resource
  machine translation without needing additional monolingual data. The method masks
  words in source and target sentences using XLM-RoBERTa, generates multiple alternatives,
  and uses LaBSE sentence embeddings to identify pairs that are likely translational
  equivalents by measuring cosine similarity.
---

# Parallel Corpus Augmentation using Masked Language Models

## Quick Facts
- arXiv ID: 2410.03194
- Source URL: https://arxiv.org/abs/2410.03194
- Reference count: 40
- Primary result: Method generates ~200 high-quality sentence pairs from a single seed pair, potentially increasing corpus size by 100×

## Executive Summary
This paper proposes a method to augment parallel corpora for low-resource machine translation without requiring additional monolingual data. The approach uses XLM-RoBERTa to mask words in source and target sentences, generates multiple alternatives, and employs LaBSE sentence embeddings to identify likely translational equivalents through cosine similarity. Generated pairs are validated using TransQuest for quality estimation. Experiments demonstrate that from a single sentence pair, approximately 200 new high-quality sentence pairs can be generated with LaBSE scores ≥0.80 and TransQuest scores >0.8, potentially achieving 100× corpus size increases.

## Method Summary
The method masks words in parallel sentence pairs using XLM-RoBERTa's fill-mask capability, generating multiple alternative sentences while preserving syntax and semantics. LaBSE sentence embeddings are computed for all generated pairs, and those with cosine similarity ≥0.80 are selected as likely translations. Selected pairs undergo final validation using TransQuest quality estimation, with only pairs scoring >0.8 being retained in the augmented corpus. This approach requires only a seed parallel corpus without needing additional monolingual data.

## Key Results
- From one sentence pair, approximately 200 new high-quality sentence pairs can be generated
- All generated pairs achieve LaBSE scores ≥0.80 and TransQuest scores >0.8
- Potential for 100× increase in corpus size without additional monolingual data
- Method validated on English-Hindi language pair with promising results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masking words with XLM-RoBERTa and generating alternatives preserves sentence syntax and semantics while creating diverse training data
- Mechanism: The Fill-Mask task of XLM-RoBERTa replaces masked words with context-appropriate alternatives, generating multiple valid sentence variations without disrupting grammatical structure
- Core assumption: Replacing one word at a time minimally disturbs sentence structure, meaning, and fluency
- Evidence anchors:
  - [abstract] "We use Multi-Lingual Masked Language Model to mask and predict alternative words in context"
  - [section] "The whole idea of replacing only one word at a time is that syntax and semantics are least disturbed when we make only small changes at a time"
  - [corpus] Weak - no direct corpus evidence for minimal disturbance claim
- Break condition: If the masked word is semantically critical or the context is ambiguous, generated alternatives may distort meaning significantly

### Mechanism 2
- Claim: Sentence embeddings can reliably identify translational equivalents among generated sentence pairs
- Mechanism: LaBSE sentence embeddings capture semantic meaning, and high cosine similarity between source and target embeddings indicates likely translational equivalence
- Core assumption: Sentences that are translations of each other should have high semantic similarity scores
- Evidence anchors:
  - [abstract] "We use Sentence Embeddings to check and select sentence pairs which are likely to be translations of each other"
  - [section] "If two sentences are translations of one another, they should be semantically similar"
  - [corpus] Moderate - examples show good translation pairs have higher similarity scores (0.85, 0.91) vs bad pairs (0.42, 0.63)
- Break condition: When sentences contain cultural references, idioms, or structural differences that preserve meaning but reduce surface similarity

### Mechanism 3
- Claim: TransQuest quality estimation provides reliable validation of generated sentence pairs
- Mechanism: TransQuest uses cross-lingual transformers to score sentence pairs, with scores above 0.8 indicating high-quality translations
- Core assumption: MT quality estimation models can effectively validate whether generated sentence pairs are true translations
- Evidence anchors:
  - [abstract] "We cross check our method using metrics for MT Quality Estimation"
  - [section] "We run TransQuest on the 200 generated sentence pairs mentioned above and we get a score of above 0.8 in all cases"
  - [corpus] Strong - explicit validation results showing scores >0.8 for all tested pairs
- Break condition: If TransQuest model is not well-calibrated for the specific language pair or domain, validation scores may be unreliable

## Foundational Learning

- Concept: Masked Language Models (MLMs) and their Fill-Mask capability
  - Why needed here: Core mechanism for generating alternative words while preserving context
  - Quick check question: What is the difference between standard BERT and XLM-RoBERTa in terms of multilingual capabilities?

- Concept: Sentence embeddings and semantic similarity measures
  - Why needed here: Used to identify which generated sentence pairs are likely translations of each other
  - Quick check question: How does cosine similarity between sentence embeddings relate to semantic equivalence?

- Concept: Neural Machine Translation quality estimation
  - Why needed here: Provides validation that generated sentence pairs are high-quality translations
  - Quick check question: What are the key architectural differences between MonoTransQuest and SiameseTransQuest approaches?

## Architecture Onboarding

- Component map: XLM-RoBERTa model -> LaBSE sentence embedding model -> TransQuest quality estimation model -> Corpus processing pipeline -> Filtering and selection logic

- Critical path:
  1. Load seed parallel corpus
  2. For each sentence pair, mask words and generate alternatives using XLM-RoBERTa
  3. Compute sentence embeddings for all generated pairs using LaBSE
  4. Filter pairs with similarity score ≥ threshold (e.g., 0.80)
  5. Validate filtered pairs using TransQuest (score > 0.8)
  6. Output augmented corpus

- Design tradeoffs:
  - Masking more words increases diversity but risks breaking semantic coherence
  - Higher similarity thresholds reduce noise but may miss valid translations
  - Using larger XLM-R models improves quality but increases computational cost

- Failure signatures:
  - Generated sentences lose original meaning when critical words are masked
  - Low LaBSE scores despite correct translations due to structural differences
  - TransQuest scores inconsistent across different language pairs

- First 3 experiments:
  1. Test on a single sentence pair with known translations, vary masking positions and topk values
  2. Compare LaBSE similarity scores for ground-truth translations vs non-translations
  3. Validate TransQuest scores on a small test set with human-annotated quality labels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal LaBSE score threshold for selecting sentence pairs that balances quality and quantity in parallel corpus augmentation?
- Basis in paper: [explicit] The paper mentions using a threshold of 0.80 for LaBSE scores but does not explore other threshold values or their impact on corpus quality and size
- Why unresolved: The paper only tests one threshold value and does not compare results with different thresholds to determine the optimal balance between quality and quantity
- What evidence would resolve it: Experimental results comparing augmented corpus size and quality across multiple LaBSE threshold values (e.g., 0.70, 0.75, 0.80, 0.85, 0.90) would provide insights into the optimal threshold

### Open Question 2
- Question: How does the quality of augmented parallel corpora vary across different language pairs and what factors influence this variation?
- Basis in paper: [inferred] The paper focuses on English-Hindi language pair but claims the method can work for all language pairs with available masked language models. It does not provide evidence or analysis of performance across diverse language pairs
- Why unresolved: The paper does not conduct experiments with multiple language pairs or analyze factors that might affect augmentation quality across different languages
- What evidence would resolve it: Comparative experiments applying the method to multiple language pairs (e.g., English-Spanish, English-Chinese, English-Arabic) with quality analysis would reveal patterns and influencing factors

### Open Question 3
- Question: What is the impact of different masked language models on the quality and diversity of augmented parallel corpora?
- Basis in paper: [explicit] The paper uses XLM-RoBERTa but mentions other models like BERT without comparing their performance in the augmentation process
- Why unresolved: The paper does not compare results using different masked language models or analyze how model choice affects corpus quality and diversity
- What evidence would resolve it: Experimental results comparing augmented corpora generated using different masked language models (e.g., BERT, RoBERTa, XLM-RoBERTa) with quality metrics would demonstrate the impact of model choice

## Limitations
- Method relies heavily on quality of XLM-RoBERTa and LaBSE models, which may vary across language pairs
- Computational cost scales with corpus size and number of masking positions, limiting applicability to very large corpora
- No evaluation of downstream MT performance provided, so quality gains remain theoretical

## Confidence
- **High Confidence**: The core mechanism of using masked language models for sentence augmentation is well-established
- **Medium Confidence**: The claim of generating ~200 high-quality pairs per seed pair is supported by limited examples but lacks comprehensive validation
- **Low Confidence**: Claims about 100× corpus size increase are theoretical projections without empirical validation on diverse datasets

## Next Checks
1. **Downstream MT Evaluation**: Train NMT models on augmented vs original corpora and measure BLEU score improvements across multiple language pairs
2. **Cross-Lingual Robustness**: Test the augmentation pipeline on language pairs with varying resource levels (high, medium, low) to assess generalizability
3. **Computational Efficiency Analysis**: Benchmark processing time and memory usage for different corpus sizes and masking strategies to establish practical limits