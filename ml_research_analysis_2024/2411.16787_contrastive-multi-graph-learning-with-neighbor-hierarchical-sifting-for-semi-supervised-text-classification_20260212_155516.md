---
ver: rpa2
title: Contrastive Multi-graph Learning with Neighbor Hierarchical Sifting for Semi-supervised
  Text Classification
arxiv_id: '2411.16787'
source_url: https://arxiv.org/abs/2411.16787
tags:
- text
- graph
- learning
- classification
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles semi-supervised text classification by addressing
  key limitations in graph contrastive learning: semantic loss from explicit augmentations,
  neglect of edge features in multi-graph learning, and false negatives in contrastive
  losses. The proposed method, ConNHS, constructs a multi-relational text graph using
  core features (titles, keywords, events) to enhance semantic connections, then employs
  a relation-aware graph convolutional network (RW-GCN) and cross-graph attention
  network (CGAN) for intra- and inter-graph propagation.'
---

# Contrastive Multi-graph Learning with Neighbor Hierarchical Sifting for Semi-supervised Text Classification

## Quick Facts
- arXiv ID: 2411.16787
- Source URL: https://arxiv.org/abs/2411.16787
- Reference count: 7
- Primary result: Achieves state-of-the-art accuracy on four text classification datasets by separating multi-relational graphs and using neighbor hierarchical sifting loss

## Executive Summary
This paper addresses key limitations in graph contrastive learning for semi-supervised text classification, specifically semantic loss from explicit augmentations, neglect of edge features in multi-graph learning, and false negatives in contrastive losses. The proposed ConNHS method constructs a multi-relational text graph using core features (titles, keywords, events) to enhance semantic connections, then employs a relation-aware graph convolutional network and cross-graph attention network for intra- and inter-graph propagation. A novel neighbor hierarchical sifting loss refines negative selection by masking first-order neighbors and excluding high-order similar nodes, reducing false negatives. Experiments on four datasets demonstrate state-of-the-art performance with strong results in few-label scenarios.

## Method Summary
ConNHS tackles semi-supervised text classification by constructing a multi-relational text graph using core features (titles, keywords, events) extracted with BGE-M3 and LangChain. The graph is separated into semantic subgraphs to avoid explicit augmentation, then processed through a relation-aware graph convolutional network (RW-GCN) for intra-graph propagation and a cross-graph attention network (CGAN) for inter-graph fusion. The neighbor hierarchical sifting loss (NHS) masks first-order neighbors and excludes high-order similar nodes from negative samples during contrastive learning. A logistic regression classifier produces final predictions, with the entire system trained using Adam optimizer.

## Key Results
- Achieves 95.86%, 97.52%, 87.43%, and 70.65% accuracy on ThuCNews, SogouNews, 20 Newsgroups, and Ohsumed datasets respectively
- Outperforms state-of-the-art methods on all four datasets
- Demonstrates strong performance in few-label scenarios
- Shows consistent improvements across different text lengths and languages (Chinese and English)

## Why This Works (Mechanism)

### Mechanism 1
Separating the multi-relational text graph into semantic subgraphs avoids explicit graph augmentation while preserving graph structural integrity. Instead of performing explicit graph augmentations (which can distort task-relevant information), the multi-relational graph is decomposed into distinct subgraphs based on relationship type (title, keyword, event). This separation provides diverse contrastive views without requiring augmentation operations that may degrade semantic consistency.

### Mechanism 2
The neighbor hierarchical sifting loss (NHS) reduces false negatives in contrastive learning by masking first-order neighbors and excluding high-order similar nodes. NHS identifies and excludes from negative samples both first-order neighbors (based on graph homophily assumption) and high-order neighbors that are highly similar to the anchor node (based on similarity score matrix). This prevents similar nodes from being treated as dissimilar, which would otherwise widen the distance between them in embedding space.

### Mechanism 3
The relation-aware graph convolutional network (RW-GCN) captures varying correlations between nodes and incorporates edge features during intra-graph propagation. RW-GCN uses an aggregation operator that computes importance weights based on edge features connecting nodes, then aggregates these weighted features. A transformation operator combines node features with aggregated edge information, allowing the model to learn varying correlations rather than assuming uniform importance across neighbors.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their message passing mechanism
  - Why needed here: The method relies on GNNs (specifically GCN variants) to propagate information through the constructed text graphs, learning node representations that capture semantic relationships.
  - Quick check question: What is the fundamental operation performed by a GCN layer when updating node representations?

- Concept: Contrastive learning and negative sampling strategies
  - Why needed here: The method employs graph contrastive learning with a novel NHS loss function, requiring understanding of how positive and negative samples are selected and how contrastive loss functions operate.
  - Quick check question: In contrastive learning, what is the relationship between positive samples and the anchor, and how does negative sampling affect the learned representations?

- Concept: Text graph construction and multi-relational graphs
  - Why needed here: The method constructs a multi-relational text graph using core features (titles, keywords, events) as edges, which is fundamental to the entire approach.
  - Quick check question: How can different types of relationships between text documents be represented in a graph structure?

## Architecture Onboarding

- Component map:
  Feature Extraction → Text Encoder → Multi-relational Text Graph Construction → Graph Separation → RW-GCN (Intra-graph Propagation) → CGAN (Inter-graph Propagation) → Projection Head → NHS Loss → Logistic Regression Classifier
  Key components: LangChain+BGE-M3 text encoder, relation-aware GCN, cross-graph attention network, neighbor hierarchical sifting loss

- Critical path: Feature extraction → multi-relational graph construction → RW-GCN propagation → CGAN fusion → projection mapping → NHS contrastive loss → classification
  - This path represents the flow from raw text to final classification predictions

- Design tradeoffs:
  - Graph augmentation vs. graph separation: Choosing separation avoids semantic distortion but may provide less diversity than augmentation
  - Edge feature incorporation vs. simplicity: RW-GCN includes edge features for richer representations but adds complexity
  - NHS vs. standard contrastive losses: NHS reduces false negatives but requires computing similarity matrices and identifying high-order neighbors

- Failure signatures:
  - Poor performance on long texts despite the method's design for longer documents (ThuCNews, SogouNews)
  - Inconsistent improvements across datasets (better on English than Chinese datasets)
  - Degradation when removing either graph structure or node attribute information from NHS

- First 3 experiments:
  1. Verify multi-relational graph construction: Check that title, keyword, and event relationships are correctly established with appropriate similarity thresholds
  2. Test RW-GCN effectiveness: Compare node representations with and without edge feature incorporation on a validation set
  3. Validate NHS loss function: Measure the impact of NHS on reducing false negatives by comparing similarity distributions before and after training with NHS

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of ConNHS compare to other graph-based text classification methods when applied to datasets with significantly different characteristics, such as varying document lengths or levels of label imbalance? The paper evaluates ConNHS on four datasets with different characteristics but does not explore performance on datasets with more extreme variations in these characteristics.

### Open Question 2
How does the performance of ConNHS change when using different types of core features (e.g., entities, topics) instead of titles, keywords, and events? The paper uses titles, keywords, and events as core features but does not explore the impact of using alternative feature types.

### Open Question 3
How does the performance of ConNHS scale with the size of the dataset and the number of labels available? The paper demonstrates effectiveness on datasets with varying numbers of documents and labels but does not explore scalability to much larger datasets or scenarios with extremely limited labels.

## Limitations

- Computational complexity of NHS loss calculation may limit scalability to very large text corpora
- Reliance on specific preprocessing tools (LangChain, BGE-M3, KeyBert, DDparser, Stanza) creates potential reproducibility challenges
- Dataset-dependent similarity thresholds and minimum association coefficients require careful tuning for different domains

## Confidence

**High confidence** in the core mechanism: The separation of multi-relational graphs to avoid semantic distortion through explicit augmentation is well-supported by the experimental results showing consistent improvements over augmentation-based baselines.

**Medium confidence** in NHS effectiveness: While the ablation studies demonstrate NHS improves performance, the specific impact of masking first-order neighbors versus excluding high-order similar nodes is not clearly separated in the results.

**Low confidence** in generalization claims: The method shows strong results on the four tested datasets, but claims about handling long texts and Chinese/English language pairs need broader validation across more diverse text types and languages.

## Next Checks

1. **Scalability validation**: Test the method on datasets with 10× more documents to measure computational overhead and memory requirements of the NHS loss calculation.

2. **Ablation of NHS components**: Conduct experiments isolating the effects of first-order neighbor masking versus high-order similarity exclusion to determine which aspect drives the most performance gains.

3. **Cross-lingual generalization**: Evaluate the method on additional languages beyond Chinese and English to verify the claimed robustness to different language structures and writing systems.