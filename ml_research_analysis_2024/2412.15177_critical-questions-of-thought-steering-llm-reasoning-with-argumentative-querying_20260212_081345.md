---
ver: rpa2
title: 'Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying'
arxiv_id: '2412.15177'
source_url: https://arxiv.org/abs/2412.15177
tags:
- reasoning
- cqot
- step
- llms
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Critical-Questions-of-Thought (CQoT)
  approach to improve LLM reasoning capabilities by combining Toulmin's argumentation
  model with critical questions. The method employs a four-step pipeline where LLMs
  first generate reasoning steps, then evaluate them against eight critical questions
  targeting Toulmin's schema elements (data, warrant, claim, etc.), iterate if needed,
  and finally produce an answer following the validated reasoning plan.
---

# Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying

## Quick Facts
- arXiv ID: 2412.15177
- Source URL: https://arxiv.org/abs/2412.15177
- Reference count: 40
- Outperforms baseline models by 5% on reasoning tasks using Toulmin's argumentation model with critical questions

## Executive Summary
This paper introduces the Critical-Questions-of-Thought (CQoT) approach to improve LLM reasoning capabilities by combining Toulmin's argumentation model with critical questions. The method employs a four-step pipeline where LLMs first generate reasoning steps, then evaluate them against eight critical questions targeting Toulmin's schema elements, iterate if needed, and finally produce an answer following the validated reasoning plan. Tested on MT-Bench Reasoning and Math tasks, CQoT consistently outperformed both baseline models and Chain-of-Thought approaches, achieving an average 5% improvement in correctness scores. Notably, smaller open-source models like Llama 3.1-70b-Instruct and Nemotron-51b-Instruct surpassed larger proprietary models on specific tasks when using CQoT.

## Method Summary
CQoT employs a four-step pipeline: (1) the LLM generates a reasoning plan by decomposing each step into premise and conclusion, (2) eight critical questions are applied to evaluate each Toulmin component (data, warrant, claim, etc.), (3) if at least 7/8 critical questions pass, the pipeline proceeds to final answer generation, otherwise it iterates up to 10 times (with threshold dropping to 5/8 after 5 iterations), and (4) the LLM generates the final answer following the validated reasoning plan. The approach was evaluated on MT-Bench Reasoning and Math tasks using five models ranging from 51B to 70B+ parameters, with an external LLM Judge providing evaluation against reference answers.

## Key Results
- CQoT achieved an average 5% improvement in correctness scores over baseline models
- Smaller open-source models (Llama 3.1-70b-Instruct, Nemotron-51b-Instruct) surpassed larger proprietary models on specific tasks when using CQoT
- Critical questions proved essential for performance gains, as demonstrated by ablation studies
- The method is model-agnostic and particularly effective for enhancing logical and mathematical reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Critical questions act as a validation filter that catches reasoning errors before final output
- Mechanism: After the LLM generates a reasoning plan (Step 1), each Toulmin component is interrogated by 8 specific critical questions (Step 2). Only if ≥7/8 pass does the pipeline proceed to final answer generation (Step 4). This creates a gate that forces the model to self-correct flawed reasoning paths.
- Core assumption: LLMs can accurately self-evaluate their own reasoning when explicitly prompted to do so via structured critical questions
- Evidence anchors: [abstract] "By probing the rationale behind the models' reasoning process, the LLM can assess whether some logical mistake is occurring and correct it before providing the final reply"; [section 4.3] "We noticed that the driving engine of the CQoT pipeline is the reasoning protocol generated in Step 1, which steers the underlying model's thinking process. The LLM's reply will be reliant on such a protocol, thus possibly leading to an erroneous response if the provided reasoning plan presents any mistakes. The employment of CQs (implemented in Step 2 and verified in Step 3) will mostly filter out each of such wrongly designed protocols"

### Mechanism 2
- Claim: Structured reasoning plans prevent the model from jumping to conclusions without proper justification
- Mechanism: Step 1 forces the LLM to explicitly decompose reasoning into premises and conclusions for each step, creating a traceable chain that can be validated. This prevents the model from using shortcuts or memorized heuristics that may not apply to novel problems.
- Core assumption: Forcing explicit premise-conclusion structure improves reasoning quality compared to free-form generation
- Evidence anchors: [abstract] "Each reasoning step is split into premise & conclusion"; [section 3.1] "Prompt the LLM to reason logically about the input query and sketch a plan to follow. The model must divide each reasoning process into premises and conclusions so that each conclusion derives from the respective premises. No final response should be provided"

### Mechanism 3
- Claim: Iterative refinement through multiple passes improves reasoning accuracy
- Mechanism: The pipeline allows up to 10 iterations (Step 3 acts as a checkpoint). If the critical questions fail to achieve ≥7/8 pass rate, the process restarts from Step 1 with the requirement dropping to ≥5/8 after 5 iterations. This iterative approach allows the model to refine its reasoning over time.
- Core assumption: Multiple reasoning attempts with feedback improve final accuracy
- Evidence anchors: [section 3.1] "If there are at least 7/8 positive answers, then the pipeline will move to Step 4. Otherwise, the pipeline will start again from Step 1. This will occur for a maximum of five iterations, after which the requirement will drop to a minimum of 5/8 positive answers"

## Foundational Learning

- Concept: Toulmin's argumentation model (Claim, Data, Warrant, Backing, Qualifier, Rebuttal)
  - Why needed here: Provides the structural framework for organizing LLM reasoning and identifying where critical questions should be applied
  - Quick check question: Can you map a simple argument (e.g., "It's raining because the ground is wet") to Toulmin's six components?

- Concept: Defeasible reasoning and critical questions
  - Why needed here: Critical questions are designed to test the strength of presumptive arguments, which is exactly what LLMs produce when reasoning
  - Quick check question: What's the difference between a critical question that attacks a premise versus one that attacks an inference?

- Concept: Test-time compute scaling
  - Why needed here: The entire approach relies on allocating more inference-time computation to improve reasoning quality rather than just generating faster answers
  - Quick check question: How does test-time compute differ from training-time compute in terms of cost-benefit trade-offs?

## Architecture Onboarding

- Component map: Step 1 (Reasoning plan) -> Step 2 (Critical questions) -> Step 3 (Iteration control) -> Step 4 (Final answer)
- Critical path: Step 1 → Step 2 → Step 3 (pass/fail check) → Step 4, with potential loop from Step 3 to Step 1 up to 10 times
- Design tradeoffs:
  - Iteration count vs. computational cost: More iterations improve accuracy but increase latency
  - Critical question threshold (7/8 vs 5/8) vs. strictness: Higher thresholds ensure better reasoning but may cause more iterations
  - Step 1 detail level vs. Step 4 adherence: More detailed plans are harder to follow but provide better guidance
- Failure signatures:
  - High iteration counts without improvement in critical question scores
  - LLM Judge consistently gives low scores despite high critical question pass rates
  - Step 1 generates incoherent premise-conclusion structures that cannot be validated
- First 3 experiments:
  1. Baseline comparison: Run standard LLM on 10 reasoning problems, record accuracy
  2. Step 1+4 only: Remove critical questions, just generate reasoning plan then answer, compare to baseline
  3. Full CQoT pipeline: Run complete 4-step process, compare iteration counts and final accuracy to previous experiments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the CQoT approach maintain its effectiveness when applied to smaller LLMs (below 50B parameters) or does it have a minimum model size threshold?
- Basis in paper: Explicit - The authors note that Nemotron-51b-Instruct performed better with CoT than CQoT on reasoning tasks, suggesting a potential size threshold around 70B parameters.
- Why unresolved: The evaluation only tested models ranging from 51B to 70B+ parameters, leaving the performance characteristics for smaller models unexplored.
- What evidence would resolve it: Systematic testing of CQoT across a broader range of model sizes (10B-50B parameters) to determine if there's a critical threshold where CQoT's effectiveness diminishes.

### Open Question 2
- Question: How does CQoT's performance compare to more resource-intensive test-time compute methods like repeated sampling or tournament-style approaches?
- Basis in paper: Explicit - The authors mention that test-time compute methods like OpenAI's o1, DeepSeek's R1-Lite-Preview, and Qwen's QwQ exist but note CQoT doesn't require "tens (or hundreds) of samples to run."
- Why unresolved: The paper establishes CQoT's effectiveness but doesn't directly benchmark it against other test-time compute techniques that might achieve similar or better results through different mechanisms.
- What evidence would resolve it: Head-to-head comparison of CQoT against multiple test-time compute methods (repeated sampling, tournament selection, etc.) on the same benchmark tasks with identical computational budgets.

### Open Question 3
- Question: Can CQoT be effectively combined with test-time training techniques to further enhance reasoning performance?
- Basis in paper: Explicit - The authors suggest future work combining CQoT with test-time training, noting it has proven successful on the ARC benchmark and could potentially augment CQoT's performance.
- Why unresolved: The paper only speculates about this combination without implementing or testing it, leaving the practical benefits and potential synergies unexplored.
- What evidence would resolve it: Experimental implementation of CQoT combined with test-time training on reasoning benchmarks, measuring whether the combination yields multiplicative or merely additive improvements.

## Limitations

- Computational inefficiency: The iterative pipeline can require up to 10 inference passes per query, creating substantial latency-accuracy trade-offs
- Limited evaluation scope: Testing only on MT-Bench Reasoning and Math tasks may not capture full reasoning capabilities or generalization to other domains
- Dependency on LLM self-evaluation: Performance relies on the model's ability to accurately answer its own critical questions, which may fail for complex or novel problems

## Confidence

**High Confidence**: The claim that structured reasoning plans improve accuracy is well-supported by the ablation study showing Step 1+4 outperforms baseline models. The mechanism of forcing premise-conclusion decomposition is clearly implemented and tested.

**Medium Confidence**: The assertion that critical questions provide the primary performance boost is supported by the overall improvement but lacks direct ablation evidence isolating the critical questions' contribution. The iteration mechanism shows promise but the optimal iteration count and threshold settings remain empirical rather than theoretically grounded.

**Low Confidence**: The claim that smaller open-source models can surpass larger proprietary models using CQoT is based on limited task-specific observations rather than comprehensive head-to-head comparisons across all task types.

## Next Checks

1. **Efficiency Benchmark**: Measure actual wall-clock time for CQoT vs baseline approaches across the same task sets, including setup overhead and iteration costs. This would quantify the latency-accuracy trade-off and help determine practical deployment scenarios.

2. **Critical Question Ablation**: Remove the critical questions entirely (Step 2 and Step 3) and compare Step 1+4 performance against the full CQoT pipeline. This would isolate whether the critical questions or the structured planning is driving the performance gains.

3. **Cross-Domain Generalization**: Test CQoT on non-mathematical reasoning tasks including causal reasoning, ethical dilemmas, and creative problem-solving to assess whether the Toulmin-based critical questions generalize beyond the evaluated domains.