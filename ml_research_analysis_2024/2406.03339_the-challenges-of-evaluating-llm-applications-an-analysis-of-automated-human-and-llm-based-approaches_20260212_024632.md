---
ver: rpa2
title: 'The Challenges of Evaluating LLM Applications: An Analysis of Automated, Human,
  and LLM-Based Approaches'
arxiv_id: '2406.03339'
source_url: https://arxiv.org/abs/2406.03339
tags:
- evaluation
- human
- arxiv
- chatbot
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of evaluating LLM-powered chatbot
  applications, particularly focusing on the reliability and effectiveness of different
  evaluation approaches: automated metrics, human evaluation, and LLM-based evaluation.
  The authors propose a factored evaluation mechanism that assesses responses based
  on multiple criteria (e.g., correctness, informativeness, relevance, clarity, and
  hallucinations) using a 5-point Likert scale.'
---

# The Challenges of Evaluating LLM Applications: An Analysis of Automated, Human, and LLM-Based Approaches

## Quick Facts
- **arXiv ID**: 2406.03339
- **Source URL**: https://arxiv.org/abs/2406.03339
- **Reference count**: 40
- **Key outcome**: Factored evaluation reveals granular weaknesses in RAG-based chatbots that holistic approaches miss, but no single evaluation method proves sufficient

## Executive Summary
This paper investigates the reliability and effectiveness of different evaluation approaches for LLM-powered chatbot applications. The authors implement a RAG-based educational chatbot called EdTalk and compare automated metrics (BLEURT), traditional human evaluation, and factored evaluation approaches using both human and LLM (GPT-3.5) assessors. The study reveals that while BLEURT offers rapid assessment, it lacks alignment with human judgment, and traditional human evaluation provides limited insights into specific improvement areas. The factored evaluation framework, which assesses responses across multiple criteria using a 5-point Likert scale, demonstrates superior granularity in identifying weaknesses, though it faces challenges with inter-rater agreement and potential bias from LLM evaluators.

## Method Summary
The authors develop a factored evaluation mechanism that breaks down chatbot response assessment into multiple criteria: correctness, informativeness, relevance, clarity, and hallucinations. They implement this framework using a RAG-based chatbot, EdTalk, designed for educational report queries. The evaluation compares four approaches: automated (BLEURT), traditional human (preferential), factored human, and factored LLM-based evaluation. Human evaluators rate responses on individual factors, while LLM evaluators provide both holistic and factored assessments. The study analyzes correlations between different evaluation methods and examines inter-rater agreement to assess reliability across approaches.

## Key Results
- Factored evaluation (both human and LLM-based) reveals that EdTalk struggles with "Remember" questions despite being RAG-based, information missed by holistic approaches
- BLEURT provides rapid, repeatable assessments but shows poor alignment with human judgment across multiple criteria
- Traditional human evaluation shows only 47% preference rate and limited ability to identify specific improvement areas
- Human inter-rater agreement is low across most factors except clarity, while LLM evaluators systematically overrate their own outputs

## Why This Works (Mechanism)
The factored evaluation approach works by decomposing complex response quality into discrete, measurable dimensions that can be assessed independently. This decomposition allows evaluators to identify specific weaknesses that holistic approaches might average out or miss entirely. The multi-criteria framework captures different aspects of response quality that may not be correlated—a response can be clear but incorrect, or relevant but lacking in informativeness. By using a 5-point Likert scale for each criterion, the approach provides granular feedback that enables targeted improvements to the underlying LLM application.

## Foundational Learning
**Evaluation metrics for text generation**: Understanding different metrics (BLEURT, ROUGE, BERTScore) and their strengths/weaknesses in capturing semantic similarity is crucial for selecting appropriate automated evaluation approaches. *Why needed*: Different metrics capture different aspects of text quality, and choosing the wrong metric can lead to misleading conclusions. *Quick check*: Compare metric outputs on known good/bad responses to understand their behavior.

**Human evaluation methodologies**: Familiarity with Likert scales, preferential ranking, and inter-rater agreement measures (Cohen's kappa, Krippendorff's alpha) is essential for designing reliable human evaluation protocols. *Why needed*: Human evaluation serves as the reference standard but requires careful design to ensure reliability and reproducibility. *Quick check*: Calculate inter-rater agreement on a small sample before full evaluation.

**RAG system evaluation**: Understanding how retrieval quality affects generation quality and how to evaluate both components separately is critical for diagnosing RAG-based system weaknesses. *Why needed*: RAG systems have two failure modes (retrieval and generation) that need different evaluation approaches. *Quick check*: Test retrieval separately from generation using known queries.

## Architecture Onboarding

**Component map**: User Query -> EdTalk RAG System -> Response Generation -> Multiple Evaluation Pipelines (BLEURT, Human Eval, LLM Eval) -> Aggregated Results

**Critical path**: Query input → Retrieval component → Generation component → Response output → Evaluation (factored criteria) → Improvement feedback loop

**Design tradeoffs**: The study prioritizes evaluation comprehensiveness over efficiency, implementing multiple evaluation approaches rather than optimizing for speed. The factored approach trades simplicity for granularity, requiring more evaluator effort but providing more actionable insights. Using GPT-3.5 for LLM evaluation balances capability with computational cost but may not reflect state-of-the-art performance.

**Failure signatures**: Low inter-rater agreement across factors except clarity indicates evaluation criteria may be poorly defined or ambiguous. Systematic overrating by LLM evaluators suggests potential bias or overconfidence in model capabilities. BLEURT misalignment with human judgment indicates automated metrics may not capture nuanced quality aspects.

**First experiments**: 1) Test evaluation framework on a simple QA system with known correct answers to validate scoring logic, 2) Run parallel evaluations with 2-3 human evaluators on the same responses to establish baseline agreement, 3) Compare LLM evaluator outputs when evaluating responses from different model families to detect bias patterns.

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- The evaluation framework focuses specifically on RAG-based educational chatbots, limiting generalizability to other LLM application types
- Small sample size of human evaluators and limited question set may affect robustness of comparisons between evaluation methods
- Use of GPT-3.5 represents an older model that may not reflect current capabilities of more advanced LLMs
- The study does not explore potential calibration techniques for improving consistency across different evaluation approaches

## Confidence
- **High confidence**: Factored evaluation provides more granular insights than holistic approaches; BLEURT lacks alignment with human judgment; human evaluators show low inter-rater agreement
- **Medium confidence**: LLM evaluators systematically overrate their own outputs; no single evaluation method is sufficient
- **Low confidence**: The specific patterns of LLM evaluator behavior would generalize to other model families or evaluation contexts

## Next Checks
1. Replicate the study with a larger, more diverse pool of human evaluators and a broader set of question types to test the stability of observed patterns
2. Compare evaluation results using different LLM models (including newer versions) to assess whether the "overrating" phenomenon is model-specific or more general
3. Implement and test calibration protocols for both human and LLM evaluators to determine if inter-rater agreement can be improved without sacrificing evaluation quality