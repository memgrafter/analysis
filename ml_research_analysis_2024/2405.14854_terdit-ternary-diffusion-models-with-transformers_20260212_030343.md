---
ver: rpa2
title: 'TerDiT: Ternary Diffusion Models with Transformers'
arxiv_id: '2405.14854'
source_url: https://arxiv.org/abs/2405.14854
tags:
- ternary
- training
- diffusion
- quantization
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of efficient deployment of large-scale
  diffusion transformer models, which are resource-intensive due to their large parameter
  counts. The authors propose TerDiT, a quantization-aware training and efficient
  deployment scheme for extremely low-bit diffusion transformer models, specifically
  focusing on ternarization (weights limited to {-1, 0, +1}).
---

# TerDiT: Ternary Diffusion Models with Transformers

## Quick Facts
- arXiv ID: 2405.14854
- Source URL: https://arxiv.org/abs/2405.14854
- Reference count: 40
- Primary result: TerDiT achieves competitive image generation quality compared to full-precision models while reducing memory usage by 8× during inference and 10× for checkpoints

## Executive Summary
This paper addresses the challenge of deploying large-scale Diffusion Transformer (DiT) models by introducing TerDiT, a quantization-aware training framework that ternarizes model weights to {-1, 0, +1}. The method achieves competitive image generation quality with dramatically reduced memory footprint, enabling efficient inference on consumer GPUs. By modifying the adaLN module with RMS Norm layers and training from scratch using straight-through estimation, TerDiT maintains FID scores comparable to full-precision Large-DiT-4.2B while using less than 2GB GPU memory for inference.

## Method Summary
TerDiT employs quantization-aware training (QAT) from scratch to ternarize DiT models, replacing full-precision linear layers with weights constrained to {-1, 0, +1}. The core innovation involves adding an RMS Norm layer after the MLP in the adaLN module to stabilize training, as ternary weights otherwise cause large activation values that destabilize the network. The quantization function uses absmean with α scaling, and the straight-through estimator enables gradient flow through the non-differentiable quantization operation. Models are trained at various scales (600M to 4.2B parameters) on ImageNet, with 2-bit deployment using pack_2bit_u8/unpack_2bit_u8 for efficient inference.

## Key Results
- TerDiT-4.2B achieves similar FID scores to full-precision Large-DiT-4.2B on ImageNet generation
- Memory usage reduced by 8× during inference and 10× for model checkpoints
- Larger models (4.2B) maintain better quality after quantization than smaller ones (600M), validating scaling law effects
- Supports 256×256 and 512×512 resolution generation with competitive quality metrics

## Why This Works (Mechanism)

### Mechanism 1
Ternary quantization combined with QAT from scratch maintains competitive image generation quality while significantly reducing model size and memory usage. The method replaces full-precision linear layers with ternary weights {-1, 0, +1} and trains from scratch using straight-through estimator (STE), allowing gradients to flow through the quantization function. An RMS Norm layer added after the MLP in the adaLN module stabilizes training by preventing large activation values that would otherwise destabilize the network. Core assumption: Large-scale models have sufficient precision redundancy such that extreme quantization (ternary) can maintain performance when combined with QAT from scratch.

### Mechanism 2
The RMS Normalized adaLN module effectively mitigates training instability caused by ternary weights in the adaLN module. The ternary linear layers in adaLN cause large activation values due to weight quantization. Adding RMS Norm after the MLP in adaLN scales these activations to reasonable ranges, preventing large dimension-wise scale and shift values that would otherwise hamper training. Core assumption: The adaLN module's absence of layer normalization makes it particularly vulnerable to ternary quantization effects, and normalization can effectively compensate.

### Mechanism 3
Large parameter scaling in DiT models enables successful ternary quantization by providing sufficient redundancy. The paper scales ternary DiT models from 600M to 4.2B parameters, demonstrating that larger models can maintain performance after ternary quantization better than smaller models. This aligns with observations that larger models have more precision redundancy. Core assumption: The scaling law observed in full-precision DiTs also applies to ternary DiTs, where larger models can better absorb the quantization noise.

## Foundational Learning

- Concept: Diffusion Models and Denoising Process
  - Why needed here: Understanding how DiTs work as transformer-based diffusion models is essential to grasp why ternarization affects training stability and how the denoising process can tolerate quantization noise.
  - Quick check question: What is the key difference between DiT and traditional U-Net-based diffusion models in terms of architecture?

- Concept: Quantization-aware Training (QAT) and Straight-Through Estimator (STE)
  - Why needed here: The method relies on training from scratch with QAT using STE to allow gradient flow through non-differentiable quantization functions, which is critical for successful ternary training.
  - Quick check question: How does the straight-through estimator enable gradient propagation through the ternary quantization function during training?

- Concept: Layer Normalization and Activation Scaling
  - Why needed here: Understanding how normalization layers affect activation distributions is crucial to grasp why RMS Norm helps stabilize training with ternary weights.
  - Quick check question: Why do ternary linear layers cause larger activation values compared to full-precision layers, and how does normalization help?

## Architecture Onboarding

- Component map: Input (Noised latent patches) → Patchify → Token sequence → Ternary DiT blocks (RMS Norm → Scale, Shift → Key-Query → Normed Ternary MHSA → Scale → RMS Norm → Scale, Shift → SwiGLU Ternary Feedforward → Scale → Conditioning) → RMS Norm → Linear and Reshape → Σ → Noised latent output

- Critical path: Input → Patchify → Ternary DiT blocks → Linear/Reshape → Output
  - Most critical: The ternary DiT blocks, especially the adaLN modifications

- Design tradeoffs:
  - Memory vs. Quality: Ternary quantization reduces memory by ~16× but may degrade quality
  - Training Stability vs. Speed: RMS Norm improves stability but adds computation
  - Model Size vs. Performance: Larger models (4.2B) perform better after quantization than smaller ones (600M)

- Failure signatures:
  - Large activation values in adaLN module leading to NaN or Inf values
  - Poor convergence indicated by loss plateauing at high values
  - Visual artifacts in generated images showing quantization noise
  - Memory usage not reducing as expected (indicates packing/deployment issues)

- First 3 experiments:
  1. Train a small ternary DiT (e.g., 300M parameters) without RMS Norm to observe activation explosion and training failure
  2. Add RMS Norm to adaLN and compare activation distributions and training stability
  3. Scale up to 600M and 4.2B models to verify scaling law effects on ternary quantization performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the training stability and convergence speed of ternary DiT models compare to full-precision DiT models across different model sizes and resolutions?
- Basis in paper: The paper mentions that "training ternary DiT models is less stable and more time-consuming than training full-precision networks" and discusses methods to enhance training stability through adding RMS Norm layers.
- Why unresolved: While the paper provides some qualitative and quantitative comparisons of training loss and FID scores, a comprehensive analysis of training stability and convergence speed across different model sizes (600M to 4.2B parameters) and resolutions (256×256 to 512×512) is not fully explored.
- What evidence would resolve it: Detailed training curves comparing convergence speed, loss stability, and final model performance across multiple runs of both ternary and full-precision DiT models at various scales would provide concrete evidence.

### Open Question 2
What is the impact of ternary quantization on the long-range dependencies and attention mechanisms in DiT models compared to full-precision models?
- Basis in paper: The paper discusses replacing linear layers in self-attention mechanisms with ternary linear layers but does not provide specific analysis of how this affects the attention patterns or long-range dependency modeling capabilities of the DiT architecture.
- Why unresolved: The DiT model relies heavily on transformer-based attention mechanisms, and ternary quantization could potentially affect the model's ability to capture long-range dependencies. The paper does not investigate this specific aspect.
- What evidence would resolve it: Comparative analysis of attention weight distributions, attention pattern visualizations, and tasks specifically designed to test long-range dependency modeling between ternary and full-precision DiT models would provide insights.

### Open Question 3
How does the performance of TerDiT scale with different quantization methods beyond ternary (e.g., binary or 2-bit quantization) and what are the trade-offs in terms of quality versus deployment efficiency?
- Basis in paper: The paper focuses on ternary quantization and compares against some 2-bit quantization baselines, but does not provide a comprehensive analysis of how different quantization levels affect performance and efficiency.
- Why unresolved: While the paper demonstrates the effectiveness of ternary quantization, it does not explore the full spectrum of low-bit quantization methods or provide a systematic comparison of trade-offs across different bit-widths.
- What evidence would resolve it: A systematic study comparing model quality (FID scores, etc.) and deployment efficiency (memory usage, inference time) across binary, ternary, and 2-bit quantized versions of DiT models would provide clear evidence of the trade-offs involved.

## Limitations

- Architecture Specificity: The RMS Norm modification is specifically designed for adaLN modules in DiT architecture and may not generalize to other transformer architectures or diffusion models using different normalization schemes.

- Scaling Limits: While successful at 600M to 4.2B parameters, the method's effectiveness at larger scales (>10B parameters) or smaller scales (<300M parameters) remains unverified, and scaling law observations may break down at these extremes.

- Training Overhead: The RMS Norm addition, while stabilizing training, introduces additional computational overhead that may impact training efficiency, particularly during early training phases.

## Confidence

- High Confidence: Claims about memory reduction (8× less inference memory, 10× smaller checkpoints) are well-supported by empirical measurements and standard quantization theory.

- Medium Confidence: Performance claims (FID scores, generation quality) are validated on ImageNet but may not transfer to other datasets or generation tasks. The scaling law observations show consistent patterns within the tested range.

- Low Confidence: Generalizability claims to other transformer architectures or diffusion model variants remain speculative without additional experiments.

## Next Checks

1. **Architecture Transfer**: Test the Ternary DiT approach on non-DiT diffusion architectures (e.g., U-Net-based models) to verify if RMS Norm modifications are universally beneficial for ternary quantization.

2. **Scaling Boundary Analysis**: Train ternary models at 100M and 10B+ parameter scales to identify the boundaries where the scaling law breaks down and determine minimum viable model sizes.

3. **Fine-tuning vs. Scratch Training**: Compare quantization-aware training from scratch against post-training quantization followed by fine-tuning to quantify the benefit of the proposed approach.