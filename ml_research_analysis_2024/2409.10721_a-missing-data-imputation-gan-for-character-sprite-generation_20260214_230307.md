---
ver: rpa2
title: A Missing Data Imputation GAN for Character Sprite Generation
arxiv_id: '2409.10721'
source_url: https://arxiv.org/abs/2409.10721
tags:
- images
- image
- missing
- character
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of automatically generating missing
  poses for pixel art character sprites in game development. The authors frame this
  as a missing data imputation task, proposing a novel Generative Adversarial Network
  (GAN) architecture based on CollaGAN that takes character images in all available
  poses as input and generates the missing one.
---

# A Missing Data Imputation GAN for Character Sprite Generation

## Quick Facts
- arXiv ID: 2409.10721
- Source URL: https://arxiv.org/abs/2409.10721
- Reference count: 40
- Primary result: Proposed GAN achieves FID of 1.508 vs 2.288 (StarGAN) and 4.091 (Pix2Pix) on pixel art sprite generation

## Executive Summary
This work addresses the challenge of automatically generating missing poses for pixel art character sprites in game development by framing it as a missing data imputation task. The authors propose a novel GAN architecture based on CollaGAN that takes character images in all available poses as input and generates the missing one. Evaluated on a dataset of 14,202 pixel art character sprites across four poses, the model achieves superior or comparable results to baseline models in terms of FID and L1 metrics.

## Method Summary
The proposed model receives all available poses of a character as input and generates the missing pose using a single generator with four encoder branches (one per pose) and skip connections to a shared decoder. The architecture includes increased capacity with 4× more channels than the original CollaGAN, a conservative input dropout strategy during training (60% keep all, 30% drop 1, 10% drop 2), and a modified replacement tactic where the generated target image only replaces the original target in the backward pass. The model is trained for 240k steps with minibatches of 4 using Adam optimizer and specific loss weight parameters.

## Key Results
- FID scores: Proposed model achieves 1.508 vs 2.288 for StarGAN and 4.091 for Pix2Pix
- L1 distances: Proposed model achieves 0.04078 vs 0.06577 for StarGAN and 0.05273 for Pix2Pix
- The model demonstrates good performance when fewer input images are available, though quality decreases progressively
- Conservative input dropout strategy improves performance in scenarios with limited available poses

## Why This Works (Mechanism)

### Mechanism 1
The proposed GAN architecture achieves superior sprite generation by leveraging collaborative multi-domain input and a modified CollaGAN architecture. The model receives all available poses of a character as input and generates the missing pose, using a single generator with multiple encoder branches and skip connections to the decoder. This allows the model to capture shared features across poses while preserving pose-specific details. Core assumption: Pixel art sprites from the same character share sufficient structural and stylistic features across poses to enable collaborative imputation.

### Mechanism 2
The modified training procedure with conservative input dropout and forward-only replacement improves model performance, especially when fewer images are available. The model is trained with random omission of input images to simulate real-world scenarios with missing data. The conservative strategy increases training with fewer missing images, while the forward-only replacement ensures the generated target image is used only for its intended pose during backward passes. Core assumption: Training with partial data availability improves generalization to scenarios with fewer available poses.

### Mechanism 3
The increased model capacity with four encoder branches and larger channel dimensions improves sprite generation quality compared to the original CollaGAN. The generator architecture is modified to have four encoder branches (one per pose) instead of eight, with channel dimensions increased to 64, 128, 256, 512, and 1024 for encoder blocks and 1024, 512, 256, 128, and 64 for decoder blocks. This provides more capacity to learn complex pixel art patterns while maintaining computational efficiency. Core assumption: Pixel art sprite generation requires sufficient model capacity to capture fine-grained color and shape details.

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: The core approach relies on adversarial training between a generator and discriminator to produce realistic missing sprite poses.
  - Quick check question: What are the two main components of a GAN and what are their respective roles during training?

- Concept: Image-to-Image Translation
  - Why needed here: The problem is framed as translating among different character poses while maintaining character identity.
  - Quick check question: How does multi-domain image-to-image translation differ from standard two-domain translation in terms of architecture requirements?

- Concept: Missing Data Imputation
  - Why needed here: The core framing of the problem treats missing poses as missing data that needs to be imputed using available information.
  - Quick check question: What are the key differences between missing data imputation in tabular data versus image data?

## Architecture Onboarding

- Component map: Input images → Four encoder branches (one per source domain) → Bottleneck concatenation → Single decoder with skip connections → Generated sprite → Discriminator evaluation → Loss computation → Parameter updates

- Critical path: Input images → encoder branches → bottleneck fusion → decoder → generated sprite → discriminator evaluation → loss computation → parameter updates

- Design tradeoffs: Single generator with multiple inputs vs. multiple specialized generators; increased capacity vs. computational efficiency; conservative dropout vs. curriculum learning

- Failure signatures: Artifacts from wrong domains (e.g., wrong number of eyes), color inconsistencies across poses, blurry or low-resolution outputs, overfitting to training styles

- First 3 experiments:
  1. Train with all four poses available to establish baseline performance and verify architecture functionality
  2. Systematically remove one pose at a time to evaluate generation quality for each missing pose scenario
  3. Test with different input dropout strategies (none, original, curriculum, conservative) to identify optimal training approach

## Open Questions the Paper Calls Out

### Open Question 1
How does the model's performance change when using non-humanoid characters (e.g., animals, vehicles, monsters) that were underrepresented in the training dataset? The authors mention their dataset contains "primarily humanoid characters" but also "a few sprites of animals, vehicles, and monsters," yet they don't evaluate performance on these non-humanoid classes specifically.

### Open Question 2
What is the optimal architecture capacity for this task - would further increasing model size beyond the 4× capacity improvement yield better results, or is there a point of diminishing returns? The authors progressively increased capacity from the original CollaGAN architecture and saw improvements, but only tested one scaling factor (4× original), leaving the relationship between capacity and performance unexplored.

### Open Question 3
How would alternative input dropout strategies, such as domain-specific dropout probabilities based on pose complexity, affect model performance compared to the uniform conservative strategy? The authors tested three dropout strategies but used uniform probabilities across all domains, without considering that some poses might be more complex to generate than others.

## Limitations

- The evaluation is based on a single dataset of pixel art character sprites, limiting generalizability to other art styles or character types
- The paper does not explore edge cases with only one or two available poses, which would be the most challenging real-world scenarios
- The increased model capacity significantly raises computational requirements and may not scale well to higher resolution sprites or larger character sets

## Confidence

**High confidence**: The core claim that the proposed GAN architecture can generate missing character poses with superior FID and L1 metrics compared to baseline models is well-supported by the experimental results.

**Medium confidence**: The generalizability of the approach to different sprite styles, resolutions, or character types remains uncertain due to the single dataset evaluation.

**Low confidence**: The claim that the forward-only replacement strategy significantly improves results compared to the original CollaGAN implementation is based on internal comparisons rather than comprehensive ablation studies.

## Next Checks

1. Test the trained model on a different set of pixel art sprites (e.g., from a different game or artist) to assess generalizability and identify any overfitting to the training dataset's specific style characteristics.

2. Systematically evaluate model performance when only one or two poses are available as input, measuring the progressive degradation in quality and identifying failure modes.

3. Conduct controlled experiments removing each modification (increased capacity, conservative dropout, forward replacement) to quantify their individual contributions to the overall performance improvement.