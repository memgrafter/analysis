---
ver: rpa2
title: Closing the Gap between TD Learning and Supervised Learning -- A Generalisation
  Point of View
arxiv_id: '2401.11237'
source_url: https://arxiv.org/abs/2401.11237
tags:
- data
- stitching
- augmentation
- policy
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the stitching property in reinforcement learning,
  which allows combining pieces of experience to solve tasks not seen during training.
  The authors formalize stitching as a form of combinatorial generalization, where
  the goal is to evaluate on (state, goal) pairs not seen together during training.
---

# Closing the Gap between TD Learning and Supervised Learning -- A Generalisation Point of View

## Quick Facts
- arXiv ID: 2401.11237
- Source URL: https://arxiv.org/abs/2401.11237
- Reference count: 40
- Key outcome: Shows that standard supervised learning-based RL methods lack combinatorial generalization (stitching) and proposes temporal data augmentation to enable this capability

## Executive Summary
This paper identifies a fundamental limitation in supervised learning-based RL methods: their inability to perform combinatorial generalization, which the authors term "stitching." Stitching refers to the ability to combine pieces of experience to solve tasks not seen during training, specifically evaluating on (state, goal) pairs that were never visited together in the same trajectory. Through theoretical analysis and empirical experiments, the authors demonstrate that standard OCBC methods fail at this task even with large datasets and models. They propose a simple yet effective solution: temporal data augmentation that samples goals from different overlapping trajectories in the dataset, enabling SL-based RL methods to successfully complete tasks not seen together during training and improve performance by up to 2.5x.

## Method Summary
The paper studies goal-conditioned and return-conditioned RL problems using Outcome-Conditioned Behavioral Cloning (OCBC) methods like RvS and DT. The key innovation is temporal data augmentation, which augments the original goal with a new goal sampled from a different overlapping trajectory in the dataset. This is implemented by clustering states using K-means, sampling waypoint states near the original goal, then sampling new goals from the future of these waypoints. The method is evaluated on point mazes, ant mazes, and image-based Miniworld environments, comparing success rates and episodic returns with and without augmentation across varying dataset sizes.

## Key Results
- Standard OCBC methods fail to perform combinatorial generalization, showing poor performance on (state, goal) pairs not seen together during training
- Temporal data augmentation enables OCBC methods to successfully complete tasks not seen together, improving performance by up to 2.5x
- The augmentation method works across diverse environments including point mazes, ant mazes, and high-dimensional image-based Miniworld tasks
- Larger datasets do not automatically improve combinatorial generalization in OCBC methods without augmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stitching corresponds to combinatorial generalization in RL.
- Mechanism: When data are collected from a mixture of policies, certain (state, goal) pairs are never visited together in the same trajectory, despite being frequented separately. Information from multiple trajectories must be stitched to complete these tasks. This is a form of combinatorial generalization because it requires inferring solutions to tasks where (state, goal) pairs are seen separately but never together.
- Core assumption: The data collection process involves multiple context-conditioned policies, and the goal is to evaluate on pairs not seen together during training.
- Evidence anchors:
  - [abstract] "Our main result is to show that the stitching property corresponds to a form of combinatorial generalization: after training on a distribution of (state, goal) pairs, one would like to evaluate on (state, goal) pairs not seen together in the training data."
  - [section] "Our analysis shows that this sort of generalization is different from i.i.d. generalization."
  - [corpus] Weak corpus support; no directly related papers found discussing stitching as combinatorial generalization.
- Break condition: If the data collection process does not involve multiple context-conditioned policies, or if the goal is to evaluate on pairs seen together during training.

### Mechanism 2
- Claim: Standard supervised learning-based RL methods lack the stitching property.
- Mechanism: OCBC methods are trained on (state, goal) pairs from individual trajectories. They cannot generalize to pairs not seen together because they lack an explicit mechanism for stitching. Even with large datasets and models, they do not perform combinatorial generalization.
- Core assumption: OCBC methods rely on supervised learning objectives and do not incorporate dynamic programming.
- Evidence anchors:
  - [abstract] "Our analysis shows that this sort of generalization is different from i.i.d. generalization. This connection between stitching and generalisation reveals why we should not expect SL-based RL methods to perform stitching, even in the limit of large datasets and models."
  - [section] "Because of this connection, we hypothesize that OCBC methods do not perform stitching."
  - [corpus] No directly related papers found discussing SL-based RL methods lacking stitching property.
- Break condition: If OCBC methods are augmented with mechanisms that enable stitching, such as the proposed temporal data augmentation.

### Mechanism 3
- Claim: Temporal data augmentation enables combinatorial generalization in SL-based RL methods.
- Mechanism: Temporal data augmentation augments the original goal with a new goal sampled from a different overlapping trajectory in the dataset. This allows the model to see (state, goal) pairs that were not present in the original dataset, enabling combinatorial generalization.
- Core assumption: There exists a distance metric in the state space to identify overlapping trajectories.
- Evidence anchors:
  - [abstract] "Based on this analysis, we construct new datasets to explicitly test for this property, revealing that SL-based methods lack this stitching property and hence fail to perform combinatorial generalization. Nonetheless, the connection between stitching and combinatorial generalisation also suggests a simple remedy for improving generalisation in SL: data augmentation."
  - [section] "We propose a form of temporal data augmentation for OCBC methods so that they acquire this stitching property and succeed in navigating between unseen (start, goal) pairs or achieving greater returns than the offline dataset."
  - [corpus] No directly related papers found discussing temporal data augmentation for RL.
- Break condition: If the distance metric is not reliable or if the overlapping trajectories cannot be identified.

## Foundational Learning

- Concept: Controlled Markov Processes
  - Why needed here: The paper studies goal-conditioned RL in a controlled Markov process with states, actions, and dynamics.
  - Quick check question: What is the difference between the discounted state occupancy distribution and its conditional counterpart?

- Concept: Combinatorial Generalization
  - Why needed here: The paper formalizes stitching as a form of combinatorial generalization, where the goal is to evaluate on (state, goal) pairs not seen together during training.
  - Quick check question: How does combinatorial generalization differ from i.i.d. generalization in the context of RL?

- Concept: Data Augmentation
  - Why needed here: The paper proposes a form of temporal data augmentation to enable combinatorial generalization in SL-based RL methods.
  - Quick check question: How does temporal data augmentation differ from standard data augmentation techniques in SL?

## Architecture Onboarding

- Component map: OCBC policy network -> K-means clustering -> Temporal data augmentation module -> Augmented training dataset
- Critical path: The training loop where the OCBC policy is trained on the augmented dataset
- Design tradeoffs: The choice of distance metric for clustering states and the number of clusters (K) in the k-means algorithm are important design tradeoffs
- Failure signatures: If the distance metric is not reliable, the temporal data augmentation may not work as intended. If K is too small, the augmentation may not be effective. If K is too large, the augmentation may introduce noise.
- First 3 experiments:
  1. Train the OCBC policy on the original dataset without augmentation and evaluate its performance on unseen (state, goal) pairs.
  2. Apply the temporal data augmentation with different values of K and evaluate the performance of the augmented OCBC policy.
  3. Compare the performance of the augmented OCBC policy with that of a TD-learning based method on the same task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we eliminate the need for a distance metric in temporal data augmentation for combinatorial generalization in RL?
- Basis in paper: [inferred] The paper acknowledges that providing a good distance metric, especially for high-dimensional states, is a limitation of their temporal data augmentation approach. It suggests that lifting this assumption and developing scalable OCBC algorithms that generalize is a promising direction for future work.
- Why unresolved: Developing a method that can perform combinatorial generalization without relying on a distance metric is a challenging open problem in the field. It requires novel approaches to identify and leverage temporal relationships in sequential data.
- What evidence would resolve it: A new algorithm or approach that demonstrates successful combinatorial generalization in RL tasks without using any distance metric or clustering of states.

### Open Question 2
- Question: Can temporal data augmentation be effectively applied to high-dimensional image-based tasks in RL?
- Basis in paper: [explicit] The paper shows that temporal data augmentation, using a simple L2 distance metric, can improve combinatorial generalization of OCBC algorithms even on high-dimensional image-based tasks. However, it also acknowledges that this approach is not guaranteed to succeed in all cases and there remains room for other scalable and robust methods.
- Why unresolved: The effectiveness of temporal data augmentation on high-dimensional image-based tasks is not fully established. The paper demonstrates some success but also highlights the limitations and the need for further research.
- What evidence would resolve it: Extensive empirical studies comparing the performance of temporal data augmentation with other methods on a wide range of high-dimensional image-based RL tasks, demonstrating consistent improvements in combinatorial generalization.

### Open Question 3
- Question: How does the choice of the number of centroids (k) in the k-means algorithm affect the performance of temporal data augmentation?
- Basis in paper: [explicit] The paper ablates the choice of the number of centroids used in k-means on two environments ("point" maze-medium and "ant" maze-medium). It shows that all choices of centroids significantly outperform the RvS method on both tasks.
- Why unresolved: While the paper demonstrates that different choices of k can lead to improvements, the optimal choice of k for different tasks and environments is not determined. It remains an open question how to select the best k for a given problem.
- What evidence would resolve it: A comprehensive study analyzing the relationship between the choice of k and the performance of temporal data augmentation across various RL tasks and environments, providing guidelines for selecting the optimal k.

## Limitations

- The effectiveness of temporal data augmentation depends on the quality of the distance metric for clustering states, which may be challenging in high-dimensional spaces
- The analysis primarily focuses on goal-conditioned and return-conditioned tasks, leaving open questions about whether similar limitations apply to other RL formulations
- The paper assumes data collection from mixture policies, which may not generalize to all RL settings

## Confidence

- **High Confidence:** The experimental demonstration that OCBC methods lack combinatorial generalization and that temporal data augmentation improves performance
- **Medium Confidence:** The theoretical framing of stitching as a form of combinatorial generalization
- **Medium Confidence:** The general applicability of these findings to broader RL settings beyond the specific tasks studied

## Next Checks

1. Test the temporal data augmentation method with alternative distance metrics (e.g., learned embeddings) to verify robustness to the choice of state similarity measure, particularly for image-based tasks
2. Evaluate whether the stitching property is essential for data efficiency by comparing the augmented OCBC methods against TD-learning approaches across varying dataset sizes
3. Investigate whether the findings extend to other RL problem settings such as continuous control tasks without explicit goal conditioning or multi-task RL scenarios