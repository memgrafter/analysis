---
ver: rpa2
title: 'Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot
  In-Context Learning'
arxiv_id: '2410.05600'
source_url: https://arxiv.org/abs/2410.05600
tags:
- post
- text
- rationale
- hate
- hateful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates cross-modality knowledge transfer for hate
  speech detection using few-shot in-context learning. The researchers evaluated whether
  text-based hate speech examples could improve the classification of vision-language
  hate speech (memes) using large language models.
---

# Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning

## Quick Facts
- **arXiv ID**: 2410.05600
- **Source URL**: https://arxiv.org/abs/2410.05600
- **Reference count**: 25
- **Key outcome**: Text-based hate speech examples significantly enhance vision-language hate speech detection accuracy using few-shot in-context learning

## Executive Summary
This study investigates cross-modality knowledge transfer for hate speech detection, exploring whether text-based hate speech examples can improve vision-language hate speech classification. Using two large language models (Mistral-7B and Qwen2-7B) and two vision-language datasets (FHM and MAMI), the researchers demonstrate that text-based demonstrations outperform both zero-shot performance and vision-language demonstrations in few-shot learning settings. The work shows that text-based data can serve as a valuable resource for improving multimodal tasks, particularly in low-resource scenarios, with up to 1.23 F1 point improvements observed.

## Method Summary
The method employs few-shot in-context learning where large language models classify vision-language hate speech using text-based demonstrations. Memes are first processed through an image captioning model (OFA) to extract textual descriptions. Relevant examples are retrieved from a text-based support dataset (Latent Hatred) using TF-IDF and BM25 similarity metrics. The models are evaluated across different shot configurations (4, 8, 16) and sampling strategies (Random, TF-IDF, BM-25), with performance compared against zero-shot baselines. Two models are tested: Mistral-7B and Qwen2-7B.

## Key Results
- Text-based hate speech examples significantly enhance vision-language hate speech classification accuracy
- Best few-shot in-context learning performance exceeded zero-shot performance by up to 1.23 F1 points
- Text-based demonstrations outperformed vision-language demonstrations in few-shot learning settings
- Cross-modality knowledge transfer shows particular promise for low-resource scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-based hate speech examples can transfer knowledge to vision-language hate speech detection
- Mechanism: Large language models can learn patterns from text demonstrations and apply them to multimodal content when given image captions and text
- Core assumption: Hate speech detection is fundamentally about identifying harmful language patterns
- Evidence anchors: Text-based demonstrations significantly enhance classification accuracy; text-based support set outperformed vision-language support set
- Break condition: When visual component contains hate speech that cannot be expressed through text

### Mechanism 2
- Claim: Few-shot in-context learning with text demonstrations outperforms zero-shot performance
- Mechanism: Demonstrations provide context and examples that help the model understand what constitutes hate speech across different formats
- Core assumption: Models can generalize from text examples to multimodal examples when given proper context
- Evidence anchors: Best few-shot performance shows significant improvement over zero-shot; text-based demonstrations outperform vision-language demonstrations
- Break condition: When demonstrations contain conflicting information or the model cannot understand text-visual connections

### Mechanism 3
- Claim: Text-based data is a valuable resource for improving performance on multimodal tasks in low-resource settings
- Mechanism: The abundance and diversity of text-based data can compensate for the scarcity of multimodal data
- Core assumption: Text-based hate speech detection capabilities can be transferred to multimodal formats when multimodal data is limited
- Evidence anchors: Text-based data can serve as a valuable resource for improving performance on multimodal tasks, particularly in low-resource settings
- Break condition: When multimodal content requires understanding of visual elements inadequately described through text captions

## Foundational Learning

- Concept: Few-shot in-context learning
  - Why needed here: The paper uses this technique to transfer knowledge from text to vision-language hate speech detection without fine-tuning
  - Quick check question: What is the difference between zero-shot, few-shot, and fine-tuning approaches in machine learning?

- Concept: Cross-modality knowledge transfer
  - Why needed here: The paper investigates whether knowledge from one modality (text) can be applied to another (vision-language)
  - Quick check question: How does knowledge transfer work between different data modalities in machine learning?

- Concept: TF-IDF and BM25 similarity metrics
  - Why needed here: These are used to retrieve relevant examples from the support set for demonstrations
  - Quick check question: What is the difference between TF-IDF and BM25, and when would you use each?

## Architecture Onboarding

- Component map:
  Meme -> Image Captioning (OFA) -> Text + Caption
  Text + Caption + Support Set -> Similarity Retrieval (TF-IDF, BM-25) -> Relevant Examples
  Relevant Examples + Mistral-7B -> Rationale Generation -> Few-shot Prompt
  Few-shot Prompt + Mistral-7B/Qwen2-7B -> Classification

- Critical path:
  1. Input meme → image captioning → text + caption
  2. Retrieve relevant examples from support set using similarity metrics
  3. Generate rationales for examples using Mistral-7B
  4. Create few-shot prompt with demonstrations
  5. Classify meme using the model

- Design tradeoffs:
  - Using text-based support vs. vision-language support (text-based is more effective)
  - Random sampling vs. similarity-based sampling for demonstrations
  - Number of shots (4, 8, 16) vs. performance gains
  - Using rationale generation vs. just using raw examples

- Failure signatures:
  - Model performance worse than zero-shot (red values in tables)
  - High number of invalid outputs (as seen in LLaVA-7B results)
  - Inconsistent performance across different sampling strategies

- First 3 experiments:
  1. Zero-shot classification on FHM and MAMI datasets to establish baseline
  2. Few-shot in-context learning with Latent Hatred support set using random sampling
  3. Few-shot in-context learning with Latent Hatred support set using TF-IDF and BM-25 sampling strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of cross-modality knowledge transfer scale with larger vision-language models compared to text-only models?
- Basis in paper: The authors note they only tested two 7B parameter models and acknowledge that evaluating larger models could provide different insights
- Why unresolved: The paper explicitly states this as a limitation, having only evaluated Mistral-7B and Qwen2-7B models due to computational constraints
- What evidence would resolve it: Direct comparison experiments using both larger text-only models and dedicated vision-language models across the same datasets

### Open Question 2
- Question: What specific characteristics of text-based hate speech examples make them more effective than vision-language examples for few-shot learning in multimodal hate speech detection?
- Basis in paper: The authors observe that text-based demonstrations outperform vision-language demonstrations in few-shot settings and speculate this might be due to oversimplification of visual information into captions
- Why unresolved: The paper acknowledges the discrepancy but only offers speculation without systematic investigation into the underlying reasons
- What evidence would resolve it: Controlled experiments isolating different factors (caption quality, topic diversity, example relevance) and measuring their individual impact

### Open Question 3
- Question: How can the issue of false positives in cross-modality knowledge transfer be systematically addressed without compromising the detection of actual hate speech?
- Basis in paper: The authors identify that text-based demonstrations can lead to overgeneralization, causing neutral content to be misclassified as hateful
- Why unresolved: While the authors identify this as a limitation and provide case studies, they don't propose concrete solutions or mitigation strategies
- What evidence would resolve it: Development and evaluation of filtering mechanisms for demonstration examples, or techniques to calibrate model sensitivity

## Limitations
- Focus on English-only datasets constrains generalizability to multilingual contexts
- Reliance on image captioning introduces potential information loss of visual hate speech elements
- Evaluation metrics do not capture full complexity of hate speech detection (false positives/negatives, censorship concerns)

## Confidence

**High Confidence**: Text-based demonstrations outperform zero-shot performance in cross-modality hate speech detection (1.23 F1 improvement maximum). Well-supported by multiple experiments across two different models and two datasets.

**Medium Confidence**: Cross-modality knowledge transfer is particularly valuable in low-resource settings. Based on the abundance of text data versus multimodal data, though not directly tested in resource-constrained scenarios.

**Low Confidence**: Generalizability to non-English contexts and different types of multimodal hate speech (audio-visual content, GIFs, emerging formats). Study's narrow focus on English memes limits broader applicability claims.

## Next Checks
1. **Multilingual Validation**: Replicate cross-modality experiments using non-English text-based hate speech datasets (Spanish, Arabic, Chinese) and corresponding vision-language datasets to test language-agnostic transfer capabilities.

2. **Visual-Only Detection Test**: Create controlled experiment where visual elements contain hate speech that cannot be expressed through text captions, then measure whether text-based demonstrations still provide benefits or whether visual understanding is essential.

3. **Longitudinal Performance Evaluation**: Track model performance over time using temporally segmented data to assess whether cross-modality knowledge transfer maintains effectiveness as hate speech patterns evolve, and whether periodic retraining with new text demonstrations is necessary.