---
ver: rpa2
title: 'GrEmLIn: A Repository of Green Baseline Embeddings for 87 Low-Resource Languages
  Injected with Multilingual Graph Knowledge'
arxiv_id: '2409.18193'
source_url: https://arxiv.org/abs/2409.18193
tags:
- embeddings
- languages
- glove
- language
- static
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GrEmLIn, a repository of static word embeddings
  for 87 mid- and low-resource languages. The embeddings are created by enhancing
  GloVe embeddings with multilingual graph knowledge from ConceptNet, using a novel
  method that projects the embeddings into a combined space.
---

# GrEmLIn: A Repository of Green Baseline Embeddings for 87 Low-Resource Languages Injected with Multilingual Graph Knowledge

## Quick Facts
- **arXiv ID**: 2409.18193
- **Source URL**: https://arxiv.org/abs/2409.18193
- **Reference count**: 37
- **Primary result**: Graph-enhanced static embeddings outperform FastText and remain competitive with contextualized models on low-resource language tasks

## Executive Summary
This paper introduces GrEmLIn, a repository of static word embeddings for 87 mid- and low-resource languages. The embeddings are created by enhancing GloVe embeddings with multilingual graph knowledge from ConceptNet, using a novel method that projects the embeddings into a combined space. Experiments show that these graph-enhanced embeddings outperform baseline models like FastText and contextualized embeddings like XLM-R on tasks such as lexical similarity, sentiment analysis, and natural language inference, with average performance gaps of 5-10% or less compared to state-of-the-art models. They are especially effective when there is sufficient vocabulary overlap with the target task. The embeddings are publicly available and offer a lightweight, parameter-free alternative for low-resource scenarios.

## Method Summary
GrEmLIn creates static word embeddings by combining GloVe co-occurrence statistics with multilingual graph knowledge from ConceptNet. The method first trains GloVe embeddings on CC100 corpus data, then constructs PPMI-based graph embeddings from ConceptNet. These embeddings are merged through SVD on the concatenated vectors for shared vocabulary, creating a unified semantic space. A linear transformation learned via gradient descent projects the full GloVe vocabulary into this enriched space. The approach handles languages with varying amounts of training data through adaptive vocabulary filtering thresholds and supports both language-specific and multilingual graph embedding configurations.

## Key Results
- GrEmLIn embeddings outperform FastText on sentiment analysis for 27 out of 57 languages
- Graph-enhanced GloVe embeddings match or exceed XLM-R performance on lexical similarity tasks
- Performance gaps versus contextualized models remain under 10% on sentiment analysis and NLI tasks
- Improvement correlates with vocabulary overlap but shows task-dependent variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-enhanced GloVe embeddings outperform static embeddings by injecting multilingual semantic relations from ConceptNet, improving representation quality for low-resource languages.
- Mechanism: The method uses SVD on concatenated GloVe and PMI-based graph embeddings to create a shared semantic space, then learns a linear transformation to project the full GloVe vocabulary into this enriched space.
- Core assumption: The shared vocabulary between GloVe and ConceptNet is sufficiently large and representative to enable accurate linear projection for the entire vocabulary.
- Evidence anchors:
  - [abstract]: "We compute GrEmLIn embeddings with a novel method that enhances GloVe embeddings by integrating multilingual graph knowledge..."
  - [section 3.3]: "We first concatenate GloVe and PPMI vectors for all words that are in the shared vocabulary, resulting in 600-dimensional vectors"
  - [corpus]: Weak - no direct coverage of ConceptNet vocabulary overlap across languages
- Break condition: If the common vocabulary between GloVe and ConceptNet is too small (e.g., <10% overlap), the projection matrix becomes poorly conditioned and degrades performance.

### Mechanism 2
- Claim: Graph-enhanced embeddings remain competitive with contextualized models (E-5, XLM-R) on extrinsic tasks when vocabulary coverage is sufficient, with performance gaps of 5-10% or less.
- Mechanism: Static embeddings enriched with graph knowledge capture enough semantic nuance to match contextual embeddings on simpler tasks (sentiment analysis, NLI) where word-level semantics dominate over contextual variation.
- Core assumption: Task performance depends primarily on word-level semantic accuracy rather than contextual flexibility for the evaluated tasks.
- Evidence anchors:
  - [abstract]: "GrEmLIn embeddings outperform state-of-the-art contextualized embeddings from E5 on the task of lexical similarity... remain competitive in extrinsic evaluation tasks like sentiment analysis and natural language inference, with average performance gaps of just 5-10% or less..."
  - [section 5.4]: "GloVe embeddings outperform FastText for 27 out of 57 languages... GloVe+PPMI (All) enhances performance for 48 out of 57 languages"
  - [corpus]: Moderate - coverage data shows vocabulary overlap varies significantly across languages and tasks
- Break condition: On tasks requiring deep contextual understanding (e.g., topic classification, complex reasoning), static embeddings consistently underperform contextualized models by >20%.

### Mechanism 3
- Claim: The improvement from graph injection correlates moderately with common vocabulary size, but the relationship is non-linear and influenced by graph knowledge quality.
- Mechanism: Larger shared vocabularies enable better linear transformation fits, but the quality and coverage of semantic relations in ConceptNet also significantly impact embedding quality beyond pure vocabulary overlap.
- Core assumption: Semantic graph knowledge provides complementary information that cannot be fully captured by co-occurrence statistics alone.
- Evidence anchors:
  - [section 5.5]: "When comparing results for Single and All configurations, the Single configuration tends to show stronger correlations... This is because Single configurations focus on one language, whereas All configurations include words from multiple languages, which dilutes the strength of the relationship"
  - [section 4.1]: "For 72 of these languages, present in both CC100 and ConceptNet, we generated additional graph embeddings"
  - [corpus]: Moderate - correlation analysis shows mixed results across tasks, with stronger effects in lexical similarity than topic classification
- Break condition: If graph knowledge is sparse or irrelevant for a language (e.g., very limited ConceptNet coverage), the improvement becomes negligible regardless of vocabulary overlap.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) for dimensionality reduction and noise filtering
  - Why needed here: SVD reduces the 600-dimensional concatenated GloVe+PPMI space to a lower-dimensional shared semantic space while preserving the most informative features
  - Quick check question: What happens to the projection quality if we use too few or too many singular values in the truncated SVD?

- Concept: Pointwise Mutual Information (PMI) for graph embedding creation
  - Why needed here: PMI captures the statistical association between terms in ConceptNet, creating embeddings that reflect semantic relationships beyond co-occurrence patterns
  - Quick check question: Why do we apply context distributional smoothing (0.75) before computing PMI in the graph embedding process?

- Concept: Linear transformation via gradient descent optimization
  - Why needed here: After creating the shared space for common vocabulary, we need to project the full GloVe vocabulary into this space using a learned linear mapping
  - Quick check question: What optimization objective are we minimizing when learning the projection matrix W?

## Architecture Onboarding

- Component map: CC100 corpus -> GloVe training -> ConceptNet extraction -> PPMI computation -> SVD fusion -> Linear transformation -> Evaluation
- Critical path: 1. Train GloVe embeddings for target languages 2. Build PPMI graph embeddings from ConceptNet 3. Compute SVD on shared vocabulary to create merged space 4. Learn linear transformation for full vocabulary projection 5. Evaluate on downstream tasks
- Design tradeoffs:
  - Memory vs. coverage: Training separate PPMI spaces per language (Single) vs. using full ConceptNet (All)
  - Dimensionality: 300D chosen as balance between expressiveness and computational efficiency
  - Vocabulary filtering: 5 co-occurrence threshold for languages with >1M tokens vs. 2 for smaller datasets
- Failure signatures:
  - Poor performance on tasks with low vocabulary overlap (<80% GloVe coverage)
  - Degradation when graph knowledge is sparse for target language
  - Overfitting to training data when common vocabulary is very small
- First 3 experiments:
  1. Train GloVe embeddings for a mid-resource language (e.g., Swahili) and evaluate on sentiment analysis
  2. Add PPMI graph embeddings and compute SVD fusion, compare performance on same task
  3. Test linear transformation learning on the full vocabulary and measure vocabulary coverage impact

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several important questions remain unresolved regarding scalability, alternative fusion techniques, and the relative contribution of different knowledge sources.

## Limitations
- Performance heavily dependent on vocabulary overlap between GloVe and ConceptNet, which varies significantly across languages
- Claims of energy efficiency are asserted but not empirically validated through actual measurements
- Limited evaluation to specific task types that may not represent the full complexity of low-resource language applications
- Method's effectiveness constrained by ConceptNet coverage quality for each target language

## Confidence
- **High Confidence**: The core methodology for combining GloVe with ConceptNet-PPMI embeddings through SVD and linear projection is technically sound and well-documented. The vocabulary filtering thresholds and training parameters are clearly specified.
- **Medium Confidence**: The performance claims on extrinsic tasks are supported by experiments but rely on task-specific conditions (vocabulary overlap) that weren't extensively tested across diverse low-resource scenarios. The correlation between vocabulary overlap and performance improvement is observed but shows inconsistent patterns across different tasks.
- **Low Confidence**: The energy efficiency claims lack empirical validation, and the generalizability to complex downstream tasks beyond the evaluated set remains unproven.

## Next Checks
1. **Energy Efficiency Validation**: Measure actual GPU/CPU energy consumption during inference for GrEmLIn embeddings versus contextualized models (XLM-R, E-5) on representative low-resource language tasks to verify the "green" claim.

2. **Vocabulary Overlap Stress Test**: Systematically evaluate performance degradation as vocabulary overlap decreases from 100% to 50% on a representative sample of languages and tasks to quantify the practical limits of the approach.

3. **Generalization to Complex Tasks**: Test GrEmLIn embeddings on more challenging NLP tasks such as question answering, document summarization, and semantic parsing for low-resource languages to assess whether the 5-10% performance gap holds for tasks requiring deeper contextual understanding.