---
ver: rpa2
title: Effective Guidance for Model Attention with Simple Yes-no Annotations
arxiv_id: '2410.22312'
source_url: https://arxiv.org/abs/2410.22312
tags:
- attention
- annotations
- rayon
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CRAYON is a method for correcting model attention in deep learning
  by using simple yes-no annotations instead of complex pixel-wise labels. The core
  idea is to use human-provided yes-no annotations on model interpretations (saliency
  maps and neuron concepts) to guide the model to focus on relevant regions and prune
  irrelevant neurons.
---

# Effective Guidance for Model Attention with Simple Yes-no Annotations

## Quick Facts
- arXiv ID: 2410.22312
- Source URL: https://arxiv.org/abs/2410.22312
- Authors: Seongmin Lee; Ali Payani; Duen Horng Chau
- Reference count: 40
- Primary result: CRAYON corrects model attention using simple yes-no annotations, achieving state-of-the-art performance across three benchmark datasets

## Executive Summary
CRAYON is a novel method for correcting model attention in deep learning by using simple yes-no annotations instead of complex pixel-wise labels. The core idea is to leverage human-provided yes-no annotations on model interpretations (saliency maps and neuron concepts) to guide models to focus on relevant regions and prune irrelevant neurons. CRAYON-ATTENTION directs the model's attention using saliency map annotations, while CRAYON-PRUNING removes neurons activated by irrelevant concepts. The method outperforms 12 existing approaches across three benchmark datasets, achieving up to 43.96 percentage points improvement in worst group accuracy and 13.15 percentage points in mean group accuracy, with near-peak performance using annotations for just 10% of training data.

## Method Summary
CRAYON addresses model attention bias through two complementary approaches. CRAYON-ATTENTION uses energy loss functions to guide model attention based on yes-no annotations of saliency maps, encouraging similarity to relevant maps and dissimilarity to irrelevant ones. CRAYON-PRUNING identifies and removes neurons activated by irrelevant concepts through concept-based interpretation, eliminating the model's capacity to rely on spurious correlations. Both methods leverage human annotations on model interpretations rather than requiring complex pixel-wise labels, significantly reducing annotation burden while maintaining or improving model performance across multiple benchmark datasets.

## Key Results
- CRAYON achieves state-of-the-art performance across three benchmark datasets (Waterbirds, Biased CelebA, Backgrounds Challenge)
- Improves worst group accuracy by up to 43.96 percentage points and mean group accuracy by up to 13.15 percentage points
- Achieves near-peak performance with annotations for just 10% of the training data
- Outperforms 12 existing methods that require more complex annotations

## Why This Works (Mechanism)

### Mechanism 1
Simple yes-no annotations on saliency maps can effectively redirect model attention from irrelevant regions to relevant ones. The energy loss function (Lrel and Lirrel) encourages the model to minimize differences between its attention map and the annotated relevant map while maximizing differences from the irrelevant map. This directly shapes the gradient flow during fine-tuning. Core assumption: Human annotators can reliably judge whether a saliency map's highlighted regions are relevant to the prediction task.

### Mechanism 2
Pruning neurons activated by irrelevant concepts removes the model's learned spurious correlations. By identifying neurons through concept-based interpretation (image patches) and removing those activated by irrelevant concepts, the model loses the capacity to rely on these spurious correlations for predictions. Core assumption: Neurons in the penultimate layer activate consistently for specific high-level visual concepts across different images.

### Mechanism 3
Combining attention guidance with neuron pruning achieves better performance than either method alone. Attention guidance redirects where the model looks while pruning removes the internal representations that support irrelevant reasoning, creating complementary effects. Core assumption: The attention mechanism and neuron activations are independently controllable aspects of model behavior.

## Foundational Learning

- **Concept**: Saliency maps and their interpretation
  - Why needed here: CRAYON-ATTENTION relies on generating and interpreting saliency maps to understand where the model focuses attention
  - Quick check question: How does Grad-CAM generate saliency maps, and what do the values in these maps represent?

- **Concept**: Concept-based neuron interpretation
  - Why needed here: CRAYON-PRUNING uses modern interpretation techniques to identify which neurons activate for specific visual concepts
  - Quick check question: What is the relationship between a neuron's activation pattern and the visual concepts it represents?

- **Concept**: Energy loss and its properties
  - Why needed here: The energy loss function is central to how CRAYON-ATTENTION guides model attention during fine-tuning
  - Quick check question: How does the energy loss function differ from cross-entropy loss, and why is it suitable for attention guidance?

## Architecture Onboarding

- **Component map**: Interpretation module -> Annotation collection system -> Fine-tuning module (CRAYON-ATTENTION: Generate saliency maps -> Collect annotations -> Compute energy loss -> Fine-tune model; CRAYON-PRUNING: Generate neuron concept patches -> Collect annotations -> Prune irrelevant neurons -> Fine-tune last layer)

- **Critical path**: For CRAYON-ATTENTION: Generate saliency maps → Collect annotations → Compute energy loss (Lrel/Lirrel) → Fine-tune model. For CRAYON-PRUNING: Generate neuron concept patches → Collect annotations → Prune irrelevant neurons → Fine-tune last layer.

- **Design tradeoffs**: Simple yes-no annotations reduce annotation burden but may miss nuanced distinctions between relevant and irrelevant attention. Neuron pruning permanently removes capacity but can be more effective than soft regularization.

- **Failure signatures**: Poor performance may indicate: (1) Annotators misunderstood the relevance questions, (2) The original model's attention patterns are too complex for simple yes-no guidance, (3) Pruned neurons were actually contributing to relevant predictions in edge cases.

- **First 3 experiments**:
  1. Run CRAYON-ATTENTION on a small subset of Waterbirds with 100 annotated images to verify the annotation interface and energy loss implementation.
  2. Test neuron pruning on Biased CelebA with manually verified irrelevant concepts to ensure the concept identification works correctly.
  3. Compare performance of CRAYON-ATTENTION vs CRAYON-PRUNING on Backgrounds Challenge to understand which approach works better for multi-label classification.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does CRAYON's performance scale with the complexity of the visual concepts being annotated (e.g., abstract vs. concrete concepts)?
  - Basis in paper: The paper demonstrates CRAYON's effectiveness with concrete visual concepts like "smiling mouth" and "blond hair," but does not explore more abstract concepts.
  - Why unresolved: The paper focuses on specific, well-defined visual concepts in benchmark datasets. Testing with more abstract or nuanced concepts could reveal limitations or require adaptations to the methodology.
  - What evidence would resolve it: Experiments applying CRAYON to datasets with more abstract visual concepts (e.g., emotions, artistic styles) and comparing performance to the current results.

- **Open Question 2**: What is the impact of CRAYON on model robustness to adversarial attacks, given its focus on improving model attention?
  - Basis in paper: The paper emphasizes improving model attention and generalization but does not address adversarial robustness, a critical aspect of model security.
  - Why unresolved: Adversarial attacks often exploit model attention mechanisms, and improving attention could potentially affect robustness. However, the relationship between attention correction and adversarial defense is not explored.
  - What evidence would resolve it: Experiments evaluating CRAYON-refined models against standard adversarial attack benchmarks and comparing their robustness to the original models.

- **Open Question 3**: How does CRAYON perform when applied to models trained on extremely large-scale datasets (e.g., billion-scale image datasets)?
  - Basis in paper: The paper evaluates CRAYON on datasets with tens of thousands of images, but does not address scalability to much larger datasets common in industrial applications.
  - Why unresolved: The time complexity analysis suggests linear scaling, but practical performance on massive datasets could reveal bottlenecks or require architectural modifications.
  - What evidence would resolve it: Experiments applying CRAYON to models trained on large-scale datasets (e.g., JFT-300M, Instagram-3.5B) and measuring performance gains relative to computational overhead.

## Limitations

- **Annotation quality dependency**: The effectiveness of CRAYON fundamentally depends on the quality and consistency of human yes-no annotations. The paper reports high annotator agreement but does not provide detailed inter-annotator reliability metrics or demonstrate how annotation quality scales with dataset size.

- **Generalizability beyond benchmark datasets**: While CRAYON shows strong performance on Waterbirds, Biased CelebA, and Backgrounds Challenge, the method's effectiveness on real-world datasets with more complex bias patterns remains untested.

- **Permanent capacity reduction in CRAYON-PRUNING**: Neuron pruning removes model capacity permanently, which could limit the model's ability to learn new relevant concepts or handle edge cases that require previously pruned neurons.

## Confidence

- **High confidence**: The experimental results showing CRAYON's performance improvements over 12 baseline methods, the theoretical foundation of using energy loss for attention guidance, and the practical advantage of simple yes-no annotations over complex pixel-wise labels.

- **Medium confidence**: The claim that human annotators can reliably distinguish relevant from irrelevant attention patterns, and that neuron concept patches accurately represent the visual concepts neurons detect.

- **Low confidence**: The assertion that CRAYON will maintain its performance advantage when scaled to larger, more diverse datasets, and the long-term stability of models corrected using CRAYON.

## Next Checks

1. **Annotator agreement validation**: Conduct a detailed analysis of inter-annotator reliability across different image types and complexity levels, measuring Cohen's kappa for both saliency map and neuron concept annotations to quantify annotation consistency.

2. **Transfer learning test**: Apply CRAYON to a held-out real-world dataset (e.g., medical imaging or satellite imagery) with known bias patterns to evaluate performance beyond the benchmark datasets used in the paper.

3. **Capacity recovery experiment**: After pruning neurons with CRAYON-PRUNING, fine-tune the pruned model on a balanced dataset to test whether the model can recover capacity for learning new relevant concepts, measuring performance degradation or recovery.