---
ver: rpa2
title: Self-rationalization improves LLM as a fine-grained judge
arxiv_id: '2410.05495'
source_url: https://arxiv.org/abs/2410.05495
tags:
- response
- score
- criteria
- rationale
- rationales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Rationalization, an iterative approach
  to improve LLM-as-a-judge models by enhancing the quality of their rationales. The
  method involves generating multiple judgments with rationales for the same input,
  curating preference pairs from these judgments, and fine-tuning the model using
  Direct Preference Optimization (DPO).
---

# Self-rationalization improves LLM as a fine-grained judge

## Quick Facts
- arXiv ID: 2410.05495
- Source URL: https://arxiv.org/abs/2410.05495
- Reference count: 37
- Primary result: Self-Rationalizing Evaluator (JSRE) improves rationale quality and scoring accuracy, achieving 62% win rate over SFT models and outperforming larger models by 3-9% on benchmarks

## Executive Summary
This paper introduces Self-Rationalization, an iterative approach to improve LLM-as-a-judge models by enhancing the quality of their rationales. The method involves generating multiple judgments with rationales for the same input, curating preference pairs from these judgments, and fine-tuning the model using Direct Preference Optimization (DPO). This process enables the model to self-improve by learning from its own rationales, leading to better alignment and evaluation accuracy. After just two iterations, the resulting Self-Rationalizing Evaluator (JSRE) significantly improves both rationale quality and scoring accuracy.

## Method Summary
The method involves fine-tuning a base judge model via Supervised Fine-Tuning (SFT) on labeled datasets containing conversation contexts, responses, scores, and rationales. The model then undergoes iterative self-rationalization where it generates multiple judgments (N=10) per input, each containing a rationale followed by a score conditioned on that rationale. Preference pairs are created using various curation methods including correct-answer pairing, meta-judging, and majority voting. The model is then fine-tuned using Direct Preference Optimization (DPO) on these preference pairs, with the process repeating for multiple iterations to obtain the final JSRE model.

## Key Results
- Human evaluation shows 62% win rate in rationale quality compared to models trained via supervised fine-tuning (SFT) on rationales alone
- JSRE achieves high scoring accuracy on BigGen Bench and Reward Bench, outperforming larger models trained using SFT, self-consistency, or best-of-N sampling by 3% to 9%
- Self-Rationalizing Evaluators outperform extensions of SFT methods (self-consistency and best-of-N) across all leaderboards

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Self-rationalization enables iterative improvement by having the model generate multiple judgments with rationales for the same input, creating preference pairs from these judgments, and fine-tuning via DPO.
- **Mechanism**: The model generates N judgments for each input, each containing a score and rationale. These judgments are curated into preference pairs where one is designated as "chosen" (higher quality) and another as "rejected" (lower quality). The model is then fine-tuned on these pairs using DPO, allowing it to learn from its own reasoning process.
- **Core assumption**: The model can generate diverse enough judgments that meaningful preference pairs can be created, and that learning from these preferences will improve both rationale quality and scoring accuracy.
- **Evidence anchors**:
  - [abstract] "Self-rationalization works by having the model generate multiple judgments with rationales for the same input, curating a preference pair dataset from its own judgements, and iteratively fine-tuning the judge via DPO."
  - [section] "To enable the model to learn from its own reasoning process, we generate multiple judgments with the tth iteration of the judge Model i.e. Jt for the same input xi."
  - [corpus] Weak evidence - the related papers discuss rationale generation and evaluation but don't specifically address the self-rationalization mechanism described here.
- **Break condition**: If the model fails to generate sufficiently diverse judgments, preference pairs cannot be meaningfully created, breaking the iterative improvement cycle.

### Mechanism 2
- **Claim**: Conditioning scores on rationales improves judging performance compared to generating scores alone.
- **Mechanism**: The model first generates a rationale, then generates a score conditioned on that rationale. This creates a dependency where the score is informed by the reasoning process, leading to more calibrated evaluations.
- **Core assumption**: The quality of the rationale directly influences the quality of the score when the score is generated conditioned on the rationale.
- **Evidence anchors**:
  - [section] "Each judgment jk = (rk, sk) comprises a rationale, followed by a score that is conditioned on the provided rationale."
  - [section] "We hypothesize that as the quality of the rationales improves, the accuracy of the scores will also improve."
  - [section] "Furthermore, our experiments show that training a judge with rationales and DPO (through self-rationalization) achieves better judging results."
- **Break condition**: If the rationale generation quality is poor or the conditioning mechanism fails, the scores may become unreliable or biased.

### Mechanism 3
- **Claim**: DPO on preference pairs derived from self-generated judgments outperforms SFT and other post-SFT methods like self-consistency and best-of-N.
- **Mechanism**: After generating multiple judgments, preference pairs are created based on various heuristics (correct-answer pairing, meta-judging, majority voting). The model is then fine-tuned using DPO on these preference pairs, which provides more nuanced learning signals than SFT alone.
- **Core assumption**: Preference optimization using pairs of judgments (one better than another) provides stronger learning signals than exposure to only correct examples (SFT) or simple aggregation methods.
- **Evidence anchors**:
  - [abstract] "This judge model also achieves high scoring accuracy on BigGen Bench and Reward Bench, outperforming even bigger sized models trained using SFT with rationale, self-consistency or best-of-N sampling by 3% to 9%."
  - [section] "Our experiments demonstrate that Self-Rationalizing Evaluators outperform extension of SFT methods, such as Self-consistency and Best-of-N, across all leaderboards."
  - [section] "Additionally, in Table 5, we show that applying Direct Preference Optimization (DPO) after Supervised Fine-Tuning (SFT) significantly improves judging performance compared to using only SFT or DPO on the seed model."
- **Break condition**: If the preference curation method creates poor quality pairs (e.g., noisy labels from self-consistency), DPO training may degrade rather than improve performance.

## Foundational Learning

- **Concept**: Direct Preference Optimization (DPO)
  - Why needed here: DPO is the core optimization method that allows the model to learn from preference pairs rather than just correct examples, enabling iterative self-improvement.
  - Quick check question: What is the key difference between DPO and standard supervised fine-tuning in terms of training data structure?

- **Concept**: Chain-of-thought reasoning
  - Why needed here: The rationale generation process resembles chain-of-thought reasoning, where the model explains its decision-making process before providing a final score.
  - Quick check question: How does generating a rationale before a score differ from generating them independently?

- **Concept**: Preference learning and ranking
  - Why needed here: The method relies on the model's ability to distinguish between better and worse judgments, which requires understanding preference learning principles.
  - Quick check question: What are the potential challenges in creating meaningful preference pairs from self-generated judgments?

## Architecture Onboarding

- **Component map**: Seed initialization (SFT) -> Self-rationalization (generate N judgments) -> Preference curation (create pairs) -> DPO fine-tuning -> Evaluation
- **Critical path**: Seed initialization → Self-rationalization → Preference data curation → Preference optimization → Evaluation
- **Design tradeoffs**:
  - Number of judgments (N) vs. computational cost
  - Preference curation method (correct-answer, meta-judging, majority voting) vs. data quality
  - Iteration count vs. diminishing returns
  - Rationale length vs. training signal dilution
- **Failure signatures**:
  - No improvement after iterations (indicates poor preference pair quality or ineffective DPO)
  - Decreased performance on benchmarks (indicates overfitting to self-generated data)
  - Inconsistent rationales for similar inputs (indicates lack of calibration)
- **First 3 experiments**:
  1. Run self-rationalization with N=5 on a small subset of training data and visualize judgment diversity
  2. Implement correct-answer preference pairing and verify that chosen/rejected pairs align with ground truth
  3. Perform single iteration of DPO and compare evaluation metrics against JSFT baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of rationales generated during Self-Rationalization impact the model's long-term performance and generalization to unseen tasks?
- Basis in paper: Explicit - The paper discusses the iterative process of improving rationales through Self-Rationalization and its impact on scoring accuracy, but does not explore long-term effects or generalization.
- Why unresolved: The paper focuses on short-term improvements within the training set and specific benchmarks, without examining how these improvements translate to broader, unseen tasks over time.
- What evidence would resolve it: Longitudinal studies comparing the performance of Self-Rationalizing Evaluators on a diverse set of tasks and datasets over extended periods, including tasks not encountered during training.

### Open Question 2
- Question: What is the optimal number of iterations and preference pair selections for Self-Rationalization to maximize performance without overfitting?
- Basis in paper: Inferred - The paper describes an iterative process but only demonstrates results after two iterations. It mentions different preference selection strategies but doesn't explore the optimal balance between iterations and data selection.
- Why unresolved: The paper provides initial results but doesn't explore the full potential of the iterative process or investigate the trade-offs between iteration count, data selection, and model performance.
- What evidence would resolve it: Systematic experiments varying the number of iterations, the percentage of data sampled, and different preference selection strategies, along with analysis of model performance and potential overfitting.

### Open Question 3
- Question: How does the performance of Self-Rationalizing Evaluators compare to human evaluators across diverse and complex evaluation tasks?
- Basis in paper: Explicit - The paper mentions human evaluation for rationale quality but doesn't compare the model's overall performance to human evaluators on complex tasks.
- Why unresolved: The paper focuses on comparing Self-Rationalizing Evaluators to other models and baselines, without exploring how they stack up against human judgment in real-world scenarios.
- What evidence would resolve it: Comprehensive studies comparing the performance of Self-Rationalizing Evaluators to human evaluators across a wide range of complex, real-world evaluation tasks, including subjective and nuanced judgments.

## Limitations
- The study demonstrates significant improvements but relies heavily on the quality of preference pair curation methods, which are not fully specified in implementation details
- Computational cost of generating multiple judgments (N=10) per input and scalability to larger datasets remains unclear
- Strong results on specific benchmarks but generalizability to other evaluation tasks or domains is not established

## Confidence
- **High Confidence**: The core mechanism of self-rationalization through iterative DPO training on preference pairs is well-supported by experimental results and comparison with baselines
- **Medium Confidence**: The claim that conditioning scores on rationales improves judging performance is supported by results but the causal relationship could benefit from additional ablation studies
- **Medium Confidence**: The assertion that DPO on self-generated preference pairs outperforms SFT and post-SFT methods is demonstrated on specific benchmarks but may not generalize across all evaluation scenarios

## Next Checks
1. **Ablation study on preference curation methods**: Test the impact of different preference selection strategies (correct-answer, meta-judging, majority voting) independently to quantify their contribution to final performance
2. **Scalability assessment**: Evaluate the approach on a larger, more diverse dataset to measure computational costs and performance trade-offs at scale
3. **Cross-task generalization**: Apply the self-rationalized judge model to evaluation tasks outside the original domain to assess robustness and transferability of improvements