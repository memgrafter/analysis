---
ver: rpa2
title: 'DEPT: Decoupled Embeddings for Pre-training Language Models'
arxiv_id: '2410.05021'
source_url: https://arxiv.org/abs/2410.05021
tags:
- data
- dept
- pre-training
- training
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DEPT addresses the challenge of training large language models
  on heterogeneous data mixtures by decoupling token embeddings from the transformer
  body during pre-training. The method trains separate embedding matrices for different
  data sources while aggregating only the transformer parameters, enabling custom
  vocabularies and reduced communication costs.
---

# DEPT: Decoupled Embeddings for Pre-training Language Models

## Quick Facts
- arXiv ID: 2410.05021
- Source URL: https://arxiv.org/abs/2410.05021
- Reference count: 40
- One-line primary result: DEPT reduces embedding memory by 4-5× and cuts communication costs by orders of magnitude while improving perplexity by up to 20% and downstream task performance by 3-7.5%

## Executive Summary
DEPT introduces a novel approach to pre-training large language models on heterogeneous data mixtures by decoupling token embeddings from the transformer body during training. This allows each data source to have its own vocabulary and tokenization strategy while sharing only the transformer parameters. The method achieves significant memory and communication cost reductions while improving both perplexity and downstream task performance. Experiments demonstrate that DEPT enables training with custom optimized vocabularies per data source and better generalization across domains and languages.

## Method Summary
DEPT operates through three main variants: GLOB (shared embeddings), TRIM (trimmed global embeddings), and SPEC (specialized embeddings). The method follows a federated learning-like approach where each data source trains locally for Nlocal steps, then parameters are aggregated. Separate embedding matrices are maintained for each source, with communication occurring only for transformer parameters. For SPEC, continued pre-training with random embeddings generates a global embedding matrix for inference. The approach enables custom vocabularies per data source while maintaining the benefits of shared transformer parameters.

## Key Results
- Reduces embedding memory requirements by 4-5× through vocabulary-specific embedding matrices
- Cuts communication costs by orders of magnitude by aggregating parameters every Nlocal steps instead of every step
- Improves average perplexity by up to 20% and downstream task performance by 3-7.5%
- Enables training with custom optimized vocabularies per data source, improving plasticity to new domains

## Why This Works (Mechanism)

### Mechanism 1
Decoupling embeddings from the transformer body enables domain/language-specific optimizations without shared vocabulary constraints. Separate embedding matrices are trained for each data source, allowing each to have its own vocabulary and tokenization strategy. Only the transformer parameters are aggregated across sources, reducing contention and dilution effects. Core assumption: Transformer body performance is largely independent of specific embedding representations, allowing it to learn abstract features while embeddings handle source-specific lexical patterns.

### Mechanism 2
Reducing communication frequency and embedding parameter size directly lowers memory and bandwidth requirements during distributed training. Instead of synchronizing gradients at every step, communication occurs only every Nlocal steps. Additionally, embeddings are sized to each data source's vocabulary rather than using a massive global vocabulary, reducing parameters by O((|V|-|Vk|)dmodel). Core assumption: Periodic aggregation maintains convergence while dramatically reducing communication overhead.

### Mechanism 3
Specialized embeddings per data source improve model generalization and plasticity by avoiding negative interference and enabling faster adaptation to new domains/languages. Each data source gets its own embedding matrix, eliminating competition for representation capacity between languages/domains. The transformer body learns more robust abstractions when not forced to share embeddings. Core assumption: Models can effectively transfer knowledge learned from one data source to another even without shared embeddings.

## Foundational Learning

- **Federated Learning and communication-efficient optimization**: DEPT builds on federated learning principles where each data source acts like a client, training locally and aggregating periodically. Quick check: What is the key difference between DEPT's aggregation approach and standard federated averaging?

- **Vocabulary construction and tokenization strategies**: Understanding unigram tokenization, vocabulary size tradeoffs, and how different tokenization strategies affect model performance across languages/domains. Quick check: Why does a large multilingual vocabulary cause performance degradation for low-resource languages?

- **Transformer architecture and embedding mechanics**: Understanding how embeddings interact with transformer layers, the role of tied weights, and why transformer bodies can theoretically operate independently of specific embeddings. Quick check: How do tied embedding weights affect the output space in TRIM versus GLOB variants?

## Architecture Onboarding

- **Component map**: Three main variants - GLOB (global embeddings shared), TRIM (global transformer with trimmed local embeddings), SPEC (fully decoupled with local embeddings). All share the same outer aggregation loop but differ in embedding management. Continued pre-training phase required for SPEC to generate global embeddings for inference.

- **Critical path**: 1) Tokenize each data source independently, 2) Initialize transformer and embeddings, 3) For each round: sample subset of sources, 4) Each source trains locally for Nlocal steps, 5) Aggregate transformer parameters across sources, 6) Repeat until convergence, 7) For SPEC: perform continued pre-training with random embeddings to generate global embedding matrix.

- **Design tradeoffs**: GLOB offers simplicity but retains vocabulary dilution issues. TRIM reduces memory but restricts output space. SPEC maximizes specialization but requires additional inference-time embedding generation. Communication frequency vs. convergence speed tradeoff.

- **Failure signatures**: Activation norm divergence during training indicates hyperparameter issues. Poor downstream performance suggests inadequate continued pre-training. Vocabulary coverage gaps manifest as high perplexity on specific data sources.

- **First 3 experiments**:
  1. Run single data source training with GLOB variant to verify basic functionality matches standard pre-training
  2. Test TRIM variant with two data sources having very different vocabularies to verify memory reduction and performance impact
  3. Implement SPEC variant with continued pre-training phase and verify inference works with generated global embeddings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal frequency for aggregation in DEPT to balance communication efficiency and model convergence?
- Basis in paper: [inferred] The paper discusses communication costs and mentions using parameter averaging (FedAvg) as the outer optimizer, but does not explore the impact of different aggregation frequencies.
- Why unresolved: The paper assumes a fixed aggregation frequency but does not experimentally investigate how varying this frequency affects model performance or communication costs.
- What evidence would resolve it: Experiments comparing different aggregation frequencies and their impact on convergence speed, final perplexity, and communication overhead.

### Open Question 2
- Question: How does DEPT's performance scale with increasingly heterogeneous data distributions beyond the ones tested?
- Basis in paper: [explicit] The paper tests on multilingual and multi-domain data but acknowledges that the complexity of current pre-training pipelines is impractical for federated scenarios where data distribution may shift.
- Why unresolved: The experiments use curated datasets with known characteristics. Real-world federated learning scenarios may involve more extreme data heterogeneity or dynamic shifts in distribution.
- What evidence would resolve it: Testing DEPT on federated datasets with extreme data heterogeneity, such as non-stationary distributions or highly imbalanced client data, and measuring performance degradation.

### Open Question 3
- Question: What are the theoretical bounds on the improvement in generalization when using DEPT compared to standard pre-training?
- Basis in paper: [explicit] The paper shows empirical improvements in perplexity and downstream tasks but does not provide theoretical analysis of why or how much DEPT should improve generalization.
- Why unresolved: The paper demonstrates DEPT's effectiveness but lacks a theoretical framework explaining the mechanisms behind its improved generalization.
- What evidence would resolve it: Developing theoretical models that relate DEPT's decoupling mechanism to generalization bounds, and validating these bounds with controlled experiments varying data heterogeneity and model capacity.

## Limitations
- Incomplete ablation studies on communication frequency and comparative analysis of the three DEPT variants
- SPEC variant's effectiveness heavily depends on the continued pre-training phase, which receives limited validation
- Does not address potential privacy concerns from aggregating parameters across different data sources

## Confidence

**High Confidence**: The core claim that decoupling embeddings from transformer bodies reduces memory requirements by 4-5× and communication costs by orders of magnitude.

**Medium Confidence**: The claim that DEPT improves perplexity by up to 20% and downstream task performance by 3-7.5%, though experimental results show mixed performance across variants.

**Low Confidence**: The claim about enhanced generalization and plasticity to new domains/languages, based primarily on indirect evidence.

## Next Checks

1. **Ablation study on communication frequency**: Systematically vary the aggregation frequency (Nlocal parameter) across a wider range of values to determine the optimal tradeoff between communication cost reduction and convergence quality. Compare against theoretical convergence bounds for federated learning.

2. **Cross-domain transfer capability test**: Train DEPT models on multiple source domains, then fine-tune on a held-out target domain. Compare transfer learning performance against standard models to directly validate the generalization claims. Include domain pairs with varying semantic distances.

3. **Memory-accuracy Pareto analysis**: Implement all three DEPT variants (GLOB, TRIM, SPEC) and systematically vary vocabulary sizes and embedding dimensions. Plot the tradeoff curve between memory footprint and perplexity/downstream performance to identify optimal configurations for different resource constraints.