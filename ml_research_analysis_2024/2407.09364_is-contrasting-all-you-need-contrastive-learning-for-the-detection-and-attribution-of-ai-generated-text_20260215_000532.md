---
ver: rpa2
title: Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution
  of AI-generated Text
arxiv_id: '2407.09364'
source_url: https://arxiv.org/abs/2407.09364
tags:
- text
- whosai
- learning
- training
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes WhosAI, a contrastive learning framework for
  detecting and attributing AI-generated text. WhosAI uses a triplet network with
  a pretrained language model to learn semantic similarity representations from multiple
  text generators simultaneously, addressing both detection (Turing Test) and attribution
  tasks in a unified manner.
---

# Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text

## Quick Facts
- arXiv ID: 2407.09364
- Source URL: https://arxiv.org/abs/2407.09364
- Reference count: 40
- Primary result: WhosAI achieves F1-score of 0.999 for Turing Test and 0.990 for Authorship Attribution on TuringBench benchmark

## Executive Summary
This paper introduces WhosAI, a contrastive learning framework for detecting and attributing AI-generated text. The method uses a triplet network with a pretrained language model to learn semantic similarity representations from multiple text generators simultaneously. WhosAI addresses both the Turing Test (binary classification) and Authorship Attribution (multi-class classification) tasks in a unified manner. Evaluated on the TuringBench benchmark containing 200K news articles from 19 AI models, WhosAI significantly outperforms existing methods with outstanding F1-scores of 0.999 and 0.990 for the two tasks respectively.

## Method Summary
WhosAI employs a triplet network architecture with BERT-base-uncased as the pretrained language model core. The framework learns semantic similarity representations through contrastive learning with triplet loss, dynamically increasing the margin parameter during training to focus on harder negative pairs. At inference, it uses a nearest centroid classifier that compares test embeddings to precomputed category centroids. The method incorporates improved triplet mining and data corruption techniques (token deletion and span cropping) during training. The approach is designed to be model-agnostic and scalable to new AI text-generation models.

## Key Results
- Achieves F1-score of 0.999 for Turing Test (detecting AI-generated text)
- Achieves F1-score of 0.990 for Authorship Attribution (identifying which AI model generated text)
- Outperforms all listed baseline methods on TuringBench benchmark
- Shows clear separation between class embeddings (inter-class similarity of -0.808 vs intra-class similarity of 0.931)

## Why This Works (Mechanism)

### Mechanism 1
Contrastive triplet loss clusters embeddings of same-author texts and separates embeddings of different-author texts. The triplet network uses anchor, positive, and negative examples, with loss function `max(d(h(a), h(p)) - d(h(a), h(n)) + λ, 0)` that pulls same-category pairs together and pushes different-category pairs apart in the embedding space.

### Mechanism 2
Dynamic margin scheduling progressively increases separation between classes during training. The margin λ(t) increases linearly over time: `λ(t) = λmin + λ∆(t mod δ)`, forcing the model to focus on harder negative pairs as training progresses and strengthening class separation.

### Mechanism 3
Nearest centroid classification uses precomputed category centroids for efficient inference. At inference time, each test text's embedding is compared to pre-computed centroids of each category using distance function d(h, ck), assigning to the closest centroid.

## Foundational Learning

- **Contrastive learning and triplet loss**: Enables learning semantic similarity representations that distinguish between human and AI-generated text based on embedding distances. *Quick check: What happens to the loss when d(anchor, positive) > d(anchor, negative) + margin?*

- **Siamese/Triplet network architecture**: Allows sharing weights across anchor, positive, and negative branches while learning a unified embedding space. *Quick check: Why does WhosAI use a single triplet network instead of separate models per generator?*

- **Nearest centroid classification**: Provides efficient inference after learning by comparing to precomputed category centroids. *Quick check: How does the cosine distance function work in the context of nearest centroid classification?*

## Architecture Onboarding

- **Component map**: Text tokenization → PLM embedding → Triplet mining → Triplet network training → Centroid computation → Inference classification
- **Critical path**: Text tokenization → PLM embedding → Triplet mining → Triplet network training → Centroid computation → Inference classification
- **Design tradeoffs**: Using BERT-base (110M params) vs larger PLMs for speed vs accuracy; triplet mining online vs offline for training efficiency
- **Failure signatures**: Poor F1 scores indicate embedding space doesn't capture authorship; high inter-class similarity indicates poor separation; low intra-class similarity indicates poor clustering
- **First 3 experiments**: 1) Train with simple triplet mining on small subset and verify basic classification works; 2) Add online triplet mining and dynamic margin scheduling, measure improvement in inter-class separation; 3) Test with data corruption techniques to evaluate robustness

## Open Questions the Paper Calls Out

- How does the performance of WhosAI compare when using different PLMs as the core component instead of BERT?
- How does WhosAI perform on datasets containing AI-generated text from models not included in the TuringBench benchmark?
- What is the minimum amount of training data required for WhosAI to achieve acceptable performance on the Turing Test and Authorship Attribution tasks?

## Limitations

- Requires access to diverse training corpus containing samples from all target generators
- Effectiveness on non-news domains (code, dialogue, creative writing) remains unverified
- Reliance on BERT-base-uncased may limit performance on newer, larger language models

## Confidence

- **High Confidence**: Core contrastive learning mechanism and impressive F1-score results (0.999 and 0.990)
- **Medium Confidence**: Scalability claims and effectiveness of data corruption techniques
- **Low Confidence**: Claims about robustness to adversarial attacks and domain transfer

## Next Checks

1. Test WhosAI on non-news domains (code generation, dialogue systems, creative writing) to verify generalizability beyond news articles
2. Design experiment with training on subset of generators and testing on known and unseen generators to evaluate unknown source detection
3. Systematically remove each innovation (dynamic margin, triplet mining, corruption) and measure performance impact to quantify component contributions