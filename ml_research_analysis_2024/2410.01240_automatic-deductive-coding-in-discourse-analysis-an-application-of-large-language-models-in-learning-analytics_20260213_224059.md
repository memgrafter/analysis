---
ver: rpa2
title: 'Automatic deductive coding in discourse analysis: an application of large
  language models in learning analytics'
arxiv_id: '2410.01240'
source_url: https://arxiv.org/abs/2410.01240
tags:
- coding
- learning
- classification
- deductive
- automatic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the labor-intensive process of deductive coding\
  \ in discourse analysis, which is commonly used in learning sciences to understand\
  \ teaching and learning interactions. To overcome this challenge, the authors evaluate\
  \ three text classification methods\u2014traditional machine learning with feature\
  \ engineering, BERT-like pre-trained models, and GPT-like large language models\
  \ (LLMs)\u2014on two datasets of student annotations and discussions."
---

# Automatic deductive coding in discourse analysis: an application of large language models in learning analytics

## Quick Facts
- arXiv ID: 2410.01240
- Source URL: https://arxiv.org/abs/2410.01240
- Reference count: 4
- The paper demonstrates that GPT with prompt engineering outperforms traditional ML and BERT-like models for automatic deductive coding in discourse analysis, achieving higher accuracy and Kappa values even with limited labeled training samples.

## Executive Summary
This study addresses the labor-intensive process of deductive coding in discourse analysis by evaluating three text classification approaches: traditional machine learning with feature engineering, BERT-like pre-trained models, and GPT-like large language models (LLMs). The authors test these methods on two educational discourse datasets and find that GPT with prompt engineering achieves superior performance, particularly with limited training samples. The research demonstrates how integrating LLMs with techniques like retrieval-augmented generation and traditional NLP can significantly enhance automatic coding performance in learning analytics.

## Method Summary
The study evaluates three classification approaches on two educational discourse datasets: annotation data (607 samples) and discussion data (404 samples). Traditional machine learning uses Random Forest with Bag-of-Words features, BERT-like models employ RoBERTa with different pooling strategies, and GPT-like models leverage prompt engineering techniques including few-shot learning, chain-of-thought reasoning, and retrieval-augmented generation. The models are compared using accuracy, precision, recall, F1-score, and Cohen's Kappa metrics, with training/testing splits of 80/20.

## Key Results
- GPT with prompt engineering outperformed both traditional ML and BERT-like models on both datasets with limited training samples
- The annotation task achieved accuracy of 0.79 and Kappa of 0.72 using GPT with context and NLP integration
- Discussion dataset classification reached accuracy of 0.77 and Kappa of 0.72 with the best GPT configuration
- Traditional ML achieved only 0.57 accuracy for annotation tasks, while RoBERTa reached 0.62 accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GPT with prompt engineering outperforms traditional ML and BERT in automatic deductive coding with limited labeled samples.
- **Mechanism**: GPT leverages few-shot learning and contextual understanding embedded in its architecture, enabling it to generalize from very few labeled examples without extensive feature engineering.
- **Core assumption**: The semantic richness of the input discourse can be captured by prompt structure and a small number of examples.
- **Evidence anchors**:
  - [abstract] "GPT with prompt engineering outperformed the other two methods on both datasets with limited number of training samples."
  - [section] "By providing detailed prompt structures, the reported work demonstrated how large language models can be used in the implementation of automatic deductive coding."
  - [corpus] Weak correlation in related papers; no direct evidence of GPT few-shot performance in this specific domain.
- **Break condition**: If prompts fail to capture the nuanced theoretical coding schema or if the dataset contains too much domain-specific jargon, GPT's performance may degrade.

### Mechanism 2
- **Claim**: Integrating retrieval-augmented generation (RAG) and traditional NLP techniques with GPT improves classification accuracy.
- **Mechanism**: RAG supplements GPT's general knowledge with domain-specific context retrieved from reference materials, reducing hallucination and improving alignment with the predefined coding schema.
- **Core assumption**: Domain-specific context retrieved from the corpus will enhance GPT's decision-making in line with the coding framework.
- **Evidence anchors**:
  - [abstract] "By introducing these advanced methods, we aim to improve the adaptability of LLM in qualitative research."
  - [section] "We used the retrieved information to augment the generated results of GPT, in which retrieval augmented generation (RAG) was implemented."
  - [corpus] No direct evidence in related papers; assumption based on general RAG literature.
- **Break condition**: If the reference database is too small or irrelevant, RAG will not provide meaningful context, and performance gains will be minimal.

### Mechanism 3
- **Claim**: Dynamic prompt construction with context knowledge outperforms static prompts in handling complex discourse tasks.
- **Mechanism**: By tailoring prompts to include specific contextual information (e.g., entire dialog turns or reference sentences), GPT can make more informed coding decisions that align with human-coded results.
- **Core assumption**: Contextual information embedded in prompts significantly improves the model's understanding of the task.
- **Evidence anchors**:
  - [abstract] "we explore the integration of fine-tuning techniques and retrieval-augmented generation to enhance the model's performance in deductive coding."
  - [section] "The performance of automatic deductive coding achieved the best result when the context and NLP were both involved."
  - [corpus] Weak evidence; related papers focus on general coding but not on dynamic prompt adaptation.
- **Break condition**: If the context information is too voluminous or noisy, it may overwhelm the prompt and reduce clarity, hurting performance.

## Foundational Learning

- **Concept**: Few-shot learning
  - **Why needed here**: GPT's ability to learn from a small number of examples is crucial given the limited labeled training samples in this study.
  - **Quick check question**: How many labeled examples are typically needed for few-shot learning to be effective in deductive coding tasks?

- **Concept**: Retrieval-augmented generation (RAG)
  - **Why needed here**: RAG integrates external knowledge sources to improve GPT's domain-specific performance, addressing the limitations of general pre-trained models.
  - **Quick check question**: What types of external data sources are most effective for RAG in educational discourse analysis?

- **Concept**: Prompt engineering techniques (CoT, RAG)
  - **Why needed here**: Techniques like chain-of-thought and dynamic prompt construction guide GPT to follow structured reasoning and incorporate context.
  - **Quick check question**: How does the inclusion of chain-of-thought prompts affect the interpretability of GPT's coding decisions?

## Architecture Onboarding

- **Component map**: Input data → Preprocessing (tokenization/similarity calculation) → Context retrieval (RAG/NLP) → Dynamic prompt construction → GPT API call → Classification output → Evaluation (accuracy, Kappa, F1)
- **Critical path**: Preprocessing → Context retrieval → Prompt construction → GPT inference
- **Design tradeoffs**: Using RAG increases accuracy but adds latency; static prompts are faster but less accurate; fine-tuning improves performance but requires more labeled data
- **Failure signatures**: Low Kappa scores indicate poor agreement with human coding; high variance across categories suggests model bias or overfitting
- **First 3 experiments**:
  1. Compare static prompt vs. dynamic prompt performance on a small subset of labeled data
  2. Evaluate the impact of including reference sentences via RAG on classification accuracy
  3. Test different similarity thresholds in the NLP preprocessing step to optimize active coding category identification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-based automatic deductive coding compare to human coders in terms of inter-rater reliability?
- Basis in paper: [explicit] The paper mentions that the Kappa value for GPT-based coding reached 0.72 for the discussion dataset, but does not provide a direct comparison with human coders.
- Why unresolved: The paper does not provide a direct comparison between GPT-based coding and human coders in terms of inter-rater reliability.
- What evidence would resolve it: A study comparing the Kappa values of GPT-based coding and human coders on the same datasets would provide a clear answer.

### Open Question 2
- Question: How does the performance of GPT-based automatic deductive coding vary across different coding schemes and data types?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of GPT-based coding on two datasets with different coding schemes (annotation and discussion), but does not explore its performance across a wider range of coding schemes and data types.
- Why unresolved: The paper only tests GPT-based coding on two specific datasets and coding schemes, limiting the generalizability of the findings.
- What evidence would resolve it: Testing GPT-based coding on a diverse set of datasets with various coding schemes and data types would provide a comprehensive understanding of its performance across different contexts.

### Open Question 3
- Question: What are the limitations and potential biases of GPT-based automatic deductive coding, and how can they be addressed?
- Basis in paper: [inferred] The paper mentions that GPT-based coding can be significantly improved by integrating techniques such as RAG and CoT, but does not discuss the limitations and potential biases of the approach.
- Why unresolved: The paper focuses on the performance of GPT-based coding but does not explore its limitations and potential biases, which are crucial for understanding its applicability and reliability.
- What evidence would resolve it: A thorough analysis of the limitations and potential biases of GPT-based coding, along with strategies to address them, would provide a more comprehensive understanding of the approach.

## Limitations
- The study focuses on two specific educational discourse datasets, limiting generalizability to other domains or coding frameworks
- Evaluation relies on manually coded ground truth, introducing potential human bias in the reference labels
- Prompt engineering approaches lack full specification, making exact replication challenging
- The paper does not compare GPT-based coding performance directly with human coder inter-rater reliability

## Confidence
- Generalization to other domains: Medium
- Performance on tested datasets: High
- Prompt engineering effectiveness: Medium
- RAG integration benefits: Medium

## Next Checks
1. Test the approach on a third, independent discourse dataset with different coding categories to validate cross-domain generalization
2. Conduct ablation studies removing RAG and dynamic prompts to quantify their individual contributions to performance
3. Evaluate model performance across different prompt engineering strategies to identify optimal configurations for various discourse analysis tasks