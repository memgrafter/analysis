---
ver: rpa2
title: Improving Multimodal Large Language Models Using Continual Learning
arxiv_id: '2410.19925'
source_url: https://arxiv.org/abs/2410.19925
tags:
- forgetting
- llav
- learning
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of catastrophic forgetting in
  multimodal large language models (MLLMs), where integration of vision capabilities
  with LLMs significantly degrades natural language understanding and generation performance.
  The authors propose treating MLLM adaptation as a continual learning problem and
  evaluate five mitigation techniques, identifying Soft Targets as the most effective
  method.
---

# Improving Multimodal Large Language Models Using Continual Learning

## Quick Facts
- arXiv ID: 2410.19925
- Source URL: https://arxiv.org/abs/2410.19925
- Reference count: 28
- Primary result: Soft Targets reduces linguistic performance degradation by up to 15% compared to standard LLaVA training while maintaining multimodal accuracy

## Executive Summary
This paper addresses catastrophic forgetting in multimodal large language models (MLLMs), where integration of vision capabilities with LLMs significantly degrades natural language understanding and generation performance. The authors propose treating MLLM adaptation as a continual learning problem and evaluate five mitigation techniques. Soft Targets emerges as the most effective method, achieving near-zero linguistic forgetting across model scales while maintaining high multimodal accuracy. The approach demonstrates robustness in continual learning scenarios where vision-language tasks are learned sequentially.

## Method Summary
The study treats MLLM adaptation as a continual learning problem, where base LLMs are first pre-trained on text-only tasks (Task 1), followed by vision-language (VL) tasks (Tasks 2+). Five continual learning methods are evaluated: Naive Fine-Tuning, LoRA, Soft Targets, Rehearsal, and mSGM. Soft Targets dynamically adjusts token distributions during training by reducing target tokens by -α and offsetting non-target tokens by +α/(N-1), preserving learned linguistic patterns. The evaluation uses 9 base LLM models (Pythia family, Phi2, LLaMA 2, Vicuna) with CLIP ViT-L/14 vision encoder, following the LLaVA 1.5 training protocol with alignment and fine-tuning stages.

## Key Results
- Soft Targets reduces linguistic performance degradation by up to 15% over standard LLaVA training
- Near-zero linguistic forgetting achieved across all model scales using Soft Targets
- Soft Targets maintains high VL accuracy while preserving linguistic capabilities
- Instruction-tuned base LLMs show positive backward transfer, improving linguistic performance after multimodal training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft Targets mitigates linguistic forgetting by dynamically adjusting the token distribution to preserve learned linguistic patterns while adapting to multimodal inputs.
- Mechanism: The method reduces target tokens by -α and offsets non-target tokens by +α/(N-1), creating a smoothed distribution that maintains sensitivity to previously learned linguistic patterns.
- Core assumption: Preserving low-probability linguistic tokens is critical for maintaining generation capabilities.
- Evidence anchors:
  - [abstract] "Our approach reduces linguistic performance degradation by up to 15% over the LLaVA recipe, while maintaining high multimodal accuracy."
  - [section] "We update the hard target vector Y by reducing the target tokens by −α, and offsetting non-target tokens by +α/(N − 1), where α controls the smoothing"
  - [corpus] Weak evidence - no direct mention of Soft Targets mechanism in neighbor papers

### Mechanism 2
- Claim: Continual Learning framework treats MLLM adaptation as sequential task learning, enabling preservation of linguistic capabilities while acquiring new multimodal skills.
- Mechanism: By treating the base LLM as Task 1 and VL datasets as subsequent tasks, CL methods like mSGM + Rehearsal can apply regularization and experience replay to prevent catastrophic forgetting.
- Core assumption: The distributional shift from text-only to multimodal contexts is significant enough to cause catastrophic forgetting.
- Evidence anchors:
  - [abstract] "This study investigates this issue using the LLaVA MLLM, treating the integration as a continual learning problem."
  - [section] "We treat the first task as the unimodal pre-training task already learned by the base LLM, followed by new VL tasks."
  - [corpus] Moderate evidence - MoE-CT paper mentions catastrophic forgetting in LLMs

### Mechanism 3
- Claim: Instruction-tuned base LLMs exhibit minimal linguistic forgetting due to distributional similarity between text-only and visual-instruction tuning.
- Mechanism: Instruction tuning creates a shared representational space that bridges text-only and multimodal instruction-following capabilities.
- Core assumption: The distributional similarity hypothesis holds across different instruction-tuning datasets and VL adaptation.
- Evidence anchors:
  - [section] "Instruction-tuned LLaMA 2 (7B) and Pythia (1.4B) models show minimal or even negative linguistic forgetting, indicating positive backward transfer."
  - [section] "We posit this may be due to the additional common-sense reasoning and world knowledge encoded in visual-language tasks and instructions"
  - [corpus] Weak evidence - no direct mention of instruction tuning in neighbor papers

## Foundational Learning

- Concept: Catastrophic Forgetting
  - Why needed here: The paper's core problem is understanding and mitigating catastrophic forgetting when LLMs are converted to MLLMs
  - Quick check question: What happens to an LLM's performance on text-only tasks after it's fine-tuned for multimodal capabilities?

- Concept: Continual Learning Framework
  - Why needed here: The solution approach treats MLLM adaptation as sequential task learning
  - Quick check question: How does treating MLLM adaptation as a sequence of tasks help preserve linguistic capabilities?

- Concept: Soft Target Regularization
  - Why needed here: This is the primary mitigation technique shown to be most effective
  - Quick check question: How does dynamically adjusting token distributions help maintain previously learned capabilities?

## Architecture Onboarding

- Component map: Base LLM -> Alignment Network -> MLLM -> Vision Encoder
- Critical path: Base LLM → Alignment Stage (frozen LLM) → Fine-tuning Stage (joint training) → Evaluation
- Design tradeoffs:
  - Freezing vision encoder ensures stability but limits adaptability
  - Single epoch training balances computational cost with performance
  - Mixed precision and gradient checkpointing enable training larger models
- Failure signatures:
  - Severe linguistic forgetting (positive ∆ values)
  - Poor VL performance despite linguistic preservation
  - Inability to learn new VL tasks while maintaining old capabilities
- First 3 experiments:
  1. Run baseline LLaVA training on Pythia 160M to establish forgetting baseline
  2. Apply Soft Targets with varying α values to find optimal smoothing parameter
  3. Compare mSGM + Rehearsal against naive fine-tuning on sequential VL task learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed continual learning methods perform on MLLMs with more than 7B parameters?
- Basis in paper: [explicit] The paper mentions that due to limited compute budget, they could not study MLLMs with more than 7B parameters and suggests it would be interesting to examine how model size impacts catastrophic forgetting in larger LLMs.
- Why unresolved: The study was limited to models with up to 7B parameters due to computational constraints.
- What evidence would resolve it: Testing the continual learning methods on MLLMs with parameter sizes larger than 7B and comparing the results to those obtained from smaller models.

### Open Question 2
- Question: How do the continual learning methods perform on MLLMs that include additional modalities beyond vision and language?
- Basis in paper: [inferred] The paper discusses MLLMs that integrate vision and language but mentions that some MLLMs include additional modalities, and it is not clear if the methods will be as effective for those.
- Why unresolved: The study focused on MLLMs integrating vision and language, and the effectiveness of the methods on MLLMs with additional modalities is unknown.
- What evidence would resolve it: Applying the continual learning methods to MLLMs that include additional modalities and evaluating their performance compared to the methods' effectiveness on vision-language MLLMs.

### Open Question 3
- Question: What is the impact of increasing the number of training epochs on the VL performance of larger MLLMs?
- Basis in paper: [explicit] The paper hypothesizes that larger MLLMs may be under-trained based on results, considering that their VL performance plateaued compared to smaller models, and suggests that adopting the standard single epoch training does help to ensure a consistent and fair analysis but for practical purposes of creating better MLLMs, it would be ideal to maximize VL performance while minimizing NLU/NLG forgetting.
- Why unresolved: The study followed the standard LLaVA training recipe with a single epoch through the VL datasets, and the impact of increased training epochs on larger MLLMs is not explored.
- What evidence would resolve it: Training larger MLLMs with multiple epochs through the VL datasets and comparing their VL performance and linguistic forgetting to those trained with a single epoch.

## Limitations

- The study focuses exclusively on the LLaVA architecture, limiting generalizability to other MLLM approaches
- Fixed dataset sizes and single-epoch training leave open questions about performance at scale and with extended training
- The Soft Targets method introduces a hyperparameter α that requires careful tuning with no systematic study of optimal values
- Evaluation relies on specific NLU/NLG benchmarks that may not capture all aspects of language performance degradation

## Confidence

**High Confidence**: The observation that MLLM adaptation causes catastrophic forgetting is well-established and consistently demonstrated across all model scales and CL methods tested.

**Medium Confidence**: The superiority of Soft Targets over other CL methods is supported by experimental results, but improvements may be specific to LLaVA architecture and chosen datasets.

**Low Confidence**: The claim that Soft Targets achieves "near-zero linguistic forgetting" requires qualification - while linguistic performance degradation is significantly reduced, some forgetting still occurs across all methods.

## Next Checks

1. **Ablation Study of α Parameter**: Systematically vary the Soft Targets smoothing parameter α across a wider range (0.01 to 0.5) and measure its impact on both linguistic preservation and VL task performance to identify optimal values and potential trade-offs.

2. **Cross-Architecture Generalization**: Apply the Soft Targets method to alternative MLLM architectures (BLIP-2, Flamingo) and larger model scales (13B+ parameters) to test whether observed benefits generalize beyond LLaVA.

3. **Long-Term Sequential Learning**: Evaluate the method in extended continual learning scenarios with 5+ sequential VL tasks and multi-epoch training to assess whether linguistic preservation holds under more challenging conditions.