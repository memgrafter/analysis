---
ver: rpa2
title: Which bits went where? Past and future transfer entropy decomposition with
  the information bottleneck
arxiv_id: '2411.04992'
source_url: https://arxiv.org/abs/2411.04992
tags:
- information
- entropy
- transfer
- past
- future
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors present a novel approach to decompose transfer entropy
  (TE) using the information bottleneck (IB) method, aiming to localize information
  flow between processes on both sides: from the source''s past to the target''s future.
  They propose two equivalent formulations of TE as constrained communication problems,
  which are optimized using machine learning to identify and decompose the transferred
  entropy.'
---

# Which bits went where? Past and future transfer entropy decomposition with the information bottleneck

## Quick Facts
- arXiv ID: 2411.04992
- Source URL: https://arxiv.org/abs/2411.04992
- Reference count: 26
- Authors: Kieran A. Murphy; Zhuowen Yin; Dani S. Bassett
- Primary result: Novel IB-based method decomposes transfer entropy by localizing information flow from source past to target future

## Executive Summary
This paper presents a novel approach to decompose transfer entropy (TE) using the information bottleneck (IB) method, aiming to localize information flow between processes on both sides: from the source's past to the target's future. The authors propose two equivalent formulations of TE as constrained communication problems, which are optimized using machine learning to identify and decompose the transferred entropy. The method is applied to synthetic recurrent processes and an experimental mouse dataset, revealing nuanced dynamics within information flow. The results demonstrate the effectiveness of the approach in identifying the origin and terminus of transferred entropy, providing insights into the intricate interplay of temporal processes in complex systems.

## Method Summary
The authors formulate transfer entropy estimation as minimizing a Lagrangian that balances compression (via KL divergence) and prediction accuracy (via InfoNCE). By varying the Lagrange multiplier β, they trace a spectrum from self-forecasting to full transfer entropy. They propose a distributed IB scheme where each source process and each past timestep is assigned its own compression variable U_i, with separate KL penalties. This allows the model to learn which specific time points and processes contribute most to the transfer entropy. The same transfer entropy value can be localized differently depending on whether the source's past or the target's future is compressed.

## Key Results
- Demonstrated transfer entropy decomposition using information bottleneck on synthetic Boolean networks
- Applied method to experimental mouse neural/behavioral data, revealing nuanced information flow dynamics
- Showed that the same transfer entropy can be localized differently depending on which side (source past vs target future) is compressed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer entropy can be decomposed by treating it as a constrained communication problem optimized with the information bottleneck.
- Mechanism: The authors formulate transfer entropy estimation as minimizing a Lagrangian that balances compression (via KL divergence) and prediction accuracy (via InfoNCE). By varying the Lagrange multiplier β, they trace a spectrum from self-forecasting to full transfer entropy.
- Core assumption: The InfoNCE bound is a tight lower bound on mutual information, and the β-VAE formulation provides a tractable surrogate for KL divergence.
- Evidence anchors:
  - [abstract]: "We employ the information bottleneck (IB) to compress the time series and identify the transferred entropy."
  - [section]: "minimizing the following Lagrangian with respect to U traverses the spectrum relevant to Eqn. 1"
  - [corpus]: Weak—no direct mention of InfoNCE or β-VAE in neighbors; this is original to the paper.
- Break condition: If the InfoNCE bound becomes loose (e.g., insufficient negative samples), the optimization will not accurately capture transfer entropy.

### Mechanism 2
- Claim: Decomposing transfer entropy into contributions from individual past timesteps or source processes is possible via a distributed IB scheme.
- Mechanism: Each source process and each past timestep is assigned its own compression variable U_i, with separate KL penalties. This allows the model to learn which specific time points and processes contribute most to the transfer entropy.
- Core assumption: The distributed IB framework can meaningfully separate information flow contributions without causing interference between the compression variables.
- Evidence anchors:
  - [abstract]: "The encapsulated information can then be decomposed in terms of multiple sources via the distributed IB"
  - [section]: "we compress each component process and/or each point in time separately by distributing an information bottleneck to each as a separate random variable"
  - [corpus]: Missing—no neighbors discuss distributed IB decomposition.
- Break condition: If the mutual information between distributed variables is high, the decomposition may not be identifiable.

### Mechanism 3
- Claim: The same transfer entropy value can be localized differently depending on whether the source's past or the target's future is compressed.
- Mechanism: The two equivalent formulations of transfer entropy (eqn 1 vs eqn 2) lead to different optimization trajectories because the prediction task differs: forecasting the target's future vs inferring the source's past.
- Core assumption: The mutual information I(Ypast; Yfuture) is larger than I(Ypast; Xpast) in typical settings, making one optimization harder than the other.
- Evidence anchors:
  - [abstract]: "we propose an IB scheme to encapsulate the transfer entropy from either side of an information flow into a learned compression space"
  - [section]: "we can focus on the effect of incorporating either the past of the red fish or the future of the blue fish into a predictive model"
  - [corpus]: Weak—no neighbors discuss dual formulations of transfer entropy.
- Break condition: If I(Ypast; Yfuture) ≈ I(Ypast; Xpast), the two formulations become equivalent and the distinction vanishes.

## Foundational Learning

- Concept: Information bottleneck (IB)
  - Why needed here: IB provides a principled way to compress time series while preserving only the information relevant to transfer entropy.
  - Quick check question: What is the role of the Lagrange multiplier β in the IB objective?
- Concept: Transfer entropy
  - Why needed here: Transfer entropy quantifies information flow from one time series to another, which is the target phenomenon to decompose.
  - Quick check question: How does transfer entropy differ from mutual information in terms of conditioning?
- Concept: Variational inference / InfoNCE
  - Why needed here: These provide tractable lower bounds for mutual information estimation, essential for training the IB model.
  - Quick check question: Why is InfoNCE preferred over direct density estimation in high-dimensional time series?

## Architecture Onboarding

- Component map: Ypast encoder (LSTM → dense) → U (bottleneck MLP) → InfoNCE predictor (MLP + LSTM)
- Critical path: Ypast → U (via encoder) → prediction of Yfuture/Xpast → InfoNCE loss
- Design tradeoffs:
  - Higher bottleneck dimension → more expressive U but less compression
  - More negative samples in InfoNCE → tighter bound but higher compute
  - Distributed IB → interpretability but potential interference between variables
- Failure signatures:
  - InfoNCE loss plateaus early → insufficient negative samples or model capacity
  - KL cost dominates → β too large, underfitting
  - KL cost near zero → β too small, overfitting or trivial compression
- First 3 experiments:
  1. Verify that I(U; Ypast; Yfuture) increases with β on a simple binary process.
  2. Test distributed IB on a two-process XOR system to confirm localization.
  3. Compare past vs future decomposition on a known transfer entropy benchmark.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of β (information cost) parameter affect the localization of transfer entropy in practical applications?
- Basis in paper: [explicit] The authors discuss optimizing the Lagrangian with respect to β, traversing a spectrum of transferred information.
- Why unresolved: The paper does not provide empirical evidence or guidelines for selecting optimal β values in real-world datasets.
- What evidence would resolve it: Systematic experiments varying β across multiple datasets and analyzing the resulting transfer entropy decompositions.

### Open Question 2
- Question: Can the information bottleneck approach handle high-dimensional multivariate time series with more than three interacting processes?
- Basis in paper: [inferred] The synthetic examples use up to four processes, but the mouse data analysis involves 23 brain regions plus behavioral signals.
- Why unresolved: The paper doesn't explore the scalability limits or performance degradation when increasing the number of interacting processes.
- What evidence would resolve it: Benchmarking the method on synthetic datasets with varying numbers of processes and analyzing computational complexity and decomposition accuracy.

### Open Question 3
- Question: How sensitive is the transfer entropy decomposition to the choice of neural network architecture and embedding dimensions?
- Basis in paper: [explicit] The authors specify particular architectures and dimensions for the bottleneck and encoders.
- Why unresolved: No ablation studies or sensitivity analyses are presented to show how different architectural choices affect the results.
- What evidence would resolve it: Systematic variation of network architectures and dimensions across multiple datasets while measuring changes in transfer entropy decomposition quality.

## Limitations
- Limited empirical validation of InfoNCE bound tightness for distributed IB decomposition
- Insufficient exploration of scalability to high-dimensional multivariate time series
- Lack of sensitivity analysis for neural network architecture and embedding dimension choices

## Confidence

- Transfer entropy decomposition mechanism: **High**
- Distributed IB for multi-component decomposition: **Medium**
- Past vs future localization distinction: **Medium**
- Application to mouse dataset: **Low**

## Next Checks

1. Test distributed IB decomposition on a synthetic system with known, separable information flows to verify identifiability
2. Compare the past and future formulations on a benchmark system where I(Ypast; Yfuture) and I(Ypast; Xpast) can be analytically computed
3. Evaluate InfoNCE bound tightness empirically by comparing against known transfer entropy values in simple systems