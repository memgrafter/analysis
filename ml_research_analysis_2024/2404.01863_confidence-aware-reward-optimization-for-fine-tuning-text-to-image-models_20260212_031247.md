---
ver: rpa2
title: Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models
arxiv_id: '2404.01863'
source_url: https://arxiv.org/abs/2404.01863
tags:
- reward
- human
- prompts
- prompt
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reward overoptimization in text-to-image generation,
  where fine-tuning models on proxy reward functions can degrade performance. The
  authors introduce the TIA2 benchmark to evaluate reward models and show that even
  models trained on human feedback often fail to capture human preferences.
---

# Confidence-aware Reward Optimization for Fine-tuning Text-to-image Models

## Quick Facts
- arXiv ID: 2404.01863
- Source URL: https://arxiv.org/abs/2404.01863
- Reference count: 32
- Key outcome: This paper addresses reward overoptimization in text-to-image generation, where fine-tuning models on proxy reward functions can degrade performance. The authors introduce the TIA2 benchmark to evaluate reward models and show that even models trained on human feedback often fail to capture human preferences. To mitigate this, they propose TextNorm, a method that calibrates rewards using model confidence estimated across semantically contrastive text prompts. TextNorm significantly improves alignment with human judgment, reducing overoptimization and achieving twice as many wins in human evaluation for text-image alignment compared to baseline reward models.

## Executive Summary
This paper addresses a fundamental challenge in text-to-image generation: reward overoptimization, where fine-tuning models on proxy reward functions leads to degraded performance despite improved reward scores. The authors introduce the TIA2 benchmark to evaluate reward model alignment with human preferences and demonstrate that even state-of-the-art reward models trained on human feedback frequently misalign with human judgment. To address this, they propose TextNorm, a method that calibrates rewards based on model confidence estimated across semantically contrastive text prompts, significantly improving text-image alignment in human evaluation. The work also shows that ensemble methods combining multiple normalized reward models can further enhance alignment.

## Method Summary
The authors tackle reward overoptimization by introducing the TIA2 benchmark to evaluate reward model alignment with human preferences. They propose TextNorm, which normalizes rewards using softmax over semantically contrastive prompts to estimate model confidence. The method generates alternative prompts that are syntactically similar but semantically distinct from the input, then calibrates the original reward based on relative comparisons. They also demonstrate that ensemble methods combining multiple normalized reward models (mean ensemble and uncertainty-regularized ensemble) can further improve alignment. The approach is validated through fine-tuning experiments on Stable Diffusion v2.1 using both supervised and RL-based methods.

## Key Results
- TextNorm reduces reward overoptimization and improves alignment with human judgment in human evaluation
- TextNorm achieves twice as many wins compared to baseline reward models in human evaluation for text-image alignment
- Ensemble methods combining multiple normalized reward models provide further improvement in alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Overoptimization occurs when reward models overfit to proxy objectives that poorly align with human preferences.
- Mechanism: Reward models trained on human feedback data, while better than pre-trained models like CLIP, still frequently misalign with human assessment. Fine-tuning text-to-image models on these poorly aligned rewards leads to degradation in both text-image alignment and image fidelity.
- Core assumption: Human preference data, while useful, does not fully capture the complexity of human judgment in text-to-image alignment.
- Evidence anchors:
  - [abstract] "Our evaluation of several state-of-the-art reward models on this benchmark reveals their frequent misalignment with human assessment."
  - [section 1] "We empirically demonstrate that overoptimization occurs notably when a poorly aligned reward model is used as the fine-tuning objective."
  - [corpus] Weak - only 0 citations, indicating limited external validation yet.
- Break condition: If reward models could perfectly capture human preferences, overoptimization would not occur.

### Mechanism 2
- Claim: TextNorm improves alignment by calibrating rewards based on model confidence estimated across semantically contrastive text prompts.
- Mechanism: By considering a set of alternative prompts that are syntactically similar but semantically distinct from the input prompt, TextNorm normalizes the reward using a softmax function. This normalization adjusts the reward based on the relative comparison of rewards conditioned on these contrastive prompts, effectively calibrating the reward based on model confidence.
- Core assumption: Prompts that differ from the input prompt in both syntax and semantics are unlikely to yield high reward values, making negligible contributions to the softmax score.
- Evidence anchors:
  - [abstract] "We propose TextNorm, a simple method that enhances alignment based on a measure of reward model confidence estimated across a set of semantically contrastive text prompts."
  - [section 4.1] "The key idea is to leverage the relative comparison of the rewards as a measure of model confidence to calibrate rewards."
  - [corpus] Weak - limited external evidence supporting this specific mechanism.
- Break condition: If the set of contrastive prompts does not adequately represent semantically distinct variations, the confidence estimation would be inaccurate.

### Mechanism 3
- Claim: Ensemble methods combining multiple normalized reward models can further enhance alignment.
- Mechanism: By averaging the normalized rewards from multiple reward models (mean ensemble) or applying a variance penalty based on the disagreement among models (uncertainty-regularized ensemble), the overall alignment with human judgment can be improved.
- Core assumption: Different reward models have complementary strengths and weaknesses, and their combination can lead to a more robust alignment metric.
- Evidence anchors:
  - [abstract] "We also propose and demonstrate that ensemble methods, which combine multiple reward models, can be used to achieve further improvement."
  - [section 4.2] "Given a set of reward models {r1, . . . , rk}, we first apply TextNorm to derive corresponding rewards{rX1 , . . . , rXk} normalized over the set of prompts X."
  - [corpus] Weak - no external citations supporting this ensemble approach.
- Break condition: If the reward models in the ensemble are highly correlated in their errors, the ensemble would not provide significant improvement.

## Foundational Learning

- Concept: Binary classification metrics (AUROC, AUPRC, AP@k, Spearman's ρ, Kendall's τ)
  - Why needed here: To evaluate the performance of reward models as classifiers in distinguishing between semantically consistent and inconsistent text-image pairs.
  - Quick check question: How would you interpret an AUROC score of 0.75 for a reward model on the TIA2 benchmark?

- Concept: Softmax normalization and temperature scaling
  - Why needed here: To estimate model confidence and calibrate rewards based on the relative comparison of rewards conditioned on semantically contrastive prompts.
  - Quick check question: What effect does increasing the temperature parameter have on the softmax normalization?

- Concept: Reinforcement learning (RL) and policy gradient methods
  - Why needed here: To fine-tune text-to-image models using the reward models as optimization objectives, iteratively collecting data and performing optimization.
  - Quick check question: How does the denoising diffusion policy optimization (DDPO) method differ from traditional policy gradient methods?

## Architecture Onboarding

- Component map:
  TIA2 benchmark -> Baseline reward models (CLIP, BLIP-2, ImageReward, PickScore) -> TextNorm normalization -> Ensemble methods -> Fine-tuning methods (RWR, DDPO) -> Human evaluation

- Critical path:
  1. Construct the TIA2 benchmark with diverse text prompts, images, and human annotations.
  2. Evaluate baseline reward models on the TIA2 benchmark using binary classification metrics.
  3. Implement TextNorm to normalize rewards based on model confidence estimated across contrastive prompts.
  4. Combine multiple normalized reward models using ensemble methods.
  5. Fine-tune text-to-image models using the reward models and evaluate the results through human evaluation.

- Design tradeoffs:
  - Tradeoff between the complexity of the contrastive prompt set and the accuracy of the confidence estimation.
  - Tradeoff between the diversity of reward models in the ensemble and the computational cost of combining them.
  - Tradeoff between the strength of the regularization in RL fine-tuning and the risk of overoptimization.

- Failure signatures:
  - If TextNorm does not improve alignment, it may indicate that the contrastive prompts are not semantically distinct enough or that the softmax normalization is not effectively estimating confidence.
  - If ensemble methods do not improve alignment, it may indicate that the reward models are highly correlated in their errors or that the combination method is not appropriate.
  - If overoptimization still occurs despite using TextNorm and ensemble methods, it may indicate that the reward models are fundamentally misaligned with human preferences or that the fine-tuning methods are too aggressive.

- First 3 experiments:
  1. Evaluate the alignment of baseline reward models (CLIP, BLIP-2, ImageReward, PickScore) on the TIA2 benchmark using AUROC and AUPRC metrics.
  2. Implement TextNorm to normalize the rewards of ImageReward and PickScore based on model confidence estimated across contrastive prompts, and evaluate the alignment improvement.
  3. Combine the normalized rewards of ImageReward and PickScore using a mean ensemble and an uncertainty-regularized ensemble, and compare their alignment with the individual models on the TIA2 benchmark.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TextNorm's performance scale with larger and more diverse prompt sets X?
- Basis in paper: [explicit] The paper mentions that TextNorm normalizes rewards over a set of semantically contrastive prompts X, but does not explore the effect of varying the size or diversity of X.
- Why unresolved: The paper uses a fixed prompt set for all experiments, without investigating the impact of different prompt set configurations on TextNorm's performance.
- What evidence would resolve it: Experiments comparing TextNorm's performance with different sizes and levels of diversity in the prompt set X, potentially revealing an optimal configuration.

### Open Question 2
- Question: Can TextNorm be effectively applied to other domains beyond text-to-image generation, such as text-to-video or text-to-3D model generation?
- Basis in paper: [inferred] TextNorm's core idea of calibrating rewards based on model confidence estimated across semantically contrastive prompts could potentially be applied to other conditional generation tasks.
- Why unresolved: The paper focuses solely on text-to-image generation and does not explore the applicability of TextNorm to other domains.
- What evidence would resolve it: Experiments applying TextNorm to other conditional generation tasks and comparing its performance to baseline methods in those domains.

### Open Question 3
- Question: How does the choice of temperature scale τ in the softmax function affect TextNorm's performance?
- Basis in paper: [explicit] The paper mentions using a temperature scale τ in the softmax function for reward normalization but does not explore the impact of different τ values.
- Why unresolved: The paper uses a fixed temperature scale for all experiments, without investigating how different τ values might affect TextNorm's performance.
- What evidence would resolve it: Experiments comparing TextNorm's performance with different temperature scale values, potentially revealing an optimal τ for different tasks or datasets.

## Limitations

- The effectiveness of TextNorm depends critically on the quality of contrastive prompt generation, but the exact LLM prompting strategy and hyperparameters remain unspecified
- The ensemble methods, while showing promise, lack external validation from prior work
- The TIA2 benchmark, though diverse, represents a single dataset that may not capture the full complexity of human preferences across different domains and use cases

## Confidence

- High confidence: The identification of reward overoptimization as a fundamental problem in text-to-image fine-tuning is well-supported by both theoretical reasoning and empirical demonstration on the TIA2 benchmark.
- Medium confidence: The TextNorm mechanism's effectiveness in calibrating rewards based on model confidence shows consistent improvements in human evaluation, but the reliance on rule-based contrastive prompt generation creates uncertainty about reproducibility.
- Low confidence: The ensemble methods' superiority over individual models is demonstrated only within the paper's experiments, with no external validation or comparison to established ensemble techniques in the literature.

## Next Checks

1. **Contrastive prompt quality analysis**: Generate a diverse set of contrastive prompts for a sample of TIA2 prompts and conduct human evaluation to assess whether these prompts are truly semantically distinct from the originals and whether they receive appropriately low reward scores from baseline models.

2. **Cross-dataset generalization**: Evaluate TextNorm and ensemble methods on an independent text-image dataset (e.g., MS-COCO or Flickr30k) to test whether the improvements in text-image alignment generalize beyond the TIA2 benchmark.

3. **Reward model error correlation analysis**: Compute pairwise correlation coefficients between reward model errors on the TIA2 benchmark to empirically verify the assumption that ensemble methods work because different models make complementary errors, or whether they share systematic biases.