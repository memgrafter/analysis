---
ver: rpa2
title: 'Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments'
arxiv_id: '2412.00323'
source_url: https://arxiv.org/abs/2412.00323
tags:
- bias
- llms
- biases
- cognitive
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper presents a survey and experimental analysis of cognitive\
  \ biases in large language models (LLMs) and evaluates mitigation techniques. It\
  \ examines six biases\u2014order bias, compassion fade, egocentric bias, bandwagon\
  \ effect, attentional bias, and verbosity bias\u2014through experiments on GPT-3.5\
  \ and GPT-4 using the CoBBLEr benchmark."
---

# Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments

## Quick Facts
- arXiv ID: 2412.00323
- Source URL: https://arxiv.org/abs/2412.00323
- Reference count: 40
- Primary result: AwaRe (Awareness Reminder) prompt modification significantly reduces cognitive biases in LLMs, while SoPro (Social Projection) is ineffective

## Executive Summary
This study investigates cognitive biases in large language models through systematic experimentation on six bias types using the CoBBLEr benchmark. The researchers evaluate two prompt-based mitigation techniques - SoPro and AwaRe - on GPT-3.5 and GPT-4. Results show that AwaRe significantly reduces bias effects, particularly for bandwagon and verbosity biases, while SoPro fails to provide effective mitigation. GPT-4 demonstrates greater resistance to biases compared to GPT-3.5. The findings suggest that prompt-based approaches, especially awareness reminders, can be effective tools for improving LLM decision-making.

## Method Summary
The study applies two prompt modification techniques - SoPro (Social Projection) and AwaRe (Awareness Reminder) - to LLM responses using the CoBBLEr benchmark dataset. SoPro prompts LLMs to answer as they believe the majority would, while AwaRe adds statements informing of specific biases and encouraging careful responses. The experiments use GPT-3.5 (gpt-3.5-turbo-1106) and GPT-4 (gpt-4-1106-preview) via OpenAI API with temperature=0 and seed=0. Bias effects are measured using specific scoring formulas for each bias type, with 12,000 evaluations per bias across 50 question-answer pairs.

## Key Results
- AwaRe significantly reduces bandwagon and verbosity bias effects compared to baseline and SoPro methods
- SoPro fails to mitigate biases and may actually increase susceptibility to bandwagon effects by encouraging conformity
- GPT-4 demonstrates greater resistance to cognitive biases compared to GPT-3.5
- The awareness reminder approach shows promise as an effective prompt-based mitigation technique

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AwaRe prompt modification reduces cognitive bias by increasing LLM awareness of specific biases during response generation
- Mechanism: By explicitly stating the presence of a particular bias and instructing careful response, LLMs activate self-monitoring processes that counteract automatic biased responses
- Core assumption: LLMs can process and apply meta-cognitive instructions about bias awareness in real-time generation
- Evidence anchors:
  - [abstract]: "AwaRe (Awareness Reminder), which makes LLMs aware of biases and encourages careful responses"
  - [section 3.2]: "AwaRe (Awareness Reminder) adds a statement that informs the presence of a bias and instructs LLMs to be careful of this bias while answering"
  - [corpus]: Weak evidence - related papers discuss awareness but not specifically this mechanism
- Break condition: If LLMs cannot process meta-level instructions or if awareness reminders become too generic to be effective

### Mechanism 2
- Claim: SoPro prompt modification fails to reduce bias because it aligns LLM responses with majority opinion rather than promoting rational evaluation
- Mechanism: By instructing LLMs to answer as they believe the majority would, SoPro creates conformity pressure that amplifies bandwagon effects rather than mitigating them
- Core assumption: LLMs interpret "majority opinion" as the correct answer rather than a social reference point
- Evidence anchors:
  - [abstract]: "while SoPro is ineffective"
  - [section 4.4]: "SoPro increases the models' susceptibility to bandwagon effect, as it inherently aligns their responses with the opinion of the majority"
  - [corpus]: No direct evidence found in corpus for this specific mechanism
- Break condition: If majority-based prompts could be reframed to encourage critical evaluation rather than conformity

### Mechanism 3
- Claim: GPT-4 demonstrates greater bias resistance than GPT-3.5 due to architectural differences in training methodology
- Mechanism: Advanced training techniques like RLHF (Reinforcement Learning from Human Feedback) and instruction tuning in GPT-4 create stronger alignment with rational response patterns
- Core assumption: Training methodology differences between GPT-3.5 and GPT-4 result in measurable bias resistance differences
- Evidence anchors:
  - [section 4.4]: "GPT-4 is generally less susceptible to cognitive biases compared to GPT-3.5"
  - [section 5.1]: "more advanced models, such as ChatGPT and GPT-4, show improvements, overcoming these errors and demonstrating rational decision-making"
  - [corpus]: Weak evidence - related papers discuss RLHF but not specific to bias resistance
- Break condition: If bias resistance is primarily due to model size rather than training methodology

## Foundational Learning

- Concept: Cognitive bias types and their manifestations in LLM outputs
  - Why needed here: Understanding the six specific biases (order, compassion fade, egocentric, bandwagon, attentional, verbosity) is essential for designing effective mitigation strategies
  - Quick check question: Can you explain how order bias manifests differently from verbosity bias in LLM outputs?

- Concept: Prompt engineering principles and template modification
  - Why needed here: The study relies on systematic prompt modifications to test bias mitigation, requiring understanding of template structure and variable insertion
  - Quick check question: How would you modify a prompt template to test for egocentric bias versus bandwagon effect?

- Concept: Evaluation metrics for bias measurement in comparative judgment tasks
  - Why needed here: The study uses specific scoring formulas (SOrder1, SComp1, SEgoc, etc.) to quantify bias effects, requiring understanding of these metrics
  - Quick check question: What does a score of 0.25 indicate for order bias metrics compared to 0?

## Architecture Onboarding

- Component map:
  - Prompt template system with bias-specific modifications
  - LLM API interface (GPT-3.5 and GPT-4)
  - CoBBLEr benchmark evaluation engine
  - Score calculation module for bias metrics
  - Results aggregation and comparison framework

- Critical path: Prompt modification → LLM API call → Response validation → Score calculation → Bias effect analysis

- Design tradeoffs: Simple prompt modifications vs. complex fine-tuning; generic vs. bias-specific approaches; single vs. multiple prompt variations

- Failure signatures: Invalid responses exceeding threshold percentages, inconsistent scores across prompt variations, baseline bias levels that prevent mitigation measurement

- First 3 experiments:
  1. Implement baseline prompt without modifications and verify score calculation matches expected random distribution
  2. Add AwaRe modification to order bias prompt and measure score reduction
  3. Add SoPro modification to bandwagon bias prompt and verify score increase (indicating failure)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are fine-tuning approaches compared to prompt-based methods for mitigating cognitive biases in LLMs?
- Basis in paper: [inferred] The paper acknowledges that their focus was primarily on prompt modification and that exploring alternative approaches like fine-tuning could potentially be more effective.
- Why unresolved: The study did not explore fine-tuning approaches, limiting the comparison between different mitigation techniques.
- What evidence would resolve it: Experiments comparing the effectiveness of prompt-based methods (SoPro and AwaRe) versus fine-tuning approaches on the same set of cognitive biases.

### Open Question 2
- Question: How do different LLM architectures and training paradigms affect their susceptibility to cognitive biases?
- Basis in paper: [inferred] The paper mentions that the experiments only used GPT-3.5 and GPT-4, without addressing variations in model tuning and training such as instruction tuning and RLHF, or differences in model sizes.
- Why unresolved: The study's scope was limited to two specific models, leaving the generalizability of the findings to other LLM architectures and training methods unknown.
- What evidence would resolve it: Experiments testing the same cognitive biases across a diverse set of LLMs with different architectures, training paradigms, and sizes.

### Open Question 3
- Question: How do cultural and linguistic variations in data affect the resilience of LLMs against various cognitive biases?
- Basis in paper: [inferred] The paper suggests that exploring additional dimensions such as cultural and linguistic variations in data could offer deeper insights into the resilience of LLMs against various cognitive biases.
- Why unresolved: The study did not investigate the impact of cultural and linguistic variations on bias susceptibility, which could be a significant factor in real-world applications.
- What evidence would resolve it: Experiments evaluating the same cognitive biases across LLMs trained on diverse datasets with different cultural and linguistic backgrounds.

## Limitations

- The study focuses only on prompt-based mitigation methods, not exploring fine-tuning or other architectural approaches
- Results are limited to GPT-3.5 and GPT-4, without testing other LLM architectures or training paradigms
- The effectiveness of bias mitigation may not generalize to non-judgmental or creative task domains

## Confidence

- Medium: AwaRe effectiveness for bandwagon and verbosity biases
- Medium: GPT-4 demonstrating greater bias resistance than GPT-3.5
- Low: Mechanism explaining SoPro's ineffectiveness across all bias types
- Medium: Generalizability of findings to non-judgmental tasks

## Next Checks

1. **Cross-domain validation**: Test AwaRe and SoPro on non-judgmental tasks (e.g., creative writing, code generation) to assess generalizability of bias mitigation effects.

2. **Temporal stability**: Conduct repeated measurements over time to evaluate whether prompt-based bias mitigation maintains effectiveness across multiple sessions and model updates.

3. **Alternative prompt formulations**: Experiment with modified SoPro prompts that explicitly encourage critical evaluation rather than majority alignment to test whether the mechanism of failure can be circumvented.