---
ver: rpa2
title: 'RAG-Fusion: a New Take on Retrieval-Augmented Generation'
arxiv_id: '2402.03367'
source_url: https://arxiv.org/abs/2402.03367
tags:
- rag-fusion
- infineon
- query
- queries
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces RAG-Fusion, a retrieval-augmented generation
  method that improves chatbot performance by generating multiple queries from the
  original user input, reranking retrieved documents using reciprocal rank fusion,
  and fusing them for answer generation. This approach yields more accurate and comprehensive
  answers compared to traditional RAG, particularly in technical, sales, and customer-facing
  contexts.
---

# RAG-Fusion: a New Take on Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2402.03367
- Source URL: https://arxiv.org/abs/2402.03367
- Authors: Zackary Rackauckas
- Reference count: 16
- Primary result: RAG-Fusion improves chatbot answer quality through multiple query generation and reciprocal rank fusion, but suffers from slower response times.

## Executive Summary
RAG-Fusion introduces an enhanced retrieval-augmented generation method that generates multiple search queries from a single user input, reranks retrieved documents using reciprocal rank fusion (RRF), and fuses these results to produce more accurate and comprehensive answers. The approach shows particular promise for technical, sales, and customer-facing applications where nuanced understanding is critical. While RAG-Fusion outperforms traditional RAG in answer quality, it introduces additional latency due to the complexity of processing multiple queries and documents, and can occasionally produce off-topic results when generated queries diverge from the original intent.

## Method Summary
RAG-Fusion combines traditional RAG with reciprocal rank fusion by first generating multiple queries from the original user input using an LLM. Each generated query is used to retrieve relevant documents from a vector database. The retrieved documents are then reranked using reciprocal rank fusion, which combines the rankings from all generated queries using a smoothing factor k. Finally, the original query, all generated queries, and the reranked documents are sent to an LLM to generate a comprehensive answer. This approach aims to capture multiple perspectives of the user's intent and improve the relevance and comprehensiveness of the retrieved documents.

## Key Results
- RAG-Fusion produces more accurate and comprehensive answers compared to traditional RAG methods.
- The approach is particularly effective in technical, sales, and customer-facing contexts where nuanced information retrieval is important.
- Manual evaluation confirms superior answer quality, though the method suffers from slower response times and occasional relevance issues when generated queries diverge from the original intent.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple query generation improves answer comprehensiveness.
- Mechanism: The model generates several queries from the original input, each contextualizing the question from different perspectives, which allows the system to retrieve and synthesize information across multiple angles.
- Core assumption: Generated queries will be semantically relevant to the original intent.
- Evidence anchors:
  - [abstract] "RAG-Fusion was able to provide accurate and comprehensive answers due to the generated queries contextualizing the original query from various perspectives."
  - [section] "Generating multiple queries is key to devising a comprehensive and multiperspective sales strategy."
- Break condition: Generated queries drift from the original intent, producing irrelevant or off-topic content.

### Mechanism 2
- Claim: Reciprocal Rank Fusion improves document prioritization over simple vector similarity.
- Mechanism: Documents are ranked by reciprocal scores across multiple query results, then reranked by accumulating scores, ensuring the most relevant documents across all generated queries surface.
- Core assumption: Combining ranks from multiple queries produces a better final list than a single-query ranking.
- Evidence anchors:
  - [abstract] "RAG-Fusion combines RAG and reciprocal rank fusion (RRF) by generating multiple queries, reranking them with reciprocal scores and fusing the documents and scores."
  - [section] "Reciprocal rank fusion is an algorithm commonly used in search to assign scores to every document and rerank them according to the scores."
- Break condition: If k (smoothing factor) is poorly tuned, RRF can overemphasize or underemphasize certain documents.

### Mechanism 3
- Claim: Fusion of original and generated queries with retrieved documents yields richer LLM context.
- Mechanism: After RRF reranking, the system sends the original query, all generated queries, and the fused document list to the LLM for answer generation.
- Core assumption: LLM can integrate multiple document sources and multiple queries into a coherent answer.
- Break condition: Excessive input length overwhelms the LLM's context window or degrades output coherence.

## Foundational Learning

- Concept: Vector embeddings and similarity search
  - Why needed here: RAG systems rely on semantic similarity via vector embeddings to retrieve relevant documents.
  - Quick check question: What is the difference between vector similarity and keyword matching in retrieval?

- Concept: Reciprocal rank fusion algorithm
  - Why needed here: RRF reorders documents by combining ranks across multiple queries, which is core to RAG-Fusion's performance.
  - Quick check question: How does the smoothing factor k affect the weighting of ranks in RRF?

- Concept: Query generation from context
  - Why needed here: Generating multiple related queries from a single input is essential for capturing different facets of the user's intent.
  - Quick check question: What are the risks if generated queries diverge semantically from the original query?

## Architecture Onboarding

- Component map:
  Query generator (LLM call) -> Vector search (document retrieval) -> Reciprocal rank fusion (reranking) -> Answer generator (LLM call with context) -> Document database (vector store)

- Critical path:
  Original query → Query generation → Multi-query vector search → RRF fusion → Context assembly → Answer generation

- Design tradeoffs:
  - Query generation increases comprehensiveness but adds latency and risk of drift.
  - RRF improves relevance but is sensitive to the k parameter.
  - Sending more documents/context improves accuracy but may hit LLM context limits.

- Failure signatures:
  - Long response times: Likely from the second LLM call or high document volume.
  - Off-topic answers: Generated queries diverged from original intent.
  - Missing information: Document database lacks coverage for certain queries.

- First 3 experiments:
  1. Vary k in RRF and measure impact on document ranking quality.
  2. Test different numbers of generated queries to balance comprehensiveness vs. latency.
  3. Evaluate effect of limiting context size sent to LLM on answer quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RAG-Fusion vary with different values of the smoothing factor k in the reciprocal rank fusion formula?
- Basis in paper: [explicit] The paper mentions that k is a constant smoothing factor that determines the weight given to the existing ranks, and that RRF is sensitive to its parameters.
- Why unresolved: The paper does not provide empirical results on how varying k affects the performance of RAG-Fusion in terms of accuracy, relevance, or comprehensiveness of answers.
- What evidence would resolve it: Systematic testing of RAG-Fusion with different values of k, measuring and comparing the accuracy, relevance, and comprehensiveness of answers using human evaluations or automated frameworks like RAGElo and Ragas.

### Open Question 2
- Question: Can the challenges of slow response times and irrelevant query generation in RAG-Fusion be addressed by optimizing the prompt engineering process?
- Basis in paper: [inferred] The paper mentions that users must sometimes engineer prompts to be more specific and comprehensible so that the large language model generates appropriate queries. It also suggests providing a prompt creation guide to users.
- Why unresolved: The paper does not explore the potential of optimizing the prompt engineering process to improve the relevance of generated queries and reduce the complexity of the second API call to the large language model.
- What evidence would resolve it: Developing and testing optimized prompt engineering techniques that improve the relevance of generated queries and reduce the complexity of the second API call, resulting in faster response times and more accurate answers.

### Open Question 3
- Question: How can the Infineon RAG-Fusion chatbot be adapted to provide negative answers when the queried information is not available in the database?
- Basis in paper: [explicit] The paper mentions that the bot tends to respond with uncertainty when it cannot find documents relating to the user's query, instead of providing a negative answer.
- Why unresolved: The paper does not explore methods to enable the chatbot to provide definitively negative answers when the queried information is not available in the database.
- What evidence would resolve it: Implementing and testing techniques that allow the chatbot to recognize when the queried information is not available in the database and provide a clear negative answer, such as updating the knowledge base with a list of negative responses or using a separate model to handle negative answers.

## Limitations
- The study lacks public reproducibility due to unspecified hyperparameters for query generation and RRF, as well as undisclosed vector embedding and LLM specifications.
- The method suffers from slower response times and occasional relevance degradation from query drift, which are practical constraints in real-world applications.
- The narrow domain focus on Infineon's technical documentation limits the generalizability of the findings to broader applications.

## Confidence
- Mechanism clarity: Medium-High
- Reproducibility: Low
- Generalization: Medium
- Performance claims: Medium

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically vary the RRF smoothing factor k and number of generated queries to quantify their impact on both answer quality and response latency across different query types.

2. **Cross-Domain Generalization Test**: Apply the RAG-Fusion architecture to a different technical documentation corpus (e.g., medical devices or automotive) to assess performance stability and identify domain-specific failure patterns.

3. **Automated Evaluation Framework Integration**: Implement RAGElo and Ragas frameworks to establish repeatable, objective metrics for RAG-Fusion performance, comparing these automated scores against the manual evaluations reported in the study.