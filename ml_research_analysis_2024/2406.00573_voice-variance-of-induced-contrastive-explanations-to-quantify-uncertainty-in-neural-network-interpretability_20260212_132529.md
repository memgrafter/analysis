---
ver: rpa2
title: 'VOICE: Variance of Induced Contrastive Explanations to quantify Uncertainty
  in Neural Network Interpretability'
arxiv_id: '2406.00573'
source_url: https://arxiv.org/abs/2406.00573
tags:
- uncertainty
- explanations
- network
- gradcam
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of quantifying predictive uncertainty
  in gradient-based visual explanations for neural networks. The authors propose a
  plug-in approach called VOICE (Variance of Induced Contrastive Explanations) to
  visualize and measure the uncertainty associated with any gradient-based explanatory
  technique.
---

# VOICE: Variance of Induced Contrastive Explanations to quantify Uncertainty in Neural Network Interpretability

## Quick Facts
- arXiv ID: 2406.00573
- Source URL: https://arxiv.org/abs/2406.00573
- Authors: Mohit Prabhushankar; Ghassan AlRegib
- Reference count: 40
- Key outcome: A plug-in approach called VOICE (Variance of Induced Contrastive Explanations) that quantifies uncertainty in gradient-based visual explanations by generating contrastive explanations and calculating their variance.

## Executive Summary
This paper addresses the critical problem of quantifying predictive uncertainty in gradient-based visual explanations for neural networks. The authors propose VOICE, a method that generates contrastive explanations between the predicted class and multiple contrast classes, then calculates the variance across these explanations to create an uncertainty heatmap. The approach introduces two quantitative metrics - Intersection over Union (IoU) and Signal to Noise Ratio (SNR) - to characterize the uncertainty of explanations. The key insight is that under incorrect predictions, explanatory techniques are often uncertain about the same features they attribute to the prediction, reducing trustworthiness.

## Method Summary
The VOICE method works by generating contrastive explanations between the predicted class and multiple contrast classes. For each contrast class, the method backpropagates a loss between the predicted class and the contrast class to obtain an explanation. These contrastive explanations are stacked, and pixelwise variance is computed across them. The resulting variance map is normalized to create the VOICE uncertainty map. Two metrics are used to quantify uncertainty: IoU measures overlap between the base explanation and uncertainty map (higher overlap indicates lower trustworthiness), while SNR measures the dispersion of uncertainty values (higher SNR indicates more uncertainty).

## Key Results
- VOICE effectively quantifies uncertainty in gradient-based explanations, with uncertainty maps highlighting regions around explanations.
- Under incorrect predictions, explanations often overlap significantly with their uncertainty maps, reducing trustworthiness.
- IoU and SNR metrics empirically behave similarly to epistemic uncertainty, with lower values indicating more reliable explanations.
- The method works across different datasets (ImageNet, CIFAR-10C), explanation techniques (GradCAM, GradCAM++, Guided Backpropagation, SmoothGrad), and network architectures (AlexNet, VGG-16, ResNet-18, DenseNet-169, SqueezeNet, Swin Transformer).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The VOICE method captures the residual variance term E[V(y|Sx)] that existing explainability evaluations ignore.
- Mechanism: By generating contrastive explanations between the predicted class P and multiple contrast classes Q, VOICE identifies the feature subsets SP,Q that cause the model to switch predictions. The variance across these contrastive explanations reveals uncertainty about the original explanation's features.
- Core assumption: Contrastive explanations effectively represent all possible feature perturbations that could change the prediction, making their variance a valid proxy for the residual uncertainty term.
- Evidence anchors:
  - [abstract]: "We theoretically show that existing evaluation strategies of visual explanatory techniques partially reduce the predictive uncertainty of neural networks."
  - [section III.B]: "The second term of the RHS in Eq. 1 is the variance in the expectation of y under explanation masked image Sx. Note that current explanatory techniques use V[E(y|Sx)] as a means of evaluating their methods."
  - [corpus]: Weak - no direct citation to this specific variance decomposition in neighbors.

### Mechanism 2
- Claim: The IoU metric quantifies overlap between explanations and their uncertainties, indicating trustworthiness.
- Mechanism: When the uncertainty map overlaps significantly with the explanation map, it suggests the network is uncertain about the very features it's using to make its prediction. Higher IoU indicates lower trustworthiness.
- Core assumption: The uncertainty map highlights regions the network is uncertain about, and overlap with the explanation indicates uncertainty about the explanation's features.
- Evidence anchors:
  - [abstract]: "The proposed uncertainty visualization and quantification yields two key observations. Firstly, oftentimes under incorrect predictions, explanatory techniques are uncertain about the same features that they are attributing the predictions to, thereby reducing the trustworthiness of the explanation."
  - [section IV.B.a]: "If the explanation-specific uncertainty map um overlaps with the explanation m, then the overlap can be interpreted as the network being uncertain about the regions in the image that it is using to make its decision."
  - [corpus]: Weak - neighbors focus on uncertainty quantification but not specifically on IoU between explanations and uncertainties.

### Mechanism 3
- Claim: SNR measures the dispersion of uncertainty, with higher values indicating more uncertainty in the explanation.
- Mechanism: SNR compares the mean intensity of the uncertainty map to its standard deviation. Higher SNR means greater dispersion of uncertainty values, suggesting the explanation is less reliable.
- Core assumption: A well-calibrated uncertainty map should have low dispersion when the explanation is trustworthy, and high dispersion when it's not.
- Evidence anchors:
  - [abstract]: "Secondly, objective metrics of an explanation's uncertainty, empirically behave similarly to epistemic uncertainty."
  - [section IV.B.b]: "SNR measures the ratio of the mean intensity of um against the dispersion of the intensity of um across the pixels. Higher the SNR of um, more is the uncertainty."
  - [corpus]: Weak - neighbors discuss uncertainty quantification but not specifically SNR as a measure of explanation uncertainty.

## Foundational Learning

- Concept: Predictive uncertainty decomposition (V[y] = V[E(y|Sx)] + E[V(y|Sx)])
  - Why needed here: Understanding this decomposition is crucial for grasping why existing explainability evaluations only partially reduce uncertainty and why VOICE targets the residual term.
  - Quick check question: What are the two terms in the predictive uncertainty decomposition, and which one does VOICE focus on quantifying?

- Concept: Contrastive explanations
  - Why needed here: VOICE relies on generating contrastive explanations to identify feature subsets that cause prediction changes. Understanding how these work is essential for implementing the method.
  - Quick check question: How do contrastive explanations differ from traditional correlation explanations, and why are they useful for quantifying uncertainty?

- Concept: Gradient-based attribution methods
  - Why needed here: VOICE is a plug-in for gradient-based explanation techniques. Familiarity with methods like Grad-CAM, Guided Backpropagation, etc., is necessary for applying the approach.
  - Quick check question: What are the key differences between semantic (e.g., Grad-CAM) and pixel-level (e.g., Guided Backpropagation) gradient-based explanation methods?

## Architecture Onboarding

- Component map: Input image → Forward pass through network → Prediction P → Base explanation (m) using gradient method → P + Q contrast classes → R contrastive explanations → Stack contrastive explanations → Compute pixelwise variance → Normalize → VOICE uncertainty map → IoU and SNR metrics

- Critical path: The forward pass and generation of contrastive explanations are the most computationally intensive steps. Ensuring efficient implementation of these is crucial.

- Design tradeoffs:
  - Number of contrast classes (R): More classes provide better uncertainty estimates but increase computation time.
  - Threshold for selecting contrast classes: Lower thresholds include more classes but may add noise; higher thresholds are more selective but might miss important uncertainties.
  - Normalization method for VOICE map: Different normalization techniques can affect the interpretability of the uncertainty visualization.

- Failure signatures:
  - VOICE map is nearly uniform (low variance) - might indicate insufficient contrast classes or a poorly trained network.
  - IoU is consistently low across all explanations - could suggest the uncertainty map is not well-calibrated.
  - SNR is consistently high - might indicate the network is generally uncertain about its predictions.

- First 3 experiments:
  1. Apply VOICE to a simple CNN (e.g., VGG-16) on a clean image with correct prediction. Verify that the VOICE map highlights regions around the explanation.
  2. Repeat with an incorrectly predicted image. Observe if the overlap between explanation and VOICE increases.
  3. Apply to a noisy version of the same image. Check if both the explanation and VOICE become more dispersed and if their overlap increases.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical justification for VOICE rests on a predictive uncertainty decomposition that lacks direct citation support.
- The core assumption that contrastive explanations capture all relevant feature perturbations is asserted but not rigorously validated.
- The IoU and SNR metrics, while showing empirical correlation with epistemic uncertainty, are defined post-hoc without theoretical grounding.

## Confidence

- **Medium**: The mechanism of using contrastive explanation variance to quantify uncertainty is plausible but relies on untested assumptions about the completeness of contrast classes.
- **Medium**: The empirical observation that IoU/SNR correlate with epistemic uncertainty is supported by results but lacks theoretical explanation for this relationship.
- **Low**: The claim that existing explainability evaluations only partially reduce uncertainty is theoretically asserted but not empirically validated through ablation studies.

## Next Checks

1. **Contrast Class Coverage Analysis**: Systematically vary the number of contrast classes (R) and threshold probability (pt) to determine how these hyperparameters affect the stability and quality of VOICE maps. Identify the minimum number of contrast classes needed for reliable uncertainty estimates.

2. **Theoretical Grounding for Metrics**: Develop theoretical justification for why IoU and SNR should correlate with explanation trustworthiness. This could involve relating these metrics to established uncertainty quantification frameworks or conducting ablation studies on synthetic datasets with known uncertainty properties.

3. **Validation on Controlled Datasets**: Apply VOICE to datasets where ground truth uncertainty is known (e.g., corrupted images with varying noise levels, out-of-distribution examples) to verify that the method correctly identifies regions of high uncertainty and correlates with prediction confidence.