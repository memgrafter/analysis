---
ver: rpa2
title: Visual Hallucinations of Multi-modal Large Language Models
arxiv_id: '2402.14683'
source_url: https://arxiv.org/abs/2402.14683
tags:
- image
- mllm
- instances
- mode
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VHTest, a tool to generate diverse visual
  hallucination (VH) instances to evaluate multi-modal large language models (MLLMs).
  Existing benchmarks rely on limited VH images from existing datasets, leading to
  biased understanding of MLLM performance.
---

# Visual Hallucinations of Multi-modal Large Language Models

## Quick Facts
- arXiv ID: 2402.14683
- Source URL: https://arxiv.org/abs/2402.14683
- Reference count: 40
- Primary result: Introduces VHTest tool that generates diverse visual hallucination instances, finding state-of-the-art MLLMs hallucinate on large fractions of test cases

## Executive Summary
This paper addresses the critical problem of visual hallucinations in multi-modal large language models (MLLMs) by introducing VHTest, a comprehensive framework for generating diverse hallucination instances and evaluating MLLM performance. The authors identify that existing benchmarks are limited by their reliance on a small number of VH images from existing datasets, leading to biased understanding of MLLM capabilities. VHTest systematically generates 1,200 VH instances across 8 distinct modes by leveraging CLIP/DINO v2 similarity discrepancies and text-to-image generation. The framework not only provides a robust evaluation benchmark but also demonstrates that fine-tuning MLLMs on VH instances significantly reduces hallucination rates without sacrificing performance on other tasks.

## Method Summary
VHTest operates through a pipeline that first identifies initial VH instances by finding image pairs with high CLIP similarity but low DINO v2 similarity in existing datasets like COCO. These initial instances are then used to generate text descriptions explaining the hallucination patterns, which are fed to text-to-image models (primarily DALL·E-3) to create diverse VH images. The framework generates 150 images per hallucination mode, covering scenarios like rotated objects, wrong color objects, occluded objects, and others. For each generated image, relevant questions and reference answers are created to form a comprehensive benchmark. The authors fine-tune LLaVA-1.5 on 80% of this benchmark and demonstrate significant reduction in hallucination rates while maintaining performance on other established benchmarks.

## Key Results
- State-of-the-art MLLMs hallucinate on a large fraction of VHTest instances, revealing significant blind spots in current models
- DALL·E-3 outperforms other text-to-image models in generating VH instances that trigger hallucinations
- Fine-tuning LLaVA-1.5 on VHTest training set reduces hallucination rates without sacrificing performance on other benchmarks
- The benchmark covers 8 distinct hallucination modes with 1,200 total instances, providing comprehensive evaluation coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial construction generates more challenging VH instances than collecting from existing datasets
- Mechanism: By finding image pairs with high CLIP similarity but low DINO v2 similarity, VHTest identifies images that are incorrectly embedded by CLIP but visually distinct, creating opportunities for hallucination when MLLMs rely on CLIP embeddings
- Core assumption: CLIP and DINO v2 embeddings capture different aspects of visual semantics, and discrepancies between them indicate potential hallucination triggers
- Evidence anchors:
  - [abstract] "We find image pairs that have large cosine similarity under CLIP but small cosine similarity under DINO v2"
  - [section 3.1] "Such image pairs have contradictory similarities from two powerful vision encoders, indicating potential VHs"
  - [corpus] Weak - corpus shows related work on hallucination detection but not this specific adversarial approach
- Break condition: If CLIP and DINO v2 become too aligned in their embeddings, or if MLLMs stop relying primarily on CLIP embeddings

### Mechanism 2
- Claim: Text-to-image models can generate VH instances following learned hallucination patterns
- Mechanism: After identifying initial VH instances, VHTest generates text descriptions explaining why hallucinations occur, then uses text-to-image models to create new images that follow these patterns
- Core assumption: Text-to-image models can accurately interpret and reproduce visual patterns that trigger hallucinations in MLLMs
- Evidence anchors:
  - [section 3.3] "We use a text-to-image generative model (e.g., DALL·E-3) to generate VH images based on the text descriptions"
  - [section 4.3] "DALL·E-3 is the most effective tool in generating VH instances that are likely to trigger VHs in MLLMs"
  - [corpus] Weak - corpus contains related hallucination work but not specific to text-to-image generation for this purpose
- Break condition: If text-to-image models change their generation patterns or become less aligned with MLLM perceptual biases

### Mechanism 3
- Claim: Fine-tuning on VH instances reduces hallucination without sacrificing general performance
- Mechanism: Training MLLMs on VH instances helps them learn to recognize and avoid the visual patterns that cause hallucinations
- Core assumption: VH instances capture generalizable patterns of hallucination that transfer to new, unseen images
- Evidence anchors:
  - [abstract] "we find that fine-tuning an MLLM using our benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks"
  - [section 4.4] "Our results indicate that fine-tuning an MLLM on our benchmark makes it less likely to hallucinate"
  - [corpus] Weak - corpus shows related hallucination mitigation but not this specific approach
- Break condition: If VH instances are too specific and don't generalize, or if fine-tuning causes catastrophic forgetting of other capabilities

## Foundational Learning

- Concept: Vision-language model architecture (vision encoder, connector, LLM)
  - Why needed here: Understanding how MLLMs process images is crucial for understanding where hallucinations occur
  - Quick check question: What are the three main components of an MLLM and how do they interact?

- Concept: Image embedding similarity metrics
  - Why needed here: VHTest relies on comparing CLIP and DINO v2 embeddings to find potential hallucination triggers
  - Quick check question: Why does VHTest look for image pairs with high CLIP similarity but low DINO v2 similarity?

- Concept: Adversarial example generation
  - Why needed here: VHTest uses adversarial construction principles to generate challenging test cases
  - Quick check question: How is VHTest's approach similar to traditional adversarial example generation?

## Architecture Onboarding

- Component map: Image pair → CLIP/DINO comparison → initial VH instances → text descriptions → text-to-image generation → questions/answers → benchmark
- Critical path: The pipeline flows from finding initial VH instances through CLIP/DINO comparison, generating text descriptions, creating VH images via text-to-image models, and finally constructing questions and answers to form the complete benchmark
- Design tradeoffs: Using DALL·E-3 provides best results but requires API access; other text-to-image models need prompt engineering; human-in-the-loop ensures quality but limits scalability
- Failure signatures: Low accuracy on benchmark indicates VHTest working as intended; high accuracy suggests MLLMs not hallucinating or VHTest not generating challenging enough instances
- First 3 experiments:
  1. Run VHTest on a small subset of COCO to verify it can find initial VH instances
  2. Generate text descriptions for one VH mode and verify they make sense
  3. Use text-to-image model to generate a few VH images and manually verify they trigger hallucinations

## Open Questions the Paper Calls Out

Open Question 1
- Question: How can VHTest be made fully automatic without requiring human workers to manually generate question-answer pairs for VH images?
- Basis in paper: [explicit] The paper acknowledges that VHTest still requires human workers to manually generate question-answer pairs for automatically generated VH images, and suggests making VHTest fully automatic as a future work
- Why unresolved: The current implementation of VHTest relies on human workers to manually analyze VH images and generate relevant questions and reference answers, which is a time-consuming and labor-intensive process. Automating this step would significantly improve the efficiency and scalability of VHTest
- What evidence would resolve it: A fully automated VHTest that can generate diverse and relevant questions and reference answers for VH images without human intervention would demonstrate the feasibility of this approach

Open Question 2
- Question: How does the choice of text-to-image generative model affect the quality and diversity of generated VH instances?
- Basis in paper: [explicit] The paper compares the performance of different text-to-image generative models (DALL-E-3, Midjourney 6, Stable Diffusion XL 1.0, and Stable Diffusion 2.1) in generating VH instances and finds that DALL-E-3 is the most effective
- Why unresolved: While the paper provides a comparison of different text-to-image generative models, it does not explore the underlying reasons for their varying performance or investigate the potential of newer or more advanced models
- What evidence would resolve it: A comprehensive analysis of the strengths and weaknesses of different text-to-image generative models in generating VH instances, along with an exploration of their underlying mechanisms, would provide insights into how to optimize VH instance generation

Open Question 3
- Question: How does the fine-tuning of different components of an MLLM (vision encoder, vision-language connector, and LLM) individually and collectively affect the mitigation of VH?
- Basis in paper: [explicit] The paper mentions that fine-tuning all components of an MLLM (vision encoder, vision-language connector, and LLM) achieves the best overall results in mitigating VH, but does not provide a detailed analysis of the individual and collective effects of fine-tuning each component
- Why unresolved: The paper does not investigate the specific contributions of fine-tuning each component of an MLLM to VH mitigation, which could provide insights into the most effective fine-tuning strategies
- What evidence would resolve it: A detailed analysis of the individual and collective effects of fine-tuning each component of an MLLM on VH mitigation, along with an exploration of the underlying mechanisms, would provide insights into the most effective fine-tuning strategies

## Limitations

- The evaluation relies heavily on CLIP and DINO v2 embeddings, which may not capture all aspects of MLLM decision-making across different architectures
- Human-in-the-loop components introduce potential bias and limit the scalability of the benchmark
- Fine-tuning results are only demonstrated on LLaVA-1.5 without testing generalization to other MLLM architectures

## Confidence

- High confidence: The methodology for identifying VH instances using CLIP/DINO v2 similarity differences is technically sound and well-explained
- Medium confidence: The effectiveness of DALL·E-3 for generating VH-triggering images, based on comparison with other text-to-image models
- Low confidence: The generalization of fine-tuning results across different MLLM architectures and the long-term stability of hallucination reduction

## Next Checks

1. Test whether VHTest-identified instances trigger hallucinations in MLLMs that use different vision encoders (not just CLIP) to verify the adversarial approach generalizes beyond CLIP-based models

2. Conduct a longitudinal study of fine-tuned MLLMs to determine if hallucination reduction persists over extended use and whether catastrophic forgetting occurs for other capabilities

3. Implement an automated version of the human-in-the-loop components (using GPT-4 or similar) to scale VH instance generation and verify that automated selection produces comparable results to manual curation