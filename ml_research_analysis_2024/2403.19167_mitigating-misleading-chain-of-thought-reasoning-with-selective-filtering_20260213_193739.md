---
ver: rpa2
title: Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering
arxiv_id: '2403.19167'
source_url: https://arxiv.org/abs/2403.19167
tags:
- reasoning
- answer
- language
- fine-tuning
- self-reasoner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SelF-Reasoner, a method that selectively
  uses chain-of-thought (CoT) reasoning to improve small language models' performance
  on reasoning tasks. The approach employs a reasoner to generate candidate reasoning
  chains, a CoT filter to assess the validity of these chains, and an answerer to
  either use the reasoning chain or predict the answer directly based on the filter's
  output.
---

# Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering

## Quick Facts
- **arXiv ID**: 2403.19167
- **Source URL**: https://arxiv.org/abs/2403.19167
- **Authors**: Yexin Wu; Zhuosheng Zhang; Hai Zhao
- **Reference count**: 10
- **Primary result**: SelF-Reasoner selectively uses CoT reasoning to improve small language models' performance on reasoning tasks, achieving accuracy comparable to human performance on ScienceQA

## Executive Summary
This paper introduces SelF-Reasoner, a method that selectively uses chain-of-thought (CoT) reasoning to improve small language models' performance on reasoning tasks. The approach employs a reasoner to generate candidate reasoning chains, a CoT filter to assess the validity of these chains, and an answerer to either use the reasoning chain or predict the answer directly based on the filter's output. SelF-Reasoner improves the fine-tuned T5 baseline consistently over ScienceQA, ECQA, and LastLetter tasks. On ScienceQA, the method outperforms the pipeline baseline and achieves comparable accuracy to a human's, advancing the effectiveness of CoT in small-scale language models.

## Method Summary
SelF-Reasoner is a three-component system that selectively applies chain-of-thought reasoning to improve small language model performance. The reasoner generates candidate reasoning chains from questions, the CoT filter evaluates whether each chain is valid by assessing entailment with the question, and the answerer predicts the final answer either directly from the question or by extracting from the question-chain pair based on the filter's judgment. The method is trained on T5 models using datasets including ScienceQA, ECQA, and LastLetter, with the CoT filter specifically trained on QR-label pairs constructed from pipeline outputs labeled by answer correctness.

## Key Results
- SelF-Reasoner consistently improves fine-tuned T5 baseline accuracy across ScienceQA, ECQA, and LastLetter tasks
- On ScienceQA, SelF-Reasoner achieves accuracy comparable to human performance and outperforms the pipeline baseline
- The method successfully mitigates the performance degradation often seen in small models when using CoT fine-tuning alone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The CoT filter improves performance by selectively discarding misleading reasoning chains that would otherwise harm answer accuracy.
- **Mechanism**: The filter assesses the entailment relationship between the question and candidate reasoning chain. When the filter judges the chain as valid, the answerer uses it to extract the answer; otherwise, the answerer predicts directly from the question alone. This prevents erroneous reasoning chains from misleading the answer extraction step.
- **Core assumption**: Invalid reasoning chains are distinguishable from valid ones through entailment analysis, and filtering them out yields better accuracy than using all chains.
- **Evidence anchors**:
  - [abstract]: "SelF-Reasoner proceeds with CoT reasoning when the reasoning chain demonstrates confidence; otherwise opting to predict the answer directly."
  - [section 4.4]: "Our SelF-Reasoner gets the best performance consistently over the ScienceQA, ECQA, and LastLetter tasks."
  - [corpus]: Weak - no direct neighbor studies on selective filtering in CoT, though some work on CoT verification exists (e.g., Lightman et al. 2023).
- **Break condition**: If the filter cannot reliably distinguish valid from invalid chains, performance may degrade to that of the unfiltered pipeline or worse.

### Mechanism 2
- **Claim**: Small language models benefit from CoT fine-tuning when combined with selective filtering, whereas CoT alone often degrades performance due to erroneous chains.
- **Mechanism**: Fine-tuning teaches the reasoner to generate reasoning chains and the answerer to extract answers from Q-R pairs. The filter removes bad chains, so the answerer only uses reliable reasoning. This allows small models to leverage the interpretability benefits of CoT without suffering from its pitfalls.
- **Core assumption**: Small models can generate reasoning chains, but their quality is inconsistent; filtering preserves only the useful ones.
- **Evidence anchors**:
  - [abstract]: "Despite its success, the efficacy of such reasoning is inherently contingent upon the quality of CoT... particularly in the case of small-scale language models."
  - [section 4.4]: "The accuracy of CoT fine-tuning is lower than that of vanilla fine-tuning, in contrast to the benefits of CoT prompting in LLMs. The main reason is that the loss of the answer part is weakened by the CoT part."
  - [corpus]: Weak - most CoT work focuses on large models or distillation, not selective filtering for small models.
- **Break condition**: If the fine-tuned reasoner cannot produce any valid chains, the model defaults to direct prediction and gains no benefit from CoT.

### Mechanism 3
- **Claim**: The selective filtering reasoner improves interpretability by generating reasoning chains only when they are likely to be valid, reducing noise in the model's reasoning process.
- **Mechanism**: The CoT filter acts as a gatekeeper, ensuring that only chains with high confidence in their entailment to the question are passed to the answerer. This selective use of CoT preserves the model's ability to provide interpretable reasoning without overwhelming the user with flawed explanations.
- **Core assumption**: Users benefit more from occasional correct, interpretable reasoning than from frequent but often incorrect chains.
- **Evidence anchors**:
  - [abstract]: "We propose a novel approach called the selective filtering reasoner (SelF-Reasoner) that assesses the entailment relationship between the question and the candidate reasoning chain."
  - [section 5.1]: "CoT fine-tuned model can produce invalid reasoning chains... BLEU and ROUGE metrics of correct samples are higher than the ones of incorrect samples, suggesting a quality gap in produced reasoning chains."
  - [corpus]: Weak - limited neighbor studies on interpretability in filtered CoT systems.
- **Break condition**: If the filter becomes overly conservative and rarely allows CoT usage, the system loses interpretability benefits.

## Foundational Learning

- **Concept**: Chain-of-thought (CoT) reasoning in language models
  - Why needed here: The method builds on CoT prompting and fine-tuning to generate intermediate reasoning steps. Understanding CoT is essential to grasp how the reasoner and answerer modules function.
  - Quick check question: What is the difference between Zero-Shot CoT and Few-Shot CoT prompting?

- **Concept**: Fine-tuning vs. in-context learning
  - Why needed here: The paper compares fine-tuned models (T5) to in-context learning approaches (GPT-3, ChatGPT). Knowing the distinction helps understand the experimental setup and results.
  - Quick check question: Why might a fine-tuned small model outperform a few-shot prompted large model on a specific task?

- **Concept**: Entailment and natural language inference
  - Why needed here: The CoT filter relies on assessing whether the reasoning chain entails the question. Familiarity with entailment detection is key to understanding the filter's operation.
  - Quick check question: How might you determine if a reasoning chain logically follows from a given question?

## Architecture Onboarding

- **Component map**: Question -> Reasoner -> Reasoning Chain -> CoT Filter (Valid/Invalid) -> Answerer -> Final Answer

- **Critical path**:
  1. Input question → Reasoner → Generated reasoning chain
  2. Q + R → CoT Filter → Valid/Invalid label
  3. If valid: Q + R → Answerer → Final answer
     If invalid: Q → Answerer → Final answer

- **Design tradeoffs**:
  - Using a filter adds model size and inference overhead but improves accuracy and interpretability.
  - Fine-tuning the reasoner and answerer separately allows specialization but requires more training data.
  - Selective filtering preserves interpretability but may reduce the frequency of reasoning explanations.

- **Failure signatures**:
  - If the filter is too permissive, invalid chains may still mislead the answerer.
  - If the filter is too strict, the model may rarely use CoT and lose interpretability.
  - If the reasoner fails to generate any valid chains, the model defaults to direct prediction.

- **First 3 experiments**:
  1. Train the reasoner on Q-R pairs and evaluate the quality of generated chains using BLEU/ROUGE and human evaluation.
  2. Train the CoT filter on QR-label pairs and measure its accuracy in classifying valid vs. invalid chains.
  3. Assemble the full SelF-Reasoner pipeline and compare its accuracy to the vanilla fine-tuning, pipeline, and random baselines on a validation set.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can we develop interpretable filtering techniques for detecting invalid reasoning chains in chain-of-thought (CoT) fine-tuning?
- **Basis in paper**: [explicit] The paper mentions that assessing the correctness of the reasoning chain, instead of merely determining if it leads to the correct answer, remains a complex task. It also suggests that future research can focus on developing interpretable filtering techniques utilizing rationalization methods.
- **Why unresolved**: The current CoT filter in SelF-Reasoner is not fully interpretable, and there is a need for more effective methods to distinguish valid from invalid reasoning chains.
- **What evidence would resolve it**: Development and evaluation of new filtering techniques that provide clear explanations for why a reasoning chain is considered valid or invalid, along with improved accuracy in detecting incorrect chains.

### Open Question 2
- **Question**: What is the impact of the reasoning chain's format on the performance of CoT fine-tuning in language models?
- **Basis in paper**: [inferred] The paper discusses the trade-off between templated reasoning chains and those of diverse forms. It mentions that the annotation of reasoning chains for existing benchmarks requires significant effort and may not adhere to the correct CoT format. The paper suggests that future work could focus on the impact of the reasoning chain's format on CoT fine-tuning.
- **Why unresolved**: The role of the reasoning chain within the prediction procedure is uncertain, and the impact of different reasoning chain formats on model performance is not fully understood.
- **What evidence would resolve it**: Experiments comparing the performance of CoT fine-tuning with different reasoning chain formats, such as templated vs. diverse forms, and analysis of how the format affects the model's ability to learn and generate correct reasoning chains.

### Open Question 3
- **Question**: How can we improve the memorization and coherence of small language models in generating longer reasoning chains?
- **Basis in paper**: [explicit] The paper mentions that small language models may encounter obstacles in generating both reasoning chains and answers in a single turn due to restricted maximum input and output length. It also notes that these models may struggle to maintain coherence in longer output sequences and may have difficulty fully internalizing intricate relationships within the training data.
- **Why unresolved**: Small language models have limited parameters and may not be able to effectively learn and generate complex, coherent reasoning chains.
- **What evidence would resolve it**: Development of new training methods, such as joint training with both rationale loss and answer loss, or incorporation of token rank information from the reasoning chains, that improve the model's ability to generate longer, more coherent reasoning chains while maintaining accuracy in the final answer prediction.

## Limitations

- **Limited empirical scope**: Evaluation is constrained to a small set of tasks (ScienceQA, ECQA, LastLetter), leaving generalizability unproven
- **Filter reliability assumptions**: The paper provides limited quantitative analysis of filter accuracy and no error analysis of false positives/negatives
- **Training data generation challenge**: Constructing QR-label pairs becomes difficult when the pipeline achieves high training accuracy, potentially affecting filter training robustness

## Confidence

**High confidence**: The basic claim that selective filtering improves accuracy over vanilla fine-tuning on the tested tasks. The experimental results show consistent improvements across multiple datasets and model sizes.

**Medium confidence**: The claim that this approach advances small model performance to levels comparable with human accuracy on ScienceQA. While the results are impressive, they are based on a single dataset and comparison point.

**Medium confidence**: The interpretability benefits of selective filtering. The paper argues that occasional correct reasoning is preferable to frequent incorrect chains, but does not provide user studies or quantitative measures of interpretability.

**Low confidence**: The generalizability of selective filtering to other reasoning tasks or model architectures. The paper focuses on T5 models and specific QA tasks without exploring broader applicability.

## Next Checks

1. **Filter ablation study**: Remove the CoT filter and compare performance between using all generated chains versus direct prediction only. This would quantify how much improvement comes specifically from filtering versus other aspects of the pipeline.

2. **Filter error analysis**: Conduct a detailed analysis of the CoT filter's false positive and false negative rates. Evaluate whether misclassifications correlate with specific types of reasoning errors or question formats.

3. **Cross-dataset validation**: Test SelF-Reasoner on additional reasoning tasks (e.g., numerical reasoning, logical inference) to assess whether the selective filtering approach generalizes beyond the current evaluation set.