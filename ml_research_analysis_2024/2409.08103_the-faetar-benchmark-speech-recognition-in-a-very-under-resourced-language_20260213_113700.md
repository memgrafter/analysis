---
ver: rpa2
title: 'The Faetar Benchmark: Speech Recognition in a Very Under-Resourced Language'
arxiv_id: '2409.08103'
source_url: https://arxiv.org/abs/2409.08103
tags:
- speech
- faetar
- languages
- data
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces the Faetar Automatic Speech Recognition Benchmark,\
  \ a new corpus designed to push the limits of low-resource speech recognition. Faetar\
  \ is a Franco-Proven\xE7al variety spoken in Italy with no standard orthography\
  \ and virtually no existing textual or speech resources."
---

# The Faetar Benchmark: Speech Recognition in a Very Under-Resourced Language

## Quick Facts
- arXiv ID: 2409.08103
- Source URL: https://arxiv.org/abs/2409.08103
- Reference count: 0
- Primary result: Best phone error rate of 30.5% achieved through combined pre-training and self-training on multilingual foundation models

## Executive Summary
The Faetar Automatic Speech Recognition Benchmark introduces a challenging new corpus for low-resource speech recognition in Faetar, a Franco-Provençal language variety spoken in Italy. The benchmark includes 5 hours of transcribed speech and 20 hours of unlabelled speech from noisy field recordings, with transcriptions that lack standardization. Baseline experiments using state-of-the-art multilingual speech foundation models achieved a best phone error rate of 30.5% through a pipeline combining continued pre-training on unlabelled data with self-training techniques. The benchmark aims to encourage research on speech recognition in under-resourced languages with inconsistent transcriptions and noisy audio.

## Method Summary
The benchmark uses multilingual foundation models (MMS or mHuBERT-147) fine-tuned on 5 hours of transcribed Faetar speech data. The pipeline includes continued pre-training on 20 hours of unlabelled speech using self-supervised learning, followed by fine-tuning on the labelled set. Initial alignments are generated using Kaldi GMM-HMM, and performance is evaluated using phone error rate (PER) computed via Levenshtein alignment. The approach combines pre-training to adapt to Faetar's acoustic characteristics with self-training to expand the effective training data through decoding unlabelled speech.

## Key Results
- Best phone error rate of 30.5% achieved using combined pre-training and self-training approach
- Continued pre-training on unlabelled data alone improved performance significantly
- Self-training through decoding unlabelled speech and retraining yielded cumulative improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuing pre-training on multilingual foundation models using the unlabelled Faetar speech data improves performance by adapting to the language's acoustic and phonetic characteristics.
- Mechanism: The model learns acoustic patterns specific to Faetar's phonology, such as geminate consonants and unique phone realizations, before fine-tuning on the small labelled set.
- Core assumption: The foundation model's representations can be adapted to a very different language with limited data through self-supervised pre-training.
- Evidence anchors:
  - [abstract]: "best phone error rate of 30.4%, using a pipeline that continues pre-training on the foundation model using the unlabelled set."
  - [section]: "continued pre-training of MMS (effective batch size 32; 300000 steps ≈258 epochs, of which the first 30% are warmup in a linear scheduler; peak learning rate 3×10−4), then fine-tune as before. As can be seen from Table 3, this leads to a large performance boost."
- Break condition: If the unlabelled data does not share sufficient acoustic-phonetic similarity with the labelled subset, or if the model overfits to noise patterns in the unlabelled data.

### Mechanism 2
- Claim: Using inconsistent phonetic transcriptions is manageable for phone-level evaluation because the error metric (PER) aligns with the transcription granularity.
- Mechanism: Since the transcriptions are already phonetic, PER measures the model's ability to produce the correct phone sequences without requiring normalization to an orthography.
- Core assumption: Phone error rate is a reasonable proxy for ASR performance when orthographic normalization is not feasible or desirable.
- Evidence anchors:
  - [abstract]: "baseline results using state-of-the-art multilingual speech foundation models achieved a best phone error rate of 30.5%"
  - [section]: "We chose PER (no spaces, affricates and long phones single units) to avoid making the task unnecessarily difficult."
- Break condition: If the transcription inconsistencies are so severe that PER no longer reflects meaningful ASR performance, or if downstream applications require standardized orthography.

### Mechanism 3
- Claim: The combination of pre-training and self-training on the unlabelled set yields cumulative improvements by both adapting representations and expanding training data diversity.
- Mechanism: Pre-training adapts the model to Faetar acoustics, while self-training generates additional training examples from unlabelled data, increasing effective training data.
- Core assumption: The model's decoded outputs from unlabelled data are accurate enough to serve as useful training targets in a second fine-tuning stage.
- Evidence anchors:
  - [section]: "Alternatively, we attempt self-training, decoding unlab using the fine-tuned (base) MMS model, then adding the resulting decoded speech to a second round of fine-tuning. Finally, we combine both approaches... leading to a best PER of 30.5% when combining both approaches."
- Break condition: If the self-training introduces significant error propagation, or if the unlabelled data is too noisy for reliable decoding.

## Foundational Learning

- Concept: Self-supervised pre-training in speech models
  - Why needed here: Faetar has very limited labelled data (5 hours), so leveraging unlabelled speech through self-supervised learning is critical to bootstrap model performance.
  - Quick check question: What is the primary objective of self-supervised pre-training in speech foundation models like MMS or HuBERT?

- Concept: Phone-level evaluation metrics (PER vs WER/CER)
  - Why needed here: Faetar lacks a standard orthography, making phone-level evaluation more appropriate than word-level metrics.
  - Quick check question: Why is PER preferred over WER in this benchmark, and how does it handle phonetic variation?

- Concept: Speaker diarization and voice activity detection
  - Why needed here: The corpus contains long-form recordings with background noise and multiple speakers, requiring preprocessing to isolate relevant speech segments.
  - Quick check question: How does speaker diarization contribute to cleaning and segmenting the Faetar corpus for ASR training?

## Architecture Onboarding

- Component map: Data preprocessing (VAD, diarization) -> Foundation model pre-training/fine-tuning -> Alignment generation (GMM-HMM) -> PER evaluation
- Critical path: Pre-training on unlabelled data → Fine-tuning on labelled data → Decoding on test set → PER evaluation
- Design tradeoffs: Using compact multilingual model (MMS) vs. larger monolingual model; phonetic vs. phonemic transcriptions; pre-training vs. self-training strategies
- Failure signatures: High PER on dev set but low improvement on test set (overfitting); large gap between constrained and unconstrained results (data scarcity); inconsistent alignments causing poor model initialization
- First 3 experiments:
  1. Run baseline MMS fine-tuning on 5h labelled data only to establish performance floor
  2. Apply continued pre-training on unlabelled set and compare PER improvement
  3. Implement self-training by decoding unlabelled data and retraining to measure cumulative gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal approach for handling inconsistent transcriptions in low-resource languages without standard orthographies?
- Basis in paper: [explicit] The paper discusses inconsistent transcriptions as a major limitation and notes that previous approaches rely on manual labelling or elicitation, which are prohibitively time-intensive.
- Why unresolved: No existing method can automatically map sub-lexical transcriptions to headwords without manual intervention, and the authors note that clustering-based lexicon approaches did not yield improvements.
- What evidence would resolve it: Development and evaluation of an unsupervised method that can accurately group variant phonetic transcriptions into consistent lexical forms, validated on multiple low-resource language corpora with inconsistent transcriptions.

### Open Question 2
- Question: How much improvement can be achieved by combining speech enhancement techniques with pre-training on unlabelled data for extremely low-resource ASR?
- Basis in paper: [inferred] The authors note that baseline PERs would generally be unacceptable without manual correction and mention that additional approaches like speech enhancement remain unexplored.
- What evidence would resolve it: Systematic experiments comparing ASR performance with and without speech enhancement preprocessing, combined with various pre-training strategies, on the Faetar benchmark and other similar low-resource corpora.

### Open Question 3
- Question: What is the intrinsic lower bound (noise threshold) for PER in the Faetar benchmark given the inconsistent transcription conventions?
- Basis in paper: [explicit] The authors state that inconsistent transcriptions mean there is an intrinsic lower bound for PER which is unknown.
- Why unresolved: The lack of standardized orthography and varying transcription conventions between phonetic and phonemic levels make it impossible to determine what constitutes "perfect" transcription.
- What evidence would resolve it: Development of a reference standard for Faetar transcription through expert linguistic analysis, followed by evaluation of human transcribers' agreement rates on the same audio material.

## Limitations

- The benchmark relies on inconsistent phonetic transcriptions that may not generalize to applications requiring standardized orthography
- Unlabelled data quality varies significantly due to background noise and overlapping speech, potentially limiting pre-training effectiveness
- The paper lacks comprehensive ablation studies isolating the contributions of each technique (pre-training, self-training, and their combination)

## Confidence

**High Confidence**: The claim that Faetar is a severely under-resourced language with no standard orthography is well-supported by the description of the corpus and its origins. The observation that multilingual foundation models can be adapted to such languages through continued pre-training is also well-established in the literature.

**Medium Confidence**: The specific performance numbers (30.5% PER as best result) are credible given the described methodology and comparison with baseline results, but would benefit from additional validation on standardized test sets. The effectiveness of the combined pre-training and self-training approach is supported but not comprehensively isolated from other factors.

**Low Confidence**: The paper's claim about the benchmark's ability to "push the limits" of low-resource speech recognition is somewhat vague and difficult to verify without broader community adoption and comparison with other extreme low-resource benchmarks.

## Next Checks

1. **Transcription Normalization Impact Study**: Conduct an experiment comparing PER with and without transcription normalization to quantify the impact of inconsistent transcriptions on model performance and determine if normalization would significantly improve results.

2. **Ablation Study of Pre-training Strategies**: Implement a comprehensive ablation study that isolates the effects of continued pre-training, self-training, and their combination by systematically removing each component and measuring the performance degradation.

3. **Cross-corpus Generalization Test**: Evaluate the pre-trained Faetar models on a small held-out set of speech from a related Franco-Provençal variety (if available) to assess whether the learned representations generalize to similar under-resourced languages.