---
ver: rpa2
title: Using Generative Agents to Create Tip Sheets for Investigative Data Reporting
arxiv_id: '2409.07286'
source_url: https://arxiv.org/abs/2409.07286
tags:
- data
- agents
- reporter
- analyst
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a generative AI pipeline using specialized
  agents (analyst, reporter, editor) to generate tip sheets for investigative data
  reporting. The system processes datasets to produce newsworthy insights and was
  evaluated against real-world investigative journalism stories.
---

# Using Generative Agents to Create Tip Sheets for Investigative Data Reporting

## Quick Facts
- arXiv ID: 2409.07286
- Source URL: https://arxiv.org/abs/2409.07286
- Authors: Joris Veerbeek; Nicholas Diakopoulos
- Reference count: 17
- Primary result: Agent-based pipeline generates more newsworthy insights (67% vs 52%) than baseline model

## Executive Summary
This paper presents a generative AI pipeline using specialized agents (analyst, reporter, editor) to generate tip sheets for investigative data reporting. The system processes datasets to produce newsworthy insights and was evaluated against real-world investigative journalism stories. Results show the agent-based approach outperforms a baseline model in newsworthiness and achieves high validity, though precision remains moderate. The system demonstrates potential for automated lead generation in investigative journalism while highlighting the importance of human editorial oversight.

## Method Summary
The study develops a three-agent pipeline using OpenAI's Assistants API to generate tip sheets from datasets. The system consists of specialized analyst, reporter, and editor agents that work through iterative feedback loops. The analyst agent executes data analyses based on questions generated by the reporter, while the editor ensures analytical validity using data journalism guidelines. The pipeline processes CSV datasets with accompanying Markdown descriptions to produce final tip sheets containing potentially newsworthy insights.

## Key Results
- Agent-based system outperforms baseline model in newsworthiness (67% vs 52%)
- High validity score achieved (89%) indicating accurate findings
- Moderate precision rate (34%) showing room for improvement in targeting relevant insights
- System demonstrates potential for automated lead generation in investigative journalism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The three-agent architecture improves both newsworthiness and validity compared to a single baseline model.
- Mechanism: Each agent has a specialized role (analyst, reporter, editor) that performs distinct subtasks and provides mutual feedback, creating a collaborative refinement loop.
- Core assumption: Specialized roles with feedback loops can outperform a monolithic model in both generating newsworthy insights and ensuring analytical validity.
- Evidence anchors:
  - [abstract] "our agent-based system generally generates more newsworthy and accurate insights compared to a baseline model without agents"
  - [section] "Overall, the user provides a dataset and a data description, and the final output is a tip sheet of potentially newsworthy insights from the data"
  - [corpus] Weak - no direct evidence in corpus papers about multi-agent collaborative refinement specifically for investigative journalism
- Break condition: If the feedback loops become too time-consuming or the agents' outputs diverge significantly, the collaborative benefit diminishes and may actually slow down the process without improving quality.

### Mechanism 2
- Claim: The feedback loop between reporter and analyst agents significantly improves the newsworthiness of generated insights.
- Mechanism: The reporter agent generates questions and assesses newsworthiness, then provides feedback to the analyst, who revises the analysis accordingly.
- Core assumption: Human-like editorial oversight through agent feedback can identify and enhance newsworthy angles that a single-pass model might miss.
- Evidence anchors:
  - [abstract] "our agent-based system generally generates more newsworthy and accurate insights compared to a baseline model without agents"
  - [section] "The reporter is instructed to assess it under one of three possible options" and "The reporter's response always includes the chosen option and, if necessary, specific feedback on the analysis"
  - [corpus] Weak - no direct evidence in corpus papers about reporter-analyst feedback loops specifically for newsworthiness assessment
- Break condition: If the reporter consistently selects option 3 (lacks newsworthy insights) for most questions, the feedback loop becomes ineffective and the system fails to generate valuable tips.

### Mechanism 3
- Claim: The editor agent ensures analytical validity by enforcing data journalism best practices.
- Mechanism: The editor agent reviews analytical plans and findings using a knowledge base of data journalism guidelines, providing bulletproofing feedback.
- Core assumption: Access to domain-specific guidelines through document retrieval functions can catch errors and ensure more valid analytical approaches.
- Evidence anchors:
  - [abstract] "our agent-based system generally generates more newsworthy and accurate insights compared to a baseline model without agents"
  - [section] "the editor is equipped with document retrieval functions, which allow it to access three general sets of guidelines for bulletproofing data-driven work"
  - [corpus] Weak - no direct evidence in corpus papers about editor agents with document retrieval for data journalism guidelines
- Break condition: If the editor's guidelines are too rigid or outdated, they may prevent innovative analytical approaches and reduce the system's ability to find novel insights.

## Foundational Learning

- Concept: Investigative data journalism workflows
  - Why needed here: Understanding the typical workflow helps design agents that mirror real-world roles and produce actionable leads
  - Quick check question: What are the three main roles in traditional investigative data journalism teams?

- Concept: Newsworthiness assessment criteria
  - Why needed here: The system must evaluate potential newsworthiness using established criteria to produce relevant tips
  - Quick check question: What are the eight news values described by Harcup and O'Neill (2016) that can determine potential newsworthiness?

- Concept: Data analysis and interpretation methods
  - Why needed here: The analyst agent must translate journalistic questions into appropriate quantitative analyses
  - Quick check question: What are the key differences between exploratory data analysis and confirmatory data analysis in investigative journalism?

## Architecture Onboarding

- Component map: User Interface -> Reporter Agent -> Analyst Agent -> Editor Agent -> Pipeline Controller -> Output Module
- Critical path: User input → Question Generation → Analytical Planning → Execution & Interpretation → Compilation → Tip Sheet output
- Design tradeoffs:
  - Number of questions vs. depth of analysis: More questions generate more potential leads but may reduce depth
  - Feedback loop iterations vs. processing time: More iterations improve quality but increase latency
  - Agent specialization vs. system complexity: More specialized agents improve performance but increase maintenance overhead
- Failure signatures:
  - High rate of option 3 selections by reporter agent indicates poor question generation
  - Editor agent consistently requesting major revisions suggests flawed analytical planning
  - Baseline outperforming agents on validity indicates insufficient editorial oversight
- First 3 experiments:
  1. Test with a simple dataset (e.g., US Census data) and measure time-to-tip-sheet vs. quality metrics
  2. Vary the number of questions (5, 10, 15) to find optimal balance between breadth and depth
  3. Disable the editor agent to measure the impact of bulletproofing on overall validity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can generative agents be optimized to better handle messy or imperfect datasets in investigative data reporting?
- Basis in paper: [inferred] The paper mentions that the civio-emergency dataset was relatively messy and might contain incorrect values, leading to incorrect interpretations when analyzing interactions between variables.
- Why unresolved: The paper does not explore strategies for improving agent performance with imperfect data, such as data cleaning techniques or validation methods.
- What evidence would resolve it: Experiments comparing agent performance on clean vs. messy datasets with and without data preprocessing steps, or evaluations of different data validation techniques.

### Open Question 2
- Question: What are the specific editorial biases introduced by generative agents in selecting newsworthy insights, and how do they compare to human editorial decisions?
- Basis in paper: [explicit] The paper notes that typically only a third of findings generated end up in the final article, highlighting the role of the editorial process in refining insights.
- Why unresolved: The paper does not analyze the types of insights agents prioritize versus those humans prioritize, nor does it examine systematic differences in selection criteria.
- What evidence would resolve it: Comparative studies analyzing the thematic focus, news values, and selection patterns between agent-generated and human-generated tip sheets across multiple datasets.

### Open Question 3
- Question: How can generative agents be adapted to incorporate more active reporter participation in the investigative process beyond just receiving final tips?
- Basis in paper: [explicit] The paper suggests that in real newsrooms, reporters should have greater control through additional input possibilities, such as participating in the brainstorming phase.
- Why unresolved: The current system design limits reporter agency to feedback loops rather than collaborative exploration, and the paper does not propose specific mechanisms for deeper reporter involvement.
- What evidence would resolve it: Prototype systems allowing reporters to guide question generation, suggest analytical approaches, or interactively refine insights, with evaluation of how this affects the quality and relevance of generated tips.

## Limitations

- Evaluation methodology relies on comparison against published stories, which may introduce selection bias
- Moderate precision rate (34%) indicates system generates many leads that don't align with actual published stories
- Results may not generalize across different types of investigative datasets and journalistic contexts

## Confidence

- **High confidence**: The three-agent architecture's general improvement over baseline models in newsworthiness and validity metrics
- **Medium confidence**: The specific mechanisms of agent feedback loops and their contribution to improved outcomes
- **Medium confidence**: The generalizability of results across different types of investigative datasets and journalistic contexts

## Next Checks

1. **External validation with newsroom partners**: Deploy the system with multiple investigative journalism teams to assess real-world utility and gather diverse feedback on workflow integration and output quality.

2. **Cross-dataset robustness testing**: Evaluate system performance across varied data types (demographic, financial, environmental) to identify domain-specific strengths and weaknesses.

3. **Longitudinal performance tracking**: Monitor how generated tip sheets perform over time in actual investigations, measuring which leads ultimately result in published stories and their impact.