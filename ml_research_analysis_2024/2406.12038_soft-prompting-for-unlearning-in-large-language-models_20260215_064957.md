---
ver: rpa2
title: Soft Prompting for Unlearning in Large Language Models
arxiv_id: '2406.12038'
source_url: https://arxiv.org/abs/2406.12038
tags:
- unlearning
- forget
- spul
- soft
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of removing sensitive or unwanted
  information from large language models (LLMs) while preserving model utility. The
  authors propose a novel soft prompting approach called SPUL (Soft Prompting for
  Unlearning) that learns lightweight prompt tokens to guide LLMs towards forgetting
  specific examples without updating model parameters.
---

# Soft Prompting for Unlearning in Large Language Models

## Quick Facts
- arXiv ID: 2406.12038
- Source URL: https://arxiv.org/abs/2406.12038
- Authors: Karuna Bhaila; Minh-Hao Van; Xintao Wu
- Reference count: 23
- One-line primary result: SPUL achieves 94-99% accuracy on retain sets while reducing accuracy on unlearning sets to 12-60%, demonstrating efficient and scalable unlearning in LLMs

## Executive Summary
This paper introduces SPUL (Soft Prompting for Unlearning), a novel approach to removing sensitive or unwanted information from large language models without updating model parameters. The method uses lightweight prompt tokens that guide the LLM to misclassify forget samples while preserving accuracy on retain samples. Experiments on sentiment classification tasks show SPUL significantly outperforms fine-tuning baselines, achieving effective unlearning with minimal utility loss.

## Method Summary
SPUL learns prompt tokens that can be appended to queries to induce unlearning of specific examples without updating LLM parameters. The method uses a multi-objective loss function combining cross-entropy loss with generic labels for forget samples, cross-entropy loss with true labels for retain samples, and KL divergence to maintain base model output similarity for retain samples. This approach allows the prompts to learn patterns that distinguish forget from retain samples and apply appropriate classification behavior while only optimizing a small number of parameters compared to full fine-tuning.

## Key Results
- SPUL achieves 94-99% accuracy on retain sets while reducing accuracy on unlearning sets to 12-60%
- The method outperforms fine-tuning baselines including GA, RL, GA+KL, and GA+GD
- SPUL is validated across multiple LLMs and shows robustness to varying sizes of unlearning data
- The approach requires only 10-50 soft prompt tokens versus billions of parameters in the base model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft prompts can encode unlearn-specific information that guides the LLM to misclassify forget samples while preserving accuracy on retain samples.
- Mechanism: The soft prompt tokens are trained using a multi-objective loss that combines cross-entropy loss with generic labels for forget samples, cross-entropy loss with true labels for retain samples, and KL divergence to maintain base model output similarity for retain samples.
- Core assumption: The soft prompts can capture semantic differences between forget and retain samples through gradient-based optimization without updating the base LLM parameters.
- Evidence anchors:
  - [abstract] "SPUL learns prompt tokens that can de-correlate text features and labels for unlearning while maintaining performance on retain samples"
  - [section] "We optimize a set of soft prompt parameters that learn to encode underlying information in the data relevant for unlearning"
- Break condition: If the soft prompts cannot effectively encode the distinguishing features between forget and retain samples, the method will fail to achieve the desired unlearning while preserving utility.

### Mechanism 2
- Claim: Soft prompting provides a more efficient alternative to fine-tuning for LLM unlearning by only updating a small number of parameters.
- Mechanism: Instead of updating all or a subset of LLM parameters, SPUL only updates the soft prompt tokens (typically 10-50 tokens vs billions of parameters in the base model).
- Core assumption: A small number of optimized prompt tokens can effectively guide the frozen LLM to achieve the desired unlearning behavior.
- Evidence anchors:
  - [abstract] "Soft Prompting for Unlearning (SPUL) learns prompt tokens that can be appended to an arbitrary query to induce unlearning of specific examples at inference time without updating LLM parameters"
  - [section] "SPUL unlearns undesirable outcomes without updating large-scale LLM parameters and can fully capitalize on the language understanding capability offered by the pre-trained LLMs"
- Break condition: If the number of soft prompt tokens is insufficient to encode the necessary unlearn information, or if the frozen LLM cannot be effectively guided by the prompts, the method will fail to achieve effective unlearning.

### Mechanism 3
- Claim: The multi-objective loss function balances the competing objectives of forgetting unwanted data and preserving utility on retained data.
- Mechanism: The loss function L = Lf + α · Lr + β · Lkl combines three objectives: forcing misclassification on forget samples, maintaining correct classification on retain samples, and constraining output distribution changes on retain samples.
- Core assumption: The multi-objective loss can effectively balance the conflicting goals of forgetting and utility preservation during optimization.
- Evidence anchors:
  - [abstract] "With losses designed to enforce forgetting as well as utility preservation, our framework Soft Prompting for Unlearning (SPUL) learns prompt tokens that can be appended to an arbitrary query to induce unlearning of specific examples"
  - [section] "We utilize Lkl in addition to Lr to avoid large deviations in the base model's output due to the influence from Lf"
- Break condition: If the loss function cannot properly balance the forgetting and utility preservation objectives, the method may either fail to forget effectively or lose too much utility.

## Foundational Learning

- Concept: Prompt tuning/soft prompting in LLMs
  - Why needed here: Understanding how soft prompts work is essential to grasp SPUL's mechanism. The method relies on optimizing continuous prompt tokens rather than updating model parameters.
  - Quick check question: What is the key difference between soft prompting and traditional fine-tuning in LLMs?

- Concept: Multi-objective optimization
  - Why needed here: SPUL uses a multi-objective loss function to balance forgetting and utility preservation. Understanding how to optimize multiple competing objectives is crucial.
  - Quick check question: How do the hyperparameters α and β in the loss function L = Lf + α · Lr + β · Lkl control the trade-off between forgetting and utility preservation?

- Concept: Unlearning in machine learning
  - Why needed here: The fundamental goal of SPUL is to remove the influence of specific training data. Understanding the principles of machine unlearning helps contextualize the method's objectives.
  - Quick check question: What is the difference between unlearning and traditional data deletion in machine learning?

## Architecture Onboarding

- Component map: Base LLM (frozen) -> Soft prompt tokens -> Multi-objective loss function -> Training loop
- Critical path:
  1. Initialize soft prompt tokens randomly
  2. Compute loss on forget samples (Lf) using generic labels
  3. Compute loss on retain samples (Lr) using true labels
  4. Compute KL divergence loss (Lkl) for retain samples
  5. Combine losses with hyperparameters α and β
  6. Update only soft prompt tokens via backpropagation
  7. Evaluate on forget and retain sets to measure unlearning and utility preservation

- Design tradeoffs:
  - Number of soft prompt tokens: More tokens may capture more information but increase computational cost
  - Hyperparameter tuning (α, β): Balancing forgetting vs. utility preservation requires careful tuning
  - Choice of generic labels: The set of generic labels used for forget samples can affect unlearning effectiveness

- Failure signatures:
  - Poor forgetting performance: If accuracy on forget set remains high after training, the soft prompts aren't effectively encoding unlearn information
  - Significant utility loss: If accuracy on retain set drops substantially, the loss function is overemphasizing forgetting at the expense of utility
  - Inconsistent results across datasets: If method works well on one dataset but poorly on another, the approach may be too dataset-specific

- First 3 experiments:
  1. Implement basic soft prompting on a simple classification task without unlearning objectives to verify the prompt tuning mechanism works
  2. Add the forgetting loss (Lf) component and test on a binary classification task with artificially created forget/retain sets
  3. Implement the full multi-objective loss with all three components (Lf, Lr, Lkl) and evaluate the trade-off between forgetting and utility preservation

## Open Questions the Paper Calls Out
- Question: How does the size of the soft prompt tokens (p) impact the effectiveness of unlearning across different types of tasks beyond text classification?
- Question: Can the soft prompting approach be extended to handle unlearning in scenarios where the forget set contains data from multiple distinct domains or tasks?
- Question: What are the implications of soft prompting-based unlearning on the interpretability and explainability of large language models?

## Limitations
- The evaluation is limited to sentiment classification tasks, leaving effectiveness on more complex tasks unproven
- Claims about robustness across varying unlearning data sizes are not comprehensively validated
- The method's performance on larger LLMs with different architectures remains untested

## Confidence

**High confidence**: The fundamental mechanism of using soft prompts for unlearning without updating base model parameters is well-supported.

**Medium confidence**: The claimed efficiency and scalability benefits over fine-tuning methods are plausible but not comprehensively validated.

**Low confidence**: Claims about robustness across varying unlearning data sizes and the method's effectiveness on more complex LLM architectures are not sufficiently supported.

## Next Checks

1. Implement SPUL on a multi-class classification task (e.g., news categorization or intent classification) with multiple forget/retain categories to test the method's effectiveness beyond binary sentiment tasks.

2. Apply SPUL to a decoder-only LLM (e.g., GPT-2 or GPT-3) and evaluate whether the soft prompting mechanism works as effectively as with BERT-style architectures, particularly focusing on how tokenization differences affect the prompt token optimization.

3. Systematically vary the size of the forget set from 5% to 50% of the training data and measure both unlearning effectiveness and utility preservation to validate the claimed robustness to unlearning data size variations.