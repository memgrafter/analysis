---
ver: rpa2
title: 'OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion Models'
arxiv_id: '2411.10501'
source_url: https://arxiv.org/abs/2411.10501
tags:
- video
- motion
- flow
- optical
- onlyflow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OnlyFlow enables precise motion control in text-to-video generation
  by conditioning on optical flow from an input video, transferring its motion to
  generated content. The method injects encoded optical flow features into temporal
  attention layers of a video diffusion model, allowing motion transfer without specific
  training for tasks like camera control or video editing.
---

# OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion Models

## Quick Facts
- arXiv ID: 2411.10501
- Source URL: https://arxiv.org/abs/2411.10501
- Reference count: 40
- Key outcome: OnlyFlow enables precise motion control in text-to-video generation by conditioning on optical flow from an input video, transferring its motion to generated content.

## Executive Summary
OnlyFlow is a method for precise motion control in text-to-video generation that conditions on optical flow from an input video to transfer its motion patterns to generated content. The approach injects encoded optical flow features into temporal attention layers of a video diffusion model, allowing users to generate videos that respect both the motion of an input video and a text prompt. The method demonstrates versatility across various video generation tasks including artistic video creation and camera movement control, with quantitative improvements in FVD and optical flow fidelity. OnlyFlow is lightweight, does not require specific training for different control tasks, and shows superior performance compared to state-of-the-art methods in both motion fidelity and overall quality according to user studies.

## Method Summary
OnlyFlow extracts optical flow from an input video using RAFT-Large, encodes it through a trainable flow encoder, and injects the resulting features into temporal attention layers of a video diffusion U-Net. The optical flow conditioning is scaled by a parameter γ that controls the strength of motion influence relative to text conditioning. The method is trained on the WebVid dataset with 10.7M video-caption pairs, using Adam optimizer for 20 hours on 8 A100 GPUs. The optical flow encoder is trained while the base video diffusion model (AnimateDiff) remains frozen. Generated videos are evaluated using FVD, optical flow fidelity metrics, CLIP score for text alignment, and user preference studies comparing motion fidelity and overall quality against state-of-the-art methods.

## Key Results
- Quantitative evaluation shows improved FVD (lower is better) and optical flow fidelity with increased conditioning strength γ
- User studies show OnlyFlow is preferred over state-of-the-art methods in both motion fidelity and overall quality
- The method demonstrates effective motion transfer for artistic video creation and camera movement control
- Generated video realism and resolution are limited by the base model capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optical flow conditioning in temporal attention layers transfers motion from input video to generated video while preserving prompt fidelity.
- Mechanism: Optical flow from an input video is extracted, encoded via a trainable flow encoder, and injected into temporal attention layers of the diffusion U-Net. This forces the temporal attention to attend to the motion patterns present in the input video.
- Core assumption: Temporal attention layers are the most appropriate location for motion conditioning because they capture sequential and causal relationships across frames, which aligns with the nature of optical flow.
- Evidence anchors:
  - [abstract]: "Using a text prompt and an input video, OnlyFlow allows the user to generate videos that respect the motion of the input video as well as the text prompt."
  - [section]: "We opt to insert the optical flow conditioning representations into the temporal attentional blocks. This decision is motivated by the ability of the temporal attention layer to capture temporal dependencies, which is consistent with the sequential and causal nature of optical flow."
  - [corpus]: Weak. Corpus papers focus on surgical or anime-specific applications; no direct evidence of temporal attention layer effectiveness for general motion transfer.
- Break condition: If the optical flow conditioning strength γ is too high, it may overpower the text conditioning and reduce prompt fidelity.

### Mechanism 2
- Claim: Increasing optical flow conditioning strength γ improves both motion fidelity and visual realism (FVD).
- Mechanism: The parameter γ scales the contribution of the encoded optical flow features in the temporal attention layers. Higher γ values inject stronger motion constraints, leading to generated videos that more closely match the motion of the input video.
- Core assumption: Adding realistic motion from real data as conditioning naturally improves FVD because the model is encouraged to produce temporally coherent motion patterns.
- Evidence anchors:
  - [abstract]: "Quantitative evaluation shows improved FVD (lower is better) and optical flow fidelity with increased conditioning strength."
  - [section]: "As for the evaluation metrics, we validate our model using the FVD, flow fidelity, and textual alignment metrics, previously detailed. Finally, to analyze the impact of our proposed motion module, we evaluate several optical flow conditioning strengths γ (Eq. (2))."
  - [corpus]: Weak. No direct evidence in corpus that higher conditioning strength universally improves both metrics.
- Break condition: Excessive γ may lead to unrealistic motion artifacts or divergence from the prompt content.

### Mechanism 3
- Claim: OnlyFlow can semantically align motion features with text prompts without requiring explicit control maps.
- Mechanism: The optical flow encoder, when trained, learns to map motion patterns to semantically meaningful regions in the generated video. If the motion in the input video matches the prompt concept (e.g., waves for "ocean"), the model generates content that aligns with both motion and text.
- Core assumption: The optical flow encoder can learn a latent representation that bridges motion and semantics, similar to how ControlNet learns to map control maps to content.
- Evidence anchors:
  - [abstract]: "We show the versatility of our approach in various video generation tasks, where several experiments demonstrate its use case."
  - [section]: "We noticed an interesting capability of semantic alignment between the descriptive information from the optical flow fed into the model and the given prompt."
  - [corpus]: Weak. Corpus does not contain evidence of semantic alignment capabilities.
- Break condition: If the motion in the input video does not match the prompt concept, the semantic alignment may fail or produce irrelevant content.

## Foundational Learning

- Concept: Optical flow estimation and its role in motion representation
  - Why needed here: Optical flow provides dense, pixel-level motion information between consecutive frames, which is essential for transferring motion from an input video to a generated one.
  - Quick check question: What is the output of an optical flow estimation model, and how does it represent motion?

- Concept: Temporal attention in video diffusion models
  - Why needed here: Temporal attention layers capture dependencies across frames, making them the ideal location for injecting motion conditioning that needs to influence the entire video sequence.
  - Quick check question: How does temporal attention differ from spatial attention in a video diffusion model?

- Concept: Classifier-free guidance and conditioning strength
  - Why needed here: The conditioning strength γ controls the balance between motion conditioning and text conditioning, similar to how classifier-free guidance balances text guidance and unconditional generation.
  - Quick check question: What happens when the classifier-free guidance scale is set too high in a diffusion model?

## Architecture Onboarding

- Component map:
  - Text prompt → Tokenizer → Text encoder
  - Input video → Optical flow estimator (RAFT) → Optical flow encoder → Feature maps
  - Noise latents → Diffusion U-Net (spatial and temporal layers) ← Injected flow features
  - Output: Generated video

- Critical path:
  1. Extract optical flow from input video using RAFT
  2. Pass flow through trainable optical flow encoder
  3. Inject encoded features into temporal attention layers of U-Net
  4. Run diffusion sampling with text and flow conditioning

- Design tradeoffs:
  - Temporal vs. spatial injection: Temporal injection captures motion across frames; spatial injection might capture static motion patterns but could miss temporal coherence.
  - Conditioning strength γ: Higher values improve motion fidelity but risk reducing prompt adherence.

- Failure signatures:
  - Low motion fidelity: Optical flow encoder not trained well or conditioning strength too low
  - Poor prompt adherence: Conditioning strength too high or flow encoder overfits to training data
  - Visual artifacts: Flow estimation errors or mismatched aspect ratios

- First 3 experiments:
  1. Baseline: Generate video with text only (no flow conditioning) to establish FVD and CLIP score
  2. Simple flow test: Use synthetic optical flow (e.g., constant translation) with low γ to verify motion transfer
  3. Ablation: Compare temporal vs. spatial injection strategies with the same γ value to validate design choice

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology suggests several areas for future investigation regarding optical flow estimation methods, semantic alignment capabilities, and performance across different video characteristics.

## Limitations
- Generated video realism and resolution are fundamentally limited by the base model capabilities
- Performance depends heavily on the quality of optical flow estimation, which may fail for complex or occluded scenes
- The semantic alignment mechanism is underspecified, making it unclear how motion and text conditioning conflicts are resolved

## Confidence
- **High confidence**: The core mechanism of injecting optical flow features into temporal attention layers is technically sound and well-supported by the theory of temporal dependencies in video diffusion models.
- **Medium confidence**: The quantitative improvements in FVD and flow fidelity with increased conditioning strength are demonstrated, but the exact relationship between γ values and metric improvements needs more systematic analysis across diverse motion types.
- **Medium confidence**: User study preferences for OnlyFlow over state-of-the-art methods are reported, but the study design details (sample size, participant expertise, prompt diversity) are limited in the paper.

## Next Checks
1. Cross-domain robustness test: Evaluate OnlyFlow on video generation tasks from domains not well-represented in WebVid (e.g., aerial footage, microscopic videos, or animation) to assess generalization beyond the training distribution.
2. Flow estimation error analysis: Systematically evaluate how RAFT's flow estimation errors (measured on ground truth flow datasets) correlate with OnlyFlow's generation quality metrics. This would quantify the sensitivity to the optical flow extraction quality.
3. Prompt-motion mismatch study: Design controlled experiments where the input video's motion concept conflicts with the text prompt (e.g., "a calm lake" with a video of crashing waves) to quantify how OnlyFlow resolves semantic conflicts between motion and text conditioning.