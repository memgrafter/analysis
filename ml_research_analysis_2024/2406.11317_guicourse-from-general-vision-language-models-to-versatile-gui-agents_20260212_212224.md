---
ver: rpa2
title: 'GUICourse: From General Vision Language Models to Versatile GUI Agents'
arxiv_id: '2406.11317'
source_url: https://arxiv.org/abs/2406.11317
tags:
- action
- agents
- arxiv
- actions
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GUICourse, a suite of datasets designed to
  train visual-based GUI agents from general VLMs. The method involves enhancing OCR
  and grounding capabilities with GUIEnv, enriching GUI knowledge using GUIAct, and
  improving interaction ability with GUIChat.
---

# GUICourse: From General Vision Language Models to Versatile GUI Agents

## Quick Facts
- arXiv ID: 2406.11317
- Source URL: https://arxiv.org/abs/2406.11317
- Reference count: 37
- One-line primary result: Even small-sized GUI agents (3.1B parameters) perform effectively on single-step and multi-step GUI tasks, outperforming baseline VLMs on tasks like Mind2Web and AITW.

## Executive Summary
This paper introduces GUICourse, a suite of datasets designed to train visual-based GUI agents from general vision-language models (VLMs). The method involves enhancing OCR and grounding capabilities with GUIEnv, enriching GUI knowledge using GUIAct, and improving interaction ability with GUIChat. Experiments show that even small-sized GUI agents (3.1B parameters) perform effectively on single-step and multi-step GUI tasks, outperforming baseline VLMs on tasks like Mind2Web and AITW. The ablation study demonstrates a positive correlation between OCR/grounding abilities and GUI navigation performance. Source codes and datasets are publicly released.

## Method Summary
The paper proposes a method to train GUI agents from general VLMs by first pre-training on GUIEnv-global (10M samples) to improve OCR and grounding, then fine-tuning with GUIEnv-local (0.7M samples) for targeted text-bounding box tasks. Subsequently, the model is fine-tuned on GUIAct (67k single-step, 5,696 multi-step web navigation, 9,157 smartphone navigation) to enrich GUI knowledge and interaction skills, and finally on GUIChat (44k single-turn QA, 6k multi-turn dialogues) to enhance conversational abilities. The method uses cosine scheduler with learning rates of 5e-7 for pre-training and 1e-5 for fine-tuning, and evaluates performance using metrics like StepSR, Cli.Acc, Type EM, Ele.Acc, and IoU scores.

## Key Results
- GUI agents with 3.1B parameters outperform baseline VLMs on Mind2Web and AITW tasks.
- Pre-training on GUIEnv-global significantly improves OCR and grounding capabilities.
- GUIAct and GUIChat fine-tuning enhance GUI knowledge and conversational abilities, respectively.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GUIEnv dataset improves OCR and grounding by providing large-scale, high-resolution website screenshots with annotated text regions.
- **Mechanism:** Pre-training on GUIEnv-global (10M samples) exposes the VLM to diverse text layouts, fonts, and positioning, while GUIEnv-local (0.7M samples) provides targeted "text2bbox" and "bbox2text" tasks for fine-tuning.
- **Core assumption:** High-resolution images and precise bounding boxes enable VLMs to learn accurate text recognition and localization in GUI contexts.
- **Evidence anchors:**
  - [abstract]: "We introduce the GUIEnv dataset to strengthen the OCR and grounding capabilities of VLMs."
  - [section 3.1]: "GUIEnv-global has 10M samples... GUIEnv-local has 0.7M samples... each sample is a QA pair on a designated region as the 'text2bbox' or 'bbox2text' task."
  - [corpus]: Weak evidence; no direct citations in corpus, but related works (e.g., GUI-Shift, ScreenExplorer) suggest OCR/grounding is a recognized bottleneck.
- **Break condition:** If dataset lacks sufficient font diversity or resolution, OCR accuracy degrades; if bounding boxes are imprecise, grounding fails.

### Mechanism 2
- **Claim:** GUIAct dataset enriches GUI-specific knowledge by providing task-oriented navigation instructions paired with screenshots and actions.
- **Mechanism:** SFT on GUIAct (web-single: 67k, web-multi: 5,696, smartphone: 9,157) teaches the model element functions and control methods through real-world GUI interaction patterns.
- **Core assumption:** Learning from human-verified and human-annotated action sequences enables the model to generalize control strategies across diverse GUI systems.
- **Evidence anchors:**
  - [abstract]: "We introduce the GUIAct... to enrich their knowledge of GUI components and interactions."
  - [section 3.2]: "GUIAct dataset... provides 67k human-verified single-step and 5,696 human-annotated multi-step GUI navigation instructions..."
  - [corpus]: Weak; no direct citations, but UIPro and GUI-Shift suggest task-oriented datasets are essential for interaction capability.
- **Break condition:** If action space is too narrow or annotations are noisy, the model fails to learn robust control strategies.

### Mechanism 3
- **Claim:** GUIChat dataset improves conversational ability by exposing the model to text-rich dialogues grounded in GUI screenshots.
- **Mechanism:** SFT on GUIChat (44k single-turn, 6k multi-turn) trains the model to answer questions and reason about GUI content in natural language.
- **Core assumption:** Multi-turn dialogues with grounding boxes teach the model to link textual queries to visual elements, enhancing interaction fluency.
- **Evidence anchors:**
  - [abstract]: "We present the GUIChat dataset, featuring 44k single-turn QA pairs and 6k multi-turn dialogues alongside website screenshots..."
  - [section 3.3]: "This dataset has about 44k single-turn QA pairs and 6k multi-turn dialogues in four aspects: visual information queries, human-centric issues..."
  - [corpus]: Weak; no direct citations, but SpiritSight Agent and UIPro suggest conversational grounding is a known challenge.
- **Break condition:** If dialogues lack diversity or grounding is imprecise, conversational accuracy drops.

## Foundational Learning

- **Concept: Vision-Language Model Architecture**
  - Why needed here: GUICourse relies on connecting visual encoders (e.g., CLIP-ViT) with language models via bridge modules (MLP or attention-based) to process GUI screenshots and generate actions.
  - Quick check question: What are the two main components of a VLM, and how do they interact in GUI tasks?

- **Concept: Supervised Fine-Tuning (SFT)**
  - Why needed here: SFT is used to adapt pre-trained VLMs to GUI-specific tasks using annotated datasets (GUIEnv-local, GUIAct, GUIChat).
  - Quick check question: How does SFT differ from pre-training, and why is it critical for GUI task adaptation?

- **Concept: Grounding and OCR in Visual Contexts**
  - Why needed here: Accurate OCR and grounding are prerequisites for GUI agents to locate and interact with elements on screenshots.
  - Quick check question: What metrics (e.g., IoU, EM) are used to evaluate grounding accuracy, and why are they important?

## Architecture Onboarding

- **Component map:**
  - Vision encoder (e.g., CLIP-ViT) → Bridge module (MLP/attention) → Language model (e.g., Vicuna/Qwen) → Action parser (JSON/CSV) → GUI action executor.

- **Critical path:**
  Pre-training with GUIEnv-global → SFT with GUIEnv-local → SFT with GUIAct → SFT with GUIChat → Evaluation on GUI tasks.

- **Design tradeoffs:**
  - High resolution (1344×1344) vs. computational cost: Improves grounding but increases training time.
  - Action space granularity: Fine-grained actions (e.g., "select_text") enable precise control but require more annotations.
  - Dataset size vs. quality: Larger datasets improve generalization but may introduce noise.

- **Failure signatures:**
  - Poor OCR/grounding: Low IoU scores, incorrect element IDs in "click" actions.
  - Weak GUI knowledge: High Type EM but low StepSR (correct action names but wrong execution).
  - Conversational errors: Inaccurate answers or missing grounding boxes in dialogues.

- **First 3 experiments:**
  1. Train MiniCPM-GUI with GUIEnv only → evaluate OCR/grounding on GUIEnv-local test set.
  2. Add GUIAct (web-single) to SFT → evaluate Type EM and Cli.Acc on web-single test set.
  3. Add GUIChat to SFT → evaluate conversational accuracy on sampled GUIChat test pairs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GUI agents scale with model size beyond the tested range (e.g., 3.1B to 9.6B parameters)?
- Basis in paper: [explicit] The paper mentions testing GUI agents with sizes ranging from 3.1B to 9.6B parameters and notes that even small-sized agents perform effectively. It also states that a larger GUI agent (similar in size to Qwen-VL) outperformed SeeClick on Mind2Web.
- Why unresolved: The paper does not provide extensive scaling analysis beyond these specific model sizes. It does not explore the performance gains or limitations when scaling to much larger models (e.g., 30B+ parameters) or smaller models (e.g., 1B parameters).
- What evidence would resolve it: Conducting experiments with a wider range of model sizes, including both smaller and larger models, and analyzing the trade-offs between performance, computational cost, and practical usability would provide insights into the optimal model size for GUI agents.

### Open Question 2
- Question: How well do GUI agents generalize to GUI systems with significantly different designs or interaction patterns (e.g., virtual reality interfaces, voice-controlled systems)?
- Basis in paper: [inferred] The paper focuses on training GUI agents for websites and smartphone environments using vision-based inputs and position-based actions. It does not explore the agents' performance on GUI systems with fundamentally different interaction paradigms, such as VR or voice-controlled interfaces.
- Why unresolved: The paper's datasets and experiments are limited to traditional GUI systems (websites and smartphones), so there is no evidence of the agents' ability to generalize to other types of GUI systems.
- What evidence would resolve it: Evaluating the performance of GUI agents on a diverse set of GUI systems, including VR interfaces, voice-controlled systems, and other emerging interaction paradigms, would demonstrate the agents' generalization capabilities and limitations.

### Open Question 3
- Question: What is the impact of incorporating reinforcement learning (e.g., RLHF) on the performance and adaptability of GUI agents?
- Basis in paper: [explicit] The paper acknowledges that pretraining and supervised fine-tuning might not be sufficient to achieve assistant-level GUI agents and mentions considering reinforcement learning methods like RLHF in the future.
- Why unresolved: The paper does not implement or evaluate the impact of reinforcement learning on GUI agents. It only mentions the potential for future exploration.
- What evidence would resolve it: Conducting experiments that compare the performance of GUI agents trained with and without reinforcement learning, and analyzing the improvements in adaptability, robustness, and user satisfaction, would provide insights into the benefits and challenges of incorporating RLHF.

## Limitations

- Evaluation primarily focuses on benchmark datasets (Mind2Web, AITW) rather than real-world deployment scenarios, limiting generalizability.
- Model's performance on complex, long-horizon tasks remains unclear, as most evaluations focus on single-step or short multi-step scenarios.
- Paper doesn't address potential biases in the training data or robustness to GUI variations across different platforms and applications.

## Confidence

- **High Confidence**: Claims about the effectiveness of pre-training on GUIEnv for OCR/grounding improvements, supported by specific dataset statistics and task definitions.
- **Medium Confidence**: Claims about GUIAct and GUIChat improving GUI knowledge and conversational abilities, as these rely on auto-annotation and human verification processes not fully detailed in the paper.
- **Low Confidence**: Claims about the model's superiority over baseline VLMs on real-world tasks, as the evaluation lacks extensive ablation studies on individual dataset contributions.

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of GUIEnv, GUIAct, and GUIChat to overall performance, isolating each dataset's impact on OCR, navigation, and conversational abilities.
2. Test the model's robustness on out-of-distribution GUI screenshots, including mobile apps, desktop applications, and websites with non-standard layouts, to assess generalization beyond the training domains.
3. Evaluate the model's performance on long-horizon, multi-step tasks requiring complex reasoning and planning, such as completing a purchase workflow or troubleshooting a technical issue, to assess real-world applicability.