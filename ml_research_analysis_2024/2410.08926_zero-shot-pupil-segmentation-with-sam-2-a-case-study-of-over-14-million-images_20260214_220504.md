---
ver: rpa2
title: 'Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images'
arxiv_id: '2410.08926'
source_url: https://arxiv.org/abs/2410.08926
tags: []
core_contribution: This study evaluates SAM 2, a vision foundation model, for zero-shot
  pupil segmentation in eye tracking datasets. SAM 2 significantly reduces annotation
  time and technical barriers while maintaining high segmentation accuracy.
---

# Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images

## Quick Facts
- arXiv ID: 2410.08926
- Source URL: https://arxiv.org/abs/2410.08926
- Reference count: 10
- Key outcome: SAM 2 achieves up to 93% mIoU for zero-shot pupil segmentation across 14M+ images without fine-tuning

## Executive Summary
This study evaluates SAM 2, a vision foundation model, for zero-shot pupil segmentation in eye tracking datasets. SAM 2 significantly reduces annotation time and technical barriers while maintaining high segmentation accuracy. Tested on over 14 million images across diverse datasets, including VR and wearable eye trackers, SAM 2 achieves competitive mean Intersection over Union (mIoU) scores of up to 93% without fine-tuning. It requires only a single click per video for segmentation, outperforming traditional methods and domain-specific models. SAM 2 demonstrates robust generalization across controlled and real-world environments, making it a promising tool for large-scale eye tracking research. The study also highlights SAM 2's accessibility on low-end hardware, further democratizing its application in human-computer interaction fields.

## Method Summary
The study evaluates SAM 2's zero-shot pupil segmentation capabilities by testing it on diverse eye-tracking datasets containing over 14 million images. The model is assessed without any fine-tuning, using only a single click prompt per video for segmentation. Performance is measured against ground truth annotations using mIoU scores, comparing SAM 2 against traditional methods and domain-specific models across various eye-tracking scenarios including VR and wearable devices.

## Key Results
- SAM 2 achieves up to 93% mean Intersection over Union (mIoU) scores for pupil segmentation without fine-tuning
- The model requires only a single click per video for segmentation, significantly reducing annotation time
- SAM 2 outperforms traditional methods and domain-specific models across controlled and real-world eye-tracking datasets

## Why This Works (Mechanism)
SAM 2 leverages its foundation model architecture with extensive pretraining on diverse visual data to generalize to pupil segmentation tasks without domain-specific fine-tuning. The model's ability to understand spatial relationships and object boundaries, combined with its interactive prompting system, enables accurate pupil detection with minimal user input. The zero-shot capability stems from the model's learned visual representations that transfer effectively to eye-tracking scenarios.

## Foundational Learning
- **Zero-shot learning**: Ability to perform tasks without task-specific training - needed to avoid time-consuming fine-tuning; quick check: verify performance on unseen datasets
- **Mean Intersection over Union (mIoU)**: Metric for segmentation quality - needed to quantify segmentation accuracy; quick check: calculate mIoU against ground truth
- **Vision foundation models**: Pretrained models that generalize across visual tasks - needed for transfer learning capabilities; quick check: confirm pretraining dataset diversity

## Architecture Onboarding
**Component Map:** Input Image -> SAM 2 Encoder -> Interactive Prompt Processing -> Mask Generation -> Output Segmentation
**Critical Path:** The pipeline from image input through encoder to mask generation is critical, with the interactive prompt serving as the key differentiator from standard segmentation approaches.
**Design Tradeoffs:** Zero-shot capability trades potential peak performance for flexibility and reduced annotation burden; single-click prompting balances ease of use with segmentation accuracy.
**Failure Signatures:** Poor performance in extreme lighting conditions, occlusions, or unusual pupil shapes; potential issues with hardware constraints on low-end devices.
**First Experiments:** 1) Test SAM 2 on a single eye-tracking dataset with known ground truth; 2) Compare mIoU scores against traditional methods on the same dataset; 3) Evaluate hardware requirements on different computing platforms

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily relies on benchmark datasets, potentially limiting generalizability to extreme conditions
- Performance claims lack statistical significance testing and confidence intervals
- Hardware accessibility claims are qualitative without specific benchmarks or minimum requirements

## Confidence
- **High Confidence**: SAM 2's zero-shot segmentation capability (validated on 14M+ images)
- **Medium Confidence**: Performance relative to domain-specific models (benchmark comparisons exist but lack statistical validation)
- **Medium Confidence**: Hardware accessibility claims (qualitative assertion without quantitative benchmarks)

## Next Checks
1. Conduct statistical significance testing with confidence intervals for mIoU scores across all tested datasets
2. Evaluate downstream eye-tracking metrics (gaze accuracy, fixation identification) when using SAM 2 segmentations
3. Perform systematic testing on diverse hardware configurations to establish minimum requirements for low-end deployment scenarios