---
ver: rpa2
title: 'Refining Translations with LLMs: A Constraint-Aware Iterative Prompting Approach'
arxiv_id: '2411.08348'
source_url: https://arxiv.org/abs/2411.08348
tags:
- translation
- language
- llms
- machine
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of translating rare words and
  low-resource languages using large language models (LLMs). The authors propose a
  multi-step prompt chain that enhances translation faithfulness by prioritizing key
  terms crucial for semantic accuracy.
---

# Refining Translations with LLMs: A Constraint-Aware Iterative Prompting Approach

## Quick Facts
- arXiv ID: 2411.08348
- Source URL: https://arxiv.org/abs/2411.08348
- Authors: Shangfeng Chen; Xiayang Shi; Pu Li; Yinlin Li; Jingjing Liu
- Reference count: 12
- Primary result: BLEU scores of 41.95 on Catalan-English and 42.12 on English-Catalan

## Executive Summary
This paper addresses the challenge of translating rare words and low-resource languages using large language models (LLMs). The authors propose a multi-step prompt chain that enhances translation faithfulness by prioritizing key terms crucial for semantic accuracy. The method first identifies keywords and retrieves their translations from a bilingual dictionary using Retrieval-Augmented Generation (RAG). It then employs an iterative self-checking mechanism where the LLM refines its translations based on lexical and semantic constraints.

Experiments using Llama and Qwen as base models on the FLORES-200 and WMT datasets demonstrate significant improvements over baselines. The proposed approach achieved BLEU scores of 41.95 on Catalan-English and 42.12 on English-Catalan language pairs, outperforming standard zero-shot translation methods. The self-checking mechanism proved particularly effective in reducing hallucinations and improving translation accuracy, especially for models like Qwen2-7B-Instruct that showed tendencies to generate unintended content in target languages.

## Method Summary
The approach employs a multi-step prompt chain to improve translation faithfulness in low-resource scenarios. First, it identifies semantically important keywords in the source sentence using LLM-based scoring. These keywords are then translated using a bilingual dictionary via Retrieval-Augmented Generation (RAG). The retrieved translations are incorporated as constraints in the translation prompt. Finally, an iterative self-checking mechanism refines the translation by repeatedly translating with the source sentence, current output, and constraints until all constraints are satisfied. The method was evaluated on FLORES-200 and WMT datasets using Llama and Qwen models.

## Key Results
- Achieved BLEU scores of 41.95 on Catalan-English and 42.12 on English-Catalan language pairs
- Outperformed standard zero-shot translation methods on low-resource language pairs
- Self-checking mechanism effectively reduced hallucinations, particularly for Qwen2-7B-Instruct

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM can accurately rank word importance in source sentences when guided by explicit scoring prompts.
- Mechanism: The model processes the full sentence and outputs a confidence score for each word, reflecting its semantic weight. Higher-scoring words are deemed more critical for translation fidelity.
- Core assumption: The LLM's internal representation of semantic importance correlates with human judgment of keyword salience.
- Evidence anchors:
  - [abstract] "Our method first identifies these keywords and retrieves their translations from a bilingual dictionary, integrating them into the LLM's context using Retrieval-Augmented Generation (RAG)."
  - [section 3.1] "We hypothesize that certain words, due to their semantic weight or specificity, play a more crucial role in preserving the intended meaning of the sentence."
  - [corpus] Weak evidence; neighboring work uses rule-based extraction or no extraction at all, so LLM-based scoring is novel here.
- Break condition: If the model consistently assigns low scores to domain-specific terms or high scores to common function words, the ranking will mislead constraint selection.

### Mechanism 2
- Claim: Retrieval-Augmented Generation with bilingual dictionaries provides accurate translations for low-resource keywords.
- Mechanism: Extracted keywords are embedded and nearest-neighbor search retrieves their dictionary translations, which are then inserted as constraints in the prompt.
- Core assumption: The bilingual dictionary entries are correct and comprehensive, and embedding similarity reflects translation accuracy.
- Evidence anchors:
  - [abstract] "Our method first identifies these keywords and retrieves their translations from a bilingual dictionary using Retrieval-Augmented Generation (RAG)."
  - [section 3.2] "The translation ti of the keyword wi is obtained by searching the nearest neighbor in the bilingual dictionary's vector space."
  - [corpus] No direct corpus evidence; related work focuses on general RAG for knowledge, not dictionary-based MT.
- Break condition: If the dictionary lacks coverage for certain low-resource terms, nearest-neighbor retrieval will yield incorrect or irrelevant translations.

### Mechanism 3
- Claim: Iterative self-checking with lexical constraints reduces hallucinations and enforces translation fidelity.
- Mechanism: The LLM generates an initial translation, then re-runs translation with both source and current output as context plus constraints, refining until constraints are satisfied.
- Core assumption: The LLM can self-evaluate and correct deviations from constraints without external supervision.
- Evidence anchors:
  - [abstract] "We further mitigate potential output hallucinations caused by long prompts through an iterative self-checking mechanism, where the LLM refines its translations based on lexical and semantic constraints."
  - [section 3.3] "we implement an iterative prompting strategy... the translation process is repeated until all constraints are fully satisfied."
  - [corpus] Weak; neighboring work uses self-consistency for reasoning, not translation constraint enforcement.
- Break condition: If the model's self-evaluation is overconfident or ignores constraints, hallucinations will persist despite iteration.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: To integrate external dictionary knowledge into LLM context for accurate rare-word translation.
  - Quick check question: How does RAG differ from simple prompt concatenation when using a bilingual dictionary?

- Concept: Prompt engineering for keyword extraction
  - Why needed here: To guide the LLM to identify semantically important words without manual annotation.
  - Quick check question: What signal in the prompt ensures the model outputs a confidence score per word rather than just a list?

- Concept: Self-consistency and iterative refinement
  - Why needed here: To let the model detect and correct hallucinations without external human feedback.
  - Quick check question: Why might a single translation pass be insufficient for constraint satisfaction in low-resource contexts?

## Architecture Onboarding

- Component map:
  - Input sentence → Keyword extraction prompt → Priority scores → Top-k keyword selection
  - Keywords → Embedding → Dictionary vector search → Translation pairs (wi, ti)
  - Prompt assembly: source sentence + (wi, ti) constraints → LLM → initial translation
  - Self-checking loop: (source, constraints, current translation) → LLM → refined translation → repeat until constraints met
  - Final selection: compare initial vs refined output → choose best

- Critical path: Keyword extraction → RAG retrieval → Constrained translation → Self-checking → Output

- Design tradeoffs:
  - More keywords → higher constraint coverage but longer prompts, risking model confusion
  - More self-checking iterations → better constraint adherence but higher latency
  - Dictionary quality vs. coverage trade-off for low-resource languages

- Failure signatures:
  - Translation ignores constraints → self-checking ineffective
  - Keywords miss domain terms → RAG retrieval fails
  - High repetition in output → self-checking loop stuck

- First 3 experiments:
  1. Baseline zero-shot translation vs. keyword-constrained translation (single pass) on a small FLORES subset.
  2. Vary k (number of keywords) to find optimal constraint coverage without prompt overload.
  3. Compare self-checking iterations (0, 1, 2) on BLEU gain vs. latency trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed approach perform on domain-specific terminology beyond general low-resource language pairs, such as medical or legal translations?
- Basis in paper: [inferred] The paper discusses challenges with rare and specialized terminology but only evaluates on general language pairs in FLORES-200 and WMT datasets.
- Why unresolved: The experiments focus on general translation tasks without exploring domain-specific contexts where specialized terminology is critical.
- What evidence would resolve it: Testing the approach on domain-specific datasets like medical or legal corpora and comparing performance metrics with baseline models.

### Open Question 2
- Question: What is the optimal balance between the number of keywords extracted and translation quality, and how does this vary across different language pairs?
- Basis in paper: [inferred] The paper mentions an adaptive threshold for keyword selection but does not systematically analyze how varying the number of keywords affects translation quality across different languages.
- Why unresolved: The study uses a fixed adaptive approach without exploring the sensitivity of translation quality to different keyword counts.
- What evidence would resolve it: Conducting controlled experiments with varying keyword counts for each language pair and analyzing the corresponding translation quality metrics.

### Open Question 3
- Question: How does the self-checking mechanism scale with longer documents or larger contexts, and what are its computational implications?
- Basis in paper: [inferred] The paper describes the self-checking mechanism but does not evaluate its performance on longer documents or analyze computational costs.
- Why unresolved: The experiments are limited to sentence-level translation, leaving uncertainty about performance in extended contexts.
- What evidence would resolve it: Testing the approach on document-level translation tasks and measuring both translation quality and computational efficiency metrics.

## Limitations

- The keyword extraction mechanism's alignment with human semantic salience judgments is not validated.
- The RAG-based dictionary retrieval assumes perfect dictionary coverage, with no analysis of coverage gaps.
- The iterative self-checking mechanism's convergence criteria are underspecified, raising concerns about potential infinite loops.

## Confidence

- High confidence: The overall framework design and experimental results showing BLEU improvements are well-supported by the data.
- Medium confidence: The effectiveness of the iterative self-checking mechanism, as implementation details and convergence criteria are underspecified.
- Low confidence: The generalizability of keyword extraction to truly low-resource languages and the robustness of RAG retrieval when dictionary coverage is incomplete.

## Next Checks

1. Conduct human evaluation studies comparing LLM-generated keyword importance scores against human-annotated semantic salience rankings across multiple language pairs.
2. Perform ablation studies systematically removing dictionary entries to quantify the impact of coverage gaps on translation quality.
3. Test the iterative self-checking mechanism on a broader range of languages including non-Indo-European low-resource languages to assess cross-linguistic generalization.