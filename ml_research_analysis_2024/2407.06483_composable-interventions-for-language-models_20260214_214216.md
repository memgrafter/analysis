---
ver: rpa2
title: Composable Interventions for Language Models
arxiv_id: '2407.06483'
source_url: https://arxiv.org/abs/2407.06483
tags:
- editing
- wanda
- unlearning
- memit
- interventions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces composable interventions, a framework to\
  \ study how different test-time updates for language models interact when applied\
  \ sequentially. The authors formalize two composability metrics\u2014Order-free\
  \ Error and Order Sensitivity\u2014to measure whether applying one intervention\
  \ harms another, regardless of or depending on the order of application."
---

# Composable Interventions for Language Models

## Quick Facts
- arXiv ID: 2407.06483
- Source URL: https://arxiv.org/abs/2407.06483
- Reference count: 40
- This work introduces composable interventions, a framework to study how different test-time updates for language models interact when applied sequentially.

## Executive Summary
This paper addresses a critical gap in language model adaptation by studying how different test-time interventions (knowledge editing, model compression, and machine unlearning) interact when applied sequentially. The authors formalize composability through two metrics—Order-free Error and Order Sensitivity—to quantify whether interventions interfere with each other's effectiveness, regardless of or depending on application order. Experiments with 310 intervention combinations on Llama3-8B reveal that compression consistently degrades editing and unlearning performance, intervention order dramatically affects outcomes, and traditional utility metrics like MMLU poorly capture composability. The work establishes composability as a crucial property for future intervention development and provides an open-source framework for evaluating and improving multi-objective model updates.

## Method Summary
The authors develop a framework to study composability of language model interventions by applying knowledge editing, model compression, and machine unlearning methods sequentially in both possible orders. They evaluate 310 combinations of these interventions on Llama3-8B using zsRE for knowledge editing (50 edits per batch, 10 batches), WMDP for unlearning (3,260 questions across cyber and bio splits), and various compression levels measured by sparsity ratio and bits. The study introduces two composability metrics: Order-free Error, which measures the worst-case degradation when interventions are composed in either order, and Order Sensitivity, which quantifies how much intervention order affects the combined outcome. The framework captures both intervention-specific metrics (edit success, edit generalization, unlearning accuracy) and general utility metrics (MMLU) to provide a comprehensive evaluation of composability across different objectives.

## Key Results
- Compression methods (SparseGPT, Wanda, GPTQ, AWQ) consistently degrade knowledge editing and unlearning performance, even at low compression levels
- Intervention order matters dramatically—applying compression before editing or unlearning often yields different results than the reverse order
- Traditional general utility metrics like MMLU poorly predict composability outcomes, suggesting they don't capture the nuanced interactions between interventions
- Existing interventions lack composability, indicating a need for new methods explicitly designed to compose well across multiple intervention types

## Why This Works (Mechanism)
The paper doesn't deeply explore underlying mechanisms but identifies that compression interferes with the effectiveness of other interventions, likely through changes to model parameter space that make targeted updates harder to achieve.

## Foundational Learning
- Composability metrics (Order-free Error, Order Sensitivity): Quantitative measures to evaluate whether interventions interfere when composed, needed to systematically study intervention interactions; quick check: verify metrics produce consistent results across repeated experiments
- Sequential intervention composition: Applying multiple model updates in sequence to study their interactions, needed to understand real-world deployment scenarios; quick check: confirm baseline metrics are measured before composition
- Multi-objective evaluation: Using both task-specific (editing, unlearning) and general (MMLU) metrics, needed to capture the full impact of composability; quick check: ensure all relevant metrics are computed for each intervention combination

## Architecture Onboarding
Component map: Knowledge Editing -> Compression -> Unlearning (with all possible orderings)
Critical path: Apply intervention A → Apply intervention B → Measure both interventions' effectiveness
Design tradeoffs: Balancing intervention effectiveness against composability vs. optimizing for single-intervention performance
Failure signatures: Compression degrading other interventions, order-dependent performance differences
First experiments: 1) Run knowledge editing baseline with Fine-tuning, MEMIT, LoRA on zsRE 2) Compose each editing method with each compression method at various levels 3) Measure WMDP unlearning accuracy after compression and editing

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do composability metrics generalize across different language model architectures beyond Llama3-8B?
- Basis in paper: [explicit] The authors note that their experiments are limited to Llama3-8B and that it is unknown whether findings generalize between language models.
- Why unresolved: This requires extensive experimentation across multiple model architectures (e.g., different sizes, transformer variants, or foundation models) which would be computationally expensive and methodologically complex.
- What evidence would resolve it: Systematic experiments applying the same composability framework to models like GPT, Claude, or smaller/faster variants of Llama, comparing composability patterns across architectures.

### Open Question 2
- Question: What is the underlying mechanism by which model compression interferes with knowledge editing and unlearning effectiveness?
- Basis in paper: [explicit] The authors observe that compression consistently hinders other interventions but do not explain the underlying mechanism.
- Why unresolved: This requires deeper analysis of how compression affects knowledge representation, parameter sensitivity, or gradient flow in ways that make targeted updates harder.
- What evidence would resolve it: Detailed studies using probing techniques, attention analysis, or ablation studies to identify specific structural changes in compressed models that correlate with reduced editability or unlearning performance.

### Open Question 3
- Question: Can new interventions be designed that are inherently composable across multiple intervention types?
- Basis in paper: [explicit] The authors conclude that there is a clear need to develop novel interventions that target composability as a crucial property.
- Why unresolved: This is a forward-looking research direction requiring new method development, evaluation, and validation that the paper does not attempt.
- What evidence would resolve it: New intervention methods that demonstrate high composability scores across multiple intervention categories and orders, validated through extensive empirical testing similar to the current framework.

## Limitations
- Experiments limited to Llama3-8B, which may not capture the full spectrum of model architectures and sizes
- Knowledge editing dataset (zsRE) focuses on factual triples, potentially limiting conclusions about other types of knowledge modifications
- Study doesn't account for potential interactions between interventions beyond sequential pairwise combinations

## Confidence
- High confidence: The formal definitions of Order-free Error and Order Sensitivity metrics, the general finding that compression often degrades other intervention performances, and the observation that intervention order matters
- Medium confidence: The specific quantitative results (e.g., exact degradation percentages), as these may vary with different model versions, datasets, or hyperparameter settings
- Medium confidence: The conclusion that general utility metrics like MMLU poorly capture composability, as this depends on the specific choice of general task

## Next Checks
1. **Dataset diversity validation**: Replicate key composability experiments using alternative knowledge editing datasets (e.g., Natural Questions) and unlearning datasets covering different sensitive topics to assess generalizability
2. **Model architecture validation**: Test the same intervention combinations on both larger (Llama3-70B) and smaller (Llama3-7B) variants to determine if composability patterns hold across model scales
3. **Intervention method expansion**: Include additional compression methods (e.g., Low-Rank Adaptation for compression) and knowledge editing approaches to verify whether the observed compression degradation pattern is consistent across broader method families