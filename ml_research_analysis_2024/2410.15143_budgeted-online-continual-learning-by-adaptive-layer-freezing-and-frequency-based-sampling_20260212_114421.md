---
ver: rpa2
title: Budgeted Online Continual Learning by Adaptive Layer Freezing and Frequency-based
  Sampling
arxiv_id: '2410.15143'
source_url: https://arxiv.org/abs/2410.15143
tags:
- memory
- freezing
- sample
- training
- setup
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new framework for fairly comparing online
  continual learning (CL) algorithms by considering both computational and memory
  budgets together. The authors argue that current evaluations ignore additional storage
  costs and varying computational demands, leading to unfair comparisons.
---

# Budgeted Online Continual Learning by Adaptive Layer Freezing and Frequency-based Sampling

## Quick Facts
- arXiv ID: 2410.15143
- Source URL: https://arxiv.org/abs/2410.15143
- Reference count: 40
- This paper proposes a new framework for fairly comparing online continual learning (CL) algorithms by considering both computational and memory budgets together.

## Executive Summary
This paper addresses the fundamental problem of fair evaluation in online continual learning by proposing a framework that considers both computational and memory budgets simultaneously. The authors argue that current CL evaluations ignore additional storage costs and varying computational demands, leading to unfair comparisons. They introduce adaptive layer freezing that selectively freezes layers per batch based on maximizing Fisher Information gained per computation, reducing FLOPs with minimal accuracy loss. They also propose similarity-aware retrieval that prioritizes samples with low effective use-frequency, accelerating learning without additional inference costs. Empirical results on CIFAR-10/100, CLEAR-10/100, and ImageNet-1K show their method outperforms state-of-the-art CL algorithms under fixed computational and memory budgets.

## Method Summary
The authors propose a budgeted continual learning framework with two key components: adaptive layer freezing and similarity-aware retrieval. The adaptive layer freezing uses Fisher Information gain per computation to selectively freeze layers that contribute little information relative to their computational cost. The similarity-aware retrieval prioritizes samples the model has learned least about by combining actual use-frequency with class similarity through an effective use-frequency metric. These methods work together to optimize both computational and memory efficiency while maintaining high accuracy in online continual learning scenarios.

## Key Results
- Outperforms state-of-the-art CL algorithms under fixed computational and memory budgets on CIFAR-10/100, CLEAR-10/100, and ImageNet-1K
- Reduces training FLOPs by ~12% when extended to multi-modal LLMs while maintaining performance
- Achieves fair comparison by using training FLOPs and total memory in bytes as metrics across different CL algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive layer freezing based on Fisher Information maximizes learning efficiency per computation.
- Mechanism: The method freezes layers that contribute little information relative to their computational cost by calculating the ratio of Fisher Information gain to computational cost for each layer per batch.
- Core assumption: Fisher Information accurately captures the information gained from a batch by each layer.
- Evidence anchors:
  - [abstract]: "we propose adaptive layer freezing that does not update the layers for less informative batches to reduce computational costs with a negligible loss of accuracy."
  - [section 3.1]: "For a computationally efficient freezing criterion in online CL, we propose to freeze layers that learn little information per computational cost by measuring 'information' (I) gained by each layer during training."
- Break condition: If the Fisher Information estimate becomes unreliable (e.g., due to distribution shift), the freezing decisions may become suboptimal.

### Mechanism 2
- Claim: Similarity-aware retrieval accelerates learning by prioritizing samples the model has learned least about.
- Mechanism: The method retrieves samples based on an "effective use-frequency" that combines actual use-frequency with class similarity, allowing the model to learn from under-trained samples without additional inference cost.
- Core assumption: Samples with low effective use-frequency contain knowledge the model hasn't learned yet, and gradient similarity between classes is a good proxy for sample similarity.
- Evidence anchors:
  - [abstract]: "we propose a memory retrieval method that allows the model to learn the same amount of knowledge as using random retrieval in fewer iterations."
  - [section 3.2]: "we propose to sample data with low use-frequency for training with high probability."
- Break condition: If the class similarity estimation becomes inaccurate, the retrieval may prioritize samples that don't provide new knowledge.

### Mechanism 3
- Claim: Using training FLOPs and total memory in bytes as metrics ensures fair comparison across CL algorithms.
- Mechanism: The method normalizes comparisons by accounting for all computational costs (including forward/backward passes) and all memory usage (including model parameters, logits, and replay memory).
- Core assumption: Different CL algorithms have varying computational and memory requirements that need to be normalized for fair comparison.
- Evidence anchors:
  - [abstract]: "we propose to use floating point operations (FLOPs) and total memory size in Byte as a metric for computational and memory budgets, respectively, to compare and develop CL algorithms in the same 'total resource budget.'"
  - [section 1]: "For a fair comparison in computational budget, we use training FLOPs per sample instead of the number of epochs."
- Break condition: If the FLOP calculation doesn't account for all computational costs (e.g., data augmentation), comparisons may be unfair.

## Foundational Learning

- Concept: Fisher Information as a measure of information gained from data
  - Why needed here: Used to determine which layers to freeze by quantifying information gained per computation
  - Quick check question: How does Fisher Information relate to the gradient of the log-likelihood?

- Concept: Exponential moving average (EMA) for online estimation
  - Why needed here: Used to estimate class similarities and Fisher Information traces without storing all historical data
  - Quick check question: What is the update rule for EMA and how does the decay factor affect the estimate?

- Concept: Gradient similarity as a proxy for sample similarity
  - Why needed here: Used to approximate effective use-frequency by considering how training on similar samples affects knowledge about a particular sample
  - Quick check question: Why might gradient similarity be a reasonable proxy for sample similarity in a classification task?

## Architecture Onboarding

- Component map:
  - Similarity-aware retrieval: Calculates effective use-frequency and class similarities to prioritize under-trained samples
  - Adaptive layer freezing: Computes Fisher Information per layer and freezes layers with low information-to-computation ratios
  - Memory management: Maintains replay memory and tracks use frequencies of stored samples
  - Model training: Standard forward/backward passes with selective layer updates based on freezing decisions

- Critical path:
  1. Update replay memory with new samples
  2. Calculate effective use-frequencies and sample retrieval probabilities
  3. Retrieve training batch based on probabilities
  4. Compute Fisher Information for each layer on the batch
  5. Determine which layers to freeze based on information-to-computation ratio
  6. Perform forward pass and compute loss
  7. Perform backward pass only for unfrozen layers
  8. Update model parameters for unfrozen layers

- Design tradeoffs:
  - Freezing layers reduces computation but may lose information from the current batch
  - Prioritizing under-trained samples accelerates learning but requires accurate similarity estimation
  - Using EMA for estimation reduces memory usage but introduces estimation error
  - Tracking use frequencies requires additional memory but enables more efficient retrieval

- Failure signatures:
  - Excessive freezing leading to poor performance on new data
  - Inaccurate similarity estimation causing suboptimal sample retrieval
  - Memory budget constraints preventing sufficient sample storage
  - Computational overhead from similarity and Fisher Information calculations

- First 3 experiments:
  1. Compare accuracy and FLOPs of adaptive freezing vs constant freezing on CIFAR-10 with varying computational budgets
  2. Evaluate retrieval performance by measuring knowledge gain per iteration for similarity-aware vs random retrieval
  3. Test robustness by measuring performance degradation when similarity estimation is perturbed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the adaptive layer freezing criterion perform on recurrent neural networks (RNNs) where layer gradients affect both preceding and subsequent layers?
- Basis in paper: [explicit] The authors state "our proposed adaptive layer freezing method cannot apply to recurrent neural networks since the gradient of a layer affects not only the preceding layers but also the subsequent layers"
- Why unresolved: The paper only evaluates on feedforward networks (CNNs and Vision Transformers) and acknowledges the limitation without exploring potential modifications for RNNs.
- What evidence would resolve it: Testing the proposed Fisher Information-based freezing criterion on RNN architectures (LSTM, GRU) and comparing performance metrics (AAUC, Alast) against the baseline and current methods.

### Open Question 2
- Question: What is the optimal trade-off between the frequency of Fisher Information trace updates and the computational budget in the adaptive layer freezing method?
- Basis in paper: [inferred] The authors update the Fisher Information trace every 4 batches and mention this is computationally expensive, suggesting a trade-off exists
- Why unresolved: The paper uses a fixed update frequency without exploring how different frequencies affect the freezing decisions and overall performance
- What evidence would resolve it: Ablation studies varying the Fisher Information update frequency (e.g., every 2, 4, 8, 16 batches) while measuring AAUC, Alast, and total FLOPs across multiple datasets.

### Open Question 3
- Question: How does the similarity-aware retrieval method perform when the episodic memory budget is extremely constrained (e.g., < 1% of total samples)?
- Basis in paper: [inferred] The authors demonstrate effectiveness across various memory budgets but do not explore extreme scarcity scenarios
- Why unresolved: The paper shows robustness across reasonable memory budgets but doesn't test the theoretical limits where the retrieval strategy might break down
- What evidence would resolve it: Experiments with increasingly small episodic memory sizes (e.g., 0.1%, 0.5%, 1% of total samples) measuring performance degradation and comparing against simple random retrieval baselines.

## Limitations

- The method requires additional computational overhead for similarity calculations, though it achieves ~12% reduction in total training FLOPs for multi-modal LLMs
- Fisher Information-based freezing assumes accurate estimation from mini-batches, which may not hold under severe distribution shifts
- The method cannot be directly applied to recurrent neural networks due to gradient dependencies between layers

## Confidence

- High confidence: The core methodology of using FLOPs and memory as comparison metrics is well-justified and empirically validated
- Medium confidence: The effectiveness of adaptive layer freezing depends on accurate Fisher Information estimation, which may be sensitive to batch size and data distribution
- Medium confidence: Similarity-aware retrieval's benefit assumes gradient similarity is a good proxy for sample similarity, which may not generalize to all task types

## Next Checks

1. Test adaptive layer freezing performance under extreme distribution shifts to validate Fisher Information estimation robustness
2. Evaluate the sensitivity of similarity-aware retrieval to different gradient similarity metrics (e.g., cosine similarity vs Euclidean distance)
3. Benchmark the method on datasets with higher class imbalance to verify effective use-frequency estimation remains accurate