---
ver: rpa2
title: Compression Represents Intelligence Linearly
arxiv_id: '2404.09937'
source_url: https://arxiv.org/abs/2404.09937
tags:
- compression
- data
- intelligence
- language
- correlation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work empirically examines the relationship between compression
  efficiency and intelligence in large language models (LLMs). It measures intelligence
  using average benchmark scores across three domains: knowledge and commonsense,
  coding, and mathematical reasoning.'
---

# Compression Represents Intelligence Linearly

## Quick Facts
- arXiv ID: 2404.09937
- Source URL: https://arxiv.org/abs/2404.09937
- Authors: Yuzhen Huang; Jinghan Zhang; Zifei Shan; Junxian He
- Reference count: 40
- One-line primary result: Strong linear correlation (ρ ≈ -0.95) between LLM compression efficiency and benchmark intelligence scores

## Executive Summary
This work empirically demonstrates that compression efficiency in large language models linearly correlates with intelligence as measured by benchmark performance. Across 31 diverse LLMs, the study finds that bits per character (BPC) serves as a reliable unsupervised metric for evaluating model capabilities, with a Pearson correlation coefficient of approximately -0.95 when compared to average benchmark scores. The findings provide empirical support for the belief that superior compression indicates greater intelligence and suggest that compression efficiency can serve as a stable proxy for general capability.

## Method Summary
The study evaluates 31 diverse LLMs using bits per character (BPC) as the compression efficiency metric on three external text corpora: Common Crawl, GitHub Python repositories, and ArXiv math papers. Intelligence is measured through average benchmark scores across three domains: knowledge and commonsense, coding, and mathematical reasoning, using 12 benchmark datasets. Models are evaluated using a unified 1900-token context window, and Pearson correlation coefficients are computed between average BPC scores and average benchmark performance. The analysis is extended to individual benchmarks to assess consistency across different evaluation tasks.

## Key Results
- Strong linear correlation (ρ ≈ -0.95) between average benchmark scores and BPC across 31 diverse LLMs
- Individual benchmarks show high linear correlation (ρ between -0.88 and -0.95) with BPC
- Root mean square error around 2 absolute points for the linear fit
- Compression efficiency serves as a reliable unsupervised metric for evaluating LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compression efficiency measured as bits per character (BPC) linearly correlates with downstream benchmark performance.
- Mechanism: Lower BPC means the model assigns higher probability to test corpus tokens, reflecting better internal representation learning; better representation → better generalization → higher benchmark scores.
- Core assumption: Models being compared share no major data leakage and are evaluated on held-out text.
- Evidence anchors:
  - [abstract] "Across 31 diverse LLMs, results show a strong linear correlation (Pearson ρ ≈ -0.95) between average benchmark scores and BPC."
  - [section 3.2] "we utilize the average bits per character (BPC) as the evaluation metric for compression efficiency."
  - [corpus] No corpus-level counter-evidence; sampling variation shows small std dev (~0.002) indicating stable BPC estimates.
- Break condition: If models are trained on data overlapping the compression corpus, BPC will inflate and break the correlation.

### Mechanism 2
- Claim: Linear correlation holds across diverse architectures, tokenizers, and training data distributions.
- Mechanism: Because compression task (modeling next-token probability) is fundamental to language modeling, differences in architecture or tokenizer affect both compression and downstream task performance proportionally, preserving linear relationship.
- Core assumption: Test corpora are sufficiently large and representative to expose differences in modeling quality.
- Evidence anchors:
  - [abstract] "our study brings together 31 public LLMs that originate from diverse organizations."
  - [section 4.1] "Despite having significantly different architectures, both Mixtral-8x7B and Jamba-52B fit well into this linear correlation."
  - [corpus] Corpus diversity (CC, GitHub, ArXiv) and sampling stability suggest robustness.
- Break condition: If a model overfits to a specific benchmark or corpus, its BPC on held-out text will not reflect its inflated benchmark score.

### Mechanism 3
- Claim: Compression efficiency serves as an unsupervised metric that avoids benchmark contamination.
- Mechanism: Since BPC is computed on raw text without task-specific labels, it can be updated with new corpora to stay ahead of model training data, making it a stable proxy for general capability.
- Core assumption: Compression corpus is kept disjoint from model pretraining data.
- Evidence anchors:
  - [abstract] "compression efficiency, as an unsupervised metric derived from raw text corpora, serves as a reliable evaluation measure."
  - [section 3.2] "we opt to use the newest corpora as a measure."
  - [corpus] Latest CommonCrawl, GitHub, and ArXiv dumps are used; no overlap with pretraining reported.
- Break condition: If corpus becomes stale or overlaps with pretraining data, metric loses validity.

## Foundational Learning

- Concept: Information theory basics (entropy, cross-entropy, optimal coding length)
  - Why needed here: Compression efficiency is quantified via cross-entropy/BPC; understanding the math is essential to interpret results.
  - Quick check question: If a model assigns probability 0.01 to a token, what is its contribution (in bits) to the total code length?
- Concept: Language modeling as next-token prediction
  - Why needed here: The link between predictive modeling and compression underpins the whole paper.
  - Quick check question: How does arithmetic coding use a predictive model to compress text?
- Concept: Correlation vs causation
  - Why needed here: The paper shows correlation; engineers must not overclaim that better compression causes higher intelligence.
  - Quick check question: What could be a hidden confounder between BPC and benchmark scores?

## Architecture Onboarding

- Component map: Data collection -> BPC computation -> Benchmark evaluation -> Correlation analysis
- Critical path: Model -> Tokenizer -> BPC on held-out corpus -> Benchmark score -> Correlation fit
- Design tradeoffs: Context window size unification (1900 tokens) balances compression accuracy vs evaluation fairness; longer windows could inflate BPC artificially.
- Failure signatures: Sudden BPC jumps when context window changes; benchmark overfitting indicated by abnormally low Min-K% scores.
- First 3 experiments:
  1. Reproduce BPC calculation on a held-out CommonCrawl subset and verify low std dev across samples.
  2. Run a single benchmark (e.g., HellaSwag) on two models with known BPC gap; check if score gap matches prediction.
  3. Perform Min-K% score check on a benchmark test split to detect potential data leakage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we automatically identify the most appropriate compression corpus that correlates with specific benchmarks?
- Basis in paper: [explicit] The paper discusses the impact of different compression corpora on the linear relationship, finding that in-domain data yields stronger correlations than out-of-domain data.
- Why unresolved: While the paper suggests that alignment between compression corpus and evaluation benchmarks matters, it does not provide a systematic method for automatically selecting or creating such corpora.
- What evidence would resolve it: Development and validation of algorithms that can automatically select or construct compression corpora that maximize correlation with specific benchmark suites, tested across diverse model families.

### Open Question 2
- Question: What is the relationship between base model compression efficiency and fine-tuned model performance on downstream tasks?
- Basis in paper: [explicit] The paper focuses on base models because fine-tuned models are not general-purpose compressors, but acknowledges this as an important question for future work.
- Why unresolved: The paper explicitly states it leaves this question for future work, noting that fine-tuned models no longer model next token distribution for arbitrary text.
- What evidence would resolve it: Empirical studies comparing base model BPC with fine-tuned model benchmark scores across various alignment techniques and domains.

### Open Question 3
- Question: How does the linear correlation between compression and intelligence manifest in long-context scenarios?
- Basis in paper: [inferred] The paper focuses on short-to-medium context regimes and defers examination of long-context scenarios, suggesting this relationship may differ.
- Why unresolved: The paper acknowledges this limitation and states it leaves examination of long-context scenarios for future work.
- What evidence would resolve it: Experiments measuring BPC and intelligence across models with varying context window lengths, including tasks specifically designed for long-context evaluation.

## Limitations

- Data Contamination Risk: Exact composition and verification of training/test set separation remains unclear, creating uncertainty about potential overlap with pretraining data.
- Benchmark Protocol Variability: Evaluation protocols across 12 diverse benchmarks aren't standardized, potentially introducing systematic measurement errors.
- Model Version and Configuration Ambiguity: Heterogeneous model collection (base, instruction-tuned, code-specific) lacks clarity on standardization, potentially confounding the compression-intelligence relationship.

## Confidence

- High Confidence: Core finding of strong linear correlation (ρ ≈ -0.95) is supported by comprehensive data across 31 models and three independent validation checks.
- Medium Confidence: Claim that correlation holds across diverse architectures is supported by examples but lacks systematic architectural analysis.
- Low Confidence: Unsupervised nature of compression as future-proof metric assumes perfect data separation with minimal evidence for long-term stability.

## Next Checks

1. **Data Contamination Audit** - Implement systematic Min-K% analysis across all 12 benchmarks to quantify potential data leakage and correlate overlap scores with observed BPC-benchmark relationships.

2. **Architectural Stress Test** - Select three model pairs with similar sizes but different architectures and verify whether linear compression-intelligence relationship holds within each pair to test architecture invariance.

3. **Temporal Validation** - Using same 31 models, compute BPC on newer CommonCrawl snapshot and re-compute correlations to test whether compression efficiency remains predictive as models and web content evolve.