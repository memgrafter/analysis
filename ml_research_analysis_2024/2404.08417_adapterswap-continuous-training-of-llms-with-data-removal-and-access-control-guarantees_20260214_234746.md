---
ver: rpa2
title: 'AdapterSwap: Continuous Training of LLMs with Data Removal and Access-Control
  Guarantees'
arxiv_id: '2404.08417'
source_url: https://arxiv.org/abs/2404.08417
tags:
- data
- adapters
- adapter
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdapterSwap enables efficient continual learning of large language
  models (LLMs) by partitioning data into access-controlled shards, training LoRA
  adapters on each shard, and dynamically composing adapters at inference time via
  a GMM-based retriever. This approach allows selective knowledge access, guarantees
  efficient data removal (by retraining only affected adapters), and prevents catastrophic
  forgetting better than iterative fine-tuning or retraining from scratch.
---

# AdapterSwap: Continuous Training of LLMs with Data Removal and Access-Control Guarantees

## Quick Facts
- arXiv ID: 2404.08417
- Source URL: https://arxiv.org/abs/2404.08417
- Reference count: 19
- Primary result: AdapterSwap achieves perplexity as low as 1.4-5.4 while enabling efficient data removal and access control.

## Executive Summary
AdapterSwap is a continual learning framework for large language models that partitions data into access-controlled shards, trains LoRA adapters on each shard, and dynamically composes adapters at inference time using a GMM-based retriever. This approach enables selective knowledge access, guarantees efficient data removal (by retraining only affected adapters), and prevents catastrophic forgetting better than iterative fine-tuning or retraining from scratch. Experiments on multiple 7B models show AdapterSwap achieves strong perplexity while enforcing access control and reducing retraining costs by up to 80x when purging data.

## Method Summary
AdapterSwap segments data into access-control categories and shards, trains a separate LoRA adapter on each shard, and uses a GMM over SBERT embeddings to retrieve relevant adapters at inference. The retrieved adapters are filtered by access-control permissions and dynamically composed with the base model. When data must be removed, only the adapter trained on that data needs to be retrained, avoiding full model fine-tuning. This enables both efficient data deletion and fine-grained access control while preventing catastrophic forgetting through static adapter storage.

## Key Results
- AdapterSwap achieves perplexity as low as 1.4-5.4 when retrieving the correct adapter, compared to 12.4-14.0 for iterative fine-tuning and full retraining in continual learning.
- The approach enforces access-control by blocking restricted adapters, preventing unauthorized data access.
- Data removal efficiency improves by up to 80x since only the affected adapter needs retraining rather than the full model.

## Why This Works (Mechanism)

### Mechanism 1
Data partitioning into adapter-specific shards allows fine-grained access control by training separate LoRA adapters for each access-control category. The core assumption is that each document can be clearly assigned to a single access-control group. Break condition: If documents require multi-category permissions, single-adapter assignment fails.

### Mechanism 2
LoRA adapters isolate update parameters so retraining after data removal is efficient. Only the adapter trained on removed data needs retraining, avoiding full model fine-tuning. Core assumption: Adapters contain no knowledge from other shards. Break condition: If adapters share parameters or leak knowledge between shards, purging one shard could corrupt others.

### Mechanism 3
Dynamic adapter composition via GMM-based retrieval preserves past knowledge while adding new data. A GMM over SBERT embeddings retrieves relevant adapters, allowing new adapters without retraining old ones. Core assumption: GMM can accurately rank adapters by relevance. Break condition: If GMM ranking is inaccurate or adapter mixing causes interference, past knowledge is lost or degraded.

## Foundational Learning

- **Low-Rank Adaptation (LoRA)**: Enables efficient fine-tuning by learning only small low-rank matrices instead of full model weights. Quick check: What is the rank parameter in LoRA and how does it affect adapter size and performance?

- **Mixture-of-Experts and adapter composition**: Allows dynamic selection and combination of multiple adapters at inference. Quick check: How does AdapterSwap differ from traditional MoE in terms of adapter independence and access control?

- **Catastrophic forgetting in continual learning**: Explains why iterative fine-tuning degrades performance on old data. Quick check: What experimental evidence shows AdapterSwap prevents forgetting compared to iterative fine-tuning?

## Architecture Onboarding

- **Component map**: Base LLM -> LoRA adapters (one per shard) -> GMM retriever (SBERT + LDA) -> Access-control filter -> Inference mixer

- **Critical path**: 1) Partition data by access-control and shard size, 2) Train LoRA adapter per shard, 3) Fit GMM retriever on held-out embeddings, 4) At inference: embed query, retrieve adapters, filter by permissions, mix, apply to base model

- **Design tradeoffs**: Smaller shards → faster retraining, more adapters, higher retrieval overhead; Larger shards → fewer adapters, slower retraining, simpler retrieval; Top-1 retrieval → simpler, possibly less robust; Top-k retrieval → more robust, higher compute

- **Failure signatures**: Poor perplexity → retriever failing to rank correct adapter high; Unauthorized data leakage → access-control filter not applied or GMM returns restricted adapter; High retraining cost → shard too large or adapters not truly isolated

- **First 3 experiments**: 1) Train single adapter on small dataset, measure document completion perplexity; 2) Train two adapters on disjoint domains, use GMM to retrieve correct one for in-domain queries; 3) Simulate data removal by purging one adapter, retrain it, verify old data cannot be recalled while new data can

## Open Questions the Paper Calls Out

### Open Question 1
How does AdapterSwap's performance scale with significantly larger adapter partitions or more granular access-control categories? The paper notes smaller partitions improve perplexity but total GPU hours remain similar. This is unresolved because experiments focus on moderate numbers of domains. Resolution requires experiments varying adapter counts by orders of magnitude, measuring perplexity, retrieval accuracy, and total training time.

### Open Question 2
Can AdapterSwap's retrieval mechanism be improved beyond GMM and LDA for better adapter selection accuracy? The paper compares GMM with LDA against GMM with PCA and notes LDA significantly improved retrieval accuracy, but suggests future exploration of better retrieval and adapter mixing methods. This is unresolved because more sophisticated retrieval architectures are not tested. Resolution requires head-to-head comparisons using alternative models.

### Open Question 3
What is the impact of AdapterSwap on downstream task performance compared to full fine-tuning or other parameter-efficient methods? The paper evaluates AdapterSwap on document completion but not specific downstream NLP tasks. This is unresolved because perplexity on document completion doesn't directly measure task-specific capabilities. Resolution requires experiments on standard benchmarks like SQuAD, GLUE, and summarization datasets.

### Open Question 4
How does AdapterSwap handle highly overlapping or ambiguous data domains where a single query could be relevant to multiple adapters? The paper shows top-2 and top-3 adapter combinations work well for out-of-domain queries but doesn't analyze significant overlap cases. This is unresolved because experiments use distinct domains with high oracle adapter accuracy. Resolution requires controlled experiments with overlapping domains, measuring retrieval accuracy and adapter mixing strategies.

## Limitations
- Assumes clean, non-overlapping data partitioning for access control, but real-world permissions often involve multi-level or conditional rules
- LoRA adapter independence assumption not empirically validated; knowledge leakage possible through shared base model parameters
- GMM-based retrieval accuracy not deeply validated with standalone metrics beyond oracle adapter perplexity
- Access-control enforcement mechanism described conceptually but not experimentally tested for actual policy enforcement

## Confidence

**High confidence**: The core mechanism of training LoRA adapters per data shard and using dynamic composition at inference is clearly described and technically sound. The perplexity improvements over iterative fine-tuning and retraining are directly measured and reported.

**Medium confidence**: Claims about efficient data removal and 80× retraining speed-up depend on adapter isolation assumptions that are not empirically validated. Access-control guarantees are asserted but not experimentally tested for actual policy enforcement.

**Low confidence**: Catastrophic forgetting comparisons are limited in scope; the GMM retrieval accuracy is not independently reported; and the paper does not address failure modes like adapter interference or permission leakage.

## Next Checks

1. **Retrieval accuracy validation**: Measure top-1 and top-5 adapter retrieval accuracy on held-out queries, separate from perplexity, to quantify GMM performance independently of downstream task success.

2. **Access-control enforcement test**: Construct a controlled experiment where restricted adapters are queried by unauthorized users and verify that the system blocks access while still allowing authorized queries.

3. **Adapter isolation stress test**: Train adapters on overlapping or semantically similar shards, then attempt data removal to check if purging one adapter degrades performance on the other, indicating knowledge leakage.