---
ver: rpa2
title: 'Know Your Limits: A Survey of Abstention in Large Language Models'
arxiv_id: '2407.18418'
source_url: https://arxiv.org/abs/2407.18418
tags:
- abstention
- arxiv
- language
- llms
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically examines abstention in large language
  models (LLMs), defining it as the refusal to answer queries across three perspectives:
  query answerability, model confidence, and human values alignment. The authors categorize
  abstention methods by their application stage (pretraining, alignment, or inference)
  and survey relevant benchmarks and evaluation metrics.'
---

# Keep Your Limits: A Survey of Abstention in Large Language Models

## Quick Facts
- arXiv ID: 2407.18418
- Source URL: https://arxiv.org/abs/2407.18418
- Reference count: 40
- Primary result: Survey of abstention in LLMs across three perspectives: query answerability, model confidence, and human values alignment

## Executive Summary
This survey systematically examines abstention in large language models (LLMs), defining it as the refusal to answer queries across three perspectives: query answerability, model confidence, and human values alignment. The authors categorize abstention methods by their application stage (pretraining, alignment, or inference) and survey relevant benchmarks and evaluation metrics. Key findings include that while instruction tuning on abstention-aware data improves abstention ability, it can lead to over-abstention, and that model knowledge and human values perspectives remain underexplored in pretraining research. The survey identifies the need for more comprehensive benchmarks and evaluation methods that encompass all three perspectives of the abstention framework.

## Method Summary
The paper conducts a comprehensive survey of existing research on abstention in large language models, categorizing methods based on when they are applied in the LLM lifecycle (pretraining, alignment, and inference stages). It introduces a three-perspective framework for understanding abstention, analyzing query answerability, model confidence, and human values alignment. The survey examines evaluation benchmarks and metrics, including statistical automated metrics, model-based evaluation using LLMs as judges, and human-centric evaluation. The authors synthesize findings across these dimensions to identify research gaps and suggest future directions.

## Key Results
- Abstention methods are categorized by lifecycle stage, with pretraining methods lacking research on model knowledge and human values perspectives
- Instruction tuning on abstention-aware data improves abstention ability but can cause over-abstention
- Current evaluation benchmarks primarily focus on single perspectives rather than comprehensive system-wide assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Abstention is modeled as a function of three independent perspectives: query answerability, model confidence, and human values alignment.
- Mechanism: The refusal function r(x, y) is defined as the conjunction of three functions a(x), c(x, y), and h(x, y), where each represents a distinct perspective. The system abstains if any of these perspectives indicates full abstention.
- Core assumption: These three perspectives are independent and can be combined to make a unified abstention decision.
- Evidence anchors:
  - [abstract]: "We introduce a framework to examine abstention from three perspectives: the query, the model, and human values."
  - [section 2.2]: "The refusal function r determines whether the LLM should abstain from responding to input x as: r(x, y) = 1, if any of a(x); c(x, y); h(x, y) = 0"
  - [corpus]: Weak - no direct evidence in corpus neighbors about this specific three-perspective framework.

### Mechanism 2
- Claim: Abstention methods can be categorized by when they are applied in the LLM lifecycle: pretraining, alignment, or inference.
- Mechanism: Different abstention techniques are applied at different stages of LLM development. Pretraining methods focus on incorporating abstention-aware data during initial training, alignment methods use finetuning or preference optimization, and inference methods apply techniques during the generation phase.
- Core assumption: The effectiveness of abstention methods depends on when they are applied in the LLM lifecycle.
- Evidence anchors:
  - [section 3]: "We categorize existing methods to improve abstention in LLMs based on the model lifecycle (pretraining, alignment, and inference)"
  - [section 3.1]: "We found no existing research that studies abstention in the pretraining stage"
  - [corpus]: Weak - no direct evidence in corpus neighbors about this specific lifecycle categorization.

### Mechanism 3
- Claim: Evaluation of abstention requires a combination of statistical automated metrics, model-based evaluation, and human-centric evaluation.
- Mechanism: Statistical metrics like Abstention Accuracy and Abstention F1-score measure performance quantitatively, model-based evaluation uses LLMs as judges to assess abstention, and human-centric evaluation gathers user perceptions of abstention behaviors.
- Core assumption: A comprehensive evaluation of abstention requires multiple perspectives and methods.
- Evidence anchors:
  - [section 4.2]: "We survey metrics that have been developed and used to evaluate abstention"
  - [section 4.2]: "Model-based evaluation... uses GPT-4-level LLMs for off-the-shelf evaluation"
  - [section 4.2]: "Human-centric evaluation... focuses on understanding user perceptions of different abstention expressions"
  - [corpus]: Weak - no direct evidence in corpus neighbors about this specific multi-faceted evaluation approach.

## Foundational Learning

- Concept: Three-perspective framework for abstention
  - Why needed here: This framework provides a comprehensive way to understand and categorize abstention behaviors in LLMs, which is essential for developing effective abstention methods.
  - Quick check question: Can you explain how the query, model knowledge, and human values perspectives contribute to the abstention decision?

- Concept: LLM lifecycle stages
  - Why needed here: Understanding when different abstention methods are applied is crucial for selecting the appropriate techniques and for evaluating their effectiveness.
  - Quick check question: What are the key differences between abstention methods applied during pretraining, alignment, and inference stages?

- Concept: Multi-faceted evaluation of abstention
  - Why needed here: A comprehensive evaluation of abstention capabilities requires multiple approaches to capture different aspects of performance and user experience.
  - Quick check question: How do statistical automated metrics, model-based evaluation, and human-centric evaluation complement each other in assessing abstention?

## Architecture Onboarding

- Component map: Input processing -> Query analysis -> Model knowledge assessment -> Output processing -> Evaluation
- Critical path:
  1. Receive input query
  2. Analyze query for answerability and human values alignment
  3. Generate potential response
  4. Estimate model confidence and safety
  5. Combine perspectives to make abstention decision
  6. Output response or abstention
  7. Evaluate performance using multiple metrics

- Design tradeoffs:
  - Balancing abstention frequency with utility (avoiding over-abstention)
  - Computational cost of uncertainty estimation vs. accuracy
  - Generalizability of abstention methods across domains and languages
  - Trade-off between safety and helpfulness in human values alignment

- Failure signatures:
  - High abstention rate on answerable queries (over-abstention)
  - Low abstention rate on unanswerable or unsafe queries (under-abstention)
  - Inconsistent abstention decisions across similar queries
  - Poor calibration of confidence estimates

- First 3 experiments:
  1. Implement the three-perspective framework and test on a simple QA dataset with answerable and unanswerable questions.
  2. Compare different uncertainty estimation methods (token-likelihoods, semantic entropy, verbalized confidence) on a calibration benchmark.
  3. Evaluate the effectiveness of multi-LLM collaboration for safety-driven abstention using a toxic prompt dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can abstention be effectively learned as a meta-capability that transfers across domains and task types, or is it fundamentally task-specific?
- Basis in paper: [explicit] The paper states "Zhang et al. (2024a) also argue that refusal-aware answering is task-independent and could benefit from multi-task training and joint inference. However, Feng et al. (2024b) present contradictory findings in their Appendix that abstention-aware instruction-tuning struggles to generalize across domains and LLMs."
- Why unresolved: Existing research presents conflicting evidence, with some studies suggesting abstention can be learned as a general capability while others show poor cross-domain generalization. The paper identifies this as an area needing further investigation.
- What evidence would resolve it: A comprehensive study comparing multi-task training approaches with task-specific fine-tuning across diverse domains, measuring transfer performance and identifying key factors that enable or inhibit meta-capability learning.

### Open Question 2
- Question: How can we develop evaluation benchmarks that comprehensively assess all three perspectives of abstention (query answerability, model confidence, and human values alignment) in a unified framework?
- Basis in paper: [explicit] The paper notes "Existing benchmarks primarily focus on a single perspective. Researchers could benefit from more comprehensive benchmarks encompassing examples across the query, model, and human values perspectives, that are capable of system-wide assessment."
- Why unresolved: Current evaluation datasets focus on individual aspects of abstention rather than providing a holistic assessment. The paper identifies this gap but doesn't provide solutions for creating unified benchmarks.
- What evidence would resolve it: Development and validation of a benchmark dataset that includes carefully constructed examples spanning all three perspectives, along with experimental results showing how different abstention methods perform across this comprehensive evaluation.

### Open Question 3
- Question: What are the optimal strategies for balancing abstention with helpfulness to avoid over-abstention while maintaining safety and reliability?
- Basis in paper: [explicit] The paper discusses multiple instances of over-abstention, stating "Cheng et al. (2024) and Brahman et al. (2024) find that SFT can make models more conservative, leading to a higher number of incorrect refusals" and "Over-abstention occurs when models abstain unnecessarily."
- Why unresolved: While the paper identifies over-abstention as a significant problem, it doesn't provide clear solutions for achieving the optimal balance. The tension between being helpful and being safe remains unresolved.
- What evidence would resolve it: Empirical studies comparing different optimization strategies (such as preference optimization, multi-objective alignment, or personalized abstention) with quantitative measures of both helpfulness (coverage, accuracy) and safety (appropriate abstention rates, user trust metrics).

## Limitations
- Limited empirical validation of the three-perspective framework through new experiments
- Definition ambiguity in how the combination function M integrates the three perspectives
- Evaluation gaps in current benchmarks, particularly for human values alignment perspective

## Confidence
- High confidence: Comprehensive categorization of abstention methods by lifecycle stage
- Medium confidence: Theoretical three-perspective framework with unspecified implementation details
- Medium confidence: Evaluation methodology acknowledging current benchmark limitations

## Next Checks
1. Implement the three-perspective abstention framework on a controlled dataset with clearly answerable/unanswerable questions and safety concerns to validate the theoretical framework empirically.
2. Systematically compare different uncertainty estimation methods (token-likelihoods, semantic entropy, verbalized confidence) on the same calibration dataset to quantify their relative performance in abstention decisions.
3. Conduct human studies to validate whether the proposed three-perspective framework aligns with user expectations and perceptions of appropriate abstention behaviors across different domains and query types.