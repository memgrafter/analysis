---
ver: rpa2
title: Bayesian Statistical Modeling with Predictors from LLMs
arxiv_id: '2406.09012'
source_url: https://arxiv.org/abs/2406.09012
tags:
- strategywta
- choicerandom
- data
- llms
- predictions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether predictions from large language
  models (LLMs) capture human decision-making patterns in pragmatic language use tasks.
  The authors conduct a reference game experiment with human participants and compare
  their responses to LLM-derived predictions.
---

# Bayesian Statistical Modeling with Predictors from LLMs

## Quick Facts
- **arXiv ID**: 2406.09012
- **Source URL**: https://arxiv.org/abs/2406.09012
- **Reference count**: 13
- **Primary result**: Item-level LLM predictions do not match human variability in pragmatic language tasks; only certain aggregation methods (average-WTA) yield adequate fits to human data

## Executive Summary
This paper investigates whether predictions from large language models (LLMs) capture human decision-making patterns in pragmatic language use tasks. The authors conduct a reference game experiment with human participants and compare their responses to LLM-derived predictions. They find that item-level LLM predictions contain variance not reflected in human behavior, making direct item-level probabilistic models inadequate. Only certain aggregation methods for deriving condition-level predictions from LLMs yield adequate fits to human data, with the "average-WTA" predictor performing best but relying on empirically implausible item-level predictions. The study highlights the importance of rigorous statistical model checking for LLMs and suggests their explanatory power may be limited due to their item-level focus rather than capturing abstract condition-level patterns.

## Method Summary
The authors assess LLM predictions using Bayesian statistical modeling with posterior predictive checks. They compute item-level scores from LLM log-probabilities and derive three types of condition-level predictions: average-scores, average-probabilities, and average-WTA. These predictors are then used in Bayesian statistical models with parameterized softmax links and random error terms. The models are fit using Stan and evaluated against human data from a reference game experiment with 302 participants. The evaluation focuses on posterior predictive p-values and visual checks to assess whether LLM-derived predictors generate data distributions that match human choice patterns.

## Key Results
- Item-level LLM predictions show systematic variance exceeding human variability, making direct probabilistic models inadequate
- The average-WTA aggregation method yields the best condition-level model fit but relies on implausible item-level predictions
- Bayesian posterior predictive checks reveal inadequacies in LLM-derived predictors that accuracy-based benchmarks miss

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLM predictions at the item level contain variance that is not reflected in human behavior, making direct item-level probabilistic models inadequate.
- **Mechanism**: Autoregressive LLMs predict next-token probabilities for concrete input strings. These item-level scores, when transformed via softmax with moderate α, predict systematic variation in human choices across items. However, human data shows much less item-level variation, leading to a mismatch.
- **Core assumption**: The item-level prediction variance in LLMs is not noise but rather captures some systematic aspect of the input that humans do not respond to.
- **Evidence anchors**:
  - [abstract]: "LLMs do not capture the variance in the human data at the item-level."
  - [section]: "LLMs make predictions for each individual item... The atomic predictions accessible to the common user are specific to each individual string tested."
  - [corpus]: Weak evidence - corpus neighbors discuss signal-to-noise variation but not human-likeness at the item level.
- **Break condition**: If item-level LLM variance correlates strongly with human variability, the mismatch disappears and item-level models become adequate.

### Mechanism 2
- **Claim**: Aggregation strategies for deriving condition-level predictions from item-level LLM scores critically determine model adequacy.
- **Mechanism**: Different aggregation methods (average-scores, average-probabilities, average-WTA) treat item-level information differently. Average-WTA often yields better condition-level fit because it smooths out item-level noise by focusing on the highest-scoring option, which can align better with human aggregate behavior.
- **Core assumption**: Human aggregate behavior is more robust to item-level fluctuations than individual item predictions, so methods that aggregate at the item-level before softmax (like average-WTA) may better capture this.
- **Evidence anchors**:
  - [abstract]: "only certain aggregation methods... yield adequate fits to human data. Specifically, the 'average-WTA' predictor... performs best."
  - [section]: "There are many ways of averaging item-level information... Methods differ in what the underlying item-level measure for aggregation is."
  - [corpus]: Moderate evidence - neighbor paper "A Statistical Assessment of Amortized Inference Under Signal-to-Noise Variation" discusses aggregation under noise, aligning with this mechanism.
- **Break condition**: If human behavior at the condition level is highly sensitive to item-level variations (e.g., due to strong item-specific effects), average-WTA would fail to capture these nuances.

### Mechanism 3
- **Claim**: Bayesian statistical model checking reveals inadequacies in LLM-derived predictors that accuracy-based benchmarks miss.
- **Mechanism**: By building full probabilistic models around LLM predictors and applying posterior predictive checks, researchers can detect distributional mismatches between LLM predictions and human data. This approach is more stringent than accuracy-based testing, which only checks for correct answer rates.
- **Core assumption**: Human decision-making involves probabilistic choice distributions, not just correct/incorrect answers, and models must match this full distribution to be adequate.
- **Evidence anchors**:
  - [abstract]: "we here investigate the human-likeness of LLMs' predictions... from the perspective of Bayesian statistical modeling."
  - [section]: "we explore strategies of building a Bayesian statistical model around them, and to scrutinize these LLM-grounded Bayesian statistical models with the usual methods of Bayesian data analysis, in particular model criticism."
  - [corpus]: Weak evidence - corpus neighbors discuss Bayesian inference and model checking but not in the context of LLM human-likeness.
- **Break condition**: If accuracy-based benchmarks were replaced with distributional checks that still showed good fit, the added value of this mechanism would diminish.

## Foundational Learning

- **Concept**: Bayesian posterior predictive checks
  - **Why needed here**: To assess whether LLM-derived predictors generate data distributions that match human data, not just point predictions.
  - **Quick check question**: What does a low Bayesian posterior predictive p-value indicate about a model's fit to observed data?
- **Concept**: Softmax parameterization and temperature scaling
  - **Why needed here**: To transform raw LLM scores into probabilistic predictions and control the impact of score differences on choice probabilities.
  - **Quick check question**: How does increasing the softmax parameter α affect the probability distribution over choices?
- **Concept**: Hierarchical regression models and random effects
  - **Why needed here**: To understand how standard statistical models handle item-level variation, providing context for why LLM item-level focus is different.
  - **Quick check question**: In a hierarchical model, what is the role of random effects at the item level?

## Architecture Onboarding

- **Component map**: Human reference game data -> LLM log-probabilities -> Item-level scores -> Aggregation method -> Condition-level predictions -> Bayesian statistical model -> Posterior predictive checks
- **Critical path**: LLM scores → Aggregation method → Bayesian model → Posterior predictive check → Model adequacy assessment
- **Design tradeoffs**: 
  - Item-level vs. condition-level modeling: Item-level captures fine-grained variation but may overfit; condition-level smooths noise but may miss nuances.
  - Aggregation method choice: Average-WTA may fit better but relies on implausible item-level predictions; average-probabilities are more interpretable but may fail.
- **Failure signatures**: 
  - High Bayesian p-values for item-level models but low for condition-level suggest item-level variance is not human-like.
  - Low p-values for average-probabilities but high for average-WTA indicate that item-level softmax probabilities are mismatched.
- **First 3 experiments**:
  1. Run Bayesian model with GPT-3.5 scores using average-scores aggregation; check posterior predictive p-values for item-level and condition-level data.
  2. Repeat with average-probabilities aggregation; compare p-values and visual posterior predictive plots.
  3. Repeat with average-WTA aggregation; verify if it passes checks and inspect α parameter posteriors for evidence of WTA reliance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Under what specific conditions do LLM predictions align with human decision-making patterns at the item-level?
- **Basis in paper**: [inferred] The paper found that item-level LLM predictions do not match human variability, but suggests this may be task-dependent.
- **Why unresolved**: The study only examined one specific reference game task. It's unclear if this limitation generalizes to other cognitive tasks or if certain task characteristics make LLMs more or less likely to capture item-level human variability.
- **What evidence would resolve it**: Systematic testing of LLM predictions against human data across multiple cognitive tasks with varying complexity, structure, and decision-making demands.

### Open Question 2
- **Question**: What is the optimal method for aggregating item-level LLM predictions to condition-level predictions across different types of tasks?
- **Basis in paper**: [explicit] The paper found that the "average-WTA" method performed best for their reference game task, but noted that different LLMs may prefer different aggregation strategies.
- **Why unresolved**: The study only tested three aggregation methods on one task type. It's unclear if these methods generalize or if other aggregation strategies might be more effective for different task domains.
- **What evidence would resolve it**: Comparative evaluation of multiple aggregation methods across diverse task types, measuring prediction accuracy against human data and identifying task characteristics that predict optimal aggregation methods.

### Open Question 3
- **Question**: To what extent does prompt engineering affect the reliability and human-likeness of LLM predictions?
- **Basis in paper**: [explicit] The paper mentions known concerns about robustness of predictions under perturbations of input prompts, but did not systematically investigate prompt effects.
- **Why unresolved**: The study used a single prompt format for all items. It's unclear how variations in prompt structure, wording, or context might influence LLM predictions and their alignment with human behavior.
- **What evidence would resolve it**: Controlled experiments varying prompt characteristics while measuring prediction consistency and human-likeness across different tasks and LLM models.

## Limitations

- The comparison assumes human data represents ground truth of "human-likeness" without accounting for potential systematic noise or biases in human responses
- The best-performing aggregation method (average-WTA) relies on empirically implausible item-level predictions, creating potential circularity in evaluation
- The study only examines one type of pragmatic language task (reference games), limiting generalizability to other decision-making contexts

## Confidence

- **High confidence**: The methodology for Bayesian statistical model checking and posterior predictive assessment is sound and well-established in the statistics literature. The finding that item-level LLM variance exceeds human variability is clearly demonstrated through posterior predictive checks.
- **Medium confidence**: The specific aggregation methods and their relative performance are well-documented, but the explanation for why average-WTA succeeds (smoothing item-level noise) remains somewhat speculative. Alternative explanations, such as average-WTA coincidentally aligning with human decision strategies, are not fully ruled out.
- **Low confidence**: The broader claim that LLMs have "limited explanatory power" for human decision-making may overstate the findings, as the study only examines one narrow task domain and one specific modeling approach.

## Next Checks

1. **Cross-task validation**: Apply the same Bayesian statistical model checking framework to LLM predictions for different pragmatic language tasks (e.g., implicature comprehension, metaphor interpretation) to assess whether the item-level variance issue generalizes beyond reference games.

2. **Alternative aggregation methods**: Test additional aggregation strategies beyond the three examined (e.g., weighted averages based on item difficulty, median aggregation, or clustering-based approaches) to determine if better methods exist for bridging item-level and condition-level predictions.

3. **Human variability assessment**: Conduct a detailed analysis of human response patterns to determine if the "excess" item-level variance in LLM predictions might actually reflect real but unmeasured aspects of human decision-making, such as individual differences or contextual factors not captured in the experimental design.