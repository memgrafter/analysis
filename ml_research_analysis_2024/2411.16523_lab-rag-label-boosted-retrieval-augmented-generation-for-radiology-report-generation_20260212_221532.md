---
ver: rpa2
title: 'LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology Report
  Generation'
arxiv_id: '2411.16523'
source_url: https://arxiv.org/abs/2411.16523
tags:
- label
- labels
- lab-rag
- image
- report
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LaB-RAG, a label-boosted retrieval-augmented
  generation method for radiology report generation. The approach uses small classification
  models trained on image embeddings to predict categorical labels, which are then
  used to filter and format retrieved text examples for a large language model.
---

# LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology Report Generation

## Quick Facts
- arXiv ID: 2411.16523
- Source URL: https://arxiv.org/abs/2411.16523
- Authors: Steven Song; Anirudh Subramanyam; Irene Madejski; Robert L. Grossman
- Reference count: 40
- Primary result: LaB-RAG achieves state-of-the-art F1-CheXbert scores for "Findings" generation and competitive results for "Impression" generation compared to both retrieval-based and fine-tuned vision-language models.

## Executive Summary
This paper introduces LaB-RAG, a novel approach for radiology report generation that combines label-based retrieval filtering with retrieval-augmented generation. The method uses small classification models trained on image embeddings to predict categorical labels, which are then used to filter and format retrieved text examples for a large language model. This allows generating radiology reports without fine-tuning the generative model or directly showing it images. The approach achieves state-of-the-art performance on MIMIC-CXR and CheXpert Plus datasets, demonstrating that simple classification models combined with zero-shot embeddings can effectively transform X-rays into text-space as radiology-specific labels.

## Method Summary
LaB-RAG uses zero-shot image embeddings from BioViL-T or GLoRIA models, which are then classified using logistic regression models trained to predict 14 radiology labels per image. These predicted labels are used to filter retrieved reports by matching label sets, and the filtered reports along with the predicted labels are formatted into prompts for an LLM (Mistral-7B-Instruct). The method avoids fine-tuning large models by leveraging frozen vision and language models with minimal trainable components, achieving competitive performance through effective label-based retrieval filtering and formatting.

## Key Results
- Achieves better F1-CheXbert scores across natural language and radiology language metrics compared with other retrieval-based RRG methods
- Reaches state-of-the-art performance on MIMIC-CXR for "Findings" generation with perfect F1-CheXbert scores
- Demonstrates competitive results for "Impression" generation while maintaining a modular architecture that can be combined with fine-tuned methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using small classification models over zero-shot embeddings to generate categorical labels improves retrieval quality by filtering irrelevant examples.
- **Mechanism:** Zero-shot embeddings capture visual features that can be mapped to interpretable radiology labels (e.g., "Cardiomegaly," "Pneumonia"). These labels are then used to filter or rerank retrieved text examples, reducing noise in the LLM's context.
- **Core assumption:** Categorical labels derived from embeddings correlate well with clinically meaningful features in radiology images.
- **Evidence anchors:**
  - [abstract] "We argue that simple classification models combined with zero-shot embeddings can effectively transform X-rays into text-space as radiology-specific labels."
  - [section] "We train a set of logistic regression models...to classify images directly, thereby preventing data-leakage."
  - [corpus] Weak; no direct corpus evidence for this mechanism yet.
- **Break condition:** If the classification models have low accuracy, the label-based filtering may remove relevant examples or retain irrelevant ones, degrading performance.

### Mechanism 2
- **Claim:** Label formatting as part of the prompt context helps the LLM focus generation on clinically relevant findings.
- **Mechanism:** Positive labels are formatted as comma-separated lists in the prompt, which acts as explicit conditioning for the LLM to generate corresponding text. This is especially effective when the LLM has strong natural language comprehension.
- **Core assumption:** LLMs can use structured label descriptors as effective generation constraints when they have strong instruction-following capabilities.
- **Evidence anchors:**
  - [abstract] "These derived text labels can be used with general-domain LLMs to generate radiology reports."
  - [section] "We use categorical label names as textual image descriptors for image captioning."
  - [corpus] Weak; no corpus evidence directly supports this claim.
- **Break condition:** If the LLM does not interpret the label formatting as meaningful context (e.g., poor instruction-following), the benefit diminishes.

### Mechanism 3
- **Claim:** Composing frozen vision and language models with minimal trainable components (logistic classifiers) yields competitive performance without fine-tuning large models.
- **Mechanism:** Domain-adapted embeddings (BioViL-T, GLoRIA) are used with logistic classifiers to generate labels, which are then used to filter and format retrieved text for an LLM. This avoids costly fine-tuning of multimodal models.
- **Core assumption:** Zero-shot embeddings from domain-adapted models retain sufficient discriminative power for downstream classification.
- **Evidence anchors:**
  - [abstract] "LaB-RAG achieves better results across natural language and radiology language metrics compared with other retrieval-based RRG methods."
  - [section] "We extract zero-shot, frozen image feature embeddings to enable retrieval of similar images."
  - [corpus] Weak; no corpus evidence yet.
- **Break condition:** If embeddings lose discriminative power (e.g., domain mismatch), classifier accuracy drops, undermining the entire pipeline.

## Foundational Learning

- **Concept:** Zero-shot embeddings and contrastive learning
  - Why needed here: They allow the model to generate meaningful image representations without task-specific fine-tuning, critical for LaB-RAG's modular approach.
  - Quick check question: What is the key difference between zero-shot embeddings and task-specific fine-tuned embeddings?

- **Concept:** Multilabel classification with logistic regression
  - Why needed here: LaB-RAG uses logistic classifiers to predict binary labels for each radiology finding, enabling label-based filtering and formatting.
  - Quick check question: Why is logistic regression sufficient for LaB-RAG's label prediction instead of deep learning?

- **Concept:** Retrieval-augmented generation (RAG) with in-context learning
  - Why needed here: RAG provides relevant text examples to the LLM, and ICL allows conditioning without model training.
  - Quick check question: How does label-based filtering enhance standard RAG's retrieved examples?

## Architecture Onboarding

- **Component map:** Image Encoder (BioViL-T/GLoRIA) -> Label Classifiers (Logistic Regressions) -> Retrieval Module -> Prompt Formatter -> LLM (Mistral-7B-Instruct)
- **Critical path:** Image → Embeddings → Labels → Filter/Rank → Format → LLM → Report
- **Design tradeoffs:**
  - Using frozen models reduces training cost but depends heavily on embedding quality
  - Label filtering improves relevance but may exclude valid examples if labels are noisy
  - Simpler prompt formatting (Simple) is less verbose but may be less precise than Verbose/Instruct
- **Failure signatures:**
  - Poor label accuracy → noisy filtering → irrelevant examples → off-topic reports
  - Weak embedding similarity → poor retrieval → incomplete context → hallucinations
  - LLM not following instructions → label formatting ignored → no improvement over standard RAG
- **First 3 experiments:**
  1. **Baseline ablation:** Run standard RAG without label filtering/formatting to establish baseline metrics
  2. **Label filter test:** Apply Exact label filter only, measure change in F1-CheXbert vs baseline
  3. **Embedding swap:** Replace BioViL-T with ResNet50, compare embedding-based retrieval quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LaB-RAG performance scale with larger language models beyond the 7B parameter models tested?
- Basis in paper: [inferred] The paper shows improvements with newer Mistral-7B versions, suggesting larger models could yield further gains, but this was not tested.
- Why unresolved: The experiments only compared Mistral-7B variants, leaving open whether performance gains continue with larger LLMs.
- What evidence would resolve it: Testing LaB-RAG with larger models (e.g., 70B+ parameter LLMs) on the same datasets and comparing F1-CheXbert and F1-RadGraph scores.

### Open Question 2
- Question: What is the precise source of the gap between F1-CheXbert and F1-RadGraph performance, and can it be eliminated?
- Basis in paper: [explicit] The authors note LaB-RAG achieves perfect F1-CheXbert but significantly lower F1-RadGraph, hypothesizing it may be due to LLM ignoring labels or noise in labeling.
- Why unresolved: The exact mechanism causing this discrepancy is unclear, and whether it's fixable through better prompting or model training remains unknown.
- What evidence would resolve it: Systematic ablation studies varying label presentation methods, testing with human-annotated reports, and analyzing which specific RadGraph entities are consistently missed.

### Open Question 3
- Question: Can LaB-RAG's modular components (label filtering/formatting) be effectively combined with fine-tuned vision-language models?
- Basis in paper: [explicit] The authors state their results "suggest broader compatibility and synergy with fine-tuned methods to further enhance RRG performance."
- Why unresolved: While the paper shows LaB-RAG improves retrieval-based methods, it doesn't test integration with fine-tuned VL models that have strong image understanding.
- What evidence would resolve it: Experiments combining LaB-RAG's label modules with fine-tuned models like BioViL or Meditron, measuring whether the label boosts still provide performance gains.

## Limitations

- The method relies heavily on the quality of zero-shot embeddings and the accuracy of logistic regression classifiers, which may degrade performance if embeddings fail to capture clinically relevant features.
- The paper does not provide detailed prompt templates, which are critical for faithful reproduction of the results.
- The effectiveness of label formatting for LLMs is supported by results but lacks rigorous validation and corpus evidence for generalizability.

## Confidence

- **High Confidence:** The modular approach of using frozen embeddings and logistic classifiers to avoid fine-tuning large models is well-justified and supported by experimental results showing competitive performance.
- **Medium Confidence:** The claim that label filtering improves retrieval relevance is plausible but lacks direct corpus evidence. The effectiveness of label formatting for LLMs is also supported by the results but not rigorously validated.
- **Low Confidence:** The generalizability of the method to other medical imaging domains or LLMs is uncertain due to the lack of ablation studies on prompt templates and embedding choices.

## Next Checks

1. **Prompt Template Validation:** Test the impact of different label formatting styles (e.g., comma-separated vs. natural language) on LLM performance to confirm the importance of prompt design.
2. **Embedding Robustness:** Swap BioViL-T/GLoRIA embeddings with other frozen models (e.g., ResNet50) to assess the sensitivity of the method to embedding quality.
3. **Classifier Accuracy Analysis:** Evaluate per-label F1 scores on the validation set to identify weak classifiers and their impact on retrieval and generation quality.