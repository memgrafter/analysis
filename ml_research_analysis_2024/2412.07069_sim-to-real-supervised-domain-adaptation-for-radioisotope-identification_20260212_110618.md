---
ver: rpa2
title: Sim-to-real supervised domain adaptation for radioisotope identification
arxiv_id: '2412.07069'
source_url: https://arxiv.org/abs/2412.07069
tags:
- domain-adapted
- best
- tbnn
- spectra
- source-only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that supervised domain adaptation significantly
  improves radioisotope identification accuracy by transferring knowledge from synthetic
  to experimental data. The authors pretrained transformer-based neural networks on
  synthetic gamma spectra from GADRAS, then fine-tuned on just 64 labeled experimental
  spectra from handheld LaBr3 and NaI(Tl) detectors.
---

# Sim-to-real supervised domain adaptation for radioisotope identification

## Quick Facts
- arXiv ID: 2412.07069
- Source URL: https://arxiv.org/abs/2412.07069
- Authors: Peter Lalor; Henry Adams; Alex Hagen
- Reference count: 40
- Primary result: 96% test accuracy achieved using supervised domain adaptation with just 64 labeled experimental spectra

## Executive Summary
This study demonstrates that supervised domain adaptation significantly improves radioisotope identification accuracy by transferring knowledge from synthetic to experimental data. The authors pretrained transformer-based neural networks on synthetic gamma spectra from GADRAS, then fine-tuned on just 64 labeled experimental spectra from handheld LaBr3 and NaI(Tl) detectors. This approach achieved 96% test accuracy, surpassing synthetic-only (75%) and experiment-only (80%) baselines. The domain-adapted models also learned more human-interpretable features, correctly identifying characteristic isotope peaks that baseline models missed.

## Method Summary
The authors implemented a supervised domain adaptation approach for radioisotope identification using gamma spectroscopy. They generated synthetic training data using GADRAS, creating 1024-bin spectra spanning 0-3000 keV for 32 radioisotopes. A custom transformer-based neural network (TBNN) was trained on this synthetic data, then fine-tuned on 64 labeled experimental spectra from LaBr3 and NaI(Tl) detectors. The model architecture featured learnable patch embeddings, tunable patch size, learnable [CLS] token, and learnable positional encoding. Performance was evaluated using test accuracy and compared against synthetic-only and experiment-only baseline models.

## Key Results
- Domain-adapted models achieved 96% test accuracy, outperforming synthetic-only (75%) and experiment-only (80%) baselines
- Models learned more human-interpretable features, correctly identifying characteristic isotope peaks
- Domain adaptation reduced overfitting to low-energy, high-count features common in small experimental datasets
- Transformer architectures showed particular benefit from pretraining compared to classical architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pretraining on synthetic data provides rich, diverse feature representations that transfer to experimental settings.
- **Mechanism:** Synthetic datasets contain a broader range of isotopic compositions and experimental conditions than small labeled experimental sets. Pretraining captures general spectral patterns (peak shapes, noise characteristics) before fine-tuning on task-specific features.
- **Core assumption:** The synthetic-to-experimental domain gap is smaller than the experimental-to-experimental gap for small datasets.
- **Evidence anchors:**
  - [abstract] "We begin by pretraining a spectral classifier on synthetic data using a custom transformer-based neural network."
  - [section] "Furthermore, we demonstrate that domain-adapted models learn more human-interpretable features than experiment-only baseline models."
  - [corpus] Weak - no direct mention of synthetic pretraining benefits, but similar works focus on sim-to-real adaptation.

### Mechanism 2
- **Claim:** Domain adaptation reduces overfitting to low-energy, high-count features common in small experimental datasets.
- **Mechanism:** Models trained only on experimental data overfit to specific features (e.g., 20-40 keV band in 239Pu), while domain-adapted models learn broader feature importance across the spectrum.
- **Core assumption:** Small experimental datasets contain limited diversity, leading to overfitting to specific spectral regions.
- **Evidence anchors:**
  - [section] "We observe that domain-adapted models more consistently produce explanations that align with human-interpretable photopeaks, while target-only models are more likely to focus on low-energy, high-count features."
  - [section] "As seen in Fig. A.8, when classifying 239Pu, the target-only model identifies the 20-40 keV band as the most salient component of the spectrum, indicative of overfitting."
  - [corpus] Weak - no direct evidence of overfitting to specific spectral regions, but sim-to-real adaptation literature suggests similar issues.

### Mechanism 3
- **Claim:** Transformer architectures benefit more from pretraining than classical architectures due to higher model complexity and data requirements.
- **Mechanism:** Transformers have higher capacity and require more data to learn effective representations. Pretraining on synthetic data provides the diverse data needed to overcome this limitation.
- **Core assumption:** Transformers are more complex and data-hungry than classical architectures.
- **Evidence anchors:**
  - [section] "Notably, these trends do not appear in the target-only model class, suggesting that transformers benefit more from pretraining than classical architectures."
  - [section] "We hypothesize that this result reflects the higher model complexity and greater data requirements of transformers, and that rigorous synthetic pretraining can partially mitigate this large-data hurdle."
  - [corpus] Weak - no direct evidence of transformer complexity advantages, but transformer literature supports this claim.

## Foundational Learning

- **Concept:** Domain adaptation and transfer learning
  - **Why needed here:** The study bridges synthetic and experimental data domains, requiring knowledge of how to transfer learned representations between domains.
  - **Quick check question:** What is the difference between supervised and unsupervised domain adaptation?

- **Concept:** Transformer architectures and attention mechanisms
  - **Why needed here:** The study uses transformer-based neural networks for spectral classification, requiring understanding of how transformers process sequential data.
  - **Quick check question:** How do transformers differ from CNNs in processing spectral data?

- **Concept:** SHAP (SHapley Additive exPlanations) for model interpretability
  - **Why needed here:** The study uses SHAP to explain model predictions and compare interpretability between domain-adapted and target-only models.
  - **Quick check question:** What does SHAP measure in the context of spectral classification?

## Architecture Onboarding

- **Component map:** Input spectra (1024 channels) -> Z-score normalization -> Transformer encoder (learnable patches, positional encoding, [CLS] token) -> Isotopic classification probabilities
- **Critical path:**
  1. Generate synthetic training data using GADRAS/PyRIID
  2. Train source-only model on synthetic data
  3. Fine-tune on small experimental dataset
  4. Evaluate on held-out test set
  5. Analyze interpretability using SHAP
- **Design tradeoffs:**
  - Patch size vs. computational efficiency
  - Learnable vs. fixed positional encoding
  - Number of attention blocks vs. model capacity
  - Fine-tuning vs. freezing pretrained layers
- **Failure signatures:**
  - Overfitting to synthetic data patterns
  - Poor generalization to experimental data
  - SHAP explanations focusing on irrelevant spectral regions
  - Accuracy degradation with larger experimental datasets
- **First 3 experiments:**
  1. Compare source-only vs. target-only training on small experimental dataset
  2. Evaluate domain adaptation benefits with varying fine-tuning dataset sizes
  3. Analyze SHAP explanations to identify interpretable features learned by different models

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of supervised domain adaptation change when using more complex experimental datasets with overlapping isotope signatures and higher background noise?
- **Basis in paper:** [explicit] The authors note that their experimental datasets were relatively simple, achieving 100% accuracy with just ~500 training examples due to single-label, high signal-to-noise ratio data collection
- **Why unresolved:** The paper used a simplified experimental dataset that doesn't reflect real-world complexity where multiple isotopes and background sources overlap
- **What evidence would resolve it:** Testing the same domain adaptation approach on experimental datasets with mixed sources, overlapping spectral features, and varying background levels would show how well the method generalizes to more challenging scenarios

### Open Question 2
- **Question:** What is the minimum size of synthetic pretraining dataset needed to achieve optimal performance when fine-tuning on experimental data?
- **Basis in paper:** [inferred] The authors used 10^6 synthetic spectra for pretraining but only tested performance after pretraining on the full dataset, not examining how pretraining dataset size affects downstream performance
- **Why unresolved:** The study used a fixed large synthetic dataset for pretraining without exploring how reducing its size impacts the quality of domain adaptation
- **What evidence would resolve it:** Systematically varying the size of the synthetic pretraining dataset while measuring performance on fixed experimental fine-tuning sets would identify the point of diminishing returns

### Open Question 3
- **Question:** How do different types of data augmentation during fine-tuning affect the robustness and generalization of domain-adapted models?
- **Basis in paper:** [inferred] The authors mention varying parameters like background count rate and live time during synthetic data generation but don't explore augmentation strategies during the fine-tuning phase
- **Why unresolved:** The study used standard z-score normalization but didn't investigate whether additional augmentation techniques during fine-tuning could improve model robustness
- **What evidence would resolve it:** Comparing models fine-tuned with various augmentation strategies (noise injection, spectral transformations, etc.) against models without augmentation would reveal the impact on performance and generalization

## Limitations
- Small experimental dataset (64 spectra) raises concerns about generalizability to larger datasets
- Synthetic-to-experimental domain gap remains unquantified with unspecified GADRAS parameters
- Study focuses on single-isotope classification with limited validation of multi-isotope detection capabilities

## Confidence

- **High Confidence:** The core finding that supervised domain adaptation improves accuracy (96% vs 75-80% baselines) is well-supported by experimental results and statistical comparisons.
- **Medium Confidence:** The interpretability findings showing domain-adapted models learn more human-aligned features are supported but rely on specific visualization methods that may not generalize.
- **Low Confidence:** Claims about transformer architectures benefiting more from pretraining than classical architectures are speculative, based on limited comparison with only one classical architecture.

## Next Checks

1. **Scale-up Validation:** Test domain adaptation performance on a larger experimental dataset (1000+ spectra) to verify benefits persist beyond small-sample scenarios.

2. **Cross-Detector Generalization:** Evaluate model performance when trained on one detector type (e.g., LaBr3) and tested on another (e.g., NaI(Tl)) to assess detector-invariant feature learning.

3. **Multi-isotope Robustness:** Validate model performance on complex mixed-source scenarios with overlapping photopeaks to test real-world applicability beyond single-isotope classification.