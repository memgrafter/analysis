---
ver: rpa2
title: 'Towards smaller, faster decoder-only transformers: Architectural variants
  and their implications'
arxiv_id: '2404.14462'
source_url: https://arxiv.org/abs/2404.14462
tags:
- architecture
- parameters
- config
- pgpt
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes three architectural variants of the decoder-only
  transformer to reduce model size and improve training/inference efficiency while
  maintaining performance. The three variants are ParallelGPT (pgpt), LinearGPT (lgpt),
  and ConvGPT (cgpt).
---

# Towards smaller, faster decoder-only transformers: Architectural variants and their implications

## Quick Facts
- arXiv ID: 2404.14462
- Source URL: https://arxiv.org/abs/2404.14462
- Reference count: 40
- Primary result: Proposed architectures achieve competitive performance with fewer parameters and faster training than traditional GPT

## Executive Summary
This paper proposes three architectural variants of the decoder-only transformer to reduce model size and improve training/inference efficiency while maintaining performance. The three variants are ParallelGPT (pgpt), LinearGPT (lgpt), and ConvGPT (cgpt). ParallelGPT splits decoder blocks into parallel paths with separate dense layers for faster training and flexible inference. LinearGPT reduces dimensions by half after every n blocks to reduce parameters. ConvGPT replaces linear compression layers with one-dimensional convolutions. Models were trained on a 10 billion token subset of fineweb-eda and evaluated on seven benchmarks, showing competitive performance with fewer parameters and faster training times.

## Method Summary
The paper introduces three architectural variants designed to improve efficiency while maintaining performance. ParallelGPT divides decoder blocks into parallel paths, each with its own dense layer, enabling faster training and flexible inference scenarios. LinearGPT progressively reduces the dimension of decoder blocks by half after every n blocks, substantially reducing parameters. ConvGPT builds on LinearGPT by replacing the linear compression layers with one-dimensional convolution layers. All models were trained on a 10 billion token subset of the fineweb-eda dataset and evaluated across seven benchmarks to compare performance against traditional GPT architectures.

## Key Results
- All three proposed architectures achieved competitive performance compared to traditional GPT with fewer parameters
- LinearGPT outperformed the traditional architecture on 4 out of 7 benchmarks while using less than half the parameters
- The proposed models demonstrated faster training times than the baseline GPT architecture
- ParallelGPT enabled flexible inference scenarios with different parallel paths

## Why This Works (Mechanism)
The proposed architectures work by strategically reducing computational complexity and parameter counts while preserving essential representational capacity. ParallelGPT distributes computation across multiple paths, allowing for concurrent processing and reducing sequential dependencies. LinearGPT's progressive dimension reduction exploits the observation that later layers in transformers may require less capacity, as they primarily refine rather than construct representations. ConvGPT's use of convolutions for dimension reduction provides a more parameter-efficient alternative to linear layers while maintaining local feature extraction capabilities. These approaches collectively challenge the assumption that uniform scaling across all decoder blocks is necessary for strong performance.

## Foundational Learning
- **Decoder-only transformer architecture**: Understanding the standard transformer decoder block (self-attention, feed-forward network, layer norm) is essential for grasping how these variants modify the baseline structure
- **Parameter efficiency vs performance trade-offs**: Knowledge of how parameter counts relate to model capacity and generalization helps evaluate the proposed approaches
- **Progressive dimension reduction**: Understanding why reducing dimensions in later layers can maintain performance while reducing computational cost
- **Parallel processing in neural networks**: Familiarity with how splitting computation across paths affects training dynamics and inference flexibility
- **Convolutional layers in sequence models**: Understanding how 1D convolutions can replace linear layers for dimension reduction while preserving local dependencies
- **Benchmark evaluation methodology**: Knowledge of how multiple benchmarks are used to assess model capabilities across different tasks

## Architecture Onboarding

**Component Map**
Token Embeddings -> ParallelGPT Paths (multiple independent decoder blocks with dense layers) OR LinearGPT Blocks (progressive dimension reduction) OR ConvGPT Blocks (conv-based compression) -> Output Projection

**Critical Path**
Input tokens → Token embeddings → Self-attention and feed-forward layers (modified per variant) → Output projection to vocabulary

**Design Tradeoffs**
- ParallelGPT: Faster training and flexible inference vs potential synchronization overhead and increased memory usage during inference
- LinearGPT: Significant parameter reduction vs possible loss of representational capacity in later layers
- ConvGPT: More parameter-efficient compression vs potentially reduced global context modeling compared to linear layers

**Failure Signatures**
- Performance degradation on tasks requiring complex reasoning or long-range dependencies
- Out-of-distribution performance drops not captured by the seven evaluation benchmarks
- Inference overhead from parallel path synchronization that negates training speed benefits
- ConvGPT variants showing weaknesses on tasks requiring precise positional information

**First 3 Experiments**
1. Evaluate all three architectures on a diverse set of datasets including non-educational text and longer sequences to test generalization
2. Conduct ablation studies isolating the impact of each architectural change (parallel paths, dimension reduction, convolution replacement)
3. Measure actual inference latency and memory usage across different batch sizes and sequence lengths to validate the claimed flexible inference benefits of ParallelGPT

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation was limited to a single 10 billion token dataset (fineweb-eda) and seven benchmarks
- Potential performance degradation on out-of-domain data or longer sequences was not addressed
- ConvGPT's reduced representational capacity may impact performance on more complex tasks
- Parallel processing approach may introduce synchronization overhead during inference that was not fully characterized

## Confidence
- High confidence in parameter reduction claims across all three architectures
- Medium confidence in training speed improvements, particularly for ParallelGPT
- Medium confidence in overall benchmark performance relative to baseline GPT
- Low confidence in generalization claims beyond the specific evaluation setup

## Next Checks
1. Evaluate all three architectures on a diverse set of datasets including non-educational text and longer sequence lengths to test generalization
2. Conduct ablation studies isolating the impact of each architectural change (parallel paths, dimension reduction, convolution replacement)
3. Measure actual inference latency and memory usage across different batch sizes and sequence lengths to validate the claimed flexible inference benefits of ParallelGPT