---
ver: rpa2
title: Countermeasures Against Adversarial Examples in Radio Signal Classification
arxiv_id: '2407.06796'
source_url: https://arxiv.org/abs/2407.06796
tags:
- adversarial
- modulation
- against
- examples
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the first countermeasure against adversarial
  attacks in radio signal modulation classification. The authors develop a neural
  rejection system that detects and rejects adversarial examples by leveraging the
  "amplification" phenomenon, where adversarial examples cause larger neuron output
  values during DNN propagation compared to normal signals.
---

# Countermeasures Against Adversarial Examples in Radio Signal Classification

## Quick Facts
- arXiv ID: 2407.06796
- Source URL: https://arxiv.org/abs/2407.06796
- Reference count: 8
- Primary result: First countermeasure against adversarial attacks in radio signal modulation classification using neural rejection system with label smoothing and Gaussian noise injection

## Executive Summary
This paper introduces the first countermeasure against adversarial attacks in radio signal modulation classification. The authors propose a neural rejection system that detects and rejects adversarial examples by leveraging the "amplification" phenomenon, where adversarial examples cause larger neuron output values during DNN propagation compared to normal signals. They enhance this approach with label smoothing and Gaussian noise injection to improve robustness. Experiments using real radio signal datasets show that their method significantly outperforms unprotected deep learning models and simpler rejection techniques, achieving high classification accuracy against adversarial attacks while forcing attackers to use higher perturbation power. The method is also effective against jamming attacks.

## Method Summary
The method employs a neural rejection system that combines a pre-trained VT-CNN2 classifier with label smoothing and Gaussian noise injection (LS-GNA) with an SVM-based rejection mechanism. The LS-GNA approach reduces model overconfidence and sensitivity to adversarial perturbations. During inference, last-layer features from the CNN are extracted and fed into a one-versus-all SVM classifier. Samples with maximum decision scores below a predefined threshold are rejected as adversarial. The system is evaluated on the GNU Radio ML dataset RML2016.19a with 11 modulation categories and SNR levels from -20dB to 18dB.

## Key Results
- The LS-GNA based neural rejection system achieves significantly higher classification accuracy against adversarial attacks compared to unprotected DNNs
- Label smoothing and Gaussian noise injection reduce the success rate of adversarial attacks by increasing the required perturbation power
- The system maintains high accuracy on clean data while rejecting 10% of benign samples as a trade-off for robustness
- Performance remains effective against both adversarial and jamming attacks

## Why This Works (Mechanism)

### Mechanism 1: Amplification Phenomenon
During DNN propagation, adversarial examples cause neuron outputs to increase progressively, leading to significantly larger values at the final layer compared to normal signals. This amplification effect is sufficiently distinct between adversarial and normal samples to enable reliable detection via a threshold-based rejection system. The mechanism breaks if adversarial examples evolve to maintain consistent neuron activation patterns similar to normal samples across all layers.

### Mechanism 2: Label Smoothing and Gaussian Noise Injection
Label smoothing converts one-hot labels to smoothed vectors, preventing the DNN from becoming overconfident, while Gaussian noise injection adds uncertainty to training data. Together, these techniques create decision boundaries that are less vulnerable to adversarial manipulation while maintaining classification accuracy on clean data. The mechanism fails if attackers can specifically target the smoothed decision boundaries or if noise injection degrades clean data performance beyond acceptable limits.

### Mechanism 3: SVM-Based Rejection Using Last-Layer Features
Features extracted from the last layer of the pre-trained CNN contain sufficient discriminative information to separate adversarial examples from clean samples using an SVM classifier. The SVM generates decision scores for each class, and samples with maximum scores below a threshold are rejected as adversarial. This mechanism fails if adversarial examples can be crafted to produce feature distributions indistinguishable from clean samples in the last-layer feature space.

## Foundational Learning

- Concept: Adversarial examples in deep learning
  - Why needed here: Understanding how small, imperceptible perturbations can cause misclassification is fundamental to appreciating why this defense is necessary and how it works
  - Quick check question: What is the key difference between adversarial examples and random noise in terms of their effect on DNN classification?

- Concept: Neural rejection systems
  - Why needed here: The proposed countermeasure relies on detecting and rejecting adversarial examples rather than correcting them, making understanding rejection systems critical
  - Quick check question: How does a neural rejection system differ from a traditional classifier in terms of its output and decision process?

- Concept: Label smoothing and its effect on model generalization
  - Why needed here: Label smoothing is a key component of the enhanced defense mechanism, and understanding its purpose and effect is essential for implementing and tuning the system
  - Quick check question: What is the primary purpose of label smoothing in training deep learning models, and how might it affect the model's response to adversarial examples?

## Architecture Onboarding

- Component map: Radio signal samples -> VT-CNN2 classifier -> Last layer feature extraction -> SVM classifier -> Rejection threshold comparison -> Output (class prediction or rejection)
- Critical path: Signal preprocessing and normalization -> CNN feature extraction (last layer) -> SVM decision score generation -> Threshold comparison and rejection decision
- Design tradeoffs: Rejection rate vs. false rejection rate, label smoothing parameter vs. accuracy, Gaussian noise variance vs. robustness, SVM kernel choice vs. performance
- Failure signatures: High rejection rate on clean data, low accuracy on adversarial examples, degraded performance on clean data
- First 3 experiments: 1) Baseline evaluation of undefended VT-CNN2, 2) Rejection system tuning with varying thresholds, 3) Ablation study comparing performance with and without label smoothing and Gaussian noise injection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed LS-GNA based NR system perform against black box and grey box attacks?
- Basis in paper: The authors state in the conclusions that future work will extend these techniques to black box and grey box attacks
- Why unresolved: The paper only evaluates the defense against white-box attacks where the adversary knows the exact input, is synchronous with the transmitter, and knows the architecture of the opponent's classifier
- What evidence would resolve it: Experimental results showing the classification accuracy and rejection rate of the LS-GNA based NR system when facing black box and grey box attacks under various conditions

### Open Question 2
- Question: What is the optimal rejection threshold (Θ) for the neural rejection system that balances classification accuracy and rejection rate across different SNR levels and modulation schemes?
- Basis in paper: The paper mentions that the rejection threshold was set so that the rejection rate is 10% when original benign data samples are applied, but does not explore how different thresholds affect performance across varying conditions
- Why unresolved: The optimal threshold likely depends on the specific application requirements and environmental conditions
- What evidence would resolve it: A systematic evaluation of the LS-GNA based NR system's performance across a range of rejection thresholds, SNR levels, and modulation schemes

### Open Question 3
- Question: How does the proposed LS-GNA based NR system compare to other defense mechanisms against adversarial attacks in radio signal classification?
- Basis in paper: The paper only compares the LS-GNA based NR system to an undefended DNN and a simple neural rejection system
- Why unresolved: Different defense mechanisms may have varying strengths and weaknesses, and the optimal approach may depend on the specific threat model and system constraints
- What evidence would resolve it: Comparative experiments evaluating the LS-GNA based NR system against other defense mechanisms, such as adversarial training and defensive distillation, under the same attack scenarios and metrics

## Limitations

- The amplification phenomenon relies on empirical observations rather than a formal theoretical explanation
- The effectiveness depends on specific hyperparameter settings without systematic sensitivity analysis
- Performance metrics are evaluated only against one specific attack type (FGM), leaving uncertainty about effectiveness against more sophisticated attacks

## Confidence

**High Confidence**: The claim that the neural rejection system can detect and reject adversarial examples using last-layer features. This is supported by direct experimental evidence showing improved classification accuracy against adversarial attacks when using the rejection system.

**Medium Confidence**: The effectiveness of label smoothing and Gaussian noise injection in improving robustness. While the paper provides theoretical justification and experimental results, the ablation study only partially quantifies the individual contributions of these techniques.

**Low Confidence**: The claim about the "amplification" phenomenon being the fundamental mechanism for detection. The paper demonstrates the phenomenon empirically but lacks a rigorous theoretical explanation for why adversarial examples consistently produce larger neuron outputs during propagation.

## Next Checks

1. **Cross-attack robustness evaluation**: Test the proposed defense against multiple attack types (including PGD, CW, and universal perturbations) to verify whether the amplification phenomenon holds consistently across different attack methodologies.

2. **Parameter sensitivity analysis**: Systematically vary the label smoothing parameter α and Gaussian noise variance to identify optimal settings and determine the robustness of performance to these hyperparameters.

3. **Theoretical analysis of amplification**: Develop a mathematical model explaining why adversarial examples produce larger neuron outputs during DNN propagation, potentially through analyzing gradient flow or activation patterns across layers.