---
ver: rpa2
title: Distilling Long-tailed Datasets
arxiv_id: '2408.14506'
source_url: https://arxiv.org/abs/2408.14506
tags:
- dataset
- long-tailed
- matching
- distillation
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Long-tailed Aware Dataset Distillation (LAD),
  the first method addressing dataset distillation for long-tailed distributions.
  Existing approaches degrade significantly on imbalanced data due to biased expert
  trajectories and poor tail-class performance.
---

# Distilling Long-tailed Datasets
## Quick Facts
- **arXiv ID**: 2408.14506
- **Source URL**: https://arxiv.org/abs/2408.14506
- **Reference count**: 40
- **Primary result**: First method addressing dataset distillation for long-tailed distributions, achieving state-of-the-art performance across imbalance factors.

## Executive Summary
This work introduces Long-tailed Aware Dataset Distillation (LAD), the first method addressing dataset distillation for long-tailed distributions. Existing approaches degrade significantly on imbalanced data due to biased expert trajectories and poor tail-class performance. LAD proposes two solutions: Weight Mismatch Avoidance, which aligns student and expert gradients to mitigate bias, and Adaptive Decoupled Matching, which jointly matches decoupled backbone and classifier experts to improve tail-class accuracy and label reliability.

## Method Summary
LAD addresses dataset distillation for long-tailed distributions by introducing two key innovations. First, Weight Mismatch Avoidance ensures the student model's gradient directions align with expert model gradients, preventing bias toward head classes. Second, Adaptive Decoupled Matching optimizes the student model's backbone and classifier separately, enabling better performance on tail classes while maintaining overall accuracy. These methods are evaluated on CIFAR-10-LT and CIFAR-100-LT, demonstrating superior performance compared to existing baselines.

## Key Results
- Achieves state-of-the-art performance on CIFAR-10-LT and CIFAR-100-LT
- Maintains accuracy across varying imbalance factors (62.6% at β=200, IPC=50)
- Outperforms baselines by up to 17.1% in cross-architecture settings

## Why This Works (Mechanism)
LAD addresses the fundamental challenge of dataset distillation on long-tailed distributions by preventing the student model from inheriting the expert model's bias toward head classes. The Weight Mismatch Avoidance technique ensures gradient alignment between student and expert models, while Adaptive Decoupled Matching separately optimizes the backbone and classifier components. This dual approach enables the distilled dataset to better represent tail classes while maintaining overall performance.

## Foundational Learning
- **Long-tailed distributions**: Understanding class imbalance and its impact on model performance. *Why needed*: Critical for identifying the problem LAD addresses. *Quick check*: Compare accuracy on head vs. tail classes.
- **Dataset distillation**: The process of creating small synthetic datasets that preserve the learning behavior of larger datasets. *Why needed*: LAD extends this technique to long-tailed scenarios. *Quick check*: Evaluate training performance using distilled datasets.
- **Gradient alignment**: Ensuring student and expert models follow similar optimization trajectories. *Why needed*: Prevents bias in the distilled dataset. *Quick check*: Measure cosine similarity between gradients.

## Architecture Onboarding
- **Component map**: Dataset → LAD Distillation → Distilled Dataset → Training → Model
- **Critical path**: The distillation process is the core, where Weight Mismatch Avoidance and Adaptive Decoupled Matching operate to create a balanced dataset.
- **Design tradeoffs**: LAD sacrifices some head-class performance to improve tail-class accuracy, which is necessary for long-tailed distributions.
- **Failure signatures**: Poor performance on tail classes indicates insufficient gradient alignment or decoupled matching.
- **First experiments**:
  1. Evaluate LAD on CIFAR-10-LT with β=100 and IPC=50.
  2. Compare LAD's tail-class accuracy to baseline methods.
  3. Test cross-architecture performance to validate generalizability.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to small-scale datasets (CIFAR-10-LT, CIFAR-100-LT).
- Restricted imbalance ratios (β=100, 200) do not cover extreme long-tailed scenarios.
- Focus on classification tasks, leaving applicability to other tasks unverified.

## Confidence
- **Methodological contributions**: High
- **Performance claims**: Medium
- **Generalizability to larger datasets**: Low

## Next Checks
1. Evaluate LAD on larger-scale long-tailed datasets (e.g., ImageNet-LT, Places-LT) to assess scalability and robustness.
2. Test the method's performance on non-classification tasks (e.g., object detection, semantic segmentation) to verify broader applicability.
3. Analyze the impact of LAD on tail-class performance across a wider range of imbalance ratios (β=10 to β=1000) to validate effectiveness in extreme long-tailed scenarios.