---
ver: rpa2
title: MMMModal -- Multi-Images Multi-Audio Multi-turn Multi-Modal
arxiv_id: '2402.11297'
source_url: https://arxiv.org/abs/2402.11297
tags:
- audio
- image
- yang
- visual
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MMMModal, a multimodal large language model
  capable of processing multiple images, audio inputs, and text within a single multi-turn
  dialogue session. The model uses SigLIP for visual encoding and Whisper for audio
  encoding, enabling bilingual understanding in English and Malay.
---

# MMMModal -- Multi-Images Multi-Audio Multi-turn Multi-Modal

## Quick Facts
- **arXiv ID**: 2402.11297
- **Source URL**: https://arxiv.org/abs/2402.11297
- **Reference count**: 11
- **Primary result**: A multimodal large language model that processes multiple images, audio inputs, and text within multi-turn dialogues using SigLIP and Whisper encoders

## Executive Summary
MMModal is a multimodal large language model designed to handle multiple images, audio inputs, and text within single multi-turn dialogue sessions. The model employs SigLIP for visual encoding and Whisper for audio encoding, enabling bilingual understanding in English and Malay. Available in two sizes (TinyLlama 1.1B and Mistral 7B parameters), MMMModal uses a synthetic dataset pipeline to generate multi-modal instruction-following data. The model is trained in two stages: first aligning features from different modalities with text embeddings, then fine-tuning on the synthetic multi-turn dataset. Experimental results demonstrate MMMModal's capability to handle complex multi-modal queries and multi-turn dialogues involving images, audio, and text.

## Method Summary
MMModal uses SigLIP and Whisper as frozen encoders for visual and audio inputs respectively, with a text encoder fine-tuned for multi-modal alignment. The training process occurs in two stages: initial feature alignment between modalities and text embeddings, followed by fine-tuning on a synthetically generated multi-turn dataset. The synthetic data pipeline includes audio transcription, visual Malaysian context, and multi-image/multi-audio relationship datasets. The model processes multiple modalities simultaneously within dialogue sessions, supporting both English and Malay languages.

## Key Results
- Successfully processes multiple images, audio inputs, and text within single multi-turn dialogue sessions
- Demonstrates bilingual understanding in English and Malay using synthetic dataset generation
- Available in two parameter sizes (1.1B and 7B) with claimed capability to handle complex multi-modal queries

## Why This Works (Mechanism)
The model's effectiveness stems from its dual-stage training approach that first establishes cross-modal alignment before fine-tuning on complex dialogue scenarios. By using established encoder architectures (SigLIP and Whisper) as frozen components, the model leverages proven feature extraction capabilities while focusing fine-tuning resources on the text model. The synthetic dataset generation pipeline enables controlled creation of diverse multi-modal scenarios, particularly incorporating Malaysian cultural context which addresses regional language needs.

## Foundational Learning

**Cross-modal feature alignment**: The process of mapping features from different modalities (images, audio, text) into a shared embedding space to enable coherent processing. Needed because different modalities have inherently different representations and scales. Quick check: Verify that cosine similarity between aligned features from different modalities increases after training.

**Multi-modal instruction tuning**: Training models to follow instructions that involve multiple input types simultaneously. Needed to handle real-world scenarios where users provide mixed input formats. Quick check: Test model's ability to correctly process simple instructions combining two modalities.

**Synthetic data generation for multi-modal tasks**: Creating artificial training data that combines multiple modalities in realistic ways. Needed because real multi-modal datasets are expensive to collect and often limited in scope. Quick check: Validate that generated data covers diverse combinations of modalities and tasks.

## Architecture Onboarding

**Component map**: Image/audio encoders (SigLIP/Whisper) -> Feature alignment layer -> Text encoder (TinyLlama/Mistral) -> Output generation

**Critical path**: Input modalities → Frozen encoders → Alignment layer → Text encoder → Response generation

**Design tradeoffs**: Using frozen encoders reduces training complexity and leverages proven feature extraction, but limits the model's ability to learn modality-specific adaptations. Synthetic data generation enables controlled training but may introduce domain gaps. Two-stage training balances efficiency with performance but requires careful curriculum design.

**Failure signatures**: The model may struggle with novel multi-modal combinations not present in synthetic training data, show reduced performance on languages beyond English/Malay, or exhibit latency issues when processing multiple audio streams simultaneously.

**First experiments**:
1. Test single-turn response quality on simple image-text queries
2. Evaluate audio transcription accuracy and integration with text responses
3. Assess bilingual performance by comparing English and Malay response quality

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on synthetic data may create domain gaps between training scenarios and real-world multi-modal interactions
- Limited evaluation scope without external benchmark validation
- Computational efficiency and inference latency for multi-modal processing not addressed
- Lack of human evaluation metrics for response quality assessment

## Confidence
**Medium** - Technical methodology is sound, but evaluation lacks external validation and detailed performance metrics

## Next Checks
1. Evaluate the model on established multi-modal benchmarks (e.g., MMMU, MMDU) to assess generalizability beyond synthetic data
2. Conduct human evaluation studies to measure response quality, coherence, and relevance in real-world multi-turn dialogues
3. Test the model's scalability and latency for processing multiple audio streams and images in real-time scenarios