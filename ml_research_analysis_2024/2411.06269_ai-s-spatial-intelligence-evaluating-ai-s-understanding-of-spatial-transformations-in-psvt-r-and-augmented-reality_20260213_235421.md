---
ver: rpa2
title: 'AI''s Spatial Intelligence: Evaluating AI''s Understanding of Spatial Transformations
  in PSVT:R and Augmented Reality'
arxiv_id: '2411.06269'
source_url: https://arxiv.org/abs/2411.06269
tags:
- rotation
- spatial
- test
- vision
- psvt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates AI models' understanding of 3D spatial rotations
  using the Revised Purdue Spatial Visualization Test (Revised PSVT:R) and Augmented
  Reality (AR) visualizations. Three AI vision models (GPT-4, Gemini 1.5 Pro, and
  Llama 3.2) were tested on their ability to recognize and describe spatial rotations
  in terms of axis, angle, and direction.
---

# AI's Spatial Intelligence: Evaluating AI's Understanding of Spatial Transformations in PSVT:R and Augmented Reality

## Quick Facts
- arXiv ID: 2411.06269
- Source URL: https://arxiv.org/abs/2411.06269
- Authors: Uttamasha Monjoree; Wei Yan
- Reference count: 40
- AI vision models achieve only 13.3-23.3% accuracy on 3D spatial rotation tasks from 2D images alone

## Executive Summary
This paper evaluates three AI vision models (GPT-4, Gemini 1.5 Pro, and Llama 3.2) on their ability to understand 3D spatial rotations using the Revised Purdue Spatial Visualization Test (PSVT:R) and Augmented Reality (AR) visualizations. The models performed poorly on standard spatial rotation tests, achieving accuracy between 13.3% to 23.3%. However, accuracy improved dramatically when supplementary textual and mathematical information was provided alongside visual inputs. GPT-4 achieved 96.7% accuracy when complete AR scene information including coordinate systems, rotation angles, and rotation matrix equations was provided. The results suggest that while current AI models struggle with spatial reasoning from visual inputs alone, they can effectively understand spatial transformations when provided with additional contextual information.

## Method Summary
The study tested three AI vision models on Revised PSVT:R rotation problems and AR-Classroom screenshots with varying levels of supplementary information. Images were uploaded to GPT-4, Gemini 1.5 Pro, and Llama 3.2 with prompts asking them to identify correct answers or describe rotation processes. The researchers compared performance across different conditions: original PSVT:R diagrams, diagrams with coordinate systems, simplified single-axis rotations, and AR visualizations with increasing levels of textual information (coordinate system only, coordinate system + angle, coordinate system + angle + rotation matrix).

## Key Results
- AI models achieved only 13.3% to 23.3% accuracy on standard PSVT:R rotation tasks
- GPT-4 showed better accuracy (35.7%) on simplified single-axis rotations compared to multi-axis rotations (12.5%)
- Accuracy improved dramatically with AR visualizations containing supplementary textual information
- GPT-4 achieved 96.7% accuracy when provided complete AR scene information including coordinate system, rotation angle, and rotation matrix equation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's spatial reasoning accuracy improves significantly when supplementary textual information is provided alongside visual inputs
- Mechanism: The model uses dual-coding theory where language and imagery systems work together to process nonverbal content. When visual cues alone are insufficient, additional textual information bridges the gap between the model's language processing and spatial reasoning capabilities
- Core assumption: AI vision models require explicit textual descriptions of spatial transformations to achieve accurate spatial reasoning
- Evidence anchors:
  - [abstract] "accuracy improved dramatically when using AR visualizations that included supplementary textual and mathematical information"
  - [section] "GPT-4 achieved 96.7% accuracy when provided with complete AR scene information including coordinate system, rotation angle, and corresponding rotation matrix equation"
  - [corpus] Weak - corpus neighbors focus on rotation detection and 3D transformations but don't directly address the textual augmentation mechanism
- Break condition: If the model could learn to extract necessary spatial information directly from visual inputs without supplementary text, or if the supplementary text becomes inconsistent with the visual representation

### Mechanism 2
- Claim: Simplified spatial rotation tasks with single-axis rotations are more accurately processed by AI models than multi-axis rotations
- Mechanism: Single-axis rotations involve fewer variables and less complex spatial relationships, reducing the cognitive load required for mental manipulation of 3D objects
- Core assumption: The complexity of spatial reasoning scales with the number of transformation axes involved
- Evidence anchors:
  - [section] "GPT-4 showed better accuracy (35.7%) compared to rotations involving multiple axes (12.5%)"
  - [abstract] "When tested on simplified single-axis rotation steps, GPT-4 showed better accuracy (35.7%) compared to rotations involving multiple axes (12.5%)"
  - [corpus] Weak - corpus neighbors discuss rotation detection but don't specifically address single vs. multi-axis complexity
- Break condition: If the model could be trained to handle multi-axis rotations with equal accuracy, or if the apparent difference is due to task presentation rather than inherent complexity

### Mechanism 3
- Claim: AR visualization provides better context for AI spatial reasoning than static 2D images alone
- Mechanism: AR combines real and virtual elements with interactive, real-time registration in three dimensions, providing richer spatial context that AI can leverage when combined with textual information
- Core assumption: AI benefits from the immersive, multi-layered presentation of spatial information that AR provides
- Evidence anchors:
  - [abstract] "accuracy improved dramatically when using AR visualizations that included supplementary textual and mathematical information"
  - [section] "We tested different versions of AR-Classroom images and observed that the performances of the vision models get better when more descriptive texts are added to the visual input"
  - [corpus] Weak - corpus neighbors discuss AR and spatial computing but don't specifically address AI's spatial reasoning in AR contexts
- Break condition: If static 2D images with equivalent textual information could achieve the same accuracy, or if the AR-specific features (real-time interaction, 3D registration) don't contribute to the performance improvement

## Foundational Learning

- Concept: Dual-coding theory - the integration of verbal and imagery systems for processing information
  - Why needed here: Understanding how AI combines textual and visual inputs for spatial reasoning
  - Quick check question: How does providing both textual rotation descriptions and visual diagrams help AI models understand spatial transformations better than either alone?

- Concept: Rotation matrix representation - mathematical description of 3D rotations
  - Why needed here: The study uses rotation matrices as supplementary information to improve AI accuracy
  - Quick check question: What information does a rotation matrix encode about a 3D transformation?

- Concept: Purdue Spatial Visualization Test (PSVT:R) - standardized assessment for spatial reasoning
  - Why needed here: The study uses this test as a baseline for evaluating AI spatial intelligence
  - Quick check question: What specific spatial abilities does the PSVT:R test measure?

## Architecture Onboarding

- Component map: AI vision models (GPT-4, Gemini 1.5 Pro, Llama 3.2) -> Input data (PSVT:R diagrams, AR visualizations) -> Supplementary information layers (coordinate systems, rotation angles, matrix equations) -> Spatial reasoning output

- Critical path: Input image → AI vision model processing → spatial reasoning output → accuracy measurement. The supplementary information layer is a critical enhancement that significantly improves the critical path's effectiveness

- Design tradeoffs: Static 2D diagrams vs. AR visualizations (simplicity vs. enhanced context), textual information vs. pure visual cues (explicit vs. implicit spatial encoding), single-axis vs. multi-axis rotations (simplicity vs. complexity)

- Failure signatures: Low accuracy on standard PSVT:R (13.3-23.3%), inability to identify similar rotation processes for different objects, failure to extract spatial information from visual inputs without textual support

- First 3 experiments:
  1. Test baseline AI accuracy on original PSVT:R diagrams without any supplementary information
  2. Add coordinate system axes to PSVT:R diagrams and measure accuracy improvement
  3. Test AI accuracy on single-axis rotations vs. multi-axis rotations to establish complexity thresholds

## Open Questions the Paper Calls Out
None explicitly called out in the provided content.

## Limitations
- Copyrighted Revised PSVT:R test materials cannot be openly shared, making exact reproduction difficult
- The dramatic accuracy improvement may reflect pattern matching with explicit textual cues rather than genuine spatial understanding
- Study only tested three large tech company models, limiting generalizability to other AI systems
- Static screenshots rather than interactive AR environments were used, potentially underestimating AR benefits

## Confidence
- High confidence: AI models struggle with spatial rotations from 2D images alone (13.3-23.3% accuracy)
- Medium confidence: Supplementary textual information dramatically improves accuracy, but may reflect training on similar mathematical notation
- Low confidence: Specific advantage of AR visualizations over static 2D images with equivalent textual information is not directly tested

## Next Checks
1. Test whether static 2D images with rotation matrix equations and angle descriptions achieve the same accuracy as AR visualizations, isolating AR-specific features versus textual supplementation
2. Test the same spatial rotation tasks with open-source vision-language models to determine if observed patterns are specific to large tech models
3. Evaluate model performance on spatial rotations of objects not found in training data to assess genuine spatial understanding versus pattern matching