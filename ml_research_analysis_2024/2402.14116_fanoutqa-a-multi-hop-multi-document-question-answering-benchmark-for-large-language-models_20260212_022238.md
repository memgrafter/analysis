---
ver: rpa2
title: 'FanOutQA: A Multi-Hop, Multi-Document Question Answering Benchmark for Large
  Language Models'
arxiv_id: '2402.14116'
source_url: https://arxiv.org/abs/2402.14116
tags:
- question
- answer
- wikipedia
- questions
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FanOutQA, a dataset of 1,034 multi-hop, multi-document
  question-answer pairs designed to evaluate large language models' ability to reason
  over a large number of entities. The dataset includes human-annotated decompositions
  of questions into sub-questions, each answerable from a single Wikipedia article.
---

# FanOutQA: A Multi-Hop, Multi-Document Question Answering Benchmark for Large Language Models
## Quick Facts
- arXiv ID: 2402.14116
- Source URL: https://arxiv.org/abs/2402.14116
- Reference count: 31
- A dataset of 1,034 multi-hop, multi-document question-answer pairs with human-annotated decompositions into single-document answerable sub-questions

## Executive Summary
FanOutQA is a benchmark dataset designed to evaluate large language models' ability to perform multi-hop reasoning across multiple documents. The dataset contains 1,034 questions about Wikipedia entities, each decomposed into sub-questions answerable from individual Wikipedia articles. Three evaluation settings are proposed: closed-book (no external knowledge), open-book (access to Wikipedia with retrieval tools), and evidence-provided (given relevant articles). The authors benchmark seven LLMs across these settings and compare their performance against human volunteers.

The results reveal that even state-of-the-art models struggle with multi-hop reasoning tasks, scoring below 50% in the closed- and open-book settings. Human volunteers significantly outperform models at 68.5% accuracy. The evidence-provided setting shows strong correlation between context length and performance, indicating current models have difficulty handling long contexts required for multi-hop reasoning. The dataset and associated tools are publicly available to encourage further research in this challenging area.

## Method Summary
The authors constructed FanOutQA by first generating questions about Wikipedia entities using GPT-4 with human oversight. Each question was then manually decomposed into sub-questions answerable from single Wikipedia articles by human annotators. The dataset includes both the original questions and their decompositions, enabling evaluation of multi-hop reasoning capabilities. Three benchmark settings were established: closed-book (no external knowledge), open-book (access to Wikipedia with retrieval tools), and evidence-provided (given relevant Wikipedia articles). The authors evaluated seven large language models across these settings and compared their performance against five human volunteers.

## Key Results
- Even best models score below 50% in closed- and open-book settings
- Human volunteers score 68.5% accuracy, significantly outperforming models
- Performance in evidence-provided setting correlates strongly with context length
- All models struggle with long-context and multi-hop reasoning despite access to relevant documents

## Why This Works (Mechanism)
The benchmark works by requiring models to perform multi-hop reasoning across multiple documents, which necessitates both information retrieval and logical inference. The human-annotated decompositions provide ground truth paths through the reasoning process, allowing evaluation of whether models can identify and follow the correct reasoning chain. The three settings progressively reduce the difficulty by controlling access to external knowledge, isolating different aspects of the reasoning challenge.

## Foundational Learning
- Multi-hop reasoning: Why needed - to evaluate complex logical inference across documents; Quick check - can the model correctly answer questions requiring information from multiple sources
- Context window limitations: Why needed - to understand architectural constraints affecting performance; Quick check - does performance improve with increased context length
- Retrieval-augmented generation: Why needed - to assess models' ability to find relevant information; Quick check - can the model retrieve and use appropriate documents from Wikipedia
- Decomposition annotation: Why needed - to provide ground truth for reasoning paths; Quick check - do human decompositions accurately represent the reasoning required

## Architecture Onboarding
**Component Map:** Question generation -> Human decomposition -> Model evaluation (3 settings) -> Performance analysis
**Critical Path:** Question generation → Decomposition annotation → Model benchmarking → Human evaluation comparison
**Design Tradeoffs:** Small scale (1,034 questions) vs. careful human annotation; Wikipedia-only entities vs. broader domain coverage; Three evaluation settings vs. single comprehensive setting
**Failure Signatures:** Poor retrieval leading to irrelevant context; Inability to chain information across multiple documents; Context window overflow preventing access to all relevant information
**3 First Experiments:**
1. Evaluate model performance on single-hop subsets of FanOutQA to establish baseline reasoning ability
2. Test models with artificially extended context windows to isolate context length effects
3. Compare model performance on human vs. GPT-4 generated questions to assess generation quality impact

## Open Questions the Paper Calls Out
None

## Limitations
- Relatively small scale with only 1,034 question-answer pairs limits generalizability
- Narrow scope restricted to Wikipedia entities may not represent broader multi-document reasoning challenges
- Human decomposition process introduces potential annotation biases affecting question difficulty
- Strong correlation between context length and performance suggests dataset may be overly sensitive to architectural constraints

## Confidence
- Dataset construction methodology: High
- Benchmark results across 7 LLMs: Medium
- Human evaluation methodology: Medium
- Claims about multi-hop reasoning difficulty: Medium

## Next Checks
1. Test model performance on a held-out subset of FanOutQA questions not used in initial benchmarking to verify result stability
2. Conduct inter-annotator reliability analysis on the human decompositions to quantify annotation consistency
3. Evaluate model performance on FanOutQA questions using non-Wikipedia sources to assess domain transfer capability