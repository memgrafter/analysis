---
ver: rpa2
title: 'BigDocs: An Open Dataset for Training Multimodal Models on Document and Code
  Tasks'
arxiv_id: '2412.04626'
source_url: https://arxiv.org/abs/2412.04626
tags:
- tasks
- bigdocs
- dataset
- data
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BigDocs-7.5M is a large-scale, license-permissive dataset of 7.5
  million multimodal documents across 30 tasks, curated to support document understanding
  and structured output generation for commercial applications. It includes tools
  for preprocessing, filtering, and metadata management, along with BigDocs-Bench,
  a suite of 10 novel tasks focusing on GUI reasoning and code generation (HTML, LaTeX,
  SVG, Markdown).
---

# BigDocs: An Open Dataset for Training Multimodal Models on Document and Code Tasks

## Quick Facts
- **arXiv ID:** 2412.04626
- **Source URL:** https://arxiv.org/abs/2412.04626
- **Reference count:** 40
- **Primary result:** BigDocs-7.5M improves document reasoning and structured output tasks by up to 25.8% over GPT-4o

## Executive Summary
BigDocs-7.5M is a large-scale, license-permissive dataset of 7.5 million multimodal documents across 30 tasks, designed to support document understanding and structured output generation for commercial applications. The dataset emphasizes accountability and transparency through filtering rules, traceable metadata, and careful content analysis. BigDocs-Bench, a suite of 10 novel tasks focusing on GUI reasoning and code generation (HTML, LaTeX, SVG, Markdown), enables evaluation of model performance on real-world use cases. Models trained on BigDocs-Bench outperform GPT-4o by up to 25.8% on document reasoning tasks, with human evaluations showing a 63-88% preference for BigDocs-trained model outputs over GPT-4o in tasks like Table2LaTeX and Screenshot2HTML generation.

## Method Summary
BigDocs-7.5M is curated from 16 public datasets and additional crawled data, with a focus on license permissibility and quality. The dataset is processed using the BigDocs Toolkit, which includes filtering, metadata tracking, and preprocessing tools. Models are trained using a three-stage process: continual pretraining (CPT) on BigDocs-7.5M, supervised finetuning (SFT) on DocDownstream, and SFT on BigDocs-Bench. The training uses 8 nodes of 8 H100 GPUs, AdamW optimizer, batch size 256, learning rate 2e-5, FSDP, and FlashAttention-2. The evaluation includes both standard document understanding benchmarks and the novel BigDocs-Bench tasks, with task-specific metrics like Tree Edit Distance and LST F1 for structured output tasks.

## Key Results
- BigDocs-Bench improves average performance by up to 25.8% over GPT-4o on document reasoning and structured output tasks
- Human evaluations show 63-88% preference for BigDocs-trained model outputs over GPT-4o in tasks like Table2LaTeX and Screenshot2HTML generation
- BigDocs-7.5M has lower contamination rates than DocStruct4M on most benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Training on BigDocs-7.5M improves performance on structured output tasks due to domain-specific pretraining. Continual pretraining on document-rich datasets aligns model representations with visual and structural patterns in text-heavy multimodal inputs, enhancing downstream task generalization. Core assumption: The pretraining corpus captures sufficient domain diversity and quality to transfer to novel structured output tasks. Evidence: Training with BigDocs-Bench improves average performance up to 25.8% over closed-source GPT-4o in document reasoning and structured output tasks. Break condition: If pretraining data lacks sufficient structural variety or contains high contamination, performance gains will not materialize.

### Mechanism 2
BigDocs-Bench introduces novel evaluation metrics for code generation tasks that are more sensitive to structural correctness than traditional metrics. Task-specific metrics like Tree Edit Distance for HTML and LST F1 for flowcharts capture structural alignment more precisely than generic similarity measures. Core assumption: These metrics correlate well with human judgments of code correctness. Evidence: For Screenshot2HTML, inspired by related works in this domain, we compute Tree Edit Distance (TE Dist.) between the Document Object Model (DOM) of ground truth and generation. Break condition: If task-specific metrics fail to correlate with human evaluation, they may not effectively measure model performance.

### Mechanism 3
The unified metadata framework enables reproducibility and transparency in dataset curation. Detailed metadata tracking (including licensing, transformations, and data origins) allows users to trace dataset provenance and assess quality. Core assumption: Comprehensive metadata significantly improves dataset usability and trustworthiness. Evidence: Our process emphasizes accountability, responsibility, and transparency through filtering rules, traceable metadata, and careful content analysis. Break condition: If metadata collection is incomplete or inconsistent, the framework's benefits are undermined.

## Foundational Learning

- **Multimodal alignment in vision-language models**
  - Why needed here: Understanding how image encoders and language models are integrated is crucial for grasping the model architecture and training pipeline described.
  - Quick check question: How does the connector component facilitate alignment between image features and LLM embeddings?

- **Continual pretraining vs. finetuning**
  - Why needed here: The paper describes a three-stage training process (pretraining, continual pretraining, and finetuning) that requires understanding these distinct phases.
  - Quick check question: What is the key difference between continual pretraining on BigDocs-7.5M and finetuning on BigDocs-Bench?

- **License permissibility and dataset curation**
  - Why needed here: The dataset's commercial viability depends on understanding licensing constraints and the curation process for ensuring permissibility.
  - Quick check question: Why is it important to track both image licenses and annotation licenses separately when curating a dataset?

## Architecture Onboarding

- **Component map:** Image encoder → Connector → Large Language Model
- **Critical path:** Data acquisition → Filtering (licensing, quality) → Metadata tracking → Model training (CPT → FT)
- **Design tradeoffs:** Balancing dataset size with quality and licensing compliance; choosing between generic pretraining and domain-specific continual pretraining
- **Failure signatures:** Performance degradation on novel tasks indicates insufficient domain coverage; high contamination rates suggest data quality issues
- **First 3 experiments:**
  1. Evaluate baseline model performance on BigDocs-Bench tasks without BigDocs training
  2. Train model on BigDocs-7.5M and evaluate on general document benchmarks
  3. Train model on BigDocs-Bench training split and evaluate on hidden test set to assess novel task capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the BigDocs dataset's contamination rate compare to other large-scale multimodal datasets, and what are the implications for model evaluation and deployment?
- Basis in paper: The paper discusses contamination assessment using CLIP embeddings and human evaluation, showing BigDocs-7.5M has lower contamination rates than DocStruct4M on most benchmarks.
- Why unresolved: The analysis focuses on specific benchmarks and may not capture all potential contamination sources. The long-term impact of residual contamination on model performance and real-world applications remains unclear.
- What evidence would resolve it: Comprehensive contamination analysis across a wider range of benchmarks and datasets, including a longitudinal study tracking contamination effects over time.

### Open Question 2
- Question: What is the optimal balance between VQA formatting, OCR tasks, and text-image alignment in the BigDocs dataset for maximizing model performance on diverse document understanding tasks?
- Basis in paper: The ablation experiments show varying impacts of these components on different tasks, suggesting trade-offs in their inclusion.
- Why unresolved: The optimal balance likely depends on the specific application and task requirements. The paper does not explore the full space of possible configurations or their effects on long-tail tasks.
- What evidence would resolve it: Systematic ablation studies varying the proportions of VQA, OCR, and captioning tasks, coupled with performance analysis across a diverse set of document understanding benchmarks.

### Open Question 3
- Question: How can the BigDocs dataset be extended to support even more complex and diverse document understanding tasks, such as multi-modal reasoning and long-form document summarization?
- Basis in paper: The paper introduces BigDocs-Bench with tasks like Chart2Caption and GUI2Summary, but these represent a limited subset of potential document understanding challenges.
- Why unresolved: The paper focuses on specific tasks and does not explore the full range of document understanding challenges, particularly those requiring advanced reasoning and summarization capabilities.
- What evidence would resolve it: Development and evaluation of new tasks in BigDocs-Bench that target multi-modal reasoning, long-form summarization, and other complex document understanding challenges, along with analysis of their impact on model performance.

## Limitations

- The evaluation framework relies heavily on synthetic data for BigDocs-Bench tasks, which may not fully capture real-world complexity and edge cases.
- The dataset's filtering pipeline is described but not fully open-sourced, making independent verification of quality and licensing claims challenging.
- The paper does not provide ablations showing the relative contribution of different data sources to final performance.

## Confidence

- **High Confidence:** The dataset curation methodology and metadata framework implementation (Sections 3-4)
- **Medium Confidence:** Performance improvements over GPT-4o (Section 5), given synthetic test sets and limited human evaluation
- **Low Confidence:** Generalization claims to commercial applications without real-world deployment data

## Next Checks

1. **Data Quality Verification:** Run CLIP-based similarity checks between training and test sets to quantify contamination risk, particularly for Screenshot2HTML and GUI2Code tasks.

2. **Metric Correlation Analysis:** Compare Tree Edit Distance and LST F1 metrics against human judgments on a larger sample to validate their effectiveness for structured output evaluation.

3. **Ablation Study:** Train models using subsets of BigDocs-7.5M (e.g., only document vs. only code tasks) to determine which data sources contribute most to performance gains on specific benchmarks.