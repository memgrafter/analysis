---
ver: rpa2
title: 'AdvisorQA: Towards Helpful and Harmless Advice-seeking Question Answering
  with Collective Intelligence'
arxiv_id: '2404.11826'
source_url: https://arxiv.org/abs/2404.11826
tags:
- advice
- advisorqa
- helpfulness
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdvisorQA is a dataset of 10,350 subjective, advice-seeking questions
  from the LifeProTips subreddit, each paired with multiple ranked answers. The ranking
  reflects collective intelligence from user upvotes, averaging 8.9 answers per question
  with 164.2 total votes.
---

# AdvisorQA: Towards Helpful and Harmless Advice-seeking Question Answering with Collective Intelligence

## Quick Facts
- **arXiv ID:** 2404.11826
- **Source URL:** https://arxiv.org/abs/2404.11826
- **Reference count:** 40
- **One-line primary result:** AdvisorQA dataset enables evaluation of LLMs on subjective advice, showing helpfulness gains but harmlessness trade-offs in RLHF.

## Executive Summary
AdvisorQA is a dataset of 10,350 subjective, advice-seeking questions from the LifeProTips subreddit, each paired with multiple ranked answers based on community upvotes. It enables evaluation of large language models on subjective decision-making in areas like social, career, and personal life. The dataset is used to train a helpfulness metric based on Plackett-Luce ranking and to assess harmlessness using a specialized toxicity model. Experiments show models trained on AdvisorQA improve in helpfulness but may lose harmlessness, highlighting trade-offs in reinforcement learning approaches.

## Method Summary
AdvisorQA was created by collecting 10,350 advice-seeking questions and their answers from the LifeProTips subreddit, with answers ranked by community upvotes. A Plackett-Luce ranking model was trained to predict helpfulness, and harmlessness was assessed using a specialized toxicity model. LLMs (Flan-T5, Llama-2, Mistral, GPT) were fine-tuned using supervised fine-tuning (SFT) and reinforcement learning from human feedback (PPO, DPO). Models were evaluated on both helpfulness (PL ranking) and harmlessness (LifeTox).

## Key Results
- AdvisorQA contains 10,350 subjective advice questions with 8.9 answers each, ranked by 164.2 average upvotes per thread.
- RLHF improves helpfulness but may reduce harmlessness; PPO increases diversity and empathy but risks safety, while DPO recovers safety.
- The Plackett-Luce model trained on upvote data outperforms baselines like BARTScore in ranking subjective advice.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The collective intelligence from Reddit upvotes provides a reliable proxy for "helpfulness" in subjective advice.
- **Mechanism:** High vote counts on answers within a community with shared norms indicate majority agreement on which advice is most useful. This collective signal can train and evaluate LLMs on subjective advice.
- **Core assumption:** Reddit users voting on advice are representative of the majority's subjective preference, and their votes are not systematically biased.
- **Evidence anchors:**
  - [abstract] "the answers for each question are ranked by an average of 164.2 votes per thread, which is a form of collective intelligence."
  - [section] "we collected the majority preferences from million-scale active users included in the community upvote system."
  - [corpus] Weak evidence; most related papers focus on LLMs in general, not specifically on the reliability of Reddit upvotes as a proxy.
- **Break condition:** If the Reddit community is not representative of broader human preferences, or if votes are manipulated or biased, the proxy fails.

### Mechanism 2
- **Claim:** The Plackett-Luce model can effectively rank subjective advice by learning from upvote data.
- **Mechanism:** PL ranking models can predict preferences among multiple advice options by learning from collective intelligence, outperforming simpler baselines like BARTScore.
- **Core assumption:** The structure of PL models can capture the nuances of subjective preferences in advice-seeking contexts.
- **Evidence anchors:**
  - [abstract] "we've completed a benchmark encompassing daily life questions, diverse corresponding responses, and majority vote ranking, which we use to train our helpfulness metric."
  - [section] "We employed the Plackett-Luce (PL) model, which learns the advice ranking from the training set and predicts the advice ranking in the test set."
  - [corpus] Weak evidence; most related papers focus on LLM alignment, not specifically on PL ranking for subjective advice.
- **Break condition:** If the upvote data is too noisy or the model overfits to specific community norms, ranking performance degrades.

### Mechanism 3
- **Claim:** RLHF trainers (PPO and DPO) have different impacts on helpfulness and harmlessness in subjective advice tasks.
- **Mechanism:** PPO explores through reward models, potentially increasing diversity and empathy but risking safety. DPO directly optimizes win/lose pairs, leading to safer but less diverse advice.
- **Core assumption:** The trade-offs between helpfulness and harmlessness are inherent to the RLHF methods themselves, not just the data.
- **Evidence anchors:**
  - [abstract] "Experiments on the two most harmless models show that SFT boosts helpfulness, but reduces harmlessness. The trend is amplified with RLHF using PPO, but most of the decline in harmlessness can be recovered with DPO."
  - [section] "PPO models explore through the reward model without demonstrations and maximizes its key portions, such as empathy, creativity, and actionability... DPO's safe learning is due to the higher proportion of safe instances in the training set."
  - [corpus] Weak evidence; most related papers focus on LLM alignment, not specifically on PPO vs DPO in subjective advice tasks.
- **Break condition:** If the reward models or demonstrations are flawed, the RLHF outcomes may not reflect the true trade-offs.

## Foundational Learning

- **Concept:** Collective Intelligence and Preference Aggregation
  - Why needed here: Understanding how to leverage community feedback (upvotes) to determine helpfulness in subjective domains.
  - Quick check question: How does upvote ranking differ from individual expert annotation in subjective tasks?

- **Concept:** Ranking Models and Preference Learning
  - Why needed here: The Plackett-Luce model is central to AdvisorQA's evaluation; understanding its mechanics is crucial.
  - Quick check question: What is the key difference between PL ranking and pairwise comparison models?

- **Concept:** Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: AdvisorQA's experiments with PPO and DPO highlight the impact of different RLHF approaches on model behavior.
  - Quick check question: How do PPO and DPO differ in their approach to optimizing model outputs?

## Architecture Onboarding

- **Component map:** AdvisorQA dataset -> Plackett-Luce model (helpfulness) -> LifeTox moderator (harmlessness) -> LLMs (Flan-T5, Llama-2, Mistral, GPT) -> SFT/PPO/DPO fine-tuning -> Evaluation
- **Critical path:** 1. Load AdvisorQA dataset 2. Train PL model on upvote rankings 3. Evaluate LLM-generated advice using PL and LifeTox 4. Apply SFT, PPO, or DPO for fine-tuning 5. Re-evaluate fine-tuned models
- **Design tradeoffs:** Using upvotes as a proxy vs. expert annotation (scalability vs. potential bias); focusing on helpfulness vs. harmlessness (trade-off in RLHF); training on safe vs. unsafe advice (broader research vs. safety risks)
- **Failure signatures:** PL model fails to predict preferences between top answers (indicates noisy data or model limitations); RLHF models become too safe or too harmful (indicates imbalance in training data or reward models); LLM-generated advice is too generic or repetitive (indicates lack of diversity in training data or methods)
- **First 3 experiments:** 1. Evaluate a base LLM (e.g., Llama-2-7B) on AdvisorQA using PL and LifeTox metrics. 2. Train the PL model on AdvisorQA's upvote rankings and assess its performance on a held-out set. 3. Apply SFT to the base LLM on AdvisorQA and compare its helpfulness and harmlessness to the base model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the collective intelligence framework in AdvisorQA generalize to other subjective domains beyond personal advice, such as creative writing or philosophical debates?
- Basis in paper: [explicit] The paper emphasizes AdvisorQA's use of collective intelligence through upvotes from the LifeProTips subreddit, suggesting it captures majority preferences for subjective advice.
- Why unresolved: The dataset is limited to advice-seeking questions from a single subreddit, so it's unclear if the upvote-based ranking method works for other subjective domains with different community norms and preferences.
- What evidence would resolve it: Testing the upvote ranking method on other subjective domains like creative writing prompts or philosophical discussion forums, and comparing the results to AdvisorQA's effectiveness.

### Open Question 2
- Question: How does the "first mover advantage" noise in upvote rankings affect the reliability of the helpfulness metric, and can this be mitigated through alternative ranking methods?
- Basis in paper: [explicit] The paper mentions that tail-ranked advice has considerable noise due to "first mover advantage," where early answers receive more visibility and upvotes regardless of quality.
- Why unresolved: The paper doesn't explore alternative ranking methods or statistical techniques to account for this noise, leaving the metric's reliability for less popular advice uncertain.
- What evidence would resolve it: Experimenting with time-adjusted ranking methods, randomization of answer visibility, or statistical debiasing techniques to reduce the impact of first-mover effects.

### Open Question 3
- Question: What is the optimal balance between safe and unsafe advice in the training data to maximize helpfulness while minimizing harm, and how does this balance vary across different subjective domains?
- Basis in paper: [inferred] The paper discusses mixing safe advice from LPT and unsafe advice from ULPT, noting that unsafe advice is easier to learn and that different RLHF methods (PPO vs DPO) have different safety profiles.
- Why unresolved: The paper doesn't systematically explore different ratios of safe to unsafe advice or test how this balance affects performance across various subjective domains.
- What evidence would resolve it: Conducting controlled experiments with varying ratios of safe to unsafe advice and testing performance across multiple subjective domains like career advice, relationship counseling, and creative feedback.

## Limitations
- The use of Reddit upvotes as a proxy for helpfulness may reflect community-specific norms rather than universal human preferences.
- The effectiveness of the LifeTox moderator in capturing all forms of harm is not fully validated, and its integration details are not provided.
- The RLHF experiments show helpfulness-harmlessness trade-offs, but the underlying reasons (e.g., reward model design, training data distribution) are not fully explored.

## Confidence
- **High:** The dataset and evaluation framework are well-constructed and novel.
- **Medium:** The mechanisms linking collective intelligence to universal helpfulness and the RLHF trade-offs require further empirical validation.
- **Low:** The generalizability of the upvote-based ranking method to other subjective domains and the impact of first-mover advantage on metric reliability are uncertain.

## Next Checks
1. Conduct a human evaluation study to compare the PL model's helpfulness rankings against expert annotations on a subset of AdvisorQA questions, to validate the upvote proxy.
2. Perform an ablation study on the RLHF reward models and training data to isolate the factors driving the helpfulness-harmlessness trade-off in PPO and DPO.
3. Test the generalizability of AdvisorQA-trained models on a different advice-seeking dataset (e.g., Quora) to assess whether the collective intelligence signal transfers across communities.