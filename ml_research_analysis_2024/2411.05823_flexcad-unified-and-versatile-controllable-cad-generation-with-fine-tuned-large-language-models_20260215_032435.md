---
ver: rpa2
title: 'FlexCAD: Unified and Versatile Controllable CAD Generation with Fine-tuned
  Large Language Models'
arxiv_id: '2411.05823'
source_url: https://arxiv.org/abs/2411.05823
tags:
- curve
- line
- loop
- mask
- face
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FlexCAD introduces a unified large language model (LLM)-based framework
  for controllable CAD generation across all construction hierarchies, including sketch-extrusion,
  extrusion, sketch, face, loop, and curve levels. The method represents CAD models
  as structured text and employs hierarchy-aware masking during fine-tuning to enable
  diverse control tasks within a single model.
---

# FlexCAD: Unified and Versatile Controllable CAD Generation with Fine-tuned Large Language Models

## Quick Facts
- **arXiv ID**: 2411.05823
- **Source URL**: https://arxiv.org/abs/2411.05823
- **Reference count**: 35
- **Key outcome**: FlexCAD achieves 91.02% prediction validity and highest realism scores in human evaluation for controllable CAD generation across all construction hierarchies using a unified LLM approach.

## Executive Summary
FlexCAD introduces a unified large language model (LLM)-based framework for controllable CAD generation across all construction hierarchies, including sketch-extrusion, extrusion, sketch, face, loop, and curve levels. The method represents CAD models as structured text and employs hierarchy-aware masking during fine-tuning to enable diverse control tasks within a single model. Extensive experiments on public datasets demonstrate that FlexCAD significantly improves generation quality and controllability compared to state-of-the-art baselines, achieving the highest prediction validity rates and realistic rates in human evaluations. The approach also supports iterative editing and can be extended to unconditional generation tasks.

## Method Summary
FlexCAD converts CAD models into structured text representations where each construction hierarchy (curve, loop, face, sketch, extrusion, sketch-extrusion) is represented as a sequence of text tokens with special end tokens marking hierarchy boundaries. During training, a hierarchy-aware field is masked with a special token that varies across epochs to represent different hierarchies, forcing the LLM to predict the masked field and learn to generate CAD content at various hierarchical levels. The model uses LoRA fine-tuning to adapt pre-trained LLMs (Llama-3-8B) with only 0.042% of parameters modified, enabling efficient adaptation while leveraging pre-trained knowledge. During inference, users can define which fields to mask for controllable generation, and the model autoregressively predicts the masked content.

## Key Results
- Achieves 91.02% prediction validity rate, significantly outperforming separate model approaches (81.57%)
- Highest realism scores in human evaluation compared to GPT-4o, SkexGen, and Hnc-cad baselines
- Successfully demonstrates iterative editing capabilities across all CAD construction hierarchies
- Shows 2.1% improvement in JSD (0.668 vs 0.685) and 4.5% improvement in COV (0.925 vs 0.886) over baselines

## Why This Works (Mechanism)

### Mechanism 1
The hierarchy-aware masking strategy enables LLMs to learn multiple controllable generation tasks across different CAD construction hierarchies in a unified model. During training, a hierarchy-aware field in the CAD text representation is masked with a special token that varies across epochs to represent different hierarchies. This forces the LLM to predict the masked field, effectively learning to generate CAD content at various hierarchical levels. The core assumption is that LLMs can learn to predict masked hierarchical CAD structures when trained with appropriately designed prompt templates that match the hierarchical context.

### Mechanism 2
Representing CAD models as structured text enables efficient processing and understanding by LLMs while maintaining CAD semantics. CAD models are converted into concise, structured text where each construction hierarchy is represented as a sequence of text tokens with special end tokens marking hierarchy boundaries. The core assumption is that LLMs can effectively process and generate structured text representations that preserve CAD semantics better than one-hot or binary representations.

### Mechanism 3
LoRA fine-tuning enables effective adaptation of pre-trained LLMs for CAD generation while maintaining computational efficiency. Instead of full fine-tuning, LoRA modifies only a small fraction (0.042%) of LLM parameters, allowing the model to learn CAD-specific generation while leveraging pre-trained knowledge. The core assumption is that the pre-trained LLM has acquired sufficient CAD-related knowledge during pre-training that can be effectively adapted with minimal parameter updates.

## Foundational Learning

- **Concept**: Transformer-based language models and attention mechanisms
  - Why needed here: The LLM backbone (Llama-3-8B) uses transformer architecture, which requires understanding of self-attention and positional encoding for effective fine-tuning.
  - Quick check question: How does multi-head self-attention in transformers enable the model to capture long-range dependencies in CAD text sequences?

- **Concept**: Masked language modeling and sequence prediction
  - Why needed here: The hierarchy-aware masking strategy relies on the LLM's ability to predict masked tokens in context, similar to BERT-style pretraining but with structured prompts.
  - Quick check question: What is the difference between standard masked language modeling and the hierarchy-aware masking used in FlexCAD?

- **Concept**: Structured text representation and tokenization
  - Why needed here: Converting CAD models to structured text requires understanding of how to represent hierarchical data in linear sequences while preserving semantic relationships.
  - Quick check question: How does the choice of tokenization strategy affect the LLM's ability to understand CAD hierarchies?

## Architecture Onboarding

- **Component map**: CAD model → Structured text representation → Hierarchy-aware masking + prompt templates → Masked CAD text → Pre-trained LLM + LoRA fine-tuning → Fine-tuned CAD generation model → Masked text + predictions → Infilled CAD text → Rendered CAD model

- **Critical path**: CAD model → Text representation → Masking → LLM prediction → Infilling → CAD model
  The most critical steps are the text representation quality and the masking strategy effectiveness.

- **Design tradeoffs**:
  - Text representation vs. one-hot encoding: Text is more concise but requires careful token design
  - LoRA vs. full fine-tuning: LoRA is more efficient but may limit adaptation capacity
  - Unified model vs. separate models: Unified is more practical but may be harder to train

- **Failure signatures**:
  - Poor prediction validity (PV) indicates issues with text representation or masking
  - Low realism scores suggest the LLM hasn't learned CAD semantics well
  - Training instability could indicate prompt template design problems

- **First 3 experiments**:
  1. Test text representation quality by comparing LLM predictions on different CAD structures
  2. Validate masking strategy by training on a single hierarchy before expanding to unified training
  3. Benchmark LoRA vs. full fine-tuning performance and efficiency tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
How does FlexCAD handle CAD models with more complex geometric structures, such as those involving multiple connected components or non-manifold geometries? The paper focuses on testing FlexCAD with a public dataset of 178,238 sketch-and-extrude sequences, but does not explicitly address its performance on more complex geometric structures.

### Open Question 2
What is the impact of increasing the size of the base LLM (e.g., using Llama-3-70B instead of Llama-3-8B) on the quality and controllability of CAD generation in FlexCAD? The paper mentions that using Llama-3-70B improves performance but is more time-consuming and costly, and suggests exploring larger models in future work.

### Open Question 3
How does FlexCAD perform in real-world CAD design scenarios, where user intent may be ambiguous or incomplete? The paper evaluates FlexCAD on a controlled dataset with clear user intents, but does not address its performance in more realistic, ambiguous scenarios.

## Limitations

- Training data limitations: Relies on DeepCAD dataset which may not represent real-world CAD diversity and has data imbalance issues, particularly for curves with 5-8 lines
- Generalization boundaries: Strong performance on DeepCAD benchmark but lacks evidence of cross-dataset generalization to different CAD domains or complexity levels
- Evaluation scope: Human evaluation uses limited criteria with relatively small evaluators, lacking detailed qualitative analysis of professional design standards

## Confidence

- **High Confidence**: The core mechanism of hierarchy-aware masking for unified CAD generation is well-supported by experimental results, showing consistent improvements over baselines in prediction validity and realism scores.
- **Medium Confidence**: The claim of "significant improvements" in generation quality is supported by quantitative metrics but limited by the evaluation scope and comparison methods.
- **Low Confidence**: The assertion that FlexCAD "unifies all tasks in a single model" without degradation is partially supported, but the paper does not thoroughly investigate potential performance tradeoffs compared to specialized models.

## Next Checks

**Validation Check 1**: Conduct cross-dataset validation by testing FlexCAD on CAD models from different sources (e.g., parametric CAD models from Fusion 360 or SolidWorks) to assess generalization beyond the DeepCAD dataset. Measure performance degradation and identify failure patterns.

**Validation Check 2**: Perform ablation studies on the hierarchy-aware masking strategy by comparing unified training against separate model training for each hierarchy, with detailed analysis of where and why the unified approach succeeds or fails.

**Validation Check 3**: Expand human evaluation with professional CAD designers to assess the practical usability of generated models, including criteria such as design intent preservation, editability, and compatibility with standard CAD workflows.