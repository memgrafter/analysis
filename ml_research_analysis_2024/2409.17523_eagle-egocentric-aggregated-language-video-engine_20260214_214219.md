---
ver: rpa2
title: 'EAGLE: Egocentric AGgregated Language-video Engine'
arxiv_id: '2409.17523'
source_url: https://arxiv.org/abs/2409.17523
tags:
- video
- arxiv
- egocentric
- understanding
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EAGLE introduces a unified framework for egocentric video understanding
  by integrating diverse tasks such as action recognition, procedure learning, and
  moment retrieval. It presents the first large-scale instruction-tuning dataset,
  EAGLE-400K, with 400K samples derived from Ego4D, EPIC-KITCHENS, and a new PTA dataset
  for procedural learning.
---

# EAGLE: Egocentric AGgregated Language-video Engine

## Quick Facts
- arXiv ID: 2409.17523
- Source URL: https://arxiv.org/abs/2409.17523
- Authors: Jing Bi; Yunlong Tang; Luchuan Song; Ali Vosoughi; Nguyen Nguyen; Chenliang Xu
- Reference count: 40
- Introduces first large-scale instruction-tuning dataset (EAGLE-400K) with 400K samples for egocentric video understanding

## Executive Summary
EAGLE presents a unified framework for egocentric video understanding that integrates diverse tasks including action recognition, procedure learning, and moment retrieval. The framework introduces EAGLE-400K, a large-scale instruction-tuning dataset derived from Ego4D, EPIC-KITCHENS, and a new PTA dataset, enabling comprehensive training of a video multimodal large language model (MLLM). The proposed model captures both spatial and temporal information using specialized adapters, demonstrating superior performance over existing models like Shikra, BLIP-2, and Video-LLaMA in understanding egocentric video content.

## Method Summary
The EAGLE framework unifies multiple egocentric video understanding tasks through a single video MLLM architecture. The core innovation lies in the integration of spatial and temporal adapters that enhance object detection and temporal boundary reasoning capabilities. The model is trained on EAGLE-400K, which combines existing egocentric datasets with newly collected procedural task annotations. Instruction-tuning is performed using a carefully designed pipeline that balances task diversity while maintaining model coherence across different egocentric video understanding scenarios.

## Key Results
- Achieves superior performance over existing models (Shikra, BLIP-2, Video-LLaMA) in egocentric video understanding tasks
- Demonstrates high accuracy, helpfulness, and detail metrics using newly proposed evaluation framework
- Successfully handles multiple tasks (action recognition, procedure learning, moment retrieval) within unified framework

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to capture both spatial and temporal dynamics in egocentric videos through specialized adapter architectures. The instruction-tuned model learns to interpret complex egocentric scenarios by leveraging diverse task training signals. The large-scale dataset provides sufficient variety for the model to generalize across different egocentric environments and activities.

## Foundational Learning

**Egocentric Video Understanding** - Why needed: Essential for analyzing first-person perspective videos in AR and real-world applications. Quick check: Model correctly identifies objects and actions from head-mounted camera footage.

**Multimodal Large Language Models** - Why needed: Enables integration of visual and temporal information with language understanding for complex video reasoning. Quick check: Model generates coherent descriptions that align with video content.

**Instruction-tuning** - Why needed: Allows models to follow natural language instructions for diverse video understanding tasks. Quick check: Model correctly executes varied instructions across different egocentric scenarios.

## Architecture Onboarding

Component Map: Input Video -> Spatial Adapter -> Temporal Adapter -> MLLM Backbone -> Task-specific Heads

Critical Path: Video encoding through spatial adapter → temporal adapter → unified representation → instruction-following output generation

Design Tradeoffs: Unified architecture vs. specialized models; larger dataset vs. annotation cost; adapter-based approach vs. full fine-tuning

Failure Signatures: Poor temporal boundary detection when actions span multiple scenes; object confusion in cluttered egocentric environments; instruction-following errors with complex multi-step commands

Three First Experiments:
1. Evaluate action recognition accuracy on Ego4D benchmark
2. Test moment retrieval precision on EPIC-KITCHENS dataset
3. Assess procedure learning performance on PTA dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on newly proposed metrics without standardized benchmark validation
- Technical details on adapter architecture and training procedures remain abstract
- Claims of being the "first large-scale instruction-tuning dataset" lack exhaustive verification of existing resources

## Confidence

| Claim | Confidence |
|-------|------------|
| Introduction of EAGLE-400K dataset | High |
| Superior performance over existing models | Medium |
| First large-scale instruction-tuning dataset claim | Low |

## Next Checks

1. Independent replication of EAGLE model's performance using standardized benchmarks like ActivityNet or Kinetics to verify superiority claims
2. Ablation studies removing proposed adapters to quantify their contribution to spatial and temporal understanding
3. Cross-dataset evaluation testing model generalization from EAGLE-400K to other egocentric video datasets like EGTEA or ADL