---
ver: rpa2
title: Extensive Self-Contrast Enables Feedback-Free Language Model Alignment
arxiv_id: '2404.00604'
source_url: https://arxiv.org/abs/2404.00604
tags:
- negative
- samples
- data
- responses
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Contrast, a feedback-free LLM alignment
  method that leverages self-generated negative responses instead of costly preference
  annotations. The method uses an embedding model to filter negatives from large sets
  of model-generated candidates and trains with Direct Preference Optimization.
---

# Extensive Self-Contrast Enables Feedback-Free Language Model Alignment

## Quick Facts
- arXiv ID: 2404.00604
- Source URL: https://arxiv.org/abs/2404.00604
- Authors: Xiao Liu; Xixuan Song; Yuxiao Dong; Jie Tang
- Reference count: 40
- Key outcome: Self-Contrast method outperforms SFT and standard DPO on three datasets using only self-generated negative responses without preference annotations.

## Executive Summary
This paper introduces Self-Contrast, a feedback-free alignment method that generates synthetic preference data by leveraging extensive self-generated negative responses instead of costly human preference annotations. The method uses an embedding model to filter out responses similar to the SFT target, treating the dissimilar ones as negatives for Direct Preference Optimization (DPO) training. Theoretically, the authors show that scaling negative samples can approximate the effect of balanced preference pairs under certain conditions. Empirically, Self-Contrast outperforms standard SFT and DPO on Nectar, UltraChat, and HH-RLHF datasets, with performance improving as more negative samples are added.

## Method Summary
Self-Contrast begins with SFT training on a base model (Mistral-7B-v0.1), then generates multiple responses per prompt from the SFT model. An embedding model (UAE-Large-V11) filters these responses by computing cosine similarity with the SFT target, excluding the most similar responses to avoid false negatives. The remaining dissimilar responses become negative samples for DPO training, with the SFT targets serving as positive samples. The method scales the number of negative samples (1, 2, 4, 8, or 16 per prompt) and adjusts the filtering threshold (a%) to optimize performance. This creates a preference dataset without requiring human annotations or external model responses.

## Key Results
- Self-Contrast outperforms standard SFT and DPO on Nectar, UltraChat, and HH-RLHF datasets
- Performance improves with more negative samples (up to 16 per prompt)
- Achieves competitive results to methods trained with preference data while using only self-generated negatives
- Shows consistent gains on MT-Bench and Alpaca-Eval evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing negative samples can approximate the effect of balanced preference pairs
- Mechanism: Leverages the theoretical insight that negative responses are more diverse than positive ones, so adding more negatives can reduce variance in gradient estimation and approach performance of balanced preference pairs
- Core assumption: Negative responses are more varied than positive ones (Assumption 1)
- Evidence anchors:
  - [abstract]: "merely scaling negative responses can still effectively approximate situations with more balanced positive and negative preference annotations"
  - [section 2.3]: "by leveraging massive self-generated negative responses, we can effectively approximate the gradient effect of standard preference pairs"
- Break condition: If positive and negative responses have similar variance, or if negative responses are not sufficiently diverse

### Mechanism 2
- Claim: Self-generated negatives can be effectively filtered using embedding similarity
- Mechanism: Uses pre-trained embedding model to calculate cosine similarity between self-generated responses and SFT target, filtering out similar responses to avoid false negatives
- Core assumption: Responses similar to SFT target are more likely to be positive samples
- Evidence anchors:
  - [section 2.4]: "responses similar to the SFT target are more likely to be positive samples and therefore better to be excluded for training"
  - [section 4.1]: "negative rewards that are smaller than the SFT target reward as true negatives"
- Break condition: If embedding model doesn't capture semantic similarity well, or if negative samples are not sufficiently dissimilar to target

### Mechanism 3
- Claim: Self-generated negatives are more specific to model's own behavior than externally generated ones
- Mechanism: Uses model's own outputs as negative samples rather than responses from other models, potentially more aligned with specific patterns and biases of the model being trained
- Core assumption: A model's own outputs are more representative of its behavior than outputs from other models
- Evidence anchors:
  - [section 3.2]: "our methods outperform DPOstd where use responses generated by other models"
  - [section 3.1]: "use of the model's own output as negative samples... plays a crucial role"
- Break condition: If model's outputs are not representative of overall behavior, or if other models provide more diverse or challenging negatives

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: The paper uses DPO as optimization method for training with synthetic preference data
  - Quick check question: What is the key difference between DPO and standard RLHF approaches in terms of how they use preference data?

- Concept: Cosine similarity in embedding space
  - Why needed here: Method uses cosine similarity between response embeddings to filter negative samples
  - Quick check question: How does cosine similarity differ from Euclidean distance when measuring similarity between embeddings?

- Concept: Variance in gradient estimation
  - Why needed here: Theoretical justification relies on understanding how adding more negative samples reduces variance in gradient estimates
  - Quick check question: In optimization, how does reducing variance in gradient estimates affect convergence and stability?

## Architecture Onboarding

- Component map: SFT training → Response generation → Embedding filtering → DPO training
- Critical path: SFT training → Response generation → Embedding filtering → DPO training
- Design tradeoffs:
  - More negative samples vs. computational cost of generating them
  - Stricter filtering (higher a%) vs. retaining challenging negatives
  - Using model's own outputs vs. externally generated responses
- Failure signatures:
  - Performance plateaus despite adding more negatives (saturation)
  - MT-Bench scores decrease when increasing negative sample count (overfitting to easy negatives)
  - Model becomes too conservative or refuses to answer (over-alignment)
- First 3 experiments:
  1. Test effect of negative sample count (1, 2, 4, 8, 16) on a small dataset
  2. Compare embedding-based filtering with random filtering to measure effectiveness
  3. Test different values of a% (100%, 75%, 50%, 25%) to find optimal filtering threshold

## Open Questions the Paper Calls Out
None

## Limitations

- Theoretical foundation relies heavily on assumption that negative responses are inherently more diverse than positive ones, which may not hold for all model architectures or alignment tasks
- Empirical validation is constrained to relatively small-scale experiments with a single base model (Mistral-7B-v0.1) and three datasets, limiting generalizability
- Filtering mechanism using embedding similarity introduces additional dependency on quality and alignment of pre-trained embedding model, which could introduce biases or errors

## Confidence

- **High Confidence**: Empirical results showing Self-Contrast outperforms standard SFT and DPO on tested datasets are robust, with clear performance metrics and statistical significance
- **Medium Confidence**: Theoretical claim that scaling negative samples approximates balanced preference pairs is plausible but relies on specific assumptions about response diversity that may not generalize
- **Low Confidence**: Claim that self-generated negatives are inherently more effective than externally generated ones lacks direct empirical validation and could be dataset or model-specific

## Next Checks

1. **Diversity Analysis**: Conduct systematic analysis of variance in positive vs. negative responses across different model architectures and datasets to validate Assumption 1, including measuring semantic diversity, topic coverage, and stylistic variation

2. **Cross-Model Validation**: Test Self-Contrast using externally generated negatives from multiple different models (e.g., Llama, GPT-4) to determine whether claimed advantage of self-generated negatives is consistent or model-specific

3. **Failure Mode Exploration**: Design targeted experiments to identify scenarios where Self-Contrast underperforms, such as prompts with inherently low response diversity or tasks requiring nuanced preference judgments, and develop diagnostic metrics for early detection