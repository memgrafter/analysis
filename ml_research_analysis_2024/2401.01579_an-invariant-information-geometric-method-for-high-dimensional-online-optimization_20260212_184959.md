---
ver: rpa2
title: An Invariant Information Geometric Method for High-Dimensional Online Optimization
arxiv_id: '2401.01579'
source_url: https://arxiv.org/abs/2401.01579
tags:
- optimization
- invariant
- syncma
- information
- optimizers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INVIGO, a fully invariant optimizer framework
  for online optimization with zeroth-order feedback and ignorant initial. The key
  idea is to approximate the IGO objective with a KL-divergence term, allowing for
  differentiable optimization and stable incorporation of historical information via
  a line search strategy.
---

# An Invariant Information Geometric Method for High-Dimensional Online Optimization

## Quick Facts
- arXiv ID: 2401.01579
- Source URL: https://arxiv.org/abs/2401.01579
- Authors: Zhengfei Zhang; Yunyue Wei; Yanan Sui
- Reference count: 39
- Key outcome: INVIGO framework with SynCMA optimizer achieves competitive sample efficiency and outperforms state-of-the-art Bayesian optimization and evolution strategies on high-dimensional tasks

## Executive Summary
This paper introduces INVIGO, a fully invariant optimizer framework for online optimization with zeroth-order feedback and ignorant initial conditions. The key innovation is approximating the Information Geometric Optimization (IGO) objective with a KL-divergence term, enabling differentiable optimization while maintaining invariance under smooth bijective transformations. When applied to multi-dimensional Gaussian distributions, the resulting SynCMA optimizer demonstrates superior performance on high-dimensional tasks including Mujoco locomotion, rover planning, and synthetic functions, achieving competitive sample efficiency against state-of-the-art methods like TuRBO.

## Method Summary
The method derives from the natural gradient flow on Gaussian distributions, approximating the IGO objective with a KL-divergence term that shares the same natural gradient at the current parameter point. SynCMA maintains fully invariant updates by evolving a self-contained term M^t(θ) that accumulates weighted gradient statistics from past generations, preserving all gradient information while keeping computational cost manageable. The optimizer uses synchronous updates (mean and covariance evolve together) and incorporates historical information through a line search strategy, achieving sample efficiency competitive with TuRBO on high-dimensional tasks.

## Key Results
- SynCMA achieves competitive sample efficiency versus TuRBO and other ES methods on high-dimensional Mujoco locomotion tasks
- Demonstrates superior performance on rover planning and synthetic function benchmarks
- Maintains fully invariant updates under smooth bijective transformations of the parameter space
- Incorporates historical information through self-evolved term M^t while preserving computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SynCMA maintains fully invariant updates by approximating the IGO objective with a KL-divergence term that shares the same natural gradient at the current parameter point
- Mechanism: The paper rewrites the IGO objective log L_θ as a sum of KL divergences, then uses the KL divergence between the reweighted distribution q_θ^t and the model p_θ as a surrogate objective. Because the natural gradients of log L_θ and -DKL(q_θ^t||p_θ) coincide at θ_t, and the gradient field is approximately the same in a small neighborhood, SynCMA can update in a way that remains invariant under smooth bijective transformations of the parameter space
- Core assumption: The fitness function g_f,θ^t is non-negative and integrable, and the KL approximation error is O(δθ) locally
- Evidence anchors: [abstract]: "approximates the IGO objective with a KL-divergence term, allowing for differentiable optimization and stable incorporation of historical information via a line search strategy"; [section 2.2]: Proposition 3 proves the natural gradients coincide at θ_t and the gradient fields are approximately equal nearby

### Mechanism 2
- Claim: SynCMA fully incorporates historical information by evolving a self-contained term M^t(θ) that accumulates weighted gradient statistics from past generations
- Mechanism: The algorithm introduces a self-evolved term M^t(θ) whose gradient is updated recursively using a weighted sum of past fitness gradients and rank-one updates. This term replaces the infinite sum of past KL divergences, preserving all gradient information while keeping computational cost O(HN) per iteration
- Core assumption: Assumption 2 holds (fitness function and Lagrange multiplier are parameterization-independent), and the recursive update of M^t(θ) accurately tracks historical information
- Evidence anchors: [abstract]: "stable incorporation of historical information via a line search strategy"; [section 3.1]: Derivation of M^t(θ) and its updates in equations (14)-(20)

### Mechanism 3
- Claim: SynCMA achieves sample efficiency competitive with TuRBO on high-dimensional tasks by combining invariant optimization with a synchronous update rule that directly follows the natural gradient flow
- Mechanism: By maintaining a fully invariant update rule (Mechanism 1) and incorporating all past gradient information (Mechanism 2), SynCMA avoids the parameter-dependent inefficiencies of non-invariant ES methods. The synchronous update (mean and covariance evolve together) preserves the exact structure required for natural gradient updates on Gaussian distributions, leading to faster convergence in high dimensions
- Core assumption: The problem domain allows Gaussian modeling, and the true objective has structure that the Gaussian search distribution can exploit
- Evidence anchors: [abstract]: "demonstrates great competence over other algorithms in sample efficiency" and competitive performance vs TuRBO; [section 4.1, 4.2, 4.3]: Experimental results showing SynCMA matches or beats TuRBO and other ES methods on Mujoco, rover planning, and synthetic benchmarks

## Foundational Learning

- Concept: Natural gradient and information geometry
  - Why needed here: SynCMA is derived from the natural gradient flow on the space of Gaussian distributions; understanding how the Fisher information metric defines invariant updates is essential
  - Quick check question: What property of the Fisher information matrix makes natural gradient invariant under reparameterization?

- Concept: KL divergence as a Bregman divergence on probability distributions
  - Why needed here: The paper uses KL divergence to construct a surrogate objective; knowing its properties (e.g., it is always non-negative, zero iff distributions match) is key to understanding the approximation
  - Quick check question: Under what conditions does minimizing KL divergence between two Gaussians correspond to matching their means and covariances?

- Concept: Evolution strategies and covariance matrix adaptation
  - Why needed here: SynCMA is a variant of CMA-ES; familiarity with how CMA-ES adapts the covariance matrix and uses rank-one and rank-μ updates is necessary to follow the derivation
  - Quick check question: In CMA-ES, how do the evolution paths for mean and covariance differ in how they accumulate information over generations?

## Architecture Onboarding

- Component map: Parameter vector θ = (m, C) -> Fitness function g_f,θ^t(x) -> Self-evolved term M^t(θ) -> KL divergence constraint -> Sampling module
- Critical path:
  1. Sample N points from current Gaussian
  2. Evaluate fitness and compute normalized weights
  3. Update self-evolved term M^t using recursive equations
  4. Solve constrained optimization (natural Lagrange condition) for next θ
  5. Sample again from updated Gaussian
- Design tradeoffs:
  - Using KL divergence instead of exact IGO objective trades exactness for differentiability and scalability
  - Historical information via M^t increases memory and computation slightly but improves sample efficiency
  - Synchronous updates preserve invariance but require careful derivation of update rules
- Failure signatures:
  - Divergence in covariance matrix (eigenvalues blow up) -> step size or λ tuning issue
  - Slow progress on ill-conditioned functions -> check rank-one vs rank-μ balance
  - Poor performance on discrete or highly non-Gaussian-structured problems -> Gaussian assumption broken
- First 3 experiments:
  1. Run SynCMA on a simple 2D quadratic with known optimum; verify mean converges and invariance holds under linear reparameterization
  2. Compare sample efficiency of SynCMA vs CMA-ES on a 64D ill-conditioned sphere; plot convergence curves
  3. Test sensitivity of SynCMA to λ0 on a Rastrigin function; plot performance vs λ0 grid

## Open Questions the Paper Calls Out

- Open Question 1: What is the impact of using different parametric distribution families beyond multi-dimensional Gaussians in the INVIGO framework?
  - Basis in paper: [explicit] The paper mentions that INVIGO currently only works for certain parametric distribution families that IGO set and it is possible to generalize to a broader family of models such as neural networks
  - Why unresolved: The paper does not provide any experiments or theoretical analysis on the performance of INVIGO with other distribution families
  - What evidence would resolve it: Experiments comparing the performance of INVIGO with different parametric distribution families on various optimization tasks

- Open Question 2: How does the performance of SynCMA change with different values of the hyperparameters λ0, ηm, and ηc?
  - Basis in paper: [explicit] The paper mentions that SynCMA uses constant hyperparameters λ0, ηm, and ηc in the experiments and suggests that they can be invariant variables with evolving rules for better optimization performance
  - Why unresolved: The paper only provides ablation studies on λ0 and does not explore the impact of ηm and ηc on SynCMA's performance
  - What evidence would resolve it: Experiments varying the values of λ0, ηm, and ηc and analyzing their impact on SynCMA's performance on various tasks

- Open Question 3: How does SynCMA perform in high-dimensional tasks with non-smooth, discontinuous, or ill-conditioned objective functions?
  - Basis in paper: [explicit] The paper mentions that SynCMA demonstrates competitive performance on tasks with such characteristics in the synthetic functions section, but does not provide a detailed analysis
  - Why unresolved: The paper does not provide a thorough analysis of SynCMA's performance on these types of functions in high-dimensional settings
  - What evidence would resolve it: Experiments evaluating SynCMA's performance on high-dimensional tasks with non-smooth, discontinuous, or ill-conditioned objective functions, comparing it to other optimizers

## Limitations
- The KL-divergence approximation's accuracy in highly curved or non-Gaussian regions remains unvalidated
- Historical information incorporation via M^t depends critically on λ tuning, but sensitivity analysis is minimal
- The claim of invariance under arbitrary smooth bijections is theoretically proven but not empirically tested beyond linear transformations

## Confidence

- Mechanism 1 (invariance): Medium - theoretical proof exists but empirical validation is limited
- Mechanism 2 (historical info): Low-Medium - derivation is sound but practical impact depends heavily on hyperparameters
- Mechanism 3 (sample efficiency): Medium - competitive results shown but baseline implementations and hyperparameter tuning details are sparse

## Next Checks

1. Test SynCMA's invariance property under nonlinear parameter transformations (e.g., exponential, logarithmic) on synthetic benchmarks and verify convergence behavior remains unchanged
2. Conduct a systematic ablation study removing the historical information term M^t to quantify its contribution to sample efficiency gains
3. Evaluate SynCMA on high-dimensional problems with non-Gaussian optimal distributions (e.g., multimodal or heavy-tailed objectives) to identify breakdown conditions of the Gaussian assumption