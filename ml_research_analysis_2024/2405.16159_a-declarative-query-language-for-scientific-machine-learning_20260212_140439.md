---
ver: rpa2
title: A Declarative Query Language for Scientific Machine Learning
arxiv_id: '2405.16159'
source_url: https://arxiv.org/abs/2405.16159
tags:
- learning
- data
- machine
- language
- generate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MQL (Machine Learning Query Language), a
  declarative query language designed to make machine learning accessible to non-experts.
  MQL allows users to perform prediction, classification, and clustering tasks using
  a SQL-like syntax without requiring low-level algorithmic knowledge.
---

# A Declarative Query Language for Scientific Machine Learning

## Quick Facts
- arXiv ID: 2405.16159
- Source URL: https://arxiv.org/abs/2405.16159
- Reference count: 40
- Primary result: MQL enables scientists to perform sophisticated ML analysis using intuitive, high-level queries while the system handles algorithmic complexity automatically

## Executive Summary
This paper introduces MQL (Machine Learning Query Language), a declarative query language designed to make machine learning accessible to non-experts. MQL allows users to perform prediction, classification, and clustering tasks using a SQL-like syntax without requiring low-level algorithmic knowledge. The language features three main constructs: GENERATE for querying and analysis, CONSTRUCT for creating ML models, and INSPECT for data preparation. The author demonstrates MQL's effectiveness through two materials science applications: quantum dye design and membrane bending modulus prediction, showing how complex ML tasks can be expressed simply.

## Method Summary
MQL uses a translational semantics approach that maps high-level declarative statements to SciKit-Learn implementations. The system implements three language constructs: GENERATE for querying and analysis, CONSTRUCT for creating ML models, and INSPECT for data preparation. MQL statements are translated into equivalent SciKit-Learn code through a mapping algorithm. The implementation uses Python with Pandas and SciKit-Learn, with plans for PostgreSQL integration using UDFs. The language follows a three-tier structure (data preparation, model construction, ML analysis) that reflects natural scientific workflows.

## Key Results
- MQL successfully demonstrates quantum dye design using ML to predict quantum efficiency from molecular properties
- MQL accurately predicts membrane bending modulus from molecular dynamics simulation data using a Graph Neural Network approach
- The declarative syntax allows complex ML tasks to be expressed in simple queries like "GENERATE PREDICTION Y FROM X"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MQL reduces the barrier to machine learning by providing a declarative syntax that abstracts away algorithmic complexity.
- Mechanism: By mapping high-level MQL statements to SciKit-Learn implementations, the system automatically handles model selection, training, and evaluation without requiring users to write procedural code.
- Core assumption: Users can express their ML needs through intuitive SQL-like constructs while the system translates these into appropriate algorithms.
- Evidence anchors:
  - [abstract] "MQL allows users to perform prediction, classification, and clustering tasks using a SQL-like syntax without requiring low-level algorithmic knowledge"
  - [section] "MQL retains part of SQL flavor to leverage the community knowledge of SQL and reduce cognitive overload"
- Break condition: If the translation mapping fails to capture the nuances of specific ML tasks, the abstraction breaks down and users must revert to procedural programming.

### Mechanism 2
- Claim: The translational semantics approach enables MQL to leverage existing, well-tested ML frameworks while providing a simpler interface.
- Mechanism: The system uses a mapping algorithm that translates MQL statements into SciKit-Learn code, allowing MQL to benefit from the maturity and performance of established ML libraries.
- Core assumption: The SciKit-Learn framework provides sufficient coverage of common ML tasks to serve as a reliable backend for MQL.
- Evidence anchors:
  - [abstract] "The system is implemented using a translational approach mapping MQL to SciKit-Learn"
  - [section] "we choose SciKit Learn for its excellent support for tabular data analysis using traditional ML tasks, and the ease of use"
- Break condition: If SciKit-Learn lacks algorithms needed for specific scientific applications, the translation approach becomes limiting and requires framework extensions.

### Mechanism 3
- Claim: MQL's three-tier language structure (data preparation, model construction, ML analysis) provides a natural workflow for scientific applications.
- Mechanism: The language constructs allow users to prepare data, build models, and perform analysis in a logical sequence, with dependencies automatically managed by the system.
- Core assumption: Scientific ML workflows naturally follow the sequence of data preparation, model construction, and analysis.
- Evidence anchors:
  - [abstract] "The language features three main constructs: GENERATE for querying and analysis, CONSTRUCT for creating ML models, and INSPECT for data preparation"
  - [section] "We stagger the language constructs in three tiers – data preparation (or wrangling), model construction, and ML analysis"
- Break condition: If scientific workflows require non-linear or iterative approaches, the three-tier structure may constrain rather than facilitate analysis.

## Foundational Learning

- Concept: Declarative programming paradigm
  - Why needed here: MQL is built on declarative principles where users specify what they want rather than how to achieve it
  - Quick check question: How does declarative programming differ from imperative programming in terms of specifying computational tasks?

- Concept: Relational algebra and SQL fundamentals
  - Why needed here: MQL syntax is designed to be SQL-like, leveraging users' familiarity with relational database concepts
  - Quick check question: What are the key differences between MQL's GENERATE statement and SQL's SELECT statement in terms of functionality?

- Concept: Basic machine learning concepts (supervised vs unsupervised learning, classification vs regression)
  - Why needed here: Understanding these concepts helps users choose appropriate MQL constructs and interpret results
  - Quick check question: How would you decide between using PREDICTION versus CLASSIFICATION in an MQL query based on your data characteristics?

## Architecture Onboarding

- Component map:
  - MQL Parser -> Translation Engine -> SciKit-Learn Backend -> Visualization Module -> Data Wrangler

- Critical path: MQL statement → Parser → Translation Engine → SciKit-Learn → Results → Visualization

- Design tradeoffs:
  - Flexibility vs simplicity: MQL prioritizes simplicity over fine-grained control of algorithms
  - Framework dependency: Tightly coupled to SciKit-Learn, limiting support for deep learning frameworks
  - Performance overhead: Translation layer adds computational overhead compared to direct SciKit-Learn usage

- Failure signatures:
  - Syntax errors in MQL statements that the parser cannot handle
  - Translation failures when mapping MQL constructs to unavailable SciKit-Learn functionality
  - Performance bottlenecks when handling large datasets through the translation layer
  - Visualization failures when generating plots for complex analysis results

- First 3 experiments:
  1. Implement a basic PREDICTION query on the Boston housing dataset using MQL's GENERATE statement
  2. Create a CLASSIFICATION model for a simple binary classification task using CONSTRUCT and GENERATE
  3. Test the INSPECT statement for data wrangling by cleaning and preparing a sample dataset with missing values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MQL be effectively implemented using multiple ML frameworks beyond SciKit-Learn, such as PyTorch, TensorFlow, and R?
- Basis in paper: [explicit] The paper states "While we present a mapping to SciKit Learn for its implementation as a proof of concept, we note that more needs to be done to make MQL a viable system for serious ML platform" and demonstrates a PyTorch translation for GNN models in the membrane bending modulus prediction experiment.
- Why unresolved: The paper only provides proof-of-concept implementations with SciKit-Learn and one PyTorch example, without comprehensive testing across multiple frameworks.
- What evidence would resolve it: A comprehensive implementation and benchmarking of MQL across multiple ML frameworks (PyTorch, TensorFlow, R, etc.) showing consistent performance and functionality.

### Open Question 2
- Question: How can an MQL query optimizer automatically select the most appropriate ML algorithm for a given task, dataset, and user requirements?
- Basis in paper: [explicit] The paper discusses "There are numerous ML frameworks and a large number of ML algorithms that are suitable for applications on a case by case basis" and acknowledges "it is imperative that an MQL query optimizer be developed to identify candidate algorithms most relevant to a specific ML task."
- Why unresolved: The paper recognizes this need but does not provide a concrete solution or implementation for such an optimizer.
- What evidence would resolve it: A working MQL query optimizer that can automatically select optimal ML algorithms based on dataset characteristics, task requirements, and performance metrics.

### Open Question 3
- Question: What is the optimal balance between declarative simplicity and algorithmic flexibility in MQL's design?
- Basis in paper: [inferred] The paper discusses trade-offs between SQL-like declarative syntax and the need for algorithmic specificity, noting that languages like sql4ml require procedural code segments that defeat the purpose of declarative languages.
- Why unresolved: The paper presents MQL's current design but doesn't empirically evaluate the trade-offs between simplicity and flexibility through user studies or comparative analysis.
- What evidence would resolve it: Empirical studies comparing MQL's declarative approach with more flexible but complex alternatives, measuring both usability and performance across different user expertise levels.

## Limitations

- The complete implementation details of the MQL translator are not fully specified, making it difficult to assess the robustness of the translation layer
- Tight coupling to SciKit-Learn limits the system's ability to handle deep learning tasks or more advanced ML algorithms
- The paper demonstrates success with two specific materials science applications, but broader validation across diverse scientific domains is needed

## Confidence

- **High**: The conceptual framework of using declarative syntax to simplify ML for non-experts is well-founded and addresses a genuine need in the scientific community
- **Medium**: The demonstration of two specific applications (quantum dye design and membrane bending modulus prediction) provides concrete evidence, though limited in scope
- **Low**: The complete implementation details of the translation mechanism and the PostgreSQL integration remain unspecified, making it difficult to evaluate the system's full capabilities

## Next Checks

1. **Translation Accuracy Validation**: Implement a comprehensive test suite that compares MQL-generated models against equivalent hand-written SciKit-Learn code across multiple ML tasks to verify the translation layer produces correct and equivalent results.

2. **Scalability Assessment**: Test MQL's performance and memory usage with increasingly large datasets (10K to 1M+ rows) to identify any bottlenecks in the translation layer or backend execution that could limit practical usability.

3. **Algorithm Coverage Analysis**: Create a systematic evaluation of which SciKit-Learn algorithms and configurations can be expressed through MQL's declarative syntax, identifying gaps where users might need to drop to procedural programming.