---
ver: rpa2
title: What Makes a Maze Look Like a Maze?
arxiv_id: '2409.08202'
source_url: https://arxiv.org/abs/2409.08202
tags:
- concept
- visual
- image
- schema
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Deep Schema Grounding (DSG), a framework
  for visual reasoning over abstract concepts by using explicit dependency graph descriptions
  (schemas) that decompose concepts into more primitive-level symbols. DSG leverages
  large language models to extract schemas and vision-language models to hierarchically
  ground concrete to abstract components of the schema onto images.
---

# What Makes a Maze Look Like a Maze?

## Quick Facts
- arXiv ID: 2409.08202
- Source URL: https://arxiv.org/abs/2409.08202
- Reference count: 19
- Primary result: DSG improves visual abstraction understanding, achieving 6.6 percentage point improvement over GPT-4o on the Visual Abstractions Benchmark

## Executive Summary
This paper introduces Deep Schema Grounding (DSG), a framework for visual reasoning over abstract concepts by using explicit dependency graph descriptions (schemas) that decompose concepts into more primitive-level symbols. DSG leverages large language models to extract schemas and vision-language models to hierarchically ground concrete to abstract components of the schema onto images. The authors evaluate DSG on their new Visual Abstractions Dataset, which consists of diverse, real-world images of abstract concepts and corresponding question-answer pairs labeled by humans. They show that DSG significantly improves the abstract visual reasoning performance of vision-language models, with a 6.6 percentage point improvement over GPT-4o on the Visual Abstractions Benchmark, and a 10 percentage point improvement for counting questions.

## Method Summary
Deep Schema Grounding (DSG) uses large language models to extract schemas for abstract concepts, which are then hierarchically grounded onto images using vision-language models. The framework first extracts dependency graph descriptions (schemas) that decompose abstract concepts into primitive-level symbols and their dependencies. It then uses VLMs to hierarchically ground each component in the concept DAG, starting with concrete components and using them as conditions to ground more abstract components. Finally, the grounded schema components are incorporated into the final question-answering prompt, giving the VLM holistic context about the image to improve abstract visual reasoning performance.

## Key Results
- DSG achieves 6.6 percentage point improvement over GPT-4o on the Visual Abstractions Benchmark
- DSG shows 10 percentage point improvement for counting questions
- DSG consistently improves performance of vision-language models across all types of questions on the Visual Abstractions Benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs possess structured knowledge of abstract concepts that can be extracted as schemas
- Mechanism: The LLM is prompted with a simple instruction and example schema to generate a decomposition of an abstract concept into primitive-level components and their dependencies
- Core assumption: LLMs trained on extensive human language data contain human-aligned schemas for abstract concepts
- Evidence anchors: [abstract] "DSG uses large language models to extract schemas", [section 3.2] "we hypothesize that they contain human-aligned schemas for a wide range of abstract concepts"
- Break condition: If LLMs lack sufficient training data on the target abstract concepts, schema extraction will fail

### Mechanism 2
- Claim: Hierarchically grounding schema components improves VLM performance on abstract visual reasoning
- Mechanism: DSG first grounds concrete components (no dependencies), then uses those as conditions to ground abstract components, providing contextual information for final reasoning
- Core assumption: Abstract concepts require understanding of their concrete subcomponents before reasoning about the whole
- Evidence anchors: [section 3.3] "DSG hierarchically grounds each component in the concept DAG", [section 5] "DSG significantly outperforms VLMs...across all types of questions"
- Break condition: If VLMs cannot handle conditional grounding or spatial relationships, hierarchical grounding will not improve performance

### Mechanism 3
- Claim: Providing resolved schemas as context improves VLM's abstract visual reasoning performance
- Mechanism: The grounded schema components are incorporated into the final question-answering prompt, giving the VLM holistic context about the image
- Core assumption: VLMs can leverage additional structured context to improve reasoning about abstract concepts
- Evidence anchors: [section 3.4] "we augment the final question-answering step by providing the VLM with the component mapping", [section 5] "DSG consistently improves performance of vision-language models across question types"
- Break condition: If VLMs cannot effectively use additional context or are overwhelmed by information, performance gains will be minimal

## Foundational Learning

- Concept: Abstract concepts vs concrete visual features
  - Why needed here: The paper distinguishes between abstract concepts (defined by lifted rules) and concrete visual features (specific object categories)
  - Quick check question: What makes a maze look like a maze - is it the specific materials used or the underlying layout pattern?

- Concept: Schema representations and dependency graphs
  - Why needed here: Schemas are the core representation used by DSG to decompose abstract concepts into primitive components
  - Quick check question: How would you represent the concept "helping" as a dependency graph of subcomponents?

- Concept: Hierarchical grounding process
  - Why needed here: DSG uses hierarchical grounding to first identify concrete components, then use them to ground more abstract components
  - Quick check question: Why would you ground the layout of a maze before grounding its entry and exit?

## Architecture Onboarding

- Component map: LLM schema extractor -> VLM hierarchical grounders -> Schema resolver -> VLM reasoner
- Critical path: LLM extraction → VLM hierarchical grounding → Schema resolution → VLM reasoning
- Design tradeoffs:
  - Schema complexity vs VLM grounding capability: More detailed schemas may be harder for VLMs to ground
  - Single-round vs multi-round processing: Multi-round provides explicit guidance but adds latency
  - Fixed vs flexible symbol sets: Fixed sets ensure consistency but limit expressiveness
- Failure signatures:
  - Poor schema extraction: LLM generates irrelevant or incomplete schemas
  - Grounding failures: VLM cannot correctly identify schema components in image
  - Reasoning failures: Even with good grounding, VLM cannot answer questions correctly
- First 3 experiments:
  1. Test schema extraction with simple concepts (maze, tic-tac-toe) to verify basic functionality
  2. Test hierarchical grounding on single image with known ground truth components
  3. Compare single-round vs multi-round performance on small subset of benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DSG handle visual abstractions requiring precise spatial layouts, like mazes with specific entry/exit configurations?
- Basis in paper: Explicit
- Why unresolved: The paper notes that current VLMs struggle with grounding spatial constraints, and DSG relies on VLM capabilities without explicitly addressing this limitation.
- What evidence would resolve it: Experimental results showing DSG's performance on spatially complex visual abstractions, or modifications to DSG that explicitly parameterize spatial relationships.

### Open Question 2
- Question: What is the optimal number of components for schemas to balance detail and generality across diverse visual abstractions?
- Basis in paper: Inferred
- Why unresolved: The paper shows that accuracy decreases with more components in the example prompt, but does not explore the optimal tradeoff between schema detail and VLM grounding capabilities.
- What evidence would resolve it: Systematic experiments varying schema component numbers and measuring performance on a range of visual abstractions.

### Open Question 3
- Question: How does DSG's performance scale with increasing VLM capabilities for grounding complex components?
- Basis in paper: Explicit
- Why unresolved: The paper notes that DSG's performance may improve as VLMs become better at handling detailed concepts, but does not explore this scaling relationship.
- What evidence would resolve it: Longitudinal studies comparing DSG performance across different VLM versions or ablations isolating the impact of VLM improvements on DSG accuracy.

## Limitations

- The framework's performance depends heavily on the VLMs' ability to handle spatial constraints and precise layout understanding, which may be challenging for current models
- Allowing unconstrained language for schema generation and grounding introduces potential schema ambiguity and lack of universality across different implementations
- The current implementation relies on manual schema generation for evaluation, and the quality and consistency of automatically generated schemas remain uncertain

## Confidence

- Major claims: Medium
  - Improvement over GPT-4o (6.6 percentage points) and counting questions (10 percentage points) is supported by the Visual Abstractions Benchmark results
  - The three key mechanisms (LLM schema extraction, hierarchical grounding, and context augmentation) are theoretically sound
  - However, the benchmark itself is newly introduced and may have limited generalizability

## Next Checks

1. **Schema Robustness Test**: Evaluate the consistency and quality of automatically generated schemas across multiple LLM runs with different prompts and temperature settings to assess the reliability of the schema extraction mechanism.

2. **Cross-Dataset Generalization**: Test DSG on established visual reasoning benchmarks (e.g., VQA, GQA) with abstract concept questions to verify that improvements generalize beyond the newly created Visual Abstractions Dataset.

3. **Ablation Study on Grounding Hierarchy**: Systematically vary the grounding order (e.g., ground abstract components first vs. concrete components first) to empirically validate the claimed benefits of the hierarchical grounding approach.