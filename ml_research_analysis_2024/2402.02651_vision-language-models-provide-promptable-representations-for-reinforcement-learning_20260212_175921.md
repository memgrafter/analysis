---
ver: rpa2
title: Vision-Language Models Provide Promptable Representations for Reinforcement
  Learning
arxiv_id: '2402.02651'
source_url: https://arxiv.org/abs/2402.02651
tags:
- representations
- image
- pr2l
- policy
- minecraft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PR2L, a framework for using vision-language
  models (VLMs) to generate task-specific representations for reinforcement learning
  (RL). The method prompts a VLM with task-relevant questions about observations,
  then uses the resulting embeddings as input to an RL policy, rather than the raw
  image.
---

# Vision-Language Models Provide Promptable Representations for Reinforcement Learning

## Quick Facts
- arXiv ID: 2402.02651
- Source URL: https://arxiv.org/abs/2402.02651
- Authors: William Chen; Oier Mees; Aviral Kumar; Sergey Levine
- Reference count: 40
- Primary result: Vision-language models can generate task-specific representations for reinforcement learning through prompting, outperforming generic image embeddings and matching domain-specific approaches

## Executive Summary
This paper introduces PR2L, a framework that leverages vision-language models (VLMs) as promptable representations for reinforcement learning. Rather than using VLMs to generate actions directly, PR2L prompts VLMs with task-relevant questions about visual observations, then uses the resulting embeddings as inputs to RL policies. This approach allows policies to benefit from the VLM's general knowledge while still learning low-level actions. Experiments in Minecraft and Habitat demonstrate that PR2L outperforms generic image embeddings and performs comparably to domain-specific representations, with particularly strong results when using chain-of-thought prompting for semantic reasoning.

## Method Summary
PR2L works by querying a VLM with task-relevant prompts about visual observations, then using the resulting embeddings as inputs to an RL policy. The method involves three main steps: (1) generating text from the VLM by prompting it with questions about observations, (2) extracting embeddings from the generated text, and (3) using these embeddings as inputs to train an RL policy. The framework supports various VLM architectures and RL algorithms, and includes a prompt evaluation method using a small annotated dataset with linear probing as a proxy for RL performance. The approach is evaluated on visually complex tasks in Minecraft and Habitat environments, comparing against multiple baselines including generic image encoders and domain-specific representation learning methods.

## Key Results
- Policies trained on VLM embeddings outperform those trained on generic, non-promptable image embeddings
- PR2L performs comparably to domain-specific representation learning approaches like VC-1 and R3M
- Chain-of-thought prompting improves policy performance in novel scenes by 1.5× through enhanced semantic reasoning
- The approach generalizes across different VLM architectures and RL algorithms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** PR2L uses task-relevant prompts to steer VLMs toward producing state representations that encode semantic features aligned with the reward structure of the control task.
- **Mechanism:** The VLM's self-attention layers attend to task-relevant image regions based on the prompt, and the resulting token embeddings capture this attended semantic content. These embeddings are then used as inputs to the RL policy, effectively preconditioning the policy's state space toward task-relevant features.
- **Core assumption:** The VLM's internal knowledge and attention mechanisms can be reliably steered by prompts to extract semantically meaningful features that correlate with task-relevant states.
- **Evidence anchors:**
  - [abstract]: "We initialize policies with VLMs by using them as promptable representations: embeddings that encode semantic features of visual observations based on the VLM's internal knowledge and reasoning capabilities, as elicited through prompts that provide task context and auxiliary information."
  - [section]: "we ask a VLM questions about observations that are related to the given control task, priming it to attend to task-relevant features in the image based on both its internal world knowledge, reasoning capabilities, and any supplemental information injected via prompting."
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism; relies on general VLM attention literature.

### Mechanism 2
- **Claim:** The VLM's representations provide better inductive biases for RL than generic image embeddings because they are conditioned on task context.
- **Mechanism:** Generic image encoders learn task-agnostic features optimized for reconstruction or contrastive objectives, while PR2L's prompted VLM learns to map observations to task-specific semantic embeddings. This reduces the RL agent's need to learn semantic abstractions from scratch.
- **Core assumption:** Task-agnostic embeddings from pre-trained image encoders are less useful for downstream control tasks than task-specific embeddings elicited through prompting.
- **Evidence anchors:**
  - [abstract]: "policies trained on embeddings from off-the-shelf, general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings."
  - [section]: "our method yields task-specific features capturing information particularly conducive to learning a considered task."
  - [corpus]: Weak - corpus contains related work on VLM embeddings but not direct comparison with generic image encoders in RL context.

### Mechanism 3
- **Claim:** Chain-of-thought prompting in PR2L elicits richer semantic reasoning from the VLM, improving policy generalization to novel scenes.
- **Mechanism:** By prompting the VLM to explain its reasoning ("Would a [target object] be found here? Why or why not?"), the resulting embeddings encode not just object detection but also common-sense spatial and categorical knowledge about where objects belong, which the policy can leverage for navigation decisions.
- **Core assumption:** The VLM's chain-of-thought reasoning produces embeddings that capture generalizable semantic knowledge beyond simple object detection.
- **Evidence anchors:**
  - [abstract]: "we show that our approach can use chain-of-thought prompting to produce representations of common-sense semantic reasoning, improving policy performance in novel scenes by 1.5 times."
  - [section]: "this CoT draws out task-relevant VLM world knowledge by explicitly reasoning about visual semantic concepts, that are useful to learning a policy."
  - [corpus]: Weak - related work exists on CoT prompting but not specifically for RL policy improvement.

## Foundational Learning

- **Concept:** Vision-Language Models (VLMs) and their attention mechanisms
  - Why needed here: PR2L relies on understanding how VLMs process images and text through self-attention to produce meaningful embeddings.
  - Quick check question: What is the difference between a VLM's image encoder embeddings and its full generative model embeddings?

- **Concept:** Reinforcement Learning state representation learning
  - Why needed here: The core idea is that PR2L provides better state representations for RL, so understanding how state representations affect policy learning is crucial.
  - Quick check question: How do task-specific state representations differ from generic ones in terms of RL sample efficiency?

- **Concept:** Prompt engineering and in-context learning
  - Why needed here: PR2L's effectiveness depends on designing prompts that elicit useful representations, requiring knowledge of how VLMs respond to different prompt styles.
  - Quick check question: What makes a good prompt for eliciting task-relevant features versus instructions?

## Architecture Onboarding

- **Component map:** Environment -> VLM (with prompt) -> Embeddings -> Policy (Transformer + MLP) -> Actions -> Environment
- **Critical path:** Environment provides observation (image + non-visual info) → VLM processes observation with prompt, generates text, outputs embeddings → Policy processes embeddings through Transformer layer to summary embedding → Policy outputs action → Environment returns next observation and reward → RL algorithm updates policy
- **Design tradeoffs:** VLM size vs. inference speed (larger VLMs may produce better representations but limit real-time performance); Prompt specificity vs. generalization (more specific prompts may work better for current task but hurt transfer); Generation decoding strategy (Greedy decoding is faster but may miss important tokens vs. sampling-based approaches)
- **Failure signatures:** VLM generates irrelevant or incorrect text but embeddings still useful (expected behavior); VLM embeddings show no structure when visualized (potential prompt failure); Policy training plateaus quickly (poor representation quality or RL algorithm issues); Embeddings are too large for efficient processing (need dimensionality reduction)
- **First 3 experiments:** Run PR2L with empty prompt vs. task-relevant prompt to verify prompting matters; Compare PR2L performance against VLM image encoder baseline on a simple task; Test different prompt styles (with vs. without auxiliary information) to find optimal prompt for a task

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- The paper does not provide detailed analysis of computational overhead and inference latency of using VLMs as representations
- Results show comparable rather than superior performance to domain-specific representation learning methods
- The effectiveness of prompt engineering is not fully characterized across diverse task types

## Confidence
High confidence: The core finding that PR2L outperforms generic image embeddings and matches domain-specific approaches is well-supported by experiments across two different environments (Minecraft and Habitat) with multiple tasks. The methodology is sound and the results are statistically significant.

Medium confidence: The claim about chain-of-thought prompting providing 1.5x improvement is based on specific tasks and may not generalize across all control scenarios. The paper demonstrates effectiveness but doesn't explore the full space of prompting strategies or their task-specific efficacy.

Low confidence: The assertion that PR2L can replace domain-specific representation learning entirely is not well-supported. The results show comparable rather than superior performance to specialized approaches, and the paper doesn't investigate whether further optimization of prompts or VLM selection could close this gap.

## Next Checks
1. **Prompt Engineering Ablation Study:** Systematically vary prompt complexity, task relevance, and reasoning depth across a diverse set of control tasks to quantify the relationship between prompt quality and policy performance.

2. **Real-time Feasibility Analysis:** Measure inference latency, memory usage, and throughput of PR2L across different VLM sizes and compare against domain-specific alternatives.

3. **Robustness to Distribution Shift:** Evaluate PR2L policies on environments with varying visual complexity, lighting conditions, and object appearances that differ from training conditions.