---
ver: rpa2
title: Interpretable LLM-based Table Question Answering
arxiv_id: '2412.12386'
source_url: https://arxiv.org/abs/2412.12386
tags:
- table
- step
- rows
- select
- opponents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Plan-of-SQLs (POS), an interpretable method
  for Large Language Model (LLM)-based Table Question Answering (Table QA). POS addresses
  the lack of transparency in existing LLM approaches by decomposing queries into
  atomic SQL steps, enabling traceable and understandable table transformations.
---

# Interpretable LLM-based Table Question Answering

## Quick Facts
- arXiv ID: 2412.12386
- Source URL: https://arxiv.org/abs/2412.12386
- Reference count: 40
- Primary result: POS achieves competitive accuracy while requiring 25x fewer LLM calls and database queries

## Executive Summary
This paper introduces Plan-of-SQLs (POS), an interpretable method for LLM-based Table Question Answering that addresses the transparency limitations of existing approaches. POS decomposes complex queries into atomic SQL steps, enabling traceable table transformations while maintaining competitive accuracy. The method demonstrates significant efficiency gains (up to 25x fewer LLM calls) and provides high-quality explanations that help users understand and debug model decisions.

## Method Summary
POS is an interpretable Table QA method that decomposes questions into atomic SQL steps, each translated into executable SQL commands. The method maintains interpretability throughout by planning in natural language and generating attribution maps that highlight contributing table cells. POS requires only four LLM calls and two database queries per question, significantly fewer than baseline methods. The approach uses atomic steps with at most one condition and one variable to ensure both accuracy and interpretability.

## Key Results
- POS achieves competitive accuracy on standard benchmarks (TabFact: 70.9%, WikiTQ: 74.5%, FeTaQA: 38.1%)
- POS requires 25x fewer LLM calls and 2x fewer database queries than baseline methods
- Human and LLM judges consistently prefer POS explanations over baselines in interpretability and debugging tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: POS achieves high accuracy by decomposing queries into atomic SQL steps
- Mechanism: Each atomic step translates to a single, deterministic SQL command that executes a simple table transformation
- Core assumption: Atomic SQL commands with at most one condition and one variable are both accurate and interpretable
- Evidence anchors:
  - [abstract]: "POS decomposes a question into a sequence of atomic steps, each directly translated into an executable SQL command on the table"
  - [section]: "Each converted SQL command must: (i) contain at most one condition (e.g., in WHERE clause); and (ii) involve at most one variable within that condition"
- Break condition: If atomic steps require complex conditions or multiple variables, SQL translation becomes error-prone

### Mechanism 2
- Claim: POS is more efficient than baselines due to fewer LLM calls and database queries
- Mechanism: Atomic steps require only two LLM calls (planning and SQL generation) and two database queries per question
- Core assumption: Self-consistency prompting and complex table transformations are unnecessary when using atomic SQL steps
- Evidence anchors:
  - [section]: "POS requires only four LLM calls per question, significantly fewer than Binder (50), DATER (100), or CoTable (25)"
  - [section]: "Regarding database queries, POS is also more efficient than others with only two queries per question"
- Break condition: If questions require creative generation rather than structured table transformations, efficiency advantage diminishes

### Mechanism 3
- Claim: POS explanations are more interpretable because they use natural language steps instead of abstract functions
- Mechanism: Natural language descriptions of atomic steps are easier for humans to understand than function names with unexplained arguments
- Core assumption: Users can follow step-by-step reasoning when each step is described in human-understandable terms
- Evidence anchors:
  - [abstract]: "POS plans in natural language, making each step simple and understandable"
  - [section]: "In contrast to end-to-end and hybrid QA methods that rely on the black-box reasoning of LLMs to generate the final answer— Fig. 1 (a) & (c), POS maintains interpretability throughout"
- Break condition: If steps become too numerous or complex, even natural language descriptions may become overwhelming

## Foundational Learning

- Concept: SQL query structure and execution
  - Why needed here: Understanding how POS converts natural language steps into executable SQL commands
  - Quick check question: What is the difference between a SELECT statement with and without a WHERE clause?

- Concept: Atomic operations and decomposition
  - Why needed here: Understanding why POS breaks down complex queries into simple, single-operation steps
  - Quick check question: Why might decomposing a query into atomic steps be more reliable than processing it as a whole?

- Concept: Attribution maps and feature importance
  - Why needed here: Understanding how POS visualizes which table cells contribute to each step's output
  - Quick check question: How does highlighting specific rows, columns, and cells help users understand model reasoning?

## Architecture Onboarding

- Component map:
  - NL Atomic Planner → Step-to-SQL → SQL Engine → Attribution Map Generator → Explanation Renderer

- Critical path: NL Atomic Planner → Step-to-SQL → SQL Engine → Attribution Map Generator → Explanation Renderer

- Design tradeoffs:
  - SQL-only vs. LLM-based transformations: SQL provides accuracy but limits creative tasks
  - Atomicity vs. efficiency: More steps improve accuracy but require more processing
  - Natural language vs. function-based planning: Natural language improves interpretability but may be less concise

- Failure signatures:
  - Planning errors: Missing condition checks or incorrect sequencing of steps
  - SQL generation errors: Incorrect column names or syntax errors
  - Execution errors: Special characters in table data breaking SQL parsing
  - Attribution errors: Incorrect identification of selected rows/columns/cells

- First 3 experiments:
  1. Run POS on a simple TabFact example to verify the complete pipeline works
  2. Test POS on a complex WikiTQ question to identify planning limitations
  3. Compare POS accuracy with and without atomicity constraints to quantify their importance

## Open Questions the Paper Calls Out
None

## Limitations
- Exact implementation details for Step-to-SQL conversion and attribution map generation are not fully specified
- Method's performance on open-domain questions requiring creative reasoning is untested
- Heavy reliance on LLM judges for evaluation may introduce bias

## Confidence

**High Confidence**: The claim that POS achieves competitive accuracy while requiring fewer LLM calls and database queries is well-supported by the benchmark results (TabFact: 70.9% vs 70.2%, WikiTQ: 74.5% vs 75.0%, FeTaQA: 38.1% vs 36.6%). The efficiency claims (25x fewer LLM calls) are directly measurable and consistent across experiments.

**Medium Confidence**: The interpretability claims are supported by human and LLM judge evaluations, but the reliance on LLM judges introduces uncertainty. While the study reports high correlation between human and LLM evaluations, this may not generalize to all types of explanations or user populations.

**Low Confidence**: The generalizability of POS to complex, open-domain questions requiring creative reasoning is uncertain. The method's atomicity constraints (single condition, single variable) may not handle questions requiring multiple conditions or variables effectively.

## Next Checks

1. **Prompt Engineering Validation**: Test the Step-to-SQL conversion with varied prompt formulations to identify the sensitivity of POS performance to prompt design choices.

2. **Attribution Map Evaluation**: Implement and validate the attribution map generation process independently to verify the claimed 90% accuracy in identifying contributing cells.

3. **Generalization Testing**: Evaluate POS on open-domain questions from datasets like FeTaQA that require creative reasoning beyond structured table transformations to assess the method's limitations.