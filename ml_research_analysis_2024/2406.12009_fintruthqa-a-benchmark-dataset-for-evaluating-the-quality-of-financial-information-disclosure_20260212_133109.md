---
ver: rpa2
title: 'FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial Information
  Disclosure'
arxiv_id: '2406.12009'
source_url: https://arxiv.org/abs/2406.12009
tags:
- financial
- question
- answer
- relevance
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'FinTruthQA is a benchmark dataset for evaluating the quality of
  financial information disclosure through Q&A platforms. It includes 6,000 real-world
  financial Q&A pairs manually annotated on four criteria: question identification,
  question relevance, answer readability, and answer relevance.'
---

# FinTruthQA: A Benchmark Dataset for Evaluating the Quality of Financial Information Disclosure

## Quick Facts
- **arXiv ID**: 2406.12009
- **Source URL**: https://arxiv.org/abs/2406.12009
- **Reference count**: 9
- **Primary result**: FinTruthQA dataset enables evaluation of financial information disclosure quality through Q&A platforms with 6,000 manually annotated pairs

## Executive Summary
FinTruthQA is a benchmark dataset designed to evaluate the quality of financial information disclosure through Q&A platforms. The dataset comprises 6,000 real-world financial Q&A pairs manually annotated on four criteria: question identification, question relevance, answer readability, and answer relevance. Various NLP models were benchmarked, including statistical ML, pre-trained language models, and large language models. Results showed that existing models perform well on question identification and question relevance but are less effective on answer readability and answer relevance. BERT-based models pre-trained on financial corpora, particularly FinBERT, achieved the best performance. The dataset provides a foundation for developing AI tools to enhance transparency and fairness in financial markets.

## Method Summary
The FinTruthQA dataset was constructed by collecting real-world financial Q&A pairs from Chinese financial platforms. Each pair was manually annotated by financial experts on four quality criteria. The annotation process involved detailed guidelines for assessing question identification (determining if content is a valid financial question), question relevance (measuring alignment with financial topics), answer readability (evaluating clarity and comprehensibility), and answer relevance (assessing how well the answer addresses the question). Multiple NLP models were then evaluated on this dataset, including traditional statistical methods, pre-trained language models like BERT and RoBERTa, and large language models such as ChatGPT and GPT-4. The models were fine-tuned and tested using standard metrics like accuracy, F1-score, and Pearson correlation.

## Key Results
- Existing models show strong performance on question identification and question relevance tasks
- BERT-based models pre-trained on financial corpora, particularly FinBERT, achieved the best overall performance
- Models demonstrate significant limitations in assessing answer readability and answer relevance
- The dataset provides a standardized benchmark for evaluating financial information quality in Q&A contexts

## Why This Works (Mechanism)
The FinTruthQA dataset works by providing a standardized, manually annotated benchmark that captures the complexity of real-world financial information disclosure. The four annotation criteria address different aspects of information quality, from basic identification to deeper semantic understanding. The manual annotation process ensures high-quality labels that reflect expert judgment, while the diverse model evaluation reveals both strengths and limitations of current NLP approaches in financial contexts.

## Foundational Learning
- **Manual annotation for quality assessment**: Why needed - ensures accurate ground truth labels for model evaluation; Quick check - verify inter-annotator agreement scores exceed 0.8
- **Multi-criteria evaluation framework**: Why needed - captures different dimensions of information quality beyond simple accuracy; Quick check - confirm all four criteria show distinct model performance patterns
- **Financial domain adaptation**: Why needed - general NLP models underperform on specialized financial terminology and concepts; Quick check - compare FinBERT vs. general BERT performance gap
- **Q&A pair analysis**: Why needed - reflects real-world information-seeking behavior in financial markets; Quick check - validate dataset covers diverse financial topics and question types
- **Benchmark standardization**: Why needed - enables fair comparison across different model architectures and approaches; Quick check - ensure consistent evaluation metrics across all tested models
- **Expert-driven evaluation**: Why needed - captures nuanced understanding of financial information quality that automated metrics might miss; Quick check - confirm expert annotators have relevant financial domain experience

## Architecture Onboarding

**Component Map**: Data Collection -> Manual Annotation -> Model Training -> Evaluation -> Benchmark Creation

**Critical Path**: The critical path involves collecting authentic financial Q&A pairs, having them annotated by domain experts, using these annotations to train and fine-tune various NLP models, and then evaluating model performance to establish benchmark metrics.

**Design Tradeoffs**: The study prioritized annotation quality over dataset size, resulting in a smaller but more reliable dataset. This tradeoff favors accuracy and reliability over coverage but may limit generalizability.

**Failure Signatures**: Models fail primarily on answer-related tasks (readability and relevance), suggesting limitations in semantic understanding and context interpretation. Performance drops when dealing with complex financial terminology or nuanced question-answer relationships.

**First 3 Experiments**:
1. Fine-tune FinBERT on the FinTruthQA dataset and evaluate on held-out test set
2. Compare general BERT vs. FinBERT performance on each of the four annotation criteria
3. Test zero-shot performance of large language models (GPT-4, ChatGPT) on the dataset

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset size of 6,000 QA pairs may limit generalizability across all financial disclosure scenarios
- Focus on Chinese financial platforms may restrict applicability to other languages and regional markets
- The four annotation criteria may not capture all dimensions of financial information quality, such as factual accuracy verification
- Manual annotation process, while ensuring quality, limits scalability for future dataset expansion

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Strong model performance on question identification and relevance | High |
| FinBERT achieves superior performance | Medium |
| Models struggle with answer readability and relevance | Medium |
| Dataset provides foundation for AI tools in financial markets | High |

## Next Checks
1. Test the FinTruthQA benchmark on additional financial corpora and different languages to assess generalizability
2. Conduct longitudinal studies to evaluate how well models maintain performance as financial reporting standards and disclosure practices evolve
3. Implement real-world deployment trials with financial analysts and regulators to assess practical utility and identify any performance gaps not captured in controlled benchmarking