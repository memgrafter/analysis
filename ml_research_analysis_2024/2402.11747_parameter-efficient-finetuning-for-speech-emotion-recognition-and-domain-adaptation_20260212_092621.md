---
ver: rpa2
title: Parameter Efficient Finetuning for Speech Emotion Recognition and Domain Adaptation
arxiv_id: '2402.11747'
source_url: https://arxiv.org/abs/2402.11747
tags:
- emotion
- peft
- data
- performance
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates parameter-efficient fine-tuning
  (PEFT) methods for speech emotion recognition (SER) across both discrete emotion
  classification and dimensional attribute prediction tasks. The proposed approach
  combines multiple PEFT adaptors (LoRA, BA, WS, WG) with frozen backbone models,
  achieving performance comparable to full fine-tuning while using only 2-3% of trainable
  parameters.
---

# Parameter Efficient Finetuning for Speech Emotion Recognition and Domain Adaptation

## Quick Facts
- arXiv ID: 2402.11747
- Source URL: https://arxiv.org/abs/2402.11747
- Authors: Nineli Lashkarashvili; Wen Wu; Guangzhi Sun; Philip C. Woodland
- Reference count: 0
- Primary result: PEFT methods achieve comparable performance to full fine-tuning using only 2-3% of trainable parameters

## Executive Summary
This paper systematically investigates parameter-efficient fine-tuning (PEFT) methods for speech emotion recognition (SER) across both discrete emotion classification and dimensional attribute prediction tasks. The proposed approach combines multiple PEFT adaptors (LoRA, BA, WS, WG) with frozen backbone models, achieving performance comparable to full fine-tuning while using only 2-3% of trainable parameters. A two-stage adaptation strategy is introduced to adapt models trained on acted emotion data to natural emotion domains, showing improved performance on both source and target domains.

## Method Summary
The paper explores parameter-efficient fine-tuning (PEFT) methods including LoRA (Low-Rank Adaptation), BA (Bottleneck Adapter), WS (Weighted Sum), and WG (Weighted Gate) for speech emotion recognition. These adaptors are inserted into pre-trained models like wav2vec 2.0 and HuBERT to minimize trainable parameters while maintaining performance. The approach is evaluated on IEMOCAP dataset for both categorical emotion classification and dimensional emotion prediction (valence, arousal, dominance). A two-stage adaptation strategy is proposed where models are first trained on acted emotion data, then adapted to natural emotion data with selective freezing of PEFT modules to prevent catastrophic forgetting.

## Key Results
- PEFT combinations achieve comparable performance to full fine-tuning with only 2-3% of trainable parameters
- LoRA+BA combination provides sufficient adaptation capacity for dimensional emotion prediction while avoiding overfitting
- Two-stage adaptation strategy improves performance on both acted and natural emotion domains
- Outperforms state-of-the-art CCC scores for dimensional emotion prediction while using significantly fewer parameters

## Why This Works (Mechanism)

### Mechanism 1
LoRA + BA combination provides sufficient adaptation capacity for dimensional emotion prediction while avoiding overfitting. Low-rank decomposition in LoRA captures cross-transformer-block correlations with minimal parameters, while BA adds task-specific scaling through bottleneck layers. The core assumption is that dimensional emotion prediction requires fewer adaptation parameters than categorical classification. Evidence shows this combination surpasses full fine-tuning with significant parameter reduction.

### Mechanism 2
WS and WG adaptors act as feature selectors that compensate for lost discriminative information in complex emotion tasks. WS assigns learnable importance weights to each transformer block's output, while WG modulates hidden states with sigmoid-gated scaling vectors. The core assumption is that different transformer layers encode complementary emotional information that needs to be weighted differently per task. Ablation studies show consistent performance drops when removing WS/WG from combined setups.

### Mechanism 3
Two-stage adaptation preserves source domain knowledge while improving target domain performance. First stage learns acted emotion features, second stage fine-tunes on natural emotion with partial freezing of BA/LoRA modules. The core assumption is that acted and natural emotions share underlying acoustic features that can be progressively adapted. Freezing BA or LoRA modules helps avoid catastrophic forgetting of the source domain.

## Foundational Learning

- **Self-supervised learning representations in speech**
  - Why needed: Foundation models (wav2vec 2.0, HuBERT) provide rich acoustic features that PEFT adapts for emotion recognition
  - Quick check: What distinguishes wav2vec 2.0's masked prediction objective from HuBERT's masked cluster prediction?

- **Low-rank matrix factorization**
  - Why needed: LoRA decomposes weight updates into smaller matrices (Wq + W_B * W_A) to reduce trainable parameters
  - Quick check: If original weight matrix has dimension 768x64, what are the dimensions of W_B and W_A when rank r=8?

- **Catastrophic forgetting in sequential fine-tuning**
  - Why needed: Two-stage adaptation must preserve acted emotion performance while learning natural emotion features
  - Quick check: What's the difference between freezing all parameters vs freezing only specific PEFT modules during stage 2?

## Architecture Onboarding

- **Component map**: wav2vec 2.0/HuBERT (12/24 layers) -> PEFT modules (BA after feedforward, LoRA in attention, WS/WG at transformer outputs) -> Two FC layers (SUPERB setup) -> CCC/accuracy evaluation
- **Critical path**: Feature extraction → PEFT adaptation → Task prediction → CCC/accuracy evaluation
- **Design tradeoffs**: Parameter efficiency vs adaptation capacity; freezing for source preservation vs flexibility for target learning
- **Failure signatures**: Overparameterization (accuracy drops with too many PEFT modules), catastrophic forgetting (source domain performance degradation), underfitting (performance stuck at PT baseline)
- **First 3 experiments**:
  1. Run standalone BA, LoRA, WS, WG on IEMOCAP categorical task to establish baseline contributions
  2. Test LoRA+BA combination vs individual methods for dimensional emotion prediction
  3. Implement two-stage adaptation with different freezing strategies (BA only, LoRA only, both) on IEMOCAP scripted→improvised

## Open Questions the Paper Calls Out

### Open Question 1
How do different combinations of PEFT methods perform across various speech emotion recognition tasks and datasets, and what are the optimal configurations for each? While the paper demonstrates that combinations of PEFT methods can surpass full fine-tuning, it does not provide a comprehensive analysis of the optimal configurations for different tasks and datasets.

### Open Question 2
How does the proposed two-stage adaptation strategy for domain adaptation from acted to natural emotion data perform compared to other domain adaptation techniques? While the paper demonstrates the effectiveness of the proposed two-stage adaptation strategy, it does not compare it to other domain adaptation techniques or provide insights into its generalizability to other tasks and datasets.

### Open Question 3
How does freezing part of the PEFT modules during domain adaptation affect the model's ability to retain knowledge from the source domain while adapting to the target domain? While the paper demonstrates the benefits of freezing PEFT modules during domain adaptation, it does not provide a detailed analysis of the trade-offs between retaining source domain knowledge and adapting to the target domain.

## Limitations
- Limited ablation studies comparing different PEFT combinations directly for dimensional tasks
- Cross-corpus validation shows performance degradation on source domain after domain adaptation
- Missing specific details on critical hyperparameters like LoRA rank values and BA bottleneck dimensions

## Confidence
- **High**: PEFT methods achieving comparable performance to full fine-tuning with 2-3% parameters
- **Medium**: Two-stage adaptation preserving source domain knowledge while improving target domain performance
- **Low**: Claims about specific adaptor combinations being optimal for particular emotion dimensions

## Next Checks
1. Implement ablation studies comparing all possible PEFT combinations (LoRA, BA, WS, WG individually and in pairs/triples) specifically for valence, arousal, and dominance prediction
2. Conduct extensive hyperparameter sensitivity analysis varying LoRA rank (r=1,4,8,16) and BA bottleneck sizes
3. Perform more rigorous cross-corpus validation with additional datasets beyond ESD to test the robustness of the two-stage adaptation strategy