---
ver: rpa2
title: 'Personal Intelligence System UniLM: Hybrid On-Device Small Language Model
  and Server-Based Large Language Model for Malay Nusantara'
arxiv_id: '2410.06973'
source_url: https://arxiv.org/abs/2410.06973
tags:
- malay
- language
- tasks
- languages
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of developing effective language
  models for Malay languages, which are often underserved by high-resource language
  models due to limited data and computational resources. The authors introduce a
  Personal Intelligence System that integrates two models: SLiM-34M for on-device
  processing, optimized for low memory and power usage, and MANYAK-1.3B for server-based
  tasks, allowing scalable, high-performance language processing.'
---

# Personal Intelligence System UniLM: Hybrid On-Device Small Language Model and Server-Based Large Language Model for Malay Nusantara

## Quick Facts
- arXiv ID: 2410.06973
- Source URL: https://arxiv.org/abs/2410.06973
- Authors: Azree Nazri; Olalekan Agbolade; Faisal Aziz
- Reference count: 31
- Key outcome: Introduces a hybrid Personal Intelligence System combining SLiM-34M (34M parameters) for on-device processing and MANYAK-1.3B (1.3B parameters) for server tasks, achieving competitive accuracy with 2x fewer pre-training tokens for Malay language applications

## Executive Summary
This paper addresses the challenge of developing effective language models for Malay languages, which are often underserved by high-resource language models due to limited data and computational resources. The authors introduce a Personal Intelligence System that integrates two models: SLiM-34M for on-device processing, optimized for low memory and power usage, and MANYAK-1.3B for server-based tasks, allowing scalable, high-performance language processing. SLiM-34M achieves high accuracy improvements compared to other large language models while using 2 times fewer pre-training tokens. The system is evaluated on tasks like machine translation, question-answering, and the Malay-translated IndoMMLU benchmark, demonstrating the effectiveness of resource-efficient models for Malay language applications. This work challenges the assumption that large-scale computational resources are necessary for building effective language models.

## Method Summary
The method involves developing two complementary models: SLiM-34M (34M parameters) optimized for on-device deployment with low memory and power requirements, and MANYAK-1.3B (1.3B parameters) for server-based processing. Both models use transformer-based architectures with decoder-only design. SLiM-34M employs Grouped Query Attention, RMSNorm, Flash Attention, and SwiGLU FFN to maximize efficiency. The system uses a joint English-Malay Nusantara BPE tokenizer with vocabulary sizes of 15K (small) or 299K (large). Training was conducted on AWS EC2 p4d.24xlarge with A100 GPUs over 16 days using 150B or 2.4B tokens depending on model variant. An orchestration layer centrally manages task distribution between on-device and server models based on complexity and resource requirements.

## Key Results
- SLiM-34M achieves competitive accuracy compared to larger models while using 2 times fewer pre-training tokens
- The hybrid system effectively routes lightweight tasks to on-device SLiM-34M and complex tasks to server-based MANYAK-1.3B
- System demonstrates strong performance on Malay language tasks including machine translation, QA, and the Malay-translated IndoMMLU benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid orchestration between SLiM-34M and MANYAK-1.3B allows task-specific allocation of computational resources.
- Mechanism: The system routes lightweight, latency-sensitive tasks to the on-device SLiM-34M, while offloading complex, high-resource tasks to the server-based MANYAK-1.3B. This ensures optimal performance without exceeding device constraints.
- Core assumption: On-device models can handle a subset of NLP tasks with sufficient accuracy, while the server model handles tasks that require deeper reasoning or larger context.
- Evidence anchors:
  - [abstract] "The system incorporates SLiM-34M for on-device processing, optimized for low memory and power usage, and MANYAK-1.3B for server-based tasks"
  - [section] "The Orchestration layer centrally manages which tasks are processed locally on the device and which are offloaded to the cloud"
  - [corpus] Weak - corpus does not provide direct evidence of orchestration effectiveness.
- Break condition: If device-side latency or power consumption exceeds acceptable thresholds, or if server-side task distribution becomes a bottleneck.

### Mechanism 2
- Claim: SLiM-34M achieves competitive accuracy with significantly fewer pre-training tokens, challenging resource assumptions.
- Mechanism: By leveraging advanced training techniques like RMSNorm, Flash Attention, and SwiGLU FFN, SLiM-34M maximizes learning efficiency per token, allowing it to match or exceed the performance of larger models trained on more data.
- Core assumption: Efficient architectural choices can compensate for reduced model size and training data.
- Evidence anchors:
  - [abstract] "SLiM-34M's ability to achieve a high improvement in accuracy compared to other LLMs while using 2 times fewer pre-training tokens"
  - [section] "The architecture of SLiM-34M adopts a decoder-only transformer-based model, aligning with the principles followed by state-of-the-art small language models"
  - [corpus] Weak - corpus lacks direct comparisons of token efficiency versus accuracy.
- Break condition: If the model's performance degrades on tasks requiring deep contextual understanding or if training data quality is insufficient.

### Mechanism 3
- Claim: Grouped Query Attention (GQA) reduces computational cost while maintaining attention quality.
- Mechanism: GQA consolidates attention heads into groups, reducing the number of unique key-value pairs computed per layer, which lowers memory and compute demands without significantly impacting attention effectiveness.
- Core assumption: Attention quality can be preserved even with reduced head diversity in grouped configurations.
- Evidence anchors:
  - [section] "Grouped Query Attention (GQA) is utilized instead of the conventional multi-head attention (MHA)"
  - [corpus] Weak - corpus does not provide empirical results specific to GQA performance in this context.
- Break condition: If attention quality degrades on complex linguistic tasks, leading to reduced model accuracy.

## Foundational Learning

- Concept: Transformer-based language models
  - Why needed here: Both SLiM-34M and MANYAK-1.3B are transformer-based models, and understanding their architecture is essential for optimizing performance and resource usage.
  - Quick check question: What is the role of self-attention in transformer models, and how does it differ from recurrent neural networks?

- Concept: Tokenization and vocabulary size optimization
  - Why needed here: Efficient tokenization is critical for reducing memory usage and improving model performance, especially in resource-constrained environments.
  - Quick check question: How does Byte Pair Encoding (BPE) tokenization work, and why is it preferred for multilingual models?

- Concept: Model quantization and compression techniques
  - Why needed here: SLiM-34M uses low-bit palletization to reduce memory usage on-device, making it crucial to understand quantization methods.
  - Quick check question: What is the difference between 2-bit and 4-bit quantization, and how does it affect model accuracy?

## Architecture Onboarding

- Component map:
  - SLiM-34M (On-device): 34M parameters, optimized for low memory and power usage, handles lightweight tasks
  - MANYAK-1.3B (Server-based): 1.3B parameters, high-performance model for complex tasks
  - Orchestration Layer: Manages task distribution between on-device and server models
  - Tokenizer: Joint English-Malay Nusantara BPE tokenizer for bilingual processing

- Critical path:
  1. Input text is tokenized using the joint BPE tokenizer
  2. Orchestration layer evaluates task complexity and latency requirements
  3. Lightweight tasks are routed to SLiM-34M for on-device processing
  4. Complex tasks are offloaded to MANYAK-1.3B on the server
  5. Results are returned to the user with minimal latency

- Design tradeoffs:
  - Memory vs. Performance: SLiM-34M prioritizes low memory usage at the cost of some accuracy compared to larger models
  - Latency vs. Accuracy: On-device processing reduces latency but may sacrifice accuracy for complex tasks
  - Privacy vs. Scalability: On-device models enhance privacy but limit scalability compared to server-based models

- Failure signatures:
  - High latency in on-device processing: Indicates inefficient task allocation or insufficient model capacity
  - Accuracy degradation: Suggests model underfitting or poor tokenization
  - Server overload: Points to suboptimal orchestration or excessive task offloading

- First 3 experiments:
  1. Benchmark SLiM-34M's accuracy on lightweight tasks (e.g., sentiment analysis) versus larger models
  2. Measure latency and memory usage of SLiM-34M on different device configurations
  3. Test the orchestration layer's ability to dynamically allocate tasks between SLiM-34M and MANYAK-1.3B

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between on-device and server-based processing for different types of Malay language tasks in the Personal Intelligence System?
- Basis in paper: [explicit] The paper describes a hybrid system with SLiM-34M for on-device and MANYAK-1.3B for server-based tasks, but doesn't specify how to optimally allocate different tasks between them
- Why unresolved: The paper demonstrates the system works but doesn't provide guidelines for determining which tasks should be processed locally versus in the cloud based on factors like complexity, privacy requirements, or network conditions
- What evidence would resolve it: Empirical studies comparing task performance and efficiency across various allocation strategies, including benchmarks showing optimal distribution for different use cases and constraints

### Open Question 2
- Question: How does the performance of the Malay Nusantara language models degrade when processing non-standard dialects or code-switched text?
- Basis in paper: [inferred] The paper focuses on standard Malay and several major dialects but doesn't address how the models handle linguistic variation beyond these or mixed-language input
- Why unresolved: The evaluation primarily uses standardized datasets and doesn't test the models' robustness to real-world linguistic variation, regional colloquialisms, or code-switching between Malay and other languages
- What evidence would resolve it: Systematic testing of model performance on diverse dialectal variations, code-switched corpora, and informal language use, with metrics comparing accuracy across different types of linguistic variation

### Open Question 3
- Question: What are the long-term energy efficiency and environmental impacts of deploying SLiM-34M versus traditional large language models at scale?
- Basis in paper: [explicit] The paper emphasizes SLiM-34M's low power usage and computational efficiency compared to larger models, but doesn't quantify the full lifecycle environmental impact
- Why unresolved: While the paper demonstrates resource efficiency during operation, it doesn't address manufacturing energy costs, device lifespan, or comparative environmental impact of producing and maintaining different model architectures at scale
- What evidence would resolve it: Comprehensive lifecycle assessment studies comparing the total energy consumption and carbon footprint of SLiM-34M deployments versus traditional large models, including manufacturing, operation, and disposal phases

## Limitations
- The paper lacks direct empirical comparisons showing how specific architectural choices contribute to the claimed 2x reduction in pre-training tokens
- Limited evidence of orchestration effectiveness in real-world deployment scenarios with concrete performance metrics
- Evaluation focuses on standard NLP benchmarks without real-world latency measurements or power consumption data for on-device deployment

## Confidence
- High confidence: The hybrid architecture concept and task allocation mechanism through an orchestration layer
- Medium confidence: Claims about SLiM-34M achieving competitive accuracy with fewer tokens
- Low confidence: Effectiveness of the orchestration layer in real-world deployment scenarios

## Next Checks
1. Conduct controlled experiments comparing SLiM-34M's performance against baseline models using identical training data and compute resources to verify the claimed 2x reduction in pre-training tokens while maintaining accuracy
2. Measure actual latency, memory usage, and power consumption of SLiM-34M on representative low-resource devices (e.g., mobile phones, IoT devices) under realistic usage patterns to validate on-device optimization claims
3. Design and execute experiments testing the dynamic task allocation mechanism under varying network conditions, device capabilities, and task complexities to quantify the performance benefits of the hybrid approach versus pure on-device or pure server-based solutions