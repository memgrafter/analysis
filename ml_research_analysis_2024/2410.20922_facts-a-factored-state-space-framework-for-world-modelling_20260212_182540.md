---
ver: rpa2
title: 'FACTS: A Factored State-Space Framework For World Modelling'
arxiv_id: '2410.20922'
source_url: https://arxiv.org/abs/2410.20922
tags:
- facts
- input
- arxiv
- learning
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FACTS, a factored state-space framework for
  world modelling that addresses the challenge of efficiently encoding spatial and
  temporal structures in complex systems. The core idea is to construct a graph-structured
  memory with a routing mechanism that learns permutable memory representations, ensuring
  invariance to input permutations while adapting through selective state-space propagation.
---

# FACTS: A Factored State-Space Framework For World Modelling

## Quick Facts
- arXiv ID: 2410.20922
- Source URL: https://arxiv.org/abs/2410.20922
- Reference count: 40
- Key outcome: Introduces FACTS, a factored state-space framework that achieves permutation-invariant world modelling with efficient history compression and robust performance across diverse tasks

## Executive Summary
FACTS introduces a novel factored state-space framework that addresses the challenge of efficiently encoding spatial and temporal structures in complex systems. The core innovation is a graph-structured memory with a routing mechanism that learns permutable memory representations, ensuring invariance to input permutations while maintaining adaptability through selective state-space propagation. By treating inputs as sets of nodes and dynamically assigning them to distinct latent factors, FACTS captures underlying system dynamics more efficiently than traditional approaches. The framework was evaluated across diverse tasks including multivariate time series forecasting, object-centric world modelling, and spatial-temporal graph prediction, consistently matching or outperforming specialized state-of-the-art models despite its general-purpose design.

## Method Summary
FACTS is a recurrent framework that constructs a graph-structured memory with a routing mechanism to learn permutable memory representations. The model treats inputs as sets of nodes and dynamically assigns them to distinct latent state-space factors using attention-based matching. State-space dynamics are updated using element-wise operations rather than matrix multiplication to preserve permutation invariance properties. The framework employs history compression through selective state-space propagation and linearises recurrence by substituting Zt-1 with Z0 for parallel computation. The model was evaluated on multivariate time series forecasting (9 datasets), object-centric world modelling (CLEVRER, OBJ3D, MOVi-A), and dynamic-graph node prediction (METR-LA) using standard metrics including MSE, MAE, MAPE, LPIPS, ARI, and mIoU.

## Key Results
- FACTS matches or exceeds specialized state-of-the-art models across diverse tasks including time series forecasting, object-centric world modelling, and graph prediction
- The framework maintains robust performance even when input feature orders are shuffled during testing, demonstrating true permutation invariance
- FACTS achieves efficient history compression enabling enhanced long-term prediction capabilities compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The selective memory-input routing dynamically assigns input features to distinct latent factors, enabling permutation-invariant state-space memory.
- Mechanism: The routing mechanism uses attention-based matching between memory Zt-1 and input features Xt to dynamically assign inputs to factors, ensuring consistent factor representations regardless of input order changes.
- Core assumption: The routing mechanism can effectively learn which input features correspond to which latent factors across time steps, even as their spatial relationships change.
- Evidence anchors:
  - [abstract]: "FACTS constructs a graph-structured memory with a routing mechanism that learns permutable memory representations, ensuring invariance to input permutations"
  - [section]: "The routing mechanism between memory and inputs must dynamically assign input features to consistent factors"
  - [corpus]: Weak - no direct citations to routing mechanisms in related papers
- Break condition: If the routing mechanism fails to learn consistent factor assignments, the permutation invariance property breaks down.

### Mechanism 2
- Claim: Linearisation of the recurrence through substitution of Zt-1 with Z0 in routing processes enables parallel computation.
- Mechanism: By substituting Zt-1 with Z0 in the routing processes, FACTS breaks the non-linear dependency between Zt and Zt-1, allowing parallel computation of the state-space propagation.
- Core assumption: The initial memory Z0 contains sufficient information to maintain temporal dependencies when substituted into routing processes.
- Evidence anchors:
  - [section]: "This formulation in equation 17 linearise the recurrence in equation 10 by breaking the non-linear dependency between Zt and Zt-1"
  - [section]: "the inputs interact only with the initial memory, enabling fast computation of Zt using equation 19, i.e., without recurrence"
  - [corpus]: Weak - no direct citations to this specific linearisation technique
- Break condition: If Z0 becomes outdated or insufficient, the parallel computation loses temporal coherence.

### Mechanism 3
- Claim: Element-wise multiplication instead of matrix multiplication conserves permutation invariance properties.
- Mechanism: FACTS uses element-wise multiplication (⊙) in state-space dynamics rather than matrix multiplication to maintain permutation equivariance and invariance properties.
- Core assumption: Element-wise operations preserve the desired invariance properties that matrix operations would violate.
- Evidence anchors:
  - [section]: "Compared to the standard SSM dynamics, i.e., equation 3-equation 4, we note two key differences: (i) FACTS relies on element-wise multiplication, instead of matrix multiplication, to conserve the invariance properties"
  - [section]: "FACTS is characterised by two key features: permutable state-space memory... and invariant recurrence with respect to permutations of the input features"
  - [corpus]: Weak - no direct citations to element-wise vs matrix multiplication comparisons
- Break condition: If the element-wise operations cannot capture complex relationships, the model loses expressiveness.

## Foundational Learning

- Concept: State-Space Models (SSMs)
  - Why needed here: FACTS builds upon SSM foundations to handle spatial-temporal dependencies efficiently
  - Quick check question: What are the key components of a standard SSM (A, B, C, D matrices) and their roles?

- Concept: Permutation Invariance and Equivariance
  - Why needed here: FACTS explicitly requires these properties for handling unordered input features
  - Quick check question: What's the difference between permutation invariance and permutation equivariance in the context of input processing?

- Concept: History Compression
  - Why needed here: FACTS leverages history compression to maintain long-term dependencies efficiently
  - Quick check question: How does the selective mechanism in FACTS achieve history compression differently from traditional RNNs?

## Architecture Onboarding

- Component map: Input processing: Set embedder → Multi-dimensional feature representation → FACTS module (routing + state-space dynamics) → Factor-graph decoder → Permutation-invariant predictions

- Critical path: 1. Input embedding creates m×d feature representation 2. Routing mechanism matches inputs to factors using attention 3. State-space dynamics update latent factors using element-wise operations 4. Output decoder aggregates factor predictions

- Design tradeoffs:
  - Element-wise vs matrix operations: Simpler operations preserve invariance but may limit expressiveness
  - Number of factors (k): More factors increase expressiveness but computational cost
  - Routing complexity: More sophisticated routing improves accuracy but increases training time

- Failure signatures:
  - Poor performance on permuted inputs indicates routing mechanism failure
  - Degradation over long sequences suggests history compression issues
  - Inconsistent predictions across runs may indicate instability in routing

- First 3 experiments:
  1. Test permutation invariance: Train on ordered inputs, test on randomly permuted inputs
  2. Compare parallel vs recurrent modes: Measure performance vs computational efficiency trade-offs
  3. Ablation study: Remove routing mechanism to verify its contribution to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FACTS scale to extremely high-dimensional input spaces (e.g., high-resolution video) and what are the practical computational limits?
- Basis in paper: [inferred] The paper mentions FACTS can handle diverse input structures including images and graphs, but does not provide experiments on very high-dimensional data like full-resolution video sequences.
- Why unresolved: The evaluation focuses on moderate-dimensional datasets (time series, synthetic videos with limited resolution, traffic graphs). Scaling to high-dimensional continuous inputs would require addressing memory and computation challenges not explored in the current work.
- What evidence would resolve it: Experiments showing FACTS performance and computational requirements on high-resolution video datasets, or theoretical analysis of memory and computational complexity as input dimensionality increases.

### Open Question 2
- Question: What is the theoretical relationship between the number of factors (k) and the model's ability to capture underlying system dynamics?
- Basis in paper: [explicit] The ablation study shows robustness to different k values but does not provide theoretical justification for optimal k selection or how k relates to the complexity of the underlying system.
- Why unresolved: While empirical results show FACTS performs well across different k values, there is no theoretical framework explaining how to choose k based on the characteristics of the target domain or how k affects the model's representational capacity.
- What evidence would resolve it: A theoretical analysis connecting k to properties like input dimensionality, temporal complexity, or system state space dimensionality, along with guidelines for selecting k for different application domains.

### Open Question 3
- Question: How does FACTS compare to specialized models in specific domains where those models have strong inductive biases?
- Basis in paper: [explicit] The paper demonstrates FACTS matches or exceeds specialized state-of-the-art models across diverse tasks, but does not analyze when domain-specific architectures might still be preferable.
- Why unresolved: While FACTS shows strong general performance, the paper does not identify scenarios where specialized architectures with domain-specific inductive biases might outperform the general-purpose FACTS framework.
- What evidence would resolve it: Systematic comparison studies identifying specific problem characteristics (e.g., particular data structures, noise patterns, or temporal dynamics) where specialized models outperform FACTS, along with analysis of trade-offs between general and specialized approaches.

## Limitations

- The routing mechanism's ability to maintain consistent factor assignments across varying input permutations lacks rigorous theoretical grounding and empirical validation
- Element-wise operations preserve invariance properties but may significantly limit the model's expressive power compared to matrix operations
- The linearisation technique for parallel computation assumes initial memory Z0 contains sufficient temporal information, which may not hold for long sequences or rapidly changing environments

## Confidence

* High confidence: The general framework architecture and task formulations are clearly specified. The empirical results show consistent performance across multiple domains and the permutation invariance property is demonstrated through practical experiments.

* Medium confidence: The theoretical properties (invariance, equivariance) are derived correctly from the mathematical formulation, but the practical implications of design choices (element-wise operations, linearisation) are not fully explored.

* Low confidence: The routing mechanism's internal workings and its ability to generalize across diverse input patterns remain unclear. The claims about computational efficiency gains from parallelisation are not substantiated with timing benchmarks.

## Next Checks

1. **Routing Mechanism Robustness**: Conduct systematic ablation studies by replacing the attention-based routing with alternative mechanisms (e.g., random assignment, fixed assignment) to quantify the routing's contribution to both performance and permutation invariance.

2. **Expressiveness vs Invariance Trade-off**: Compare FACTS against variants that use matrix operations in the state-space dynamics to measure the actual performance penalty from preserving permutation invariance through element-wise operations.

3. **Long-term Temporal Coherence**: Evaluate the model's performance degradation over progressively longer sequences to assess whether the initial memory Z0 substitution in linearisation maintains temporal dependencies effectively, and identify at what sequence lengths this approach breaks down.