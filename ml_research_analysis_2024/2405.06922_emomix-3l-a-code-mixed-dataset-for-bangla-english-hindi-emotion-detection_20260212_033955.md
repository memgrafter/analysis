---
ver: rpa2
title: 'EmoMix-3L: A Code-Mixed Dataset for Bangla-English-Hindi Emotion Detection'
arxiv_id: '2405.06922'
source_url: https://arxiv.org/abs/2405.06922
tags:
- dataset
- data
- emotion
- code-mixed
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces EmoMix-3L, a novel dataset for multi-label
  emotion detection in Bangla-English-Hindi code-mixed texts. The dataset contains
  1,071 instances annotated with five emotion labels: Happy, Surprise, Neutral, Sad,
  and Angry.'
---

# EmoMix-3L: A Code-Mixed Dataset for Bangla-English-Hindi Emotion Detection

## Quick Facts
- arXiv ID: 2405.06922
- Source URL: https://arxiv.org/abs/2405.06922
- Authors: Nishat Raihan; Dhiman Goswami; Antara Mahmud; Antonios Anastasopoulos; Marcos Zampieri
- Reference count: 9
- Primary result: EmoMix-3L is a novel dataset for multi-label emotion detection in Bangla-English-Hindi code-mixed texts, with MuRIL achieving a weighted F1 score of 0.54 on the test set.

## Executive Summary
This paper introduces EmoMix-3L, a high-quality dataset for multi-label emotion detection in Bangla-English-Hindi code-mixed texts. The dataset contains 1,071 instances annotated with five emotion labels and is designed to serve as a test set for evaluating models' ability to handle transliterated words and misspellings in code-mixed data. The authors evaluate several monolingual, bilingual, and multilingual models, finding that MuRIL outperforms others, achieving a weighted F1 score of 0.54 on the EmoMix-3L test set. Additionally, they create a synthetic training set of 100,000 instances using code-mixing algorithms and demonstrate that MuRIL also performs best on synthetic data with a weighted F1 score of 0.67.

## Method Summary
The authors collect EmoMix-3L by recruiting native speakers to generate and annotate code-mixed sentences with emotion labels. They then generate a synthetic training set of 100,000 instances using the Social Media Emotion Dataset (SMED) and code-mixing algorithms (Random Code-mixing Algorithm and r-CM). Multiple models (MuRIL, XLM-R, mBERT, IndicBERT, BanglishBERT, HingBERT, BERT, DistilBERT, roBERTa, BanglaBERT, HindiBERT, emoBERTa) are fine-tuned on the synthetic dataset and evaluated on EmoMix-3L using weighted F1 score as the metric. Zero-shot prompting with GPT-3.5-turbo is also explored.

## Key Results
- MuRIL achieves the highest weighted F1 score of 0.54 on the EmoMix-3L test set.
- Synthetic data augmentation improves performance, with MuRIL achieving a weighted F1 score of 0.67 on synthetic data.
- Transliterated words and misspellings constitute 39% of tokens, posing significant challenges for emotion detection models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MuRIL outperforms other models because it is pre-trained on a broader set of Indian languages, including transliterated forms that appear frequently in the dataset.
- Mechanism: Pre-training on multilingual data with transliterated and code-mixed representations allows the model to better handle language boundary ambiguity and spelling variations.
- Core assumption: Transliterated tokens and misspellings are the main source of performance degradation in code-mixed emotion detection.
- Evidence anchors:
  - [abstract] The paper notes that MuRIL achieves the highest weighted F1 score (0.54) on the EmoMix-3L test set and 0.67 on synthetic data.
  - [section] Table 2 shows that "Other" tokens, including transliterated words, constitute 39% of the dataset.
  - [corpus] Corpus signals show related work on code-mixed embeddings, supporting the importance of multilingual pre-training for such tasks.
- Break condition: If transliterated forms are not present in the pre-training corpus or if the transliteration schemes differ significantly from those in the dataset.

### Mechanism 2
- Claim: Synthetic data generation via code-mixing algorithms improves model performance by augmenting limited natural code-mixed examples.
- Mechanism: Random code-mixing and r-CM algorithms synthesize large volumes of training data, exposing the model to diverse language alternation patterns and improving generalization.
- Core assumption: The synthetic data sufficiently covers the distribution of real code-mixed patterns, including emotional nuance and linguistic variability.
- Evidence anchors:
  - [section] The paper explicitly describes using the Social Media Emotion Dataset (SMED) and code-mixing algorithms to generate 100,000 synthetic instances.
  - [section] Performance on synthetic data (F1 0.67) exceeds that on natural test data (F1 0.54), indicating synthetic data effectiveness.
  - [corpus] Related literature on synthetic data for low-resource languages supports this approach.
- Break condition: If synthetic data does not accurately reflect real user-generated code-mixed patterns or emotional contexts.

### Mechanism 3
- Claim: Controlled data collection by native speakers ensures higher annotation quality and dataset reliability for testing.
- Mechanism: By recruiting fluent volunteers and using a two-stage annotation process with high inter-annotator agreement, the dataset minimizes label noise and reflects authentic language use.
- Core assumption: Human-generated code-mixed data is more representative of real-world usage than mined social media data, and careful annotation ensures high-quality labels.
- Evidence anchors:
  - [section] The authors describe recruiting students fluent in the three languages and achieving raw agreement of 86% and Cohen's Kappa of 0.72 in the second annotation stage.
  - [section] The dataset is explicitly positioned as a high-quality test set, not a training set, underscoring the importance of label accuracy.
  - [corpus] Corpus signals show related work emphasizing the value of gold-standard labels in code-mixing datasets.
- Break condition: If annotator fluency or agreement is insufficient, or if natural language variation is not captured.

## Foundational Learning

- Concept: Multilingual and code-mixed language modeling
  - Why needed here: EmoMix-3L contains Bangla-English-Hindi code-mixed text, requiring models that can handle multiple languages and their interactions.
  - Quick check question: Can the model process transliterated words and handle context switching between languages within the same sentence?

- Concept: Emotion detection in text
  - Why needed here: The task is multi-label emotion classification, requiring understanding of nuanced emotional cues in multilingual contexts.
  - Quick check question: Does the model differentiate between similar emotions (e.g., Happy vs. Surprise) in code-mixed sentences?

- Concept: Synthetic data generation for low-resource languages
  - Why needed here: Limited natural code-mixed data necessitates synthetic augmentation to train robust models.
  - Quick check question: Does the synthetic data preserve the emotional and linguistic diversity of the target domain?

## Architecture Onboarding

- Component map: Data ingestion → Preprocessing (tokenization, language detection) → Model (multilingual transformer) → Evaluation (weighted F1) → Error analysis
- Critical path: Collect natural test data → Generate synthetic training data → Preprocess and tokenize → Train multilingual model → Evaluate on test set → Analyze misclassifications
- Design tradeoffs: Larger synthetic datasets improve training but may introduce noise; multilingual models are more flexible but computationally heavier; controlled collection ensures quality but limits scale
- Failure signatures: Low F1 scores on transliterated tokens; high confusion between certain emotion labels; model fails to generalize from synthetic to natural data
- First 3 experiments:
  1. Train and evaluate a multilingual transformer (e.g., MuRIL) on the synthetic dataset and test on EmoMix-3L.
  2. Compare performance of models trained only on natural vs. synthetic data.
  3. Analyze error patterns focusing on transliterated and misspelled tokens.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do transliterated words and misspellings specifically impact model performance on EmoMix-3L, and what are the most effective strategies to mitigate these challenges?
- Basis in paper: [explicit] The paper explicitly mentions that transliterated words and misspellings are significant challenges for models, with over 39% of tokens classified as "Other" due to these issues.
- Why unresolved: While the paper identifies the problem, it does not provide detailed strategies or experiments to address these specific challenges.
- What evidence would resolve it: Detailed experiments testing various preprocessing techniques, transliteration normalization methods, and robust tokenization strategies specifically designed to handle transliterated words and misspellings in code-mixed datasets.

### Open Question 2
- Question: What is the optimal size and composition of a training dataset for multi-label emotion detection in Bangla-English-Hindi code-mixed texts, and how does it compare to the current synthetic dataset approach?
- Basis in paper: [inferred] The paper uses a synthetic dataset of 100,000 instances for training, but the optimal size and composition are not explored. The EmoMix-3L dataset, while high-quality, is limited to 1,071 instances.
- Why unresolved: The paper does not explore the impact of different dataset sizes or compositions on model performance, leaving questions about the sufficiency and effectiveness of the synthetic dataset.
- What evidence would resolve it: Comparative experiments with varying sizes and compositions of training datasets, including natural data collection, to determine the most effective approach for training multi-label emotion detection models.

### Open Question 3
- Question: How do large language models (LLMs) perform on EmoMix-3L compared to traditional transformer-based models, and what are the potential benefits of fine-tuning LLMs on code-mixed datasets?
- Basis in paper: [explicit] The paper mentions the potential for exploring the performance of large language models by fine-tuning them on code-mixed datasets, but does not provide experimental results.
- Why unresolved: The paper does not include experiments with LLMs, leaving their potential benefits and limitations unexplored.
- What evidence would resolve it: Experiments comparing the performance of LLMs with traditional transformer-based models on EmoMix-3L, including fine-tuning results and analysis of their ability to handle code-mixed scenarios.

## Limitations

- Dataset size is relatively small (1,071 instances) for robust evaluation and may limit generalizability.
- Synthetic data generation may not fully capture the complexity and diversity of naturally occurring code-mixed emotional expressions.
- The evaluation focuses primarily on weighted F1 scores without extensive analysis of per-class performance or linguistic error patterns.

## Confidence

- High confidence: MuRIL outperforms other models on both natural and synthetic datasets; controlled data collection yields high annotation quality.
- Medium confidence: Synthetic data augmentation improves model performance; transliterated words are a significant challenge.
- Low confidence: Claims about MuRIL's superiority being solely due to transliterated token coverage; synthetic data fully representing real-world code-mixed patterns.

## Next Checks

1. Evaluate model performance on a larger, independently collected code-mixed emotion detection dataset to assess generalization beyond EmoMix-3L.
2. Conduct ablation studies removing transliterated and misspelled tokens to isolate their specific impact on model performance differences.
3. Perform qualitative analysis of model errors on synthetic vs. natural data to identify systematic gaps in synthetic data generation.