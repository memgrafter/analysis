---
ver: rpa2
title: Autoregressive Speech Synthesis without Vector Quantization
arxiv_id: '2407.08551'
source_url: https://arxiv.org/abs/2407.08551
tags:
- melle
- speech
- sampling
- language
- all-e
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MELLE, a continuous-valued token-based autoregressive
  language model for text-to-speech synthesis that bypasses the need for vector quantization.
  MELLE directly predicts mel-spectrogram frames from text prompts using a decoder-only
  architecture with a novel latent sampling module and spectrogram flux loss to improve
  diversity and robustness.
---

# Autoregressive Speech Synthesis without Vector Quantization

## Quick Facts
- arXiv ID: 2407.08551
- Source URL: https://arxiv.org/abs/2407.08551
- Reference count: 26
- One-line primary result: MELLE achieves 47.9% relative WER reduction on continuation tasks compared to VALL-E variants while matching ground truth in subjective naturalness

## Executive Summary
MELLE introduces a continuous-valued token-based autoregressive language model for text-to-speech synthesis that bypasses vector quantization entirely. The model directly predicts mel-spectrogram frames from text using a decoder-only architecture with a novel latent sampling module and spectrogram flux loss to improve diversity and robustness. Experiments on large-scale datasets show MELLE outperforms existing VQ-based approaches across multiple metrics while achieving comparable subjective quality to ground truth speech.

## Method Summary
MELLE is a Transformer decoder-only architecture that predicts continuous mel-spectrogram frames conditioned on text input. The model uses variational inference with a learned latent sampling module to introduce diversity, and incorporates a spectrogram flux loss to encourage dynamic variation between frames. Training employs a four-loss objective combining regression (L1+L2), KL divergence, flux loss, and stop prediction. The architecture eliminates the need for separate NAR refinement stages used in discrete codec models.

## Key Results
- 47.9% relative WER reduction on continuation tasks compared to VALL-E and variants
- MELLE-C (continuous mel-spectrogram) outperforms MELLE-D (discrete mel-spectrogram) variants across all metrics
- Subjective naturalness evaluations show MELLE matches or surpasses ground truth in MOS/SMOS/CMOS scores
- Consistent performance improvements across 50K-hour Libriheavy and 960-hour LibriSpeech datasets

## Why This Works (Mechanism)

### Mechanism 1
Continuous-valued mel-spectrogram tokens eliminate fidelity loss from vector quantization while preserving richer acoustic detail. MELLE directly predicts continuous mel-spectrogram frames rather than discrete codec codes, avoiding information compression inherent in quantization. The core assumption is that mel-spectrogram representations contain sufficient information for high-quality speech reconstruction when paired with an appropriate vocoder.

### Mechanism 2
Variational inference with learned latent sampling enables robust, diverse generation in continuous space without discrete sampling strategies. A latent sampling module predicts Gaussian distributions for each timestep, samples from them using reparameterization, and maps to spectrogram space, introducing controlled diversity. The core assumption is that modeling the latent space as Gaussian conditioned on model output allows effective sampling while maintaining generation quality.

### Mechanism 3
Spectrogram flux loss encourages dynamic variation between consecutive frames, preventing repetitive or static outputs. A loss term penalizes low variability between predicted Gaussian means and previous ground truth frames, rewarding changes in generated sequence. The core assumption is that explicitly encouraging frame-to-frame variation during training prevents overly static or repetitive speech patterns.

## Foundational Learning

- Concept: Autoregressive language modeling
  - Why needed here: MELLE treats mel-spectrogram generation as sequence prediction where each frame depends on previous frames and text context
  - Quick check question: How does conditioning on both text and previous acoustic frames enable zero-shot TTS synthesis?

- Concept: Variational inference and latent variable modeling
  - Why needed here: Enables sampling in continuous space to introduce diversity and robustness without discrete token sampling strategies
  - Quick check question: What role does KL divergence loss play in regularizing learned latent space distribution?

- Concept: Regression loss vs. cross-entropy loss
  - Why needed here: Continuous-valued tokens require regression objectives rather than discrete classification objectives used in VQ-based models
  - Quick check question: Why is L1/L2 regression loss more appropriate than cross-entropy for predicting continuous mel-spectrogram values?

## Architecture Onboarding

- Component map: Text tokenizer (BPE) → Text embedding layer → Transformer decoder LM → Latent sampling module → Stop prediction layer → Post-net (convolutional blocks) → Vocoder
- Critical path: Text → LM embedding → Latent sampling → Mel-spectrogram generation → Post-net refinement → Vocoder → Speech output
- Design tradeoffs: Single-stage vs. two-stage (MELLE eliminates NAR refinement stage), reduction factor r balances inference speed vs. generation quality, continuous vs. discrete tokens trade fidelity for sampling complexity
- Failure signatures: Excessive silence/repetition (spectrogram flux loss issues), poor speaker similarity (latent sampling module problems), unstable generation (improper KL loss weighting)
- First 3 experiments: 1) Train baseline MELLE without latent sampling or flux loss, 2) Enable latent sampling module only, 3) Enable both components with different reduction factors (r=1,2,3,4,5)

## Open Questions the Paper Calls Out

1. How does MELLE's performance scale with training data size beyond 50K hours, and is there a point of diminishing returns?
2. What is the impact of different continuous speech representations (e.g., VAE latent states) compared to mel-spectrograms in autoregressive TTS models like MELLE?
3. How does MELLE's latent sampling module compare to explicit sampling strategies used in discrete codec models like top-p sampling in terms of diversity and quality trade-offs?

## Limitations
- Lacks ablation studies isolating impact of continuous tokens, variational inference, and flux loss on performance improvements
- All experiments limited to LibriSpeech/LibriVox-derived datasets without testing generalization to diverse real-world speech
- No comprehensive computational complexity analysis comparing MELLE to VQ-based approaches across different hardware configurations

## Confidence

- **High Confidence**: Experimental methodology is well-documented with clear datasets, metrics, and training procedures following established speech synthesis patterns
- **Medium Confidence**: Claims about continuous tokens eliminating VQ-related fidelity loss are plausible but not rigorously validated through direct empirical evidence
- **Low Confidence**: Novel components like spectrogram flux loss and variational inference implementation lack direct empirical justification through ablation studies

## Next Checks

1. Conduct ablation study training MELLE variants with individual components disabled to quantify each mechanism's contribution to performance improvements
2. Evaluate MELLE on challenging out-of-domain datasets including noisy speech, accented speech, and spontaneous dialogue to assess generalization beyond clean read speech
3. Perform comprehensive benchmarking comparing MELLE's inference speed, memory usage, and training efficiency against VALL-E and other SOTA TTS models across different reduction factors and hardware configurations