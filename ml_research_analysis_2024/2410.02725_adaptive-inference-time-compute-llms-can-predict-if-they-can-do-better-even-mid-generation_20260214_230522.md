---
ver: rpa2
title: 'Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even
  Mid-Generation'
arxiv_id: '2410.02725'
source_url: https://arxiv.org/abs/2410.02725
tags:
- samples
- reward
- adaptive
- compute
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces capability-aware and mid-generation self-evaluations
  to make inference-time compute more efficient for large language models (LLMs).
  The key idea is to train LLMs to predict mid-generation the probability that restarting
  will yield a better response, enabling adaptive decisions about sampling and pruning
  without external reward models.
---

# Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation

## Quick Facts
- **arXiv ID**: 2410.02725
- **Source URL**: https://arxiv.org/abs/2410.02725
- **Reference count**: 7
- **Key outcome**: Llama 3.1 8B win rate against GPT-4 increases from 21% to 34% with 16 samples; GSM8K accuracy improves from 84% to 91%

## Executive Summary
This work introduces capability-aware and mid-generation self-evaluations to make inference-time compute more efficient for large language models. The approach trains LLMs to predict mid-generation whether restarting will yield a better response, enabling adaptive decisions about sampling and pruning without external reward models. By leveraging token prediction with on-policy pairwise preference data, the method achieves significant efficiency gains while maintaining or improving performance. Experimental results demonstrate that adaptive sampling achieves 74% of the improvement with only 1.2 samples on average, while early pruning saves 56% of tokens generated with minimal performance degradation.

## Method Summary
The method trains Llama 3.1 8B on an on-policy pairwise preference dataset with ties to predict the probability that restarting generation will yield better responses. The self-evaluation is performed by appending a prompt ("Would you do better if you started over? ("Yes." or "No.")") and predicting a single token ("No" for good, "Yes" for bad). This capability-aware evaluation enables adaptive sampling with exponentially increasing batch sizes and temperature annealing, as well as early pruning of unpromising samples based on mid-generation self-evaluations. The approach is evaluated on AlpacaEval (800 prompts, win rate vs GPT-4) and GSM8K (8.5K math problems), showing significant efficiency improvements over traditional Best-of-N sampling.

## Key Results
- AlpacaEval: Win rate against GPT-4 increases from 21% to 34% with 16 samples
- GSM8K: Pass@1 accuracy improves from 84% to 91%
- Adaptive sampling achieves 74% of the improvement with only 1.2 samples on average
- Early pruning saves 56% of tokens generated with minimal performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Token prediction enables cost-effective self-evaluation by reusing KV cache and leveraging zero-shot capability
- **Mechanism**: Instead of requiring a separate reward model that processes inputs from scratch, the LLM appends a self-evaluation prompt and predicts a predefined token (good/bad). This single-token prediction reuses the KV cache from the original generation, making it computationally inexpensive while maintaining the model's existing knowledge about its own capabilities.
- **Core assumption**: The model's zero-shot capability to judge its own responses is sufficiently accurate for self-evaluation
- **Evidence anchors**: [abstract]: "This capability is very inexpensive as it involves generating a single predefined token"; [section]: "This approach allows us to acquire rewards without any external reward model, making it highly cost-effective as we can reuse the KV cache obtained during the generation of the response"

### Mechanism 2
- **Claim**: On-policy pairwise preferences with ties enable modeling probability that model cannot generate better response
- **Mechanism**: By training on responses generated by the same model with preference labels including ties, the model learns to predict the probability that restarting generation will not yield a better response. Ties are crucial because they represent cases where the model's capability is fully utilized.
- **Core assumption**: On-policy data with ties provides sufficient signal about model capability boundaries
- **Evidence anchors**: [abstract]: "These predictions are obtained without an external reward model and can be used to decide whether or not to generate more samples"; [section]: "Accounting for ties in reward modeling is especially important for on-policy pairwise data where responses coming from the same model are very likely to be similar"

### Mechanism 3
- **Claim**: Mid-generation self-evaluations enable early pruning of unpromising samples
- **Mechanism**: The model can evaluate partial responses (truncated sequences) to determine if they are likely to result in poor completions. This allows stopping computation early on unpromising samples, saving resources while maintaining quality through continued generation of promising samples.
- **Core assumption**: Partial responses contain sufficient information to predict final quality
- **Evidence anchors**: [abstract]: "We further demonstrate that 50-75% of samples can be pruned early in generation with minimal degradation in performance"; [section]: "Modeling these probabilities with an on-policy pairwise preference dataset with ties... includes the same examples but with responses randomly truncated"

## Foundational Learning

- **Concept**: Bradley-Terry model for pairwise comparisons
  - **Why needed here**: Understanding how preference data is structured and used to train reward models
  - **Quick check question**: How does the Bradley-Terry model handle ties versus strict wins/losses?

- **Concept**: Temperature scaling in sampling
  - **Why needed here**: The paper uses temperature annealing schedules to balance exploration/exploitation during adaptive sampling
  - **Quick check question**: What happens to the sampling distribution as temperature approaches 0 versus 1?

- **Concept**: Cross-entropy loss for token prediction
  - **Why needed here**: The self-evaluation model is trained using SFT loss with token prediction targets
  - **Quick check question**: How does token prediction loss differ from traditional reward model training?

## Architecture Onboarding

- **Component map**: Base LLM (Llama 3.1 8B) -> Self-evaluation prompt -> Preference dataset -> Adaptive sampling controller -> Pruning controller
- **Critical path**: Generate response → Evaluate capability → Decide resample/prune → Select best sample
- **Design tradeoffs**: Single token prediction vs full reward model (lower cost but potentially less nuanced); On-policy vs off-policy training (more relevant but requires more data collection); Early pruning vs wait for full generation (faster but risk of incorrect pruning)
- **Failure signatures**: Overconfident self-evaluation (always says "No" to improvement); Underconfident self-evaluation (always says "Yes" to improvement); Poor distinction between ties and clear wins/losses; Latency spikes from too many resampling iterations
- **First 3 experiments**:
  1. Verify self-evaluation accuracy on simple prompts with known answers
  2. Test adaptive sampling on GSM8K with varying thresholds
  3. Measure pruning effectiveness at different truncation points (64 vs 128 tokens)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of capability-aware self-evaluations scale with the size of the underlying reward model used to generate the on-policy pairwise preference dataset?
- **Basis in paper**: [explicit] The paper uses ArmoRM (trained on ~1M preferences) to generate training data for Llama 3.1 8B, and notes that a 70B model achieves 38.1% on AlpacaEval vs 33.8% for 8B, suggesting model size matters
- **Why unresolved**: The paper only tests one reward model size and one base model size. No ablation on reward model quality/size is provided
- **What evidence would resolve it**: Experiments varying the size/quality of the underlying reward model (e.g., 100K vs 1M vs 10M preferences) while keeping the self-evaluation model fixed

### Open Question 2
- **Question**: What is the optimal timing and number of tokens for mid-generation self-evaluation in early pruning?
- **Basis in paper**: [explicit] The paper tests pruning at 64 and 128 tokens but notes "the timing of when we perform self-evaluations significantly affects performance"
- **Why unresolved**: Only two fixed points tested; no systematic exploration of the tradeoff between evaluation cost and accuracy across different token lengths
- **What evidence would resolve it**: Experiments varying evaluation timing (e.g., 32, 64, 128, 256 tokens) and measuring both accuracy and computational savings across diverse tasks

### Open Question 3
- **Question**: Can capability-aware self-evaluations be extended to other inference-time compute strategies beyond Best-of-N, such as beam search or tree search?
- **Basis in paper**: [inferred] The paper discusses adaptive sampling and early pruning as two primitives but only demonstrates them with Best-of-N; section 5 mentions "It may be possible to do different types of search with active capability-aware self-evaluations"
- **Why unresolved**: No experiments or theoretical analysis of how self-evaluation probabilities would work with other search strategies
- **What evidence would resolve it**: Implementation and evaluation of capability-aware self-evaluations with beam search, showing how win/tie probabilities guide beam expansion and pruning decisions

## Limitations

- The effectiveness depends heavily on the quality of on-policy preference data and the model's ability to generalize from this specific distribution
- Self-evaluation accuracy degrades as evaluation difficulty increases (73% for mid-generation truncation vs 80% for simple prompts)
- Critical hyperparameters like the win/tie threshold and ArmoRM configuration are not fully detailed
- The approach may overfit to the LMSYS prompt distribution without testing on cross-distribution data

## Confidence

- **High Confidence**: The core claim that token prediction enables cost-effective self-evaluation (Mechanism 1) is well-supported by the experimental results showing significant efficiency gains (1.2 average samples vs 16 for baseline, 56% token savings from pruning)
- **Medium Confidence**: The claim about on-policy pairwise preferences with ties enabling accurate capability boundary detection (Mechanism 2) is supported by the AlpacaEval and GSM8K results, but the exact contribution of ties versus other factors is not isolated
- **Medium Confidence**: The mid-generation pruning effectiveness (Mechanism 3) shows strong quantitative results, but the qualitative analysis of why pruning works (or fails) at different truncation points is limited

## Next Checks

1. **Ablation Study on Tie Threshold**: Systematically vary the win/tie threshold (0.005 to 0.05) to quantify its impact on adaptive sampling performance and self-evaluation accuracy, isolating the contribution of ties to the overall approach

2. **Cross-Distribution Generalization**: Test the trained self-evaluation model on prompts from distributions not seen during training (e.g., different benchmarks or real-world usage data) to assess whether the model overfits to the LMSYS prompt distribution

3. **Latent Capability Analysis**: Examine the relationship between model capability (e.g., Llama 3.1 8B vs 70B) and self-evaluation accuracy, particularly for tasks requiring complex reasoning, to determine if the approach scales to more capable models or different model families