---
ver: rpa2
title: 'Squid: Long Context as a New Modality for Energy-Efficient On-Device Language
  Models'
arxiv_id: '2408.15518'
source_url: https://arxiv.org/abs/2408.15518
tags:
- context
- language
- squid
- arxiv
- long
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Squid, a novel decoder-decoder architecture
  designed to address the energy consumption and latency challenges of on-device language
  models when processing long contexts. Squid employs a compact 0.5B parameter decoder
  to distill extensive contextual information into memory embeddings, reducing the
  input length for the primary 7B parameter decoder.
---

# Squid: Long Context as a New Modality for Energy-Efficient On-Device Language Models

## Quick Facts
- arXiv ID: 2408.15518
- Source URL: https://arxiv.org/abs/2408.15518
- Authors: Wei Chen; Zhiyuan Li; Shuo Xin; Yihao Wang
- Reference count: 40
- One-line primary result: 10x improvement in energy efficiency and 5x reduction in latency while maintaining accuracy for long-context on-device language models

## Executive Summary
Squid introduces a novel decoder-decoder architecture that treats long context as a new modality, achieving 10-fold energy efficiency improvement and 5-fold latency reduction compared to conventional full-length context processing. The approach uses a compact 0.5B parameter decoder to compress long contexts into memory embeddings, which are then projected into the embedding space of a 7B parameter main decoder. This architecture enables on-device language models to handle substantially longer contexts while maintaining high accuracy across various task categories.

## Method Summary
Squid employs a decoder-decoder architecture where a small 0.5B parameter decoder (πs) processes long contexts and extracts memory embeddings through special memory tokens, which are then projected via an MLP into the 7B parameter main decoder's (πl) embedding space. The system uses multi-stage training: restoration training reconstructs compressed contexts, continual training generates context continuations, and instruction fine-tuning produces responses to queries. This approach effectively reduces input length from L tokens to N tokens while preserving semantic information, enabling energy-efficient processing of long contexts on resource-constrained devices.

## Key Results
- 10-fold improvement in energy efficiency compared to conventional full-length context processing
- 5-fold reduction in latency while maintaining high accuracy
- Successfully processes substantially longer contexts without compromising quality
- Maintains performance across various task categories including questions about context, situation, and sentiment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating long context as a new modality via projector-encoder decoupling enables efficient compression.
- Mechanism: The compact 0.5B decoder encodes context into memory embeddings, and the projector maps these into the main decoder's embedding space, effectively reducing input length from L to N tokens while preserving semantic information.
- Core assumption: The projector can accurately map embeddings from the small decoder's space to the large decoder's space without significant information loss.
- Evidence anchors:
  - [abstract]: "Inspired by vision-language models, we repurpose the image embedding projector to encode long textual contexts, effectively treating extended context as a distinct modality."
  - [section]: "Drawing inspiration from vision-language models, we repurpose the image embedding projector to encode long textual contexts, effectively treating extended context as a distinct modality."
  - [corpus]: Weak evidence - no direct citations about projector usage in language models found in neighbor corpus.

### Mechanism 2
- Claim: Memory tokens provide a compact latent representation of long contexts.
- Mechanism: Special tokens [memory_i] are appended to the context and processed by the small decoder, with their embeddings extracted to form a compressed representation M that captures the essence of the long context.
- Core assumption: The small decoder can effectively compress long contexts into a fixed number of memory tokens without losing critical information.
- Evidence anchors:
  - [section]: "To facilitate the extraction of information from long contexts using the text encoder model πs, we introduce the concept of memory tokens... These additional tokens serve to capture a latent representation of the long context C."
  - [corpus]: Weak evidence - no direct citations about memory token usage in language models found in neighbor corpus.

### Mechanism 3
- Claim: Multi-stage training progressively enhances the model's ability to handle long contexts and generate appropriate responses.
- Mechanism: Three training stages (restoration, continual, instruction fine-tuning) build capabilities sequentially, first learning to reconstruct compressed contexts, then generating context continuations, and finally producing responses to queries.
- Core assumption: Each training stage provides necessary prerequisite skills for the next stage, culminating in effective long-context understanding and response generation.
- Evidence anchors:
  - [section]: "Our training process for the Squid model comprises three distinct stages: restoration training, continual training, and instruction fine-tuning. This multi-stage approach is designed to progressively enhance the model's ability to handle long contexts and generate appropriate responses."
  - [corpus]: Weak evidence - no direct citations about multi-stage training for long context models found in neighbor corpus.

## Foundational Learning

- Concept: Transformer decoder architecture
  - Why needed here: Squid is built on Qwen2 decoder models, so understanding transformer decoder mechanics is essential for modifying and debugging the architecture.
  - Quick check question: What is the difference between a transformer decoder and a transformer encoder in terms of attention mechanism?

- Concept: Modality-specific embedding spaces
  - Why needed here: The projector bridges different embedding dimensions (896 for small decoder, 3584 for large decoder), requiring understanding of how different model components can share information.
  - Quick check question: Why might different components of a multimodal model have different embedding dimensions?

- Concept: Multi-stage training methodology
  - Why needed here: Squid's effectiveness depends on the specific sequence of training stages, so understanding why this progression works is critical for troubleshooting and improvements.
  - Quick check question: What is the purpose of each training stage in Squid's multi-stage training process?

## Architecture Onboarding

- Component map:
  - Text encoder (Qwen2 0.5B) -> Projector (MLP) -> Main decoder (Qwen2 7B) -> Response
  - Memory tokens capture compressed context information

- Critical path:
  1. Context C enters text encoder πs
  2. Memory token embeddings M are extracted
  3. Projector Φ transforms M to match main decoder's embedding space
  4. Main decoder πl receives query Q and projected embeddings
  5. Response R is generated

- Design tradeoffs:
  - Compression ratio vs. information retention: Higher compression (smaller N) reduces computation but may lose information
  - Memory token count vs. context length: More tokens can capture more information but increase computational cost
  - Projector complexity vs. accuracy: More complex projectors may better map embedding spaces but increase parameters and computation

- Failure signatures:
  - Context compression fails: Responses contain irrelevant or missing information from the original context
  - Projector mapping fails: Responses are nonsensical or contain artifacts from embedding space mismatch
  - Training stage issues: Model struggles with tasks appropriate to specific stages (e.g., cannot reconstruct contexts during restoration training)

- First 3 experiments:
  1. Test context compression quality: Feed a long context through the small decoder and projector, then measure reconstruction accuracy using the main decoder
  2. Evaluate projector mapping: Compare embeddings before and after projector transformation to ensure proper dimension alignment
  3. Validate multi-stage training progression: Test model capabilities after each training stage to confirm prerequisite skills are learned

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Architecture Details Uncertainty: The paper lacks specific implementation details for critical components like the projector Φ that bridges the two decoders with different embedding dimensions.
- Dataset Composition Uncertainty: While specific dataset sizes are mentioned for each training stage, details about content, sources, or curation methods are not provided.
- Energy Efficiency Measurement Uncertainty: The claimed 10-fold improvement lacks detailed methodology for measurement on actual hardware or normalization procedures.

## Confidence
**High Confidence Claims**:
- The fundamental concept of using a decoder-decoder architecture for context compression is technically sound and well-grounded in existing transformer research.

**Medium Confidence Claims**:
- The specific performance metrics (10x energy efficiency, 5x latency reduction) depend heavily on implementation details and measurement methodology that are not fully specified.

**Low Confidence Claims**:
- Claims about maintaining "high accuracy across various task categories" without specifying the exact evaluation methodology or baseline comparisons.

## Next Checks
1. Implement and test the projector Φ component by creating a controlled experiment that measures embedding space alignment, comparing cosine similarity and reconstruction accuracy against a baseline where embeddings are directly passed without projection.

2. Conduct ablation studies varying the number of memory tokens (N) while keeping other parameters constant, measuring compression quality using context reconstruction metrics and identifying the minimum N that achieves acceptable performance.

3. Reproduce the energy efficiency measurements using standardized hardware and measurement protocols, comparing Squid against both the full-length context baseline and other context compression approaches under identical conditions.