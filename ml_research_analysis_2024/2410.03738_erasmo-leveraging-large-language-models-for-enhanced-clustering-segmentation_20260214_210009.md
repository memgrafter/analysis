---
ver: rpa2
title: 'ERASMO: Leveraging Large Language Models for Enhanced Clustering Segmentation'
arxiv_id: '2410.03738'
source_url: https://arxiv.org/abs/2410.03738
tags:
- clustering
- data
- erasmo
- embeddings
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ERASMO, a framework that leverages large language
  models (LLMs) to generate high-quality embeddings from tabular data for enhanced
  clustering analysis. The key innovation lies in fine-tuning a pretrained LLM on
  textually encoded tabular data using techniques like random feature sequence shuffling
  and number verbalization.
---

# ERASMO: Leveraging Large Language Models for Enhanced Clustering Segmentation

## Quick Facts
- **arXiv ID**: 2410.03738
- **Source URL**: https://arxiv.org/abs/2410.03738
- **Reference count**: 24
- **Primary result**: ERASMO framework significantly outperforms state-of-the-art methods in clustering performance across multiple metrics

## Executive Summary
ERASMO introduces a novel framework that leverages large language models (LLMs) to generate high-quality embeddings from tabular data for enhanced clustering analysis. The key innovation lies in fine-tuning a pretrained LLM on textually encoded tabular data using techniques like random feature sequence shuffling and number verbalization. The framework consists of two main stages: fine-tuning a pretrained LLM on textually encoded tabular data, and generating embeddings from the fine-tuned model. Extensive experiments on multiple real-world datasets demonstrate that ERASMO significantly outperforms state-of-the-art methods in clustering performance, achieving superior results across multiple metrics including Silhouette Score, Calinski-Harabasz Index, and Davies-Bouldin Index.

## Method Summary
ERASMO transforms tabular data into textual format using subject-predicate-object clauses, then fine-tunes a pretrained LLM on this encoded data. The framework incorporates random feature sequence shuffling to remove positional dependencies and optional number verbalization to enhance numerical context understanding. After fine-tuning, the model generates embeddings that capture semantic relationships in the data, which are then used with standard clustering algorithms like k-means, agglomerative hierarchical clustering (AHC), and spectral clustering.

## Key Results
- ERASMO achieves superior clustering performance compared to state-of-the-art methods across multiple metrics
- The framework demonstrates significant improvements in Silhouette Score, Calinski-Harabasz Index, and Davies-Bouldin Index
- ERASMO's tailored embeddings provide more precise and contextually relevant clusters for tabular datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Textual encoding of tabular data enables LLMs to process heterogeneous features effectively.
- Mechanism: The textual converter transforms each row into a sequence of subject-predicate-object clauses, allowing the LLM to interpret both categorical and numerical data within a natural language framework.
- Core assumption: The LLM's language understanding capabilities extend meaningfully to structured tabular semantics when data is textually represented.
- Evidence anchors:
  - [abstract] "ERASMO employs a textual converter to transform tabular data into a textual format, enabling the language model to process and understand the data more effectively."
  - [section 3.1] "Standard pretrained generativeLLMs expect sequences of words as inputs. Hence, we convert each row of our dataset into a textual representation to apply anLLM to tabular data."
- Break condition: If the LLM fails to learn meaningful relationships between features when trained on textually encoded tabular data, the entire approach collapses.

### Mechanism 2
- Claim: Random feature sequence shuffling removes artificial positional dependencies and improves generalization.
- Mechanism: By permuting the order of textually encoded features for each sample, the model learns feature relationships independent of column order, enabling more robust embeddings.
- Core assumption: Tabular data has no inherent spatial ordering relationship between features, and LLMs can learn these relationships regardless of feature sequence.
- Evidence anchors:
  - [section 3.1] "There is no spatial ordering relationship between features in tabular datasets. We randomly permute the encoded short sentencesti,j of the full textual representationti to reconstruct the feature order independence."
- Break condition: If shuffling degrades model performance, it suggests the LLM relies on positional information that should not exist in tabular data.

### Mechanism 3
- Claim: Number verbalization improves the model's understanding of numerical context and relationships.
- Mechanism: Converting numerical tokens to their verbal representations helps the LLM capture contextual meaning of numerical values rather than treating them as isolated tokens.
- Core assumption: Verbalizing numbers enhances the model's semantic understanding of numerical data in specific contexts.
- Evidence anchors:
  - [section 3.1] "There is evidence that verbalizing numerical tokens can enhance effectiveness in specific scenarios [8]."
  - [section 3.1] "In some NLP tasks, such as clustering with embeddings, sentiment analysis, and text classification, verbalizing numbers can improve the model's understanding of the context and meaning of numerical values."
- Break condition: If verbalization consistently underperforms non-verbalized numerical tokens, it suggests the benefit is task-specific rather than universal.

## Foundational Learning

- Concept: Textual encoding of structured data
  - Why needed here: LLMs process text, not structured tables, so data must be transformed into a language model-compatible format
  - Quick check question: How does the subject-predicate-object encoding preserve the relationship between feature names and their values?

- Concept: Feature order permutation in machine learning
  - Why needed here: Tabular data has no inherent feature order, and removing this artificial dependency improves model generalization
  - Quick check question: What would happen if we trained the model on consistently ordered features versus randomly ordered features?

- Concept: Number representation in language models
  - Why needed here: Numerical tokens may be processed differently than verbalized numbers, affecting semantic understanding
  - Quick check question: When might verbalizing numbers actually hurt model performance rather than help it?

## Architecture Onboarding

- Component map: Textual Converter → Random Feature Shuffler → (Optional Number Verbalizer) → LLM Fine-tuning → Embedding Generator → Clustering Algorithm
- Critical path: The pipeline must complete fine-tuning before generating embeddings for clustering analysis
- Design tradeoffs: Number verbalization improves semantic understanding but increases token count and computational cost; shuffling improves generalization but may reduce training stability
- Failure signatures: Poor clustering scores (low SS, high DBI) indicate issues in any pipeline stage; specifically low SS with high CHI suggests well-separated but poorly cohesive clusters
- First 3 experiments:
  1. Run ERASMObase (without verbalization) on a small dataset to verify the basic pipeline works
  2. Compare ERASMObase vs ERASMONV on the same dataset to measure the impact of number verbalization
  3. Test different clustering algorithms (k-means, AHC, Spectral) with the generated embeddings to identify the best combination for a given dataset

## Open Questions the Paper Calls Out
None

## Limitations

- **Dataset Bias and Generalization** (Medium Confidence): Performance on larger, noisier real-world datasets with missing values, outliers, or complex feature interactions remains uncertain.
- **Computational Efficiency** (Medium Confidence): Fine-tuning large language models requires substantial computational resources, and reported training times are not provided.
- **Clustering Algorithm Dependency** (High Confidence): Performance varies significantly across different clustering algorithms, suggesting effectiveness may depend more on downstream clustering choices than acknowledged.

## Confidence

- Dataset Bias and Generalization: Medium
- Computational Efficiency: Medium
- Clustering Algorithm Dependency: High

## Next Checks

1. **Scale and Noise Testing**: Evaluate ERASMO on large-scale datasets (10K+ samples) with varying levels of noise, missing values, and feature correlation to assess robustness and computational scalability.

2. **Cross-Domain Transferability**: Test whether a single fine-tuned ERASMO model can effectively cluster data across different domains without domain-specific retraining.

3. **Ablation Study on Core Components**: Systematically remove or modify each innovation (feature shuffling, number verbalization) to quantify their individual contributions to performance gains.