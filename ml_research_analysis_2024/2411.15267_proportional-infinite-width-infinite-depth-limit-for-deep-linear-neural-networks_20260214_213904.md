---
ver: rpa2
title: Proportional infinite-width infinite-depth limit for deep linear neural networks
arxiv_id: '2411.15267'
source_url: https://arxiv.org/abs/2411.15267
tags:
- neural
- where
- distribution
- limit
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies deep linear neural networks in a proportional
  limit regime where both depth and width grow at the same rate. The authors extend
  previous work showing that such networks converge to a mixture of Gaussians rather
  than a single Gaussian process, which is the case in the infinite-width limit.
---

# Proportional infinite-width infinite-depth limit for deep linear neural networks

## Quick Facts
- arXiv ID: 2411.15267
- Source URL: https://arxiv.org/abs/2411.15267
- Authors: Federico Bassetti; Lucia Ladelli; Pietro Rotondo
- Reference count: 6
- Key outcome: Deep linear networks in proportional limit converge to mixture of Gaussians, not single GP, enabling feature learning

## Executive Summary
This paper extends previous work on deep linear neural networks by studying the proportional limit regime where both depth and width grow at the same rate. Unlike the infinite-width limit which produces a single Gaussian process, the proportional limit yields a non-trivial mixture of Gaussians characterized by multiple stochastic Brownian integrals. This mixture structure retains correlations between outputs and allows the network to learn data-dependent features, addressing a key limitation of the Gaussian process regime. The authors derive explicit expressions for both prior and posterior distributions under Gaussian likelihood, showing that the covariance structure depends on observed labels in the proportional limit.

## Method Summary
The paper studies deep linear neural networks where both depth L and width N grow proportionally (L/N → a > 0). Using Wishart-distributed random weight matrices and Bartlett decomposition, the authors derive the asymptotic distribution of the product of random matrices through stochastic integral representations. The limiting distribution is characterized as a mixture of Gaussians, with explicit formulas for prior and posterior distributions. The analysis extends from single-input/single-output cases to general dimensions, using tools from random matrix theory and stochastic calculus.

## Key Results
- In proportional limit, deep linear networks converge to a non-trivial mixture of Gaussians, not a single Gaussian process
- The limiting distribution is characterized by multiple stochastic Brownian integrals involving independent Brownian motions
- Posterior covariance structure depends on observed labels, unlike in infinite-width Gaussian process regime
- Diagonal elements converge to log-normal distribution while off-diagonal elements preserve inter-output correlations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proportional limit avoids Gaussian universality because the depth-to-width ratio a > 0 preserves correlations in the random weight products.
- Mechanism: As both depth L and width N diverge while L/N → a > 0, the central limit effect that would produce a single Gaussian is counteracted by the non-vanishing ratio. The product of Wishart matrices V_L ··· V_1 retains structure through the stochastic integrals involving Brownian motions.
- Core assumption: The limit regime (6) holds with 0 < a < ∞ and the Wishart structure of the random matrices is preserved in the scaling.
- Evidence anchors:
  - [abstract] "the limiting distribution can be characterized by multiple stochastic Brownian integrals, leading to a non-trivial mixture of Gaussians that retains correlations between outputs"
  - [section 3.1] "it is very easy to describe the limiting distribution of V̄_L,N. First of all one has V̄_L,N = exp(1/2 ∑_{ℓ=1}^L log(Q_ℓ,N))" showing how the sum of logs remains non-Gaussian
  - [corpus] Weak; no direct matches to Brownian integral representation, but related work on Gaussian process limits in neighbor 139088 discusses similar kernel structures
- Break condition: If a = 0 or if the Wishart assumption fails (e.g., heavy-tailed initializations), the Gaussian process regime is recovered.

### Mechanism 2
- Claim: The limiting distribution is a non-trivial mixture of Gaussians whose covariance depends on observed labels, enabling feature learning.
- Mechanism: In the proportional limit, the posterior Q_{L,N}(·|y₁:ₚ) remains non-degenerate and label-dependent, so the covariance Σ*(Q|˜X) varies with the data, unlike the fixed NNGP covariance.
- Core assumption: The likelihood L(y₁,...,yₚ|s₁,...,sₚ) is bounded and continuous (as required by Proposition 3.3).
- Evidence anchors:
  - [abstract] "the covariance structure depends on observed labels in the proportional limit, unlike in the infinite-width case"
  - [section 3.3] Proposition 3.4 shows the explicit mixture form N(ds₀:ₚ|m*(Q|˜X,y₁:ₚ), Σ*(Q|˜X)) with Q∞(dQ|˜X,y₁:ₚ) label-dependent
  - [corpus] No direct match; neighbor 30258 discusses proportional regime but focuses on information-theoretic reduction, not mixture structure
- Break condition: If the likelihood is degenerate or the prior collapses (e.g., λ_L^* → 0), the posterior reduces to a single Gaussian.

### Mechanism 3
- Claim: The diagonal elements converge to log-normal while off-diagonal elements vanish, giving a structured limit matrix V̄_∞.
- Mechanism: By Lemma 4.1, the diagonal entries (V̄_L,N)_{r,r} → e^{Z_r} with Z_r ~ N(-a r/2, a/2), producing log-normal scaling; Lemma 4.2 shows off-diagonal variances → 0 when a = 0, but remain O(1) when a > 0, preserving inter-output correlations.
- Core assumption: min(N_ℓ:ℓ=1,...,L) > D so Wishart matrices are well-defined and the Bartlett decomposition applies.
- Evidence anchors:
  - [section 4.1] Lemma 4.1 explicitly computes E[e^{s log(V̄_L,N)_{r,r}}] → e^{a s²/2 - s r a/2}, proving log-normality
  - [section 4.2] Proposition 4.3 constructs the Skorokhod representation linking V_N^{ℓ,r,r} to independent Brownian motions
  - [corpus] No direct matches; neighbor 88892 discusses continuous-space limits but not log-normal diagonal structure
- Break condition: If D = 1 the result reduces to a single log-normal (known from Hanin 2024); if a = 0 the diagonals → 1 and off-diagonals → 0.

## Foundational Learning

- Concept: Wishart distributions and Bartlett decomposition
  - Why needed here: The prior over outputs is expressed in terms of products of Wishart matrices (Proposition 2.1), and their limiting law requires understanding the Bartlett factors V_ℓ.
  - Quick check question: If Q ~ W(N, I_D), what is the distribution of the diagonal entries of its lower-triangular Cholesky factor V?

- Concept: Stochastic calculus and multiple integrals
  - Why needed here: The limit representation of V̄_L,N involves stochastic integrals over Brownian motions (equations (7)-(8)); proving convergence requires control of these integrals.
  - Quick check question: For independent standard Brownian motions W^{(i,j)}_t, is the integral ∫₀¹ e^{Z^{(k)}_t} dW^{(k)}_t well-defined, and what is its distribution?

- Concept: Matrix normal and Kronecker product identities
  - Why needed here: The posterior derivation in Appendix C uses properties of matrix-normal distributions and the equivalence vec(MN(0,Σ₁,Σ₂)) ~ N(0,Σ₂⊗Σ₁).
  - Quick check question: If Z ~ MN(0, Σ₁, Σ₂), what is the covariance of vec(Z)?

## Architecture Onboarding

- Component map: Prior → Wishart product V̄_L,N → Skorokhod representation → Brownian integrals → Limit V̄_∞ → Posterior mixture → Predictive
- Critical path: Compute V̄_L,N via Bartlett factors → Show convergence (Propositions 3.1, 3.2) → Derive posterior (Proposition 3.4) → Analyze predictive covariance
- Design tradeoffs: Using Bartlett factors gives explicit limit but requires careful handling of triangular structure; alternative Cholesky decompositions would complicate the stochastic integral representation.
- Failure signatures: If min(N_ℓ) ≤ D, Wishart matrices are singular → prior collapses; if a = 0, Gaussian limit recovered → no feature learning; if likelihood unbounded, posterior may not exist.
- First 3 experiments:
  1. Simulate V̄_L,N for D=1, varying a∈(0,1], verify log-normal convergence and compare to analytic N(-a/2, a/2).
  2. For D=2, simulate V̄_L,N, plot empirical covariance of off-diagonal entries vs. 1/N, check decay rate matches Lemma 4.2 prediction.
  3. Implement posterior sampling from Q_{L,N}(·|y₁:ₚ) for synthetic Gaussian data, compare empirical predictive covariance to NNGP baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the non-Gaussian mixture limit in the proportional regime and feature learning capabilities in finite networks?
- Basis in paper: [explicit] The paper states that "the proportional limit is closer to finite network behaviour than the infinite-width setting" and discusses how the mixture of Gaussians allows the network to learn dependencies in the responses
- Why unresolved: While the paper demonstrates that the proportional limit retains correlations and data-dependent features, it doesn't provide quantitative comparisons of feature learning performance between finite networks, proportional limit, and infinite-width regimes
- What evidence would resolve it: Empirical studies comparing feature learning metrics (e.g., mutual information between features and labels) across finite networks, proportional limit approximations, and NNGP models on standard datasets

### Open Question 2
- Question: How does the mixture distribution's dependence on observed labels in the posterior affect generalization bounds compared to the NNGP regime?
- Basis in paper: [explicit] The paper notes that "unlike in the NNGP regime, in the proportional limit the output correlations depend on the observed labels"
- Why unresolved: The paper derives the posterior distribution but doesn't analyze how this label-dependent covariance structure impacts generalization performance or theoretical bounds
- What evidence would resolve it: Formal generalization bounds for the proportional limit regime and empirical validation showing differences in test performance compared to NNGP

### Open Question 3
- Question: Can the stochastic Brownian integral representation of the limiting distribution be extended to non-linear activation functions in the proportional limit?
- Basis in paper: [inferred] The paper focuses exclusively on linear activation functions and derives the limiting distribution using specific properties of linear networks, while mentioning that the proportional limit was previously studied for ReLU activations
- Why unresolved: The paper's characterization relies heavily on the linearity property that allows the network output to be expressed as a product of Wishart-distributed matrices, which may not generalize to non-linear cases
- What evidence would resolve it: Derivation of a similar stochastic integral representation for non-linear activations or proof that such a characterization is impossible due to non-linear interactions

## Limitations
- The theoretical framework relies on specific assumptions about the proportional growth regime and Wishart matrix structure
- Computational verification of the stochastic integral representations remains incomplete
- Extension to non-linear activation functions is not addressed, limiting practical applicability

## Confidence
- High confidence in the theoretical derivation of the prior distribution and its log-normal diagonal structure
- Medium confidence in the posterior mixture characterization, as it depends on the boundedness and continuity assumptions of the likelihood function
- Low confidence in the practical implications for feature learning without empirical validation on real datasets

## Next Checks
1. **Log-normal convergence verification**: Implement numerical simulations to verify that the diagonal entries of V̄_L,N converge to the predicted log-normal distribution N(-a/2, a/2) for D=1 across different values of a ∈ (0,1].

2. **Covariance decay analysis**: For D=2, empirically measure the decay rate of off-diagonal covariance elements as N increases, comparing against Lemma 4.2's predictions to confirm the preservation of inter-output correlations in the proportional limit.

3. **Posterior mixture sampling**: Implement a practical algorithm for sampling from the posterior Q_{L,N}(·|y₁:ₚ) in the proportional regime, then compare the empirical predictive covariance against the NNGP baseline on synthetic Gaussian data to quantify the difference in feature learning capacity.