---
ver: rpa2
title: Personalized Negative Reservoir for Incremental Learning in Recommender Systems
arxiv_id: '2403.03993'
source_url: https://arxiv.org/abs/2403.03993
tags:
- negative
- learning
- incremental
- user
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GraphSANE, the first personalized negative
  reservoir strategy for incremental learning in graph-based recommender systems.
  The method addresses catastrophic forgetting during frequent model updates by balancing
  stability and plasticity through user-specific preference tracking across time blocks.
---

# Personalized Negative Reservoir for Incremental Learning in Recommender Systems

## Quick Facts
- arXiv ID: 2403.03993
- Source URL: https://arxiv.org/abs/2403.03993
- Reference count: 40
- Key outcome: GraphSANE improves Recall@20 by 11.2%, 8.3%, and 6.4% on average across three incremental learning frameworks

## Executive Summary
GraphSANE introduces the first personalized negative reservoir strategy for incremental learning in graph-based recommender systems. The method addresses catastrophic forgetting during frequent model updates by maintaining user-specific negative samples that track changing preferences over time. It balances stability and plasticity by encouraging the model to remember stable user preferences while selectively forgetting when interests change. The approach is compatible with existing incremental learning frameworks and shows consistent improvements across multiple datasets and metrics.

## Method Summary
GraphSANE maintains a personalized reservoir of negative samples for each user that evolves across incremental time blocks. It tracks user interest shifts by monitoring category interaction patterns and increases sampling probability of negatives from categories users are losing interest in. The method also prioritizes "hard" negatives - items ranked highly by the model but not interacted with - using a hierarchical Bayesian model with Dirichlet prior on category sampling probabilities. GraphSANE integrates seamlessly with existing incremental learning frameworks like GraphSAIL, SGCT, and LWC-KD, and is evaluated on six large-scale datasets with bipartite graph structures.

## Key Results
- Average Recall@20 improvements of 11.2%, 8.3%, and 6.4% across GraphSAIL, SGCT, and LWC-KD frameworks respectively
- Consistent performance gains across multiple metrics (Precision@20, MAP@20, NDCG@20)
- Particularly effective for users with high interest shifts
- Outperforms both uniform random sampling and inverse degree-based sampling baselines

## Why This Works (Mechanism)

### Mechanism 1
Personalized negative reservoir improves incremental learning by sampling negatives from item categories users are losing interest in. Tracks user interest shift via item category interaction histograms across time blocks, increasing sampling probability of negatives from categories with decreased user engagement. Core assumption: User interest shifts can be accurately tracked via category interaction histograms.

### Mechanism 2
Sampling "hard" negatives (items ranked highly by the model but not interacted with) accelerates convergence. Uses top-ranked negative items from previous time block as source for reservoir, prioritizing these in sampling distribution to induce larger gradients. Core assumption: Hard negatives provide more informative gradients than random negatives.

### Mechanism 3
Hierarchical Bayesian model for negative sampling distribution balances stability and plasticity. Uses Dirichlet prior on category sampling probabilities with hyperparameters derived from user interest shift, updating posterior with observed hard negative counts. Core assumption: Dirichlet-multinomial conjugate pair provides appropriate framework for modeling category sampling probabilities.

## Foundational Learning

- Concept: Incremental learning and catastrophic forgetting
  - Why needed here: Addresses catastrophic forgetting in recommender systems during frequent model updates
  - Quick check question: What is catastrophic forgetting and why is it a problem in incremental learning?

- Concept: Graph neural networks and message passing
  - Why needed here: Method is designed for graph-based recommender systems using GNN backbones
  - Quick check question: How do graph neural networks aggregate information from neighboring nodes?

- Concept: Negative sampling in implicit feedback recommendation
  - Why needed here: Core contribution is novel negative sampling strategy for triplet loss training
  - Quick check question: Why is negative sampling necessary in implicit feedback recommendation systems?

## Architecture Onboarding

- Component map: Base GNN backbone (MGCCF) -> Knowledge distillation components (GraphSAIL, SGCT, LWC-KD) -> Personalized negative reservoir (GraphSANE) -> Clustering module for item categories -> Loss function combining base losses + reservoir loss + clustering loss

- Critical path: 1. Train base model on initial data block 2. For each incremental block: Update item cluster assignments, update negative reservoir for each user, sample negatives from reservoir + random negatives, train model with combined loss

- Design tradeoffs: Reservoir size vs. computational cost, Number of clusters vs. granularity of interest tracking, Frequency of reservoir updates vs. stability

- Failure signatures: Poor performance on users with high interest shifts, Slow convergence due to ineffective negative sampling, Overfitting to recent data blocks

- First 3 experiments: 1. Compare GraphSANE with uniform random negative sampling on a single dataset 2. Test sensitivity to reservoir size parameter |Q| 3. Evaluate performance on users with high vs. low interest shifts

## Open Questions the Paper Calls Out

### Open Question 1
How does GraphSANE perform when applied to non-bipartite graph structures or sequential recommendation settings? The paper focuses on bipartite graphs and explicitly states the method is not applicable to settings where users/items are represented only by features rather than identity nodes.

### Open Question 2
What is the optimal frequency for updating the negative reservoir, and how does this frequency affect model performance and computational cost? The paper mentions updating every two epochs but does not provide systematic analysis of different update frequencies.

### Open Question 3
How does GraphSANE handle the cold-start problem for new users and items, particularly during initial incremental blocks? The paper acknowledges cold-start users are not considered but does not provide a concrete solution.

## Limitations

- Evidence quality relies heavily on self-reported results with limited external validation and lack of ablation studies isolating individual component impacts
- Generalizability concerns due to dependence on meaningful item categorization, with unverified performance on datasets with less distinct item groupings
- Claims about superiority of hard negative sampling over other sophisticated negative sampling strategies not directly tested

## Confidence

- High Confidence: Catastrophic forgetting is a significant problem in incremental recommender systems; general approach of maintaining negatives from items users are losing interest in is intuitively sound
- Medium Confidence: Specific implementation details of hierarchical Bayesian model and interest shift tracking through category interaction histograms show promise but lack direct comparative evidence
- Low Confidence: Claims about hard negative sampling superiority over other sophisticated strategies not directly tested

## Next Checks

1. Conduct ablation study isolating impact of hierarchical Bayesian model versus simpler alternatives (exponential decay, moving averages) for tracking user interest shifts

2. Evaluate GraphSANE on a dataset with less distinct item categories to assess performance degradation when category assumption becomes weaker

3. Compare GraphSANE's hard negative sampling approach against other sophisticated negative sampling strategies like adversarial negative generators or curriculum-based sampling