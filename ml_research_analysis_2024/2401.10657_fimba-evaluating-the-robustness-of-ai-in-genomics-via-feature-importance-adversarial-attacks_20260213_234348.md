---
ver: rpa2
title: 'FIMBA: Evaluating the Robustness of AI in Genomics via Feature Importance
  Adversarial Attacks'
arxiv_id: '2401.10657'
source_url: https://arxiv.org/abs/2401.10657
tags:
- data
- attack
- dataset
- attacks
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FIMBA, a black-box adversarial attack method
  that targets machine learning models used in genomics by transforming feature importance
  vectors to reduce model confidence. The approach uses SHAP values to identify critical
  features, interpolates between vulnerable and target samples, and generates poisoned
  data via a variational autoencoder.
---

# FIMBA: Evaluating the Robustness of AI in Genomics via Feature Importance Adversarial Attacks

## Quick Facts
- **arXiv ID:** 2401.10657
- **Source URL:** https://arxiv.org/abs/2401.10657
- **Reference count:** 11
- **Primary result:** FIMBA successfully reduced model accuracy by up to 99.9% on genomic datasets while increasing false positives and negatives, demonstrating significant vulnerability in genomic AI pipelines.

## Executive Summary
This paper introduces FIMBA, a black-box adversarial attack method targeting machine learning models in genomics by transforming feature importance vectors to reduce model confidence. The approach uses SHAP values to identify critical features, interpolates between vulnerable and target samples, and generates poisoned data via a variational autoencoder. Tested on TCGA cancer and COVID-19 datasets, FIMBA successfully reduced model accuracy by up to 99.9% while increasing false positives and negatives. Spectral analysis showed the attack could mimic natural model errors, evading detection. Results demonstrate significant vulnerability in genomic AI pipelines, highlighting the need for robust defense mechanisms.

## Method Summary
FIMBA operates through a three-phase pipeline: first, SHAP values identify the most critical features affecting model decisions; second, interpolation between vulnerable and target samples generates poisoned data that mimics false positives and false negatives; third, a variational autoencoder (VAE) creates synthetic samples that closely resemble original data but with perturbed features. The attack was tested on five model architectures (Random Forest, XGBoost, CNN, ResNet, Vision Transformer) using TCGA cancer and COVID-19 single-cell RNA sequencing datasets, with success measured by accuracy reduction, increased false positive/negative rates, and undetectability via spectral analysis.

## Key Results
- Successfully reduced model accuracy by up to 99.9% on TCGA and COVID-19 datasets
- Significantly increased false positives and false negatives in all tested models
- Evaded spectral analysis detection by mimicking natural model error patterns
- Demonstrated effectiveness across five different model architectures including Random Forest, XGBoost, CNN, ResNet, and Vision Transformer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FIMBA transforms feature importance vectors to reduce model confidence
- Mechanism: SHAP values identify critical features; interpolation between vulnerable and target samples generates poisoned data that mimics FP/FN to evade detection
- Core assumption: For a select set of features, the model makes decisions on a boundary represented as a high-dimensional hyperplane L, and modifying these features can shift samples across this boundary
- Evidence anchors:
  - [abstract] "This paper introduces FIMBA, a black-box adversarial attack method that targets machine learning models used in genomics by transforming feature importance vectors to reduce model confidence"
  - [section] "To select a feature to modify, we want to pick the most impactful ones as fewer modification results in a more efficient attack and will be harder to detect. To perform this selection we utilize the SHAP-based method, which is model agnostic and relies on repeated feature permutation to extract the importance scores"
  - [corpus] Weak; no direct neighbor evidence on feature importance vector transformation in genomics
- Break condition: If SHAP values fail to accurately identify the most critical features, the attack's effectiveness would diminish

### Mechanism 2
- Claim: The attack uses a variational autoencoder (VAE) to generate poisoned synthetic data that mimics natural model errors
- Mechanism: VAE encodes original and target vectors, performs interpolation in the encoded form, and decodes to produce a synthetic sample that closely matches the original but with perturbed features
- Core assumption: The VAE can generate synthetic data that is structurally similar to the original data, allowing it to bypass detection methods like spectral analysis
- Evidence anchors:
  - [abstract] "Further, we enhance our approach by generating poisoned data using a variational autoencoder-based model"
  - [section] "To study the possibility of insertion of synthetic poisoned data into existing pipelines as well as to address the potential limitation of access to data for the attack we investigated a way to use generative models in order to create a fake FP/FN sample that was inspired by our main method"
  - [corpus] Weak; no direct neighbor evidence on using VAE for generating poisoned data in adversarial attacks on genomics
- Break condition: If the VAE fails to accurately reconstruct the data or if the synthetic data is easily detectable by other methods, the attack's effectiveness would be compromised

### Mechanism 3
- Claim: Spectral analysis (FFT) is used to detect adversarial attacks by comparing the power spectrum of the original and attacked datasets
- Mechanism: FFT is applied to the gene expression values treated as signals, and the resulting power spectra are compared using structural similarity index (SSIM) to measure detectability
- Core assumption: Adversarial samples will have a different power spectrum compared to natural model errors, allowing them to be detected
- Evidence anchors:
  - [section] "We utilize spectral analysis as a popular AML defense method in computer vision to showcase the ability of our attack to mask adversarial samples as natural model error"
  - [section] "We then combine the samples into a single signal and compute a fast Fourier Transform (FFT) to identify the power spectrum difference between the attacked and original datasets for each model at maximum severity setting of the attack"
  - [corpus] Weak; no direct neighbor evidence on using FFT for detecting adversarial attacks in genomics
- Break condition: If the attack successfully mimics natural model errors such that their power spectra are indistinguishable, the detection method would fail

## Foundational Learning

- **Concept: Feature importance and SHAP values**
  - Why needed here: To identify the most critical features that, when modified, will have the greatest impact on the model's decision boundary
  - Quick check question: What is the purpose of using SHAP values in the FIMBA attack method?

- **Concept: Adversarial attacks and robustness in machine learning**
  - Why needed here: To understand how models can be manipulated by carefully crafted inputs and the importance of developing robust models that are resistant to such attacks
  - Quick check question: Why is it important to study adversarial attacks on machine learning models used in genomics?

- **Concept: Variational autoencoders (VAEs) and generative models**
  - Why needed here: To generate synthetic poisoned data that mimics natural model errors, allowing the attack to bypass detection methods
  - Quick check question: How does the VAE contribute to the FIMBA attack method?

## Architecture Onboarding

- **Component map:** Data preprocessing and normalization -> Model training (Random Forest, XGBoost, CNN, ResNet, Vision Transformer) -> Feature importance extraction using SHAP values -> Interpolation between vulnerable and target samples -> VAE for generating poisoned synthetic data -> Spectral analysis for detecting adversarial attacks
- **Critical path:** Data preprocessing → Model training → Feature importance extraction → Interpolation → Attack generation → Detection analysis
- **Design tradeoffs:** Balancing attack effectiveness (reducing model accuracy) with undetectability (mimicking natural model errors)
- **Failure signatures:** High SSIM between original and attacked datasets indicates successful evasion of detection; low accuracy indicates successful attack
- **First 3 experiments:**
  1. Train models on TCGA and COVID-19 datasets and evaluate baseline performance
  2. Perform FIMBA attack with varying numbers of features modified and analyze the impact on accuracy and FP/FN rates
  3. Apply spectral analysis to compare the power spectra of original and attacked datasets and calculate SSIM values

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the theoretical limit of attack effectiveness when using fewer than 400 feature modifications on complex datasets like COVID-19?
- **Basis in paper:** [explicit] The paper states that both attack formats demonstrate great effectiveness but notes that for larger subsets of features it would make more practical sense to go with brute force for higher efficiency
- **Why unresolved:** The paper shows effectiveness at varying intensities but does not establish the minimum number of features required for significant impact on complex datasets
- **What evidence would resolve it:** Systematic testing of attack effectiveness across different feature modification counts on multiple complex datasets to determine the threshold for meaningful model degradation

### Open Question 2
- **Question:** How does the attack performance differ between single-cell RNA sequencing data and bulk RNA sequencing data?
- **Basis in paper:** [inferred] The paper focuses on single-cell RNA sequencing data but does not compare it to bulk RNA sequencing data
- **Why unresolved:** The paper demonstrates attack effectiveness on single-cell data but does not explore whether the attack would be more or less effective on bulk RNA sequencing data
- **What evidence would resolve it:** Comparative analysis of attack effectiveness on both single-cell and bulk RNA sequencing datasets using the same models and attack methods

### Open Question 3
- **Question:** What is the long-term impact of adversarial training on model robustness against feature importance-based attacks?
- **Basis in paper:** [explicit] The paper suggests that detecting and keeping track of vulnerable subsets by pre-attacking the model can help improve robustness
- **Why unresolved:** The paper discusses potential countermeasures but does not evaluate the effectiveness of adversarial training in improving model robustness against these specific attacks
- **What evidence would resolve it:** Longitudinal study of model performance after adversarial training, measuring resistance to feature importance-based attacks over time and across multiple iterations of training

## Limitations
- Limited evaluation on only two genomic datasets (TCGA and COVID-19), raising questions about generalizability to other genomic data types
- Implementation details for the VAE model and spectral analysis method are not fully specified, hindering exact reproduction
- The paper does not explore potential countermeasures beyond spectral analysis, leaving open questions about defense mechanisms

## Confidence

**Major Uncertainties:**
- The paper lacks detailed hyperparameter specifications for the VAE model used to generate poisoned data, making exact reproduction challenging
- Implementation details of the spectral analysis method for measuring undetectability are not fully specified
- The effectiveness of the attack on other genomic datasets beyond TCGA and COVID-19 remains untested

**Confidence Assessment:**
- **High Confidence:** The fundamental mechanism of using SHAP values to identify critical features and the overall attack pipeline
- **Medium Confidence:** The effectiveness of the VAE in generating undetectable poisoned data, as implementation details are sparse
- **Medium Confidence:** The spectral analysis method for detecting adversarial attacks, due to limited details on the exact procedure

## Next Checks

1. Implement and test the FIMBA attack on additional genomic datasets to verify generalizability beyond TCGA and COVID-19
2. Conduct ablation studies to quantify the individual contributions of SHAP-based feature selection, interpolation, and VAE-generated data to the overall attack effectiveness
3. Evaluate the attack's performance against other detection methods beyond spectral analysis to assess robustness to different defense mechanisms