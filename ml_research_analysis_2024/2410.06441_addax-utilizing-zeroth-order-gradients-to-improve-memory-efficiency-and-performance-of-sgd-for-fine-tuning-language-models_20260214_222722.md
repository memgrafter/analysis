---
ver: rpa2
title: 'Addax: Utilizing Zeroth-Order Gradients to Improve Memory Efficiency and Performance
  of SGD for Fine-Tuning Language Models'
arxiv_id: '2410.06441'
source_url: https://arxiv.org/abs/2410.06441
tags:
- addax
- memory
- mezo
- ip-sgd
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Addax is a memory-efficient optimizer for fine-tuning large language
  models that combines zeroth- and first-order gradient estimates. It assigns data
  points to either MeZO or IP-SGD based on sequence length, then combines the resulting
  gradients to update model parameters.
---

# Addax: Utilizing Zeroth-Order Gradients to Improve Memory Efficiency and Performance of SGD for Fine-Tuning Language Models

## Quick Facts
- arXiv ID: 2410.06441
- Source URL: https://arxiv.org/abs/2410.06441
- Reference count: 40
- Addax reduces memory usage while maintaining competitive accuracy, outperforming MeZO by 14-16% and running 15-30× faster across various model scales

## Executive Summary
Addax is a memory-efficient optimizer for fine-tuning large language models that combines zeroth- and first-order gradient estimates. The method assigns data points to either MeZO or IP-SGD based on sequence length, then combines the resulting gradients to update model parameters. This approach reduces memory usage while maintaining fast convergence and strong final performance. Addax demonstrates dimension-independent convergence rates with less restrictive hyperparameters than MeZO, consistently outperforming MeZO in accuracy/F1 score by 14-16% while running 15-30× faster across model scales from OPT-13B to Llama-70B.

## Method Summary
Addax combines zeroth-order and first-order gradient estimates to optimize memory efficiency during large language model fine-tuning. The algorithm assigns data points to either MeZO (zeroth-order) or IP-SGD (first-order) based on their sequence length, using a threshold parameter LT. For short sequences, it uses MeZO with parameter K0 to compute zeroth-order gradients, while long sequences use IP-SGD with parameter K1. The method then combines these gradients through a weighted average update rule. This dual-approach allows Addax to leverage the memory efficiency of zeroth-order methods for short sequences while maintaining the convergence speed of first-order methods for long sequences, resulting in improved overall performance.

## Key Results
- Addax consistently outperforms MeZO in accuracy/F1 score by 14-16% across SuperGLUE tasks
- Runs 15-30× faster than MeZO across various model scales (OPT-13B to Llama-70B) while using similar memory
- Surpasses standard methods like IP-SGD and Adam in most tasks with significantly lower memory requirements
- Demonstrates dimension-independent convergence rates with less restrictive hyperparameters than MeZO

## Why This Works (Mechanism)
Addax works by strategically partitioning the data based on sequence length to optimize the trade-off between memory efficiency and convergence speed. Short sequences benefit from MeZO's zeroth-order approach, which requires minimal memory but may converge slowly, while long sequences benefit from IP-SGD's first-order approach, which converges faster but requires more memory. By combining these methods through a weighted gradient update, Addax achieves the best of both worlds: memory efficiency for short sequences and fast convergence for long sequences. The threshold parameter LT determines the partition point, and the hyperparameters K0 and K1 control the perturbation magnitude for each method. This selective application of gradient estimation methods allows Addax to maintain competitive performance while significantly reducing memory requirements.

## Foundational Learning
- Zeroth-order optimization: Gradient-free optimization method using function evaluations; needed for memory-efficient updates without storing full gradients; quick check: verify perturbations don't exceed model parameter ranges
- In-place gradient updates: Memory-efficient parameter updates without intermediate storage; needed to reduce memory footprint during training; quick check: monitor GPU memory usage during training
- Sequence-length-based partitioning: Data-driven method to assign optimization strategies; needed to optimize the trade-off between memory and convergence; quick check: validate threshold LT with validation set performance
- Perturbation magnitude control: Hyperparameter tuning for gradient estimation accuracy; needed to balance convergence speed and memory usage; quick check: verify K0 and K1 values maintain stable training
- Weighted gradient combination: Fusion of multiple gradient estimates; needed to integrate zeroth- and first-order updates; quick check: ensure combined gradients don't cause divergence

## Architecture Onboarding

### Component Map
Optimizer (Addax) -> Sequence Partitioner -> MeZO Module (K0) + IP-SGD Module (K1) -> Gradient Combiner -> Parameter Updater

### Critical Path
Data sequences → Length evaluation → Partition decision (LT threshold) → Gradient estimation (MeZO or IP-SGD) → Weighted combination → Parameter update

### Design Tradeoffs
Memory vs. Convergence: MeZO saves memory but converges slowly; IP-SGD converges faster but uses more memory. Addax balances this through sequence-length partitioning.

Hyperparameter Sensitivity: K0 and K1 control perturbation magnitude, affecting both convergence speed and memory usage. Too large causes instability; too small slows convergence.

Threshold Selection: LT determines the partition point between MeZO and IP-SGD. Optimal value depends on model architecture and task characteristics.

### Failure Signatures
- Out-of-memory errors during long-sequence processing indicate insufficient memory allocation or suboptimal LT threshold
- Poor convergence or accuracy suggests inappropriate K0/K1 values or suboptimal threshold selection
- Training instability may result from excessive perturbation magnitudes in either MeZO or IP-SGD modules

### First Experiments
1. Run Addax on OPT-13B with default hyperparameters (K1=4, K0=6, LT=170) on a single A100 GPU to verify basic functionality
2. Compare memory usage and accuracy with MeZO baseline using identical batch sizes and learning rates
3. Vary the threshold parameter LT to determine optimal sequence-length partitioning for the target task

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on idealized assumptions about function landscape that may not hold for real-world fine-tuning tasks
- Computational efficiency gains are primarily demonstrated on A100 GPUs, with unverified performance on other hardware architectures
- Method's effectiveness for extremely long sequences (>2048 tokens) is not thoroughly explored for document-level understanding tasks
- Memory savings come at the cost of increased computational complexity through the dual-gradient mechanism
- Hyperparameter sensitivity analysis is relatively shallow, focusing mainly on K1 and K0 values

## Confidence
**High confidence**: Addax reduces memory usage compared to first-order methods while maintaining competitive accuracy; Addax outperforms MeZO in accuracy/F1 score by 14-16% on SuperGLUE tasks.

**Medium confidence**: Addax runs 15-30× faster than MeZO across various model scales; Addax surpasses standard methods like IP-SGD and Adam in most tasks with significantly lower memory requirements.

**Low confidence**: The theoretical convergence rates directly translate to practical performance gains; Addax is universally superior across all language model fine-tuning scenarios.

## Next Checks
1. **Reproduce the core memory-efficiency claim**: Implement Addax on a commodity GPU setup (e.g., RTX 3090) with OPT-13B to verify the 15-30× speedup claim and measure actual memory consumption during training.

2. **Validate generalization across tasks**: Test Addax on non-SuperGLUE benchmarks including commonsense reasoning (HellaSwag), summarization (XSum), and code generation (HumanEval) to assess performance beyond the reported tasks.

3. **Analyze long-sequence performance**: Evaluate Addax on tasks with sequences exceeding 2048 tokens (e.g., book summarization or long-document QA) to determine if the sequence-length threshold mechanism remains effective for extended contexts.