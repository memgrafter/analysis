---
ver: rpa2
title: Derivational Morphology Reveals Analogical Generalization in Large Language
  Models
arxiv_id: '2411.07990'
source_url: https://arxiv.org/abs/2411.07990
tags:
- gpt-j
- language
- adjective
- ness
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether linguistic generalization in large
  language models (LLMs) is based on rule-like abstractions or analogical processes
  operating on stored exemplars. The researchers developed a novel method comparing
  LLM predictions with cognitive models trained on the same data.
---

# Derivational Morphology Reveals Analogical Generalization in Large Language Models

## Quick Facts
- arXiv ID: 2411.07990
- Source URL: https://arxiv.org/abs/2411.07990
- Reference count: 40
- This study shows that GPT-J's linguistic generalization is best explained by analogical processes rather than rule-like abstractions

## Executive Summary
This study investigates whether linguistic generalization in large language models (LLMs) is based on rule-like abstractions or analogical processes operating on stored exemplars. The researchers developed a novel method comparing LLM predictions with cognitive models trained on the same data. Focusing on English adjective nominalization with -ity and -ness, they examined both regular patterns (like -able adjectives preferring -ity) and variable patterns (like -ive adjectives showing mixed preferences).

The key finding is that GPT-J's behavior is best explained by an exemplar-based analogical model rather than a rule-based model, particularly for variable patterns. GPT-J shows frequency effects even for regular forms, matching the predictions of token-based analogical models. This contradicts rule-based theories which predict no frequency effects for regular forms. The study also reveals a fundamental difference between human and LLM generalization: humans generalize based on word types while GPT-J generalizes based on word tokens, leading to less human-like predictions.

## Method Summary
The researchers compared GPT-J's predictions on English adjective nominalization with two cognitive models: MGL (a rule-based model) and GCM (an analogical model). They trained these cognitive models on the same training data as GPT-J and tested all three on nonce adjectives from four adjective classes (-able, -ish, -ive, -ous). The study used 12 different prompts to measure GPT-J's confidence in selecting -ity versus -ness derivatives. They also collected human annotations from 22 native English speakers to compare with model predictions.

## Key Results
- GPT-J's behavior is best explained by an exemplar-based analogical model rather than a rule-based model
- GPT-J shows frequency effects even for regular forms, contradicting rule-based theories
- GPT-J generalizes based on word tokens rather than word types, unlike human processing

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GPT-J's linguistic generalization is driven by analogical processes operating on stored exemplars rather than abstract rules.
- **Mechanism:** The model stores individual word forms in its weights and retrieves them based on similarity during prediction. When encountering a novel word, it finds the most similar stored forms and predicts based on their distribution.
- **Core assumption:** Word tokens (individual occurrences) are stored and retrieved during inference, not just word types (abstract vocabulary entries).
- **Evidence anchors:** [abstract] "GPT-J's behavior is best explained by an exemplar-based analogical model rather than a rule-based model"
- **Break condition:** If a systematic rule-based pattern emerges that consistently overrides token-level similarity effects, particularly for high-regularity patterns.

### Mechanism 2
- **Claim:** The degree of variability in training data directly determines whether GPT-J behaves regularly or analogically.
- **Mechanism:** When training data shows high consistency for a pattern, the model produces regular outputs; when data shows variability, the model shows analogical behavior weighted by individual token frequencies.
- **Core assumption:** The model's behavior is a direct reflection of training data statistics, not abstract generalization beyond the data.
- **Evidence anchors:** [abstract] "for adjectives with variable nominalization patterns, the analogical model provides a much better match"
- **Break condition:** If the model shows consistent rule-like behavior for any pattern regardless of training data variability.

### Mechanism 3
- **Claim:** GPT-J's lexicon is organized around tokens rather than types, creating a fundamental difference from human language processing.
- **Mechanism:** The model stores each word occurrence separately with its frequency information, and this token-level organization drives all generalization rather than type-level abstractions that humans use.
- **Core assumption:** Language models cannot abstract from individual tokens to form type-level generalizations the way humans do.
- **Evidence anchors:** [abstract] "humans generalize based on types while GPT-J generalizes based on tokens"
- **Break condition:** If the model demonstrates type-level abstraction capabilities (e.g., consistent behavior across frequency variations for regular patterns).

## Foundational Learning

- **Concept:** Analogical reasoning in morphology
  - Why needed here: The paper's central claim is that GPT-J uses analogy rather than rules, so understanding how analogical models work is essential.
  - Quick check question: How does an exemplar-based model like GCM decide between two competing derivatives for a novel word?

- **Concept:** Derivational morphology and affix stacking
  - Why needed here: The study focuses on adjective nominalization with -ity and -ness, requiring understanding of how suffixes combine and compete.
  - Quick check question: Why are cases of affix stacking (like adjectives with derivational suffixes) particularly useful for distinguishing rule-based from analogical generalization?

- **Concept:** Frequency effects in morphological processing
  - Why needed here: The paper emphasizes how token frequencies influence GPT-J's predictions even for regular forms, contradicting rule-based theories.
  - Quick check question: Why would a rule-based theory predict no frequency effects for regular complex forms?

## Architecture Onboarding

- **Component map:** Prompt text -> Tokenizer -> GPT-J model -> Comparison layer -> Selected derivative
- **Critical path:**
  1. Receive base adjective and prompt
  2. Generate both possible derivatives (-ity and -ness forms)
  3. Tokenize each derivative
  4. Compute total log probability for each derivative
  5. Select derivative with higher total log probability

- **Design tradeoffs:**
  - Tokenization granularity affects probability calculation accuracy
  - Prompt wording significantly impacts predictions (12 different prompts used)
  - Model size (6B parameters) enables storage of many exemplars but increases computational cost

- **Failure signatures:**
  - Inconsistent predictions across similar prompts
  - Over-reliance on surface features rather than morphological structure
  - Failure to generalize beyond training data statistics

- **First 3 experiments:**
  1. Test GPT-J on nonce adjectives with varying prompt formulations to measure consistency
  2. Compare GPT-J predictions against both rule-based (MGL) and analogical (GCM) cognitive models
  3. Analyze frequency effects by comparing high-frequency vs low-frequency attested derivatives for the same base

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompting strategies affect the frequency-based predictions of LLMs like GPT-J and GPT-4?
- Basis in paper: Explicit - The paper notes that varying prompts can heavily affect LLM behavior and uses 12 different prompts to measure probabilities.
- Why unresolved: The paper only provides averaged results across prompts and does not analyze how individual prompts might affect predictions differently.
- What evidence would resolve it: Systematic analysis of how different prompt formulations (explicit vs. implicit nominalization requests, different grammatical contexts) affect the confidence scores and derivative preferences for both regular and variable adjective classes.

### Open Question 2
- Question: Do LLMs develop type-level representations when trained on larger datasets or with more sophisticated architectures?
- Basis in paper: Explicit - The paper shows GPT-4 relies even more heavily on token frequency than GPT-J, suggesting inverse scaling effects, but also notes humans generalize over types.
- Why unresolved: The paper only examines two models and cannot determine whether larger models or different architectures might develop more human-like type-based generalization.
- What evidence would resolve it: Comparative studies of multiple LLM architectures and sizes (including those with explicit type-level mechanisms) on nonce word tasks, measuring whether type-level generalization emerges with scale or architectural changes.

### Open Question 3
- Question: How do semantic relationships between adjectives and their nominalizations influence analogical generalization in LLMs?
- Basis in paper: Explicit - The paper notes that GCM works on word forms only without considering meaning similarities, while LLMs can consider semantic relationships but may be offset by token frequency effects.
- Why unresolved: The paper focuses on orthographic similarity but does not systematically test how semantic relatedness between adjectives and their derivatives affects predictions.
- What evidence would resolve it: Controlled experiments varying semantic similarity between adjectives and their nominalizations while controlling for orthographic similarity, measuring whether LLMs show semantic effects that override token frequency patterns.

## Limitations

- The study's central finding rests on comparing against cognitive models that themselves make strong assumptions about human processing
- The study focuses on a specific morphological phenomenon in English, limiting generalizability to other languages or morphological processes
- The prompting methodology, while extensive with 12 different prompts, still represents an artificial task that may not capture natural language processing in the model

## Confidence

- **High confidence:** The empirical finding that GPT-J shows frequency effects even for regular forms contradicts rule-based predictions and supports token-based processing
- **Medium confidence:** The claim that GPT-J's behavior is "best explained" by analogical models requires comparison with rule-based models, and the superiority of the analogical explanation depends on the specific cognitive models used as benchmarks
- **Medium confidence:** The assertion that humans generalize based on types while GPT-J generalizes based on tokens is based on theoretical assumptions about cognitive models rather than direct empirical comparison

## Next Checks

1. **Cross-linguistic validation:** Test GPT-J on adjective nominalization in other languages with similar morphological patterns to determine whether the analogical generalization finding generalizes beyond English.

2. **Human-model processing comparison:** Conduct a controlled experiment comparing human participants' judgments of nonce adjectives with GPT-J predictions, measuring not just accuracy but also confidence ratings and reaction times.

3. **Rule-based model benchmarking:** Implement and test additional rule-based models beyond MGL (such as constraint-based or optimality-theoretic approaches) to ensure that the superiority of analogical models is robust across different rule-based frameworks.