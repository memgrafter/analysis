---
ver: rpa2
title: 'Differential Privacy in Continual Learning: Which Labels to Update?'
arxiv_id: '2411.04680'
source_url: https://arxiv.org/abs/2411.04680
tags:
- task
- data
- learning
- privacy
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of combining differential privacy
  (DP) and continual learning (CL) to protect sensitive training data while learning
  from sequential tasks without catastrophic forgetting. The authors identify a critical
  privacy leakage through the classifier's output label space when it directly depends
  on the training data.
---

# Differential Privacy in Continual Learning: Which Labels to Update?

## Quick Facts
- arXiv ID: 2411.04680
- Source URL: https://arxiv.org/abs/2411.04680
- Authors: Marlon Tobaben; Talal Alrawajfeh; Marcus Klasson; Mikko Heikkilä; Arno Solin; Antti Honkela
- Reference count: 40
- Primary result: Shows that failing to account for privacy leakage through output label spaces breaks DP guarantees in continual learning, and demonstrates effective DP-compliant solutions using pre-trained models

## Executive Summary
This work addresses the critical challenge of combining differential privacy with continual learning, where the output label space can leak sensitive information even when the classifier is trained under DP. The authors identify that choosing the output label space based on training data violates DP, and propose two solutions: using a large data-independent prior label set or learning labels through a separate DP mechanism. Through extensive experiments with ViT-Base-16 on multiple benchmarks, they demonstrate that pre-trained models significantly outperform naive baselines, with the PEFT Ensemble achieving up to 85% accuracy on Split-CIFAR-100 at ε=8 while maintaining privacy guarantees.

## Method Summary
The paper proposes two DP-compliant approaches for continual learning that address privacy leakage through output label spaces. The first approach uses a large data-independent prior label set (Sprior) where the classifier's output space is chosen independently of the training data, assuming a large public label taxonomy that covers all possible dataset labels. The second approach learns labels through a separate DP mechanism (Slearned), where a DP mechanism releases the labels from the dataset before training. Both methods use ViT-Base-16 pre-trained on ImageNet-21K as the feature extractor, with classifiers trained using DP-SGD. The Cosine Classifier uses cosine similarity between feature vectors and class prototypes, while the PEFT Ensemble uses parameter-efficient fine-tuning with FiLM adapters. Privacy is maintained through proper composition of DP guarantees across tasks.

## Key Results
- The Cosine Classifier with large prior label sets achieves high accuracy while maintaining DP guarantees, showing minimal utility loss compared to non-private baselines
- PEFT Ensemble provides up to 85% accuracy on Split-CIFAR-100 at ε=8, outperforming naive DP baselines significantly
- Using pre-trained models provides strong feature representations that are robust to large output spaces, enabling effective privacy-utility tradeoffs
- Methods based on pre-trained models show superior performance compared to training from scratch, especially under strict privacy constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Releasing the classifier's output label space directly based on the training data leaks sensitive information, even if the classifier itself is trained under differential privacy.
- Mechanism: When the output label space is chosen as the set of labels present in the training data (Ot = Odata_t), an attacker can distinguish between datasets by observing differences in the classifier's label set. For example, if dataset D_t contains an extra label y* compared to D'_t, then Mt(D_t) and Mt(D'_t) have disjoint label spaces, violating differential privacy.
- Core assumption: The output label space is a deterministic function of the training data, and releasing it constitutes a privacy breach.
- Evidence anchors:
  - [abstract]: "We highlight that failing to account for privacy leakage through the set of labels a model can output can break the privacy of otherwise valid DP algorithms."
  - [section]: "We argue that choosing the output label space based on the sensitive data is not DP in the following proposition, because then the classifier output space is a function of the sensitive dataset, and releasing a function of the dataset is not DP."
  - [corpus]: Weak - No direct evidence in related papers about output label space leakage in DP CL.
- Break condition: If the output label space is chosen independently of the training data, or if the labels are released through a separate DP mechanism.

### Mechanism 2
- Claim: Using a large data-independent prior label set for the classifier's output space has minimal negative impact on utility when fine-tuning a pre-trained model under differential privacy.
- Mechanism: By assuming a large set of public labels that likely covers all or most dataset labels (Ot = Oprior_t), the classifier can handle unseen labels through remapping or dropping. Experiments show that this approach maintains high accuracy even when the assumed label set is much larger than the actual dataset labels.
- Core assumption: Pre-trained models provide strong feature representations that are robust to a large output space, and the additional computational cost is manageable.
- Evidence anchors:
  - [abstract]: "We show that mitigating the issue with a data-independent overly large label space can have minimal negative impact on utility when fine-tuning a pre-trained model under DP..."
  - [section]: "Through DP adapted CL methods based on pre-trained models (Sec. 6), we show that this knowledge only needs to include the labels actually present in the data, but can otherwise be coarse and contain many labels that never appear during CL (Fig. 3), enabling the use of large existing taxonomies like ICD-11 in medicine."
  - [corpus]: Weak - No direct evidence in related papers about using large data-independent label sets in DP CL.
- Break condition: If the prior label set is too small to cover most dataset labels, or if the pre-trained model's features are not robust to a large output space.

### Mechanism 3
- Claim: Learning the labels through a separate differential privacy mechanism risks losing small classes due to the privacy budget being split between label release and model training.
- Mechanism: A DP mechanism can be used to release the labels from the dataset, but this approach has limitations. The privacy budget must be split between label release and model training, and protecting labels with DP implies that any label can be potentially dropped, especially small classes.
- Core assumption: The DP mechanism for label release is (ε, δ)-DP, and the privacy budget is properly allocated between label release and model training.
- Evidence anchors:
  - [abstract]: "...while learning the labels with a separate DP mechanism (which risks losing small classes)."
  - [section]: "The privacy budget needs to be split between this mechanism and the DP weights training θt. Protecting the labels with DP implies that any label can be potentially dropped from Odata_t."
  - [corpus]: Weak - No direct evidence in related papers about DP mechanisms for label release in DP CL.
- Break condition: If the privacy budget is insufficient to protect both labels and model weights, or if the DP mechanism for label release is not properly implemented.

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: DP provides a formal privacy definition that ensures the inclusion or exclusion of any single data point does not significantly impact the outcome of the learning process, which is crucial for protecting sensitive training data in continual learning.
  - Quick check question: What are the privacy parameters ε and δ in DP, and how do they control the allowed privacy loss?

- Concept: Continual Learning (CL)
  - Why needed here: CL develops models that learn from a stream of tasks while retaining previous knowledge, but it faces the challenge of catastrophic forgetting, which conflicts with the privacy requirements of DP.
  - Quick check question: What are the main approaches to mitigate catastrophic forgetting in CL, and how do they differ from DP?

- Concept: Composition of Differential Privacy
  - Why needed here: In CL, privacy loss accumulates over multiple tasks, so it's essential to understand how to compose privacy guarantees over a sequence of DP mechanisms to ensure the total privacy budget is not exceeded.
  - Quick check question: What are the different types of composition in DP (parallel, sequential, adaptive), and when are they applicable?

## Architecture Onboarding

- Component map: Pre-trained ViT-Base-16 -> Feature extractor -> Classifier (Cosine or PEFT Ensemble) -> Output labels (Sprior or Slearned) -> DP mechanism

- Critical path:
  1. Initialize pre-trained model and classifier with output label space based on prior knowledge (Sprior) or learned through DP mechanism (Slearned).
  2. For each task, update the classifier using DP-SGD or other DP-compliant method.
  3. Release the classifier, ensuring the output label space does not leak sensitive information.

- Design tradeoffs:
  - Using a large data-independent prior label set (Sprior) vs. learning labels through a DP mechanism (Slearned): Sprior has minimal impact on utility but may include many unused labels, while Slearned risks losing small classes but can be more efficient.
  - Fine-tuning the entire model vs. using parameter-efficient fine-tuning (PEFT): PEFT is more computationally efficient but may have slightly lower accuracy, especially with larger domain shifts.

- Failure signatures:
  - If the output label space is chosen based on the training data (Sdata), the classifier will leak sensitive information, violating DP.
  - If the prior label set is too small, the classifier may not be able to handle unseen labels, leading to poor performance.
  - If the privacy budget is insufficient, the DP guarantees will be violated, either in label release or model training.

- First 3 experiments:
  1. Implement the Cosine Classifier with a large data-independent prior label set (Sprior) on Split-CIFAR-100, and evaluate its accuracy and privacy guarantees.
  2. Implement the PEFT Ensemble with parameter-efficient fine-tuning on Split-ImageNet-R, and compare its performance to the Cosine Classifier.
  3. Experiment with different granularities of prior label sets (e.g., exact match, 10×, 100×, 1000× the actual labels) and measure the impact on utility and privacy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DP continual learning methods degrade when the privacy budget (ε) approaches zero?
- Basis in paper: [explicit] The paper evaluates utility/privacy trade-offs at specific ε values (1.0, 8.0) but doesn't explore the extreme case of very low ε values where privacy becomes very strict.
- Why unresolved: The experiments only test at moderate ε values, leaving the behavior at very low privacy budgets unexplored.
- What evidence would resolve it: Additional experiments showing accuracy and forgetting metrics across a wider range of ε values (e.g., 0.1, 0.01, 0.001) would reveal the practical limits of DP continual learning.

### Open Question 2
- Question: Can the output label space privacy leakage be completely eliminated through architectural modifications rather than just using data-independent priors?
- Basis in paper: [inferred] The paper identifies label space leakage as a privacy vulnerability and proposes workarounds (data-independent priors or DP label learning), but doesn't explore whether architectural changes could prevent this leakage at the source.
- Why unresolved: The proposed solutions are workarounds rather than addressing the fundamental architectural cause of the leakage.
- What evidence would resolve it: Experimental comparison of different classifier architectures (e.g., architectures that don't directly expose the label space) in terms of both privacy guarantees and utility would show whether architectural solutions are viable.

### Open Question 3
- Question: How does the proposed task-wise DP formulation handle scenarios where tasks have overlapping data or shared users?
- Basis in paper: [explicit] The paper's task-wise DP definition assumes disjoint privacy units between tasks, but real-world scenarios often involve overlapping data or shared users across tasks.
- Why unresolved: The theoretical framework and experiments assume non-overlapping tasks, but this assumption may not hold in many practical applications.
- What evidence would resolve it: Experiments and theoretical analysis of DP guarantees when tasks share data or users would clarify the practical applicability of the task-wise DP formulation.

## Limitations

- The analysis assumes perfect DP mechanisms and doesn't account for potential side-channel leaks or implementation-specific vulnerabilities
- Experiments are limited to vision benchmarks with ViT models, leaving questions about applicability to other modalities or architectures
- The claim that large data-independent label sets have "minimal negative impact" on utility depends heavily on pre-trained feature quality and may not generalize

## Confidence

- **High confidence**: The formal privacy analysis identifying label space leakage as a privacy breach, and the experimental validation showing pre-trained model benefits are robust across datasets.
- **Medium confidence**: The proposed solutions (Cosine Classifier and PEFT Ensemble) work well empirically, but their performance under different domain shifts or with alternative architectures needs verification.
- **Low confidence**: The claim that large data-independent label sets have "minimal negative impact" on utility - this depends heavily on the quality of pre-trained features and may not generalize to all scenarios.

## Next Checks

1. Test the Cosine Classifier approach on non-vision datasets (e.g., text classification) to verify the generality of the label space independence principle.
2. Conduct ablation studies varying the size of the prior label set more systematically to identify the threshold where utility begins degrading.
3. Implement the label learning approach (Slearned) with a proper DP mechanism for label release and compare its privacy-utility tradeoff to the prior-based approach under the same privacy budget.