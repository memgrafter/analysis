---
ver: rpa2
title: 'Fietje: An open, efficient LLM for Dutch'
arxiv_id: '2412.15450'
source_url: https://arxiv.org/abs/2412.15450
tags:
- uni00000010
- uni00000057
- uni00000048
- uni0000004c
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fietje is a family of small language models (SLMs) specifically
  designed for the Dutch language, based on Phi 2, an English-centric model of 2.7
  billion parameters. The model was trained on 28 billion high-quality Dutch tokens
  from open, filtered web data, including Dutch Wikipedia and CulturaX.
---

# Fietje: An open, efficient LLM for Dutch

## Quick Facts
- arXiv ID: 2412.15450
- Source URL: https://arxiv.org/abs/2412.15450
- Authors: Bram Vanroy
- Reference count: 6
- Primary result: Small 2.7B parameter model achieves competitive performance on Dutch language tasks

## Executive Summary
Fietje is a family of small language models (SLMs) specifically designed for the Dutch language, based on Phi 2 with 2.7 billion parameters. The model was trained on 28 billion high-quality Dutch tokens from filtered web data, including Dutch Wikipedia and CulturaX. Fietje demonstrates competitive performance with larger models, particularly in reasoning and knowledge-based tasks, and outperforms its base model on reasoning tasks like ARC and MMLU. The model is fully open-source with all weights, datasets, training, and evaluation code publicly available.

## Method Summary
Fietje was developed through continued pretraining of the English-centric Phi 2 model on 28 billion high-quality Dutch tokens from filtered web data. The training process included instruction tuning on semi-structured Dutch conversations and preference alignment using Direct Preference Optimization on human preference data. The model was evaluated on a comprehensive benchmark suite covering reasoning, sentiment analysis, world knowledge, linguistic acceptability, and word sense disambiguation in a zero-shot setting.

## Key Results
- Fietje 2B Chat outperforms larger 7B models like GEITje and Tweety on multiple benchmarks
- Competitive performance with larger models, particularly in reasoning and knowledge-based tasks
- Zero-shot results demonstrate strong general language understanding capabilities

## Why This Works (Mechanism)

### Mechanism 1
High-quality, curated training data enables small models to outperform larger models. Filtering out noisy, low-quality data during pretraining allows the model to learn more effectively from cleaner, more representative language samples. Break Condition: If filtering introduces significant bias or removes essential linguistic diversity, performance could degrade.

### Mechanism 2
Multilingual pretraining improves performance on non-English benchmarks. Incorporating diverse language data during pretraining allows the model to develop better cross-lingual understanding and transfer learning capabilities. Break Condition: If multilingual pretraining dilutes language-specific knowledge too much, performance in the target language may suffer.

### Mechanism 3
Instruction tuning and preference alignment significantly enhance model capabilities beyond base pretraining. Fine-tuning on conversational data and human preferences teaches the model to follow instructions and align with human values, improving practical usability. Break Condition: If post-training overfits to specific instruction formats or introduces unintended biases, model generalization may be impaired.

## Foundational Learning

- **Tokenization and subword segmentation**: Understanding how text is broken into tokens is crucial for interpreting model efficiency metrics like fertility and processing speed. Quick check: What does a higher fertility score indicate about a tokenizer's efficiency for a given language?

- **Continued pretraining vs. fine-tuning**: The paper distinguishes between adapting an existing model through continued pretraining (training on new data) versus fine-tuning (training on task-specific data). Quick check: How does continued pretraining differ from instruction tuning in terms of goals and data requirements?

- **Zero-shot vs. few-shot learning**: The benchmarks are conducted in a zero-shot setting, which affects how model performance should be interpreted compared to few-shot scenarios. Quick check: Why might zero-shot performance be a more stringent test of a model's general language understanding than few-shot performance?

## Architecture Onboarding

- **Component map**: Base model (Phi-2) -> Continued pretraining (28B Dutch tokens) -> Instruction tuning (Dutch conversations) -> Preference alignment (DPO) -> Evaluation (zero-shot benchmarks)

- **Critical path**: Data filtering → Continued pretraining → Instruction tuning → Preference alignment → Evaluation

- **Design tradeoffs**: Small model size (2.7B) vs. performance: Fietje demonstrates competitive results despite being much smaller than 7B models; Data quality vs. quantity: Strict filtering ensures high quality but may limit diversity; Zero-shot evaluation vs. few-shot: Zero-shot is more challenging but better reflects general capabilities

- **Failure signatures**: High fertility scores indicate poor tokenization for target language; Poor performance on Dutch-specific tasks despite good general performance suggests inadequate language adaptation; Large confidence intervals in benchmark results indicate model uncertainty

- **First 3 experiments**:
  1. Test base model on Dutch Wikipedia to establish baseline fertility and processing speed
  2. Run a small subset of the filtered dataset through continued pretraining to verify data quality and training stability
  3. Evaluate the instruct-tuned model on a simple Dutch instruction-following task to verify post-training effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal balance between pretraining data quantity and quality for Dutch language models? While the paper establishes that quality matters, it doesn't empirically determine the optimal trade-off between quantity and quality of training data for Dutch language models.

### Open Question 2
How do translation artifacts in machine-translated benchmarks affect model performance evaluation for non-English languages? While the paper identifies this as a potential issue, it doesn't quantify the extent of these biases or determine their impact on model ranking.

### Open Question 3
What is the relationship between tokenizer efficiency and model performance for Dutch language processing? While the paper shows correlation between tokenizer efficiency and processing speed, it doesn't establish a clear causal relationship between tokenizer efficiency and downstream task performance.

## Limitations

- Limited benchmark coverage with only five specific tasks and lack of commonly used benchmarks like GLUE and SuperGLUE for Dutch
- Incomplete disclosure of filtering criteria and specific dataset subsets used for training
- Missing implementation details including exact configuration parameters and complete evaluation methodology

## Confidence

**High Confidence**: Fietje demonstrates competitive performance with larger models across multiple benchmarks
**Medium Confidence**: Instruction tuning and preference alignment significantly enhance model capabilities
**Low Confidence**: Fietje represents a significant advance over older, larger Dutch-adapted models

## Next Checks

1. **Data Quality Audit**: Analyze the filtered training dataset to quantify potential bias and ensure adequate linguistic diversity across text types and domains

2. **Extended Benchmark Evaluation**: Evaluate Fietje on additional established benchmarks for Dutch language understanding including GLUE and SuperGLUE tasks

3. **Ablation Study of Post-Training Components**: Systematically evaluate individual contributions of instruction tuning and preference alignment by testing variants with different post-training combinations