---
ver: rpa2
title: Election of Collaborators via Reinforcement Learning for Federated Brain Tumor
  Segmentation
arxiv_id: '2412.20253'
source_url: https://arxiv.org/abs/2412.20253
tags:
- collaborators
- federated
- learning
- data
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of optimally selecting collaborators
  in federated learning for brain tumor segmentation, a critical issue in dynamic
  FL environments where data distributions and available collaborators evolve over
  time. The authors propose RL-HSimAgg, a novel approach combining reinforcement learning
  with similarity-weighted aggregation using harmonic mean to manage outlier data
  points.
---

# Election of Collaborators via Reinforcement Learning for Federated Brain Tumor Segmentation

## Quick Facts
- arXiv ID: 2412.20253
- Source URL: https://arxiv.org/abs/2412.20253
- Reference count: 24
- RL-HSimAgg approach achieved Dice scores of 0.7334 (ET), 0.7432 (TC), 0.8252 (WT) using UCB collaborator selection

## Executive Summary
This study addresses the challenge of optimally selecting collaborators in federated learning for brain tumor segmentation, a critical issue in dynamic FL environments where data distributions and available collaborators evolve over time. The authors propose RL-HSimAgg, a novel approach combining reinforcement learning with similarity-weighted aggregation using harmonic mean to manage outlier data points. They apply multi-armed bandit algorithms, specifically Epsilon-greedy (EG) and Upper Confidence Bound (UCB), to improve collaborator selection and model generalization. The UCB method outperformed EG across all metrics, achieving higher Dice scores for Enhancing Tumor (0.7334 vs 0.6797), Tumor Core (0.7432 vs 0.6821), and Whole Tumor (0.8252 vs 0.7931) segmentation.

## Method Summary
The proposed RL-HSimAgg approach uses multi-armed bandit algorithms (Epsilon-greedy and UCB) for collaborator selection in federated learning. Collaborators are selected based on their performance scores, with UCB balancing exploration and exploitation through an upper confidence bound that includes both average performance and an exploration bonus. Selected collaborators' model parameters are aggregated using a harmonic mean weighted by similarity to the average parameters and sample size. The approach was evaluated on mpMRI data from the FeTS 2022 challenge using a 3D U-Net architecture, comparing UCB and EG methods across 25 communication rounds with 33 collaborators.

## Key Results
- UCB collaborator selection achieved higher Dice scores than EG: ET 0.7334 vs 0.6797, TC 0.7432 vs 0.6821, WT 0.8252 vs 0.7931
- UCB resulted in smaller Hausdorff (95%) distances compared to EG for all tumor regions
- Harmonic similarity weighted aggregation effectively handled outlier data points and improved model robustness
- Alternating selection of top and bottom performers (top 20% in even rounds, bottom 20% in odd rounds) showed promising results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UCB collaborator selection improves segmentation performance by balancing exploration and exploitation of collaborators based on uncertainty.
- Mechanism: The UCB algorithm selects collaborators by maximizing an upper confidence bound that includes both the average performance (exploitation) and an exploration bonus proportional to the standard deviation of rewards divided by the square root of selection count. This encourages selecting collaborators with uncertain but potentially high rewards.
- Core assumption: The exploration bonus in UCB correctly captures the uncertainty in collaborator performance, and collaborators with high uncertainty are more likely to yield better performance.
- Evidence anchors:
  - [abstract] "The UCB method outperformed EG across all metrics, achieving higher Dice scores for Enhancing Tumor (0.7334 vs 0.6797), Tumor Core (0.7432 vs 0.6821), and Whole Tumor (0.8252 vs 0.7931) segmentation."
  - [section 1.3] "Select the arm that maximizes the upper confidence bound: =rgmx(Q () +C ·√ln(t)/N ())"
  - [corpus] Weak evidence - only 5 related papers found, none specifically comparing UCB vs EG performance in FL settings.
- Break condition: If collaborator performance distributions are stationary and well-known, the exploration bonus becomes negligible and UCB degenerates to pure exploitation, losing the exploration advantage.

### Mechanism 2
- Claim: Harmonic similarity weighted aggregation (RL-HSimAgg) improves model robustness by down-weighting outlier collaborators and emphasizing similar contributions.
- Mechanism: Collaborators are assigned similarity weights based on their distance from the unweighted average parameters, then combined with sample size weights to compute aggregation weights. The harmonic mean is used to aggregate parameters, which is more robust to outliers than arithmetic mean.
- Core assumption: Collaborators with parameters far from the average are likely to be outliers due to data heterogeneity, and down-weighting them improves overall model quality.
- Evidence anchors:
  - [section 1.4] "Collaborators whose parameters closely align with the average are assigned greater similarity weights, whereas those with more significant deviations receive comparatively lower weights. This methodology can effectively mitigate the influence of outliers or instances of substantial divergence."
  - [section 1.4] "The harmonic mean operates by calculating the reciprocal of the arithmetic mean of the reciprocals of the values, and thus it is suitable for cases where proportionality and balance are key factors."
  - [corpus] Weak evidence - no specific corpus papers discussing harmonic mean aggregation in FL settings.
- Break condition: If all collaborators have similar data distributions and performance, similarity weighting becomes uniform and provides no benefit over simple averaging.

### Mechanism 3
- Claim: Alternating selection of top and bottom performers in even and odd rounds improves both exploitation of good collaborators and exploration of potentially useful ones.
- Mechanism: In even rounds, top 20% of collaborators by UCB score are selected to maximize expected performance. In odd rounds, bottom 20% are selected to explore uncertain collaborators that might become high performers.
- Core assumption: Collaborators that perform poorly in early rounds may improve over time as the global model improves, and exploring these collaborators can yield better long-term performance.
- Evidence anchors:
  - [section 1.3] "In even rounds, the top 20% of collaborators by highest UCB score are chosen to maximize expected performance. In odd rounds, the bottom 20% are selected to explore uncertain collaborators."
  - [section 2.2] "The difference between the two approaches is notable in terms of Dice score. However, the UCB approach resulted in a smaller Hausdorff (95%) as compared to the EG approach for all tumor regions."
  - [corpus] Weak evidence - no corpus papers specifically discussing alternating selection strategies in FL.
- Break condition: If collaborator performance is stable over rounds, alternating selection may waste resources on consistently poor performers.

## Foundational Learning

- Concept: Multi-armed bandit problem formulation
  - Why needed here: The collaborator selection problem is naturally framed as a multi-armed bandit where each collaborator is an "arm" and the reward is the contribution to model performance. This provides theoretical foundation for balancing exploration and exploitation.
  - Quick check question: What is the key difference between epsilon-greedy and UCB strategies in handling exploration vs exploitation?

- Concept: Federated learning aggregation techniques
  - Why needed here: Understanding how to combine model updates from multiple collaborators is crucial for FL systems. Different aggregation methods (FedAvg, weighted averaging, harmonic mean) have different properties regarding robustness to heterogeneity.
  - Quick check question: Why might harmonic mean be more robust to outliers than arithmetic mean in FL aggregation?

- Concept: Reinforcement learning in non-stationary environments
  - Why needed here: The collaborator selection problem involves a non-stationary environment where the utility of selecting different collaborators may change over time as the global model improves.
  - Quick check question: How does the non-stationarity of collaborator performance affect the choice of RL algorithm?

## Architecture Onboarding

- Component map: Collaborator selection (UCB) -> Local training -> Parameter aggregation (RL-HSimAgg) -> Global model update -> Validation -> Selection for next round
- Critical path: Collaborator selection → Local training → Parameter aggregation → Global model update → Validation → Selection for next round
- Design tradeoffs: UCB provides better theoretical guarantees but requires maintaining reward statistics and may be more complex to implement than simpler strategies like epsilon-greedy.
- Failure signatures: Poor performance could indicate issues with UCB exploration rate, similarity weight calculation, or harmonic mean sensitivity to extreme values.
- First 3 experiments:
  1. Compare UCB vs epsilon-greedy collaborator selection with fixed aggregation method
  2. Compare harmonic mean vs arithmetic mean aggregation with fixed collaborator selection
  3. Test alternating vs fixed selection strategies (always top performers)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed RL-based collaborator selection approach perform when data distributions and collaborator populations change dynamically over time, as would occur in real-world FL scenarios?
- Basis in paper: [explicit] The authors note this is a limitation of their current study and suggest this as future work, stating "To adapt this selection strategy for such evolving conditions, we could employ more advanced multi-armed bandit algorithms that dynamically select collaborators to maximize the overall expected reward or utility in each round."
- Why unresolved: The current study uses a simplified scenario with fixed data distributions and collaborator populations. The authors propose adapting the approach for dynamic conditions but have not implemented or tested this.
- What evidence would resolve it: Experiments comparing the current approach to dynamic multi-armed bandit algorithms (e.g., Thompson sampling, UCB) in scenarios with changing collaborator populations and data distributions would provide evidence of the approach's effectiveness in real-world conditions.

### Open Question 2
- Question: Would alternative RL methodologies, such as Deep Q-Networks (DQN), improve collaborator selection and model performance compared to the Epsilon-greedy and UCB approaches tested in this study?
- Basis in paper: [explicit] The authors propose exploring "advanced RL paradigms" as future work, specifically mentioning "a DQN-based multi-objective participant selection" as a potential direction.
- Why unresolved: The study only tests Epsilon-greedy and UCB algorithms. While these show promising results, the authors suggest that more sophisticated RL methods could potentially yield better performance.
- What evidence would resolve it: Direct comparison of DQN-based collaborator selection with the current approaches using the same datasets and evaluation metrics would demonstrate if more advanced RL methods improve performance.

### Open Question 3
- Question: How does the proposed RL-HSimAgg approach compare to other aggregation methods in terms of robustness to outliers and non-IID data distributions?
- Basis in paper: [explicit] The authors introduce RL-HSimAgg as a novel approach using harmonic mean for aggregation and claim it "effectively handles outlier data points" and is "well-suited for non-IID data scenarios."
- Why unresolved: While the authors present RL-HSimAgg as an improvement, they do not provide direct comparisons with other aggregation methods (e.g., FedAvg, Krum) to validate these claims.
- What evidence would resolve it: Comparative experiments testing RL-HSimAgg against other aggregation methods under various data distribution scenarios (IID vs non-IID, presence of outliers) would demonstrate its relative effectiveness.

## Limitations

- The study lacks direct comparison with other state-of-the-art aggregation methods (FedAvg, Krum) to validate the claimed advantages of RL-HSimAgg
- The alternating selection strategy between top and bottom performers is based on limited theoretical justification rather than extensive empirical validation
- The approach was only tested on brain tumor segmentation with fixed collaborator populations and data distributions, limiting generalizability

## Confidence

- **High confidence**: The general framework of using RL for collaborator selection in FL is well-established, and the observed performance improvements (Dice scores of 0.7334 vs 0.6797 for ET) are statistically meaningful.
- **Medium confidence**: The specific mechanisms of UCB outperforming EG and harmonic mean improving robustness are plausible based on theoretical properties, but lack direct empirical validation in this exact context.
- **Low confidence**: The alternating selection strategy and its impact on long-term performance requires further validation, as does the robustness of the approach across different data distributions and collaborator counts.

## Next Checks

1. **Ablation study**: Systematically evaluate the contribution of each component (UCB selection, harmonic mean aggregation, alternating strategy) by removing them individually and measuring performance impact.

2. **Generalization testing**: Validate the approach on different medical imaging tasks and datasets to assess robustness beyond brain tumor segmentation, particularly for tasks with varying data heterogeneity.

3. **Sensitivity analysis**: Test the approach with different numbers of collaborators (beyond 33) and communication rounds to understand scalability limits and optimal configuration parameters.