---
ver: rpa2
title: Limitations of Automatic Relevance Assessments with Large Language Models for
  Fair and Reliable Retrieval Evaluation
arxiv_id: '2411.13212'
source_url: https://arxiv.org/abs/2411.13212
tags:
- judgements
- systems
- retrieval
- runs
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether LLM-generated relevance judgements
  can fairly and reliably evaluate retrieval systems, particularly at the top ranks
  and in statistical significance testing. Using synthetic test collections with both
  human and LLM judgements, the authors find that LLM-based judgements perform poorly
  at identifying top-performing systems, with significant ranking shifts and unfairness
  across runs.
---

# Limitations of Automatic Relevance Assessments with Large Language Models for Fair and Reliable Retrieval Evaluation

## Quick Facts
- **arXiv ID**: 2411.13212
- **Source URL**: https://arxiv.org/abs/2411.13212
- **Reference count**: 29
- **Primary result**: LLM-generated relevance judgments fail to reliably evaluate retrieval systems, especially at top ranks and in statistical significance testing.

## Executive Summary
This paper investigates whether LLM-generated relevance judgments can fairly and reliably evaluate retrieval systems, particularly at top ranks and in statistical significance testing. Using synthetic test collections with both human and LLM judgments, the authors find that LLM-based judgments perform poorly at identifying top-performing systems, with significant ranking shifts and unfairness across runs. In statistical tests, LLM judgments yield an excessively high rate of false positives—incorrectly marking non-significant differences as significant—though this decreases when topic counts are matched. Top-heavy correlations further reveal degraded accuracy at the top. Overall, the results suggest LLMs are not yet reliable substitutes for human judgments in IR evaluation.

## Method Summary
The study uses a synthetic test collection (SynDL) built from the TREC Deep Learning Track (2019-2023), containing both human-judged and GPT-4-generated relevance judgments for disjoint topic sets. The authors compute ranking correlations (standard Kendall's τ and top-weighted measures τ_AP, RBO) between system rankings from human versus LLM judgments. They analyze fairness by measuring per-run ranking position changes and perform statistical significance tests (Wilcoxon Signed-Rank test) for all pairwise system comparisons, examining false positive rates. To address sample size imbalances, they also test with undersampled LLM topics matched to human topic counts.

## Key Results
- LLM judgments show degraded accuracy at top ranks, with top-weighted correlations revealing significant performance drops.
- Significant unfairness across runs occurs, with some systems experiencing ranking position changes of up to 50 positions.
- Statistical significance tests using LLM judgments produce excessively high false positive rates, incorrectly marking non-significant differences as significant.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated relevance judgements exhibit poor accuracy at the top of the ranking due to top-heavy degradation in correlation.
- Mechanism: As focus narrows to the highest-ranked systems, LLM-based judgements fail to preserve the same ordering as human judgements, leading to larger discrepancies at the top where correct ordering is most critical.
- Core assumption: Kendall's τ is insufficient for evaluating top-heavy performance because it treats all rank differences equally.
- Evidence anchors:
  - [abstract] "These correlations are helpful in large-scale experiments but less informative if we want to focus on top-performing systems."
  - [section 2] "Correlations are not as good as they seem when we focus on the top-performing systems and degrade as we focus more and more on the first few systems."
  - [corpus] Weak - no direct corpus evidence for this mechanism, only indirect support from related works.
- Break condition: If top-weighted correlations (e.g., τ_AP, RBO) show comparable values to standard Kendall's τ, indicating LLMs can preserve top ranking.

### Mechanism 2
- Claim: LLM-based judgements introduce significant unfairness across runs, causing large ranking position shifts for some systems.
- Mechanism: When evaluated with LLM judgements, certain runs experience dramatic ranking changes (up to 50 positions), indicating inconsistent treatment and unfairness in the evaluation process.
- Core assumption: Fairness is measured by the consistency of ranking positions between human and LLM judgements.
- Evidence anchors:
  - [section 2.1] "there are, in all cases, regardless of the metric or the collection, runs that suffer changes of 5 or more positions, even reaching changes of more than 40 positions."
  - [section 2.1] "almost all runs suffer some change in position when evaluated with LLM judgments."
  - [corpus] Weak - corpus shows related works but no direct evidence for this specific unfairness mechanism.
- Break condition: If ranking position changes are uniformly small (e.g., < 5 positions) across all runs.

### Mechanism 3
- Claim: LLM judgements produce an excessively high rate of false positives in statistical significance testing.
- Mechanism: When performing pairwise significance tests, LLM judgements incorrectly mark non-significant differences as significant at a much higher rate than human judgements, leading to incorrect conclusions about system performance.
- Core assumption: Human judgements serve as the ground truth for determining true significance.
- Evidence anchors:
  - [abstract] "In statistical tests, LLM judgements yield an excessively high rate of false positives—incorrectly marking non-significant differences as significant."
  - [section 3.2] "However, we observe a high number of false positives for every year and metric."
  - [section 3.2] "These cases represent situations were the LLM judgements marked a significant difference when there was none."
  - [corpus] Weak - corpus mentions related works but no direct evidence for this specific false positive mechanism.
- Break condition: If false positive rates drop to acceptable levels (e.g., < 10%) when sample sizes are matched.

## Foundational Learning

- Concept: Statistical significance testing (Wilcoxon Signed-Rank test)
  - Why needed here: The paper uses statistical tests to compare system performance between human and LLM judgements, requiring understanding of p-values and significance levels.
  - Quick check question: What does a p-value represent in the context of comparing two systems' performance?

- Concept: Ranking correlation measures (Kendall's τ, τ_AP, RBO)
  - Why needed here: The paper evaluates how well LLM judgements preserve system rankings compared to human judgements using different correlation measures.
  - Quick check question: How does Kendall's τ differ from τ_AP in terms of penalizing ranking errors?

- Concept: Test collection construction and relevance judgements
  - Why needed here: Understanding the basic structure of IR evaluation and the role of relevance judgements in comparing systems.
  - Quick check question: What is the difference between a "topic" and a "query" in the context of IR test collections?

## Architecture Onboarding

- Component map:
  - Data ingestion: SynDL dataset containing human and LLM judgements for different topics
  - Correlation computation: Standard Kendall's τ and top-weighted measures (τ_AP, RBO)
  - Fairness analysis: Per-run ranking position changes between human and LLM judgements
  - Statistical testing: Wilcoxon Signed-Rank test for pairwise system comparisons
  - Analysis and visualization: Distribution plots of ranking changes and significance drops

- Critical path:
  1. Load human and LLM judgements for disjoint topic sets
  2. Compute per-topic scores for all runs using both judgement sets
  3. Calculate ranking correlations and analyze top-heavy performance
  4. Evaluate fairness by comparing per-run ranking positions
  5. Perform statistical significance tests and analyze false positive rates
  6. Visualize results and draw conclusions

- Design tradeoffs:
  - Using disjoint topic sets prevents direct comparison but reflects real-world scenario where human and LLM judgements cover different topics
  - Top-weighted correlations provide better insight into top performance but may be less stable with smaller sample sizes
  - Statistical tests with unequal sample sizes may inflate false positive rates, addressed through undersampling

- Failure signatures:
  - High false positive rates in statistical tests (> 50%)
  - Large per-run ranking position changes (> 5 positions for many runs)
  - Significant degradation in top-weighted correlations compared to standard Kendall's τ
  - Non-uniform distribution of ranking changes or significance drops

- First 3 experiments:
  1. Reproduce the correlation analysis comparing human vs LLM judgements using standard Kendall's τ and top-weighted measures (τ_AP, RBO)
  2. Implement the fairness analysis by computing per-run ranking position changes and visualizing their distribution
  3. Conduct the statistical significance testing experiment with and without undersampling to verify the false positive rate findings

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. The limitations section suggests areas for future work, including exploring hybrid human-LLM approaches and investigating mitigation strategies for the identified problems.

## Limitations
- Findings are based on a single synthetic test collection from the TREC Deep Learning Track, limiting generalizability to other domains.
- The study assumes human judgements serve as ground truth, though human assessments can also contain biases and inconsistencies.
- Only one LLM model (GPT-4) is evaluated, without exploring how different models or prompting strategies might affect results.

## Confidence

- **High confidence**: The identification of unfairness in per-run ranking position changes is well-supported by the data and methodology. The statistical significance testing results showing excessive false positives are robust and clearly demonstrated.
- **Medium confidence**: The top-heavy correlation degradation findings are convincing but could benefit from additional validation across different domains and LLM models.
- **Low confidence**: The generalizability of findings to other evaluation scenarios beyond the Deep Learning Track is uncertain without additional empirical validation.

## Next Checks
1. Replicate the analysis using a different test collection (e.g., newswire or web document collections) to assess domain generalizability of the findings.
2. Compare results across multiple LLM models (GPT-3.5, Claude, LLaMA) with varying prompting strategies to determine if the observed limitations are model-specific.
3. Investigate hybrid evaluation approaches where LLM judgements are used in conjunction with human assessments to potentially improve reliability while reducing costs.