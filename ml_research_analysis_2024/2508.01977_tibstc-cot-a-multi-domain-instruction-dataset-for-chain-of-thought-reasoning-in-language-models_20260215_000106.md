---
ver: rpa2
title: 'TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning
  in Language Models'
arxiv_id: '2508.01977'
source_url: https://arxiv.org/abs/2508.01977
tags:
- uni00000048
- uni00000051
- arxiv
- language
- uni00000010
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TIBSTC-CoT, the first large-scale, multi-domain
  Tibetan instruction dataset designed to address the severe data scarcity in Tibetan
  NLP. Built via an automated pipeline leveraging multilingual LLMs, the dataset covers
  diverse domains and emphasizes chain-of-thought reasoning to improve interpretability
  and task performance.
---

# TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models

## Quick Facts
- arXiv ID: 2508.01977
- Source URL: https://arxiv.org/abs/2508.01977
- Reference count: 11
- First large-scale, multi-domain Tibetan instruction dataset designed to address data scarcity in Tibetan NLP

## Executive Summary
This paper introduces TIBSTC-CoT, the first large-scale, multi-domain Tibetan instruction dataset designed to address the severe data scarcity in Tibetan NLP. Built via an automated pipeline leveraging multilingual LLMs, the dataset covers diverse domains and emphasizes chain-of-thought reasoning to improve interpretability and task performance. Based on TIBSTC-CoT, the authors develop Sunshine-thinking, a family of Tibetan-centric LLMs fine-tuned for chain-of-thought reasoning. Extensive evaluations show that Sunshine-Thinking models achieve generation quality on par with state-of-the-art multilingual LLMs, setting a new baseline for low-resource language modeling and multilingual translation.

## Method Summary
The authors developed an automated multi-stage pipeline to generate TIBSTC-CoT, using three specialized multilingual LLMs: Claude-3.5-Sonnet for question generation, DeepSeek-R1 for CoT/answer generation, and Gemini-1.5-Flash for automatic evaluation. The pipeline includes domain-specific prompts, automatic scoring (≥3.5 threshold), and manual verification by native Tibetan speakers. The resulting dataset was used to fine-tune Sunshine-Thinking models (1.7B and 8B parameters) using supervised fine-tuning with AdamW optimizer, learning rate 1e-5, mixed precision, batch size 8, for 2-3 epochs.

## Key Results
- TIBSTC-CoT covers 10 domains with 100K instruction samples emphasizing chain-of-thought reasoning
- Sunshine-Thinking-1.7B achieves 35.00% on Social Sciences domain, outperforming GPT-4.1
- Models demonstrate strong translation quality (chrF++) across BO→ZH, BO→EN, ZH→BO, EN→BO directions

## Why This Works (Mechanism)

### Mechanism 1: Chain-of-Thought Reasoning
CoT prompts LLMs to produce step-by-step explanations before final answers, enhancing interpretability and reducing reasoning errors. Core assumption: explicit intermediate reasoning steps are more reliable than direct answers for complex tasks in low-resource languages. Break condition: if intermediate steps introduce noise or errors, CoT reasoning may degrade performance.

### Mechanism 2: Multilingual LLM-Based Dataset Generation
The automated pipeline uses multilingual LLMs (Claude-3.5-Sonnet, DeepSeek-R1, Gemini-1.5-Flash) with distinct roles to mitigate bias and address data scarcity. Core assumption: strong multilingual LLMs can generate high-quality Tibetan instruction data when guided by domain-specific instructions. Break condition: if LLMs lack sufficient Tibetan language competence, generated data may be inaccurate or culturally inappropriate.

### Mechanism 3: Efficient Fine-Tuning of Smaller Models
Supervised fine-tuning on TIBSTC-CoT enables smaller models (1.7B, 8B) to achieve competitive performance with much larger models. Core assumption: high-quality, domain-specific instruction data can compensate for smaller model size in low-resource language tasks. Break condition: if dataset size or quality is insufficient, smaller models may fail to match larger model performance.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: CoT enhances interpretability and reasoning performance in low-resource languages by generating explicit intermediate steps.
  - Quick check question: How does CoT prompting differ from direct answer prompting in terms of model reasoning?

- Concept: Multilingual LLM-based dataset generation
  - Why needed here: Automated dataset generation using multilingual LLMs addresses severe data scarcity in Tibetan NLP.
  - Quick check question: What roles do different multilingual LLMs play in the TIBSTC-CoT generation pipeline?

- Concept: Supervised fine-tuning (SFT) on instruction data
  - Why needed here: SFT aligns pretrained models with task-specific instruction datasets, enabling better performance on Tibetan language tasks.
  - Quick check question: What hyperparameters and training strategies are used for fine-tuning Sunshine-Thinking models?

## Architecture Onboarding

- Component map: Question Generation Agent (Claude-3.5-Sonnet) → CoT/Answer Generation Agent (DeepSeek-R1) → Evaluation Agent (Gemini-1.5-Flash) → Manual Verification Stage → Sunshine-Thinking Model Family (1.7B, 8B variants)

- Critical path: Question generation → CoT/answer generation → automatic evaluation → manual verification → model fine-tuning

- Design tradeoffs:
  - Using smaller models (1.7B, 8B) vs. larger models for efficiency
  - Automated vs. manual dataset generation for quality vs. scalability
  - CoT reasoning vs. direct answers for interpretability vs. computational cost

- Failure signatures:
  - Poor translation quality in Tibetan directions
  - Low scores on TLUE benchmark domains
  - Inaccurate or culturally inappropriate generated content

- First 3 experiments:
  1. Evaluate translation quality (chrF++) for Sunshine-Thinking models on BO→ZH, BO→EN, ZH→BO, EN→BO directions
  2. Test understanding performance on TLUE benchmark across China-specific, Other, Social Sciences, Humanities, and STEM domains
  3. Assess generation quality using Ti-MMLU benchmark and compare with baseline models

## Open Questions the Paper Calls Out

### Open Question 1
How effective is the TIBSTC-CoT dataset for instruction tuning of Tibetan-specific tasks beyond reasoning, such as dialogue or summarization? The paper emphasizes the dataset's effectiveness for chain-of-thought reasoning but does not explore its utility for other downstream tasks like dialogue or summarization.

### Open Question 2
What is the long-term impact of using multilingual LLMs for generating low-resource language datasets, particularly regarding cultural and linguistic biases? The paper mentions manual verification for cultural appropriateness but does not address the broader implications of using multilingual models for dataset generation in low-resource settings.

### Open Question 3
How scalable is the TIBSTC-CoT pipeline for other low-resource languages, and what challenges might arise in adapting it? The paper claims the pipeline is "scalable and reproducible" for low-resource languages but does not provide empirical evidence or discuss potential challenges in adapting it to other languages.

## Limitations

- Reliance on automated LLM-based dataset generation without sufficient empirical validation of data quality
- Evaluation methodology lacks baseline comparisons with other Tibetan-specific models
- Absence of ablation studies to isolate the contribution of CoT reasoning versus other factors

## Confidence

- High Confidence: TIBSTC-CoT addresses data scarcity in Tibetan NLP with concrete numbers and domain coverage
- Medium Confidence: Sunshine-Thinking models achieve "strong reasoning and generation performance, comparable to state-of-the-art multilingual LLMs" based on TLUE benchmark results
- Low Confidence: CoT reasoning is essential for achieving these results due to absence of ablation studies and direct comparisons

## Next Checks

1. **Ablation Study**: Fine-tune identical Sunshine-Thinking models on TIBSTC-CoT with and without CoT annotations, then compare performance across all benchmark tasks to isolate the contribution of CoT reasoning.

2. **Human Evaluation**: Conduct blind human evaluation where native Tibetan speakers rate the quality, cultural appropriateness, and factual accuracy of LLM-generated samples versus human-curated samples from other Tibetan datasets.

3. **Cross-Lingual Transfer Analysis**: Test whether Sunshine-Thinking models fine-tuned on TIBSTC-CoT show improved performance on closely related low-resource languages (e.g., Dzongkha, Sherpa) compared to models fine-tuned on multilingual datasets without Tibetan-specific CoT reasoning.