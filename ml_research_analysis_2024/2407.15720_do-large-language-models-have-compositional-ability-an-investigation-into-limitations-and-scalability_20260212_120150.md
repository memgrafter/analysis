---
ver: rpa2
title: Do Large Language Models Have Compositional Ability? An Investigation into
  Limitations and Scalability
arxiv_id: '2407.15720'
source_url: https://arxiv.org/abs/2407.15720
tags:
- tasks
- composite
- task
- have
- simple
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  compose simple tasks to solve unseen composite tasks through in-context learning.
  The authors design a test suite of composite tasks involving linguistic and logical
  challenges, where models are given examples of simple tasks and asked to tackle
  composite tasks they have not encountered during pretraining or in-context learning.
---

# Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability

## Quick Facts
- **arXiv ID**: 2407.15720
- **Source URL**: https://arxiv.org/abs/2407.15720
- **Reference count**: 40
- **Primary result**: LLMs demonstrate compositional ability on separable composite tasks but fail on sequential reasoning tasks, with scaling providing no improvements for the latter.

## Executive Summary
This paper investigates whether large language models can compose simple tasks to solve unseen composite tasks through in-context learning. The authors design a test suite of composite tasks involving linguistic and logical challenges, where models are given examples of simple tasks and asked to tackle composite tasks they have not encountered during pretraining or in-context learning. The results show that models exhibit divergent behaviors: for simpler composite tasks that apply distinct mapping mechanisms to different input segments, the models demonstrate decent compositional ability, while scaling up the model enhances this ability; for more complex composite tasks involving reasoning multiple steps, where each step represents one task, models typically underperform, and scaling up generally provides no improvements.

## Method Summary
The study uses synthetic data to create composite tasks, minimizing the risk of data leakage. The tasks include logical rules and linguistic translation, with inputs designed to be distinct segments or perspectives. The authors use in-context learning, where models are given examples of simple tasks and then asked to solve composite tasks. Experiments are conducted across different LLM families, including Llama and GPT models. Performance is measured using exact match accuracy for logical tasks and word error rate (WER) for linguistic translation tasks.

## Key Results
- LLMs show decent compositional ability on separable composite tasks that apply distinct mapping mechanisms to different input segments
- Scaling up model size enhances compositional ability only for separable composite tasks
- Models underperform on composite tasks requiring sequential reasoning, with scaling providing no improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Models exhibit compositional ability when simple tasks handle different input segments separately (separable composite tasks).
- **Mechanism:** The model decomposes input embeddings into distinct "regions" where each region corresponds to a specific task type. When tasks operate on separate regions, the model can process each subtask independently within its corresponding region.
- **Core assumption:** Input embeddings have confined support - each task's embedding only has significant values within its active index set, with bounded values outside.
- **Evidence anchors:**
  - [abstract] "We observe that models exhibit divergent behaviors: (1) For simpler composite tasks that apply distinct mapping mechanisms to different input segments, the models demonstrate decent compositional ability..."
  - [section 3.1] "We refer to these tasks as 'separable composite tasks', which are relatively easy for the model to solve."
  - [corpus] Weak evidence - corpus neighbors discuss generalization but not specifically confined support mechanisms.
- **Break condition:** When tasks share overlapping support in the embedding space (mixed steps reasoning), information mixes together and the model cannot process them distinctly.

### Mechanism 2
- **Claim:** Scaling up model size improves compositional ability only when tasks have confined support.
- **Mechanism:** Larger models have higher-rank parameter matrices (WPV and WKQ) that can process and store more information. When tasks have confined support, increased model capacity allows better separation and processing of distinct task regions.
- **Core assumption:** Rank of parameter matrices serves as a measure of model scale and processing capacity.
- **Evidence anchors:**
  - [section 4.2] "Theorem 2 shows the expected accuracy of a model on composite tasks is subjected to a lower upper bound as the scale of the model diminishes."
  - [section 3.1] "For simpler composite tasks that apply distinct mapping mechanisms to different input segments, the models demonstrate decent compositional ability, while scaling up the model enhances this ability."
  - [corpus] Weak evidence - corpus neighbors discuss scaling but not specifically rank-parameter relationships.
- **Break condition:** When tasks don't have confined support, scaling provides no improvements as the fundamental processing limitation remains.

### Mechanism 3
- **Claim:** Models fail on composite tasks requiring sequential reasoning because information from different steps mixes in the embedding space.
- **Mechanism:** Sequential reasoning tasks require processing information from multiple steps that share the same embedding space support. This mixing prevents the model from effectively distinguishing and processing each step's information separately.
- **Core assumption:** Sequential reasoning tasks inherently involve mixed steps that share embedding space support.
- **Evidence anchors:**
  - [section 3.1] "On composite steps tasks involving the arithmetic calculation of numerical numbers (G) + (H), the model has the worst performance, and increasing the model scale does not provide benefits."
  - [section 3.1] "We also substantiate this observation with Corollary 1, which establishes that when two tasks share overlapping support in the embedding space..."
  - [corpus] Weak evidence - corpus neighbors discuss reasoning limitations but not specifically sequential step mixing.
- **Break condition:** When tasks can be decomposed into separate, non-overlapping processing steps.

## Foundational Learning

- **Concept:** In-context learning (ICL)
  - Why needed here: The paper evaluates LLMs on composite tasks using only in-context examples, not parameter updates. Understanding ICL is crucial for interpreting the experimental setup.
  - Quick check question: What distinguishes in-context learning from traditional fine-tuning in terms of parameter updates?

- **Concept:** Linear self-attention networks
  - Why needed here: The theoretical analysis uses linear self-attention networks to explain compositional behavior, providing a simplified framework for understanding complex transformer behavior.
  - Quick check question: How does the linear self-attention network formulation differ from standard transformer attention?

- **Concept:** Confined support in input embeddings
  - Why needed here: The key theoretical insight is that models perform well when tasks have confined support in their respective embedding subspaces, which explains the variable compositional performance.
  - Quick check question: What does "confined support" mean in the context of input embeddings and task processing?

## Architecture Onboarding

- **Component map:** Input embedding → region decomposition → task-specific processing → output combination
- **Critical path:** Input embedding → region decomposition → task-specific processing → output combination
- **Design tradeoffs:** 
  - Simplicity vs. expressiveness in theoretical model
  - Computational efficiency vs. processing capacity (scaling)
  - Generalizability vs. task-specific optimization
- **Failure signatures:**
  - Poor performance on sequential reasoning tasks
  - No improvement with scaling when tasks share embedding space
  - Successful task completion only when provided composite in-context examples
- **First 3 experiments:**
  1. Test simple capitalization task to verify basic model capability
  2. Test composite capitalization+swap task to observe failure case
  3. Test separable composite tasks (capitalization+plus one) to verify scaling benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed compositional ability in LLMs on separable tasks extend to real-world natural language tasks that involve more complex and ambiguous structures?
- Basis in paper: The paper demonstrates that LLMs perform well on separable composite tasks in both linguistic and logical domains, but raises the question of whether this ability holds for more complex, real-world scenarios.
- Why unresolved: The study focuses on synthetic data and controlled tasks to mitigate pretraining data leakage, but real-world language often involves nuanced contexts and ambiguities not present in these controlled experiments.
- What evidence would resolve it: Testing LLMs on a diverse set of real-world compositional tasks, such as those involving idiomatic expressions or sarcasm, and comparing their performance to that on the synthetic tasks used in the study.

### Open Question 2
- Question: How do different model architectures beyond transformers, such as recurrent neural networks or graph neural networks, perform on the same composite tasks?
- Basis in paper: The study focuses on transformer-based LLMs and their in-context learning capabilities, but does not explore other model architectures.
- Why unresolved: The paper's theoretical analysis and empirical results are specific to transformers, leaving open the question of whether similar compositional abilities are present in other architectures.
- What evidence would resolve it: Conducting similar experiments with other model architectures on the same composite tasks and comparing their performance to that of transformers.

### Open Question 3
- Question: What is the impact of increasing the number of simple tasks combined into a composite task on the model's ability to perform compositional reasoning?
- Basis in paper: The paper investigates the composition of two simple tasks but does not explore scenarios where more tasks are combined.
- Why unresolved: The complexity of compositional reasoning likely increases with the number of tasks, and it is unclear how this affects the model's performance.
- What evidence would resolve it: Designing experiments that gradually increase the number of simple tasks combined into composite tasks and analyzing the model's performance as the complexity increases.

## Limitations
- The exact mechanism by which input embeddings achieve confined support remains underspecified
- The synthetic nature of evaluation tasks limits generalizability to real-world compositional scenarios
- The relationship between model scale and compositional ability shows variability across different model families

## Confidence
- **High Confidence:** The empirical observation that LLMs perform better on separable composite tasks than on sequential reasoning tasks
- **Medium Confidence:** The theoretical analysis connecting confined support to compositional ability
- **Low Confidence:** The claim that scaling up models universally improves performance on separable tasks

## Next Checks
1. **Validate the confined support hypothesis** by conducting ablation studies that systematically vary the degree of overlap between task embedding regions, measuring how performance degrades as support regions become less confined.

2. **Test with naturalistic compositional tasks** by creating evaluation benchmarks using real-world data that require combining common language tasks (e.g., summarization + translation) to verify whether the theoretical insights generalize beyond synthetic examples.

3. **Investigate alternative architectures** by comparing transformer performance against models with explicit structural biases for compositionality (such as modular networks or tree-structured models) on the same benchmark to determine whether the observed limitations are fundamental to the transformer architecture or can be overcome through architectural modifications.