---
ver: rpa2
title: Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?
arxiv_id: '2410.10476'
source_url: https://arxiv.org/abs/2410.10476
tags:
- temporal
- have
- llama2
- llms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  replace encoder-only models like RoBERTa for temporal relation classification (TRC).
  The authors evaluate seven open and closed-sourced LLMs using in-context learning
  and fine-tuning approaches across three benchmark datasets (MATRES, TIMELINE, TB-Dense).
---

# Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?

## Quick Facts
- arXiv ID: 2410.10476
- Source URL: https://arxiv.org/abs/2410.10476
- Authors: Gabriel Roccabruna; Massimo Rizzoli; Giuseppe Riccardi
- Reference count: 33
- Key outcome: Encoder-only models (RoBERTa) significantly outperform LLMs (65.3% vs 87.6% micro-F1) on temporal relation classification across three benchmark datasets

## Executive Summary
This paper investigates whether large language models can replace encoder-only models for temporal relation classification (TRC). The authors evaluate seven open and closed-sourced LLMs using in-context learning and fine-tuning approaches across three benchmark datasets (MATRES, TIMELINE, TB-Dense). Results show that LLMs with in-context learning significantly underperform smaller encoder-only models based on RoBERTa, with the best LLM achieving only 65.3% micro-F1 compared to RoBERTa's 87.6% on MATRES. The authors use explainable methods to analyze this performance gap, finding that LLMs focus primarily on the last part of the input sequence due to their autoregressive nature, while RoBERTa uses the entire context. Word embedding analysis suggests the gap may also stem from different pre-training strategies between the two model types.

## Method Summary
The study evaluates temporal relation classification using three benchmark datasets (MATRES, TIMELINE, TB-Dense) with encoder-only RoBERTa and seven LLMs (Llama2 7B/13B/70B, Mistral 7B, Mixtral 8×7B, GPT-3, GPT-3.5). Models are tested using in-context learning with three prompt types (example-label pattern, single-question QA, sequential QA) and fine-tuning with LoRA. Performance is measured using micro-F1 score after removing the vague class. Explainability is analyzed using KernelShap attribution to compare token importance, and word embeddings are compared to investigate pre-training strategy effects.

## Key Results
- RoBERTa achieves 87.6% micro-F1 on MATRES compared to best LLM at 65.3% micro-F1
- LLMs with in-context learning significantly underperform encoder-only models across all datasets
- Fine-tuning reduces but doesn't eliminate the performance gap between LLMs and RoBERTa
- Explainability analysis shows LLMs focus on sequence endings while RoBERTa uses full context
- Word embedding differences suggest pre-training strategies may contribute to performance gaps

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs underperform encoder-only models in temporal relation classification due to their autoregressive nature causing them to focus primarily on the last part of the input sequence.
- **Mechanism**: Encoder-only models like RoBERTa use bidirectional attention to process the entire context, while autoregressive LLMs generate tokens sequentially and attend primarily to recent tokens. This sequential processing leads LLMs to focus on the final portions of input sequences when making predictions.
- **Core assumption**: The temporal relation classification task requires holistic understanding of the full context rather than just recent information.
- **Evidence anchors**:
  - [abstract]: "The outcome suggests a limitation of LLMs in this task due to their autoregressive nature, which causes them to focus only on the last part of the sequence."
  - [section 6.1]: "Most of the tokens with the highest attribution score are at the end of the sequence, meaning that the model tends to use only the last few tokens to make a prediction. Conversely, the distribution of the encoder-only model is more uniform, meaning that the decision process of RoBERTa considers the entire sequence."
  - [corpus]: Weak evidence - the paper analyzes three benchmark datasets (MATRES, TIMELINE, TB-Dense) but doesn't directly show how sequence position affects performance across different relation types.
- **Break condition**: This mechanism would break if temporal relations could be reliably predicted from just the final tokens of a sequence, or if the task inherently prioritized recent information over full context.

### Mechanism 2
- **Claim**: The performance gap between LLMs and encoder-only models stems from different pre-training strategies, specifically masked language modeling versus autoregressive language modeling.
- **Mechanism**: RoBERTa's masked language modeling pre-training task encourages bidirectional context understanding, while LLMs' autoregressive pre-training focuses on next-token prediction. This creates different representations that affect downstream task performance.
- **Core assumption**: The pre-training task significantly influences the learned representations and their suitability for temporal relation classification.
- **Evidence anchors**:
  - [abstract]: "Word embedding analysis suggests the gap may also stem from different pre-training strategies between the two model types."
  - [section 6.2]: "The results suggest that the word embeddings yielded by RoBERTa are more effective in the TRC task, supporting the outcome of the explainability studies for which one of the probable reasons for the performance gap is in the different pre-training tasks."
  - [corpus]: Weak evidence - the paper evaluates three datasets but doesn't systematically compare how different pre-training strategies perform across various relation types or datasets.
- **Break condition**: This mechanism would break if fine-tuning could completely overcome the pre-training differences, or if other factors (like model size or architectural differences) were the dominant cause of performance gaps.

### Mechanism 3
- **Claim**: Encoder-only models maintain superior performance even when LLMs are fine-tuned, indicating that the architectural differences are fundamental rather than just a matter of pre-training.
- **Mechanism**: Even after fine-tuning with LoRA techniques, LLMs cannot match the performance of encoder-only models on temporal relation classification, suggesting that the bidirectional processing architecture provides inherent advantages for this task.
- **Core assumption**: The architectural differences between encoder-only and decoder-only models create fundamental performance differences that cannot be fully overcome through fine-tuning.
- **Evidence anchors**:
  - [abstract]: "Results show that LLMs with in-context learning significantly underperform smaller encoder-only models based on RoBERTa, with the best LLM achieving only 65.3% micro-F1 compared to RoBERTa's 87.6% on MATRES."
  - [section 6]: "Although fine-tuning Llama2 7B and 13B substantially reduces this gap on MATRES, the differences are still high in the other two datasets."
  - [corpus]: Evidence is mixed - while the paper shows overall performance differences across three datasets, it doesn't analyze whether certain relation types or dataset characteristics make the task more or less suitable for different architectures.
- **Break condition**: This mechanism would break if larger LLMs (beyond those tested) could match or exceed encoder-only performance, or if specific fine-tuning strategies could eliminate the performance gap.

## Foundational Learning

- **Concept**: Temporal Relation Classification (TRC)
  - Why needed here: Understanding the task definition is essential for interpreting model performance and comparing different approaches. TRC involves identifying temporal relationships between events in text.
  - Quick check question: What are the key differences between temporal relation classification and other relation extraction tasks like causal or discourse relation classification?

- **Concept**: Encoder-only vs Decoder-only (LLM) architectures
  - Why needed here: The paper directly compares these two architectural paradigms, and understanding their fundamental differences is crucial for interpreting why one performs better than the other.
  - Quick check question: How does bidirectional attention in encoder-only models differ from the causal attention in decoder-only models, and what implications does this have for sequence processing?

- **Concept**: In-context learning vs Fine-tuning
  - Why needed here: The paper evaluates both approaches for LLMs, and understanding these different adaptation methods is important for interpreting the results and their implications.
  - Quick check question: What are the key differences between in-context learning and fine-tuning in terms of parameter updates, computational requirements, and performance characteristics?

## Architecture Onboarding

- **Component map**: Data preprocessing and dataset handling for MATRES, TIMELINE, TB-Dense -> Model implementations (RoBERTa-based encoder model, Llama2 variants, Mistral, Mixtral, GPT-3/3.5) -> Evaluation and explainability modules (KernelShap attribution analysis, word embedding comparison)

- **Critical path**: Load and preprocess dataset → run models with specified prompts → collect predictions → compute micro-F1 scores → analyze results with explainability methods → compare word embeddings

- **Design tradeoffs**: Performance vs adaptability (RoBERTa achieves 87.6% vs best LLM at 65.3% micro-F1), computational efficiency vs generalization (fine-tuning LLMs requires significant GPU resources), prompt design simplicity vs reasoning depth

- **Failure signatures**: LLMs failing on this task show characteristic patterns: focusing primarily on the last portion of input sequences, generating contradictory responses in QA1 format, systematic bias toward certain relation classes, poor performance on datasets with more relation classes

- **First 3 experiments**:
  1. Run the RoBERTa-based model on MATRES with default hyperparameters to establish the baseline performance (87.6% micro-F1)
  2. Test Llama2 7B with in-context learning using prompt P on MATRES to verify the significant performance gap (expected ~31% micro-F1)
  3. Apply KernelShap attribution analysis to compare token importance distributions between Llama2 7B and RoBERTa on a subset of MATRES examples to observe the end-focused vs uniform attention patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does pre-training on masked language modeling versus autoregressive language modeling fundamentally affect temporal reasoning capabilities in LLMs versus encoder-only models?
- Basis in paper: [explicit] The authors observe that LLMs focus primarily on the last part of the input sequence due to their autoregressive nature, while RoBERTa uses the entire context. They also note that the word embeddings from RoBERTa are more effective for temporal relation classification.
- Why unresolved: While the paper demonstrates that pre-training tasks differ between model types, it doesn't experimentally isolate how these pre-training strategies specifically impact temporal reasoning capabilities.
- What evidence would resolve it: Experimental results comparing models pre-trained on identical datasets but with different objectives (MLM vs autoregressive), or ablation studies showing how much performance is attributable to pre-training strategy versus architecture.

### Open Question 2
- Question: What is the upper bound performance of LLMs for temporal relation classification when trained on substantially larger amounts of task-specific data?
- Basis in paper: [inferred] The authors find that fine-tuning improves LLM performance but still doesn't reach RoBERTa levels, suggesting data efficiency may be an issue.
- Why unresolved: The paper only explores fine-tuning with standard datasets and doesn't investigate whether increasing training data quantity or quality could close the performance gap.
- What evidence would resolve it: Results showing LLM performance scaling with training data size, particularly when training on multiple times the standard dataset sizes used for temporal relation classification.

### Open Question 3
- Question: How do different prompt engineering strategies affect LLM performance on temporal relation classification, and can optimal prompts bridge the performance gap?
- Basis in paper: [explicit] The authors experiment with example-label and QA prompts, finding that QA2 (answering questions in sequence) performs best among LLM approaches, but still underperforms RoBERTa.
- Why unresolved: While the paper tests several prompt formats, it doesn't exhaustively explore the prompt space or investigate whether more sophisticated prompt engineering could achieve parity with encoder-only models.
- What evidence would resolve it: Comparative results showing LLM performance across a wide range of prompt strategies, including chain-of-thought prompting, structured reasoning prompts, and dynamically generated prompts.

## Limitations
- Evaluation limited to three specific benchmark datasets, may not generalize to other temporal reasoning tasks
- Study uses relatively small LLMs (7B-13B parameters) for fine-tuning, leaving open question of whether larger models could bridge performance gap
- Analysis of pre-training strategy differences is suggestive but not conclusive, word embedding analysis provides weak evidence
- Prompt engineering approaches may not represent optimal configurations for LLMs on this task

## Confidence
- **High confidence**: Core finding that encoder-only models significantly outperform LLMs on temporal relation classification across multiple datasets and evaluation methods
- **Medium confidence**: Explainability findings showing LLMs focus on sequence endings while encoder models use full context
- **Low confidence**: Claim about pre-training strategy differences being a primary cause of performance gaps

## Next Checks
1. **Scale validation**: Test whether larger LLMs (e.g., 70B+ parameters) with fine-tuning can match or exceed encoder-only model performance on temporal relation classification, controlling for computational resources and training data

2. **Cross-task generalization**: Evaluate the same models and approaches on related temporal reasoning tasks such as temporal ordering, duration prediction, or cross-document temporal inference to determine if the performance gap is specific to classification or reflects broader architectural differences

3. **Controlled ablation study**: Systematically vary pre-training strategies by fine-tuning encoder-only models with autoregressive objectives and LLMs with masked language modeling to isolate the impact of pre-training from architectural differences