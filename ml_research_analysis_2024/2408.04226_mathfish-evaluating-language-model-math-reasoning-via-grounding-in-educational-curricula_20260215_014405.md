---
ver: rpa2
title: 'Mathfish: Evaluating Language Model Math Reasoning via Grounding in Educational
  Curricula'
arxiv_id: '2408.04226'
source_url: https://arxiv.org/abs/2408.04226
tags:
- problem
- activity
- problems
- level
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MathFish, a novel approach to evaluating
  language models'' (LMs) mathematical reasoning by grounding them in educational
  curricula. The authors create two datasets: Achieve the Core (ATC), containing 385
  fine-grained descriptions of K-12 math skills and concepts, and MathFish, consisting
  of 9.9K math problems labeled with these standards.'
---

# Mathfish: Evaluating Language Model Math Reasoning via Grounding in Educational Curricula

## Quick Facts
- arXiv ID: 2408.04226
- Source URL: https://arxiv.org/abs/2408.04226
- Reference count: 40
- Models struggle with verifying and tagging math problems against educational standards

## Executive Summary
This paper introduces MathFish, a novel approach to evaluating language models' mathematical reasoning by grounding them in educational curricula. The authors create two datasets: Achieve the Core (ATC), containing 385 fine-grained descriptions of K-12 math skills and concepts, and MathFish, consisting of 9.9K math problems labeled with these standards. They develop two tasks to assess LMs' abilities: verifying whether a problem aligns with a given standard and tagging a problem with all aligned standards. The experiments show that LMs, including GPT-4, Mixtral-8x7B, and Llama-2-70B, struggle with these tasks, particularly when standards are conceptually similar.

## Method Summary
The authors create the MathFish dataset by extracting math problems from the Achieve the Core website and labeling them with 385 fine-grained K-12 math standards. They develop two evaluation tasks: verification (checking if a problem aligns with a specific standard) and tagging (selecting all standards a problem aligns with from a hierarchical tree). Three models (GPT-4, Mixtral-8x7B, Llama-2-70B) are tested using various prompt templates with zero/one/three-shot approaches. Human expert annotations serve as ground truth for validation.

## Key Results
- LMs achieve reasonable but not expert-level accuracy on both verification and tagging tasks
- Models struggle more with conceptually similar standards and higher grade-level problems
- LMs often overestimate alignment when generating problems, requiring careful scrutiny for curricular applications
- GSM8k only covers about a third of all K-12 standards, highlighting the need for broader evaluation datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MathFish evaluates LMs' mathematical reasoning by grounding them in educational curricula.
- **Mechanism:** By creating a dataset of 9.9K math problems labeled with 385 fine-grained K-12 math standards, the authors provide a granular framework to assess whether LMs can identify specific skills and concepts in math problems.
- **Core assumption:** The ability of LMs to correctly identify math standards in problems correlates with their mathematical reasoning abilities.
- **Evidence anchors:**
  - [abstract] "Our work presents a novel angle for evaluating language models' (LMs) mathematical abilities, by investigating whether they can discern skills and concepts enabled by math content."
  - [section] "Our work bridges this gap and examines a novel angle for evaluating LMs' mathematical understanding. We ask, how well can models identify specific skills and concepts that students learn or practice when completing math problems?"

### Mechanism 2
- **Claim:** The two task formats (verification and tagging) provide a comprehensive evaluation of LMs' understanding of math pedagogy, skills, and concepts.
- **Mechanism:** Verification asks if a problem aligns with a given standard, while tagging involves selecting all standards a problem aligns with. This allows for assessment of both fine-grained and broader understanding.
- **Core assumption:** LMs can accurately perform both verification and tagging tasks, demonstrating their understanding of math concepts and skills.
- **Evidence anchors:**
  - [abstract] "We develop two tasks for evaluating LMs' abilities to assess math problems: (1) verifying whether a problem aligns with a given standard, and (2) tagging a problem with all aligned standards."
  - [section] "Our experiments demonstrate that models achieve reasonable accuracy, but are not yet at expert-level performance across both task formats."

### Mechanism 3
- **Claim:** Applying the best-performing models to generated problems and GSM8k provides insights into LMs' overestimation of alignment and the relationship between problem-solving performance and grade levels of tagged standards.
- **Mechanism:** By evaluating generated problems and GSM8k with the best-performing models, the authors can identify overestimation of alignment and understand how problem-solving performance relates to the grade levels of tagged standards.
- **Core assumption:** The best-performing models can accurately assess the alignment of generated problems and GSM8k, providing insights into LM behavior.
- **Evidence anchors:**
  - [abstract] "We also show that LMs often generate problems that do not fully align with standards described in prompts, suggesting the need for careful scrutiny on use cases involving LMs for generating curricular materials."
  - [section] "We find that GSM8k only covers around a third of all K-12 standards, and that LMs struggle more to solve problems tagged with higher grade levels' standards."

## Foundational Learning

- **Concept:** Common Core State Standards (CCSS)
  - **Why needed here:** CCSS provides the fine-grained and comprehensive coverage of K-12 math skills/concepts used in MathFish and ATC datasets.
  - **Quick check question:** What are the four levels of the CCSS hierarchy used in MathFish?
    - Answer: Grade, Domain, Cluster, Standard

- **Concept:** Textual entailment and claim verification
  - **Why needed here:** These concepts are used as inspiration for the verification task format, where models assess if a problem aligns with a given standard.
  - **Quick check question:** What is the purpose of the verification task format in MathFish?
    - Answer: To assess whether a problem aligns with a given standard

- **Concept:** Embodied instruction-following
  - **Why needed here:** This concept is related to the tagging task format, where models navigate a hierarchical decision tree to select relevant standards.
  - **Quick check question:** How does the tagging task format in MathFish differ from the verification task format?
    - Answer: Tagging involves selecting all standards a problem aligns with, while verification asks if a problem aligns with a given standard

## Architecture Onboarding

- **Component map:** ATC dataset (385 standards) -> MathFish dataset (9.9K problems) -> Model wrapper (GPT-4, Mixtral-8x7B, Llama-2-70B) -> Verification/Tagging tasks -> Human annotations

- **Critical path:** 1. Load MathFish and ATC datasets 2. Initialize model wrapper with desired model 3. Run verification or tagging task on a problem 4. Compare model output to ground truth annotations 5. Analyze model performance and identify areas for improvement

- **Design tradeoffs:** Text-only vs. multimodal evaluation (focus on text-only for simplicity), human annotations vs. automated evaluation (rely on human annotations for accuracy), inclusion of all problems vs. a subset for evaluation (use a subset for efficiency)

- **Failure signatures:** Low accuracy on verification or tagging tasks, overestimation of alignment in generated problems, poor correlation between problem-solving performance and grade levels of tagged standards

- **First 3 experiments:** 1. Run verification task on a sample of MathFish problems using GPT-4 2. Run tagging task on a sample of MathFish problems using Mixtral-8x7B 3. Evaluate generated problems using the best-performing model from experiments 1 and 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do language models perform on verification tasks when given problem-standard pairs that are conceptually distant versus conceptually similar?
- Basis in paper: [explicit] The paper states that models' accuracy decreases as negative examples become more conceptually similar to positive ones during verification tasks.
- Why unresolved: While the paper shows this trend, it doesn't quantify the exact performance drop or explore the threshold at which similarity becomes too challenging for models.
- What evidence would resolve it: Conducting a systematic study with varying degrees of similarity between positive and negative standards, measuring exact performance drops, and identifying a similarity threshold beyond which models struggle significantly.

### Open Question 2
- Question: Can language models accurately identify misalignment between math problems and standards when the misalignment is subtle or nuanced?
- Basis in paper: [inferred] The paper discusses how models struggle with false positives and false negatives related to subtle differences in mathematical language, suggesting difficulty with nuanced misalignment.
- Why unresolved: The paper identifies this challenge but doesn't explore specific strategies or model architectures that could improve detection of subtle misalignments.
- What evidence would resolve it: Developing and testing new prompting strategies, model architectures, or fine-tuning approaches specifically designed to capture subtle nuances in math problem-standard relationships.

### Open Question 3
- Question: How do language models perform on the tagging task when navigating the hierarchical decision tree independently versus with oracle assistance?
- Basis in paper: [explicit] The paper compares assisted traversal (with oracle assistance) to self-guided traversal in the tagging task, finding significantly lower performance in the self-guided setting.
- Why unresolved: While the paper demonstrates the performance gap, it doesn't explore why models struggle with independent navigation or what specific aspects of the hierarchy pose the greatest challenges.
- What evidence would resolve it: Analyzing model decisions at each level of the hierarchy during self-guided traversal, identifying common failure points, and testing interventions to improve independent navigation skills.

## Limitations

- Limited corpus evidence: Related work search found only 25 papers with weak citation connections
- Human annotation dependency: Reliance on expert annotations limits scalability and introduces potential subjectivity
- Framework specificity: Focus on Common Core standards may limit applicability to other educational frameworks

## Confidence

- **High confidence**: The core methodology of grounding LM math evaluation in educational standards is well-founded and the dataset creation process is clearly described.
- **Medium confidence**: The correlation findings between problem-solving performance and grade-level standards are reasonable but may be influenced by the specific models tested.
- **Low confidence**: The corpus evidence for related work is weak (average citations = 0.0), making it difficult to assess how MathFish fits into the broader landscape.

## Next Checks

1. **Cross-framework validation**: Test MathFish methodology with non-Common Core standards (e.g., international curricula) to assess generalizability beyond the specific educational framework used.

2. **Model architecture sensitivity**: Systematically vary prompt templates and few-shot exemplars to determine if performance improvements are due to model capabilities versus prompt engineering.

3. **Real-world application stress test**: Apply MathFish-validated models to actual classroom-generated math problems to assess practical utility and identify deployment challenges not captured in curated datasets.