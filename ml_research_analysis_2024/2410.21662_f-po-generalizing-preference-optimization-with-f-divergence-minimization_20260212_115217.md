---
ver: rpa2
title: '$f$-PO: Generalizing Preference Optimization with $f$-divergence Minimization'
arxiv_id: '2410.21662'
source_url: https://arxiv.org/abs/2410.21662
tags:
- preference
- optimization
- divergence
- which
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces $f$-divergence Preference Optimization ($f$-PO),
  a unified framework that generalizes existing preference optimization methods through
  $f$-divergence minimization between the optimized policy and optimal policy. The
  approach recovers algorithms like DPO and EXO as special cases while offering new
  variants through different choices of $f$-divergences.
---

# $f$-PO: Generalizing Preference Optimization with $f$-divergence Minimization

## Quick Facts
- arXiv ID: 2410.21662
- Source URL: https://arxiv.org/abs/2410.21662
- Authors: Jiaqi Han; Mingjian Jiang; Yuxuan Song; Stefano Ermon; Minkai Xu
- Reference count: 11
- One-line primary result: f-PO framework generalizes preference optimization methods through $f$-divergence minimization, achieving state-of-the-art performance on language model alignment benchmarks

## Executive Summary
This paper introduces $f$-divergence Preference Optimization ($f$-PO), a unified framework that generalizes existing preference optimization methods through $f$-divergence minimization between the optimized policy and optimal policy. The approach recovers algorithms like DPO and EXO as special cases while offering new variants through different choices of $f$-divergences. Extensive experiments on language models demonstrate that $f$-PO achieves superior performance on benchmarks like AlpacaEval 2, Arena-Hard, and MT-Bench, with the $\alpha$-divergence variant achieving up to 45.3% length-controlled win rate on AlpacaEval 2 when fine-tuning Llama3-8B-instruct.

## Method Summary
$f$-PO formulates preference optimization as minimizing $f$-divergences between the optimized policy and the optimal policy derived from pairwise preference data. The framework uses the Bradley-Terry model to represent pairwise preferences and applies Monte Carlo estimation to handle high-dimensional integrals in the objective. Different choices of $f$-divergences (KL, reverse KL, $\alpha$-divergence) recover existing algorithms while enabling new variants. The method incorporates practical approximations like reference-free rewards and length normalization for improved training efficiency, and uses the SimPO-style parameterization for better empirical performance.

## Key Results
- Achieves 45.3% length-controlled win rate on AlpacaEval 2 when fine-tuning Llama3-8B-instruct
- Outperforms existing methods like DPO and EXO across multiple benchmarks including Arena-Hard and MT-Bench
- Ablation studies show $\alpha$-divergence with $\alpha=0.1$ provides optimal performance trade-offs
- Demonstrates superior performance on both Bradley-Terry model-based and reward model-based preference datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: f-PO generalizes existing preference optimization methods by minimizing f-divergences between the optimized policy and optimal policy
- Mechanism: The framework formulates preference optimization as a distribution-matching problem where different choices of f-divergences (like KL, reverse KL, α-divergence) recover existing algorithms while enabling new variants
- Core assumption: The Bradley-Terry model provides an accurate representation of pairwise preferences, and the f-divergence family captures meaningful distinctions between distributions
- Evidence anchors:
  - [abstract] "f-PO minimizes f-divergences between the optimized policy and the optimal policy, encompassing a broad family of alignment methods using various divergences"
  - [section 3.3] "Different choices of f-divergence can cover a wide class of popular divergences, including forward and reverse Kullback-Leibler divergence, Jensen-Shannon(JS)divergence, and Jeffrey's divergence, etc."
  - [corpus] Weak - only 1 citation for "divergence minimization" in related work, suggesting limited direct evidence for this specific framing
- Break condition: The framework breaks when the Bradley-Terry model assumptions fail or when the f-divergence choice doesn't capture meaningful differences between distributions

### Mechanism 2
- Claim: Different f-divergences provide distinct trade-offs between regularization and performance in offline preference optimization
- Mechanism: The choice of f-divergence affects how the algorithm handles tail behavior and mode-seeking vs mode-covering properties, with α-divergence allowing interpolation between these behaviors
- Core assumption: The f-divergence family provides meaningful control over the optimization landscape and regularization effects
- Evidence anchors:
  - [abstract] "Ablation studies reveal that different f-divergences offer trade-offs between regularization and performance in offline preference optimization"
  - [section 4.3] "α-divergence covers a family of divergences with varying α values. As |α| increases, the divergence becomes more sensitive to differences in the tails of the distributions"
  - [corpus] Weak - limited direct evidence for how different f-divergences specifically affect LLM alignment performance
- Break condition: The mechanism breaks when the empirical differences between f-divergences are negligible or when the α parameter doesn't meaningfully control the trade-offs

### Mechanism 3
- Claim: f-PO can incorporate practical approximations like reference-free rewards and length normalization without losing theoretical grounding
- Mechanism: The framework's generality allows integration of empirical improvements like SimPO-style parameterizations while maintaining the theoretical foundation of f-divergence minimization
- Core assumption: Practical approximations can be viewed as special cases or modifications of the general f-divergence framework
- Evidence anchors:
  - [section 4.4] "empirically we can take this approximation into our method by substituting the gθ odds in Eq. (13) with the ĝθ one above"
  - [section 5.2] "we also utilize the SimPO-style parameterization of f-PO in Eq. (15) due to its superior performance"
  - [corpus] Moderate - related work on practical approximations exists but specific integration with f-divergence framework needs more evidence
- Break condition: The mechanism breaks when practical approximations significantly deviate from the theoretical assumptions of f-divergence minimization

## Foundational Learning

- Concept: f-divergence family (Ali-Silvey distances)
  - Why needed here: Provides the mathematical foundation for unifying various preference optimization methods under a single theoretical framework
  - Quick check question: What properties must a function f satisfy to define a valid f-divergence?

- Concept: Bradley-Terry model for pairwise comparisons
  - Why needed here: Enables the derivation of preference optimization objectives from ranked pair data without requiring explicit reward labeling
  - Quick check question: How does the Bradley-Terry model relate the probability of preferring one option over another to the underlying utility difference?

- Concept: Monte Carlo estimation for high-dimensional integrals
  - Why needed here: Makes the f-divergence objectives tractable by approximating partition functions through sampling from preference datasets
  - Quick check question: Why does the f-PO objective become an unbiased estimator of the true f-divergence as the number of samples approaches infinity?

## Architecture Onboarding

- Component map: Pre-trained model -> Preference dataset -> f-divergence selection -> Objective formulation -> Optimization -> Evaluation
- Critical path: Data → f-divergence selection → objective formulation → optimization → evaluation
- Design tradeoffs: Different f-divergences offer varying regularization effects and computational complexity; practical approximations improve efficiency but may deviate from theoretical guarantees
- Failure signatures: Poor performance on benchmarks, instability during training, or failure to generalize across different preference datasets
- First 3 experiments:
  1. Implement f-PO with reverse KL divergence and verify it recovers DPO behavior on a small preference dataset
  2. Compare α-divergence with different α values on the same dataset to observe trade-off effects
  3. Apply the reference-free approximation and measure training efficiency improvements while monitoring performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of α in α-divergence affect the mode-seeking vs. mode-covering behavior in practice?
- Basis in paper: [explicit] The paper states "As|α|increases, the divergence becomes more sensitive to differences in the tails of the distributions" and shows experimental results in Figure 2
- Why unresolved: While the paper shows empirical results for α=0.1 being optimal, it doesn't provide theoretical guidance on how to select α for different tasks or explain the mechanism behind the optimal choice
- What evidence would resolve it: A theoretical analysis explaining the relationship between α values and the resulting policy behavior, plus experimental validation across diverse task types

### Open Question 2
- Question: What are the computational trade-offs between different f-divergence choices beyond the performance metrics reported?
- Basis in paper: [inferred] The paper compares multiple f-divergences (Table 1) and shows performance differences (Figure 3), but doesn't discuss computational complexity or memory requirements
- Why unresolved: The theoretical framework suggests many possible f-divergences, but the paper only empirically tests a subset without analyzing the practical computational implications of each choice
- What evidence would resolve it: Detailed computational complexity analysis and empirical timing/memory benchmarks for different f-divergence implementations

### Open Question 3
- Question: How does f-PO perform with non-pairwise preference data (e.g., rankings or ratings)?
- Basis in paper: [explicit] The paper explicitly states it focuses on "common pair-wise preference data" and mentions that "numerous methodspropose different training objectives to emphasize various behaviors" for other data types
- Why unresolved: The current framework is built around pairwise comparisons, but real-world preference datasets often include ranked lists or numerical ratings, and the paper doesn't explore these extensions
- What evidence would resolve it: Experiments showing f-PO adaptation to ranked lists or numerical ratings, plus theoretical analysis of how the f-divergence framework extends to these cases

## Limitations
- Experimental validation relies heavily on benchmark performance rather than theoretical guarantees, with limited ablation studies on fundamental properties of different f-divergences
- The framework assumes the Bradley-Terry model accurately captures pairwise preferences, which may not hold for all preference datasets
- Integration of practical approximations lacks rigorous theoretical analysis of how modifications affect fundamental properties of f-divergence minimization

## Confidence
- **High Confidence**: The theoretical framework of f-PO and its ability to recover existing methods (DPO, EXO) as special cases is well-established mathematically
- **Medium Confidence**: The empirical performance improvements on benchmarks are demonstrated but may be sensitive to hyperparameter choices and implementation details not fully specified in the paper
- **Low Confidence**: The claims about specific trade-offs between different f-divergences and their effects on regularization are based on limited ablation studies without comprehensive theoretical analysis

## Next Checks
1. Conduct systematic comparison of different f-divergences' effects on optimization landscape by analyzing gradient properties and mode-seeking/covering behavior on synthetic preference datasets with known optimal policies
2. Evaluate f-PO's performance across multiple preference dataset types (Bradley-Terry vs reward model-based) and model architectures to assess generalizability beyond specific experimental setup
3. Perform comprehensive ablation studies on the α parameter in α-divergence and impact of different reference-free reward parameterizations to understand stability and sensitivity of performance to critical choices