---
ver: rpa2
title: 'Abstract Reward Processes: Leveraging State Abstraction for Consistent Off-Policy
  Evaluation'
arxiv_id: '2410.02172'
source_url: https://arxiv.org/abs/2410.02172
tags:
- state
- abstract
- states
- policy
- abstraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STAR, a framework for off-policy evaluation
  (OPE) that achieves lower mean squared prediction errors than existing methods.
  The core idea is to use state abstraction to distill complex problems into compact
  discrete models called abstract reward processes (ARPs), which are then estimated
  from off-policy data using importance weighting.
---

# Abstract Reward Processes: Leveraging State Abstraction for Consistent Off-Policy Evaluation

## Quick Facts
- arXiv ID: 2410.02172
- Source URL: https://arxiv.org/abs/2410.02172
- Reference count: 40
- This paper introduces STAR, a framework for off-policy evaluation (OPE) that achieves lower mean squared prediction errors than existing methods.

## Executive Summary
This paper introduces STAR, a framework for off-policy evaluation (OPE) that achieves lower mean squared prediction errors than existing methods. The core idea is to use state abstraction to distill complex problems into compact discrete models called abstract reward processes (ARPs), which are then estimated from off-policy data using importance weighting. The framework provides theoretical guarantees for consistent estimation and includes existing OPE methods as special cases. Empirical results on twelve cases across three domains (CartPole, ICU-Sepsis, Asterix) show that the best STAR estimator outperforms baselines in all cases, with the median STAR estimator surpassing baselines in seven out of twelve cases. The approach addresses the key challenge of combining consistency and low variance in OPE.

## Method Summary
The STAR framework uses state abstraction to create finite discrete models (abstract reward processes) that can be estimated from off-policy data using importance weighting. The approach maps complex MDPs to compact ARPs by applying a state abstraction function, then estimates ARP components (transition matrix, reward vector, initial state distribution) from off-policy trajectories using weighted maximum likelihood estimation. Importance weights reweight abstract state occurrences to reflect the evaluation policy distribution, while weight clipping mitigates variance when the abstraction function is c-th order Markov. The framework provides theoretical consistency guarantees and includes existing OPE methods as special cases.

## Key Results
- The best STAR estimator (across abstraction functions and clipping factors) outperformed baselines in all twelve cases tested
- The median STAR estimator surpassed baselines in seven out of twelve cases
- STAR achieved lower mean squared prediction errors than existing OPE methods across three domains (CartPole, ICU-Sepsis, Asterix)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: State abstraction eliminates model class mismatch by creating finite, tabular models that can perfectly represent any discrete distribution over abstract states.
- Mechanism: By mapping continuous or large state spaces to finite discrete abstract states, the resulting abstract reward process (ARP) becomes a finite Markov reward process that can be represented exactly with tabular models.
- Core assumption: The state abstraction function partitions the state space into disjoint sets of discrete abstract states, making the ARP finite and tabular representation possible.
- Evidence anchors:
  - [abstract]: "ARPs estimated from off-policy data are provably consistent (asymptotically correct)"
  - [section]: "Since Z is a finite set, i.e., the abstract states are discrete, the components of the ARP can be represented by matrices"
  - [corpus]: "Off-policy Evaluation with Deeply-abstracted States" - this related work suggests state abstraction is being actively explored for OPE, supporting the general approach
- Break condition: If the state abstraction function creates abstract states that are too coarse, critical information for policy evaluation may be lost, introducing bias.

### Mechanism 2
- Claim: Importance weighting enables consistent estimation of ARPs from off-policy data by reweighting abstract state occurrences to reflect the evaluation policy distribution.
- Mechanism: Importance weights (ρ0:t) are applied to abstract state occurrences in the off-policy dataset, updating the estimated distribution of abstract states to reflect those resulting from the evaluation policy.
- Core assumption: Assumption 2.1 holds - there exists ε > 0 such that (πb(s, a) < ε) implies (πe(s, a) = 0), ensuring sufficient overlap between behavior and evaluation policies.
- Evidence anchors:
  - [abstract]: "Predictions from ARPs estimated from off-policy data are provably consistent (asymptotically correct)"
  - [section]: "The weighted maximum likelihood estimate (MLE) of Rπeϕ obtained from the weighted off-policy data is denoted by bRπb→πe n,ϕ"
  - [corpus]: Missing concrete evidence - this is a novel theoretical contribution not yet validated in related work
- Break condition: If the importance weights have extremely high variance (e.g., when πb(s,a) approaches zero while πe(s,a) is non-zero), the weighted estimates become unstable.

### Mechanism 3
- Claim: Weight clipping mitigates variance while preserving consistency when the state abstraction function is c-th order Markov.
- Mechanism: Clipping importance weights to the c most recent terms implies that the c most recent abstract states are sufficient to determine current abstract state transitions, reducing variance while maintaining asymptotic correctness.
- Core assumption: The abstraction function ϕ is c-th order Markov, meaning Pr(ϕ(St+1)|ϕ(St),...,ϕ(S(t-c+1)+); π) = Pr(ϕ(St+1)|ϕ(St),...,ϕ(S0); π) for both policies.
- Evidence anchors:
  - [abstract]: "The integration of importance sampling into model estimation permits a favorable interpretation of weight clipping for mitigating variance"
  - [section]: "c-clipping does not introduce asymptotic bias when the previous c abstract states form a sufficient statistic for predicting the current abstract state transition distribution"
  - [corpus]: Missing concrete evidence - this theoretical result appears novel and not yet validated in related work
- Break condition: If the abstraction function is not c-th order Markov, weight clipping introduces asymptotic bias that cannot be eliminated.

## Foundational Learning

- Concept: Importance sampling for policy evaluation
  - Why needed here: The framework uses importance weighting to estimate ARPs from off-policy data, requiring understanding of how importance sampling enables unbiased estimation under distribution shift
  - Quick check question: What condition must hold between πb and πe for importance sampling to provide unbiased estimates?

- Concept: State abstraction and its properties
  - Why needed here: The framework relies on state abstraction to create finite ARPs, requiring understanding of how abstraction preserves performance information while reducing state space complexity
  - Quick check question: Why does Theorem 3.1 guarantee that J(π; Rπϕ) = J(π; M) even though state information is abstracted away?

- Concept: Markov reward processes and their properties
  - Why needed here: The framework constructs ARPs as special cases of Markov reward processes, requiring understanding of MRP components and how expected returns are computed
  - Quick check question: How does the expected return J(π; Rπϕ) = (I-Pπϕ)-1Rπϕηϕ relate to the performance of policy π?

## Architecture Onboarding

- Component map: State abstraction function ϕ -> Abstract reward process (ARP) -> Importance weighting with ρ0:t weights -> Weighted maximum likelihood estimation -> Expected return J(πe; bRπb→πe ϕ,c)

- Critical path:
  1. Apply state abstraction to off-policy dataset D(πb)n
  2. Compute importance weights ρ0:t for each trajectory
  3. Perform weighted maximum likelihood estimation of ARP components
  4. Compute expected return J(πe; bRπb→πe ϕ,c) using closed-form solution

- Design tradeoffs:
  - Abstraction granularity vs. bias: Finer abstractions preserve more information but increase computational complexity
  - Weight clipping factor c vs. variance: Larger c reduces bias but increases variance; smaller c reduces variance but may introduce bias
  - Model-based vs. importance sampling approaches: Framework combines benefits of both but inherits challenges from each

- Failure signatures:
  - Extremely high importance weights indicate poor overlap between πb and πe
  - ARP components with zero variance suggest insufficient data coverage of abstract states
  - Expected return estimates that diverge from baselines suggest implementation errors in weighted estimation

- First 3 experiments:
  1. Verify on-policy consistency: Estimate ARP from on-policy data and compare J(π; bRπ n,ϕ) to true J(π; M)
  2. Test weight clipping variance reduction: Compare MSE of J(πe; bRπb→πe ϕ,c) across different c values
  3. Validate abstraction performance preservation: Test J(π; Rπϕ) = J(π; M) for various abstraction functions on simple MDPs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal methods for discovering abstraction functions that are c-th order Markov with small values of c for off-policy evaluation?
- Basis in paper: [explicit] The paper states "An automated approach to discovering such abstraction functions, however, remains elusive" and "we expect the following factors to play an important role in the search for good abstraction functions"
- Why unresolved: The paper identifies the importance of finding c-th order Markov abstractions but does not provide concrete methods for discovering them automatically. The authors acknowledge this as an open research area.
- What evidence would resolve it: Empirical demonstrations of automated methods that can discover abstraction functions satisfying c-th order Markov properties for various domains, with quantitative comparisons showing their effectiveness in OPE tasks.

### Open Question 2
- Question: How can we develop principled methods for selecting well-performing configurations of abstraction functions and weight clipping factors in practice?
- Basis in paper: [explicit] The paper states "a principled method for selecting well-performing configurations of the abstraction function and the weight clipping factor remain elusive" and "combining this work with regression IS would be a practical extension"
- Why unresolved: While the paper shows that varying configurations of (ϕ, c) can produce different bias-variance tradeoffs, it does not provide a principled method for selecting the best configuration in practice, especially without access to ground truth performance.
- What evidence would resolve it: Development and validation of principled selection criteria or algorithms that can identify optimal (ϕ, c) configurations from off-policy data alone, with demonstrated improvements in OPE accuracy across multiple domains.

### Open Question 3
- Question: What are the theoretical properties of state abstraction functions beyond c-th order Markovness that could further improve OPE performance?
- Basis in paper: [inferred] The paper focuses on c-th order Markovness as a sufficient condition for consistent OPE, but does not explore other potential properties of abstraction functions that could enhance performance.
- Why unresolved: The paper establishes one important property (c-th order Markovness) but does not systematically explore what other theoretical properties of abstraction functions might be beneficial for OPE.
- What evidence would resolve it: Theoretical analysis and empirical validation of additional properties of abstraction functions (beyond c-th order Markovness) that could improve OPE performance, along with methods to verify these properties in practice.

## Limitations

- The framework's performance depends heavily on the choice of state abstraction function, but systematic guidance for selecting appropriate abstractions is not provided
- Empirical validation is limited to twelve cases across three domains, which may not capture the full range of real-world complexity
- The approach requires sufficient overlap between behavior and evaluation policies (Assumption 2.1), which may not hold in practice

## Confidence

- **High confidence**: The theoretical consistency guarantees for ARPs (Theorems 3.1 and 3.2) are well-established within the MDP framework, assuming the stated assumptions hold.
- **Medium confidence**: The empirical results showing STAR's superiority over baselines are promising but based on a limited number of domains and scenarios.
- **Medium confidence**: The weight clipping mechanism for variance reduction is theoretically sound under the c-th order Markov assumption, but practical benefits depend on the specific abstraction function used.

## Next Checks

1. **Robustness to abstraction quality**: Systematically vary the granularity of state abstraction (|Z| values) and measure how MSE changes, particularly testing the break condition where coarse abstractions introduce bias.
2. **Importance weight distribution analysis**: Characterize the variance of importance weights across different domain configurations and verify that weight clipping effectively reduces this variance without introducing asymptotic bias.
3. **Cross-domain Generalization**: Apply STAR to additional domains with different characteristics (e.g., continuous control tasks, recommendation systems) to test whether the framework's performance advantages generalize beyond the three domains studied.