---
ver: rpa2
title: Text or Image? What is More Important in Cross-Domain Generalization Capabilities
  of Hate Meme Detection Models?
arxiv_id: '2402.04967'
source_url: https://arxiv.org/abs/2402.04967
tags:
- meme
- hate
- text
- image
- memes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates cross-domain generalization in multimodal
  hate meme detection, focusing on whether the textual or visual component drives
  model performance. Through experiments on three hate meme datasets (HARMEME, MAMI,
  FB), the authors find that hate text classifiers achieve similar performance to
  multimodal models in zero-shot settings, suggesting that text is the primary driver
  of generalization.
---

# Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?

## Quick Facts
- arXiv ID: 2402.04967
- Source URL: https://arxiv.org/abs/2402.04967
- Reference count: 17
- Key outcome: Text is the primary driver of cross-domain generalization in hate meme detection, with text-only classifiers achieving similar performance to multimodal models in zero-shot settings.

## Executive Summary
This paper investigates whether text or image is more important for cross-domain generalization in hate meme detection. Through experiments on three hate meme datasets (HARMEME, MAMI, FB), the authors find that hate text classifiers achieve similar performance to multimodal models when evaluated in zero-shot settings. The study reveals that text contributes 83% to model predictions according to Shapley value analysis, and adding image captions to text classifiers improves performance by 0.05 F1 while adding captions to multimodal models reduces performance by 0.02 F1. A confounder dataset shows models are more sensitive to text changes (ΔF1=0.18) than image changes, indicating that text is the dominant factor in cross-domain generalization for hate meme detection.

## Method Summary
The authors evaluate cross-domain generalization capabilities of multimodal hate meme detection models by training and testing on three hate meme datasets (HARMEME, MAMI, FB). They fine-tune VisualBert, Uniter, and Rob+Resnet multimodal models on each dataset separately and train text-only classifiers (BERT, SVM, Perspective API) on nine hate speech datasets. The study also investigates the impact of image captions by generating them using ClipCap and BLIP, then evaluating models with and without captions. Shapley value analysis quantifies modality contribution, and a confounder dataset tests model sensitivity to text vs image changes.

## Key Results
- Hate text classifiers achieve similar performance to multimodal models in zero-shot settings (average F1 of .59 vs .52-.55)
- Adding captions to text classifiers improves performance by 0.05 F1, but adding captions to multimodal models reduces performance by 0.02 F1
- Shapley value analysis reveals text contributes 83% to predictions, dropping to 52% when captions are included
- Confounder dataset shows models are more sensitive to text changes (ΔF1=0.18) than image changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text is the primary driver of cross-domain generalization in hate meme detection.
- Mechanism: Hate text classifiers trained on diverse datasets generalize well to unseen domains, while multimodal models relying on images show poor cross-domain performance.
- Core assumption: The textual component carries the core hateful meaning while images provide contextual support that is domain-specific.
- Evidence anchors:
  - [abstract] "hate-text classifiers achieve similar performance to multimodal models in zero-shot settings, suggesting that text is the primary driver of generalization"
  - [section 6.2] "BERT model performance is superior to the rest of the two with an average F1 score of .51 followed by Perspective API (F1 of .59)" and "multimodal hateful meme classifiers (average F1 of .52 for VisualBert and .55 for Uniter and .47 for Rob+Resnet)"
- Break condition: If domain transfer requires understanding culturally specific visual contexts that cannot be captured in text alone.

### Mechanism 2
- Claim: Adding captions to multimodal models actually hurts performance.
- Mechanism: Captions introduce noise that misdirects attention to low-level image-text alignment rather than abstract meaning, causing models to overfit to specific training domains.
- Core assumption: Current multimodal models are optimized for concrete-level alignment rather than understanding metaphorical or abstract connections between text and image.
- Evidence anchors:
  - [abstract] "adding captions to multimodal models reduces performance by 0.02 F1"
  - [section 6.3] "we find small performance drops (average ∆F1 of 0.02) in both in-domain as well as out-of-domain settings regardless of the presence of captions in the test sets"
- Break condition: If captions are generated using models that understand abstract relationships rather than just describing visual content.

### Mechanism 3
- Claim: Multimodal models are more sensitive to text confounders than image confounders.
- Mechanism: When text content is changed while keeping images the same, models show significant performance drops, but performance remains stable when images change while text stays the same.
- Core assumption: The model's attention mechanism prioritizes text over image features when making hate predictions.
- Evidence anchors:
  - [abstract] "A confounder dataset shows models are more sensitive to text changes (ΔF1=0.18) than image changes"
  - [section 6.5] "Overall, the performance on T is notably higher than that on the I across all variants of models"
- Break condition: If image-based confounders are designed to target the specific visual features that models actually use for hate detection.

## Foundational Learning

- Concept: Zero-shot learning and domain generalization
  - Why needed here: The paper evaluates models' ability to generalize to domains not seen during training
  - Quick check question: What is the key difference between in-domain and out-of-domain evaluation, and why is this distinction critical for hate meme detection?

- Concept: Shapley value explanation method
  - Why needed here: Used to quantify the contribution of text vs image modalities to model predictions
  - Quick check question: How does the Shapley value method handle the interaction between text and image features, and what assumption does it make about feature independence?

- Concept: Multimodal fusion strategies
  - Why needed here: The paper compares early fusion (VisualBert, Uniter) vs late fusion (Rob+Resnet) approaches
  - Quick check question: What are the trade-offs between early and late fusion in terms of feature interaction and computational efficiency?

## Architecture Onboarding

- Component map: Text extraction -> Image preprocessing -> Text encoder (BERT-based) -> Image encoder (ResNet/CLIP-based) -> Modality fusion -> Classification head -> Shapley value explainer
- Critical path: Image preprocessing → Text extraction → Feature encoding → Modality fusion → Classification → Shapley value computation
- Design tradeoffs: Text-only vs multimodal approaches (simplicity vs potential context), Early vs late fusion (feature interaction vs modularity), Caption inclusion (contextual information vs noise)
- Failure signatures: Performance drops in cross-domain settings, Shapley values showing high text contribution, Confounder experiments showing text sensitivity
- First 3 experiments:
  1. Train text-only BERT classifier on hate speech datasets and evaluate on meme text only
  2. Train multimodal model with and without captions, compare cross-domain performance
  3. Create and evaluate on confounder dataset to test modality sensitivity

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- The study assumes text is the dominant factor without exploring whether certain types of hate memes might require visual context for accurate detection
- The confounder experiments show text sensitivity but don't fully explore the types of visual information that might be crucial for hate detection
- The datasets used may have inherent biases that affect cross-domain generalization results

## Confidence

- **High Confidence**: Text-only classifiers achieve comparable performance to multimodal models in zero-shot settings (supported by quantitative F1 scores across multiple datasets)
- **Medium Confidence**: Shapley value analysis showing 83% text contribution (methodologically sound but may not capture complex feature interactions)
- **Medium Confidence**: Caption addition hurting multimodal model performance (consistent results but mechanism needs further exploration)

## Next Checks
1. Conduct ablation studies testing different types of visual information (e.g., explicit vs. implicit hate cues) to determine if specific visual contexts are critical for hate detection
2. Evaluate model performance on memes where text and image meanings are in direct opposition to test whether multimodal models can handle such cases better than text-only models
3. Test whether fine-tuning multimodal models with domain adaptation techniques can overcome the observed cross-domain generalization limitations