---
ver: rpa2
title: 'Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven
  Molecule Generation'
arxiv_id: '2412.14642'
source_url: https://arxiv.org/abs/2412.14642
tags:
- molecule
- llms
- molecular
- molecules
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces TOMG-Bench, the first benchmark to evaluate
  open-domain molecule generation by large language models (LLMs). Unlike previous
  targeted molecule generation tasks, TOMG-Bench measures LLMs' ability to generate
  diverse, valid molecular structures based on natural language prompts without requiring
  exact matches.
---

# Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation

## Quick Facts
- arXiv ID: 2412.14642
- Source URL: https://arxiv.org/abs/2412.14642
- Authors: Jiatong Li; Junxian Li; Weida Wang; Yunqing Liu; Changmeng Zheng; Dongzhan Zhou; Xiao-yong Wei; Qing Li
- Reference count: 17
- The best proprietary model (Claude-3.5) achieves only 35.92% weighted average accuracy on TOMG-Bench

## Executive Summary
This paper introduces TOMG-Bench, the first benchmark for evaluating open-domain molecule generation by large language models (LLMs). Unlike previous targeted molecule generation tasks that require exact matches, TOMG-Bench measures LLMs' ability to generate diverse, valid molecular structures based on natural language prompts. The benchmark consists of three tasks—molecule editing, optimization, and customized generation—each with three subtasks. An automated evaluation system assesses both accuracy and quality of generated molecules. The study benchmarks 25 LLMs, revealing significant room for improvement, with the best proprietary model achieving only 35.92% weighted average accuracy.

## Method Summary
The paper introduces TOMG-Bench, a benchmark for evaluating open-domain molecule generation using LLMs. The benchmark consists of three tasks (MolEdit, MolOpt, MolCustom) with 5,000 samples each, generated from the Zinc-250K database. LLMs are evaluated using natural language prompts, and outputs are validated using RDKit for chemical validity and similarity. The authors also introduce OpenMolIns, an instruction-tuning dataset with five data levels, which enables Llama-3.1-8B to outperform GPT-3.5-turbo by 46.5% on TOMG-Bench. Fine-tuning uses LoRA parameters (r=64, α=128, dropout=0.1).

## Key Results
- TOMG-Bench reveals significant limitations in current LLMs for open-domain molecule generation, with top models achieving only 35.92% accuracy
- OpenMolIns instruction-tuning dataset enables Llama-3.1-8B to surpass GPT-3.5-turbo by 46.5% on TOMG-Bench
- Larger LLMs consistently outperform smaller models, demonstrating the importance of model capacity for complex molecular reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OpenMolIns enables LLMs to surpass GPT-3.5-turbo by bridging the gap between molecular structures and natural language instructions through large-scale instruction tuning.
- Mechanism: The instruction-tuning dataset OpenMolIns provides diverse examples across five data levels, allowing LLMs to generalize from targeted molecule generation to open-domain molecule generation by learning task-specific patterns and structural representations.
- Core assumption: The molecular structure knowledge captured during instruction tuning on OpenMolIns can be effectively transferred to the TOMG-Bench tasks.
- Evidence anchors:
  - [abstract] "We also introduce OpenMolIns, a large-scale instruction tuning dataset that enables Llama-3.1-8B to surpass the most powerful LLMs like GPT-4o and Claude-3.5 on S^2-Bench."
  - [section] "OpenMolIns is structured across five distinct data levels (i.e., light, small, medium, large, and extra-large) to tailor different training purposes."
  - [corpus] "Weak evidence - the corpus shows related work on molecular discovery and LLMs but lacks specific evidence about instruction tuning effectiveness for open-domain generation."
- Break condition: If the instruction-tuning examples in OpenMolIns do not adequately represent the diversity of molecular structures and operations required by TOMG-Bench, the transfer learning effect will be minimal.

### Mechanism 2
- Claim: TOMG-Bench's one-to-many evaluation framework reveals the creative potential of LLMs by accepting multiple valid molecular solutions rather than requiring exact matches.
- Mechanism: By using chemical toolboxes like RDKit to validate whether generated molecules meet specified criteria, the benchmark shifts focus from pattern recall to genuine molecular understanding and generation capabilities.
- Core assumption: Chemical toolboxes can reliably determine whether generated molecules satisfy the structural and property requirements specified in prompts.
- Evidence anchors:
  - [abstract] "S^2-Bench is specifically designed for one-to-many relationships, challenging LLMs to demonstrate genuine molecular understanding and generation capabilities."
  - [section] "Unlike this targeted generation task, in this paper, we propose a Text-based Open Molecule Generation task to enable LLMs to generate an exactly matched molecule rather than set a specific target."
  - [corpus] "Weak evidence - corpus contains related work on molecular generation but lacks specific evidence about one-to-many evaluation frameworks."
- Break condition: If RDKit validation fails to capture important molecular properties or allows structurally invalid molecules, the benchmark's effectiveness will be compromised.

### Mechanism 3
- Claim: Larger LLMs consistently outperform smaller models on TOMG-Bench due to their enhanced capacity for molecular structural reasoning and pattern recognition.
- Mechanism: The correlation between model size and performance indicates that complex molecular generation tasks require substantial parameter capacity to capture the nuanced relationships between textual descriptions and molecular structures.
- Core assumption: Model size directly correlates with the ability to perform complex reasoning about molecular structures and properties.
- Evidence anchors:
  - [section] "Across all the LLMs we benchmarked, a clear trend emerged: the more powerful the LLM is, the higher performance it can achieve on the TOMG-Bench."
  - [section] "Similarly, within the Llama-3 series, we could also observe that larger models tend to achieve superior results on the TOMG-Bench."
  - [corpus] "Weak evidence - corpus shows related work on molecular discovery but lacks specific evidence about model size effects on open-domain generation."
- Break condition: If smaller models with efficient architectures or specialized molecular knowledge can outperform larger general-purpose models, the size correlation may break down.

## Foundational Learning

- Concept: Simplified Molecular Input Line Entry System (SMILES)
  - Why needed here: TOMG-Bench uses SMILES strings as the molecular representation format, so understanding this notation is essential for working with the benchmark and interpreting results.
  - Quick check question: What does the symbol '#' represent in SMILES notation?

- Concept: Chemical validation using RDKit
  - Why needed here: The benchmark relies on RDKit to validate molecular structures and properties, making familiarity with this toolbox crucial for implementation and debugging.
  - Quick check question: How does RDKit verify the validity of a generated SMILES string?

- Concept: Instruction tuning methodology
  - Why needed here: OpenMolIns uses instruction tuning to adapt LLMs for molecular generation tasks, requiring understanding of how to construct effective training examples and evaluate transfer learning.
  - Quick check question: What distinguishes instruction tuning from standard fine-tuning in LLM training?

## Architecture Onboarding

- Component map: Dataset construction pipeline (Zinc-250K sampling, RDKit processing) -> Prompt template system (MolEdit, MolOpt, MolCustom templates) -> Evaluation framework (accuracy, novelty, similarity metrics) -> Model benchmarking infrastructure (proprietary and open-source model support) -> OpenMolIns instruction tuning dataset generator

- Critical path:
  1. Sample molecules from Zinc-250K database
  2. Generate prompt templates with RDKit-calculated properties
  3. Run LLMs on generated prompts
  4. Validate outputs using RDKit
  5. Calculate accuracy, novelty, and similarity metrics
  6. Rank model performance using weighted average accuracy

- Design tradeoffs:
  - Using Zinc-250K instead of larger databases for faster metric calculation vs. potentially less diverse test samples
  - Implementing one-to-many evaluation vs. exact match evaluation for more realistic assessment
  - Creating five data levels in OpenMolIns for scalability analysis vs. increased dataset management complexity

- Failure signatures:
  - Low novelty scores indicating over-reliance on training data patterns
  - High similarity scores with poor accuracy suggesting inability to modify molecules appropriately
  - Inconsistent performance across data levels suggesting data scaling issues

- First 3 experiments:
  1. Run a small subset of TOMG-Bench prompts through GPT-4o to establish baseline performance metrics
  2. Test RDKit validation on randomly generated SMILES strings to verify the evaluation pipeline
  3. Generate and validate a small OpenMolIns dataset to confirm instruction tuning format correctness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does increasing prompt diversity beyond the current templates affect LLM performance on TOMG-Bench?
- Basis in paper: [explicit] The paper notes "we adopt several different prompt templates and randomly choose from them" but "we still find the number of prompt templates is not enough to satisfy the prompt diversity."
- Why unresolved: The study used a limited set of prompt templates without systematically varying their diversity to measure impact on model performance.
- What evidence would resolve it: Experiments comparing performance across datasets with systematically varied prompt template diversity, measuring effects on accuracy, novelty, and validity metrics.

### Open Question 2
- Question: What is the relationship between training data scale and model performance for different molecular property prediction tasks?
- Basis in paper: [explicit] The authors observe a "pronounced data scaling law" with Galactica-125M, noting "as the size of the corpus increases, the performance of LLMs also improves."
- Why unresolved: While the paper shows performance improves with scale, it doesn't quantify the relationship across different property types (LogP, MR, QED) or determine if there are diminishing returns.
- What evidence would resolve it: Systematic experiments varying training data sizes across all three property optimization tasks, measuring performance curves and identifying optimal data scales for each task.

### Open Question 3
- Question: How do different molecule representation formats (SMILES vs SELFIES) impact LLM performance on open-domain generation?
- Basis in paper: [explicit] The authors note that "BioT5 is designed to use SELFIES as input instead of SMILES" and had to convert formats for evaluation.
- Why unresolved: The paper evaluates models using SMILES format but doesn't compare performance across different molecular representation formats.
- What evidence would resolve it: Head-to-head comparisons of LLM performance on identical tasks using both SMILES and SELFIES representations, measuring accuracy, novelty, and validity across all benchmark tasks.

## Limitations

- Dataset representativeness uncertainty: TOMG-Bench relies on Zinc-250K database samples, which may not fully capture pharmaceutical and materials science chemical space
- Evaluation pipeline reliability: RDKit-based automated validation may miss chemically meaningful but structurally divergent solutions
- Instruction tuning scalability concerns: The minimum effective dataset size for robust performance remains unclear

## Confidence

**High confidence** in the benchmark design and evaluation methodology. The TOMG-Bench framework provides a well-structured approach to evaluating open-domain molecule generation with clear task definitions, standardized evaluation metrics, and automated validation procedures. The one-to-many evaluation paradigm represents a meaningful advancement over exact-match approaches.

**Medium confidence** in the OpenMolIns instruction-tuning results. While the dataset successfully improves Llama-3.1-8B performance by 46.5% over GPT-3.5-turbo, the specific prompt templates and fine-tuning procedures are not fully detailed in the paper. The reported performance gains are substantial but may be sensitive to implementation details not specified in the methodology.

**Low confidence** in cross-model performance comparisons. The benchmark includes both proprietary and open-source models, but API rate limits, version differences, and potential configuration variations across models introduce uncertainties in direct performance comparisons. The weighted average accuracy metric may not fully account for task-specific strengths and weaknesses of different model architectures.

## Next Checks

1. **Human evaluation validation**: Recruit chemistry experts to manually validate a random sample of 100 generated molecules from top-performing models on TOMG-Bench to verify that RDKit-based automated evaluation aligns with expert chemical knowledge and identify cases where automated validation may miss chemically meaningful solutions.

2. **Cross-database generalization test**: Evaluate the same set of prompts and models on molecules sampled from alternative chemical databases (e.g., ChEMBL, PubChem) to assess whether TOMG-Bench performance generalizes beyond the Zinc-250K dataset and identify any database-specific biases in the benchmark.

3. **Fine-tuning ablation study**: Conduct systematic ablation studies on OpenMolIns by training models with varying data levels (e.g., only light, only medium, only large, combined) and different training durations to determine the minimum effective dataset size and identify potential overfitting or underfitting patterns in the instruction-tuning process.