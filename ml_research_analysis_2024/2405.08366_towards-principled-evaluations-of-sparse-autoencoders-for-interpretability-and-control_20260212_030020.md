---
ver: rpa2
title: Towards Principled Evaluations of Sparse Autoencoders for Interpretability
  and Control
arxiv_id: '2405.08366'
source_url: https://arxiv.org/abs/2405.08366
tags:
- features
- feature
- name
- dictionaries
- saes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a principled framework for evaluating sparse
  autoencoders (SAEs) for interpretability by comparing them against supervised feature
  dictionaries on specific tasks. The authors demonstrate that supervised dictionaries,
  computed using attribute labels, achieve excellent approximation, control, and interpretability
  of model computations on the IOI task.
---

# Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control

## Quick Facts
- arXiv ID: 2405.08366
- Source URL: https://arxiv.org/abs/2405.08366
- Authors: Aleksandar Makelov; George Lange; Neel Nanda
- Reference count: 40
- Primary result: Proposes framework comparing SAEs to supervised feature dictionaries for interpretability evaluation

## Executive Summary
This paper introduces a principled framework for evaluating sparse autoencoders (SAEs) in mechanistic interpretability by benchmarking them against supervised feature dictionaries derived from attribute labels. The authors apply this framework to the IOI (Indirect Object Identification) task, demonstrating that supervised dictionaries achieve excellent approximation, control, and interpretability of model computations. Using these supervised features as ground truth, the paper evaluates task-specific and full-distribution SAEs across the same three axes, revealing that while SAEs capture interpretable features for IOI, they are less successful than supervised features in controlling the model. The work also identifies two key qualitative phenomena in SAE training: feature occlusion and feature over-splitting.

## Method Summary
The authors propose evaluating SAEs by comparing them against supervised feature dictionaries computed using attribute labels on specific tasks. The framework assesses features across three axes: approximation (how well features capture relevant information), control (ability to manipulate model behavior), and interpretability (human-understandable explanations). For the IOI task, the authors compute supervised feature dictionaries using known attribute labels and evaluate both SAEs and supervised features against these benchmarks. The evaluation includes quantitative metrics for feature reconstruction and qualitative analysis of feature properties, with specific attention to the phenomena of feature occlusion (where higher-magnitude features overshadow causally relevant ones) and feature over-splitting (where binary concepts split into less interpretable features).

## Key Results
- Supervised feature dictionaries achieve excellent approximation, control, and interpretability on the IOI task
- SAEs capture interpretable features for IOI but are less effective than supervised features at controlling model behavior
- Feature occlusion occurs when causally relevant concepts are overshadowed by higher-magnitude features during SAE training
- Feature over-splitting causes binary concepts to split into less interpretable features in SAE representations

## Why This Works (Mechanism)
The framework works by establishing supervised feature dictionaries as ground truth benchmarks, allowing for objective comparison of SAEs against known feature representations. By leveraging attribute labels, the supervised dictionaries capture the "true" features relevant to the task, providing a reference point for evaluating whether SAEs can discover these same features. The three-axis evaluation (approximation, control, interpretability) provides a comprehensive assessment that goes beyond simple reconstruction metrics to examine practical utility and human-understandability of discovered features.

## Foundational Learning
- **Sparse Autoencoders (SAEs)**: Neural networks trained to reconstruct inputs with sparsity constraints, used to discover interpretable features in model activations. Why needed: SAEs are the primary tool being evaluated for mechanistic interpretability. Quick check: Can explain how L1 regularization enforces sparsity in SAEs.
- **Feature Occlusion**: Phenomenon where higher-magnitude features overshadow causally relevant but lower-magnitude concepts during SAE training. Why needed: Understanding this limitation is crucial for interpreting SAE results and designing better architectures. Quick check: Can describe how feature magnitude affects learned representations in SAEs.
- **Feature Over-splitting**: Process where binary concepts split into multiple less interpretable features in SAE representations. Why needed: Recognizing this phenomenon helps understand the limitations of SAEs in capturing discrete concepts. Quick check: Can explain how continuous SAE features might fail to capture discrete concepts.

## Architecture Onboarding
**Component Map**: Input Activations -> SAE Encoder -> Sparse Features -> SAE Decoder -> Reconstructed Activations
**Critical Path**: Input representations flow through the SAE encoder, producing sparse features that are then decoded back to the original space for reconstruction.
**Design Tradeoffs**: Sparsity vs completeness (more sparse features may miss relevant information), feature magnitude vs causal relevance (high-magnitude features may overshadow important low-magnitude ones), discrete vs continuous representations (SAEs produce continuous features that may not capture discrete concepts).
**Failure Signatures**: Feature occlusion (relevant concepts overshadowed), feature over-splitting (binary concepts fragmented), poor reconstruction of task-relevant features, features that correlate with task attributes but cannot effectively control model behavior.
**First Experiments**: 1) Train SAEs with varying sparsity levels on IOI task to observe occlusion and over-splitting phenomena, 2) Compare SAE feature control effectiveness against supervised dictionary ablation studies, 3) Evaluate feature interpretability through human evaluation of SAE vs supervised features.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond the observations made about feature occlusion and over-splitting phenomena.

## Limitations
- Reliance on supervised feature dictionaries assumes these represent "true" features, which may not hold for complex tasks
- Framework evaluation primarily focused on single task (IOI), limiting generalizability to other domains
- Tension between sparsity and completeness in SAEs not fully resolved, with observed phenomena suggesting fundamental tradeoffs
- Quantitative metrics for control and interpretability not fully standardized across different feature sets

## Confidence
- High confidence in general framework design and potential utility for SAE evaluation
- Medium confidence in specific findings regarding feature occlusion and over-splitting
- Medium confidence in quantitative comparisons between SAEs and supervised dictionaries

## Next Checks
1. Test the framework on multiple tasks beyond IOI to assess generalizability and identify task-specific limitations
2. Conduct ablation studies varying sparsity hyperparameters to better understand the tradeoff between feature occlusion and over-splitting
3. Develop more robust quantitative metrics for comparing control effectiveness across different feature sets, potentially using interventional studies or causal analysis