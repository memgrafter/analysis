---
ver: rpa2
title: 'A Survey on Occupancy Perception for Autonomous Driving: The Information Fusion
  Perspective'
arxiv_id: '2405.05173'
source_url: https://arxiv.org/abs/2405.05173
tags:
- occupancy
- perception
- arxiv
- semantic
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews recent advances in 3D occupancy perception
  for autonomous driving, focusing on information fusion from multiple sensors, modalities,
  and frames. The paper categorizes methods into LiDAR-centric, vision-centric, and
  multi-modal approaches, and discusses network training strategies and loss functions.
---

# A Survey on Occupancy Perception for Autonomous Driving: The Information Fusion Perspective

## Quick Facts
- arXiv ID: 2405.05173
- Source URL: https://arxiv.org/abs/2405.05173
- Reference count: 40
- Key outcome: Vision-centric 3D occupancy methods have significantly closed the accuracy gap with LiDAR-centric approaches but still lag in geometric precision, while multi-modal fusion has not yet demonstrated clear advantages over single-modal methods.

## Executive Summary
This survey comprehensively reviews recent advances in 3D occupancy perception for autonomous driving, with particular focus on information fusion from multiple sensors and modalities. The authors categorize existing methods into LiDAR-centric, vision-centric, and multi-modal approaches, analyzing their respective strengths and limitations. While LiDAR-centric methods maintain accuracy advantages due to precise depth measurements, vision-centric approaches have made remarkable progress through sophisticated 2D-to-3D transformations and spatial-temporal fusion techniques. The survey also examines training strategies ranging from strongly-supervised to self-supervised learning, highlighting ongoing challenges in achieving real-time performance, robustness in complex environments, and generalization across diverse driving scenarios.

## Method Summary
The paper surveys 3D occupancy perception methods that predict voxel-wise occupied states or semantic categories in 3D grid volumes using multi-source inputs. Three main approaches are identified: LiDAR-centric methods that voxelize point clouds and use 2D/3D branches for occupancy prediction; vision-centric methods that transform multi-camera images to 3D through projection, back-projection, or cross-attention mechanisms followed by spatial and temporal fusion; and multi-modal methods that combine LiDAR, camera, and radar data through concatenation, summation, or cross-attention fusion. Training strategies include strongly-supervised learning with occupancy labels, weakly-supervised approaches using alternative labels, semi-supervised methods with partial labels, and self-supervised techniques based on volume rendering and photometric consistency. Performance is evaluated using voxel-level IoU for geometric accuracy and mIoU for semantic occupancy, with ray-level metrics addressing occlusion challenges.

## Key Results
- LiDAR-centric methods (e.g., S3CNet, SalsaNext) achieve higher geometric accuracy (IoU up to 72.15%) than vision-centric methods (IoU up to 43.27%) due to precise depth measurements
- Vision-centric methods (e.g., TPVFormer, SurroundOcc) have significantly closed the accuracy gap through advanced 2D-to-3D transformation techniques
- Multi-modal fusion approaches (e.g., OpenOccupancy, Co-Occ) have not yet surpassed single-modal methods, indicating current fusion architectures fail to fully exploit complementary sensor strengths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LiDAR-centric methods outperform vision-centric ones in accuracy because LiDAR provides precise depth measurements unaffected by lighting or weather conditions.
- Mechanism: LiDAR captures accurate 3D geometry directly via time-of-flight sensing, producing dense and precise point clouds. This geometric precision is critical for voxel-wise occupancy prediction, where errors in depth directly degrade accuracy.
- Core assumption: Depth precision is the dominant factor in occupancy prediction accuracy, more so than semantic richness or input density.
- Evidence anchors:
  - [section] "Due to the accurate depth information provided by LiDAR sensing, LiDAR-centric occupancy methods have more precise perception with higher IoU and mIoU scores."
  - [corpus] Weak or missing.

### Mechanism 2
- Claim: Multi-modal fusion (LiDAR + camera) has not yet surpassed single-modal methods because current fusion architectures fail to fully exploit complementary strengths.
- Mechanism: Multi-modal models should combine LiDAR's geometric robustness with camera's semantic richness, but existing fusion methods (e.g., concatenation, summation, cross-attention) may not effectively align or integrate these features, leading to suboptimal performance.
- Core assumption: Proper alignment and integration of multi-modal features can unlock performance gains that current fusion strategies do not achieve.
- Evidence anchors:
  - [section] "However, we observe that the multi-modal approaches (e.g., OpenOccupancy [11] and Co-Occ [103]) do not outperform single-modal (i.e., LiDAR-centric or vision-centric) methods, indicating that they have not fully leveraged the benefits of multi-modal fusion and the richness of input data."
  - [corpus] Weak or missing.

### Mechanism 3
- Claim: Self-supervised learning has not yet matched strongly-supervised performance because it lacks the precise voxel-wise occupancy labels needed to constrain learning.
- Mechanism: Self-supervised methods rely on photometric consistency and volume rendering to generate training signals, but these signals are weaker and noisier than explicit occupancy labels, leading to lower accuracy.
- Core assumption: Occupancy labels provide strong, unambiguous constraints that self-supervised signals cannot replicate.
- Evidence anchors:
  - [section] "SelfOcc [87] and OccNeRF [38] are two representative occupancy works based on self-supervised learning. They utilize volume rendering and photometric consistency to acquire self-supervised signals, proving that a network can learn 3D occupancy perception without any labels. However, their performance remains limited, with SelfOcc achieving an mIoU of 7.97% and OccNeRF an mIoU∗ of 10.81%."
  - [corpus] Weak or missing.

## Foundational Learning

- Concept: 3D occupancy perception and voxel-based scene representation
  - Why needed here: The entire survey revolves around predicting whether each voxel in a 3D grid is occupied, which is the core task of 3D occupancy perception.
  - Quick check question: What is the primary output format of a 3D occupancy prediction network?

- Concept: Multi-modal data fusion techniques (concatenation, summation, cross-attention)
  - Why needed here: Different sensor modalities (LiDAR, camera, radar) provide complementary information, and fusion is essential for robust occupancy prediction.
  - Quick check question: What are the three main types of multi-modal information fusion described in the paper?

- Concept: Training strategies and loss functions (BCE, CE, focal, etc.)
  - Why needed here: The choice of loss function directly impacts the network's ability to learn accurate occupancy and semantic predictions.
  - Quick check question: Which loss function is most commonly used for geometric accuracy in occupancy prediction?

## Architecture Onboarding

- Component map: Input → 2D feature extraction → 2D-to-3D transformation → Spatial fusion (multi-camera) → Temporal fusion (optional) → Occupancy head → Output voxel-wise predictions
- Critical path: Multi-camera images → 2D backbones (ResNet/ViT) → 2D-to-3D transformation (projection/back-projection/cross-attention) → Spatial fusion → Occupancy head
- Design tradeoffs:
  - Accuracy vs. speed: LiDAR-centric methods are more accurate but require expensive sensors; vision-centric methods are cheaper but less accurate.
  - Fusion complexity vs. robustness: Multi-modal fusion can improve robustness but adds computational cost and complexity.
  - Supervision type vs. label cost: Strongly-supervised learning is accurate but requires expensive voxel-wise labels; self-supervised learning is label-efficient but less accurate.
- Failure signatures:
  - Low IoU/mIoU scores: Likely due to poor 2D-to-3D transformation, inadequate feature fusion, or insufficient training data.
  - Slow inference: May be caused by large model size, inefficient fusion operations, or high-resolution inputs.
  - Poor generalization: Could result from overfitting to a narrow dataset or insufficient multi-modal alignment.
- First 3 experiments:
  1. Compare IoU/mIoU scores of a baseline LiDAR-centric method (e.g., S3CNet) with a vision-centric method (e.g., TPVFormer) on SemanticKITTI.
  2. Evaluate the impact of spatial fusion (average vs. cross-attention) on multi-camera occupancy prediction accuracy.
  3. Test the effect of temporal fusion (current vs. multi-frame input) on occupancy prediction in occluded scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can self-supervised 3D occupancy prediction methods achieve comparable performance to strongly-supervised methods on large-scale autonomous driving datasets?
- Basis in paper: [explicit] The paper states that self-supervised methods like SelfOcc and OccNeRF have poor performance compared to strongly-supervised methods, with mIoU scores significantly lower (7.97% vs 40.75% for FastOcc on Occ3D-nuScenes).
- Why unresolved: Self-supervised learning for 3D occupancy is a relatively new area with limited research. Current methods rely on photometric consistency, which may not be sufficient for complex outdoor scenes with varying lighting conditions and occlusions.
- What evidence would resolve it: A self-supervised method achieving mIoU scores within 10-15% of the best strongly-supervised methods on a large-scale autonomous driving dataset like Occ3D-nuScenes or OpenScene.

### Open Question 2
- Question: How can multi-modal fusion techniques be improved to leverage the complementary strengths of LiDAR, radar, and camera data for 3D occupancy perception?
- Basis in paper: [explicit] The paper notes that multi-modal approaches like OpenOccupancy and Co-Occ do not outperform single-modal methods, indicating they have not fully leveraged the benefits of multi-modal fusion.
- Why unresolved: Multi-modal fusion for 3D occupancy is a complex problem involving different data representations (point clouds, images, radar signals) and requires effective fusion strategies to combine information from these modalities.
- What evidence would resolve it: A multi-modal method achieving significantly higher IoU and mIoU scores than the best single-modal methods on a large-scale autonomous driving dataset.

### Open Question 3
- Question: What are the key architectural and training strategies for achieving real-time 3D occupancy prediction on edge devices with limited computational resources?
- Basis in paper: [explicit] The paper discusses the need for deployment-efficient occupancy methods with high accuracy and fast inference speed. It mentions some recent efforts like FastOcc, SparseOcc, and GaussianFormer, but notes they are still far from practical deployment.
- Why unresolved: Real-time 3D occupancy prediction requires efficient network architectures, lightweight designs, and optimized training strategies to balance accuracy and speed on resource-constrained edge devices.
- What evidence would resolve it: A 3D occupancy method achieving high accuracy (mIoU > 35%) and real-time inference speed (FPS > 10Hz) on a common edge device like an NVIDIA Jetson Xavier NX.

## Limitations

- Performance comparisons rely heavily on published benchmarks without independent reproduction, limiting empirical validation of stated conclusions
- Critical architectural details remain underspecified for many methods, making faithful reproduction difficult
- Multi-modal fusion results are particularly inconclusive, with stated limitations that current fusion architectures fail to fully exploit complementary sensor strengths

## Confidence

- **High confidence**: LiDAR-centric methods outperform vision-centric approaches in geometric accuracy due to depth precision advantages
- **Medium confidence**: Vision-centric methods have closed the gap significantly but still lag in accuracy; the trajectory of improvement is well-documented
- **Low confidence**: Multi-modal fusion performance claims and self-supervised learning limitations, as these areas lack comprehensive comparative analysis and architectural details

## Next Checks

1. **Replicate the vision-centric vs LiDAR-centric accuracy gap**: Implement a minimal vision-centric occupancy network (using multi-camera images with 2D-to-3D transformation) and compare its IoU performance against a standard LiDAR-centric baseline on SemanticKITTI, controlling for input resolution and model complexity.

2. **Test multi-modal fusion effectiveness**: Build a controlled experiment comparing single-modal (LiDAR-only, camera-only) against multi-modal fusion variants using the same base architectures, measuring whether concatenation, summation, or cross-attention fusion actually improves performance over individual modalities.

3. **Evaluate self-supervised learning constraints**: Implement a simplified self-supervised occupancy network using volume rendering and photometric consistency, then systematically introduce varying amounts of occupancy labels to measure the performance gap and identify at what label ratio self-supervised approaches become competitive.