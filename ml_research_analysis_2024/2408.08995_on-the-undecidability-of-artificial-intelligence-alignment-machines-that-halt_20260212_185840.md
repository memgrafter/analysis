---
ver: rpa2
title: 'On the Undecidability of Artificial Intelligence Alignment: Machines that
  Halt'
arxiv_id: '2408.08995'
source_url: https://arxiv.org/abs/2408.08995
tags:
- alignment
- would
- function
- problem
- halting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work demonstrates that the inner alignment problem in AI\u2014\
  determining whether an arbitrary AI model always satisfies a given alignment function\u2014\
  is undecidable, proven by reduction to Turing's Halting Problem. While arbitrary\
  \ AI models cannot be proven aligned or misaligned, the authors show that a countable\
  \ set of AI systems can be constructed from a finite set of provenly aligned operations\
  \ and models, guaranteeing alignment through architectural design."
---

# On the Undecidability of Artificial Intelligence Alignment: Machines that Halt
## Quick Facts
- arXiv ID: 2408.08995
- Source URL: https://arxiv.org/abs/2408.08995
- Reference count: 28
- Primary result: The inner alignment problem for arbitrary AI models is undecidable, but can be made decidable by ensuring AI systems always halt in finite steps

## Executive Summary
This paper demonstrates that determining whether an arbitrary AI model always satisfies a given alignment function is undecidable, proven by reduction to Turing's Halting Problem. While arbitrary AI models cannot be proven aligned or misaligned, the authors show that a countable set of AI systems can be constructed from a finite set of provenly aligned operations and models, guaranteeing alignment through architectural design. They propose that alignment should be built into AI architectures from the ground up rather than imposed post-hoc. A key contribution is showing that ensuring AI systems always halt in finite steps makes alignment (and other properties) decidable, enabling computational verification.

## Method Summary
The paper employs formal mathematical proofs to establish the undecidability of the inner alignment problem by reducing it to Turing's Halting Problem. It constructs adversarial models that can fool any hypothetical alignment verifier, demonstrating the impossibility of general alignment verification. The authors then propose a constructive approach where a countable set of provably aligned AI systems can be built from a finite set of provenly aligned base operations and models. They introduce the concept of halting guarantees as a mechanism to make alignment decidable, with examples using neural networks and large language models to illustrate implementation approaches.

## Key Results
- The inner alignment problem for arbitrary AI models is undecidable, proven via reduction to Turing's Halting Problem
- A countable set of provably aligned AI systems can be constructed from a finite set of provenly aligned operations and models
- Ensuring AI systems always halt in finite steps makes alignment (and other properties) decidable, enabling computational verification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inner alignment verification is undecidable because arbitrary AI models are equivalent to Turing Machines and Rice's theorem applies to non-trivial semantic properties.
- Mechanism: The proof reduces the inner alignment problem to Turing's Halting Problem. If a verifier existed that could decide whether any AI model always satisfies an alignment function, this could be used to construct a contradiction via an adversarial model that fools the verifier.
- Core assumption: AI models can be represented as programs (Turing Machines) and alignment properties are semantic (non-trivial).
- Evidence anchors:
  - [abstract] "The inner alignment problem, which asserts whether an arbitrary artificial intelligence (AI) model satisfices a non-trivial alignment function of its outputs given its inputs, is undecidable."
  - [section] "The formal proof could simply be reduced to a restatement of Rice's Theorem, given the equivalence of an AI model to its Turing Machine."
- Break condition: The undecidability holds for arbitrary AI models but not for restricted classes of models with guaranteed halting behavior.

### Mechanism 2
- Claim: A countable set of provably aligned AI systems can be constructed from a finite set of proven aligned operations.
- Mechanism: Starting from base models and operations that are proven to satisfy the alignment property, composition through finite applications preserves the property, creating an enumerable infinite set of aligned AIs.
- Core assumption: The finite set of base operations preserves alignment when composed, and composition is always finite.
- Evidence anchors:
  - [abstract] "there is an enumerable set of provenly aligned AIs that are constructed from a finite set of provenly aligned operations"
  - [section] "We will begin with a simple example of deep artificial neural networks (D-ANN). A Special Decidable Case"
- Break condition: The composition must always terminate in finite steps; otherwise, undecidability may re-emerge through loops.

### Mechanism 3
- Claim: Ensuring AI systems always halt in finite steps makes alignment (and other properties) decidable.
- Mechanism: If a system is guaranteed to halt, its behavior can be exhaustively verified by checking all possible inputs or by masking outputs through a judge function that enforces alignment.
- Core assumption: The system has finite state space and finite execution time for all inputs.
- Evidence anchors:
  - [abstract] "ensuring AI systems always halt in finite steps makes alignment (and other properties) decidable, enabling computational verification"
  - [section] "A Special Decidable Case...These feed-forward neural networks, and even recurrent neural networks (unrolled in time to a finite number of steps), are always guaranteed to have a finite runtime during their inference, in other words, they always halt."
- Break condition: If the system can enter infinite loops or has unbounded state space, decidability is lost.

## Foundational Learning

- Concept: Rice's Theorem and its application to semantic properties of Turing Machines
  - Why needed here: The core undecidability result relies on Rice's theorem, which states that any non-trivial semantic property of Turing Machines is undecidable
  - Quick check question: What makes a property "non-trivial" in the context of Rice's theorem?

- Concept: Reduction proofs and the Halting Problem
  - Why needed here: The paper uses reduction from the Halting Problem to prove undecidability of alignment
  - Quick check question: How does constructing an adversarial model that fools a verifier lead to a contradiction?

- Concept: Countable vs uncountable sets and Cantor's diagonalization
  - Why needed here: The paper mentions diagonalization to explain why arbitrary AI models cannot be enumerated or verified
  - Quick check question: Why does Cantor's diagonalization argument apply to the set of all possible AI models?

## Architecture Onboarding

- Component map: Base aligned operations -> Composition mechanism -> Judge function -> Halting guarantee -> Output masking/feedback loop
- Critical path: Design base aligned operations → Define judge function → Implement halting guarantee → Compose operations → Verify alignment
- Design tradeoffs:
  - Expressiveness vs. decidability: More powerful operations may break decidability
  - Performance vs. safety: Halting constraints and masking may reduce efficiency
  - Generality vs. tractability: Enumerating all states is intractable for large systems
- Failure signatures:
  - System enters infinite loops
  - Judge function cannot be computed in finite time
  - Composition of operations breaks alignment guarantee
  - State space becomes too large for practical verification
- First 3 experiments:
  1. Build a simple feed-forward neural network with guaranteed halting and verify alignment for small input spaces
  2. Implement output masking with a trivial judge function and measure performance impact
  3. Create a composition of two aligned operations and verify the composed system remains aligned

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we construct a finite set of base operations that guarantees alignment when composed, and what would those operations be?
- Basis in paper: [explicit] The paper states "we can compose those models and operations and construct an enumerable infinite set of AI that is guaranteed to have the desired property" starting from "a finite set of base models and operations that are proved to have the desired property."
- Why unresolved: The paper provides the theoretical framework but does not specify concrete operations that would serve as the foundation for this construction.
- What evidence would resolve it: A concrete demonstration of a finite set of operations (e.g., specific neural network layers, mathematical functions) that when composed produce only aligned systems, along with proofs of their alignment-preserving properties.

### Open Question 2
- Question: How can we practically implement the "halting constraint" in complex AI systems like large language models without severely compromising their utility?
- Basis in paper: [explicit] The paper discusses that "ensuring the decidability of the halting property of autonomous systems also ensures the decidability of other non-trivial properties of such systems" and presents the idea of using a "θ parameter" to guarantee halting, but acknowledges this may reduce capabilities.
- Why unresolved: While the paper proposes theoretical mechanisms for ensuring halting (like global execution counters or self-referential weights), it does not address the practical trade-offs between guaranteed halting and system performance in real-world applications.
- What evidence would resolve it: Empirical studies comparing the performance of AI systems with and without halting constraints, demonstrating the impact on capabilities and identifying architectures that minimize utility loss while ensuring halting.

### Open Question 3
- Question: Is it possible to design a judge function Jv(i,o) that captures human values and preferences in a way that is both computable and aligned with diverse human perspectives?
- Basis in paper: [explicit] The paper states "the main difficulty of such a solution is the definition of the judge function Jv(i,o) in the first place, the outer alignment" and acknowledges that "the accurate description of such a function may be uncomputable in terms of human values."
- Why unresolved: The paper focuses on the computational decidability of alignment given a judge function, but does not address how to construct such a function that adequately represents complex human values, especially given potential contradictions in human preferences.
- What evidence would resolve it: A formal specification of a judge function that has been validated against diverse human value systems, demonstrating both computational feasibility and alignment with human preferences across different contexts and cultures.

## Limitations

- The undecidability proof assumes AI models can be represented as arbitrary Turing Machines, which may not capture practical AI systems with bounded resources
- The construction of provably aligned AI systems requires careful specification of base operations, which are not fully elaborated in the paper
- The proposed solutions (halting guarantees, output masking) may significantly limit the capabilities and expressiveness of AI systems

## Confidence

- **High confidence**: The undecidability proof using reduction to Turing's Halting Problem and Rice's theorem. This is a well-established mathematical framework with clear logical steps.
- **Medium confidence**: The construction of a countable set of provably aligned AI systems from finite base operations. While the theoretical framework is sound, the practical implementation details and verification of alignment preservation during composition are not fully specified.
- **Medium confidence**: The claim that ensuring halting behavior makes alignment decidable. This follows logically from the undecidability proof but requires careful implementation to avoid reintroducing undecidability through composition or state explosion.

## Next Checks

1. **Formal verification of the undecidability proof**: Recreate the reduction from the alignment problem to Turing's Halting Problem using a formal proof assistant (e.g., Coq or Isabelle) to verify the logical steps and identify any gaps in the reasoning.

2. **Implementation of base aligned operations**: Develop concrete examples of base AI operations and models that can be proven aligned using formal verification methods, and test whether these operations preserve alignment when composed in various ways.

3. **Performance evaluation of halting constraints**: Implement a simple AI system with guaranteed halting behavior (e.g., a feed-forward neural network with bounded depth) and measure the impact of halting constraints and output masking on performance and accuracy compared to unconstrained versions.