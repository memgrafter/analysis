---
ver: rpa2
title: 'EventFlow: Forecasting Temporal Point Processes with Flow Matching'
arxiv_id: '2410.07430'
source_url: https://arxiv.org/abs/2410.07430
tags:
- event
- events
- which
- distribution
- eventflow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EventFlow introduces a non-autoregressive generative model for
  temporal point processes using flow matching. Instead of predicting events one-by-one,
  it learns the joint distribution over event times by interpolating between a reference
  and data distribution through a learned vector field.
---

# EventFlow: Forecasting Temporal Point Processes with Flow Matching

## Quick Facts
- arXiv ID: 2410.07430
- Source URL: https://arxiv.org/abs/2410.07430
- Authors: Gavin Kerrigan; Kai Nelson; Padhraic Smyth
- Reference count: 29
- Primary result: EventFlow achieves state-of-the-art multi-step forecasting with 20%-53% error reduction

## Executive Summary
EventFlow introduces a non-autoregressive generative model for temporal point processes using flow matching. Instead of predicting events one-by-one, it learns the joint distribution over event times by interpolating between a reference and data distribution through a learned vector field. This approach avoids the compounding errors and complexity of autoregressive sampling. EventFlow achieves state-of-the-art performance on multi-step forecasting, reducing error by 20%-53% compared to the nearest baseline. It also matches or exceeds baselines on unconditional generation tasks. Notably, EventFlow can reach competitive results with only a single model evaluation during sampling, making it both efficient and effective.

## Method Summary
EventFlow extends the flow matching framework to temporal point processes by learning a vector field that transports events from a reference distribution to the data distribution. The model decouples event count prediction from event time prediction, training a separate distribution for counts and using flow matching for joint event times given a fixed count. At sampling time, it generates events from a reference distribution (typically N(0,I)) and flows them along the learned vector field using an ODE solver. The architecture uses transformer-based encoders and decoders with sinusoidal positional embeddings, trained to minimize MSE loss between true and predicted event times plus cross-entropy for event count prediction with optimal transport regularization.

## Key Results
- Achieves 20%-53% reduction in forecasting error compared to nearest baseline
- Matches or exceeds baseline performance on unconditional generation tasks
- Can achieve competitive results with only a single model evaluation (1 NFE)
- Outperforms autoregressive models while avoiding compounding errors

## Why This Works (Mechanism)

### Mechanism 1
EventFlow learns a joint distribution over event times by interpolating between a reference and data distribution using a learned vector field, avoiding the compounding errors of autoregressive sampling. The model constructs a path of TPP distributions by linearly interpolating between a reference sample and a data sample, then adding Gaussian noise. This defines a vector field that transports events from the reference to the data distribution. The core assumption is that the path from reference to data can be approximated by a continuous flow in the configuration space Γ.

### Mechanism 2
EventFlow achieves state-of-the-art multi-step forecasting by decoupling event count prediction from event time prediction. The model trains a separate distribution pϕ(n|H) for event counts given history H, and uses flow matching only for the joint distribution of event times conditioned on a fixed count. The core assumption is that event count and event time distributions can be modeled independently without loss of fidelity.

### Mechanism 3
EventFlow requires only a single model evaluation at sampling time while maintaining competitive performance. The vector field learned by flow matching is approximately linear, allowing the ODE to be solved with minimal steps (even 1 NFE) without significant accuracy loss. The core assumption is that the learned vector fields are close to linear, making the ODE solution stable with few evaluations.

## Foundational Learning

- Concept: Temporal Point Processes (TPPs)
  - Why needed here: EventFlow operates on the mathematical framework of TPPs, which characterize distributions over continuous-time event sequences
  - Quick check question: What is the difference between the conditional intensity function and the conditional probability density in TPPs?

- Concept: Flow Matching Framework
  - Why needed here: EventFlow extends this framework from image generation to temporal point processes, learning continuous flows between distributions
  - Quick check question: How does flow matching differ from normalizing flows in terms of training objective?

- Concept: Configuration Space Γ
  - Why needed here: EventFlow operates in the configuration space of counting measures, where sequences are represented as collections of Dirac deltas
  - Quick check question: Why is it mathematically convenient to represent TPPs as random measures rather than random sets of event times?

## Architecture Onboarding

- Component map: History encoder (transformer) -> Event count predictor (transformer with mean pooling) -> Vector field model (transformer decoder) -> ODE solver
- Critical path: Input history -> Event count prediction -> Initial event generation -> Vector field evaluation -> ODE integration -> Output sequence
- Design tradeoffs: Non-autoregressive sampling (faster, fewer errors) vs. autoregressive (potentially more flexible for single-step prediction)
- Failure signatures: Poor forecasting accuracy suggests issues with vector field learning; unstable ODE solutions suggest non-linear vector fields; incorrect event counts suggest issues with the count predictor
- First 3 experiments:
  1. Train on a synthetic Hawkes process dataset and verify the model can generate sequences with similar characteristics
  2. Test the ODE solver with different NFE values (1, 10, 25) on a simple dataset to measure accuracy vs. efficiency tradeoff
  3. Compare event count predictions against ground truth on a held-out test set to validate the count predictor component

## Open Questions the Paper Calls Out

### Open Question 1
How does EventFlow's performance scale with the dimensionality of the flow time discretization (number of steps K in the ODE solver)? The paper mentions that EventFlow can achieve competitive performance with only a single forward pass (1 NFE), but varying K from 1 to 25 shows minimal performance differences. While the paper demonstrates that fewer steps work well, it doesn't explore the limits of this efficiency or analyze how performance degrades as K approaches 1.

### Open Question 2
Can EventFlow be extended to handle marked temporal point processes where events have associated marks (e.g., event types or magnitudes)? The paper explicitly states it focuses on modeling event times only and doesn't experiment with marked TPPs, noting this as a promising direction. The current architecture processes only event times, and no modifications or experiments with marks are presented.

### Open Question 3
How sensitive is EventFlow to the choice of reference distribution q in the mixed binomial process construction? The paper uses q = N(0, I) after normalizing sequences to [-1, 1], but notes this is a design choice and doesn't explore alternatives. The paper doesn't report experiments varying q or analyze how different choices affect model performance.

## Limitations
- The linearity assumption for learned vector fields is empirically supported but lacks theoretical guarantees
- The decoupling of event count and time prediction may not generalize to scenarios where these variables are inherently coupled
- The paper does not extensively explore the impact of different ODE solvers or discretization schemes on performance

## Confidence

- **High confidence**: EventFlow's state-of-the-art performance on multi-step forecasting (20%-53% error reduction) is well-supported by experimental results across seven real-world datasets.
- **Medium confidence**: The claim that EventFlow can achieve competitive results with a single model evaluation is empirically demonstrated but lacks theoretical justification for the linearity assumption in learned vector fields.
- **Medium confidence**: The decoupling of event count and time prediction is shown to work in practice, but the paper does not rigorously prove that this approach preserves the joint distribution fidelity.

## Next Checks

1. **Test linearity assumption**: Generate synthetic datasets with known non-linear event patterns (e.g., multi-modal distributions) and measure EventFlow's performance with varying NFE values to validate the linearity assumption.

2. **Evaluate coupling effects**: Create a controlled experiment where event counts and times are explicitly coupled (e.g., through a deterministic function) and compare EventFlow's performance against a fully autoregressive baseline.

3. **Explore numerical stability**: Implement alternative ODE solvers (e.g., Runge-Kutta methods) and compare their performance and stability with EventFlow's forward Euler approach on datasets with varying levels of noise and discontinuity.