---
ver: rpa2
title: Pretraining with random noise for uncertainty calibration
arxiv_id: '2412.17411'
source_url: https://arxiv.org/abs/2412.17411
tags:
- confidence
- random
- noise
- pretraining
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Uncertainty calibration in deep neural networks remains challenging,
  with many models exhibiting overconfidence and hallucinations - producing confident
  yet inaccurate responses. This work identifies that the standard practice of random
  initialization is a fundamental cause of this miscalibration, leading to excessive
  confidence in untrained networks.
---

# Pretraining with random noise for uncertainty calibration

## Quick Facts
- arXiv ID: 2412.17411
- Source URL: https://arxiv.org/abs/2412.17411
- Reference count: 40
- Pretraining with random noise and labels calibrates network confidence to chance levels before data training

## Executive Summary
This work addresses the fundamental challenge of uncertainty calibration in deep neural networks, where models often exhibit overconfidence and hallucinations - producing confident yet inaccurate predictions. The authors identify that random initialization, the standard practice in deep learning, is a primary cause of miscalibration, leading to excessive confidence in untrained networks. They propose a simple yet effective solution: pretraining networks with random noise and random labels to calibrate confidence to chance levels across the input space before training on real data. This pre-calibration ensures that confidence aligns with accuracy during subsequent data training, significantly reducing Expected Calibration Error (ECE) across various network depths and training data sizes.

## Method Summary
The proposed method involves two distinct training phases. First, networks undergo pretraining with random Gaussian noise inputs and uniform random labels using cross-entropy loss, continuing until the loss stabilizes while maintaining chance-level accuracy. This pretrains the network to treat all outputs equally likely, eliminating initial confidence biases. Second, the pretrained networks are trained on real data using standard backpropagation with Adam optimizer. The approach works with both backpropagation and biologically plausible learning rules like feedback alignment, requiring no additional post-processing or specialized architectures.

## Key Results
- Random initialization causes overconfidence in untrained networks, with confidence far exceeding chance levels
- Random noise pretraining reduces Expected Calibration Error (ECE) significantly across various network depths and training data sizes
- Pretrained networks show improved detection of out-of-distribution samples, maintaining low confidence on unseen data
- Confidence and accuracy remain well-aligned during data training when networks start from a pre-calibrated state

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random initialization causes overconfidence by creating initial biases in untrained networks.
- Mechanism: Randomly initialized weights create arbitrary decision boundaries before training, leading the network to produce confident but incorrect predictions on untrained inputs.
- Core assumption: The initial weight distribution directly influences the confidence distribution across the input space.
- Evidence anchors:
  - [abstract] "the common practice of random initialization in deep learning... is an underlying cause of this miscalibration, leading to excessively high confidence in untrained networks."
  - [section] "We observed that the initial state of a conventionally processed network — randomly initialized and untrained — displayed a highly biased distribution of confidence across the input space."
- Break condition: If the network uses weight initialization schemes that explicitly constrain early layer outputs to uniform distributions.

### Mechanism 2
- Claim: Pretraining with random noise calibrates confidence to chance levels across the input space.
- Mechanism: Training on random noise with random labels forces the network to learn that all outputs are equally likely, eliminating initial confidence biases.
- Core assumption: The network can learn to match uniform output distributions when input-label pairs are uncorrelated.
- Evidence anchors:
  - [abstract] "Our method... addresses this issue by simply pretraining networks with random noise and labels, reducing overconfidence and bringing initial confidence levels closer to chance."
  - [section] "During training with random noise, we observed that the loss gradually decreased, while accuracy remained at the chance level."
- Break condition: If the noise distribution is too structured or the training process fails to converge on uniform outputs.

### Mechanism 3
- Claim: Pre-calibration enables subsequent learning with aligned confidence and accuracy.
- Mechanism: By starting from a well-calibrated initial state, the network's confidence naturally tracks accuracy during data training rather than diverging due to initial miscalibration.
- Core assumption: The learning trajectory from a calibrated starting point maintains the alignment between confidence and accuracy.
- Evidence anchors:
  - [section] "In pretrained networks, confidence and accuracy remained well-aligned, consistently following the diagonal line during training."
  - [section] "This alignment of confidence and accuracy supports more synchronized learning during subsequent data training."
- Break condition: If the data training process introduces new sources of miscalibration not present in the pretraining phase.

## Foundational Learning

- Concept: Expected Calibration Error (ECE)
  - Why needed here: Provides the quantitative metric for measuring confidence miscalibration throughout the paper.
  - Quick check question: What does an ECE of zero indicate about a model's confidence calibration?

- Concept: Reliability diagrams
  - Why needed here: Visualizes the relationship between predicted confidence and actual accuracy, complementing the ECE metric.
  - Quick check question: In a perfectly calibrated model's reliability diagram, what shape should the accuracy-vs-confidence curve take?

- Concept: SoftMax probability outputs
  - Why needed here: The paper's calibration analysis assumes the final layer produces calibrated probability distributions.
  - Quick check question: What property must SoftMax outputs satisfy for them to be interpretable as calibrated probabilities?

## Architecture Onboarding

- Component map: Random noise pretraining -> Confidence calibration -> Data training -> Final evaluation
- Critical path: Gaussian noise + random labels pretraining (BCE/cross-entropy loss) → Calibrated initialization → Real data training (Adam optimizer, batch norm, ReLU) → ECE measurement
- Design tradeoffs: Simpler architecture vs. more complex state-of-the-art models; random noise pretraining adds one training phase but eliminates post-processing needs
- Failure signatures: Confidence consistently exceeding accuracy in reliability diagrams; high ECE values that don't decrease with training
- First 3 experiments:
  1. Train a small network without pretraining and measure ECE on CIFAR-10
  2. Repeat with random noise pretraining and compare ECE reduction
  3. Test OOD detection performance on SVHN after CIFAR-10 training with and without pretraining

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does random noise pretraining generalize to state-of-the-art large language models (LLMs) and vision-language models?
- Basis in paper: [explicit] The authors note their model was not directly validated through real-world applications or benchmarking and was based on relatively simple tasks and smaller-scale networks compared to state-of-the-art models like LLMs. They state "we are confident that the results obtained in this study can be replicated in more complex, real-world models" but acknowledge this needs validation.
- Why unresolved: LLMs have different architectures (transformer-based), different training objectives, and different input modalities compared to the feedforward networks studied. The computational cost of systematically investigating calibration in LLMs is prohibitive in typical laboratory settings.
- What evidence would resolve it: Empirical demonstration that random noise pretraining reduces Expected Calibration Error (ECE) in transformer-based architectures like BERT, GPT, or vision-language models like CLIP when applied during pretraining or fine-tuning phases.

### Open Question 2
- Question: What is the optimal duration and intensity of random noise pretraining for achieving maximum calibration improvement?
- Basis in paper: [inferred] The authors show that random noise pretraining reduces calibration error but do not systematically explore the relationship between pretraining duration, noise intensity, and calibration quality. The study uses fixed pretraining conditions without examining trade-offs between pretraining time and calibration gains.
- Why unresolved: The paper demonstrates effectiveness but doesn't provide guidelines for hyperparameter selection in practical applications. Different network architectures and data complexities might require different pretraining schedules.
- What evidence would resolve it: Systematic ablation studies varying pretraining epoch count, noise distribution parameters, and learning rates to identify optimal pretraining configurations across different network depths and data sizes, measuring the point of diminishing returns.

### Open Question 3
- Question: How does random noise pretraining affect downstream task performance beyond calibration, such as accuracy, robustness, and generalization?
- Basis in paper: [explicit] The authors note that random noise pretraining leads to "a more substantial reduction in test loss" and "higher accuracy" during subsequent data training, but do not comprehensively examine broader performance impacts beyond calibration metrics.
- Why unresolved: The study focuses primarily on calibration error and out-of-distribution detection performance, but real-world deployment requires understanding the full impact on task performance, including potential trade-offs between calibration and accuracy.
- What evidence would resolve it: Comprehensive benchmarking showing how random noise pretraining affects accuracy, robustness to adversarial examples, transfer learning capabilities, and computational efficiency across multiple datasets and tasks compared to standard initialization methods.

## Limitations
- The computational overhead of the additional pretraining phase could be prohibitive for very large models or datasets
- The assumption that random noise pretraining calibrates confidence across the entire input space may not hold for highly structured or domain-specific data distributions
- The theoretical justification for why random noise pretraining specifically calibrates confidence remains heuristic rather than mechanistic

## Confidence
- **High Confidence**: The experimental demonstration that random initialization leads to overconfidence in untrained networks is well-supported by multiple experiments and visualizations. The correlation between pretraining and improved calibration metrics is consistently observed across different architectures and dataset sizes.
- **Medium Confidence**: The claim that pretraining works through prenatal neural development inspiration is largely metaphorical rather than mechanistic. While the empirical results are strong, the theoretical justification for why random noise pretraining specifically calibrates confidence remains heuristic.
- **Medium Confidence**: The extension to feedback alignment learning rules shows promise but is only briefly demonstrated in supplementary materials. The core contribution focuses on backpropagation, with the biological plausibility claims requiring more thorough validation.

## Next Checks
1. **Robustness to initialization schemes**: Test whether different weight initialization methods (orthogonal, Xavier, etc.) affect the magnitude of overconfidence in untrained networks, and whether random noise pretraining still provides calibration benefits.

2. **Scaling analysis**: Evaluate the pretraining approach on larger architectures (ResNet, Vision Transformer) and datasets (ImageNet) to determine if the calibration benefits scale or diminish with model complexity.

3. **Alternative calibration baselines**: Compare the random noise pretraining approach against established post-hoc calibration methods (temperature scaling, ensemble methods) on the same architectures to quantify relative effectiveness and computational tradeoffs.