---
ver: rpa2
title: Comprehensive Performance Modeling and System Design Insights for Foundation
  Models
arxiv_id: '2410.00273'
source_url: https://arxiv.org/abs/2410.00273
tags:
- time
- memory
- gpus
- performance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a parameterized analytical model for training
  large transformer models (GPT-3 and Vision Transformers) across diverse system configurations.
  The model explores the full design space of parallelization strategies (data, tensor,
  and pipeline parallelism) and hardware features (GPU generations, memory capacity/bandwidth,
  network topology) to identify optimal configurations that minimize training time.
---

# Comprehensive Performance Modeling and System Design Insights for Foundation Models

## Quick Facts
- arXiv ID: 2410.00273
- Source URL: https://arxiv.org/abs/2410.00273
- Reference count: 40
- Primary result: Introduces parameterized analytical model for training large transformers, identifying optimal parallelism strategies and hardware configurations that minimize training time

## Executive Summary
This work presents a comprehensive performance model for training large transformer models that explores the full design space of parallelization strategies and hardware configurations. The model predicts optimal configurations for different transformer types (GPT-3 and Vision Transformers) across diverse system setups including different GPU generations, NVSwitch domains, and network topologies. Experiments demonstrate that GPT-3 benefits from 3D parallelism and large NVSwitch domains at pre-training scales, while Vision Transformers require 4D parallelism with more uniform dependence on network and capacity. The approach enables system designers to predict and optimize performance for different transformer types and training regimes.

## Method Summary
The research introduces a parameterized analytical performance model that counts FLOPs, memory accesses, and communication volume for transformer operations across different parallelization strategies. The model converts these counts to compute and communication times using a roofline model and dual-bandwidth communication formula, then searches over all feasible configurations to find the optimal setup that minimizes training time while fitting in HBM capacity. The approach is validated against empirical results across multiple GPU generations and scales.

## Key Results
- GPT-3 models benefit significantly from 3D parallelism and large NVSwitch domains at pre-training scales
- Vision Transformers require 4D parallelism with more uniform dependence on network and capacity
- Higher GPU generations (B200) drastically reduce training time from O(30) days on A100 to O(3-5) days on B200 for GPT-3 scale models
- Alternative memory technologies (e.g., LPDDR) can be viable under specific conditions with reduced parallelism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different transformer types require fundamentally different parallelism strategies due to distinct computational bottlenecks.
- Mechanism: LLMs with small sequence lengths are compute-bound and benefit from 1D tensor parallelism and 3D parallelism, while long-sequence transformers are memory-bound and require 2D tensor parallelism to handle activation memory pressure.
- Core assumption: Sequence length determines the dominant bottleneck—small l leads to compute-bound operations, large l leads to memory-bound operations.
- Evidence anchors:
  - [abstract]: "Large Language Models are performant with 3D parallelism... long-sequence transformers... place a more uniform dependence on network and capacity with necessary 4D parallelism."
  - [section]: "GPT3-1T class models can benefit greatly with larger NVS domains in pre-training scales... VIT class models demonstrate a contrasting extreme with more dependence on NVS, HBM, and higher-dimensional parallelism (with 2D TP variants)."

### Mechanism 2
- Claim: NVSwitch domain size fundamentally alters optimal parallelization configuration by enabling better communication hiding.
- Mechanism: Larger NVS domains allow more data parallelism by hiding DP communication costs behind compute, reducing pipeline bubbles at scale, while smaller NVS forces more tensor parallelism to minimize inter-node communication.
- Core assumption: Communication time can be overlapped with computation when both occur within the same fast network domain.
- Evidence anchors:
  - [section]: "We observe that in both high and low DP configurations the fastest configuration involves only 1D TP with n2 = 1... With larger NVS , large DP (low PP) is preferred."
  - [section]: "The dual-bandwidth domain introduces subtle non-convexities in the training time within the design space... as DP increases, the DP communications follow a non-convex pattern, increasing to a peak transition point and then declining."

### Mechanism 3
- Claim: GPU generation improvements provide multiplicative performance gains primarily through tensor core and network bandwidth improvements.
- Mechanism: Higher GPU generations increase TFLOPs/s and HBM bandwidth while maintaining similar memory capacity, reducing training time from O(30) days on A100 to O(3-5) days on B200 for GPT-3 scale models.
- Core assumption: Memory capacity scales sub-linearly with performance improvements, making higher bandwidth more critical than capacity at scale.
- Evidence anchors:
  - [section]: "GPT3-1T model benefits significantly with higher GPU generation... drastically drops to O(3-5) days on the B200 GPU, owing to increased tensor core performance and network bandwidths."
  - [section]: "Both models benefit most from FP16 tensor core performance and network bandwidths, but other memory technologies (lower bandwidths/more capacity like LPDDR) can be viable alternatives."

## Foundational Learning

- Concept: Roofline model for performance estimation
  - Why needed here: The model converts FLOPs and memory accesses into compute time using peak hardware capabilities, essential for comparing different parallelization strategies
  - Quick check question: Given 1e12 FLOPs and hardware peak of 300 TFLOPs/s, what's the compute time estimate? (Answer: ~3.33 seconds)

- Concept: Communication collectives and their overlap
  - Why needed here: Understanding how AllGather, ReduceScatter, and Broadcast operations can be overlapped with computation determines optimal GPU placement within NVS domains
  - Quick check question: If a collective has prologue time tprologue and nbtexposed exposed time, what's the total communication time formula? (Answer: tcomm = tprologue + nbtexposed)

- Concept: Pipeline bubble time and 1F1B scheduling
  - Why needed here: Pipeline parallelism introduces idle time that must be hidden by microbatching; understanding the trade-off between bubble time and communication is crucial for configuration selection
  - Quick check question: For np pipeline stages with forward time tf and backward time tb, what's the bubble time formula? (Answer: tbubble = (np-1)(tf + tb))

## Architecture Onboarding

- Component map: Performance model core -> System characterization -> Configuration search -> Validation layer
- Critical path:
  1. Count FLOPs/memory/communication for each operation
  2. Convert to time using roofline model and communication formulas
  3. Search over all feasible configurations for minimum time
  4. Validate against empirical results
- Design tradeoffs:
  - Memory vs Communication: 2D tensor parallelism reduces memory pressure but increases communication volume
  - Compute vs Bubble time: More pipeline parallelism reduces communication but increases idle time
  - HBM capacity vs Bandwidth: High capacity/low bandwidth configurations can compete with low capacity/high bandwidth when parallelism is reduced
- Failure signatures:
  - Configuration overflow: Model doesn't fit in HBM → need more tensor parallelism or SUMMA
  - Communication bottleneck: High DP/TP communication time → need larger NVS or different GPU placement
  - Bubble time dominant: Pipeline bubbles >50% of iteration time → need more microbatches or less pipeline parallelism
- First 3 experiments:
  1. Run performance model on small scale (128 GPUs) with GPT3-1T to verify 1D TP selection and identify communication bottlenecks
  2. Test NVS domain effects by comparing nNVS=4 vs nNVS=8 configurations for same model
  3. Validate alternate memory technology hypothesis by comparing B200 configurations with reduced HBM bandwidth but increased capacity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relative performance benefit of interleaved pipeline schedules versus non-interleaved schedules for GPT-3 and Vision Transformers at different scales?
- Basis in paper: [explicit] "We do not consider interleaved pipeline schedules [16] that can drop bubble time further."
- Why unresolved: The paper uses non-interleaved 1F1B pipeline schedules and shows that pipeline bubbles dominate training time at scale, but doesn't explore whether interleaved schedules could provide significant performance improvements.
- What evidence would resolve it: Comparative training time measurements between interleaved and non-interleaved pipeline schedules for both GPT-3 and Vision Transformers across multiple GPU scales.

### Open Question 2
- Question: How would alternative memory technologies like LPDDR perform compared to HBM for Vision Transformers at different scales and training regimes?
- Basis in paper: [explicit] "We also show that, depending on the type of model, NVS size effects show up at different scales, lending to different requirements for pre-training and fine-tuning jobs... Both models show good performance with alternate low bandwidth/high capacity memory, which may help alleviate the dependence on NVS through increased capacity."
- Why unresolved: While the paper shows that large capacity/low bandwidth configurations can be viable alternatives to HBM, it doesn't quantify the performance differences or identify the specific conditions where these alternatives would be optimal.
- What evidence would resolve it: Systematic performance comparisons between HBM and alternative memory technologies (LPDDR, etc.) across different model scales, sequence lengths, and training regimes.

### Open Question 3
- Question: What is the optimal balance between tensor parallelism dimensions for Vision Transformers when using SUMMA versus standard 2D tensor parallelism?
- Basis in paper: [explicit] "Further, we show that, depending on the type of model, NVS size effects show up at different scales... However, SUMMA also introduces more communication overlaps that may reduce with more partitioning."
- Why unresolved: The paper shows that 2D tensor parallelism with SUMMA can reduce memory pressure for Vision Transformers but has higher communication volume than standard 2D tensor parallelism, yet doesn't determine the conditions where each approach would be optimal.
- What evidence would resolve it: Performance comparisons between SUMMA and standard 2D tensor parallelism for Vision Transformers across different sequence lengths, model sizes, and GPU configurations.

## Limitations

- The model assumes perfect communication overlap and doesn't account for real-world network congestion or variability in communication patterns during training.
- While the roofline-based approach provides reasonable estimates, it may not capture complex interactions between multiple parallelism strategies at extreme scales.
- The performance predictions are validated against empirical results but primarily focus on Nvidia GPU architectures, limiting generalizability to other hardware platforms.

## Confidence

- **High confidence**: The identification of 3D vs 4D parallelism requirements for different transformer types (LLM vs Vision Transformer) is well-supported by both theoretical analysis and empirical validation.
- **Medium confidence**: The NVSwitch domain size effects and their interaction with DP/PP configurations show consistent patterns but may have non-linearities at scales beyond tested configurations.
- **Low confidence**: The viability claims for alternative memory technologies (LPDDR) are based on theoretical modeling rather than extensive empirical validation, particularly at production scales.

## Next Checks

1. **Scale-out validation**: Test the model's predictions at GPU counts beyond 4096 (tested up to 65,536) to verify if the identified parallelism strategies remain optimal at exascale training deployments.

2. **Hardware portability**: Implement the performance model on non-Nvidia architectures (AMD, Intel) to validate the generalization of the roofline-based approach and identify architecture-specific bottlenecks.

3. **Dynamic configuration testing**: Evaluate whether the static optimal configurations identified by the model maintain their advantage under varying workload conditions, such as non-uniform batch sizes or mixed precision training regimes.