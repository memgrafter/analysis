---
ver: rpa2
title: 'Fine-tuning large language models for domain adaptation: Exploration of training
  strategies, scaling, model merging and synergistic capabilities'
arxiv_id: '2409.03444'
source_url: https://arxiv.org/abs/2409.03444
tags:
- performance
- collagen
- material
- materials
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates fine-tuning strategies for large language
  models in materials science, comparing Continued Pretraining (CPT), Supervised Fine-Tuning
  (SFT), and preference-based methods (DPO, ORPO). The study reveals that model merging
  using Spherical Linear Interpolation (SLERP) produces emergent capabilities beyond
  individual parent models, with nonlinear performance gains observed in 7-8B parameter
  models but not in smaller 1.7B parameter models.
---

# Fine-tuning large language models for domain adaptation: Exploration of training strategies, scaling, model merging and synergistic capabilities

## Quick Facts
- arXiv ID: 2409.03444
- Source URL: https://arxiv.org/abs/2409.03444
- Reference count: 40
- Primary result: Model merging with SLERP creates emergent capabilities beyond parent models, with optimal fine-tuning strategies varying by architecture (Llama vs Mistral)

## Executive Summary
This study investigates fine-tuning strategies for large language models in materials science, comparing Continued Pretraining (CPT), Supervised Fine-Tuning (SFT), and preference-based methods (DPO, ORPO). The research reveals that model merging using Spherical Linear Interpolation (SLERP) produces emergent capabilities beyond individual parent models, with nonlinear performance gains observed in 7-8B parameter models but not in smaller 1.7B parameter models. Across Llama-3.1 and Mistral architectures, merging with SLERP consistently outperforms baseline approaches, with optimal strategies being CPT-SFT-ORPO-SLERP for Llama and CPT-SFT-SLERP for Mistral. The work also demonstrates successful agentic applications in image generation for architectural and materials design, leveraging bioinspired concepts from spider silk, collagen, and leaf microstructures.

## Method Summary
The study employs a systematic fine-tuning pipeline using Llama-3.1, Mistral, and SmolLM models, following a CPT → SFT → DPO/ORPO sequence on scientific papers corpus covering biological materials, mechanics, and spider silk. Model merging is implemented using Spherical Linear Interpolation (SLERP) to combine fine-tuned models, with performance evaluated on domain-specific benchmarks measuring depth of reasoning, creativity, clarity, and quantitative precision. The research compares model merging against weighted averaging and tests different architectures to identify optimal strategies for domain adaptation.

## Key Results
- Model merging with SLERP creates emergent capabilities beyond parent models, with 7-8B parameter models showing significant nonlinear performance gains
- Llama architecture benefits most from ORPO optimization while Mistral performs better with simpler CPT-SFT-SLERP approach
- Smaller 1.7B parameter models do not exhibit emergent capabilities through merging, suggesting a threshold effect at ~7-8B parameters
- Successfully applied fine-tuned models to agentic applications in architectural design using bioinspired materials concepts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model merging using SLERP creates emergent capabilities through nonlinear parameter interactions
- Mechanism: SLERP interpolates between model parameters along a curved path on a unit hypersphere, avoiding destructive interference while preserving beneficial interactions
- Core assumption: Parameter space contains regions representing synergistic combinations neither parent model occupies
- Evidence anchors: Abstract statement on emergent capabilities, section on SLERP's nonlinear interactions, weak corpus support
- Break condition: If parameter space is not spherical or models have incompatible learned features

### Mechanism 2
- Claim: Model size determines SLERP merging success, with threshold around 7-8B parameters
- Mechanism: Larger models have richer high-dimensional parameter spaces enabling novel parameter combinations
- Core assumption: Parameter count correlates with capacity to represent complex feature interactions
- Evidence anchors: Section on small LLMs lacking emergent capabilities, comparison of 7-8B vs 1.7B models, weak corpus support
- Break condition: If smaller models achieve similar properties through alternative strategies or threshold varies significantly

### Mechanism 3
- Claim: Llama and Mistral architectures respond differently to optimization strategies
- Mechanism: Architectural differences in attention mechanisms and pre-training create varying sensitivities to preference optimization
- Core assumption: Internal structure and training history create different optimization landscapes
- Evidence anchors: Analysis showing Llama favors ORPO while Mistral prefers simpler approaches, trends in relative improvement data
- Break condition: If architectural differences prove less significant than other factors like dataset quality

## Foundational Learning

- Concept: Spherical Linear Interpolation (SLERP) in high-dimensional spaces
  - Why needed here: Core mechanism enabling emergent capabilities through model merging
  - Quick check question: How does SLERP differ from linear interpolation when combining model parameters, and why does this matter for preserving parameter relationships?

- Concept: Catastrophic forgetting and knowledge retention in fine-tuning
  - Why needed here: Different fine-tuning strategies have varying effects on model performance
  - Quick check question: What mechanisms allow some fine-tuning approaches to preserve existing knowledge while incorporating new domain-specific information?

- Concept: Overparameterization and generalization in neural networks
  - Why needed here: Emergent capabilities from SLERP linked to overparameterized nature of large models
  - Quick check question: How does overparameterization enable models to discover beneficial parameter combinations that smaller models cannot?

## Architecture Onboarding

- Component map: Dataset preparation → Continued Pretraining → Supervised Fine-Tuning → Preference Optimization → Model Merging → Evaluation
- Critical path: Scientific papers corpus → Structured training datasets → Fine-tuning pipeline → SLERP merging → Domain-specific benchmarks
- Design tradeoffs: Model merging provides emergent capabilities but requires computational resources; simpler approaches like LoRA are computationally efficient but may not unlock same performance gains
- Failure signatures: Poor merging performance indicates architectural incompatibility or insufficient model size; degraded performance after fine-tuning suggests catastrophic forgetting
- First 3 experiments:
  1. Implement SLERP merging between Llama Base and Llama Instruct to verify spherical interpolation path
  2. Compare model merging performance against simple parameter averaging to demonstrate emergent capabilities
  3. Test model merging across different architectures (Llama + Mistral) to identify architectural compatibility constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between model size and emergence of novel capabilities through model merging?
- Basis in paper: [explicit] Paper states very small LLMs don't feature emergent capabilities under merging, contrasting 7-8B with 1.7B parameter models
- Why unresolved: Demonstrates threshold effect but doesn't quantify relationship or determine if threshold is continuous, discrete, or dependent on other factors
- What evidence would resolve it: Systematic experiments varying model size across multiple orders of magnitude (100M to 100B parameters) while keeping other variables constant

### Open Question 2
- Question: How does geometric complexity of parameter space relate to emergence of new capabilities in merged models?
- Basis in paper: [explicit] Paper discusses how SLERP explores richer parameter space by interpolating along curved path on unit sphere
- Why unresolved: Proposes geometric interpretations but lacks quantitative analysis of parameter space geometry or its relationship to capability emergence
- What evidence would resolve it: Mathematical analysis of parameter space curvature and dimensionality in merged models, coupled with capability emergence measurements

### Open Question 3
- Question: What are optimal data quality and composition strategies for different model sizes and architectures during fine-tuning?
- Basis in paper: [inferred] Paper shows larger datasets aren't necessarily beneficial and notes data quality is major issue
- Why unresolved: Demonstrates data quality matters but doesn't provide systematic analysis of how composition should vary by model size, architecture, or training stage
- What evidence would resolve it: Controlled experiments varying data quality metrics across different model scales and architectures

## Limitations
- Analysis based on narrow materials science dataset that may not generalize to other scientific domains
- Limited comparison between model sizes (1.7B vs 7-8B) lacks intermediate sizes to identify precise threshold for emergent capabilities
- Architectural comparisons between Llama and Mistral lack deeper mechanistic explanation for different optimization responses
- Evaluation framework relies heavily on domain-specific benchmarks that may not capture broader generalization capabilities

## Confidence

**High Confidence (8-10/10):**
- SLERP merging produces emergent capabilities beyond parent models
- Model size matters for emergent capabilities
- CPT → SFT → optimization sequence improves domain adaptation

**Medium Confidence (5-7/10):**
- Llama vs Mistral architectural differences drive optimization strategy preferences
- 7-8B parameter models represent threshold for emergent capabilities
- Model merging provides consistent benefits across all tested architectures

**Low Confidence (1-4/10):**
- Specific parameter ranges for SLERP merging are optimal
- Domain-specific fine-tuning generalizes to agentic applications

## Next Checks
1. **Architecture-Agnostic SLERP Analysis**: Systematically test SLERP merging across wider range of architectures (GPT, Gemma, others) with controlled parameter ranges to determine if emergent capabilities are architecture-independent
2. **Size Threshold Investigation**: Conduct experiments with intermediate model sizes (3B, 5B) to precisely map relationship between parameter count and emergent capability emergence
3. **Cross-Domain Transfer Validation**: Evaluate merged models on completely different scientific domains (chemistry, physics, biology) to assess whether domain-specific fine-tuning creates specialized capabilities or general reasoning improvements that transfer across fields