---
ver: rpa2
title: 'LongSafety: Enhance Safety for Long-Context LLMs'
arxiv_id: '2411.06899'
source_url: https://arxiv.org/abs/2411.06899
tags:
- safety
- harmful
- uni00000003
- long-context
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces LongSafety, the first dataset designed specifically
  for aligning large language models (LLMs) with long-context safety. The dataset
  includes 17,000 samples across 8 tasks and an average length of 40.9k tokens.
---

# LongSafety: Enhance Safety for Long-Context LLMs

## Quick Facts
- arXiv ID: 2411.06899
- Source URL: https://arxiv.org/abs/2411.06899
- Authors: Mianqiu Huang; Xiaoran Liu; Shaojun Zhou; Mozhi Zhang; Qipeng Guo; Linyang Li; Chenkun Tan; Yang Gao; Pengyu Wang; Linlin Li; Qun Liu; Yaqian Zhou; Xipeng Qiu; Xuanjing Huang
- Reference count: 26
- One-line primary result: Training with LongSafety significantly improves both long-context and short-context safety performance while maintaining general capabilities

## Executive Summary
This work introduces LongSafety, the first dataset specifically designed for aligning large language models with long-context safety. The dataset includes 17,000 samples across 8 tasks with an average length of 40.9k tokens. The authors categorize long-context safety into three scenarios: query harmful, partially harmful, and fully harmful. They also propose LongSafetyBench, a comprehensive evaluation benchmark with 10 tasks, 1,000 samples, and an average length of 41.9k tokens. Experiments demonstrate that training with LongSafety significantly improves safety performance across both long and short contexts while maintaining general capabilities, and shows that long-context safety cannot be achieved simply by combining short-context safety data with long-context alignment data.

## Method Summary
The LongSafety approach uses three scalable data construction pipelines to create datasets for three long-context safety scenarios: query harmful, partially harmful, and fully harmful. These pipelines employ techniques like KeywordRAG for context retrieval, DocAttack for harmful content injection, and back-translation methods. The resulting dataset contains 17k samples across 8 tasks with an average length of 40.9k tokens. Models are fine-tuned using the InternEvo framework on A100 GPUs. Evaluation is performed using LongSafetyBench, which uses multiple-choice questions to assess HarmAwareness (ability to recognize harmful information) and SafeResponse (ability to provide safe responses).

## Key Results
- Training with LongSafety significantly improves long-context safety performance while also enhancing short-context safety
- Long-context safety cannot be achieved simply by combining short-context safety data with long-context alignment data
- LongSafety exhibits generalization across tasks and context lengths, requiring only a small amount of data for effective alignment
- Models show improvement even when tested on contexts longer than the 32k tokens used during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LongSafety dataset specifically aligns models to recognize and respond safely to harmful content within long contexts
- Mechanism: The dataset construction pipelines create three distinct scenarios (query harmful, partially harmful, fully harmful) that expose models to diverse forms of harmful content embedded within long contexts
- Core assumption: Models can learn safety behaviors from targeted training data that generalizes across different context lengths and task types
- Evidence anchors:
  - [abstract]: "training with LongSafety can enhance long-context safety performance while enhancing short-context safety"
  - [section 3.1]: "We design three data construction pipelines for these three long-context safety scenarios respectively"
  - [corpus]: Weak evidence - corpus shows related work on long-context safety but no direct evidence this specific mechanism works
- Break condition: If the model fails to generalize safety behaviors to unseen tasks or context lengths beyond training distribution

### Mechanism 2
- Claim: LongSafetyBench provides reliable evaluation of long-context safety through multiple-choice format with two metrics (HarmAwareness and SafeResponse)
- Mechanism: The evaluation format standardizes safety assessment by requiring models to select from pre-defined options representing different safety behaviors
- Core assumption: Multiple-choice format with carefully constructed options can accurately capture model safety behaviors in long-context scenarios
- Evidence anchors:
  - [abstract]: "We design the evaluation format as multiple-choice questions, ensuring stable and reliable evaluation"
  - [section 3.3]: "Following Hendrycks et al. (2020) and Zhang et al. (2024b), we design the evaluation format as multiple-choice questions"
  - [corpus]: Moderate evidence - corpus shows related work on safety evaluation but no direct evidence this specific format is reliable
- Break condition: If model performance varies significantly based on prompt phrasing or if multiple-choice format fails to capture nuanced safety behaviors

### Mechanism 3
- Claim: LongSafety dataset achieves safety alignment with minimal data due to high-quality construction and targeted design
- Mechanism: The dataset contains only 17k samples but achieves significant safety improvements, suggesting efficient learning through targeted construction for specific safety scenarios
- Core assumption: Quality and relevance of training data matters more than quantity for safety alignment in long-context scenarios
- Evidence anchors:
  - [abstract]: "LongSafety exhibits generalization across tasks and context lengths, requiring only a small amount of data for effective alignment"
  - [section 5]: "during the training process, the safety performance of the LLMs generally shows an upward trend...after 400 steps, the LLMs' safety performance stabilized"
  - [corpus]: Weak evidence - corpus shows related work on efficient training but no direct evidence this specific dataset is data-efficient
- Break condition: If performance plateaus before seeing sufficient data or if additional data fails to improve safety metrics

## Foundational Learning

- Concept: Long-context safety scenarios
  - Why needed here: Understanding the three safety scenarios (query harmful, partially harmful, fully harmful) is essential for designing effective safety datasets and evaluation benchmarks
  - Quick check question: What distinguishes "partially harmful" from "fully harmful" contexts in long-context safety?

- Concept: Data construction pipelines
  - Why needed here: The specific methods for creating each safety scenario (retrieval, injection, back-translation) determine the quality and diversity of training data
  - Quick check question: How does the "KeywordRAG" pipeline differ from "DocAttack" in terms of data construction approach?

- Concept: Safety metrics design
  - Why needed here: HarmAwareness and SafeResponse metrics must be clearly understood to interpret evaluation results and compare model performance
  - Quick check question: What distinguishes a model that achieves HarmAwareness but fails SafeResponse from one that achieves both metrics?

## Architecture Onboarding

- Component map: LongSafety consists of dataset construction pipelines feeding into training process, which outputs fine-tuned models evaluated on LongSafetyBench. Key components include context retrieval systems, harmful content injection mechanisms, and multiple-choice evaluation frameworks.

- Critical path: Context generation → Harmful content insertion → Question-answer pair creation → Model training → Evaluation with LongSafetyBench. Any failure in the pipeline (e.g., poor context retrieval or ineffective harmful content injection) will compromise the entire system.

- Design tradeoffs: The dataset balances between comprehensive coverage of safety scenarios and manageable size (17k samples). The multiple-choice evaluation format trades off nuanced response analysis for standardized, reliable assessment.

- Failure signatures: If models show high HarmAwareness but low SafeResponse, this indicates they can detect harmful content but struggle to respond appropriately. If performance degrades on longer contexts than training data, this suggests overfitting to specific context lengths.

- First 3 experiments:
  1. Train a base model on LongSafety and evaluate on LongSafetyBench to establish baseline performance improvement
  2. Compare LongSafety training with mixing short-context safety data and long-context alignment data to validate the claim that long-context safety ≠ combination approach
  3. Test model generalization by evaluating on contexts longer than 32k tokens used during training to assess length generalization capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does long-context safety alignment performance change when models are trained on context lengths exceeding their maximum context length?
- Basis in paper: [explicit] The paper notes that LongSafety contributes to enhancement of LLM's long-context safety performance with a certain level of generalizability, both in terms of tasks and context length, showing improvement even when tested on contexts longer than the tuning context length
- Why unresolved: The paper only tests on contexts up to 128k tokens, which exceeds the 32k training length, but does not explore extreme scaling to contexts like 256k or 512k tokens
- What evidence would resolve it: Testing LongSafety-trained models on significantly longer contexts (e.g., 256k-1M tokens) to measure degradation in safety performance

### Open Question 2
- Question: Does the effectiveness of LongSafety generalize to open-ended generation tasks, or is it limited to multiple-choice formats?
- Basis in paper: [inferred] The paper acknowledges a limitation that LongSafetyBench focuses on multiple-choice questions, limiting research on open-ended model generation, and notes more work is needed on metrics for evaluating model outputs
- Why unresolved: All evaluations use multiple-choice formats, leaving the performance in free-form generation scenarios unknown
- What evidence would resolve it: Evaluating LongSafety-trained models on open-ended safety tasks where they must generate responses without predefined options

### Open Question 3
- Question: What is the relative importance of context length versus task-specific safety content in achieving long-context safety alignment?
- Basis in paper: [explicit] The paper constructs LongSafety-short (LSS) with the same questions and responses as LongSafety but without the corresponding long context to explore whether the effect of long-context safety data is related to the length of the context or the task itself
- Why unresolved: While the paper shows LSS is less effective than full LongSafety, it does not quantify the exact contribution of context length versus task-specific safety content
- What evidence would resolve it: Ablation studies isolating the effects of context length and task-specific safety content through controlled experiments

## Limitations

- The evaluation relies on multiple-choice format rather than free-form responses, which may not fully capture nuanced safety behaviors in real-world scenarios
- The dataset size (17k samples) is relatively small compared to typical large-scale alignment datasets, raising questions about whether observed improvements will scale to more diverse or complex safety scenarios
- The focus on Chinese datasets and evaluation may limit generalizability to other languages and cultural contexts

## Confidence

- High confidence: The core observation that training on LongSafety improves safety metrics compared to baselines
- Medium confidence: Claims about data efficiency and generalization across tasks/context lengths
- Low confidence: The assertion that long-context safety cannot be achieved by combining existing short-context safety data with long-context alignment data

## Next Checks

1. **Cross-lingual validation**: Test whether models trained on LongSafety maintain safety improvements when evaluated on English-language harmful content in long contexts, addressing potential language-specific limitations.

2. **Open-ended response evaluation**: Supplement multiple-choice evaluation with human evaluation of free-form responses to verify that SafeResponse metric improvements translate to practical safety behaviors.

3. **Out-of-distribution context length testing**: Evaluate models on contexts exceeding 32k tokens (the maximum used in training) to empirically verify claims about length generalization capability.