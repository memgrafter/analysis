---
ver: rpa2
title: 'TracrBench: Generating Interpretability Testbeds with Large Language Models'
arxiv_id: '2409.13714'
source_url: https://arxiv.org/abs/2409.13714
tags:
- rasp
- make
- element
- sequence
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TracrBench, a dataset of 121 RASP programs
  and their corresponding transformer weights to facilitate the evaluation of interpretability
  methods for transformer-based language models. The authors attempt to use large
  language models (LLMs) to automatically generate RASP programs, which are then compiled
  into transformers with known mappings from weights to functional form.
---

# TracrBench: Generating Interpretability Testbeds with Large Language Models

## Quick Facts
- arXiv ID: 2409.13714
- Source URL: https://arxiv.org/abs/2409.13714
- Authors: Hannes Thurnherr; Jérémy Scheurer
- Reference count: 40
- Key outcome: State-of-the-art LLMs achieve only 56% pass rate on RASP program generation, with GPT-4o reaching 0.31 difficulty-weighted score out of 1.0

## Executive Summary
This paper introduces TracrBench, a dataset of 121 RASP programs and their corresponding transformer weights designed to evaluate interpretability methods for transformer-based language models. The authors investigate whether large language models can automatically generate RASP programs - a non-Turing-complete domain-specific language for modeling transformer behavior. Through systematic testing with various prompt formats and multiple LLM models, they demonstrate that even frontier models struggle with this task, achieving only modest success rates. The resulting TracrBench dataset combines manually written and LLM-generated programs to serve as a testbed for validating and comparing interpretability methods.

## Method Summary
The authors use large language models to generate RASP programs through carefully constructed prompts that include detailed language descriptions and example programs. A five-step verification pipeline evaluates generated programs by testing compilation, execution, output correctness, Tracr validation, and compiled transformer correctness. The study tests various prompt formats (zero-shot, one-shot, 20-shot) across multiple LLM models (GPT-4-turbo, GPT-4o, Claude models) and measures performance using both pass-rate and difficulty-weighted scoring metrics. The dataset consists of 121 algorithms, with 49 generated by GPT-4 and 72 manually written.

## Key Results
- GPT-4-turbo achieves only 56% pass rate on 101 test programs
- Best-performing model GPT-4o reaches 0.31 difficulty-weighted score out of 1.0 maximum
- RASP program generation remains challenging for frontier LLMs despite extensive prompt engineering
- TracrBench dataset provides 121 RASP programs with verified transformer weights for interpretability testing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can be guided to generate RASP programs through carefully constructed prompts.
- Mechanism: By providing detailed descriptions of RASP language components, example programs, and a structured prompt format, LLMs can leverage their code generation capabilities to produce syntactically valid RASP programs.
- Core assumption: The LLM has sufficient training data on similar domain-specific languages or programming concepts to understand and generate RASP code.
- Evidence anchors:
  - [abstract] The paper mentions using a 20-shot prompt with best-of-5 sampling to improve LLM performance in generating RASP programs.
  - [section] The method section describes conditioning LLMs on prompts that include a detailed description of the RASP language and relevant source code.
- Break condition: If the LLM lacks sufficient exposure to similar programming concepts or domain-specific languages in its training data, it may struggle to generate valid RASP code.

### Mechanism 2
- Claim: A five-step verification pipeline can effectively evaluate the correctness of generated RASP programs.
- Mechanism: The pipeline tests compilation, execution, output correctness, Tracr validation, transformer compilation, and compiled transformer correctness, ensuring comprehensive evaluation of the generated programs.
- Core assumption: Each step in the pipeline accurately captures a critical aspect of RASP program correctness and can be reliably implemented.
- Evidence anchors:
  - [section] The paper describes a five-step verification pipeline that tests compilation, output correctness, Tracr validation, transformer weights compilation, and compiled transformer correctness.
  - [abstract] The authors state that a program is considered correct if it passes all five steps of the verification pipeline.
- Break condition: If any step in the pipeline is not accurately implemented or fails to capture a critical aspect of RASP program correctness, the evaluation may produce false positives or negatives.

### Mechanism 3
- Claim: Difficulty-weighted scoring provides a more accurate representation of LLM performance in generating RASP programs.
- Mechanism: By weighting each successful program generation by the number of RASP functions used, the difficulty-weighted score accounts for the varying complexity of different RASP programs, providing a more nuanced evaluation metric.
- Core assumption: The number of RASP functions used in a program is a reliable proxy for its difficulty or complexity.
- Evidence anchors:
  - [section] The paper introduces a difficulty-weighted score that weights each success by the number of RASP functions in the program.
  - [section] The authors state that the number of RASP functions is a better indicator of task complexity than the number of lines of code.
- Break condition: If the number of RASP functions is not a reliable proxy for program difficulty, the difficulty-weighted score may not accurately reflect LLM performance.

## Foundational Learning

- Concept: Understanding of transformer architecture and its components
  - Why needed here: RASP is designed to model and analyze transformer behavior, so a deep understanding of transformers is crucial for working with RASP programs and interpreting their results.
  - Quick check question: Can you explain the key components of a transformer and how they relate to RASP primitives?

- Concept: Familiarity with domain-specific languages (DSLs) and their generation
  - Why needed here: RASP is a domain-specific language for modeling transformers, and the paper explores using LLMs to generate RASP programs. Understanding DSLs and their generation is essential for working with RASP and evaluating the LLM's performance.
  - Quick check question: What are the key challenges in generating programs for a domain-specific language like RASP using LLMs?

- Concept: Knowledge of interpretability methods and their evaluation
  - Why needed here: The paper introduces TracrBench as a testbed for evaluating interpretability methods, so understanding these methods and how they are evaluated is crucial for working with the dataset and interpreting the results.
  - Quick check question: What are the main challenges in evaluating interpretability methods for transformer-based models, and how does TracrBench address these challenges?

## Architecture Onboarding

- Component map:
  - LLM (e.g., GPT-4-turbo, GPT-4o, Claude models)
  - Prompt generator (creates prompts with RASP descriptions and examples)
  - RASP program verifier (implements the five-step verification pipeline)
  - Tracr compiler (compiles RASP programs into transformer weights)
  - Dataset manager (organizes and stores the generated RASP programs and compiled transformers)

- Critical path:
  1. Generate prompt with RASP descriptions and examples
  2. Condition LLM on prompt and generate RASP program
  3. Verify generated program using the five-step pipeline
  4. Compile verified RASP program into transformer weights using Tracr
  5. Store program and weights in dataset

- Design tradeoffs:
  - Number of examples in prompt (zero-shot, one-shot, 20-shot) vs. LLM performance
  - Complexity of verification pipeline vs. accuracy of program evaluation
  - Size of dataset (number of programs) vs. quality of programs
  - Use of different LLM models vs. consistency of results

- Failure signatures:
  - LLM consistently fails to generate valid RASP programs
  - Verification pipeline produces false positives or negatives
  - Tracr compilation fails for valid RASP programs
  - Dataset contains low-quality or incorrect programs

- First 3 experiments:
  1. Evaluate LLM performance with different numbers of examples in the prompt (zero-shot, one-shot, 20-shot) to determine the optimal balance between performance and prompt complexity.
  2. Test the robustness of the five-step verification pipeline by introducing controlled errors in generated RASP programs and observing the pipeline's ability to detect them.
  3. Assess the impact of different LLM models on RASP program generation performance by comparing results across models (e.g., GPT-4-turbo, GPT-4o, Claude-3-opus).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of RASP's non-Turing-complete nature make it particularly challenging for LLMs to generate correct programs?
- Basis in paper: [explicit] The paper mentions that RASP is "an unconventional, non-Turing-complete programming language that requires algorithms to be implemented differently than in standard Turing-complete languages like Python."
- Why unresolved: The paper identifies the challenge but doesn't provide a detailed breakdown of which specific features of RASP's non-Turing-completeness are most problematic for LLMs.
- What evidence would resolve it: A systematic analysis comparing LLM performance on RASP vs. similar Turing-complete languages, identifying specific operations or constructs that cause failures.

### Open Question 2
- Question: How does the performance of LLMs on RASP program generation scale with the number of training examples provided in the prompt?
- Basis in paper: [explicit] The paper tests zero-shot, one-shot, and 20-shot prompts, finding performance improvement with more examples, but doesn't explore intermediate values or the upper limit of this trend.
- Why unresolved: The paper only tests three prompt variations and doesn't explore the full spectrum of possible example counts or their diminishing returns.
- What evidence would resolve it: Testing with 5, 10, 50, and 100-shot prompts to map the learning curve and identify the optimal number of examples.

### Open Question 3
- Question: Could fine-tuning LLMs specifically on RASP code significantly improve their performance compared to few-shot prompting?
- Basis in paper: [inferred] The paper relies on few-shot prompting with limited examples, suggesting that the models have limited exposure to RASP in their training data.
- Why unresolved: The paper doesn't explore fine-tuning approaches, only testing the models' zero/few-shot capabilities.
- What evidence would resolve it: Comparing few-shot performance against a model fine-tuned on a large corpus of RASP programs.

## Limitations
- Even state-of-the-art LLMs struggle significantly with RASP program generation, suggesting fundamental limitations in current LLM capabilities
- The five-step verification pipeline may not comprehensively capture all aspects of program correctness
- The dataset's relatively small size (121 programs) may not fully represent the diversity of transformer behaviors

## Confidence
- High Confidence: The methodology for generating and verifying RASP programs is clearly described and implemented
- Medium Confidence: The claim that LLMs struggle with RASP generation is supported by empirical results, but the reasons for these limitations require further investigation
- Low Confidence: The dataset's completeness and ability to capture all relevant transformer behaviors is uncertain given its current size and scope

## Next Checks
1. **Prompt Optimization Study**: Systematically vary prompt formats, example selection, and shot counts to identify optimal prompt engineering strategies for RASP generation, testing whether performance improvements are possible through better prompting rather than fundamental LLM limitations.

2. **Pipeline Robustness Testing**: Introduce controlled errors into known-good RASP programs and evaluate the verification pipeline's ability to detect them, ensuring the pipeline doesn't produce false positives or negatives.

3. **Dataset Expansion and Diversity Analysis**: Generate additional RASP programs covering edge cases and rare transformer behaviors, then analyze whether the current dataset adequately represents the space of transformer behaviors needed for robust interpretability method evaluation.