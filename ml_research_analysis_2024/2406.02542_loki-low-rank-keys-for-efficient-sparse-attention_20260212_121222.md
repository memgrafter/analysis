---
ver: rpa2
title: 'Loki: Low-rank Keys for Efficient Sparse Attention'
arxiv_id: '2406.02542'
source_url: https://arxiv.org/abs/2406.02542
tags:
- loki
- attention
- keys
- rank
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Loki, a method to reduce computational cost
  of transformer self-attention by exploiting the low intrinsic dimensionality of
  key vectors. The authors show that key vectors in attention layers typically reside
  in a space with much lower effective rank than the full head dimension, a property
  consistent across models and datasets.
---

# Loki: Low-rank Keys for Efficient Sparse Attention

## Quick Facts
- arXiv ID: 2406.02542
- Source URL: https://arxiv.org/abs/2406.02542
- Reference count: 40
- Primary result: Achieves up to 45% speedup in transformer self-attention computation while maintaining model quality by exploiting low intrinsic dimensionality of key vectors

## Executive Summary
This paper introduces Loki, a method to accelerate transformer self-attention by leveraging the observation that key vectors typically reside in a lower-dimensional space than the full head dimension. The approach uses PCA to transform keys into a reduced-dimensional space, enabling faster approximate attention score computation for top-k token selection, followed by full-dimensional computation only for selected tokens. Loki achieves significant speedups (up to 45% for Llama2-13B) with minimal accuracy loss, outperforming other sparse attention methods like H2O while maintaining the full KV-cache without token deletion.

## Method Summary
Loki exploits the low intrinsic dimensionality of key vectors in transformer attention by computing approximate attention scores in a PCA-transformed reduced space. The method first applies PCA to key vectors from a calibration dataset, then uses the principal components to project queries and keys into a lower-dimensional space for initial attention score computation. The top-k most relevant tokens are selected based on these approximate scores, and final attention scores are computed only for these tokens in full dimensionality. Optimized Triton kernels enable efficient implementation by avoiding dense KV-cache copies and performing computations directly within GPU registers.

## Key Results
- Achieves up to 45% speedup in attention computation for Llama2-13B while maintaining model quality
- Outperforms H2O sparse attention method in both speed and accuracy metrics across multiple benchmarks
- Maintains full KV-cache without token deletion, preserving information compared to methods that evict tokens
- Shows consistent performance improvements across multiple models including Llama2, Llama3, TinyLlama, Pythia, Mistral, Mixtral, and Phi3

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Key vectors in transformer attention lie in a lower-dimensional space than the full head dimension, enabling computational savings by operating in reduced dimensions.
- **Mechanism**: PCA transforms high-dimensional key vectors into a lower-dimensional space that captures most variance (e.g., 90% at ~80 dimensions vs full 128). Attention scores computed in this reduced space can rank tokens accurately, and final scores are only computed for top-k tokens in full dimensionality.
- **Core assumption**: Intrinsic dimensionality of key vectors is consistently low across layers, heads, models, and datasets.
- **Evidence anchors**: Figure 1 shows average Rank@90 across layers for 128-dim keys, indicating significantly lower rank than full dimensionality; paper states key vectors consistently lie in lower-dimensional space across datasets and models.

### Mechanism 2
- **Claim**: Computing attention scores in reduced PCA space provides good approximation for ranking tokens, allowing selection of top-k without full attention computation.
- **Mechanism**: Lemma 4.1 proves PCA-transformed attention scores are equivalent to original for any orthogonal transform. Lemma 4.2 shows PCA minimizes reconstruction error, so first d principal components give good ranking approximation.
- **Core assumption**: Top-k most relevant tokens can be identified accurately using approximate scores from reduced dimensions.
- **Evidence anchors**: Jaccard similarity of ~0.9 between top-k tokens selected by Loki and Exact-TopK validates accurate selection; theoretical lemmas provide mathematical justification.

### Mechanism 3
- **Claim**: Optimized Triton kernels enable efficient implementation by avoiding dense KV-cache copies and enabling direct register-level access.
- **Mechanism**: Standard PyTorch operations create temporary dense copies of KV-cache data, causing slowdowns. Triton kernels directly access relevant subsets of KV-cache and perform computations within GPU registers, eliminating need for dense copies.
- **Core assumption**: Matrix multiplication kernels can be optimized to handle sparse indexing patterns efficiently without memory overhead.
- **Evidence anchors**: Paper notes standard PyTorch creates temporary dense copies leading to slowdowns; comparison with SparQ kernels shows Loki kernels are nearly 2-3× faster in certain scenarios.

## Foundational Learning

- **Concept: Principal Component Analysis (PCA)**
  - Why needed here: PCA transforms high-dimensional key vectors into lower-dimensional space while preserving most variance, enabling computational savings.
  - Quick check question: If key vectors have dimension 128 and we want to capture 90% variance, what would be the approximate rank needed based on the paper's findings?

- **Concept: Self-Attention Mechanism**
  - Why needed here: Understanding how self-attention works (query-key dot products, softmax, value weighting) is essential to grasp why dimensionality reduction of keys is beneficial.
  - Quick check question: In standard self-attention, what is the computational complexity with respect to sequence length S?

- **Concept: Rotary Position Embeddings (RoPE)**
  - Why needed here: The paper notes that rotary embeddings increase the dimensionality of keys for most models, affecting the choice between pre-rotary and post-rotary PCA transforms.
  - Quick check question: How do rotary embeddings modify the key vectors, and why might this affect dimensionality analysis?

## Architecture Onboarding

- **Component map**: Token sequence → Query, Key, Value computation → PCA projection → Reduced-dimension attention scores → Top-k selection → Full-dimension attention for selected tokens → Attention-weighted values

- **Critical path**: Token generation → Compute QKV → Project keys to reduced dimension → Compute approximate attention scores → Select top-k → Compute final attention scores → Generate next token

- **Design tradeoffs**:
  - Memory vs. speed: Loki maintains full KV-cache (no deletion) vs. methods like H2O that reduce memory but lose information
  - Accuracy vs. compression: Higher df improves accuracy but reduces speedup; higher kf improves coverage but increases computation
  - Pre-rotary vs. post-rotary: Pre-rotary often generalizes better across datasets but post-rotary matches original attention computation

- **Failure signatures**:
  - Significant perplexity increase (>0.1) indicates poor top-k selection or information loss
  - Performance degradation on specific tasks suggests calibration dataset mismatch
  - No speedup despite theoretical gains indicates bottleneck in top-k selection or memory-bound operations

- **First 3 experiments**:
  1. Implement PCA on keys from calibration dataset and verify rank distribution matches paper's findings (e.g., 90% variance at ~80 dimensions for 128-dim keys)
  2. Compare attention scores from reduced-dimension vs full-dimension for same query to validate approximation quality
  3. Benchmark top-k selection accuracy (Jaccard similarity) between Loki and Exact-TopK on sample data to confirm selection reliability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Loki vary when using different explained variance thresholds for selecting the reduced dimensionality (df) across different layers and heads?
- Basis in paper: Paper mentions some heads within layers have significantly higher rank values than others, suggesting potential for variable df policies.
- Why unresolved: Paper briefly experimented with variable df policy but did not observe significant improvements.
- What evidence would resolve it: Systematic experiments testing different explained variance thresholds for each layer and head, measuring resulting performance trade-offs.

### Open Question 2
- Question: What is the impact of using pre-rotary versus post-rotary PCA transformations on Loki's performance, and why does this differ across models?
- Basis in paper: Paper observes post-rotary PCA transformations perform significantly worse than pre-rotary ones for some models like Llama3-8B and Mistral-7B, but not for others.
- Why unresolved: Authors suggest post-rotary PCA might capture token distributions tied to specific positions while pre-rotary PCA might generalize better, but exact mechanism remains unclear.
- What evidence would resolve it: Detailed analysis of PCA-transformed key vectors and their impact on attention scores for different models.

### Open Question 3
- Question: Can Loki be effectively combined with other sparse attention methods like H2O to achieve both memory savings and computational speedup?
- Basis in paper: Paper states Loki and H2O are orthogonal methods, with Loki reducing memory bandwidth and H2O reducing memory footprint through token eviction.
- Why unresolved: While combination is theoretically possible, paper does not provide experimental evidence of its effectiveness.
- What evidence would resolve it: Experiments demonstrating performance of combined approach using both Loki and H2O.

## Limitations
- Relies heavily on assumption that key vectors exhibit low intrinsic dimensionality across all contexts, which may break down in specialized domains
- Evaluation focuses primarily on perplexity and task accuracy metrics without extensive exploration of qualitative aspects or potential artifacts
- Optimized Triton kernels are critical for reported speedups but implementation details are not fully disclosed

## Confidence

**High Confidence (8-10/10):**
- Core observation that key vectors exhibit lower intrinsic dimensionality than full head dimension is well-supported by empirical evidence
- Theoretical framework using PCA for dimensionality reduction is mathematically sound and properly justified
- Experimental results demonstrating improved speed-accuracy tradeoff compared to baselines are robust

**Medium Confidence (5-7/10):**
- Generalization of findings across different model architectures and tasks requires more extensive validation
- Optimization potential for extremely large models is suggested but not fully explored
- Impact of different calibration datasets on model performance needs further investigation

**Low Confidence (1-4/10):**
- Long-term stability and consistency of performance improvements across extended inference sessions
- Behavior on specialized domains or non-English languages
- Interaction effects between Loki's dimensionality reduction and other optimization techniques

## Next Checks

1. **Cross-Domain Robustness Test**: Implement Loki on diverse tasks including code generation, mathematical reasoning, and multilingual datasets to verify universality of low-rank key property across different domains.

2. **Memory Consumption Analysis**: Conduct detailed profiling of memory usage patterns during inference, comparing Loki against both full attention and H2O methods across varying sequence lengths to quantify memory-speed tradeoff more precisely.

3. **Calibration Dataset Sensitivity**: Systematically vary calibration dataset composition and size to determine minimum requirements for effective PCA transformation generation, and assess how different calibration strategies affect downstream task performance.