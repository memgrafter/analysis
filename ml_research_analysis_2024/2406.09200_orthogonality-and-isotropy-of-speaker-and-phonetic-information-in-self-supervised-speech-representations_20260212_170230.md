---
ver: rpa2
title: Orthogonality and isotropy of speaker and phonetic information in self-supervised
  speech representations
arxiv_id: '2406.09200'
source_url: https://arxiv.org/abs/2406.09200
tags:
- speaker
- isotropy
- representations
- phone
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cumulative Residual Variance (CRV) to measure
  the degree of orthogonality and isotropy between subspaces in self-supervised speech
  representations. The authors analyze six pre-trained models and two untrained baselines,
  finding that all trained models exhibit high orthogonality between speaker and phonetic
  subspaces.
---

# Orthogonality and isotropy of speaker and phonetic information in self-supervised speech representations

## Quick Facts
- arXiv ID: 2406.09200
- Source URL: https://arxiv.org/abs/2406.09200
- Reference count: 0
- Primary result: Self-supervised speech models exhibit high orthogonality between speaker and phonetic subspaces, with phone classification accuracy strongly correlated with subspace orthogonality and phone-centroid isotropy.

## Executive Summary
This paper introduces Cumulative Residual Variance (CRV) to measure orthogonality and isotropy between subspaces in self-supervised speech representations. The authors analyze six pre-trained models and two untrained baselines, finding that all trained models exhibit high orthogonality between speaker and phonetic subspaces. Importantly, phone classification accuracy is strongly correlated with both the degree of orthogonality and the isotropy of the phone centroids, while speaker accuracy shows weaker correlations. The isotropy of frame representations themselves did not consistently predict performance. These findings suggest that geometric properties of subspaces, especially phone-centroid isotropy, are critical for capturing phonetic information in self-supervised speech models.

## Method Summary
The study extracts frame-level representations from six pre-trained speech models (HuBERT, wav2vec 2.0, WavLM, WavLM+, Data2Vec, CPC-big) and two untrained baselines. Phone and speaker centroids are computed from these representations, then CRV values (Ph\Spk, Spk\Ph, Ph\Ph, Spk\Spk) and IsoScore are calculated to measure orthogonality and isotropy. Logistic regression classifiers are trained for speaker and phone ID using half the data for training and half for testing. Spearman rank correlations are computed between CRV/Isoscore and classification accuracies, with results compared across models and layers.

## Key Results
- All trained models show high orthogonality between speaker and phonetic subspaces, with CRV values indicating minimal overlap
- Phone classification accuracy is strongly correlated with both orthogonality (Ph\Spk) and phone-centroid isotropy (self-CRV)
- Speaker classification accuracy shows weaker correlations with geometric properties, suggesting different underlying mechanisms
- Frame-level isotropy does not consistently predict performance, unlike centroid isotropy

## Why This Works (Mechanism)

### Mechanism 1
Orthogonality between speaker and phonetic subspaces correlates with phone classification accuracy. CRV measures orthogonality by computing the area under the curve of residual variance as principal directions of one subspace are collapsed. Lower residual variance (less area under the curve) indicates higher orthogonality. This works because the principal directions of speaker and phonetic subspaces are independent enough that collapsing one's directions meaningfully affects the other's variance.

### Mechanism 2
Isotropy of phone centroids (not frame representations) correlates with phone classification accuracy. Higher isotropy means centroids are more evenly distributed across the embedding space, making them easier to separate linearly. Self-CRV quantifies this isotropy by measuring how uniformly the variance is distributed across dimensions. This works because evenly distributed class centroids are more linearly separable, which is beneficial for classification tasks.

### Mechanism 3
CRV is a more robust measure of orthogonality than cosine similarity between principal directions. CRV accounts for the explained variance in each principal direction, weighting the contribution of each pair of directions by their relative importance. Cosine similarity treats all directions equally. This works because directions with higher explained variance are more important for capturing the overall structure of the subspace.

## Foundational Learning

- **Principal Component Analysis (PCA)**: Used to find the principal directions of the speaker and phone centroids, which are then used to compute CRV. Quick check: What does the first principal component represent in a dataset?
- **Isotropic distributions**: Isotropy is a key geometric property being measured. Understanding isotropic distributions is crucial for interpreting self-CRV and IsoScore. Quick check: How does the covariance matrix of an isotropic distribution differ from a non-isotropic one?
- **Linear separability**: The paper assumes that higher orthogonality and isotropy lead to better linear separability, which is why linear classifiers are used for probing. Quick check: What geometric property of class centroids makes them more linearly separable?

## Architecture Onboarding

- **Component map**: Extract representations -> Compute centroids -> Apply PCA -> Compute CRV and isotropy -> Train classifiers -> Correlate with accuracy
- **Critical path**: Extract representations → Compute centroids → Apply PCA → Compute CRV and isotropy → Train classifiers → Correlate with accuracy
- **Design tradeoffs**: CRV vs. cosine similarity: CRV accounts for explained variance but is computationally more expensive. Frame-level vs. pooled representations: Frame-level allows analysis of information in individual embeddings but may be noisier.
- **Failure signatures**: Low correlation between CRV/isotropy and accuracy: Could indicate that the geometric properties are not the primary drivers of performance, or that the subspaces are not truly independent. High CRV but poor classification: Could indicate that the subspaces are orthogonal but not informative for the task.
- **First 3 experiments**:
  1. Compute CRV between random and trained models to verify that training increases orthogonality.
  2. Compare self-CRV of frame representations vs. centroids to confirm that centroid isotropy is more predictive.
  3. Test CRV and isotropy on a different dataset (e.g., different language or genre) to check generalizability.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do geometric properties of self-supervised speech representations generalize across different languages and genres beyond English LibriSpeech? The authors note they hope to examine how much results generalize to other genres and languages in future work, stating "We hope in future to examine how much these results generalize, by extending the analyses to other genres and languages, either using different pre-trained models or by testing these models on other data."

- **Open Question 2**: Why is the geometry of speaker information less correlated with speaker classification accuracy compared to the strong correlation between phonetic subspace geometry and phone classification accuracy? The authors note "It is less clear why the geometry of speaker information is less correlated to speaker classification, and to what extent this result is due to model training that is implicitly focused on ASR performance."

- **Open Question 3**: How do different theoretical frameworks of representational geometry (e.g., class manifold isotropy, distance between class centroids) relate to each other and to downstream task performance in self-supervised speech models? The authors mention they hope their work "may inspire further exploration of these connections" referring to connections between their geometric analyses and theoretical frameworks like that of B. Sorscher et al. which discusses four different geometric properties.

## Limitations
- The analysis is limited to English LibriSpeech data, which may not generalize to other languages or domains with different phonetic and speaker characteristics.
- The causal mechanism linking geometric properties to classification accuracy remains theoretical, despite strong empirical correlations.
- The paper doesn't fully explore alternative explanations for why speaker classification accuracy shows weaker correlations with geometric properties.

## Confidence
- **High confidence**: The correlation between phone classification accuracy and phone-centroid isotropy is consistently observed across multiple models and layers.
- **Medium confidence**: The orthogonality measurements via CRV reliably capture the independence between speaker and phonetic subspaces, though the exact relationship to performance could be more nuanced.
- **Low confidence**: The lack of consistent correlation between frame-level isotropy and performance suggests that other factors may be more important for speaker classification, but the paper doesn't fully explore alternative explanations.

## Next Checks
1. Test CRV and isotropy metrics on a different language corpus (e.g., multilingual data or non-English speech) to verify generalizability of the findings.
2. Investigate whether the correlation between phone-centroid isotropy and accuracy holds when using non-linear classifiers, to determine if linear separability is the key mechanism.
3. Conduct an ablation study where speaker information is explicitly removed from representations to see if orthogonality and isotropy increase while maintaining or improving phone classification performance.