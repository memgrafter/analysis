---
ver: rpa2
title: 'RoTHP: Rotary Position Embedding-based Transformer Hawkes Process'
arxiv_id: '2405.06985'
source_url: https://arxiv.org/abs/2405.06985
tags:
- hawkes
- process
- rothp
- temporal
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new Rotary Position Embedding-based Transformer
  Hawkes Process (RoTHP) architecture to address the sequence prediction issue and
  timestamp noise sensitivity problem in existing attention-based Hawkes process models.
  The core idea is to apply the rotary position embedding method into temporal processes,
  which makes the model only depend on the relative position and satisfy the translation
  invariance property.
---

# RoTHP: Rotary Position Embedding-based Transformer Hawkes Process

## Quick Facts
- arXiv ID: 2405.06985
- Source URL: https://arxiv.org/abs/2405.06985
- Reference count: 31
- Primary result: RoTHP outperforms RMTPP, THP, NHP, and SAHP on log-likelihood, accuracy, and RMSE metrics while demonstrating superior robustness to timestamp noise and translation

## Executive Summary
This paper introduces RoTHP (Rotary Position Embedding-based Transformer Hawkes Process), a novel architecture that addresses critical limitations in existing attention-based Hawkes process models. The core innovation applies rotary position embedding to temporal processes, enabling the model to depend only on relative positions and satisfy translation invariance properties. This approach significantly improves sequence prediction flexibility and robustness to timestamp noise compared to traditional absolute positional encoding methods.

The model demonstrates superior performance across three diverse datasets - Synthetic (generated from Hawkes processes), Financial Transactions, and StackOverflow - consistently outperforming established baselines including RMTPP, THP, NHP, and SAHP. The experimental results show not only improved predictive accuracy but also enhanced robustness under conditions of time stamp translation and Gaussian noise injection, making RoTHP a more reliable choice for real-world Hawkes process applications.

## Method Summary
The RoTHP architecture modifies traditional Transformer Hawkes Process models by replacing sinusoidal positional encoding with rotary position embedding. This key change allows the model to operate on relative temporal positions rather than absolute timestamps, providing translation invariance and improved robustness to timestamp noise. The model is trained using a combined loss function that includes log-likelihood, event prediction loss, and time prediction loss. The implementation involves integrating the rotary position embedding mechanism into the attention mechanism of the Transformer architecture, specifically modifying how temporal information is encoded and processed during both training and inference phases.

## Key Results
- RoTHP consistently outperforms RMTPP, THP, NHP, and SAHP baselines on Synthetic, Financial Transactions, and StackOverflow datasets across all three metrics (log-likelihood, accuracy, and RMSE)
- The model demonstrates superior robustness to timestamp translation and Gaussian noise compared to existing methods
- RoTHP shows better performance in sequence prediction tasks, particularly for long temporal sequences where relative positioning becomes crucial

## Why This Works (Mechanism)
The success of RoTHP stems from its fundamental shift from absolute to relative temporal positioning. By using rotary position embeddings, the model captures temporal dependencies based on the relative distance between events rather than their absolute timestamps. This approach naturally handles translation invariance - the model's predictions remain consistent even when all timestamps are shifted by a constant value. The relative positioning also makes the model more robust to timestamp noise since small perturbations in absolute times don't significantly affect the relative distances that drive the attention mechanism.

## Foundational Learning
**Hawkes Process**: A self-exciting point process where the occurrence of an event increases the probability of future events. *Why needed*: Forms the theoretical foundation for modeling asynchronous event sequences with temporal dependencies. *Quick check*: Can you explain how the intensity function depends on past events in a Hawkes process?

**Transformer Architecture**: A neural network architecture based on self-attention mechanisms that can capture long-range dependencies. *Why needed*: Provides the backbone for processing sequences of events in the Hawkes process context. *Quick check*: What are the three main components of a transformer block?

**Rotary Position Embedding**: A position encoding method that represents positions as rotations in the embedding space, making representations dependent only on relative positions. *Why needed*: Enables translation invariance and robustness to timestamp noise by focusing on relative rather than absolute temporal information. *Quick check*: How does rotary position embedding differ from sinusoidal positional encoding?

**Point Process Log-Likelihood**: The objective function used to train Hawkes process models by maximizing the likelihood of observed event sequences. *Why needed*: Provides the probabilistic framework for evaluating and training the model on temporal event data. *Quick check*: What is the relationship between intensity function and log-likelihood in point process modeling?

## Architecture Onboarding

**Component Map**: Input Events -> Rotary Position Embedding Layer -> Multi-Head Self-Attention -> Feed-Forward Network -> Output Layer (Event/Time Prediction)

**Critical Path**: The core processing path involves converting event sequences into embeddings, applying rotary position embedding to encode temporal information, passing through self-attention mechanisms that capture temporal dependencies, and finally producing predictions through output layers that estimate both event occurrence and timing.

**Design Tradeoffs**: The shift from absolute to relative positioning improves robustness but may lose some information about absolute temporal scales that could be relevant in certain applications. The model trades computational simplicity for increased robustness to timestamp perturbations.

**Failure Signatures**: The model may underperform when absolute temporal scales are genuinely important (e.g., circadian patterns), and the rotary embedding may introduce additional computational overhead compared to simpler positional encodings.

**Three First Experiments**:
1. Train RoTHP on the Synthetic dataset and compare log-likelihood with RMTPP baseline to verify basic functionality
2. Apply timestamp translation to test data and measure performance degradation compared to baselines
3. Inject Gaussian noise into timestamps and evaluate robustness metrics across all three datasets

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The paper lacks detailed hyperparameter configurations, making exact reproduction challenging
- Limited exploration of how different sequence lengths affect model performance
- Minimal discussion of computational complexity compared to baseline models

## Confidence

**Major Claim Clusters Confidence:**
- Architectural innovation and mathematical formulation: High
- Performance improvements over baselines: High
- Robustness to timestamp translation and noise: High
- Generalization to long sequences: Medium

**Major Limitations:**
- Missing detailed hyperparameter values for exact reproduction
- Limited analysis of sequence length effects
- Insufficient computational complexity comparison with baselines

## Next Checks

1. Reproduce the baseline models (RMTPP, THP, NHP, SAHP) with identical hyperparameter search to verify the claimed performance gaps are consistent across different random seeds.

2. Conduct an ablation study removing the rotary position embedding component to quantify exactly how much of the performance improvement comes from this specific innovation versus other architectural choices.

3. Test the model's behavior on datasets with varying degrees of timestamp noise (beyond just Gaussian) and different temporal patterns to better understand the robustness claims.