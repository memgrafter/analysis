---
ver: rpa2
title: The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language
  Models
arxiv_id: '2401.12117'
source_url: https://arxiv.org/abs/2401.12117
tags:
- reasoning
- visual
- arxiv
- yellow
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the nonverbal abstract reasoning capabilities
  of multi-modal large language models (MLLMs) using Raven''s Progressive Matrices
  benchmarks. The authors assess 24 open-source and closed-source MLLMs across three
  datasets: IQ50, RAVEN-S, and CCSE.'
---

# The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models

## Quick Facts
- **arXiv ID**: 2401.12117
- **Source URL**: https://arxiv.org/abs/2401.12117
- **Reference count**: 36
- **Primary result**: Closed-source MLLMs like GPT-4V outperform open-source models by up to 100% on Raven's Progressive Matrices benchmarks, with critical gaps in visual perception and reasoning across all models

## Executive Summary
This paper evaluates nonverbal abstract reasoning capabilities of multi-modal large language models using Raven's Progressive Matrices benchmarks. The authors assess 24 open-source and closed-source MLLMs across three datasets (IQ50, RAVEN-S, and CCSE), revealing significant performance gaps between model categories. Manual inspection uncovers critical shortcomings in visual perception (missing fine-grained details) and textual reasoning (overly descriptive responses) across all models. The study demonstrates that closed-source models benefit substantially from Chain-of-Thought prompting and corrective guidance, improving performance by up to 100%. These findings highlight the need for more grounded evaluations of MLLMs' reasoning abilities, even for seemingly simple tasks that humans can solve easily.

## Method Summary
The authors evaluate 24 MLLMs (12 open-source and 12 closed-source) across three Raven's Progressive Matrices datasets: IQ50 (50 images), RAVEN-S (1000 images), and CCSE (240 images). Each model is prompted to identify the missing image in 3x3 matrices using text-based answers (A-H) with Chain-of-Thought prompting. Performance is measured using accuracy and confusion matrices, with manual inspection of model responses to analyze reasoning patterns and visual perception capabilities. The study compares model performance across different dataset complexities and evaluates the impact of Chain-of-Thought prompting and corrective guidance on reasoning accuracy.

## Key Results
- Closed-source models like GPT-4V significantly outperform open-source models, with performance gaps reaching 100% in some cases
- Visual perception deficiencies are universal across all models, with fine-grained details often missed in complex images
- Chain-of-Thought prompting and corrective guidance improve closed-source model performance by up to 100%
- Overly descriptive reasoning patterns and hallucinations are common across all models, regardless of performance level

## Why This Works (Mechanism)
The evaluation framework works by systematically testing MLLMs' ability to recognize patterns and infer missing elements in visual matrices. The Raven's Progressive Matrices benchmark provides controlled complexity progression, allowing researchers to isolate specific reasoning capabilities. Chain-of-Thought prompting enables step-by-step reasoning that can be analyzed for both correct and incorrect patterns. The combination of quantitative accuracy metrics and qualitative manual inspection provides comprehensive insight into both performance and underlying reasoning processes.

## Foundational Learning

**Raven's Progressive Matrices**: Non-verbal intelligence test using 3x3 visual matrices with one missing element
- *Why needed*: Provides standardized benchmark for abstract visual reasoning without language dependence
- *Quick check*: Can the model identify patterns in 2x2 matrices before attempting 3x3?

**Chain-of-Thought prompting**: Technique where models generate intermediate reasoning steps before final answers
- *Why needed*: Enables analysis of reasoning process and can improve complex problem-solving
- *Quick check*: Does the model produce coherent intermediate steps that logically connect to final answers?

**Multi-modal integration**: Models combining visual and textual processing capabilities
- *Why needed*: Essential for tasks requiring both image understanding and abstract reasoning
- *Quick check*: Can the model accurately describe visual elements before attempting reasoning?

## Architecture Onboarding

**Component map**: Visual encoder -> Multi-modal transformer -> Textual reasoning layer -> Output generator
**Critical path**: Image input → Visual feature extraction → Cross-modal attention → Sequential reasoning → Answer generation
**Design tradeoffs**: Visual detail preservation vs. computational efficiency; reasoning depth vs. response conciseness
**Failure signatures**: Missing fine-grained details, overly descriptive reasoning, pattern recognition errors, hallucinations
**3 first experiments**:
1. Test visual perception by asking models to describe fine-grained details in complex images
2. Evaluate basic pattern recognition in simplified 2x2 matrices before 3x3
3. Compare performance with and without Chain-of-Thought prompting on simple reasoning tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gaps may be influenced by differences in training data exposure and fine-tuning approaches between open-source and closed-source models
- The evaluation focuses primarily on Raven's Progressive Matrices, representing only one domain of abstract visual reasoning
- Results may be partially attributable to specific prompt formulations rather than fundamental architectural differences

## Confidence

**High confidence**:
- Visual perception deficiencies (missing fine-grained details) are consistently observed across multiple models and datasets
- Overly descriptive reasoning patterns are common across all models regardless of performance level

**Medium confidence**:
- 100% performance gap between top closed-source and open-source models, as this may be influenced by specific prompt formulations
- Effectiveness of Chain-of-Thought prompting and corrective guidance, as improvements may not generalize to all abstract reasoning tasks

## Next Checks

1. Conduct cross-dataset validation using non-RPM abstract reasoning benchmarks (e.g., progressive matrices with different rules or visual reasoning tasks) to verify if observed patterns generalize

2. Perform ablation studies comparing model performance with identical prompting strategies across all models to isolate the impact of architectural differences versus engineering approaches

3. Implement fine-grained error analysis categorizing mistakes into visual perception failures, reasoning errors, and language generation issues to better understand the root causes of performance limitations