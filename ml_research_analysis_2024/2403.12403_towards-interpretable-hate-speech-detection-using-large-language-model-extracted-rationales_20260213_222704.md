---
ver: rpa2
title: Towards Interpretable Hate Speech Detection using Large Language Model-extracted
  Rationales
arxiv_id: '2403.12403'
source_url: https://arxiv.org/abs/2403.12403
tags:
- hate
- speech
- rationales
- arxiv
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of making hate speech detection
  models interpretable. The authors propose SHIELD, a framework that leverages Large
  Language Models (LLMs) like ChatGPT to extract textual features and rationales from
  input text.
---

# Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales

## Quick Facts
- arXiv ID: 2403.12403
- Source URL: https://arxiv.org/abs/2403.12403
- Authors: Ayushi Nirmal; Amrita Bhattacharjee; Paras Sheth; Huan Liu
- Reference count: 8
- Primary result: SHIELD retains hate speech detection performance while enabling interpretability through LLM-extracted rationales

## Executive Summary
This paper addresses the challenge of making hate speech detection models interpretable without sacrificing accuracy. The authors propose SHIELD, a framework that leverages Large Language Models (LLMs) like ChatGPT to extract textual features and rationales from input text. These LLM-extracted rationales are then used to augment the training of a base hate speech detector, such as HateBERT, to facilitate faithful interpretability. The study evaluates the goodness of the LLM-extracted rationales by comparing them with human-annotated rationales and measures the alignment using similarity metrics.

## Method Summary
The SHIELD framework uses LLMs as feature extractors to generate rationales, derogatory language, and cuss words from input text in structured JSON format. These features are embedded using BERT and concatenated with the base detector's embeddings before classification through an MLP layer. The approach avoids task-specific fine-tuning of the LLM by leveraging its instruction-following capabilities, while the feature embedding model is frozen during training to simplify the process.

## Key Results
- SHIELD performs at par with base models (HateBERT, BERT, RoBERTa) on hate speech detection while adding interpretability
- LLM-extracted rationales show significant overlap and high semantic similarity with human-annotated rationales
- The framework successfully mitigates the typical accuracy-interpretability tradeoff in hate speech detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-extracted rationales align well with human judgment and retain detection performance
- Mechanism: LLM extracts rationales, derogatory language, and cuss words as features, which are embedded and concatenated with base detector embedding before classification
- Core assumption: LLMs can identify task-relevant features without fine-tuning, and these features are more discriminative than raw text
- Evidence anchors: High semantic similarity between LLM and human rationales, SHIELD performs at par with base model
- Break condition: If LLM fails to extract coherent or relevant rationales, or if rationales are noisy

### Mechanism 2
- Claim: SHIELD mitigates accuracy-interpretability tradeoff by augmenting rather than replacing base detector
- Mechanism: Concatenates LLM-extracted feature embeddings with base detector's embeddings, preserving contextual information while adding interpretability
- Core assumption: Concatenated embeddings preserve sufficient discriminative power while enabling rationale-based interpretability
- Evidence anchors: SHIELD shows "surprising retention of detector performance even after training to ensure interpretability"
- Break condition: If feature embedding model fails to encode rationales meaningfully, or concatenation dilutes contextual signals

### Mechanism 3
- Claim: LLM's instruction-following capability allows rationale extraction without task-specific fine-tuning
- Mechanism: Carefully crafted prompts extract features in structured JSON format, enabling consistent automated rationale generation
- Core assumption: Instruction-tuned LLMs generalize well enough to extract task-relevant features from new data without further fine-tuning
- Evidence anchors: Unlike extractor models in prior work, SHIELD does not require additional task-specific fine-tuning
- Break condition: If LLM fails to follow instructions consistently or prompt engineering is insufficient

## Foundational Learning

- Concept: Prompt engineering for structured output
  - Why needed here: LLM must output rationales, derogatory language, and cuss words in consistent JSON format for downstream processing
  - Quick check question: What are the key components of the prompt to ensure structured JSON output from the LLM?

- Concept: Embedding fusion strategies
  - Why needed here: Framework concatenates two different embeddings (base detector + feature extractor) before classification
  - Quick check question: Why does SHIELD use concatenation instead of weighted averaging or another fusion method?

- Concept: Trade-off between interpretability and accuracy
  - Why needed here: Paper claims SHIELD maintains performance while adding interpretability, which is counterintuitive in most ML contexts
  - Quick check question: How does SHIELD's design help avoid the typical accuracy loss when adding interpretability?

## Architecture Onboarding

- Component map: LLM Feature Extractor → Feature Embedding Model (BERT) → Concatenation → MLP → Binary Classifier; Base Hate Speech Detector (HateBERT) → Embedding (CLS token) → Concatenation → MLP → Binary Classifier
- Critical path: LLM extraction → Feature embedding → Concatenation → MLP → Classification
- Design tradeoffs: Using LLM as feature extractor avoids task-specific fine-tuning but introduces dependency on external API; concatenation preserves base detector context but increases embedding dimensionality; freezing feature embedding model simplifies training but may miss task-specific nuances
- Failure signatures: LLM outputs malformed or irrelevant rationales → downstream embeddings are noisy; concatenation causes overfitting → performance drops on test sets; base detector embedding is too dominant → rationales have minimal impact
- First 3 experiments: 1) Test LLM prompt with sample inputs to ensure consistent JSON output structure; 2) Verify feature embedding quality by comparing cosine similarity between LLM and human rationales; 3) Train SHIELD on small dataset and compare accuracy vs. base HateBERT to confirm performance retention

## Open Questions the Paper Calls Out

- Question: How can we evaluate the faithfulness of SHIELD beyond comparing LLM-extracted rationales with human-annotated rationales?
- Basis in paper: Authors state future work could explore better ways to evaluate faithfulness of resulting detector, as current approach only verifies goodness of extracted rationales by comparing with ground truth for one dataset
- Why unresolved: Paper acknowledges need for better automated ways to evaluate and verify quality of LLM-extracted rationales, but does not provide concrete solution
- What evidence would resolve it: Study comparing multiple evaluation metrics for SHIELD's faithfulness, including human evaluation, automated metrics, and cross-dataset validation

## Limitations

- Core claims rely heavily on LLM's ability to extract meaningful rationales without task-specific fine-tuning, which may not generalize to all hate speech contexts
- Dependency on external LLM APIs introduces potential variability in rationale quality based on prompt interpretation, model updates, or API constraints
- Frozen feature embedding model may limit framework's ability to adapt to dataset-specific nuances in rationale representation

## Confidence

- **High confidence**: Retention of detection performance when adding interpretability is well-supported by experimental results across multiple datasets
- **Medium confidence**: Claim that LLM-extracted rationales align well with human judgment is supported by similarity metrics but qualitative analysis is limited
- **Medium confidence**: Mechanism of using LLMs as feature extractors without fine-tuning is theoretically sound but practical robustness remains untested

## Next Checks

1. **Prompt robustness test**: Systematically vary LLM prompt parameters (temperature, wording, structure) and measure stability of extracted rationales across different configurations
2. **Cross-dataset generalizability**: Apply SHIELD to hate speech dataset from different domain (e.g., news comments or forum discussions) not included in original evaluation
3. **Ablation study on feature fusion**: Replace concatenation strategy with alternative fusion methods (weighted averaging, attention-based merging) to determine optimal design choices