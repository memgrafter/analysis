---
ver: rpa2
title: An Entailment Tree Generation Approach for Multimodal Multi-Hop Question Answering
  with Mixture-of-Experts and Iterative Feedback Mechanism
arxiv_id: '2412.05821'
source_url: https://arxiv.org/abs/2412.05821
tags:
- entailment
- tree
- question
- fact
- answering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces an entailment tree generation approach for
  multimodal multi-hop question answering that addresses two key challenges: (1) redundant
  information in retrieved evidence misleading predictions, and (2) lack of interpretable
  reasoning steps for complex questions. The method innovatively treats multimodal
  multi-hop QA as a joint entailment tree generation and question answering problem,
  using a multi-task learning framework with mixture-of-experts to share knowledge
  across tasks while preventing interference.'
---

# An Entailment Tree Generation Approach for Multimodal Multi-Hop Question Answering with Mixture-of-Experts and Iterative Feedback Mechanism

## Quick Facts
- **arXiv ID**: 2412.05821
- **Source URL**: https://arxiv.org/abs/2412.05821
- **Reference count**: 30
- **Primary result**: State-of-the-art performance on WebQA dataset with QA score of 84.1 and competitive results on MultimodalQA

## Executive Summary
This paper introduces an entailment tree generation approach for multimodal multi-hop question answering that addresses two key challenges: redundant information in retrieved evidence misleading predictions, and lack of interpretable reasoning steps for complex questions. The method innovatively treats multimodal multi-hop QA as a joint entailment tree generation and question answering problem, using a multi-task learning framework with mixture-of-experts to share knowledge across tasks while preventing interference. An iterative feedback mechanism further refines both tasks by regenerating entailment trees based on joint training results. The approach achieved state-of-the-art results on the WebQA dataset and competitive performance on MultimodalQA, demonstrating effectiveness in both accuracy and interpretability.

## Method Summary
The approach consists of two main stages: Entailment Tree Initialization and Iterative Mixture-of-Experts Optimization. First, the system decomposes multi-hop questions and uses GPT-3.5 to generate an initial entailment tree structure. This tree is then converted to text description and fed into a shared encoder. The Mixture-of-Experts (MoE) architecture with two gating networks and specialized experts for fact retrieval generation and question answering performs joint learning on these tasks. An iterative feedback mechanism uses the small model's joint outputs to identify and correct errors in the LLM's initial tree generation, regenerating the entailment tree for further refinement. The final answer is generated through this iterative process.

## Key Results
- Achieved state-of-the-art performance on WebQA with QA score of 84.1, outperforming baselines by 3.1 points
- Competitive performance on MultimodalQA with EM score of 43.3 and F1 score of 53.3
- Significant improvement in clue retrieval F1 score (60.1 vs 43.2 for the best baseline)
- Demonstrated effectiveness in handling complex multi-hop questions (3+ hops) where other methods struggle

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning with mixture-of-experts enables effective knowledge sharing while preventing interference between tasks.
- Mechanism: The mixture-of-experts model contains two gating networks that select experts specifically for fact retrieval generation and question answering tasks, plus a shared expert network that allows interaction between tasks. This architecture enables specialized processing while maintaining beneficial cross-task knowledge transfer.
- Core assumption: The gating mechanism can effectively route tokens to the appropriate experts based on task requirements.
- Evidence anchors:
  - [abstract] "Specifically, we design a multi-task learning framework with a focus on facilitating common knowledge sharing across interpretability and prediction tasks while preventing task-specific errors from interfering with each other via mixture of experts."
  - [section] "We use multi-task learning with smaller models, and use an iterative feedback mechanism to re-predict the structure of the entailment tree based on the leaf nodes and the answers predicted by the small model. Inspired by the idea of [9, 23, 24] , to facilitate mutual enhancement among the small models' multi-task learning, we employ a shared multi-task mixture-of-experts model, allowing interactions between the fact selection and supervised QA tasks as guidance for LLM."
  - [corpus] Weak - corpus contains related work on mixture-of-experts but no direct evidence about this specific task combination.
- Break condition: If the gating mechanism fails to properly distinguish between task types, or if task interference becomes too significant despite the separation.

### Mechanism 2
- Claim: Iterative feedback mechanism corrects errors in the initial entailment tree structure by leveraging joint task results.
- Mechanism: After the initial entailment tree is generated by the LLM, the system uses the joint outputs of fact retrieval generation and question answering to identify and correct errors in leaf node selection and tree structure. The corrected information is then fed back to the LLM to regenerate the entailment tree.
- Core assumption: The small model's joint learning outputs contain sufficient information to identify and correct errors in the LLM's initial tree generation.
- Evidence anchors:
  - [abstract] "Afterward, we design an iterative feedback mechanism to further enhance both tasks by feeding back the results of the joint training to the LLM for regenerating entailment trees, aiming to iteratively refine the potential answer."
  - [section] "Due to the possible leaf node selection errors and entailment tree structural errors in the previously mentioned initialized entailment tree, we use a hybrid expert model to jointly learn the fact retrieval generation task (retrieving leaf nodes) and question answering tasks, and correct the entailment tree structure through an iterative feedback mechanism."
  - [corpus] Weak - corpus mentions iterative approaches but no direct evidence about this specific feedback mechanism.
- Break condition: If the small model cannot identify errors in the LLM's output, or if the feedback loop fails to converge to a correct solution.

### Mechanism 3
- Claim: Decomposing multi-hop questions into sub-questions and generating focused image captions reduces information redundancy.
- Mechanism: The system decomposes the original multi-hop question into sub-questions for each evidence source, then uses these sub-questions as prompts for image captioning and table processing. This focused approach generates only relevant information rather than converting all multimodal data indiscriminately.
- Core assumption: Decomposing questions before processing multimodal evidence will generate more relevant and less redundant information.
- Evidence anchors:
  - [abstract] "we propose a unified LLMs-based approach but without heavily relying on them due to the LLM's potential errors, and innovatively treat multimodal multi-hop question answering as a joint entailment tree generation and question answering problem."
  - [section] "Decompose Multi-Hop Question First, we need to retrieve the multimodal evidence required to answer the multi-hop question. However, since our method generates image captions by decomposing the question, we use the global image caption and image attribute features to retrieve evidences..."
  - [corpus] Weak - corpus contains related work on decomposition but no direct evidence about this specific approach to multimodal information processing.
- Break condition: If question decomposition introduces its own errors, or if the focused information is insufficient for answering the question.

## Foundational Learning

- Concept: Entailment tree generation
  - Why needed here: Provides interpretable reasoning steps that show how conclusions follow from evidence, addressing the challenge of lacking interpretability in multimodal reasoning.
  - Quick check question: Can you explain how an entailment tree differs from a simple chain of reasoning steps?

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Enables efficient multi-task learning by activating only relevant experts for each task while sharing common knowledge, crucial for handling both fact retrieval and question answering.
  - Quick check question: How does the top-K selection mechanism in MoE help with computational efficiency?

- Concept: Iterative refinement and feedback loops
  - Why needed here: Allows the system to correct errors in initial outputs by leveraging results from joint learning tasks, essential for improving both accuracy and interpretability.
  - Quick check question: What are the key conditions that would cause an iterative feedback loop to fail to converge?

## Architecture Onboarding

- Component map:
  Fact Base Construction Module -> Entailment Tree Generation Module -> Shared Encoder -> Mixture-of-Experts -> FRG Decoder and QA Decoder -> Iterative Feedback Mechanism -> LLM Regenerator

- Critical path:
  1. Question decomposition and fact base construction
  2. Initial entailment tree generation by LLM
  3. Joint learning with MoE for fact retrieval and QA
  4. Iterative feedback to refine tree structure
  5. Final answer generation

- Design tradeoffs:
  - Accuracy vs. interpretability: The entailment tree adds interpretability but may introduce additional complexity
  - Computation cost vs. performance: Iterative refinement improves results but increases computation time
  - Model complexity vs. generalization: MoE architecture is complex but enables better task-specific performance

- Failure signatures:
  - Entailment tree structure remains incorrect after multiple iterations
  - Fact retrieval performance degrades despite MoE training
  - Question decomposition produces irrelevant sub-questions
  - Iterative feedback loop fails to converge or causes performance degradation

- First 3 experiments:
  1. Baseline comparison without entailment tree generation to measure interpretability impact
  2. Ablation study removing MoE components to assess contribution of mixture-of-experts
  3. Iteration count analysis to determine optimal number of feedback cycles

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several significant unanswered questions emerge from the research:

1. How does the iterative feedback mechanism's effectiveness scale with increasing numbers of refinement iterations beyond the current two-round limit?

2. How does the entailment tree generation approach perform on datasets with more complex reasoning patterns (beyond the 11.3% of questions with 3+ hops mentioned)?

3. How does the method's performance change when using different VQA models for image fact generation instead of LLaVA-1.5?

## Limitations

- The approach relies heavily on GPT-3.5 for multiple components (question decomposition, entailment tree generation, and iterative feedback), making it dependent on external LLM services and potentially expensive to scale.
- The method shows limited effectiveness on complex reasoning questions, with the paper acknowledging that "most data in current multi-modal multi-hop question answering datasets lacks complex reasoning."
- The iterative feedback mechanism only uses 2 iteration rounds without exploring whether more iterations would yield better results or when diminishing returns occur.

## Confidence

- **High confidence**: The experimental results showing state-of-the-art performance on WebQA and competitive results on MultimodalQA are well-documented and verifiable.
- **Medium confidence**: The effectiveness of the mixture-of-experts architecture for this specific task combination, as the paper references related work but lacks direct evidence of this particular implementation.
- **Low confidence**: The iterative feedback mechanism's ability to consistently correct errors in entailment tree generation, given the limited explanation of convergence criteria and potential failure modes.

## Next Checks

1. Conduct an ablation study removing the iterative feedback mechanism to quantify its contribution to overall performance and assess whether it consistently improves results across different question types.

2. Perform detailed error analysis on the entailment tree generation component, examining cases where the tree structure fails to capture the correct reasoning path despite multiple iterations.

3. Evaluate the approach on a held-out dataset or domain not seen during training to assess whether the mixture-of-experts architecture generalizes beyond the specific WebQA and MultimodalQA datasets.