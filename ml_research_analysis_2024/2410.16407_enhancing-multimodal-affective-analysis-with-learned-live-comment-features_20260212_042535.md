---
ver: rpa2
title: Enhancing Multimodal Affective Analysis with Learned Live Comment Features
arxiv_id: '2410.16407'
source_url: https://arxiv.org/abs/2410.16407
tags:
- live
- video
- comments
- multimodal
- comment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of limited availability of live
  comments (Danmaku) across video platforms, which restricts their use in multimodal
  affective analysis. To overcome this, the authors construct LCAffect, a large-scale
  dataset containing live comments for English and Chinese videos spanning diverse
  genres.
---

# Enhancing Multimodal Affective Analysis with Learned Live Comment Features

## Quick Facts
- arXiv ID: 2410.16407
- Source URL: https://arxiv.org/abs/2410.16407
- Authors: Zhaoyuan Deng; Amith Ananthram; Kathleen McKeown
- Reference count: 14
- Primary result: State-of-the-art performance on sentiment analysis, emotion recognition, and sarcasm detection using synthetic live comment features

## Executive Summary
This paper addresses the challenge of limited availability of live comments (Danmaku) across video platforms, which restricts their use in multimodal affective analysis. To overcome this, the authors construct LCAffect, a large-scale dataset containing live comments for English and Chinese videos spanning diverse genres. They then train a contrastive video encoder to produce synthetic live comment features for any video. The resulting model, augmented with these synthetic features, achieves state-of-the-art performance on sentiment analysis, emotion recognition, and sarcasm detection tasks.

## Method Summary
The paper constructs LCAffect, a large-scale dataset containing live comments for English and Chinese videos spanning diverse genres. A video encoder is trained using contrastive learning to produce synthetic live comment features that capture affective context. These synthetic features are then concatenated with multimodal features (text, acoustic, visual) and processed through a fusion encoder to improve affective task performance. The approach is evaluated on sentiment analysis, emotion recognition, and sarcasm detection tasks, demonstrating significant improvements over state-of-the-art methods.

## Key Results
- 3.18-point increase in accuracy on CH-SIMS v2
- 2.89-point increase in F1 on MELD
- 3.0-point increase in F1 on MuSTARD

## Why This Works (Mechanism)

### Mechanism 1
The contrastive pre-training aligns video segments with their corresponding live comments in a shared embedding space. A video-to-live comment (V2LC) encoder is trained to maximize the cosine similarity between video segment embeddings and live comment embeddings using a CLIP-style contrastive objective. Core assumption: Live comments and their corresponding video segments share a strong semantic relationship that can be captured through contrastive learning.

### Mechanism 2
Synthetic live comment features capture affective context that enhances multimodal affective analysis. The synthetic live comment features, produced by the V2LC encoder, are concatenated with multimodal features (text, acoustic, visual) and processed through a fusion encoder to improve affective task performance. Core assumption: Live comments contain affective information that complements and enriches the information from text, acoustic, and visual modalities.

### Mechanism 3
The large-scale, diverse LCAffect dataset enables learning of broad affective patterns that generalize across different video genres and languages. By including diverse video content from multiple platforms and genres, the dataset provides rich affective contexts that the V2LC encoder can learn from during pre-training. Core assumption: A diverse dataset is necessary to learn affective patterns that generalize beyond the specific content seen during pre-training.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To align video segments with live comments in a shared embedding space without requiring labeled pairs
  - Quick check question: What is the primary objective function used in contrastive learning for alignment tasks?

- Concept: Multimodal fusion
  - Why needed here: To combine information from text, acoustic, visual, and synthetic live comment modalities for affective analysis
  - Quick check question: How does the cross-modality encoder integrate features from multiple modalities?

- Concept: Affective computing
  - Why needed here: To understand and categorize the emotional content and dynamics of videos using multimodal information
  - Quick check question: What are the key differences between sentiment analysis, emotion recognition, and sarcasm detection?

## Architecture Onboarding

- Component map: Data collection pipeline (LCAffect dataset) -> Video-to-Live Comment (V2LC) encoder for synthetic feature generation -> Multimodal Fusion Encoder for combining original and synthetic features -> Downstream task-specific classifiers

- Critical path: V2LC encoder → Synthetic live comment features → Multimodal Fusion Encoder → Downstream task performance

- Design tradeoffs:
  - Segment length (8 seconds) balances context preservation with computational efficiency
  - Number of live comments per segment (5) balances richness of affective information with training efficiency
  - Choice of pre-trained encoders (Chinese-RoBERTa, XLM-RoBERTa, TimeSformer) affects feature quality and cross-lingual capabilities

- Failure signatures:
  - Poor downstream performance despite successful pre-training (synthetic features not useful)
  - Degraded performance when adding synthetic features (features introduce noise)
  - Limited generalization across different video genres or languages

- First 3 experiments:
  1. Train V2LC encoder and evaluate similarity between video segments and their corresponding live comments using nearest neighbor retrieval
  2. Compare performance of downstream affective analysis tasks with and without synthetic live comment features
  3. Test cross-lingual transfer by evaluating the bilingual V2LC encoder on English datasets

## Open Questions the Paper Calls Out

### Open Question 1
How do synthetic live comment features perform on video genres not well-represented in the LCAffect dataset, such as sports or educational content? The paper notes that LCAffect includes TV shows, movies, and user-generated content but does not explicitly test on sports or educational videos, which may have different emotional dynamics.

### Open Question 2
Can the synthetic live comment features generalize to video platforms outside of China and Japan where live comments are not culturally prevalent? The authors highlight that live comments are rare outside China and Japan, motivating their synthetic approach, but do not test cross-cultural generalization.

### Open Question 3
How does the quality of synthetic live comment features degrade as video content deviates from the emotional intensity and diversity of the pre-training data? The paper constructs LCAffect to include diverse emotional content, but does not analyze performance degradation for low-emotion or highly specialized content.

## Limitations
- The paper doesn't provide detailed statistics about the distribution of affective content across genres or languages in LCAffect
- The paper demonstrates downstream performance improvements but doesn't directly evaluate whether synthetic live comment features meaningfully resemble actual live comments
- The model is trained on specific platforms and evaluated on specific datasets, but generalization to other video platforms is not addressed

## Confidence
- **High confidence**: Experimental results showing performance improvements on CH-SIMS v2, MELD, and MuSTARD are well-documented
- **Medium confidence**: The contrastive learning mechanism for aligning video segments with live comments is plausible but implementation details are sparse
- **Low confidence**: The claim about synthetic features capturing affective context is supported primarily by downstream performance rather than direct evaluation

## Next Checks
1. Conduct a human evaluation study where annotators assess whether synthetic live comment features produced by the V2LC encoder capture meaningful affective content comparable to actual live comments for the same video segments
2. Systematically vary the diversity of the pre-training dataset to measure the impact on downstream task performance and assess whether diversity is truly driving generalization
3. Evaluate the trained V2LC encoder on live comment data from platforms not seen during pre-training to assess how well the synthetic features generalize across different live comment cultures and formats