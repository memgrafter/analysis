---
ver: rpa2
title: 'Good Teachers Explain: Explanation-Enhanced Knowledge Distillation'
arxiv_id: '2402.03119'
source_url: https://arxiv.org/abs/2402.03119
tags:
- teacher
- explanations
- e2kd
- b-cos
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Knowledge distillation typically matches only logit distributions
  between teacher and student models, but this often fails to transfer the teacher's
  full functionality. This work introduces explanation-enhanced knowledge distillation
  (e2KD), which additionally matches model explanations (e.g., GradCAM or B-cos) during
  training.
---

# Good Teachers Explain: Explanation-Enhanced Knowledge Distillation

## Quick Facts
- arXiv ID: 2402.03119
- Source URL: https://arxiv.org/abs/2402.03119
- Reference count: 40
- Primary result: Matching model explanations during knowledge distillation significantly improves student accuracy, agreement, and interpretability, especially with limited data.

## Executive Summary
Knowledge distillation typically matches only logit distributions between teacher and student models, but this often fails to transfer the teacher's full functionality. This work introduces explanation-enhanced knowledge distillation (e2KD), which additionally matches model explanations (e.g., GradCAM or B-cos) during training. This simple, parameter-free approach significantly improves both accuracy and student-teacher agreement, especially when training data is limited. Experiments on ImageNet, Waterbirds-100, and PASCAL VOC show that e2KD helps students learn to be "right for the right reasons" and maintain interpretability. Notably, e2KD also transfers architectural priors (e.g., from CNNs to ViTs) and remains effective even with approximate frozen explanations, making it robust across model architectures and training regimes.

## Method Summary
e2KD extends standard knowledge distillation by adding an explanation matching term to the loss function. During training, the student model is optimized to match both the teacher's predicted class probabilities and the spatial attention patterns revealed by model explanations like GradCAM or B-cos. The combined loss function is L = L_KD + λL_exp, where L_KD is the standard distillation loss and L_exp maximizes the similarity between teacher and student explanations for the teacher's predicted class. This approach is parameter-free and can use frozen teacher explanations for computational efficiency.

## Key Results
- e2KD consistently improves student accuracy over vanilla knowledge distillation across multiple datasets and architectures
- The method significantly increases student-teacher agreement while maintaining interpretability
- e2KD successfully transfers architectural priors (e.g., shift equivariance) from CNN teachers to ViT students
- The approach is particularly effective when training data is limited and helps models learn "right for the right reasons" on biased datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: e2KD improves student accuracy and agreement by aligning not just logits but also the spatial attention patterns of teacher and student models.
- Mechanism: During distillation, the student is trained to match the teacher's GradCAM or B-cos explanations for the teacher's predicted class, thereby enforcing that both models focus on the same input features for each prediction.
- Core assumption: GradCAM and B-cos explanations meaningfully reflect the internal reasoning of the model and are differentiable so they can be used as a training signal.
- Evidence anchors:
  - [abstract] "explanation-enhanced knowledge distillation (e2KD), which additionally matches model explanations (e.g., GradCAM or B-cos) during training."
  - [section] "Lexp = 1 − sim (E(T, x, yˆT ), E(S, x, yˆT ))" and "we maximize the similarity between the models' explanations."
  - [corpus] Weak: neighbors discuss saliency or attention alignment but not direct explanation matching during KD.
- Break condition: If explanations do not correlate with actual model reasoning or are non-differentiable, the loss term becomes ineffective.

### Mechanism 2
- Claim: e2KD promotes learning the "right" features by transferring the teacher's guided attention from biased training data to the student.
- Mechanism: When the teacher has been explicitly guided to ignore spurious correlations (e.g., background in Waterbirds-100), matching its explanations forces the student to also focus on the correct features even under distribution shift.
- Core assumption: The teacher's explanations accurately encode the desired feature focus, and the student can generalize this focus from limited data.
- Evidence anchors:
  - [section] "we use a teacher that has explicitly been guided to use the bird features instead of the background" and "e2KD significantly improves performance over KD."
  - [abstract] "ensures that students learn to be 'right for the right reasons'."
  - [corpus] Weak: neighbors mention attention transfer but not explicitly for removing spurious correlations.
- Break condition: If the teacher's explanations are themselves biased or noisy, the student will inherit incorrect feature focus.

### Mechanism 3
- Claim: e2KD can transfer architectural priors (e.g., shift equivariance) from teacher to student by matching explanation patterns across transformed inputs.
- Mechanism: When explanations of the teacher are robust to small image shifts, matching them during distillation encourages the student to develop similar robustness, even if its architecture (e.g., ViT) would not naturally have it.
- Core assumption: Explanation patterns are consistent under transformations for models with the desired prior, and the student can approximate this behavior.
- Evidence anchors:
  - [section] "we find that by learning from a CNN teacher under e2KD, the explanations of a ViT student without convolutions also become largely equivariant to image shifts."
  - [abstract] "Notably, e2KD also transfers architectural priors (e.g., from CNNs to ViTs)."
  - [corpus] Weak: neighbors do not discuss explanation-based transfer of architectural inductive biases.
- Break condition: If the student architecture fundamentally cannot approximate the teacher's explanation dynamics, the prior will not transfer.

## Foundational Learning

- Concept: GradCAM and B-cos explanations
  - Why needed here: They provide differentiable spatial attention maps that can be matched during training to enforce feature focus similarity.
  - Quick check question: Can you implement a function that takes a model, input, and class index and returns a GradCAM heatmap?

- Concept: Knowledge distillation loss (logit matching)
  - Why needed here: e2KD builds on standard KD by adding an explanation matching term; understanding the base loss is essential to implement the extension.
  - Quick check question: Given teacher and student logits, can you write the KL divergence loss used in vanilla KD?

- Concept: Distribution shift and spurious correlations
  - Why needed here: e2KD's benefit on biased datasets relies on recognizing when models use incorrect features and correcting them via explanation matching.
  - Quick check question: In a binary classification with background bias, how would you test if a model relies on the background instead of the object?

## Architecture Onboarding

- Component map: Teacher model (frozen during distillation) -> Explanation extractor (GradCAM/B-cos) -> Similarity loss -> Student model optimizer
- Critical path: Forward pass through teacher -> Compute teacher explanations -> Forward pass through student -> Compute student explanations -> Compute combined loss (logit + explanation) -> Backpropagate to student only
- Design tradeoffs: Matching explanations increases training time and memory; using frozen explanations reduces cost but may be less accurate; choice of explanation method affects differentiability and interpretability.
- Failure signatures: Student accuracy does not improve despite explanation loss; explanations diverge over training; training becomes unstable due to explanation gradients.
- First 3 experiments:
  1. Implement vanilla KD on a small CNN-to-CNN setup and verify accuracy improvement over training from scratch.
  2. Add e2KD with GradCAM explanations and confirm increased student-teacher agreement on ImageNet subset.
  3. Test e2KD on Waterbirds-100 with a guided teacher and measure out-of-distribution accuracy gain.

## Open Questions the Paper Calls Out
None

## Limitations
- The effectiveness of explanation matching depends heavily on the quality and interpretability of the chosen explanation method (GradCAM or B-cos). If these methods do not accurately reflect the model's reasoning, the additional loss term may be ineffective or even harmful.
- The frozen explanation variant offers computational benefits but may provide suboptimal guidance if the explanations are outdated or noisy relative to the student's evolving predictions.
- The method requires differentiable explanation methods, limiting the choice of explanation techniques that can be used.

## Confidence
- **High confidence**: The core claim that matching explanations improves student-teacher agreement and maintains interpretability is well-supported by experimental results across multiple datasets and architectures.
- **Medium confidence**: The claim about transferring architectural priors (e.g., shift equivariance) from CNNs to ViTs via explanation matching is demonstrated but requires further validation on additional architectural priors and transformations.
- **Medium confidence**: The assertion that e2KD helps students learn "right for the right reasons" on biased datasets is supported by Waterbirds-100 results, but the generalization to other spurious correlation scenarios needs broader testing.

## Next Checks
1. **Ablation study on explanation methods**: Systematically compare GradCAM vs B-cos vs other explanation methods (e.g., integrated gradients) to determine which provides the most effective guidance across different architectures and tasks.
2. **Robustness to noisy explanations**: Evaluate e2KD performance when teacher explanations are deliberately corrupted or when the teacher has incorrect reasoning, to quantify the method's sensitivity to explanation quality.
3. **Generalization to other architectural priors**: Test whether e2KD can transfer additional architectural properties beyond shift equivariance (e.g., rotational invariance, texture bias) from teacher to student across diverse model families.