---
ver: rpa2
title: Label Set Optimization via Activation Distribution Kurtosis for Zero-shot Classification
  with Generative Models
arxiv_id: '2410.19195'
source_url: https://arxiv.org/abs/2410.19195
tags:
- label
- performance
- classification
- computational
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of label options in prompts
  for zero-shot classification with large language models (LLMs). Through systematic
  analysis, the authors demonstrate that the lexical choice of label words significantly
  affects model performance and sensitivity to label order, while label elaborations
  have minimal impact.
---

# Label Set Optimization via Activation Distribution Kurtosis for Zero-shot Classification with Generative Models

## Quick Facts
- arXiv ID: 2410.19195
- Source URL: https://arxiv.org/abs/2410.19195
- Authors: Yue Li; Zhixue Zhao; Carolina Scarton
- Reference count: 40
- Primary result: LOADS improves zero-shot ICL performance by selecting optimal label sets based on activation distribution kurtosis

## Executive Summary
This paper addresses the critical problem of label set optimization in zero-shot classification with large language models (LLMs). The authors systematically analyze how different label options affect model performance and discover that lexical choice of label words significantly impacts classification accuracy, while label elaborations have minimal effect. They propose LOADS (Label set Optimization via Activation Distribution Kurtosis), a novel post-hoc method that selects optimal label sets by measuring neuron activation kurtosis in LLM feed-forward networks. LOADS achieves consistent improvements across diverse classification tasks, datasets, model architectures, and languages without requiring labeled data or gradient computation.

## Method Summary
LOADS operates by computing kurtosis scores for candidate label sets using activation distributions from a single forward pass through the LLM's feed-forward network on unlabeled validation data. The method first generates diverse label sets using WordNet synonyms and LLM-based expansions, then evaluates each set by measuring the kurtosis of neuron activations in the last decoder layer's FFN. The label set with the lowest average kurtosis score is selected for zero-shot classification. This approach requires only 100 unlabeled samples and avoids gradient-based optimization, making it computationally efficient and broadly applicable to any zero-shot classification task.

## Key Results
- LOADS consistently improves zero-shot ICL performance, achieving maximum gains from 0.54 to 0.76 F1 score compared to conventional approaches
- Lexical choice of label words significantly affects model performance and sensitivity to label order, while label elaborations have minimal impact
- Cross-lingual transferability is demonstrated when using English instructions for non-English datasets, with selected English labels improving performance on French and Portuguese tasks
- Optimal label words activate fewer outlier neurons in LLM feed-forward networks, as measured by kurtosis scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal label words activate fewer outlier neurons in LLM feed-forward networks, improving zero-shot ICL performance
- Mechanism: Label words that produce neuron activations with lower kurtosis values (fewer statistical outliers) lead to more stable and predictable model behavior during classification
- Core assumption: Neuron activation distributions with fewer outliers correlate with better generalization in zero-shot settings
- Evidence anchors:
  - [abstract] "optimal label words tend to activate fewer outlier neurons in LLMs' feed-forward networks, as measured by kurtosis"
  - [section 4] "we establish a new hypothesis: the model performance influenced by label names is correlated with the number of outliers in neurons in the feed-forward network"
  - [corpus] Weak evidence - no direct corpus support for kurtosis-neuron outlier relationship
- Break condition: If activation distributions are inherently multimodal or the task requires outlier responses for discrimination

### Mechanism 2
- Claim: LOADS leverages single forward pass kurtosis measurement to select optimal label sets without gradient computation
- Mechanism: By computing activation distribution kurtosis scores for candidate label sets on unlabeled validation data, LOADS identifies labels that produce the most stable neural responses
- Core assumption: Kurtosis of activation distributions serves as a reliable proxy for label quality across diverse tasks and model architectures
- Evidence anchors:
  - [abstract] "LOADS then leverages kurtosis to measure the neuron activation distribution for label selection, requiring only a single forward pass without gradient propagation or labelled data"
  - [section 5.1] "Rank the list of label set based on the kurtosis score of the neuron activation in FFN of the last decoder layer"
  - [corpus] Weak evidence - kurtosis as activation metric not well-established in literature
- Break condition: If task requires complex label semantics that kurtosis cannot capture, or if validation data distribution differs significantly from test data

### Mechanism 3
- Claim: Cross-lingual transferability exists when using English instructions with non-English inputs
- Mechanism: English label sets selected by LOADS transfer effectively to non-English classification tasks when prompts use English task instructions
- Core assumption: Label selection based on activation patterns generalizes across languages when task framing remains consistent
- Evidence anchors:
  - [abstract] "demonstrates cross-lingual transferability when using English instructions for non-English datasets"
  - [section 5.3] "we investigate whether the optimal English label sets selected by LOADS on English dataset can also enhance performance for non-English datasets"
  - [corpus] Moderate evidence - Zhang et al. (2023a) supports English instruction superiority for non-English tasks
- Break condition: If language-specific label semantics or cultural context significantly impact classification performance

## Foundational Learning

- Concept: Neuron activation distribution analysis
  - Why needed here: Understanding how label choices affect internal model representations through statistical measures of activation patterns
  - Quick check question: How does kurtosis differ from variance in characterizing activation distributions, and why might it be more informative for outlier detection?

- Concept: In-context learning sensitivity to prompt design
  - Why needed here: Recognizing that zero-shot classification performance depends heavily on subtle prompt variations like label word choice
  - Quick check question: What distinguishes few-shot from zero-shot ICL sensitivity to label options, and why does this matter for LOADS?

- Concept: Post-hoc optimization without gradient propagation
  - Why needed here: Designing methods that improve performance without fine-tuning or gradient-based prompt tuning
  - Quick check question: What are the computational advantages of LOADS compared to gradient-based prompt optimization methods?

## Architecture Onboarding

- Component map: Validation samples + candidate label sets → Forward pass through LLM → Activation extraction from last decoder FFN → Kurtosis computation → Label ranking → Zero-shot classification
- Critical path: Label set → Forward pass → Activation extraction → Kurtosis computation → Label ranking → Zero-shot classification
- Design tradeoffs: LOADS sacrifices potential optimality from gradient-based methods for computational efficiency and zero-shot applicability
- Failure signatures: Poor performance when: (1) activation patterns don't correlate with label quality, (2) validation data distribution differs from test, (3) task requires nuanced label semantics
- First 3 experiments:
  1. Verify kurtosis correlation: Test Spearman correlation between kurtosis scores and classification accuracy across different label sets
  2. Ablation study: Compare LOADS performance against random label selection and original dataset labels
  3. Cross-lingual validation: Test English-selected labels on translated non-English datasets with English instructions

## Open Questions the Paper Calls Out
- The efficiency of LOADS may be challenged when the classification task contains a very large number of class categories.
- The optimal label sets identified by LOADS may correlate with the model's internal representation of the classification concept, which warrants further investigation.

## Limitations
- The kurtosis-outlier correlation hypothesis lacks direct empirical validation and may not capture all aspects of label quality
- Cross-lingual transferability is limited to English instructions and may not generalize to all language pairs or cultural contexts
- Performance depends on the quality and representativeness of the 100 unlabeled validation samples

## Confidence

**High Confidence:** The empirical observation that label word choice affects zero-shot ICL performance and sensitivity to label order. This is well-established through controlled experiments with clear statistical significance.

**Medium Confidence:** The LOADS method's effectiveness in improving performance across diverse tasks and languages. While results are consistently positive, the underlying mechanism (kurtosis-outlier correlation) remains correlative rather than definitively proven.

**Low Confidence:** The generalizability of cross-lingual transferability beyond the tested language pairs, and the robustness of the method to significant distribution shifts between validation and test data.

## Next Checks
1. **Mechanism validation:** Design an experiment to directly test whether reducing activation outliers through alternative means (not just label selection) improves zero-shot performance, establishing causal rather than correlative evidence for the kurtosis-outlier hypothesis.

2. **Distribution sensitivity analysis:** Evaluate LOADS performance when validation data is drawn from significantly different distributions than test data, quantifying the method's robustness to distribution shifts and identifying failure modes.

3. **Alternative optimization comparison:** Compare LOADS against gradient-based prompt optimization methods on the same tasks to quantify the tradeoff between computational efficiency and potential performance gains from more sophisticated optimization approaches.