---
ver: rpa2
title: 'LiMe: a Latin Corpus of Late Medieval Criminal Sentences'
arxiv_id: '2404.12829'
source_url: https://arxiv.org/abs/2404.12829
tags:
- latin
- language
- type
- text
- been
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The LiMe dataset is a newly released annotated corpus of 325 Latin
  legal documents from late medieval Milan, specifically criminal sentences from the
  Liber sententiarum potestatis Mediolani (1385-1429). The dataset includes detailed
  annotations of named entities, events, relations, and text segmentation, enabling
  advanced computational analysis of these historical legal records.
---

# LiMe: a Latin Corpus of Late Medieval Criminal Sentences

## Quick Facts
- arXiv ID: 2404.12829
- Source URL: https://arxiv.org/abs/2404.12829
- Reference count: 0
- Primary result: LiMe is a newly released annotated corpus of 325 Latin legal documents from late medieval Milan with detailed annotations of named entities, events, relations, and text segmentation

## Executive Summary
LiMe is a newly released annotated corpus of 325 Latin legal documents from late medieval Milan, specifically criminal sentences from the Liber sententiarum potestatis Mediolani (1385-1429). The dataset includes detailed annotations of named entities, events, relations, and text segmentation, enabling advanced computational analysis of these historical legal records. Named entities cover persons, places, dates, items, animals, measures, and quantities, with over 5,000 unique entities. Events are categorized into trial stages, trial integration, eschatocol, offenses, and death, with 37 subtypes. Relations between entities are captured using 37 unique predicates, resulting in over 3,000 relations. The dataset supports various NLP tasks, including document classification and text segmentation. A fine-tuned Latin BERT model achieved a weighted F1 score of 0.96 for document classification, while a rewired conditional random field model achieved a weighted F1 score of 0.84 for text segmentation. This resource facilitates interdisciplinary research in medieval history, linguistics, and computational social sciences.

## Method Summary
The LiMe dataset was created through expert annotation of digitized medieval Latin legal documents. A team of domain experts defined custom guidelines and performed multi-layered annotations including named entities (persons, places, dates, items, animals, measures, quantities), events (with 5 types and 37 subtypes), relations between entities (using 37 unique predicates), and text segmentation for document structure. The corpus includes 325 documents, with 276 used for document classification and 64 for text segmentation tasks. Two NLP models were trained: a fine-tuned Latin BERT model for document classification and a rewired conditional random field model for text segmentation.

## Key Results
- The dataset contains over 5,000 unique named entities across multiple categories including persons, places, dates, items, animals, measures, and quantities
- Events are categorized into 5 types (trial stages, trial integration, eschatocol, offenses, death) with 37 subtypes, capturing detailed legal proceedings
- A fine-tuned Latin BERT model achieved a weighted F1 score of 0.96 for document classification on the LiMe dataset
- A rewired conditional random field model achieved a weighted F1 score of 0.84 for text segmentation on the "sentences" documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LiMe dataset enables high-accuracy supervised NLP tasks on medieval Latin legal texts by providing detailed multi-layered annotations (entities, events, relations, and document structure).
- Mechanism: Detailed expert annotation creates ground truth that captures domain-specific entities (persons, places, items, measures), event subtypes (trial stages, offenses, deaths), and relations, allowing models to learn the precise linguistic and structural patterns of medieval legal discourse.
- Core assumption: Expert annotators can consistently identify and label entities, events, and relations in highly formulaic medieval Latin texts with minimal ambiguity.
- Evidence anchors:
  - [abstract] "The dataset includes detailed annotations of named entities, events, relations, and text segmentation, enabling advanced computational analysis of these historical legal records."
  - [section 4.2] "The annotation activity has been performed by a team of domain experts, that defined and mutually agreed on the custom guidelines followed throughout the entire process."
  - [corpus] No corpus-level performance statistics provided, but annotations are described as "thoroughly annotated by experts."
- Break condition: Inconsistent or ambiguous annotation guidelines leading to inter-annotator disagreement, or highly variable medieval Latin syntax making pattern learning unreliable.

### Mechanism 2
- Claim: LiMe's combination of document classification and text segmentation tasks is effective because the legal documents have highly regular structural templates (significatio, inquisitio, motivazioni, dispositivo).
- Mechanism: The rigid, formulaic structure of medieval legal sentences creates predictable document and segment types, making classification and segmentation tasks more tractable than for free-form text.
- Core assumption: Medieval legal documents follow consistent structural patterns that can be reliably learned by NLP models.
- Evidence anchors:
  - [section 4.1] "Each verdict, preceded by the verbal invocation - In nomine Domini, amen - is pronounced by the podestÃ  in accordance with the seigniorial decrees and statutes of the municipality of Milan. It contains the names of the accused, the narration of the legal proceeding... with the salient phases of the trial and the final pronouncement."
  - [section 4.2] "For documents of type 'sentences', the text has been divided into segments, each of them classified with a label that specifies the section in which they appear... significatio, inquisitio, motivazioni, dispositivo."
  - [corpus] No explicit corpus-level structural variability data, but the paper emphasizes homogeneity and uniformity across manuscripts.
- Break condition: Introduction of non-standard documents or significant structural variation across the corpus that violates the assumed template.

### Mechanism 3
- Claim: The LiMe dataset supports interdisciplinary research by linking linguistic annotation with historical and social context, enabling quantitative analysis of medieval society.
- Mechanism: Rich entity annotations (including professions, social class, relationships) combined with event and relation data allow researchers to extract demographic, social, and economic patterns from the legal records.
- Core assumption: The legal sentences contain sufficient contextual detail to reconstruct social and economic conditions of medieval Milan.
- Evidence anchors:
  - [section 4.1] "For each person involved in the facts, demographic and social information have been identified: name and nicknames, biological gender, social class (dominus), profession, place of origin or residency, possible relationships with relatives, and roles played in the events."
  - [section 5.1] "It is also interesting to notice the difference in gender distribution of victims and criminals: despite them being mainly men in both cases, the percentage of females is almost triplicated when it comes to victims."
  - [corpus] No corpus-level statistical results provided, but the paper describes potential for exploratory analysis of crime types, condemnations, and gender patterns.
- Break condition: Insufficient contextual information in the source documents to support meaningful social or economic analysis.

## Foundational Learning

- Concept: Medieval Latin legal terminology and document structure
  - Why needed here: Understanding the specific vocabulary and formulaic patterns in medieval legal texts is essential for interpreting annotations and designing effective NLP models.
  - Quick check question: Can you identify the purpose of the "significatio" section in a medieval criminal sentence?

- Concept: Named entity recognition and relation extraction in historical texts
  - Why needed here: The LiMe dataset requires identifying domain-specific entities (e.g., medieval professions, legal roles) and their relationships, which differs from modern NER tasks.
  - Quick check question: How would you distinguish between a "PERSON" entity and a "ROLE" entity in a medieval legal document?

- Concept: Document classification and text segmentation fundamentals
  - Why needed here: The paper presents two NLP tasks (document classification and text segmentation) that are central to the LiMe dataset's utility and require understanding of supervised learning approaches.
  - Quick check question: What is the difference between document classification and text segmentation in the context of the LiMe dataset?

## Architecture Onboarding

- Component map:
  Raw text extraction from digitized manuscripts -> Expert annotation pipeline (entities, events, relations, segmentation) -> Data storage and query interface for annotated corpus -> NLP model training and evaluation framework -> Interdisciplinary analysis tools (statistical, ML-based)

- Critical path:
  1. Digitize and transcribe medieval manuscripts
  2. Expert annotation of entities, events, relations, and segmentation
  3. Validate annotation quality and consistency
  4. Develop and train NLP models for classification and segmentation
  5. Evaluate model performance and iterate
  6. Make annotated corpus and tools publicly available

- Design tradeoffs:
  - Annotation depth vs. annotation speed: More detailed annotations provide richer data but require more expert time
  - Model complexity vs. data availability: Advanced models may overfit with limited training data
  - Historical accuracy vs. computational tractability: Preserving medieval Latin nuances may complicate NLP processing

- Failure signatures:
  - High inter-annotator disagreement rates indicating unclear guidelines
  - Poor model performance suggesting insufficient training data or annotation inconsistencies
  - Inability to extract meaningful historical insights indicating annotation gaps or data quality issues

- First 3 experiments:
  1. Train a baseline NER model on the LiMe dataset and evaluate entity recognition performance across different entity types
  2. Perform exploratory data analysis on the annotated events to identify patterns in crime types and social dynamics
  3. Fine-tune a pre-trained Latin language model on the document classification task and compare performance with the reported Latin BERT results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the annotation schema for events in LiMe compare to annotation schemas used in other legal corpora or historical text datasets?
- Basis in paper: [explicit] The paper mentions that LiMe has a custom annotation schema with 5 event types and 37 subtypes, but does not compare it to other schemas.
- Why unresolved: The paper does not provide a comparison to other annotation schemas, making it unclear how LiMe's schema differs or aligns with existing approaches.
- What evidence would resolve it: A comparison of LiMe's event annotation schema to those used in other legal or historical text corpora, highlighting similarities and differences.

### Open Question 2
- Question: What is the impact of the fine-tuned Latin BERT model's performance on document classification for downstream tasks, such as information retrieval or legal document analysis?
- Basis in paper: [explicit] The paper reports a weighted F1 score of 0.96 for document classification using a fine-tuned Latin BERT model, but does not discuss its implications for other tasks.
- Why unresolved: The paper focuses on the performance metric but does not explore how this performance translates to practical applications or benefits for other NLP tasks.
- What evidence would resolve it: An analysis of how the document classification performance affects tasks like information retrieval or legal document analysis, demonstrating the practical utility of the model.

### Open Question 3
- Question: How does the text segmentation model's performance on the "sentences" documents impact the overall analysis of the LiMe dataset?
- Basis in paper: [explicit] The paper reports a weighted F1 score of 0.84 for text segmentation using a rewired conditional random field model, but does not discuss its broader implications.
- Why unresolved: The paper presents the segmentation performance but does not explain how this affects the interpretation or analysis of the entire dataset.
- What evidence would resolve it: An evaluation of how accurate text segmentation influences the analysis of the LiMe dataset, such as its impact on event extraction or named entity recognition tasks.

## Limitations
- Limited training data: With only 276 documents for document classification and 64 for text segmentation, model performance may not generalize well to larger or more diverse medieval Latin corpora
- Expert annotation dependency: The quality and consistency of annotations rely entirely on expert judgment, but inter-annotator agreement statistics are not reported
- Medieval Latin specificity: The dataset's effectiveness may be limited to formulaic legal texts from this specific time period and region

## Confidence
- High confidence: The dataset creation methodology and annotation schema are clearly described and follow established practices in digital humanities
- Medium confidence: The text segmentation results are reasonable but may be influenced by the limited test set size
- Low confidence: Claims about the dataset's potential for interdisciplinary research are largely speculative, as the paper provides only preliminary examples

## Next Checks
1. Calculate and report inter-annotator agreement statistics (e.g., Cohen's kappa) to quantify annotation consistency across multiple experts
2. Perform 5-fold cross-validation on the document classification task to assess model stability and ensure the 0.96 F1 score isn't an artifact of the specific train/test split
3. Evaluate model performance on a small sample of medieval Latin texts from different genres (e.g., charters, chronicles) to assess generalizability beyond criminal sentences