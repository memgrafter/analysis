---
ver: rpa2
title: 'CompetEvo: Towards Morphological Evolution from Competition'
arxiv_id: '2405.18300'
source_url: https://arxiv.org/abs/2405.18300
tags:
- agents
- agent
- training
- evolution
- morph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CompetEvo, a method for co-evolving agent
  morphology and fighting tactics in adversarial multi-agent scenarios. Unlike prior
  work focusing on fixed morphologies, this approach allows agents to evolve their
  physical design and corresponding combat strategies simultaneously through self-play.
---

# CompetEvo: Towards Morphological Evolution from Competition

## Quick Facts
- arXiv ID: 2405.18300
- Source URL: https://arxiv.org/abs/2405.18300
- Reference count: 10
- Key outcome: Co-evolves agent morphology and fighting tactics through self-play, achieving win rate improvements up to 100% in adversarial tasks.

## Executive Summary
This paper introduces CompetEvo, a method for simultaneously evolving agent morphology and combat tactics in adversarial multi-agent scenarios. Unlike prior work with fixed morphologies, CompetEvo encodes agent designs as parameter vectors (leg length, size, joint capabilities) and uses a two-stage training process: morph-generation to create new designs, followed by arena-confrontation where agents learn tactics. Experiments in run-to-goal and sumo environments show that evolved agents consistently outperform their original morphologies across symmetric and asymmetric species matchups, with emergent behaviors like throwing and wrestling arising from morphological optimization.

## Method Summary
CompetEvo encodes agent morphologies as continuous parameter vectors (e.g., five parameters per leg) and trains through a two-stage process. First, a morph-generation stage optimizes morphology parameters through policy gradient methods. Then, an arena-confrontation stage trains tactics policies while using the evolved morphologies. The system employs δ-uniform opponent sampling to maintain stable training by competing against previous opponent versions, and uses annealed reward scheduling that transitions from dense rewards (for basic locomotion) to sparse combat rewards. Training uses PPO with self-play in run-to-goal and sumo environments across three species (ant, bug, spider).

## Key Results
- Evolved agents consistently outperform original morphologies across symmetric and asymmetric species matchups
- Win rate improvements reach up to 100% in some cases
- Emergent behaviors (throwing, wrestling, defensive stances) arise from morphological optimization
- Cross-species performance shows evolved agents maintain advantages in asymmetric matchups

## Why This Works (Mechanism)

### Mechanism 1
Co-evolving morphology and tactics through self-play produces agents that are both physically and behaviorally optimized for adversarial tasks. The system jointly optimizes a morph policy that generates morphology parameters and a tactics policy that generates actions, trained in alternating stages. This creates feedback where better morphology enables better tactics, and vice versa.

### Mechanism 2
The δ-uniform opponent sampling strategy maintains stable training by preventing strategy collapse. Instead of training against only the current opponent, agents play against a uniform sample of N previous opponent policies from the last δ% of training, ensuring robust strategies that work against diverse opponents.

### Mechanism 3
Annealed reward scheduling bridges the exploration gap by gradually transitioning from dense to sparse rewards. Early training uses dense rewards for basic locomotion skills, while later stages emphasize sparse fighting rewards, preventing agents from getting stuck in local optima during early exploration.

## Foundational Learning

- Concept: Two-player Markov games with self-play
  - Why needed here: The entire framework relies on agents learning through repeated competition against versions of themselves or their co-evolving opponents
  - Quick check question: In a two-player Markov game, what determines the transition to the next state?

- Concept: Parameterized morphology encoding
  - Why needed here: Agent designs must be represented as learnable parameters that can be optimized through policy gradients
  - Quick check question: How would you encode a four-legged agent's morphology as a parameter vector?

- Concept: Curriculum learning through staged training
  - Why needed here: Agents need to learn basic locomotion before they can effectively learn combat strategies, requiring a structured training progression
  - Quick check question: Why might training from scratch on sparse combat rewards fail?

## Architecture Onboarding

- Component map: Morph policy network → Morphology generator → Physics simulator → Tactics policy network → Action executor
- Critical path: Morph generation → Arena creation → Combat episode → Reward collection → Policy update → Opponent sampling → Next episode
- Design tradeoffs: Fixed vs. evolvable morphologies (simplicity vs. performance), dense vs. sparse rewards (exploration vs. exploitation), uniform vs. prioritized opponent sampling (stability vs. efficiency)
- Failure signatures: Agents learn to avoid combat (indicates sparse rewards too difficult), morphological parameters explode (encoding issues), training stalls after initial progress (opponent sampling problems)
- First 3 experiments:
  1. Run warming-up phase only - verify agents learn basic locomotion toward goals
  2. Train with fixed morphologies only - confirm tactics learning works without morphological evolution
  3. Test opponent sampling with δ=1.0 - verify stable training with current opponent only

## Open Questions the Paper Calls Out

### Open Question 1
How do the evolved morphological parameters specifically contribute to the emergence of throwing, wrestling, and defensive behaviors in different species? While the paper demonstrates these behaviors emerge, it doesn't provide detailed analysis of how specific morphological parameters directly enable or enhance each behavior type across different species.

### Open Question 2
How does the performance of CompetEvo scale with increasing morphological complexity and degrees of freedom? The paper tests three species but doesn't systematically explore how performance changes as morphological complexity increases or with different numbers of controllable degrees of freedom.

### Open Question 3
What is the relationship between morphological evolution and transfer learning capabilities in novel adversarial environments? While the paper shows cross-species performance, it doesn't investigate whether morphologically evolved agents can transfer their advantages to completely different adversarial environments or tasks not seen during training.

### Open Question 4
How does the self-play training curriculum affect the diversity of evolved morphologies and strategies? The paper uses δ-Uniform opponent sampling and describes a two-stage training process but doesn't analyze how these choices affect the diversity of solutions or whether it converges to similar solutions across different runs.

## Limitations
- Limited testing to three specific animal morphologies (ant, bug, spider) in two adversarial tasks
- Effectiveness of δ-uniform opponent sampling lacks direct empirical validation against alternatives
- Sparse reward signal for combat behavior may still pose challenges for scaling to more complex environments

## Confidence
- High Confidence: The two-stage training pipeline and morphology encoding as parameter vectors are clearly defined
- Medium Confidence: The effectiveness of δ-uniform opponent sampling shows promise but requires more comparative analysis
- Low Confidence: Claims about emergent behaviors arising specifically from morphological optimization are largely anecdotal

## Next Checks
1. Conduct ablation studies varying the annealing schedule κ to determine optimal transition timing from dense to sparse rewards
2. Test alternative opponent sampling strategies against δ-uniform sampling to quantify its contribution to stable training
3. Apply the methodology to significantly different morphology classes (e.g., wheeled agents, soft robots) to assess generalizability beyond tested animal morphologies