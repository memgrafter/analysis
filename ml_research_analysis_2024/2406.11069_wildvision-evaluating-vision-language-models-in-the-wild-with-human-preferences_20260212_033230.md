---
ver: rpa2
title: 'WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences'
arxiv_id: '2406.11069'
source_url: https://arxiv.org/abs/2406.11069
tags:
- image
- wildvision
- text
- prompt
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WildVision introduces an interactive platform and benchmark for
  evaluating vision-language models (VLMs) in real-world scenarios using human preferences.
  The authors launched WildVision-Arena (WV-Arena), a chatbot-style platform that
  enables multi-round conversations with 20+ VLMs and collects crowdsourced human
  votes.
---

# WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences

## Quick Facts
- arXiv ID: 2406.11069
- Source URL: https://arxiv.org/abs/2406.11069
- Reference count: 40
- Key outcome: Interactive platform evaluating 20+ VLMs with human preferences achieves 0.94 Spearman correlation between GPT-4 and human ratings

## Executive Summary
WildVision introduces an interactive platform and benchmark for evaluating vision-language models (VLMs) in real-world scenarios using human preferences. The authors launched WildVision-Arena (WV-Arena), a chatbot-style platform that enables multi-round conversations with 20+ VLMs and collects crowdsourced human votes. They curated WildVision-Bench (WV-Bench) by selecting 500 high-quality samples from 8,000 user submissions in WV-Arena. Using GPT-4 as a judge to compare each VLM with Claude-3-Sonnet, WV-Bench achieves a Spearman correlation of 0.94 with the WV-Arena Elo ratings, significantly outperforming other benchmarks like MMVet, MMMU, and MMStar.

## Method Summary
WildVision presents a comprehensive evaluation framework for vision-language models using human preferences. The methodology centers on WildVision-Arena (WV-Arena), an interactive chatbot platform that enables users to engage in multi-round conversations with 20+ different VLMs. Users can upload images and receive responses from multiple models simultaneously, then vote on their preferred responses. The platform collected 8,000+ user submissions and 20,000+ multimodal conversations. From this data, the authors curated WildVision-Bench (WV-Bench) by selecting 500 high-quality examples that best represent real-world VLM interactions. The benchmark evaluation uses GPT-4 as an automated judge to compare each VLM against Claude-3-Sonnet, with the resulting rankings showing strong correlation (0.94 Spearman) with the human preference data from WV-Arena.

## Key Results
- WV-Bench achieves 0.94 Spearman correlation with WV-Arena Elo ratings when using GPT-4 as judge
- Significantly outperforms existing benchmarks (MMVet, MMMU, MMStar) in correlation with human preferences
- Analysis of 20,000+ multimodal conversations reveals VLM challenges with contextual cues, spatial reasoning, visual imagination, expert knowledge, hallucinations, and safety

## Why This Works (Mechanism)
WildVision works by creating an authentic evaluation environment that mirrors how users actually interact with VLMs in practice. By collecting real user conversations rather than synthetic prompts, the platform captures the nuanced and often ambiguous nature of real-world queries. The multi-model comparison format allows users to directly compare responses side-by-side, making their preferences more meaningful. The Elo rating system for aggregating human votes provides a robust way to rank models based on pairwise comparisons. Using GPT-4 as an automated judge bridges the gap between scalable evaluation and human preference, though this introduces its own potential biases that need consideration.

## Foundational Learning

**Vision-Language Models (VLMs)**: AI systems that process both visual and textual inputs to generate relevant responses. Needed to understand the evaluation target. Quick check: Can the model correctly describe objects and their relationships in an image while answering text questions?

**Human Preference Evaluation**: Methodology where human users directly compare model outputs and indicate preferences. Needed because traditional metrics often fail to capture real-world utility. Quick check: Are human raters consistent in their preferences across multiple similar examples?

**Benchmark Curation**: Process of selecting representative examples from a larger dataset for standardized evaluation. Needed to create manageable yet comprehensive test sets. Quick check: Does the curated benchmark maintain the diversity and difficulty distribution of the original data?

**Elo Rating System**: Competitive ranking method originally from chess that updates ratings based on pairwise comparisons. Needed for aggregating human preference data into meaningful model rankings. Quick check: Do Elo ratings stabilize after sufficient comparisons, or do rankings continue to fluctuate?

## Architecture Onboarding

Component map: User Uploads -> Multiple VLM Responses -> Human Voting -> Elo Rating Update -> GPT-4 Automated Evaluation -> Benchmark Curation

Critical path: The evaluation pipeline flows from user interactions through the platform to model ranking. User uploads images and questions, multiple VLMs generate responses, humans vote on preferred responses, Elo ratings are calculated, GPT-4 judges are applied for automated evaluation, and top examples are curated into the benchmark.

Design tradeoffs: The platform trades evaluation scale for authenticity - collecting real user interactions is more resource-intensive than using synthetic prompts but captures more realistic use cases. The use of GPT-4 as an automated judge enables rapid evaluation but may not perfectly align with human preferences.

Failure signatures: Poor performance occurs when VLMs struggle with subtle contextual cues, complex spatial reasoning, visual imagination tasks, expert domain knowledge, or when generating hallucinations. Safety issues may also emerge in certain edge cases.

First experiments:
1. Run a pilot with 10 users evaluating 5 VLMs on 20 diverse image-text pairs to test platform usability
2. Compare GPT-4 judge rankings against a subset of human preferences to validate correlation methodology
3. Test benchmark curation by having multiple annotators independently select examples from the same pool

## Open Questions the Paper Calls Out

None specified in the source material.

## Limitations

- Sample size of 500 curated examples from 8,000 submissions may not fully represent VLM interaction diversity
- GPT-4 automated judge introduces potential biases despite strong correlation with human preferences
- Platform's scope of 20+ VLMs may not capture the complete landscape of available models
- Crowdsourced data collection could introduce demographic or cultural biases in human preferences

## Confidence

High confidence: The methodology for collecting human preferences through interactive conversations is sound and well-documented

Medium confidence: The selection process for WV-Bench from WV-Arena submissions is reasonable but may introduce selection bias

Medium confidence: The correlation analysis between GPT-4 and human preferences provides useful validation but is not definitive

Low confidence: The generalizability of findings to all VLM applications and use cases

## Next Checks

1. Conduct cross-cultural validation of human preferences to assess potential demographic biases in the collected data

2. Expand the automated evaluation methodology to include multiple judge models beyond GPT-4 to reduce potential bias

3. Test the WV-Bench's predictive validity by correlating its results with actual real-world deployment performance metrics across different VLM applications