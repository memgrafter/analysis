---
ver: rpa2
title: Efficient Long Context Language Model Retrieval with Compression
arxiv_id: '2412.18232'
source_url: https://arxiv.org/abs/2412.18232
tags:
- retrieval
- color
- compression
- passages
- comp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of long context language model
  retrieval by introducing CoLoR, a compression method trained to optimize retrieval
  performance while minimizing passage length. The method uses preference optimization
  on synthetic data, where compressed passages are automatically labeled as chosen
  or rejected based on retrieval success and brevity.
---

# Efficient Long Context Language Model Retrieval with Compression

## Quick Facts
- arXiv ID: 2412.18232
- Source URL: https://arxiv.org/abs/2412.18232
- Authors: Minju Seo; Jinheon Baek; Seongyun Lee; Sung Ju Hwang
- Reference count: 40
- Key outcome: CoLoR improves retrieval performance by 6% while compressing context size by factor of 1.91 compared to raw passages

## Executive Summary
This paper addresses the inefficiency of long context language model (LCLM) retrieval by introducing CoLoR, a compression method trained to optimize retrieval performance while minimizing passage length. The method uses preference optimization on synthetic data, where compressed passages are automatically labeled as chosen or rejected based on retrieval success and brevity. CoLoR improves retrieval performance by 6% and reduces context size by a factor of 1.91 compared to raw passages, outperforming existing compression approaches on 9 datasets spanning single- and multi-document retrieval tasks.

## Method Summary
CoLoR trains a compression model using preference optimization with dynamic regularization. The method generates synthetic training data by creating multiple compressed versions of raw passages using various LLMs, then evaluates each compressed version's retrieval success with LCLM. Compressed passages are labeled as chosen or rejected based on whether they achieve better retrieval performance with shorter length. The model is trained to prefer compressed passages that yield better retrieval performance with minimal length, using a dynamic regularization term that adjusts based on the length difference between rejected and chosen samples.

## Key Results
- CoLoR improves retrieval performance by 6% compared to raw passages
- CoLoR reduces context size by a factor of 1.91 compared to raw passages
- CoLoR outperforms existing compression approaches on 9 benchmark datasets across single- and multi-document retrieval tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The CoLoR model improves retrieval performance by 6% compared to raw passages.
- Mechanism: CoLoR uses preference optimization with a dynamic regularization term that encourages brevity while maintaining retrieval accuracy. The model is trained to prefer compressed passages that yield better retrieval performance with the shortest length.
- Core assumption: The LCLM retrieval performance is directly correlated with the relevance and conciseness of the input passages.
- Evidence anchors:
  - [abstract] "CoLoR improves the retrieval performance by 6% while compressing the in-context size by a factor of 1.91"
  - [section 3.2.1] "we further propose to use a dynamic regularization term that adjusts the odds ratio loss based on the length difference between rejected and chosen samples"
- Break condition: If the dynamic regularization term over-prioritizes brevity at the expense of relevant information, leading to decreased retrieval performance.

### Mechanism 2
- Claim: CoLoR reduces context size by a factor of 1.91 compared to raw passages.
- Mechanism: The model learns to compress passages by filtering out irrelevant details while retaining core information crucial for retrieval. This is achieved through synthetic data generation where compressed passages are automatically labeled as chosen or rejected based on their retrieval success and brevity.
- Core assumption: The compression process can effectively identify and retain the most relevant information for retrieval while removing noise.
- Evidence anchors:
  - [abstract] "CoLoR improves the retrieval performance by 6% while compressing the in-context size by a factor of 1.91"
  - [section 3.2] "CoLoR generates compressed passages by learning to balance two objectives: maintaining high retrieval accuracy and reducing passage length"
- Break condition: If the compression model fails to accurately identify and retain crucial information for retrieval, leading to a significant drop in performance.

### Mechanism 3
- Claim: CoLoR generalizes well to out-of-domain datasets.
- Mechanism: The model is trained on a diverse set of datasets spanning different retrieval categories (fact-checking, multi-document, and argument). This exposure to varied data helps the model learn general compression strategies that can be applied to new, unseen datasets.
- Core assumption: The compression strategies learned from one retrieval category can be effectively applied to others.
- Evidence anchors:
  - [section 5] "we evaluate its performance in out-of-domain settings by excluding a set of datasets from each retrieval category... we observe that CoLoR consistently enhances retrieval performance while significantly reducing the input context size"
  - [table 2] "CoLoR consistently enhances retrieval performance while significantly reducing the input context size"
- Break condition: If the out-of-domain datasets have significantly different characteristics from the training data, leading to poor compression and retrieval performance.

## Foundational Learning

- Concept: Preference Optimization
  - Why needed here: To train the CoLoR model without requiring ground truth labels for compressed passages, using synthetic data where passages are automatically labeled based on retrieval success.
  - Quick check question: How does preference optimization differ from traditional supervised learning in this context?

- Concept: Dynamic Regularization
  - Why needed here: To encourage the model to generate concise compressed passages while maintaining retrieval accuracy, addressing the trade-off between brevity and information retention.
  - Quick check question: What is the role of the length difference between rejected and chosen samples in the dynamic regularization term?

- Concept: LCLM Retrieval
  - Why needed here: Understanding the underlying retrieval mechanism is crucial for designing an effective compression model, as the model needs to optimize for the specific requirements of LCLM retrieval.
  - Quick check question: How does LCLM retrieval differ from traditional sparse or dense retrieval methods in terms of input processing and output generation?

## Architecture Onboarding

- Component map: Raw passages -> Compression Model (CoLoR) -> Compressed passages -> LCLM -> Retrieval results
- Critical path:
  1. Generate synthetic data by creating multiple compressed versions of raw passages
  2. Evaluate retrieval success of compressed passages with LCLM
  3. Label compressed passages as chosen or rejected based on retrieval success and brevity
  4. Train CoLoR using preference optimization with dynamic regularization
  5. Use trained CoLoR to compress passages for LCLM retrieval

- Design tradeoffs:
  - Compression rate vs. retrieval performance: Higher compression rates may lead to information loss, while lower rates may not significantly reduce context size.
  - Training time vs. model performance: More training epochs may improve performance but increase computational costs.
  - Model complexity vs. generalization: A more complex model may perform better on training data but struggle to generalize to new datasets.

- Failure signatures:
  - Decreased retrieval performance compared to raw passages
  - Inconsistent compression across different datasets or passage types
  - High computational costs for compression or retrieval

- First 3 experiments:
  1. Train CoLoR on a single dataset and evaluate retrieval performance and compression rate compared to raw passages.
  2. Vary the compression rate (e.g., 0.3 vs. 0.6) and assess the trade-off between compression and retrieval performance.
  3. Test CoLoR on out-of-domain datasets to evaluate its generalization capabilities.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the following limitations and uncertainties emerge from the methodology and evaluation:

- The paper does not investigate the maximum corpus size at which the compression and retrieval pipeline remains computationally feasible.
- The evaluation focuses on general knowledge datasets without exploring domain-specific retrieval tasks requiring expert knowledge.
- The analysis doesn't distinguish between different types of multi-document queries, particularly those requiring distant document relationships.

## Limitations

- Limited comparison to other compression methods with only 3 baselines tested
- Out-of-domain generalization tested only within the same retrieval categories, not truly novel domains
- Evaluation focuses solely on retrieval performance metrics without examining compressed passage quality
- Training methodology relies heavily on synthetic data generation without investigating the impact of different LLMs

## Confidence

**High confidence**: The core claim that CoLoR improves retrieval performance by 6% while reducing context size by 1.91x is well-supported by the experimental results presented across 9 datasets.

**Medium confidence**: The generalization claims are reasonably supported but limited by the evaluation scope, testing only within the same retrieval categories.

**Low confidence**: Claims about being the "first" to use preference optimization for LCLM retrieval compression are difficult to verify given the rapidly evolving research area.

## Next Checks

1. **Cross-LLM validation**: Test whether CoLoR's performance is consistent when different LLMs are used for the synthetic data generation phase to determine if the model learns genuine compression strategies.

2. **Human evaluation of compressed passages**: Conduct qualitative assessments of compressed passages to verify they retain meaningful information and aren't simply shorter versions with degraded content quality.

3. **Ablation study on dynamic regularization**: Remove the dynamic regularization component and compare performance to understand whether improvements come primarily from preference optimization or specifically from the length-aware regularization term.