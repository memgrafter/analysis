---
ver: rpa2
title: Large Language Model Informed Patent Image Retrieval
arxiv_id: '2404.19360'
source_url: https://arxiv.org/abs/2404.19360
tags:
- patent
- image
- retrieval
- images
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a language-informed, distribution-aware multimodal
  approach for patent image retrieval, addressing challenges in the abstract visual
  features of patent images, skewed classification distributions, and limited semantic
  information in image descriptions. The method enriches patent image representations
  by integrating Large Language Models (LLMs) to generate detailed descriptions and
  employs distribution-aware contrastive losses to improve performance for underrepresented
  classes.
---

# Large Language Model Informed Patent Image Retrieval

## Quick Facts
- arXiv ID: 2404.19360
- Source URL: https://arxiv.org/abs/2404.19360
- Authors: Hao-Cheng Lo; Jung-Mei Chu; Jieh Hsiang; Chun-Chieh Cho
- Reference count: 40
- One-line primary result: State-of-the-art patent image retrieval with mAP +53.3%, Recall@10 +41.8%, MRR@10 +51.9% improvements

## Executive Summary
This paper addresses the challenge of patent image retrieval by proposing a language-informed, distribution-aware multimodal approach. The method enriches patent image representations by integrating Large Language Models (LLMs) to generate detailed descriptions and employs distribution-aware contrastive losses to improve performance for underrepresented classes. The approach tackles three key challenges: abstract visual features of patent images, skewed classification distributions, and limited semantic information in image descriptions. Extensive experiments on the DeepPatent2 dataset demonstrate significant improvements over state-of-the-art methods, validated through both quantitative metrics and user studies.

## Method Summary
The method uses a one-stage visual language model approach with CLIP-based text encoder (frozen), visual encoders (ResNet50, EfficientNetB-0, ViT-B-32, SwinV2-B), and distribution-aware contrastive losses (Lclip, Lcls, Lcat) with uncertainty weighting. Patent images are processed with augmentation techniques including flipping, random cropping, random erasing, and gridmask. Text enrichment is performed using BLIP-2 captioner and GPT-4 LLM with predefined templates to generate detailed, alias-containing descriptions. The model is trained to align visual features with enriched text embeddings while accounting for the long-tail distribution of patent classes through class-wise and category-wise contrastive losses.

## Key Results
- State-of-the-art performance with mAP +53.3% improvement over existing methods
- Recall@10 improved by +41.8% and MRR@10 by +51.9% on DeepPatent2 dataset
- Significant performance gains for underrepresented classes through distribution-aware contrastive losses
- User study validation demonstrating practical utility for real-world patent retrieval tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal contrastive learning with LLM-generated text embeddings improves semantic alignment in patent image representations.
- Mechanism: Patent images lack semantic richness in their original descriptions, so the method uses LLMs to generate detailed, alias-containing, free-form descriptions. These enriched texts are then aligned with visual features through distribution-aware contrastive losses, improving retrieval performance especially for underrepresented classes.
- Core assumption: LLM-generated descriptions capture the semantic intent of patent images better than original captions, and this semantic signal is learnable by a visual encoder.
- Evidence anchors: [abstract]: "enriches the semantic understanding of patent image by integrating Large Language Models"; [section 3.3]: "employ captioners and LLMs for producing detailed, enriched descriptions that enhance the semantic understanding of the images"
- Break condition: If LLM-generated text drifts from technical accuracy or introduces hallucinated features irrelevant to the image, alignment will degrade and retrieval will fail.

### Mechanism 2
- Claim: Distribution-aware contrastive losses improve learning in long-tail patent classification scenarios.
- Mechanism: The method introduces class-wise (Lcls) and category-wise (Lcat) contrastive losses, each weighted by learnable homoscedastic uncertainty. This allows the model to give more importance to underrepresented classes and categories, preventing them from being overshadowed by head classes during training.
- Core assumption: The long-tail distribution of Locarno classes is the main source of retrieval bias, and correcting for it via coarse-grained contrastive losses improves performance on tail classes without hurting head class retrieval.
- Evidence anchors: [abstract]: "improves the performance of underrepresented classes with our proposed distribution-aware contrastive losses"; [section 3.2]: "we introduce three types of loss functions...class-based and category-based coarse-grained losses...distribution-aware"
- Break condition: If uncertainty weighting is miscalibrated or if class/category boundaries are ambiguous, the loss may destabilize training or introduce noise.

### Mechanism 3
- Claim: Temporal filtering in retrieval metrics ensures practical relevance for patent novelty detection.
- Mechanism: Retrieval is constrained to images granted before the query image's patent date, ensuring that the system evaluates similarity against prior art only, not concurrent or future inventions. This aligns the model's evaluation with the legal workflow of novelty and infringement searches.
- Core assumption: The practical use case of patent retrieval is novelty detection, which requires temporal filtering, and that users will trust a system that mimics this workflow.
- Evidence anchors: [abstract]: "Our model is validated on a large dataset with metrics specifically tailored for novelty detection"; [section 3.4]: "the following retrieval metrics should be calculated over ùê∑‚Ä≤ùëû given vùëû" where ùê∑‚Ä≤ùëû excludes images granted after the query
- Break condition: If temporal metadata is incorrect or missing, filtering will either exclude relevant prior art or include non-prior inventions, breaking legal validity.

## Foundational Learning

- Concept: Multimodal contrastive learning (CLIP-style)
  - Why needed here: Patent images are abstract and lack rich semantic context; pairing them with detailed textual descriptions enables the model to learn semantic alignment between vision and language.
  - Quick check question: What is the difference between instance-level and class-level contrastive loss, and why would both be used here?

- Concept: Long-tail learning and distribution-aware weighting
  - Why needed here: Patent classes follow a heavy-tailed distribution (fig. 1), so standard losses overfit to frequent classes; distribution-aware losses help the model learn useful representations for rare classes.
  - Quick check question: How does homoscedastic uncertainty weighting affect the contribution of different loss terms during training?

- Concept: Temporal constraints in retrieval evaluation
  - Why needed here: Patent novelty and infringement searches only consider prior art; ignoring this leads to unrealistic evaluation and unusable results in practice.
  - Quick check question: What happens if retrieval metrics are computed without temporal filtering, and why is this problematic for patent work?

## Architecture Onboarding

- Component map: Patent image ‚Üí augmentation (flipping, random crop, random erase, gridmask) ‚Üí visual backbone (ResNet50/EfficientNet/ViT/SwinV2) ‚Üí (projector MLP) ‚Üí visual features ‚Üí contrastive loss with enriched text features ‚Üí trained visual encoder ‚Üí embedding DB

- Critical path: Image ‚Üí augmentation ‚Üí visual backbone ‚Üí (projector) ‚Üí visual features ‚Üí contrastive loss with enriched text features ‚Üí trained visual encoder ‚Üí embedding DB

- Design tradeoffs:
  - Using frozen CLIP text encoder avoids retraining but may limit adaptation to patent-specific jargon.
  - GPT-4-based captioner is powerful but costly and introduces latency.
  - Learnable uncertainty weighting is robust but adds complexity to loss optimization.

- Failure signatures:
  - Poor retrieval on tail classes ‚Üí check class-level loss weighting or text enrichment quality.
  - Visual embeddings cluster by appearance but not by class ‚Üí check alignment between visual and text encoders.
  - High latency in retrieval ‚Üí check captioner or LLM call overhead.

- First 3 experiments:
  1. Ablation: Remove LLM-generated descriptions, use only original image captions; compare mAP on tail vs head classes.
  2. Ablation: Remove class/category losses, keep only instance-level InfoNCE; measure degradation in long-tail performance.
  3. Sanity: Train with perfect text labels (e.g., class name only), then with enriched LLM text; measure semantic alignment via t-SNE or nearest neighbor qualitative check.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper lacks ablation studies isolating the impact of LLM-generated descriptions versus distribution-aware losses
- Use of expensive LLMs (GPT-4) for text generation raises concerns about practical scalability and real-time deployment feasibility
- Evaluation focuses on DeepPatent2, limiting generalizability to other patent datasets or domains

## Confidence

- **High**: Overall retrieval performance improvements (mAP +53.3%, Recall@10 +41.8%, MRR@10 +51.9%) on DeepPatent2 dataset
- **Medium**: Distribution-aware contrastive losses improve long-tail class performance
- **Medium**: LLM-generated descriptions enhance semantic alignment between images and text
- **Low**: Specific contribution of each component (LLM descriptions, loss functions, temporal filtering) can be isolated

## Next Checks

1. **Ablation Study**: Remove LLM-generated descriptions and use only original captions, then remove distribution-aware losses while keeping instance-level contrastive loss. Compare performance degradation across head and tail classes to isolate individual component contributions.

2. **Cross-Dataset Validation**: Test the trained model on a different patent dataset (e.g., DesignPatent or USPTO bulk data) to assess generalizability beyond DeepPatent2. Measure whether performance gains translate to new data distributions.

3. **Cost-Performance Tradeoff Analysis**: Benchmark retrieval accuracy using different text generation methods (BLIP-2 only, GPT-3.5 only, GPT-4) to quantify the relationship between generation cost and retrieval performance. Include inference latency measurements for practical deployment assessment.