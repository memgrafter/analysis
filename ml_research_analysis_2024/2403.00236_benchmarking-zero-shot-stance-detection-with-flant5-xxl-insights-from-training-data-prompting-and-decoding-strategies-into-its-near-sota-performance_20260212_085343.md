---
ver: rpa2
title: 'Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from training
  data, prompting, and decoding strategies into its near-SoTA performance'
arxiv_id: '2403.00236'
source_url: https://arxiv.org/abs/2403.00236
tags:
- task
- stance
- performance
- prompt
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks the zero-shot stance detection performance
  of FlanT5-XXL on English tweets from SemEval 2016 Tasks 6A, 6B, and the P-Stance
  dataset. The authors evaluate the model's sensitivity to different prompts, instructions,
  and decoding strategies while investigating potential biases.
---

# Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from training data, prompting, and decoding strategies into its near-SoTA performance

## Quick Facts
- **arXiv ID:** 2403.00236
- **Source URL:** https://arxiv.org/abs/2403.00236
- **Reference count:** 40
- **Primary result:** FlanT5-XXL achieves state-of-the-art zero-shot stance detection on English tweets, matching or exceeding fine-tuned models without task-specific training

## Executive Summary
This study benchmarks FlanT5-XXL's zero-shot stance detection performance on English tweets from SemEval 2016 Tasks 6A, 6B, and P-Stance datasets. The authors systematically evaluate how different prompts, instructions, and decoding strategies affect performance while investigating potential biases. FlanT5-XXL delivers competitive results, achieving state-of-the-art performance on SemEval 2016 Task 6B and matching or exceeding previous zero-shot approaches on other datasets. The study reveals that performance correlates with prompt perplexity, exhibits positivity bias, and shows that pre-processing tweets by expanding abbreviations and splitting hashtags improves performance on some datasets but not others.

## Method Summary
The study uses FlanT5-XXL (11B parameter, instruction-tuned LLM) to perform zero-shot stance detection on English tweets. The approach reframes stance detection as a question-answering task using various prompt templates (Statement, Question, Instruction) and applies different decoding strategies (greedy, PMI-based, constrained decoding). Tweets are pre-processed by expanding abbreviations and splitting hashtags to improve token compatibility. The authors systematically vary prompt templates and decoding strategies while measuring performance across three benchmark datasets.

## Key Results
FlanT5-XXL achieves state-of-the-art zero-shot stance detection performance, particularly excelling on SemEval 2016 Task 6B. The model shows strong performance on other datasets, often matching or exceeding previous zero-shot approaches. Performance varies significantly with prompt choice, with lower perplexity prompts generally yielding better results. Constrained decoding and PMI-based approaches improve accuracy by better capturing stance relationships. Pre-processing tweets through abbreviation expansion and hashtag splitting improves performance on some datasets but shows mixed effects overall. The study also reveals a positivity bias in the model's predictions.

## Why This Works (Mechanism)
FlanT5-XXL's strong performance stems from its instruction-tuning on diverse tasks, which enables effective zero-shot generalization to stance detection. The question-answering reformulation leverages the model's natural language understanding capabilities, allowing it to infer stance from contextual relationships in the text. The correlation between prompt perplexity and performance suggests that prompts closer to the model's training distribution yield better results. Constrained decoding and PMI-based approaches work by explicitly modeling the relationships between stances and target topics, guiding the model toward more accurate predictions. The model's inherent bias toward positive stances may reflect training data patterns or architectural tendencies in handling sentiment-laden language.

## Foundational Learning
The study demonstrates that large language models pre-trained on diverse web text and fine-tuned with instruction-tuning can effectively generalize to specialized NLP tasks like stance detection without task-specific training. This suggests that the breadth of pretraining data and the quality of instruction-tuning are crucial for zero-shot performance. The findings also highlight the importance of prompt engineering in zero-shot learning, where carefully crafted prompts that align with the model's training distribution can significantly impact performance. Additionally, the work shows that post-processing strategies like abbreviation expansion and hashtag splitting can help bridge the gap between informal text (tweets) and the model's pretraining data.

## Architecture Onboarding
FlanT5-XXL is an 11B parameter encoder-decoder model based on the T5 architecture, fine-tuned with instruction-tuning across multiple tasks. The model uses a standard transformer architecture with self-attention mechanisms, enabling it to capture long-range dependencies in text. For stance detection, the model processes the tweet and target as input and generates a stance label (Favor, Against, or Neither) as output. The instruction-tuning phase exposes the model to various task formats, including question-answering, which the authors leverage by reformulating stance detection as a QA task. The large parameter count and diverse pretraining data enable the model to capture nuanced language patterns necessary for stance detection.

## Open Questions the Paper Calls Out
- How does performance scale with model size beyond 11B parameters?
- What is the impact of different pretraining corpora on zero-shot stance detection?
- How can we better quantify and mitigate the observed positivity bias?
- What are the limitations of the question-answering reformulation for stance detection?
- How do different tweet pre-processing strategies affect performance across various domains?

## Limitations
The study focuses exclusively on English tweets, limiting generalizability to other languages and domains. The positivity bias observed in the model's predictions could impact real-world applications, particularly in detecting negative stances. The effectiveness of pre-processing strategies varies across datasets, suggesting that a one-size-fits-all approach may not be optimal. The study does not explore multilingual stance detection or the model's performance on longer, more formal text. Additionally, the computational cost of using an 11B parameter model for zero-shot inference may be prohibitive for some applications.

## Confidence
The study presents systematic experiments with clear methodology and reproducible results across multiple datasets. The correlation between prompt perplexity and performance provides a quantitative basis for understanding model behavior. However, the observed positivity bias and mixed effects of pre-processing strategies suggest that there are still aspects of the model's behavior that are not fully understood. The findings are likely robust for English tweet-based stance detection but may not generalize to other languages or text types without further validation.

## Next Checks
- Validate the model's performance on multilingual stance detection datasets
- Investigate the impact of different pretraining corpora on zero-shot performance
- Explore methods to quantify and mitigate the observed positivity bias
- Test the model's performance on longer, more formal text beyond tweets
- Evaluate the computational efficiency of using large models for zero-shot inference in real-world applications