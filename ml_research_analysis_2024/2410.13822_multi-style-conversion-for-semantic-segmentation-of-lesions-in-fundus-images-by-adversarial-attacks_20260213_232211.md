---
ver: rpa2
title: Multi-style conversion for semantic segmentation of lesions in fundus images
  by adversarial attacks
arxiv_id: '2410.13822'
source_url: https://arxiv.org/abs/2410.13822
tags:
- segmentation
- style
- image
- images
- conversion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of semantic segmentation of retinal
  lesions in fundus images, which faces difficulties due to the lack of standardization
  in annotation styles across diverse databases. The authors propose a novel method
  called "adversarial style conversion" to address this issue.
---

# Multi-style conversion for semantic segmentation of lesions in fundus images by adversarial attacks

## Quick Facts
- arXiv ID: 2410.13822
- Source URL: https://arxiv.org/abs/2410.13822
- Authors: ClÃ©ment Playout; Renaud Duval; Marie Carole Boucher; Farida Cheriet
- Reference count: 39
- Key outcome: Adversarial style conversion enables semantic segmentation models to spontaneously adapt annotation style based on input image characteristics, significantly improving cross-database generalization

## Executive Summary
This paper addresses the challenge of semantic segmentation of retinal lesions in fundus images when faced with heterogeneous annotation styles across different databases. The authors propose a novel adversarial style conversion method that trains a single segmentation model on combined databases, allowing it to spontaneously modify its segmentation style depending on the input image's origin. By employing a linear probe to detect dataset origin from encoder features and using adversarial attacks to control segmentation style, the method demonstrates significant qualitative and quantitative improvements in cross-database performance while offering new avenues for uncertainty estimation and continuous interpolation between annotation styles.

## Method Summary
The method trains a single segmentation architecture on multiple databases with different annotation styles, incorporating a linear probe on encoder features to detect dataset origin. At inference, adversarial attacks are applied to manipulate the probe's predictions, thereby conditioning the segmentation model to adopt the annotation style of a target database. This approach enables style conversion without requiring paired style-annotated data, allowing the model to generalize across heterogeneous annotation protocols while maintaining semantic integrity of the lesion segmentations.

## Key Results
- Single model trained on combined databases shows spontaneous style adaptation based on input image characteristics
- Adversarial attacks successfully force segmentation style conversion to target database annotation style
- Significant quantitative gains in cross-database segmentation performance compared to single-dataset training
- Method enables uncertainty estimation and continuous interpolation between annotation styles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A segmentation model trained on multiple databases with heterogeneous annotation styles spontaneously adapts its segmentation style based on the input image's origin marker.
- Mechanism: The model implicitly learns to detect subtle image characteristics (origin marker) that betray the database of origin, then uses this to condition its segmentation style to maximize performance on the corresponding annotation style.
- Core assumption: The image appearance contains enough information to reliably identify its database of origin, even after standardization.
- Evidence anchors:
  - [abstract] "the model spontaneously modifies its segmentation style depending on the input"
  - [section 3.5.2] "when tested on the different databases' test sets, the model spontaneously converts its segmentation style to match the expected one"
  - [corpus] Weak - the related papers focus on lesion segmentation but don't discuss style conversion mechanisms

### Mechanism 2
- Claim: A linear probe trained on encoder features can accurately identify the database origin of an image.
- Mechanism: The encoder features contain sufficient information about the image's origin, which a linear classifier can exploit to predict the source database.
- Core assumption: The encoder learns features that capture database-specific characteristics beyond just the lesion content.
- Evidence anchors:
  - [section 3.5.3] "we build upon the idea of linear probes...to identify each image's database using the encoder's features"
  - [section 4.3] "we measured the ability of the probe P^(l) to predict P^(l)(B^(i))? = i"
  - [corpus] Weak - related work focuses on segmentation performance, not probe-based style detection

### Mechanism 3
- Claim: Adversarial attacks on the linear probe can modify the input image in a way that forces the segmentation model to adopt a specific annotation style.
- Mechanism: By attacking the probe to predict a target database, the adversarial perturbation also affects the segmentation model's internal representations, causing it to segment in the style of the target database.
- Core assumption: The segmentation model and probe share internal representations, so attacking one affects the other.
- Evidence anchors:
  - [section 3.6] "we propose to modify the image using adversarial attacks...forcing it to adopt the style of our choice"
  - [section 4.4.2] "the adversarial attack does not only affect the probe, but also the whole segmentation model"
  - [corpus] Weak - related papers discuss adversarial attacks for segmentation but not for style conversion

## Foundational Learning

- Concept: Domain adaptation and distribution shift
  - Why needed here: Understanding how models handle differences between training and test data distributions is crucial for grasping why combining databases with different annotation styles is challenging
  - Quick check question: What is the difference between domain adaptation and style conversion in the context of this paper?

- Concept: Adversarial attacks and their application to segmentation
  - Why needed here: The paper uses adversarial attacks not to fool the model, but to control its behavior, which requires understanding how these attacks work
  - Quick check question: How does the Fast Gradient Sign Method work, and why is it suitable for this application?

- Concept: Uncertainty estimation in segmentation
  - Why needed here: The paper proposes using style conversion to generate uncertainty maps, so understanding uncertainty estimation methods is important
  - Quick check question: What is the difference between aleatoric and epistemic uncertainty in segmentation?

## Architecture Onboarding

- Component map: Segmentation model (UNet with ResNet-34) -> Linear probe (on encoder features) -> Adversarial attack module (gradient-based perturbation)

- Critical path: 1. Train segmentation model on combined databases 2. Train linear probe to detect database origin from encoder features 3. Apply adversarial attacks to manipulate probe predictions 4. Use perturbed images for style-converted segmentation

- Design tradeoffs:
  - Simple linear probe vs. complex classifier: Simpler is better for interpretability and efficiency
  - Magnitude of adversarial perturbation: Must be small enough to preserve semantics but large enough to affect style
  - Probe placement: Lower encoder levels provide better accuracy but less context

- Failure signatures:
  - Probe accuracy below 50%: Not better than random guessing
  - Adversarial perturbations cause visible artifacts: Too large, breaking the imperceptible requirement
  - Style conversion doesn't improve external dataset performance: Probe not capturing relevant style information

- First 3 experiments:
  1. Train segmentation model on IDRiD only, test on all datasets to observe style bias
  2. Add linear probe to segmentation model and measure origin detection accuracy
  3. Apply adversarial attack to convert IDRiD-style segmentations to RETINAL-LESIONS style and visually inspect results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which the model detects the database origin from the input image, and how robust is this detection to various image perturbations?
- Basis in paper: [explicit] The paper mentions that the model spontaneously converts its segmentation style based on the input image and that a linear probe can detect the dataset origin from the encoder features. However, the exact mechanism and robustness to perturbations are not fully explored.
- Why unresolved: The paper does not provide a detailed analysis of the specific features or patterns in the images that the model uses to detect the database origin, nor does it extensively test the robustness of this detection to various perturbations.
- What evidence would resolve it: A detailed analysis of the features used by the model to detect database origin, along with extensive testing of the model's detection robustness to various image perturbations, would provide clarity.

### Open Question 2
- Question: How does the adversarial style conversion method perform on databases with more subtle differences in annotation styles, and can it generalize to databases with entirely different lesion types or imaging modalities?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the adversarial style conversion method on databases with distinct annotation styles but does not explore its performance on databases with more subtle differences or entirely different lesion types or imaging modalities.
- Why unresolved: The paper's experiments are limited to a specific set of databases with distinct annotation styles, and it does not explore the method's performance on databases with more subtle differences or entirely different lesion types or imaging modalities.
- What evidence would resolve it: Experiments testing the adversarial style conversion method on databases with more subtle differences in annotation styles, as well as on databases with entirely different lesion types or imaging modalities, would provide insight into its generalizability.

### Open Question 3
- Question: How does the proposed method compare to other uncertainty estimation techniques in terms of accuracy and computational efficiency, and can it be integrated with other deep learning architectures for semantic segmentation?
- Basis in paper: [explicit] The paper mentions that the proposed method for uncertainty estimation is inspired by Garifullin et al. (2021) and proposes a local perturbation-based approach. However, it does not compare this method to other uncertainty estimation techniques or discuss its integration with other deep learning architectures.
- Why unresolved: The paper does not provide a comprehensive comparison of the proposed uncertainty estimation method with other techniques or discuss its integration with other deep learning architectures.
- What evidence would resolve it: A comparative study of the proposed uncertainty estimation method with other techniques, as well as an exploration of its integration with other deep learning architectures, would provide a clearer understanding of its strengths and limitations.

## Limitations
- The approach assumes annotation styles are visually distinguishable in encoder features, which may not hold for highly standardized or similar annotation protocols
- Adversarial perturbations are constrained to be imperceptible, but small perturbations may be insufficient to overcome strong style biases
- The method requires multiple datasets with different annotation styles, limiting applicability when only single-style data is available

## Confidence
- High: The spontaneous style adaptation phenomenon is observed and measurable (Probe accuracy and qualitative style conversion results)
- Medium: The adversarial attack mechanism successfully controls style conversion (Attack effectiveness demonstrated but perturbation magnitude tradeoffs need validation)
- Low: The clinical utility of converted segmentations (Visual inspection suggests plausibility but quantitative clinical validation is needed)

## Next Checks
1. Measure probe accuracy across different encoder levels to determine optimal placement and confirm style information is captured at multiple feature levels
2. Quantify semantic preservation after adversarial perturbations using metrics like SSIM and perceptual loss alongside clinical evaluation of lesion detection accuracy
3. Test style interpolation between annotation styles by targeting intermediate probe outputs and evaluating the continuous style spectrum generated