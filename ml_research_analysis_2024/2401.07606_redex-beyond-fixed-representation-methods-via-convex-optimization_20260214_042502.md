---
ver: rpa2
title: 'RedEx: Beyond Fixed Representation Methods via Convex Optimization'
arxiv_id: '2401.07606'
source_url: https://arxiv.org/abs/2401.07606
tags:
- u1d456
- u1d434
- u1d449
- u1d461
- u1d443
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RedEx, a novel neural network architecture
  that bridges the gap between neural networks and fixed representation methods like
  kernels. RedEx can be efficiently trained layer-wise using convex semi-definite
  programming while maintaining the expressiveness of neural networks.
---

# RedEx: Beyond Fixed Representation Methods via Convex Optimization

## Quick Facts
- arXiv ID: 2401.07606
- Source URL: https://arxiv.org/abs/2401.07606
- Authors: Amit Daniely; Mariano Schain; Gilad Yehudai
- Reference count: 40
- Primary result: RedEx bridges neural networks and fixed representation methods by enabling expressive feature learning with convex optimization guarantees

## Executive Summary
RedEx is a novel neural network architecture that combines the expressiveness of neural networks with the optimization guarantees of fixed representation methods. The key innovation is a reduced extractor-expander architecture that performs quadratic expansion of extracted features while maintaining convexity in the optimization landscape. RedEx can be trained layer-wise using convex semi-definite programming, enabling polynomial-time training while preserving the ability to learn complex hierarchical representations that fixed methods cannot capture.

The paper demonstrates a theoretical separation between RedEx and fixed representation methods by showing that RedEx can efficiently learn a variation of the sparse parity function under certain distributions, while polynomial-time fixed representation methods cannot. The authors also extend RedEx to convolutional settings and provide a norm formulation that allows training with standard gradient descent for one-dimensional outputs.

## Method Summary
RedEx uses a reduced extractor-expander architecture where input features are extracted through orthogonal matrices with bounded Frobenius norm, then expanded quadratically to enable expressive computation. The architecture is trained layer-wise using convex semi-definite programming with orthogonality constraints on extractor matrices. For one-dimensional outputs, the RedEx norm is equivalent to the trace norm, allowing unconstrained convex optimization with standard gradient methods instead of SDP. The layer-wise approach builds hierarchical representations while avoiding the hardness of joint optimization.

## Key Results
- RedEx provably surpasses fixed representation methods by efficiently learning a family of target functions (sparse parity variation) that fixed methods cannot learn
- The architecture maintains convexity guarantees while being as expressive as neural networks
- For one-dimensional outputs, RedEx norm equivalence to trace norm enables efficient gradient-based training without SDP constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RedEx bridges neural networks and fixed representation methods by learning expressive features while retaining convex optimization guarantees.
- Mechanism: RedEx uses a reduced extractor-expander architecture that performs a quadratic expansion of extracted features, enabling expressive computation while maintaining convexity in the optimization landscape.
- Core assumption: The feature extraction can be parameterized as orthogonal matrices with bounded Frobenius norm, and the quadratic expansion preserves convexity in the resulting objective.
- Evidence anchors:
  - [abstract]: "RedEx... is as expressive as neural networks and can also be trained in a layer-wise fashion via a convex program with semi-definite constraints and optimization guarantees."
  - [section 3]: "We present an efficient polynomial-time algorithm for training RedEx, based on convex Semi-definite Programming (SDP)"
  - [corpus]: Weak - neighbors discuss convexity and optimization but not specifically RedEx's mechanism.
- Break condition: If the orthogonality constraint is violated or the quadratic expansion introduces non-convexity, the training algorithm may fail to find the global optimum.

### Mechanism 2
- Claim: RedEx can learn functions that fixed representation methods cannot by exploiting layer-wise representation learning.
- Mechanism: The layer-wise training allows RedEx to build hierarchical representations that fixed methods cannot achieve, demonstrated by learning a variation of the sparse parity function under certain distributions.
- Core assumption: The input distribution allows the extractor to identify relevant features, and the layer-wise training can build increasingly complex representations.
- Evidence anchors:
  - [abstract]: "We also show that RedEx provably surpasses fixed representation methods, in the sense that it can efficiently learn a family of target functions which fixed representation methods cannot."
  - [section 5]: "We consider the problem of learning a function of the form... RedEx can efficiently learn, while any fixed-representation methods cannot."
  - [corpus]: Weak - neighbors discuss limitations of fixed representations but not the specific separation achieved by RedEx.
- Break condition: If the input distribution doesn't allow effective feature extraction or the layer-wise training gets stuck in poor local optima, RedEx may fail to learn the target functions.

### Mechanism 3
- Claim: The RedEx norm provides an efficient alternative to SDP constraints for training, particularly for one-dimensional outputs.
- Mechanism: For one-dimensional outputs, the RedEx norm is equivalent to the trace norm, allowing unconstrained convex optimization with standard gradient methods instead of SDP.
- Core assumption: When the output dimension is 1, the RedEx norm can be efficiently computed and optimized without SDP.
- Evidence anchors:
  - [section 6.1]: "For this case, we can use the characterization in Lemma 6.2, where for /u1D458= 1 the RedEx norm is equivalent to the trace norm."
  - [section 6.1]: "In this case, we can replace the minimization problem... by: min... + /u1D7061 ∥ /u1D434 ∥Tr + 2/u1D7062 ∥ /u1D434 ∥2 fr"
  - [corpus]: Weak - neighbors discuss trace norms but not the specific RedEx norm equivalence.
- Break condition: If the output dimension exceeds 1, the RedEx norm is no longer equivalent to the trace norm, and the efficient gradient-based training approach fails.

## Foundational Learning

- Concept: Semi-definite programming and convex optimization
  - Why needed here: RedEx training relies on SDP to handle orthogonality and norm constraints while maintaining convexity
  - Quick check question: Can you explain why the RedEx constraint "− /u1D445 ⪯ /u1D434 /u1D456 ⪯ /u1D445" ensures both the extractor orthogonality and the bounded norm?

- Concept: Feature extraction and quadratic expansion
  - Why needed here: RedEx's expressive power comes from extracting relevant features and expanding them quadratically
  - Quick check question: How does the quadratic expansion "( /u1D449 x) ⊗ 2" in the extractor-expander increase the model's expressiveness compared to linear transformations?

- Concept: Layer-wise training vs. end-to-end training
  - Why needed here: RedEx uses sequential layer training to avoid the hardness of joint optimization while building hierarchical representations
  - Quick check question: What's the key advantage of training RedEx layer-by-layer compared to training a traditional neural network with backpropagation?

## Architecture Onboarding

- Component map:
  Input: ℝ^d → Extractor (A) → Quadratic expansion → Linear transformation (V) → Output
  Layer-wise: Each layer takes previous layer's output as input, trained separately with fresh data
  Constraints: ∥A∥_F^2 ≤ width, ∥V∥_sp ≤ 1 for spectral norm

- Critical path:
  1. Initialize extractor (A) with orthogonal rows
  2. Compute quadratic features (A x) ⊗ 2
  3. Apply linear transformation (V) to features
  4. Compute loss and optimize via SDP or gradient methods
  5. For multi-layer: use layer output as next layer input

- Design tradeoffs:
  - Width parameter: Higher width → more expressive but requires more samples for generalization
  - Layer-wise vs. end-to-end: Layer-wise enables convex optimization but may miss global optima
  - SDP vs. gradient: SDP guarantees global optimum but is computationally heavier; gradient is faster but may get stuck

- Failure signatures:
  - Training diverges: Check orthogonality constraints on A are being enforced
  - Poor generalization: Width parameter may be too large relative to available data
  - Layer outputs collapse: Check that each layer is actually learning useful representations

- First 3 experiments:
  1. Train single-layer RedEx on a simple quadratic function to verify basic functionality
  2. Compare single-layer RedEx with kernel methods on sparse parity to demonstrate separation
  3. Train multi-layer RedEx on a hierarchical feature learning task to verify layer-wise benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the rate of generalization in Theorem 4.4 be improved from 1/sqrt(A) to 1/A, similar to what is done in Wang and Lin (2021), but for non-smooth regularizers?
- Basis in paper: [inferred] from the discussion of generalization bounds in Section 4, where the authors note that their result scales at a rate of 1/sqrt(A) and pose it as an interesting question whether this rate can be improved.
- Why unresolved: The authors do not provide a proof or discussion of how to achieve a faster rate of generalization.
- What evidence would resolve it: A proof showing that the generalization bound can indeed be improved to 1/A for non-smooth regularizers, or a counterexample demonstrating that such an improvement is not possible.

### Open Question 2
- Question: Is there a simple expression for the RedEx norm ∥·∥Rx when the output dimension is greater than 1, similar to the case when the output dimension is 1?
- Basis in paper: [explicit] from Lemma 6.2 (3), which provides a simple expression for the RedEx norm when the output dimension is 1, but the authors note that they are not aware of a simple expression for ∥·∥Rx when the output dimension is greater than 1.
- Why unresolved: The authors do not provide a proof or discussion of how to derive a simple expression for the RedEx norm in the general case.
- What evidence would resolve it: A proof deriving a simple expression for the RedEx norm when the output dimension is greater than 1, or a demonstration that no such simple expression exists.

### Open Question 3
- Question: Can the layer-wise training approach of RedEx be extended to provide stronger separation results between RedEx and fixed representation methods under milder assumptions, such as when the output is one-dimensional?
- Basis in paper: [inferred] from the discussion of limitations and future work in Section 6, where the authors mention that it is interesting to provide stronger separation results between RedEx and fixed representation methods under milder assumptions.
- Why unresolved: The authors do not provide a proof or discussion of how to extend the layer-wise training approach to achieve stronger separation results under milder assumptions.
- What evidence would resolve it: A proof demonstrating that the layer-wise training approach of RedEx can be extended to provide stronger separation results between RedEx and fixed representation methods under milder assumptions, or a counterexample showing that such an extension is not possible.

## Limitations

- The paper provides theoretical guarantees but lacks practical implementation details, particularly around optimal regularization parameter tuning for layer-wise training
- SDP solvers for RedEx may face scalability challenges for larger problems, with computational complexity not thoroughly analyzed
- Generalization bounds assume specific data distributions and width parameters without concrete guidance for real-world applications

## Confidence

- **High Confidence**: The theoretical separation result between RedEx and fixed representation methods is well-supported by rigorous proofs in Sections 4 and 5.
- **Medium Confidence**: The layer-wise convex optimization framework appears sound, but practical performance may vary depending on implementation choices and problem-specific parameters.
- **Low Confidence**: The generalization bounds in Section 6.2 assume specific data distributions and width parameters without providing concrete guidance for real-world applications.

## Next Checks

1. Implement a minimal RedEx prototype to verify that the SDP formulation can be solved efficiently for small-scale problems and that the orthogonality constraints are properly enforced.
2. Conduct empirical experiments comparing RedEx with kernel methods on the sparse parity function to validate the theoretical separation result under practical conditions.
3. Test the RedEx norm formulation for one-dimensional outputs to confirm that gradient-based training can match SDP-based results in terms of convergence speed and solution quality.