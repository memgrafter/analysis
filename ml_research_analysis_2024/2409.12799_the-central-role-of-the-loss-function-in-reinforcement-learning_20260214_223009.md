---
ver: rpa2
title: The Central Role of the Loss Function in Reinforcement Learning
arxiv_id: '2409.12799'
source_url: https://arxiv.org/abs/2409.12799
tags:
- loss
- learning
- bounds
- where
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper highlights the central role of loss functions in reinforcement
  learning, demonstrating how different regression loss functions significantly impact
  sample efficiency and adaptivity of value-based decision-making algorithms. The
  authors prove that algorithms using the binary cross-entropy (BCE) loss achieve
  first-order bounds scaling with the optimal policy's cost and are much more efficient
  than the commonly used squared loss.
---

# The Central Role of the Loss Function in Reinforcement Learning

## Quick Facts
- arXiv ID: 2409.12799
- Source URL: https://arxiv.org/abs/2409.12799
- Reference count: 40
- Key outcome: Different regression loss functions significantly impact sample efficiency and adaptivity of RL algorithms, with BCE achieving first-order bounds and MLE achieving second-order bounds.

## Executive Summary
This paper establishes that the choice of loss function is a critical determinant of sample efficiency in reinforcement learning. The authors prove that binary cross-entropy (BCE) loss enables first-order PAC bounds that scale with the optimal policy's cost rather than √V⋆, achieving O(1/n) convergence in low-cost regimes. They further demonstrate that maximum likelihood estimation (MLE) loss yields second-order PAC bounds scaling with policy variance, providing even sharper guarantees than first-order bounds. These theoretical results are complemented by empirical evidence showing the practical benefits of BCE and MLE over traditional squared loss across online, offline, and hybrid RL settings.

## Method Summary
The paper proposes RL algorithms that use different loss functions - squared loss, binary cross-entropy (BCE), and maximum likelihood estimation (MLE) - for both online and offline settings. The algorithms construct version spaces based on empirical losses and select optimistic or pessimistic policies for exploration. For distributional RL variants, the algorithms learn full conditional distributions instead of just means. The theoretical analysis focuses on proving regret and PAC bounds for these algorithms under various assumptions including realizability and Bellman completeness.

## Key Results
- BCE loss achieves first-order PAC bounds scaling with optimal policy cost V⋆, yielding O(1/n) convergence in low-cost regimes
- MLE loss achieves second-order PAC bounds scaling with policy variance, providing sharper O(1/n) convergence in near-deterministic settings
- Pessimistic MLE removes dependence on learned policy variance in offline RL, yielding tighter bounds

## Why This Works (Mechanism)

### Mechanism 1
BCE loss directly bounds policy suboptimality via squared Hellinger distance between Bernoulli distributions, linking to optimal cost through a deterministic inequality. This allows regret to scale with V⋆ instead of √V⋆ as in squared loss. Core assumption: Realizability of the conditional mean function and bounded costs in [0,1]. Break condition: If V⋆≳1/√n, BCE loss degrades to √n rate.

### Mechanism 2
MLE loss bounds policy suboptimality via squared Hellinger distance between full conditional distributions, which decomposes into a term scaling with the variance of the optimal policy. This enables faster convergence when the policy is near-deterministic. Core assumption: Distributional realizability and bounded costs in [0,1]. Break condition: If σ²(π⋆) is large, the second-order term dominates and the bound becomes worse than first-order.

### Mechanism 3
Pessimistic MLE removes dependence on learned policy variance by learning a pessimistic distribution that upper bounds the true mean. The regret decomposition only involves the variance of the comparator policy, which is a fixed quantity. Core assumption: Distributional realizability plus ability to optimize over a version space. Break condition: If the comparator policy's variance σ²(˜π) is large or version space optimization is intractable.

## Foundational Learning

- Concept: Hellinger distance and its relationship to mean/variance differences
  - Why needed here: Central to all three mechanisms for translating distributional prediction errors into policy regret bounds
  - Quick check question: What is the relationship between squared Hellinger distance h²(p,q) and mean difference |p̄-q̄| for distributions on [0,1]?

- Concept: Eluder dimension as a measure of function class complexity for exploration
  - Why needed here: Used to bound the error between online TD errors and their expectations, crucial for proving regret bounds in RL
  - Quick check question: How does the ℓ₂-eluder dimension relate to the number of samples needed to learn a function class in RL?

- Concept: Bellman completeness and its role in ensuring TD learning converges to the correct Q-function
  - Why needed here: Without BC, TD learning can diverge or converge to suboptimal points even with realizability
  - Quick check question: What is the difference between Bellman completeness and Bellman realizability, and why is BC needed for sample-efficient RL?

## Architecture Onboarding

- Component map: Online RL (Algorithm 2) → version space construction → optimistic selection → data collection → repeat. Offline RL (Algorithm 4) → version space construction → pessimistic selection → output. Distributional RL variants use MLE loss and learn full distributions instead of means.
- Critical path: For online RL: 1) Compute empirical TD loss for each f∈F, 2) Construct version space Cβ(D), 3) Select optimistic f∈Cβ(D), 4) Roll-in with greedy policy, 5) Collect data, 6) Repeat. For distributional RL: same structure but with MLE loss and full distribution predictions.
- Design tradeoffs: BCE loss offers better first-order bounds but requires only mean predictions (easier). MLE loss offers best second-order bounds but requires full distribution predictions (harder) and stronger assumptions (distributional realizability). Pessimistic variants remove dependence on learned policy variance but require optimizing over version spaces (NP-hard in general).
- Failure signatures: 1) BCE loss degrades to √n rate when V⋆ is not small. 2) MLE loss fails when σ²(π⋆) is large. 3) Pessimistic MLE becomes intractable when version space is large. 4) All variants fail when BC/DistBC is violated.
- First 3 experiments:
  1. Implement CSC with squared loss, BCE loss, and MLE loss on synthetic data with varying V⋆ and σ² to verify first- and second-order rate differences.
  2. Implement online RL with squared loss vs BCE loss on low-rank MDPs with small optimal cost to verify V⋆ scaling.
  3. Implement pessimistic MLE for offline RL on a simple MDP with known optimal policy variance to verify variance removal.

## Open Questions the Paper Calls Out

### Open Question 1
Does pessimistic MLE consistently outperform optimistic MLE across diverse reinforcement learning environments?
Basis in paper: [explicit] The paper contrasts pessimistic MLE (achieving bounds dependent only on optimal policy variance) with optimistic MLE (achieving bounds dependent only on learned policy variance) in Theorem 4, noting that pessimistic bounds may be preferred since optimal policy variance is a fixed quantity.
Why unresolved: The paper proves theoretical bounds but does not provide empirical comparisons between pessimistic and optimistic approaches across different environments.
What evidence would resolve it: Controlled experiments comparing regret and sample efficiency of pessimistic vs optimistic MLE algorithms across multiple MDP classes (e.g., low-rank, block MDPs) with varying variance structures.

### Open Question 2
How does the computational complexity of optimizing over version spaces scale with function class size in practical reinforcement learning settings?
Basis in paper: [explicit] The paper acknowledges that version space optimization is NP-hard even in tabular MDPs (page 10) and suggests computational efficiency as a limitation of the proposed algorithms.
Why unresolved: While the paper proves statistical efficiency of version space methods, it does not analyze their computational complexity or provide scalable implementations.
What evidence would resolve it: Empirical studies measuring computational runtime of version space optimization as function of state space size, action space size, and horizon in benchmark RL environments, comparing with alternative exploration strategies.

### Open Question 3
Can the benefits of distributional RL be achieved with less stringent assumptions than distributional Bellman completeness?
Basis in paper: [inferred] The paper notes that distributional Bellman completeness (Assump. 5) is a stronger requirement than standard Bellman completeness (Assump. 3) and that the conditional distribution class P is generally larger than the regression class F (page 14).
Why unresolved: The theoretical results require distributional Bellman completeness, but the paper acknowledges this may be more stringent than standard assumptions while noting empirical success of distributional methods.
What evidence would resolve it: Development and analysis of distributional RL algorithms that achieve similar theoretical guarantees under weaker assumptions, such as partial distributional completeness or realizability of certain statistics rather than full distributions.

## Limitations
- Theoretical claims rely on strong realizability assumptions that may not hold in practice
- Empirical validation limited to synthetic examples rather than complex RL benchmarks
- Computational tractability of pessimistic MLE variant unclear, particularly version space optimization in high dimensions

## Confidence

- High: BCE loss achieves first-order bounds scaling with optimal cost V⋆ (supported by formal proofs in Section 2.3)
- High: MLE loss achieves second-order bounds scaling with policy variance (supported by formal proofs in Section 2.4)
- Medium: Pessimistic MLE effectively removes dependence on learned policy variance in offline RL (proofs provided but no empirical validation)
- Medium: Computational feasibility of version space methods in practice (theoretical analysis only)

## Next Checks

1. **Empirical scaling verification**: Implement synthetic experiments varying V⋆ and σ² to empirically verify the claimed O(1/n) vs O(1/√n) convergence rates for BCE vs squared loss, and O(1/n) vs O(1/√n) for MLE vs BCE in low-variance regimes.

2. **Realizability robustness test**: Evaluate the algorithms under approximate realizability conditions where the optimal Q-function is only approximately in the hypothesis class, measuring degradation in performance as approximation error increases.

3. **Computational tractability assessment**: Benchmark the runtime and memory requirements of the pessimistic MLE variant on MDPs of increasing state-action space size, comparing against standard TD-learning approaches to quantify the practical cost of variance removal.