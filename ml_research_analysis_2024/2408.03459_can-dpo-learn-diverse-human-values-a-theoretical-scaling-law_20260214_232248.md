---
ver: rpa2
title: Can DPO Learn Diverse Human Values? A Theoretical Scaling Law
arxiv_id: '2408.03459'
source_url: https://arxiv.org/abs/2408.03459
tags:
- learning
- reward
- arxiv
- training
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the generalization behavior of preference learning
  in large language models (LLMs) using direct preference optimization (DPO). The
  authors introduce a theoretical framework that studies how models trained with DPO
  can correctly distinguish between preferred and non-preferred responses, particularly
  focusing on the dynamics of reward margins throughout training.
---

# Can DPO Learn Diverse Human Values? A Theoretical Scaling Law

## Quick Facts
- arXiv ID: 2408.03459
- Source URL: https://arxiv.org/abs/2408.03459
- Authors: Shawn Im; Sharon Li
- Reference count: 40
- Primary result: Theoretical framework showing DPO models can correctly predict preferences on unseen data with high probability, validated on LLaMA-2-7B using Anthropic Persona dataset

## Executive Summary
This paper analyzes the generalization behavior of preference learning in large language models using Direct Preference Optimization (DPO). The authors introduce a theoretical framework that studies how models trained with DPO can correctly distinguish between preferred and non-preferred responses, particularly focusing on the dynamics of reward margins throughout training. They provide rigorous learning guarantees showing that under specific conditions, models can correctly predict preferences on unseen data with high probability. Empirically, they validate their theoretical insights on contemporary LLMs using the Anthropic Persona dataset, demonstrating that training reward margins grow more rapidly with fewer preference concepts (clusters) and that test reward margins follow the same trend.

## Method Summary
The method involves training LLaMA-2-7B using DPO on the Anthropic Persona dataset, which contains 135 behavioral styles with 500 aligned and 500 misaligned statements each. The training procedure uses AdamW optimizer with learning rate 1e-5 and batch size 32 per GPU. The theoretical framework analyzes reward margin trajectories during training, providing learning guarantees based on the assumption that preference data consists of K pairs of clusters with Q i.i.d. samples per cluster. The key metric is the reward margin (log-likelihood difference between preferred and non-preferred responses), which evolves predictably under gradient flow.

## Key Results
- Training reward margins grow more rapidly with fewer preference concepts (clusters)
- Test reward margins follow the same trend as training margins, validating theoretical predictions
- Generalization error decreases as the number of samples per concept increases
- Empirical validation on LLaMA-2-7B confirms theoretical scaling law predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model can correctly distinguish preferred vs non-preferred responses on unseen data by analyzing reward margin trajectories during training.
- Mechanism: The reward margin (log-likelihood difference between preferred and non-preferred responses) evolves predictably under gradient flow. By bounding this trajectory, we can guarantee correct classification of both training and new samples.
- Core assumption: The reward margin dynamics follow the differential equation τ˙ri = (1/N)Σβ²σ(-ri)C(xi,xj) where C captures preference sharing and embedding correlations.
- Evidence anchors:
  - [abstract] "We provide rigorous learning guarantees showing that under specific conditions, models trained with DPO can correctly predict preferences on unseen data with high probability."
  - [section 3.2] "The crux of our framework thus lies in analyzing the reward associated with each sample and its evolution throughout training."
  - [corpus] Weak evidence - no direct citations to similar reward margin trajectory analysis in related works.
- Break condition: If the embedding correlation structure C(xi,xj) becomes too complex or if preference sharing patterns are irregular, the trajectory bounds may not hold.

### Mechanism 2
- Claim: The generalization error decreases as the number of samples per concept increases, due to stronger learning guarantees.
- Mechanism: More samples per concept (higher Q) reduces the time needed to achieve a given training loss and strengthens the guarantee on training samples, which extends to better generalization bounds.
- Core assumption: The data distribution consists of K pairs of clusters representing different concepts, with Q i.i.d. samples from each cluster.
- Evidence anchors:
  - [abstract] "Our theorems indicate that as the number of samples per concept increases, the time needed to achieve a given training loss or generalization bound decreases."
  - [section 4.2] "As the number of samples per cluster or Q increases, the guarantee on the training samples becomes stronger and reduces the training time needed for the guarantee."
  - [corpus] Weak evidence - related works focus on preference optimization methods but not on sample quantity scaling effects.
- Break condition: If the concepts are not well-separated in embedding space or if the number of clusters K becomes too large relative to dimension d, the guarantees weaken.

### Mechanism 3
- Claim: The model's behavior on new samples depends on their correlation with training samples, which can lead to unexpected behavior if new samples are not well-represented in training.
- Mechanism: The reward dynamics for new samples follow the same form as training samples, weighted by their embedding correlations. Poor representation in training leads to unreliable predictions.
- Core assumption: New samples' reward trajectories are influenced by their correlations with training sample embeddings.
- Evidence anchors:
  - [abstract] "These results shed light on practical aspects of aligning LLMs, helping explain the benefit of scale and characterizing the behavior of alignment loss on new samples."
  - [section 4.2] "Another aspect of the guarantees to consider is that they are for samples within the training distribution. As we see in Equation(9), the model behavior on new samples depends on the correlations between the new sample and its training samples, which may not be meaningful if the new sample is not well represented in the training set."
  - [corpus] Weak evidence - related works mention generalization but don't specifically address representation issues in preference learning.
- Break condition: If the test distribution shifts significantly from the training distribution, or if the embedding space doesn't capture relevant similarities, predictions become unreliable.

## Foundational Learning

- Concept: Bradley-Terry model for preference distributions
  - Why needed here: Forms the theoretical foundation for how preferences are modeled as σ(r*(x,yw) - r*(x,yl))
  - Quick check question: What does the sigmoid function represent in the Bradley-Terry model?

- Concept: Direct Preference Optimization (DPO) objective
  - Why needed here: The theoretical framework specifically analyzes DPO, which implicitly learns a reward model
  - Quick check question: How does the DPO objective relate to the reward margin?

- Concept: Reward margin as generalization metric
  - Why needed here: The population risk is defined based on whether the reward margin is positive, making it the key metric for preference learning
  - Quick check question: Why is a positive reward margin equivalent to correct preference prediction?

## Architecture Onboarding

- Component map: Input prompt -> embedding layer -> policy output -> reward margin calculation -> gradient update -> new policy parameters
- Critical path: The critical path involves tracking reward margin evolution during training
- Design tradeoffs: Fixed vs tunable feature backbone (affects tractability), single vs multi-token responses (complexity), number of clusters vs dimension (generalization guarantees)
- Failure signatures: Slow reward margin growth indicates poor preference separation; high generalization error suggests insufficient samples per concept or poor embedding correlation structure
- First 3 experiments:
  1. Verify the data assumption by checking cosine similarities between persona embeddings and after mean subtraction
  2. Measure training reward margin growth rate with varying numbers of clusters (K=1,2,4,8,16)
  3. Test generalization by measuring test reward margin on held-out personas

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the theoretical framework extend to other preference optimization methods beyond DPO?
- Basis in paper: [explicit] The paper discusses potential extensions to a more general class of objectives presented in GPO and SimPO in Section 6, noting that the only modification would be replacing the sigmoid function with f'(r_i).
- Why unresolved: The paper acknowledges this as future work without providing concrete theoretical guarantees or empirical validation for these other methods.
- What evidence would resolve it: Formal theoretical analysis showing similar learning guarantees for GPO and SimPO methods, coupled with empirical validation demonstrating the theory holds for these methods.

### Open Question 2
- Question: What is the impact of multi-token responses on the learning dynamics and generalization guarantees?
- Basis in paper: [explicit] Section 4.3 discusses the extension to multi-token generation, noting that the dynamics become significantly more complex and providing a decomposition of the reward into token-wise terms.
- Why unresolved: While the paper provides a decomposition and interpretation of the reward gradient for multi-token responses, it acknowledges that providing strong guarantees regarding training accuracy or generalization is highly non-trivial.
- What evidence would resolve it: Theoretical analysis extending the single-token guarantees to multi-token settings, supported by empirical results demonstrating how the complexity of multi-token responses affects learning dynamics and generalization.

### Open Question 3
- Question: How does the scale of data and model influence the ability to learn diverse human preferences?
- Basis in paper: [inferred] The paper's results suggest that as the number of clusters or concepts increases, more training is necessary, and that increasing scale and diversity of data can bolster a model's ability to generalize.
- Why unresolved: While the paper provides theoretical insights and empirical validation on the impact of the number of clusters, it does not explicitly study the effect of model scale or provide concrete recommendations on how much scale is needed for different levels of preference diversity.
- What evidence would resolve it: Empirical studies varying both model scale and dataset diversity, demonstrating the relationship between scale, diversity, and the ability to learn and generalize diverse human preferences.

## Limitations

- Data Assumption Dependency: The theoretical framework relies heavily on the assumption that preference data consists of well-separated clusters, which may not generalize to all preference datasets.
- Generalization Bound Validity: The bounds depend on the embedding correlation structure C(xi,xj), which may become complex in practice and invalidate the guarantees.
- Scalability Assumptions: The theoretical analysis focuses on samples per concept rather than absolute model size, leaving the relationship between model capacity and alignment quality unclear.

## Confidence

- High Confidence: The empirical observation that training reward margins grow more rapidly with fewer preference concepts is directly validated through experiments on LLaMA-2-7B.
- Medium Confidence: The theoretical learning guarantees are mathematically sound but their practical applicability depends on the validity of the data assumptions.
- Low Confidence: The claim about the importance of scale and diversity for effective alignment, while intuitive, is primarily supported by the theoretical framework rather than comprehensive empirical validation.

## Next Checks

1. Cross-dataset validation: Test the theoretical framework on alternative preference datasets (e.g., OpenWebText, Reddit preferences) with different cluster structures to assess generalizability beyond the Anthropic Persona dataset.

2. Embedding space analysis: Systematically analyze how different embedding methods (sentence transformers, CLIP embeddings) affect the reward margin dynamics and generalization bounds, particularly for datasets with overlapping concepts.

3. Out-of-distribution testing: Evaluate model performance on preference data that deliberately shifts from the training distribution to test the robustness of the generalization guarantees and identify failure modes when new samples are poorly represented in training.