---
ver: rpa2
title: Breaking Symmetry When Training Transformers
arxiv_id: '2402.05969'
source_url: https://arxiv.org/abs/2402.05969
tags:
- attention
- residual
- connections
- causal
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how Transformer architectures retain position
  information without positional encodings, focusing on the role of causal attention
  and residual connections. The authors demonstrate that Transformers without causal
  attention are permutation-invariant, making them unable to model input sequences
  where order matters.
---

# Breaking Symmetry When Training Transformers

## Quick Facts
- arXiv ID: 2402.05969
- Source URL: https://arxiv.org/abs/2402.05969
- Authors: Chunsheng Zuo; Michael Guerzhoy
- Reference count: 3
- Primary result: Transformers without causal attention are permutation-invariant and cannot model input sequences where order matters

## Executive Summary
This paper investigates how Transformer architectures retain position information without positional encodings, focusing on the role of causal attention and residual connections. The authors demonstrate that Transformers without causal attention are permutation-invariant, making them unable to model input sequences where order matters. Through experiments on a 3-digit addition task, they show that removing residual connections impairs model convergence and degrades the retention of position-specific information. Correlation analysis of activations reveals that residual connections help maintain information related to specific token positions within vertical slices of the network. The findings suggest that causal attention is necessary to break symmetry in no-positional-encoding settings, and residual connections contribute to preserving position information across layers.

## Method Summary
The authors use a NanoGPT 6-layer transformer architecture to investigate position information retention. They conduct experiments on a 3-digit addition task where the model must generate answers in reverse order. The study includes baseline training with learnable absolute positional encoding, training without positional encoding, and ablation studies removing residual connections. They perform correlation analysis of activations between layers to analyze how information about specific token positions is preserved. The experiments test whether causal attention and residual connections are necessary for breaking permutation symmetry and retaining position information.

## Key Results
- Transformers without causal attention are permutation-invariant and fail on sequence tasks where order matters
- Removing residual connections impairs model convergence and degrades position information retention
- Correlation analysis shows residual connections help maintain information related to specific token positions within vertical slices of the network

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal attention is necessary to break permutation symmetry in transformers without positional encodings.
- Mechanism: The causal attention mask ensures that each output position can only attend to previous positions, breaking the equivariance under input permutations.
- Core assumption: Without causal attention, the transformer's attention weights are symmetric across input positions, making predictions invariant to input permutations.
- Evidence anchors:
  - [abstract] "The prediction for output token n + 1 of Transformer architectures without one of the mechanisms of positional encodings and causal attention is invariant to permutations of input tokens 1, 2, ..., n − 1."
  - [section 4] "We make the observation that 'non-causal attention' – attention performed using a non-masked attention matrix – is inherently invariant to permutations of the input tokens."
  - [corpus] Weak signal; related works discuss symmetry breaking in transformers but not this specific mechanism.
- Break condition: Removing the causal attention mask or using bidirectional attention breaks the symmetry.

### Mechanism 2
- Claim: Residual connections help retain position-specific information across transformer layers.
- Mechanism: Residual connections create vertical "slices" where information about specific token positions is preserved and propagated through layers.
- Core assumption: Without residual connections, position information gets diluted or mixed across token positions as it flows through layers.
- Evidence anchors:
  - [abstract] "We hypothesize that residual connections contribute to this phenomenon, and demonstrate evidence for this."
  - [section 6] "The observations that there are more pronounced 'off-diagonal' blocks when there are fewer residual connections indicate that residual connections play a role in keeping information from token k in the k-th vertical slice of the transformer."
  - [corpus] Weak signal; no direct corpus evidence for this specific mechanism.
- Break condition: Ablating enough residual connections causes the model to lose position information and fail to converge.

### Mechanism 3
- Claim: Attention mechanisms create correlation structures that preserve positional relationships.
- Mechanism: The attention weights create correlation patterns between activations at different layers that reflect the positional relationships in the input.
- Core assumption: Attention weights between similar tokens remain similar across layers when residual connections are present.
- Evidence anchors:
  - [section 6] "We believe this is caused in part by the fact that WQX1 and WKX2 will tend to be similar when X1 and X2 are similar, making each block attend to the one under it"
  - [corpus] Weak signal; related works discuss attention mechanisms but not this specific correlation-based preservation.
- Break condition: When residual connections are ablated, the correlation structure breaks down and position information is lost.

## Foundational Learning

- Concept: Permutation invariance and equivariance
  - Why needed here: Understanding why transformers without causal attention or positional encodings are permutation invariant is crucial to the paper's main argument.
  - Quick check question: If a transformer uses non-causal attention, would its prediction for token n+1 change if you swap tokens 1 and 2 in the input?

- Concept: Attention mechanisms and masking
  - Why needed here: The paper heavily relies on understanding how causal attention masks work to break symmetry.
  - Quick check question: How does the causal attention mask ensure that output position k only attends to input positions 1 through k?

- Concept: Residual connections and gradient flow
  - Why needed here: The paper argues that residual connections are crucial for preserving position information across layers.
  - Quick check question: What happens to the gradient flow when you remove residual connections from a deep neural network?

## Architecture Onboarding

- Component map:
  Input embeddings → Multi-head attention (with causal mask) → Residual connection → Feed-forward network → Residual connection → Output

- Critical path:
  Input → Attention (with causal mask) → Residual addition → FFN → Residual addition → Output
  The causal attention mask and residual connections are critical for breaking symmetry and preserving position information

- Design tradeoffs:
  Causal attention vs. positional encodings: Both break permutation symmetry but have different computational and representational implications
  Residual connections: Necessary for gradient flow and position information preservation but add parameters and computation

- Failure signatures:
  Model converges with positional encodings but not without them (when causal attention is removed)
  Model converges with both positional encodings and causal attention, but fails when too many residual connections are ablated
  Correlation matrices show loss of block structure when residual connections are removed

- First 3 experiments:
  1. Train transformer with and without causal attention on permutation-sensitive task
  2. Train transformer with and without positional encodings on same task
  3. Train transformer with progressive ablation of residual connections and observe convergence behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which residual connections enable Transformers to retain position information without positional encodings?
- Basis in paper: [explicit] The authors hypothesize that residual connections contribute to retaining position information and demonstrate correlation evidence, but do not provide a complete mechanistic explanation.
- Why unresolved: The paper shows empirical evidence through correlation analysis but does not explain the underlying reason why residual connections specifically help preserve positional information across layers.
- What evidence would resolve it: Detailed ablation studies showing which specific aspects of residual connections (e.g., additive vs. multiplicative, skip connections to different layer outputs) are most critical, combined with theoretical analysis of how these connections preserve or propagate positional information through the network.

### Open Question 2
- Question: How do Transformers without positional encodings generalize to sequences longer than those seen during training?
- Basis in paper: [inferred] The paper demonstrates that Transformers can learn without positional encodings, but does not address the length generalization capability of such models.
- Why unresolved: The experiments focus on fixed-length sequences (3-digit addition), and the paper does not investigate whether the learned position information can scale to longer sequences beyond the training distribution.
- What evidence would resolve it: Experiments testing model performance on sequences longer than those seen during training, measuring the degradation in accuracy as sequence length increases beyond the maximum training length.

### Open Question 3
- Question: Are there alternative architectural mechanisms beyond causal attention and residual connections that could enable Transformers to learn position information without explicit positional encodings?
- Basis in paper: [explicit] The authors focus on causal attention and residual connections as the primary mechanisms, but acknowledge that "positional encodings are not necessary" based on recent results.
- Why unresolved: The paper investigates two specific mechanisms but does not systematically explore other architectural choices (e.g., relative positional encodings, different attention patterns, or novel layer designs) that might also enable position learning.
- What evidence would resolve it: Systematic exploration of alternative architectures that successfully learn position information without positional encodings, comparing their performance and analyzing how they achieve this capability.

## Limitations

- The evidence for residual connections preserving position information is primarily observational through correlation analysis rather than controlled ablation studies
- The correlation analysis approach relies on specific assumptions about information flow through vertical slices that are not fully validated
- The study focuses on a specific 3-digit addition task, which may not generalize to all sequence modeling problems

## Confidence

- Mechanism 1 (Causal attention breaks symmetry): **High** - Strong theoretical argument supported by experimental evidence
- Mechanism 2 (Residual connections preserve position): **Medium** - Observational evidence from correlation analysis, but limited ablation testing
- Mechanism 3 (Attention creates correlation structures): **Low** - Mainly speculative with weak empirical support

## Next Checks

1. **Systematic residual ablation study**: Conduct a more granular ablation study that isolates input-to-pre-MLP and pre-MLP-to-output residual connections separately, with more than 5 random seeds per configuration to establish statistical significance of convergence differences.

2. **Alternative positional encoding comparison**: Test whether learned absolute positional encodings behave differently from causal attention in terms of symmetry breaking, and whether combining both mechanisms provides additive benefits for sequence modeling tasks.

3. **Correlation structure validation**: Verify that the observed correlation patterns in activation matrices are not artifacts of the specific 3-digit addition task by testing on multiple sequence modeling tasks with different input-output relationships.