---
ver: rpa2
title: 'Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study
  in E-Commerce Opinion Summarization'
arxiv_id: '2402.15473'
source_url: https://arxiv.org/abs/2402.15473
tags:
- opinion
- human
- reviews
- reward
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to reduce the amount of human
  preference annotations required for training reward models in reinforcement learning
  from human feedback (RLHF). The key idea is to infuse domain knowledge into the
  reward model by leveraging 7 task-specific features (e.g., aspect-coverage, opinion-faithfulness,
  hallucination) computed by a large language model.
---

# Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization

## Quick Facts
- arXiv ID: 2402.15473
- Source URL: https://arxiv.org/abs/2402.15473
- Reference count: 18
- Humans prefer proposed model's outputs over SOTA 68% of the time

## Executive Summary
This paper introduces a novel approach to reduce human annotation requirements for training reward models in RLHF by leveraging domain knowledge through task-specific features. The method is validated on e-commerce opinion summarization, where it achieves state-of-the-art performance with only 940 human preference samples. The approach demonstrates that interpretable domain features can effectively capture human preferences while significantly reducing annotation costs.

## Method Summary
The approach uses a feature-based reward model that computes 7 domain-specific features (aspect-coverage, opinion-faithfulness, opinion-coverage, conciseness, relevance, hallucination, language-correctness) using a large language model. These features serve as input to a feedforward network that learns to predict human preferences. The policy is trained using Proximal Policy Optimization (PPO) with the feature-based reward model. The method requires two datasets: PROMPT OPIN SUMM (synthetic summaries) and OPIN PREF (human preferences).

## Key Results
- Humans prefer proposed model outputs over SOTA 68% of the time
- Achieves state-of-the-art performance on e-commerce opinion summarization
- Successfully trained with only 940 human preference samples
- Demonstrates alignment with human values while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain knowledge features reduce the effective complexity of the reward model's function space.
- Mechanism: By decomposing the reward function into interpretable features, the model learns a lower-dimensional manifold that captures essential dimensions of human preference.
- Core assumption: Human preferences for opinion summaries can be effectively characterized by a small set of domain-specific features.
- Evidence anchors: [abstract] "φ lies in a low-dimensional manifold, whose dimensions can be deduced using domain knowledge"
- Break condition: If human preferences cannot be effectively captured by a small set of interpretable features.

### Mechanism 2
- Claim: Feature-based reward modeling enables alignment with fewer human preference annotations.
- Mechanism: Using precomputed feature values from Mistral-7B as input to the reward model allows effective learning from smaller datasets.
- Core assumption: Precomputed feature values are sufficiently stable and informative to support effective reward model training.
- Evidence anchors: [abstract] "Our approach advances the state of the art: humans prefer our models' outputs > 68% over the SOTA" with only 940 samples
- Break condition: If precomputed features are not sufficiently accurate or stable.

### Mechanism 3
- Claim: Domain knowledge infusion provides interpretability and avoids alignment tax.
- Mechanism: The feature-based approach makes it clear which aspects of summaries influence human preference, and since the same model can be used across tasks without performance degradation on general benchmarks, no alignment tax occurs.
- Core assumption: Interpretability of the reward model correlates with its effectiveness in capturing human preferences.
- Evidence anchors: [abstract] "Such a formulation for φop brings interpretability – which features influence human preference the most, and is free from Alignment Tax"
- Break condition: If interpretability does not correlate with effectiveness, or if using domain knowledge actually degrades performance on general tasks.

## Foundational Learning

- Concept: Feature engineering and domain knowledge representation
  - Why needed here: The approach depends on identifying and computing meaningful features that capture human preferences in opinion summarization
  - Quick check question: Can you list three features that would be important for evaluating product review summaries?

- Concept: Preference modeling and Bradley-Terry models
  - Why needed here: The reward model is trained on pairwise preference data using techniques like Bradley-Terry
  - Quick check question: What is the Bradley-Terry model used for in the context of preference learning?

- Concept: Reinforcement learning from human feedback (RLHF) pipeline
  - Why needed here: The method extends standard RLHF by using domain knowledge-infused reward models
  - Quick check question: What are the key components of a standard RLHF pipeline?

## Architecture Onboarding

- Component map: Reviews -> Mistral-7B feature extractor -> Reward model φop -> PPO -> BART-Large policy πθ -> Summaries

- Critical path:
  1. Generate synthetic training data with precomputed features
  2. Train reward model φop on human preference data
  3. Train policy πθ using PPO with φop as reward
  4. Evaluate summaries using both automatic metrics and human evaluation

- Design tradeoffs:
  - Using precomputed features vs. computing features online during training (computational efficiency vs. potential staleness)
  - 7 features vs. more/less (coverage vs. complexity)
  - BART-Large vs. larger/smaller models (performance vs. resource requirements)
  - Synthetic data vs. human-annotated data (scalability vs. quality)

- Failure signatures:
  - Reward model not learning (loss plateaus, poor correlation with human preferences)
  - Policy collapse (generating repetitive or degenerate summaries)
  - High computational cost during training
  - Poor generalization to new domains or product categories

- First 3 experiments:
  1. Train reward model φop on OPIN PREF and evaluate feature influence analysis to verify interpretability
  2. Train policy πθ using PPO with φop reward on PROMPT OPIN SUMM and evaluate on validation set
  3. Compare INDUCTIVE-BIAS model against baselines (SUPERVISED, NAIVE MEAN, SYNTH-FEEDBACK) using both automatic metrics and human evaluation on test benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the proposed domain knowledge infusion technique generalize to other NLP tasks beyond e-commerce opinion summarization?
- Basis in paper: [explicit] The paper mentions this is a limitation and suggests future work to verify the approach in other domains.
- Why unresolved: The paper only tests the approach on one specific task (opinion summarization), so its effectiveness for other tasks is unknown.
- What evidence would resolve it: Applying the same technique to other tasks like text classification, machine translation, or other types of summarization, and comparing results to existing methods.

### Open Question 2
- Question: What is the minimum size of the human preference dataset required for the domain knowledge infused reward model to achieve comparable results?
- Basis in paper: [inferred] The paper shows success with 940 samples, but it's unclear if this is the minimum needed.
- Why unresolved: The paper doesn't experiment with varying sizes of the preference dataset.
- What evidence would resolve it: Systematically reducing the size of the preference dataset while measuring performance to find the minimum effective size.

### Open Question 3
- Question: How do the feature weights in the domain knowledge infused reward model change across different domains or tasks?
- Basis in paper: [inferred] The paper shows the relative influence of features for opinion summarization, but doesn't explore how these might vary.
- Why unresolved: The paper only analyzes feature importance for one specific task.
- What evidence would resolve it: Computing feature importance weights for multiple different tasks/domains and comparing them.

### Open Question 4
- Question: How does the proposed approach compare to other techniques for reducing human annotation requirements, such as active learning or semi-supervised learning?
- Basis in paper: [explicit] The paper mentions RLAIF as an alternative but doesn't compare to other techniques.
- Why unresolved: The paper only compares to RLHF baselines, not other annotation-efficient methods.
- What evidence would resolve it: Implementing and comparing to other annotation-efficient techniques on the same tasks.

## Limitations
- Reliance on domain-specific features may limit generalizability to other summarization tasks
- Computational efficiency gains depend on assumption that precomputed features remain stable
- Synthetic data generation introduces potential biases that could affect final summary quality

## Confidence

- **High Confidence**: The effectiveness of domain knowledge infusion in reducing annotation requirements for e-commerce opinion summarization is well-supported by both automatic metrics and human evaluation (68% preference rate).
- **Medium Confidence**: The generalizability of this approach to other domains and tasks requires further validation, as the current work focuses specifically on one application area.
- **Medium Confidence**: The computational efficiency claims are reasonable given the methodology, but comprehensive benchmarking against alternative approaches would strengthen these claims.

## Next Checks

1. **Cross-Domain Validation**: Test the same feature-based reward modeling approach on a different summarization domain (e.g., news or scientific abstracts) to assess generalizability of the methodology.

2. **Feature Stability Analysis**: Conduct experiments measuring how feature values computed by Mistral-7B evolve during training, and assess whether feature drift affects reward model performance.

3. **Alternative Feature Sets**: Experiment with different combinations of features (more, fewer, or different features) to identify which specific features contribute most to the model's effectiveness and human preference alignment.