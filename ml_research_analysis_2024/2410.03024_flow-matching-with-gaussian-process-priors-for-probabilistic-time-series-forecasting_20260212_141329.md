---
ver: rpa2
title: Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting
arxiv_id: '2410.03024'
source_url: https://arxiv.org/abs/2410.03024
tags:
- prior
- conditional
- time
- tsflow
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TSFlow is a conditional flow matching model for time series forecasting
  that incorporates Gaussian process priors to simplify the generative process. It
  uses informed priors to align better with data distribution and reduce complexity.
---

# Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting

## Quick Facts
- arXiv ID: 2410.03024
- Source URL: https://arxiv.org/abs/2410.03024
- Reference count: 40
- TSFlow achieves state-of-the-art performance on 8 real-world datasets, outperforming diffusion-based methods

## Executive Summary
TSFlow introduces a conditional flow matching model for probabilistic time series forecasting that incorporates Gaussian process priors to simplify the generative process. By aligning the prior distribution more closely with temporal structure through domain-specific kernels (squared exponential, Ornstein-Uhlenbeck, periodic), TSFlow reduces the complexity of the transport path and achieves superior performance. The model supports both unconditional generation and conditional forecasting through innovative techniques including Langevin dynamics for conditional prior sampling and guidance mechanisms.

## Method Summary
TSFlow builds on Conditional Flow Matching (CFM) by replacing the standard isotropic prior with Gaussian process priors that encode temporal structure. The model uses a DiffWave backbone with S4 layers, trained via mini-batch optimal transport to align prior and data samples. For conditional forecasting, TSFlow employs two approaches: Langevin dynamics to sample from a conditional prior distribution given observed past values, and guidance techniques that modify the vector field to incorporate conditioning information. The architecture includes sinusoidal timestep embeddings, residual blocks with S4 layers, and outputs a vector field for the generative process.

## Key Results
- TSFlow achieves state-of-the-art CRPS on 8 real-world datasets (Electricity, Exchange, KDDCup, M4-Hourly, Solar, Traffic, UberTLC-Hourly, Wikipedia)
- Periodic kernels often yield best results across datasets, with squared exponential and Ornstein-Uhlenbeck kernels also performing well
- TSFlow matches or exceeds performance of diffusion-based methods while requiring fewer numerical function evaluations (NFEs)

## Why This Works (Mechanism)

### Mechanism 1
Using domain-specific Gaussian process priors reduces the complexity of the generative path and improves alignment between prior and data distributions. GP priors encode temporal structure directly, so the prior already captures smooth, rough, or periodic patterns in the data, shortening the transport path from prior to data.

### Mechanism 2
Conditional prior sampling via Langevin dynamics enables an unconditional model to perform conditional forecasting by conditioning the prior distribution on observed past values. Given observed past yp, Langevin dynamics samples from q0(x0 | yp) by combining the gradient of the prior and the gradient of a likelihood term that guides samples toward consistency with yp.

### Mechanism 3
Guidance techniques directly condition the generative process by modifying the vector field to incorporate observed past values, producing forecasts consistent with history. The guidance term s∇xt log pt(yp | xt) is subtracted from the base vector field, pulling the generation trajectory toward samples that match yp.

## Foundational Learning

- **Concept: Flow matching and optimal transport in generative modeling**
  - Why needed here: TSFlow builds on Conditional Flow Matching (CFM), which learns a vector field to transport samples from a prior to a data distribution. Understanding how the vector field is trained via regression and how optimal transport couplings can be used to align samples is foundational.
  - Quick check question: In CFM, what objective is minimized to train the flow field, and what role does the optimal transport coupling play?

- **Concept: Gaussian processes and kernel functions**
  - Why needed here: TSFlow uses GPs as structured priors to encode temporal dependencies. Knowing how kernels (SE, OU, PE) define covariance and how GPR conditions on past observations is essential to grasp why certain priors work better.
  - Quick check question: How does the periodic kernel KPE(τ, τ′) capture repeating patterns, and what does the length scale ℓ control?

- **Concept: Langevin dynamics and score-based sampling**
  - Why needed here: Conditional prior sampling uses Langevin dynamics to sample from q0(x0 | yp). Understanding the update rule, the role of the score function, and the convergence properties is key to implementing and tuning this component.
  - Quick check question: In Langevin dynamics, what two terms are combined in the update, and what is the effect of the step size η?

## Architecture Onboarding

- **Component map**: Input encoder (sinusoidal embeddings + optional past features) -> Backbone (3 residual blocks with S4 layers) -> Output (vector field uθ(t, x) or uθ(t, x, c)) -> Training pipeline (mini-batch OT + regression) -> Inference (ODE solve with optional Langevin/guidance)

- **Critical path**: 1. Sample x0 from prior (or via Langevin for conditional) 2. Sample time t ~ U[0,1] 3. Compute mean µt and sample xt from conditional Gaussian path 4. Compute uθ(t, xt) and loss against true vector field 5. Backprop and update θ 6. At inference: Solve ODE from x0 using uθ to generate forecast

- **Design tradeoffs**: Prior choice: Isotropic (simpler, more flexible) vs GP (structured, may align better, but requires kernel selection); NFEs: More steps improve accuracy but increase compute; GP priors can match performance with fewer NFEs; Conditional sampling: Langevin + guidance vs direct conditional training; latter is more stable but requires retraining

- **Failure signatures**: Poor forecast quality: Likely due to misaligned prior (wrong kernel) or insufficient conditioning (low guidance scale); Unstable training: May arise from ill-conditioned covariance matrices in GP; ensure white noise is added to kernel; Overfitting to past: If guidance scale is too high or Langevin iterations too many, forecasts lose diversity

- **First 3 experiments**:
  1. Train TSFlow with isotropic prior and minimal NFEs; verify unconditional generation quality via Wasserstein distance
  2. Replace isotropic prior with SE kernel; compare Wasserstein distance and LPS on a small dataset (e.g., Electricity)
  3. Implement conditional prior sampling (Langevin, 4 iterations); test on a dataset with missing values in the context window and compare to baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does TSFlow perform on multivariate time series forecasting compared to univariate benchmarks? The paper suggests extending it to multivariate settings using multivariate Gaussian processes but only provides brief comparison results without detailed analysis.

### Open Question 2
What is the optimal kernel selection strategy for different time series characteristics without relying on validation set performance? The paper demonstrates that periodic kernels often yield best results but doesn't provide a principled method for kernel selection based on data properties.

### Open Question 3
How do different guidance strength values (s parameter) affect the trade-off between forecast accuracy and diversity in TSFlow's conditional generation? The paper mentions using s values between 8 and 32 but doesn't systematically analyze how different guidance strengths affect quality-diversity trade-off.

## Limitations
- Specific kernel functions (e.g., periodic) are claimed to consistently yield best results across all datasets, but this lacks rigorous testing against dataset-specific kernel tuning
- Conditional prior sampling via Langevin dynamics is described theoretically but lacks ablation studies on iteration count and step size
- The superiority of Gaussian process priors over isotropic priors is supported by indirect metrics rather than direct analysis of transport cost or KL divergence

## Confidence

- **High confidence**: TSFlow achieves state-of-the-art CRPS on 8 real-world datasets, outperforming diffusion-based and other flow matching baselines
- **Medium confidence**: The use of Gaussian process priors improves alignment with temporal structure and reduces the need for high NFEs compared to isotropic priors
- **Low confidence**: Specific kernel functions (e.g., periodic) consistently yield best results across all datasets; conditional prior sampling via Langevin dynamics is robust and superior to guidance-based conditioning

## Next Checks

1. **Kernel ablation study**: Train TSFlow with SE, OU, and PE kernels on a diverse subset of datasets and compare not only forecasting metrics (CRPS) but also measures of transport cost (Wasserstein distance) and model complexity (number of parameters, NFEs). Include a baseline with an isotropically tuned kernel to isolate the benefit of structured priors.

2. **Conditional sampling comparison**: Implement both Langevin-based conditional prior sampling and guidance-based conditioning on the same dataset (e.g., Traffic). Systematically vary the number of Langevin iterations and guidance scale, and compare forecasting diversity, calibration, and computational cost. Report CRPS, quantile coverage, and qualitative sample diversity.

3. **Transport path analysis**: For a selected dataset, compute and compare the average transport cost (e.g., KL divergence or 2-Wasserstein distance) between prior and data distributions for isotropic vs. GP priors, both unconditional and conditional. Correlate these metrics with forecasting performance and NFE requirements to directly test the claim that GP priors shorten the generative path.