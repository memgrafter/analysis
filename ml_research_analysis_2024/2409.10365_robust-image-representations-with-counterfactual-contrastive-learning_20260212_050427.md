---
ver: rpa2
title: Robust image representations with counterfactual contrastive learning
arxiv_id: '2409.10365'
source_url: https://arxiv.org/abs/2409.10365
tags:
- training
- labels
- counterfactual
- contrastive
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of robustness to domain shifts
  in contrastive learning for medical imaging, specifically focusing on acquisition
  shifts caused by differences in imaging hardware and protocols. The authors propose
  a novel framework called counterfactual contrastive learning, which leverages recent
  advances in causal image synthesis to create realistic cross-domain positive pairs
  for contrastive learning.
---

# Robust image representations with counterfactual contrastive learning

## Quick Facts
- arXiv ID: 2409.10365
- Source URL: https://arxiv.org/abs/2409.10365
- Reference count: 22
- Primary result: Counterfactual contrastive learning improves robustness to domain shifts in medical imaging, achieving up to 6% ROC-AUC improvement on external datasets

## Executive Summary
This paper addresses the challenge of domain shifts in medical imaging caused by differences in imaging hardware and protocols. The authors propose counterfactual contrastive learning, a novel framework that leverages causal image synthesis to create realistic cross-domain positive pairs for contrastive learning. Instead of relying on predefined augmentation pipelines, their method generates domain counterfactuals to simulate realistic scanner differences, encouraging learned representations to be invariant to such domain variations. The approach is evaluated on chest radiography and mammography datasets, demonstrating consistent improvements over standard contrastive learning baselines, particularly for under-represented domains and in limited label settings.

## Method Summary
The proposed counterfactual contrastive learning framework combines Deep Structural Causal Models (DSCM) with hierarchical variational autoencoders (HVAE) to generate domain counterfactuals. The method first trains HVAE models on each modality's training data to learn the causal relationships between images and their domains (scanners). These counterfactual generators then create cross-domain positive pairs by transforming real images into counterfactual versions that simulate different scanner characteristics. The contrastive learning objectives (SimCLR and DINO-v2) are trained using these generated pairs, encouraging the encoder to learn representations invariant to domain variations. The framework is evaluated on two medical imaging modalities across five public datasets, showing consistent improvements in downstream classification tasks compared to standard contrastive learning approaches.

## Key Results
- Counterfactual contrastive learning consistently outperforms standard contrastive learning baselines across all tested datasets
- Improvements are particularly pronounced on under-represented domains, with up to 6% ROC-AUC improvement on external datasets
- The method demonstrates effectiveness for both limited label settings and full label scenarios
- Models trained with counterfactual contrastive learning show better invariance to domain shifts, as evidenced by reduced scanner separability in feature space

## Why This Works (Mechanism)
The method works by explicitly encouraging invariance to domain shifts through the generation of realistic cross-domain positive pairs. By using causal image synthesis to create counterfactual versions of images that simulate different scanner characteristics, the contrastive learning framework learns to map these domain variations to similar representations. This approach addresses the fundamental limitation of standard contrastive learning, which often fails to generalize across domains when the positive pairs are limited to minor augmentations of the same image. The counterfactuals provide a more challenging and realistic form of invariance learning, as they simulate the actual variations that occur between different imaging systems.

## Foundational Learning

### Deep Structural Causal Models (DSCM)
- **Why needed**: Provides the theoretical foundation for modeling causal relationships between images and their domains
- **Quick check**: Verify that the causal graph correctly captures the relationships between images, scanner types, and other relevant variables

### Hierarchical Variational Autoencoders (HVAE)
- **Why needed**: Enables the generation of realistic counterfactual images that maintain content while changing domain attributes
- **Quick check**: Assess the quality of generated counterfactuals through visual inspection and classifier-based detection of domain changes

### Contrastive Learning Objectives (SimCLR, DINO-v2)
- **Why needed**: Provides the framework for learning invariant representations from positive and negative pairs
- **Quick check**: Verify that the contrastive loss effectively pulls together counterfactual pairs while pushing apart negative pairs

## Architecture Onboarding

### Component Map
Data (scans) -> HVAE (counterfactual generation) -> Contrastive Learning (SimCLR/DINO-v2) -> Encoder (ResNet-50/ViT-Base) -> Downstream Classifier

### Critical Path
HVAE training → Counterfactual generation → Contrastive pretraining → Downstream evaluation

### Design Tradeoffs
- Computational cost: Two-stage training process (HVAE + contrastive) vs. single-stage standard contrastive learning
- Flexibility: Method is agnostic to contrastive objective choice but requires domain-specific counterfactual generation
- Realism: Counterfactuals must balance between being realistic enough to be useful while still providing meaningful domain variation

### Failure Signatures
- Poor counterfactual quality leading to ineffective cross-domain pairs
- Insufficient robustness improvements despite successful counterfactual generation
- Computational inefficiency making the approach impractical for large-scale deployment

### First Experiments to Run
1. Train HVAE models on each modality's training data and evaluate counterfactual generation quality
2. Generate counterfactual pairs and measure scanner separability in feature space
3. Compare downstream performance of counterfactual contrastive learning vs. standard contrastive learning on in-distribution and external datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Computational cost of the two-stage training process is not thoroughly discussed
- Performance on truly unseen scanner types in clinical deployment remains untested
- Limited evaluation beyond acquisition and demographic shifts to other types of domain variations

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Core technical approach and implementation quality | High |
| Robustness claims across multiple datasets | Medium |
| Method's generalizability beyond tested applications | Medium |

## Next Checks
1. Test the method on truly unseen scanner types not present in any training data to assess generalization to clinical deployment scenarios
2. Evaluate computational efficiency and training time compared to standard contrastive learning baselines, including both stages of the proposed approach
3. Extend evaluation to additional types of domain shifts (e.g., disease prevalence shifts, protocol variations) to assess broader applicability beyond acquisition differences