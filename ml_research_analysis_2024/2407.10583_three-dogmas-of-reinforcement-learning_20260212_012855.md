---
ver: rpa2
title: Three Dogmas of Reinforcement Learning
arxiv_id: '2407.10583'
source_url: https://arxiv.org/abs/2407.10583
tags:
- learning
- agents
- reinforcement
- reward
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies three implicit assumptions, or "dogmas,"
  in modern reinforcement learning: (1) focusing on modeling environments rather than
  agents, (2) treating learning as finding a solution to a task, and (3) the reward
  hypothesis, which states that all goals can be well thought of as maximization of
  a reward signal. The authors argue that these dogmas have shaped the field but also
  limited its potential.'
---

# Three Dogmas of Reinforcement Learning

## Quick Facts
- arXiv ID: 2407.10583
- Source URL: https://arxiv.org/abs/2407.10583
- Authors: David Abel; Mark K. Ho; Anna Harutyunyan
- Reference count: 23
- Primary result: Identifies three implicit assumptions in modern RL and proposes alternative approaches

## Executive Summary
This paper identifies three implicit assumptions, or "dogmas," in modern reinforcement learning: (1) focusing on modeling environments rather than agents, (2) treating learning as finding a solution to a task, and (3) the reward hypothesis, which states that all goals can be well thought of as maximization of a reward signal. The authors argue that these dogmas have shaped the field but also limited its potential. They propose alternative approaches: (1) emphasizing the study of agents as central objects, (2) considering learning as continual adaptation rather than finding solutions, and (3) recognizing the limitations of reward-based goal specification. The paper does not present specific experimental results or metrics but instead offers a conceptual framework for future research directions in reinforcement learning.

## Method Summary
The paper presents a conceptual critique of modern reinforcement learning through identification and analysis of three implicit assumptions. The authors examine the historical development of the field and its current practices to identify limiting dogmas, then propose alternative research directions. The methodology consists of theoretical analysis, reference to prior work on each dogma's limitations, and discussion of open questions and future opportunities. No experimental methodology or quantitative evaluation is provided.

## Key Results
- Identifies three dogmas shaping modern RL: environment spotlight, learning as finding solutions, and reward hypothesis
- Proposes agent-centric modeling as alternative to environment-focused approaches
- Suggests learning as continual adaptation rather than solution-finding
- Highlights limitations of reward-based goal specification and need for alternative frameworks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Identifying and questioning implicit assumptions in a field can catalyze paradigm shifts by opening new research directions.
- Mechanism: By explicitly naming the three "dogmas," the authors create a cognitive lever that allows researchers to reconsider foundational commitments without discarding the entire framework. This selective critique preserves useful aspects while challenging limiting ones.
- Core assumption: The field has developed around these implicit assumptions without explicit examination, and bringing them to the surface enables conscious choice about their continued relevance.
- Evidence anchors:
  - [abstract] "Modern reinforcement learning has been conditioned by at least three dogmas... These three dogmas shape much of what we think of as the science of reinforcement learning."
  - [section] "These shifts are each subtle departures from three 'dogmas', or implicit assumptions, summarized as follows"
  - [corpus] Weak evidence - no direct corpus support for this specific mechanism of paradigm critique
- Break condition: If the field has already been explicitly examining these assumptions, the mechanism loses novelty and impact.

### Mechanism 2
- Claim: Shifting emphasis from environment-centric to agent-centric modeling can unlock new theoretical frameworks and research questions.
- Mechanism: The authors argue that the dominance of MDP-based environmental modeling has created a blind spot around agent modeling, limiting the field's ability to develop general principles of agency. By proposing agent-centric research, they create space for new mathematical models and analysis tools.
- Core assumption: A canonical mathematical model of agents is both possible and valuable for advancing the field.
- Evidence anchors:
  - [section] "We lack a canonical formal model of an agent... we struggle to even precisely define and differentiate between key agent families such as 'model-based' and 'model-free' agents"
  - [section] "Our suggestion is simple: it is important to define, model, and analyse agents in addition to environments"
  - [corpus] No direct corpus evidence - this is a novel proposal
- Break condition: If agent-centric modeling proves intractable or less productive than environment-centric approaches.

### Mechanism 3
- Claim: Recognizing the limitations of reward-based goal specification can lead to more expressive and appropriate objective formulations.
- Mechanism: By fully characterizing the conditions under which the reward hypothesis holds (via von Neumann-Morgenstern axioms), the authors reveal that many real-world goals cannot be properly captured by reward functions. This opens research into alternative goal specification languages.
- Core assumption: The reward hypothesis, while useful, has been over-applied and its limitations should be explicitly acknowledged.
- Evidence anchors:
  - [section] "recent analysis by Bowling et al. (2023), building on the work of Pitis (2019); Abel et al. (2021) and Shakerinava & Ravanbakhsh (2022), fully characterizes the implicit conditions required for the hypothesis to be true"
  - [section] "if we take the reward hypothesis to be true, we can only encode goals or purposes in a reward function that reject both incomparability and incommeasurability"
  - [corpus] No direct corpus evidence - this is a novel characterization
- Break condition: If alternative goal specification methods prove significantly more complex or less effective than reward functions.

## Foundational Learning

- Concept: Paradigms and scientific revolutions
  - Why needed here: The paper explicitly references Kuhn's framework to position its critique within the context of paradigm shifts in science
  - Quick check question: What distinguishes "normal science" from "revolutionary" phases according to Kuhn?

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: MDPs are presented as the canonical environment model that has shaped RL thinking, and understanding their structure is crucial for grasping what's being challenged
  - Quick check question: What are the key assumptions embedded in the MDP formalism that the authors suggest have become limiting?

- Concept: von Neumann-Morgenstern utility axioms
  - Why needed here: The paper uses these axioms to characterize when the reward hypothesis is valid, so understanding them is essential for evaluating the critique
  - Quick check question: What are the five axioms that must be satisfied for a preference relation to be representable by a reward function?

## Architecture Onboarding

- Component map:
  - Core critique framework: Three dogmas identification and analysis
  - Alternative proposals: Agent-centric modeling, learning as adaptation, nuanced goal specification
  - Supporting evidence: References to prior work on each dogma's limitations
  - Future directions: Open questions and research opportunities

- Critical path:
  1. Understand current RL paradigm and its environmental focus
  2. Identify limitations of each dogma through analysis
  3. Propose alternative framings and research directions
  4. Connect to existing literature and future opportunities

- Design tradeoffs:
  - Abstraction vs. specificity: The paper deliberately stays conceptual rather than providing specific algorithms
  - Critique vs. construction: Balances pointing out problems with suggesting alternatives
  - Field history vs. future vision: Anchors critique in RL's development while proposing new directions

- Failure signatures:
  - If readers interpret the critique as dismissing all of RL rather than proposing refinements
  - If the alternative proposals seem too vague or impractical to implement
  - If the connection to existing work isn't clear enough to show continuity

- First 3 experiments:
  1. Develop a canonical agent model that captures key properties across different agent types
  2. Design a learning framework that evaluates progress without reference to optimal solutions
  3. Create a testbed for comparing reward-based vs. alternative goal specification methods on tasks with incommensurable objectives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the canonical mathematical model of an agent in reinforcement learning, and what are its key components?
- Basis in paper: [explicit] The authors argue that while environments have canonical models like MDPs, there is no clear canonical model of an agent. They suggest that defining such a model is crucial for advancing the field.
- Why unresolved: The paper does not provide a specific model but highlights the lack of one and the need for its development. Different researchers have proposed various agent models, but none have been universally accepted.
- What evidence would resolve it: A formal mathematical framework that captures the essential properties of agents across different domains, with clear axioms and theorems, would provide a canonical model.

### Open Question 2
- Question: How can we define and measure learning progress in agents that engage in continual adaptation rather than finding a solution?
- Basis in paper: [explicit] The authors propose shifting focus from agents that find solutions to those that continually adapt and improve. They question how to evaluate such agents without relying on optimality or task-specific benchmarks.
- Why unresolved: Current evaluation methods in RL are designed for agents that converge to optimal policies. There is no established framework for assessing agents that are designed to keep learning and adapting indefinitely.
- What evidence would resolve it: New metrics and benchmarks that capture the ability of agents to adapt to changing environments, transfer knowledge across tasks, and improve over time without a fixed endpoint.

### Open Question 3
- Question: What are the alternative languages for describing an agent's goals beyond reward maximization, and how can they be integrated into reinforcement learning frameworks?
- Basis in paper: [explicit] The authors discuss the limitations of the reward hypothesis and suggest exploring other ways to specify goals, such as preferences, logical languages, or open-ended objectives.
- Why unresolved: While there are some proposals for alternative goal-specification methods, they are not yet widely adopted or integrated into mainstream RL frameworks. The field lacks a unified approach to handling diverse goal types.
- What evidence would resolve it: Successful integration of alternative goal-specification methods into RL algorithms, with empirical demonstrations of their effectiveness in complex, real-world scenarios. This would involve developing new algorithms, benchmarks, and evaluation criteria.

## Limitations

- No empirical validation of proposed alternative approaches
- Limited quantitative evidence for the extent to which current practices are constrained by these dogmas
- No systematic survey of researcher beliefs about these implicit assumptions

## Confidence

- High confidence in the identification of environmental focus as a limiting factor in RL research
- Medium confidence in the characterization of learning as solution-finding as problematic
- Low confidence in the practical implications of the reward hypothesis critique without specific alternative frameworks

## Next Checks

1. Survey reinforcement learning researchers to assess the prevalence and impact of these three implicit assumptions in current research practices
2. Develop a small-scale benchmark comparing agent-centric vs. environment-centric modeling approaches on a set of transfer learning tasks
3. Create a taxonomy of goal specification methods, categorizing tasks by whether reward functions adequately capture their objectives based on the von Neumann-Morgenstern axioms analysis