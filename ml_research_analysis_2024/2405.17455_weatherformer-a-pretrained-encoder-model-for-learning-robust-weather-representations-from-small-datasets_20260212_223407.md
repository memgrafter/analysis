---
ver: rpa2
title: 'WeatherFormer: A Pretrained Encoder Model for Learning Robust Weather Representations
  from Small Datasets'
arxiv_id: '2405.17455'
source_url: https://arxiv.org/abs/2405.17455
tags:
- weather
- data
- transformer
- dataset
- yield
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WeatherFormer, a transformer encoder model
  pretrained on 39 years of satellite weather data to learn robust weather representations.
  It uses a novel spatiotemporal encoding to capture geographical, annual, and seasonal
  variations, and a pretraining strategy robust to missing weather features.
---

# WeatherFormer: A Pretrained Encoder Model for Learning Robust Weather Representations from Small Datasets

## Quick Facts
- arXiv ID: 2405.17455
- Source URL: https://arxiv.org/abs/2405.17455
- Authors: Adib Hasan; Mardavij Roozbehani; Munther Dahleh
- Reference count: 40
- One-line primary result: Pretrained transformer encoder achieves state-of-the-art performance in soybean yield prediction and influenza forecasting using small datasets

## Executive Summary
This paper introduces WeatherFormer, a transformer encoder model pretrained on 39 years of satellite weather data to learn robust weather representations. The model addresses the challenge of limited labeled weather data for downstream tasks by leveraging transfer learning from a large pretraining dataset. WeatherFormer incorporates a novel spatiotemporal positional encoding to capture geographical, annual, and seasonal variations, and uses a pretraining strategy robust to missing weather features. When fine-tuned for soybean yield prediction and influenza forecasting, the model achieves state-of-the-art performance, demonstrating the effectiveness of pretraining large encoder models for weather-dependent tasks.

## Method Summary
WeatherFormer is a transformer encoder model pretrained on a large dataset of 39 years of satellite weather measurements across the Americas. The pretraining task involves predicting masked weather features from the remaining observations, encouraging the model to learn robust representations from partial data. A novel spatiotemporal positional encoding is used to capture both spatial (latitude/longitude) and temporal (year/season) dependencies in the weather data. After pretraining, the model is fine-tuned for downstream tasks including county-level soybean yield prediction and influenza forecasting, where it consistently outperforms existing methods.

## Key Results
- Achieves state-of-the-art performance in county-level soybean yield prediction (RMSE in Bu/Acre)
- Achieves state-of-the-art performance in influenza forecasting (MAE in %)
- Demonstrates effective transfer learning from large pretraining dataset to small downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining on 39 years of satellite weather data allows the model to learn robust representations that transfer to downstream tasks with small datasets.
- Mechanism: The model is exposed to a wide variety of weather patterns across different geographies, seasons, and years during pretraining. This allows it to learn general weather features that are useful even when only limited weather data is available for a specific task.
- Core assumption: Weather patterns have enough regularity across space and time that features learned from a large dataset will be useful on small, task-specific datasets.
- Evidence anchors:
  - [abstract] "WeatherFormer was pretrained on a large pretraining dataset comprised of 39 years of satellite measurements across the Americas... achieves state-of-the-art performance in county-level soybean yield prediction and influenza forecasting."
  - [section] "By training the model to predict missing weather variables on a pretraining dataset, we encourage the development of robust representations derived from partial observations, making it well-suited for our downstream tasks."
  - [corpus] Weak evidence - no directly related papers on pretraining weather models found in corpus.
- Break condition: If weather patterns are too task-specific or if the pretraining dataset does not adequately represent the conditions in the downstream task.

### Mechanism 2
- Claim: The spatiotemporal positional encoding captures both spatial and temporal dependencies in weather data, improving model performance.
- Mechanism: Standard positional encodings only capture temporal information. The new encoding includes latitude and longitude, allowing the model to learn location-specific weather patterns and their temporal evolution.
- Core assumption: Weather patterns vary systematically with location and time, and this variation can be captured by the sinusoidal encoding of coordinates and time.
- Evidence anchors:
  - [section] "It also incorporates a new positional encoding mechanism sensitive to geographical location, year, and seasonality, thus enabling it to capture weather patterns' dynamic and cyclic nature across different times and places."
  - [section] "The input of WEATHER FORMER is a time series of weather measurements, which has both temporal and spatial dependency. The original sinusoidal positional encoding of the transformers will be unable to take this into account. For this reason, we designed a new Spatiotemporal Positional Encoding for the WEATHER FOMER."
  - [corpus] Weak evidence - no directly related papers on spatiotemporal encodings for weather found in corpus.
- Break condition: If weather patterns do not vary systematically with location or if the sinusoidal encoding is not expressive enough to capture the variation.

### Mechanism 3
- Claim: The pretraining task of predicting masked weather features encourages the model to learn robust representations.
- Mechanism: By masking 10 target weather variables and training the model to predict them from the remaining 21, the model learns to extract useful features from partial observations. Swapping input and output variables during training ensures the model can predict any variable from any combination of others.
- Core assumption: The ability to predict any weather variable from any combination of others indicates the model has learned robust, general features of weather.
- Evidence anchors:
  - [section] "To do so, 10 target weather variables were masked with the input mask. Then the model learned to predict the missing values... In every batch, one input and one output variable were swapped, and consequently, over time the model learned to predict every weather variable from the combination of other variables."
  - [section] "By training the model to predict missing weather variables on a pretraining dataset, we encourage the development of robust representations derived from partial observations, making it well-suited for our downstream tasks."
  - [corpus] Weak evidence - no directly related papers on masked feature prediction for weather found in corpus.
- Break condition: If the pretraining task does not adequately challenge the model or if the model learns to cheat rather than extract meaningful features.

## Foundational Learning

- Concept: Transformers and self-attention
  - Why needed here: Weather data has both spatial and temporal dependencies that can be captured by the self-attention mechanism of transformers.
  - Quick check question: Can you explain how self-attention allows a model to weigh the importance of different parts of the input when making predictions?

- Concept: Pretraining and transfer learning
  - Why needed here: Pretraining on a large dataset allows the model to learn general features that can be transferred to downstream tasks with small datasets.
  - Quick check question: Why might features learned from a large, diverse dataset be useful on a small, task-specific dataset?

- Concept: Positional encodings
  - Why needed here: Weather data is inherently sequential, and the model needs to know the position of each input in the sequence.
  - Quick check question: What information would be lost if we didn't use positional encodings in a transformer model for weather data?

## Architecture Onboarding

- Component map:
  - Input weather data (31 features, variable length up to 365) -> Feature mask -> Input scalers -> Input projection -> Spatiotemporal encoding -> Transformer encoder -> Output projection -> Output embeddings

- Critical path: Input → Feature mask → Input scalers → Input projection → Spatiotemporal encoding → Transformer encoder → Output projection → Output

- Design tradeoffs:
  - Input scalers vs fixed scaling: Learnable scalers allow the model to adapt to the data, but add parameters.
  - Spatiotemporal encoding vs learned position embeddings: The encoding is fixed and doesn't require additional parameters, but may not capture all position information.
  - Masked feature prediction vs other pretraining tasks: This task encourages robust feature learning, but may not capture all aspects of weather.

- Failure signatures:
  - Poor performance on downstream tasks: Could indicate the pretraining task is not effective or the model is overfitting.
  - High loss on pretraining task: Could indicate the model is too simple or the learning rate is too low.
  - Low variance in learned input scalers: Could indicate the model is not adapting to the data.

- First 3 experiments:
  1. Train a small model on a subset of the pretraining data and evaluate on a small downstream task to verify the basic approach works.
  2. Compare the performance of the spatiotemporal encoding to learned position embeddings on a downstream task.
  3. Try different pretraining tasks (e.g., predicting future weather instead of masked features) and compare performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would expanding the pretraining dataset to include ground-level measurements affect the model's performance?
- Basis in paper: [explicit] The authors mention that the pretraining dataset could be expanded to include ground-level measurements and that this limitation merits discussion.
- Why unresolved: The current model relies solely on satellite-based weather data, which may lack the granularity of ground-level measurements. The impact of incorporating ground-level data on model performance is not explored.
- What evidence would resolve it: Conducting experiments with a pretraining dataset that includes both satellite and ground-level measurements, and comparing the performance with the current model on downstream tasks.

### Open Question 2
- Question: What is the optimal model size for WeatherFormer, and how does it scale with the amount of pretraining data?
- Basis in paper: [explicit] The authors note that larger models require more data to be effective and observed no performance improvement with larger sizes, suggesting a potential limitation in scaling.
- Why unresolved: The relationship between model size, pretraining data volume, and downstream task performance is not fully understood. The optimal balance between model capacity and data requirements remains unclear.
- What evidence would resolve it: Systematic experiments varying model sizes and pretraining dataset sizes, followed by fine-tuning on downstream tasks to identify the point of diminishing returns.

### Open Question 3
- Question: How does the performance of WeatherFormer compare to domain-specific models for individual tasks like soybean yield prediction or influenza forecasting?
- Basis in paper: [explicit] The authors demonstrate WeatherFormer's state-of-the-art performance on multiple tasks but do not compare it to models specifically designed for each task.
- Why unresolved: While WeatherFormer shows promise as a general-purpose weather encoder, it is unclear how it stacks up against specialized models optimized for particular domains.
- What evidence would resolve it: Benchmarking WeatherFormer against state-of-the-art models for individual tasks, such as crop yield prediction or disease forecasting, to quantify the trade-offs between generalization and specialization.

## Limitations

- Limited validation of novel contributions (spatiotemporal encoding, masked feature prediction) due to lack of directly comparable prior research
- Performance improvements shown only on two specific downstream tasks (soybean yield and influenza forecasting)
- Potential overfitting during pretraining due to large model size relative to dataset size

## Confidence

- **High**: Performance improvements on soybean yield and influenza forecasting tasks
- **Medium**: General architecture design and pretraining approach
- **Low**: Novel contributions (spatiotemporal encoding, masked feature prediction) due to limited external validation

## Next Checks

1. **Ablation study on spatiotemporal encoding**: Remove the spatiotemporal encoding and compare performance using standard learned position embeddings to isolate its contribution.

2. **Cross-task generalization test**: Apply the pretrained WeatherFormer to a third weather-dependent task (e.g., crop disease prediction or energy demand forecasting) with a small dataset to verify transfer learning capabilities.

3. **Robustness to data distribution shift**: Evaluate WeatherFormer on weather data from a different geographic region than the pretraining data to test whether learned representations generalize beyond the pretraining domain.