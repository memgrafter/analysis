---
ver: rpa2
title: An Empirical Study on Improving SimCLR's Nonlinear Projection Head using Pretrained
  Autoencoder Embeddings
arxiv_id: '2408.14514'
source_url: https://arxiv.org/abs/2408.14514
tags:
- projector
- simclr
- projection
- layer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving SimCLR's nonlinear
  projection head for contrastive learning in image classification. The authors propose
  replacing SimCLR's randomly initialized input layer with a pretrained autoencoder
  embedding, which is then frozen during training.
---

# An Empirical Study on Improving SimCLR's Nonlinear Projection Head using Pretrained Autoencoder Embeddings

## Quick Facts
- arXiv ID: 2408.14514
- Source URL: https://arxiv.org/abs/2408.14514
- Reference count: 17
- Using pretrained autoencoder embeddings in SimCLR's projection head increases classification accuracy by up to 2.9% (1.7% on average)

## Executive Summary
This paper investigates improving SimCLR's nonlinear projection head for contrastive learning by replacing the randomly initialized input layer with a pretrained autoencoder embedding. The authors demonstrate that frozen autoencoder embeddings can significantly improve classification accuracy while reducing the dimensionality of the projection space. They also show that sigmoid and tanh activation functions outperform ReLU in the projector, particularly for datasets with ten classes. The approach is evaluated across five image classification datasets with varying resolutions and complexities.

## Method Summary
The authors propose replacing SimCLR's random projection head input layer with a frozen pretrained autoencoder embedding. A 4-layer autoencoder is first trained separately on each dataset to learn compressed representations. These embeddings are then used as the input layer of SimCLR's 2-layer projection head, which is kept frozen during contrastive training. The study varies projector width (128, 256, 512 output features in input layer; 32, 64, 128 in output layer) and activation functions (ReLU, SiLU, sigmoid, tanh). Training uses NT-Xent loss with ResNet34 backbone, SGD optimizer, and dataset-specific batch sizes (392-1280) over 150 epochs. Classification accuracy is evaluated using a linear classifier trained on backbone features.

## Key Results
- Pretrained autoencoder embeddings increase classification accuracy by up to 2.9% (1.7% on average) compared to random initialization
- Frozen autoencoder embeddings outperform non-frozen counterparts in all experiments
- Sigmoid and tanh activation functions outperform ReLU in terms of peak and average classification accuracy, especially for datasets with ten classes
- Smaller projectors with pretrained embeddings can match or exceed performance of wider random projectors

## Why This Works (Mechanism)

### Mechanism 1
Pretraining the projection head with an autoencoder embedding provides prior knowledge about the data distribution, improving gradient flow in early training. The autoencoder learns compressed representations that capture the high-level structure of the training data. By freezing these weights in the projection head, the model can leverage these predetermined features, guiding the backbone more effectively during contrastive learning. If the autoencoder fails to learn meaningful representations (e.g., poor reconstruction loss), the projection head gains no benefit.

### Mechanism 2
Using sigmoid or tanh activation functions in the projection head improves classification accuracy compared to ReLU. The shallow architecture of the projection head (2 layers) prevents the vanishing gradient problem that typically occurs with sigmoid/tanh in deep networks. The dampening effect of these activations may help stabilize training. If the projection head becomes deeper, the vanishing gradient problem would likely emerge, making sigmoid/tanh ineffective.

### Mechanism 3
Freezing the autoencoder embedding weights leads to better performance than allowing them to be updated during training. The pretrained features provide a stable foundation for the projection head, and allowing them to change during training may disrupt this beneficial prior knowledge. If the autoencoder embedding is poorly trained or the downstream task significantly differs from reconstruction, freezing may prevent beneficial adaptation.

## Foundational Learning

- Concept: Contrastive Learning (SimCLR)
  - Why needed here: The paper builds upon SimCLR's framework, modifying only the projection head while keeping the contrastive learning objective intact
  - Quick check question: What is the main purpose of the projection head in SimCLR, and how does it differ from the backbone's role?

- Concept: Autoencoders
  - Why needed here: The paper uses pretrained autoencoder embeddings as input to the projection head, leveraging their ability to learn compressed representations
  - Quick check question: How does an autoencoder learn compressed representations, and what is the role of the embedding layer?

- Concept: NT-Xent Loss
  - Why needed here: Understanding this loss function is crucial for grasping how the projection head influences the backbone's learning through contrastive pairs
  - Quick check question: How does the NT-Xent loss function encourage the backbone to learn representations invariant to data augmentations?

## Architecture Onboarding

- Component map: Data Augmentation -> ResNet34 Backbone -> Frozen Autoencoder Embedding -> 2-Layer MLP Projection Head -> NT-Xent Loss
- Critical path: Pretrain autoencoder → Replace projection head input with frozen embedding → Train SimCLR with contrastive loss → Evaluate with linear classifier
- Design tradeoffs:
  - Frozen vs. trainable embeddings: Frozen embeddings provide stable prior knowledge but prevent adaptation; trainable embeddings could adapt but risk losing beneficial features
  - Activation function choice: Sigmoid/tanh work well for shallow heads but may cause vanishing gradients in deeper architectures
  - Projector width: Smaller projectors with pretrained embeddings can match or exceed performance of wider random projectors
- Failure signatures:
  - Poor autoencoder reconstruction loss → No benefit from pretrained embeddings
  - Large performance gap between frozen and non-frozen settings → Pretrained features may not be optimal
  - Worse performance than random projection head → Autoencoder features may be detrimental for the specific task
- First 3 experiments:
  1. Train autoencoder on target dataset and verify reconstruction quality (MSE loss should be reasonably low)
  2. Replace random projection head input with frozen autoencoder embedding and train SimCLR, comparing to baseline
  3. Test different activation functions (ReLU, sigmoid, tanh, SiLU) in the projection head to identify optimal choice for the dataset

## Open Questions the Paper Calls Out

### Open Question 1
Does increasing the depth and width of the backbone beyond ResNet34 further improve classification accuracy when using pretrained autoencoder embeddings in the projector? The paper notes that ResNet34 was chosen due to compatibility with the autoencoder's 512-dimensional embedding layer, and suggests that deeper variants like ResNet50 (with 2048 output features) would be incompatible. Testing with deeper backbones would require architectural modifications or larger autoencoder embeddings.

### Open Question 2
Do more advanced autoencoder architectures (beyond the basic 4-layer architecture used in this study) further improve the performance of SimCLR's projector? The authors state that the same features which are suitable for image reconstruction tasks, can also enhance contrastive representation learning and suggest that pretraining more advanced autoencoder architectures could further increase the projector's overall performance.

### Open Question 3
What is the theoretical explanation for why the sigmoid and tanh activation functions outperform ReLU in the projector for datasets with ten classes? The authors observe that on datasets with ten classes, both projector types achieve their highest classification accuracies when trained with the sigmoid or tanh activation function and speculate that the outstanding performance of the sigmoid activation function in the projector may be enabled by its shallow architecture.

## Limitations

- The study is limited to five datasets and doesn't explore robustness across diverse domain shifts or significantly different data distributions
- The approach assumes autoencoder embeddings learned on the same dataset will transfer effectively, without investigating cross-dataset embedding transfer
- The ablation study on freezing vs. non-freezing autoencoder weights is limited in scope

## Confidence

- **High Confidence**: The empirical results showing performance improvements with frozen autoencoder embeddings (up to 2.9% accuracy gain) are well-supported by experimental data and systematic ablation studies across multiple datasets and hyperparameters
- **Medium Confidence**: The claim that sigmoid and tanh activation functions outperform ReLU is supported by experimental results but requires additional theoretical justification to fully explain why this occurs specifically in shallow projection heads
- **Low Confidence**: The broader generalization of these findings to other backbone architectures beyond ResNet34 and to datasets with substantially different characteristics remains uncertain

## Next Checks

1. Cross-dataset validation: Train autoencoder embeddings on one dataset and evaluate their effectiveness when used as projection head inputs for contrastive learning on a different dataset to assess generalization

2. Autoencoder architecture ablation: Systematically vary the depth and width of the autoencoder architecture to determine the optimal configuration for generating useful embeddings for contrastive learning

3. Robustness to domain shift: Evaluate the approach on datasets with significant domain differences (e.g., natural images vs. medical images vs. satellite imagery) to test the limits of autoencoder-based projection heads