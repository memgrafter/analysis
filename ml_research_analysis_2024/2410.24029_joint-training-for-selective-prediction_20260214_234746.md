---
ver: rpa2
title: Joint Training for Selective Prediction
arxiv_id: '2410.24029'
source_url: https://arxiv.org/abs/2410.24029
tags:
- deferral
- jtsp
- accuracy
- training
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JTSP, a joint training approach for selective
  prediction (SP) that simultaneously optimizes a classifier and a deferral policy
  to decide when to defer decisions to humans. Unlike prior SP methods that train
  modules separately, JTSP uses a novel architecture where both modules learn distinct
  representations, combined through a shared loss function incorporating reinforcement
  learning.
---

# Joint Training for Selective Prediction

## Quick Facts
- arXiv ID: 2410.24029
- Source URL: https://arxiv.org/abs/2410.24029
- Reference count: 16
- Key outcome: JTSP significantly outperforms strong baselines on SP accuracy across four STEM assessment datasets, while improving both classifier and deferral policy performance

## Executive Summary
This paper introduces JTSP, a joint training approach for selective prediction that simultaneously optimizes a classifier and deferral policy to decide when to defer decisions to humans. Unlike prior SP methods that train modules separately, JTSP uses a novel architecture where both modules learn distinct representations, combined through a shared loss function incorporating reinforcement learning. The method was tested on four STEM assessment datasets using two classifier architectures (BERT and SFRN), with RoBERTa as the deferral policy encoder. Results show JTSP significantly outperforms strong baselines on SP accuracy across all datasets, while also improving the accuracy of both modules. The approach effectively manages the tradeoff between SP accuracy and deferral rate through a reward signal, though sensitivity analysis reveals varying performance depending on classifier choice.

## Method Summary
JTSP employs a three-phase training pipeline: first, the classifier (CL) is trained alone with cross-entropy loss; second, the deferral policy (DP) is trained alone with cross-entropy using concatenated CL and DP representations; finally, both modules are jointly trained with a weighted loss combining CL CE, DP CE, and policy gradient reward. The method uses separate BERT or SFRN encoders for the classifier and RoBERTa for the deferral policy, with a reward signal matrix managing the accuracy-deferral tradeoff. Hyperparameters α, β, and γ are tuned per dataset to optimize the weighted loss components.

## Key Results
- JTSP significantly outperforms softmax threshold, logistic regression policy, and separate training baselines on SP accuracy across all four STEM datasets
- Joint training improves both CL and DP module performance compared to their individual training phases
- SFRN-based JTSP shows greater susceptibility to representation learning improvements compared to BERT-based JTSP
- The reward signal effectively manages the accuracy-deferral tradeoff, though optimal values vary by dataset and architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JTSP improves SP accuracy by allowing the classifier and deferral policy to learn complementary representations
- Mechanism: The classifier and deferral policy each learn distinct hidden representations of the input. The deferral policy's decision is based on the concatenation of both representations, allowing it to leverage the classifier's output and its own assessment of prediction quality
- Core assumption: The classifier's representation captures task-relevant features while the deferral policy's representation captures confidence-related features
- Evidence anchors:
  - [abstract] "simultaneously optimizes learned representations used by the classifier module and a learned deferral policy"
  - [section] "JTSP learns different representations to support each of two decisions"
  - [corpus] Weak evidence - corpus neighbors focus on confidence estimation but don't discuss joint representation learning
- Break condition: If the representations learned by the classifier and deferral policy are too similar, the benefit of joint training would diminish

### Mechanism 2
- Claim: The policy gradient loss term in JTSP manages the tradeoff between SP accuracy and deferral rate
- Mechanism: The reward signal R provides different scalar values for the four possible outcomes of correctness (classifier correct/defer correct, etc.). By weighting these outcomes differently, the training process can be steered toward favoring either higher accuracy or lower deferral rates
- Core assumption: The reward signal can be designed to effectively encode the desired tradeoff between accuracy and deferral rate
- Evidence anchors:
  - [section] "The final loss for the joint training phase is a weighted sum of the cross-entropy losses from both models and the policy gradient loss"
  - [section] "A sensitivity analysis of deferral weight d, based on a somewhat more constrained reward signal, demonstrated that sensitivity to d varies dramatically"
  - [corpus] Moderate evidence - corpus neighbors discuss confidence estimation but don't explore reward signal design for tradeoff management
- Break condition: If the reward signal cannot be effectively tuned for a particular dataset or classifier architecture, the tradeoff management would fail

### Mechanism 3
- Claim: Joint training improves both the classifier and deferral policy modules compared to training them separately
- Mechanism: By jointly optimizing both modules, the training process can leverage synergies between the classifier's task performance and the deferral policy's confidence assessment, leading to improvements in both
- Core assumption: The classifier and deferral policy can benefit from shared training signals
- Evidence anchors:
  - [abstract] "joint training not only leads to better SP outcomes over two strong baselines, but also improves the performance of both modules"
  - [section] "Table 2 shows that with joint training, the CL classifiers improve, thus both tables show that joint training benefits both the CL and DP modules"
  - [corpus] Moderate evidence - corpus neighbors discuss confidence estimation but don't explore joint training benefits
- Break condition: If the classifier and deferral policy interfere with each other during joint training, performance could degrade

## Foundational Learning

- Concept: Cross-entropy loss
  - Why needed here: Used to train both the classifier and deferral policy modules
  - Quick check question: What is the formula for cross-entropy loss and how does it measure prediction error?
- Concept: Policy gradient methods
  - Why needed here: Used to incorporate the reward signal into the training process and manage the accuracy-deferral tradeoff
  - Quick check question: How does policy gradient reinforcement learning update model parameters based on rewards?
- Concept: Representation learning
  - Why needed here: JTSP relies on learning complementary representations for the classifier and deferral policy
  - Quick check question: What is the difference between learning a single shared representation versus distinct representations for different tasks?

## Architecture Onboarding

- Component map: Input → classifier encoder → classifier output → deferral policy encoder → combined representation → deferral decision → joint loss → parameter updates
- Critical path: Input → classifier encoder → classifier output → deferral policy encoder → combined representation → deferral decision → joint loss → parameter updates
- Design tradeoffs:
  - Separate vs. shared encoders: Separate encoders allow learning complementary representations but increase model complexity
  - Reward signal design: Different reward signal values affect the accuracy-deferral tradeoff but are difficult to optimize
  - Classifier choice: BERT vs. SFRN affects joint training performance but the optimal choice depends on the dataset
- Failure signatures:
  - Low SP accuracy despite high individual module accuracy: Indicates poor coordination between modules
  - Very high deferral rate: Suggests the reward signal is overly penalizing incorrect decisions
  - Degradation of classifier accuracy during joint training: Indicates interference between modules
- First 3 experiments:
  1. Implement JTSP with a simple classifier (e.g., logistic regression) and deferral policy to verify the joint training framework works
  2. Test JTSP on a small dataset with different reward signal values to understand the accuracy-deferral tradeoff
  3. Compare JTSP performance with separate training of classifier and deferral policy to verify the benefit of joint training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the reward signal A be learned or optimized automatically rather than manually tuned for different datasets?
- Basis in paper: [explicit] The paper identifies the reward signal A as having three degrees of freedom that are difficult to optimize, and notes this as a key limitation requiring manual tuning across datasets
- Why unresolved: The authors acknowledge the challenge of optimizing A but do not propose a systematic method for learning it, instead relying on manual exploration and empirical selection
- What evidence would resolve it: A proposed algorithm that can learn or adapt the reward signal parameters based on training data and task requirements, with experimental validation showing improved performance over manual tuning

### Open Question 2
- Question: Why does the JTSP model show different sensitivity patterns to the deferral weight d across different classifier architectures (SFRN vs BERT)?
- Basis in paper: [explicit] The sensitivity analysis in Figure 3 shows that SFRN exhibits gradual changes in performance metrics as d varies, while BERT shows abrupt changes at d=0.95, with the authors noting this as an unexpected finding
- Why unresolved: The authors speculate that BERT's general-purpose nature versus SFRN's task-specific design may explain this, but do not provide a definitive explanation for the architectural differences in sensitivity
- What evidence would resolve it: Comparative analysis of internal representations and gradients during training for both architectures, potentially revealing why one is more susceptible to representation learning improvements than the other

### Open Question 3
- Question: How would JTSP perform when applied to classification tasks beyond short answer grading, such as medical diagnosis or financial risk assessment?
- Basis in paper: [explicit] The authors state that "Selective prediction could be used for any classification task, and with any classifier architecture" but limit their experiments to short answer grading
- Why unresolved: The paper only tests JTSP on four datasets within a single domain (STEM assessment), leaving its generalizability to other domains unexplored
- What evidence would resolve it: Implementation and evaluation of JTSP on diverse classification tasks from different domains, comparing performance against domain-specific selective prediction methods

## Limitations
- The reward signal optimization remains a challenge with significant sensitivity to parameter tuning across datasets
- The study focuses exclusively on STEM assessment datasets, limiting generalizability to other domains
- The reliance on ground truth labels for deferred instances during evaluation creates an unrealistic assumption about human-in-the-loop availability

## Confidence

**High Confidence**: The core claim that JTSP outperforms separate training methods is well-supported by extensive experiments across four datasets and two classifier architectures. The improvement in both SP accuracy and individual module performance is consistently demonstrated.

**Medium Confidence**: The mechanism explanation regarding complementary representation learning is plausible given the architecture design, but the paper lacks detailed analysis of what these representations actually capture. The reward signal design for tradeoff management shows effectiveness but the sensitivity to parameter tuning suggests limitations in generalizability.

**Low Confidence**: Claims about the specific benefits of SFRN versus BERT for joint training are based on limited comparison. The assertion that joint training universally improves both modules needs more rigorous ablation studies across diverse architectures.

## Next Checks

1. **Cross-domain validation**: Test JTSP on non-STEM datasets (e.g., sentiment analysis, medical diagnosis) to verify generalizability beyond the current domain-specific focus.

2. **Reward signal robustness**: Conduct systematic experiments varying reward signal values across a wider range to identify patterns in optimal configurations and test the framework's sensitivity to hyperparameter tuning.

3. **Human-in-the-loop evaluation**: Design experiments that simulate realistic deferral scenarios where deferred decisions are handled by human annotators rather than ground truth labels, measuring the practical utility of the SP system in real-world settings.