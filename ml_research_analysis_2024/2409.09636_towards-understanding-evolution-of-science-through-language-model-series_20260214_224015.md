---
ver: rpa2
title: Towards understanding evolution of science through language model series
arxiv_id: '2409.09636'
source_url: https://arxiv.org/abs/2409.09636
tags:
- language
- tasks
- arxiv
- papers
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AnnualBERT, a series of language models specifically
  designed to capture the temporal evolution of scientific text. Unlike prevailing
  subword tokenization methods, AnnualBERT uses whole words as tokens and consists
  of a base RoBERTa model pretrained on 1.7 million arXiv papers until 2008, followed
  by progressively trained models on papers published annually thereafter.
---

# Towards understanding evolution of science through language model series

## Quick Facts
- arXiv ID: 2409.09636
- Source URL: https://arxiv.org/abs/2409.09636
- Authors: Junjie Dong; Zhuoqi Lyu; Qing Ke
- Reference count: 40
- This paper proposes AnnualBERT, a series of language models designed to capture the temporal evolution of scientific text through continual pretraining on yearly corpora.

## Executive Summary
This paper introduces AnnualBERT, a novel approach to capturing the temporal evolution of scientific language through a series of language models progressively trained on annual corpora. Unlike conventional models that use subword tokenization and are trained on static datasets, AnnualBERT employs whole-word tokenization and undergoes continual pretraining from 2009 to 2020, building upon a base RoBERTa model trained on papers until 2008. The resulting series of models demonstrates superior performance on domain-specific tasks and provides insights into the development of scientific discourse over time, with the models and code available for public use.

## Method Summary
AnnualBERT uses whole words as tokens and consists of a base RoBERTa model (M2008) pretrained on 1.7 million arXiv papers until 2008, followed by progressively trained models (M2009-M2020) on papers published annually thereafter. The approach deviates from prevailing subword tokenization methods and enables each model instance to capture evolving language and terminology specific to its publication year. The models are evaluated on standard NLP tasks (NER, classification, relation extraction) as well as domain-specific tasks including citation link prediction in the arXiv network.

## Key Results
- AnnualBERT models achieve comparable performance to existing models on standard NLP tasks while outperforming them on domain-specific tasks
- The series captures meaningful temporal evolution of scientific discourse through representation learning and forgetting processes
- Interpolation between adjacent-year models produces interpolated models that perform comparably to actual models for specific time periods
- Link prediction experiments demonstrate that AnnualBERT models encode semantic information useful for predicting citation relationships

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: AnnualBERT's continual pretraining on yearly corpora captures evolving scientific terminology and semantics more effectively than a single static model.
- **Mechanism**: By pretraining a base model on papers until 2008 and then progressively retraining on papers from each subsequent year, the model adapts its internal representations to reflect changing language and concepts in scientific discourse. This temporal adaptation allows the model to encode information specific to each year's publications.
- **Core assumption**: The scientific literature exhibits meaningful temporal changes in vocabulary, concepts, and writing styles that can be captured by continual training on chronologically organized corpora.
- **Evidence anchors**:
  - [abstract] "Our approach enables the pretrained models to not only improve performances on scientific text processing tasks but also to provide insights into the development of scientific discourse over time."
  - [section] "Through this approach, we allow each instance of our AnnualBERT to capture evolving language and terminology specific to that year's publications..."
  - [corpus] The corpus preparation section shows papers are organized by year with the number of papers and tokens growing significantly over time.

### Mechanism 2
- **Claim**: Using whole words as tokens allows AnnualBERT to better represent domain-specific terminology and capture semantic relationships at the word level.
- **Mechanism**: Traditional subword tokenizers break complex scientific terms into smaller pieces, which can fragment semantic meaning. By using whole words as tokens, AnnualBERT preserves the integrity of scientific terms, allowing the model to learn more meaningful representations of domain-specific concepts.
- **Core assumption**: Whole word tokenization is more effective than subword tokenization for capturing the semantic meaning of complex scientific terms and their evolution over time.
- **Evidence anchors**:
  - [abstract] "Deviating from the prevailing paradigms of subword tokenizations and 'one model to rule them all', AnnualBERT adopts whole words as tokens..."
  - [section] "Subword tokenization, however, is not suitable for our purpose of examining temporal evolution of scientific discourse at the word level."
  - [corpus] The vocabulary construction process explicitly creates a vocabulary of whole words with frequency ≥ 50, resulting in approximately 53,000 words.

### Mechanism 3
- **Claim**: Interpolation of AnnualBERT models between different years can create models that perform comparably to the actual models for specific time periods, suggesting that model weights encode temporal information.
- **Mechanism**: By averaging the weights of models from adjacent years (e.g., averaging M2019 and M2021 to create an interpolated M2020), we can create a model that performs similarly to the actual M2020 on prediction tasks. This suggests that model weights contain information about the temporal progression of scientific knowledge.
- **Core assumption**: The model weights change smoothly over time as the model adapts to new scientific literature, and this smooth change can be captured by linear interpolation of weights.
- **Evidence anchors**:
  - [section] "We derive the interpolated model M∆t t0 by 'averaging' Mt0−∆t and Mt0+∆t, treating it as a linear system..."
  - [section] "All the interpolated models perform well and exhibit comparable performances with the real model."
  - [corpus] The PCA visualizations of model weights show that they lie on a quadratic curve, suggesting a smooth progression over time.

## Foundational Learning

- **Concept**: Continual learning and catastrophic forgetting
  - Why needed here: Understanding how the model adapts to new information while retaining knowledge from previous years is crucial for interpreting results and designing experiments to measure learning and forgetting.
  - Quick check question: What is catastrophic forgetting, and how does it relate to the continual pretraining approach used in AnnualBERT?

- **Concept**: Tokenization strategies (subword vs. whole word)
  - Why needed here: The choice of tokenization strategy has significant implications for how the model represents and processes scientific text, particularly domain-specific terminology.
  - Quick check question: What are the advantages and disadvantages of using whole words as tokens compared to subword tokenization methods like WordPiece or BPE?

- **Concept**: Graph neural networks and link prediction
  - Why needed here: The link prediction experiments use GraphSAGE to combine topological information from the citation network with node features encoded by the BERT models.
  - Quick check question: How does GraphSAGE generate node embeddings, and how are these embeddings used for link prediction in citation networks?

## Architecture Onboarding

- **Component map**: Base RoBERTa (M2008) -> Continual pretraining on annual corpora (M2009-M2020) -> Whole word tokenizer (53K vocabulary) -> Downstream task heads (NER, CLS, REL) -> GraphSAGE for link prediction

- **Critical path**:
  1. Corpus preparation: Download and process arXiv papers, organize by year, create vocabulary
  2. Base model pretraining: Train RoBERTa from scratch on papers until 2008
  3. Continual pretraining: For each year t > 2008, pretrain Mt-1 on papers from year t
  4. Downstream evaluation: Fine-tune models on various NLP tasks and evaluate performance
  5. Link prediction: Use models to encode paper abstracts, combine with GraphSAGE for citation link prediction
  6. Model analysis: Visualize model weights, conduct probing tasks to measure learning and forgetting

- **Design tradeoffs**:
  - Whole word vs. subword tokenization: Whole words preserve semantic meaning but result in a larger vocabulary and may struggle with out-of-vocabulary terms
  - Continual pretraining vs. single model: Continual pretraining captures temporal evolution but requires more computational resources and may suffer from catastrophic forgetting
  - Number of models: More models provide finer granularity for tracking evolution but increase storage and computational requirements

- **Failure signatures**:
  - Poor performance on downstream tasks compared to similar models: Could indicate issues with base model pretraining, continual pretraining process, or whole word tokenization approach
  - Catastrophic forgetting: If models significantly degrade on tasks from previous years, it suggests continual pretraining is not effectively retaining knowledge
  - Unstable model weights: If PCA visualizations of model weights show erratic behavior rather than smooth progression, it could indicate issues with continual pretraining process

- **First 3 experiments**:
  1. Reproduce the major category prediction task on arXiv abstracts to verify the model's performance on a domain-specific task
  2. Conduct the link prediction experiment on the static arXiv citation network to evaluate the model's ability to encode semantic information useful for predicting citations
  3. Visualize the PCA of model weights for a subset of layers to observe the temporal progression and verify the smooth change hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do AnnualBERT models perform on biomedical benchmark datasets that were not included in their training corpus?
- Basis in paper: [explicit] The paper states that biomedical datasets were excluded from evaluation because "texts from those fields are largely absent from our training corpus."
- Why unresolved: The authors intentionally avoided biomedical datasets due to domain mismatch, leaving a gap in understanding how AnnualBERT generalizes to biomedical text despite its success on other scientific domains.
- What evidence would resolve it: Systematic evaluation of AnnualBERT on established biomedical NLP tasks like named entity recognition, relation extraction, and question answering using datasets such as BC5CDR, NCBI-disease, and MedQA.

### Open Question 2
- Question: What is the optimal frequency for updating AnnualBERT models to balance computational cost with representation quality?
- Basis in paper: [inferred] The paper trains models annually, but the methodology section mentions that "the learning and forgetting processes are highly task-specific" and that "as ∆t increases...the performance gap between M∆2014 and M2014 tend to increase," suggesting temporal granularity affects performance.
- Why unresolved: The annual update frequency was chosen as a compromise, but the paper doesn't explore whether bi-annual, quarterly, or continuous updates would yield better temporal representation with reasonable computational overhead.
- What evidence would resolve it: Comparative experiments training AnnualBERT at different temporal frequencies (monthly, quarterly, bi-annually) while measuring representation quality and computational costs across various downstream tasks.

### Open Question 3
- Question: How does AnnualBERT's whole-word tokenization strategy affect its ability to handle scientific neologisms and emerging terminology compared to subword tokenization?
- Basis in paper: [explicit] The paper states that AnnualBERT "adopts whole words as tokens" and notes that "Existing language models use different tokenization methods" including subword approaches like WordPiece and BPE.
- Why unresolved: While the paper explains the rationale for whole-word tokenization and notes it's "scatteredly taken in previous studies," it doesn't empirically compare AnnualBERT's handling of novel scientific terms against subword-tokenized models or test its ability to generalize to unseen terminology.
- What evidence would resolve it: Controlled experiments introducing novel scientific terms to AnnualBERT versus subword-tokenized models and measuring their ability to encode, retrieve, and apply these terms in downstream tasks.

## Limitations

- The whole word tokenization approach with a vocabulary of approximately 53,000 words may be insufficient to cover the full diversity of scientific terminology, particularly as new concepts emerge.
- The link prediction experiments are conducted on a relatively small citation network (approximately 50,000 nodes and 150,000 edges), which may not generalize to larger, more complex citation networks.
- The improvements in domain-specific tasks could be attributed to the general benefits of continual learning and exposure to expanding data rather than specifically capturing temporal dynamics.

## Confidence

**High Confidence**: The technical implementation of the AnnualBERT series and methodology for creating a sequence of models through continual pretraining is well-documented and reproducible. Baseline performance comparisons on standard NLP tasks are straightforward and verifiable.

**Medium Confidence**: Claims about capturing temporal evolution of scientific discourse are supported by experimental results, but alternative explanations exist. The improvements could be attributed to general benefits of continual learning and exposure to expanding data rather than specifically capturing temporal dynamics.

**Low Confidence**: The interpolation experiments suggesting that model weights encode temporal information are intriguing but based on limited analysis. The finding that interpolated models perform comparably to actual models could have multiple interpretations.

## Next Checks

1. **Ablation Study on Temporal vs. Data Volume**: Create control experiments comparing AnnualBERT models with models trained on shuffled temporal data of equivalent size to isolate the contribution of temporal ordering from the benefits of continual pretraining on expanding corpora.

2. **Vocabulary Coverage Analysis**: Systematically analyze the coverage of the 53,000-word vocabulary across different scientific fields and time periods, identifying gaps where important terminology is missing and assessing the impact on model performance for underrepresented domains.

3. **Scalability Validation**: Evaluate AnnualBERT's link prediction performance on a substantially larger citation network spanning multiple disciplines to determine whether the observed advantages hold beyond the current domain-specific, relatively small network.