---
ver: rpa2
title: Inverting Gradient Attacks Makes Powerful Data Poisoning
arxiv_id: '2410.21453'
source_url: https://arxiv.org/abs/2410.21453
tags:
- data
- gradient
- attacks
- poisoning
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that data poisoning attacks can replicate
  the effects of gradient attacks on non-convex neural networks, achieving availability
  attacks that degrade model performance to random levels with as little as 1% poisoned
  data. By leveraging gradient inversion techniques from privacy attacks, the authors
  reconstruct data points whose gradients mimic malicious gradients.
---

# Inverting Gradient Attacks Makes Powerful Data Poisoning

## Quick Facts
- arXiv ID: 2410.21453
- Source URL: https://arxiv.org/abs/2410.21453
- Authors: Wassim Bouaziz; El-Mahdi El-Mhamdi; Nicolas Usunier
- Reference count: 20
- Primary result: Data poisoning attacks can replicate gradient attack effects on non-convex neural networks with as little as 1% poisoned data

## Executive Summary
This work demonstrates that data poisoning attacks can achieve availability attacks on non-convex neural networks by inverting malicious gradients to generate poisoned data points. Through gradient inversion techniques, the authors show that reconstructing data points from malicious gradients can perform a range of attacks including gradient ascent and orthogonal gradient attacks. Experiments on CIFAR10 with CNNs and Vision Transformers show that data poisoning can slow training or cause complete failure under various optimization algorithms and aggregation rules, including defenses like MultiKrum.

## Method Summary
The authors leverage gradient inversion to convert malicious gradients into poisoned data points that can be injected into the training process. They implement three attack types: Gradient Ascent (GA), Orthogonal Gradient (OG), and Little is Enough (LIE). The method involves initializing poisons randomly within a feasible set, iteratively updating them using Adam optimizer to minimize a poisoning objective, and projecting back to the feasible set each iteration. The attacks are validated on CIFAR10 using custom CNN and ViT-tiny architectures under different optimization algorithms and aggregation rules.

## Key Results
- Data poisoning can achieve availability attacks (degrading performance to random levels) with as little as 1% poisoned data
- Data poisoning attacks match the destructive power of gradient attacks on non-convex neural networks
- Attacks succeed under various optimization algorithms and aggregation rules, including MultiKrum defense
- Feasible set constraints and auxiliary dataset size impact attack effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data poisoning can achieve availability attacks on non-convex neural networks by inverting malicious gradients to generate poisoned data points.
- Mechanism: The attacker uses gradient inversion to reconstruct data points whose gradients mimic a malicious gradient attack, then injects these into the training process to degrade model performance.
- Core assumption: The feasible set of valid data points is large enough that gradient inversion can find a solution that closely approximates the malicious gradient.
- Evidence anchors:
  - [abstract]: "Through gradient inversion, commonly used to reconstruct data points from actual gradients, we show how reconstructing data points out of malicious gradients can be sufficient to perform a range of attacks."
  - [section]: "At each iteration t, the attacker computes a given gradient attack gatk in the same manner as above and inverts the gradient operator to compute an associated set of data points Sp"
  - [corpus]: Weak - The corpus contains related work on poisoning attacks but does not specifically discuss gradient inversion for availability attacks.
- Break condition: If the feasible set is too constrained, gradient inversion may fail to find a solution that closely approximates the malicious gradient, preventing the attack.

### Mechanism 2
- Claim: Data poisoning attacks can slow down training or cause complete failure under various optimization algorithms and aggregation rules, including defenses like MultiKrum.
- Mechanism: The attacker crafts poisoned data points whose gradients, when aggregated with clean gradients, either stall training (orthogonal gradient) or cause gradient ascent (gradient ascent attack), leading to poor model performance.
- Core assumption: The aggregation rule and optimization algorithm are vulnerable to the crafted poisoned gradients.
- Evidence anchors:
  - [abstract]: "Experiments on CIFAR10 with CNNs and Vision Transformers show that data poisoning can slow training or cause complete failure under various optimization algorithms and aggregation rules, including defenses like MultiKrum."
  - [section]: "In our experiments, we exhibit a successful availability attack on recent neural network architectures, trained on an image classification task with different optimization algorithms, even when protected by a state-of-the-art defense mechanism against gradient attacks."
  - [corpus]: Weak - The corpus contains related work on poisoning attacks but does not specifically discuss bypassing defenses like MultiKrum.
- Break condition: If the aggregation rule or optimization algorithm is robust to the crafted poisoned gradients, the attack may fail.

### Mechanism 3
- Claim: The success of data poisoning attacks depends on the size of the auxiliary dataset and the constraints on the feasible set of data points.
- Mechanism: A larger auxiliary dataset provides better gradient estimates for crafting poisoned data points, while a less constrained feasible set allows for more effective poisons.
- Core assumption: The auxiliary dataset is representative of the training data distribution and the feasible set is not too restrictive.
- Evidence anchors:
  - [section]: "The role of the auxiliary dataset. In our experiments, it appears that the size of the auxiliary dataset plays only a small part in the success of the attack above a sufficient size."
  - [section]: "Influence of the feasible set (Result B)...Figure 7 shows that the more constrained the feasible set, the less effective the resulting attack."
  - [corpus]: Weak - The corpus contains related work on poisoning attacks but does not specifically discuss the role of the auxiliary dataset or feasible set constraints.
- Break condition: If the auxiliary dataset is too small or not representative, or if the feasible set is too restrictive, the attack may fail.

## Foundational Learning

- Concept: Gradient attacks
  - Why needed here: Understanding how gradient attacks work is crucial for understanding how data poisoning can mimic them.
  - Quick check question: What is the goal of a gradient attack in the context of distributed learning?

- Concept: Data poisoning
  - Why needed here: Data poisoning is the primary method used in this work to achieve availability attacks.
  - Quick check question: How does data poisoning differ from gradient attacks in terms of the attacker's capabilities?

- Concept: Gradient inversion
  - Why needed here: Gradient inversion is the key technique used to convert malicious gradients into poisoned data points.
  - Quick check question: What is the main challenge in using gradient inversion for data poisoning attacks?

## Architecture Onboarding

- Component map: Threat model -> Gradient attacks -> Data poisoning -> Gradient inversion -> Experiments
- Critical path: Threat model -> Gradient attacks -> Data poisoning -> Gradient inversion -> Experiments
- Design tradeoffs:
  - Auxiliary dataset size vs. attack success rate
  - Feasible set constraints vs. attack effectiveness
  - Computational cost of gradient inversion vs. attack stealth
- Failure signatures:
  - Poor approximation of malicious gradients by poisoned data points
  - Robust aggregation rules or optimization algorithms mitigating the attack
  - Insufficient auxiliary dataset size or overly restrictive feasible set
- First 3 experiments:
  1. Validate gradient inversion on a simple XOR task to demonstrate the basic concept.
  2. Test data poisoning attacks on a CNN trained on CIFAR10 with various optimization algorithms and aggregation rules.
  3. Evaluate the impact of auxiliary dataset size and feasible set constraints on attack success.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size and distribution of the auxiliary dataset affect the success rate of data poisoning attacks on neural networks?
- Basis in paper: [explicit] The paper mentions that the size of the auxiliary dataset plays only a small part in the success of the attack above a sufficient size, but suggests further experiments with different distributions or no auxiliary dataset at all.
- Why unresolved: The paper does not provide detailed experimental results on how varying the auxiliary dataset's size and distribution impacts attack success rates.
- What evidence would resolve it: Conducting experiments with auxiliary datasets of varying sizes and distributions, including scenarios with no auxiliary dataset, to measure the resulting attack success rates.

### Open Question 2
- Question: Can data poisoning attacks achieve availability attacks on neural networks without access to the model's weights?
- Basis in paper: [inferred] The paper assumes the attacker has knowledge of the model weights, but suggests exploring attacks when the attacker estimates the victim's model with a surrogate model.
- Why unresolved: The paper does not explore scenarios where the attacker does not have direct access to the model's weights, leaving open the question of whether availability attacks are still feasible.
- What evidence would resolve it: Experiments comparing attack success rates with and without access to the model's weights, using surrogate models for estimation.

### Open Question 3
- Question: How does the number of iterations allowed for crafting poisons affect the success rate of data poisoning attacks?
- Basis in paper: [explicit] The paper mentions that future work should explore data poisoning availability attacks limiting the number of times the attacker can craft its poisons during the training phase.
- Why unresolved: The paper does not investigate how restricting the number of iterations for crafting poisons impacts the effectiveness of the attack.
- What evidence would resolve it: Experiments with varying limits on the number of iterations allowed for crafting poisons, measuring the resulting attack success rates.

## Limitations

- Threat model assumes attacker can control 1-48% of training data and has access to auxiliary dataset, which may be challenging in specialized domains
- Gradient inversion stability across different network architectures and loss landscapes remains unclear
- Computational feasibility of iterative poisoning optimization may be prohibitive in large-scale distributed systems

## Confidence

- High Confidence: The fundamental equivalence between data poisoning and gradient attacks through gradient inversion is well-supported by the experimental results across multiple architectures and settings.
- Medium Confidence: The claim that data poisoning can achieve availability attacks with 1% contamination is supported, but the generalizability to other datasets and tasks requires further validation.
- Low Confidence: The scalability and practical feasibility of these attacks in real-world distributed learning systems with many participants and heterogeneous data remains uncertain.

## Next Checks

1. **Cross-Architecture Validation**: Test the gradient inversion approach on larger, more complex architectures (ResNets, EfficientNets) and diverse tasks (object detection, segmentation) to assess generalizability beyond CIFAR10 classification.

2. **Real-World Feasibility Study**: Evaluate the practical constraints of the threat model by analyzing the availability of auxiliary datasets in real federated learning scenarios and the computational overhead of iterative poisoning optimization.

3. **Robustness Analysis**: Investigate the effectiveness of advanced Byzantine-robust aggregation methods (FLtrust, FoolsGold) against these attacks, particularly in asynchronous settings where convergence dynamics differ from synchronous training.