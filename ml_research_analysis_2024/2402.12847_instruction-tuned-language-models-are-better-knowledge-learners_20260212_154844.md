---
ver: rpa2
title: Instruction-tuned Language Models are Better Knowledge Learners
arxiv_id: '2402.12847'
source_url: https://arxiv.org/abs/2402.12847
tags:
- documents
- knowledge
- training
- questions
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of updating factual knowledge
  in large language models (LLMs) to adapt to evolving information needs. The standard
  approach of continued pre-training on new documents followed by instruction-tuning
  on question-answer (QA) pairs shows limited success, even when perplexity is minimized.
---

# Instruction-tuned Language Models are Better Knowledge Learners

## Quick Facts
- arXiv ID: 2402.12847
- Source URL: https://arxiv.org/abs/2402.12847
- Reference count: 22
- LLMs improve knowledge absorption by 17.8% on Llama-2 7B and 16.3% on Llama-2 70B using pre-instruction-tuning

## Executive Summary
This paper addresses the challenge of updating factual knowledge in large language models to adapt to evolving information needs. The standard approach of continued pre-training on new documents followed by instruction-tuning on question-answer pairs shows limited success. The authors propose pre-instruction-tuning (PIT), which involves training LLMs on QA pairs before continued pre-training on documents. This approach significantly outperforms standard instruction-tuning, improving QA accuracies by 17.8% on Llama-2 7B and 16.3% on Llama-2 70B. The key insight is that learning how to access knowledge through questions before encoding knowledge from complex documents leads to better knowledge absorption.

## Method Summary
The proposed method, pre-instruction-tuning (PIT), involves a sequential training approach where LLMs are first trained on question-answer pairs to learn knowledge access patterns, then continued pre-trained on documents to encode knowledge, and finally fine-tuned with QA pairs to reinforce the connection between document content and question patterns. This contrasts with standard instruction-tuning, which learns how to extract knowledge after training on documents. The approach is evaluated using the Wiki2023 dataset, with performance measured through exact match (EM), recall, and ROUGE-L metrics. The paper also explores various PIT variants and sequencing strategies through ablation studies.

## Key Results
- PIT improves QA accuracy by 17.8% on Llama-2 7B (30.3% to 48.1%)
- PIT improves QA accuracy by 16.3% on Llama-2 70B (46.4% to 62.7%)
- The performance gain stems from learning knowledge access patterns before encoding knowledge from documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-instruction-tuning improves knowledge absorption by first teaching the model how knowledge is accessed through questions before it learns to encode knowledge from complex documents.
- Mechanism: The model first learns QA patterns on simple question-answer pairs, which helps it identify key information types. When subsequently exposed to documents, it can more effectively extract and encode relevant knowledge because it already understands the access patterns.
- Core assumption: QA pairs are simpler and more straightforward than documents, making them easier to learn from first.
- Evidence anchors:
  - [abstract] "QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner."
  - [section 5.1] "The intuition is that questions help LLMs recognize key types of information, enabling LLMs to focus on important information during pre-training on subsequent documents"
  - [corpus] Weak evidence - the corpus neighbors do not directly support this mechanism, though related papers mention instruction-tuning and knowledge learning.

### Mechanism 2
- Claim: Sequential training on QA pairs followed by documents (with associated QA pairs) prevents catastrophic forgetting and reinforces knowledge encoding.
- Mechanism: Training on QA pairs first establishes knowledge access patterns. When documents are introduced, the model can map document content to these established patterns. Including QA pairs during document training prevents the model from forgetting the question-answering format and reinforces the connection between document content and question patterns.
- Core assumption: The model can maintain separate representations for question patterns and document content that reinforce each other when trained together.
- Evidence anchors:
  - [section 5.1] "After training on documents (train doc in Fig. 4➅), the accuracy for corresponding questions (train QA in Fig. 4➅) dropped from almost perfect to 30%, indicating severe forgetting."
  - [section 5.1] "We train on the associated QA pairs and documents together (Fig. 4 ➆). As shown in Tab. 1, this significantly improves the performance"
  - [corpus] No direct evidence in corpus neighbors supporting this specific mechanism.

### Mechanism 3
- Claim: Understanding knowledge access patterns before encoding knowledge from documents is more effective than the reverse order.
- Mechanism: The model first learns how to retrieve and format knowledge through QA training. This pre-existing knowledge of access patterns guides the subsequent document encoding process, making it more targeted and effective. The model learns to encode information in a way that aligns with how it will be accessed later.
- Core assumption: The order of learning (access patterns before encoding) matters more than simply having both types of training.
- Evidence anchors:
  - [section 5.2] "positioning QA pairs before corresponding documents achieves better performance in both grouped and interleaved arrangements, indicating that during PIT, the learning mechanism prioritizes understanding how to access knowledge before learning to absorb information from the more complex and information-dense documents."
  - [section 5.2] "PIT++ significantly outperforms PIT (Fig. 4➆) from 45.4% to 48.1%, while training on QA data after on the mix (PIT-- in Tab. 2) does not yield additional benefits."
  - [corpus] Weak evidence - related papers discuss instruction-tuning but not specifically the ordering effect described here.

## Foundational Learning

- Concept: Language model pretraining and continued pretraining
  - Why needed here: Understanding how LLMs acquire and update knowledge through different training phases is crucial for implementing and debugging the PIT approach.
  - Quick check question: What is the difference between standard pretraining, continued pretraining, and instruction-tuning in terms of objectives and data formats?

- Concept: Knowledge representation in neural networks
  - Why needed here: The paper's effectiveness relies on understanding how factual knowledge is stored and retrieved from model parameters, which is fundamental to the PIT approach.
  - Quick check question: How do language models store factual knowledge in their parameters, and what mechanisms allow for retrieval through prompting or instruction-tuning?

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper explicitly addresses forgetting issues when training on documents after QA pairs, making this concept essential for understanding the PIT approach.
  - Quick check question: What causes catastrophic forgetting in neural networks, and what are common strategies to mitigate it during sequential training?

## Architecture Onboarding

- Component map: Document preprocessing pipeline -> Training orchestrator -> Evaluation framework
- Critical path: 1) Preparing Wiki2023 dataset with documents and QA pairs, 2) Implementing training scheduler for PIT variants, 3) Setting up evaluation with EM, recall, and ROUGE-L metrics, 4) Running ablation studies
- Design tradeoffs: Balancing training time against performance gains - PIT requires more complex training schedules but provides significant accuracy improvements. Another tradeoff is between dataset specificity and generalizability to other domains.
- Failure signatures: Model forgetting QA format after document training, insufficient training on either QA pairs or documents, overfitting to Wiki2023 domain limiting cross-domain generalization.
- First 3 experiments:
  1. Implement and test baseline continued pretraining followed by instruction-tuning to establish performance floor.
  2. Implement simplest PIT variant (QA only before document training) to verify basic hypothesis.
  3. Implement full PIT++ variant with optimized sequencing to confirm maximum performance gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does pre-instruction-tuning (PIT) perform when applied to non-Wikipedia documents or documents from different domains?
- Basis in paper: [explicit] The paper mentions evaluating PIT on non-Wikipedia documents (biographies synthesized in Zhu and Li (2023a)) and questions asked by real users.
- Why unresolved: The paper only provides results for a limited set of non-Wikipedia documents (biographies) and questions from real users. It's unclear how PIT would perform on a wider variety of document types and user queries.
- What evidence would resolve it: Conducting experiments on a diverse set of document types (e.g., web pages, scientific papers, news articles) and a large corpus of real user queries would provide a comprehensive understanding of PIT's performance across different domains.

### Open Question 2
- Question: What is the optimal number of epochs for pre-instruction-tuning (PIT) and how does it affect the performance of the model?
- Basis in paper: [explicit] The paper mentions fixing the number of epochs to 3 for PIT, but also conducts ablation studies with 1, 5, and 10 epochs.
- Why unresolved: The paper doesn't provide a clear explanation for choosing 3 epochs as the default. It's unclear if this is the optimal number or if there's a trade-off between performance and computational cost.
- What evidence would resolve it: Conducting a thorough ablation study with a wider range of epoch numbers and analyzing the performance gains and computational costs would help determine the optimal number of epochs for PIT.

### Open Question 3
- Question: How does pre-instruction-tuning (PIT) compare to other knowledge injection methods, such as retrieval-augmented generation (RAG) or parameter-efficient fine-tuning techniques?
- Basis in paper: [inferred] The paper focuses on PIT as a method for improving knowledge absorption in LLMs. It mentions RAG as a widely used approach for incorporating new knowledge but doesn't directly compare PIT to it or other fine-tuning techniques.
- Why unresolved: The paper doesn't provide a comprehensive comparison of PIT with other knowledge injection methods, making it difficult to assess its relative effectiveness and efficiency.
- What evidence would resolve it: Conducting experiments comparing PIT to RAG, parameter-efficient fine-tuning techniques (e.g., LoRA, prefix-tuning), and other knowledge injection methods on the same datasets would provide insights into the strengths and weaknesses of each approach.

## Limitations
- The findings are primarily based on experiments using the Wiki2023 dataset, which may limit generalizability to other domains or time periods.
- The paper does not address potential trade-offs in other aspects of model performance such as reasoning capabilities, generation quality, or computational efficiency during training.
- The mechanism by which PIT improves knowledge learning is inferred from ablation studies but not directly measured.

## Confidence
**High Confidence**: The empirical results showing that PIT outperforms standard instruction-tuning on the Wiki2023 dataset. The improvements in EM accuracy (17.8% for 7B, 16.3% for 70B models) are well-documented through controlled experiments with clear baselines and metrics.

**Medium Confidence**: The hypothesized mechanism that learning knowledge access patterns before encoding knowledge from documents is more effective than the reverse order. While supported by ablation studies, this remains an inference about internal model behavior rather than a directly measured phenomenon.

**Medium Confidence**: The claim that QA pairs are inherently simpler than documents and therefore better for initial training. This is logically sound but not empirically validated across diverse data types or domains.

## Next Checks
1. **Cross-domain validation**: Test PIT on non-Wikipedia datasets (e.g., scientific papers, news articles, technical documentation) to verify that the knowledge learning improvements generalize beyond the Wiki2023 domain.

2. **Ablation on knowledge encoding**: Design experiments that directly measure what knowledge is being encoded and how it's structured in the model parameters before and after PIT training, using techniques like probing classifiers or representation analysis.

3. **Long-term retention study**: Evaluate model performance after extended periods or additional training on unrelated tasks to measure whether PIT provides lasting improvements in knowledge retention or if the benefits decay over time.