---
ver: rpa2
title: 'Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models'
arxiv_id: '2408.13979'
source_url: https://arxiv.org/abs/2408.13979
tags:
- nemesis
- coop
- norm
- cocoop
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the impact of soft-prompt vector norms in
  vision-language models and uncovers the Low-Norm Effect, where reducing the norms
  of certain learned prompts can enhance model performance while increasing them often
  degrades it. To address this, the authors propose Nemesis, a method that normalizes
  soft-prompt vectors using either Position-Uniform Normalization (PUN) or Position-Aware
  Normalization (PAN) losses.
---

# Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models

## Quick Facts
- arXiv ID: 2408.13979
- Source URL: https://arxiv.org/abs/2408.13979
- Authors: Shuai Fu; Xiequn Wang; Qiushi Huang; Yu Zhang
- Reference count: 40
- Primary result: Nemesis improves CoOp soft-prompt tuning performance on 11 image classification datasets by normalizing soft-prompt vector norms using Position-Uniform (PUN) or Position-Aware (PAN) normalization losses

## Executive Summary
This paper investigates the impact of soft-prompt vector norms in vision-language models and uncovers the Low-Norm Effect, where reducing the norms of certain learned prompts can enhance model performance while increasing them often degrades it. To address this, the authors propose Nemesis, a method that normalizes soft-prompt vectors using either Position-Uniform Normalization (PUN) or Position-Aware Normalization (PAN) losses. Extensive experiments on 11 datasets show that Nemesis consistently improves performance over the baseline CoOp method, with the PAN loss variant demonstrating more robust results. The method also improves domain generalization and base-to-new generalization performance. Ablation studies confirm the effectiveness of the proposed normalization losses and provide insights into their impact on training dynamics and computational costs.

## Method Summary
Nemesis normalizes soft-prompt vectors in vision-language models using two proposed losses: Position-Uniform Normalization (PUN) and Position-Aware Normalization (PAN). PUN applies uniform normalization to all prompt vectors, while PAN selectively normalizes positions identified through pre-inference corruption experiments that induce the Low-Norm Effect. The method is integrated into the CoOp soft-prompt tuning framework, adding a normalization regularization term to the cross-entropy loss during training. The PAN loss includes an additional inference step before each training batch to identify and selectively normalize positions that would benefit from norm reduction.

## Key Results
- Nemesis consistently improves CoOp performance across 11 datasets, with PAN showing more robust results than PUN
- PAN loss achieves better performance than PUN by selectively normalizing positions that induce the Low-Norm Effect
- The method improves domain generalization and base-to-new generalization performance
- Computational overhead for PAN is minimal (~1% additional training time)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing soft-prompt vector norms at specific positions can enhance model performance, while increasing them degrades it (Low-Norm Effect).
- Mechanism: The normalization process reduces norm values at positions where the Low-Norm Effect occurs, allowing the model to achieve better performance by avoiding over-parameterization or noisy representations at these positions.
- Core assumption: Certain positions in soft-prompt vectors are more sensitive to norm changes than others, and the Low-Norm Effect is consistent across multiple runs and datasets.
- Evidence anchors:
  - [abstract]: "This work investigates the impact of soft-prompt vector norms in vision-language models and uncovers the Low-Norm Effect, where reducing the norms of certain learned prompts can enhance model performance while increasing them often degrades it."
  - [section]: "The corruption experiments reveal an interesting phenomenon: reducing the norms of soft prompts at specific positions enhances the performance, while increasing them could degrade the performance, which is termed Low-Norm Effect."
- Break condition: If the Low-Norm Effect is not consistent across multiple runs and datasets, or if certain positions do not show sensitivity to norm changes, this mechanism would break down.

### Mechanism 2
- Claim: Position-Aware Normalization (PAN) loss identifies and normalizes soft-prompt vectors at positions that induce the Low-Norm Effect, leading to improved performance.
- Mechanism: The PAN loss incorporates an additional inference process before each training batch to identify prompting positions that induce the Low-Norm Effect. It then selectively normalizes these positions, improving the model's performance.
- Core assumption: The pre-inference step can accurately identify positions that induce the Low-Norm Effect, and selectively normalizing these positions improves performance.
- Evidence anchors:
  - [abstract]: "To harness this effect, we propose a novel method named Normalizing the soft-prompt vectors of vision-language models (Nemesis) to normalize soft-prompt vectors in VLMs."
  - [section]: "To handle this, the Position-Aware Normalization (PAN) loss is proposed as a refined substitute for the PUN loss. Specifically, a pre-inference step is introduced before each training batch to identify positions that are likely to induce the Low-Norm Effect."
- Break condition: If the pre-inference step fails to accurately identify positions that induce the Low-Norm Effect, or if selectively normalizing these positions does not improve performance, this mechanism would break down.

### Mechanism 3
- Claim: Position-Uniform Normalization (PUN) loss can improve model performance by normalizing soft-prompt vectors at all positions, but it may degrade performance if it normalizes vectors unaffected by the Low-Norm Effect.
- Mechanism: The PUN loss normalizes the norms of all prompt vectors uniformly, imposing an equal weight on the norms of soft prompts at all positions. This approach can be easily integrated into existing soft-prompt methods but may degrade performance by restricting weight updates at unaffected positions.
- Core assumption: Normalizing soft-prompt vectors at all positions can improve performance, but it may also restrict weight updates at unaffected positions.
- Evidence anchors:
  - [abstract]: "To harness this effect, we propose a novel method named Normalizing the soft-prompt vectors of vision-language models (Nemesis) to normalize soft-prompt vectors in VLMs."
  - [section]: "To handle the Low-Norm Effect during prompt-tuning VLMs, we propose two losses for normalizing the norms of soft prompts: Position-Uniform Normalization (PUN) loss and Position-Aware Normalization (PAN) loss."
- Break condition: If normalizing soft-prompt vectors at all positions does not improve performance, or if it significantly restricts weight updates at unaffected positions, this mechanism would break down.

## Foundational Learning

- Concept: Soft-prompt tuning
  - Why needed here: Soft-prompt tuning is a parameter-efficient method for adapting large-scale pretrained vision-language models to downstream tasks. Understanding this concept is crucial for comprehending the role of soft-prompt vector norms in model performance.
  - Quick check question: What is the primary advantage of using soft-prompt tuning over full fine-tuning in vision-language models?

- Concept: Low-Norm Effect
  - Why needed here: The Low-Norm Effect is a phenomenon where reducing the norms of certain learned prompts enhances model performance, while increasing them often degrades it. This concept is central to the proposed Nemesis method.
  - Quick check question: How does the Low-Norm Effect impact the performance of vision-language models, and what are its implications for soft-prompt tuning?

- Concept: Position-aware vs. Position-uniform normalization
  - Why needed here: The proposed Nemesis method includes two types of normalization losses: Position-Aware Normalization (PAN) and Position-Uniform Normalization (PUN). Understanding the differences between these approaches is essential for grasping the method's effectiveness.
  - Quick check question: What are the key differences between Position-Aware Normalization (PAN) and Position-Uniform Normalization (PUN), and how do they impact the model's performance?

## Architecture Onboarding

- Component map:
  Image encoder -> Nemesis normalization -> Text encoder -> Cross-entropy loss

- Critical path:
  1. Encode input image using the image encoder
  2. Generate soft-prompt vectors using the Nemesis method
  3. Encode soft-prompt vectors and class names using the text encoder
  4. Calculate the similarity between visual and textual features
  5. Optimize the cross-entropy loss

- Design tradeoffs:
  - PUN loss vs. PAN loss: PUN loss is simpler but may degrade performance by restricting weight updates at unaffected positions, while PAN loss is more complex but can selectively normalize positions that induce the Low-Norm Effect
  - Normalization strength (ω): Larger ω values may lead to better performance for small shots but worse performance for large shots

- Failure signatures:
  - Inconsistent performance across datasets or runs
  - Degradation in performance when using PUN loss but not PAN loss
  - Inability to identify positions that induce the Low-Norm Effect

- First 3 experiments:
  1. Implement the PUN loss and evaluate its impact on model performance across multiple datasets
  2. Implement the PAN loss and compare its performance to the PUN loss
  3. Analyze the computational costs of the PUN and PAN losses and their impact on training time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the occurrence of the Low-Norm Effect vary across different types of vision-language models beyond CLIP?
- Basis in paper: [explicit] The paper focuses on CLIP-based models and investigates the Low-Norm Effect in this context, but does not explore its applicability to other VLMs like Flamingo or BLIP.
- Why unresolved: The paper's experiments are limited to CLIP-based models, and there is no evidence provided regarding the presence or absence of the Low-Norm Effect in other VLMs.
- What evidence would resolve it: Experiments on different VLMs, such as Flamingo or BLIP, demonstrating the presence or absence of the Low-Norm Effect and comparing it to CLIP-based models.

### Open Question 2
- Question: What is the optimal strategy for dynamically adjusting the normalization strength (ω) during training to maximize performance across different datasets and tasks?
- Basis in paper: [inferred] The paper mentions that a larger value of ω generally yields better performance for small shots, whereas a smaller value performs well for large shots, but does not provide a concrete strategy for dynamically adjusting ω.
- Why unresolved: The paper only provides a general observation about the relationship between ω and shot size, but does not offer a method for dynamically adjusting ω during training.
- What evidence would resolve it: A method for dynamically adjusting ω during training that outperforms fixed values across different datasets and tasks, supported by experimental results.

### Open Question 3
- Question: How does the Low-Norm Effect interact with other prompt-tuning methods, such as prefix-tuning or P-tuning, and can the proposed normalization techniques be effectively applied to these methods?
- Basis in paper: [explicit] The paper briefly mentions the potential applicability of the proposed normalization techniques to other PEFT methods like prefix-tuning and P-tuning, but does not provide concrete results or experiments.
- Why unresolved: The paper only provides preliminary experiments on visual prompt tuning and prefix-tuning, but does not thoroughly investigate the interaction between the Low-Norm Effect and these methods or explore the effectiveness of the proposed normalization techniques in these contexts.
- What evidence would resolve it: Experiments on various PEFT methods, such as prefix-tuning and P-tuning, demonstrating the presence of the Low-Norm Effect and the effectiveness of the proposed normalization techniques in improving performance.

## Limitations
- The fundamental reason why certain positions are more sensitive to norm changes than others remains unexplained
- PAN loss's pre-inference step is described but not fully specified, creating procedural ambiguity
- Computational efficiency claims need broader validation across different hardware configurations and scaling scenarios

## Confidence

**High Confidence**: The Nemesis method improves CoOp performance across multiple datasets when using either PUN or PAN losses.

**Medium Confidence**: The PAN loss variant demonstrates more robust results than PUN across different experimental settings.

**Medium Confidence**: The proposed method improves domain generalization and base-to-new generalization performance.

**Low Confidence**: The Low-Norm Effect is a consistent phenomenon across vision-language models and tasks.

## Next Checks
1. **Cross-model validation**: Test the Nemesis method with VLMs other than CLIP (e.g., OpenCLIP, BLIP) to verify whether the Low-Norm Effect and subsequent performance improvements generalize across different vision-language model architectures.

2. **Theoretical investigation**: Conduct ablation studies that systematically vary prompt vector dimensions, position sensitivity, and normalization strength to develop a theoretical understanding of why certain positions are more sensitive to norm changes than others.

3. **Large-scale efficiency validation**: Evaluate the PAN loss's computational overhead on multi-GPU setups with larger batch sizes and higher-resolution images to confirm the claimed ~1% overhead holds when scaling beyond the 2080Ti, 16-shot configuration used in the paper.