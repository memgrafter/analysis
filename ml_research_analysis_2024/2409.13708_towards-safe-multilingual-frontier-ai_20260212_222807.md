---
ver: rpa2
title: Towards Safe Multilingual Frontier AI
arxiv_id: '2409.13708'
source_url: https://arxiv.org/abs/2409.13708
tags:
- languages
- multilingual
- language
- safety
- capabilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the safety risks posed by multilingual jailbreaks
  in large language models (LLMs), which can exploit low-resource languages to evade
  safety measures. The authors propose policy recommendations to enhance multilingual
  capabilities while mitigating these risks, focusing on the European Union context.
---

# Towards Safe Multilingual Frontier AI

## Quick Facts
- arXiv ID: 2409.13708
- Source URL: https://arxiv.org/abs/2409.13708
- Reference count: 40
- Primary result: Multilingual jailbreaks exploit low-resource languages, with GPT-4o and Mistral Large 2 showing higher vulnerability; authors recommend policy measures for EU context

## Executive Summary
This paper addresses the safety risks posed by multilingual jailbreaks in large language models (LLMs), which can exploit low-resource languages to evade safety measures. The authors propose policy recommendations to enhance multilingual capabilities while mitigating these risks, focusing on the European Union context. They test five frontier LLMs across 24 official EU languages and find that two models (GPT-4o and Mistral Large 2) show higher vulnerability to multilingual jailbreaks in low-resource languages. The authors recommend mandatory reporting on model capabilities and susceptibility to multilingual jailbreaks, state support for multilingual dataset creation, and public preference research.

## Method Summary
The study evaluates multilingual jailbreak vulnerability across 24 EU languages using the OR-Bench dataset (toxic subset + hard-1k subset, 100 prompts each). Prompts were translated using Google Translate and evaluated through LLM APIs. Responses were classified using GPT-4o mini, and logistic regression analyzed the relationship between jailbreak success rates and language resourcedness (measured via CommonCrawl corpus share). The analysis tested whether jailbreak success and harmless prompt rejection rates correlate with language resourcedness.

## Key Results
- Two models (GPT-4o and Mistral Large 2) show higher vulnerability to multilingual jailbreaks in low-resource languages
- Jailbreak attack success rates tend to be higher for low-resource languages
- Translation artifacts can inadvertently neutralize harmful prompts in some cases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Low-resource languages are more vulnerable to multilingual jailbreaks because they have less safety-aligned training data.
- **Mechanism**: Models trained on predominantly high-resource language data have weaker safety filters in low-resource languages, allowing harmful prompts to bypass safeguards.
- **Core assumption**: The distribution of safety-aligned training data correlates with the CommonCrawl corpus share of a language.
- **Evidence anchors**:
  - [abstract]: "two models (GPT-4o and Mistral Large 2) show higher vulnerability to multilingual jailbreaks in low-resource languages"
  - [section]: "jailbreak attack success rates tend to be higher for low-resource languages"
  - [corpus]: "Weak - Only 8 neighbor papers found. FMR scores vary widely (0.0 to 0.657). Most neighbor papers focus on jailbreak attacks but few directly address low-resource language vulnerabilities."
- **Break condition**: If safety training data is equally distributed across languages regardless of resource level, this mechanism breaks.

### Mechanism 2
- **Claim**: Translation artifacts can inadvertently neutralize harmful prompts when translated to low-resource languages.
- **Mechanism**: Automated translation can distort the semantic intent of harmful prompts, making them appear harmless to the model.
- **Core assumption**: Translation quality affects the preservation of harmful intent across languages.
- **Evidence anchors**:
  - [section]: "2 of the 19 'Harmful' prompts that were accepted in Latvian, had actually become harmless through translation"
  - [corpus]: "Weak - Neighbor papers don't discuss translation artifacts as a safety mechanism."
- **Break condition**: If human evaluation of translations confirms preservation of harmful intent, this mechanism breaks.

### Mechanism 3
- **Claim**: Transparency requirements create competitive pressure for improving multilingual safety and capabilities.
- **Mechanism**: Mandating reporting on model performance across languages incentivizes providers to invest in underrepresented languages.
- **Core assumption**: Market competition drives investment in safety and capability improvements.
- **Evidence anchors**:
  - [section]: "Transparency allows consumers to compare model capabilities in the language they are interested in, supporting for a better consumer choice"
  - [corpus]: "Weak - No direct evidence in corpus about transparency driving competitive improvements."
- **Break condition**: If transparency requirements are ignored or if providers find ways to game the reporting system, this mechanism breaks.

## Foundational Learning

- **Concept**: Logistic regression for binary outcome prediction
  - **Why needed here**: To analyze the relationship between language resourcedness and jailbreak success rates
  - **Quick check question**: What does the β1 coefficient represent in the logistic regression model?

- **Concept**: Benjamini-Hochberg adjustment for multiple comparisons
  - **Why needed here**: To control for false discovery rate when testing multiple hypotheses across models
  - **Quick check question**: Why is p-value adjustment necessary when testing multiple models?

- **Concept**: Corpus share as a proxy for language resourcedness
  - **Why needed here**: To quantify the amount of digital data available for training in each language
  - **Quick check question**: What are the limitations of using CommonCrawl corpus share as a measure of language resourcedness?

## Architecture Onboarding

- **Component map**: Data collection → Translation → Model evaluation → Response classification → Statistical analysis
- **Critical path**: Prompt translation → API evaluation → Classification → Regression analysis
- **Design tradeoffs**: Translation accuracy vs. coverage vs. cost; automation vs. human evaluation
- **Failure signatures**: High variance in classification; translation artifacts; API rate limiting
- **First 3 experiments**:
  1. Test translation preservation by having native speakers evaluate translated prompts
  2. Compare automated vs. human classification accuracy on a subset of responses
  3. Run sensitivity analysis on the regression model with different language groupings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the level of language resourcedness affect the ability of large language models to resist multilingual jailbreak attacks?
- Basis in paper: [explicit] The paper states that jailbreak attack success rates tend to be higher for low-resource languages in GPT-4o and Mistral Large 2.
- Why unresolved: The relationship between language resourcedness and multilingual jailbreak vulnerability was only tested on a limited number of models and prompts. More comprehensive testing across a wider range of models and prompts is needed to confirm the findings.
- What evidence would resolve it: Conducting further tests with a larger sample size of prompts and a broader selection of language models would provide more conclusive evidence.

### Open Question 2
- Question: What is the impact of translation quality on the effectiveness of multilingual jailbreak attacks?
- Basis in paper: [explicit] The paper acknowledges that translation issues may bias the results and even invalidate them in borderline-significant cases.
- Why unresolved: The study relied on prompts translated from English, which may not capture the nuances of different languages. The impact of translation quality on the effectiveness of multilingual jailbreak attacks needs further investigation.
- What evidence would resolve it: Developing culture-specific harm datasets with native speaker input and implementing systematic human evaluation protocols would help assess the impact of translation quality on multilingual jailbreak attacks.

### Open Question 3
- Question: How can multilingual jailbreak vulnerabilities be effectively mitigated without compromising model capabilities?
- Basis in paper: [inferred] The paper mentions that some risk mitigation measures, such as instructing the model to "think in English" or generating multilingual training data, can limit the model's utility in non-English languages.
- Why unresolved: The study identifies the challenge of balancing safety and capabilities in multilingual models. More research is needed to develop effective mitigation strategies that do not compromise model performance.
- What evidence would resolve it: Conducting experiments to evaluate the effectiveness of different mitigation strategies and their impact on model capabilities would provide insights into the best approaches for addressing multilingual jailbreak vulnerabilities.

## Limitations
- Reliance on Google Translate introduces translation artifacts that may invalidate results in borderline-significant cases
- CommonCrawl corpus share proxy may not accurately capture actual distribution of safety-aligned training data
- Analysis focuses on 24 EU languages, limiting generalizability to non-European languages
- GPT-4o mini classification may introduce bias despite spot-checking with GPT-4o

## Confidence
- **High Confidence**: GPT-4o and Mistral Large 2 show higher vulnerability to multilingual jailbreaks in low-resource languages (supported by direct empirical testing)
- **Medium Confidence**: Jailbreak success rates correlate with language resourcedness (supported by statistical analysis but relies on proxy measures)
- **Low Confidence**: Proposed policy recommendations will effectively improve multilingual safety (logically connected but lack empirical validation)

## Next Checks
1. Conduct native speaker evaluations of translated prompts to determine the extent to which translation artifacts affect prompt harmfulness
2. Compare GPT-4o mini classification results against human expert classification on a stratified random sample of responses
3. Replicate the statistical analysis using alternative measures of language resourcedness to test the robustness of the correlation