---
ver: rpa2
title: Offset Unlearning for Large Language Models
arxiv_id: '2404.11045'
source_url: https://arxiv.org/abs/2404.11045
tags:
- unlearning
- forget
- offset
- llms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of unlearning sensitive information\
  \ from black-box large language models (LLMs) without requiring internal model access.\
  \ The proposed \u03B4-UNLEARNING framework achieves this by learning a logit offset\
  \ from a pair of smaller white-box models, which is then applied to the black-box\
  \ LLM's outputs."
---

# Offset Unlearning for Large Language Models

## Quick Facts
- arXiv ID: 2404.11045
- Source URL: https://arxiv.org/abs/2404.11045
- Reference count: 9
- This paper introduces δ-UNLEARNING, a framework for unlearning sensitive information from black-box LLMs by learning logit offsets from smaller white-box models

## Executive Summary
This paper addresses the challenge of unlearning sensitive information from black-box large language models without requiring internal model access. The proposed δ-UNLEARNING framework achieves this by learning a logit offset from a pair of smaller white-box models, which is then applied to the black-box LLM's outputs. The method avoids retaining sensitive data for inference, enhancing privacy protection. Experiments on the TOFU unlearning benchmark demonstrate that δ-UNLEARNING matches or outperforms direct fine-tuning baselines in both forget quality and model utility, while also being compatible with various unlearning algorithms.

## Method Summary
δ-UNLEARNING learns a logit offset term by contrasting the logits from two smaller models - one frozen (Mo) and one trainable (M'o) - initialized from the same checkpoint. During training, only M'o is updated to change its behavior on sensitive queries while Mo remains frozen. The difference between their logits captures how M'o deviates from Mo specifically on forget set data. This offset is then added to the black-box model's logits to steer predictions away from sensitive content without requiring weight access. The method is compatible with various unlearning algorithms including Gradient Ascent, Data Relabeling, and P-Adapter.

## Key Results
- δ-UNLEARNING achieves comparable or superior forget quality to direct fine-tuning baselines on TOFU benchmark
- Maintains model utility on out-of-forget-scope data including real author profiles and world facts
- Compatible with multiple unlearning algorithms while reducing computational overhead by tuning fewer parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The logit offset learned by contrasting two smaller models can approximate the unlearning behavior needed for the larger black-box model
- Mechanism: By initializing two smaller models (Mo and M'o) from the same checkpoint and updating only M'o, the difference between their logits captures how M'o deviates from Mo on forget set data. Adding this offset to the black-box model's logits steers predictions away from sensitive content
- Core assumption: The logit offset generalizes across model scales
- Evidence anchors:
  - [abstract]: "Instead of tuning the black-box LLM itself, δ-Unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models"
  - [section]: "The intuition behind this is that we can learn the offset term that approximates how a larger model should modify its prediction in the face of sensitive queries from the behavior adaptation of a smaller model"
- Break condition: If the logit offset fails to generalize across model scales, or if the black-box model's internal representations differ fundamentally from the smaller models

### Mechanism 2
- Claim: The product-of-experts formulation ensures the ensemble maintains performance on out-of-forget-scope data while unlearning sensitive content
- Mechanism: The ensemble combines the black-box model's logits with the offset term through additive combination in log space. For non-sensitive queries, the probability ratio between M'o and Mo remains close to one, preserving original behavior
- Core assumption: The logit offset naturally approaches zero for non-sensitive queries during training
- Evidence anchors:
  - [section]: "When querying non-sensitive, out-of-forget-scope information, the probability ratio between M'o and Mo should be close to one"
  - [section]: "When querying sensitive information that the model should forget, the token distribution of M'o differs from that of Mo to adjust the probability ratio"
- Break condition: If training fails to learn appropriate offsets for both sensitive and non-sensitive queries, leading to degradation on out-of-forget-scope data

### Mechanism 3
- Claim: Learning the offset through contrastive training between Mo and M'o is more parameter-efficient than direct fine-tuning of the black-box model
- Mechanism: Since only M'o is updated during training and both models are smaller than the black-box model, the parameter count for unlearning is reduced
- Core assumption: Computational savings from using smaller models outweighs potential performance trade-offs
- Evidence anchors:
  - [section]: "The computational overhead for training is minimal since the logits of the two frozen models M and Mo can be pre-computed in one forward pass prior to unlearning"
  - [section]: "This leads to an overall reduction in training time as δ-UNLEARNING tunes fewer parameters than direct fine-tuning"
- Break condition: If smaller models are too small to capture necessary unlearning behavior, requiring more parameters than saved by not fine-tuning the black-box model

## Foundational Learning

- Concept: Contrastive learning between paired models
  - Why needed here: The core mechanism relies on learning the difference between two models' behaviors on the same queries
  - Quick check question: If two models produce identical logits for a query, what should their logit offset be?

- Concept: Logit space manipulation for model control
  - Why needed here: The unlearning effect is achieved by adding an offset to logits rather than modifying model weights
  - Quick check question: What happens mathematically when you add a constant to all logits in a softmax distribution?

- Concept: Knowledge entanglement and its implications for unlearning
  - Why needed here: Understanding why maintaining performance on related but non-sensitive data is challenging
  - Quick check question: If you unlearn a specific fact about a person, what related information might also be affected due to knowledge entanglement?

## Architecture Onboarding

- Component map: Black-box LLM (frozen) -> Logits output -> Offset model pair (Mo frozen, M'o trainable) -> Logit offset computation -> Ensemble logits -> Final prediction
- Critical path: Query -> Black-box logits -> Offset logits -> Offset computation -> Ensemble -> Output
- Design tradeoffs: Parameter efficiency vs. potential generalization gap between model scales; training overhead vs. inference latency
- Failure signatures: Complete degradation on out-of-forget-scope data; inability to unlearn sensitive content; model becoming unusable at high offset strengths
- First 3 experiments:
  1. Verify that Mo and M'o initialized from same checkpoint produce identical logits on sample data
  2. Test offset computation by manually perturbing M'o logits and observing ensemble behavior
  3. Run small-scale unlearning on toy dataset to validate both forget quality and retention of unrelated knowledge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of δ-UNLEARNING compare to other unlearning methods when applied to larger LLMs (e.g., GPT-4 or Claude)?
- Basis in paper: [inferred] The paper focuses on Llama2 models (13B and 7B parameters) and suggests that δ-UNLEARNING incurs higher inference latency, which diminishes with larger models. However, it does not provide empirical results on larger models.
- Why unresolved: The paper only tests δ-UNLEARNING on Llama2 models. Larger models like GPT-4 or Claude are not evaluated, and their performance characteristics are unknown.
- What evidence would resolve it: Experiments applying δ-UNLEARNING to larger black-box LLMs and comparing their performance (forget quality, model utility, and inference latency) to other unlearning methods.

### Open Question 2
- Question: Can δ-UNLEARNING be adapted to handle dynamic unlearning scenarios where the forget set changes over time?
- Basis in paper: [explicit] The paper mentions that δ-UNLEARNING facilitates efficient version control and user customization by maintaining a pool of smaller models. However, it does not address scenarios where the forget set is updated dynamically.
- Why unresolved: The paper does not explore how δ-UNLEARNING handles updates to the forget set. It is unclear if the framework can efficiently adapt to changing unlearning requirements without retraining.
- What evidence would resolve it: Empirical studies showing the performance of δ-UNLEARNING when the forget set is updated incrementally, including the computational cost and effectiveness of the adaptation process.

### Open Question 3
- Question: How does the choice of offset strength (α) affect the trade-off between forget quality and model utility in real-world applications?
- Basis in paper: [explicit] The paper discusses the effect of varying offset strength (α) on the model's performance in the TOFU benchmark, showing that higher α values can lead to better forget quality but may degrade model utility.
- Why unresolved: The paper provides insights into the effect of α on the TOFU benchmark but does not explore its impact in real-world applications or provide guidelines for selecting an optimal α value.
- What evidence would resolve it: Real-world experiments applying δ-UNLEARNING with different α values to various tasks, measuring both forget quality and model utility, and identifying the optimal α range for practical use cases.

## Limitations

- The method assumes gray-box access to black-box LLM logits, which may not be available in all real-world scenarios
- All experiments use the TOFU fictitious author profiles dataset, limiting generalizability to real-world sensitive information
- The critical assumption that logit offsets generalize across model scales lacks direct empirical validation

## Confidence

- High confidence: The mathematical formulation of logit ensemble combination is sound
- High confidence: The contrastive learning framework between Mo and M'o is well-defined
- High confidence: Compatibility with existing unlearning algorithms is clearly demonstrated
- Medium confidence: Performance claims on TOFU benchmark, though limited to specific dataset
- Medium confidence: Parameter efficiency claims lack concrete computational comparisons
- Low confidence: Generalization to larger models and real-world sensitive data scenarios

## Next Checks

1. Test offset generalization by training δ-UNLEARNING with progressively smaller offset models (1b, 3b, 7b) and measuring performance degradation

2. Evaluate on real-world sensitive data scenarios beyond the TOFU benchmark, including medical and financial domains

3. Conduct ablation studies measuring actual computational resources (GPU hours, memory) required versus direct fine-tuning across different model sizes