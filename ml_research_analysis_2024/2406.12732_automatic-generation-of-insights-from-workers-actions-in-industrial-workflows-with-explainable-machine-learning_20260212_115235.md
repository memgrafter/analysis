---
ver: rpa2
title: Automatic generation of insights from workers' actions in industrial workflows
  with explainable Machine Learning
arxiv_id: '2406.12732'
source_url: https://arxiv.org/abs/2406.12732
tags:
- industrial
- workers
- worker
- data
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of standardized Key Performance Indicators
  (KPIs) for evaluating worker productivity in industrial workflows, a critical gap
  given the growing skilled workforce shortage. The proposed method combines Machine
  Learning classification with automated explainability techniques to differentiate
  between expert and inexpert workers, using sensor data from manufacturing machines
  and worker interactions captured via a mobile assistant.
---

# Automatic generation of insights from workers' actions in industrial workflows with explainable Machine Learning

## Quick Facts
- arXiv ID: 2406.12732
- Source URL: https://arxiv.org/abs/2406.12732
- Reference count: 40
- Primary result: Achieved >90% classification accuracy and F-measure for expert/inexpert worker differentiation using ML and explainable techniques in industrial workflows

## Executive Summary
This work addresses the critical gap in standardized Key Performance Indicators (KPIs) for evaluating worker productivity in industrial workflows, particularly given the growing skilled workforce shortage. The proposed method combines Machine Learning classification with automated explainability techniques to differentiate between expert and inexpert workers using sensor data from manufacturing machines and worker interactions captured via a mobile assistant. In a quality assessment workstation testbed, the system achieved classification accuracy and F-measure values exceeding 90%, providing actionable insights that enable knowledge transfer from expert to inexpert workers. The methodology supports in-house training and productivity enhancement in industrial environments, with potential extensions to other workflows and integration of real-time sensor data.

## Method Summary
The approach employs feature engineering and selection, worker-specific KPIs, and interpretable ML models to predict expertise levels and extract actionable insights. Manufacturing data and worker interactions are captured through a cobot workstation and mobile assistant interface, stored in a NoSQL database. Features are engineered from manufacturing data (e.g., output delay, time between valid pieces) and worker actions (e.g., number of reloads, buffer usage), then selected through Pearson correlation and MDI-based feature importance. These features feed into classifiers (SVC, Random Forest, AdaBoost) that predict expertise with >90% accuracy. The explainable ML component uses LIME to provide textual and visual reports highlighting relevant features and performance trends, enabling knowledge transfer from expert to inexpert workers.

## Key Results
- Classification accuracy and F-measure values exceeding 90% for expert/inexpert worker differentiation
- Nearly 100% performance metrics for session-level classification models
- Textual and visual reports highlighting relevant features and performance trends from explainable ML component
- Successful knowledge transfer from expert to inexpert workers through interpretable insights

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The combination of supervised ML with engineered worker KPIs enables accurate differentiation between expert and inexpert workers.
- Mechanism: Features engineered from manufacturing data and worker actions are selected through Pearson correlation and MDI-based feature importance, feeding into classifiers that predict expertise with >90% accuracy.
- Core assumption: Worker behavior differs measurably in feature space, and these differences are stable enough to train a classifier.
- Evidence anchors:
  - [abstract] "The approach employs feature engineering and selection, worker-specific KPIs, and interpretable ML models to predict expertise levels and extract actionable insights."
  - [section] "The latter are KPIs of interest to studying the evolution of inexpert and expert workers."
  - [corpus] Weak or missing; the corpus focuses on general AI and agentic systems, not worker performance classification.
- Break condition: If feature distributions overlap significantly between expert and inexpert workers, classification accuracy will drop below usable thresholds.

### Mechanism 2
- Claim: Local Interpretable Model-agnostic Explanations (LIME) provides actionable, human-readable insights from the black-box RF classifier.
- Mechanism: LIME simulates perturbed samples, predicts with the trained RF model, and fits a local interpretable model to extract feature relevance. These relevance scores are mapped to natural language templates explaining decisions.
- Core assumption: Perturbed samples in LIME's neighborhood sufficiently represent the decision boundary for interpretable explanations.
- Evidence anchors:
  - [abstract] "The explainable ML component provided textual and visual reports highlighting relevant features and performance trends."
  - [section] "The explainability module is based on Local Interpretable Model-agnostic Explanations (LIME ) [52]."
  - [corpus] Weak or missing; no direct mention of LIME or local interpretability in corpus neighbors.
- Break condition: If the black-box model is too complex or the neighborhood is too sparse, LIME explanations may be misleading or uninformative.

### Mechanism 3
- Claim: The dual scenario approach (piece-level and session-level classification) balances granularity and performance for real-time insights.
- Mechanism: Scenario 1 (piece-level) captures fine-grained behavior but suffers from lower accuracy due to noise; Scenario 2 (session-level) aggregates data, improving accuracy to nearly 100% and enabling richer explanations.
- Core assumption: Session-level aggregation smooths out transient variations while preserving meaningful worker behavior patterns.
- Evidence anchors:
  - [abstract] "In a quality assessment workstation testbed, the system achieved classification accuracy and F-measure values exceeding 90% for expert/inexpert worker differentiation."
  - [section] "Differentiating between expert and inexpert workers at the piece level was more challenging. However, the results at the session level were very satisfactory, with nearly 100 % performance metrics for all models."
  - [corpus] Weak or missing; corpus neighbors do not discuss multi-level classification strategies.
- Break condition: If sessions are too short or heterogeneous, aggregation may mask important distinctions.

## Foundational Learning

- Concept: Feature engineering and selection for industrial ML
  - Why needed here: Raw sensor and interaction logs are noisy and high-dimensional; meaningful features must be constructed to capture worker expertise.
  - Quick check question: What two methods are used to select features before classification?

- Concept: Local interpretable model-agnostic explanations (LIME)
  - Why needed here: The RF classifier is a black box; LIME extracts feature relevance for human-readable explanations.
  - Quick check question: What is the three-stage process LIME follows to generate explanations?

- Concept: Overall Equipment Effectiveness (OEE) and its worker analog
  - Why needed here: OEE standardizes machine efficiency; the work introduces equivalent KPIs for worker performance in its absence.
  - Quick check question: What are the two types of worker performance analysis proposed in the paper?

## Architecture Onboarding

- Component map:
  Manufacturing machine sensors -> NoSQL database (Elasticsearch) -> Data processing -> Feature engineering -> Feature selection -> KPI calculation -> ML classification (RF/AB/SVC) -> LIME explainability -> Dashboard

- Critical path:
  Data ingestion -> Feature engineering -> Classification -> Explanation generation

- Design tradeoffs:
  RF chosen for interpretability over SVC/AB despite similar accuracy; real-time constraints favor simpler models; NoSQL chosen for schema flexibility over relational.

- Failure signatures:
  Low classification accuracy -> feature selection or model choice problem; vague explanations -> LIME failure or overly complex model; dashboard errors -> data pipeline or KPI logic bug.

- First 3 experiments:
  1. Validate feature selection by training classifier with all features vs. selected features and comparing accuracy.
  2. Compare RF, SVC, and AB models on held-out test set to confirm >90% accuracy claim.
  3. Run LIME on a sample prediction and verify that top features align with domain knowledge (e.g., output delay, buffer usage).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed explainable ML system be extended to provide real-time feedback to workers during their tasks?
- Basis in paper: [inferred] The paper mentions the potential for real-time worker positioning and integrating body sensor data in future work, suggesting the system could provide real-time insights.
- Why unresolved: The current system focuses on post-hoc analysis and explanations, but the paper does not detail how real-time feedback could be implemented or its potential impact on worker performance.
- What evidence would resolve it: A study implementing real-time feedback mechanisms and measuring their effect on worker productivity, error rates, and learning curves compared to the current post-hoc system.

### Open Question 2
- Question: What are the specific thresholds or criteria used to determine whether a worker is classified as expert or inexpert in the current system?
- Basis in paper: [explicit] The paper mentions that target labels are extracted from the experience and position in the human resources records of the company, but does not provide specific details on the criteria used.
- Why unresolved: The lack of transparency in the classification criteria makes it difficult to assess the fairness and generalizability of the system across different industrial settings.
- What evidence would resolve it: A detailed description of the specific experience levels, skill assessments, or other criteria used to label workers as expert or inexpert in the company's HR records.

### Open Question 3
- Question: How does the proposed system handle the potential bias in the training data, particularly if the initial labeling of workers as expert or inexpert is subjective or inconsistent?
- Basis in paper: [inferred] The paper does not explicitly address bias in the training data, but the use of subjective HR records for labeling suggests this could be a concern.
- Why unresolved: Biased training data could lead to unfair or inaccurate predictions, limiting the system's effectiveness and potentially reinforcing existing inequalities in the workplace.
- What evidence would resolve it: An analysis of the training data for potential biases, such as imbalanced representation of different worker demographics or inconsistencies in the labeling criteria, and strategies to mitigate these biases.

## Limitations
- Absence of detailed technical specifications for the mobile assistant interface and exact feature engineering methodology, which are critical for faithful reproduction
- Limited analysis of potential overfitting given the relatively small testbed environment
- Reliance on LIME for explainability may produce misleading interpretations if the RF model's decision boundaries are highly non-linear or if the neighborhood sampling is insufficient

## Confidence
- **High Confidence**: The classification accuracy results (>90% for expert/inexpert differentiation) are well-supported by the methodology description and experimental setup.
- **Medium Confidence**: The explainability mechanism using LIME is technically sound but depends heavily on implementation details not fully specified in the paper.
- **Medium Confidence**: The feature engineering approach is reasonable but lacks detailed validation of feature importance rankings.

## Next Checks
1. Implement the full feature engineering pipeline and systematically compare classifier performance with all features versus the selected subset to verify the claimed improvement from feature selection.
2. Generate explanations for a diverse set of predictions and cross-validate the top features against domain expert knowledge to ensure the explanations are meaningful and not artifacts of the explanation method.
3. Test the trained model on data from different industrial workflows or different worker populations to assess the robustness and generalizability of the approach beyond the specific quality assessment workstation used in the study.