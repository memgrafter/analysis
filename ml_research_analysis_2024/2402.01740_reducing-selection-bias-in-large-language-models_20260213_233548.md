---
ver: rpa2
title: Reducing Selection Bias in Large Language Models
arxiv_id: '2402.01740'
source_url: https://arxiv.org/abs/2402.01740
tags:
- list
- bias
- temp
- figure
- primacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates selection bias in large language models
  (LLMs) when choosing objects from lists, a key operation in digital navigation and
  decision-making. The authors systematically evaluate how factors like temperature,
  list length, object identity, object type, and prompt complexity affect selection
  behavior in two LLMs: gpt-3.5-turbo-0613 and claude-instant-1.2.'
---

# Reducing Selection Bias in Large Language Models

## Quick Facts
- arXiv ID: 2402.01740
- Source URL: https://arxiv.org/abs/2402.01740
- Authors: J. E. Eicher; R. F. Irgolič
- Reference count: 31
- One-line primary result: Selection bias in LLMs shows strong model-dependent primacy effects that can be mitigated by separating guard rail enforcement from selection tasks

## Executive Summary
This paper systematically investigates selection bias in large language models when choosing objects from lists, a fundamental operation in digital navigation and decision-making. The authors evaluate how temperature, list length, object identity, object type, and prompt complexity affect selection behavior in two prominent LLMs. Their key finding reveals a strong primacy bias where early list items are disproportionately selected, with bias patterns varying significantly across models and object types. The study introduces a novel two-step method that separates the selection process from guard rail enforcement, effectively reducing bias while maintaining instruction adherence.

## Method Summary
The study employs a controlled experimental framework using two LLMs (gpt-3.5-turbo-0613 and claude-instant-1.2) to select objects from lists of letters or numbers. The researchers test multiple conditions including different list lengths (5-26 items), temperatures (ranging from 0 to 1.5), and object types. They implement two prompt engineering methods: a direct guard rail approach that combines selection and formatting, and a sample step method that separates these tasks. Each condition is repeated 1000 times to ensure statistical significance. The analysis quantifies selection probabilities, measures various bias metrics (primacy bias, correspondence, correct count), and calculates mutual information between object identity and position.

## Key Results
- LLMs exhibit strong primacy bias, with early list items being disproportionately selected across all tested conditions
- Guard rails increase selection bias and decrease instruction adherence, suggesting cognitive load effects
- The two-step method (separating selection from guard rail enforcement) significantly reduces bias while improving performance
- Selection bias patterns are model-dependent, with gpt-3.5-turbo showing stronger primacy effects than claude-instant-1.2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit selection bias that is strongly model-dependent and varies with object type and list length.
- Mechanism: The order in which objects are presented in a list influences selection probability, with early items being disproportionately chosen (primacy bias). This bias is modulated by the cognitive load imposed by complex tasks such as guard rail enforcement.
- Core assumption: The selection bias observed is not merely due to the semantic content of the objects but is a structural property of how LLMs process ordered information.
- Evidence anchors:
  - [abstract] "Our findings show that bias structure is strongly dependent on the model, with object type modulating the magnitude of the effect. With a strong primacy effect, causing the first objects in a list to be disproportionately represented in outputs."
  - [section] "For all conditions gpt-3.5-turbo showed a significantly increased primacy bias compared to claude-instant-1.2 (Figure 3). This trend was consistent across all temperatures and list lengths."

### Mechanism 2
- Claim: Introducing guard rails increases cognitive load, which in turn increases selection bias and reduces instruction adherence.
- Mechanism: Guard rails require the LLM to format its output in a specific structure (e.g., JSON), adding complexity. This increased cognitive load causes the model to compensate by relying more heavily on heuristics like primacy, leading to higher bias and lower adherence to instructions.
- Core assumption: The act of formatting output imposes additional cognitive load on the LLM, analogous to human cognitive load effects.
- Evidence anchors:
  - [abstract] "The usage of guard rails, a prompt engineering method of ensuring a response structure, increases bias and decreases instruction adherence when to a selection task."
  - [section] "The introduction of a guard rail resulted in a model agnostic gain in primacy bias... while decreasing the correct count, a metric of instruction-following behavior."

### Mechanism 3
- Claim: The mutual information between object identity and position increases with cognitive load, indicating stronger coupling between selection and order.
- Mechanism: Under high cognitive load (e.g., with direct guard rails), the LLM's selection becomes more dependent on the position of objects, as evidenced by increased mutual information. This suggests that the model is less able to independently consider object identity.
- Core assumption: Mutual information is a valid measure of the coupling between object identity and position in the LLM's selection process.
- Evidence anchors:
  - [abstract] "The bias is ablated when the guard rail step is separated from the list sampling step, lowering the complexity of each individual task."
  - [section] "When considering the mutual information, we see that the direct application of guard rails causes a large spike for all studied conditions... to be expected given the high primacy of direct guard rail applications reducing the number of possible positions."

## Foundational Learning

- Concept: Primacy Bias
  - Why needed here: Understanding that the order of items in a list affects selection probability is crucial for interpreting the study's findings.
  - Quick check question: If a list is presented as [A, B, C, D], which item is most likely to be selected first by an LLM exhibiting primacy bias?

- Concept: Cognitive Load Theory
  - Why needed here: The study suggests that LLMs experience a form of cognitive load when processing complex tasks, affecting their decision-making.
  - Quick check question: How might increasing the complexity of a task (e.g., adding a guard rail) affect an LLM's performance in terms of bias and instruction adherence?

- Concept: Mutual Information
  - Why needed here: Mutual information quantifies the relationship between object identity and position, helping to understand how selection is influenced by order.
  - Quick check question: What does a high mutual information value between object identity and position indicate about the LLM's selection process?

## Architecture Onboarding

- Component map:
  - Input: List of objects (letters or numbers) of varying lengths
  - LLM Models: gpt-3.5-turbo-0613 and claude-instant-1.2
  - Parameters: Temperature, list length, object type
  - Processing: Two methods - direct guard rail (combined selection and formatting) and sample step (separated tasks)
  - Output: Selected objects, bias metrics (primacy, correspondence, correct count), mutual information

- Critical path:
  1. Construct prompt with list and selection instruction
  2. Send prompt to LLM
  3. Process output with or without guard rail
  4. Analyze results for bias and adherence

- Design tradeoffs:
  - Direct guard rail vs. sample step: Direct is simpler but increases bias; sample step reduces bias but adds complexity
  - Temperature settings: Higher temperatures may reduce bias but increase randomness
  - List length: Longer lists may reduce primacy bias but introduce other biases

- Failure signatures:
  - High primacy bias: Indicates strong order dependency
  - Low correspondence: Suggests hallucination or incorrect object selection
  - Low correct count: Indicates failure to follow instructions

- First 3 experiments:
  1. Compare primacy bias between gpt-3.5-turbo and claude-instant-1.2 with a list of 10 letters at temperature 0
  2. Measure the effect of adding a guard rail on primacy bias using gpt-3.5-turbo with a list of 15 numbers
  3. Analyze mutual information between object identity and position for claude-instant-1.2 with a list of 20 letters at temperature 1.0

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does semantic content of objects affect selection bias compared to naive letters/numbers?
- Basis in paper: [explicit] Authors note that current study used "naive numbers and letters" and suggest "semantic content of the objects and the selection patterns" as a future research direction.
- Why unresolved: Current experiments used abstract symbols (A-Z, 1-26) without meaningful semantic content that would reflect real-world use cases.
- What evidence would resolve it: Experiments using semantically meaningful objects (e.g., real words, product names, or contextual items) showing how selection probabilities differ from the current abstract symbol results.

### Open Question 2
- Question: Does the order of objects in a list affect the joint probability of multiple objects being selected together?
- Basis in paper: [inferred] Authors mention "patterns in the objects that were selected together" as an area for future research but did not analyze P(ℓi∈xs|ℓj∈xs).
- Why unresolved: Current analysis focused on individual object and position probabilities, not conditional dependencies between objects.
- What evidence would resolve it: Analysis showing how the presence of one selected object influences the probability of other specific objects being selected from the same list.

### Open Question 3
- Question: How does cognitive load vary across different prompt structures beyond guard rails?
- Basis in paper: [explicit] Authors propose a "model of cognitive load" and suggest measuring "cognitive load of a LLM is measured in order to determine the appropriate compensation for a given task."
- Why unresolved: Study only examined guard rails vs sample step separation, but many other prompt engineering techniques could impose varying cognitive loads.
- What evidence would resolve it: Systematic measurement of selection bias across diverse prompt structures (multi-step reasoning, few-shot examples, chain-of-thought) to establish a cognitive load hierarchy.

## Limitations

- The study focuses on two specific LLM models, limiting generalizability to other architectures
- Experiments use abstract letters and numbers rather than semantically meaningful content
- The controlled experimental setup may not fully capture real-world selection scenarios and their complexities

## Confidence

- High Confidence: The existence of primacy bias across both tested models and the finding that guard rails increase selection bias while reducing instruction adherence
- Medium Confidence: The mechanism linking cognitive load to increased bias through the guard rail implementation
- Low Confidence: The generalizability of findings to other LLM models, task types, or real-world applications

## Next Checks

1. **Cross-Model Validation**: Test the same experimental protocol with additional LLM architectures (e.g., GPT-4, LLaMA, or open-source alternatives) to determine whether primacy bias and guard rail effects are universal properties or model-specific artifacts.

2. **Semantic Content Analysis**: Replace simple letter/number lists with semantically meaningful objects (e.g., product names, image descriptions) to assess whether bias patterns persist when object identity carries more contextual weight.

3. **Real-World Application Testing**: Implement a prototype agentic system that uses LLM selection from lists (e.g., a recommendation engine or navigation assistant) and measure whether the identified biases translate to measurable impacts on system performance and fairness in practical scenarios.