---
ver: rpa2
title: Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation
arxiv_id: '2405.03085'
source_url: https://arxiv.org/abs/2405.03085
tags:
- llms
- concepts
- concept
- context
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel concept-based Retrieval Augmented Generation
  (RAG) framework that utilizes Abstract Meaning Representation (AMR) to distill essential
  concepts from long-context supporting documents. The AMR-based concept distillation
  algorithm compresses the retrieved documents into a compact set of crucial concepts
  by formatting the informative nodes of the AMR graph, explicitly constraining LLMs
  to focus solely on vital information during inference.
---

# Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation

## Quick Facts
- **arXiv ID**: 2405.03085
- **Source URL**: https://arxiv.org/abs/2405.03085
- **Reference count**: 40
- **Primary result**: Concept-based RAG using AMR distillation significantly outperforms vanilla RAG on open-domain QA datasets, with >60% compression and improved robustness across document counts and LLM backbones.

## Executive Summary
This paper proposes a novel concept-based Retrieval Augmented Generation (RAG) framework that leverages Abstract Meaning Representation (AMR) to distill essential concepts from long-context supporting documents. By parsing documents into AMR graphs and extracting key concept nodes via DFS traversal, the method compresses cluttered raw documents into compact, informative concept sets. These distilled concepts are then used in a faithful-intensive prompt template with backbone LLMs, achieving higher accuracy and integration scores while reducing inference latency. The framework demonstrates strong robustness across varying numbers of supporting documents and different LLM architectures.

## Method Summary
The approach uses a retriever to obtain top-K supporting documents for a question, then parses each document into an AMR graph. A concept distillation algorithm performs DFS traversal on the AMR graph to extract and format key concept nodes, filtering out auxiliary and redundant information. The resulting concept set is integrated into a prompt template with the question and fed to a backbone LLM for answer generation. The method is evaluated on PopQA and EntityQuestions datasets using Accuracy and Integration metrics across various LLM backbones.

## Key Results
- Outperforms vanilla RAG with >60% average word-level compression while preserving core concepts
- Shows improved accuracy and integration scores, especially as the number of supporting documents increases
- Demonstrates robustness across various backbone LLMs and document counts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AMR-based concept distillation reduces attention dilution by removing irrelevant lexical detail while preserving semantic structure.
- Mechanism: AMR parses sentences into directed acyclic graphs where nodes are canonical concepts and edges encode relations. The algorithm performs DFS traversal to extract only concept nodes, filters out special frame-roles and rarely occurring nodes, then backtraces to match original lexical forms. This yields a compact, concept-focused context that bypasses LLM's tendency to get distracted by noisy document content.
- Core assumption: LLMs can infer complete scenarios from discrete concepts when those concepts are semantically faithful to the original document.
- Evidence anchors:
  - [abstract] "compresses the cluttered raw retrieved documents into a compact set of crucial concepts distilled from the informative nodes of AMR by referring to reliable linguistic features."
  - [section 3.2] "AMR incorporates special numerical annotations for certain parsing nodes... LLMs provide supporting documents comprising a set of concepts... the contextual learning capability of LLMs to distinguish between polysemous concepts."
  - [corpus] Weak—no corpus mention of AMR's effectiveness in RAG compression.
- Break condition: If LLM cannot infer missing lexical detail from context, the concept-only input will fail.

### Mechanism 2
- Claim: Discrete concept inputs reduce the input length and inference latency without sacrificing accuracy.
- Mechanism: By extracting only key concepts and filtering out frame arguments and low-frequency nodes, the algorithm reduces word count by >60% on average. Shorter prompts mean less time spent in self-attention computation and fewer tokens to process, directly reducing inference time per sample.
- Core assumption: The distilled concept set retains enough semantic information to allow accurate answer generation.
- Evidence anchors:
  - [section 5] "Compared to the standard Vanilla method, our method can compress the average word-level length by over 60% while preserving the core concepts to the greatest extent possible."
  - [section 5] "the context compressed by our method has a significant advantage in the inference of most LLMs."
  - [corpus] No explicit latency comparison in corpus, only compression ratios.
- Break condition: If compression removes critical context, accuracy will drop sharply.

### Mechanism 3
- Claim: Concept-based RAG improves robustness to varying document counts and backbone LLMs.
- Mechanism: The distilled concepts are less sensitive to document length and lexical variation, so the LLM can focus on semantic consistency rather than syntactic noise. This allows the same concept set to work well across different numbers of supporting documents (K) and different LLM architectures.
- Core assumption: Semantic features captured by AMR are more stable across document sets than raw text features.
- Evidence anchors:
  - [abstract] "outperforms other baseline methods, particularly as the number of supporting documents increases, while also exhibiting robustness across various backbone LLMs."
  - [section 5] "the proposed framework can be effectively coupled with diverse LLMs, making it a versatile solution for enhancing inference performance in long-context scenarios."
  - [corpus] No corpus evidence of cross-LLM robustness; claim inferred from experimental results.
- Break condition: If AMR parsing fails to capture core semantics for certain domains, robustness will break.

## Foundational Learning

- **AMR graph parsing and DFS traversal**
  - Why needed here: AMR provides the semantic backbone; DFS preserves the relative order of concepts in the source text, which is critical for LLM context understanding.
  - Quick check question: Given a sentence, can you manually draw its AMR graph and perform a DFS traversal to list the concept nodes in order?

- **Context compression via semantic filtering**
  - Why needed here: Removing frame-role edges and low-frequency nodes reduces noise without losing meaning, improving efficiency.
  - Quick check question: What is the effect on LLM performance if you keep frame-role edges versus removing them?

- **Inverse Document Frequency (IDF) filtering**
  - Why needed here: Filters out overly common concepts that do not help distinguish the answer, keeping the context concise.
  - Quick check question: How would you compute IDF for a set of concept nodes, and what threshold would you use?

## Architecture Onboarding

- **Component map**: Retriever -> Document set D -> AMR Parser -> AMR graph G -> Concept Distiller (DFS + filter + backtrace) -> Concept set C -> LLM + prompt template -> Answer A

- **Critical path**:
  1. Retrieve documents
  2. Parse each to AMR
  3. Extract concepts via DFS
  4. Format and backtrace
  5. Feed concept set to LLM
  6. Return answer

- **Design tradeoffs**:
  - DFS vs random traversal: DFS preserves order, random can lose key concept positions.
  - Keep vs drop frame-role edges: keeping increases semantic richness but also noise; dropping improves efficiency but risks losing nuance.
  - Backtrace to original text vs keep AMR canonical forms: backtrace improves LLM readability but adds complexity.

- **Failure signatures**:
  - Accuracy drops when K increases → concept extraction may be too aggressive.
  - Latency does not decrease → parsing or backtrace overhead dominates.
  - LLM output is incomplete or wrong → concept set too sparse or semantically ambiguous.

- **First 3 experiments**:
  1. Vary K=1 to 10, compare Acc. of concept-based vs Vanilla to confirm scaling advantage.
  2. Swap AMR parser for keyword extraction baseline, measure Acc. and compression ratio.
  3. Test on small LLM (e.g., bloom-560m) to see if concept-only inputs fail due to weak contextual inference.

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Performance gains primarily evaluated on two open-domain QA datasets, limiting generalization to other domains or complex reasoning tasks.
- Computational overhead of AMR parsing and DFS traversal not thoroughly analyzed, raising questions about practical efficiency.
- Reliance on accurate AMR parsing introduces potential failure points with ambiguous or domain-specific language.

## Confidence
- **High Confidence**: The mechanism of using AMR to extract and format essential concepts is well-supported by linguistic theory and the paper's ablation studies. The claim that concept-based RAG outperforms vanilla RAG on accuracy and integration metrics is strongly evidenced.
- **Medium Confidence**: The robustness across backbone LLMs and the inference latency reduction are plausible but rely on experimental results without systematic cross-LLM validation or explicit latency measurements.
- **Low Confidence**: The scalability and generalization of the approach to other domains or tasks beyond open-domain QA are not thoroughly explored.

## Next Checks
1. Test the concept-based RAG framework on datasets from different domains (e.g., biomedical, legal, or technical) to assess whether the AMR-based distillation generalizes beyond open-domain QA.
2. Systematically evaluate the framework with a broader range of LLM architectures (e.g., Mistral, Gemma, or domain-specific models) to validate robustness claims and identify potential failure points.
3. Measure the end-to-end latency of the AMR parsing, concept distillation, and RAG inference pipeline to determine if the compression benefits outweigh the parsing costs in real-world scenarios.