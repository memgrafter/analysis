---
ver: rpa2
title: Quantifying Feature Space Universality Across Large Language Models via Sparse
  Autoencoders
arxiv_id: '2410.06981'
source_url: https://arxiv.org/abs/2410.06981
tags:
- layers
- features
- scores
- saes
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the Universality Hypothesis for large language
  models (LLMs) by examining whether different models learn similar internal concept
  representations. The authors introduce a new approach using sparse autoencoders
  (SAEs) to disentangle polysemantic neurons into monosemantic features, then compare
  feature spaces across models.
---

# Quantifying Feature Space Universality Across Large Language Models via Sparse Autoencoders

## Quick Facts
- **arXiv ID**: 2410.06981
- **Source URL**: https://arxiv.org/abs/2410.06981
- **Reference count**: 40
- **Primary result**: High similarity scores in SAE feature spaces across multiple LLM pairs, particularly in middle layers, providing evidence for feature space universality

## Executive Summary
This paper investigates whether different large language models learn similar internal concept representations through a novel approach using sparse autoencoders (SAEs) to disentangle polysemantic neurons into monosemantic features. The authors develop a method to compare feature spaces across models by pairing SAE features using activation correlation and measuring spatial relation similarities with representational similarity measures like SVCCA and RSA. Their experiments demonstrate that SAE feature spaces show notably higher similarity scores across multiple LLM pairs compared to neuron-level comparisons, particularly in middle layers, providing empirical support for the Universality Hypothesis in deep learning.

## Method Summary
The paper introduces a novel methodology for quantifying feature space universality across large language models by first training sparse autoencoders (SAEs) on different models to extract monosemantic features, then comparing these feature spaces using activation correlation for feature pairing and rotation-invariant similarity measures (SVCCA and RSA) to quantify spatial relationships. The approach filters non-concept features and many-to-1 mappings to improve similarity scores, and validates results through semantic subspace matching experiments with predefined concept categories.

## Key Results
- SAE feature spaces show notably higher similarity scores across model pairs compared to neuron-level comparisons
- Similarity is particularly pronounced in middle layers of the networks
- Semantically meaningful subspaces exhibit notable similarity across models when analyzed with concept-specific filtering
- Statistical significance is achieved with p < 0.05 for paired feature scores compared to random baselines

## Why This Works (Mechanism)

### Mechanism 1
Activation correlation can pair features across models even when SAEs learn different feature sets. Features that activate on the same tokens across models are likely representing similar concepts, allowing feature matching despite different learned representations. The core assumption is that token-level activation patterns are sufficient to identify semantically similar features across models. Break condition: If models use different tokenizers or if activation patterns are not comparable across models due to architectural differences.

### Mechanism 2
Representational similarity measures reveal hidden relational similarities in feature spaces. Rotation-invariant measures like SVCCA and RSA transform feature spaces to reveal geometric similarities in how features relate to each other, regardless of specific feature identities. The core assumption is that feature spaces have similar geometric arrangements that can be detected through transformation-invariant similarity measures. Break condition: If feature spaces lack consistent geometric relationships or if transformation-invariant measures fail to capture meaningful similarities.

### Mechanism 3
Filtering non-concept features and many-to-1 mappings improves similarity scores by reducing noise. Removing features that don't represent clear concepts and reducing information collapse from many-to-1 mappings yields cleaner feature space comparisons. The core assumption is that non-concept features and many-to-1 mappings introduce noise that obscures true feature space similarities. Break condition: If filtering removes meaningful features or if many-to-1 mappings actually represent valid feature relationships.

## Foundational Learning

- **Concept**: Polysemanticity and monosemanticity
  - Why needed here: Understanding why SAEs are needed to disentangle polysemantic neurons into monosemantic features is fundamental to the paper's approach
  - Quick check question: Why can't we directly compare features across models without using SAEs?

- **Concept**: Representational similarity measures (SVCCA, RSA)
  - Why needed here: These are the core metrics used to quantify feature space similarity and understanding their mechanics is essential
  - Quick check question: What makes SVCCA and RSA "rotation-invariant" and why is this important for comparing different models?

- **Concept**: Feature pairing by activation correlation
  - Why needed here: This is the key method for aligning features across different SAEs before applying similarity measures
  - Quick check question: How does activation correlation pairing work when comparing features from SAEs trained on different models?

## Architecture Onboarding

- **Component map**: Load LLM activations -> Train SAEs on different models -> Compute feature activation correlations -> Filter and pair features -> Apply SVCCA/RSA -> Generate similarity scores -> Analyze semantic subspaces

- **Critical path**: 1. Train or load SAEs for multiple models 2. Obtain activations from test dataset 3. Compute feature activation correlations across models 4. Filter and pair features 5. Apply representational similarity measures 6. Analyze and interpret results

- **Design tradeoffs**:
  - Tokenization consistency vs model diversity: Requires same tokenizer but limits model comparison
  - Feature width matching vs width variation: Same width simplifies comparison but may miss insights from width differences
  - Filtering stringency vs feature retention: Stricter filtering reduces noise but may discard meaningful features

- **Failure signatures**:
  - Low similarity scores across all layer pairs → Feature spaces fundamentally different or pairing method failing
  - High variance in p-values → Insufficient samples or unstable correlation estimates
  - No improvement after filtering → Filtering criteria too strict or not targeting correct noise sources

- **First 3 experiments**:
  1. Compare SAEs from same model with different seeds to verify method works within consistent feature space
  2. Compare SAEs from same architecture (e.g., Pythia-70m vs Pythia-160m) to establish baseline similarity
  3. Compare SAEs from different model families (e.g., Gemma vs Llama) to test generalizability

## Open Questions the Paper Calls Out

### Open Question 1
What specific transformations are needed to transfer steering vectors across models with similar SAE feature spaces? The paper demonstrates feature space similarity but doesn't identify the exact transformation mechanisms needed to transfer steering vectors between models.

### Open Question 2
How do SAEs trained on different datasets or with different hyperparameters capture feature universality differently? While the paper shows that SAE quality affects feature space similarity, it doesn't systematically investigate how dataset choice or training parameters influence the ability to capture universal features.

### Open Question 3
What proportion of LLM features are truly universal versus idiosyncratic to specific models? The paper demonstrates both universal and non-universal feature patterns but doesn't quantify the exact proportions or determine whether non-universal features represent limitations of SAEs versus genuine model-specific representations.

## Limitations

- The paper doesn't fully specify SAE training hyperparameters or exact filtering criteria for non-concept features, affecting reproducibility
- While demonstrating feature space similarity, the paper doesn't conclusively prove that matched features represent the same semantic concepts across models
- The study focuses on middle layers without comprehensively exploring whether feature space universality holds across all network depths

## Confidence

- **High Confidence**: The methodology for comparing SAE feature spaces using SVCCA and RSA is well-established and experimental results showing higher similarity in SAE feature spaces versus neuron spaces are robust
- **Medium Confidence**: The claim that feature space universality provides evidence for the Universality Hypothesis is plausible but requires additional semantic validation
- **Low Confidence**: The assertion that activation correlation pairing reliably identifies semantically equivalent features assumes token-level activation patterns are sufficient for semantic matching

## Next Checks

1. **Semantic Validation of Paired Features**: Implement direct semantic validation where matched features are manually inspected or tested with concept-specific probes to verify they capture the same underlying concepts across models, not just similar activation patterns

2. **Cross-Architectural Comparison**: Test the feature pairing and similarity measurement approach on fundamentally different architectures (e.g., transformer vs LSTM) to determine if observed universality extends beyond similar model families

3. **Temporal Stability Analysis**: Evaluate whether paired features maintain their similarity scores across different time periods or dataset shifts to assess the robustness and practical utility of the feature matching approach