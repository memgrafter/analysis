---
ver: rpa2
title: Transductive Zero-Shot and Few-Shot CLIP
arxiv_id: '2405.18437'
source_url: https://arxiv.org/abs/2405.18437
tags:
- few-shot
- transductive
- clip
- learning
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a transductive inference approach for zero-shot
  and few-shot CLIP classification, addressing the challenge of jointly classifying
  a mini-batch of unlabeled query samples. The method constructs informative vision-text
  probability features and models data distributions using Dirichlet laws, formulating
  the classification as an optimization problem on the unit simplex.
---

# Transductive Zero-Shot and Few-Shot CLIP

## Quick Facts
- arXiv ID: 2405.18437
- Source URL: https://arxiv.org/abs/2405.18437
- Reference count: 40
- Primary result: Near 20% improvement in ImageNet accuracy over standard zero-shot CLIP via transductive inference

## Executive Summary
This paper introduces a transductive inference approach for zero-shot and few-shot classification using CLIP, addressing the challenge of jointly classifying a mini-batch of unlabeled query samples. The method constructs informative vision-text probability features and models data distributions using Dirichlet laws, formulating the classification as an optimization problem on the unit simplex. A novel block Majorization-Minimization algorithm is proposed to efficiently estimate distribution parameters and class assignments. Experiments on 11 datasets demonstrate near 20% improvement in ImageNet accuracy over standard zero-shot CLIP and superior performance compared to state-of-the-art few-shot methods.

## Method Summary
The approach uses CLIP's visual and text encoders to generate probability features via softmax over cosine similarities between image and text embeddings. These probability vectors, which naturally lie on the unit simplex, are modeled using Dirichlet distributions for each class. Classification is formulated as a joint optimization over Dirichlet parameters and class assignments, solved via a novel block Majorization-Minimization algorithm. The method includes an MDL-based regularization term to prevent over-partitioning and uses graph matching for zero-shot cluster-to-class assignment.

## Key Results
- Near 20% improvement in ImageNet accuracy over CLIP's zero-shot performance
- Superior performance compared to state-of-the-art few-shot methods including TIP-Adapter, CoOp, and transductive methods like BD-CSPN and Laplacian Shot
- Consistent improvement across 11 diverse datasets including Caltech101, OxfordPets, StanfordCars, and Flowers102

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using Dirichlet distributions over the simplex directly models the probability nature of CLIP's zero-shot predictions, avoiding distributional mismatch.
- Mechanism: By modeling each class's feature vectors with a Dirichlet law, the method aligns the statistical assumptions with the simplex-constrained, non-negative, sum-to-one nature of the probability features constructed via softmax over cosine similarities.
- Core assumption: CLIP's zero-shot probability vectors lie naturally on the unit simplex and their empirical distribution is well-approximated by Dirichlet.
- Evidence anchors:
  - [abstract]: "Our strategy consists in defining, for every n ∈ {1, . . . , N}, the feature vector for the data sample xn as CLIP's zero-shot probability... the task becomes a classification problem on the unit simplex of RK"
  - [section]: "Given feature vectors lying within the unit simplex set of RK, we advocate modeling the data using Dirichlet distributions."
- Break condition: If CLIP's probability vectors exhibit multimodality or heavy tails not captured by Dirichlet, or if feature vectors deviate strongly from the simplex (e.g., due to temperature tuning or different prompt styles), the model fit will degrade.

### Mechanism 2
- Claim: Transductive inference improves classification by jointly exploiting query set statistics rather than treating each sample independently.
- Mechanism: The method estimates Dirichlet parameters and class assignments simultaneously over the entire query batch, leveraging shared information across samples. The MDL partition-complexity term discourages overly balanced predictions, making the method robust to imbalanced few-shot scenarios.
- Core assumption: Query samples share sufficient statistical structure to be jointly modeled; the Dirichlet mixture assumption holds across the batch.
- Evidence anchors:
  - [abstract]: "our batch inference approach... yields near 20% improvement in ImageNet accuracy over CLIP's zero-shot performance"
  - [section]: "In the transductive paradigm, one makes joint predictions for a batch of query samples, taking advantage of the query set statistics."
- Break condition: If the query set is too small, too heterogeneous, or contains outliers, joint modeling may introduce noise rather than benefits; also, if classes are extremely imbalanced, MDL term may over-penalize minority classes.

### Mechanism 3
- Claim: The novel block Majorization-Minimization algorithm avoids cumbersome inner iterations in Dirichlet parameter estimation, improving computational efficiency.
- Mechanism: Instead of iterative Newton-type steps on digamma functions (as in prior methods), a tight quadratic majorant of the negative log-likelihood is constructed, yielding closed-form updates at each iteration.
- Core assumption: The quadratic majorant is sufficiently tight to ensure rapid convergence while preserving computational tractability.
- Evidence anchors:
  - [abstract]: "The minimization problem is then tackled with a novel block Majorization-Minimization algorithm, which simultaneously estimates the distribution parameters and class assignments."
  - [section]: "At each iteration of our MM procedure, the minimizer of the majorizing function (13) is the positive root of a quadratic polynomial equation, resulting in Algorithm 1."
- Break condition: If the majorant is too loose, convergence may be slow; if numerical stability issues arise (e.g., near-zero probabilities), closed-form updates may fail or produce inaccurate estimates.

## Foundational Learning

- Concept: Dirichlet distribution properties (conjugacy, simplex support, density form).
  - Why needed here: Core to modeling the probability simplex features and deriving tractable updates.
  - Quick check question: What constraints must the parameters of a Dirichlet distribution satisfy, and why do these align with probability simplex features?

- Concept: Transductive learning theory (advantages of joint inference over independent).
  - Why needed here: Explains why transductive CLIP inference can outperform inductive methods.
  - Quick check question: Under what conditions does transductive inference provide a provable advantage over inductive inference in classification tasks?

- Concept: Expectation-Maximization (EM) algorithm structure and convergence.
  - Why needed here: The proposed algorithm is closely related to EM for mixture models, and understanding this link clarifies both the objective and the optimization path.
  - Quick check question: How does the objective in this work reduce to a standard EM objective when the MDL term is removed and λ is set to |Q|?

## Architecture Onboarding

- Component map: CLIP visual and text encoders -> probability features via softmax over cosine similarities -> Dirichlet mixture model fitting (per class) -> EM-like alternating optimization (Dirichlet params <-> class assignments) -> MDL partition complexity regularization -> Graph matching for zero-shot cluster-to-class assignment

- Critical path: 1. Encode query batch -> compute probability features 2. Initialize assignments (CLIP probabilities) and Dirichlet params 3. Alternate: (a) update Dirichlet params via MM, (b) update assignments via softmax 4. Apply MDL regularization to discourage over-partitioning 5. For zero-shot: match clusters to classes via bipartite graph matching

- Design tradeoffs:
  - Dirichlet vs. Gaussian modeling: Dirichlet better fits simplex features but may require more careful initialization; Gaussian is simpler but mismatches data geometry
  - MDL term strength (λ): Higher λ encourages fewer clusters but may hurt fine-grained classification; lower λ may overfit to noise
  - Temperature T in softmax: Controls sharpness of CLIP probabilities; affects feature smoothness and Dirichlet fit

- Failure signatures:
  - Poor Dirichlet fit -> multimodal or skewed class distributions not captured; consider alternative simplex distributions
  - Slow convergence -> majorant may be too loose; try Minka's original MM or adaptive temperature tuning
  - Over-regularization -> MDL term too strong; check cluster purity vs. cluster count trade-off
  - Class mismatch in zero-shot -> graph matching fails; consider alternative assignment criteria or allow one-to-many assignments

- First 3 experiments:
  1. Ablation: Run clustering with and without MDL term on a small zero-shot dataset; verify impact on cluster count and accuracy
  2. Parameter sweep: Vary λ in [0.1|Q|, |Q|] on a few-shot validation set; plot accuracy vs. λ
  3. Distribution test: Replace Dirichlet with Gaussian on probability features; compare accuracy and runtime on ImageNet zero-shot

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed transductive approach perform on other vision-language models beyond CLIP?
- Basis in paper: [inferred] The paper focuses exclusively on CLIP and does not evaluate the approach on other models like Flamingo or BLIP.
- Why unresolved: The authors do not provide any comparative analysis or justification for focusing only on CLIP.
- What evidence would resolve it: Experimental results comparing the transductive approach on multiple vision-language models, including zero-shot and few-shot classification tasks.

### Open Question 2
- Question: What is the impact of different Dirichlet distribution parameterizations on the classification accuracy?
- Basis in paper: [explicit] The paper mentions that the Dirichlet distribution is characterized by positive parameters but does not explore different parameterizations.
- Why unresolved: The authors do not provide an ablation study on the effect of different Dirichlet parameter settings.
- What evidence would resolve it: Systematic evaluation of classification accuracy using different Dirichlet parameterizations, including uniform, informative, and learned parameters.

### Open Question 3
- Question: How does the transductive approach handle noisy or ambiguous class labels in the support set?
- Basis in paper: [inferred] The paper assumes clean support set labels but does not address potential label noise or ambiguity.
- Why unresolved: The authors do not discuss robustness to label noise or provide experiments with noisy support sets.
- What evidence would resolve it: Experiments evaluating the transductive approach's performance with varying levels of label noise in the support set, including both random and systematic noise patterns.

## Limitations
- The Dirichlet distribution assumption for CLIP's probability features is critical but not empirically validated across all 11 datasets; deviations from the simplex or multimodal class distributions could degrade performance
- The effectiveness of the MDL regularization term depends on the choice of λ, but the paper does not provide a systematic sensitivity analysis or clear guidelines for tuning across different datasets
- The graph matching step for zero-shot cluster-to-class assignment is mentioned but details are deferred to an appendix, raising concerns about reproducibility and robustness in ambiguous or overlapping class cases

## Confidence
- High confidence: The overall transductive inference framework (joint batch modeling) and the superiority over standard CLIP zero-shot are well-supported by experimental results
- Medium confidence: The Dirichlet modeling assumption and the specific form of the MDL regularization are theoretically justified but may not generalize to all CLIP probability feature distributions
- Low confidence: The exact implementation of the graph matching procedure and the robustness of the method to extreme class imbalance or outliers in the query set

## Next Checks
1. **Dirichlet fit validation:** For each of the 11 datasets, visualize and quantify the goodness-of-fit of the fitted Dirichlet distributions to the CLIP probability features (e.g., using Kolmogorov-Smirnov or likelihood ratio tests)
2. **MDL term sensitivity:** Perform a systematic sweep of λ values on a representative subset of datasets; plot accuracy vs. λ and identify the optimal range and any signs of overfitting or under-regularization
3. **Graph matching robustness:** Construct controlled synthetic scenarios where class boundaries are ambiguous or overlapping; assess the accuracy and stability of the graph matching step and explore alternative assignment strategies (e.g., one-to-many or probabilistic matching)