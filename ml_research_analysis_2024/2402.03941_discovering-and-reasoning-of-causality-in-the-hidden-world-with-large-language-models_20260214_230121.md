---
ver: rpa2
title: Discovering and Reasoning of Causality in the Hidden World with Large Language
  Models
arxiv_id: '2402.03941'
source_url: https://arxiv.org/abs/2402.03941
tags:
- causal
- llms
- data
- discovery
- factors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COAT, a framework that leverages large language
  models (LLMs) to discover causal factors from unstructured data by iteratively proposing,
  annotating, and refining candidate variables. Instead of directly using LLMs for
  causal reasoning, COAT uses them as factor proposers and annotators, with causal
  discovery algorithms providing feedback to improve the proposals.
---

# Discovering and Reasoning of Causality in the Hidden World with Large Language Models

## Quick Facts
- arXiv ID: 2402.03941
- Source URL: https://arxiv.org/abs/2402.03941
- Reference count: 20
- Primary result: COAT achieves up to 100% F1-score in recovering causal relations from unstructured data using LLM-driven factor discovery

## Executive Summary
This paper introduces COAT, a framework that leverages large language models (LLMs) to discover causal factors from unstructured data through an iterative process of factor proposal, data annotation, and causal discovery. Rather than using LLMs directly for causal reasoning, COAT employs them as factor proposers and annotators, with causal discovery algorithms providing feedback to refine the proposals. The framework demonstrates reliable causal discovery from unstructured observational data, achieving significant improvements over direct LLM reasoning baselines across synthetic and real-world benchmarks.

## Method Summary
COAT operates through an iterative framework where an LLM proposes potential causal factors from unstructured data, another LLM annotates the data based on these factors, and a causal discovery algorithm (FCI) identifies causal relationships. The system provides feedback by identifying samples poorly explained by current factors, prompting the LLM to propose additional factors in subsequent iterations. This process continues until no new factors are proposed, with the goal of recovering the Markov Blanket of the target variable.

## Key Results
- COAT achieved up to 100% F1-score in recovering causal relations in the AppleGastronome benchmark
- Significant improvements over direct LLM reasoning baselines across both synthetic and real-world benchmarks
- The framework successfully handles subjective attributes and demonstrates robust performance in multi-iteration refinement
- Markov Blanket recovery theoretical guarantees established under specific conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can effectively propose high-level causal factors from unstructured data when guided by proper prompts
- Mechanism: LLMs leverage pre-trained knowledge from massive observations to relate unstructured inputs to potential causal factors, similar to how human experts would brainstorm factors
- Core assumption: LLMs have learned sufficient world knowledge during pre-training to understand the domain context and propose relevant causal factors
- Evidence anchors:
  - [abstract] "Trained from massive observations of the world, LLMs demonstrate impressive capabilities in comprehending unstructured inputs, and leveraging the learned rich knowledge to resolve a variety of general tasks"
  - [section 3.2] "LLM will be instructed by st to find potential causal factors using the pre-trained knowledge from large observations of the world"
  - [corpus] Weak - related papers focus on causal discovery with LLMs but don't specifically address factor proposal from unstructured data
- Break condition: If the LLM lacks sufficient domain knowledge or fails to comprehend the unstructured data context, it may propose irrelevant or incomplete factors

### Mechanism 2
- Claim: Causal discovery algorithms can provide reliable feedback to improve LLM factor proposals
- Mechanism: The causal discovery algorithm (e.g., FCI) analyzes the proposed factors and identifies samples where current factors poorly explain the target variable, which are then used to refine future factor proposals
- Core assumption: The causal discovery algorithm can identify gaps in current factor proposals through prediction errors
- Evidence anchors:
  - [abstract] "Instead of directly using LLMs for causal reasoning, COAT uses them as factor proposers and annotators, with causal discovery algorithms providing feedback to improve the proposals"
  - [section 3.3] "we could find useful information and thus provide feedbacks to further improve the factor proposal"
  - [corpus] Weak - related papers mention combining LLMs with causal discovery but don't elaborate on feedback mechanisms
- Break condition: If the causal discovery algorithm fails to identify meaningful gaps or provides misleading feedback, the refinement process may degrade rather than improve factor proposals

### Mechanism 3
- Claim: LLMs can effectively annotate unstructured data into structured format based on proposed factors
- Mechanism: A second LLM is prompted with annotation guidelines derived from the proposed factors to parse unstructured observations into structured data suitable for causal discovery
- Core assumption: LLMs can follow annotation guidelines and accurately map unstructured data to structured factor values
- Evidence anchors:
  - [abstract] "LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria)"
  - [section 3.2] "Another LLM could be leveraged to read the guidelines and parse the unstructured observations into structured data"
  - [section 4.2] "Both LLMs are generally good at annotating subjective attributes" (from AppleGastronome results)
- Break condition: If the LLM introduces significant annotation noise or biases that correlate with the target variable, it may introduce confounding rather than useful information

## Foundational Learning

- Concept: Markov Blanket and its role in causal discovery
  - Why needed here: COAT aims to discover the Markov Blanket of a target variable, which contains all variables that directly influence or are influenced by the target
  - Quick check question: If variable A is in the Markov Blanket of target Y, what does this imply about the conditional independence relationship between A and Y?

- Concept: Structure of causal discovery algorithms (particularly FCI)
  - Why needed here: Understanding how FCI handles latent confounders and outputs partial ancestral graphs is crucial for interpreting COAT's results
  - Quick check question: What is the key difference between FCI and PC algorithm in terms of handling latent confounders?

- Concept: Large Language Model prompting techniques
  - Why needed here: Effective prompting is essential for both factor proposal and data annotation stages of COAT
  - Quick check question: What are the key components of an effective prompt for LLM-based factor proposal?

## Architecture Onboarding

- Component map:
  - LLM factor proposer (Ψ) → LLM data annotator (Ψs) → Causal discovery algorithm (A) → Feedback constructor (F) → LLM factor proposer (next iteration)
  - External tools/resources for data collection (optional)

- Critical path: Factor proposal → Data annotation → Causal discovery → Feedback construction → Next iteration factor proposal

- Design tradeoffs:
  - Using two different LLMs vs. same LLM for both proposal and annotation
  - Number of iterations vs. computational cost
  - Granularity of feedback vs. LLM prompt complexity

- Failure signatures:
  - Factor proposal stage: LLM repeatedly proposes irrelevant factors or fails to propose new factors in later iterations
  - Data annotation stage: Annotated data shows poor correlation with ground truth or introduces confounding
  - Causal discovery stage: Algorithm fails to converge or produces inconsistent results across iterations

- First 3 experiments:
  1. Single iteration COAT on AppleGastronome with GPT-4 to verify basic factor proposal and annotation capabilities
  2. Multi-iteration COAT on AppleGastronome comparing GPT-3.5 vs GPT-4 to assess feedback effectiveness
  3. COAT with external data collection (instead of LLM annotation) on Neuropathic benchmark to validate alternative parsing strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimal number of unstructured examples required for LLMs to reliably propose useful causal factors?
- Basis in paper: [inferred] The paper mentions that "Usually, k is a relatively small number (i.e., k ≤ n) due to the limited context understanding capability of LLMs," but does not specify a precise threshold.
- Why unresolved: The paper only provides qualitative observations about LLM capabilities without quantifying the sample efficiency.
- What evidence would resolve it: Controlled experiments varying the number of examples while measuring factor proposal quality across different LLM models.

### Open Question 2
- Question: How does the performance of COAT scale with increasing complexity of the underlying causal structure?
- Basis in paper: [explicit] The authors note that "Tu et al. (2023) find that the performance of LLMs in more complex causal discovery remains limited" and that COAT aims to address this limitation.
- Why unresolved: The paper only tests COAT on relatively simple causal structures (AppleGastronome and Neuropathic), leaving open whether it maintains effectiveness on more complex graphs.
- What evidence would resolve it: Experiments on synthetic benchmarks with varying graph complexity, measured by number of nodes, density, and presence of long-range dependencies.

### Open Question 3
- Question: What is the impact of LLM annotation noise on the reliability of causal discovery results?
- Basis in paper: [explicit] The authors conduct independence tests showing that "the introduced noises are independent of the attributes," but do not quantify the impact on causal discovery accuracy.
- Why unresolved: While independence is established, the relationship between noise levels and discovery performance remains unclear.
- What evidence would resolve it: Systematic experiments varying the annotation noise level and measuring downstream causal discovery performance metrics.

## Limitations

- The framework's performance depends heavily on LLM quality and may not generalize well to domains where LLMs lack sufficient pre-trained knowledge
- Theoretical guarantees for Markov Blanket recovery may not hold in practice due to real-world data complexity and required assumptions
- The iterative refinement process assumes causal discovery algorithms can effectively identify gaps through prediction errors, which may not always be reliable

## Confidence

- **High Confidence**: The framework's basic architecture and iterative refinement process are well-defined and logically sound. The empirical results on synthetic benchmarks demonstrate the feasibility of using LLMs for causal factor discovery from unstructured data.
- **Medium Confidence**: The claim that LLMs can effectively propose high-level causal factors from unstructured data when guided by proper prompts is supported by the results, but the specific prompting strategies and their impact on performance are not fully explored.
- **Medium Confidence**: The assertion that causal discovery algorithms can provide reliable feedback to improve LLM factor proposals is supported by the experimental results, but the generalizability of this approach to other domains and causal discovery algorithms is uncertain.
- **Low Confidence**: The theoretical guarantees for Markov Blanket recovery may not hold in practice due to the complexity of real-world data and the assumptions required for their derivation.

## Next Checks

1. **Cross-Domain Validation**: Test COAT on a diverse set of real-world datasets from different domains to assess its generalizability and robustness to varying data characteristics and causal structures.

2. **Alternative Causal Discovery Algorithms**: Evaluate the performance of COAT when using different causal discovery algorithms (e.g., PC, MMHC) to determine the impact of algorithm choice on factor proposal and refinement.

3. **Ablation Studies on Prompting Strategies**: Conduct systematic experiments to analyze the impact of different prompting strategies on LLM performance in factor proposal and annotation, identifying optimal approaches for various types of unstructured data.