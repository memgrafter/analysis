---
ver: rpa2
title: Towards Case-based Interpretability for Medical Federated Learning
arxiv_id: '2408.13626'
source_url: https://arxiv.org/abs/2408.13626
tags:
- federated
- learning
- data
- case-based
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates case-based interpretability for federated
  learning in medical imaging, focusing on pleural effusion diagnosis from chest X-rays.
  The authors propose using a deep generative model (Medfusion) to create synthetic
  case-based explanations in a privacy-preserving federated learning setting where
  past data is inaccessible.
---

# Towards Case-based Interpretability for Medical Federated Learning

## Quick Facts
- arXiv ID: 2408.13626
- Source URL: https://arxiv.org/abs/2408.13626
- Authors: Laura Latorre; Liliana Petrychenko; Regina Beets-Tan; Taisiya Kopytova; Wilson Silva
- Reference count: 19
- Primary result: Federated model achieves F1-scores of 0.716 (in-distribution) and 0.781 (out-of-distribution) with synthetic case-based explanations

## Executive Summary
This paper proposes a case-based interpretability framework for federated learning in medical imaging, specifically for pleural effusion diagnosis from chest X-rays. The approach uses a federated discriminative model (DenseNet-121) trained with differential privacy across multiple hospital datasets, while each client maintains a local generative model (Medfusion) to create synthetic case-based explanations. When classifying a new case, the system retrieves similar synthetic cases from the federated clients as explanations. The method achieves competitive F1-scores (0.716 in-distribution, 0.781 out-of-distribution) while providing interpretable case-based explanations that a radiologist qualitatively validated as realistic and comparable to real X-rays.

## Method Summary
The method employs a federated learning architecture where a global discriminative model (DenseNet-121) is trained across three hospital clients using DP-SGD and DP-Adam with differential privacy. Each client also trains a local Medfusion generative model on their data to create synthetic chest X-rays. For case-based interpretability, when a new case is classified, the system computes normalized Euclidean distance in the feature space of the second-to-last layer between the test case and all synthetic cases, retrieving the top-3 most similar synthetic cases from each client as explanations. The approach is evaluated on four public chest X-ray datasets (CheXpert, MIMIC-CXR-JPG, BRAX, VinDr-CXR) for pleural effusion classification.

## Key Results
- Federated discriminative model achieves F1-scores of 0.716 (in-distribution) and 0.781 (out-of-distribution)
- Retrieval performance (nDCG) of the proposed method outperforms SSIM baseline across all test cases (0.921 vs 0.588 for test image 1)
- Radiologist qualitatively evaluated synthetic cases as realistic and comparable to real X-rays
- Model's ranking better matches radiologist judgment than SSIM baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The federated discriminative model achieves competitive F1-scores despite privacy constraints.
- Mechanism: Federated learning with DP-SGD/DP-Adam allows training across multiple hospital datasets without sharing raw data, while the generative model compensates for lack of accessible past cases.
- Core assumption: Synthetic cases generated by the generative model are sufficiently realistic and clinically relevant.
- Evidence anchors: Abstract mentions deep generative model for synthetic examples that protect privacy and explain decisions; results show F1-scores of 0.716 (in-distribution) and 0.781 (out-of-distribution).

### Mechanism 2
- Claim: The retrieval system using normalized Euclidean distance in feature space provides effective case-based explanations.
- Mechanism: By computing distances between test cases and synthetic cases in the latent feature space of the trained discriminative model, the system retrieves the most semantically similar cases for explanation purposes.
- Core assumption: The feature space of the discriminative model captures clinically meaningful similarities between cases.
- Evidence anchors: Section states similarity is computed based on normalized Euclidean distance in the feature space of the previous to the last layer; ranking provided by the model mimics better the ranking of a radiologist than the SSIM.

### Mechanism 3
- Claim: The radiologist qualitatively evaluated the synthetic cases as realistic and comparable to real X-rays, validating the quality of the generated explanations.
- Mechanism: The generative model successfully creates synthetic chest X-rays that a radiologist cannot distinguish from real images, indicating high visual quality and clinical relevance.
- Core assumption: Radiologists can reliably assess the quality and clinical relevance of synthetic cases.
- Evidence anchors: Section states a radiologist qualitatively evaluated the synthetic cases as realistic and comparable to real X-rays; none of the samples was considered unreal or generated; the radiologist didn't realize the assessment was on AI generated images.

## Foundational Learning

- Concept: Federated learning fundamentals (client-server architecture, federated averaging, differential privacy)
  - Why needed here: Understanding how the model is trained across multiple hospitals without sharing raw data is crucial for grasping the privacy-preserving aspect
  - Quick check question: How does federated averaging differ from traditional centralized training?

- Concept: Generative models for medical imaging (diffusion models, synthetic data generation)
  - Why needed here: The Medfusion model is central to creating the case-based explanations when past data is inaccessible
  - Quick check question: What makes diffusion models particularly suitable for generating realistic medical images?

- Concept: Case-based reasoning and interpretability in medical AI
  - Why needed here: The core contribution is using synthetic cases as explanations, which requires understanding both the reasoning approach and interpretability requirements in healthcare
  - Quick check question: Why is case-based reasoning particularly natural for radiology compared to other interpretability methods?

## Architecture Onboarding

- Component map: Client nodes (local discriminative models, local generative models) -> Server node (federated averaging aggregator, global discriminative model) -> Synthetic case database (generated cases from all clients) -> Retrieval engine (distance computation in feature space) -> Evaluation components (F1-score calculation, nDCG for retrieval, radiologist assessment)

- Critical path: New case → Feature extraction → Distance computation → Top-3 synthetic case retrieval from each client → Display to clinician

- Design tradeoffs: Privacy vs performance (DP reduces F1 by ~10-15%), realism vs diversity in synthetic cases, computational cost of generative models vs benefit of explanations

- Failure signatures: Low F1 scores despite training, synthetic cases rejected by radiologists, retrieval nDCG scores close to random baseline

- First 3 experiments:
  1. Train the federated discriminative model on synthetic data only to test if the generative model alone can produce usable explanations
  2. Replace the generative model with a simpler GAN to compare explanation quality and computational efficiency
  3. Test the retrieval system with ground truth labels to establish an upper bound for nDCG performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we ensure that synthetic case-based explanations generated by diffusion models do not leak sensitive patient identity information while maintaining medical utility?
- Basis in paper: Future research should focus on exploring privacy-preserving generative models to guarantee that no identity information present in the client's data is leaked in the generated images.
- Why unresolved: The paper acknowledges this as a critical future research direction but does not propose or evaluate any specific methods for preventing identity leakage in synthetic medical images.
- What evidence would resolve it: Implementation and evaluation of differential privacy techniques or adversarial identity loss functions applied to the generative model, followed by empirical testing to verify that identity cannot be reconstructed while maintaining diagnostic relevance.

### Open Question 2
- Question: What is the optimal balance between privacy guarantees (via differential privacy) and model performance in federated medical imaging, and how does this trade-off vary across different medical conditions?
- Basis in paper: As expected, due to the distinct client distributions and inclusion of differential privacy, the federated learning model led to slightly worse results (Table I). and the use of DP-SGD and DP-Adam.
- Why unresolved: The paper demonstrates performance degradation due to privacy constraints but does not systematically explore how different privacy budgets affect both privacy guarantees and classification performance across various medical conditions.
- What evidence would resolve it: Systematic experiments varying differential privacy parameters (epsilon values) and measuring both privacy leakage (using membership inference attacks) and model performance across multiple medical conditions.

### Open Question 3
- Question: How can the quality and utility of synthetic case-based explanations be quantitatively evaluated beyond radiologist qualitative assessment?
- Basis in paper: The radiologist was asked to label all the images, the queries and the case-based explanations, and assess their quality (or realism) for a proper pleural effusion diagnosis.
- Why unresolved: The paper relies solely on qualitative radiologist assessment for evaluating synthetic cases, without proposing quantitative metrics for measuring the realism, diagnostic utility, or similarity to real cases.
- What evidence would resolve it: Development and validation of quantitative metrics for synthetic medical image quality assessment that correlate with radiologist judgment and can be used to optimize generative model parameters.

## Limitations

- The radiologist validation was qualitative and subjective, with no quantitative metrics for synthetic case realism
- The privacy-utility tradeoff is demonstrated but the specific DP hyperparameters are not fully specified, making exact replication difficult
- The generalizability of the approach to other medical imaging tasks or disease types remains unproven

## Confidence

- **High confidence**: The federated learning with DP-SGD achieves the reported F1-scores (0.716 in-distribution, 0.781 out-of-distribution)
- **Medium confidence**: The retrieval system outperforms SSIM baseline based on nDCG metrics
- **Low confidence**: The qualitative radiologist assessment of synthetic case realism without quantitative metrics

## Next Checks

1. **Quantitative radiologist assessment**: Conduct a blinded study with multiple radiologists scoring synthetic cases on realism, relevance, and diagnostic value using standardized scales

2. **Hyperparameter sensitivity analysis**: Systematically vary DP noise multipliers and clipping norms to map the privacy-utility tradeoff curve

3. **Cross-task validation**: Apply the approach to a different medical imaging task (e.g., pneumonia detection) to test generalizability of the case-based interpretability framework