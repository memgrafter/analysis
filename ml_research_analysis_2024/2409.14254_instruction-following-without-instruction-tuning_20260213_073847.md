---
ver: rpa2
title: Instruction Following without Instruction Tuning
arxiv_id: '2409.14254'
source_url: https://arxiv.org/abs/2409.14254
tags:
- instruction
- tuning
- response
- language
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that language models can exhibit instruction-following
  behavior without explicit instruction tuning. Two key findings show that (1) training
  solely on responses without instructions, and (2) finetuning on narrow-domain tasks
  like poetry or code generation, both lead to broad instruction-following capabilities.
---

# Instruction Following without Instruction Tuning

## Quick Facts
- arXiv ID: 2409.14254
- Source URL: https://arxiv.org/abs/2409.14254
- Authors: John Hewitt; Nelson F. Liu; Percy Liang; Christopher D. Manning
- Reference count: 40
- Primary result: Response-tuned models achieve 43.3% win rate against instruction-tuned models in AlpacaEval

## Executive Summary
This paper challenges the conventional wisdom that instruction tuning is necessary for models to exhibit instruction-following behavior. Through a series of experiments, the authors demonstrate that language models can develop robust instruction-following capabilities through alternative adaptation methods that don't explicitly teach instruction-response mappings. The findings suggest that pretrained models already contain latent instruction-response mappings that can be revealed through various adaptation techniques, including response-only training, single-task finetuning on narrow domains, and even simple rule-based probability adjustments.

## Method Summary
The paper explores three adaptation methods applied to pretrained language models: response tuning (training only on responses without instructions), single-task finetuning on narrow domains like poetry or code generation, and a hand-written rule-based adapter that modifies token probabilities through three simple rules. These methods are evaluated using AlpacaEval's LLM-as-a-judge framework, comparing their instruction-following performance against traditional instruction-tuned models. The response tuning approach modifies the training objective to only predict responses, while single-task finetuning applies standard finetuning on domain-specific datasets. The rule-based adapter implements three probability adjustments - increasing end-of-sequence probability, penalizing token repetition, and uniformly adjusting probabilities for 15 tokens - which are combined with the base model using a product-of-experts approach.

## Key Results
- Response-tuned Llama-2-7B models achieve 43.3% win rate against instruction-tuned models in AlpacaEval
- Single-task finetuning on narrow domains (poetry, code generation) produces general instruction-following behavior
- A simple rule-based adapter with three rules achieves 24.4% win rate against instruction-tuned models
- Models finetuned on single tasks like GSM8k still follow instructions dissimilar from the finetuning domain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretrained models already encode instruction-response mappings but responses are too low-probability to be generated without adaptation.
- Mechanism: During pretraining, the model implicitly learns the relationship between instruction-like text and appropriate responses, but this mapping remains latent because the probability mass is insufficient. Adaptation methods like response tuning increase the probability of desirable responses, making the latent instruction-following behavior emerge.
- Core assumption: The pretraining data contained sufficient examples of instruction-like text paired with appropriate responses for the model to learn the mapping, even if not explicitly labeled.
- Evidence anchors:
  - [abstract] "This suggests pretrained models have an instruction-response mapping which is revealed by teaching the model the desired distribution of responses."
  - [section 4.2] "One possibility is that base models can rank a desired response for an instruction higher than a desirable response for another instructions, but scores a string that is not a desired response at all higher than both."
  - [corpus] Weak evidence - no direct citations found about pretrained models containing instruction-response mappings, though this is a common assumption in the field.
- Break condition: If pretraining data did not contain examples of instruction-like text paired with responses, or if the mapping learned during pretraining was fundamentally different from what's needed for instruction following.

### Mechanism 2
- Claim: Simple changes to the probability distribution of a pretrained model can cause instruction-following behavior.
- Mechanism: The difference between a pretrained model's distribution and an instruction-following distribution is relatively simple in token space. Small multiplicative changes to token probabilities (like ending sequences sooner, penalizing repetition, or uniformly adjusting certain tokens) can shift the model's behavior from continuation to instruction following.
- Core assumption: The space of token decisions that need to change to go from continuation to instruction following is sparse and localized.
- Evidence anchors:
  - [abstract] "We support this by hand-writing a rule-based language model which yields instruction following in a product-of-experts with a pretrained model."
  - [section 6.1] "We provide direct evidence for the simplicity of a mapping from pretrained to instruction-following distribution by hand-writing a rule-based mapping with three rules."
  - [corpus] Weak evidence - no direct citations found about simple distribution changes causing instruction following, though this is supported by the empirical results.
- Break condition: If the distribution changes needed for instruction following were complex and distributed across many tokens, making simple rules insufficient.

### Mechanism 3
- Claim: Single-task finetuning implicitly teaches the general instruction-following behavior because instructions that are dissimilar from the finetuning domain don't trigger the task-specific constraints.
- Mechanism: When a model is finetuned on a narrow task like poetry generation, it learns task-specific constraints (like formatting). However, when given instructions dissimilar from the finetuning domain, these constraints don't apply, allowing the base model's instruction-following capability to emerge. The model effectively "breaks" from the narrow task behavior when the input doesn't match the expected pattern.
- Core assumption: The task-specific constraints learned during single-task finetuning are only triggered by inputs similar to the finetuning instructions.
- Evidence anchors:
  - [abstract] "In particular, when instructions are very different from those in the narrow finetuning domain, models' responses do not adhere to the style of the finetuning domain."
  - [section 5.2] "We study how similarity of an instruction to the GSM instructions relates to the similarity of the model response to GSM responses versus general responses, expecting that at least the math-like instructions lead to GSM-like responses from the model."
  - [corpus] Weak evidence - no direct citations found about task-specific constraints being bypassed for dissimilar instructions, though this is supported by the empirical results.
- Break condition: If the task-specific constraints learned during single-task finetuning were applied universally regardless of instruction similarity, preventing general instruction following from emerging.

## Foundational Learning

- Concept: Cross-entropy loss minimization during pretraining
  - Why needed here: Understanding how language models learn probability distributions over text sequences is fundamental to grasping why adaptation methods can shift behavior.
  - Quick check question: What does minimizing cross-entropy loss during pretraining accomplish in terms of the model's learned distribution?

- Concept: Product-of-experts for combining distributions
  - Why needed here: The rule-based adapter uses a product-of-experts approach to combine the base model with rule-based adjustments, which is key to understanding how simple rules can influence behavior.
  - Quick check question: How does multiplying probability distributions (product-of-experts) differ from averaging them, and why is this important for the rule-based adapter?

- Concept: Greedy decoding and its limitations
  - Why needed here: The paper uses greedy decoding to observe when instruction-following responses are locally most likely, which is important for understanding the evaluation methodology.
  - Quick check question: What is greedy decoding, and how might it differ from sampling-based decoding in terms of which responses are generated?

## Architecture Onboarding

- Component map: Pretrained language model (Llama-2-7B or OLMo-7B) -> Adaptation method (response tuning, single-task finetuning, or rule-based adapter) -> Evaluation using AlpacaEval LLM-as-a-judge framework
- Critical path: For implementing a new adaptation method, the critical path is: (1) prepare adaptation data, (2) implement finetuning loop with appropriate loss function, (3) run hyperparameter search, (4) evaluate using AlpacaEval against a reference instruction-tuned model.
- Design tradeoffs: Response tuning vs. instruction tuning trades explicit instruction-response mapping learning for simplicity and reduced data requirements. Single-task finetuning trades task-specific performance for potential emergence of general instruction following. The rule-based adapter trades learnable parameters for interpretability and control but requires manual rule design.
- Failure signatures: If an adaptation method produces win rates close to the base model's 2-4%, it's not inducing instruction-following behavior. If win rates are high but responses are nonsensical, the model may be gaming the evaluation. If win rates are high but only for specific instruction types, the adaptation may be too narrow.
- First 3 experiments:
  1. Implement response tuning on a small subset of data and verify it achieves higher win rates than the base model.
  2. Implement single-task finetuning on one of the five datasets and verify it produces general instruction following.
  3. Implement the three-rule adapter and verify each rule contributes to instruction following when ablated.

## Open Questions the Paper Calls Out

I. Introduction
1. Are there fundamental differences between how implicit and explicit instruction tuning lead to instruction-following behavior?
   - Basis in paper: [inferred] The paper shows that response tuning and single-task finetuning both lead to instruction following without explicit instruction tuning.
   - Why unresolved: The paper doesn't provide a mechanistic explanation for why these different approaches converge on similar behavior.
   - What evidence would resolve it: Comparative analysis of internal representations and attention patterns between implicitly and explicitly instruction-tuned models.

2. What specific aspects of pretraining data make models susceptible to implicit instruction tuning?
   - Basis in paper: [explicit] The paper notes uncertainty about whether pretraining included instruction-like data.
   - Why unresolved: The paper doesn't analyze the pretraining data distribution or its relationship to instruction-following capabilities.
   - What evidence would resolve it: Detailed analysis of pretraining corpora for instruction-like patterns and their correlation with implicit instruction-following success.

3. How does the distribution of instruction-response pairs in LIMA specifically enable response tuning to work?
   - Basis in paper: [explicit] The paper uses LIMA dataset for response tuning experiments.
   - Why unresolved: The paper doesn't characterize what makes LIMA particularly suitable for response tuning.
   - What evidence would resolve it: Statistical analysis of LIMA's instruction-response pairs to identify patterns that enable implicit instruction tuning.

4. Is there a minimal set of rules needed for the rule-based adapter to achieve instruction following?
   - Basis in paper: [explicit] The paper uses three rules in their adapter.
   - Why unresolved: The paper doesn't explore whether fewer rules could achieve similar results.
   - What evidence would resolve it: Systematic ablation studies removing individual rules to find the minimal effective set.

5. How do different instruction formats (semantic vs. non-semantic tags) affect implicit instruction tuning?
   - Basis in paper: [explicit] The paper tests A/B tags vs. user/assistant tags.
   - Why unresolved: The paper only tests one alternative format.
   - What evidence would resolve it: Systematic testing of various formatting schemes to identify which structural elements matter.

II. Methods
6. Can implicit instruction tuning be combined with explicit instruction tuning for improved performance?
   - Basis in paper: [inferred] The paper focuses on implicit methods but doesn't explore combinations.
   - Why unresolved: No experiments combining both approaches are presented.
   - What evidence would resolve it: Direct comparison of models trained with both methods versus either method alone.

7. How does the effectiveness of implicit instruction tuning scale with model size?
   - Basis in paper: [explicit] Experiments use Llama-2-7B and OLMo-7B models.
   - Why unresolved: No analysis of how results vary across different model scales.
   - What evidence would resolve it: Experiments across multiple model sizes from small to very large.

8. What is the relationship between response ranking capability and instruction-following performance?
   - Basis in paper: [explicit] The paper introduces and measures response ranking capability.
   - Why unresolved: No analysis of how this capability correlates with actual instruction-following success.
   - What evidence would resolve it: Correlation analysis between response ranking scores and downstream instruction-following metrics.

9. How does single-task finetuning on different domains affect instruction-following generalization?
   - Basis in paper: [explicit] Experiments use five different single-task datasets.
   - Why unresolved: The paper doesn't systematically compare generalization across domains.
   - What evidence would resolve it: Cross-domain evaluation measuring how well models finetuned on one task follow instructions for other tasks.

10. What is the optimal trade-off between rule complexity and performance in the rule-based adapter?
    - Basis in paper: [explicit] The paper uses three specific rules with hand-tuned weights.
    - Why unresolved: No exploration of alternative rule sets or weight optimization.
    - What evidence would resolve it: Systematic exploration of different rule combinations and weight configurations.

III. Results
11. How does the rule-based adapter's performance compare to more sophisticated adapter methods?
    - Basis in paper: [explicit] The paper presents rule-based adapter results.
    - Why unresolved: No comparison to other adapter architectures like LoRA or prefix tuning.
    - What evidence would resolve it: Direct comparison of rule-based adapter against established adapter methods.

12. What causes the dramatic difference in Chess dataset performance compared to other single-task datasets?
    - Basis in paper: [explicit] Chess finetuning shows very different behavior than other datasets.
    - Why unresolved: The paper speculates but doesn't systematically investigate the cause.
    - What evidence would resolve it: Analysis of Chess dataset properties (entropy, structure, etc.) compared to other datasets.

13. How does instruction similarity to finetuning data affect response quality across different model architectures?
    - Basis in paper: [explicit] The paper shows instruction similarity affects GSM-finetuned model responses.
    - Why unresolved: No comparison across different model families or architectures.
    - What evidence would resolve it: Cross-architecture evaluation measuring how instruction similarity affects different model types.

14. What is the relationship between the number of training epochs and implicit instruction-tuning effectiveness?
    - Basis in paper: [explicit] The paper sweeps over different epoch counts.
    - Why unresolved: No analysis of optimal epoch count or learning curves.
    - What evidence would resolve it: Detailed learning curve analysis showing performance vs. training duration.

15. How do different base model families compare in their susceptibility to implicit instruction tuning?
    - Basis in paper: [explicit] Experiments use Llama-2 and OLMo models.
    - Why unresolved: Only two model families are tested.
    - What evidence would resolve it: Experiments across diverse model families (GPT, Claude, Mistral) to identify architectural factors.

IV. Discussion
16. What are the safety implications of implicit instruction tuning for deployed models?
    - Basis in paper: [explicit] The paper discusses practical consequences for model deployment.
    - Why unresolved: No systematic safety evaluation is presented.
    - What evidence would resolve it: Comprehensive safety testing of implicitly instruction-tuned models across various scenarios.

17. How can we predict when a finetuning task will implicitly instruction-tune a model?
    - Basis in paper: [inferred] The paper shows this happens unexpectedly across various tasks.
    - Why unresolved: No predictive framework is provided.
    - What evidence would resolve it: Development of metrics or indicators that predict implicit instruction-tuning potential.

18. What role does memorization play in the success of implicit instruction tuning?
    - Basis in paper: [inferred] The paper doesn't analyze memorization effects.
    - Why unresolved: No distinction between learned behaviors and memorized patterns.
    - What evidence would resolve it: Analysis separating model behaviors that stem from memorization versus generalization.

19. How does implicit instruction tuning affect a model's ability to follow novel instructions?
    - Basis in paper: [explicit] The paper evaluates on various instruction types.
    - Why unresolved: No systematic testing of truly novel instructions.
    - What evidence would resolve it: Evaluation on instructions designed to be unlike any in pretraining or finetuning data.

20. What is the theoretical explanation for why such simple rule changes can induce instruction following?
    - Basis in paper: [explicit] The paper presents simple rules that work surprisingly well.
    - Why unresolved: No theoretical framework explains this phenomenon.
    - What evidence would resolve it: Mathematical or theoretical analysis explaining the relationship between simple distribution changes and complex behaviors.

## Limitations

- The paper cannot definitively prove that pretrained models contain explicit instruction-response mappings, only that adaptation methods can induce instruction-following behavior
- The evaluation methodology using LLM-as-a-judge may introduce biases and doesn't provide direct human evaluation of instruction-following quality
- The findings are limited to Llama-2-7B and OLMo-7B models, leaving open questions about generalizability to other architectures and scales

## Confidence

**High Confidence**: The empirical observation that response-tuned models achieve 43.3% win rate against instruction-tuned models in AlpacaEval. This is a direct experimental result with clear methodology and reproducible evaluation.

**Medium Confidence**: The claim that pretrained models encode latent instruction-response mappings that can be revealed through adaptation. While the evidence is suggestive and mechanistically plausible, alternative explanations exist and cannot be ruled out definitively.

**Low Confidence**: The specific mechanisms proposed for how pretraining leads to instruction-response mappings, particularly Mechanism 1's claim about explicit encoding of instruction-response pairs during pretraining. This remains speculative without direct evidence from the pretraining corpus or model internals.

## Next Checks

1. **Ablation Study on Pretraining Data**: Conduct experiments where models are pretrained on corpora with systematically varied instruction-like content presence. Compare the instruction-following emergence of models trained with high vs. low instruction-response pair density to directly test whether pretraining data composition affects the ability to reveal latent instruction following through adaptation.

2. **Controlled Rule-Based Adapter Experiments**: Systematically test rule-based adapters with varying numbers and types of rules on the same base models. Quantify the relationship between rule complexity and instruction-following performance to determine whether the simplicity of the three-rule system is essential or whether more complex rules yield proportionally better results.

3. **Cross-Architecture Generalization Study**: Replicate the response tuning experiments across multiple model families (GPT, Claude, Mistral) and sizes (7B, 13B, 70B). Analyze whether the 43.3% win rate is consistent across architectures or whether certain model types are more amenable to implicit instruction following emergence.