---
ver: rpa2
title: Graph Neural Thompson Sampling
arxiv_id: '2406.10686'
source_url: https://arxiv.org/abs/2406.10686
tags:
- graph
- lemma
- have
- neural
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of online decision-making with graph-structured
  data by formulating it as a graph action bandit problem. The proposed GNN-TS algorithm
  combines Thompson Sampling with Graph Neural Networks for reward estimation and
  uncertainty quantification.
---

# Graph Neural Thompson Sampling

## Quick Facts
- arXiv ID: 2406.10686
- Source URL: https://arxiv.org/abs/2406.10686
- Authors: Shuang Wu; Arash A. Amini
- Reference count: 40
- This paper studies the problem of online decision-making with graph-structured data by formulating it as a graph action bandit problem.

## Executive Summary
This paper introduces GNN-TS, a novel algorithm that combines Thompson Sampling with Graph Neural Networks for online decision-making in graph-structured action spaces. The method achieves sublinear regret bounds that are independent of the number of graph nodes, scaling instead with an effective dimension that captures the complexity of the reward function. Through extensive experiments on synthetic graph data, GNN-TS demonstrates competitive performance against baseline algorithms across various graph structures and reward functions.

## Method Summary
The GNN-TS algorithm formulates online decision-making with graph-structured data as a graph action bandit problem, where the learner sequentially selects graphs from a finite action space and observes noisy rewards. The method uses a Graph Neural Network to approximate the reward function and estimates uncertainty using the graph neural tangent kernel. Thompson Sampling is adapted for graph exploration, with the algorithm sampling from normal distributions whose means are current GNN estimates and variances reflect uncertainty. The algorithm achieves a regret bound of $\tilde{O}((\tilde{d}T)^{1/2})$ that is sublinear in rounds T and independent of graph node count.

## Key Results
- GNN-TS achieves sublinear regret bound $\tilde{O}((\tilde{d}T)^{1/2})$ independent of graph node count
- Effective dimension $\tilde{d}$ captures reward function complexity and scales well with large graphs
- Empirical results show GNN-TS outperforms baseline algorithms across various graph structures and reward functions

## Why This Works (Mechanism)

### Mechanism 1
Thompson Sampling combined with Graph Neural Networks achieves sublinear regret in graph action bandit problems. The algorithm approximates the reward function using a GNN and estimates uncertainty using the graph neural tangent kernel (GNTK). This combination allows effective exploration and exploitation in graph-structured action spaces. Core assumption: The reward function lies within the RKHS corresponding to the GNTK, and the GNN model can approximate this function well. Evidence: [abstract] "GNN-TS achieves a regret bound of $\tilde{O}((\tilde{d}T)^{1/2})$ that is sublinear in the number of rounds T and independent of the number of graph nodes."

### Mechanism 2
The effective dimension $\tilde{d}$ captures the complexity of the reward function and is independent of the number of graph nodes. The effective dimension is defined based on the GNTK matrix and regularization, providing a measure of the underlying reward function space complexity that scales well with large graphs. Core assumption: The effective dimension is a robust measure of the actual underlying dimension of the reward function space as seen by the bandit problem. Evidence: [section 4] "We define the effective dimension $\tilde{d}$ of the GNTK matrix K with regularization λ as $\tilde{d} := \log \det(I_{|G|} + T K/\lambda) / \log(1 + T \rho_{max}/\lambda)$."

### Mechanism 3
The exploration-exploitation tradeoff is managed through the uncertainty estimation and sampling process in GNN-TS. The algorithm samples from a normal distribution with mean as the current GNN estimate and variance based on the uncertainty estimate. This allows for both exploration (through randomness) and exploitation (through the estimated mean). Core assumption: The uncertainty estimate accurately reflects the true uncertainty in the reward function approximation. Evidence: [section 3.3] "The variance of the normal distribution $\nu^2\sigma_t^2(G)$ is our current measure of uncertainty about the true reward of graph G."

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GNNs are used to approximate the reward function over graph-structured data, which is the core of the GNN-TS algorithm.
  - Quick check question: Can you explain how a single-layer graph convolution works in a GNN?

- Concept: Thompson Sampling
  - Why needed here: Thompson Sampling is the exploration strategy adapted for graph action bandit problems in the GNN-TS algorithm.
  - Quick check question: What is the key difference between Thompson Sampling and Upper Confidence Bound (UCB) approaches in bandit problems?

- Concept: Neural Tangent Kernel (NTK) and Graph Neural Tangent Kernel (GNTK)
  - Why needed here: The GNTK is used for uncertainty estimation in the GNN-TS algorithm, providing a connection between the GNN and kernel methods.
  - Quick check question: How does the Neural Tangent Kernel relate to the training dynamics of infinitely wide neural networks?

## Architecture Onboarding

- Component map: Graph Neural Network -> Graph Neural Tangent Kernel -> Thompson Sampling -> Optimization Loop
- Critical path:
  1. Initialize GNN parameters and uncertainty estimate
  2. For each round:
     a. Compute uncertainty estimates for all graphs
     b. Sample rewards from normal distributions
     c. Select graph with highest sampled reward
     d. Observe reward and update GNN parameters
     e. Update uncertainty estimate
- Design tradeoffs:
  - GNN architecture complexity vs. computational efficiency
  - Width of GNN (m) vs. approximation accuracy
  - Exploration scale (ν) vs. exploration-exploitation balance
  - Regularization strength (λ) vs. overfitting
- Failure signatures:
  - High regret: Possible issues with GNN model capacity or exploration-exploitation balance
  - Poor uncertainty estimation: Check GNTK approximation and matrix inversion stability
  - Slow convergence: Review learning rate and optimization process
- First 3 experiments:
  1. Implement and test GNN model on a simple graph property prediction task
  2. Verify uncertainty estimation using synthetic data with known reward functions
  3. Test GNN-TS on a small graph action bandit problem with a linear reward function

## Open Questions the Paper Calls Out

### Open Question 1
How does the regret bound change if we use a deeper GNN architecture with more than one graph convolution layer? Basis: The paper uses a relatively simple GNN architecture with a single graph convolution layer. It's unclear how the regret bound would change with more complex architectures. Why unresolved: The theoretical analysis in the paper is specific to the chosen GNN architecture. Extending it to deeper architectures would require new theoretical tools and analysis. What evidence would resolve it: Empirical experiments comparing regret bounds for different GNN depths, or theoretical extensions of the analysis to deeper architectures.

### Open Question 2
Can the GNN-TS algorithm be adapted to handle continuous action spaces, rather than discrete graphs? Basis: The paper formulates the problem as a graph action bandit with discrete graphs. It's unclear if and how the algorithm could be extended to continuous action spaces. Why unresolved: The theoretical analysis and algorithm are designed for discrete graphs. Extending to continuous spaces would require new mathematical tools and potentially different algorithms. What evidence would resolve it: A theoretical framework for continuous action spaces, or empirical results demonstrating the algorithm's performance on continuous graph optimization problems.

### Open Question 3
How does the performance of GNN-TS scale with the size of the graphs (number of nodes) and the graph density? Basis: The regret bound is independent of the number of graph nodes, but it's unclear how the practical performance scales with graph size and density. Why unresolved: The theoretical analysis focuses on the number of graphs rather than the size or density of individual graphs. Practical scalability with graph properties is not explicitly addressed. What evidence would resolve it: Empirical experiments varying graph size and density, and analyzing the algorithm's performance and computational requirements as these parameters change.

## Limitations
- Theoretical analysis focuses on sublinear regret bounds without explicit runtime complexity guarantees for large graphs
- Effective dimension behavior for complex reward functions remains theoretically unexplored
- Reliance on accurate GNTK approximations introduces potential numerical instability in matrix inversion operations

## Confidence
- Theoretical regret bounds: High - The sublinear regret analysis is mathematically rigorous and well-established within the bandit literature framework
- GNTK uncertainty estimation: Medium - While theoretically sound, practical implementation challenges and approximation errors may affect performance
- Experimental validation: Medium - Results demonstrate competitive performance but are limited to synthetic benchmarks with controlled conditions

## Next Checks
1. Test GNN-TS scalability by evaluating performance on graphs with N > 1000 nodes to verify theoretical independence from node count
2. Conduct ablation studies removing the GNTK uncertainty component to quantify its contribution to exploration-exploitation balance
3. Evaluate GNN-TS on real-world graph datasets with known reward structures to validate synthetic benchmark findings