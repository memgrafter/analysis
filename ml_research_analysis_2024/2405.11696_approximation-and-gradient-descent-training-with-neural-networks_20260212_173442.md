---
ver: rpa2
title: Approximation and Gradient Descent Training with Neural Networks
arxiv_id: '2405.11696'
source_url: https://arxiv.org/abs/2405.11696
tags:
- neural
- networks
- gradient
- lemma
- descent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of understanding the simultaneous
  approximation and optimization performance of neural networks. While existing literature
  provides approximation results for networks with hand-picked weights and optimization
  results for over-parametrized networks with zero training error, the current work
  aims to bridge this gap by analyzing the combined approximation and optimization
  errors.
---

# Approximation and Gradient Descent Training with Neural Networks

## Quick Facts
- arXiv ID: 2405.11696
- Source URL: https://arxiv.org/abs/2405.11696
- Authors: G. Welper
- Reference count: 40
- Primary result: Shows gradient descent on NTK-linearized loss achieves direct approximation bounds with exponentially decaying error for under-parametrized networks

## Executive Summary
This paper bridges the gap between approximation and optimization results for neural networks by analyzing the combined approximation and optimization errors during gradient descent training. The work establishes convergence results for both shallow networks in 1D and deep networks in multiple dimensions, showing that as long as the training has not achieved the direct approximation inequality, the error decays exponentially. The analysis relies on the neural tangent kernel (NTK) and its coercivity properties in Sobolev norms, providing a unified framework for understanding when gradient descent can provably achieve good approximation performance.

## Method Summary
The paper analyzes neural network training using gradient descent on a continuous L2 loss function, placing results in an under-parametrized regime. The method involves training shallow networks in 1D with fixed first-layer weights and trained biases, and deep networks in multiple dimensions with trained second-to-last layer weights. The analysis relies on NTK linearization, requiring verification of coercivity properties in Sobolev norms and perturbation bounds to show exponential error decay during training.

## Key Results
- Gradient descent achieves direct approximation bounds with exponentially decaying error when error exceeds final bound
- Shallow ReLU networks in 1D with untrained first-layer weights achieve improved rates (m^{-(1-s)/2}) for s < 1/2
- Deep fully connected networks with smooth activations achieve direct approximation bounds in multiple dimensions under NTK coercivity conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient descent on the NTK-linearized loss achieves direct approximation bounds for under-parametrized networks.
- Mechanism: The paper shows that as long as the current error exceeds the direct approximation bound, gradient descent reduces the L2 error exponentially because the NTK is coercive in Sobolev norms.
- Core assumption: The NTK linearization remains accurate and coercive in Sobolev norms throughout training, and weights stay close to their initialization.
- Evidence anchors:
  - [abstract]: "Recent work uses the smoothness that is required for approximation results to extend a neural tangent kernel (NTK) optimization argument to an under-parametrized regime"
  - [section 2.1]: "The key outcome is the establishment of a unified analysis of approximation and optimization errors, showing that gradient descent training can provably achieve good approximation performance"
  - [corpus]: Weak. Related papers focus on convergence and overfitting but not the specific NTK-coercivity mechanism in under-parametrized regimes.
- Break condition: If the NTK loses coercivity or if weights drift too far from initialization (violating the perturbation bounds), the exponential decay will fail.

### Mechanism 2
- Claim: Shallow ReLU networks in 1D with untrained first-layer weights can achieve lower approximation rates than piecewise linear theory predicts.
- Mechanism: The paper shows that for smoothness s < 1/2, the approximation error decays as m^{-(1-s)/2} rather than the expected m^{-s} for piecewise linear functions, due to NTK-based optimization achieving better rates than the structure of the network alone would suggest.
- Core assumption: The network uses fixed first-layer weights (a_r = ±1) and only trains the biases, creating a non-convex optimization problem that the NTK analysis can handle.
- Evidence anchors:
  - [section 2.1]: "Since gradient flow is a non-practical idealization of vanishing learning rate, the current paper shows comparable results for regular gradient descent"
  - [section 3]: "Both Theorems 2.1 and 2.2 are shown by providing the assumptions of the last theorem"
  - [corpus]: Weak. No corpus papers discuss the specific shallow network setup with untrained first-layer weights achieving improved rates.
- Break condition: If s ≥ 1/2 or if the network architecture is modified to train all weights, the improved rate may not hold.

### Mechanism 3
- Claim: Deep fully connected networks with smooth activations can achieve direct approximation bounds in multiple dimensions.
- Mechanism: The paper extends the shallow network results to deep networks by showing that the NTK remains coercive in Sobolev norms and that the forward process satisfies certain positivity conditions.
- Core assumption: The NTK is coercive in Sobolev norms (condition 9) and the forward process Σ_k(x,x) is uniformly bounded away from zero (condition 10).
- Evidence anchors:
  - [abstract]: "The primary result is that for sufficiently smooth target functions and appropriately chosen network architectures, gradient descent training can achieve direct approximation bounds"
  - [section 2.2]: "Unlike the shallow case, we need one more assumption on the (NTK) linearization of the networks that is currently known for non-smooth ReLU activations"
  - [corpus]: Weak. Related papers discuss NTK and optimization but not the specific coercivity conditions for deep networks with smooth activations.
- Break condition: If the NTK loses coercivity or if the forward process violates the positivity condition, the direct approximation bound will not be achieved.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its role in optimization
  - Why needed here: The paper's main results rely on analyzing the NTK and its coercivity properties to show exponential error decay during gradient descent training.
  - Quick check question: What is the NTK and how does it relate to the optimization of neural networks in the infinite-width limit?

- Concept: Sobolev spaces and their norms
  - Why needed here: The paper defines the smoothness of target functions and the error in terms of Sobolev norms, which are essential for stating the approximation and optimization results.
  - Quick check question: What are Sobolev spaces and how do they measure the smoothness of functions?

- Concept: Gradient descent and its convergence properties
  - Why needed here: The paper's main results are about the convergence of gradient descent on the NTK-linearized loss, showing that it achieves the direct approximation bound with an exponentially decaying error.
  - Quick check question: How does gradient descent work and what are its convergence properties for non-convex optimization problems?

## Architecture Onboarding

- Component map:
  - Shallow networks: 1D, ReLU activation, fixed first-layer weights (a_r = ±1), trained biases
  - Deep networks: Multiple dimensions, smooth activations, trained second-to-last layer weights, fixed other weights
  - NTK analysis: Coercivity in Sobolev norms, perturbation bounds, concentration inequalities

- Critical path:
  1. Initialize network weights randomly
  2. Compute NTK and verify coercivity conditions
  3. Train network using gradient descent on L2 loss
  4. Monitor error decay and verify it exceeds the direct approximation bound
  5. Stop training when the direct approximation bound is achieved

- Design tradeoffs:
  - Shallow vs. deep networks: Shallow networks are simpler but may have worse approximation rates, while deep networks can achieve better rates but require more complex NTK analysis
  - Smooth vs. non-smooth activations: Smooth activations allow for easier NTK analysis but may not be as expressive as non-smooth activations like ReLU
  - Trained vs. fixed weights: Training all weights may lead to better performance but makes the NTK analysis more difficult, while fixing some weights simplifies the analysis but may limit the achievable rates

- Failure signatures:
  - Error decay stops before reaching the direct approximation bound: This could indicate that the NTK has lost coercivity or that the perturbation bounds are violated
  - Error increases during training: This could indicate that the learning rate is too high or that the NTK linearization is no longer accurate
  - Error plateaus at a value higher than the direct approximation bound: This could indicate that the network architecture is not expressive enough or that the target function is not smooth enough

- First 3 experiments:
  1. Train a shallow network on a 1D smooth function and verify that the error decays exponentially until it reaches the direct approximation bound
  2. Train a deep network on a multi-dimensional smooth function and verify that the error decays exponentially until it reaches the direct approximation bound
  3. Compare the performance of shallow and deep networks on the same target function and verify that the deep network achieves better approximation rates when the NTK coercivity conditions are satisfied

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the perturbation assumption (15) in Theorem 3.1 be verified for the deep network setup without relying on numerical tests?
- Basis in paper: [inferred] The paper states that coercivity is known for ReLU activations but only tested numerically for smoother activations required in Theorem 2.2.
- Why unresolved: The proof of Theorem 2.2 relies on verifying the perturbation assumption (15), which is currently only supported by numerical tests rather than rigorous proofs.
- What evidence would resolve it: A rigorous mathematical proof that the perturbation assumption holds for the smooth activation functions used in the deep network setup.

### Open Question 2
- Question: Can the learning rate bounds in Theorems 2.1 and 2.2 be relaxed or improved?
- Basis in paper: [explicit] The theorems specify learning rates γ ≲ h√m for shallow networks and γ ≲ h√m with additional constraints for deep networks.
- Why unresolved: The learning rate bounds are derived from the proof technique and may not represent the optimal rates achievable by gradient descent.
- What evidence would resolve it: Experimental or theoretical results showing that gradient descent converges with larger learning rates than those specified in the theorems.

### Open Question 3
- Question: How does the approximation error rate in Theorem 2.1 compare to the optimal rate achievable by piecewise linear functions with m breakpoints?
- Basis in paper: [explicit] The paper mentions that the networks (with trained ar) are piecewise linear with m breakpoints, for which one would expect approximation errors with a higher rate than in Theorem 2.1.
- Why unresolved: The theorem provides a specific approximation error rate, but it is unclear how this rate compares to the optimal rate achievable by piecewise linear functions with the same number of breakpoints.
- What evidence would resolve it: A comparison of the approximation error rates achieved by the trained networks and piecewise linear functions with m breakpoints for various target functions.

## Limitations

- The results rely heavily on NTK coercivity assumptions that are verified for specific activation functions but may not hold universally
- The analysis assumes weights remain close to initialization during training, which may not be true for larger learning rates or longer training times
- The improved approximation rates for shallow networks (s < 1/2) are shown only for the specific architecture with fixed first-layer weights

## Confidence

- **High confidence**: The shallow network results in 1D (Theorem 2.1) are well-supported by the NTK analysis and concentration inequalities, with clear assumptions and error bounds.
- **Medium confidence**: The deep network results in multiple dimensions (Theorem 2.2) rely on more complex NTK coercivity assumptions that are currently only verified for smooth activations, making the results less robust.
- **Low confidence**: The improved approximation rates for shallow networks with s < 1/2 are shown only for a specific architecture and may not generalize to other network setups or training procedures.

## Next Checks

1. **Numerical verification of NTK coercivity**: Implement numerical tests to verify that the NTK remains coercive in Sobolev norms for various activation functions and network architectures during training.

2. **Empirical study of weight dynamics**: Track the distance between trained weights and their initialization during gradient descent to empirically validate the perturbation bounds assumed in the theoretical analysis.

3. **Robustness to learning rate scaling**: Test the training procedure with a range of learning rates to determine how sensitive the results are to the assumed learning rate scaling (γ ≲ h/√m) and identify any critical thresholds where the analysis breaks down.