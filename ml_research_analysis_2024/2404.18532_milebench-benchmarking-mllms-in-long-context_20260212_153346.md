---
ver: rpa2
title: 'MileBench: Benchmarking MLLMs in Long Context'
arxiv_id: '2404.18532'
source_url: https://arxiv.org/abs/2404.18532
tags:
- images
- image
- mllms
- tasks
- multi-image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MileBench, a benchmark designed to evaluate
  the multimodal long-context capabilities of MLLMs. The benchmark consists of two
  evaluation sets: diagnostic and realistic, which assess MLLMs'' long-context adaptation
  capacity and their ability to complete tasks in long-context scenarios.'
---

# MileBench: Benchmarking MLLMs in Long Context

## Quick Facts
- **arXiv ID**: 2404.18532
- **Source URL**: https://arxiv.org/abs/2404.18532
- **Reference count**: 40
- **Primary result**: Introduces MileBench benchmark with 6,440 multimodal long-context samples to evaluate MLLMs' performance on tasks involving multiple images and long text contexts

## Executive Summary
This paper introduces MileBench, a comprehensive benchmark designed to evaluate the multimodal long-context capabilities of Multimodal Large Language Models (MLLMs). The benchmark addresses a critical gap in existing evaluation frameworks by focusing on scenarios involving multiple images combined with long textual contexts, which are common in real-world applications. Through testing 22 models across diagnostic and realistic evaluation sets, the study reveals significant performance disparities between closed-source and open-source models, particularly as the number of images increases. The findings highlight the need for more focused research to enhance MLLMs' capabilities in complex, long-context scenarios.

## Method Summary
MileBench comprises 6,440 multimodal long-context samples with an average of 15.2 images and 422.3 words each. The benchmark includes two evaluation sets: diagnostic (to assess long-context adaptation capacity) and realistic (to evaluate task completion in long-context scenarios). The study employs zero-shot evaluation across 22 models, including closed-source, open-source image, and open-source video models. Performance is measured using task-specific metrics such as accuracy for multiple-choice tasks and ROUGE-L for generation tasks. The evaluation methodology includes strategies for handling inputs that exceed model context length limits, such as truncation from the left while preserving instructions.

## Key Results
- GPT-4o significantly outperforms other models in long-context multimodal scenarios
- Performance gaps between closed-source and open-source models widen with increasing image count
- Most open-source MLLMs struggle with multi-image tasks, likely due to insufficient training on multi-image data
- Models show negligible performance drop (0.1-1.2%) on adversarial sets, indicating minimal data contamination risk

## Why This Works (Mechanism)

### Mechanism 1
Open-source MLLMs suffer performance decline as the number of images increases due to insufficient training on multi-image data. The models are primarily trained on single-image datasets, limiting their ability to generalize to multi-image contexts. When faced with multiple images, they cannot effectively integrate information across frames.

### Mechanism 2
MLLMs exhibit "Lost in the Middle" phenomenon in multimodal contexts, struggling to retrieve information from the middle of long contexts. When processing long multimodal contexts, models may lose track of information in the middle sections, similar to text-only models experiencing this issue.

### Mechanism 3
Data contamination risk is minimal because models show negligible performance drop on adversarial sets. The evaluation set uses existing datasets, but models' consistent performance on adversarial variants suggests they weren't trained on the specific test data.

## Foundational Learning

- **Multimodal long-context processing**: Understanding how models handle multiple images combined with long text contexts is crucial for evaluating their real-world applicability.
  - Quick check: What is the average number of images and words per sample in MileBench?

- **Task-specific evaluation metrics**: Different tasks require different evaluation approaches - accuracy for multiple choice, ROUGE-L for generation tasks, etc.
  - Quick check: Which metric is used for the Visual Change Captioning task in MileBench?

- **Context truncation strategies**: When inputs exceed model capacity, understanding how to prioritize information (keeping instructions vs. truncating from left) is critical for fair evaluation.
  - Quick check: When input length exceeds maximum context length, which part of the sample is truncated?

## Architecture Onboarding

- **Component map**: Vision encoder -> Language model -> Multimodal fusion layer -> KV cache management -> Output generation layer
- **Critical path**: 1. Input preprocessing (image resizing, tokenization) 2. Multimodal encoding and fusion 3. Context window management 4. Attention computation across modalities 5. Answer generation
- **Design tradeoffs**: Resolution vs. context length (higher resolution images reduce available context tokens), Single vs. multi-image training (affects generalization), KV cache size vs. inference efficiency (affects long-context performance)
- **Failure signatures**: Performance drops with increasing image count (indicates multi-image training deficiency), Zero scores on image needle tasks (indicates OCR capability issues), Significant performance gaps between combined-image and multi-image sets (indicates resolution limitations)
- **First 3 experiments**: 1. Test model performance with varying numbers of images (2, 10, 32, 64) to quantify the performance decline curve 2. Evaluate model performance on adversarial variants of the test data to check for data contamination 3. Compare model performance on combined-image set vs. multi-image set to assess resolution impact

## Open Questions the Paper Calls Out

1. **How do MLLMs perform in real-world, long-context, multi-image tasks compared to existing benchmarks?**
   - Basis: The paper states that real-world applications often demand processing long contexts and multi-image tasks, which existing benchmarks fail to fully capture.
   - Why unresolved: The paper presents experimental results on MileBench but does not compare these results to performance on existing benchmarks in real-world scenarios.
   - What evidence would resolve it: Conducting experiments where MLLMs are tested on both MileBench and existing benchmarks in simulated real-world tasks, comparing their performance across these different settings.

2. **What is the impact of image resolution and quality on MLLMs' performance in long-context, multi-image tasks?**
   - Basis: The paper mentions that MLLMs struggle with long-context tasks, especially as the number of images increases.
   - Why unresolved: The paper does not investigate how image resolution and quality affect MLLMs' ability to process and understand long-context, multi-image scenarios.
   - What evidence would resolve it: Systematically varying image resolution and quality in MileBench tasks and analyzing how these changes impact MLLMs' performance.

3. **How can training data for MLLMs be optimized to improve their performance in long-context, multi-image tasks?**
   - Basis: The paper highlights that most open-source MLLMs struggle in long-context situations and suggests that this is likely because they have only been trained on single images.
   - Why unresolved: While the paper identifies the need for more focused research on enhancing MLLMs' long-context capabilities, it does not propose specific strategies for optimizing training data.
   - What evidence would resolve it: Designing and evaluating training curricula that progressively introduce longer contexts and more images.

## Limitations
- Evaluation methodology relies on zero-shot testing without fine-tuning, potentially not capturing practical deployment capabilities
- Benchmark design assumes diagnostic task performance translates directly to real-world applicability
- Focus on English-language content limits generalizability to other languages and cultural contexts

## Confidence
- **High Confidence**: GPT-4o's superior performance in long-context multimodal scenarios is well-supported by experimental results
- **Medium Confidence**: Performance gaps widening due to insufficient multi-image training data is plausible but requires further investigation
- **Low Confidence**: Data contamination minimal claim warrants caution due to potential statistical noise in adversarial set results

## Next Checks
1. Cross-dataset validation: Evaluate the same models on independent multimodal long-context datasets to verify performance pattern generalizability
2. Fine-tuning impact assessment: Conduct controlled experiments with selected models fine-tuned on multi-image data to quantify improvement
3. Architectural ablation study: Test models with modified attention mechanisms or KV cache management to determine if limitations stem from architectural constraints