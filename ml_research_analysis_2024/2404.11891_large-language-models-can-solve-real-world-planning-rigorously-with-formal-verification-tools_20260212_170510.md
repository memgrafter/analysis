---
ver: rpa2
title: Large Language Models Can Solve Real-World Planning Rigorously with Formal
  Verification Tools
arxiv_id: '2404.11891'
source_url: https://arxiv.org/abs/2404.11891
tags:
- city
- flight
- variables
- index
- cities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the challenge of generating correct travel
  plans for complex, multi-constraint planning problems using large language models
  (LLMs). They propose a framework that formalizes and solves such problems as constrained
  satisfiability problems using SMT solvers, combining LLM capabilities with rigorous
  algorithmic solving.
---

# Large Language Models Can Solve Real-World Planning Rigorously with Formal Verification Tools

## Quick Facts
- arXiv ID: 2404.11891
- Source URL: https://arxiv.org/abs/2404.11891
- Reference count: 40
- Authors: Yilun Hao; Yongchao Chen; Yang Zhang; Chuchu Fan
- One-line primary result: Achieves 93.9% success rate on travel planning benchmark by combining LLM parsing with SMT solver verification

## Executive Summary
This paper addresses the challenge of generating correct travel plans for complex, multi-constraint planning problems using large language models (LLMs). The authors propose a framework that formalizes these problems as constrained satisfiability problems (CSPs) and solves them with SMT solvers, combining LLM parsing with rigorous algorithmic solving. Their approach achieves a 93.9% success rate on a travel planning benchmark, significantly outperforming existing methods, and demonstrates strong zero-shot generalization to unseen constraints and tasks. Additionally, the framework can interactively repair unsatisfiable queries, successfully modifying and solving 81.6% to 91.7% of infeasible cases, and is robust to diverse paraphrased prompts.

## Method Summary
The framework converts natural language travel queries into structured step-by-step instructions, then into executable Python code that encodes constraints for an SMT solver. LLMs are prompted with few-shot examples to perform this generation, relying on in-context learning without additional fine-tuning. Once the LLM generates code encoding all constraints, the SMT solver is called to find a valid plan or prove unsatisfiability. When queries are infeasible, the framework iteratively repairs them by having the LLM analyze reasons, collect additional information, and propose modifications to the query until a plan is found or a limit is reached.

## Key Results
- Achieves 93.9% success rate on travel planning benchmark, significantly outperforming existing methods
- Demonstrates strong zero-shot generalization to unseen constraints and tasks
- Interactive repair mode successfully modifies and solves 81.6% to 91.7% of infeasible cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate formal constraint satisfaction encodings when provided with examples.
- Mechanism: The framework prompts the LLM to convert natural language travel queries into structured step-by-step instructions, then into executable Python code that encodes constraints for an SMT solver.
- Core assumption: LLMs can generalize from a small number of in-context examples to unseen constraints without additional fine-tuning.
- Evidence anchors:
  - [abstract] "we propose an LLM-based planning framework that formalizes and solves complex multi-constraint planning problems as constrained satisfiability problems"
  - [section 3.3.1] "we teach the LLM to perform such generation by providing three human-crafted examples"
  - [corpus] No direct corpus evidence; assumption based on in-context learning literature.
- Break condition: If the LLM fails to parse a new constraint type or produces incorrect code that the SMT solver cannot interpret, the plan will fail or runtime errors will occur.

### Mechanism 2
- Claim: SMT solvers provide sound and complete verification of plans, guaranteeing correctness if a solution exists.
- Mechanism: Once the LLM generates code encoding all constraints, the SMT solver is called to find a valid plan or prove unsatisfiability.
- Core assumption: The encoding of constraints into SMT is faithful to the original natural language requirements.
- Evidence anchors:
  - [abstract] "further consumed by sound and complete satisfiability solvers"
  - [section 3.3.3] "Since the SMT solver is sound and complete, it guarantees to find a solution if there exists one"
  - [corpus] No direct corpus evidence; relies on established properties of SMT solvers.
- Break condition: If the constraint encoding is incomplete or incorrect, the solver may return "unsatisfiable" when a valid plan exists, or return an invalid plan.

### Mechanism 3
- Claim: Iterative repair with interactive suggestions can convert unsatisfiable queries into satisfiable ones.
- Mechanism: When the SMT solver returns unsatisfiable, the LLM analyzes the reasons, collects additional information, and proposes modifications to the query, iterating until a plan is found or a limit is reached.
- Core assumption: The LLM can reason about why a plan is unsatisfiable and suggest realistic modifications that users will accept.
- Evidence anchors:
  - [abstract] "when user input queries are infeasible, our framework can identify the unsatisfiable core, provide failure reasons, and offers personalized modification suggestions"
  - [section 3.4] "LLM is prompted to reason about the situations to give suggestions to modify the constraints"
  - [corpus] No direct corpus evidence; assumption based on LLM reasoning capabilities demonstrated in other domains.
- Break condition: If the LLM cannot identify the root cause of unsatisfiability or proposes unrealistic modifications, the repair process will fail.

## Foundational Learning

- Concept: Constraint Satisfaction Problems (CSP) and Satisfiability Modulo Theories (SMT)
  - Why needed here: The framework relies on formalizing planning problems as CSPs and using SMT solvers for verification.
  - Quick check question: What is the difference between SAT and SMT solvers, and why is SMT more suitable for travel planning?

- Concept: In-context learning and prompt engineering
  - Why needed here: The framework uses few-shot examples in prompts to teach LLMs how to convert natural language to formal encodings.
  - Quick check question: How many examples are typically needed for effective in-context learning, and what makes a good demonstration?

- Concept: Interactive system design and user feedback loops
  - Why needed here: The framework includes an interactive repair mode where the LLM suggests modifications and incorporates user feedback.
  - Quick check question: What are the key design considerations for creating effective user feedback loops in AI systems?

## Architecture Onboarding

- Component map:
  Natural Language Query Parser -> Step Generator -> Code Generator -> SMT Solver -> Plan Output
  For unsatisfiable cases: SMT Solver -> Unsatisfiable Reason Extractor -> LLM Analyzer -> Suggestion Generator -> User Feedback -> Code Modifier -> SMT Solver (loop)

- Critical path:
  1. Parse natural language query
  2. Generate encoding steps
  3. Generate executable code
  4. Call SMT solver
  5. Parse and return solution or initiate repair

- Design tradeoffs:
  - Prompt complexity vs. LLM capability: More detailed prompts improve performance but require more engineering effort
  - Solver runtime vs. completeness: Longer timeouts increase success rate but reduce responsiveness
  - Interactive repair iterations vs. user patience: More iterations improve success rate but may frustrate users

- Failure signatures:
  - Runtime errors in generated code -> Code generation issue
  - SMT solver returns "unsatisfiable" -> Constraint encoding issue or truly infeasible query
  - LLM fails to generate appropriate suggestions -> Reasoning or domain knowledge limitation

- First 3 experiments:
  1. Test the basic pipeline with a simple satisfiable travel query and verify plan correctness
  2. Test with an unsatisfiable query to verify the repair mechanism identifies the correct unsatisfiable core
  3. Test with a query containing an unseen constraint type to verify zero-shot generalization capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the framework's performance scale with increasingly complex travel planning problems, such as those involving many more cities, constraints, or larger geographic areas?
- Basis in paper: [inferred] The paper mentions the solution space is prohibitive for TravelPlanner and runtime limitations exist, suggesting scalability concerns.
- Why unresolved: The paper only evaluates on the TravelPlanner dataset with a fixed number of cities and constraints, not testing how the framework performs on significantly larger or more complex problems.
- What evidence would resolve it: Experiments testing the framework on travel planning problems with substantially more cities, diverse constraints, and larger geographic areas, measuring performance, runtime, and success rate.

### Open Question 2
- Question: How does the choice of SMT solver impact the framework's performance and runtime, and are there solver-specific optimizations that could improve results?
- Basis in paper: [explicit] The paper uses Z3 SMT solver and mentions runtime limitations, but doesn't explore other solvers or solver-specific optimizations.
- Why unresolved: The paper only uses one SMT solver without comparing to alternatives or exploring solver-specific tuning, leaving open the question of whether other solvers or optimizations could yield better performance.
- What evidence would resolve it: Comparative experiments using different SMT solvers or solver configurations, measuring impact on success rate, runtime, and resource utilization.

### Open Question 3
- Question: How robust is the framework to ambiguous or incomplete natural language queries, and what mechanisms could be added to handle such cases more effectively?
- Basis in paper: [inferred] The framework assumes well-formed natural language queries and doesn't explicitly address handling ambiguity or incompleteness in user inputs.
- Why unresolved: The paper focuses on queries with complete information and doesn't explore how the framework handles queries with missing details, ambiguous constraints, or conflicting requirements.
- What evidence would resolve it: Experiments testing the framework with intentionally ambiguous or incomplete queries, and evaluating different strategies for clarifying or inferring missing information.

## Limitations
- Performance depends heavily on the quality and availability of external APIs (FlightSearch, CitySearch, etc.) which are not fully specified
- Generalization capability relies on in-context learning from a small number of examples, which may not scale well to more complex or diverse constraint types
- Long-term reliability in production environments with real-time API availability, varying user preferences, and edge cases not covered in the benchmark dataset

## Confidence
- **High Confidence**: The core mechanism of using SMT solvers for sound and complete verification is well-established and mathematically rigorous. The claim that the framework achieves 93.9% success rate on the travel planning benchmark is supported by empirical evaluation.
- **Medium Confidence**: The claim of zero-shot generalization to unseen constraints is supported by experiments but may be limited to constraint types similar to those in the training distribution. The effectiveness of interactive repair depends on the LLM's reasoning capabilities, which can vary across different LLM models.
- **Low Confidence**: The long-term reliability of the framework in production environments with real-time API availability, varying user preferences, and edge cases not covered in the benchmark dataset.

## Next Checks
1. **Robustness Testing**: Evaluate the framework's performance with intentionally degraded API responses (e.g., missing data, incorrect information) to assess its resilience to real-world data quality issues.
2. **Cross-Domain Generalization**: Test the framework on planning tasks from other domains (e.g., event planning, logistics) with different constraint types to validate the claimed zero-shot generalization capability beyond travel planning.
3. **Interactive Repair Analysis**: Conduct a detailed user study of the interactive repair process to measure user satisfaction, acceptance rates of suggested modifications, and the quality of final plans compared to manually crafted solutions.