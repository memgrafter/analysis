---
ver: rpa2
title: Explainable AI Approach using Near Misses Analysis
arxiv_id: '2411.16895'
source_url: https://arxiv.org/abs/2411.16895
tags:
- concept
- explanation
- concepts
- which
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel XAI approach based on near-misses
  analysis (NMA) that reveals hierarchical logical concepts inferred from a neural
  network's latent decision-making process without examining its explicit structure.
  The method analyzes probability vectors from image classifications to construct
  a connections graph, then applies hierarchical clustering and automatic concept
  naming using WordNet to generate visual and verbal explanations.
---

# Explainable AI Approach using Near Misses Analysis

## Quick Facts
- arXiv ID: 2411.16895
- Source URL: https://arxiv.org/abs/2411.16895
- Reference count: 15
- Primary result: Novel XAI method reveals hierarchical logical concepts from neural network latent decision-making through near-misses analysis of probability vectors

## Executive Summary
This paper introduces a novel explainable AI (XAI) approach based on near-misses analysis that reveals hierarchical logical concepts inferred from a neural network's latent decision-making process without examining its explicit structure. The method analyzes probability vectors from image classifications to construct a connections graph, then applies hierarchical clustering and automatic concept naming using WordNet to generate visual and verbal explanations. Experiments on multiple architectures across ImageNet and CIFAR100 datasets demonstrate the approach's ability to reflect networks' concept generation processes.

## Method Summary
The proposed method analyzes the probability vectors from image classifications, focusing on K nearest misses (top K non-correct predictions) to construct a weighted connections graph where edge weights represent dissimilarity between labels based on misclassification patterns. Hierarchical clustering is then applied to this graph to generate a dendrogram representing the hierarchy of label clusters. Automatic concept naming is achieved using WordNet by finding lowest common hypernyms for each cluster. The approach supports user-aided fine-tuning for concept naming and generates verbal explanations by combining dendrograms and concept names. A novel explainability metric quantifies explanation performance by comparing machine-generated explanations to human-annotated hypernyms.

## Key Results
- ResNet50 achieved the highest explainability scores across both ImageNet and CIFAR100 datasets
- VGG16, despite being shallower, achieved concept generation quality competitive with ResNet50
- MobileNetV2 showed significantly lower explainability scores, suggesting simpler architectures may struggle with hierarchical concept generation
- The automatic concept naming using WordNet successfully aligned with human semantic understanding in most cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Near-misses analysis reveals latent conceptual relationships by examining probability distributions beyond the top prediction
- Mechanism: The method analyzes the probability vector from image classifications, focusing on the K nearest misses (top K non-correct predictions) to construct a weighted connections graph where edge weights represent the dissimilarity between labels based on misclassification patterns
- Core assumption: The probability distribution of near-misses contains meaningful information about conceptual relationships that the model has implicitly learned
- Evidence anchors:
  - [abstract] "This approach reveals a hierarchy of logical 'concepts' inferred from the latent decision-making process of a Neural Network (NN) without delving into its explicit structure"
  - [section] "Analysing the near misses in the probability vector reveals connections which the given model makes among the labels and provides a glance to its latent decision-making process"
- Break condition: If the probability distribution of near-misses doesn't correlate with semantic similarity, the connections graph would be meaningless

### Mechanism 2
- Claim: Hierarchical clustering over the connections graph creates meaningful abstract concepts that reflect the model's internal reasoning
- Mechanism: The connections graph is transformed into a shortest-paths-metric using Floyd-Warshall algorithm, then hierarchical clustering is applied to generate a dendrogram representing the hierarchy of label clusters
- Core assumption: The shortest-path distances in the connections graph meaningfully represent conceptual distances that hierarchical clustering can organize
- Evidence anchors:
  - [section] "Compute a shortest-paths-metric by applying Floyd-Warshall algorithm for finding all-pairs-shortest-paths in a given graph. Then, run a hierarchical clustering algorithm on the connections graph based on the shortest-paths-metric"
  - [section] "This dendogram supplies a visual explanation – a high-level view of the classification decision-making process of the given image classification model"
- Break condition: If the connections graph doesn't form a meaningful metric space, hierarchical clustering would produce arbitrary or meaningless groupings

### Mechanism 3
- Claim: WordNet-based automatic concept naming provides human-readable explanations that align with human semantic understanding
- Mechanism: For each cluster in the dendrogram, the method finds the lowest common hypernym in WordNet for all labels in the cluster, creating human-readable concept names
- Core assumption: WordNet's hierarchical structure of synsets corresponds to the conceptual relationships learned by the neural network
- Evidence anchors:
  - [section] "We exploit WordNet lexical dataset for defining an automatic human-readable concept naming procedure as follows. At each stage of the dendrogram a group is formed. The automatic procedure scans WordNet hypernyms and finds the lowest common hypernym for all members in the group"
  - [section] "For example, if the group {’bed’, ’chair’} is formed, then for the label ’chair’, WordNet returns the linkage ’chair’-’furniture’-’entity’, and for the word ’bed’, WordNet returns the linkage ’bed’-’furniture’-’entity’. Thus, the lowest common hypernym is ’furniture’ for this set"
- Break condition: If the neural network's learned concepts don't align with WordNet's semantic hierarchy, the automatic naming would produce misleading or confusing explanations

## Foundational Learning

- Concept: Graph theory and shortest path algorithms
  - Why needed here: The method uses Floyd-Warshall algorithm to compute all-pairs-shortest-paths in the connections graph, which is essential for creating the metric space used in hierarchical clustering
  - Quick check question: What algorithm would you use to find the shortest path between all pairs of nodes in a weighted graph, and why is it appropriate here?

- Concept: Hierarchical clustering and dendrogram interpretation
  - Why needed here: The method applies hierarchical clustering to the shortest-paths-metric to generate a dendrogram that represents the conceptual hierarchy learned by the network
  - Quick check question: How does the dendrogram structure help visualize the relationships between different concepts learned by the neural network?

- Concept: WordNet lexical database and hypernym relationships
  - Why needed here: The method uses WordNet to automatically generate human-readable concept names by finding lowest common hypernyms for label clusters
  - Quick check question: How would you programmatically find the lowest common hypernym for a set of words using WordNet's API?

## Architecture Onboarding

- Component map: Data preprocessing → Test image classification → Probability vector extraction → Near-misses analysis → Connections graph construction → Shortest-path metric computation → Hierarchical clustering → Dendrogram generation → Concept naming → Verbal explanation

- Critical path: Test image → Probability vector → Near-misses analysis → Connections graph → Shortest-path metric → Hierarchical clustering → Dendrogram → Concept naming → Verbal explanation

- Design tradeoffs:
  - K value vs. noise: Higher K captures more relationships but may introduce noise from semantically unrelated classes
  - t-cutoff threshold vs. connectivity: Lower thresholds create more connected graphs but may include weak relationships
  - WordNet dependency vs. flexibility: Automatic naming is efficient but may not capture domain-specific concepts

- Failure signatures:
  - Empty or sparse connections graph: Indicates near-misses analysis isn't capturing meaningful relationships
  - Dendrogram with no meaningful clustering: Suggests the metric space doesn't reflect actual conceptual relationships
  - Concept names that don't match human intuition: Indicates WordNet alignment issues with learned concepts

- First 3 experiments:
  1. Verify connections graph construction by manually checking probability vectors for a few test images and confirming edge weights match expected misclassification patterns
  2. Test hierarchical clustering on a small synthetic connections graph with known structure to verify dendrogram correctness
  3. Validate WordNet concept naming by manually checking lowest common hypernyms for small label clusters against human intuition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed XAI approach improve adversarial robustness when integrated with white-box dissection methods?
- Basis in paper: [explicit] The paper states "In future work, we intend to perform white-box testing as well as examine the suggested approach effect on adversarial attacks construction and prevention."
- Why unresolved: The current experiments only demonstrate black-box explainability. The paper does not investigate how NMA analysis with hierarchical clustering can be used within white-box dissection methods or its effect on adversarial robustness.
- What evidence would resolve it: Experiments showing that models using the proposed XAI approach integrated with white-box analysis demonstrate improved resistance to adversarial attacks compared to standard models.

### Open Question 2
- Question: What is the relationship between network efficiency (parameters) and explainability quality beyond the architectures tested?
- Basis in paper: [explicit] The paper states "our experiments suggest that efficient architectures, which achieve a similar accuracy level with much less neurons may still pay the price of explainability and robustness in terms of concepts generation."
- Why unresolved: The experiments only compare a limited set of architectures (ResNet, EfficientNet, VGG, MobileNet). The relationship between efficiency and explainability needs broader validation across more diverse architectures.
- What evidence would resolve it: Extensive experiments testing the proposed XAI approach across a wider range of architectures with varying efficiency metrics, demonstrating whether the observed trade-off holds consistently.

### Open Question 3
- Question: How does the quality of automatic concept naming via WordNet compare to human-annotated hierarchies across different domains?
- Basis in paper: [explicit] The paper mentions that "in some cases, the provided names do not well express the shared property by the labels in a given cluster" and that "our XAI approach supports a user-aided fine-tuning for the automatic concept naming procedure."
- Why unresolved: While the paper demonstrates the automatic naming procedure and user refinement capability, it does not quantitatively compare the quality of WordNet-based naming versus human annotations across different domains or datasets.
- What evidence would resolve it: Systematic evaluation comparing WordNet-based automatic naming with human-annotated hierarchies across multiple datasets and domains, measuring agreement rates and concept quality scores.

## Limitations
- The approach relies on probability distributions from test classifications, which may not fully capture the model's learned concepts, especially for models with high confidence predictions
- WordNet-based concept naming assumes semantic alignment between learned concepts and linguistic hierarchies, which may not hold for all domains or languages
- The explainability metric, while novel, depends on human annotations that may vary in quality and completeness across datasets

## Confidence
- High confidence in the core mechanism of near-misses analysis revealing conceptual relationships through probability distributions
- Medium confidence in hierarchical clustering producing meaningful conceptual hierarchies, pending validation on diverse datasets
- Medium confidence in WordNet-based automatic naming aligning with human semantic understanding across different domains

## Next Checks
1. Test the approach on domain-specific datasets (medical imaging, satellite imagery) to evaluate generalization beyond ImageNet and CIFAR100
2. Compare explainability scores against human-generated explanations from domain experts to validate the quality of automatic concept naming
3. Analyze the impact of K-near misses parameter on explanation quality to determine optimal configuration for different network architectures and dataset sizes