---
ver: rpa2
title: Mining Reasons For And Against Vaccination From Unstructured Data Using Nichesourcing
  and AI Data Augmentation
arxiv_id: '2406.19951'
source_url: https://arxiv.org/abs/2406.19951
tags:
- reasons
- human
- vaccination
- labeled
- gpt4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a dataset (RFAV) for predicting reasons for
  and against vaccination, annotated through nichesourcing and expanded using GPT4
  and GPT3.5-Turbo. The authors show that it is possible to mine reasons from unstructured
  text under different task definitions, despite the high level of subjectivity involved.
---

# Mining Reasons For And Against Vaccination From Unstructured Data Using Nichesourcing and AI Data Augmentation

## Quick Facts
- arXiv ID: 2406.19951
- Source URL: https://arxiv.org/abs/2406.19951
- Authors: Damián Ariel Furman; Juan Junqueras; Z. Burçe Gümüslü; Edgar Altszyler; Joaquin Navajas; Ophelia Deroy; Justin Sulik
- Reference count: 19
- One-line primary result: A dataset for predicting vaccination reasons, annotated through nichesourcing and expanded using GPT4 and GPT3.5-Turbo, with models achieving F1 scores up to 0.64 on reason identification

## Executive Summary
This paper presents RFAV, a dataset for identifying reasons for and against vaccination from unstructured text. The dataset was created through nichesourcing using psychology and philosophy students, then expanded using GPT4 and GPT3.5-Turbo. The authors evaluate transformer-based models (RoBERTa, Longformer, XLM-Roberta, BETO, SpanBERTa) on four classification tasks and find that while models perform well on binary reason classification (F1 up to 0.64), performance decreases when training with augmented data, suggesting GPT models may not fully capture subjective human annotation criteria.

## Method Summary
The authors scraped web documents using SERPAPI and Trafilatura with vaccination-related keywords, then annotated 1000 examples each in English and Spanish through nichesourcing using domain experts. GPT4 and GPT3.5-Turbo were used to label additional examples, expanding the dataset. Transformer models were fine-tuned on the annotated data using token classification, with datasets partitioned into train (80%), development (10%), and test (10%) sets. Models were evaluated on four tasks: binary reason classification, 6-class stance classification, 4-class compressed stance classification, and scientific authority identification.

## Key Results
- Moderate inter-annotator agreement (Kappa ~0.49) achieved on identifying reasons, considered satisfactory given task subjectivity
- Longformer model achieved F1 score of 0.64 for binary reason identification, close to human performance
- Models performed well on supporting reasons (71.59% of labeled reasons) but struggled with minority classes like "Against" and "Neutral" stances
- Performance decreased when training with augmented data, suggesting GPT models introduced annotation drift

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Nichesourcing provides high-quality annotations for subjective tasks like identifying vaccination reasons.
- Mechanism: Domain experts with targeted training and iterative review achieve moderate agreement despite subjectivity.
- Core assumption: Domain expertise and structured training improve annotation consistency on subjective tasks.
- Evidence anchors:
  - [section] Moderate agreement (Kappa ~0.49) on identifying reasons, considered satisfactory given the task's subjective nature.
  - [section] Annotators received 2 hours of training and iterative reviews to refine criteria.
- Break condition: If annotators lack sufficient domain expertise or training, agreement would drop significantly, reducing dataset quality.

### Mechanism 2
- Claim: GPT-based data augmentation can scale labeled datasets but may introduce annotation drift.
- Mechanism: GPT models rapidly label large volumes but may follow different criteria than human experts, leading to class distribution shifts.
- Core assumption: GPT models can learn annotation tasks through few-shot prompting but may not fully capture nuanced human criteria.
- Evidence anchors:
  - [section] GPT4 and GPT3.5-Turbo labeled 80% more examples than humans and twice as many words as reasons, but with different stance distributions.
  - [section] Performance decreased when training with augmented data, suggesting GPT models didn't fully capture human annotation criteria.
- Break condition: If the prompt doesn't adequately capture annotation nuances, GPT models will consistently misalign with human criteria, making augmented data unreliable.

### Mechanism 3
- Claim: Transformer-based models can effectively identify vaccination reasons and stances when fine-tuned on high-quality data.
- Mechanism: Pre-trained language models can be fine-tuned to perform token classification for reasons and stances, achieving performance close to or above human agreement.
- Core assumption: Transformer models pre-trained on large corpora can adapt to specialized classification tasks with sufficient domain-specific fine-tuning.
- Evidence anchors:
  - [section] Longformer achieved F1 of 0.64 for reason identification, close to human performance (0.56-0.58).
  - [section] Models performed well on the binary reason classification task but struggled with the six-class stance classification, especially for minority classes.
- Break condition: If the dataset is too small or imbalanced, even fine-tuned transformers will struggle to generalize, particularly for minority stance classes.

## Foundational Learning

- Concept: Token classification and sequence labeling
  - Why needed here: The task requires identifying spans of text (reasons) and classifying each token's role (reason/non-reason, stance value).
  - Quick check question: How would you modify a standard BERT model to output token-level labels instead of sentence-level classification?

- Concept: Cohen's Kappa for inter-annotator agreement
  - Why needed here: The authors use Kappa to measure agreement between annotators on a subjective task, which is critical for assessing dataset quality.
  - Quick check question: What's the difference between Cohen's Kappa and simple percentage agreement, and why is Kappa preferred for this task?

- Concept: Data augmentation and its impact on model performance
  - Why needed here: The paper explores using GPT-generated annotations to expand the dataset, which affects model training and evaluation.
  - Quick check question: What are potential risks of using model-generated labels for training, and how might they affect model generalization?

## Architecture Onboarding

- Component map:
  Corpus collection (SERPAPI + Trafilatura) -> Annotation pipeline (nichesourcing platform + manual + review) -> Data augmentation (GPT4/GPT3.5-Turbo) -> Model training (transformer fine-tuning) -> Evaluation (F1/Precision/Recall + human agreement comparison)

- Critical path: Corpus collection → Annotation → Model training → Evaluation
  - Each stage must complete successfully before the next can begin

- Design tradeoffs:
  - Nichesourcing vs. crowdsourcing: Higher quality but slower and more expensive
  - GPT augmentation vs. manual annotation: Faster scaling but potential annotation drift
  - Binary vs. multi-class classification: Simpler tasks perform better but lose nuance

- Failure signatures:
  - Low inter-annotator agreement → Annotation criteria unclear or too subjective
  - Performance drop with augmented data → GPT models not capturing human criteria
  - Poor minority class performance → Dataset imbalance or insufficient examples

- First 3 experiments:
  1. Fine-tune RoBERTa on the human-annotated dataset for binary reason classification and measure performance
  2. Compare GPT4 vs. human annotations on a held-out sample to quantify annotation drift
  3. Train models with different ratios of human vs. GPT-augmented data to find optimal augmentation level

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different levels of detail in annotation guidelines affect inter-annotator agreement in subjective classification tasks like reason stance detection?
- Basis in paper: [explicit] The paper shows moderate agreement (0.49) on identifying reasons for vaccination, with better performance on supporting reasons (71.59% of labeled reasons). Agreement was calculated using Cohen's Kappa between annotators.
- Why unresolved: The paper does not explore how variations in guideline specificity impact agreement rates, though it mentions the iterative annotation process helped refine criteria.
- What evidence would resolve it: Experiments comparing agreement rates across multiple guideline versions with varying detail levels would clarify this relationship.

### Open Question 2
- Question: What is the optimal balance between human-annotated and AI-augmented data for training models on subjective classification tasks?
- Basis in paper: [explicit] Performance decreased when training with augmented data, suggesting GPT models may not fully capture subjective annotation criteria. The paper notes that models trained with more augmented data became more conservative in predictions.
- Why unresolved: The paper does not systematically explore different ratios of human vs. AI-annotated data to find the optimal balance.
- What evidence would resolve it: Experiments varying the proportion of human vs. AI-augmented data in training sets while measuring model performance would identify optimal ratios.

### Open Question 3
- Question: How do minority class representations (against/neutral stances) in training data affect model performance on these classes?
- Basis in paper: [explicit] Results show poor performance for "Against" or "Neutral" classes, which were least frequent in the dataset. The paper acknowledges this data imbalance limits the tool's effectiveness in identifying arguments against vaccination.
- Why unresolved: The paper does not explore techniques like oversampling, class weighting, or synthetic minority generation to address this imbalance.
- What evidence would resolve it: Experiments applying different imbalance mitigation techniques and measuring their impact on minority class performance would clarify effective solutions.

## Limitations
- Moderate inter-annotator agreement (Kappa ~0.49) indicates the task's inherent subjectivity and limits reliability of ground truth labels
- Dataset imbalance with more supporting (71.59%) than opposing reasons may bias models toward pro-vaccination arguments
- Performance decreases when training with augmented data suggest GPT models may not fully capture nuanced human annotation criteria

## Confidence

**High Confidence**:
- Transformer models can effectively identify vaccination reasons with reasonable performance (F1 ~0.64 for Longformer)
- Nichesourcing with domain experts can produce usable annotations for subjective tasks despite moderate agreement
- GPT augmentation significantly increases dataset size but may introduce annotation inconsistencies

**Medium Confidence**:
- The observed performance drop with augmented data definitively proves GPT models don't capture human criteria (could be due to other factors like dataset imbalance)
- Nichesourcing is superior to crowdsourcing for this task (no direct comparison provided)
- The annotation criteria are sufficiently clear for consistent human annotation (agreement is only moderate)

## Next Checks

1. **Annotation Agreement Analysis**: Conduct a detailed error analysis comparing GPT4 vs. human annotations on a held-out sample to identify specific types of disagreements and quantify annotation drift. This would clarify whether GPT models are systematically misinterpreting certain argument structures.

2. **Cross-Lingual Transfer Validation**: Test whether models trained on English data can effectively transfer to Spanish without language-specific fine-tuning, given that the same annotation criteria were applied to both languages. This would validate the claim that the methodology generalizes across languages.

3. **Minority Class Performance Study**: Analyze model performance specifically on minority stance classes (e.g., authorities for and against vaccination) to determine if the performance drop is due to class imbalance or genuine difficulty in identifying these cases. This would inform whether dataset balancing techniques are needed.