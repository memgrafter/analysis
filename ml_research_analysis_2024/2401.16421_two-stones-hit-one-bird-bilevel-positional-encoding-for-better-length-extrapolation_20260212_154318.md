---
ver: rpa2
title: 'Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation'
arxiv_id: '2401.16421'
source_url: https://arxiv.org/abs/2401.16421
tags:
- encoding
- positional
- length
- language
- bipe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a bilevel positional encoding (BiPE) for length
  extrapolation in language models. The key idea is to use two distinct positional
  encodings for each token: an intra-segment encoding (using absolute positional encoding)
  to identify its location within a segment, and an inter-segment encoding (using
  relative positional encoding) to specify the segment index and model relationships
  between segments.'
---

# Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation

## Quick Facts
- **arXiv ID**: 2401.16421
- **Source URL**: https://arxiv.org/abs/2401.16421
- **Reference count**: 38
- **Primary result**: BiPE improves length extrapolation in language models by using two distinct positional encodings for intra-segment and inter-segment positions, achieving better perplexity on longer sequences while maintaining performance on normal-length text.

## Executive Summary
This paper proposes a bilevel positional encoding (BiPE) scheme that leverages the natural hierarchical structure of text data to improve length extrapolation in language models. The key insight is that text sequences can be segmented into modular units (sentences, code blocks, etc.) where the number of tokens per segment is bounded but the number of segments scales with sequence length. By using absolute positional encoding for intra-segment positions and relative positional encoding for inter-segment relationships, BiPE can effectively extrapolate to longer sequences. The method shows consistent improvements across diverse text modalities including PG-19, ArXiv, GitHub, and the SCROLLS benchmark.

## Method Summary
BiPE introduces a two-level positional encoding approach where each token receives both an intra-segment encoding (absolute positional encoding identifying its position within a segment) and an inter-segment encoding (relative positional encoding specifying the segment index and modeling relationships between segments). The method builds on the observation that language data has natural hierarchical structure with bounded segment lengths. The model uses full stops and newlines for segmentation in text data, and the inter-segment relative encoding enables the model to generalize to longer sequences by focusing on segment count rather than absolute token positions. Two variants are explored: BiPE-RoPE using rotary position encoding for inter-segment encoding, and BiPE-ALiBi using attention bias.

## Key Results
- BiPE consistently outperforms standard positional encodings (Sinusoidal, RoPE, ALiBi) on length extrapolation tasks across PG-19, ArXiv, and GitHub datasets
- On SCROLLS benchmark, BiPE achieves comparable or better performance on long context understanding tasks
- Mathematical reasoning tasks show BiPE's superiority aligns with theoretical parameter efficiency advantages
- BiPE maintains competitive performance on normal-length sequences while excelling at extrapolation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The two-level structure leverages natural segmentation in text to improve length extrapolation.
- **Mechanism**: Text data inherently contains boundaries (sentences, code blocks, mathematical steps) that segment sequences. The number of tokens within each segment has bounded support while the number of segments grows with sequence length. By separating positional encoding into intra-segment (absolute) and inter-segment (relative) components, the model can extrapolate along the segment dimension rather than the token dimension.
- **Core assumption**: Language sequences have consistent segmentation patterns where segment lengths are bounded but the number of segments scales with sequence length.
- **Evidence anchors**: [abstract] "we leverage the intrinsic segmentation of language sequences"; [section] "empirically, we observed that for sequences with different lengths, the distribution of the token number in each modular segment usually has bounded support"
- **Break condition**: If text data lacks consistent segmentation patterns or if segment lengths are unbounded, the two-level approach loses its theoretical advantage.

### Mechanism 2
- **Claim**: Disentangling positional information makes learning more parameter-efficient.
- **Mechanism**: The hierarchical structure of text (tokens within segments, segments within documents) can be modeled with Bi-NFA, which has lower parameter complexity than standard NFA. By using different positional encodings at each level, the model exploits this structure rather than treating all positions as independent.
- **Core assumption**: Language data can be modeled as a hierarchical process where tokens are generated within segments, and segments are generated within documents.
- **Evidence anchors**: [abstract] "Theoretical analysis shows this disentanglement of positional information makes learning more effective"; [section] "we leverage the (non-deterministic) finite automata... we introduce a simplified model, Bi-NFA"
- **Break condition**: If the hierarchical generation assumption doesn't hold for the target data, or if the segment boundaries are arbitrary rather than linguistically meaningful.

### Mechanism 3
- **Claim**: Relative positional encoding at the inter-segment level provides better extrapolation capability.
- **Mechanism**: While absolute positional encoding saturates at training length, relative positional encoding can generalize to unseen distances. By applying RPE at the segment level (modeling relationships between segments), the model can handle longer sequences even when individual segment positions are bounded.
- **Core assumption**: Relative positional encodings generalize better than absolute encodings for long-range dependencies.
- **Evidence anchors**: [abstract] "the inter-segment encoding specifies the segment index, models the relationships between segments, and aims to improve extrapolation capabilities via relative positional encoding"; [section] "we employ relative positional encodings... which results in the attention being a function of the relative distance between tokens"
- **Break condition**: If relative positional encodings don't generalize well to the segment index distances in very long sequences.

## Foundational Learning

- **Concept**: Hierarchical data structures
  - **Why needed here**: The paper builds on the observation that text has natural hierarchical structure (tokens → segments → documents) which can be exploited for more efficient modeling.
  - **Quick check question**: Can you identify three different hierarchical levels in a typical research paper?

- **Concept**: Positional encoding in transformers
  - **Why needed here**: Understanding the difference between absolute and relative positional encodings is crucial for grasping why the two-level approach works.
  - **Quick check question**: What is the key limitation of absolute positional encoding when applied to sequences longer than training length?

- **Concept**: Transformer attention mechanisms
  - **Why needed here**: The paper modifies how attention computes positional information by separating intra-segment and inter-segment encodings.
  - **Quick check question**: How does rotary position encoding (RoPE) modify the attention computation compared to standard absolute positional encoding?

## Architecture Onboarding

- **Component map**: Token embedding + intra-segment absolute PE → attention input; Segment index → inter-segment relative PE → attention bias; Attention computes scores using relative segment distances → feed-forward network
- **Critical path**: 
  1. Token embedding + intra-segment absolute PE → input to attention
  2. Segment index → inter-segment relative PE → attention bias/rotation
  3. Attention computes scores using relative segment distances
  4. Output passed to feed-forward network
  5. Segment boundaries detected during preprocessing
- **Design tradeoffs**:
  - Pro: Better length extrapolation by focusing on segment count rather than token count
  - Pro: Parameter efficiency by exploiting hierarchical structure
  - Con: Requires accurate segmentation which may not exist in all data types
  - Con: Additional complexity in positional encoding computation
  - Tradeoff: Using absolute vs. relative encoding at each level - absolute works for bounded intra-segment positions, relative needed for inter-segment extrapolation
- **Failure signatures**:
  - Poor performance on data without clear segmentation boundaries
  - Degradation when segment lengths become unbounded
  - Issues when segment count doesn't scale linearly with sequence length
  - Problems with segmentation errors propagating through the model
- **First 3 experiments**:
  1. Compare BiPE vs. standard PE on synthetic data with perfect segmentation and bounded segment lengths
  2. Test BiPE performance when segment boundaries are removed or randomized
  3. Evaluate BiPE on data where segment lengths follow heavy-tailed distribution (unbounded segments)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can BiPE be extended to a hierarchical design that captures multiple levels of segmentation in text data?
- **Basis in paper**: [explicit] The paper mentions that "The intrinsic segmentation of text data yields a hierarchical structure, e.g., sentences→paragraphs→documents" and suggests that "expanding our bilevel design to a hierarchical version results in improved length extrapolation" as a future direction.
- **Why unresolved**: The paper only proposes a bilevel positional encoding scheme and does not explore hierarchical extensions. The effectiveness of a multi-level hierarchical design on length extrapolation remains untested.
- **What evidence would resolve it**: Empirical results comparing BiPE's performance with a hierarchical extension (e.g., three-level encoding for sentences, paragraphs, and documents) on length extrapolation tasks across diverse text modalities.

### Open Question 2
- **Question**: How does BiPE perform on sequence data without clear segmentation boundaries, such as time series, amino acid sequences, or gene sequences?
- **Basis in paper**: [explicit] The paper states that "There exist sequence data that do not have clear boundary of segmentations, e.g., time series, amino acid and gene sequence" and suggests exploring "better and more comprehensive segmentation methods for general purposes with our BiPE method" as a future direction.
- **Why unresolved**: The paper only evaluates BiPE on text data with clear segmentation (e.g., sentences, lines, functions). Its performance on non-text data or text data with ambiguous segmentation is unknown.
- **What evidence would resolve it**: Experiments applying BiPE to non-text sequence data (e.g., time series forecasting, protein structure prediction) or text data with ambiguous segmentation (e.g., free-form dialogue) and comparing its performance with standard positional encodings.

### Open Question 3
- **Question**: How does the choice of segmentation strategy affect BiPE's performance, and can adaptive segmentation improve results?
- **Basis in paper**: [explicit] The paper mentions that "In Section 4.2, we use full stop '.' and newline '\n' for text segmentation" and conducts an ablation study on fixed segment lengths, finding that "this naive approach does not achieve the same level of performance as using natural segmentation."
- **Why unresolved**: The paper only explores a few simple segmentation strategies (e.g., punctuation, fixed length) and does not investigate more sophisticated or adaptive segmentation methods. The impact of segmentation choice on BiPE's effectiveness is not fully understood.
- **What evidence would resolve it**: Experiments comparing BiPE's performance using different segmentation strategies (e.g., semantic segmentation, syntactic parsing, topic modeling) on length extrapolation tasks and analyzing the correlation between segmentation quality and BiPE's effectiveness.

## Limitations
- The method requires accurate segmentation of text data, which may not exist for all data types or may be computationally expensive to obtain
- Theoretical analysis of parameter efficiency for Bi-NFA representation lacks direct empirical validation through actual parameter measurements
- Performance on data without clear segmentation boundaries or with unbounded segment lengths remains untested

## Confidence
- **Length extrapolation improvement**: High confidence - Multiple datasets show consistent improvements with statistical significance
- **Parameter efficiency claims**: Low confidence - Theoretical analysis shows Bi-NFA has lower parameter complexity, but no empirical measurements of actual parameter savings
- **Segmentation strategy effectiveness**: Medium confidence - Demonstrates effectiveness on structured data but lacks testing on data without clear segmentation boundaries
- **Generalizability across text modalities**: Medium confidence - Performance on SCROLLS benchmark suggests some generalizability, but may not fully capture real-world diversity

## Next Checks
1. **Parameter efficiency validation**: Measure and compare actual parameter counts and FLOPs for BiPE vs. standard positional encodings across different sequence lengths, directly testing the theoretical claim about parameter efficiency gains.

2. **Segmentation robustness test**: Evaluate BiPE performance on datasets with varying segmentation quality - from perfect segmentation to randomized boundaries to completely boundary-free text - to identify the threshold at which the two-level approach fails.

3. **Cross-domain generalization study**: Test BiPE on highly unstructured text (e.g., social media posts, conversational data, or randomly concatenated text) to determine whether the hierarchical structure assumption holds beyond the structured datasets used in the paper.