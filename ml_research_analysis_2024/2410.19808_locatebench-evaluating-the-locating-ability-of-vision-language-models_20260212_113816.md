---
ver: rpa2
title: 'LocateBench: Evaluating the Locating Ability of Vision Language Models'
arxiv_id: '2410.19808'
source_url: https://arxiv.org/abs/2410.19808
tags:
- which
- language
- dataset
- arxiv
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LocateBench, a high-quality benchmark for
  evaluating vision language models' (VLMs) ability to locate objects in images based
  on natural language instructions. The benchmark uses a multiple-choice format where
  models must select the correct bounding box from four candidates.
---

# LocateBench: Evaluating the Locating Ability of Vision Language Models

## Quick Facts
- arXiv ID: 2410.19808
- Source URL: https://arxiv.org/abs/2410.19808
- Reference count: 25
- Primary result: VLMs lag behind human accuracy by more than 10% on object localization tasks

## Executive Summary
This paper introduces LocateBench, a benchmark for evaluating vision language models' (VLMs) ability to locate objects in images based on natural language instructions. The benchmark uses a multiple-choice format where models must select the correct bounding box from four candidates. Experiments with several advanced VLMs (GPT-4o, GPT-4 Turbo, Gemini-1.0-pro, Gemini-1.5-pro, Claude-3-Opus, LLaVA-1.6-vicuna-7b, and LLaVA-1.6-mistral-7b) using different prompting methods show that GPT-4o performs best but still lags behind human accuracy by more than 10%. The study demonstrates that current VLMs have significant room for improvement in object localization tasks, despite their strong performance on downstream applications.

## Method Summary
The LocateBench dataset was constructed by converting descriptions from the RefCOCO series datasets into natural language questions, paired with images from COCO. Each sample includes an image, a question formulated from the original description, and four candidate bounding boxes. The paper experiments with four prompting methods: ABCD (alphabet letters), Colors (different colors), Coordinate (bounding box coordinates), and 1-by-1 (yes/no questions for each candidate). VLMs are evaluated on their accuracy in selecting the correct bounding box, with results compared to human performance on a subset of the data.

## Key Results
- GPT-4o achieves the highest accuracy among tested VLMs at 80.6% on LocateBench
- VLMs show significant performance gaps compared to human accuracy (95%)
- Prompt format significantly affects performance, with alphabet letters yielding the highest accuracy
- Gemini-1.5-pro shows the largest sensitivity to prompt methods (43.9% difference between best and worst)
- Even the best-performing VLM lags behind human performance by more than 10%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multiple-choice format isolates VLM's object localization ability from generation capabilities.
- Mechanism: By constraining the output to selecting among four predefined bounding boxes, the evaluation removes ambiguity about whether the model understands the instruction or simply generates plausible but incorrect outputs.
- Core assumption: The candidate bounding boxes are sufficiently distinct and the correct answer is unambiguous.
- Evidence anchors:
  - [abstract] "The multiple choice setup of LocateBench allows for evaluation of VLMs that do not have a dedicated input/output field for bounding boxes or image segmentation masks."
  - [section] "LocateBench is a multiple choice question dataset. Each sample includes an image, a description formulated as a question, and four bounding boxes representing candidate answers to the question."

### Mechanism 2
- Claim: Prompt format significantly affects VLM performance on localization tasks.
- Mechanism: Different ways of presenting the same localization task (alphabet letters, colors, coordinates, or yes/no questions) engage different reasoning patterns in VLMs, with some formats being more interpretable to the model than others.
- Core assumption: VLMs process visual information and language instructions differently depending on how the task is framed.
- Evidence anchors:
  - [section] "Overall, multi-choice by alphabet letters led to the highest accuracies. Gemini-1.5-pro is most sensitive to prompt methods, showing a difference in performance between the best and worst settings of 43.9%."

### Mechanism 3
- Claim: VLM performance on object localization is an upstream bottleneck for downstream tasks.
- Mechanism: The ability to accurately locate objects based on natural language instructions is fundamental to many vision-language applications, and deficiencies in this core capability propagate to higher-level tasks.
- Core assumption: Downstream tasks like visual question answering and image captioning depend on accurate object localization as a prerequisite step.
- Evidence anchors:
  - [abstract] "This ability also contributes to many downstream tasks such as visual question answering and image captioning."
  - [section] "The ability to locate an object in an image according to natural language instructions is crucial for many real-world applications."

## Foundational Learning

- Concept: Object detection and bounding box representation
  - Why needed here: The benchmark evaluates whether models can correctly identify which bounding box contains the target object described in natural language.
  - Quick check question: Can you explain the difference between object detection and object localization, and how bounding boxes represent object locations?

- Concept: Visual grounding in natural language
  - Why needed here: Models must understand how linguistic descriptions map to visual regions in images, connecting semantic concepts to spatial locations.
  - Quick check question: How would you describe the spatial relationship "to the left of the yellow one" in terms of coordinate systems and object positioning?

- Concept: Multiple-choice evaluation methodology
  - Why needed here: The benchmark uses a multiple-choice format to provide clear evaluation criteria and eliminate generation ambiguity.
  - Quick check question: What are the advantages and limitations of using multiple-choice formats versus open-ended generation for evaluating vision-language models?

## Architecture Onboarding

- Component map: Image → Bounding box candidates → Natural language question → Prompt formatting → VLM inference → Answer extraction → Accuracy evaluation
- Critical path: Image → Bounding box candidates → Natural language question → Prompt formatting → VLM inference → Answer extraction → Accuracy evaluation
- Design tradeoffs: Multiple-choice format provides clear evaluation but may not reflect real-world open-ended localization; prompt sensitivity reveals model limitations but complicates fair comparison; using existing datasets reduces cost but may introduce bias.
- Failure signatures: Models consistently fail on questions requiring relative positioning or counting; certain prompt formats show drastically different performance; extraction errors occur when models don't follow specified output formats.
- First 3 experiments:
  1. Run the same set of images through all four prompt formats (ABCD, colors, coordinates, 1-by-1) to quantify prompt sensitivity
  2. Test with human annotators on a subset to establish upper bounds and identify ambiguous questions
  3. Filter out noisy examples by having multiple annotators agree on the correct answer to ensure benchmark quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would VLMs perform on LocateBench with more than four candidate bounding boxes per image?
- Basis in paper: [explicit] The paper mentions that they focus on multiple-choice problems with only four candidates and acknowledge this may not fully reflect the complexity of some real-world tasks.
- Why unresolved: The authors only tested their benchmark with four candidate bounding boxes, leaving open the question of how VLMs would handle more candidates.
- What evidence would resolve it: Experiments with LocateBench using varying numbers of candidate bounding boxes (e.g., 5, 8, 12) would show how VLM performance scales with increased choice complexity.

### Open Question 2
- Question: What is the impact of using different LLMs for converting descriptions to questions on LocateBench performance?
- Basis in paper: [explicit] The authors mention they used only a single LLM (Reka Core) for this conversion due to budget constraints.
- Why unresolved: The choice of LLM for question generation could potentially affect the quality and difficulty of the questions, which in turn might impact VLM performance.
- What evidence would resolve it: Comparing LocateBench performance when using different LLMs (e.g., GPT-4, Claude, Gemini) for question generation would reveal any systematic differences.

### Open Question 3
- Question: How would VLMs perform on LocateBench if trained on the RefCOCO series datasets?
- Basis in paper: [explicit] The authors note that VLMs may have been exposed to COCO data and mention that their contributions (questions and bounding-box-to-label mappings) are not in VLM training data.
- Why unresolved: It's unclear whether training VLMs on the source datasets (RefCOCO series) would significantly improve their performance on LocateBench.
- What evidence would resolve it: Training VLMs on the RefCOCO series datasets and then evaluating their performance on LocateBench would show if there's a significant improvement compared to non-trained models.

## Limitations

- The multiple-choice format may artificially constrain VLM performance compared to open-ended localization tasks
- Prompt sensitivity results may not generalize across different VLM architectures
- The benchmark's focus on RefCOCO-style descriptions may not capture the full range of real-world localization challenges

## Confidence

- High: General performance gap between VLMs and human accuracy on object localization
- Medium: Specific impact of prompt formats on VLM performance
- Low: Claims about downstream task implications due to limited direct evidence

## Next Checks

1. **Cross-dataset generalization test**: Evaluate the same VLMs on a different object localization benchmark (e.g., GQA or Visual Genome) to assess whether the performance gaps are consistent across different types of visual grounding tasks.

2. **Open-ended vs. multiple-choice comparison**: Implement a subset of the benchmark using open-ended bounding box generation rather than multiple-choice selection to quantify the impact of the constrained format on VLM performance.

3. **Fine-grained error analysis**: Conduct a detailed error analysis categorizing failures by question type (relative positioning, counting, attribute-based, etc.) to identify specific weaknesses and potential model improvements.