---
ver: rpa2
title: Non-myopic Generation of Language Models for Reasoning and Planning
arxiv_id: '2410.17195'
source_url: https://arxiv.org/abs/2410.17195
tags:
- planning
- arxiv
- action
- preprint
- sheep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Predictive-Decoding, a novel approach to
  improve planning accuracy in Large Language Models (LLMs) by addressing their myopic
  nature. Predictive-Decoding leverages Model Predictive Control to re-weight LLM
  distributions based on foresight trajectories, enabling non-myopic planning.
---

# Non-myopic Generation of Language Models for Reasoning and Planning

## Quick Facts
- arXiv ID: 2410.17195
- Source URL: https://arxiv.org/abs/2410.17195
- Reference count: 40
- Primary result: Predictive-Decoding improves LLM planning accuracy by 7.2% on GSM8K and 25.3% on AlfWorld using foresight-based reweighting

## Executive Summary
This paper introduces Predictive-Decoding, a novel approach that addresses the myopic nature of Large Language Models (LLMs) in planning tasks. The method leverages Model Predictive Control principles to re-weight LLM distributions based on foresight trajectories, enabling non-myopic planning. By sampling multiple future trajectories and rescaling the original generation distribution based on their evaluations, Predictive-Decoding mitigates early errors and promotes global-optimal planning. Experiments on math, coding, and agent tasks demonstrate significant improvements in planning and reasoning accuracy while maintaining computational efficiency.

## Method Summary
Predictive-Decoding addresses LLM myopia by implementing a sampling-importance-resampling (SIR) technique that evaluates multiple foresight trajectories before making each generation decision. The method samples K future trajectories of length T0, evaluates their quality using an LLM-based reward function, and reweights the original generation distribution accordingly. This process follows Model Predictive Control principles, selecting actions that optimize not just immediate rewards but also long-term outcomes. The approach requires no additional training and can be applied to any pre-trained LLM, making it computationally efficient compared to search-based baselines while achieving superior performance on planning-intensive tasks.

## Key Results
- Achieves 7.2% improvement on GSM8K math problem accuracy
- Achieves 25.3% improvement on AlfWorld agent task accuracy over ReAct baseline
- Demonstrates computational efficiency with FLOPS competitive to search baselines while using inference compute more effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM autoregressive planning is inherently myopic and leads to suboptimal solutions
- Mechanism: LLMs generate each step based only on current context without considering future consequences, causing errors to accumulate
- Core assumption: LLM generation probability at each step is based solely on immediate conditional probability without foresight
- Evidence anchors:
  - [abstract] "LLMs face challenges in ensuring reliable and optimal planning due to their inherent myopic nature of autoregressive decoding"
  - [section 3.1] "LLM autoregressive planning is myopic... a positive gap indicates that the LLM's action selection is myopic for at least one intermediate step"
  - [corpus] Weak - corpus contains related work on code generation and multi-agent systems but no direct myopia analysis
- Break condition: If LLM can perfectly predict future states and incorporate them into current step probability

### Mechanism 2
- Claim: Incorporating foresight through future trajectory evaluation improves planning accuracy
- Mechanism: By sampling multiple future trajectories and reweighting current generation distribution based on their evaluations, the model can avoid early errors and promote global-optimal planning
- Core assumption: LLM can accurately evaluate future trajectories and use this evaluation to improve current decision-making
- Evidence anchors:
  - [abstract] "By re-weighting LLM distributions based on foresight trajectories, Predictive-Decoding aims to mitigate early errors and promote non-myopic planning"
  - [section 4.1] "Our method follows the sampling-importance-resampling (SIR) technique... to achieve the optimization goal"
  - [corpus] Weak - corpus focuses on code generation with world models but doesn't address foresight-based reweighting
- Break condition: If future trajectory evaluation is unreliable or computationally prohibitive

### Mechanism 3
- Claim: Model Predictive Control (MPC) principles enable efficient global-optimal planning without exponential search
- Mechanism: Instead of searching through all possible future trajectories, MPC solves a series of linear optimization problems by sequentially selecting the best action based on current context and limited foresight
- Core assumption: MPC formulation can be adapted to LLM generation by using sampling-importance-resampling to approximate the optimal distribution
- Evidence anchors:
  - [section 2] "Model Predictive Control (MPC) introduces a different paradigm for planning: instead of optimizing an entire action sequence at once, this method selects the best action at each timestep"
  - [section 4.1] "Our decision-making strategy draws inspiration from model predictive control (MPC)"
  - [corpus] Weak - corpus contains related work on agent-guided tree search but not MPC adaptation for LLMs
- Break condition: If MPC formulation cannot be effectively approximated through sampling techniques

## Foundational Learning

- Concept: Autoregressive generation
  - Why needed here: Understanding how LLMs generate sequences token by token is crucial for recognizing why myopic planning occurs
  - Quick check question: Why does autoregressive generation naturally lead to myopic planning behavior?

- Concept: Energy-based models and sampling-importance-resampling (SIR)
  - Why needed here: The method uses SIR technique to sample from a distribution that reflects future trajectory evaluations
  - Quick check question: How does the SIR technique transform an optimization problem into a probability distribution?

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The paper frames planning as a POMDP problem, requiring understanding of states, actions, and rewards
  - Quick check question: How does framing planning as a POMDP problem help identify the limitations of current LLM approaches?

## Architecture Onboarding

- Component map: LLM generator -> Foresight sampler -> Evaluation function -> Reweighting module -> Action selector -> Environment (for agent tasks)
- Critical path: LLM generation -> Foresight sampling -> Future trajectory evaluation -> Distribution reweighting -> Next action selection
- Design tradeoffs: Foresight length vs. computational cost, sampling diversity vs. accuracy, lookahead depth vs. hallucination risk
- Failure signatures: Poor performance on tasks requiring long-range planning, sensitivity to prompt quality, hallucination in future state prediction
- First 3 experiments:
  1. Implement basic foresight sampling without reweighting to verify improvement over baseline
  2. Add trajectory recycling to test efficiency gains
  3. Compare different foresight lengths to find optimal balance between performance and computation

## Open Questions the Paper Calls Out
- How does Predictive-Decoding perform on tasks requiring long-context reasoning beyond the scope of current benchmarks?
- How sensitive is Predictive-Decoding to the quality of the LLM's world model when used in agent tasks?
- Can Predictive-Decoding be effectively integrated with training-based approaches to improve LLM reasoning capabilities?
- What is the optimal balance between foresight length (T0) and sampling number (K) across different task domains?

## Limitations
- Effectiveness depends heavily on quality of foresight trajectory evaluation function, which is task-specific and not fully detailed
- Computational efficiency gains are reported but scaling behavior with larger models and longer horizons remains unclear
- Method introduces additional hyperparameters (T0, K, τ) that require careful tuning for optimal performance

## Confidence
- High confidence in theoretical framework connecting MPC to LLM planning improvements
- Medium confidence in empirical results, given strong performance across diverse benchmarks but limited ablation studies on critical hyperparameters
- Medium confidence in computational efficiency claims, as FLOPS comparisons don't account for wall-clock time variations

## Next Checks
1. Conduct ablation study on foresight parameters (T0, K, τ) to identify individual contributions and establish optimal settings for different task categories
2. Apply method to a novel reasoning task outside paper's scope to validate generalizability beyond demonstrated benchmarks
3. Measure actual wall-clock time and memory usage during inference across different hardware setups to verify claimed computational efficiency