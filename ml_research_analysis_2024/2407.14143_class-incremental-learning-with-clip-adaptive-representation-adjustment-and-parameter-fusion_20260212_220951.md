---
ver: rpa2
title: 'Class-Incremental Learning with CLIP: Adaptive Representation Adjustment and
  Parameter Fusion'
arxiv_id: '2407.14143'
source_url: https://arxiv.org/abs/2407.14143
tags:
- learning
- categories
- pre-trained
- clip
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting in class-incremental
  learning (CIL) by leveraging CLIP's text features and a parameter fusion strategy.
  The key idea is to adjust the image representation of old classes using their similarity
  with new class text features, and then apply a decomposed parameter fusion technique
  to the linear adapter to reduce forgetting.
---

# Class-Incremental Learning with CLIP: Adaptive Representation Adjustment and Parameter Fusion

## Quick Facts
- arXiv ID: 2407.14143
- Source URL: https://arxiv.org/abs/2407.14143
- Authors: Linlan Huang; Xusheng Cao; Haori Lu; Xialei Liu
- Reference count: 40
- Primary result: Achieves state-of-the-art CIL performance using CLIP text features and decomposed parameter fusion

## Executive Summary
This paper addresses catastrophic forgetting in class-incremental learning by leveraging CLIP's fixed text features and a novel parameter fusion strategy. The approach, called RAPF, adjusts old class image representations using semantic similarity with new class text features, and applies decomposed parameter fusion to the adapter module to maintain performance across tasks. Experiments show RAPF outperforms existing methods on CIFAR100, ImageNet100, and ImageNet-R datasets, achieving average accuracies of 87.59% and 80.23% last task accuracy on ImageNet100.

## Method Summary
RAPF freezes CLIP's image and text encoders, fine-tuning only a linear adapter. During each task, text feature distances identify semantically similar old-new class pairs. Old class features are approximated with Gaussian distributions and synthetic samples are generated to maintain separation via hinge loss. After each task, adapter parameters are decomposed using SVD into orthonormal basis and task-specific matrices, then fused using a soft mask based on parameter differences to balance plasticity and stability.

## Key Results
- Achieves 87.59% average accuracy and 80.23% last task accuracy on ImageNet100
- Outperforms previous state-of-the-art methods including PROOF, L2P++, and DualPrompt
- Demonstrates consistent improvements across CIFAR100, ImageNet100, and ImageNet-R datasets
- Shows effectiveness of text-guided representation adjustment and parameter fusion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP's fixed text features provide stable semantic anchors for class boundaries, reducing forgetting.
- Mechanism: Text features from CLIP act as invariant class descriptors, so when new classes are added, similarity between new and old class texts identifies neighboring categories. Adjusting old class image features to maintain separation from similar new classes prevents them from being misclassified as new classes.
- Core assumption: The text encoder in CLIP produces semantically meaningful, fixed embeddings for class names that correlate with visual similarity.
- Evidence anchors:
  - [abstract] "During training for new data, we measure the influence of new classes on old ones and adjust the representations, using textual features."
  - [section 4.2] "Using the text features, we can infer the relationship between the old and new categories."
  - [corpus] Weak. The neighboring papers mention CLIP but do not explicitly validate the use of fixed text features as semantic anchors for CIL.
- Break condition: If text embeddings drift during fine-tuning, or if semantic similarity in text does not correlate with visual similarity, the selection of neighboring categories becomes unreliable.

### Mechanism 2
- Claim: Decomposed parameter fusion stabilizes the adapter by blending task-specific and shared knowledge without increasing parameter count.
- Mechanism: After each task, the adapter parameters from the previous and current tasks are decomposed onto a common orthonormal basis. The difference between task-specific matrices is used to compute a soft mask that weights how much of the new task knowledge to fuse in, balancing plasticity and stability.
- Core assumption: Task-specific parameter changes can be meaningfully decomposed and recombined without losing discriminative power.
- Evidence anchors:
  - [abstract] "After training, we employ a decomposed parameter fusion to further mitigate forgetting during adapter module fine-tuning."
  - [section 4.3] "We decompose Wold to an orthonormal basis B by SVD... We substitute the Rnew and Rold... into the Wnew and Wold... to calculate M."
  - [corpus] Weak. The corpus neighbors mention CLIP-based CIL but do not describe similar decomposition strategies.
- Break condition: If the orthonormal basis does not capture the shared knowledge space effectively, or if the mask M over- or under-fuses, performance degrades.

### Mechanism 3
- Claim: Sampling from Gaussian distributions of old class features approximates replay without storing real data.
- Mechanism: Centroids and covariance matrices of old class embeddings are stored. During training, synthetic samples are drawn from these distributions and passed through the adapter with a hinge loss to separate neighboring classes.
- Core assumption: The Gaussian approximation captures the true feature distribution of old classes sufficiently well for training purposes.
- Evidence anchors:
  - [section 3] "Leveraging the well-learned representations by the pre-trained backbone, we can approximate the feature distribution of each category using a Gaussian distribution."
  - [section 4.2] "We sample the distribution of the old class and calculate the hinge loss."
  - [corpus] Weak. No corpus neighbor explicitly validates this sampling approach for CIL.
- Break condition: If the real data distribution is multimodal or heavy-tailed, the Gaussian approximation fails, leading to poor separation.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks.
  - Why needed here: The paper's goal is to mitigate forgetting in class-incremental learning; understanding the phenomenon is essential.
  - Quick check question: What is the difference between rehearsal and regularization approaches to forgetting?

- Concept: Vision-language pre-training (CLIP) and zero-shot transfer.
  - Why needed here: RAPF relies on CLIP's frozen text encoder and image encoder; understanding how CLIP works is key to grasping the method.
  - Quick check question: How does CLIP's contrastive loss enable zero-shot classification?

- Concept: Linear adapter fine-tuning.
  - Why needed here: RAPF freezes the backbone and only fine-tunes a small linear adapter; knowing how adapter modules work is important.
  - Quick check question: What is the computational benefit of using an adapter instead of fine-tuning the entire model?

## Architecture Onboarding

- Component map:
  CLIP backbone (frozen) -> Linear adapter -> Text similarity module -> Gaussian sampler -> Hinge loss -> Cross-entropy loss combiner

- Critical path:
  1. Compute text feature distances to select neighboring classes.
  2. Sample old class features and compute hinge loss for separation.
  3. Train adapter with combined loss on new and synthetic data.
  4. After task, decompose and fuse parameters for next round.

- Design tradeoffs:
  - Freezing backbone vs fine-tuning: stability vs adaptability.
  - Using synthetic replay vs real exemplars: memory efficiency vs fidelity.
  - Parameter fusion vs distillation: no extra loss terms vs more complex optimization.

- Failure signatures:
  - High confusion between old and new classes → text similarity threshold too low or hinge loss too weak.
  - Rapid forgetting after many tasks → parameter fusion mask not balancing plasticity/stability.
  - Degraded performance on old classes → Gaussian approximation inaccurate.

- First 3 experiments:
  1. Verify that text feature similarity correlates with visual confusion between classes.
  2. Test hinge loss on synthetic samples: does it improve separation without harming accuracy?
  3. Validate decomposed parameter fusion: compare with simple averaging and no fusion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the threshold for selecting neighboring categories (α) impact the balance between reducing forgetting and maintaining model plasticity?
- Basis in paper: [explicit] The paper discusses the threshold α for selecting neighboring categories and its effect on performance, noting that a threshold that is too large will select many pairs of dissimilar categories, which will interfere with the classification and increase the computation.
- Why unresolved: The paper only explores a single threshold value (0.65) and does not provide a systematic analysis of how different thresholds affect the model's performance. A more thorough investigation is needed to determine the optimal threshold for different datasets and settings.
- What evidence would resolve it: Experiments comparing the performance of the model using different threshold values (e.g., 0.5, 0.65, 0.7, 0.8) on multiple datasets and settings, analyzing the trade-off between reducing forgetting and maintaining plasticity.

### Open Question 2
- Question: Can the parameter fusion strategy be further optimized to improve the balance between stability and plasticity?
- Basis in paper: [explicit] The paper proposes a decomposed parameter fusion strategy to reduce forgetting, but notes that a more efficient mechanism to fuse the parameters could be explored in future work.
- Why unresolved: The current parameter fusion strategy is based on the difference between the parameters after training and the parameters from the previous task. However, other approaches, such as using the importance of each parameter or the gradient information, could potentially lead to better results.
- What evidence would resolve it: Experiments comparing the performance of the model using different parameter fusion strategies (e.g., using parameter importance, gradient information, or a combination of methods) on multiple datasets and settings, analyzing the trade-off between stability and plasticity.

### Open Question 3
- Question: How does the proposed method perform in more complex and realistic scenarios, such as those with large-scale datasets or diverse data distributions?
- Basis in paper: [explicit] The paper evaluates the proposed method on several conventional benchmarks (CIFAR100, ImageNet100, ImageNet-R, and CUB200) and shows that it achieves state-of-the-art results. However, the paper does not explore more complex and realistic scenarios.
- Why unresolved: The performance of the proposed method in more complex and realistic scenarios is unknown. It is possible that the method may not scale well to large-scale datasets or may not be robust to diverse data distributions.
- What evidence would resolve it: Experiments evaluating the proposed method on large-scale datasets (e.g., ImageNet1K) and datasets with diverse data distributions (e.g., domain adaptation datasets). Analyzing the performance of the method in terms of accuracy, forgetting, and computational efficiency.

## Limitations
- Decomposed parameter fusion approach lacks direct empirical validation and mathematical rigor
- Gaussian sampling approximation may fail on multimodal or heavy-tailed distributions
- Fixed text similarity threshold (0.65) without justification for optimality across datasets

## Confidence
- High: CLIP's fixed text features provide semantic anchors (supported by CLIP's known zero-shot properties)
- Medium: Decomposed parameter fusion reduces forgetting (method described but not independently validated)
- Low: Gaussian sampling accurately approximates old class distributions (no distributional analysis provided)

## Next Checks
1. Test robustness of text similarity threshold by sweeping from 0.5-0.8 and measuring forgetting on ImageNet100
2. Compare Gaussian sampling with exemplar replay on a subset to quantify approximation error
3. Evaluate decomposed fusion vs. simple averaging and elastic weight consolidation on task ordering sensitivity