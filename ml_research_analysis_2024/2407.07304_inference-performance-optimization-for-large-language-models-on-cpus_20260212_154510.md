---
ver: rpa2
title: Inference Performance Optimization for Large Language Models on CPUs
arxiv_id: '2407.07304'
source_url: https://arxiv.org/abs/2407.07304
tags:
- performance
- inference
- optimization
- cpus
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a CPU-based inference optimization solution
  for large language models (LLMs) that addresses the challenge of deploying LLMs
  in low-resource environments. The solution introduces three key optimizations: SlimAttention
  for attention layer efficiency, an INT8 KV cache approach for memory reduction,
  and distributed inference optimization using oneAPI Collective Communications Library.'
---

# Inference Performance Optimization for Large Language Models on CPUs

## Quick Facts
- arXiv ID: 2407.07304
- Source URL: https://arxiv.org/abs/2407.07304
- Reference count: 5
- Primary result: CPU-based inference optimizations achieve 2.85x latency reduction for Llama2-70B with 8 sockets

## Executive Summary
This paper presents a comprehensive CPU-based inference optimization solution for large language models, addressing the challenge of deploying LLMs in low-resource environments. The authors introduce three key optimizations: SlimAttention for attention layer efficiency, INT8 KV cache for memory reduction, and distributed inference optimization using oneAPI Collective Communications Library. Experiments on Intel Xeon CPU 8563C demonstrate significant performance improvements, including superior throughput for batch processing and better performance than FlashAttention on CPU.

## Method Summary
The method introduces three optimization approaches for LLM inference on CPUs. SlimAttention uses one-dimensional decomposition of query-key scores to reduce memory overhead and eliminate redundant computations. The INT8 KV cache approach maintains unique scale values per token and head, converting INT8 cache data to FP32 during MatMul operations using AVX512 instructions. For distributed inference, the solution broadcasts token IDs instead of embeddings and performs reduction after top-k computation, with zero-copy implementation for shared memory between computation and communication modules.

## Key Results
- 2.85x latency reduction for Llama2-70B with 8 sockets compared to 2 sockets
- Superior throughput for batch processing compared to baseline implementations
- SlimAttention demonstrates better performance than FlashAttention on CPU architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SlimAttention reduces KV cache memory usage and improves computational efficiency on CPUs.
- Mechanism: Uses one-dimensional decomposition of query-key scores, requiring each thread to maintain only a buffer of block size instead of a full tile.
- Core assumption: CPU architectures benefit more from reduced memory footprint than from parallel tile-based computations optimized for GPUs.
- Evidence anchors:
  - [abstract] "Compared with FlashAttention, SlimAttention entails no redundant computations but does necessitate a larger intermediate buffer."
  - [section 2.1] "In terms of computation sequence, it involves first calculating a horizontal score, followed by applying softmax to the score... each thread needs only to maintain a buffer of the block size, thereby reducing memory usage and enhancing computational efficiency."
  - [corpus] Weak correlation - neighbor papers discuss CPU inference but don't specifically analyze SlimAttention's mechanism.

### Mechanism 2
- Claim: INT8 KV cache optimization significantly reduces memory bandwidth bottlenecks during inference.
- Mechanism: Maintains unique scale values per token and head, converting INT8 cache data to FP32 during MatMul operations using AVX512 instructions.
- Core assumption: The precision loss from INT8 quantization is acceptable when combined with per-token/head scaling, and the CPU can efficiently handle dynamic INT8→FP32 conversion.
- Evidence anchors:
  - [section 2.2] "To enhance performance by effectively reducing the KV cache size, we have implemented an INT8 KV cache approach... we maintain a unique scale for each token and each head... we engineered a custom kernel capable of supporting MatMul operations with hybrid data types."
  - [section 2.2] "This approach allows for a more efficient use of memory without significantly compromising the quality of the model's output."
  - [corpus] Weak correlation - neighbor papers discuss CPU optimization but don't specifically address INT8 KV cache strategies.

### Mechanism 3
- Claim: Distributed inference with oneCCL and zero-copy optimization reduces communication overhead and improves scalability.
- Mechanism: Broadcasts token IDs instead of full embeddings and performs reduction after top-k computation rather than reducing all logits. Zero-copy implementation allows computation and communication modules to share memory locations.
- Core assumption: Broadcasting smaller token IDs and reducing partial results is more efficient than traditional all-reduce approaches on CPU clusters.
- Evidence anchors:
  - [section 2.3] "In the proposed solution, our solution broadcasts token IDs instead of broadcasting the values of the Embedding part obtained based on token IDs. We perform the reduction after each worker computes the top-k, rather than directly reducing the logits of all tokens."
  - [section 2.3] "This involves the computation module, during its last operation before communication, directly writing the results to the location of the communication module, achieving a zero-copy implementation."
  - [section 3.2] "With 4 bare-mental machines (8 sockets) running the proposed distributed solution, it could bring 2.85X performance gain for Llama2-70B compared with 1 bare-mental machine (2 sockets)."

## Foundational Learning

- Concept: Transformer attention mechanism and its quadratic complexity
  - Why needed here: Understanding why attention optimization is critical for long sequences and why SlimAttention's one-dimensional approach differs from FlashAttention's two-dimensional approach
  - Quick check question: What is the computational complexity of standard self-attention with respect to sequence length, and how does block-wise attention reduce this?

- Concept: CPU memory hierarchy and SIMD instruction sets
  - Why needed here: Understanding how AVX512 instructions enable efficient INT8→FP32 conversion and why CPU-specific optimizations differ from GPU approaches
  - Quick check question: How do AVX512 FMA instructions differ from traditional multiply-add operations in terms of throughput and latency on modern CPUs?

- Concept: Distributed systems communication patterns and collective operations
  - Why needed here: Understanding why token ID broadcasting and partial reduction are more efficient than traditional all-reduce approaches in the LLM inference context
  - Quick check question: What is the communication complexity difference between broadcasting embeddings versus token IDs in a distributed inference setup?

## Architecture Onboarding

- Component map:
  - SlimAttention: CPU-optimized attention computation replacing standard multi-head attention
  - INT8 KV cache: Memory-optimized cache with per-token/head scaling and hybrid data type support
  - oneCCL distributed runtime: Communication layer for multi-socket/multi-machine inference
  - Custom MatMul kernels: AVX512-optimized matrix multiplication with dynamic type conversion
  - Model support layer: Interfaces for Qwen, Llama, ChatGLM, Baichuan, and Opt series

- Critical path: Token generation loop → SlimAttention computation → INT8 KV cache access → MatMul with type conversion → Communication (if distributed) → Top-k sampling

- Design tradeoffs:
  - SlimAttention vs FlashAttention: Reduced memory usage but potentially lower parallelism on CPUs
  - INT8 quantization vs FP16: Better memory efficiency but requires careful scaling to maintain quality
  - Token ID broadcasting vs embedding broadcasting: Lower bandwidth but requires additional mapping overhead
  - Zero-copy vs traditional copy: Better performance but tighter coupling between computation and communication

- Failure signatures:
  - SlimAttention: Memory allocation failures for large block buffers, incorrect softmax calculations due to numerical precision
  - INT8 KV cache: Model quality degradation from poor scaling, performance regression on CPUs without AVX512
  - Distributed inference: Communication deadlocks, incorrect token ID mapping, synchronization issues in zero-copy mode
  - Overall: Latency spikes from cache thrashing, throughput drops from NUMA effects in multi-socket configurations

- First 3 experiments:
  1. Single-socket performance baseline: Run Llama2-7B with standard attention and FP16 KV cache to establish baseline latency/throughput
  2. SlimAttention validation: Replace attention layer and measure performance improvement while verifying numerical correctness against baseline
  3. INT8 KV cache evaluation: Enable INT8 cache with scaling and measure memory usage reduction and any quality impact using perplexity or generation quality metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SlimAttention approach scale with extremely long sequence lengths compared to traditional attention mechanisms?
- Basis in paper: [explicit] The paper mentions that SlimAttention is proposed to optimize attention for long sequence inputs, stating "optimizing attention is particularly important for long sequence inputs."
- Why unresolved: The paper only provides performance comparisons for sequence lengths up to 4096 tokens. It does not explore how the approach performs with extremely long sequences (e.g., 16k, 32k, or longer) which are becoming increasingly common in modern LLMs.
- What evidence would resolve it: Empirical benchmarks comparing SlimAttention performance with traditional attention mechanisms across a wide range of sequence lengths (1k, 2k, 4k, 8k, 16k, 32k) would provide insights into scaling behavior and potential bottlenecks.

### Open Question 2
- Question: What is the impact of the INT8 KV cache approach on model accuracy and output quality for different types of tasks (e.g., reasoning vs. generation)?
- Basis in paper: [explicit] The paper states "we implement an INT8 KV cache approach... This approach allows for a more efficient use of memory without significantly compromising the quality of the model's output."
- Why unresolved: The paper mentions that the INT8 approach maintains output quality but does not provide quantitative measurements of accuracy degradation across different task types. The trade-off between performance gains and accuracy loss remains unclear.
- What evidence would resolve it: Comprehensive accuracy benchmarks comparing FP16/BF16 models with INT8 models across multiple task categories (e.g., MMLU, HellaSwag, arithmetic reasoning, creative writing) would quantify the impact on different types of outputs.

### Open Question 3
- Question: How does the proposed distributed inference solution perform on heterogeneous CPU clusters with different core counts and frequencies?
- Basis in paper: [inferred] The paper presents distributed inference optimization using oneAPI Collective Communications Library but only tests on homogeneous systems with identical CPUs.
- Why unresolved: Real-world deployments often involve heterogeneous clusters where different machines have varying specifications. The paper does not address load balancing, communication overhead, or performance scaling in such scenarios.
- What evidence would resolve it: Performance benchmarks comparing distributed inference across heterogeneous clusters (mixing different CPU models, core counts, and frequencies) would reveal the solution's robustness and identify potential bottlenecks in mixed environments.

## Limitations

- Performance gains may vary significantly on different CPU architectures beyond the tested Intel Xeon 8563C platform
- The paper does not provide extensive quality evaluation metrics to validate INT8 quantization's impact on model accuracy
- SlimAttention's superiority over FlashAttention on CPU is claimed but not rigorously demonstrated with head-to-head comparisons

## Confidence

- **High confidence**: Claims about memory reduction from INT8 KV cache and performance improvements in distributed inference scenarios are well-supported by the experimental setup and results.
- **Medium confidence**: The SlimAttention mechanism's performance benefits are supported by the described architecture but lack comprehensive benchmarking against alternative approaches.
- **Low confidence**: Claims about the solution's broad applicability across different CPU architectures and LLM variants are not sufficiently validated.

## Next Checks

1. **Quality Preservation Validation**: Run perplexity and generation quality tests comparing INT8 KV cache with FP16/BF16 baselines across multiple LLM models and tasks to quantify any degradation from quantization.

2. **Cross-Platform Performance Testing**: Evaluate the proposed optimizations on different CPU architectures (AMD EPYC, ARM-based servers) to assess generalizability and identify platform-specific bottlenecks or limitations.

3. **SlimAttention vs FlashAttention Benchmark**: Conduct controlled head-to-head comparisons of SlimAttention and FlashAttention on the same CPU platform with varying sequence lengths and batch sizes to validate the claimed performance benefits and identify scenarios where each approach excels.