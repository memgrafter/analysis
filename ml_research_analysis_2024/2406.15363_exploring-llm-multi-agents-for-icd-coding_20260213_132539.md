---
ver: rpa2
title: Exploring LLM Multi-Agents for ICD Coding
arxiv_id: '2406.15363'
source_url: https://arxiv.org/abs/2406.15363
tags:
- codes
- coding
- agent
- code
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a multi-agent system for automated ICD coding
  that mimics real-world healthcare workflows, comprising five specialized agents
  (patient, physician, coder, reviewer, and adjuster) that interact to assign diagnostic
  codes to clinical notes. The system leverages the SOAP structure of electronic health
  records to enable self-correction and reduce hallucinations.
---

# Exploring LLM Multi-Agents for ICD Coding

## Quick Facts
- arXiv ID: 2406.15363
- Source URL: https://arxiv.org/abs/2406.15363
- Authors: Rumeng Li; Xun Wang; Hong Yu
- Reference count: 0
- Primary result: Multi-agent system with five specialized agents outperforms zero-shot chain-of-thought prompting and self-consistency methods on ICD coding, achieving comparable results to state-of-the-art methods without extensive pre-training.

## Executive Summary
This study introduces a multi-agent system for automated ICD coding that mimics real-world healthcare workflows, comprising five specialized agents (patient, physician, coder, reviewer, and adjuster) that interact to assign diagnostic codes to clinical notes. The system leverages the SOAP structure of electronic health records to enable self-correction and reduce hallucinations. Evaluated on the MIMIC-III dataset, the proposed multi-agent approach outperforms zero-shot chain-of-thought prompting and self-consistency methods, achieving comparable results to state-of-the-art ICD coding methods that require extensive pre-training or fine-tuning.

## Method Summary
The method employs five LLM agents with specialized roles that mimic real-world healthcare workflows: patient, physician, coder, reviewer, and adjuster. The physician optionally converts clinical notes to SOAP format, generating assessment and plan sections that are compared against gold standards for self-correction. The coder assigns codes with the goal of maximizing coverage while the patient reviews to prevent overbilling. The reviewer verifies codes and the adjuster resolves conflicts. The system uses GPT-4 API with external code descriptions injected as knowledge, and is evaluated on MIMIC-III top 50 codes using Micro-F1 and Macro-F1 metrics.

## Key Results
- Multi-agent approach outperforms zero-shot chain-of-thought and self-consistency baselines on both common and rare ICD codes
- System achieves comparable results to state-of-the-art ICD coding methods that require extensive pre-training or fine-tuning
- SOAP structure integration demonstrates superior performance on rare code accuracy and provides enhanced explainability
- Ablation studies validate the effectiveness of designated agent roles and the confrontation strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The SOAP structure integration enables self-correction by comparing generated assessment/plan sections against gold standards, reducing hallucinations.
- Mechanism: The physician agent first generates assessment and plan sections based only on subjective and objective information. By comparing these generated sections with the gold standard assessment and plan, the model can identify inconsistencies and self-correct before proceeding to code assignment.
- Core assumption: The assessment and plan sections contain sufficient information to detect discrepancies in the generated content, and the LLM can accurately identify and correct these inconsistencies.
- Evidence anchors:
  - [abstract]: "We feed them the subjective and objective sections, and prompt them to generate the assessment and plan sections. Then we instruct them to compare their own generation with the gold standard, and self-correct any hallucinations."
  - [section]: "Our method consists of two steps: first, we use an LLM to convert the discharge summary notes into the SOAP format; second, we apply our agents to perform the ICD coding task based on the following workflows."
  - [corpus]: Weak evidence - no direct mention of SOAP structure in corpus papers, suggesting this may be a novel contribution.
- Break condition: If the assessment and plan sections don't contain sufficient diagnostic information, or if the LLM cannot accurately detect inconsistencies between generated and gold standard sections.

### Mechanism 2
- Claim: The confrontation strategy between agents improves code coverage by creating competing incentives.
- Mechanism: The coder agent is instructed to assign as many codes as possible to optimize payment for healthcare facilities, while the patient agent reviews codes to prevent overbilling. This competition creates a balanced outcome that maximizes both coverage and accuracy.
- Core assumption: Agents can be effectively prompted to adopt conflicting roles that create productive tension, leading to better final outcomes than a single agent approach.
- Evidence anchors:
  - [abstract]: "We designed multi-agents to accomplish the whole process. We observe that this reasoning and self-correction process improves the ICD prediction performance."
  - [section]: "1. Confrontation Strategy: The coder (or the physician in method II) is instructed to assign as many codes as possible to reflect all the services rendered and also optimize the payment for the health care facilities, while the patient is encouraged to review the codes to prevent being overbilled."
  - [corpus]: Weak evidence - no direct mention of confrontation strategy in corpus papers, suggesting this may be a novel contribution.
- Break condition: If agents cannot be effectively prompted to adopt their designated roles, or if the confrontation creates deadlock rather than productive tension.

### Mechanism 3
- Claim: Specialized agent roles mimic real-world healthcare workflows, improving explainability and reliability.
- Mechanism: Each agent (patient, physician, coder, reviewer, adjuster) has a specific role that mirrors actual healthcare processes. This specialization allows each agent to focus on relevant information and reasoning patterns, while the adjuster serves as a final arbiter for conflicts.
- Core assumption: Real-world healthcare workflows can be effectively translated into agent-based computational processes, and that this translation improves performance over monolithic approaches.
- Evidence anchors:
  - [abstract]: "We introduce an innovative multi-agent approach for ICD coding which mimics the ICD coding assignment procedure in real-world settings, comprising five distinct agents: the patient, physician, coder, reviewer, and adjuster."
  - [section]: "The multi-agent system consisted of five participants as stated above: a patient, a physician, a coder, a reviewer, and an adjuster. It mimics the coding procedure in a large healthcare system."
  - [corpus]: Moderate evidence - several corpus papers mention multi-agent approaches for medical coding, though the specific role structure differs.
- Break condition: If the real-world workflow mapping doesn't translate well to computational processes, or if agent specialization creates bottlenecks rather than improvements.

## Foundational Learning

- Concept: Multi-label classification with long-tail distribution
  - Why needed here: ICD coding involves assigning multiple codes to each clinical note, with a highly skewed distribution where common codes dominate and rare codes are underrepresented.
  - Quick check question: Why does a long-tail distribution make ICD coding particularly challenging for machine learning models?

- Concept: Chain-of-thought prompting and self-consistency
- Why needed here: These are the baseline methods against which the multi-agent approach is compared, and understanding them is crucial for appreciating the improvements.
  - Quick check question: How does chain-of-thought prompting differ from standard prompting, and what problem does self-consistency address?

- Concept: Role-based prompting and agent specialization
  - Why needed here: The multi-agent system relies on assigning specific roles to different agents, each with tailored prompts and responsibilities.
  - Quick check question: What are the key differences between role-based prompting and general prompting strategies?

## Architecture Onboarding

- Component map: Patient -> Physician -> Coder -> Reviewer -> Adjuster
- Critical path: SOAP conversion (if used) → physician generates assessment/plan → physician assigns codes → reviewer verifies codes → patient reviews codes → adjuster resolves conflicts if needed.
- Design tradeoffs: The multi-agent approach trades computational overhead (multiple LLM calls) for improved accuracy and explainability. The SOAP structure adds preprocessing steps but enables self-correction.
- Failure signatures: Performance degradation when agents fail to adopt their roles, conflicts that cannot be resolved by the adjuster, or when the SOAP conversion fails to capture relevant information.
- First 3 experiments:
  1. Compare single-agent vs. multi-agent performance on MIMIC-III top 50 codes to validate the benefit of specialization.
  2. Test the impact of SOAP structure by running both MAC-I (without SOAP) and MAC-II (with SOAP) on the same dataset.
  3. Evaluate the confrontation strategy by comparing results with and without the patient agent's review step.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of the multi-agent system compare when using open-source LLM models versus the proprietary GPT-4 model, particularly in terms of accuracy, hallucination reduction, and explainability?
- Basis in paper: [inferred] The paper acknowledges that proprietary models like GPT-4 are used for ethical and security reasons, and suggests that open-source models are recommended for deployment in healthcare platforms.
- Why unresolved: The paper only uses GPT-4 and does not explore the performance of open-source alternatives, which could offer more flexibility and transparency in healthcare applications.
- What evidence would resolve it: Direct comparison of the multi-agent system's performance using both GPT-4 and leading open-source LLMs (e.g., Llama, Mistral) on the same ICD coding tasks, with metrics for accuracy, hallucination rates, and explainability.

### Open Question 2
- Question: What is the optimal number and configuration of agents for the multi-agent system to maximize ICD coding performance, and how does this vary with different types of clinical notes or coding scenarios?
- Basis in paper: [explicit] The paper presents ablation studies showing the impact of individual agents, but does not explore different configurations or numbers of agents.
- Why unresolved: While the paper identifies key agents, it does not systematically investigate how changing the number or roles of agents affects performance across diverse clinical scenarios.
- What evidence would resolve it: Experimental results comparing the performance of the multi-agent system with varying numbers and configurations of agents on multiple clinical note datasets and coding scenarios.

### Open Question 3
- Question: How can the multi-agent system be adapted to handle the full range of ICD-10 codes, and what are the specific challenges and performance implications of scaling from ICD-9 to ICD-10?
- Basis in paper: [explicit] The paper mentions that the method allows for embedding external knowledge within context limits and conducted experiments excluding top-50/rare ICD-9 constraints, but does not address ICD-10 specifically.
- Why unresolved: The paper focuses on ICD-9 and does not explore the complexities and performance impacts of extending the system to the larger and more detailed ICD-10 code set.
- What evidence would resolve it: Implementation and evaluation of the multi-agent system on a comprehensive ICD-10 coding task, with analysis of performance metrics, challenges encountered, and strategies for managing the increased code complexity.

## Limitations

- Evaluation relies heavily on MIMIC-III dataset, which may not generalize to other clinical settings or coding systems
- Performance comparison against pre-trained models may not be entirely fair due to different training approaches
- Study doesn't address potential biases in LLM's medical knowledge or performance on extremely rare codes

## Confidence

- **High confidence**: The multi-agent system outperforms chain-of-thought and self-consistency baselines on both common and rare ICD codes. The SOAP structure integration demonstrably reduces hallucinations through self-correction.
- **Medium confidence**: The confrontation strategy meaningfully improves code coverage without sacrificing accuracy. The explainability benefits of the multi-agent approach are significant but may vary with different clinical note types.
- **Low confidence**: The computational efficiency claims are not fully supported, as the study doesn't provide detailed runtime comparisons. The generalizability to other coding systems (ICD-10) or clinical contexts remains untested.

## Next Checks

1. **Cross-dataset validation**: Test the multi-agent system on a completely different clinical dataset (e.g., MDACE Profee or other hospital systems) to assess generalizability beyond MIMIC-III.
2. **Computational efficiency analysis**: Measure and compare the actual inference time and cost per document for the multi-agent system versus baseline methods, including API call overhead.
3. **Extreme rare code performance**: Evaluate the system's performance on the rarest 10-20 codes in MIMIC-III to determine if the improvements hold for the most challenging cases.