---
ver: rpa2
title: 'Quam: Adaptive Retrieval through Query Affinity Modelling'
arxiv_id: '2410.20286'
source_url: https://arxiv.org/abs/2410.20286
tags:
- retrieval
- documents
- graph
- affinity
- adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of bounded recall in traditional
  re-ranking pipelines, where the final results are limited by the initial retrieval
  stage. To overcome this, the authors propose Quam, an adaptive retrieval approach
  that leverages query affinity modeling to improve recall.
---

# Quam: Adaptive Retrieval through Query Affinity Modelling

## Quick Facts
- arXiv ID: 2410.20286
- Source URL: https://arxiv.org/abs/2410.20286
- Reference count: 40
- The paper addresses bounded recall in re-ranking pipelines through adaptive retrieval using query affinity modeling.

## Executive Summary
Quam addresses the fundamental limitation of bounded recall in traditional re-ranking pipelines by introducing an adaptive retrieval approach that leverages query affinity modeling. The method constructs a document affinity graph based on learned edge weights from co-relevance information, allowing for more informed selection of candidate documents for re-ranking. By using a query affinity model that estimates the affinity of documents to a ranked set, Quam prioritizes documents with high set affinity scores, significantly improving recall beyond what traditional re-ranking can achieve.

The approach demonstrates substantial improvements over existing baselines, achieving up to 26% better recall and 12% better nDCG compared to standard re-ranking pipelines on TREC DL19 and DL20 datasets. Additionally, Quam proves to be robust to dense corpus graphs and maintains comparable or better efficiency compared to existing adaptive retrieval methods, making it a practical solution for improving retrieval effectiveness.

## Method Summary
Quam is an adaptive retrieval approach that improves recall by leveraging query affinity modeling. The method constructs a document affinity graph using learned edge weights from co-relevance information, then employs a query affinity model (SetAff) to estimate the affinity of documents to a ranked set. The algorithm alternates between initial ranking and graph neighbor expansion, using SetAff scores to select high-priority candidates for re-ranking. The approach uses MonoT5 scores as refined relevance estimates for SetAff calculation and demonstrates effectiveness across both sparse (BM25) and dense (TCT) retrievers on the TREC DL19 and DL20 datasets.

## Key Results
- Quam improves recall by up to 26% and nDCG by up to 12% compared to standard re-ranking pipelines on TREC DL19 and DL20 datasets
- The approach is robust to dense corpus graphs and maintains comparable or better efficiency compared to existing adaptive retrieval methods
- Significant performance gains are observed across both sparse (BM25) and dense (TCT) retriever configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quam improves recall by using affinity scores to prioritize neighbors during adaptive retrieval.
- Mechanism: Learnt-affinity model (ùëì) predicts co-relevance between documents and assigns edge weights to corpus graph edges. Query affinity model (SetAff) then uses these weights to score candidate neighbors based on their expected relevance to already re-ranked documents.
- Core assumption: Documents that are co-relevant to the same queries will have high affinity scores, and these scores correlate with relevance to new queries.
- Evidence anchors:
  - [abstract] "query affinity model that exploits the relevance-aware document similarity graph to improve recall"
  - [section 3.2] "train a modelf (learnt-affinity model) that learns affinity score between the pairs of documents"
  - [corpus] Missing: no direct comparison of affinity scores vs actual relevance across held-out queries.

### Mechanism 2
- Claim: Alternating between initial ranking and graph neighbors allows efficient exploration of relevant documents.
- Mechanism: Quam alternates between scoring from the initial pool (ùëÖ0) and the affinity graph frontier (ùêπ), using SetAff scores to select high-priority neighbors. This balances exploration of missed relevant documents against computational budget.
- Core assumption: Relevant documents are likely to be near other relevant ones in the affinity graph (Cluster Hypothesis), and SetAff effectively ranks these neighbors.
- Evidence anchors:
  - [section 3.4] "alternate between initial ranking and the neighbors to select the documents for re-ranking until we reach the re-ranking budget"
  - [section 4.2.3] "Gar is an adaptive re-ranking approach that alternates between initial retrieved documents and neighbors of these documents in the corpus graph"
  - [corpus] Weak: no ablation showing impact of alternation strategy vs other selection methods.

### Mechanism 3
- Claim: Using re-ranking scores as relevance estimates improves SetAff accuracy over retrieval scores.
- Mechanism: Equation 2 uses ùúô(ùëû,ùëë) (MonoT5 scores) as ùëÉ(ùëÖùëíùëô(ùëë‚Ä≤)), which are more refined relevance estimates than initial retrieval scores.
- Core assumption: MonoT5 scores better capture fine-grained relevance than BM25 scores, making them superior for SetAff calculation.
- Evidence anchors:
  - [section 3.3] "we can use the re-ranking scores as better relevance estimates in comparison to retrieval scores or rank positions"
  - [section 5.1] "we observe that both Gar Ret and Quam Ret degrade when the retriever scores are used"
  - [corpus] Weak: only tested on TREC DL datasets; generalization to other domains unclear.

## Foundational Learning

- Concept: Document affinity modeling
  - Why needed here: Quam requires understanding that co-relevance between documents can be learned and used to guide retrieval.
  - Quick check question: What is the difference between document relevance and document affinity in the context of Quam?

- Concept: Graph-based adaptive retrieval
  - Why needed here: Quam builds on existing adaptive retrieval frameworks that use corpus graphs to expand retrieval beyond initial results.
  - Quick check question: How does Quam's use of SetAff scores differ from Gar's simple neighbor selection strategy?

- Concept: Cross-encoder relevance modeling
  - Why needed here: The learnt-affinity model uses a cross-encoder (BERT-base) to predict document-document affinity, requiring understanding of cross-encoder architecture and training.
  - Quick check question: Why might a cross-encoder be preferred over a bi-encoder for the affinity prediction task?

## Architecture Onboarding

- Component map:
  - Learnt-affinity model (cross-encoder BERT-base) ‚Üí affinity graph edges
  - Query affinity model (SetAff calculation) ‚Üí neighbor selection
  - Alternating scheduler ‚Üí balance initial and graph-based retrieval
  - MonoT5 ranker ‚Üí relevance scoring and SetAff inputs

- Critical path: Learnt-affinity model training ‚Üí affinity graph construction ‚Üí SetAff calculation during query processing ‚Üí MonoT5 re-ranking

- Design tradeoffs:
  - Higher graph depth ùëò improves recall but increases noise and computation
  - Larger batch size ùëè improves hardware utilization but may waste budget on low-affinity candidates
  - Using MonoT5 scores for SetAff improves accuracy but adds re-ranking overhead

- Failure signatures:
  - Low recall despite high graph depth ‚Üí affinity model may not generalize
  - Degraded performance with larger ùëò ‚Üí graph may contain too much noise
  - High latency ‚Üí SetAff computation or alternating overhead may be too costly

- First 3 experiments:
  1. Ablation: Compare Quam with and without affinity graph (using Gar baseline) to isolate Laff score impact
  2. Sensitivity: Vary graph depth ùëò and batch size ùëè to find optimal settings for different budgets
  3. Efficiency: Measure latency overhead of SetAff computation vs recall gains to assess practical viability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Quam change when using different types of document affinity graphs, such as those constructed from more complex or specialized similarity metrics?
- Basis in paper: [explicit] The paper mentions using affinity graphs based on BM25 and TCT retrievers, but does not explore other types of similarity metrics or more complex graph constructions.
- Why unresolved: The paper only evaluates Quam with affinity graphs constructed from BM25 and TCT retrievers, leaving the impact of other similarity metrics or more complex graph constructions unexplored.
- What evidence would resolve it: Conducting experiments with affinity graphs constructed using various similarity metrics (e.g., semantic embeddings, topic models) or more complex graph constructions (e.g., incorporating document metadata) and comparing their performance to the current affinity graphs.

### Open Question 2
- Question: Can the Quam approach be extended to handle more structured retrieval tasks, such as those involving entity-based or faceted search?
- Basis in paper: [inferred] The paper mentions the potential for extending adaptive retrieval to more structured retrieval settings in the future, but does not provide concrete methods or experiments.
- Why unresolved: The paper focuses on ad-hoc retrieval tasks and does not explore the application of Quam to more structured retrieval tasks, leaving the feasibility and effectiveness of such an extension unclear.
- What evidence would resolve it: Developing and evaluating Quam variants tailored to structured retrieval tasks, such as entity-based or faceted search, and comparing their performance to existing methods in those domains.

### Open Question 3
- Question: How does the performance of Quam vary with different document corpus sizes or characteristics, such as the presence of duplicate or near-duplicate documents?
- Basis in paper: [explicit] The paper mentions the potential impact of document corpus characteristics on adaptive retrieval performance but does not provide specific experiments or analyses.
- Why unresolved: The paper evaluates Quam on a specific document corpus (MSMARCO passage corpus) without exploring how its performance might change with different corpus sizes or characteristics.
- What evidence would resolve it: Conducting experiments with Quam on document corpora of varying sizes and characteristics, including those with duplicate or near-duplicate documents, and analyzing the impact on its performance.

### Open Question 4
- Question: What are the potential trade-offs between the effectiveness and efficiency of Quam when using different batch sizes or graph depths, and how can these trade-offs be optimized for specific use cases?
- Basis in paper: [explicit] The paper discusses the effect of batch size and graph depth on Quam's performance but does not provide a comprehensive analysis of the trade-offs between effectiveness and efficiency.
- Why unresolved: While the paper explores the impact of batch size and graph depth on Quam's performance, it does not provide a detailed analysis of the trade-offs between effectiveness and efficiency or guidance on how to optimize these parameters for specific use cases.
- What evidence would resolve it: Conducting experiments to systematically analyze the trade-offs between effectiveness and efficiency of Quam under different batch sizes and graph depths, and developing guidelines or heuristics for parameter optimization based on specific use case requirements.

## Limitations

- The approach relies heavily on the quality of pseudo co-relevant pairs from MSMARCO training data, with potential domain shift issues not adequately addressed
- Computational overhead of the alternating retrieval strategy may vary significantly based on hardware configuration and corpus characteristics
- Long-term stability of the learned affinity model and performance on out-of-distribution queries remains unclear

## Confidence

- **High Confidence**: The core mechanism of using SetAff scores for neighbor selection is well-supported by theoretical framework and experimental results. The alternating strategy between initial and graph-based retrieval is clearly defined and validated.
- **Medium Confidence**: The improvement in recall and nDCG metrics is demonstrated on tested datasets, but robustness across different domains and query distributions needs further validation. The claim of comparable efficiency to existing methods is supported but lacks detailed breakdown.
- **Low Confidence**: The long-term stability of the learned affinity model and its performance on out-of-distribution queries is not adequately addressed. The impact of different graph construction parameters on final performance remains unclear.

## Next Checks

1. **Domain Generalization Test**: Evaluate Quam's performance on datasets from different domains (e.g., biomedical or legal) to assess the robustness of the learned affinity model and SetAff scoring across diverse query distributions.

2. **Efficiency Profiling**: Conduct detailed computational analysis of each component in the Quam pipeline, including SetAff calculation overhead and alternating strategy efficiency, to validate the claimed comparable performance to existing methods.

3. **Affinity Model Stability**: Perform a stability analysis of the learned affinity model by testing on progressively out-of-distribution queries and measuring performance degradation to understand the model's limitations and failure modes.