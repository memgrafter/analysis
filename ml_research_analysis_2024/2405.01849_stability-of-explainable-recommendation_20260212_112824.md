---
ver: rpa2
title: Stability of Explainable Recommendation
arxiv_id: '2405.01849'
source_url: https://arxiv.org/abs/2405.01849
tags:
- explainable
- arxiv
- recommendation
- explanations
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the stability of explainable recommendation\
  \ models under noise. It evaluates three state-of-the-art feature-aware explainable\
  \ recommenders\u2014CER, A2CF, and EFM\u2014on two datasets (Electronics and Yelp)\
  \ by adding random and adversarial noise to model parameters."
---

# Stability of Explainable Recommendation

## Quick Facts
- arXiv ID: 2405.01849
- Source URL: https://arxiv.org/abs/2405.01849
- Authors: Sairamvinay Vijayaraghavan; Prasant Mohapatra
- Reference count: 40
- Key outcome: All three evaluated explainable recommender models show decreasing explainability performance under noise, with adversarial attacks causing more severe degradation than random noise

## Executive Summary
This paper investigates the stability of explainable recommendation models under noise attacks. The authors evaluate three state-of-the-art feature-aware explainable recommenders (CER, A2CF, and EFM) on Amazon Electronics and Yelp datasets by adding random and adversarial noise to model parameters. Results show that all models exhibit decreasing explainability performance (measured via Precision, Recall, and F1 scores) as noise levels increase, with adversarial noise causing more severe degradation. This demonstrates that explainable recommenders are vulnerable to attacks that can potentially manipulate recommendations through feature adjustments.

## Method Summary
The study evaluates three explainable recommender models (CER, A2CF, EFM) on Electronics and Yelp datasets. Models are trained with both random Gaussian noise and FGSM adversarial noise applied to embeddings and weights. Noise levels range from 0 to 1 (Yelp) or 0 to 2 (Electronics). Evaluation uses NDCG@100 for recommendation quality and Precision/Recall/F1@5,5 for feature-level explanation accuracy. Datasets are preprocessed to retain users with ≥20 (Yelp) or ≥10 (Electronics) reviews, with test sets containing last 5 interactions and 100 negatives per user.

## Key Results
- All three models show decreasing F1@5,5 scores as noise levels increase
- CER's F1@5,5 drops from 0.059 (vanilla) to 0.025 under FGSM noise with level 1 on Electronics
- Adversarial FGSM noise causes greater explainability degradation than random noise at equivalent levels
- EFM (explicit factors only) shows less degradation than models with hidden factors, suggesting architectural vulnerability

## Why This Works (Mechanism)

### Mechanism 1
- Adding random Gaussian noise degrades feature-aware explainability scores by perturbing learned embeddings
- Noise changes the representation space so feature importance rankings shift away from ground-truth aspects
- Core assumption: Explanation quality is tightly coupled to stability of internal parameter values
- Evidence: Tables 2 and 3 show F1@5,5 scores dropping from ~0.059 to ~0.025 under FGSM noise

### Mechanism 2
- Adversarial FGSM noise causes greater degradation than random noise because it's optimized to maximize loss
- FGSM computes gradient sign w.r.t. loss and perturbs parameters in opposite direction
- Core assumption: Loss surface contains directions that cause both recommendation and explanation quality to drop sharply
- Evidence: Figure 2 and tables show larger F1 drops under FGSM vs random at same noise levels

### Mechanism 3
- Models with both explicit and hidden factors show amplified noise effects through interaction
- Noise in explicit factor matrices changes explanation sources while hidden factors modulate interactions
- Core assumption: Intertwined explicit/hidden factor models are more vulnerable due to shared loss terms
- Evidence: EFM (explicit only) shows less degradation than CER/A2CF with hidden layers

## Foundational Learning

- **Feature-level evaluation metrics (Precision, Recall, F1)**: Quantify how well model's top-K explained features match ground-truth aspects in reviews. Quick check: Given true aspects {screen, battery} and model's top-5 {screen, price, battery, design, camera}, what is Precision@5,5? (Answer: 2/5 = 0.4)

- **FGSM adversarial attack optimization**: Generates worst-case perturbations to maximally degrade explainability. Quick check: In FGSM, if gradient sign is positive for a parameter, in which direction is perturbation applied? (Answer: Negative direction)

- **Embedding normalization and max-norm constraint**: Ensures noise perturbations remain bounded and comparable across model sizes. Quick check: If noise vector has L2 norm 2 and max-norm constraint is 1, what scaling factor is applied? (Answer: 1/2)

## Architecture Onboarding

- **Component map**: User-item interactions -> Aspect extraction (Sentires) -> Feature matrices (B, C) -> Model (CER/A2CF/EFM) -> Noise injection -> Retraining -> Evaluation (NDCG, Precision/Recall/F1)
- **Critical path**: Preprocess → Build matrices → Train vanilla model → Perturb parameters → Retrain → Evaluate rec & expl → Log metrics
- **Design tradeoffs**: Random noise (simple, less targeted) vs FGSM noise (realistic attack, requires extra computation); Noise level choice (detectability vs robustness measurement)
- **Failure signatures**: Explainer metrics plateau while rec metrics drop (noise not affecting explanation pathway); Both metrics drop equally (noise affects joint objective)
- **First 3 experiments**: 1) Baseline CER on Electronics with vanilla F1@5,5/NDCG@100; 2) Random noise (norm=0.5), retrain CER, compare metrics; 3) FGSM noise (norm=0.5), retrain CER, compare metrics

## Open Questions the Paper Calls Out

- **Open Question 1**: How do other types of explainable recommender models (e.g., knowledge graph-based or session-based models) compare in stability to feature-aware models under adversarial attacks? The paper focuses on feature-oriented models and suggests comparative analysis across model types is an open research direction.

- **Open Question 2**: What is the relationship between model explainability performance and recommendation accuracy under increasing noise levels? While both metrics are measured separately, the paper doesn't investigate their correlation or potential trade-offs under attack conditions.

- **Open Question 3**: What defense mechanisms could effectively protect explainable recommenders from adversarial attacks while maintaining their interpretability? The paper demonstrates vulnerability but doesn't propose or evaluate countermeasures against the attacks.

## Limitations

- Exact implementation details of Sentires tool for aspect-opinion-sentiment extraction are not specified
- Absolute magnitude of F1@5,5 scores (0.059 for vanilla CER) suggests relatively low baseline explanation quality
- Choice of FGSM attack may not represent most effective attack vectors for explainable recommenders

## Confidence

- **High confidence**: Adversarial noise causes more severe degradation than random noise (supported by clear numerical comparisons)
- **Medium confidence**: All three model types are vulnerable to noise (based on consistent trends across experiments)
- **Low confidence**: Specific mechanism involving interaction between explicit and hidden factors (inferred rather than directly measured)

## Next Checks

1. Verify feature extraction accuracy by testing Sentires on manually annotated review subset from both datasets
2. Replicate noise injection experiments with different random seeds to confirm stability of degradation patterns
3. Test additional attack methods (PGD, CW) to determine if FGSM represents worst-case scenario for explainable recommenders