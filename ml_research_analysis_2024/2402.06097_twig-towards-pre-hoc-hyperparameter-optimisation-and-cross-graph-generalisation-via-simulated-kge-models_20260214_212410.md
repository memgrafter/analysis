---
ver: rpa2
title: 'TWIG: Towards pre-hoc Hyperparameter Optimisation and Cross-Graph Generalisation
  via Simulated KGE Models'
arxiv_id: '2402.06097'
source_url: https://arxiv.org/abs/2402.06097
tags:
- graph
- hyperparameter
- learning
- twig
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "TWIG introduces a novel embedding-free paradigm for simulating\
  \ KGE outputs using only graph topology and hyperparameters. By constructing a neural\
  \ network that maps structural features and hyperparameter settings to KGE model\
  \ predictions, TWIG achieves an R\xB2 of 86.18% in predicting MRR across 1215 hyperparameter\
  \ combinations while using only 2,590 parameters\u20140.008832% of the original\
  \ KGE parameter cost."
---

# TWIG: Towards pre-hoc Hyperparameter Optimisation and Cross-Graph Generalisation via Simulated KGE Models

## Quick Facts
- arXiv ID: 2402.06097
- Source URL: https://arxiv.org/abs/2402.06097
- Authors: Jeffrey Sardina; John D. Kelleher; Declan O'Sullivan
- Reference count: 33
- Key outcome: TWIG achieves R² of 86.18% in predicting MRR across 1215 hyperparameter combinations using only 2,590 parameters (0.008832% of KGE parameter cost)

## Executive Summary
TWIG introduces an embedding-free paradigm for simulating KGE outputs using only graph topology and hyperparameters. By constructing a neural network that maps structural features and hyperparameter settings to KGE model predictions, TWIG demonstrates that KGE performance can be predicted without embeddings. The results suggest KGEs primarily learn structural patterns rather than latent semantics, and that hyperparameter optimization is deterministic based on graph structure. This work challenges the necessity of embeddings for link prediction and opens possibilities for cross-graph learning.

## Method Summary
TWIG is a neural network trained to predict KGE model performance (MRR) using only graph structural features and hyperparameter configurations. The model takes 34 inputs (17 hyperparameters + 17 structural features) and outputs predicted MRR scores. It uses a three-component architecture with dense layers, trained first with KL divergence loss on ranked lists for 50 epochs, then with both MSE and KL divergence for 100 epochs. The approach is tested on the UMLS knowledge graph with ComplEx-N3 models across 1215 hyperparameter combinations.

## Key Results
- TWIG achieves R² of 86.18% in predicting MRR across 1215 hyperparameter combinations
- The model uses only 2,590 parameters (0.008832% of original KGE parameter cost)
- Results support two claims: KGEs learn structural patterns rather than latent semantics, and hyperparameter choice is deterministic based on graph structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TWIG can predict KGE model performance using only graph structure and hyperparameters
- Mechanism: The neural network learns a deterministic mapping from topological features and hyperparameter configurations to MRR scores
- Core assumption: Graph structure and hyperparameters contain sufficient information to predict KGE performance
- Evidence anchors:
  - [abstract] "TWIG learns weights from inputs that consist of topological features of the graph data, with no coding for latent representations of entities or edges"
  - [section] "TWIG was constructed as a neural network made of exclusively Dense Layers that form three key components"
  - [corpus] Weak - corpus contains no direct evidence about TWIG's structural mapping mechanism
- Break condition: If different random seeds produce significantly different MRR values (correlation < 0.9), the deterministic mapping assumption fails

### Mechanism 2
- Claim: KGE models primarily learn structural patterns rather than latent semantics
- Mechanism: The high R² value (86.18%) achieved by TWIG indicates that KGE performance can be explained almost entirely by structural features
- Core assumption: Embedding-free methods can achieve comparable predictive power to embedding-based methods
- Evidence anchors:
  - [abstract] "Based on these results, we make two claims: 1) that KGEs do not learn latent semantics, but only latent representations of structural patterns"
  - [section] "Our core result is this: The fully-trained TWIG model achieves an R2 value of 86.18% for predicting MRR of a hyperparameter combination"
  - [corpus] Missing - corpus lacks evidence about structural learning in KGEs
- Break condition: If TWIG fails to predict MRR accurately on datasets with complex semantic relationships

### Mechanism 3
- Claim: Hyperparameter optimization is deterministic based on graph structure
- Mechanism: TWIG learns the mapping from hyperparameter settings to performance, demonstrating that optimal hyperparameters can be predicted without exhaustive search
- Core assumption: Optimal hyperparameter choices follow predictable patterns based on graph characteristics
- Evidence anchors:
  - [abstract] "that hyperparameter choice in KGEs is a deterministic function of the KGE model and graph structure"
  - [section] "The Hyperparameter Determinism Hypothesis states that optimal hyperparameter choice is a deterministic function of graph structure and the KGE model being used"
  - [corpus] Weak - corpus mentions pre-hoc predictions but lacks evidence about hyperparameter determinism
- Break condition: If hyperparameter performance varies unpredictably across structurally similar graphs

## Foundational Learning

- Concept: Graph topology analysis
  - Why needed here: TWIG relies on structural features like node degrees, co-occurrence frequencies, and neighbor statistics to make predictions
  - Quick check question: What structural features would you extract from a graph with 100 nodes, 50 edges, and an average degree of 2?

- Concept: Knowledge Graph Embedding fundamentals
  - Why needed here: Understanding how KGEs work (link prediction, scoring functions, negative sampling) is essential for interpreting TWIG's simulation results
  - Quick check question: How does ComplEx-N3 score a triple (s,p,o) using embeddings?

- Concept: Neural network regression
  - Why needed here: TWIG is trained as a regression model to predict MRR values, requiring understanding of loss functions, batch processing, and evaluation metrics
  - Quick check question: Why does TWIG use both MSE loss and KL divergence loss during training?

## Architecture Onboarding

- Component map:
  Input layer (34 features) -> Hyperparameter Learning Component (2 dense layers) -> Structure Learning Component (2 dense layers) -> Integration Component (1 dense layer) -> Output layer (1 value)

- Critical path: Input → Hyperparameter component → Structure component → Integration → Output → Loss calculation

- Design tradeoffs:
  - Embedding-free approach vs. potential loss of semantic information
  - Parameter efficiency (2,590 parameters) vs. model expressiveness
  - Batch-based learning vs. individual triple predictions

- Failure signatures:
  - Low correlation between predicted and actual MRR (R² < 0.5)
  - High variance in predictions across different random seeds
  - Training instability or poor convergence

- First 3 experiments:
  1. Train TWIG on UMLS dataset with default hyperparameters, evaluate R²
  2. Test TWIG on held-out hyperparameter combinations, compare predictions to actual MRR
  3. Vary structural feature selection, measure impact on prediction accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Structural Learning Hypothesis hold across other KGE models beyond ComplEx-N3 and datasets beyond UMLS?
- Basis in paper: [explicit] The authors explicitly state "We hypothesise that the Structural Learning Hypothesis will hold across other KGE models and datasets, but leave testing that hypothesis for future work."
- Why unresolved: The current study is limited to a single KGE model (ComplEx-N3) and a single dataset (UMLS), preventing generalization.
- What evidence would resolve it: Testing TWIG's ability to simulate other KGE models (e.g., TransE, DistMult) on diverse datasets (e.g., FB15K-237, WN18RR) with comparable R² performance.

### Open Question 2
- Question: Can embedding-free ("twiggy") methods solve the Link Prediction Task in full, rather than just simulate KGE outputs?
- Basis in paper: [explicit] The authors note "Whether embedding-free ('twiggy') methods can solve the Link Prediction Task in full, rather than just simulate the output of KGEs, remains to be tested."
- Why unresolved: TWIG currently only simulates KGE output rankings and MRR scores, but does not generate actual predictions for unseen triples.
- What evidence would resolve it: Demonstrating that a TWIG-like architecture can generate accurate predictions for (s,p,?) or (?,p,o) queries on held-out test sets without relying on pre-trained embeddings.

### Open Question 3
- Question: What is the theoretical relationship between graph structure features and optimal hyperparameter selection across different KGE models?
- Basis in paper: [explicit] The authors propose the "Hyperparameter Determinism Hypothesis" stating that optimal hyperparameters are a deterministic function of graph structure and the KGE model.
- Why unresolved: While TWIG shows high correlation between structure and performance, the exact functional form or mechanism remains unexplained.
- What evidence would resolve it: Developing a formal mathematical model or framework that maps specific structural metrics (e.g., degree distribution, clustering coefficient) to optimal hyperparameter ranges for various KGE models.

## Limitations

- Structural feature completeness: The study relies on 17 structural features but doesn't validate whether these capture all relevant graph properties affecting KGE performance
- Dataset generalizability: Results are demonstrated only on the UMLS knowledge graph, with untested performance on larger, more complex graphs
- KL divergence implementation details: The "soft histogram" approach using sigmoid functions lacks detailed specification, which could impact training stability

## Confidence

**High confidence**: TWIG successfully predicts MRR scores on UMLS dataset with R² of 86.18%. The technical implementation of the neural network architecture and training procedure is clearly specified and reproducible.

**Medium confidence**: KGEs primarily learn structural patterns rather than latent semantics. While the high R² supports this claim, it's based on a single dataset and could be influenced by UMLS-specific properties.

**Low confidence**: Hyperparameter optimization is deterministic based on graph structure. This claim requires more extensive validation across multiple datasets and random seed variations to establish statistical significance.

## Next Checks

1. **Cross-dataset validation**: Test TWIG on at least 3 additional knowledge graphs with varying sizes (100-10,000 nodes), relation types, and semantic richness. Compare R² scores and analyze which structural features contribute most to prediction accuracy across datasets.

2. **Random seed stability analysis**: Train 10 instances of TWIG with different random seeds on the same dataset. Calculate variance in R² scores and correlation between predictions. Additionally, train 10 instances of ComplEx-N3 with different seeds and measure MRR variance to validate the deterministic hyperparameter hypothesis.

3. **Feature ablation study**: Systematically remove individual structural features from TWIG's input and measure the impact on R² score. Identify which features are most critical for prediction accuracy and whether any features provide negligible value, suggesting the feature set could be optimized.