---
ver: rpa2
title: Few-shot Model Extraction Attacks against Sequential Recommender Systems
arxiv_id: '2411.11677'
source_url: https://arxiv.org/abs/2411.11677
tags:
- data
- extraction
- recommendation
- black-box
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a few-shot model extraction framework against
  sequential recommender systems, addressing the gap in existing research where adversaries
  have access to limited raw data (10% or less). The framework consists of two main
  components: an autoregressive augmentation generation strategy and a bidirectional
  repair loss-facilitated model distillation procedure.'
---

# Few-shot Model Extraction Attacks against Sequential Recommender Systems

## Quick Facts
- arXiv ID: 2411.11677
- Source URL: https://arxiv.org/abs/2411.11677
- Authors: Hui Zhang; Fu Liu
- Reference count: 12
- One-line primary result: Proposed framework achieves superior surrogate models with limited raw data (10% or less) compared to existing data-free methods

## Executive Summary
This paper addresses the gap in model extraction research for sequential recommender systems, where adversaries typically have access to limited raw data. The authors propose a few-shot model extraction framework that generates synthetic data closely approximating the distribution of raw data using an autoregressive augmentation generation strategy. This strategy integrates a probabilistic interaction sampler and a synthesis determinant signal module to capture user behavioral patterns. The framework also introduces a bidirectional repair loss as an auxiliary loss to rectify erroneous predictions from the surrogate model, enhancing the model distillation process. Experiments on three datasets demonstrate that the proposed framework outperforms existing data-free model extraction methods, particularly on denser datasets.

## Method Summary
The proposed few-shot model extraction framework consists of two main components: an autoregressive augmentation generation strategy and a bidirectional repair loss-facilitated model distillation procedure. The autoregressive augmentation strategy generates synthetic data that approximates the distribution of limited raw data by employing an attention-based probabilistic interaction sampler to mine sequence-dependent information and a synthesis determinant signal module to discern user behavioral patterns. The bidirectional repair loss is designed as an auxiliary loss to rectify erroneous predictions from the surrogate model, effectively transferring knowledge from the black-box victim model to the white-box surrogate model by minimizing the divergence between their recommendation lists.

## Key Results
- The proposed framework outperforms existing data-free model extraction methods (MEA-random and MEA-autoregressive) in terms of ranking performance (N@10 and R@10) and agreement measurement (Agr@1 and Agr@10) across all datasets.
- The framework demonstrates greater efficacy on denser datasets such as MovieLens-1M and Steam compared to sparser datasets like Beauty.
- The bidirectional repair loss contributes to improved model distillation efficacy, particularly in scenarios where the surrogate model's predictions deviate from the black-box model's recommendations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot model extraction works because synthetic data generation closely approximates the distribution of limited raw data.
- Mechanism: The autoregressive augmentation generation strategy integrates a probabilistic interaction sampler to extract inherent dependencies and a synthesis determinant signal module to characterize user behavioral patterns, guiding the generation of synthetic data.
- Core assumption: Limited raw data (10% or less) contains sufficient information to guide the generation of high-quality synthetic data that approximates the true data distribution.
- Evidence anchors:
  - [abstract] The autoregressive augmentation generation strategy integrates a probabilistic interaction sampler and a synthesis determinant signal module to generate synthetic data that closely approximates the distribution of raw data.
  - [section 4.1] To generate synthetic data that approximates the distribution of raw data with the utilization of raw data as supervisory information, the autoregressive augmentation generation strategy operates by employing an attention-based probabilistic interaction sampler to mine sequence-dependent information from few-shot raw data and a synthesis determinant signal module to discern user behavioral patterns.
- Break condition: If the raw data is too sparse or noisy, the synthetic data generation will fail to capture the true data distribution, leading to poor surrogate model performance.

### Mechanism 2
- Claim: Bidirectional repair loss improves model distillation by rectifying erroneous predictions from the surrogate model.
- Mechanism: Bidirectional repair loss is designed as an auxiliary loss to rectify erroneous predictions from surrogate models, transferring knowledge from the victim model to the surrogate model effectively by minimizing the divergence between the black-box recommendation list and its white-box equivalent.
- Core assumption: The discrepancies between the black-box recommendation list and the white-box surrogate model's predictions contain valuable information that can be used to improve the surrogate model's performance.
- Evidence anchors:
  - [abstract] Bidirectional repair loss, which target the discrepancies between the recommendation lists, is designed as auxiliary loss to rectify erroneous predictions from surrogate models, transferring knowledge from the victim model to the surrogate model effectively.
  - [section 4.2.2] The bidirectional repair loss is introduced, which leverages these discrepancies as additional supervisory signals to rectify the white-box surrogate model's erroneous predictions, thereby enhancing the model distillation efficacy.
- Break condition: If the black-box model's predictions are too noisy or inconsistent, the bidirectional repair loss may introduce more errors than it corrects, degrading the surrogate model's performance.

### Mechanism 3
- Claim: Few-shot model extraction is more effective on denser datasets due to richer user-item interaction patterns.
- Mechanism: The performance of few-shot model extraction improves with dataset density because denser datasets provide more comprehensive information about user behavior and item relationships, enabling better synthetic data generation and model distillation.
- Core assumption: Denser datasets contain more diverse and representative user-item interactions, which are crucial for generating high-quality synthetic data and training effective surrogate models.
- Evidence anchors:
  - [abstract] Experiments on three datasets validate the effectiveness of the overall few-shot model extraction framework, with better performance observed on denser datasets like ML-1M and Steam.
  - [section 5.2] Model extraction attack demonstrates greater efficacy on denser datasets such as ML-1M and Steam, whereas its performance is less pronounced on sparser datasets like Beauty.
- Break condition: If the dataset density is too low, even with the proposed framework, the limited user-item interactions may not provide enough information to generate meaningful synthetic data or train a high-quality surrogate model.

## Foundational Learning

- Concept: Sequential recommendation systems
  - Why needed here: Understanding how sequential recommendation systems work is crucial for developing effective model extraction attacks, as the attack targets the model's ability to predict user behavior based on historical interactions.
  - Quick check question: How do sequential recommendation systems differ from traditional recommendation systems in terms of capturing user preferences?

- Concept: Model distillation
  - Why needed here: The proposed framework relies on model distillation to transfer knowledge from the black-box victim model to the white-box surrogate model, making it essential to understand the principles and techniques of model distillation.
  - Quick check question: What are the key differences between model distillation in traditional machine learning tasks and model distillation in recommendation systems?

- Concept: Adversarial attacks on machine learning models
  - Why needed here: Few-shot model extraction is a type of adversarial attack, so understanding the broader context of adversarial attacks on machine learning models is necessary to appreciate the significance and implications of this work.
  - Quick check question: What are the main types of adversarial attacks on machine learning models, and how do they differ in terms of their objectives and techniques?

## Architecture Onboarding

- Component map:
  1. Autoregressive augmentation generation strategy
     - Probabilistic interaction sampler
     - Synthesis determinant signal module
  2. Bidirectional repair loss-facilitated model distillation procedure
     - Pair-wise rank loss
     - Bidirectional repair loss

- Critical path: Raw data → Autoregressive augmentation generation → Synthetic data → Model distillation → Surrogate model

- Design tradeoffs:
  - Balancing the amount of raw data used for synthetic data generation versus model distillation
  - Choosing between autoregressive generation and random generation strategies
  - Selecting appropriate hyperparameters for the bidirectional repair loss

- Failure signatures:
  - Poor ranking performance of the surrogate model compared to the victim model
  - Low agreement between the black-box and white-box recommendation lists
  - High variance in the surrogate model's predictions across different runs

- First 3 experiments:
  1. Evaluate the performance of the autoregressive augmentation generation strategy with different proportions of raw data (1%, 5%, 10%) on a single dataset (e.g., ML-1M) to assess the impact of raw data quantity on synthetic data quality.
  2. Compare the effectiveness of the bidirectional repair loss-facilitated model distillation with and without the bidirectional repair loss on the same dataset to demonstrate the contribution of the bidirectional repair loss.
  3. Test the robustness of the proposed framework against architectural discrepancies by training the surrogate model with a different architecture (e.g., BERT4Rec) than the victim model (e.g., NARM) on the same dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of few-shot model extraction scale with different proportions of available raw data (e.g., 1%, 5%, 10%) across various datasets?
- Basis in paper: [explicit] The paper presents results for 1%, 5%, and 10% raw data availability in Table 5, showing performance variations.
- Why unresolved: While the paper shows performance differences, it doesn't provide a comprehensive analysis of the scaling relationship or identify the optimal raw data proportion for different dataset characteristics.
- What evidence would resolve it: Systematic experiments varying raw data proportions (e.g., 0.1%, 0.5%, 1%, 5%, 10%) across all three datasets, analyzing performance curves and identifying optimal points.

### Open Question 2
- Question: How effective are potential defense mechanisms against the proposed few-shot model extraction attacks?
- Basis in paper: [inferred] The paper concludes with mentioning future research aims to develop countermeasures, implying current defenses are unknown.
- Why unresolved: The paper focuses on attack methodology without evaluating existing or potential defense strategies.
- What evidence would resolve it: Implementation and evaluation of various defense mechanisms (e.g., adversarial training, differential privacy, query rate limiting) against the proposed attack framework.

### Open Question 3
- Question: What is the impact of architectural differences between black-box victim models and white-box surrogate models on extraction success?
- Basis in paper: [explicit] Figure 3 analyzes performance with architectural discrepancies, showing NARM performs better than SASRec/BERT4Rec in mismatched scenarios.
- Why unresolved: While initial observations are made, the paper doesn't provide a comprehensive analysis of which architectural differences are most detrimental or how to optimize extraction when architectures differ.
- What evidence would resolve it: Systematic experiments varying architectural differences (e.g., different numbers of layers, attention heads, embedding dimensions) and analyzing their impact on extraction success metrics.

## Limitations
- The proposed framework relies heavily on the quality of the limited raw data available to the adversary; if the raw data is too sparse or noisy, the autoregressive augmentation generation strategy may fail to generate high-quality synthetic data.
- The effectiveness of the bidirectional repair loss depends on the consistency and reliability of the black-box model's predictions, which may not always be the case.
- The framework's performance on extremely sparse datasets remains to be thoroughly evaluated.

## Confidence
- High confidence in the overall effectiveness of the few-shot model extraction framework, as demonstrated by the experimental results on three datasets.
- Medium confidence in the robustness of the framework against various types of sequential recommender system architectures, as the paper only considers two specific architectures (NARM and BERT4Rec).
- Low confidence in the scalability of the framework to larger datasets or more complex recommendation scenarios, as the paper focuses on three relatively small datasets.

## Next Checks
1. Evaluate the framework's performance on a larger and more diverse set of datasets, including both dense and sparse datasets, to assess its scalability and robustness across different recommendation scenarios.
2. Test the framework's effectiveness against a wider range of sequential recommender system architectures, including both attention-based and non-attention-based models, to determine its generalizability.
3. Investigate the impact of varying the amount of raw data available to the adversary (e.g., 1%, 5%, 10%) on the framework's performance, to understand the trade-off between raw data quantity and model extraction quality.