---
ver: rpa2
title: 'Less is More: Accurate Speech Recognition & Translation without Web-Scale
  Data'
arxiv_id: '2406.19674'
source_url: https://arxiv.org/abs/2406.19674
tags:
- data
- speech
- canary
- training
- hours
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Canary, a FastConformer-based attention encoder-decoder model,
  achieves state-of-the-art multilingual ASR and speech translation performance in
  English, French, Spanish, and German while being trained on an order of magnitude
  less data than current models. Key innovations include encoder initialization from
  a pre-trained ASR checkpoint, dynamic bucketing batching, noise-robust fine-tuning,
  and synthetic data generation via machine translation.
---

# Less is More: Accurate Speech Recognition & Translation without Web-Scale Data

## Quick Facts
- arXiv ID: 2406.19674
- Source URL: https://arxiv.org/abs/2406.19674
- Authors: Krishna C. Puvvada, Piotr Żelasko, He Huang, Oleksii Hrinchuk, Nithin Rao Koluguri, Kunal Dhawan, Somshubra Majumdar, Elena Rastorgueva, Zhehuai Chen, Vitaly Lavrukhin, Jagadeesh Balam, Boris Ginsburg
- Reference count: 0
- Primary result: Achieves state-of-the-art multilingual ASR/AST performance using 86K hours of data versus 680K-5M hours for comparable models

## Executive Summary
Canary is a FastConformer-based attention encoder-decoder model that achieves state-of-the-art multilingual speech recognition and translation performance in English, French, Spanish, and German while training on an order of magnitude less data than current models. The model outperforms Whisper, OWSM, and Seamless-M4T on established benchmarks while using only 86K hours of training data versus 680K-5M hours for comparable models. Training completed in under 48 hours using 128 NVIDIA A100 GPUs, and the model and code are open-sourced.

## Method Summary
Canary uses a FastConformer encoder with a Transformer decoder, trained with dynamic bucketing batching, data balancing via stratified sampling, encoder initialization from a pre-trained ASR checkpoint, and synthetic data generation via machine translation. The model uses 128-dim log-mel features as input and generates text in target languages with optional punctuation/capitalization control. Training employs AdamW optimizer with a two-stage learning rate schedule (3e-4 for 150K updates, then 2e-5 for 75K updates) on 128 A100 GPUs.

## Key Results
- Achieves 6.20% WER on English, 6.27% WER on German, 4.09% WER on Spanish, and 5.39% WER on French
- Outperforms Whisper, OWSM, and Seamless-M4T on established benchmarks
- Uses only 86K hours of training data versus 680K-5M hours for comparable models
- Training completed in under 48 hours using 128 NVIDIA A100 GPUs

## Why This Works (Mechanism)

### Mechanism 1
Encoder initialization from a pre-trained ASR checkpoint accelerates convergence and improves multilingual ASR performance by providing learned speech representations and acoustic understanding before training on the combined ASR/AST task. This works when the pre-trained encoder's domain and language coverage align with the target training data.

### Mechanism 2
Dynamic bucketing with quadratic duration penalty enables faster training with better GPU utilization by reducing padding and balancing batch durations. This technique groups utterances by duration and samples batches from single buckets, achieving only 3% padding compared to 50% with non-stratified sampling.

### Mechanism 3
Training on synthetic data generated via machine translation enables high-quality AST performance without requiring parallel speech-translation corpora. Machine translation models automatically generate target language transcriptions from English ASR data, creating pseudo-labels for training without expensive human-annotated speech-translation pairs.

## Foundational Learning

- **Attention mechanisms in transformer architectures**: Why needed - The model uses FastConformer encoder and Transformer decoder, both relying heavily on attention mechanisms for sequence modeling. Quick check - How does multi-head self-attention enable the model to capture different aspects of the input sequence simultaneously?

- **Subword tokenization and vocabulary design**: Why needed - The model uses SentencePiece with concatenated tokenizer and separate vocabularies for each language. Quick check - What are the advantages and disadvantages of using separate vocabularies versus shared vocabularies across languages in multilingual models?

- **Curriculum learning and data balancing strategies**: Why needed - The model uses stratified sampling with temperature scaling to balance multiple languages and datasets throughout training. Quick check - How does temperature scaling affect the sampling distribution when balancing datasets of different sizes?

## Architecture Onboarding

- **Component map**: 128-dim log-mel features → FastConformer encoder → cross-attention with Transformer decoder → text generation

- **Critical path**: Log-mel feature extraction → FastConformer encoder → cross-attention with decoder → text generation; Data loading with dynamic bucketing → gradient computation → optimizer update (AdamW, peak LR 3e-4 stage 1, 2e-5 stage 2)

- **Design tradeoffs**: FastConformer vs standard Conformer (2.8x speedup vs careful handling of downsampling effects); Separate vs shared vocabularies (better language-specific modeling vs increased memory usage); Two-stage training (faster convergence vs additional coordination)

- **Failure signatures**: High padding in batches (incorrect bucketing parameters or duration distribution mismatch); Slow convergence (poor encoder initialization or suboptimal learning rate schedule); Hallucinations on non-speech (insufficient non-speech data or inadequate noise-robust training); Poor multilingual performance (imbalanced data sampling or inadequate language coverage in pre-training)

- **First 3 experiments**: Verify bucketing efficiency by measuring padding percentage across buckets and adjusting quadratic duration penalty; Test encoder initialization by comparing training curves with random vs pre-trained encoder initialization; Evaluate synthetic data quality by measuring translation quality of machine-translated pseudo-labels vs human annotations

## Open Questions the Paper Calls Out

- What is the impact of dynamic bucketing batching on model convergence and training efficiency compared to static bucketing approaches?
- How does the model's performance change when using different sub-sampling strategies within the language-level stratification?
- What is the trade-off between model size and performance for speech recognition and translation tasks across different languages?

## Limitations
- Performance depends on undisclosed proprietary training data composition beyond the 67K hours of public datasets
- Synthetic data quality relies on unspecified machine translation models and quality thresholds
- No quantitative analysis of translation quality or error propagation from synthetic labels

## Confidence

- **High Confidence**: FastConformer architecture provides 2.8x speedup while maintaining modeling capacity; Encoder initialization accelerates convergence and improves multilingual ASR performance
- **Medium Confidence**: Dynamic bucketing with quadratic duration penalty achieves optimal GPU utilization and faster training
- **Low Confidence**: Synthetic data via NMT enables AST performance "without requiring parallel speech-translation corpora"

## Next Checks

1. Measure actual padding percentages across different quadratic duration penalties (10, 20, 30 seconds) on the target dataset to verify the claimed 3% padding rate
2. Evaluate the BLEU scores of machine-translated pseudo-labels against human-annotated speech translation pairs to quantify quality gaps
3. Train identical models with random encoder initialization versus pre-trained initialization on the same dataset subset to measure actual convergence speedup