---
ver: rpa2
title: 'ViT-MUL: A Baseline Study on Recent Machine Unlearning Methods Applied to
  Vision Transformers'
arxiv_id: '2403.09681'
source_url: https://arxiv.org/abs/2403.09681
tags:
- unlearning
- machine
- forget
- learning
- mufac
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive baseline study of recent machine
  unlearning (MUL) methods applied to Vision Transformers (ViT). The authors evaluate
  popular MUL algorithms on two widely-used ViT models, ViT-Base and ViT-Large, using
  the MUFAC and MUCAC datasets.
---

# ViT-MUL: A Baseline Study on Recent Machine Unlearning Methods Applied to Vision Transformers

## Quick Facts
- **arXiv ID**: 2403.09681
- **Source URL**: https://arxiv.org/abs/2403.09681
- **Reference count**: 29
- **Primary result**: ViT models outperform ResNet18 in utility and are amenable to existing MUL methods, with AdvNegGrad and ARU achieving the best trade-off between forgetting and utility.

## Executive Summary
This paper presents a comprehensive baseline study of recent machine unlearning (MUL) methods applied to Vision Transformers (ViT). The authors evaluate popular MUL algorithms on two widely-used ViT models, ViT-Base and ViT-Large, using the MUFAC and MUCAC datasets. The results show that ViT models generally outperform ResNet18 in terms of utility, and all baseline MUL methods are effective on ViT models. Advanced Negative Gradient (AdvNegGrad) and Attack-and-Reset (ARU) methods achieve the best performance, with AdvNegGrad showing higher utility and ARU demonstrating better forgetting. The study highlights the importance of hyperparameter tuning and the need for stable unlearning algorithms.

## Method Summary
The study applies recent MUL algorithms (Re-training, Fine-tuning, CF-k, AdvNegGrad, UNSIR, SCRUB, ARU) to pre-trained ViT-Base and ViT-Large models using the MUFAC and MUCAC datasets. The performance is evaluated using NoMUS score, utility (test accuracy), and forgetting metrics. The authors perform experiments to compare the effectiveness of each MUL method on ViT models and investigate the impact of hyperparameters on performance.

## Key Results
- ViT models outperform ResNet18 in terms of utility across all MUL methods.
- Advanced Negative Gradient (AdvNegGrad) and Attack-and-Reset (ARU) achieve the best trade-off between forgetting and utility.
- Hyperparameter tuning, particularly for ARU's pruning ratio and SCRUB's coefficient, is critical for optimal MUL performance on ViT models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision Transformers (ViT) outperform ResNet18 in utility and are amenable to existing MUL methods.
- Mechanism: ViT's self-attention architecture allows more efficient representation learning, leading to higher baseline accuracy and better retention of utility after unlearning.
- Core assumption: The ViT architecture inherently captures more generalizable features than ResNet, which is reflected in both initial performance and unlearning resilience.
- Evidence anchors:
  - [abstract] "The results show that ViT models generally outperform ResNet18 in terms of utility..."
  - [section] "Vision Transformers (ViT) show better general performance compared to ResNet18, with ViT-L-14 outperforming ViT-B-16..."
  - [corpus] Weak evidence—corpus neighbors focus on MUL in other contexts (recommendation, LLMs, quantized NNs) without direct comparison to ViT vs ResNet.
- Break condition: If ViT models show high overfitting to training data, unlearning could degrade utility more than expected.

### Mechanism 2
- Claim: Advanced Negative Gradient (AdvNegGrad) and Attack-and-Reset (ARU) achieve the best trade-off between forgetting and utility.
- Mechanism: AdvNegGrad combines fine-tuning with gradient ascent on forget set, actively suppressing the influence of forgotten data; ARU identifies and reinitializes parameters most responsible for overfitting to forget data.
- Core assumption: These methods can effectively isolate and remove influence of specific data points without destroying overall model performance.
- Evidence anchors:
  - [abstract] "Advanced Negative Gradient (AdvNegGrad) and Attack-and-Reset (ARU) methods achieve the best performance..."
  - [section] "AdvNegGrad shows notable forgetting performance... ARU demonstrates state-of-the-art performance..."
  - [corpus] Weak evidence—corpus papers focus on MUL generalization but not on specific effectiveness of AdvNegGrad or ARU.
- Break condition: If the forget set is very large relative to the retain set, both methods may struggle to balance forgetting with utility preservation.

### Mechanism 3
- Claim: Hyperparameter tuning (e.g., pruning ratio for ARU, coefficient for SCRUB) is critical for MUL performance on ViT.
- Mechanism: ViT models have different parameter efficiency compared to ResNet, making fixed hyperparameters from ResNet-based MUL methods suboptimal.
- Core assumption: ViT's parameter utilization differs enough from ResNet that MUL hyperparameters must be recalibrated for ViT models.
- Evidence anchors:
  - [section] "ARU's performance is highly dependent on the pruning ratio... pruning more than 10% of the ViT parameters lead to considerable degradation in performance, while the optimal pruning rate being 50% for ResNet18."
  - [corpus] No direct corpus evidence—MUL literature generally does not discuss ViT-specific hyperparameter tuning.
- Break condition: If a hyperparameter search is not performed, the method may underperform or even harm utility.

## Foundational Learning

- Concept: Instance-based vs Class-based unlearning
  - Why needed here: The paper focuses on forgetting specific individuals (instances) rather than entire classes, which is more aligned with real-world privacy needs.
  - Quick check question: What is the key difference between instance-based and class-based unlearning in terms of evaluation metrics?

- Concept: Trade-off between utility and forgetting
  - Why needed here: MUL methods must balance maintaining overall model accuracy while ensuring the forget set's influence is removed.
  - Quick check question: Why does increasing forgetting performance often lead to a decrease in utility?

- Concept: NoMUS score
  - Why needed here: NoMUS is the primary metric used to evaluate MUL methods, combining utility and forgetting into a single score.
  - Quick check question: How is the NoMUS score calculated and what does a higher value indicate?

## Architecture Onboarding

- Component map:
  ViT model (ViT-Base or ViT-Large) -> MUFAC or MUCAC dataset -> MUL algorithm (AdvNegGrad, ARU, etc.) -> Evaluation pipeline -> Hyperparameter tuning loop

- Critical path:
  1. Load pre-trained ViT model
  2. Prepare dataset splits (ensure instance-based separation)
  3. Apply MUL algorithm with chosen hyperparameters
  4. Evaluate utility on test set
  5. Evaluate forgetting using unseen set and regression classifier
  6. Compute NoMUS score
  7. Iterate hyperparameters for optimal NoMUS

- Design tradeoffs:
  - Re-training from scratch gives best forgetting but hurts utility; incremental MUL methods trade off between the two.
  - Larger k in CF-k improves forgetting but risks utility loss; must be tuned per dataset.
  - AdvNegGrad and SCRUB use gradient ascent which can become unstable; need careful balancing of loss terms.
  - ARU's pruning ratio is highly sensitive; over-pruning severely damages utility.

- Failure signatures:
  - Utility drops sharply after unlearning (method over-forgets).
  - Forgetting score remains high (method fails to unlearn).
  - NoMUS score plateaus early (hyperparameters not tuned).
  - Training instability (e.g., exploding gradients in AdvNegGrad/SCRUB).

- First 3 experiments:
  1. Baseline: Apply Fine-tuning on retain set only; measure utility and forgetting.
  2. Compare AdvNegGrad vs ARU on MUFAC with default hyperparameters; record NoMUS.
  3. Perform hyperparameter sweep for ARU pruning ratio (10%, 30%, 50%, 70%, 90%) on ViT-B-16; observe utility/forgetting trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal pruning ratio for ARU differ between ResNet and ViT models, and what architectural factors contribute to this difference?
- Basis in paper: [explicit] The paper observes that ARU's performance is highly dependent on the pruning ratio when applied to Vision Transformer (ViT) models, with optimal pruning rates differing significantly from those in ResNet18.
- Why unresolved: While the paper provides empirical results showing the disparity in optimal pruning ratios between ViT and ResNet, it does not delve into the architectural reasons behind this difference.
- What evidence would resolve it: Comparative studies analyzing the internal structure and parameter utilization efficiency of ViT and ResNet models, possibly through detailed ablation studies or architectural analyses, could provide insights into why ViT models might employ parameters more efficiently, leading to lower redundancy and different optimal pruning rates.

### Open Question 2
- Question: What are the long-term effects of using gradient ascent methods like AdvNegGrad and SCRUB on model stability and performance in machine unlearning?
- Basis in paper: [explicit] The paper notes that models using gradient ascent methods become relatively unstable over time, speculating this is due to the gradient ascent term easily becoming the dominant loss term.
- Why unresolved: The paper identifies the instability issue but does not explore long-term effects or propose solutions to balance the two loss terms for stable unlearning.
- What evidence would resolve it: Longitudinal studies tracking model performance and stability over extended unlearning processes, along with experiments testing various methods to balance gradient ascent and descent terms, could clarify the long-term impacts and potential solutions for instability.

### Open Question 3
- Question: How does the choice of ViT model size (e.g., ViT-Base vs. ViT-Large) impact the effectiveness of machine unlearning algorithms in terms of utility and forgetting performance?
- Basis in paper: [explicit] The paper shows that larger ViT-L models outperform smaller ViT-B models in general performance, but does not extensively explore how model size affects unlearning effectiveness.
- Why unresolved: While the paper provides initial comparisons, it does not deeply investigate the relationship between model size and unlearning effectiveness across different algorithms.
- What evidence would resolve it: Comprehensive experiments varying model sizes and analyzing the trade-offs in utility and forgetting performance for each unlearning algorithm could elucidate how model architecture influences unlearning outcomes.

## Limitations
- The study does not provide specific hyperparameter values for the MUL algorithms, particularly for SCRUB and ARU, which are critical for reproducing the results.
- Implementation details of the MUL algorithms, especially AdvNegGrad and UNSIR, are not specified, which may lead to variations in performance.
- The paper does not discuss potential biases in the MUFAC and MUCAC datasets that could affect the generalizability of the findings.

## Confidence

**High**: The claim that ViT models outperform ResNet18 in utility is supported by direct results in the abstract and section.

**Medium**: The assertion that AdvNegGrad and ARU achieve the best performance is based on reported results, but the lack of specific hyperparameters and implementation details introduces uncertainty.

**Low**: The conclusion that hyperparameter tuning is critical for MUL on ViT models is inferred from the sensitivity of ARU to pruning ratio, but the study does not provide a comprehensive hyperparameter analysis.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Perform a detailed hyperparameter sweep for all MUL algorithms on ViT models to determine optimal settings and their impact on utility and forgetting.

2. **Implementation Verification**: Reimplement the MUL algorithms (AdvNegGrad, UNSIR, etc.) based on the paper's descriptions and compare their performance with the reported results.

3. **Dataset Bias Investigation**: Analyze the MUFAC and MUCAC datasets for potential biases and evaluate how these biases might influence the MUL performance across different ViT models.