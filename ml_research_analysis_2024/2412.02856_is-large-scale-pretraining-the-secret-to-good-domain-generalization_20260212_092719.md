---
ver: rpa2
title: Is Large-Scale Pretraining the Secret to Good Domain Generalization?
arxiv_id: '2412.02856'
source_url: https://arxiv.org/abs/2412.02856
tags:
- domain
- office
- home
- clipart
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the role of large-scale pretraining in domain
  generalization (DG), where models are trained on multiple source domains and tested
  on unseen target domains. While prior work suggested that perceptual similarity
  to pretraining data predicts zero-shot performance, the authors find this hypothesis
  limited in DG settings.
---

# Is Large-Scale Pretraining the Secret to Good Domain Generalization?

## Quick Facts
- arXiv ID: 2412.02856
- Source URL: https://arxiv.org/abs/2412.02856
- Reference count: 40
- Primary result: State-of-the-art DG methods rely heavily on pre-training alignment, performing well on pretraining-aligned (IP) data but poorly on misaligned (OOP) data

## Executive Summary
This paper investigates whether large-scale pretraining is the key to domain generalization (DG) success. While prior work suggested perceptual similarity to pretraining data predicts performance, the authors find this limited in DG settings. They propose the Alignment Hypothesis: pre-training alignment between image and text embeddings better predicts DG success than perceptual similarity. By splitting five DG datasets into well-aligned (In-Pretraining/IP) and poorly-aligned (Out-of-Pretraining/OOP) subsets, they demonstrate that current state-of-the-art DG methods excel on IP data but struggle significantly on OOP data, suggesting these methods primarily leverage pre-trained features rather than learning new generalizable features.

## Method Summary
The authors introduce a novel evaluation framework for DG methods by computing alignment scores between image and text embeddings for all samples in target domains. They split DG datasets into IP (high alignment) and OOP (low alignment) subsets using a threshold of 0.4. This creates a more nuanced evaluation setting where methods can be tested on both well-aligned and poorly-aligned data. The framework is applied to five standard DG benchmarks (VLCS, PACS, OfficeHome, TerraIncognita, DomainNet) and evaluated using leave-one-out cross-validation. State-of-the-art DG methods are benchmarked on both IP and OOP subsets to reveal their reliance on pre-training alignment.

## Key Results
- DG methods achieve near-perfect performance on IP data (well-aligned with pretraining) but drop to near-zero on OOP data
- The Alignment Hypothesis shows that image-text embedding alignment during pretraining is a better predictor of DG performance than perceptual similarity
- Even the best DG methods underperform compared to simple baselines on OOP samples, highlighting the need for methods that can learn beyond pre-training alignment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Alignment between image and text embeddings during pre-training predicts downstream domain generalization performance.
- **Mechanism**: The model learns features that align well with intended class labels during pre-training. Methods relying on pre-trained features perform well on target data maintaining this alignment but fail on misaligned data.
- **Core assumption**: Pre-trained alignment captures semantic relationships between visual concepts and textual descriptions that transfer to unseen domains.
- **Evidence anchors**:
  - [abstract] "we introduce the Alignment Hypothesis, which states that the final DG performance will be high if and only if alignment of image and class label text embeddings is high"
  - [section 3.2] "we find that performance for low alignment samples can be almost 0, while performance for high alignment samples is close to perfect"
- **Break condition**: Target domains containing visual concepts poorly represented or misaligned during pre-training will result in low alignment scores and poor performance.

### Mechanism 2
- **Claim**: Domain generalization methods primarily rely on pre-trained features rather than learning new generalizable features from source data.
- **Mechanism**: State-of-the-art DG methods achieve high performance on IP data by leveraging well-learned pre-trained features, but struggle on OOP data because they cannot effectively learn new features that generalize beyond pre-training alignment.
- **Core assumption**: Primary source of generalization capability comes from pre-training rather than fine-tuning process.
- **Evidence anchors**:
  - [abstract] "we show that all evaluated DG methods struggle on DomainBed-OOP, while recent methods excel on DomainBed-IP"
  - [section 5.2] "DG methods perform well on DomainBed-IP... but struggle significantly on OOP data"
- **Break condition**: If a DG method can successfully learn features that generalize beyond pre-training alignment, or if target domain is sufficiently similar to pre-training data.

### Mechanism 3
- **Claim**: Splitting evaluation datasets into IP and OOP subsets based on alignment scores reveals true generalization capabilities of DG methods.
- **Mechanism**: Separating data into well-aligned (IP) and poorly-aligned (OOP) subsets measures whether DG methods actually generalize or simply exploit pre-training alignment.
- **Core assumption**: Alignment score reliably indicates pre-training learning quality that transfers to downstream tasks.
- **Evidence anchors**:
  - [section 4.2] "we find an AlignmentScore threshold to split DG datasets into well aligned In-Pretraining (IP) and poorly aligned Out-of-Pretraining (OOP) evaluation subsets"
  - [section 5.2] "Using DomainBed-IP/OOP we find that leading DG methods perform well on data well-aligned by pre-training but struggle on misaligned samples"
- **Break condition**: If alignment score is not a reliable predictor of pre-training learning quality, or if split threshold is poorly chosen.

## Foundational Learning

- **Concept**: Cosine similarity as a measure of alignment
  - Why needed here: The paper uses cosine similarity between image and text embeddings to quantify alignment, central to the Alignment Hypothesis
  - Quick check question: If two vectors have cosine similarity of 0.8, what does this indicate about their alignment?

- **Concept**: Domain generalization task setup
  - Why needed here: Understanding the multi-source DG setup (training on multiple source domains, testing on unseen target domains) is crucial for interpreting results
  - Quick check question: In DG, why is it problematic if a method performs well only on target domains similar to the pre-training data?

- **Concept**: Catastrophic forgetting in fine-tuning
  - Why needed here: The paper discusses how DG methods can catastrophically forget features, especially for IP data
  - Quick check question: What is catastrophic forgetting and why is it a concern when fine-tuning pre-trained models?

## Architecture Onboarding

- **Component map**: CLIP backbone -> Alignment scoring module -> Dataset splitting logic (IP/OOP threshold) -> DG methods (MIRO, CLIPood, VL2V-SD, etc.) -> Evaluation framework

- **Critical path**: 1. Compute AlignmentScore for all target domain samples 2. Split data into IP and OOP subsets using threshold 3. Train DG methods on full source data 4. Evaluate separately on IP and OOP subsets 5. Compare performance across methods and subsets

- **Design tradeoffs**:
  - Using pre-trained models vs training from scratch (tradeoff between strong initialization and potential bias)
  - Choosing alignment threshold (affects balance between IP and OOP subsets)
  - Focusing on CLIP-based methods vs exploring other architectures

- **Failure signatures**:
  - Large performance gap between IP and OOP subsets indicates reliance on pre-training
  - Poor overall performance across all subsets suggests fundamental learning issues
  - High variance across datasets may indicate sensitivity to domain characteristics

- **First 3 experiments**:
  1. Replicate the AlignmentScore vs accuracy correlation on a small dataset subset
  2. Implement the IP/OOP split on one dataset and verify the distribution of alignment scores
  3. Train one DG method (e.g., MIRO) on the full dataset and evaluate separately on IP and OOP subsets to observe the performance gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do domain generalization methods perform on low-alignment data when using backbones other than CLIP?
- Basis in paper: [inferred] The authors benchmark 2 additional backbones (DINOv2 and OpenAI CLIP) using MIRO + MPA on OfficeHome and PACS, finding consistent results with CLIP. They note this consistency is likely due to similar pre-training datasets and scale, suggesting the alignment hypothesis may extend to other backbones.
- Why unresolved: The analysis was limited to two additional backbones and two datasets. The performance on low-alignment data across a wider range of backbones and datasets remains unexplored.
- What evidence would resolve it: Benchmarking a diverse set of DG methods on low-alignment data using various backbones (e.g., DINOv2, OpenCLIP, OpenAI CLIP, DINO) across multiple DG datasets (e.g., VLCS, PACS, OfficeHome, TerraIncognita, DomainNet) would provide a comprehensive understanding of backbone dependence.

### Open Question 2
- Question: Can domain generalization methods be improved to learn new, generalizable features from source data, rather than relying on pre-trained alignment?
- Basis in paper: [explicit] The authors observe that current DG methods excel on high-alignment (IP) data but struggle significantly on low-alignment (OOP) data. This suggests that existing methods primarily leverage pre-trained features rather than learning new, generalizable features from source data.
- Why unresolved: The paper does not propose or test new DG methods designed to overcome this limitation. It focuses on analyzing the reliance on pre-training rather than developing solutions.
- What evidence would resolve it: Developing and evaluating novel DG methods that incorporate mechanisms to learn features independent of pre-training alignment (e.g., domain-invariant feature learning, meta-learning, or adversarial training) on both IP and OOP subsets would demonstrate whether it's possible to improve generalization on low-alignment data.

### Open Question 3
- Question: How does the size of objects within images influence their alignment scores and subsequent DG performance?
- Basis in paper: [explicit] The authors investigate the relationship between object size and alignment score using bounding boxes from OWLv2. They find a positive correlation between object size and alignment score across most datasets, with TerraIncognita as an exception.
- Why unresolved: While a correlation is established, the causal relationship and its impact on DG performance are not fully explored. The exception of TerraIncognita also warrants further investigation.
- What evidence would resolve it: Analyzing DG performance on images with varying object sizes within the same class and dataset would reveal if smaller objects consistently lead to lower performance. Additionally, investigating the characteristics of TerraIncognita that make it an exception could provide insights into handling background clutter in other datasets.

## Limitations

- The study primarily relies on CLIP-based models, raising questions about generalizability to other pre-trained architectures
- The dataset splitting threshold (0.4) may not be universally optimal across different DG scenarios
- The focus on image classification tasks may limit applicability to other vision tasks like object detection or segmentation

## Confidence

**High Confidence**:
- State-of-the-art DG methods show significantly better performance on IP data compared to OOP data across multiple datasets and methods
- Alignment between image and text embeddings during pre-training is a strong predictor of downstream DG performance

**Medium Confidence**:
- Current DG methods primarily rely on pre-trained features rather than learning new generalizable features (supported but may oversimplify different DG approaches)
- Future DG research should focus on methods that can generalize beyond pre-training alignment (reasonable but may need qualification)

**Low Confidence**:
- Large-scale pretraining is the "secret" to good domain generalization (may be overstated given limitations on OOP data)
- Specific numerical thresholds used to split datasets may not be universally applicable

## Next Checks

1. **Cross-Architecture Validation**: Replicate main experiments using different pre-trained models (e.g., CLIP with different backbones, or non-CLIP models like BERT-based vision models) to verify whether the alignment hypothesis holds across architectures.

2. **Threshold Sensitivity Analysis**: Systematically vary the alignment score threshold used to split IP and OOP subsets across different datasets to determine how sensitive results are to this parameter and whether an optimal threshold can be identified.

3. **Task Generalization Test**: Extend the evaluation framework to non-classification tasks such as object detection or semantic segmentation to assess whether the alignment hypothesis and observed performance gaps generalize to other vision tasks beyond image classification.