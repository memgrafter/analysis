---
ver: rpa2
title: Can Shape-Infused Joint Embeddings Improve Image-Conditioned 3D Diffusion?
arxiv_id: '2402.01241'
source_url: https://arxiv.org/abs/2402.01241
tags:
- generation
- image
- shape
- clip
- shapes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the question of whether incorporating 3D knowledge
  into generative models can improve image-conditioned 3D shape synthesis. The authors
  propose CISP, a model that aligns 2D images and 3D shapes in a shared embedding
  space, designed to capture 3D characteristics overlooked by CLIP's text-image focus.
---

# Can Shape-Infused Joint Embeddings Improve Image-Conditioned 3D Diffusion?

## Quick Facts
- arXiv ID: 2402.01241
- Source URL: https://arxiv.org/abs/2402.01241
- Authors: Cristian Sbrolli; Paolo Cudrano; Matteo Matteucci
- Reference count: 40
- Primary result: CISP-guided 3D diffusion models achieve higher image-to-shape coherence (0.649 IoU) than CLIP-guided models (0.586 IoU) while matching generation quality and diversity

## Executive Summary
This study investigates whether incorporating 3D knowledge into generative models can improve image-conditioned 3D shape synthesis. The authors propose CISP, a model that aligns 2D images with 3D shapes in a shared embedding space designed to capture 3D characteristics overlooked by CLIP's text-image focus. They compare CISP-guided and CLIP-guided 3D diffusion models across generation quality, diversity, coherence, and out-of-distribution generalization. Results demonstrate that CISP-guided models produce shapes with significantly higher coherence to input images while maintaining comparable quality and diversity, validating the value of 3D-informed embeddings for generative tasks.

## Method Summary
The authors develop CISP by training a contrastive learning model to align 2D images with 3D shapes in a shared embedding space. The model uses a DeiT-B image encoder and a 3D-adapted transformer (3D-DeiT) for voxel shapes. This embedding space is then used to condition a 3D diffusion probabilistic model (DDPM) with classifier-free guidance. For comparison, they train a baseline using CLIP embeddings to condition an identical DDPM architecture. Both models are trained on ShapeNet data, with CISP learning the joint embedding space through contrastive loss while CLIP uses pre-trained text-image embeddings.

## Key Results
- CISP-guided models achieve IoU of 0.649 vs 0.586 for CLIP-guided models when generating 10 shapes per input image
- F-Score improvements: 0.414 (CISP) vs 0.333 (CLIP) for image-conditioned generation
- Both models achieve similar generation quality and diversity (1-NNA, CD, EMD metrics)
- CISP-guided models show smoother, more semantically consistent interpolations between shapes
- CISP demonstrates better generalization to out-of-distribution data including sketches and real-world images

## Why This Works (Mechanism)

### Mechanism 1
CISP embeddings encode 3D structural knowledge that CLIP lacks, leading to better image-to-shape coherence. By learning a joint image-shape embedding space through contrastive training, CISP forces the embedding structure to reflect 3D shape characteristics beyond what's visible in 2D images. This captures structural features that CLIP's text-image contrastive training misses.

### Mechanism 2
CISP's 3D-informed embeddings produce smoother, more semantically consistent interpolations between shapes than CLIP. The 3D encoding maintains continuity in shape attributes during interpolation, preventing abrupt transitions seen in CLIP-guided interpolations. This smoothness stems from the embedding space's geometric structure reflecting actual 3D relationships.

### Mechanism 3
CISP generalizes better to out-of-distribution data despite being trained on a smaller dataset than CLIP. The 3D-informed embeddings capture fundamental shape semantics, making the model more robust to variations in input style and background. This suggests that 3D structural knowledge provides a more stable foundation for shape generation across diverse inputs.

## Foundational Learning

- **Contrastive learning and embedding space alignment**: CISP relies on contrastive loss to align images and shapes in a shared space. Understanding how this loss shapes embedding structure is critical.
  - Quick check: How does the temperature parameter τ affect the sharpness of the learned embedding distribution?

- **Diffusion probabilistic models and classifier-free guidance**: The study uses DDPMs conditioned on embeddings. Knowing how guidance scale and null tokens affect generation is essential.
  - Quick check: What happens to generation quality if the guidance scale w is set too high?

- **3D voxel representations and point cloud sampling**: Shapes are represented as voxels, and evaluation uses point cloud metrics. Understanding voxel-to-point conversion is important.
  - Quick check: Why is point sampling (2048 points) used for EMD and F-Score metrics instead of direct voxel comparison?

## Architecture Onboarding

- **Component map**: Input image → Ei/Ec → joint embedding → projected tokens → DDPM attention → DDPM denoises → voxel shape → thresholding → output
- **Critical path**: 1) Input image processed by Ei/Ec to generate joint embedding; 2) Joint embedding projected to attention tokens for DDPM; 3) DDPM denoises to generate voxel shape
- **Design tradeoffs**: CLIP uses large-scale pre-training but lacks 3D info; CISP is smaller-scale but 3D-aware. 3D-DeiT vs CNN for shape encoding: transformers give better accuracy but cost more. Batch size vs embedding dimension: larger batch size improves retrieval accuracy more than larger embeddings.
- **Failure signatures**: Poor IoU/F-Score indicates guidance embeddings not aligned with target shapes. Abrupt interpolations suggest embedding space lacks smoothness due to insufficient 3D encoding. OOD failure indicates embeddings overfit to training data distribution.
- **First 3 experiments**: 1) Train CISP and CLIP-conditioned DDPMs on ShapeNet, evaluate IoU/F-Score on held-out set. 2) Perform interpolation between two embeddings in each space, generate shapes, assess transition smoothness. 3) Test sketch-to-shape and in-the-wild image generation, compare coherence and generalization.

## Open Questions the Paper Calls Out

### Open Question 1
Can scaling up CISP to match CLIP's training scale further improve generation quality and diversity in image-conditioned 3D shape synthesis? The authors suggest that investing in large-scale multimodal systems including 3D representations would be beneficial, but CISP was trained on a smaller dataset due to resource constraints.

### Open Question 2
What specific 3D structural features does CISP capture that CLIP misses, and how do these features translate to improved shape generation? The paper demonstrates improved coherence but doesn't systematically identify or categorize the specific 3D features that distinguish CISP from CLIP.

### Open Question 3
How does the performance gap between CISP and CLIP change across different 3D shape categories with varying levels of visual ambiguity and occlusion? The study focuses on general categories without analyzing performance variation across categories with different occlusion patterns or visual ambiguity.

## Limitations
- Evaluation metrics for 3D shape generation (IoU, F-Score) may not fully capture perceptual quality or semantic consistency
- Generalization claims to out-of-distribution data are based on limited qualitative examples rather than extensive quantitative analysis
- The attribution of improvements to CISP's 3D structural encoding is reasonable but not definitively proven

## Confidence
- **High Confidence**: CISP-guided models achieve higher coherence with input images (0.649 vs 0.586 IoU) is well-supported by quantitative metrics
- **Medium Confidence**: The attribution of improvements to 3D structural encoding is reasonable but not definitively proven
- **Low Confidence**: Claims about better OOD generalization are based on limited qualitative evidence

## Next Checks
1. Compare CISP's coherence improvements against alternative 3D shape generation metrics (e.g., normal consistency, surface coverage) to ensure findings are not metric-specific
2. Perform an ablation study isolating the impact of 3D-DeiT vs CNN encoders on coherence and interpolation smoothness
3. Systematically test CISP on a broader range of out-of-distribution data (e.g., medical imaging, industrial design sketches) to validate its robustness