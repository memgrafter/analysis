---
ver: rpa2
title: 'SEUF: Is Unlearning One Expert Enough for Mixture-of-Experts LLMs?'
arxiv_id: '2411.18797'
source_url: https://arxiv.org/abs/2411.18797
tags:
- unlearning
- arxiv
- experts
- expert
- seuf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEUF, the first dedicated unlearning framework
  for sparse MoE LLMs. It addresses the challenge of expert selection shift in MoE
  models during unlearning, which causes poor performance and model collapse.
---

# SEUF: Is Unlearning One Expert Enough for Mixture-of-Experts LLMs?

## Quick Facts
- **arXiv ID**: 2411.18797
- **Source URL**: https://arxiv.org/abs/2411.18797
- **Reference count**: 20
- **Primary result**: SEUF improves forget quality by up to 5% and model utility by up to 35% on WMDP and RWKU benchmarks

## Executive Summary
This paper introduces SEUF, the first dedicated unlearning framework for sparse MoE LLMs. It addresses the challenge of expert selection shift in MoE models during unlearning, which causes poor performance and model collapse. SEUF selects the top-1 expert most relevant to the forget set via expert attribution and stabilizes its activation with an anchor loss, limiting updates to only 0.06% of parameters. Experiments show SEUF improves forget quality by up to 5% and model utility by up to 35% across WMDP and RWKU benchmarks, outperforming standard unlearning methods and PEFT baselines on MoE architectures.

## Method Summary
SEUF addresses unlearning in MoE LLMs by identifying the most relevant expert for the forget set through expert attribution, then stabilizing that expert's activation during unlearning. The framework employs a router anchor loss to prevent expert selection shift, ensuring unlearning efforts remain focused on the target expert while preserving utility from non-target experts. SEUF is compatible with various standard unlearning algorithms (GA, GDIFF, NPO, RMU) and updates only a minimal set of parameters, making it highly efficient for sparse MoE architectures.

## Key Results
- SEUF improves forget quality (FE) by up to 5% compared to baseline unlearning methods
- SEUF improves model utility (UT) by up to 35% on WMDP and RWKU benchmarks
- SEUF limits parameter updates to only 0.06% of total parameters in MoE models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic routing in MoE LLMs causes expert selection shift during unlearning, leading to poor performance.
- Mechanism: Existing unlearning methods update router parameters, which changes the gating scores and shifts token assignment away from target experts to non-target experts. This creates "short-cuts" where non-target experts are unlearned instead, causing excessive forgetting and utility drops.
- Core assumption: Router updates are not frozen during unlearning, and the gating scores directly influence expert selection.
- Evidence anchors:
  - [abstract] "Our pilot study shows that the dynamic routing nature of MoE LLMs introduces unique challenges, leading to excessive forgetting, uncontrolled knowledge erasure and substantial utility drops when existing unlearning methods are applied."
  - [section] "To look into this phenomenon, we begin by performing a careful sanity check on unlearning methods in MoE LLMs and conduct an in-depth analysis of failure cases. Ideally, in MoE LLMs, given an input, the routers should evaluate and direct it to the most relevant experts, with unlearning targeting and erasing the corresponding knowledge in these experts. However, by monitoring expert selection during unlearning, we find that the process often prompts routers to constantly switch the activated experts."

### Mechanism 2
- Claim: SEUF improves unlearning by focusing updates only on the most relevant expert.
- Mechanism: SEUF identifies the top-1 expert based on affinity scores and freezes all other experts and router parameters except those of the selected expert and its corresponding router. This ensures that unlearning is concentrated on the target expert and prevents expert selection shift.
- Core assumption: The top-1 expert identified by affinity scores is the most relevant for the forget set and contains the target knowledge.
- Evidence anchors:
  - [abstract] "SEUF employs expert attribution to pinpoint the experts most actively involved with the forget set, which is designated as the primary target for unlearning. Unlearning efforts are exclusively focused on this identified expert."
  - [section] "Based on the insight above, we define the frequently activated experts as topic-target experts, and the others as non-target. Thus, by eliminating the knowledge stored in these target experts, MoE LLM unlearning can be achieved more effectively."

### Mechanism 3
- Claim: The router anchor loss stabilizes expert selection during unlearning.
- Mechanism: The router anchor loss encourages the router to maintain high gating scores for the selected target expert throughout the unlearning process. This prevents the router from shifting token assignment to non-target experts and ensures focused unlearning.
- Core assumption: The router anchor loss is effective in controlling the gating scores and preventing expert selection shift.
- Evidence anchors:
  - [abstract] "Concurrently, an anchor loss is applied to the router to stabilize the active status of this targeted expert throughout the unlearning process. This approach prevents the frequent switching of expert selection, ensuring that unlearning is both focused and controlled."
  - [section] "To mitigate this, we propose the router anchor loss, which encourages the previously identified target expert to remain consistently activated throughout unlearning."

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding how MoE LLMs work is crucial for understanding the challenges of unlearning in these models.
  - Quick check question: What is the main difference between a dense LLM and an MoE LLM?

- Concept: Expert selection and routing
  - Why needed here: The routing mechanism in MoE LLMs determines which experts are activated for a given input, and this directly impacts the unlearning process.
  - Quick check question: How does the router in an MoE LLM decide which experts to activate for a given input?

- Concept: Affinity scores and expert attribution
  - Why needed here: SEUF uses affinity scores to identify the most relevant experts for unlearning, so understanding how these scores are calculated is important.
  - Quick check question: How are affinity scores calculated for each expert in an MoE LLM?

## Architecture Onboarding

- Component map: Input -> Router -> Top-K Experts -> Aggregated Output -> SEUF (Expert Attribution + Anchor Loss) -> Updated Expert
- Critical path:
  1. Token sequence enters MoE layer
  2. Router computes gating scores for all experts
  3. Top-K experts are selected based on gating scores
  4. Selected experts process the token sequence
  5. Expert outputs are aggregated and combined with residual
  6. SEUF identifies target expert and applies anchor loss
  7. Unlearning updates only target expert and router parameters
- Design tradeoffs:
  - Update only top-1 expert vs. multiple experts: Updating only the top-1 expert minimizes interference but may miss some target knowledge if it's distributed across multiple experts.
  - Router anchor loss strength: Too weak and expert selection may still shift; too strong and it may prevent necessary updates.
  - Expert attribution accuracy: Inaccurate attribution may lead to unlearning the wrong expert.
- Failure signatures:
  - Expert selection shift: Router starts activating different experts during unlearning
  - Utility drop: Model performance on retain set degrades significantly
  - Incomplete unlearning: Forget set knowledge is not fully removed
- First 3 experiments:
  1. Run SEUF on a small MoE model with a simple unlearning task and monitor expert selection over time to verify it stays focused on the target expert.
  2. Compare SEUF with standard unlearning methods on the same task to measure utility preservation and forget quality.
  3. Test SEUF with different values of the router anchor loss strength to find the optimal balance between stability and effectiveness.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided.

## Limitations
- Limited evaluation to only three specific MoE architectures (Qwen1.5-MoE, DeepSeek-V2-Lite, Mixtral-8x7B) with similar routing strategies
- No investigation of long-term stability after unlearning when models undergo subsequent fine-tuning or adaptation
- Unclear optimal implementation details for expert attribution mechanism and router anchor loss parameters

## Confidence

**High Confidence** (Evidence strongly supports):
- The core insight that expert selection shift is a major challenge for MoE unlearning
- SEUF's basic mechanism of freezing non-target experts and using anchor loss
- The quantitative improvement over baselines on WMDP and RWKU benchmarks

**Medium Confidence** (Evidence supports but with limitations):
- The claim that SEUF updates only 0.06% of parameters while maintaining effectiveness
- The assertion that SEUF works "out-of-the-box" with all tested unlearning methods
- The explanation that expert selection shift causes "excessive forgetting"

**Low Confidence** (Claims need more validation):
- The scalability claim that SEUF works equally well on 7B models and potentially larger models
- The assertion that SEUF is "the first dedicated unlearning framework for sparse MoE LLMs" without acknowledging related work on expert-level interventions
- The claim of "zero interference" when the paper still observes some utility drops

## Next Checks

**Check 1: Expert Attribution Ablation Study** - Systematically compare SEUF performance using different attribution methods (token assignment ratios vs. gating scores) and selection strategies (layer-wise vs. cross-layer consensus) to determine the optimal attribution approach.

**Check 2: Parameter Update Analysis** - Measure and verify the actual percentage of parameters updated during SEUF unlearning across different model sizes and tasks, comparing against the claimed 0.06% to validate the parameter efficiency claim.

**Check 3: Long-term Stability Test** - After SEUF unlearning, perform standard fine-tuning on retain-set data and measure whether expert selection stability persists, or if the router reverts to selection shift behavior during subsequent adaptation.