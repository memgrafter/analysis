---
ver: rpa2
title: 'TEXT2AFFORD: Probing Object Affordance Prediction abilities of Language Models
  solely from Text'
arxiv_id: '2402.12881'
source_url: https://arxiv.org/abs/2402.12881
tags:
- affordance
- object
- reasoning
- objects
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TEXT2AFFORD, a novel dataset for evaluating
  object affordance prediction from text alone. The dataset contains 35,520 instances
  (2,368 sentence-object pairs annotated with 15 affordance classes each) to assess
  how well language models can predict object affordances given only textual context.
---

# TEXT2AFFORD: Probing Object Affordance Prediction abilities of Language Models solely from Text

## Quick Facts
- arXiv ID: 2402.12881
- Source URL: https://arxiv.org/abs/2402.12881
- Authors: Sayantan Adak; Daivik Agrawal; Animesh Mukherjee; Somak Aditya
- Reference count: 40
- One-line primary result: State-of-the-art language models achieve limited accuracy (<55%) on zero-shot affordance prediction from text alone

## Executive Summary
This paper introduces TEXT2AFFORD, a novel dataset for evaluating object affordance prediction from text alone. The dataset contains 35,520 instances (2,368 sentence-object pairs annotated with 15 affordance classes each) to assess how well language models can predict object affordances given only textual context. Experiments show that state-of-the-art models achieve limited accuracy (<55%) on zero-shot affordance prediction, with only modest improvement when combined with images or through fine-tuning. Few-shot in-context learning helps, but fine-tuning remains more effective. The results indicate that even advanced models struggle with grounded reasoning about object affordances from text alone.

## Method Summary
The authors create TEXT2AFFORD by extracting sentences containing object mentions from the XNLI dataset, then annotating each sentence-object pair with 15 possible affordances. They evaluate multiple model types including pre-trained language models (BERT, RoBERTa, BART), vision-language models (CLIP, ViLT, InstructBLIP), and large language models (FLAN-T5, Falcon, Llama-3, ChatGPT) using zero-shot, few-shot, and fine-tuning approaches. The evaluation includes MLM, NLI, and multimodal setups with synthetic image augmentation.

## Key Results
- Text-only models achieve <55% accuracy on zero-shot affordance prediction
- Fine-tuning on commonsense reasoning datasets improves performance
- Multimodal models with synthetic images show modest improvements over text-only models
- Few-shot in-context learning helps but is less effective than fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-only language models struggle to predict object affordances because they lack grounded visual knowledge of object properties.
- Mechanism: Affordance prediction requires understanding physical properties (shape, size, texture) that typically come from visual input. Text-only models can only infer affordances from linguistic context, leading to limited accuracy (<55%) on zero-shot tasks.
- Core assumption: Physical object properties are essential for accurate affordance prediction and cannot be fully captured through text alone.
- Evidence anchors:
  - [abstract]: "Experiments show that state-of-the-art models achieve limited accuracy (<55%) on zero-shot affordance prediction"
  - [section]: "Authors in Bender and Koller (2020) have pointed the lack of symbol grounding to be a fundamental factor behind PTLMs failing to grasp meaning from form (surface form text)"
  - [corpus]: Weak evidence - neighboring papers focus on multimodal affordance learning, supporting the need for visual input
- Break condition: If language models could implicitly learn all necessary physical properties from text alone, this mechanism would break down.

### Mechanism 2
- Claim: Fine-tuning on commonsense reasoning datasets improves affordance prediction by providing additional reasoning capabilities.
- Mechanism: Commonsense reasoning datasets help models learn abstract reasoning patterns that transfer to affordance prediction, as both require understanding object properties and their implications.
- Core assumption: Affordance prediction and commonsense reasoning share underlying reasoning capabilities that can be transferred through fine-tuning.
- Evidence anchors:
  - [section]: "We observe that the fine-tuned model on commonsense reasoning task... show improved performance for the affordance prediction task"
  - [abstract]: "Through few-shot fine-tuning, we demonstrate improvement in affordance knowledge in PTLMs"
  - [corpus]: Mixed evidence - some neighboring papers focus on commonsense reasoning, others on multimodal learning
- Break condition: If affordance prediction requires domain-specific knowledge that cannot be transferred from commonsense reasoning, this mechanism would fail.

### Mechanism 3
- Claim: Multimodal models perform better on affordance prediction when provided with images, but still struggle with text-only input.
- Mechanism: Images provide crucial visual information about object properties that text alone cannot convey, enabling more accurate affordance prediction when combined with language models.
- Core assumption: Visual information about object properties is necessary for accurate affordance prediction, and multimodal models can leverage this information effectively.
- Evidence anchors:
  - [abstract]: "The performance gets slightly enhanced when using powerful VLMs in presence of synthetic images"
  - [section]: "In case of NLI based setup, the fine-tuned RoBERTa and BART models show improvement in the performance"
  - [corpus]: Strong evidence - neighboring papers focus on multimodal affordance learning and visual-language models
- Break condition: If text alone could provide sufficient information about object properties, this mechanism would break down.

## Foundational Learning

- Concept: Grounding in NLP
  - Why needed here: Understanding why text-only models fail at affordance prediction requires grasping the concept of grounding - the ability to connect linguistic symbols to real-world referents
  - Quick check question: Why do language models trained only on text struggle with tasks requiring physical reasoning about objects?

- Concept: Commonsense reasoning transfer
  - Why needed here: The paper shows that fine-tuning on commonsense reasoning datasets improves affordance prediction, suggesting transfer of reasoning capabilities
  - Quick check question: How might training on commonsense reasoning tasks help a model predict object affordances?

- Concept: Multimodal learning advantages
  - Why needed here: Understanding why multimodal models perform better than text-only models requires knowledge of how different modalities complement each other
  - Quick check question: What visual information about objects might be missing from text descriptions but crucial for affordance prediction?

## Architecture Onboarding

- Component map:
  Data pipeline: TEXT2AFFORD dataset (2,368 sentence-object pairs × 15 affordance classes × 3 annotations) → PTLMs (BERT, RoBERTa, BART), VLMs (CLIP, ViLT, InstructBLIP), LLMs (FLAN-T5, Falcon, Llama-3, ChatGPT) → Task types: MLM, NLI, text-only probing, multimodal probing → Evaluation: Accuracy, mAP@K

- Critical path:
  1. Dataset annotation and quality control
  2. Zero-shot evaluation of multiple model types
  3. Fine-tuning experiments on commonsense datasets
  4. Few-shot in-context learning experiments
  5. Analysis and interpretation of results

- Design tradeoffs:
  - Text-only vs. multimodal approaches: Text-only is more scalable but less accurate
  - Zero-shot vs. fine-tuning: Zero-shot is more general but less accurate than fine-tuning
  - Synthetic vs. real images: Synthetic images are easier to generate but may be less realistic

- Failure signatures:
  - Low accuracy (<55%) on zero-shot affordance prediction
  - No improvement from commonsense fine-tuning
  - Multimodal models performing worse than text-only models

- First 3 experiments:
  1. Zero-shot MLM evaluation of RoBERTa-large on TEXT2AFFORD
  2. Fine-tuning BERT-large on PIQA then evaluating on TEXT2AFFORD
  3. Retrieval-based image augmentation with CLIP-ViT for affordance prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific visual properties or features do multimodal models capture that improve affordance prediction compared to text-only models?
- Basis in paper: [explicit] The paper shows that ensembling language models with multimodal models improves performance, suggesting that visual information provides complementary affordances knowledge
- Why unresolved: The paper doesn't analyze what specific visual features (object shape, texture, spatial relationships) contribute most to affordance prediction
- What evidence would resolve it: Ablation studies testing different types of visual features (shape, texture, context) in isolation, or analyzing model attention maps to identify what visual elements drive predictions

### Open Question 2
- Question: How transferable is affordance knowledge across different domains or object categories?
- Basis in paper: [inferred] The paper shows that few-shot learning and fine-tuning on commonsense datasets improves affordance prediction, suggesting some transfer exists but is limited
- Why unresolved: The paper doesn't test how well models generalize affordance knowledge from common objects to novel or domain-specific objects
- What evidence would resolve it: Cross-domain experiments testing affordance prediction on objects from different domains (household vs industrial vs biological objects)

### Open Question 3
- Question: What is the relationship between affordance prediction ability and other forms of physical reasoning?
- Basis in paper: [explicit] The paper shows that fine-tuning on TEXT2AFFORD improves performance on physical reasoning datasets like PROST
- Why unresolved: The paper doesn't establish whether affordance knowledge is a prerequisite for physical reasoning or if they develop independently
- What evidence would resolve it: Controlled experiments testing whether improving affordance prediction specifically (not general reasoning) transfers to physical reasoning tasks

## Limitations
- The dataset size (2,368 sentence-object pairs) may limit generalizability of findings
- Synthetic image generation process is not fully specified, raising concerns about artifacts or biases
- The paper does not address potential domain shifts between the XNLI source corpus and real-world affordance scenarios
- Evaluation focuses primarily on accuracy metrics without exploring failure case analysis or human evaluation

## Confidence
- **High confidence**: The finding that text-only models achieve limited accuracy (<55%) on zero-shot affordance prediction is well-supported by experimental results across multiple model types.
- **Medium confidence**: The claim that fine-tuning on commonsense reasoning datasets improves affordance prediction is supported by results but the mechanism of transfer remains underspecified.
- **Medium confidence**: The assertion that multimodal models perform better with images but still struggle with text-only input is supported by results, though the synthetic nature of images introduces uncertainty.

## Next Checks
1. Conduct human evaluation studies comparing model predictions against human annotations on a held-out test set to validate the quality and reliability of the dataset annotations.
2. Perform ablation studies on the synthetic image generation process to determine the impact of different image quality levels and prompt variations on multimodal model performance.
3. Test model performance on real-world object affordance scenarios from robotics or robotics simulation datasets to assess practical applicability beyond the curated TEXT2AFFORD dataset.