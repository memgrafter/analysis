---
ver: rpa2
title: 'M-RewardBench: Evaluating Reward Models in Multilingual Settings'
arxiv_id: '2410.15522'
source_url: https://arxiv.org/abs/2410.15522
tags:
- reward
- language
- multilingual
- performance
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work constructs M-RewardBench, the first multilingual benchmark
  for reward model evaluation, covering 23 typologically diverse languages and 2.87k
  preference instances across chat, safety, reasoning, and translation tasks. Systematic
  evaluation of 24 reward models reveals a significant performance gap between English
  and non-English languages, with maximum drops up to 13% in multilingual settings.
---

# M-RewardBench: Evaluating Reward Models in Multilingual Settings

## Quick Facts
- arXiv ID: 2410.15522
- Source URL: https://arxiv.org/abs/2410.15522
- Reference count: 18
- Constructs first multilingual reward model benchmark covering 23 languages

## Executive Summary
This paper introduces M-RewardBench, the first multilingual benchmark for evaluating reward models across 23 typologically diverse languages. The benchmark includes 2.87k preference instances covering chat, safety, reasoning, and translation tasks. Systematic evaluation of 24 reward models reveals significant performance gaps between English and non-English languages, with up to 13% drops in multilingual settings. The study demonstrates that generative reward models show superior multilingual generalization, while translation quality and language resource availability positively correlate with reward model performance.

## Method Summary
The authors construct M-RewardBench by translating and filtering 2.87k preference instances from existing English datasets (RewardBench and MAPLE) into 23 typologically diverse languages. They evaluate 24 reward models including classifier, generative, and implicit types using accuracy metrics that compare predicted preferences against human-annotated reference labels. The benchmark covers four task types: chat, safety, reasoning, and translation. Performance is analyzed across linguistic dimensions including resource availability, language family, and script type.

## Key Results
- Reward models show up to 13% performance drop in multilingual settings compared to English
- Generative reward models demonstrate superior multilingual generalization compared to classifier and implicit models
- Translation quality improvement (+1-3%) correlates with better reward model performance
- Higher language resource availability correlates with improved reward model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative reward models exhibit superior multilingual generalization compared to classifier and implicit models
- Mechanism: Generative models leverage broader language understanding capabilities, allowing them to better handle linguistic variations and cultural nuances across languages
- Core assumption: The same architectural strengths that enable generative models to perform well in text generation also apply to reward modeling tasks in multilingual settings
- Evidence anchors:
  - [abstract]: "Generative reward models demonstrate superior multilingual generalization compared to classifier and implicit models"
  - [section 5.1]: "Generative RMs occupy higher positions in the chart suggesting strong multilingual LLM-as-judge capabilities compared to other RM types"
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- Break Condition: If generative models show performance degradation in low-resource languages where training data is scarce

### Mechanism 2
- Claim: Translation quality positively impacts reward model performance
- Mechanism: Higher-quality translations preserve semantic meaning and cultural context better, enabling reward models to make more accurate preference judgments
- Core assumption: The reward model's ability to evaluate responses depends on receiving inputs that maintain the original intent and quality of the English source material
- Evidence anchors:
  - [abstract]: "We show that the performance of RMs is improved with improved translation quality"
  - [section 6.1]: "We find that translation quality influences reward model performance across all model types... +1-3% when using a better automatic translator"
  - [corpus]: Weak - only indirect evidence from translation quality studies, not specific to reward modeling
- Break Condition: If reward models demonstrate language-specific evaluation capabilities that are independent of translation quality

### Mechanism 3
- Claim: Language resource availability correlates positively with reward model performance
- Mechanism: Models trained on more abundant data for certain languages develop stronger representations and evaluation capabilities for those languages
- Core assumption: The quantity and quality of available training data for a language directly influences the reward model's ability to accurately evaluate responses in that language
- Evidence anchors:
  - [abstract]: "Similarly, we demonstrate that the models exhibit better performance for high-resource languages"
  - [section 6.2]: "We study the influence of resource availability... The trend demonstrates that RMs tend to perform better on data-rich languages"
  - [corpus]: Weak - no direct corpus evidence for reward modeling specifically, though language resource literature supports this claim
- Break Condition: If reward models can effectively transfer knowledge from high-resource to low-resource languages without degradation

## Foundational Learning

- Concept: Bradley-Terry model for preference learning
  - Why needed here: The paper mentions RMs are trained based on the Bradley-Terry model, which is fundamental to understanding how preference data is converted into reward signals
  - Quick check question: How does the Bradley-Terry model transform pairwise comparisons into a probability distribution for ranking?

- Concept: Reward model evaluation metrics (accuracy scoring)
  - Why needed here: The paper evaluates RMs using accuracy metrics that compare predicted preferences against human-annotated reference labels
  - Quick check question: What is the difference between accuracy metrics for reward models versus traditional classification tasks?

- Concept: Multilingual data quality assessment
  - Why needed here: The paper emphasizes the importance of translation quality and resource availability, requiring understanding of how to evaluate multilingual data quality
  - Quick check question: What metrics would you use to quantify the quality of machine-translated text for reward modeling purposes?

## Architecture Onboarding

- Component map: Preference instances (prompt, chosen response, rejected response) -> Reward model inference -> Preference prediction -> Accuracy calculation -> Performance analysis
- Critical path: Translation → Model Inference → Preference Prediction → Accuracy Calculation → Performance Analysis
- Design tradeoffs:
  - Translation method: Automatic translation (scalable but potentially lower quality) vs. human translation (higher quality but expensive)
  - Model selection: Generative models (better multilingual performance) vs. classifier models (more efficient inference)
  - Evaluation granularity: Per-language analysis vs. aggregated multilingual scores
- Failure signatures:
  - Large performance gaps between English and non-English languages
  - Inconsistent preferences across languages for the same content
  - Sensitivity to translation quality variations
  - Poor performance on low-resource languages despite high performance on high-resource languages
- First 3 experiments:
  1. Compare reward model performance using Google Translate vs. NLLB translations for the same multilingual instances
  2. Evaluate the correlation between language resource availability and reward model accuracy across the 23 languages
  3. Test whether fine-tuning reward models on multilingual data reduces the English-non-English performance gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does high performance on M-RewardBench correlate with high performance on downstream multilingual benchmarks when reward models are used for DPO or PPO fine-tuning?
- Basis in paper: [inferred] The paper explicitly states this as a limitation, noting that it is unclear if high performance on M-RewardBench correlates to high performance on downstream multilingual benchmarks.
- Why unresolved: This question remains unanswered because the study focused on evaluating reward models directly on M-RewardBench without conducting experiments to measure their impact on downstream DPO or PPO performance.
- What evidence would resolve it: Conducting experiments that use high-performing reward models from M-RewardBench to fine-tune language models using DPO or PPO, then evaluating the resulting models on established multilingual benchmarks like Global-MMLU or INCLUDE would provide the necessary evidence.

### Open Question 2
- Question: How would reward model performance and rankings change if human-written translations were used instead of automatic translations for M-RewardBench?
- Basis in paper: [inferred] The paper mentions this as a limitation, noting that they did not explore whether performance would change with human-written translations, though they observed that using higher-quality automatic translators improved performance.
- Why unresolved: The study relied on automatic translations (Google Translate and NLLB) for scalability, but did not compare these to human-written translations which might better preserve cultural nuances and preferences.
- What evidence would resolve it: Creating a version of M-RewardBench with human-written translations for a subset of the data and re-evaluating the same set of reward models would reveal whether rankings and absolute performance scores differ significantly.

### Open Question 3
- Question: How do cultural preferences impact reward model evaluations in multilingual settings, and can M-RewardBench be extended to explicitly test for cultural alignment?
- Basis in paper: [explicit] The paper acknowledges in the limitations section that M-RewardBench does not explicitly test for cultural preferences and notes that their analysis found instances where preferences inverted between English and other languages.
- Why unresolved: While the paper identified instances of preference inversion across languages, it did not systematically investigate the cultural factors driving these differences or create benchmarks specifically designed to test cultural alignment.
- What evidence would resolve it: Developing a systematic methodology for identifying culturally-specific preferences and incorporating these into M-RewardBench, then evaluating reward models on these culturally-aligned instances, would clarify how cultural factors impact multilingual reward modeling.

## Limitations
- Reliance on automatically translated data may introduce quality variations affecting reward model performance
- Benchmark covers only 2.87k instances across 23 languages, potentially insufficient for robust generalization claims
- Evaluation methodology depends on accuracy metrics comparing against human-annotated reference labels without reporting inter-annotator agreement

## Confidence

- Generative reward models show superior multilingual generalization: Medium confidence
- Translation quality positively impacts performance: Medium confidence
- Language resource availability correlates with performance: High confidence

## Next Checks

1. Conduct a controlled experiment comparing reward model performance on human-translated vs. machine-translated instances of the same content to isolate the translation quality effect from other confounding variables.

2. Perform an ablation study on generative reward models to identify which architectural components contribute most to multilingual generalization, separating effects of pretraining data, model size, and fine-tuning procedures.

3. Test the transferability hypothesis by fine-tuning high-performing reward models on low-resource languages and measuring whether performance gains persist, which would validate the language resource correlation claim causally rather than correlationally.