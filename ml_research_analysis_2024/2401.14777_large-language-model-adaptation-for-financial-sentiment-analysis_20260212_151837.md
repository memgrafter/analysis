---
ver: rpa2
title: Large Language Model Adaptation for Financial Sentiment Analysis
arxiv_id: '2401.14777'
source_url: https://arxiv.org/abs/2401.14777
tags:
- financial
- dataset
- llms
- language
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a two-stage fine-tuning approach for adapting
  smaller large language models (1.4B parameters) to the financial domain. The method
  first further pre-trains models on mixed financial and general domain documents,
  then fine-tunes them on an augmented instruction dataset.
---

# Large Language Model Adaptation for Financial Sentiment Analysis

## Quick Facts
- arXiv ID: 2401.14777
- Source URL: https://arxiv.org/abs/2401.14777
- Reference count: 4
- Small LLMs (1.4B parameters) adapted to financial domain outperform GPT-4 on sentiment analysis tasks

## Executive Summary
This study presents a two-stage fine-tuning approach for adapting smaller large language models (1.4B parameters) to the financial domain. The method first further pre-trains models on mixed financial and general domain documents, then fine-tunes them on an augmented instruction dataset. Using Pythia-1.4B and OPT-1.3B as foundation models, the approach achieves strong performance on financial sentiment analysis tasks while using significantly fewer parameters than comparable models like FinMA-30B. The instruction dataset was expanded using synthetic samples generated by LLaMA-2-13B, improving performance on classification tasks.

## Method Summary
The approach uses a two-stage fine-tuning process. First, foundation models are further pre-trained on a mixed dataset of financial documents (EDGAR, Reuters, in-house data, The Pile) and general domain documents for 2 epochs with AdamW optimizer (learning rate 1e-4, weight decay 0.1, batch size 32, gradient accumulation 4). Second, the best checkpoint from pre-training is fine-tuned on an augmented instruction dataset for 1 epoch with the same hyperparameters. The instruction dataset combines original financial NLP datasets (FPB, FIQA-SA, Headlines, NER) with synthetic samples generated by LLaMA-2-13B. Models are evaluated using the FLARE benchmark for financial sentiment analysis and other financial NLP tasks.

## Key Results
- Adapted small LLMs (Pythia-1.4B, OPT-1.3B) outperform GPT-4 on financial sentiment analysis tasks
- Models achieve comparable results to much larger FinMA-30B model while using 21x fewer parameters
- Augmented instruction dataset with synthetic samples improves classification performance over using original datasets alone

## Why This Works (Mechanism)
The two-stage fine-tuning approach works by first establishing domain-specific knowledge through pre-training on financial documents, then adapting to instruction-following capabilities through fine-tuning on task-specific instructions. The use of synthetic data augmentation allows the model to encounter a broader range of instruction patterns without requiring extensive manual annotation. The relatively small model size (1.4B parameters) enables efficient fine-tuning while maintaining sufficient capacity for domain adaptation when properly initialized and trained.

## Foundational Learning
- **Fine-tuning vs. Pre-training**: Fine-tuning adapts a pre-trained model to a specific task, while pre-training establishes general language understanding. Both are needed - pre-training establishes financial domain knowledge, fine-tuning adapts to instruction following.
- **Synthetic Data Generation**: Using a larger LLM to generate training data expands the instruction dataset without manual annotation. This is needed to improve model performance on classification tasks by exposing it to more diverse instruction patterns.
- **Domain Adaptation**: Adapting general-purpose models to specific domains (financial) improves performance on domain-specific tasks. This is critical for achieving competitive results in specialized fields like finance.
- **Parameter Efficiency**: Smaller models (1.4B) can achieve comparable performance to larger models (30B) when properly domain-adapted. This enables more efficient deployment and fine-tuning.

## Architecture Onboarding
- **Component Map**: Financial Documents -> Pre-training -> Instruction Dataset -> Fine-tuning -> FLARE Evaluation
- **Critical Path**: Foundation Model -> Document Pre-training (2 epochs) -> Instruction Fine-tuning (1 epoch) -> Evaluation
- **Design Tradeoffs**: Smaller models require less compute but may need more careful fine-tuning; synthetic data improves classification but may have limitations for generative tasks.
- **Failure Signatures**: Underperformance on generative tasks indicates insufficient diversity in augmented instruction dataset; poor generalization suggests inadequate domain coverage in pre-training data.
- **First Experiments**: 1) Evaluate model on out-of-distribution financial tasks, 2) Conduct ablation study on synthetic vs. real instruction ratio, 3) Test approach on additional foundation model architectures.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of the adapted small LLMs compare to larger financial LLMs when fine-tuned with the same domain adaptation strategy?
- Basis in paper: The paper states that adapted small LLMs outperform GPT-4 and BloombergGPT but doesn't compare to larger models (e.g., LLaMA-2-7B) using the same fine-tuning approach.
- Why unresolved: The paper lacks direct comparison of larger LLMs fine-tuned with the same strategy.
- What evidence would resolve it: Conducting domain adaptation experiments on larger LLMs and comparing their performance with adapted small LLMs would clarify if improvements are due to model size or fine-tuning approach.

### Open Question 2
- Question: How does the performance of the adapted small LLMs generalize to other financial tasks not included in the FLARE benchmark?
- Basis in paper: The paper focuses on FLARE benchmark tasks and doesn't explore generalization to other financial tasks.
- Why unresolved: The paper doesn't provide information on performance across a broader range of financial tasks.
- What evidence would resolve it: Evaluating adapted models on diverse financial tasks (fraud detection, risk assessment, forecasting) would assess their versatility.

### Open Question 3
- Question: How does the quality and diversity of synthetic instructions impact the performance of adapted small LLMs?
- Basis in paper: The paper mentions using LLaMA-2-13B for synthetic instruction generation but doesn't analyze the quality, diversity, or biases of these instructions.
- Why unresolved: The paper lacks detailed analysis of synthetic instruction characteristics and their impact on model performance.
- What evidence would resolve it: Thorough analysis of synthetic instruction quality, diversity, and biases, followed by evaluation of their impact on adapted model performance.

## Limitations
- Focuses primarily on Pythia-1.4B and OPT-1.3B models, limiting understanding of approach transferability to other architectures
- Relies heavily on FLARE benchmark, which may not capture all aspects of financial domain performance
- Acknowledges potential limitations for generative tasks like named entity recognition due to synthetic data characteristics

## Confidence
- **High confidence** in methodology and implementation of two-stage fine-tuning approach
- **Medium confidence** in benchmark performance claims due to limited comparison details with larger models
- **Medium confidence** in generalizability across different financial NLP tasks

## Next Checks
1. Test the two-stage fine-tuning approach on additional foundation models (e.g., LLaMA, Mistral) to assess architecture transferability
2. Conduct ablation studies to determine the optimal ratio of synthetic to real instruction samples in the augmented dataset
3. Evaluate model performance on out-of-distribution financial tasks not included in the FLARE benchmark to assess true generalization capability