---
ver: rpa2
title: 'Hard ASH: Sparsity and the right optimizer make a continual learner'
arxiv_id: '2404.17651'
source_url: https://arxiv.org/abs/2404.17651
tags:
- learning
- hard
- continual
- task
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that a simple multilayer perceptron (MLP)
  with sparse activation functions and adaptive learning rate optimizers can achieve
  competitive performance in class-incremental continual learning on Split-MNIST,
  without requiring specialized continual learning algorithms. The paper introduces
  Hard Adaptive SwisH (Hard ASH), a novel sparse activation function that enhances
  learning retention compared to existing methods.
---

# Hard ASH: Sparsity and the right optimizer make a continual learner

## Quick Facts
- arXiv ID: 2404.17651
- Source URL: https://arxiv.org/abs/2404.17651
- Authors: Santtu Keskinen
- Reference count: 13
- This study demonstrates that a simple MLP with sparse activation functions and adaptive learning rate optimizers can achieve competitive performance in class-incremental continual learning on Split-MNIST.

## Executive Summary
This paper presents Hard ASH, a novel sparse activation function that enables a simple multilayer perceptron to achieve competitive performance in class-incremental continual learning without requiring specialized continual learning algorithms. The method combines sparse activations with adaptive optimizers (particularly Adagrad) to achieve 78.3% mean accuracy on Split-MNIST with only 1 training epoch per task, outperforming established approaches like EWC (61%) and SDMLP+EWC (83%) while using a simpler architecture. The results demonstrate that sparse representations and carefully tuned adaptive optimizers can serve as effective, conceptually simple continual learners.

## Method Summary
The method uses a simple MLP architecture with one hidden layer (1000 neurons) and a novel sparse activation function called Hard ASH, which is a clipped and thresholded version of the Adaptive SwisH (ASH) activation. The network is trained on class-incremental tasks (Split-MNIST) using adaptive optimizers (primarily Adagrad) for only 1 epoch per task, without any specialized continual learning algorithms like replay or regularization. The sparsity is controlled by the zk parameter, which determines the flatness of the activation function, reducing gradient flow and limiting weight updates to preserve old-task representations while learning new ones.

## Key Results
- Hard ASH with Adagrad achieved 78.3% mean accuracy over 5 runs on Split-MNIST
- Outperformed EWC (61% accuracy) and SDMLP+EWC (83% accuracy) with simpler architecture and fewer training epochs
- All sparse activation functions outperformed non-sparse functions, with Hard ASH showing best retention
- Only 1 training epoch per task required, compared to 500 epochs for baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse activation functions reduce catastrophic forgetting by limiting gradient updates to only a small subset of neurons per input.
- Mechanism: By making most activations zero and saturating gradients, the method constrains weight updates to active pathways, preserving representations for old tasks.
- Core assumption: Catastrophic forgetting occurs primarily due to overlapping weight updates that interfere with old-task representations.
- Evidence anchors:
  - [abstract] "sparse representations... provide strong regularization against catastrophic forgetting"
  - [section] "activation function should be flat in most places... reduce the gradient flow"
  - [corpus] Weak - no direct evidence from neighbor papers.
- Break condition: If sparsity becomes too extreme (e.g., zk â‰¥ 3), performance degrades during initial learning, suggesting a balance between sparsity and representational capacity.

### Mechanism 2
- Claim: Adaptive optimizers like Adagrad maintain plasticity and stability by automatically adjusting learning rates per parameter.
- Mechanism: Adagrad's per-parameter adaptive learning rates allow continued learning on new tasks while preserving old-task performance by reducing updates to frequently updated parameters.
- Core assumption: Fixed learning rates cause either rapid forgetting (high LR) or inability to learn new tasks (low LR).
- Evidence anchors:
  - [section] "Adagrad performed the best out of the optimizers tested"
  - [section] "adaptive, per parameter, learning rates" are key for continual learning
  - [corpus] Weak - no direct optimizer-related evidence in neighbor papers.
- Break condition: Using standard Adam without bias correction performs worse, suggesting the specific form of adaptation matters.

### Mechanism 3
- Claim: The combination of sparse activations and adaptive optimizers creates a synergistic effect that outperforms either component alone.
- Mechanism: Sparse activations reduce interference while adaptive optimizers maintain effective gradient flow where needed, creating a system that learns new tasks without overwriting old ones.
- Core assumption: Neither sparsity nor adaptive learning rates alone are sufficient; their combination is necessary for optimal performance.
- Evidence anchors:
  - [section] "combining sparsity with an adaptive learning rate optimizer is enough to make a conceptually simple but surprisingly effective continual learner"
  - [section] "all the sparse activation functions performed better than all of the non-sparse functions"
  - [corpus] Weak - no direct evidence of synergistic effects in neighbor papers.
- Break condition: When trained on i.i.d. data, the same hyperparameters perform worse than standard settings, suggesting the method is specifically tuned for task-incremental scenarios.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding why standard neural networks fail in continual learning scenarios is crucial for appreciating the proposed solution
  - Quick check question: What happens to a neural network's performance on old tasks when trained on new tasks without any special techniques?

- Concept: Sparse representations
  - Why needed here: The core innovation relies on making most neuron activations zero, which requires understanding how this affects learning and memory
  - Quick check question: How does forcing most activations to zero affect the network's ability to update weights during training?

- Concept: Adaptive learning rate optimization
  - Why needed here: The method depends on optimizers that adjust learning rates per parameter, which is fundamental to its success
  - Quick check question: How does Adagrad's per-parameter learning rate adaptation differ from standard SGD's single global learning rate?

## Architecture Onboarding

- Component map:
  Input layer (784 neurons) -> Hidden layer (1000 neurons with Hard ASH activation) -> Output layer (10 neurons with softmax)

- Critical path:
  1. Forward pass through sparse activation
  2. Loss computation
  3. Backward pass with limited gradient flow
  4. Parameter update with adaptive learning rates
  5. Repeat for new task while maintaining old-task performance

- Design tradeoffs:
  - Sparsity vs. representational capacity: Higher sparsity (zk > 3) degrades initial learning
  - Initial high learning rates vs. stability: Necessary for plasticity but can cause instability
  - Simple architecture vs. performance: MLP outperforms complex continual learning methods

- Failure signatures:
  - Rapid performance degradation on old tasks: Likely too little sparsity or wrong optimizer
  - Inability to learn new tasks: Possibly too much sparsity or inappropriate hyperparameters
  - Overall poor performance: Check if activation function and optimizer are properly tuned

- First 3 experiments:
  1. Test Hard ASH with Adagrad on Split-MNIST and measure per-task accuracy
  2. Compare with ReLU baseline using same optimizer and architecture
  3. Vary zk parameter to find optimal sparsity level for the given task sequence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Hard ASH compare to Top-K and other sparse activation functions in larger-scale continual learning tasks beyond Split-MNIST?
- Basis in paper: [explicit] The paper shows Hard ASH outperforming Top-K on Split-MNIST, but notes this was a relatively simple task with a small MLP
- Why unresolved: The study only tested on Split-MNIST with a single hidden layer MLP; scaling to more complex architectures and datasets remains untested
- What evidence would resolve it: Experiments on permuted MNIST, Split CIFAR, or more complex continual learning benchmarks with deeper networks

### Open Question 2
- Question: What is the relationship between sparsity level (controlled by zk parameter) and the stability-plasticity tradeoff in continual learning?
- Basis in paper: [inferred] The plasticity experiments showed varying zk had minimal effect on plasticity but improved stability, suggesting a complex relationship
- Why unresolved: The study only tested on permuted MNIST with Adagrad; the relationship may differ with other optimizers or architectures
- What evidence would resolve it: Systematic experiments varying zk across multiple datasets and optimizers, measuring both stability and plasticity metrics

### Open Question 3
- Question: How do adaptive optimizers with bias correction removed (like Adam without bias correction) compare to standard implementations across different continual learning scenarios?
- Basis in paper: [explicit] The paper found Adam without bias correction performed nearly as well as RMSprop, while standard Adam performed poorly
- Why unresolved: Only tested on Split-MNIST; bias correction's role in other architectures and tasks is unknown
- What evidence would resolve it: Benchmarking standard vs bias-corrected versions of adaptive optimizers across multiple continual learning tasks and architectures

### Open Question 4
- Question: What mechanisms beyond sparse activations are needed to achieve both high stability and high plasticity in continual learning?
- Basis in paper: [inferred] The plasticity experiments showed sparse representations improve stability but not plasticity, suggesting additional mechanisms are needed
- Why unresolved: The paper only tested sparse activations with standard optimization; novel methods combining multiple approaches weren't explored
- What evidence would resolve it: Experiments combining sparse activations with methods like Continual backprop, Shrink and perturb, or model expansion techniques

## Limitations
- Results primarily validated on a single benchmark (Split-MNIST) with limited testing on more complex datasets
- Optimal sparsity parameter appears task-dependent, with performance degrading if sparsity is too extreme
- Method's effectiveness with other neural network architectures (CNNs, transformers) remains unexplored
- Computational efficiency gains from sparsity are not quantified

## Confidence
- **High confidence**: The claim that sparse activations with adaptive optimizers improve continual learning performance on Split-MNIST is well-supported by the experimental results showing Hard ASH + Adagrad achieving 78.3% mean accuracy.
- **Medium confidence**: The mechanism explanations for why sparsity reduces catastrophic forgetting are plausible but lack direct experimental validation beyond the observed performance improvements.
- **Medium confidence**: The claim about Adagrad being the optimal optimizer is supported within the tested set, but the comparison may be incomplete as not all possible optimizer configurations were explored.

## Next Checks
1. **Cross-dataset validation**: Test the Hard ASH + Adagrad combination on Split-CIFAR10 and other image classification benchmarks to verify generalizability beyond MNIST.
2. **Hyperparameter sensitivity analysis**: Systematically vary the zk parameter and initial learning rates across a wider range to identify robustness boundaries and potential failure modes.
3. **Architecture scalability test**: Implement the method in a convolutional neural network and evaluate performance on more complex vision tasks to assess architectural limitations.