---
ver: rpa2
title: 'Knowledge Fusion of Chat LLMs: A Preliminary Technical Report'
arxiv_id: '2402.16107'
source_url: https://arxiv.org/abs/2402.16107
tags:
- llms
- fusion
- chat
- knowledge
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents FusionChat, a framework for knowledge fusion
  of chat LLMs, which extends the FuseLLM framework to fuse structurally and scale-varied
  chat LLMs into a more powerful one. FusionChat employs a two-stage approach: (1)
  pairwise knowledge fusion to obtain multiple target LLMs of identical structure
  and size via lightweight fine-tuning, and (2) model merging in the parameter space
  using a novel method (VARM) that determines merging weights based on the variation
  ratio of parameter matrices before and after fine-tuning.'
---

# Knowledge Fusion of Chat LLMs: A Preliminary Technical Report

## Quick Facts
- arXiv ID: 2402.16107
- Source URL: https://arxiv.org/abs/2402.16107
- Reference count: 19
- One-line primary result: FusionChat-7B outperforms all source LLMs and fine-tuned baselines, even surpassing GPT-3.5 (March) and approaching Mixtral-8x7B-Instruct

## Executive Summary
This paper presents FusionChat, a framework for knowledge fusion of chat LLMs that extends the FuseLLM framework to fuse structurally and scale-varied chat LLMs into a more powerful unified model. The approach uses a two-stage method: pairwise knowledge fusion to obtain target LLMs of identical structure via lightweight fine-tuning, followed by model merging using a novel variation ratio-based method (VARM). Experiments with three prominent chat LLMs demonstrate that FusionChat-7B outperforms all source models and baselines, achieving performance approaching Mixtral-8x7B-Instruct and surpassing GPT-3.5 (March) on MT-Bench.

## Method Summary
FusionChat employs a two-stage approach to knowledge fusion: first, pairwise knowledge fusion where each source LLM generates probability distribution matrices for a training dataset, which are then fused using MinCE and used to fine-tune a target LLM matching the pivot architecture; second, model merging in parameter space using VARM, which determines merging weights based on the variation ratio of parameter matrices before and after fine-tuning. This allows scalable integration of new source LLMs without retraining from scratch, supporting plug-and-play addition of models at any scale.

## Key Results
- FusionChat-7B outperforms all source LLMs (NH2-Mixtral-8x7B, NH2-Solar-10.7B, OpenChat-3.5-7B) and fine-tuned baselines at 7B and 34B scales
- Achieves performance surpassing GPT-3.5 (March) and approaching Mixtral-8x7B-Instruct on MT-Bench
- VARM-based merging with matrix-level granularity achieves the best results compared to SLERP, TA, and parameter-level merging
- Supports plug-and-play integration of new source LLMs without retraining from scratch

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FusionChat uses pairwise knowledge fusion to transfer knowledge from structurally diverse source LLMs into a target LLM of identical structure and size via lightweight fine-tuning.
- Mechanism: Each source LLM generates a probability distribution matrix for a training dataset. These matrices are fused (using MinCE) to create a target distribution, and the target LLM is fine-tuned to match it. This aligns the target's output distribution with the collective knowledge of the source models.
- Core assumption: The probability distribution matrix of a language model reflects its inherent knowledge and can be used as a transferable representation of its capabilities.
- Evidence anchors:
  - [abstract] "This approach externalizes the knowledge of multiple source LLMs using their generated probability distribution matrices and transfers their collective knowledge into a target LLM through lightweight continual training."
  - [section 3.3] "To incorporate fine-grained advantages of source LLMs, we introduce VARM (Variation Ratio Merge), a novel method for determining the merging weights based on the variation ratio of parameter matrices before and after fine-tuning."

### Mechanism 2
- Claim: VARM assigns merging weights to parameter matrices based on their variation ratio during fine-tuning, enabling fine-grained integration of distinct model strengths.
- Mechanism: For each parameter matrix, VARM computes the average squared variation before and after fine-tuning. The weight for each target model is proportional to its average squared variation, normalized across all target models.
- Core assumption: Parameter matrices that change more during fine-tuning (higher variation ratio) are more important for integrating knowledge from the corresponding source model.
- Evidence anchors:
  - [section 3.3] "Since the parameters of the target LLMs continuously evolve to align their generated distribution matrices with the corresponding source LLMs, we propose Variation Ratio Merge (VARM) to utilize the variation ratio of parameters before and after fine-tuning each target LLM as an indicator of knowledge updates, determining its importance in the Merge function."
  - [section 4.3] "It is observed that FusionChat-7B with SLERP, TA, and our VaRM outperform all the fine-tuned target LLMs, showcasing FUSION CHAT's ability to integrate the unique strengths and collective capabilities of different target LLMs."

### Mechanism 3
- Claim: The fuse-then-merge strategy allows scalable integration of new source LLMs without retraining from scratch.
- Mechanism: Pairwise fusion creates target LLMs matching the pivot architecture. New source models can be fused with the pivot and merged with the existing fused model, avoiding the need to combine all source distributions simultaneously.
- Core assumption: Pairwise fusion reduces the complexity of combining multiple structurally diverse models compared to direct fusion of all sources.
- Evidence anchors:
  - [section 3.4] "The framework of FUSE LLM requires the combination of distribution matrices from all source LLMs during continual training, which does not seamlessly support the inclusion of new source LLMs. In contrast, FUSION CHAT supports plug-and-play integration of a new source LLM at any scale, requiring only obtaining a target LLM by fusing the new source LLM and the pivot, and then merging it with the existing version of FUSION CHAT."

## Foundational Learning

- Concept: Probability distribution matrices as knowledge representations
  - Why needed here: FusionChat's knowledge transfer relies on generating and fusing probability distributions from source models. Understanding how these matrices encode model knowledge is critical.
  - Quick check question: How does a probability distribution matrix differ from a single token prediction, and why is it more useful for knowledge fusion?

- Concept: Model merging in parameter space
  - Why needed here: FusionChat's final stage merges multiple fine-tuned target models. Understanding different merging strategies (linear, SLERP, TA, etc.) and their tradeoffs is essential.
  - Quick check question: What are the advantages and disadvantages of merging models at different granularities (model, layer, matrix, parameter)?

- Concept: Variation ratio as importance metric
  - Why needed here: VARM uses parameter variation during fine-tuning to determine merging weights. Understanding how parameter changes reflect knowledge updates is key to the method.
  - Quick check question: Why might parameter matrices with higher variation ratios be more important for the final fused model?

## Architecture Onboarding

- Component map: Data preparation -> Probability distribution generation -> Pairwise fusion (pivot + each source) -> Fine-tuning of target models -> VARM merging -> Evaluation
- Core modules: Distribution generator, MinCE fusion, fine-tuning trainer, VARM merger
- Critical path: Data preparation → pairwise fusion (pivot + each source) → fine-tuning of target models → VARM merging → evaluation
- Design tradeoffs:
  - Pairwise vs. direct fusion: Pairwise reduces complexity but may miss cross-model interactions
  - VARM granularity: Matrix-level gives best results, but parameter-level disrupts correlations
  - Training vs. merging: Fine-tuning is lightweight, but merging requires careful weight assignment
- Failure signatures:
  - Degraded performance after merging: Likely due to inappropriate merging weights or granularity
  - Inconsistent domain performance: May indicate unbalanced knowledge transfer during pairwise fusion
  - Training instability: Could result from improper loss weighting (λ in equation 5)
- First 3 experiments:
  1. Implement pairwise fusion with a simple averaging strategy and compare to VARM
  2. Test different merging granularities (model, layer, matrix) to verify VARM's matrix-level advantage
  3. Evaluate the impact of different λ values in the combined loss function on final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FusionChat scale with the number of source LLMs fused, and is there an optimal number beyond which performance plateaus or degrades?
- Basis in paper: [inferred] The paper demonstrates fusion of three LLMs and suggests future work exploring fusion of more source LLMs.
- Why unresolved: The paper only experiments with three source LLMs and does not investigate how performance changes with different numbers of fused models.
- What evidence would resolve it: Experiments fusing varying numbers of LLMs (e.g., 2, 4, 6, 10) and measuring performance on benchmarks to identify scaling patterns.

### Open Question 2
- Question: What is the impact of different merging granularities (e.g., layer-level vs parameter-level) on the ability to preserve and combine specific capabilities from individual source LLMs?
- Basis in paper: [explicit] The paper discusses different merging granularities and shows matrix-level merging performs best, but does not deeply analyze why or explore the preservation of specific capabilities.
- Why unresolved: The paper only shows that matrix-level merging achieves the best overall performance, without analyzing which capabilities are preserved or lost at different granularities.
- What evidence would resolve it: Detailed analysis of how different merging granularities affect specific capabilities (e.g., reasoning, coding, creative writing) and whether certain capabilities are better preserved at particular granularities.

### Open Question 3
- Question: How does the knowledge fusion framework perform when fusing LLMs trained on different datasets or with different training objectives (e.g., chat-focused vs general-purpose)?
- Basis in paper: [inferred] The paper uses chat LLMs but does not explicitly explore how differences in training data or objectives affect fusion outcomes.
- Why unresolved: The paper uses chat LLMs but does not investigate how variations in training approaches impact the effectiveness of knowledge fusion.
- What evidence would resolve it: Experiments fusing LLMs with different training datasets (e.g., chat-focused, code-focused, general web data) and objectives to measure how these differences affect fusion performance across various tasks.

## Limitations

- Training Data Dependency: The success of FusionChat heavily depends on the quality and diversity of the FUSION CHAT MIXTURE dataset, which is not fully specified.
- Evaluation Scope: The evaluation is limited to MT-Bench, a multi-turn dialogue benchmark, without testing factual accuracy, reasoning depth, or robustness to adversarial prompts.
- Computational Efficiency: The resource requirements for pairwise fusion and model merging are not thoroughly analyzed, and scalability to larger models remains unclear.

## Confidence

**High Confidence**: The pairwise fusion approach and VARM method are technically sound and well-explained. The experimental setup and baseline comparisons are clearly defined, supporting the claim that FusionChat outperforms source models and fine-tuned baselines.

**Medium Confidence**: The claim that FusionChat approaches Mixtral-8x7B-Instruct performance is supported by MT-Bench results, but the evaluation is limited to a single benchmark. The generalization of these results to other tasks or domains is uncertain.

**Low Confidence**: The paper's claim about seamless plug-and-play integration of new source models is theoretically supported but not empirically validated. The practical implications of this feature for real-world applications are unclear.

## Next Checks

1. **Cross-Domain Evaluation**: Test FusionChat on diverse benchmarks beyond MT-Bench, including factual accuracy tests, reasoning challenges, and adversarial prompts to assess generalization and robustness.

2. **Scalability Analysis**: Evaluate the computational efficiency and performance of FusionChat when scaling to larger models (e.g., 70B parameters) or incorporating additional source models beyond the three used in the study.

3. **Dataset Transparency**: Obtain or recreate the FUSION CHAT MIXTURE dataset with detailed composition information to verify the reproducibility of results and assess potential biases in the training data.