---
ver: rpa2
title: 'Graph Neural Machine: A New Model for Learning with Tabular Data'
arxiv_id: '2402.02862'
source_url: https://arxiv.org/abs/2402.02862
tags:
- graph
- nodes
- neural
- number
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a novel connection between multi-layer perceptrons
  (MLPs) and graph neural networks (GNNs), showing that MLPs are equivalent to asynchronous
  message passing GNNs on directed acyclic graphs. Based on this insight, the authors
  propose Graph Neural Machines (GNMs), which generalize MLPs by replacing the acyclic
  graph with a nearly complete directed graph and employing synchronous message passing.
---

# Graph Neural Machine: A New Model for Learning with Tabular Data

## Quick Facts
- arXiv ID: 2402.02862
- Source URL: https://arxiv.org/abs/2402.02862
- Reference count: 40
- Primary result: GNMs consistently outperform or match MLPs on 15 classification and 6 regression tabular datasets, with better resistance to overfitting when model size increases.

## Executive Summary
This paper introduces Graph Neural Machines (GNMs), a novel neural architecture that generalizes multi-layer perceptrons (MLPs) by replacing their directed acyclic graph structure with a nearly complete directed graph and employing synchronous message passing. The authors establish a theoretical equivalence between MLPs and asynchronous message passing graph neural networks (GNNs) on directed acyclic graphs, which motivates the GNM design. GNMs can simulate multiple MLPs simultaneously and demonstrate universal approximation capabilities. Experimental results on 15 classification and 6 regression datasets show that GNMs consistently match or outperform MLPs, particularly in regression tasks, while showing better resistance to overfitting when model size increases.

## Method Summary
The GNM model is implemented as a synchronous message passing network on a nearly complete directed graph, where every node can connect to every other node (including self-loops and bias-to-all connections). The architecture consists of input nodes (one per feature), a single bias node, hidden nodes, and output nodes. Edge weights are learned through trainable adjacency matrices for each layer, with all layers employing the same aggregation function. During training, GNMs are compared against MLPs with equivalent parameter budgets on various tabular datasets, using Adam optimizer with tuned learning rates and regularization techniques including L1 penalties to encourage sparsity.

## Key Results
- GNMs consistently outperform or match MLPs on 15 classification and 6 regression datasets, with particularly strong performance in regression tasks
- GNMs demonstrate better resistance to overfitting compared to MLPs when increasing model size, showing smaller gaps between training and validation loss
- A single GNM can simulate multiple MLPs simultaneously, providing a universal approximation capability while learning optimal computational graph structures automatically

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GNM generalizes MLP by replacing the directed acyclic graph with a nearly complete directed graph and using synchronous message passing
- Mechanism: By converting the acyclic MLP graph into a dense graph where every node can connect to every other node, GNM allows multiple paths of information flow per update step. Synchronous updates let each node simultaneously integrate information from all incoming neighbors, whereas MLPs update layers sequentially
- Core assumption: The dense graph representation and synchronous updates do not degrade the ability to approximate continuous functions compared to the layered MLP structure
- Evidence anchors:
  - [abstract] "Graph Neural Machine (GNM), which replaces the MLP's directed acyclic graph with a nearly complete graph and which employs a synchronous message passing scheme"
  - [section 5] "The GNM model consists of a series of message passing layers... All these layers employ the same AGGREGATE (k) function"
  - [corpus] No direct evidence in corpus neighbors; claim is theoretical
- Break condition: If dense connectivity causes over-parameterization without regularization, training may diverge or overfit despite the theoretical approximation capability

### Mechanism 2
- Claim: A single GNM can simulate multiple MLPs simultaneously, making it a universal approximator
- Mechanism: By learning a set of adjacency matrices (one per layer) over a dense graph, GNM can encode multiple sub-networks within one architecture. The learned weights can be structured to mimic several MLPs with different topologies
- Core assumption: The parameter space of GNM strictly contains that of all MLPs with the same number of non-bias nodes and layers
- Evidence anchors:
  - [section 5] "We first show that when the number of hidden neurons is greater than 1, a single GNM model can simulate several different MLPs"
  - [section 5] "Theorem 5.1... ∀MMLP ∈ MLPN,K , ∃MGNM ∈ GNMN,K such that MGNM represents MMLP"
  - [corpus] No direct evidence in corpus neighbors; claim is theoretical
- Break condition: If the optimization landscape is too complex for gradient descent to find the MLP-simulating sub-network within the dense graph, the universal approximation property is not realized in practice

### Mechanism 3
- Claim: GNM is more robust to overfitting than MLP when model size increases
- Mechanism: The dense graph and synchronous updates allow the model to learn sparse effective subnetworks during training. Regularization (e.g., ℓ1 penalties) can prune unnecessary connections
- Core assumption: Sparsity-inducing regularization combined with the dense initial topology leads to better generalization than a dense MLP with fixed acyclic layers
- Evidence anchors:
  - [section 6.4] "The MLP suffers significantly from overfitting when the number of parameters is large... the proposed GNM model seems to be less prone to overfitting"
  - [section 6.5] "To achieve sparsity, we added an ℓ1 regularization term to the loss function"
  - [corpus] No direct evidence in corpus neighbors; claim is empirical
- Break condition: If regularization is too weak or inappropriate for the dataset, the dense graph may still overfit; if too strong, performance may degrade

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: GNM is defined as a synchronous message passing model on a dense graph; understanding how node features are aggregated and updated is essential to implementing and debugging it
  - Quick check question: In a synchronous GNN layer, are node updates computed sequentially or in parallel across all nodes?

- Concept: Universal approximation theory for neural networks
  - Why needed here: GNM's claim as a universal approximator relies on the theorem that MLPs can approximate any continuous function; this property extends to GNM by inclusion
  - Quick check question: What condition must a neural network satisfy to be a universal approximator of continuous functions on compact sets?

- Concept: Overfitting and regularization techniques (ℓ1, ℓ2 penalties, dropout)
  - Why needed here: GNM's dense graph can lead to overparameterization; understanding how to apply regularization and prune connections is critical for practical use
  - Quick check question: How does an ℓ1 regularization term encourage sparsity in the learned adjacency matrices?

## Architecture Onboarding

- Component map: Input nodes -> Hidden nodes -> Output nodes, with bias node connected to all nodes
- Critical path:
  1. Initialize adjacency matrices A(1), ..., A(K) for K layers
  2. For each input sample, set input node features to feature values, bias node to 1, others to 0
  3. For each layer k = 1 to K: Update all non-bias node features: h(k) = f(A(k) · h(k-1)); bias node remains 1
  4. Use output node features for prediction and loss computation
- Design tradeoffs:
  - Dense graph: higher expressive power but more parameters and risk of overfitting
  - Synchronous updates: parallel computation but less inductive bias than sequential layers
  - Number of nodes vs. layers: more nodes increase parameter count quadratically; more layers increase linearly
- Failure signatures:
  - Training loss decreases but validation loss increases → overfitting
  - All weights remain large despite ℓ1 penalty → insufficient regularization
  - Convergence is very slow or unstable → learning rate or architecture size may be inappropriate
- First 3 experiments:
  1. Train GNM on a small synthetic dataset (e.g., XOR) with and without ℓ1 regularization; compare learned adjacency matrices to verify sparsity
  2. Vary the number of nodes and layers on a simple tabular dataset; measure training time and overfitting trends relative to MLP
  3. Apply GNM to a real tabular dataset (e.g., Adult); compare accuracy/F1 to MLP with the same parameter budget and observe robustness to increasing model size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the expressive power of GNM compare to other neural architectures beyond MLPs, such as transformers or CNNs?
- Basis in paper: [explicit] The paper shows GNM's universal approximation capability and its ability to simulate MLPs, but does not compare its expressive power to other neural architectures
- Why unresolved: The paper focuses on establishing GNM's relationship to MLPs and demonstrating its universal approximation capability, but does not explore its potential advantages or disadvantages compared to other neural architectures
- What evidence would resolve it: Comparative studies evaluating GNM's performance against transformers, CNNs, and other neural architectures on various tasks and datasets would provide insights into its expressive power relative to other models

### Open Question 2
- Question: What is the impact of different graph structures on GNM's performance and learning dynamics?
- Basis in paper: [explicit] The paper proposes GNM as a generalization of MLPs by allowing a wider range of graph structures, but does not extensively explore the effects of different graph structures on performance
- Why unresolved: While the paper demonstrates GNM's potential by replacing the directed acyclic graph with a nearly complete directed graph, it does not investigate how different graph structures might influence learning and performance
- What evidence would resolve it: Experiments comparing GNM's performance using various graph structures, such as sparse graphs, graphs with specific topologies, or graphs learned during training, would provide insights into the impact of graph structure on GNM's effectiveness

### Open Question 3
- Question: How does GNM's performance scale with increasing dataset size and complexity?
- Basis in paper: [explicit] The paper evaluates GNM on several classification and regression datasets, but does not investigate its performance on very large or complex datasets
- Why unresolved: The experiments in the paper use relatively small to medium-sized datasets, leaving the question of GNM's scalability to larger and more complex datasets unanswered
- What evidence would resolve it: Experiments testing GNM's performance on large-scale datasets, such as ImageNet or large language models, and comparing its scalability to other architectures would provide insights into its ability to handle complex real-world problems

## Limitations
- The practical scalability of GNMs to large tabular datasets may be limited due to quadratic growth in parameters with node count
- Lack of ablation studies isolating the impact of dense connectivity versus synchronous updates on performance
- Theoretical universal approximation claims are untested for convergence speed compared to MLPs

## Confidence
- Universal approximation claim: Medium - theoretical derivation is clear but practical realization depends on optimization success
- Equivalence between MLPs and asynchronous GNNs: High - direct structural mapping is well-established
- Overfitting resistance claim: Medium - results show trends but lack statistical significance testing across varied datasets

## Next Checks
1. Conduct ablation experiments comparing dense vs. sparse GNM graphs with fixed synchronous updates to isolate the effect of connectivity
2. Perform scaling experiments to measure training time and memory usage as node count increases, comparing to MLP layer growth
3. Test GNM on larger, high-dimensional tabular datasets (e.g., Kaggle competitions) to evaluate practical robustness and overfitting trends beyond the 15+6 studied datasets