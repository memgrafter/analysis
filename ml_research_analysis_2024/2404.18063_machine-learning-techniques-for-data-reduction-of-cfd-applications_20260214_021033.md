---
ver: rpa2
title: Machine Learning Techniques for Data Reduction of CFD Applications
arxiv_id: '2404.18063'
source_url: https://arxiv.org/abs/2404.18063
tags:
- data
- compression
- species
- error
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present GBATC, a guaranteed block autoencoder for compressing
  large spatiotemporal scientific datasets generated by computational fluid dynamics
  (CFD) simulations. GBATC employs a 3D convolutional autoencoder to capture spatiotemporal
  correlations within data blocks, followed by a tensor correction network to further
  refine the reconstruction.
---

# Machine Learning Techniques for Data Reduction of CFD Applications

## Quick Facts
- arXiv ID: 2404.18063
- Source URL: https://arxiv.org/abs/2404.18063
- Reference count: 40
- Authors: Jaemoon Lee, Ki Sung Jung, Qian Gong, Xiao Li, Scott Klasky, Jacqueline Chen, Anand Rangarajan, Sanjay Ranka
- Primary result: GBATC achieves two to three orders of magnitude compression while maintaining NRMSE ≤ 0.001 on CFD data, outperforming SZ by 2.67x at equivalent error levels

## Executive Summary
This paper presents GBATC, a guaranteed block autoencoder for compressing large spatiotemporal scientific datasets from computational fluid dynamics simulations. The method uses a 3D convolutional autoencoder to capture spatiotemporal correlations within data blocks, followed by a tensor correction network and PCA-based error bounding to ensure scientific accuracy. GBATC demonstrates substantially higher compression ratios compared to state-of-the-art error-bounded compressors while maintaining scientifically acceptable error bounds on both primary data and quantities of interest.

## Method Summary
GBATC employs a 3D convolutional autoencoder to process spatiotemporal blocks of CFD data, treating 58 species as separate channels. After initial reconstruction, a tensor correction network refines the output through overcomplete fully connected layers. To guarantee error bounds, PCA is applied to residuals between original and reconstructed data, with coefficients selected iteratively to meet specified thresholds. The method includes quantization and Huffman coding for additional compression. The approach is validated on a 2D S3D dataset with 58 species over 50 time steps.

## Key Results
- GBATC achieves compression ratios of 400 with NRMSE of 0.001 on primary data
- Outperforms SZ compressor (compression ratio 150) at same error level by 2.67x
- Maintains high accuracy on quantities of interest including production rates of chemical species
- Successfully captures spatiotemporal and interspecies correlations in CFD data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GBATC captures both spatiotemporal and interspecies relationships within CFD tensors.
- Mechanism: 3D convolutional layers treat each species as a separate channel, processing spatiotemporal blocks to learn joint representations.
- Core assumption: Species within the same spatiotemporal block exhibit meaningful correlations that can be captured by shared convolutional filters.
- Evidence anchors: [abstract] "It uses a multidimensional block of tensors... capturing the spatiotemporal and interspecies relationship"; [section] "The 58 species are treated as individual channels for the 3D convolutional layers"
- Break condition: If species correlations are negligible or spatially/temporally localized, channel-based 3D convolution provides minimal benefit.

### Mechanism 2
- Claim: Error bounds are guaranteed through residual PCA with iterative coefficient selection.
- Mechanism: PCA applied to residuals with coefficients selected incrementally until ℓ2 norm falls below threshold τ.
- Core assumption: Leading principal components of residuals contain sufficient information to correct reconstruction errors below target threshold.
- Evidence anchors: [abstract] "principal component analysis (PCA) is applied to the residual"; [section] "select the leading coefficients such that the ℓ2 norm of the corrected residual falls below the specified threshold τ"
- Break condition: If residual distribution lacks dominant principal components or threshold is too strict, compression benefits are negated.

### Mechanism 3
- Claim: Tensor correction network learns reverse pointwise mapping to reduce reconstruction errors.
- Mechanism: Overcomplete network trained to map reconstructed tensors back toward original tensors.
- Core assumption: AE reconstruction errors have patterns that can be learned and corrected by pointwise mapping network.
- Evidence anchors: [abstract] "The tensor correction network takes the reconstructed tensors and learns a reverse point-wise mapping"; [section] "The network is employed to adjust the reconstructed data from the AE"
- Break condition: If AE reconstruction errors are random noise rather than systematic patterns, correction network learns nothing meaningful.

## Foundational Learning

- Concept: 3D Convolutional Autoencoders
  - Why needed here: CFD data has inherent spatiotemporal structure that standard 2D convolutions cannot capture. 3D convolutions process temporal sequences within spatial blocks simultaneously.
  - Quick check question: What is the key difference between 2D and 3D convolutions when processing spatiotemporal data?

- Concept: Principal Component Analysis for Error Correction
  - Why needed here: PCA provides orthogonal basis that can efficiently represent residual errors. Selecting leading components achieves error guarantees with minimal storage overhead.
  - Quick check question: How does PCA help in selecting which residual components to store for error correction?

- Concept: Quantization and Entropy Coding
  - Why needed here: Floating-point latent representations and PCA coefficients compressed further using quantization and Huffman coding, improving compression ratios without sacrificing error bounds.
  - Quick check question: Why is quantization followed by entropy coding more effective than storing raw floating-point values?

## Architecture Onboarding

- Component map: Input Block → 3D Conv AE → Latent Space → Quantization/Huffman → AE Decoder → Tensor Correction → Residual PCA → Coefficient Selection → Output

- Critical path: Input Block → 3D Conv AE → Latent Space → Quantization/Huffman → AE Decoder → Tensor Correction → Residual PCA → Coefficient Selection → Output

- Design tradeoffs:
  - Channel-based 3D convolutions vs. separate 2D convolutions per species
  - Overcomplete correction network vs. larger AE capacity
  - Number of PCA components vs. error bound satisfaction
  - Quantization bin size vs. compression ratio and error accumulation

- Failure signatures:
  - High NRMSE despite high compression ratio: Likely insufficient PCA components or poor AE training
  - Compression ratio close to input size: Quantization or entropy coding ineffective, or too many PCA coefficients needed
  - Inconsistent error bounds: Coefficient selection algorithm not converging properly

- First 3 experiments:
  1. Train AE alone on synthetic spatiotemporal data with known species correlations to verify 3D channel-based convolution captures interspecies relationships
  2. Test residual PCA coefficient selection on AE outputs to verify error bounds can be met with fewer than full-rank representations
  3. Validate tensor correction network on AE outputs to confirm it can learn systematic reconstruction errors without overcomplete capacity

## Open Questions the Paper Calls Out

- Question: How does GBATC perform on QoIs that are nonlinear functions of the primary data, particularly when QoIs are O(N) in nature?
  - Basis in paper: [explicit] Paper mentions GBATC doesn't directly address O(N) QoIs like reaction rates
  - Why unresolved: Paper acknowledges none of existing methods directly address O(N) QoIs
  - What evidence would resolve it: Experimental results showing GBATC's performance on O(N) QoIs

- Question: Would integrating tensor correction network into AE decoder and training end-to-end improve overall compression quality?
  - Basis in paper: [explicit] Paper suggests this integration but notes as future work
  - Why unresolved: Paper separates AE and correction network training
  - What evidence would resolve it: Comparative experiments between current approach and end-to-end trained model

- Question: How does efficiency change when applied to entire time span of DNS dataset, particularly during low-temperature and high-temperature ignition phases?
  - Basis in paper: [explicit] Paper mentions investigating efficiency over entire time span t=0-3ms as future work
  - Why unresolved: Current study focuses on specific time range t=1.5-2.0ms
  - What evidence would resolve it: Results from applying GBATC to entire dataset including ignition phases

## Limitations

- Method effectiveness depends heavily on presence of meaningful spatiotemporal and interspecies correlations within data blocks
- Reliance on residual PCA assumes reconstruction errors have structured patterns amenable to low-rank representation
- Tensor correction network lacks extensive validation in broader ML literature for scientific data compression

## Confidence

- **High confidence** in 3D convolutions for spatiotemporal processing (supported by S3D application and established 3D convolution effectiveness)
- **Medium confidence** in error bounding through residual PCA (theoretically sound but sensitive to data characteristics)
- **Medium confidence** in tensor correction network's ability to learn systematic reconstruction errors (lacks extensive validation)

## Next Checks

1. Test GBATC on synthetic dataset with known spatiotemporal and interspecies correlations to verify 3D convolutional approach effectively captures relationships and performance degrades when correlations are removed

2. Evaluate sensitivity of error bounds to number of PCA components selected and quantization bin size by systematically varying these parameters and measuring resulting NRMSE on primary data and QoIs

3. Compare GBATC's performance on datasets with varying degrees of feature localization (smooth spatiotemporal fields vs. highly localized turbulent features) to assess robustness across different CFD simulation regimes