---
ver: rpa2
title: 'Findings of the Second BabyLM Challenge: Sample-Efficient Pretraining on Developmentally
  Plausible Corpora'
arxiv_id: '2412.05149'
source_url: https://arxiv.org/abs/2412.05149
tags:
- language
- data
- year
- track
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The BabyLM Challenge aims to close the data-efficiency gap between
  human and computational language learners by optimizing language model training
  on a fixed budget of 100 million words or less. This year's challenge included text-only
  and multimodal tracks, with participants employing diverse methods such as new architectures,
  training objectives, and dataset construction.
---

# Findings of the Second BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora

## Quick Facts
- arXiv ID: 2412.05149
- Source URL: https://arxiv.org/abs/2412.05149
- Reference count: 24
- Primary result: Hybrid causal-masked language model architecture (GPT-BERT) outperformed other approaches in text-only track; no multimodal submissions outperformed baselines

## Executive Summary
The BabyLM Challenge aims to close the data-efficiency gap between human and computational language learners by optimizing language model training on a fixed budget of 100 million words or less. This year's challenge included text-only and multimodal tracks, with participants employing diverse methods such as new architectures, training objectives, and dataset construction. A hybrid causal-masked language model architecture outperformed other approaches, but no submissions outperformed baselines in the multimodal track. The best-performing submissions proposed changes to training data, training objective, and model architecture. Results showed a strong relationship between training FLOPs and average performance across tasks, indicating that high-compute training regimes still tend to perform better even in low-data settings. The challenge highlighted the difficulty of multimodal learning and the potential for innovation in data-efficient language modeling.

## Method Summary
The BabyLM Challenge provided participants with 100M word corpora for text-only models and 50M words of text paired with 50M words of image data for multimodal models. Participants trained language models using diverse approaches including new architectures (like GPT-BERT), training objectives (hybrid CLM-MLM), and dataset construction methods. Models were evaluated on zero-shot tasks (BLiMP, GLUE, SuperGLUE) and fine-tuning tasks (EWoK for text, VQA and Winoground for multimodal). The challenge organizers provided baseline models (LTG-BERT for text, GIT and Flamingo for multimodal) trained with default hyperparameters as comparison points.

## Key Results
- GPT-BERT's hybrid causal-masked language model architecture outperformed other approaches in the text-only track
- No submissions outperformed baselines in the multimodal track
- Strong positive relationship between training FLOPs and performance across all tracks (β = 2.7, p < 0.01)
- Best-performing submissions proposed changes to training data, training objectives, and model architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-BERT's hybrid causal-masked language model architecture improves performance by aligning MLM predictions with next-token predictions from CLM.
- Mechanism: GPT-BERT duplicates the training data and masks/processes each copy differently for causal and masked language modeling. It then chooses between the CLM dataset copy with probability p and the MLM dataset with probability 1-p, allowing both objectives to train simultaneously.
- Core assumption: The alignment between MLM predictions (shifted one position to the right) and next-token predictions from CLM creates synergistic learning effects.
- Evidence anchors:
  - [abstract] "a hybrid causal-masked language model architecture outperformed other approaches"
  - [section 5.2] "GPT-BERT merges the causal (CLM) and masked language modeling (MLM) objectives from GPT and BERT, respectively, using the following key insight: by shifting MLM predictions one position to the right, the MLM predictions become aligned with next-token predictions from CLM"
  - [corpus] Weak - corpus evidence is not directly relevant to this architectural mechanism
- Break condition: If the alignment between MLM and CLM predictions does not create synergistic effects, or if the probability p is not optimally tuned, the hybrid approach may not outperform single-objective baselines.

### Mechanism 2
- Claim: Training on diverse datasets (including custom-created datasets) leads to better performance than using only the provided baseline corpus.
- Mechanism: Participants who constructed their own training datasets by adding data with simpler sentences, shorter words, or data better suited to certain downstream evaluations achieved higher scores.
- Core assumption: Dataset quality and relevance to evaluation tasks significantly impacts model performance, beyond just the quantity of training data.
- Evidence anchors:
  - [abstract] "the best-performing submissions proposed changes to the training data"
  - [section 6.3] "Many contestants modified our provided data by procuring new sources, generating data from auxiliary language models, or filtering the existing data"
  - [corpus] Weak - while the corpus itself is not the focus, the analysis shows that dataset construction was an effective method

### Mechanism 3
- Claim: Higher training FLOPs correlate with better performance, even in low-data settings.
- Mechanism: Models that underwent more computationally intensive training (more epochs, larger models, etc.) achieved higher scores across evaluation tasks.
- Core assumption: Computational resources remain a crucial factor for model performance, even when data is limited.
- Evidence anchors:
  - [abstract] "Results showed a strong relationship between training FLOPs and average performance across tasks, indicating that high-compute training regimes still tend to perform better even in low-data settings"
  - [section 6.1] "We observe a positive relationship across all three tracks" and "more training FLOPs leads to better performance (β = 2.7, p < 0.01)"
  - [corpus] Weak - corpus evidence is not directly relevant to this computational resource mechanism

## Foundational Learning

- Concept: Language model architectures (CLM vs MLM)
  - Why needed here: Understanding the difference between causal language modeling (CLM) and masked language modeling (MLM) is crucial for grasping the GPT-BERT architecture innovation
  - Quick check question: What is the key difference between how CLM and MLM predict tokens during training?

- Concept: Evaluation paradigms (zero-shot vs fine-tuning)
  - Why needed here: The BabyLM challenge used both zero-shot and fine-tuning evaluation methods, which impacts how models are trained and assessed
  - Quick check question: Why did the challenge organizers choose to use fine-tuning for the (Super)GLUE tasks instead of zero-shot evaluation?

- Concept: Data efficiency in language learning
  - Why needed here: The entire challenge is based on the premise that children learn language more efficiently than current language models, so understanding this gap is fundamental
  - Quick check question: How many words are children exposed to by age 13 compared to the training data used for typical large language models?

## Architecture Onboarding

- Component map: BabyLM corpus -> Language model (LTG-BERT/GPT-BERT/BabyLlama) -> Training objective (CLM/MLM/hybrid) -> Evaluation pipeline (BLiMP/GLUE/SuperGLUE/EWoK/VQA/Winoground)
- Critical path: Training the language model effectively on the limited dataset, as this directly impacts all downstream evaluation performance
- Design tradeoffs: Using a hybrid CLM-MLM objective trades off training complexity for potential performance gains; constructing custom datasets trades off using the provided baseline for potentially better task-specific performance
- Failure signatures: Poor performance across multiple tasks, inability to outperform baseline models, or failure to generalize to held-out evaluation tasks
- First 3 experiments:
  1. Train a basic LTG-BERT model on the provided BabyLM corpus to establish a performance baseline
  2. Implement the GPT-BERT hybrid objective with different probability values for p to find the optimal CLM-MLM ratio
  3. Create a simple custom dataset by filtering the provided corpus for shorter sentences and re-train the model to measure performance impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to LTG-BERT would maximize performance within the BabyLM constraints?
- Basis in paper: [explicit] The paper notes that GPT-BERT modifies LTG-BERT by adding gates on attention heads and residual connection reweighting from ELC-BERT, suggesting these changes were beneficial.
- Why unresolved: The paper doesn't systematically ablate which modifications contribute most to performance gains.
- What evidence would resolve it: A controlled study varying individual architectural components while keeping other factors constant would identify the most impactful modifications.

### Open Question 2
- Question: Why did multimodal models perform worse than text-only baselines despite being trained on paired data?
- Basis in paper: [explicit] The paper states "No submissions outperformed the baselines in the multimodal track" and discusses challenges like models learning unimodal shortcuts.
- Why unresolved: The paper identifies the problem but doesn't deeply analyze what prevents effective multimodal learning in this setting.
- What evidence would resolve it: Detailed analysis of model attention patterns and feature representations across modalities would reveal whether models are truly integrating information or relying on shortcuts.

### Open Question 3
- Question: What data characteristics make EWoK so challenging for BabyLM models compared to other tasks?
- Basis in paper: [explicit] The paper notes "Most submissions perform near chance, at 50%" on EWoK while performing better on other tasks.
- Why unresolved: The paper suggests the BabyLM corpus may lack world knowledge content, but doesn't analyze what specific types of knowledge are missing.
- What evidence would resolve it: Analysis of EWoK question content and comparison to pretraining corpus statistics would identify knowledge gaps.

## Limitations
- The evaluation was conducted on a held-out set of tasks, and specific performance metrics for individual tasks are not provided in the abstract
- Strong relationship between training FLOPs and performance may reflect limited architectural innovation rather than fundamental constraints of low-data regimes
- The multimodal track showed no submissions outperforming baselines, indicating significant challenges that are not fully explained

## Confidence
- High confidence: The observation that hybrid CLM-MLM architectures outperformed other approaches in the text-only track
- Medium confidence: The claim that dataset construction and custom data curation improved performance
- Low confidence: The assertion that the multimodal track highlighted "difficulty" without explaining specific failure modes

## Next Checks
1. Conduct ablation studies on the GPT-BERT architecture to quantify the individual contributions of CLM and MLM objectives to performance improvements
2. Perform controlled experiments varying the probability parameter p in the hybrid objective to determine optimal weighting between CLM and MLM training
3. Analyze the specific characteristics of custom datasets used by top-performing teams to identify which data properties most strongly correlate with downstream task performance