---
ver: rpa2
title: 'Minimizing Live Experiments in Recommender Systems: User Simulation to Evaluate
  Preference Elicitation Policies'
arxiv_id: '2409.17436'
source_url: https://arxiv.org/abs/2409.17436
tags:
- user
- simulation
- data
- users
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes using simulation to reduce costly live experiments
  in evaluating recommender systems' preference elicitation policies. The approach
  trains user models on historical onboarding session data, capturing artist selection
  behaviors and session dynamics.
---

# Minimizing Live Experiments in Recommender Systems: User Simulation to Evaluate Preference Elicitation Policies

## Quick Facts
- **arXiv ID**: 2409.17436
- **Source URL**: https://arxiv.org/abs/2409.17436
- **Reference count**: 40
- **Primary result**: Simulation can reliably predict policy performance and reduce costly live experiments in recommender systems.

## Executive Summary
This work proposes using simulation to reduce costly live experiments in evaluating recommender systems' preference elicitation policies. The approach trains user models on historical onboarding session data, capturing artist selection behaviors and session dynamics. These models generate realistic synthetic user populations and session trajectories, enabling offline evaluation of new policies. The simulation integrates with production infrastructure, preserving real-system interactions. Experiments with YouTube Music onboarding policies show that simulated metrics closely match live experiments and post-launch results, with tighter confidence intervals. This suggests simulation can reliably predict policy performance and help optimize new algorithms before costly live testing, reducing development cycle time and user costs.

## Method Summary
The method involves training user models on historical onboarding session data to capture artist selection behaviors and session dynamics. These models generate synthetic user populations and session trajectories, which are then used to evaluate new preference elicitation policies offline. The simulation integrates with production infrastructure to preserve real-system interactions. By comparing simulated metrics with live experiment results, the approach demonstrates that simulation can reliably predict policy performance, allowing for more efficient optimization of recommender systems.

## Key Results
- Simulation metrics closely match live experiments and post-launch results with tighter confidence intervals
- Large-scale simulation can test multiple policy variants quickly and identify promising candidates for live experiments
- Simulated ordering of policies predicts the ordering observed in live experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simulation can reliably predict policy performance and reduce costly live experiments.
- Mechanism: Training user models on historical onboarding session data captures artist selection behaviors and session dynamics, allowing offline evaluation of new preference elicitation policies.
- Core assumption: User behavior models trained on historical data are counterfactually robust and can generalize to previously unseen policies.
- Evidence anchors:
  - [abstract]: "These models generate realistic synthetic user populations and session trajectories, enabling offline evaluation of new policies."
  - [section]: "Our user models are trained on this data, and we evaluate simulation quality by assessing whether it reproduces the distribution of key statistics observed in live (ground-truth) data."
  - [corpus]: Weak. Corpus contains papers about simulation and evaluation but not specific to counterfactual robustness of user models in recommender systems.
- Break condition: If user models cannot generalize to new policies or if the synthetic data distribution significantly differs from real user behavior.

### Mechanism 2
- Claim: Integration of synthetic users with production infrastructure preserves real-system interactions.
- Mechanism: The simulation service mocks user interactions while maintaining separation between simulated and live systems, ensuring feature parity with production RS.
- Core assumption: Production infrastructure can handle the usage patterns generated by a simulator without disruption.
- Evidence anchors:
  - [abstract]: "The simulation integrates with production infrastructure, preserving real-system interactions."
  - [section]: "To ensure feature parity between the simulator and production RS, the simulator queries a front-end service which returns all elements to be rendered on the YouTube Music app."
  - [corpus]: Weak. Corpus mentions simulation frameworks but not specifically about integrating synthetic users with production infrastructure.
- Break condition: If the simulator cannot maintain separation from live systems or if production infrastructure is not designed to handle simulator usage patterns.

### Mechanism 3
- Claim: Simulated metrics closely match live experiments and post-launch results with tighter confidence intervals.
- Mechanism: Large-scale simulation can test multiple policy variants quickly and identify promising candidates for live experiments.
- Core assumption: Simulated user behavior is representative enough to predict real-world policy performance.
- Evidence anchors:
  - [abstract]: "Experiments with YouTube Music onboarding policies show that simulated metrics closely match live experiments and post-launch results, with tighter confidence intervals."
  - [section]: "To test the hypothesis that large-scale simulation can more accurately order policies w.r.t. their launch metrics than small LEs...our results in Table 3 show the variation across the three small 'true LEs,' none of which order the policies in the same way, and only one of which conforms to the 'launch;' while the simulation ordering predicts (informally) that of the launch."
  - [corpus]: Weak. Corpus contains papers about evaluating recommender systems but not specific to simulation metrics matching live experiments.
- Break condition: If simulated user behavior does not accurately represent real user behavior or if the simulation cannot handle the complexity of real-world interactions.

## Foundational Learning

- Concept: Counterfactual robustness
  - Why needed here: To ensure user models trained on historical data can generalize to new, previously unseen policies.
  - Quick check question: What is the main challenge in using historical data to train user models for policy evaluation?

- Concept: Off-policy evaluation
  - Why needed here: To assess the performance of new policies using data generated by different behavior policies.
  - Quick check question: Why is off-policy evaluation challenging in recommender systems?

- Concept: Production infrastructure integration
  - Why needed here: To ensure simulation results reflect real-world interactions and can replace or reduce live experiments.
  - Quick check question: What are the key requirements for integrating a simulator with production infrastructure?

## Architecture Onboarding

- Component map:
  User state generator (RNN/Transformer-based) -> User session generator (RNN/Transformer-based) -> Simulation service -> User data serving service (UDSS) -> Data overlay service (DOS)

- Critical path:
  1. Train user models on historical onboarding session data
  2. Generate synthetic user population with user state generator
  3. Simulate user interactions with user session generator
  4. Integrate simulation with production infrastructure
  5. Evaluate new policies using simulated metrics
  6. Select promising policies for live experiments

- Design tradeoffs:
  - Model complexity vs. counterfactual robustness
  - Simulation scale vs. infrastructure capacity
  - Feature parity vs. separation of simulated and live data

- Failure signatures:
  - Simulated metrics significantly differ from live experiment results
  - Production infrastructure cannot handle simulator usage patterns
  - User models fail to generalize to new policies

- First 3 experiments:
  1. Train user models on historical onboarding session data and evaluate their ability to reproduce observed session statistics.
  2. Simulate a new policy and compare its simulated metrics to those from a live experiment.
  3. Use simulation to optimize a policy parameter and test the optimized policy in a live experiment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we measure the counterfactual robustness of user models to ensure they can accurately evaluate new policies not seen during training?
- Basis in paper: [explicit] The paper discusses the challenge of counterfactual robustness and mentions potential solutions like causal modeling and $m$-transportability.
- Why unresolved: While the paper acknowledges the importance of counterfactual robustness, it does not provide a concrete method for measuring it.
- What evidence would resolve it: A method for quantifying the counterfactual robustness of user models, along with empirical results demonstrating its effectiveness.

### Open Question 2
- Question: How can we determine the optimal set of behavior policies to use for training user models to ensure they capture a diverse range of user behaviors?
- Basis in paper: [inferred] The paper mentions the need for a reasonable variety of data/trajectories for effective model training but does not provide guidance on selecting this set.
- Why unresolved: The paper does not discuss how to select the minimal set of behavior policies needed for training.
- What evidence would resolve it: A framework for selecting behavior policies that maximizes the diversity of user behaviors captured in the training data.

### Open Question 3
- Question: How can we effectively quantify the uncertainty in simulation-based policy evaluations to determine when simulation results can be trusted?
- Basis in paper: [explicit] The paper discusses the need for uncertainty estimates and mentions potential heuristics like measuring the difference between test and training policies.
- Why unresolved: While the paper suggests potential heuristics, it does not provide a comprehensive approach for quantifying uncertainty in simulation results.
- What evidence would resolve it: A method for quantifying uncertainty in simulation-based policy evaluations, along with empirical results demonstrating its effectiveness in determining when simulation results are trustworthy.

## Limitations

- The simulation's predictive power is limited by the counterfactual robustness of user models trained on historical data
- The approach's effectiveness depends on maintaining feature parity with production infrastructure
- Validation is limited to a specific domain (music onboarding) and policy space

## Confidence

- **High confidence**: The simulation can generate realistic synthetic user sessions that preserve key statistics from historical data
- **Medium confidence**: Simulated metrics will closely match live experiment results for policy variants within the training distribution
- **Medium confidence**: The approach can reduce live experiment costs while maintaining evaluation quality for onboarding policies

## Next Checks

1. **Counterfactual validation**: Test the simulation's ability to accurately predict performance for policies that are structurally different from those in the training data, measuring prediction error rates systematically.

2. **Domain generalization**: Apply the same simulation framework to a different recommender system domain (e.g., video or news) to assess whether the methodology generalizes beyond music onboarding.

3. **Long-term dynamics**: Evaluate whether the simulation can capture evolving user preferences over multiple sessions and months, not just single-session onboarding behavior.