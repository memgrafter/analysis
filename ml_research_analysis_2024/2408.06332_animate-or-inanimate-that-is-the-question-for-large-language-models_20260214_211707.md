---
ver: rpa2
title: Animate, or Inanimate, That is the Question for Large Language Models
arxiv_id: '2408.06332'
source_url: https://arxiv.org/abs/2408.06332
tags:
- animacy
- llms
- language
- humans
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large language models (LLMs) can\
  \ process animacy in a manner similar to humans by conducting systematic prompting\
  \ experiments based on human-designed psychological tests. The authors evaluate\
  \ multiple LLMs\u2014including GPT-3.5, GPT-4, various Llama2 models, Mixtral, and\
  \ Mistral-7B\u2014across four experimental settings: typical animacy in transitive\
  \ and passive constructions, typical animacy in sentence plausibility tasks, atypical\
  \ animacy through repetition of animate/inanimate entities, and atypical animacy\
  \ through contextual adaptation."
---

# Animate, or Inanimate, That is the Question for Large Language Models

## Quick Facts
- **arXiv ID:** 2408.06332
- **Source URL:** https://arxiv.org/abs/2408.06332
- **Reference count:** 22
- **Primary result:** LLMs generally prefer sentences adhering to animacy constraints, with OpenAI models closely matching human performance and Meta/Mistral models approaching human-level accuracy.

## Executive Summary
This paper investigates whether large language models can process animacy similarly to humans by conducting systematic prompting experiments based on human-designed psychological tests. The authors evaluate multiple LLMs—including GPT-3.5, GPT-4, various Llama2 models, Mixtral, and Mistral-7B—across four experimental settings involving typical and atypical animacy in different sentence constructions. Results show that despite being trained on text-only data, LLMs can adapt to unconventional scenarios and recognize oddities as animate, though larger models better replicate human adaptation levels. The findings suggest LLMs possess emergent capabilities for semantic processing that mirror human cognitive patterns in animacy perception.

## Method Summary
The study employs systematic prompting experiments based on established psychological tests of animacy processing, adapting human experimental paradigms for LLM evaluation. Four experimental settings are used: typical animacy in transitive and passive constructions, typical animacy in sentence plausibility tasks, atypical animacy through repetition of animate/inanimate entities, and atypical animacy through contextual adaptation. Multiple LLMs are evaluated including GPT-3.5, GPT-4, various Llama2 models, Mixtral, and Mistral-7B, with performance measured against human behavioral and neural response data. The methodology simulates N400-like responses (a neural signature of semantic processing) using log-likelihood differences between expected and unexpected animacy constructions.

## Key Results
- LLMs generally prefer sentences adhering to animacy constraints, with OpenAI models closely matching human performance
- In atypical animacy tasks, LLMs exhibit decreasing surprise for inanimate entities as context increases, mirroring human adaptation patterns
- Larger models better replicate human adaptation levels, suggesting parameter capacity influences semantic flexibility

## Why This Works (Mechanism)
The paper demonstrates that LLMs can process animacy through learned patterns from text data that approximate human semantic processing mechanisms. The mechanism appears to involve surprisal modeling (N400-like responses) that captures semantic violations when animate/inanimate expectations are violated. This suggests LLMs develop internal representations of animacy hierarchies through exposure to linguistic patterns, even without explicit training on semantic categories. The adaptation to atypical contexts indicates LLMs can dynamically adjust their semantic expectations based on contextual information, similar to human cognitive flexibility.

## Foundational Learning
- **Animacy hierarchies:** Why needed - Fundamental cognitive distinction between living and non-living entities that affects sentence processing and pronoun resolution. Quick check - Models should show surprisal for animacy violations in subject/object positions.
- **N400 response:** Why needed - Established neural signature of semantic processing difficulty that serves as benchmark for comparing human and machine processing. Quick check - Log-likelihood differences should correlate with human N400 amplitude patterns.
- **Surprisal theory:** Why needed - Framework linking unexpected linguistic events to processing difficulty, applicable to both humans and models. Quick check - Higher surprisal should correspond to lower likelihood scores for unexpected continuations.
- **Syntactic-semantic interface:** Why needed - Understanding how grammatical structure constrains animacy preferences and vice versa. Quick check - Passive constructions should show different animacy preferences than active voice.
- **Contextual adaptation:** Why needed - Human ability to adjust expectations based on discourse context, crucial for testing model flexibility. Quick check - Repeated exposure to atypical animacy should reduce surprisal over time.

## Architecture Onboarding
- **Component map:** Input text -> Token embedding layer -> Transformer blocks (self-attention + feed-forward) -> Output distribution over vocabulary
- **Critical path:** Token embedding -> Self-attention computation -> Context vector generation -> Likelihood scoring for next token
- **Design tradeoffs:** Next-token prediction vs. masked language modeling; parameter count vs. computational efficiency; English-centric vs. multilingual capabilities
- **Failure signatures:** Over-reliance on frequency statistics rather than semantic processing; inability to adapt to atypical contexts; inconsistent handling across model families
- **First experiments:** 1) Test animacy processing on out-of-distribution linguistic constructions 2) Compare N400-like responses across different prompting strategies 3) Evaluate model behavior when animacy hierarchies are explicitly manipulated

## Open Questions the Paper Calls Out
None

## Limitations
- The N400-like analysis uses computational proxies rather than direct neural measurements, making the equivalence to human processing an inference
- Comparison to human data involves different experimental paradigms and participant populations, introducing potential confounds
- The evaluation focuses primarily on English-language models and stimuli, limiting generalizability to other linguistic contexts
- The paper does not account for potential confounds from training data frequency effects independent of genuine semantic processing

## Confidence
- Core claims about LLM animacy processing: Medium - patterns align but underlying mechanisms remain unverified
- Comparison to human N400 responses: Medium - computational proxy validated against behavioral data but not direct neural measurements
- Claim that larger models better replicate human adaptation: Medium - supported by data but could reflect memorization rather than genuine cognitive flexibility

## Next Checks
1. Replicate the N400-like analysis using masked language modeling objectives rather than next-token prediction to verify robustness across different architectural assumptions
2. Conduct controlled experiments manipulating training corpus frequencies of animate/inanimate constructions to isolate frequency effects from genuine animacy processing
3. Extend the evaluation to non-English languages with different animacy hierarchies (e.g., languages with fluid noun class systems) to test the universality of observed patterns