---
ver: rpa2
title: 'Stream of Search (SoS): Learning to Search in Language'
arxiv_id: '2404.03683'
source_url: https://arxiv.org/abs/2404.03683
tags:
- search
- operation
- language
- state
- numbers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Stream of Search (SoS) framework, which
  trains language models to learn search and planning by representing the search process
  as a serialized string. The authors propose a unified language for search that captures
  exploration, backtracking, and pruning strategies.
---

# Stream of Search (SoS): Learning to Search in Language

## Quick Facts
- arXiv ID: 2404.03683
- Source URL: https://arxiv.org/abs/2404.03683
- Reference count: 16
- Language models can learn search strategies through serialized search string representations, significantly improving performance on combinatorial search tasks.

## Executive Summary
This paper introduces the Stream of Search (SoS) framework, which trains language models to learn search and planning by representing the search process as a serialized string. The authors propose a unified language for search that captures exploration, backtracking, and pruning strategies. They demonstrate their approach on the Countdown game, where the goal is to combine input numbers with arithmetic operations to reach a target number. A GPT-Neo model is pretrained on a dataset of search trajectories generated by symbolic solvers, including both successful and unsuccessful attempts. The SoS pretraining significantly improves search accuracy (51.27% vs 25.73% for models trained on optimal paths only). Further finetuning with policy improvement methods (APA and STaR) enables the model to solve 36% of previously unsolved problems, including some that no symbolic solver can solve.

## Method Summary
The SoS framework trains language models to learn search strategies by representing the search process as a serialized string. The approach uses a unified language for search that captures exploration, backtracking, and pruning strategies. A GPT-Neo model is pretrained on a dataset of search trajectories generated by symbolic solvers, including both successful and unsuccessful attempts. The pretrained model is then finetuned using policy improvement methods (APA and STaR) to further enhance its search capabilities. The method is demonstrated on the Countdown game, where the model learns to combine input numbers with arithmetic operations to reach a target number.

## Key Results
- SoS pretraining significantly improves search accuracy from 25.73% to 51.27% compared to models trained on optimal paths only
- Finetuning with policy improvement methods (APA and STaR) enables the model to solve 36% of previously unsolved problems
- The model can solve some problems that no symbolic solver can solve

## Why This Works (Mechanism)
The SoS framework works by training language models to learn search strategies through exposure to diverse search trajectories. By representing the search process as a serialized string, the model can learn from both successful and unsuccessful attempts, capturing exploration, backtracking, and pruning strategies. The pretraining on diverse trajectories allows the model to develop a general understanding of search processes, while finetuning with policy improvement methods further refines its strategies. This approach enables the model to develop search capabilities that can generalize beyond the specific problems seen during training.

## Foundational Learning
- Language model pretraining: Why needed - provides general language understanding; Quick check - measure perplexity on held-out text
- Search trajectory generation: Why needed - provides training data for search strategies; Quick check - analyze diversity of generated trajectories
- Policy improvement methods (APA and STaR): Why needed - refines search strategies through self-improvement; Quick check - compare performance before and after finetuning
- Serialized search string representation: Why needed - enables language models to process search processes; Quick check - test model's ability to parse and generate search strings
- Combinatorial search problems: Why needed - provides benchmark for evaluating search capabilities; Quick check - measure success rate on held-out problems
- Symbolic solvers: Why needed - generate ground truth search trajectories; Quick check - compare generated trajectories with optimal solutions

## Architecture Onboarding

Component map: Data generation (symbolic solvers) -> SoS pretraining (GPT-Neo) -> Policy improvement (APA/STaR) -> Evaluation (Countdown problems)

Critical path: The most critical component is the SoS pretraining phase, where the model learns to understand and generate search trajectories. This phase requires careful design of the serialized search string representation and a diverse dataset of search trajectories.

Design tradeoffs: The main tradeoff is between the expressiveness of the serialized search string representation and its tractability for language models. More detailed representations may capture more nuanced search strategies but could be harder for models to process.

Failure signatures: Common failure modes include the model getting stuck in local optima, failing to generalize to new problem instances, and producing invalid search trajectories.

First experiments:
1. Evaluate model's ability to generate valid search trajectories on held-out Countdown problems
2. Compare performance of SoS pretraining versus training on optimal paths only
3. Test model's ability to generalize to Countdown problems with different number ranges

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is constrained to a single combinatorial game (Countdown), raising questions about generality across different problem domains
- Scalability concerns for larger, more complex search spaces where serialized search string representations could become prohibitively long
- Unclear whether finetuned model represents genuine strategic improvement versus better pattern matching from training data

## Confidence
- Claim: SoS pretraining significantly improves search accuracy - High confidence
- Claim: Model can solve problems no symbolic solver can solve - Medium confidence
- Claim: Approach generalizes to other search domains - Medium confidence

## Next Checks
1. Evaluate the SoS framework on multiple diverse search domains (e.g., Sokoban planning, theorem proving, or combinatorial optimization problems) to test domain generalization beyond Countdown.

2. Conduct ablation studies comparing SoS pretraining with alternative representation methods (e.g., graph neural networks or tree-based encodings) to isolate the benefits of the serialized string representation.

3. Measure and report the computational overhead of SoS training and inference compared to traditional symbolic solvers, including memory usage and wall-clock time for problems of varying sizes.