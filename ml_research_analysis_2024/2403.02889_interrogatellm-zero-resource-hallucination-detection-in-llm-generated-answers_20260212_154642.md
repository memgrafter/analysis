---
ver: rpa2
title: 'InterrogateLLM: Zero-Resource Hallucination Detection in LLM-Generated Answers'
arxiv_id: '2403.02889'
source_url: https://arxiv.org/abs/2403.02889
tags:
- llama-2
- gpt3
- query
- ensemble
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InterrogateLLM, a zero-resource method for
  detecting hallucinations in LLM-generated answers. The approach leverages the principle
  that hallucinated responses tend to produce inconsistent reconstructions when queried
  multiple times using the same generated content.
---

# InterrogateLLM: Zero-Resource Hallucination Detection in LLM-Generated Answers

## Quick Facts
- **arXiv ID:** 2403.02889
- **Source URL:** https://arxiv.org/abs/2403.02889
- **Reference count:** 6
- **Primary result:** Zero-resource method achieving up to 81% balanced accuracy in hallucination detection without external knowledge

## Executive Summary
InterrogateLLM introduces a novel zero-resource approach for detecting hallucinations in LLM-generated answers by leveraging the principle of consistency checking. The method performs a forward pass to generate an answer, then repeatedly reconstructs the original query in a backward pass using reversed few-shot prompts. By measuring the similarity between the original query and reconstructed queries via text embeddings, InterrogateLLM identifies potential hallucinations based on inconsistency. Evaluated across three datasets (Movies, Books, GCI) and multiple LLMs including Llama-2, the method achieves up to 81% balanced accuracy without requiring external knowledge, outperforming baselines like SelfCheckGPT.

## Method Summary
InterrogateLLM operates through a two-phase process: forward generation and backward reconstruction. In the forward pass, a few-shot prompt containing example query-answer pairs is used with the original query to generate an answer. The backward pass then reconstructs the original query from this generated answer using a reversed few-shot prompt. This reconstruction process is repeated K times with variable temperature values to create multiple reconstructed queries. The method calculates similarity scores between the original query and reconstructed queries using text embeddings, and predicts hallucination if the similarity exceeds a threshold τ=0.91. The approach leverages the observation that hallucinated responses tend to produce inconsistent reconstructions when queried multiple times, while factual responses maintain consistency across reconstructions.

## Key Results
- Achieves up to 81% balanced accuracy in hallucination detection without requiring external knowledge
- Outperforms baselines like SelfCheckGPT, particularly when using an ensemble of models for backward pass
- Variable temperature values during reconstruction enhance performance, with temperature set to Ti = T0 + 1.0 - T0/(K*i)
- Hallucination rates as high as 87% for Llama-2 on the Movies dataset, underscoring the importance of detection methods

## Why This Works (Mechanism)
The method exploits the principle that factual LLM responses, when conditioned on their own output, can reconstruct the original query consistently across multiple attempts. Hallucinated responses lack this self-consistency property because they are generated without grounding in factual knowledge. By reversing the few-shot prompt and using the generated answer as input, the backward reconstruction process tests whether the LLM can "interrogate" its own answer to recover the original query. Variable temperatures during reconstruction help explore different reasoning paths and prevent mode collapse, while the ensemble approach leverages multiple models' perspectives to improve detection reliability.

## Foundational Learning
- **Few-shot prompting**: Why needed - to provide context for both generation and reconstruction phases; Quick check - verify prompts contain relevant examples and follow the described format
- **Text embedding similarity**: Why needed - to quantify consistency between original and reconstructed queries; Quick check - ensure ada002 embeddings are properly computed and compared
- **Temperature scheduling**: Why needed - to introduce diversity in backward reconstructions and prevent identical outputs; Quick check - verify Ti formula implementation produces increasing temperatures across K iterations
- **Backward prompt reversal**: Why needed - to condition the LLM on the generated answer while reconstructing the original query; Quick check - confirm prompt structure swaps query-answer pairs appropriately

## Architecture Onboarding
- **Component map:** Datasets -> Forward LLM (FLLM) -> Generated Answer -> Backward LLMs (BLLM) -> Reconstructed Queries -> Similarity Scoring -> Hallucination Prediction
- **Critical path:** Forward generation → Backward reconstruction (K iterations) → Similarity calculation → Threshold comparison → Prediction
- **Design tradeoffs:** Single vs ensemble BLLMs (accuracy vs complexity), fixed vs variable temperatures (stability vs exploration), threshold selection (sensitivity vs specificity)
- **Failure signatures:** Poor reconstruction quality → false positives; inappropriate threshold → low sensitivity/specificity; mode collapse → identical reconstructions
- **First experiments:** 1) Test backward reconstruction with a single example to verify prompt reversal works; 2) Run forward-backward cycle with K=1 to establish baseline similarity scores; 3) Implement variable temperature scheduling and verify Ti values increase across iterations

## Open Questions the Paper Calls Out
- How does InterrogateLLM perform when applied to Retrieval Augmented Generation (RAG) settings where queries are provided with retrieved context?
- What is the impact of increasing K beyond 5 in the backward pass reconstruction process?
- How does InterrogateLLM handle semi-truth answers where only a portion of the generated content is hallucinated?

## Limitations
- Requires specific prompt templates and few-shot examples that are not provided in the paper
- Ground truth verification heuristics are described only at a high level without implementation details
- Limited evaluation to three narrow-domain datasets, raising questions about generalizability to broader domains

## Confidence
- **Medium Confidence**: Core methodology of forward-backward reconstruction with consistency checking is well-described
- **Medium Confidence**: Evaluation methodology using balanced accuracy and AUC metrics is appropriate
- **Low Confidence**: Specific implementation details required for exact reproduction are insufficiently specified

## Next Checks
1. **Prompt Template Validation**: Implement the few-shot prompts using the described format and test whether the backward reconstruction produces coherent queries that can recover the original query
2. **Temperature Sensitivity Analysis**: Systematically vary the temperature parameters (T0 and the incremental scheme) across different values to identify optimal settings for each LLM model
3. **Threshold Calibration Testing**: Perform ROC analysis across different threshold values (τ) to identify the optimal operating point for the similarity score