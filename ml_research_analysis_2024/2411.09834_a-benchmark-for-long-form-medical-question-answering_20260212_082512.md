---
ver: rpa2
title: A Benchmark for Long-Form Medical Question Answering
arxiv_id: '2411.09834'
source_url: https://arxiv.org/abs/2411.09834
tags:
- medical
- questions
- question
- answer
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new benchmark for long-form medical question
  answering with human annotations from medical doctors. The benchmark includes real-world
  consumer medical questions and evaluates responses from various open and closed-source
  LLMs on criteria like correctness, helpfulness, harmfulness, and bias.
---

# A Benchmark for Long-Form Medical Question Answering

## Quick Facts
- arXiv ID: 2411.09834
- Source URL: https://arxiv.org/abs/2411.09834
- Reference count: 40
- Key outcome: New benchmark for long-form medical QA with human annotations, showing open medical LLMs' performance relative to commercial models

## Executive Summary
This paper introduces a comprehensive benchmark for evaluating long-form medical question answering systems using real-world consumer medical questions and human annotations from medical doctors. The benchmark assesses LLM responses across multiple criteria including correctness, helpfulness, harmfulness, and bias. The study reveals that smaller-scale open medical LLMs like AlpaCare-13B can outperform general-purpose models, while very large open models like Llama-3.1-405B-Instruct can match or exceed specialized commercial models like GPT-4o. Interestingly, additional medical pretraining doesn't necessarily improve performance over base models.

## Method Summary
The benchmark collects real-world consumer medical questions from the Lavita Medical AI Assist platform, uses GPT-4 for medical question identification and grammar correction, and employs medical doctors for human annotation of LLM responses. The evaluation uses pairwise comparisons across five criteria: correctness, helpfulness, harmfulness, bias, and efficiency. The study also incorporates LLM-as-a-judge analysis to compare automated evaluation with human assessments. The dataset includes 19,967 questions from 1,621 patients, with responses generated by various open and closed-source LLMs.

## Key Results
- Smaller-scale open medical LLMs like AlpaCare-13B outperform general-purpose models like BioMistral-7B
- Llama-3.1-405B-Instruct outperforms GPT-4o across all evaluation criteria
- Additional medical pretraining (Meditron3-70B) doesn't necessarily improve performance over base models (Llama-3.1-70B-Instruct)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller-scale open medical LLMs like AlpaCare-13B outperform smaller-scale models like BioMistral-7B when evaluated on real-world medical QA tasks.
- Mechanism: Open medical LLMs benefit from domain-specific instruction tuning and curated medical datasets, allowing them to better handle medical terminology and reasoning compared to general-purpose models of similar size.
- Core assumption: Domain-specific pretraining and instruction tuning on medical data provides performance advantages over general-purpose pretraining, even at smaller model scales.
- Evidence anchors:
  - [abstract] "Human evaluations show that smaller-scale open medical LLMs like AlpaCare-13B outperform BioMistral-7B"
  - [section 4] "we observe that AlpaCare-13B outperforms the smaller-scale model, BioMistral-7B"
- Break condition: If the domain-specific medical training data is noisy, unrepresentative, or contains outdated information that conflicts with current medical consensus.

### Mechanism 2
- Claim: Llama-3.1-405B-Instruct outperforms GPT-4o across all evaluation criteria in medical QA tasks.
- Mechanism: Very large general-purpose open models with strong instruction-following capabilities can match or exceed specialized commercial models in domain-specific tasks when they have sufficient scale and quality instruction tuning.
- Core assumption: Model scale and quality of instruction tuning can compensate for lack of domain-specific pretraining when the task complexity is within the model's general capabilities.
- Evidence anchors:
  - [abstract] "Notably, Llama-3.1-405B-Instruct outperforms GPT-4o across all criteria"
  - [section 4] "when comparing one of the flagship closed models, GPT-4o, with the state-of-the-art open model at the time, Llama-3.1 405B-Instruct, where Llama-3.1 outperforms GPT-4o across all aspects"
- Break condition: If the medical questions require specialized clinical knowledge that wasn't adequately represented in the general pretraining data.

### Mechanism 3
- Claim: Additional pretraining on medical data (e.g., Meditron3-70B) does not necessarily improve performance over its base model (Llama-3.1-70B-Instruct).
- Mechanism: Domain-specific pretraining may introduce conflicting knowledge or bias that doesn't generalize well to real-world medical QA, and the base model's general reasoning capabilities may be more valuable.
- Core assumption: The benefits of domain-specific pretraining are outweighed by potential negative effects like overfitting to training data or introducing domain-specific biases that don't translate to real-world performance.
- Evidence anchors:
  - [abstract] "the study also reveals that additional pretraining on medical data (e.g., Meditron3-70B) does not necessarily improve performance over its base model (Llama-3.1-70B-Instruct)"
  - [section 4] "when comparing Meditron3-70B with its base vanilla model, Llama-3.1-70B-Instruct, we find that Meditron3-70B does not necessarily offer improvements over its base model"
- Break condition: If the medical pretraining data is carefully curated, up-to-date, and representative of the real-world questions being evaluated.

## Foundational Learning

- Concept: Medical terminology and domain knowledge
  - Why needed here: Understanding medical concepts, terminology, and clinical reasoning is essential for evaluating and building medical QA systems
  - Quick check question: Can you explain the difference between a symptom and a diagnosis in medical contexts?

- Concept: Large language model evaluation metrics
  - Why needed here: Understanding both automatic metrics and human evaluation criteria is crucial for interpreting benchmark results
  - Quick check question: What are the limitations of using BLEU or ROUGE scores for evaluating medical QA responses?

- Concept: Data annotation and quality control
  - Why needed here: The benchmark relies on human expert annotations, requiring understanding of annotation schemes and inter-annotator agreement
  - Quick check question: How would you calculate Cohen's kappa for measuring agreement between two annotators?

## Architecture Onboarding

- Component map: Data collection → Query preprocessing → Medical question identification → Semantic deduplication → Difficulty annotation → Model inference → Human annotation → LLM-as-a-judge analysis
- Critical path: Data collection → Medical question identification → Human annotation → Model evaluation → Analysis
- Design tradeoffs: Real-world consumer questions vs. curated medical questions (authenticity vs. quality control)
- Failure signatures: Low inter-annotator agreement indicates ambiguous evaluation criteria; LLM-as-a-judge inconsistencies suggest position bias or unclear instructions
- First 3 experiments:
  1. Run medical question identification on a small sample and manually verify accuracy
  2. Test annotation scheme with a few questions and gather feedback from medical doctors
  3. Compare human evaluation results with LLM-as-a-judge on a small subset to check alignment

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on pairwise comparisons rather than absolute quality assessments, potentially introducing relative bias
- Single annotator per question-response pair without reported inter-annotator agreement metrics
- LLM-as-a-judge analysis uses a single prompt template without exploring alternative evaluation frameworks

## Confidence

| Claim | Confidence |
|-------|------------|
| Smaller-scale open medical LLMs outperform general-purpose models | High |
| Llama-3.1-405B-Instruct outperforms GPT-4o across all criteria | Medium |
| Additional medical pretraining doesn't improve performance | Medium |

## Next Checks
1. Conduct inter-annotator agreement analysis with multiple medical doctors evaluating the same question-response pairs to establish reliability of human annotations
2. Test the LLM-as-a-judge framework with multiple prompt variations and adversarial examples to assess robustness and identify potential biases
3. Expand the evaluation to include more diverse medical domains and question types to validate the generalizability of the benchmark findings across different clinical scenarios