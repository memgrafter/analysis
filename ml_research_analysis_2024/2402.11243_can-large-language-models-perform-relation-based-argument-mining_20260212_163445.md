---
ver: rpa2
title: Can Large Language Models perform Relation-based Argument Mining?
arxiv_id: '2402.11243'
source_url: https://arxiv.org/abs/2402.11243
tags:
- llms
- datasets
- dataset
- argument
- rbam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of large language models (LLMs)
  for relation-based argument mining (RbAM), a task focused on identifying support
  and attack relations between arguments. The authors employ a few-shot priming and
  prompting approach with five open-source LLMs (Llama 2, Mistral, and Mixtral) across
  ten datasets.
---

# Can Large Language Models perform Relation-based Argument Mining?

## Quick Facts
- arXiv ID: 2402.11243
- Source URL: https://arxiv.org/abs/2402.11243
- Reference count: 21
- Llama 70B-4bit achieves highest macro F1 score of 75 for relation-based argument mining

## Executive Summary
This paper investigates the effectiveness of large language models (LLMs) for relation-based argument mining (RbAM), a task focused on identifying support and attack relations between arguments. The authors employ a few-shot priming and prompting approach with five open-source LLMs (Llama 2, Mistral, and Mixtral) across ten datasets. Their method significantly outperforms the best RoBERTa baseline, with Llama 70B-4bit achieving the highest macro F1 score of 75. The study demonstrates that LLMs can effectively perform RbAM, with Llama 70B-4bit and Mixtral 8x7B-4bit showing the strongest performance across datasets. The authors also discuss limitations and potential future work, including extending the approach to ternary RbAM (support/attack/no relation).

## Method Summary
The paper investigates relation-based argument mining using a few-shot priming and prompting approach with five open-source LLMs (Llama 2 13B, Llama 2 70B-4bit, Mistral 7B, Mixtral 8x7B-4bit) across ten datasets containing argument pairs labeled with support/attack relations. The method involves priming the models with four fixed examples of attack and support relations, then prompting them to classify new argument pairs. The study compares these LLMs against a RoBERTa baseline and evaluates performance using macro F1 scores for both support and attack relations. The authors also experiment with GPTQ 4-bit quantization to optimize the trade-off between accuracy and memory usage.

## Key Results
- Llama 70B-4bit achieves the highest macro F1 score of 75 for RbAM
- LLM approach outperforms the best RoBERTa baseline by 6 points in macro F1
- Llama 70B-4bit and Mixtral 8x7B-4bit show strongest performance across all datasets
- LLMs perform well on support relations but show lower accuracy on attack relations compared to RoBERTa

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot priming with a small set of labeled examples allows LLMs to learn the mapping from argument pairs to relation labels without full fine-tuning.
- Mechanism: The primer provides four fixed examples of attack and support relations, establishing a pattern the model can generalize from during prompting.
- Core assumption: LLMs can extrapolate the relation classification task from a small number of demonstrations.
- Evidence anchors:
  - [abstract] "appropriately primed and prompted"
  - [section] "Our method is overviewed in Figure 1. It consists of few-shot priming... followed by prompting."
  - [corpus] Weak - no direct corpus evidence for priming effectiveness in RbAM.
- Break condition: If the priming examples are not representative of the diversity in the target datasets, performance may degrade.

### Mechanism 2
- Claim: GPTQ 4-bit quantization provides a favorable trade-off between model size and performance for LLMs in RbAM.
- Mechanism: Quantization reduces memory requirements while preserving enough model capacity to maintain classification accuracy.
- Core assumption: The reduction in precision does not significantly impact the model's ability to capture argumentative nuances.
- Evidence anchors:
  - [abstract] "we also experimented with 4bit quantisation (so each weight is stored in 4bits on the GPU) as it had the best trade-off between accuracy and space."
  - [section] "Llama 13B-4bit... with GPTQ quantisation, the performance improves."
  - [corpus] Weak - no direct corpus evidence for quantization impact on RbAM.
- Break condition: If the quantization introduces too much noise, the model's ability to distinguish between support and attack relations may suffer.

### Mechanism 3
- Claim: The task framing as binary relation classification (support/attack) simplifies the problem and improves LLM performance compared to ternary classification.
- Mechanism: By focusing only on two relation types, the model can concentrate on distinguishing between support and attack without the added complexity of a no-relation class.
- Core assumption: The binary classification is sufficient for the datasets used and captures the essential argumentative relations.
- Evidence anchors:
  - [abstract] "RbAM is a form of AM focusing on identifying agreement (support) and disagreement (attack) relations amongst arguments."
  - [section] "Specifically, we will focus on the form of AM framed as the following (binary) relation-based AM (RbAM) task..."
  - [corpus] Weak - no direct corpus evidence for the impact of binary vs. ternary classification.
- Break condition: If the datasets contain many pairs with no relation, the binary approach may misclassify them as either support or attack.

## Foundational Learning

- Concept: Few-shot learning
  - Why needed here: It allows LLMs to perform RbAM without resource-intensive fine-tuning, making the approach more practical.
  - Quick check question: Can the LLM generalize the relation classification task from a small set of examples?

- Concept: Argument structure
  - Why needed here: Understanding the components of arguments (claims, premises) and their relations is crucial for accurate RbAM.
  - Quick check question: Can you identify the argumentative structure in a given text?

- Concept: Text classification
  - Why needed here: RbAM is fundamentally a text classification task, requiring the model to categorize argument pairs.
  - Quick check question: Can you classify a given text into predefined categories based on its content?

## Architecture Onboarding

- Component map:
  - Datasets (Essays, Microtexts, Nixon-Kennedy, Debatepedia/Procon, IBM-Debater, ComArg, CDCP, UKP, Web-Content, Kialo) -> LLMs (Llama 2 13B, Llama 2 70B-4bit, Mistral 7B, Mixtral 8x7B-4bit) -> Primer (4 examples) -> Prompt (argument pairs) -> Macro F1 score evaluation

- Critical path:
  1. Prepare datasets and split into train/validation/test sets
  2. Load and quantize LLMs as needed
  3. Create primer and prompt templates
  4. Perform few-shot priming with the primer
  5. Prompt the LLM with argument pairs and collect predictions
  6. Evaluate performance using macro F1 score

- Design tradeoffs:
  - Model size vs. inference time: Larger models (e.g., Llama 70B) may achieve better performance but require more resources and time.
  - Binary vs. ternary classification: Binary classification simplifies the task but may not capture all relation types.
  - Quantization: 4-bit quantization reduces memory usage but may impact performance.

- Failure signatures:
  - Poor performance on specific datasets: May indicate a need for more diverse priming examples or dataset-specific tuning.
  - High inference time: May require optimization or use of smaller/quantized models.
  - Inconsistent results across runs: May indicate sensitivity to prompt phrasing or random initialization.

- First 3 experiments:
  1. Evaluate the performance of Llama 70B-4bit on the Debatepedia/Procon dataset using the provided primer and prompt.
  2. Compare the performance of different LLMs (e.g., Llama 13B, Mistral 7B, Mixtral 8x7B) on the IBM-Debater dataset.
  3. Assess the impact of binary vs. ternary classification on a subset of the Kialo dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of LLMs on relation-based argument mining change if the task were extended to include a "no relation" class, making it a ternary classification problem?
- Basis in paper: [explicit] The authors discuss extending the work to the more challenging ternary RbAM task, which includes identifying support, attack, or no relation between two arguments.
- Why unresolved: The current study focuses on binary RbAM (support/attack), and the authors acknowledge that ternary RbAM is more challenging but have not yet investigated its performance with LLMs.
- What evidence would resolve it: Experimental results comparing the performance of LLMs on binary versus ternary RbAM tasks, including metrics like macro F1 scores, would provide insights into the impact of adding a "no relation" class.

### Open Question 2
- Question: How would the performance of LLMs on relation-based argument mining vary across different languages, given that the current study only uses English datasets?
- Basis in paper: [inferred] The authors mention that the datasets used are in English and express uncertainty about whether LLMs will perform as well on RbAM in other languages.
- Why unresolved: The study does not explore the multilingual capabilities of LLMs for RbAM, leaving open the question of how language differences might affect performance.
- What evidence would resolve it: Conducting experiments with RbAM datasets in multiple languages and comparing the performance of LLMs across these languages would provide evidence on the multilingual effectiveness of the approach.

### Open Question 3
- Question: What impact would incorporating argumentative structure, such as masking entities, have on the performance of LLMs in relation-based argument mining?
- Basis in paper: [explicit] The authors suggest that masking entities to outline their argumentative structure could improve performance, as shown in argument retrieval tasks.
- Why unresolved: The study does not implement or test the effect of masking entities on RbAM performance, leaving the potential benefits unexplored.
- What evidence would resolve it: Experimental results comparing the performance of LLMs on RbAM with and without entity masking would reveal the impact of incorporating argumentative structure on the task's effectiveness.

## Limitations

- The study only evaluates binary classification (support/attack) and does not investigate ternary RbAM which includes a "no relation" class
- Performance on attack relations is significantly lower than support relations, with 43 instances where LLMs generated labels other than attack or support
- The priming approach uses a fixed set of four examples across all datasets without investigating sensitivity to priming diversity or quantity

## Confidence

**High confidence** in: The methodology of using few-shot priming and prompting for RbAM is sound and well-described. The experimental setup with multiple datasets and LLMs is appropriate for benchmarking.

**Medium confidence** in: The quantitative results showing LLM superiority over RoBERTa, though the lack of variance reporting and statistical tests reduces confidence in the magnitude of improvements.

**Low confidence** in: Claims about quantization benefits and the robustness of the prompting approach, given the limited empirical support provided.

## Next Checks

1. Run statistical significance tests comparing LLM performance against RoBERTa across all datasets, including confidence intervals and p-values to determine if observed differences are meaningful rather than due to chance.

2. Test priming sensitivity by varying the number and diversity of priming examples (e.g., using 2, 4, and 8 examples) and measuring the impact on performance across different dataset types to assess whether the current four-example approach is optimal.

3. Validate quantization impact by comparing 4-bit quantized models against 8-bit and full-precision versions of the same models on a subset of datasets, measuring both performance degradation and inference speed improvements to verify the claimed trade-off.