---
ver: rpa2
title: 'MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs'
arxiv_id: '2411.15296'
source_url: https://arxiv.org/abs/2411.15296
tags:
- evaluation
- arxiv
- mllms
- benchmarks
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey provides a comprehensive overview of the evaluation\
  \ landscape for Multimodal Large Language Models (MLLMs), addressing the rapidly\
  \ growing need for standardized assessment methods. The paper categorizes existing\
  \ benchmarks by evaluation capabilities\u2014foundational, model self-analysis,\
  \ and extended applications\u2014covering over 100 benchmarks spanning perception,\
  \ reasoning, OCR, medical imaging, and autonomous driving."
---

# MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs

## Quick Facts
- arXiv ID: 2411.15296
- Source URL: https://arxiv.org/abs/2411.15296
- Authors: Chaoyou Fu; Yi-Fan Zhang; Shukang Yin; Bo Li; Xinyu Fang; Sirui Zhao; Haodong Duan; Xing Sun; Ziwei Liu; Liang Wang; Caifeng Shan; Ran He
- Reference count: 40
- This survey provides a comprehensive overview of the evaluation landscape for Multimodal Large Language Models (MLLMs), addressing the rapidly growing need for standardized assessment methods.

## Executive Summary
This survey systematically categorizes over 100 benchmarks for evaluating Multimodal Large Language Models (MLLMs) across three main capability types: foundational capabilities, model self-analysis, and extended applications. The paper outlines a comprehensive benchmark construction pipeline including data collection, annotation, and evaluation methods, while introducing evaluation toolkits like VLMEvalKit and LMMs-Eval. It addresses key challenges in MLLM evaluation such as the lack of unified capability taxonomy, gaps in task-specific and multi-modal evaluations, and the need for incorporating additional modalities like audio and 3D.

## Method Summary
The survey synthesizes existing evaluation approaches by analyzing over 100 benchmarks and categorizing them into foundational capabilities, model behavior, and extended applications. It describes the typical benchmark construction process including data collection (from existing datasets, modifications, or new gathering), annotation methods (automatic, LLM/MLLM prompting, or manual), and precautions like data leakage prevention. The evaluation pipeline covers human-based, LLM/MLLM-based, and script-based approaches, with metrics for both deterministic (accuracy, F1, mAP) and non-deterministic tasks. The survey also introduces four major evaluation toolkits including VLMEvalKit and LMMs-Eval for streamlined assessment.

## Key Results
- Categorized over 100 benchmarks into foundational capabilities, model self-analysis, and extended applications
- Established a systematic benchmark construction pipeline from data collection through evaluation methods
- Introduced evaluation toolkits (VLMEvalKit, LMMs-Eval) that standardize data preprocessing, model wrapping, and result aggregation
- Identified key challenges including lack of unified capability taxonomy and gaps in task-specific and multi-modal evaluations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The survey provides a hierarchical taxonomy that organizes benchmarks into foundational capabilities, model self-analysis, and extended applications, which helps researchers systematically locate relevant evaluation methods.
- **Mechanism:** By categorizing over 100 benchmarks into three main capability types, the survey reduces cognitive load and accelerates the discovery of suitable benchmarks for specific research needs.
- **Core assumption:** A well-structured taxonomy improves accessibility and reduces the time required to find relevant benchmarks.
- **Evidence anchors:**
  - [abstract] "the summarised benchmarks types divided by the evaluation capabilities, including foundation capabilities, model self-analysis, and extended applications"
  - [section] "On the top level, these benchmarks can be categorized as evaluations of foundational capabilities, model behavior, and extended applications"
  - [corpus] "A Survey on Benchmarks of Multimodal Large Language Models" (0 citations, weak evidence)
- **Break condition:** If the taxonomy becomes outdated as new benchmarks emerge faster than the survey can be updated, the classification system will lose its utility.

### Mechanism 2
- **Claim:** The systematic evaluation pipeline (data collection → annotation → evaluation methods → metrics → toolkits) provides a clear roadmap for building new benchmarks.
- **Mechanism:** By outlining each step of the benchmark construction process, the survey reduces uncertainty and errors in benchmark design, leading to more robust and reliable evaluations.
- **Core assumption:** A standardized pipeline ensures consistency and quality across different benchmarks.
- **Evidence anchors:**
  - [abstract] "the typical process of benchmark construction, consisting of data collection, annotation, and precautions"
  - [section] "We collate typical approaches in the pipeline of benchmark construction, including the gathering of samples and the annotation of Question-Answer (QA) pairs"
  - [corpus] "A Survey on Evaluation of Multimodal Large Language Models" (0 citations, weak evidence)
- **Break condition:** If the pipeline is too rigid and doesn't account for emerging evaluation needs or novel data types, it may hinder innovation in benchmark design.

### Mechanism 3
- **Claim:** Introducing evaluation toolkits like VLMEvalKit and LMMs-Eval simplifies the process of running comprehensive evaluations across multiple datasets and models.
- **Mechanism:** These toolkits standardize data preprocessing, model wrapping, and result aggregation, reducing the technical barriers to conducting large-scale evaluations.
- **Core assumption:** Standardization through toolkits leads to more reproducible and comparable evaluation results.
- **Evidence anchors:**
  - [abstract] "we also introduce two major types of evaluation metrics as well as four evaluation toolkits"
  - [section] "The evaluation of LLMs, OpenAI introduces the Evals framework... In the context of MLLMs, representative toolkits include VLMEvalKit [212] and LMMs-Eval [213]"
  - [corpus] "Empowering Multimodal LLMs with External Tools: A Comprehensive Survey" (0 citations, weak evidence)
- **Break condition:** If the toolkits don't keep pace with new model architectures or evaluation needs, they may become obsolete and limit the scope of evaluations.

## Foundational Learning

- **Concept:** Multimodal data processing and alignment
  - **Why needed here:** Understanding how different modalities (text, image, video, audio) are encoded and aligned is crucial for designing effective benchmarks that truly test multimodal capabilities.
  - **Quick check question:** What are the key differences between vision-language models and traditional language models in terms of input processing?

- **Concept:** Chain-of-thought reasoning in MLLMs
  - **Why needed here:** Many benchmarks evaluate complex reasoning abilities, so understanding how MLLMs generate intermediate reasoning steps is essential for interpreting benchmark results.
  - **Quick check question:** How does chain-of-thought prompting improve the performance of MLLMs on complex reasoning tasks?

- **Concept:** Evaluation metric selection and interpretation
  - **Why needed here:** Choosing appropriate metrics (deterministic vs. non-deterministic) is critical for accurately assessing model performance across different task types.
  - **Quick check question:** When would you use F1 score versus accuracy for evaluating MLLM performance?

## Architecture Onboarding

- **Component map:**
  Data collection → Annotation → Evaluation module → Metrics module → Toolkits module

- **Critical path:**
  1. Define evaluation capabilities needed
  2. Select appropriate benchmarks from the taxonomy
  3. Collect and annotate data following best practices
  4. Choose evaluation methods and metrics
  5. Run evaluations using appropriate toolkits
  6. Analyze and interpret results

- **Design tradeoffs:**
  - Multiple-choice questions are easier to automate but may leak information; open-ended questions are more challenging but require more sophisticated evaluation methods
  - Automatic annotation is cost-effective but may introduce noise; manual annotation ensures quality but is expensive and time-consuming
  - Human evaluation is most accurate but labor-intensive; LLM-based evaluation is scalable but may have systematic biases

- **Failure signatures:**
  - Poor performance on fine-grained perception tasks (small objects, dense text, high resolution)
  - Inability to handle multi-image reasoning or interleaved text-image content
  - Hallucinations or biases in responses to visual questions
  - Struggles with out-of-distribution data or adversarial examples

- **First 3 experiments:**
  1. Run VLMEvalKit on a small subset of benchmarks with 2-3 popular MLLMs to verify the evaluation pipeline works
  2. Test different annotation methods (automatic vs. manual) on a small dataset to compare quality and cost
  3. Evaluate a single MLLM on both deterministic and non-deterministic metrics for the same task to understand metric differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal approach for designing a universally accepted capability taxonomy for multimodal large language models (MLLMs)?
- Basis in paper: [explicit] The paper highlights the lack of a well-defined, universally accepted capability taxonomy for MLLMs, with existing benchmarks defining their own disparate ability dimensions.
- Why unresolved: Creating a standardized taxonomy requires balancing comprehensive coverage of capabilities with practical applicability across diverse tasks and applications, while also aligning with human cognition and established cognitive science research.
- What evidence would resolve it: Development and validation of a standardized taxonomy through extensive empirical studies, expert consensus, and cross-benchmark evaluations.

### Open Question 2
- Question: How can multimodal evaluation benchmarks be designed to effectively assess multi-turn dialogue capabilities and creative task performance in MLLMs?
- Basis in paper: [explicit] The paper identifies gaps in current evaluations, noting limited focus on instruction following, multi-turn dialogue, and creative tasks such as text generation based on image and textual prompts.
- Why unresolved: These capabilities require nuanced assessment methods beyond traditional objective metrics, and there is a lack of established benchmarks specifically targeting these areas.
- What evidence would resolve it: Creation of new benchmarks incorporating real-world dialogue scenarios, subjective evaluation methods, and metrics tailored to creative multimodal tasks.

### Open Question 3
- Question: What is the most effective approach for evaluating MLLM performance on specialized, task-oriented applications such as invoice recognition, multimodal knowledge base comprehension, and industrial visual inspection?
- Basis in paper: [explicit] The paper emphasizes the need for task-specific evaluations for MLLMs, particularly in commercially relevant domains, while considering computational costs and inference speeds compared to traditional computer vision methods.
- Why unresolved: Balancing the depth of evaluation for specific tasks with the breadth of general capabilities, and developing metrics that accurately reflect real-world performance and practical applicability.
- What evidence would resolve it: Development and validation of task-specific evaluation frameworks, including comprehensive performance comparisons with traditional methods and assessments of computational efficiency.

## Limitations
- Rapid obsolescence risk as the fast-paced evolution of MLLM evaluation methods may quickly render the taxonomy outdated
- Limited empirical validation of the taxonomy's practical utility in real research workflows
- Treatment of evaluation toolkits is constrained by the rapidly changing landscape, with some mentioned toolkits having minimal citations suggesting limited adoption

## Confidence
- Benchmark taxonomy organization: Medium
- Evaluation pipeline framework: High
- Toolkits effectiveness claims: Low-Medium
- Challenge identification: High

## Next Checks
1. Verify the current maintenance status and performance statistics of at least 10 randomly selected benchmarks from each capability category to assess taxonomy relevance
2. Conduct a user study with 5-10 researchers to measure time savings and usability when using the proposed taxonomy versus searching without it
3. Test the complete evaluation pipeline on a new benchmark construction task to identify practical gaps or simplifications needed