---
ver: rpa2
title: Coefficient Decomposition for Spectral Graph Convolution
arxiv_id: '2405.03296'
source_url: https://arxiv.org/abs/2405.03296
tags:
- graph
- coef
- linear
- decomposition
- spectral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a general form of spectral graph convolution
  (SGCL) based on polynomial filters, where the coefficients are stored in a third-order
  tensor. By analyzing existing SGCNs, the authors show that their convolution layers
  can be derived by performing coefficient decomposition operations on the coefficient
  tensor.
---

# Coefficient Decomposition for Spectral Graph Convolution

## Quick Facts
- arXiv ID: 2405.03296
- Source URL: https://arxiv.org/abs/2405.03296
- Authors: Feng Huang; Wen Zhang
- Reference count: 40
- Key outcome: Proposed CoDeSGC-CP and CoDeSGC-Tucker achieve favorable performance improvements, outperforming state-of-the-art methods on 8 out of 10 real-world datasets for node classification.

## Executive Summary
This paper introduces a general framework for spectral graph convolution (SGCL) based on polynomial filters, where coefficients are stored in a third-order tensor. By analyzing existing SGCNs, the authors demonstrate that their convolution layers can be derived through coefficient decomposition operations on this tensor. To extend these architectures, they develop two novel SGCLs - CoDeSGC-CP and CoDeSGC-Tucker - by applying CP and Tucker tensor decomposition respectively. Extensive experiments on node classification tasks demonstrate significant performance improvements over state-of-the-art methods.

## Method Summary
The paper proposes a generalized spectral graph convolution framework where polynomial coefficients are stored in a third-order tensor. By applying CP (CANDECOMP/PARAFAC) or Tucker tensor decomposition to this coefficient tensor, the authors derive two novel architectures: CoDeSGC-CP and CoDeSGC-Tucker. These methods learn multilinear relationships between polynomial coefficients through factor matrices, allowing for more sophisticated filtering compared to traditional fixed-coefficient approaches. The decomposition enables decoupling of polynomial coefficient learning from graph structure learning, potentially improving generalization.

## Key Results
- CoDeSGC-CP and CoDeSGC-Tucker outperform state-of-the-art method JacobiConv on 8 out of 10 real-world datasets
- The Tucker decomposition variant achieves significant performance gains by capturing more complex interactions between polynomial orders
- The proposed methods show favorable performance improvements across diverse graph structures including both homophilic and heterophilic graphs

## Why This Works (Mechanism)

### Mechanism 1
Coefficient tensor decomposition enables learning richer multilinear relationships between polynomial coefficients compared to fixed or single-factor linear decompositions. The third-order coefficient tensor stores all polynomial coefficients across input channels, output channels, and polynomial orders. By applying CP or Tucker decomposition, the model learns shared factors across these dimensions, allowing each output channel's filter to be a weighted combination of basis patterns learned from the data, rather than a simple linear combination. This works because the multilinear structure of the coefficient tensor captures meaningful correlations between different polynomial orders and channels that simpler decompositions would miss.

### Mechanism 2
Decoupling polynomial coefficient learning from graph structure learning through coefficient decomposition improves generalization. By separating the coefficient tensor into factor matrices (for CP) or core tensor and factor matrices (for Tucker), the model learns independent representations for input channels, output channels, and polynomial order relationships. This separation allows the model to adapt the polynomial coefficients to the specific characteristics of the input data while maintaining the graph structure through the polynomial basis functions. The graph structure information is sufficiently captured by the polynomial basis functions (Pk(S)), and the coefficient decomposition only needs to learn optimal weighting patterns.

### Mechanism 3
The Tucker decomposition variant (CoDeSGC-Tucker) can capture more complex interactions between polynomial orders than CP decomposition due to its core tensor. Tucker decomposition introduces a core tensor that acts as a linear mapping between the factor matrices, allowing for interactions between different polynomial orders and channels that aren't captured by the simpler CP decomposition where factors are combined through outer products. This additional complexity is beneficial for learning polynomial coefficient patterns, though it may introduce more parameters relative to the available data.

## Foundational Learning

- Concept: Tensor decomposition (CP and Tucker)
  - Why needed here: Understanding how to decompose the coefficient tensor is fundamental to implementing CoDeSGC-CP and CoDeSGC-Tucker. CP decomposes a tensor into rank-one components, while Tucker decomposes it into a core tensor and factor matrices.
  - Quick check question: What is the key difference between CP and Tucker decomposition in terms of how they represent tensor structure?

- Concept: Graph signal processing and spectral graph convolution
  - Why needed here: The paper builds on spectral graph convolution theory, where filters are defined in the spectral domain using graph Laplacian eigenvalues. Understanding this foundation is crucial for grasping why polynomial approximations are used.
  - Quick check question: Why do most spectral graph convolutional networks use polynomial approximations of graph filters instead of direct eigendecomposition?

- Concept: Polynomial basis functions for graph filtering
  - Why needed here: Different polynomial bases (Chebyshev, Monomial, Jacobi, etc.) can be substituted into the general spectral convolution framework. Understanding their properties helps in selecting appropriate bases for different graph structures.
  - Quick check question: What is the main advantage of using Chebyshev polynomials over Monomial basis for spectral graph convolution?

## Architecture Onboarding

- Component map: Input signals -> Polynomial basis functions (Pk(S)) -> Coefficient tensor W -> Tensor decomposition (CP or Tucker) creating factor matrices -> Linear transformations using factor matrices -> Propagation steps using sparse matrix multiplication -> Output layer

- Critical path:
  1. Transform input features to input signals (if hybrid model)
  2. Apply polynomial basis functions to graph matrix
  3. Compute factor matrix multiplications
  4. Perform iterative sparse matrix multiplications for propagation
  5. Apply final linear transformation to get output signals
  6. Apply softmax for classification

- Design tradeoffs:
  - CP vs Tucker: CP is more parameter-efficient but Tucker can capture more complex interactions
  - Order of polynomial K: Higher K allows more complex filters but increases computational cost
  - Rank/R in decomposition: Higher rank allows more expressive factors but increases parameters

- Failure signatures:
  - Overfitting on small datasets: Model may memorize training data rather than learn generalizable patterns
  - Poor convergence: Learning rates may need adjustment for different factor matrices
  - Performance worse than simpler models: Decomposition may not capture useful structure for certain graph types

- First 3 experiments:
  1. Implement CoDeSGC-CP with Jacobi basis and compare against JacobiConv on Cora dataset
  2. Vary the rank R in CoDeSGC-CP to find optimal trade-off between performance and parameters
  3. Implement CoDeSGC-Tucker and compare its performance against CoDeSGC-CP on heterophilic graphs like Chameleon

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CoDeSGC-CP and CoDeSGC-Tucker change when using different polynomial bases (e.g., Monomial, Chebyshev, Bernstein, Jacobi) and graph matrices (e.g., normalized adjacency, Laplacian)? The paper only tests one specific combination of polynomial basis and graph matrix, leaving the impact of other choices unexplored.

### Open Question 2
What is the impact of the order of the polynomial filter K on the performance of CoDeSGC-CP and CoDeSGC-Tucker, and is there an optimal value for different datasets? The paper uses a fixed polynomial order for all experiments, preventing analysis of its impact on performance.

### Open Question 3
How do CoDeSGC-CP and CoDeSGC-Tucker perform on graph classification and link prediction tasks, compared to their performance on node classification? The paper only evaluates the models on node classification, leaving their performance on other common graph learning tasks unexplored.

## Limitations
- The method requires careful hyperparameter tuning of decomposition ranks, which can be computationally expensive and may lead to overfitting on small datasets
- The paper doesn't provide extensive ablation studies on how different polynomial bases interact with tensor decomposition
- The theoretical analysis focuses on decomposition mechanics but doesn't fully explain why specific decompositions work better for certain graph heterophily patterns

## Confidence
- High confidence: The core mechanism of coefficient tensor decomposition is mathematically sound and the experimental methodology is rigorous with proper baselines and evaluation protocols
- Medium confidence: Performance claims are well-supported by extensive experiments across 10 datasets, though some results show smaller margins than others
- Low confidence: The paper's claims about why specific decompositions (CP vs Tucker) work better for certain graph types lack comprehensive theoretical justification

## Next Checks
1. Conduct controlled experiments varying polynomial basis functions (Chebyshev, Monomial, Jacobi) while keeping decomposition method fixed to isolate their effects
2. Perform ablation studies on the effect of decomposition rank on both performance and parameter efficiency across different graph sizes
3. Test the models on synthetic graphs with controlled heterophily levels to better understand when Tucker decomposition provides advantages over CP