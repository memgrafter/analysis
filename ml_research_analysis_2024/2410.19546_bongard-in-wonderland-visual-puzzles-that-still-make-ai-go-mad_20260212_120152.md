---
ver: rpa2
title: 'Bongard in Wonderland: Visual Puzzles that Still Make AI Go Mad?'
arxiv_id: '2410.19546'
source_url: https://arxiv.org/abs/2410.19546
tags:
- task
- visual
- rule
- vlms
- left
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLMs struggle to solve Bongard visual reasoning puzzles, with top
  model o1 solving only 43/100 problems. Humans significantly outperform VLMs (68
  vs.
---

# Bongard in Wonderland: Visual Puzzles that Still Make AI Go Mad?

## Quick Facts
- arXiv ID: 2410.19546
- Source URL: https://arxiv.org/abs/2410.19546
- Reference count: 40
- Primary result: VLMs solve only 43/100 Bongard problems, significantly underperforming humans at 68/100

## Executive Summary
VLMs struggle to solve Bongard visual reasoning puzzles, with top model o1 solving only 43/100 problems compared to human performance of 68/100. The primary failure modes include inability to recognize basic visual concepts like spirals and spatial relations, and a disconnect between hypothesis generation and problem-solving. While VLMs can often generate correct hypotheses about underlying rules, they cannot apply these rules to solve the full problem. This highlights a fundamental gap in how current VLMs integrate visual perception with abstract reasoning, suggesting that impressive performance on established VLM benchmarks may not translate to more challenging visual reasoning tasks.

## Method Summary
The study evaluates pre-trained vision-language models on 100 original Bongard problems using three tasks: open-ended solving, detecting specific concepts given ground truth rules, and formulating hypotheses. No training is performed on the models. Performance is compared against human participants solving the same problems. The evaluation uses an LLM-as-a-judge approach with few-shot examples for consistency. Bongard problems consist of 12 black-and-white diagrams divided into two groups, each following a distinct rule that does not apply to the other group, requiring identification of these rules in natural language.

## Key Results
- VLMs solve only 43/100 Bongard problems versus human performance of 68/100
- Models frequently fail at basic visual concepts like spirals and spatial relations
- VLMs can generate correct hypotheses but cannot apply them to solve full problems

## Why This Works (Mechanism)

### Mechanism 1
VLMs fail Bongard problems due to perceptual limitations in recognizing basic visual concepts like spirals and spatial relations, not abstract reasoning deficits. The visual feature extraction pipeline is insufficient to detect fine-grained shape properties, causing errors even when the abstract rule is simple (e.g., "left vs. right" or "clockwise vs. counter-clockwise"). If the perceptual model is replaced or augmented with explicit low-level feature detectors that accurately extract orientation, curvature, and symmetry, performance should improve significantly.

### Mechanism 2
VLMs can generate correct hypotheses for Bongard rules but cannot apply them to solve the full problem, indicating a reasoning application gap. The hypothesis generation task is treated as a free-form brainstorming exercise, where the model can propose valid rules without needing to validate them. However, in the open-ended task, the model must simultaneously perceive the images, apply the rule, and output a coherent description, which requires integrating perception and reasoning in a single pipeline—a capability current VLMs lack. If the open-ended task is decomposed into perception → rule application → output stages, and models are evaluated per stage, the hypothesis-application gap should narrow.

### Mechanism 3
Bongard problems expose a generalization failure in VLMs: models cannot transfer learned visual concept recognition to novel configurations or unseen rule types. Bongard problems require forming a concept from a small set of examples and applying it to a new set, without prior exposure. VLMs, trained on large-scale datasets, overfit to common patterns and lack mechanisms for one-shot or few-shot abstraction. If VLMs are trained or fine-tuned on few-shot abstraction tasks or explicit concept formation, generalization to Bongard-style problems should improve.

## Foundational Learning

- **Concept**: Perceptual feature extraction and representation learning for low-level visual properties (orientation, curvature, symmetry, spatial relations)
  - Why needed here: Bongard problems rely on detecting and differentiating fine-grained visual features (e.g., clockwise vs. counter-clockwise spirals, left vs. right positioning). Without robust feature extraction, higher-level reasoning fails.
  - Quick check question: Can you describe the exact pixel-level or shape-level difference between a clockwise and counter-clockwise spiral? How would a VLM detect this without prior training on such examples?

- **Concept**: Concept formation and abstraction from limited examples (few-shot learning)
  - Why needed here: Bongard problems present only 6 examples per side; models must form a concept and apply it to the other 6. This requires generalization beyond memorized patterns.
  - Quick check question: Given 6 examples of shapes with a "triangle on top of a circle," how would you define the concept so it applies to a new set without overfitting to specific shapes?

- **Concept**: Integration of perception and reasoning in a single pipeline (visual reasoning)
  - Why needed here: Solving Bongard problems requires not only recognizing features but also reasoning about their relationships and expressing them in language. Disconnected perception and reasoning modules lead to brittle performance.
  - Quick check question: If a model correctly identifies that "all shapes on the left are convex" but misclassifies individual shapes, what does this say about the integration between its visual and reasoning modules?

## Architecture Onboarding

- **Component map**: Input: Image encoder (CNN/ViT) → Feature extractor → Concept detector → Rule hypothesis generator → Rule validator → Output formatter
- **Critical path**: Image → Feature extraction → Concept classification → Rule application → Language generation
- **Design tradeoffs**: Tradeoff between fine-grained feature detection (better perception) vs. abstract reasoning capacity; end-to-end vs. modular architectures
- **Failure signatures**: Consistent misclassification of basic shapes or spatial relations; correct rule generation but incorrect image classification; inability to transfer learned concepts to new configurations
- **First 3 experiments**:
  1. Replace the image encoder with a specialized shape/geometry detector and evaluate Bongard performance on problems requiring fine-grained shape analysis
  2. Decompose the Bongard task into perception → concept detection → rule generation stages and evaluate per-stage accuracy
  3. Train a few-shot abstraction model on synthetic Bongard-like problems and test transfer to the original Bongard set

## Open Questions the Paper Calls Out

### Open Question 1
What specific architectural or training modifications would enable VLMs to reliably solve Bongard problems that require spatial reasoning (e.g., BP#8, BP#42)? The paper identifies spatial reasoning as one of the most challenging categories for VLMs, with all models solving under 25% of spatial BPs compared to humans solving over 60%. This question remains unresolved as the paper demonstrates VLMs' failures but does not investigate potential architectural solutions or training approaches that could address spatial reasoning deficits. Comparative experiments testing VLMs with enhanced spatial reasoning modules (e.g., graph neural networks, spatial transformers) against baseline models on Bongard problems would provide evidence to resolve this question.

### Open Question 2
Does the gap between VLM hypothesis generation and problem-solving in Bongard problems indicate a fundamental limitation in how VLMs integrate visual perception with abstract reasoning? The paper reveals a surprising finding where models frequently generate correct hypotheses but fail to solve problems, suggesting they can identify rules in isolation but struggle to apply them. This phenomenon is documented across multiple task settings, but it remains unclear whether this reflects architectural limitations or can be addressed through improved reasoning frameworks. Experiments testing whether models trained with explicit reasoning chains or multi-step verification procedures show improved performance on Bongard problems would help resolve this question.

### Open Question 3
How does the performance of VLMs on Bongard problems relate to their performance on other established visual reasoning benchmarks? The paper notes that VLMs perform impressively on various established VLM benchmarks but struggle with Bongard problems, raising questions about benchmark validity and the true nature of VLM reasoning capabilities. The paper provides no direct comparison between Bongard performance and other benchmark results for the same models, leaving open whether Bongard problems measure fundamentally different capabilities. Systematic correlation analysis between Bongard problem performance and performance on established benchmarks like VQA, GQA, and ARC across multiple VLM architectures would provide evidence to resolve this question.

## Limitations

- The study's findings are based on a single benchmark of 100 Bongard problems, which may not fully represent the diversity of visual reasoning challenges
- The LLM-as-a-judge methodology introduces potential bias and subjectivity in evaluation
- Performance gaps between models and humans could be influenced by task framing, prompt engineering, or the specific VLMs selected for evaluation

## Confidence

- **High confidence**: VLMs significantly underperform humans on Bongard problems (43 vs. 68 problems solved)
- **Medium confidence**: Models frequently fail at basic visual concepts like spirals and spatial relations
- **Medium confidence**: VLMs can generate correct hypotheses but struggle to apply them to solve full problems
- **Low confidence**: The exact mechanisms of perceptual failure vs. reasoning deficits are definitively established

## Next Checks

1. Replicate the Bongard problem evaluation using different image resolution settings and prompt engineering approaches to isolate the impact of these factors on performance
2. Conduct ablation studies by providing models with explicit feature extraction tools (e.g., shape detectors, spatial relation annotators) to determine if perceptual limitations drive the performance gap
3. Test the hypothesis-application gap by decomposing the Bongard task into perception → rule application → output stages and evaluating model performance at each stage separately