---
ver: rpa2
title: MoRe Fine-Tuning with 10x Fewer Parameters
arxiv_id: '2408.17383'
source_url: https://arxiv.org/abs/2408.17383
tags:
- more
- monarch
- matrices
- lora
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoRe, a parameter-efficient fine-tuning method
  that uses Monarch matrices to achieve 10x-20x better parameter efficiency than LoRA
  while maintaining or improving performance across multiple NLP tasks. The key insight
  is that rectangular Monarch matrices allow for flexible architecture search while
  maintaining computational efficiency.
---

# MoRe Fine-Tuning with 10x Fewer Parameters

## Quick Facts
- arXiv ID: 2408.17383
- Source URL: https://arxiv.org/abs/2408.17383
- Reference count: 21
- Primary result: MoRe achieves 10x-20x better parameter efficiency than LoRA while maintaining or improving performance across multiple NLP tasks

## Executive Summary
This paper introduces MoRe, a parameter-efficient fine-tuning method that leverages Monarch matrices to achieve significantly better parameter efficiency than LoRA. The key innovation is using rectangular Monarch matrices instead of low-rank matrices, allowing for more flexible architecture search while maintaining computational efficiency. The method demonstrates superior performance across commonsense reasoning, math reasoning, and language understanding tasks while requiring as few as 5% of LoRA's parameters. The authors provide both theoretical analysis showing MoRe is more expressive than LoRA and extensive empirical validation across multiple benchmarks.

## Method Summary
MoRe Fine-Tuning is a parameter-efficient method that uses Monarch matrices to update model weights during fine-tuning. The method replaces the low-rank matrix updates of LoRA with a structured matrix formed by the product of two block-diagonal matrices (L and R) and two fixed permutation matrices (P1 and P2). The architecture uses a fixed 4-block configuration with block-diagonal matrices of shape (N, rblk, n/N), where N=4 blocks and rblk is the rank of each block. During training, only L and R are updated while P1 and P2 remain fixed, and the resulting matrix is absorbed into the original model weights during inference.

## Key Results
- Achieves 10x-20x better parameter efficiency compared to LoRA
- Maintains or improves performance across commonsense reasoning, math reasoning, and GLUE benchmark tasks
- Requires as few as 5% of LoRA's parameters while achieving comparable or better performance
- Demonstrates superior results with minimal hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MoRe achieves superior parameter efficiency by using rectangular Monarch matrices instead of low-rank matrices.
- Mechanism: Rectangular Monarch matrices allow for more flexible rank-sparsity trade-offs, enabling better performance with fewer parameters than LoRA's low-rank assumption.
- Core assumption: The rectangular structure of Monarch matrices provides a richer parameterization space than low-rank matrices while maintaining computational efficiency.
- Evidence anchors:
  - [abstract] "The key insight is that rectangular Monarch matrices allow for flexible architecture search while maintaining computational efficiency."
  - [section 3] "MoRe Fine-Tuning... update L and R, where L has shape (N, rblk, n/N) and R has shape (N, n/N, rblk)."
  - [corpus] No direct evidence in corpus papers about Monarch matrix superiority
- Break condition: If rectangular Monarch matrices cannot maintain computational efficiency or if their parameterization does not provide significant advantages over low-rank matrices.

### Mechanism 2
- Claim: MoRe is more expressive than LoRA due to its ability to represent a wider range of structured matrices.
- Mechanism: Monarch matrices can represent any structured matrix and various transforms, making them more expressive than LoRA's low-rank constraint.
- Core assumption: The ability to represent a wider range of structured matrices translates to better performance on diverse tasks.
- Evidence anchors:
  - [abstract] "Theoretically, we show that MoRe is more expressive than LoRA."
  - [section 1] "Monarch matrices... subsume butterfly matrices and belong to the Kaleidoscope matrices, a class of matrices that can represent any structured matrix."
  - [corpus] No direct evidence in corpus papers about expressiveness comparison
- Break condition: If the increased expressiveness does not translate to measurable performance improvements on real tasks.

### Mechanism 3
- Claim: MoRe requires minimal hyperparameter tuning compared to other parameter-efficient fine-tuning methods.
- Mechanism: The fixed block configuration (N=4) and fewer tunable hyperparameters simplify the optimization process.
- Core assumption: A simpler hyperparameter space leads to better performance with less tuning effort.
- Evidence anchors:
  - [abstract] "The method requires minimal hyperparameter tuning and demonstrates superior performance with as few as 5% of LoRA's parameters."
  - [section 3.1] "Interestingly, our search converged to a minimal 4-block architecture with the fewest tunable hyperparameters among all methods."
  - [corpus] No direct evidence in corpus papers about hyperparameter tuning requirements
- Break condition: If the fixed architecture proves suboptimal for certain tasks or model architectures.

## Foundational Learning

- Concept: Low-rank matrix approximation
  - Why needed here: Understanding LoRA's foundation helps appreciate MoRe's improvements
  - Quick check question: What is the computational complexity of a low-rank matrix product compared to a full matrix product?

- Concept: Structured matrices and their properties
  - Why needed here: Monarch matrices are a class of structured matrices; understanding their properties is crucial
  - Quick check question: How do Monarch matrices differ from butterfly matrices in terms of expressiveness and computational efficiency?

- Concept: Parameter-efficient fine-tuning techniques
  - Why needed here: MoRe is a PEFT method; understanding the landscape helps contextualize its contributions
  - Quick check question: What are the key trade-offs between different PEFT methods like LoRA, Adapters, and ReFT?

## Architecture Onboarding

- Component map:
  - Input layer: Original model weights W
  - MoRe adapter: Monarch matrix M = P1LP2R
    - L: Block-diagonal matrix (N, rblk, n/N)
    - R: Block-diagonal matrix (N, n/N, rblk)
    - P1, P2: Fixed permutation matrices
  - Output layer: Adapted model weights absorbed during inference

- Critical path:
  1. Initialize L and R with small random values
  2. Apply permutations P1 and P2
  3. Compute MoRe matrix M = P1LP2R
  4. Update only L and R during training
  5. Absorb M into W during inference

- Design tradeoffs:
  - Flexibility vs. computational efficiency: Rectangular Monarch matrices offer more flexibility but may introduce overhead
  - Expressiveness vs. simplicity: More expressive matrices may require more complex architectures
  - Parameter efficiency vs. performance: Fewer parameters may lead to reduced performance on certain tasks

- Failure signatures:
  - Poor convergence: May indicate suboptimal hyperparameter choices or architecture
  - Memory overflow: Could result from overly large block sizes or numbers
  - Performance degradation: Might suggest the rectangular structure is not suitable for the task

- First 3 experiments:
  1. Compare MoRe with LoRA on a simple task (e.g., CoLA) with fixed rank
  2. Test MoRe with different block numbers (N) while keeping rank constant
  3. Evaluate MoRe on a larger model (e.g., Llama 7B) to assess scalability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MoRe's performance scale with different block ranks (rblk) beyond the tested range, particularly when rblk approaches or exceeds the number of blocks (N)?
- Basis in paper: [explicit] The paper mentions that when fixing block configuration, they tested different block ranks and numbers of blocks, but the relationship between rblk and N is not fully explored.
- Why unresolved: The paper only briefly touches on the relationship between rblk and N, stating that performance drops when N > 4, but doesn't explore the full spectrum of possible rblk values.
- What evidence would resolve it: Comprehensive experiments varying rblk across a wider range of values while keeping N fixed would clarify how the rank of individual blocks impacts overall performance.

### Open Question 2
- Question: Can MoRe be effectively initialized from dense matrices' principal components to improve convergence and performance, similar to approaches in Meng et al. (2024)?
- Basis in paper: [inferred] The paper mentions that attempts to initialize MoRe from principal components failed on reasoning tasks, but doesn't provide detailed analysis of why this approach was unsuccessful.
- Why unresolved: The paper only briefly mentions the failure of dense-to-sparse initialization without exploring alternative initialization strategies or understanding the underlying reasons for the failure.
- What evidence would resolve it: Successful implementation of principal component initialization for MoRe, along with analysis of which components are most important for different tasks, would demonstrate the viability of this approach.

### Open Question 3
- Question: What is the theoretical relationship between MoRe's block-diagonal structure and the subspace components that are strengthened during fine-tuning?
- Basis in paper: [inferred] The paper shows that trained block-diagonal matrices approximate Gaussian distribution but doesn't explore how this relates to the specific subspaces being modified.
- Why unresolved: While the paper provides empirical evidence of weight distributions, it doesn't connect this to the theoretical understanding of which components of the original weight matrix are being adapted.
- What evidence would resolve it: Detailed analysis comparing the singular value decomposition of dense matrices versus MoRe-adapted matrices would reveal which subspaces are being strengthened and how this relates to task performance.

## Limitations
- Computational overhead from Monarch matrix operations, particularly permutation operations P1 and P2
- Fixed 4-block architecture may not be optimal for all model architectures or task types
- Scalability to extremely large models (beyond Llama 7B) remains unexplored
- Performance on specialized domains (code generation, multilingual tasks) not thoroughly evaluated

## Confidence
**High Confidence Claims:**
- MoRe achieves superior parameter efficiency compared to LoRA (10x-20x fewer parameters)
- The rectangular Monarch matrix structure enables more flexible architecture search
- MoRe maintains or improves performance across commonsense reasoning, math reasoning, and GLUE tasks

**Medium Confidence Claims:**
- MoRe is theoretically more expressive than LoRA
- The method requires minimal hyperparameter tuning
- MoRe provides computational efficiency advantages

**Low Confidence Claims:**
- MoRe will scale effectively to models larger than 7B parameters
- The fixed 4-block architecture is universally optimal
- Performance gains will transfer to specialized domains without modification

## Next Checks
1. **Scalability Validation**: Test MoRe on models significantly larger than Llama 7B (e.g., 30B+ parameters) to assess computational efficiency and parameter savings at scale. Measure both training throughput and inference latency compared to LoRA implementations.

2. **Cross-Domain Generalization**: Evaluate MoRe on specialized tasks including code generation benchmarks, multilingual understanding tasks, and domain-specific scientific text. This will test whether the theoretical expressiveness advantages translate to diverse real-world applications.

3. **Ablation on Architecture Choices**: Systematically vary the block number N and rank rblk across a wider range (not just the fixed 4-block configuration) to identify optimal architectures for different model sizes and task types. Compare both performance and computational overhead across these configurations.