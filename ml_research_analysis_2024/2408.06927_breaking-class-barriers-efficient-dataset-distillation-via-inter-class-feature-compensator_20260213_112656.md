---
ver: rpa2
title: 'Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class Feature
  Compensator'
arxiv_id: '2408.06927'
source_url: https://arxiv.org/abs/2408.06927
tags:
- dataset
- distillation
- infer
- synthetic
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses inefficiencies in dataset distillation arising
  from the class-specific synthesis paradigm, which leads to feature duplication and
  neglect of inter-class features. The proposed Inter-class Feature Compensator (INFER)
  introduces a Universal Feature Compensator (UFC) that integrates features across
  classes, allowing generation of multiple synthetic instances from a single input
  and significantly improving distillation budget efficiency.
---

# Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class Feature Compensator

## Quick Facts
- **arXiv ID**: 2408.06927
- **Source URL**: https://arxiv.org/abs/2408.06927
- **Reference count**: 37
- **Primary result**: Achieves 51.8% top-1 accuracy on ImageNet-1k with 4.04% dataset size using only 50 images per class

## Executive Summary
This paper addresses inefficiencies in dataset distillation arising from the class-specific synthesis paradigm, which leads to feature duplication and neglect of inter-class features. The proposed Inter-class Feature Compensator (INFER) introduces a Universal Feature Compensator (UFC) that integrates features across classes, allowing generation of multiple synthetic instances from a single input and significantly improving distillation budget efficiency. INFER also reduces soft label storage by 99.3% through static label interpolation while enabling MixUp augmentation. Experiments show state-of-the-art performance: on ImageNet-1k with IPC=50 and ResNet18, INFER achieves 51.8% top-1 accuracy with only 4.04% of the original dataset size, outperforming SRe2L by 34.5%.

## Method Summary
INFER uses Universal Feature Compensators (UFCs) to capture inter-class features that can be integrated with natural instances to generate multiple synthetic samples. Each base subset contains natural instances and corresponding UFCs, where synthetic instances are created through simple addition. The method employs static soft labels generated from multiple architectures and enables MixUp augmentation without dynamic label storage. UFCs are optimized with BN regularization, and the synthetic dataset is trained using standard distillation procedures with MixUp augmentation.

## Key Results
- Achieves 51.8% top-1 accuracy on ImageNet-1k with only 4.04% dataset size (IPC=50, ResNet18)
- Outperforms SRe2L by 34.5% on ImageNet-1k with same IPC
- Reduces soft label storage by 99.3% through static label interpolation
- Demonstrates superior efficiency across CIFAR, Tiny-ImageNet, and ImageNet benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Universal Feature Compensator (UFC) enables multiple synthetic instances from a single input, improving distillation budget efficiency.
- **Mechanism**: UFC integrates inter-class features through additive combination with natural data instances, allowing one UFC to generate multiple synthetic samples when paired with different natural instances.
- **Core assumption**: Inter-class features can be captured and represented in a universal form that remains beneficial when combined with any class-specific natural instance.
- **Evidence anchors**:
  - [abstract]: "INFER leverages a Universal Feature Compensator (UFC) to enhance feature integration across classes, enabling the generation of multiple additional synthetic instances from a single UFC input."
  - [section]: "Each base subset S k consists of a pair (P k, U k), where U k represents the set of UFCs, and P k âŠ‚ T contains natural instances to be integrated with UFCs... allowing the generation of multiple synthetic instances from a single input."
  - [corpus]: Weak. No direct mention of UFC or similar universal compensators in corpus neighbors.

### Mechanism 2
- **Claim**: Depreciating class-specific pre-assigned labels reduces feature duplication and enables better inter-class feature capture.
- **Mechanism**: By focusing UFC optimization on inter-class features rather than class-specific features, synthetic instances avoid capturing redundant intra-class features when ipc increases.
- **Core assumption**: Class-specific optimization leads to feature duplication as ipc increases, while inter-class feature optimization provides complementary information.
- **Evidence anchors**:
  - [abstract]: "Under this paradigm, synthetic data is optimized exclusively for a pre-assigned one-hot label, creating an implicit class barrier in feature condensation... leads to inefficient utilization of the distillation budget and oversight of inter-class feature distributions."
  - [section]: "Recent advancements in dataset distillation have enabled individual synthetic data instances to capture more features specific to a class... However, as ipc increases, additional synthetic data instances tend to capture distinctive yet duplicated intra-class features."
  - [corpus]: Weak. No direct discussion of feature duplication in relation to class-specific vs inter-class optimization in corpus neighbors.

### Mechanism 3
- **Claim**: Static soft labels enable MixUp augmentation without the massive storage overhead of dynamic soft labels.
- **Mechanism**: By ensuring that UFCs are optimized to maintain linear label interpolation properties when combined with natural instances, MixUp can use static labels that match the interpolation pattern of natural datasets.
- **Core assumption**: The linear interpolation property of MixUp labels can be preserved when applied to synthetic data generated from UFCs and natural instances.
- **Evidence anchors**:
  - [abstract]: "By allowing for the linear interpolation of labels similar to those in the original dataset, INFER meticulously optimizes the synthetic data and dramatically reduces the size of soft labels in the synthetic dataset to almost zero."
  - [section]: "We use P k, a subset of natural datasets, for integration with UFCs because natural instances inherently follow the linear interpolation of labels... UFCs, which are integrated with P k for optimization, also embody the characteristic of linear label interpolation."
  - [corpus]: Weak. No discussion of MixUp with static vs dynamic labels in corpus neighbors.

## Foundational Learning

- **Concept**: Dataset distillation and its optimization objectives
  - **Why needed here**: Understanding how INFER differs from traditional dataset distillation methods that optimize synthetic data for class-specific labels.
  - **Quick check question**: What is the key difference between INFER's optimization objective and traditional dataset distillation methods?

- **Concept**: MixUp data augmentation and its label interpolation properties
  - **Why needed here**: Understanding why INFER can use static labels for MixUp while previous methods required dynamic labels.
  - **Quick check question**: Why does MixUp normally require dynamic soft labels, and how does INFER enable static labels instead?

- **Concept**: Feature duplication and inter-class feature relationships in deep learning
  - **Why needed here**: Understanding the motivation behind INFER's approach to reducing feature duplication and capturing inter-class features.
  - **Quick check question**: How does feature duplication typically occur in dataset distillation as ipc increases?

## Architecture Onboarding

- **Component map**:
  Natural dataset T -> Universal Feature Compensators (UFCs) -> Base subsets S_k = (P_k, U_k) -> Integration function -> Synthetic instances -> Static soft labels -> MixUp augmentation -> Trained model

- **Critical path**:
  1. Initialize P_k with one random instance per class from T
  2. Initialize UFCs U_k as zero vectors
  3. Iteratively optimize UFCs to minimize loss with BN regularization
  4. Generate static soft labels by averaging predictions across architectures
  5. Train target model using synthetic dataset with MixUp augmentation

- **Design tradeoffs**:
  - UFC size vs. inter-class feature representation capacity
  - Number of architectures in ensemble vs. label quality and computational cost
  - Static vs. dynamic soft labels (storage vs. supervision quality)
  - Integration method (addition vs. more complex operations)

- **Failure signatures**:
  - Poor performance despite high ipc values (feature duplication not resolved)
  - Degradation when applying MixUp (linear interpolation property not maintained)
  - Inconsistent performance across different architectures (label generation issue)
  - Memory issues with large-scale datasets (UFC storage scaling problems)

- **First 3 experiments**:
  1. Compare feature duplication (cosine similarity) between SRe2L and INFER across different ipc values on CIFAR-10
  2. Evaluate performance impact of static vs. dynamic soft labels on ImageNet-1k with M=3 architectures
  3. Test ablation of UFCs by training with only natural instances (P_k) to measure inter-class feature contribution

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but the following questions arise from the limitations and scope of the work:

## Limitations
- The assumption that inter-class features can be effectively captured in a universal form requires validation across diverse datasets and model architectures.
- The static soft label generation process may compromise supervision quality compared to dynamic labels, though this tradeoff is not thoroughly evaluated.
- The method's performance scaling with increasing class counts (e.g., 1000+ classes) is not systematically explored.

## Confidence

- **High confidence**: The core mechanism of using UFCs to generate multiple synthetic instances from single inputs (Mechanism 1) is well-supported by the mathematical formulation and experimental results.
- **Medium confidence**: The claim about reducing feature duplication through inter-class feature optimization (Mechanism 2) is supported by the theoretical framework but lacks direct empirical validation of feature similarity comparisons.
- **Medium confidence**: The MixUp augmentation with static labels (Mechanism 3) is technically feasible but the claim of dramatic storage reduction requires more rigorous comparison with dynamic label approaches.

## Next Checks

1. **Feature duplication analysis**: Compute and compare the average cosine similarity between synthetic instances generated by INFER versus SRe2L across different IPC values on CIFAR-10 to directly validate the reduction in feature duplication claim.

2. **Static vs dynamic label ablation**: Conduct controlled experiments on ImageNet-1k comparing model performance when using static soft labels versus dynamic soft labels, measuring both accuracy and storage requirements to quantify the claimed 99.3% reduction.

3. **Architecture generalization test**: Evaluate INFER's performance when trained on ResNet18 but tested on VGG16 and MobileNetV2 on CIFAR-10 to assess the robustness of static label generation across different architectures.