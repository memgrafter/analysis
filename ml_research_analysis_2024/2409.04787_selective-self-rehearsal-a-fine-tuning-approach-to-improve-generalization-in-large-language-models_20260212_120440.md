---
ver: rpa2
title: 'Selective Self-Rehearsal: A Fine-Tuning Approach to Improve Generalization
  in Large Language Models'
arxiv_id: '2409.04787'
source_url: https://arxiv.org/abs/2409.04787
tags:
- base
- md2d
- unanswerable
- response
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the overfitting problem in large language
  model fine-tuning, where models become overly specialized to training data and lose
  generalization. The proposed Selective Self-Rehearsal (SSR) method fine-tunes models
  using their own correct predictions for some samples and gold responses for others,
  reducing overfitting while maintaining performance.
---

# Selective Self-Rehearsal: A Fine-Tuning Approach to Improve Generalization in Large Language Models

## Quick Facts
- arXiv ID: 2409.04787
- Source URL: https://arxiv.org/abs/2409.04787
- Reference count: 13
- Primary result: SSR reduces overfitting in LLM fine-tuning, achieving only 2% average performance drop on benchmarks vs 16.7% with standard SFT

## Executive Summary
This paper addresses the overfitting problem in large language model fine-tuning, where models become overly specialized to training data and lose generalization. The proposed Selective Self-Rehearsal (SSR) method fine-tunes models using their own correct predictions for some samples and gold responses for others, reducing overfitting while maintaining performance. Experiments on content-grounded question answering tasks show SSR achieves close to 2% average performance drop on benchmarks like MMLU and TruthfulQA, compared to 16.7% drop with standard supervised fine-tuning. The approach successfully retains base model capabilities while improving generalization to unseen datasets.

## Method Summary
SSR fine-tunes large language models by selectively using either the model's own correct predictions or gold responses as training targets. The method uses an LLM-as-a-judge to evaluate model predictions on training data, classifying responses as acceptable or unacceptable. For acceptable predictions, SSR uses the model's own output in the loss function; for unacceptable predictions, it uses the gold response. This selective approach prevents the model from being forced to adopt distributions far from its learned space, reducing catastrophic forgetting while still correcting mistakes. The method is implemented using LoRA fine-tuning with specific hyperparameters and evaluated on multiple benchmarks to measure both task performance and generalization retention.

## Key Results
- SSR achieves only 2% average performance drop on generalization benchmarks compared to 16.7% with standard SFT
- On GSM8k specifically, SSR shows 31% less performance degradation than standard SFT (6.4% vs 31.0% drop)
- The approach successfully retains base model capabilities while improving generalization to unseen datasets
- SSR requires additional computational overhead for judging but provides significant performance benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using the model's own correct predictions during fine-tuning helps maintain the base model's learned distribution and prevents catastrophic forgetting
- Mechanism: When the model generates a response that is as good as the gold answer (judged by LLM-as-a-judge), using this prediction in the loss function reinforces the model's existing behavior patterns rather than forcing it to adopt a new distribution that may be far from its learned space
- Core assumption: The model's own predictions are often valid responses that preserve semantic meaning, even if they differ from gold answers
- Evidence anchors: "By utilizing the model's correct responses, SSR reduces model specialization during the fine-tuning stage"; "training exclusively on gold responses can lead to a drift from the original distribution, compromising the model's generality"
- Break condition: If the model's predictions are systematically worse than gold answers, or if the judge system incorrectly labels poor responses as acceptable

### Mechanism 2
- Claim: Selective use of gold responses only for instances where the model fails prevents overfitting to the specific characteristics of the training data
- Mechanism: The model is trained on gold responses only for samples where its prediction was deemed unacceptable by the judge, creating a curriculum that focuses on correcting mistakes while preserving successful behaviors
- Core assumption: The judge system (LLM-as-a-judge) can reliably distinguish between acceptable and unacceptable model responses
- Evidence anchors: "SSR fine-tunes the model on its own generated output for cases where it behaves desirably and on gold output for the remaining data"; "SSR results in close to 2% drop on average, indicating better generalization capabilities compared to standard SFT"
- Break condition: If the judge system has high false positive or false negative rates, or if the distribution of mistakes is systematically different from the test distribution

### Mechanism 3
- Claim: Reducing the number of training samples that force distributional shift helps maintain base model capabilities across diverse benchmarks
- Mechanism: By using the model's own predictions for a subset of training data, SSR reduces the number of gradient updates that push the model away from its original distribution, as evidenced by better retention of capabilities on MMLU, TruthfulQA, GSM8k, and Hellaswag
- Core assumption: The base model's original distribution contains valuable capabilities that should be preserved during task-specific fine-tuning
- Evidence anchors: "SSR results in an average drop of only 2.3% and 2.0% when trained on MD2D and NQ, respectively, with most of the drop (5.8 and 6.4) coming from GSM8k"; "In contrast, the corresponding drop in SFT on GSM8k is 31.0% and 23.9%"
- Break condition: If the task requires fundamental changes to the model's capabilities that cannot be achieved through incremental adjustment

## Foundational Learning

- Concept: Supervised fine-tuning (SFT) basics
  - Why needed here: Understanding how standard SFT works is essential to grasp why SSR is an improvement - SFT uses gold answers for all training samples, which can cause distributional drift
  - Quick check question: In standard SFT, what loss function is used and what inputs does it compare?

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: SSR specifically addresses this problem by preserving model capabilities learned during pre-training and instruction tuning
  - Quick check question: What happens to a model's performance on previously learned tasks when fine-tuned on new data without preservation techniques?

- Concept: Distributional alignment and semantic equivalence
  - Why needed here: The core insight is that multiple responses can be semantically equivalent even if they have different log probabilities, and forcing gold responses can push the model away from its learned distribution
  - Quick check question: Why might a model assign very different log probabilities to two responses that convey the same semantic meaning?

## Architecture Onboarding

- Component map: Base model (Mistral-7B-Instruct-v2) -> Judge system (Mixtral-8x7B or GPT-4) -> SSR training pipeline -> Evaluation benchmarks (MMLU, TruthfulQA, GSM8k, Hellaswag)
- Critical path: Model inference on training data -> Judge evaluation -> Dataset split into R (acceptable predictions) and G (unacceptable predictions) -> SSR training with mixed loss -> Evaluation on multiple benchmarks
- Design tradeoffs: SSR requires additional inference cost for judging all training samples but provides better generalization; judge reliability is critical but can be computationally expensive
- Failure signatures: Poor performance on generalization benchmarks suggests judge system is not working correctly or base model predictions are systematically worse than gold; significant performance drop on target task suggests too few gold responses are being used
- First 3 experiments:
  1. Run SSR on a small dataset with manual verification of judge accuracy to establish baseline performance
  2. Compare SSR performance on target task vs SFT while measuring generalization on a held-out benchmark
  3. Vary the judge system threshold for "acceptable" predictions to find optimal balance between retention and task performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations and scope of the research, several important questions emerge that warrant further investigation:

## Limitations

- Limited evaluation scope focused on content-grounded QA tasks without testing on open-ended generation tasks
- Heavy reliance on LLM-as-a-judge system quality, which introduces potential sources of error
- Significant computational overhead from requiring inference through the judge system for every training sample
- Lack of comprehensive analysis of judge failure modes or systematic error rates

## Confidence

**High Confidence Claims:**
- SSR outperforms standard SFT on the specific datasets and benchmarks tested (MMLU, TruthfulQA, GSM8k, Hellaswag)
- The computational overhead of the judge system is significant but manageable for the model scales tested
- The approach demonstrates reduced overfitting compared to standard fine-tuning methods

**Medium Confidence Claims:**
- SSR's effectiveness in preventing catastrophic forgetting is supported but could benefit from more extensive testing across diverse tasks
- The semantic equivalence between model predictions and gold responses is reasonable but relies on the judge's accuracy
- The approach generalizes beyond content-grounded QA to other task types (based on benchmark performance)

**Low Confidence Claims:**
- SSR will work equally well on models significantly larger or smaller than Mistral-7B
- The approach is robust to different judge system architectures and prompt designs
- SSR provides similar benefits for open-ended generation tasks as for structured QA tasks

## Next Checks

1. **Judge System Reliability Analysis**: Conduct a systematic evaluation of the LLM-as-a-judge's error rates across different response types and complexity levels, including human validation on a random sample of judged responses to establish false positive and false negative rates.

2. **Cross-Domain Generalization Test**: Apply SSR to a completely different task domain (e.g., code generation, creative writing) using the same Mistral-7B base model and compare performance degradation against standard SFT to assess generalizability beyond QA tasks.

3. **Ablation Study on Judge Threshold**: Systematically vary the judge's acceptance threshold for "acceptable" responses and measure the resulting trade-off between target task performance and generalization retention to identify optimal hyperparameter settings.