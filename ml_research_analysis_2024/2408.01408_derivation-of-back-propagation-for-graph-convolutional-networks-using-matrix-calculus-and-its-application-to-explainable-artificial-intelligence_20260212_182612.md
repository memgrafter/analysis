---
ver: rpa2
title: Derivation of Back-propagation for Graph Convolutional Networks using Matrix
  Calculus and its Application to Explainable Artificial Intelligence
arxiv_id: '2408.01408'
source_url: https://arxiv.org/abs/2408.01408
tags:
- matrix
- node
- function
- where
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work provides a detailed matrix calculus derivation of the
  back-propagation algorithm for graph convolutional networks (GCNs), supporting arbitrary
  activation functions and network depths. The study validates the derived closed-form
  gradient expressions by comparing them to reverse-mode automatic differentiation
  across two tasks: node classification on Zachary''s Karate Club and link prediction
  on a drug-drug interaction network.'
---

# Derivation of Back-propagation for Graph Convolutional Networks using Matrix Calculus and its Application to Explainable Artificial Intelligence

## Quick Facts
- arXiv ID: 2408.01408
- Source URL: https://arxiv.org/abs/2408.01408
- Reference count: 40
- Primary result: Median sum of squared errors between 10^-18 and 10^-14 between analytical and automatic differentiation gradients

## Executive Summary
This paper provides a rigorous matrix calculus derivation of back-propagation for graph convolutional networks (GCNs) with arbitrary activation functions and network depths. The authors validate their closed-form gradient expressions by comparing them to reverse-mode automatic differentiation across two tasks: node classification on Zachary's Karate Club and link prediction on a drug-drug interaction network. The derived expressions enable sensitivity analysis for explainable AI by quantifying how input features influence model outputs.

## Method Summary
The authors derive GCN back-propagation using matrix calculus theorems, particularly focusing on derivatives of element-wise activations using Kronecker and Hadamard products. They validate their analytical gradients by comparing weight updates to reverse-mode automatic differentiation across 1060 training runs, achieving median sum of squared errors between 10^-18 and 10^-14. The framework also enables sensitivity analysis by computing ∂L/∂H0 and ∂Ŷ/∂H0 to identify influential features.

## Key Results
- Analytical gradients match reverse-mode automatic differentiation with median SSE of 10^-18 to 10^-14
- Sensitivity analysis shows feature influence decreases during training for Karate Club node classification
- Framework successfully extends to both node classification and link prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
The closed-form gradient expressions for GCNs derived using matrix calculus are mathematically equivalent to reverse-mode automatic differentiation gradients. The derivation uses Kronecker and Hadamard products to express weight gradients analytically, avoiding iterative approximation errors inherent in automatic differentiation. Matrix calculus operations are correctly applied across all network layers and graph structures.

### Mechanism 2
Sensitivity analysis using derived gradients enables explainable AI by quantifying input feature influence on GCN predictions. The same matrix calculus framework computes ∂L/∂H0 and ∂Ŷ/∂H0, revealing which features most affect loss and output. Gradient magnitudes directly correlate with feature importance in the learned model.

### Mechanism 3
The derivation framework generalizes to arbitrary activation functions and network depths while maintaining mathematical tractability. Theorem 2.3 provides a unified formula for ∂Σ(F(W))/∂W that works for any element-wise activation Σ, enabling extension from 3-layer to d-layer GCNs. Element-wise activation derivatives exist and are well-behaved for all considered functions.

## Foundational Learning

- Concept: Matrix calculus and Kronecker/Hadamard product operations
  - Why needed here: Essential for expressing GCN gradients in closed form without iterative computation
  - Quick check question: Can you derive ∂(AXW)/∂W for matrices A, X, W using matrix calculus rules?

- Concept: Chain rule application in multivariate matrix functions
  - Why needed here: Enables backprop through multiple GCN layers while maintaining matrix notation
  - Quick check question: How does the chain rule extend when computing ∂L/∂W3 through intermediate matrices H2 and H1?

- Concept: Graph convolutional layer operations and their derivatives
  - Why needed here: The core GCN operation ÂHWD requires specialized derivative treatment due to graph structure
  - Quick check question: What is the derivative of the normalized adjacency matrix Â with respect to the adjacency matrix A?

## Architecture Onboarding

- Component map: GCN layers → Activation functions → Loss computation → Gradient derivation → Weight update
- Critical path: Matrix calculus derivation → Validation against automatic differentiation → Sensitivity analysis implementation
- Design tradeoffs: Analytical gradients provide exactness but require complex matrix operations; automatic differentiation is simpler but approximate
- Failure signatures: NaN values in loss calculation, gradient divergence during training, sensitivity maps showing unexpected patterns
- First 3 experiments:
  1. Verify gradient equivalence on simple 2-layer GCN with identity activation
  2. Test sensitivity analysis on Karate Club with known feature importance
  3. Extend to link prediction on small synthetic graph with ground truth gradients

## Open Questions the Paper Calls Out

### Open Question 1
How does the computational complexity of the matrix-based back-propagation method scale with increasing graph size and number of layers compared to reverse-mode automatic differentiation? The paper states that "our method incurs a significantly higher computational cost compared to reverse mode AD" but does not provide detailed complexity analysis or runtime benchmarks.

### Open Question 2
Can the matrix-based back-propagation derivation be extended to handle non-linear graph convolutions beyond the standard GCN layer? The derivation accommodates arbitrary element-wise activation functions but is specific to GCN layers and does not explore more advanced graph convolution variants.

### Open Question 3
How does the sensitivity analysis methodology perform on larger, real-world graphs with noisy or incomplete node features? The paper demonstrates sensitivity analysis on the Karate Club and a 100-node DDI network, but these are relatively small and clean datasets.

## Limitations
- Computational complexity scales poorly with network depth and feature dimension due to matrix calculus operations
- Derivation assumes well-behaved activation function derivatives; functions with discontinuities could break the framework
- Validation limited to two specific tasks (Karate Club and drug-drug interaction networks)

## Confidence
- Gradient Derivation: High - Mathematical framework and validation results strongly support correctness
- Sensitivity Analysis: Medium - Theoretically sound but practical utility depends on gradient magnitude interpretation
- Generalizability: Medium - Framework extends to arbitrary activations and depths, but untested with non-standard functions or very deep networks

## Next Checks
1. Stress test with non-standard activation functions (Swish, Softplus) to verify derivative properties
2. Benchmark computational overhead of analytical gradients vs. automatic differentiation for GCNs with 10+ layers
3. Validate sensitivity analysis on graph datasets from different domains (citation networks, biological graphs)