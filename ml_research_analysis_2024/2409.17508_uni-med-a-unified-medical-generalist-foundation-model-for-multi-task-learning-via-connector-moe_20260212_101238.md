---
ver: rpa2
title: 'Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning
  Via Connector-MoE'
arxiv_id: '2409.17508'
source_url: https://arxiv.org/abs/2409.17508
tags:
- task
- medical
- arxiv
- image
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Uni-Med introduces a unified medical generalist foundation model
  that addresses the multi-task interference problem in medical MLLMs by proposing
  a Connector-MoE module. This approach leverages a mixture of projection experts
  at the connector level to adaptively align visual and language embedding spaces
  for different tasks.
---

# Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE

## Quick Facts
- arXiv ID: 2409.17508
- Source URL: https://arxiv.org/abs/2409.17508
- Authors: Xun Zhu; Ying Hu; Fanbin Mo; Miao Li; Ji Wu
- Reference count: 33
- Key outcome: Achieves up to 8% performance gains on six medical tasks (QA, VQA, RG, REC, REG, CLS) using Connector-MoE module

## Executive Summary
Uni-Med introduces a unified medical generalist foundation model that addresses the multi-task interference problem in medical multi-modal learning. The key innovation is the Connector-MoE (CMoE) module, which uses a mixture of projection experts to adaptively align visual and language embedding spaces for different tasks. This approach enables effective multi-task learning across six medical tasks using 12 diverse datasets while avoiding catastrophic forgetting and performance degradation that typically occurs in unified models.

## Method Summary
Uni-Med leverages a pre-trained visual encoder (ViT-G/14), a Connector-MoE module with 5 projection experts, and a large language model (LLaMA2-Chat 7B) fine-tuned with LoRA. The CMoE module dynamically routes tokens through different projection experts based on task requirements, enabling adaptive visual-language alignment. The model is trained end-to-end on a single A800 GPU for approximately 10 hours using AdamW optimizer with learning rate 1e-6 decaying to 1e-7 over 100k iterations. Training uses a batch size of 4 and incorporates LoRA adapters with rank 8 to reduce computational overhead.

## Key Results
- Achieves up to 8% performance gains across six medical tasks compared to baseline approaches
- Demonstrates competitive or superior performance compared to state-of-the-art medical MLLMs
- Successfully handles diverse medical modalities including CT, MRI, X-ray, and pathology images
- Shows effective multi-task learning without catastrophic forgetting across different task types

## Why This Works (Mechanism)
The CMoE module addresses the fundamental challenge of multi-task interference by allowing different tasks to have task-specific visual-language alignment spaces. Rather than forcing a single universal projection that may be suboptimal for all tasks, the mixture of experts approach enables each task to learn the most appropriate alignment strategy. The soft router with combined token-task information allows the model to dynamically adapt its processing based on both the content and the task requirements, leading to more effective cross-modal reasoning in the medical domain.

## Foundational Learning
- **Multi-modal foundation models**: Why needed - Medical tasks require integration of visual and textual information; Quick check - Model can process both image and text inputs simultaneously
- **Mixture of Experts (MoE)**: Why needed - Different tasks require different embedding alignments; Quick check - Router assigns different experts to different task types
- **LoRA fine-tuning**: Why needed - Enables efficient adaptation of large models without full fine-tuning; Quick check - Model maintains performance with reduced parameters
- **Multi-task learning interference**: Why needed - Unified models often suffer from task conflicts; Quick check - Performance on each task remains stable when adding new tasks
- **Medical image modalities**: Why needed - Healthcare uses diverse imaging techniques; Quick check - Model handles CT, MRI, X-ray, and pathology images
- **Visual-language alignment**: Why needed - Medical reasoning requires connecting image features to clinical concepts; Quick check - Model generates clinically relevant descriptions

## Architecture Onboarding
**Component Map**: Input Image -> ViT-G/14 Visual Encoder -> CMoE (5 Experts + Router) -> LLM (LLaMA2-7B with LoRA) -> Output

**Critical Path**: The CMoE module sits between the visual encoder and LLM, acting as the adaptive interface that routes visual features to the language model based on task requirements. This is where the model's task-specific reasoning capability is implemented.

**Design Tradeoffs**: Uses 5 projection experts as a balance between expressivity and computational efficiency. Fixed router vs. learned router trade-off favors soft routing with combined token-task information for better adaptability.

**Failure Signatures**: Poor task-specific performance indicates router isn't selecting appropriate experts; degraded overall performance suggests interference between tasks; training instability may indicate LoRA rank is too low or learning rate is inappropriate.

**3 First Experiments**:
1. Verify CMoE routing by examining expert selection patterns across different task types
2. Test task-specific performance degradation when removing CMoE and using direct visual-to-text projection
3. Validate LoRA adapter effectiveness by comparing full fine-tuning vs. LoRA fine-tuning on a subset of tasks

## Open Questions the Paper Calls Out
**Open Question 1**: How does Uni-Med's performance scale with increasing numbers of medical tasks and modalities beyond the 6 tasks and 12 datasets currently evaluated? The current evaluation only covers a limited number of tasks and datasets, and scaling to hundreds or thousands of medical tasks/modalities could reveal fundamental limits of the CMoE approach.

**Open Question 2**: What is the optimal routing strategy for the CMoE module when handling extremely heterogeneous medical tasks with vastly different input/output formats and requirements? The current router design may not optimally handle tasks with fundamentally different processing needs, such as segmentation vs. classification vs. report generation.

**Open Question 3**: How does Uni-Med's performance compare when using larger LLM backbones (e.g., LLaMA2-13B, LLaMA2-70B) or alternative vision backbones for medical image understanding? The choice of 7B parameter model and specific vision backbone may limit Uni-Med's capabilities, and larger models or medical-specific vision backbones might provide significant performance improvements.

## Limitations
- Performance evaluation limited to English-language datasets, potentially limiting generalizability to other healthcare systems
- Reliance on a single GPU for training may constrain scalability to larger models or datasets
- Fixed number of projection experts (5) without comprehensive ablation studies on optimal configuration
- Claims of competitive performance vs. state-of-the-art require direct standardized benchmarking across all models

## Confidence
- **High Confidence**: Technical feasibility of CMoE approach and integration with foundation models is well-established with clear training procedures
- **Medium Confidence**: Performance improvements over baselines are statistically significant but may vary with implementation details
- **Low Confidence**: Claims of superior performance compared to all state-of-the-art medical MLLMs are difficult to verify without direct comparison on standardized benchmarks

## Next Checks
1. **Ablation study on projection experts**: Conduct experiments varying the number of projection experts in CMoE (e.g., 3, 5, 7) to determine optimal configuration and validate performance robustness
2. **Cross-domain generalization test**: Evaluate Uni-Med on non-medical datasets to assess whether medical specialization improves or hinders performance on general-purpose tasks
3. **Efficiency analysis**: Measure inference latency and memory usage compared to single-task models and other generalist approaches to quantify practical trade-offs in clinical settings