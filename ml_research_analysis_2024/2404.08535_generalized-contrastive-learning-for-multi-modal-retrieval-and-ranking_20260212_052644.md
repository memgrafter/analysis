---
ver: rpa2
title: Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking
arxiv_id: '2404.08535'
source_url: https://arxiv.org/abs/2404.08535
tags:
- retrieval
- learning
- ranking
- contrastive
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of traditional contrastive
  learning methods in retrieval tasks, which struggle to incorporate ranking information
  beyond binary relevance. The authors propose Generalized Contrastive Learning (GCL),
  a novel framework that extends contrastive learning to handle continuous ranking
  scores by applying a score-to-weight function to the loss.
---

# Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking

## Quick Facts
- arXiv ID: 2404.08535
- Source URL: https://arxiv.org/abs/2404.08535
- Authors: Tianyu Zhu; Myong Chol Jung; Jesse Clark
- Reference count: 40
- Key outcome: GCL achieves 29.3% increase in NDCG@10 and 46.9% increase in ERR on MarqoGS-10M dataset compared to fine-tuned CLIP baseline

## Executive Summary
This paper addresses the limitations of traditional contrastive learning methods in retrieval tasks, which struggle to incorporate ranking information beyond binary relevance. The authors propose Generalized Contrastive Learning (GCL), a novel framework that extends contrastive learning to handle continuous ranking scores by applying a score-to-weight function to the loss. GCL also supports multi-field learning, allowing both queries and documents to be represented by multiple text and image fields. To evaluate GCL, the authors curate MarqoGS-10M, a large-scale multi-modal dataset of 10 million query-document pairs with ranking scores derived from Google Shopping. GCL is evaluated against several public contrastive learning methods and shows significant improvements.

## Method Summary
GCL extends traditional contrastive learning by incorporating continuous ranking scores into the loss function through a score-to-weight transformation. The framework also supports multi-field representations where document fields (text and image) are encoded separately and combined using weighted averaging. The method uses weighted contrastive loss where higher ranking scores receive higher weights, encouraging embeddings of highly relevant documents to be closer to queries in the embedding space. The evaluation employs a quadruple-split framework with training, novel queries, novel documents, and zero-shot sets to assess real-world performance across different retrieval scenarios.

## Key Results
- GCL achieves 29.3% increase in NDCG@10 and 46.9% increase in ERR for in-domain evaluations on MarqoGS-10M compared to fine-tuned CLIP baseline
- For cold-start evaluations, GCL exhibits relative improvements of 6.0 - 10.0% in NDCG@10, 3.5 - 8.1% in ERR, and 5.7 - 8.6% in RBP
- Offline evaluation on proprietary user interaction data shows an 11.2% gain for in-domain evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GCL improves retrieval ranking by incorporating continuous relevance scores into the contrastive loss, rather than relying solely on binary relevance.
- Mechanism: The score-to-weight function transforms ground-truth ranking scores into weights applied to the loss. Higher weights penalize low similarity scores more strongly, encouraging embeddings of highly relevant documents to be closer to queries in the embedding space.
- Core assumption: The ranking scores derived from listing positions accurately reflect user preferences and relevance.
- Evidence anchors:
  - [abstract] "GCL encodes both relevance and ranking information into a unified embedding space by applying ranking scores to the loss function."
  - [section] "The higher score ùë†ùëñ of a document ùëëùëñ, the greater its corresponding weight ùë§ùëñ will be... The weights are converted from ground-truth ranking score by a score-to-weight function."
- Break condition: If ranking scores do not correlate with actual user relevance (e.g., if listing positions are manipulated or biased), the weighting mechanism will misguide the model.

### Mechanism 2
- Claim: Multi-field learning enhances document representation by combining information from multiple modalities (text and image) into a single embedding.
- Mechanism: Document fields (e.g., title and image) are encoded separately and then combined using a weighted average. This produces richer embeddings that capture more aspects of the document.
- Core assumption: Different fields provide complementary information that improves retrieval when combined.
- Evidence anchors:
  - [abstract] "We expand the conventional single-field representation of queries and documents to encompass multiple fields."
  - [section] "Our framework extends contrastive learning to multi-field, allowing both queries and documents to be represented by multiple text and image fields."
- Break condition: If certain fields are noisy or irrelevant, their inclusion may degrade performance rather than improve it.

### Mechanism 3
- Claim: The quadruple split dataset evaluation method provides more realistic assessment of model performance across different retrieval scenarios.
- Mechanism: By splitting data into training, novel queries, novel documents, and zero-shot sets, the evaluation captures model behavior in real-world conditions where new queries or documents are encountered.
- Core assumption: Real-world search systems encounter all four types of retrieval scenarios, and evaluating on all provides meaningful insights.
- Evidence anchors:
  - [abstract] "We introduce an innovative split of the dataset that facilitates comprehensive evaluation insights."
  - [section] "This approach results in four sets: training, novel query, novel document, and zero-shot... The quadruple-split framework... mirrors the varied challenges faced by real-world search systems."
- Break condition: If the novel queries/documents are not sufficiently distinct from training data, the evaluation may not accurately reflect real-world performance.

## Foundational Learning

- Concept: Contrastive learning with binary relevance
  - Why needed here: Understanding the baseline approach that GCL improves upon is essential for grasping the novelty and motivation of the work.
  - Quick check question: How does traditional contrastive learning determine positive and negative pairs in retrieval tasks?

- Concept: Embedding space optimization
  - Why needed here: GCL's effectiveness depends on understanding how similarity scores in the embedding space relate to relevance and ranking.
  - Quick check question: What is the relationship between dot product similarity in the embedding space and document relevance?

- Concept: Multi-modal representation learning
  - Why needed here: GCL combines text and image fields, so understanding how to effectively merge different modalities is crucial.
  - Quick check question: What are the challenges in combining text and image embeddings into a single representation?

## Architecture Onboarding

- Component map: Query encoder (ùê∏ùëû) -> Document encoder (ùê∏ùëë) -> Score-to-weight function -> Weighted contrastive loss -> Multi-field averaging

- Critical path: Query/Document encoding ‚Üí Field embedding normalization ‚Üí Weighted average ‚Üí Dot product similarity ‚Üí Weighted loss computation ‚Üí Backpropagation

- Design tradeoffs:
  - Single-field vs multi-field: Multi-field provides richer representations but increases complexity and training time
  - Score-to-weight function choice: Different functions prioritize different ranking ranges; affects which documents receive more training emphasis
  - Batch size vs memory: Larger batches provide more negative samples but require more memory

- Failure signatures:
  - Poor in-domain performance but good cold-start: Model may be overfitting to training distribution
  - Good in-domain but poor cold-start: Model may not generalize well to unseen queries/documents
  - Degraded performance with multi-field: Fields may be conflicting or one field may dominate inappropriately

- First 3 experiments:
  1. Implement GCL with constant score-to-weight function (baseline) and compare against standard contrastive learning on a small subset of MarqoGS-10M
  2. Test different score-to-weight functions (linear, inverse, piecewise) to identify which best improves NDCG@10
  3. Implement multi-field GCL with title and image fields, experimenting with different weighting schemes (ùõæùëÖ1 values) to find optimal field combination

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to product search domain: Evaluation is confined to one type of retrieval task, making generalization to other domains uncertain
- Score-to-weight function selection: The paper provides limited guidance on how to select optimal score-to-weight functions for different ranking score distributions
- Single dataset evaluation: Performance is only validated on MarqoGS-10M and proprietary user interaction data, limiting external validity

## Confidence

- High confidence in the core mechanism: The weighted contrastive loss framework is theoretically sound and the empirical results on the MarqoGS-10M dataset demonstrate consistent improvements across multiple evaluation metrics.
- Medium confidence in the ranking score interpretation: While the use of listing positions as ranking scores is reasonable, the assumption that these directly correlate with user relevance preferences needs further validation.
- Low confidence in generalization: The evaluation is limited to a single proprietary dataset and one type of retrieval task (product search), making it unclear how well GCL would perform in other domains or with different ranking score distributions.

## Next Checks

1. Validate the score-to-weight function selection on datasets with different ranking score distributions to determine if the inverse function is universally optimal.
2. Implement an ablation study comparing GCL with and without multi-field representations across different field combinations to quantify the contribution of each modality.
3. Conduct cross-domain evaluation by applying GCL to datasets from different retrieval tasks (e.g., web search, image retrieval) to assess generalization beyond product search.