---
ver: rpa2
title: 'MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data'
arxiv_id: '2403.11207'
source_url: https://arxiv.org/abs/2403.11207
tags:
- image
- brain
- mindeye2
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MindEye2, a shared-subject model that enables
  high-quality fMRI-to-image reconstruction using only 1 hour of training data. By
  pretraining across 7 subjects and fine-tuning on a new subject, the model achieves
  state-of-the-art performance across reconstruction and retrieval metrics.
---

# MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data

## Quick Facts
- arXiv ID: 2403.11207
- Source URL: https://arxiv.org/abs/2403.11207
- Authors: Paul S. Scotti, Mihir Tripathy, Cesar Kadir Torrico Villanueva, Reese Kneeland, Tong Chen, Ashutosh Narang, Charan Santhirasegaran, Jonathan Xu, Thomas Naselaris, Kenneth A. Princeton, Tanishq Mathew Abraham
- Reference count: 40
- Primary result: Achieves state-of-the-art fMRI-to-image reconstruction using only 1 hour of subject-specific training data

## Executive Summary
MindEye2 introduces a shared-subject model that enables high-quality fMRI-to-image reconstruction using only 1 hour of training data. By pretraining across 7 subjects and fine-tuning on a new subject, the model achieves state-of-the-art performance across reconstruction and retrieval metrics. A novel functional alignment procedure linearly maps brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. Fine-tuning Stable Diffusion XL allows reconstruction from these embeddings. MindEye2 demonstrates accurate reconstructions from a single MRI visit, improving practical utility over previous methods requiring extensive training data.

## Method Summary
MindEye2 uses a shared-subject approach where fMRI voxel patterns are first linearly mapped to a 4096-dimensional shared latent space using subject-specific ridge regression. This shared latent space is then processed by a shared MLP backbone with 4 residual blocks, followed by a diffusion prior that maps to OpenCLIP ViT-bigG/14 image embeddings. The model is trained with three complementary submodules: a retrieval submodule using contrastive learning, a low-level submodule for VAE embeddings, and the main diffusion prior. The model is pretrained on 7 subjects from the Natural Scenes Dataset, then fine-tuned on a held-out 8th subject using only 1 hour of their data before generating reconstructions via fine-tuned Stable Diffusion XL.

## Key Results
- Achieves state-of-the-art performance across all reconstruction metrics (PixCorr, SSIM, AlexNet, Inception, CLIP, EfficientNet, SwAV) compared to single-subject models
- Outperforms baseline models trained with 40 hours of data when using only 1 hour of subject-specific training
- Demonstrates robust cross-subject generalization with consistent performance improvements across all 8 NSD subjects
- Ablation studies show each component (diffusion prior, retrieval submodule, low-level submodule) contributes complementary benefits to overall performance

## Why This Works (Mechanism)

### Mechanism 1
The shared-subject functional alignment enables effective pretraining across subjects without requiring shared stimuli. Each subject's fMRI voxels are linearly mapped to a 4096-dimensional shared-subject latent space via ridge regression, allowing the same downstream model to process all subjects' data. The core assumption is that the linear mapping can sufficiently align functionally similar but anatomically different brain regions across subjects.

### Mechanism 2
The diffusion prior effectively aligns fMRI latents to CLIP image space while preserving reconstruction quality. A diffusion model trained from scratch maps fMRI latents to OpenCLIP ViT-bigG/14 image space, enabling reconstruction via a fine-tuned SDXL unCLIP model. The core assumption is that the diffusion prior can learn to translate between the fMRI latent space and the CLIP image embedding space without losing semantic information.

### Mechanism 3
The multi-objective training with multiple submodules provides complementary information for improved reconstructions. The model is trained with three losses: diffusion prior (MSE), retrieval submodule (contrastive), and low-level submodule (VAE + auxiliary loss), allowing it to optimize for both semantic and low-level image features. The core assumption is that training with multiple objectives prevents the model from overfitting to any single aspect of the reconstruction task.

## Foundational Learning

- Concept: fMRI preprocessing and normalization
  - Why needed here: The model requires standardized input across subjects and sessions to learn meaningful mappings.
  - Quick check question: What preprocessing steps are applied to fMRI data before feeding it to the model?

- Concept: Contrastive learning and embedding spaces
  - Why needed here: The model maps brain activity to CLIP embeddings, requiring understanding of how contrastive models create semantic representations.
  - Quick check question: How does CLIP create shared embeddings for images and text, and why is this useful for reconstruction?

- Concept: Diffusion models and latent space manipulation
  - Why needed here: The diffusion prior and unCLIP model rely on diffusion principles to generate images from embeddings.
  - Quick check question: What is the role of the diffusion prior in bridging fMRI latents to image embeddings?

## Architecture Onboarding

- Component map: fMRI voxels (13,000-18,000) → subject-specific linear layer → MLP backbone (4 residual blocks) → diffusion prior → OpenCLIP ViT-bigG/14 embeddings → SDXL unCLIP → base SDXL refinement

- Critical path: fMRI → subject-specific linear → MLP backbone → diffusion prior → SDXL unCLIP → base SDXL refinement

- Design tradeoffs:
  - Linear vs. MLP for initial mapping: Linear is simpler and less prone to overfitting with limited data
  - OpenCLIP ViT-bigG/14 vs. CLIP ViT-L/14: BigG has larger capacity but may be less effective for nearest neighbor retrieval
  - Single vs. multiple objectives: Multiple objectives provide complementary information but increase training complexity

- Failure signatures:
  - Poor reconstruction quality: Check if the diffusion prior is properly aligning fMRI and CLIP spaces
  - Unstable training: Verify the balance between the three loss terms
  - Poor cross-subject generalization: Examine the effectiveness of the subject-specific linear mapping

- First 3 experiments:
  1. Train MindEye2 on subject 1 only (no pretraining) to establish baseline performance
  2. Train MindEye2 with pretraining on other subjects but evaluate on subject 1 held-out data
  3. Ablate each submodule (remove retrieval, remove low-level, remove both) to measure contribution to overall performance

## Open Questions the Paper Calls Out
The paper notes that MindEye2 was only demonstrated on COCO natural scenes and suggests additional data/specialized models would be needed for other distributions, leaving open questions about generalization to abstract art or text images.

## Limitations
- The linear mapping approach for functional alignment lacks comparison with more sophisticated nonlinear alignment methods
- The model's performance with truly minimal data (less than 1 hour) remains untested, though the paper claims 1-hour sufficiency
- The approach is only validated on natural scenes from COCO, with unclear generalizability to other image domains

## Confidence

**High Confidence**: The core claim that MindEye2 achieves state-of-the-art performance across reconstruction and retrieval metrics is well-supported by quantitative results on the NSD benchmark.

**Medium Confidence**: The claim that functional alignment via linear mapping is sufficient for cross-subject generalization is supported by results but lacks comparison with alternative alignment strategies.

**Low Confidence**: The assertion that MindEye2's approach will generalize to clinical or real-world scenarios with truly limited data is not directly tested.

## Next Checks
1. Test MindEye2 on subjects with truly minimal scanning time (e.g., 15-30 minutes) to verify the 1-hour claim holds in more constrained settings.
2. Compare the linear functional alignment approach with nonlinear alternatives (e.g., deep learning-based alignment) to determine if more sophisticated methods provide additional benefits.
3. Evaluate MindEye2 on different fMRI datasets with varying acquisition protocols and subject populations to assess robustness and generalizability beyond the NSD benchmark.