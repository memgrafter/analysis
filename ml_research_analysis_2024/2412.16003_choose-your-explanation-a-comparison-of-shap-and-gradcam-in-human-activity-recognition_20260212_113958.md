---
ver: rpa2
title: 'Choose Your Explanation: A Comparison of SHAP and GradCAM in Human Activity
  Recognition'
arxiv_id: '2412.16003'
source_url: https://arxiv.org/abs/2412.16003
tags:
- grad-cam
- shap
- body
- explanation
- right
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic comparison of SHAP and Grad-CAM
  explainability methods for human activity recognition using graph convolutional
  networks on skeleton data. The authors evaluate these methods on two real-world
  datasets, including a healthcare-critical cerebral palsy case.
---

# Choose Your Explanation: A Comparison of SHAP and GradCAM in Human Activity Recognition

## Quick Facts
- arXiv ID: 2412.16003
- Source URL: https://arxiv.org/abs/2412.16003
- Reference count: 40
- Primary result: SHAP provides detailed feature-level attribution while Grad-CAM offers faster, spatially-oriented explanations for HAR using GCNs

## Executive Summary
This paper systematically compares SHAP and Grad-CAM explainability methods for human activity recognition using graph convolutional networks on skeleton data. The authors evaluate these methods on two real-world datasets, including a healthcare-critical cerebral palsy case. SHAP provides detailed feature-level attribution while Grad-CAM offers faster, spatially-oriented explanations. The study employs perturbation experiments and feature ranking to compare reliability and interpretability. Results show SHAP delivers more granular insights but with higher computational cost, while Grad-CAM provides quicker visual feedback but lacks feature interaction capture. Both methods outperform random perturbation, validating their effectiveness.

## Method Summary
The study compares SHAP and Grad-CAM explanations for GCN-based human activity recognition on skeleton data. SHAP uses DeepExplainer with 100 background samples to provide feature-level attributions based on Shapley values, while Grad-CAM extracts spatial explanations from gradient-weighted class activation maps. The authors apply perturbation experiments by systematically modifying edges in the GCN's edge importance matrix based on ranked SHAP and Grad-CAM values. They evaluate both methods on NTU RGB+D 60 and Cerebral Palsy datasets, measuring feature importance, interpretability, and model sensitivity. The research also investigates optimal layer selection for Grad-CAM across different convolutional layers.

## Key Results
- SHAP provides more granular feature-level insights but requires higher computational resources than Grad-CAM
- Grad-CAM offers faster visual feedback but cannot capture feature interactions in skeleton data
- Both explanation methods outperform random perturbation in identifying important body key points
- The temporal convolutional layer produces the most informative Grad-CAM explanations for GCN-based HAR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHAP and Grad-CAM are complementary rather than competing explainability methods for HAR.
- Mechanism: SHAP provides detailed feature-level attribution by decomposing model predictions into individual feature contributions using Shapley values, while Grad-CAM offers spatially-oriented explanations highlighting influential body key points through gradient-based localization.
- Core assumption: Both methods capture different aspects of model decision-making - SHAP captures feature interactions and contributions while Grad-CAM captures spatial-temporal patterns in skeletal data.
- Evidence anchors:
  - [abstract] "While SHAP provides detailed input feature attribution, Grad-CAM delivers faster, spatially oriented explanations, making both methods complementary depending on the application's requirements."
  - [section] "SHAP considers feature interactions and how the importance of a body key point can change depending on other body key points, while Grad-CAM does not explicitly consider interactions."
- Break condition: If the model architecture doesn't support gradient extraction for Grad-CAM or if feature interactions are minimal in the dataset.

### Mechanism 2
- Claim: Perturbation experiments validate the reliability of both explanation methods by showing performance degradation when important features are altered.
- Mechanism: By systematically modifying edges in the GCN's edge importance matrix based on ranked SHAP and Grad-CAM values, the study measures how much model accuracy drops when important body key points are perturbed versus unimportant ones.
- Core assumption: The ranking of features by SHAP and Grad-CAM accurately identifies truly important features that the model relies on for predictions.
- Evidence anchors:
  - [section] "We apply the perturbation technique from [19] to test the two explanation methods and perturb specific parts of the model architecture related to skeleton body key points."
  - [section] "Both XAI methods perform better than randomly perturbing the body key points, approving their correctness."
- Break condition: If the perturbation methodology doesn't accurately reflect realistic data variations or if the model has high redundancy in feature usage.

### Mechanism 3
- Claim: Different convolutional layers in GCNs produce varying quality of Grad-CAM explanations, with later layers capturing more relevant information.
- Mechanism: Grad-CAM values from the temporal convolutional layer (TCN) and attention activation layer are compared, showing that TCN layer gradients preserve more spatial information about body key points relevant to final predictions.
- Core assumption: The final convolutional layers contain the most discriminative spatial features that directly influence the classifier's output.
- Evidence anchors:
  - [section] "Our experiments indicate that the last convolutional layer within the network preserved the most information relevant to the final decision."
  - [section] "We also showed that the early-layer activations do not correlate with the activations from the TCN or attention layer, which shows the best results in the perturbation experiment."
- Break condition: If the model architecture places important feature extraction in earlier layers or if attention mechanisms override spatial information.

## Foundational Learning

- Concept: Graph Convolutional Networks (GCNs)
  - Why needed here: The study uses GCNs to process skeleton-based human activity recognition data, where body key points form a graph structure with spatial relationships.
  - Quick check question: How do GCNs differ from standard convolutional neural networks when processing skeleton data?

- Concept: Shapley values and cooperative game theory
  - Why needed here: SHAP explanations are based on Shapley values, which measure each feature's contribution to model predictions by evaluating all possible feature combinations.
  - Quick check question: What are the three axioms that SHAP explanations satisfy according to cooperative game theory?

- Concept: Gradient-weighted class activation mapping
  - Why needed here: Grad-CAM generates explanations by computing gradients with respect to feature maps in convolutional layers, highlighting regions important for specific class predictions.
  - Quick check question: How does Grad-CAM use the ReLU activation in its final localization map computation?

## Architecture Onboarding

- Component map: Skeleton preprocessing -> GCN feature extraction -> Classification -> Explanation generation (SHAP: reference dataset + DeepExplainer; Grad-CAM: gradient extraction at target layer)
- Critical path: Skeleton preprocessing → GCN feature extraction → Classification → Explanation generation (SHAP: reference dataset + DeepExplainer; Grad-CAM: gradient extraction at target layer)
- Design tradeoffs: SHAP provides granular feature-level attribution but requires multiple forward passes and large reference datasets; Grad-CAM is computationally efficient but lacks feature interaction capture and depends on appropriate layer selection
- Failure signatures: Poor explanation quality when features are highly correlated (affecting SHAP), when important spatial patterns are in early layers (affecting Grad-CAM), or when perturbation experiments don't reflect realistic data variations
- First 3 experiments:
  1. Run both SHAP and Grad-CAM on a simple skeleton action class to compare spatial explanations vs feature attributions
  2. Perform perturbation experiments by modifying edges in the GCN's edge importance matrix based on SHAP and Grad-CAM rankings
  3. Test Grad-CAM on different convolutional layers (initialization, TCN, attention) to identify which layer produces most informative spatial explanations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of background dataset size and composition affect SHAP explanation reliability and runtime in skeleton-based HAR?
- Basis in paper: [explicit] The paper notes SHAP's dependence on background dataset, mentions current n=100 random sampling, and references variance scaling with 1/sqrt(n)
- Why unresolved: The paper uses n=100 as a practical compromise but acknowledges this may impact both explanation quality and computational feasibility
- What evidence would resolve it: Systematic experiments varying background dataset size and composition, measuring explanation stability and runtime across different HAR scenarios

### Open Question 2
- Question: How do different temporal averaging strategies affect perturbation experiment outcomes in skeleton-based HAR?
- Basis in paper: [inferred] The paper discusses averaging over entire video frames but suggests this may obscure critical short-duration movements
- Why unresolved: The paper identifies this as a limitation but doesn't explore alternative temporal perturbation strategies
- What evidence would resolve it: Experiments comparing full-sequence perturbation vs. segment-level perturbation approaches, measuring impact on explanation reliability

### Open Question 3
- Question: How would hybrid explanation methods combining SHAP and Grad-CAM perform compared to individual methods?
- Basis in paper: [explicit] The paper suggests SHAP and Grad-CAM should be viewed as complementary tools and mentions potential for hybrid approaches
- Why unresolved: The paper only compares the two methods individually without exploring combined approaches
- What evidence would resolve it: Development and evaluation of hybrid explanation methods that integrate spatial and feature-level explanations, comparing performance to individual methods

## Limitations

- Limited evaluation of explanation methods on out-of-distribution samples beyond the two studied datasets
- Small reference dataset size (n=100) for SHAP may affect attribution stability and computational feasibility
- Lack of ablation studies on model architecture variations and their impact on explanation quality

## Confidence

- Confidence in the complementary nature of methods (High): Both methods outperform random perturbation in identifying important features
- Confidence in SHAP's feature interaction capture (Medium): Qualitative comparisons suggest feature interaction consideration but lack quantitative measures
- Confidence in Grad-CAM layer selection (Low-Medium): Optimal layer appears dataset-dependent without systematic exploration of why

## Next Checks

1. **Layer correlation analysis**: Verify the correlation patterns between different convolutional layers for Grad-CAM as shown in Table 1 across multiple dataset splits
2. **SHAP stability test**: Evaluate SHAP explanations with varying background dataset sizes (n=50, 100, 200) to assess sensitivity to reference set size
3. **Cross-dataset generalization**: Test both explanation methods on a third HAR dataset with different characteristics to validate the layer selection recommendations and perturbation methodology robustness