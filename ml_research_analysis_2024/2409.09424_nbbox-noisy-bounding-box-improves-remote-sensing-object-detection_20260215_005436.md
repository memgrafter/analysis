---
ver: rpa2
title: 'NBBOX: Noisy Bounding Box Improves Remote Sensing Object Detection'
arxiv_id: '2409.09424'
source_url: https://arxiv.org/abs/2409.09424
tags:
- object
- bounding
- detection
- data
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving remote sensing
  object detection by proposing a novel data augmentation strategy called NBBOX, which
  focuses on transforming bounding boxes rather than images. The core idea is to add
  noise to bounding boxes through scaling, rotation, and translation, enabling more
  robust training in potentially inaccurate bounding box annotations.
---

# NBBOX: Noisy Bounding Box Improves Remote Sensing Object Detection

## Quick Facts
- arXiv ID: 2409.09424
- Source URL: https://arxiv.org/abs/2409.09424
- Authors: Yechan Kim; SooYeon Kim; Moongu Jeon
- Reference count: 36
- Primary result: Proposed NBBOX augmentation achieves up to 0.6% mAP improvement on DOTA and 0.5% on DIOR-R compared to state-of-the-art methods

## Executive Summary
This paper introduces NBBOX, a novel data augmentation strategy that improves remote sensing object detection by applying geometric transformations directly to bounding boxes rather than images. The approach addresses the challenge of inconsistent bounding box annotations common in aerial imagery by adding controlled noise through scaling, rotation, and translation. NBBOX demonstrates significant performance improvements on rotated object detection benchmarks DOTA and DIOR-R while being more computationally efficient than traditional image-level augmentation methods.

## Method Summary
NBBOX applies random geometric transformations—scaling, rotation, and translation—to bounding box parameters instead of images during training. The method operates on oriented bounding boxes (xc, yc, w, h, θ) by applying transformations within controlled ranges: scaling near [0.95, 1.01], rotation near ±0.01°, and translation near ±1-2 pixels. The approach forces models to learn invariance to annotation variability and improves localization accuracy under potentially noisy labels. NBBOX integrates into the data preprocessing pipeline before the detector model, transforming ground truth boxes while keeping images unchanged.

## Key Results
- Achieves 0.6% mAP improvement on DOTA and 0.5% on DIOR-R compared to state-of-the-art augmentation strategies
- More time-efficient than image-level augmentations like MosaicMix (9.63min vs 21.24min per epoch on DIOR-R)
- Demonstrates effectiveness across multiple detector architectures including Faster R-CNN, RetinaNet, and FCOS

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Bounding box noise injection improves localization robustness by regularizing the detector against inconsistent annotations.
- **Mechanism:** NBBOX applies random geometric transformations to bounding box parameters instead of images, forcing the model to learn invariance to annotation variability.
- **Core assumption:** Mismatch between optimal minimum enclosing boxes and provided annotations is a major source of training instability.
- **Evidence anchors:** [abstract] argues for bounding box transformations as model regularization; [section II-A] discusses mismatch between optimal and actual annotations.
- **Break condition:** If bounding box labels are already near-optimal, added noise may degrade performance.

### Mechanism 2
- **Claim:** Transforming bounding boxes alone is computationally more efficient than image-level augmentations.
- **Mechanism:** NBBOX operates directly on bounding box coordinates and labels, avoiding pixel data manipulation.
- **Core assumption:** Bounding box transformations are less computationally expensive than pixel-level augmentations.
- **Evidence anchors:** [abstract] states NBBOX is more time-efficient; [section IV-D] shows 9.63min vs 21.24min per epoch on DIOR-R.
- **Break condition:** If detector architecture heavily depends on pixel-level augmentations, omitting them may limit performance gains.

### Mechanism 3
- **Claim:** Random scaling, rotation, and translation increase model generalization by exposing it to varied spatial configurations.
- **Mechanism:** Varying bounding box geometry within controlled ranges helps the model recognize objects under diverse orientations and sizes.
- **Core assumption:** Aerial imagery contains objects with significant orientation variance, benefiting from augmented bounding box examples.
- **Evidence anchors:** [section II-B] describes adding noise with geometric transformations; [section III-C] provides recommended hyperparameters.
- **Break condition:** If transformations are too aggressive (e.g., large rotations), the model may fail to converge.

## Foundational Learning

- **Concept:** Minimum enclosing bounding box vs. annotation bounding box
  - Why needed here: Understanding why NBBOX targets bounding box noise requires knowing that provided labels often don't match optimal minimal boxes.
  - Quick check question: In DOTA, what is the difference between the annotated bounding box and the smallest possible rotated rectangle around the object?

- **Concept:** Oriented bounding box parameterization (xc, yc, w, h, θ)
  - Why needed here: NBBOX modifies these five parameters directly; knowing how they map to image coordinates is critical.
  - Quick check question: How does changing θ by ±0.01° affect the visual placement of the box in an image coordinate system?

- **Concept:** Mean Average Precision (mAP) at IoU=0.5
  - Why needed here: NBBOX's performance gains are reported using mAP@50; understanding this metric is essential.
  - Quick check question: If a detector's predicted box has IoU=0.6 with ground truth, does it contribute to mAP@50?

## Architecture Onboarding

- **Component map:** Dataset -> NBBOX augmentation layer -> Detector model
- **Critical path:** During each training epoch, for every image: load image and ground truth boxes → apply NBBOX transformations to bounding boxes → pass transformed boxes and unchanged image to model → compute loss and backpropagate
- **Design tradeoffs:** Trade-off between augmentation strength and training stability; choice of transformation ranges (scaling near [0.95, 1.01], rotation near ±0.01°, translation near ±1–2px); computational efficiency vs. augmentation diversity
- **Failure signatures:** Performance drops when transformation ranges are too large; training instability with extreme box deformations in early epochs; overfitting if augmentation is disabled after pretraining
- **First 3 experiments:** 1) Baseline: Train Faster R-CNN on DIOR-R with standard augmentations, measure mAP@50; 2) NBBOX-only: Add NBBOX with recommended ranges, compare mAP@50; 3) Ablation: Disable each transformation type one at a time with NBBOX active, measure impact on mAP@50

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the accuracy of bounding box annotations and the magnitude of performance improvement achieved by NBBOX?
- Basis in paper: [explicit] The paper mentions NBBOX is particularly effective for aerial imagery due to potentially inconsistent bounding box annotations, and argues that investigating bounding box transformation is necessary to boost model performance in the presence of potentially inaccurate bounding boxes.
- Why unresolved: The paper does not provide a quantitative analysis of how the degree of annotation inaccuracy affects the performance gain from NBBOX.
- What evidence would resolve it: A study varying the level of synthetic noise in bounding box annotations and measuring the corresponding performance improvement with NBBOX compared to a baseline without NBBOX.

### Open Question 2
- Question: How does NBBOX perform on datasets with different object density and size distributions, such as very dense or very small objects?
- Basis in paper: [inferred] The experiments were conducted on DOTA and DIOR-R, which contain rotated and densely placed objects, but the paper does not explore the performance of NBBOX on datasets with extreme object densities or sizes.
- Why unresolved: The paper does not provide evidence on how NBBOX generalizes to datasets with different object density and size distributions.
- What evidence would resolve it: Experiments on additional datasets with varying object densities and sizes to assess the robustness and adaptability of NBBOX.

### Open Question 3
- Question: What is the impact of NBBOX on the model's ability to detect objects at different scales, and does it introduce any bias towards certain scales?
- Basis in paper: [inferred] The paper mentions that FPN-1x is added as a neck to improve multi-scale invariance, but it does not specifically analyze how NBBOX affects the model's performance across different object scales.
- Why unresolved: The paper does not provide a detailed analysis of how NBBOX influences the model's performance on objects of different scales.
- What evidence would resolve it: A comprehensive evaluation of NBBOX's performance on objects of varying scales, including a comparison with baseline methods to identify any potential biases or improvements in scale detection.

## Limitations
- Limited empirical evidence quantifying actual annotation noise in DOTA and DIOR-R datasets
- Computational efficiency comparisons lack context about hardware configurations and optimization settings
- Core mechanism explanation lacks extensive ablation studies validating bounding box transformations over image-level augmentations

## Confidence

- **High confidence**: The effectiveness of NBBOX for improving mAP scores on DOTA and DIOR-R datasets, as this is directly measured and reported with statistical significance
- **Medium confidence**: The computational efficiency claims, as timing comparisons are provided but lack context about hardware configurations and optimization settings
- **Medium confidence**: The mechanism explaining why bounding box transformations work better than image-level augmentations, as this is theoretically sound but lacks extensive ablation studies

## Next Checks

1. **Annotation noise quantification**: Measure the actual discrepancy between provided bounding boxes and minimum enclosing rectangles in DOTA and DIOR-R to validate the core assumption about annotation inconsistency

2. **Transformation sensitivity analysis**: Systematically vary the transformation ranges (scaling, rotation, translation) beyond the recommended values to determine the stability boundaries of NBBOX

3. **Cross-dataset generalization**: Test NBBOX on non-rotated aerial datasets and natural image datasets to evaluate whether the performance gains are specific to rotated object detection or generalize to broader object detection scenarios