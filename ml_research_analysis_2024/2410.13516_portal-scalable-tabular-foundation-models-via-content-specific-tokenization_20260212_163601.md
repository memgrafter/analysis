---
ver: rpa2
title: 'PORTAL: Scalable Tabular Foundation Models via Content-Specific Tokenization'
arxiv_id: '2410.13516'
source_url: https://arxiv.org/abs/2410.13516
tags:
- data
- datasets
- embeddings
- tabular
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PORTAL introduces a scalable transformer-based framework for self-supervised\
  \ learning on tabular data, addressing the challenge of integrating diverse data\
  \ modalities without requiring data cleaning or preprocessing. The core method employs\
  \ content-specific tokenization\u2014separating text, numerical, and date values\
  \ into distinct embeddings combined with column name information\u2014and uses masked\
  \ cell modeling for pre-training."
---

# PORTAL: Scalable Tabular Foundation Models via Content-Specific Tokenization

## Quick Facts
- arXiv ID: 2410.13516
- Source URL: https://arxiv.org/abs/2410.13516
- Reference count: 5
- Primary result: Achieves 77.8% classification accuracy and 73.8% R2 score for regression on 51 datasets, outperforming CatBoost, XGBoost, CM2, and matching AutoGluon and CARTE

## Executive Summary
PORTAL introduces a scalable transformer-based framework for self-supervised learning on tabular data, addressing the challenge of integrating diverse data modalities without requiring data cleaning or preprocessing. The core method employs content-specific tokenization—separating text, numerical, and date values into distinct embeddings combined with column name information—and uses masked cell modeling for pre-training. This approach enables effective transfer learning to downstream classification and regression tasks. On 51 datasets from Kim et al. (2024), PORTAL with bagging achieves state-of-the-art performance with 77.8% classification accuracy and 73.8% R2 score for regression, surpassing CatBoost, XGBoost, CM2, and matching or exceeding AutoGluon and CARTE.

## Method Summary
PORTAL uses content-specific tokenization to encode different data types (text, numerical, date) in a way that naturally handles scale variation, outliers, and missing values. Each cell is tokenized using modality-specific encoders—text via LLM embeddings, numbers via scientific notation decomposition (sign, exponent, fraction), dates via decomposed integer embeddings—then summed with column name embeddings. The method uses masked cell modeling with 30% masking rate and constrained random replacement for pre-training, followed by fine-tuning on downstream classification or regression tasks using appropriate loss functions (cross-entropy for classification, Huber/L2 for regression).

## Key Results
- Achieves 77.8% classification accuracy and 73.8% R2 score for regression on 51 datasets from Kim et al. (2024)
- Outperforms CatBoost (76.0% accuracy, 71.6% R2), XGBoost (76.1% accuracy, 71.4% R2), and CM2 (73.4% accuracy, 68.6% R2)
- Matches or exceeds AutoGluon (77.8% accuracy, 73.8% R2) and CARTE (77.8% accuracy, 73.5% R2)
- On 45 numerical datasets from Grinsztajn et al. (2022), achieves 84.5% accuracy and 75.3% R2 score, competitive with CatBoost (85.3% accuracy, 77.6% R2) and AutoGluon (85.3% accuracy, 77.6% R2)

## Why This Works (Mechanism)

### Mechanism 1
Content-specific tokenization eliminates the need for data cleaning by encoding different data types in a way that naturally handles scale variation, outliers, and missing values. Each cell is tokenized using a modality-specific encoder—text via LLM embeddings, numbers via scientific notation decomposition (sign, exponent, fraction), dates via decomposed integer embeddings—then summed with column name embeddings. This representation inherently captures semantic and structural information without preprocessing.

### Mechanism 2
Masked cell modeling with 30% masking rate and constrained random replacement improves generalization by forcing the model to learn robust imputation and contextual inference. Each cell is masked with 30% probability and either zeroed (80%), left unchanged (10%), or replaced by a value from the same column (10%). This simulates realistic missing data scenarios and prevents overfitting to specific values.

### Mechanism 3
The modified transformer backbone with column name embeddings instead of positional encodings is better suited for tabular data because table semantics depend on column identity, not sequence order. Column name embeddings are concatenated with cell embeddings and fed to the transformer encoder, which learns relationships between features based on column identity. The [CLS] token is omitted because no next-sentence classification task is performed.

## Foundational Learning

- **Transformer encoder architecture and masked language modeling**: PORTAL uses a transformer encoder for self-supervised pre-training on tabular data, requiring understanding of how transformers process sequences and how masked prediction works. *Quick check*: How does the transformer encoder differ from the decoder in handling masked tokens, and why is this distinction important for PORTAL's pre-training?

- **Multi-task learning with heterogeneous loss functions**: PORTAL optimizes different loss functions for different data types (cross-entropy for discrete, Huber for text, L2 for regression), requiring understanding of when and how to apply each loss. *Quick check*: Why does PORTAL use Huber loss for text embeddings instead of L2 or cross-entropy, and what advantages does this provide?

- **Scientific notation encoding for numerical values**: PORTAL encodes numbers as ±α·2^β to handle scale variation and outliers, requiring understanding of how this representation differs from standard normalization. *Quick check*: How does encoding numbers in scientific notation with soft-binning of the fractional part improve robustness compared to min-max scaling?

## Architecture Onboarding

- **Component map**: Type-specific encoders (text LLM, number scientific notation, date decomposition) → Column name embeddings → Summation → Transformer encoder → Type-specific decoders (masked prediction) → Output losses
- **Critical path**: Row encoding → Masked prediction → Loss computation → Parameter updates
- **Design tradeoffs**: Using column name embeddings instead of positional encodings simplifies the architecture but requires informative column names; scientific notation encoding handles scale variation but increases model complexity
- **Failure signatures**: Poor masked prediction accuracy indicates encoding issues; unstable training suggests masking rate problems; low downstream performance indicates pre-training dataset mismatch
- **First 3 experiments**:
  1. Verify that column name embeddings are being correctly concatenated with cell embeddings by inspecting the input representation
  2. Test masked prediction accuracy on a small synthetic dataset to validate the masking and decoding mechanisms
  3. Compare pre-training with and without column name embeddings to confirm their importance for the model's performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of embedding model (e.g., sentence transformers vs. larger language models) impact PORTAL's performance on text-heavy tabular datasets? The paper uses all-MiniLM-L6-v2 for efficiency, but notes that larger embedding models have shown better performance in various domains, which was not examined in detail.

### Open Question 2
Can PORTAL's performance be further improved by incorporating broader and more diverse pretraining datasets beyond Wikipedia infoboxes and wikidata? The paper suggests that using Wikipedia data for pretraining may not adequately represent diverse tabular datasets and proposes incorporating datasets like OpenTabs or the CSV subset from The Stack.

### Open Question 3
What is the optimal configuration for the masking strategy during pretraining to maximize PORTAL's effectiveness on downstream tasks? The paper uses a 30% masking probability with specific strategies for handling masked cells, but does not explore variations in masking strategies or their impact on performance.

## Limitations

- Performance gap exists between PORTAL and AutoGluon on numerical datasets (84.5% vs 85.3% accuracy), suggesting limited universal superiority
- Method's reliance on informative column names presents practical limitations for real-world datasets with missing, ambiguous, or non-informative headers
- Paper does not address computational efficiency comparisons with tree-based methods, leaving scalability questions open

## Confidence

- **High confidence**: The core mechanism of content-specific tokenization and its ability to handle diverse data modalities without preprocessing is well-supported by experimental results
- **Medium confidence**: The architectural claim that column name embeddings can replace positional encodings is plausible but not definitively proven through ablation studies
- **Medium confidence**: The superiority over existing tabular models is demonstrated on specific datasets but may not generalize to all tabular prediction tasks

## Next Checks

1. Conduct an ablation study on column name embeddings by comparing PORTAL performance with and without them to quantify their contribution and test whether they truly substitute for positional encodings in tabular contexts.

2. Evaluate PORTAL's training time and inference latency compared to CatBoost and XGBoost on datasets of increasing size to assess practical deployment considerations for industrial applications.

3. Test PORTAL performance on datasets with progressively degraded column names (missing headers, uninformative names, random strings) to determine the minimum quality threshold required for effective operation.