---
ver: rpa2
title: Automated Trustworthiness Testing for Machine Learning Classifiers
arxiv_id: '2406.05251'
source_url: https://arxiv.org/abs/2406.05251
tags:
- trustworthiness
- tower
- noise
- explanations
- trustworthy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TOWER introduces automated trustworthiness oracles for ML text
  classifiers by leveraging word embeddings to evaluate if explanation words are semantically
  related to predicted class names. The method filters and aggregates semantic relatedness
  scores from multiple embedding models to judge prediction trustworthiness.
---

# Automated Trustworthiness Testing for Machine Learning Classifiers

## Quick Facts
- arXiv ID: 2406.05251
- Source URL: https://arxiv.org/abs/2406.05251
- Authors: Steven Cho; Seaton Cousins-Baxter; Stefano Ruberto; Valerio Terragni
- Reference count: 38
- Primary result: TOWER uses semantic relatedness between explanation words and class names to judge prediction trustworthiness, but shows poor performance against human-labeled data (F1 scores of 0.56 for trustworthy and 0.21 for untrustworthy predictions)

## Executive Summary
TOWER introduces an automated approach for assessing the trustworthiness of machine learning text classifiers by analyzing the semantic relatedness between words in prediction explanations and the predicted class names. The method leverages an ensemble of word embedding models and post-hoc explanation techniques like LIME to determine if predictions are trustworthy based on whether their explanations contain semantically related words. While experiments using artificially noisy training data show that TOWER can detect decreases in trustworthiness as noise increases, validation against human-labeled explanations reveals significant performance limitations, suggesting the need for more sophisticated trustworthiness assessment methods.

## Method Summary
TOWER creates trustworthiness oracles for ML text classifiers by generating explanations (via LIME) for predictions, then measuring semantic relatedness between explanation words and predicted class names using an ensemble of word embedding models (FAST TEXT, GLOVE, NNLM, SWIVEL, USE). The system filters explanation words by importance threshold, computes relatedness scores using multiple embeddings, and aggregates these scores to determine trustworthiness. To find optimal configuration parameters, the method trains models on progressively noisier data (label noise, removal noise, bias noise, natural noise) and selects parameters that show the strongest correlation between noise level and trustworthiness degradation.

## Key Results
- TOWER successfully detected trustworthiness decreases as noise increased, with slope values ranging from -0.010 to -0.567 across different noise types and model configurations
- The method achieved precision, recall, and F1 scores of 0.92, 0.40, and 0.56 for trustworthy predictions against human-labeled explanations
- Performance on untrustworthy predictions was particularly poor with precision, recall, and F1 scores of 0.14, 0.48, and 0.21 respectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic relatedness between explanation words and class names can serve as a proxy for prediction trustworthiness.
- Mechanism: The system uses word embeddings to measure the semantic distance between each word in the explanation and the predicted class. If these words are semantically related to the class, the prediction is considered trustworthy.
- Core assumption: Words that contribute to a trustworthy prediction will be semantically related to the class name.
- Evidence anchors:
  - [abstract]: "Our hypothesis is that a prediction is trustworthy if the words in its explanation are semantically related to the predicted class."
  - [section]: "Our intuition is that untrustworthy predictions should contain unrelated words in their explanations."
  - [corpus]: Found 25 related papers with average FMR=0.481, but no direct evidence linking semantic relatedness to trustworthiness in existing literature.
- Break condition: If the semantic relatedness measure is unreliable (e.g., due to bias in word embeddings) or if trustworthy explanations contain unrelated words for legitimate reasons.

### Mechanism 2
- Claim: An ensemble of multiple word embedding models reduces individual model bias and increases reliability.
- Mechanism: TOWER uses multiple word embedding techniques (FAST TEXT, GLOVE, NNLM, SWIVEL, USE) and combines their results through aggregation and voting.
- Core assumption: Different word embedding models trained on different datasets will have complementary strengths and weaknesses.
- Evidence anchors:
  - [section]: "To mitigate this, TOWER uses an ensemble approach, combining the results of multiple word embedding techniques trained on different datasets."
  - [section]: "This strategy should also enhance the effectiveness of the semantic similarity score computation."
  - [corpus]: No direct evidence in corpus about ensemble effectiveness for trustworthiness testing.
- Break condition: If all embedding models share similar biases or if the ensemble averaging/voting mechanism fails to capture nuanced semantic relationships.

### Mechanism 3
- Claim: Artificially induced noise in training data creates models with proportional trustworthiness issues that can be detected.
- Mechanism: The system trains models on increasingly noisy data (label noise, removal noise, bias noise, natural noise) and assumes that more noise correlates with less trustworthy predictions.
- Core assumption: Models trained on noisy data produce explanations that are less semantically related to class names, creating a measurable trustworthiness gradient.
- Evidence anchors:
  - [section]: "We make the assumption that models trained on noisy data will produce less trustworthy explanations, in proportion to the level of noise."
  - [section]: "Indeed, it is a common practice to use increasing artificial noise levels to produce relatively untrustworthy models."
  - [section]: "For each noise type, the test set is the same for each noise level."
  - [corpus]: Found 25 related papers with average FMR=0.481, but no direct evidence about noise-to-trustworthiness correlation in existing literature.
- Break condition: If the relationship between noise level and trustworthiness is not linear or monotonic, or if certain types of noise don't affect explanation quality as expected.

## Foundational Learning

- Concept: Word embeddings and semantic similarity
  - Why needed here: TOWER's core mechanism relies on measuring semantic relatedness between explanation words and class names using word embeddings.
  - Quick check question: How does cosine similarity between word vectors relate to semantic relatedness, and what are the limitations of this approach?

- Concept: Explainable ML techniques (LIME, SHAP)
  - Why needed here: TOWER uses post-hoc explanations from techniques like LIME to identify which words influenced the prediction.
  - Quick check question: What is the difference between faithfulness and plausibility in explanations, and why does TOWER focus on plausibility?

- Concept: Ensemble methods and threshold calibration
  - Why needed here: TOWER uses an ensemble of word embeddings and needs to set appropriate thresholds for determining relatedness.
  - Quick check question: How does TOWER determine individual thresholds for each embedding method, and why is this necessary?

## Architecture Onboarding

- Component map:
  - Input layer: ML model M, input instance x, predicted class c
  - Explanation generator: LIME (5,000 iterations)
  - Word embedding ensemble: FAST TEXT, GLOVE, NNLM, SWIVEL, USE
  - Threshold processor: Individual thresholds per embedding method
  - Aggregation engine: Combines relatedness scores via aggregation/voting
  - Trustworthiness oracle: Final decision maker (trustworthy/untrustworthy/undefined)

- Critical path:
  1. Receive input (M, x, c)
  2. Generate explanation via LIME
  3. Filter explanation words by importance threshold
  4. Compute semantic relatedness for each word using ensemble
  5. Aggregate relatedness scores
  6. Make trustworthiness decision

- Design tradeoffs:
  - Single vs. multiple embedding models: Multiple models reduce bias but increase complexity
  - Aggregation vs. voting: Aggregation provides nuanced results but voting is simpler
  - Threshold range: Wider range reduces false positives but may increase false negatives

- Failure signatures:
  - High undefined rate: Thresholds too strict or embedding models uncertain
  - Poor precision: Thresholds too lenient or ensemble not capturing semantic relationships
  - Bias toward trustworthy: Configuration parameters favor positive predictions

- First 3 experiments:
  1. Test TOWER on a simple dataset (e.g., 20 Newsgroups) with clean models to establish baseline performance
  2. Vary the explanation threshold parameter to find optimal balance between precision and recall
  3. Compare TOWER's performance using different aggregation methods (aggregation vs. voting) on the same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics of explanations are most predictive of trustworthiness across different domains and model types?
- Basis in paper: [explicit] The authors note that "further research is needed to understand the relationship between explanations and trustworthiness issues" and observe that TOWER performs differently across datasets with varying class specificity.
- Why unresolved: Current experiments show TOWER works better with specific class names versus general ones, suggesting current semantic relatedness measures are insufficient for capturing trustworthiness across all domains.
- What evidence would resolve it: Systematic evaluation of explanation characteristics (e.g., word specificity, contextual coherence, domain relevance) across diverse datasets and model types, with correlation analysis to human trustworthiness judgments.

### Open Question 2
- Question: Are there more effective noise types than those tested that better simulate real-world trustworthiness issues?
- Basis in paper: [explicit] The authors state "further research is needed to better understand the disconnect between artificial noise proxies and real-world trustworthiness issues" and observe poor performance on human-labeled data.
- Why unresolved: Current noise types (removal, label, bias, natural) create models with different characteristics than those exhibiting real-world trustworthiness problems, as evidenced by the gap between noise-based and human-based evaluation results.
- What evidence would resolve it: Testing TOWER on datasets with known trustworthiness issues (e.g., data leakage, dataset shift) and comparing performance to noise-injected models, along with identifying which noise types produce explanations most similar to real-world untrustworthy cases.

### Open Question 3
- Question: How can TOWER be extended to handle multi-word class descriptions and more complex explanation structures?
- Basis in paper: [explicit] The authors note TOWER "currently supports classes composed of a single word" and suggest that "a possible solution is to have the category instead be a collection of words describing the concept in more detail."
- Why unresolved: The current implementation's limitation to single-word classes and simple word-level explanations restricts its applicability to real-world scenarios where classes and explanations are more complex.
- What evidence would resolve it: Development and evaluation of TOWER variants that can handle multi-word class descriptions (e.g., using sentence embeddings) and more sophisticated explanation structures (e.g., phrase-level or contextual analysis).

## Limitations

- TOWER's core hypothesis that semantic relatedness indicates trustworthiness shows promise but faces significant validation challenges, particularly the disconnect between artificial noise experiments and real-world trustworthiness problems
- The poor performance against human-labeled explanations (precision 0.14, recall 0.48 for untrustworthy predictions) suggests that semantic relatedness may be too simplistic a measure of trustworthiness
- Current implementation is limited to single-word class descriptions, restricting its applicability to real-world scenarios with more complex class structures

## Confidence

- High confidence: TOWER's architecture and implementation details are well-specified
- Medium confidence: The noise-to-trustworthiness correlation hypothesis has experimental support but limited external validation
- Low confidence: Semantic relatedness as a universal trustworthiness indicator given poor human validation results

## Next Checks

1. **Class Representation Validation**: Replace single-word class names with multi-word definitions or context-aware representations to test if more sophisticated class matching improves semantic relatedness accuracy.

2. **Cross-Dataset Generalizability**: Test TOWER on datasets with different class structures (e.g., multi-label, hierarchical) to assess whether semantic relatedness scales beyond the current single-label classification setup.

3. **Human Trustworthiness Correlation**: Conduct a controlled study comparing TOWER's trustworthiness assessments against human expert evaluations on the same predictions to identify systematic differences and calibration opportunities.