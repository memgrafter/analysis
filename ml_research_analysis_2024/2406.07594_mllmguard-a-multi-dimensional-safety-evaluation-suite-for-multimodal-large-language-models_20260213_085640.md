---
ver: rpa2
title: 'MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large
  Language Models'
arxiv_id: '2406.07594'
source_url: https://arxiv.org/abs/2406.07594
tags:
- uni00000048
- uni0000004c
- uni00000013
- mllms
- uni00000010
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MLLMGuard, a comprehensive safety evaluation
  suite for multimodal large language models (MLLMs). The suite includes a bilingual
  (English and Chinese) evaluation dataset, inference utilities, and a lightweight
  evaluator called GuardRank.
---

# MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2406.07594
- Source URL: https://arxiv.org/abs/2406.07594
- Reference count: 40
- Primary result: MLLMGuard is a comprehensive safety evaluation suite for multimodal large language models, featuring a bilingual dataset, a lightweight evaluator GuardRank, and coverage of five safety dimensions with 12 subtasks.

## Executive Summary
This paper introduces MLLMGuard, a comprehensive safety evaluation suite designed to assess multimodal large language models (MLLMs) across five critical dimensions: Privacy, Bias, Toxicity, Truthfulness, and Legality. The suite includes a bilingual (English and Chinese) evaluation dataset of 2,282 image-text pairs, manually curated by human experts and enhanced with adversarial examples using red teaming techniques. A lightweight evaluator called GuardRank, fine-tuned on human-annotated data, is shown to outperform GPT-4 in evaluation accuracy. Experiments on 13 advanced models reveal significant room for improvement in MLLM safety, with model scaling and alignment techniques having varied impacts on safety performance.

## Method Summary
MLLMGuard evaluates MLLM safety using a manually constructed dataset of 2,282 image-text pairs, sourced primarily from social media to avoid data leakage. The dataset is annotated across five safety dimensions and enhanced with red teaming techniques to increase difficulty. A lightweight evaluator, GuardRank, is fine-tuned on human-annotated responses using LLaMA-2 and achieves higher accuracy than GPT-4. The evaluation pipeline uses metrics such as Attack Success Degree (ASD) and Perfect Answer Rate (PAR) to assess model safety. The study tests 13 advanced MLLMs and analyzes the impact of alignment techniques, model scaling, and the trade-off between honesty and harmlessness.

## Key Results
- MLLMGuard's dataset construction ensures quality and challenge, with over 82% of images sourced from social media and red teaming applied to generate adversarial examples.
- GuardRank, a lightweight evaluator fine-tuned on human-annotated data, achieves significantly higher evaluation accuracy than GPT-4.
- Experiments on 13 advanced MLLMs reveal substantial room for improvement in safety, with MiniGPT-v2 showing strong safety performance but underperforming in truthfulness.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLLMGuard's dataset avoids data leakage by sourcing over 82% of images from social media platforms.
- Mechanism: Crowd workers manually construct image-text pairs using real-world, non-curated social media images, reducing overlap with pre-training data.
- Core assumption: Social media images are not commonly included in public multimodal training datasets.
- Evidence anchors:
  - [abstract]: "we source over 82% of our dataset's images from social media platforms"
  - [section]: "To prevent data from being exposed to the training set of evaluated models, we manually construct text prompts, ensuring the absence of identical image-text pairs in any publicly available datasets."
  - [corpus]: Weak anchor — no direct citation of social media leakage studies; this is an assumed safeguard.
- Break condition: If social media images are later included in training corpora or the model training set is known, the leakage prevention claim weakens.

### Mechanism 2
- Claim: GuardRank outperforms GPT-4 in evaluation accuracy by being trained on human-annotated responses.
- Mechanism: Fine-tuning a lightweight PLM (LLaMA-2) on labeled prompt-response pairs captures safety judgment patterns more consistently than zero-shot GPT-4.
- Core assumption: Human labels on safety dimensions can be reliably captured by a smaller PLM.
- Evidence anchors:
  - [abstract]: "GuardRank achieves significantly higher evaluation accuracy than GPT-4"
  - [section]: "GUARD RANK is trained on a human-annotated dataset, employing LLaMA-2 [...] for the dimension of Privacy, Bias, Toxicity, and Legality"
  - [corpus]: Weak — no citation of comparable PLM fine-tuning results for safety tasks.
- Break condition: If human annotation introduces bias or noise, the fine-tuned model may propagate those errors.

### Mechanism 3
- Claim: Red teaming techniques increase dataset difficulty and model robustness testing.
- Mechanism: Applying text- and image-based red teaming (e.g., role-play, malicious labeling, position swapping) creates adversarial examples that stress-test model safety filters.
- Core assumption: These techniques simulate realistic attack vectors and expose safety gaps.
- Evidence anchors:
  - [section]: "human experts meticulously curate all text data, fortified with red teaming techniques"
  - [section]: "we break down our evaluation strategy into [...] the taxonomy of the threats"
  - [corpus]: Weak — no comparative ablation study showing performance with/without red teaming.
- Break condition: If red teaming techniques do not reflect real-world threat patterns, evaluation may over- or under-estimate safety.

## Foundational Learning

- Concept: Multimodal safety evaluation
  - Why needed here: MLLMs process both text and images, so safety assessment must consider visual content and cross-modal risks.
  - Quick check question: Why is it insufficient to evaluate MLLM safety using text-only benchmarks?

- Concept: Red teaming
  - Why needed here: Adversarial prompts expose vulnerabilities that standard benign queries miss.
  - Quick check question: What is the difference between text-based and image-based red teaming in MLLMs?

- Concept: Alignment techniques (chat vs. safety alignment)
  - Why needed here: Different fine-tuning stages affect how models respond to malicious inputs.
  - Quick check question: How might chat alignment differ from safety alignment in their impact on MLLM safety?

## Architecture Onboarding

- Component map: Dataset builder (crowdsourced + red teaming) -> GuardRank evaluator (fine-tuned PLM) -> Evaluation pipeline (multidimensional scoring)
- Critical path: Dataset → Human annotation → GuardRank fine-tuning → Model evaluation
- Design tradeoffs:
  - Manual dataset construction ensures quality but is slow and expensive.
  - Lightweight GuardRank is fast but may miss nuanced cases GPT-4 catches.
  - Red teaming increases challenge but may produce unrealistic scenarios.
- Failure signatures:
  - Low accuracy on new MLLM → dataset may not reflect model training distribution.
  - GuardRank scores diverge from human judgments → fine-tuning data bias or domain shift.
  - MLLM fails red teaming but not real attacks → red teaming overfits to synthetic patterns.
- First 3 experiments:
  1. Run GuardRank on a held-out subset of human-labeled data to confirm consistency.
  2. Compare GuardRank vs GPT-4 scores on the same sample set to measure performance gap.
  3. Test a subset of red teaming prompts on an MLLM trained without exposure to similar attacks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does scaling up MLLM parameters improve safety performance across all dimensions?
- Basis in paper: [explicit] The paper states "an increase in model parameters does not significantly enhance safety levels across all dimensions, even leading to a drop in some cases."
- Why unresolved: The paper only tests three groups of MLLMs with different parameter sizes. More extensive testing with a wider range of model sizes is needed to confirm the relationship between model size and safety performance.
- What evidence would resolve it: Experiments testing MLLMs with a broader range of parameter sizes across all five safety dimensions (Privacy, Bias, Toxicity, Truthfulness, Legality) to determine if there is a consistent relationship between model size and safety performance.

### Open Question 2
- Question: Is there a trade-off between honesty and harmlessness in MLLMs?
- Basis in paper: [explicit] The paper observes that MiniGPT-v2 exhibits strong safety across several dimensions but underperforms on Truthfulness, suggesting a potential trade-off between honesty and harmlessness.
- Why unresolved: The paper only provides a single example of a potential trade-off. More research is needed to determine if this trade-off is a general phenomenon in MLLMs or specific to certain models or tasks.
- What evidence would resolve it: A comprehensive study comparing the honesty and harmlessness of a wide range of MLLMs across various tasks and dimensions to determine if a consistent trade-off exists.

### Open Question 3
- Question: How effective are current alignment techniques in enhancing MLLM safety?
- Basis in paper: [explicit] The paper compares DeepSeek-VL-Base with its chat-aligned version and Gemini with its safety-aligned version, finding that both alignment techniques can enhance MLLM safety to varying degrees.
- Why unresolved: The paper only tests two specific alignment techniques. More research is needed to determine the effectiveness of other alignment techniques and their impact on different safety dimensions.
- What evidence would resolve it: Experiments testing a variety of alignment techniques on a diverse set of MLLMs and safety dimensions to determine which techniques are most effective and how they impact different aspects of safety.

## Limitations
- The dataset construction relies on social media sourcing, but no empirical validation is provided that these sources are truly disjoint from MLLM training corpora.
- GuardRank's superiority over GPT-4 is demonstrated, but the evaluation lacks comparison with other established safety benchmarks or ablation studies showing the impact of red teaming.
- The study focuses on 13 models, but the generalizability to other MLLMs or different safety contexts remains unclear.

## Confidence
- **High Confidence:** The dataset construction methodology (manual curation with red teaming) and the evaluation pipeline are clearly specified and reproducible.
- **Medium Confidence:** The claim that GuardRank outperforms GPT-4 is supported by the paper's results, but lacks broader comparative validation.
- **Low Confidence:** The assumption that social media images are not in training datasets is unverified and critical to the dataset's validity.

## Next Checks
1. **Data Leakage Validation:** Conduct a systematic check to ensure that the MLLMGuard dataset does not overlap with the training data of the evaluated models, possibly using embedding-based similarity searches.
2. **Evaluator Generalization:** Test GuardRank's performance on a held-out set of models not included in the original evaluation to assess its robustness and generalizability.
3. **Red Teaming Effectiveness:** Perform an ablation study to compare model safety performance with and without red teaming prompts, and validate whether the red teaming techniques reflect real-world attack patterns.