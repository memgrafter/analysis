---
ver: rpa2
title: 'Poor Man''s Training on MCUs: A Memory-Efficient Quantized Back-Propagation-Free
  Approach'
arxiv_id: '2411.05873'
source_url: https://arxiv.org/abs/2411.05873
tags:
- training
- gradient
- memory
- perturbation
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training neural networks
  on resource-constrained microcontrollers (MCUs) with limited memory. The proposed
  method, "Poor Man's Training," uses a memory-efficient, back-propagation-free approach
  based on quantized zeroth-order optimization.
---

# Poor Man's Training on MCUs: A Memory-Efficient Quantized Back-Propagation-Free Approach

## Quick Facts
- arXiv ID: 2411.05873
- Source URL: https://arxiv.org/abs/2411.05873
- Reference count: 40
- Key outcome: Achieves comparable performance to backpropagation training on adapting pre-trained image classifiers to corrupted data while significantly reducing memory usage on MCUs

## Executive Summary
This paper addresses the challenge of training neural networks on resource-constrained microcontrollers (MCUs) with limited memory. The proposed "Poor Man's Training" approach uses quantized zeroth-order optimization to estimate gradients through forward passes only, eliminating the need for backpropagation and gradient storage. By employing layer-wise adaptive weight/node perturbation and task-adaptive sparse training, the method achieves full-model training on an MCU with 1024-KB SRAM and sparse training with 256-KB SRAM. Experiments show comparable performance to traditional backpropagation training while using integer arithmetic for both inference and training.

## Method Summary
The method bypasses backpropagation by using quantized zeroth-order optimization to estimate gradients of quantized model parameters. It perturbs quantized parameters directly and computes gradients from quantized forward evaluations, avoiding storage of intermediate activations. The approach employs layer-wise adaptive weight/node perturbation where it perturbs either weights or activations at each layer based on their dimensionalities, reducing gradient estimation variance. Additionally, it uses task-adaptive sparse training that selects which layers to train based on their contribution to accuracy on the target dataset, freezing the rest of the model parameters. This combination enables memory-efficient training on MCUs while maintaining competitive accuracy.

## Key Results
- Achieves full-model training on an MCU with 1024-KB SRAM and sparse training with 256-KB SRAM
- Outperforms quantized backpropagation training in low-precision settings
- Matches backpropagation performance on fine-grained vision classification datasets
- Enables adaptation of pre-trained image classifiers to corrupted data with comparable accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The quantized zeroth-order optimization enables training with only forward passes and minimal extra memory compared to inference.
- Mechanism: By perturbing quantized model parameters directly and estimating gradients from quantized forward evaluations, the method avoids storing intermediate activations and complex backpropagation computation graphs.
- Core assumption: The bias in quantized zeroth-order gradient estimation can be mitigated by layer-wise adaptive weight/node perturbation and learning-rate scaling.
- Evidence anchors:
  - [abstract] "We adopt a quantized zeroth-order method to estimate the gradients of quantized model parameters"
  - [section] "Our key idea is to completely bypass the complicated BP implementation by proposing a quantized zeroth-order (ZO) method to train a real-quantized neural network model on MCU"
  - [corpus] Weak evidence - related works focus on BP-free methods but don't provide direct comparison evidence for quantized ZO effectiveness
- Break condition: When the variance of gradient estimation becomes too large relative to the true gradient norm, causing training instability or divergence.

### Mechanism 2
- Claim: Layer-wise adaptive weight/node perturbation reduces gradient estimation variance compared to model-wise perturbation.
- Mechanism: The method perturbs either weights or activations at each layer based on their dimensionalities, reducing the variance in gradient estimation compared to perturbing all parameters simultaneously.
- Core assumption: Disentangling gradient estimation layer-by-layer improves convergence by reducing cross-layer correlation effects.
- Evidence anchors:
  - [section] "Layer-wise gradient estimation outperforms model-wise gradient estimation in both weight and node perturbations"
  - [section] "Our layer-wise gradient estimation with adaptive weight/node perturbation outperforms all other methods and achieves the best training convergence"
  - [corpus] Weak evidence - related works mention zero-order methods but don't provide direct comparison evidence for layer-wise vs model-wise perturbation effectiveness
- Break condition: When the dimensionalities of weights and activations become similar across layers, reducing the benefit of adaptive perturbation.

### Mechanism 3
- Claim: Task-adaptive sparse training further reduces memory requirements while maintaining accuracy.
- Mechanism: The method selects which layers to train based on their contribution to accuracy on the target dataset, freezing the rest of the model parameters.
- Core assumption: Distribution shifts in specific layers can be identified through brief BP-free test training, allowing effective parameter selection.
- Evidence anchors:
  - [section] "Since different corruption types introduce varying distribution shifts from the pre-trained data, they benefit from fine-tuning different layers"
  - [section] "Our adaptive approach consistently outperforms all fixed selection methods"
  - [corpus] Weak evidence - related works focus on sparse training but don't provide direct comparison evidence for task-adaptive selection effectiveness
- Break condition: When the distribution shift affects all layers equally, making layer selection ineffective.

## Foundational Learning

- Concept: Zeroth-order optimization
  - Why needed here: Enables gradient estimation without backpropagation, crucial for memory-constrained MCUs
  - Quick check question: How does zeroth-order optimization estimate gradients without explicit derivative information?

- Concept: Quantized neural networks
  - Why needed here: Allows efficient inference and training on resource-constrained devices using integer arithmetic
  - Quick check question: What is the difference between quantization-aware training and real-quantized training?

- Concept: Memory-efficient layer-wise computation
  - Why needed here: Reduces memory overhead by processing one layer at a time rather than storing all intermediate activations
  - Quick check question: How does layer-wise computation reduce peak memory usage compared to standard backpropagation?

## Architecture Onboarding

- Component map: Quantized inference engine (TinyEngine) -> Control unit -> Gradient buffer -> Pseudo-random number generator -> External storage (SD card)

- Critical path: Forward pass → Perturbation generation → Loss computation → Gradient estimation → Parameter update → Repeat

- Design tradeoffs:
  - Memory vs. accuracy: Full-model training provides better accuracy but requires more memory than sparse training
  - Latency vs. convergence: More perturbation queries per layer improve convergence but increase per-iteration latency
  - Hardware complexity vs. flexibility: Using existing inference hardware simplifies design but may limit optimization opportunities

- Failure signatures:
  - Training diverges or shows unstable loss curves → Check learning rate scaling and perturbation variance
  - Accuracy plateaus below target → Verify layer selection and perturbation budget
  - Memory overflow → Profile memory usage of each component and reduce batch size or model size

- First 3 experiments:
  1. Validate quantized zeroth-order gradient estimation on a simple linear layer with known gradients
  2. Compare layer-wise vs. model-wise perturbation on a small CNN with controlled dimensionalities
  3. Test task-adaptive layer selection on a pre-trained model with synthetic distribution shifts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate of the proposed BP-free training method compare to BP-based training when the memory budget is increased?
- Basis in paper: [explicit] The paper mentions that the proposed method has a slower convergence rate compared to BP-based training, but does not provide a direct comparison under different memory budgets.
- Why unresolved: The paper focuses on the performance of the proposed method under extremely low memory budgets, but does not explore how it performs when more memory is available.
- What evidence would resolve it: Experimental results comparing the convergence rate of the proposed method to BP-based training under various memory budgets.

### Open Question 2
- Question: What is the impact of different activation functions on the performance of the proposed BP-free training method?
- Basis in paper: [inferred] The paper does not discuss the impact of different activation functions on the performance of the proposed method.
- Why unresolved: The choice of activation function can significantly affect the training process and final performance, but the paper does not explore this aspect.
- What evidence would resolve it: Experimental results comparing the performance of the proposed method using different activation functions.

### Open Question 3
- Question: How does the proposed BP-free training method perform on more complex neural network architectures, such as recurrent neural networks (RNNs) or transformers?
- Basis in paper: [explicit] The paper focuses on convolutional neural networks (CNNs) and does not discuss the performance of the proposed method on other architectures.
- Why unresolved: The proposed method may not be directly applicable to other architectures, and its performance on them is unknown.
- What evidence would resolve it: Experimental results applying the proposed method to RNNs or transformers and comparing their performance to BP-based training.

## Limitations

- The method shows slower convergence compared to backpropagation-based training, though this tradeoff is accepted for reduced memory usage
- Evaluation focuses primarily on adapting pre-trained models to corrupted data, with limited testing on truly untrained models or diverse task types
- Memory usage claims are based on specific MCU configurations and may not generalize to other hardware platforms

## Confidence

- Quantized zeroth-order optimization mechanism: Medium
- Layer-wise adaptive perturbation strategy: Medium
- Task-adaptive sparse training component: Medium

## Next Checks

1. Conduct ablation studies varying the number of perturbation queries per layer to establish the relationship between query budget and convergence quality
2. Test the method on completely untrained models from scratch, not just pre-trained model adaptation
3. Evaluate performance across diverse task types including regression, reinforcement learning, and few-shot learning scenarios