---
ver: rpa2
title: '3AM: An Ambiguity-Aware Multi-Modal Machine Translation Dataset'
arxiv_id: '2404.18413'
source_url: https://arxiv.org/abs/2404.18413
tags:
- dataset
- translation
- visual
- data
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces 3AM, an ambiguity-aware multimodal machine
  translation dataset containing 26,000 English-Chinese sentence pairs with corresponding
  images. The dataset is constructed using a semi-automatic approach that leverages
  a word sense disambiguation model to select ambiguous data from vision-and-language
  datasets, resulting in more challenging and visually diverse content compared to
  existing MMT datasets.
---

# 3AM: An Ambiguity-Aware Multi-Modal Machine Translation Dataset

## Quick Facts
- arXiv ID: 2404.18413
- Source URL: https://arxiv.org/abs/2404.18413
- Authors: Xinyu Ma; Xuebo Liu; Derek F. Wong; Jun Rao; Bei Li; Liang Ding; Lidia S. Chao; Dacheng Tao; Min Zhang
- Reference count: 0
- Primary result: Introduces 3AM, a 26,000-sentence English-Chinese multimodal translation dataset with images selected for lexical ambiguity to improve visual information exploitation in translation

## Executive Summary
This paper introduces 3AM, an ambiguity-aware multimodal machine translation dataset containing 26,000 English-Chinese sentence pairs with corresponding images. The dataset is constructed using a semi-automatic approach that leverages a word sense disambiguation model to select ambiguous data from vision-and-language datasets, resulting in more challenging and visually diverse content compared to existing MMT datasets. The authors benchmark several state-of-the-art MMT models on 3AM and show that models trained on this dataset exhibit a greater ability to exploit visual information than those trained on other MMT datasets.

## Method Summary
The authors construct 3AM through a semi-automatic pipeline that first filters and scores ambiguous sentences from vision-and-language datasets using a word sense disambiguation model (ConSeC), then translates selected sentences into Chinese with professional translators. The dataset is designed to force models to leverage visual information for disambiguation by selecting sentences containing ambiguous words that have multiple distinct senses. Five vision-and-language datasets (VE, COCO, SBU, CC, CC12M) are used as source material, with sentences filtered for length, quality, and proper noun content. The WSD model scores sentences based on the probability difference between top two senses, with ambiguous sentences (score > 0.4) ranked by diversity and selected for translation.

## Key Results
- 3AM achieves higher lexical richness than existing MMT datasets, with longer captions containing more unique nouns and verbs
- Models trained on 3AM show greater image awareness, with multimodal models outperforming text-only models by a large margin
- LPIPS and Inception Score metrics demonstrate greater image diversity in 3AM compared to existing datasets
- Visual information plays a vital role in 3AM, with multimodal models significantly outperforming text-only models

## Why This Works (Mechanism)
The dataset forces models to leverage visual information by containing more ambiguous words that require context for disambiguation. By selecting sentences where word sense disambiguation is challenging (probability difference between top senses > 0.4), the dataset creates situations where visual context becomes necessary for accurate translation. The combination of ambiguous content and diverse images creates a more challenging environment that better tests a model's ability to integrate visual and textual information.

## Foundational Learning

**Word Sense Disambiguation**: Identifying different meanings of words based on context. Why needed: Essential for selecting truly ambiguous sentences that require visual context for translation. Quick check: Verify WSD model correctly identifies multiple senses for target words with high probability differences.

**Image Feature Extraction**: Converting images into numerical representations that models can process. Why needed: Multimodal models require visual features to integrate with textual information. Quick check: Confirm visual features capture relevant image content and maintain consistency across similar images.

**BLEU Score Calculation**: Automated metric for evaluating translation quality by comparing n-gram matches. Why needed: Standard metric for measuring translation performance across different models and datasets. Quick check: Validate BLEU scores are calculated correctly with proper tokenization and smoothing.

## Architecture Onboarding

**Component Map**: Vision-and-language datasets -> Filtering & Scoring -> WSD Model -> Sentence Selection -> Professional Translation -> 3AM Dataset

**Critical Path**: Source data collection -> Ambiguous sentence selection (WSD scoring) -> Professional translation -> Model training and evaluation

**Design Tradeoffs**: The semi-automatic approach balances automation efficiency with human quality control, but introduces uncertainty in reproducibility due to unspecified filtering thresholds and manual translation requirements.

**Failure Signatures**: 
- If models don't show improved visual information exploitation: WSD model may not be identifying truly ambiguous cases or dataset may lack sufficient ambiguity
- If image awareness evaluation fails: Incongruent images may not be truly irrelevant or BERT model may not be properly fine-tuned
- If lexical richness metrics are low: Filtering criteria may be too restrictive or source datasets may lack diversity

**First Experiments**:
1. Train a text-only Transformer on 3AM and compare its performance on ambiguous vs. unambiguous sentences
2. Evaluate model performance with congruent vs. incongruent images to confirm image awareness
3. Compare ambiguity scores (WSD probability differences) between 3AM and existing MMT datasets

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of 3AM-trained models compare to models trained on other datasets when tested on out-of-domain data or different language pairs?
- Basis in paper: The paper focuses on evaluating models on the 3AM dataset but does not explore their generalization capabilities to out-of-domain data or different language pairs.
- Why unresolved: The paper's scope is limited to benchmarking on 3AM without testing cross-domain or cross-lingual performance.
- What evidence would resolve it: Experiments comparing 3AM-trained models on out-of-domain data and different language pairs would provide insights into their generalization capabilities.

### Open Question 2
What is the impact of varying the level of ambiguity in the dataset on the performance of MMT models?
- Basis in paper: The paper constructs 3AM with ambiguous data but does not systematically analyze how different levels of ambiguity affect model performance.
- Why unresolved: The paper does not provide a controlled analysis of ambiguity levels versus model performance.
- What evidence would resolve it: Experiments varying the level of ambiguity in the dataset and evaluating model performance on these variations would reveal the relationship between ambiguity and model effectiveness.

### Open Question 3
How does the performance of 3AM-trained models compare to models trained on other datasets when using different visual feature extraction methods?
- Basis in paper: The paper mentions using Transformer-based visual features but does not explore the impact of different visual feature extraction methods.
- Why unresolved: The paper does not compare model performance using alternative visual feature extraction approaches.
- What evidence would resolve it: Experiments comparing 3AM-trained models using different visual feature extraction methods would clarify the importance of visual features in the translation task.

## Limitations

- The dataset construction methodology lacks specific implementation details for filtering criteria, making exact replication difficult
- Image awareness evaluation relies on manual selection of incongruent images, introducing potential subjectivity
- The paper does not explore generalization capabilities of 3AM-trained models to out-of-domain data or different language pairs
- The specific contribution of ambiguity versus other dataset characteristics (length, diversity) to model performance is not fully isolated

## Confidence

**High Confidence**: The claim that 3AM contains more lexical richness and diversity than existing MMT datasets is supported by concrete metrics (LPIPS, Inception Score, object entropy) and clear comparison results.

**Medium Confidence**: The assertion that 3AM forces models to leverage visual information more effectively is supported by performance gaps between multimodal and text-only models, but the specific contribution of ambiguity is not fully isolated.

**Low Confidence**: The claim about 3AM being more challenging than existing datasets relies heavily on qualitative assessments without establishing absolute difficulty benchmarks or providing detailed examples of challenging cases.

## Next Checks

1. **Dataset Composition Analysis**: Replicate the WSD filtering process using the described ConSeC model and verify that the resulting dataset contains significantly more ambiguous words (according to the defined criteria) compared to existing MMT datasets. Calculate the average sense probability difference and confirm it meets the stated threshold.

2. **Image Awareness Robustness**: Conduct additional experiments using automatically generated incongruent images (e.g., random image swaps or style transfer) to verify that the observed performance drop with incongruent images is consistent and not dependent on specific manual image selections.

3. **Baseline Model Behavior**: Train text-only baseline models on 3AM and analyze their attention patterns to determine whether they are genuinely struggling with ambiguity or if the performance gap is primarily due to other dataset characteristics (length, lexical diversity). This would help isolate the specific contribution of ambiguity to model performance.