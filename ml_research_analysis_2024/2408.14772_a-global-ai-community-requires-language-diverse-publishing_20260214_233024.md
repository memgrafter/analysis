---
ver: rpa2
title: A global AI community requires language-diverse publishing
arxiv_id: '2408.14772'
source_url: https://arxiv.org/abs/2408.14772
tags:
- ingl
- para
- global
- english
- como
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that the dominance of English in AI research perpetuates
  linguistic exclusion and reinforces global inequalities in the field. The authors
  identify that top AI conferences and journals publish exclusively in English, disadvantaging
  non-native English speakers who face potential rejection based on language quality
  in peer review.
---

# A global AI community requires language-diverse publishing

## Quick Facts
- arXiv ID: 2408.14772
- Source URL: https://arxiv.org/abs/2408.14772
- Authors: Haley Lepp; Parth Sarin
- Reference count: 13
- Top AI conferences and journals publish exclusively in English, disadvantaging non-native English speakers who face potential rejection based on language quality in peer review.

## Executive Summary
This paper argues that the dominance of English in AI research publishing perpetuates linguistic exclusion and reinforces global inequalities. The authors identify that monolingual publishing creates barriers for non-native English speakers, limits global knowledge diversity, and reduces the richness of perspectives in AI research. They propose three interventions: administering conferences in local languages, instructing reviewers not to evaluate language appropriateness, and providing resources for publishing in multiple languages. The paper calls for a fundamental shift in AI publishing culture to embrace linguistic diversity and challenge English hegemony in computing.

## Method Summary
The paper presents a qualitative argument based on literature review and analysis of historical conference reviews. It examines peer review data from ICLR proceedings over six years, demonstrating thousands of reviews that critique authors' language either explicitly or implicitly. The authors also reference preliminary interviews with multilingual ICLR scholars about their experiences with language barriers. The work is a commentary/provocation rather than a computational study, using qualitative analysis to build its argument for cultural change in AI publishing practices.

## Key Results
- Peer review data shows thousands of reviews critiquing authors' language either explicitly ("The paper is full of English mistakes") or implicitly ("There are numerous grammatical errors and poorly-phrased sentences")
- Machine translation and writing assistance tools are symptoms of exclusion rather than solutions to linguistic barriers
- Publishing in only one language alienates readers in other languages and reduces global knowledge diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linguistic exclusion is reinforced when conferences and journals are exclusively administered in English.
- Mechanism: English-only administration creates barriers for non-native English speakers who must invest time, money, and effort into English-language training and copy-editing services. This increases the likelihood of rejection based on language quality rather than research merit.
- Core assumption: Peer reviewers unconsciously or consciously penalize papers with grammatical errors or non-native English expressions.
- Evidence anchors:
  - [abstract] "instructing peer reviewers not to evaluate language appropriateness"
  - [section] "thousands of reviews over the years critique the language of authors either explicitly... or implicitly"
  - [corpus] Weak evidence; related papers focus on bias in non-English NLP rather than English-dominant publishing systems.
- Break condition: If peer review processes become explicitly language-blind or if reviewers are trained to ignore linguistic style and focus solely on scientific merit.

### Mechanism 2
- Claim: Machine translation and writing assistance tools are symptoms of exclusion, not solutions.
- Mechanism: Scholars who do not speak English fluently feel compelled to use tools like ChatGPT to "fix" their writing before submission. This creates an extra barrier and fails to address the root cause of linguistic exclusion.
- Core assumption: Authors should be able to express their findings in their native language without the mediation of translation tools.
- Evidence anchors:
  - [abstract] "regard their use as a symptom of linguistic exclusion"
  - [section] "Authors should have the right to describe their findings in the language of their choice, without the mediation of translation"
  - [corpus] Weak evidence; related papers discuss multilingual LLMs but do not address the publishing process.
- Break condition: If authors are allowed to submit papers in their native languages and conferences provide translation services for presentations and discussions.

### Mechanism 3
- Claim: Publishing in only one language alienates readers in other languages and reduces global knowledge diversity.
- Mechanism: When research is published only in English, scholars who do not read English are excluded from accessing and building upon that knowledge. This limits the diversity of perspectives and knowledge systems that can contribute to AI research.
- Core assumption: Knowledge is culturally and linguistically situated; translation is never one-to-one.
- Evidence anchors:
  - [abstract] "producing research in a single language also alienates readers in other languages"
  - [section] "Translation is also never one-to-one, so by only publishing in one language, the community loses out on the vast diversity of other ways of knowing"
  - [corpus] Weak evidence; related papers discuss multilingual NLP datasets but do not address publishing diversity.
- Break condition: If conferences and journals offer opportunities to publish and present in multiple languages, and if infrastructure supports multilingual submissions.

## Foundational Learning

- Concept: Linguistic hegemony in academic publishing
  - Why needed here: Understanding how English dominance in academic publishing creates barriers for non-native English speakers and perpetuates global inequalities in AI research.
  - Quick check question: What are the three main consequences of English-only publishing in AI research?
- Concept: Peer review bias and linguistic discrimination
  - Why needed here: Recognizing how peer reviewers may unconsciously or consciously penalize papers with grammatical errors or non-native English expressions.
  - Quick check question: How do reviewers typically critique language in AI research papers?
- Concept: Multilingual knowledge systems and translation
  - Why needed here: Understanding that knowledge is culturally and linguistically situated, and that translation is never one-to-one, which limits the diversity of perspectives and knowledge systems that can contribute to AI research.
  - Quick check question: Why is publishing in only one language problematic for global knowledge diversity?

## Architecture Onboarding

- Component map: Conference organizers -> Submission guidelines update -> Peer review platform update -> Translation services integration -> Reviewer training
- Critical path: 1. Conference organizers update submission guidelines to allow multilingual submissions
2. Peer review platform adds support for multilingual papers
3. Translation services are hired and integrated
4. Funding is allocated for translation costs
5. Reviewers are trained to ignore language appropriateness
- Design tradeoffs:
  - Cost vs. inclusivity: Providing translation services is expensive but necessary for true inclusivity
  - Quality vs. accessibility: Ensuring high-quality translations while making research accessible to a global audience
  - Standardization vs. diversity: Balancing the need for standardized formats with the desire to embrace linguistic diversity
- Failure signatures:
  - Low submission rates from non-English speaking countries
  - High rejection rates for papers with grammatical errors or non-native English expressions
  - Limited diversity in conference attendees and presenters
- First 3 experiments:
1. Pilot a multilingual submission track at a small conference
2. Conduct a study on peer review bias and linguistic discrimination
3. Implement a translation service for a single conference and measure its impact on submission rates and diversity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would implementing multilingual publishing policies affect the quality and quantity of AI research submissions and peer review processes?
- Basis in paper: [explicit] The paper proposes administering conferences in local languages and instructing reviewers not to evaluate language appropriateness, but doesn't provide evidence of how this would impact research quality.
- Why unresolved: This is a policy proposal that would require empirical testing in real conference settings to measure its effects on submission diversity, review times, acceptance rates, and citation impacts.
- What evidence would resolve it: Controlled studies comparing conferences that implement multilingual policies versus traditional English-only conferences, measuring submission diversity, review times, acceptance rates, and citation impacts.

### Open Question 2
- Question: What is the actual economic impact of requiring translation services for AI conferences and journals?
- Basis in paper: [explicit] The paper suggests publications should set aside funds for translation services but doesn't provide cost estimates or funding models.
- Why unresolved: The paper identifies this as a necessary resource but doesn't quantify the financial implications or propose sustainable funding mechanisms.
- What evidence would resolve it: Detailed cost analyses of translation services for different conference sizes, funding models (registration fees, institutional support, grants), and ROI calculations comparing costs to increased participation benefits.

### Open Question 3
- Question: How do multilingual researchers currently navigate the AI publishing landscape, and what specific barriers do they face beyond language?
- Basis in paper: [explicit] The paper mentions preliminary interviews with multilingual ICLR scholars but doesn't provide detailed findings about their experiences or barriers.
- Why unresolved: The paper references future studies but doesn't present current empirical data on the lived experiences of multilingual researchers.
- What evidence would resolve it: Systematic qualitative and quantitative studies documenting the experiences of multilingual researchers, including submission strategies, revision processes, career impacts, and perceived barriers beyond language.

## Limitations

- The paper's argument is primarily qualitative and lacks quantitative evidence for its central claims about rejection rates correlated with linguistic background
- Proposed interventions are presented without cost-benefit analysis or implementation feasibility studies
- Does not address potential challenges such as maintaining scientific rigor across languages or handling specialized technical terminology in non-English contexts

## Confidence

- **High Confidence:** The existence of linguistic exclusion in AI publishing is well-documented through reviewer comments and author experiences
- **Medium Confidence:** The claim that language diversity enhances knowledge systems is theoretically sound but lacks empirical validation in the AI research context
- **Low Confidence:** The assertion that current peer review processes systematically disadvantage non-native speakers requires more rigorous statistical analysis to confirm causality rather than correlation

## Next Checks

1. Conduct a large-scale analysis of acceptance rates by authors' linguistic backgrounds across multiple AI conferences, controlling for research quality metrics and institutional affiliations
2. Implement A/B testing where identical papers with native vs. non-native English are submitted to conferences to measure differential treatment in the review process
3. Pilot the proposed multilingual conference model at a regional AI workshop and systematically evaluate impacts on submission diversity, review quality, and participant satisfaction