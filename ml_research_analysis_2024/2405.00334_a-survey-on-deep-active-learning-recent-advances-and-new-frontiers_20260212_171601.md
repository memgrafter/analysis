---
ver: rpa2
title: 'A Survey on Deep Active Learning: Recent Advances and New Frontiers'
arxiv_id: '2405.00334'
source_url: https://arxiv.org/abs/2405.00334
tags:
- learning
- active
- samples
- data
- proc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively reviews deep active learning (DAL),
  which aims to achieve strong performance with fewer labeled training samples by
  iteratively selecting the most informative samples for annotation. The paper categorizes
  DAL methods from five perspectives: annotation types (hard, soft, hybrid, explanatory,
  random/multi-agent), query strategies (uncertainty, representative, influence, Bayesian,
  hybrid), deep model architectures (RNNs, CNNs, GNNs, pre-trained models), learning
  paradigms (traditional, semi-supervised, contrastive, adversarial, meta, reinforcement,
  curriculum, continual, transfer, imitation, multi-task), and training processes
  (traditional, curriculum learning, pre-training & fine-tuning).'
---

# A Survey on Deep Active Learning: Recent Advances and New Frontiers

## Quick Facts
- arXiv ID: 2405.00334
- Source URL: https://arxiv.org/abs/2405.00334
- Reference count: 40
- One-line primary result: Comprehensive survey of deep active learning methods, taxonomy, and future directions

## Executive Summary
This survey provides a comprehensive review of deep active learning (DAL), a paradigm that aims to achieve strong model performance with fewer labeled training samples by iteratively selecting the most informative samples for annotation. The paper categorizes DAL methods from five perspectives: annotation types, query strategies, deep model architectures, learning paradigms, and training processes. Key findings include DAL's potential for few-shot learning with large pre-trained models, its combination with semi-supervised methods, and the need for a universal framework. The survey addresses challenges including inefficient human annotation, cross-domain transfer, unstable performance, scalability, data scarcity, and class distribution mismatch.

## Method Summary
The survey conducts a systematic literature review of deep active learning methods, collecting and filtering relevant papers using keyword searches and database queries. The authors define the DAL task formally and develop a high-level taxonomy to categorize previous studies across five orthogonal perspectives: annotation types (hard, soft, hybrid, explanatory, random/multi-agent), query strategies (uncertainty, representative, influence, Bayesian, hybrid), deep model architectures (RNNs, CNNs, GNNs, pre-trained models), learning paradigms (traditional, semi-supervised, contrastive, adversarial, meta, reinforcement, curriculum, continual, transfer, imitation, multi-task), and training processes (traditional, curriculum learning, pre-training & fine-tuning). The survey synthesizes findings and identifies open challenges while proposing directions for future research.

## Key Results
- DAL taxonomy organized across five systematic perspectives enables structured navigation of methods
- Pre-training and fine-tuning can reduce annotation requirements to 10-20% while maintaining competitive performance
- Hybrid query strategies combining uncertainty, diversity, and influence metrics show promise for balancing multiple objectives

## Why This Works (Mechanism)

### Mechanism 1
The taxonomy structure enables systematic navigation of deep active learning methods. The paper organizes methods into five orthogonal perspectives (annotation types, query strategies, model architectures, learning paradigms, training processes), allowing researchers to locate relevant methods quickly and understand trade-offs. This assumes different method categories address different challenges and can be mixed and matched to create hybrid approaches.

### Mechanism 2
Pre-training and fine-tuning significantly reduces annotation requirements. Large pre-trained language models contain rich prior knowledge that can be leveraged with only 10-20% labeled samples to achieve comparable performance to full-data training. This assumes the knowledge captured during pre-training is transferable to downstream tasks.

### Mechanism 3
Hybrid query strategies balance multiple objectives for better sample selection. Combining uncertainty, diversity, and influence-based criteria through serial, criteria-selection, or parallel-form hybrids addresses individual method limitations. This assumes different query strategies capture complementary information about sample informativeness.

## Foundational Learning

- **Concept**: Bayesian uncertainty estimation
  - Why needed here: Many DAL methods rely on uncertainty quantification to select informative samples
  - Quick check question: How does Bayesian neural network uncertainty differ from standard model confidence scores?

- **Concept**: Active learning pipeline
  - Why needed here: Understanding the iterative process of sample selection, annotation, and model retraining is fundamental to DAL
  - Quick check question: What are the key components of a pool-based active learning loop?

- **Concept**: Transfer learning principles
  - Why needed here: DAL often leverages knowledge from source domains to improve sample selection in target domains
  - Quick check question: How does domain adaptation differ from standard transfer learning?

## Architecture Onboarding

- **Component map**: Data module (handles unlabeled pool and labeled training sets) -> Model module (contains base model and acquisition functions) -> Query strategy module (implements uncertainty, diversity, influence metrics) -> Annotation interface (manages oracle interactions) -> Training module (handles model updates and evaluation)

- **Critical path**: Data selection → Annotation → Model training → Evaluation → Next iteration

- **Design tradeoffs**:
  - Batch size vs. diversity: Larger batches reduce iterations but may reduce diversity
  - Uncertainty vs. coverage: Pure uncertainty sampling may miss important regions
  - Computational cost vs. accuracy: More sophisticated acquisition functions improve quality but increase cost

- **Failure signatures**:
  - Model performance plateaus early: May indicate poor initial sample selection
  - Selected samples are redundant: Acquisition function may not capture diversity
  - Annotation budget exhausted without convergence: Query strategy may be too conservative

- **First 3 experiments**:
  1. Implement basic uncertainty sampling with entropy-based acquisition
  2. Add diversity constraint using CoreSet approach
  3. Combine uncertainty and diversity through weighted sum for hybrid selection

## Open Questions the Paper Calls Out

### Open Question 1
How can deep active learning be effectively combined with semi-supervised learning to avoid the vicious cycle of incorrect pseudo-labeling? The paper states that semi-supervised methods are sensitive to outliers and error labels, potentially leading to a cycle of incorrect pseudo-labeling, but does not provide a definitive solution for effectively integrating DAL with semi-supervised learning to prevent this cycle.

### Open Question 2
What are the most effective strategies for designing stopping criteria in deep active learning to minimize human labor and prevent the inclusion of noisy or redundant samples? The paper identifies insufficient research on stopping strategies in DAL and highlights the need to limit human labeling and prevent noisy/redundant samples, but existing stopping strategies are limited and often rely on heuristic approaches.

### Open Question 3
How can deep active learning be made more robust to class distribution mismatch, where unlabeled data may contain classes not present in the labeled data? The paper discusses the challenge of class distribution mismatch in DAL, where unlabeled data may contain unknown classes, leading to wasted annotation budgets, but existing DAL methods tend to select samples from unknown classes, assuming they are informative.

## Limitations

- The survey's comprehensive nature relies heavily on the completeness of literature coverage, which may miss emerging methods published after the review cutoff
- The taxonomy framework, while systematic, may not fully capture hybrid approaches that span multiple categories
- Implementation details for specific query strategies and learning paradigm combinations are not provided, limiting reproducibility
- The effectiveness claims for pre-training and fine-tuning are based on cited potential rather than systematic empirical validation across diverse tasks

## Confidence

- **High confidence**: The systematic organization of DAL methods into five perspectives is well-supported by the literature review structure
- **Medium confidence**: The claimed benefits of pre-training and fine-tuning for reducing annotation requirements are supported by general trends in deep learning but lack DAL-specific empirical validation
- **Medium confidence**: The identification of key challenges (annotation efficiency, cross-domain transfer, scalability) is well-grounded in the literature but solutions are not fully explored

## Next Checks

1. Conduct systematic experiments comparing pure uncertainty sampling vs. hybrid strategies across diverse datasets to quantify the claimed performance improvements

2. Implement and test pre-training & fine-tuning approaches on standard DAL benchmarks to validate the few-shot learning claims with large language models

3. Design a cross-domain DAL experiment where models trained on one domain are tested on another to assess the transferability claims and identify failure modes