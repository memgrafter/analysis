---
ver: rpa2
title: 'Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models'
arxiv_id: '2411.07140'
source_url: https://arxiv.org/abs/2411.07140
tags:
- chinese
- simpleqa
- llms
- language
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Chinese SimpleQA is a new benchmark designed to evaluate the factuality
  of large language models (LLMs) in Chinese. It addresses the lack of comprehensive
  Chinese-specific benchmarks for assessing LLMs' ability to provide short, accurate
  answers to fact-seeking questions.
---

# Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models

## Quick Facts
- arXiv ID: 2411.07140
- Source URL: https://arxiv.org/abs/2411.07140
- Reference count: 24
- New benchmark with 3,000 Chinese question-answer pairs evaluates LLM factuality across 6 major topics and 99 subtopics

## Executive Summary
Chinese SimpleQA is a new benchmark designed to evaluate the factuality of large language models (LLMs) in Chinese. It addresses the lack of comprehensive Chinese-specific benchmarks for assessing LLMs' ability to provide short, accurate answers to fact-seeking questions. The dataset consists of 3,000 high-quality question-answer pairs across 6 major topics and 99 subtopics, with a focus on objectivity, uniqueness, and timelessness of answers. It includes rigorous quality control processes, ensuring the questions are challenging and answerable as of 2023. The evaluation uses simple, short answers, making it easy to grade using existing LLMs like OpenAI's API. The results show that only a few models, such as o1-preview and Doubao-pro-32k, achieve high performance, indicating the benchmark's difficulty. Larger models generally perform better, and retrieval-augmented generation (RAG) significantly improves accuracy, reducing performance gaps among models. The study highlights the need for better factuality in LLMs and suggests that Chinese SimpleQA can guide developers in understanding and improving their models' capabilities.

## Method Summary
Chinese SimpleQA was constructed through a multi-step process involving automated question generation from authoritative sources, quality control with difficulty filtering, and human verification. The benchmark contains 3,000 question-answer pairs across 6 major topics and 99 subtopics, focusing on objective, unique, and timeless answers. Models are evaluated using five metrics: Correct (CO), Not attempted (NA), Incorrect (IN), Correct given attempted (CGA), and F-score. Evaluation is performed via OpenAI's API for automated grading of short answers, with RAG strategies also tested to assess their impact on factuality performance.

## Key Results
- Only a few models (o1-preview, Doubao-pro-32k) achieve high performance, indicating benchmark difficulty
- Larger models demonstrate better factuality calibration and performance
- RAG significantly improves accuracy and reduces performance gaps between models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-quality factuality benchmarks require adversarial question generation to probe knowledge boundaries
- Mechanism: Questions are generated from authoritative sources and filtered through difficulty testing with multiple strong models to ensure they are challenging
- Core assumption: Simple questions that all strong models can answer don't reveal model limitations
- Evidence anchors:
  - [section 2.2] "we filter some simple samples to discover the knowledge boundaries of the LLMs and improve the difficulty of Chinese SimpleQA"
  - [corpus] Weak evidence - related works focus on factuality but not specifically on adversarial difficulty filtering

### Mechanism 2
- Claim: Short-form answers enable reliable automated grading through existing LLM APIs
- Mechanism: By constraining answers to be concise and factual, grading can be performed by comparing predicted answers to reference answers without requiring human evaluation
- Core assumption: Short, unambiguous answers can be reliably compared using existing LLMs without hallucination
- Evidence anchors:
  - [abstract] "the grading process is easy-to-evaluate based on OpenAI API"
  - [section 2.1] "as the questions and answers are very short, the grading procedure is fast to run via existing LLMs"
  - [corpus] Strong evidence - SimpleQA and related benchmarks use similar short-form evaluation approaches

### Mechanism 3
- Claim: Larger models demonstrate better factuality calibration and performance
- Mechanism: Model size correlates with both absolute performance and confidence calibration accuracy
- Core assumption: Larger models have more comprehensive parametric knowledge and better self-awareness of their knowledge boundaries
- Evidence anchors:
  - [section 3.3.1] "larger model sizes result in better calibration"
  - [section 3.2] "we observe that better performance is obtained when the model is larger"
  - [corpus] Moderate evidence - general trend in LLM research supports model scaling benefits

## Foundational Learning

- Chinese language understanding
  - Why needed here: The benchmark specifically evaluates Chinese factuality, requiring understanding of Chinese cultural context and linguistic nuances
  - Quick check question: Can you identify the difference between simplified and traditional Chinese characters used in the dataset?

- Factuality evaluation metrics
  - Why needed here: Understanding correct vs incorrect vs not attempted classification is crucial for interpreting benchmark results
  - Quick check question: What's the difference between "Correct" and "Correct given attempted" metrics?

- RAG (Retrieval-Augmented Generation)
  - Why needed here: The paper discusses RAG's significant impact on factuality performance
  - Quick check question: How does RAG improve factuality compared to relying solely on parametric knowledge?

## Architecture Onboarding

- Component map:
  - Data generation pipeline → Difficulty filtering → Quality control → Human verification → Evaluation
  - Question generation system → Answer validation → RAG integration → Automated grading API

- Critical path: Data collection → Quality control → Model evaluation → Analysis and insights

- Design tradeoffs:
  - Short vs long answers: Short answers enable easy grading but may miss nuanced information
  - Static vs dynamic answers: Static answers preserve evergreen property but may become outdated
  - Automated vs human grading: Automated grading is scalable but may have systematic biases

- Failure signatures:
  - Low sample retention rate (e.g., only 30% of generated data passes quality control)
  - Significant performance gaps between models that shouldn't exist
  - Calibration curves that consistently overestimate accuracy

- First 3 experiments:
  1. Test automated grading consistency by having multiple LLM APIs grade the same samples
  2. Evaluate RAG impact by comparing model performance with and without retrieval augmentation
  3. Test calibration accuracy by comparing stated confidence with actual accuracy across different confidence ranges

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Chinese-specific LLMs on Chinese SimpleQA compare to their performance on English benchmarks like SimpleQA, and what factors contribute to these differences?
- Basis in paper: [explicit] The paper states, "The performance of several LLMs focusing on Chinese (Doubao-pro-32k, and GLM-4-Plus) is close to the high-performance o1-preview. In particular, in the 'Chinese Culture' topic, these Chinese community LLMs are significantly better than GPT or o1 series models."
- Why unresolved: While the paper provides some insights into the performance differences, it does not delve into the specific factors that contribute to these differences, such as the training data, model architecture, or alignment strategies used by Chinese-specific LLMs.
- What evidence would resolve it: A detailed comparative analysis of the training data, model architectures, and alignment strategies of Chinese-specific LLMs and their English counterparts, along with their performance on both Chinese SimpleQA and English benchmarks like SimpleQA.

### Open Question 2
- Question: How does the effectiveness of RAG vary across different LLMs and topics in Chinese SimpleQA, and what factors influence its impact on performance?
- Basis in paper: [explicit] The paper states, "When introducing the RAG strategy into existing LLMs, the performance gaps between different LLMs decrease a lot. For example, for GPT-4o and Qwen2.5-3B, the performance gap decreases from 42.4% to 9.3% within using RAG."
- Why unresolved: While the paper demonstrates the effectiveness of RAG in improving performance and reducing performance gaps, it does not explore how the effectiveness of RAG varies across different LLMs and topics, or what factors influence its impact on performance.
- What evidence would resolve it: A comprehensive study analyzing the effectiveness of RAG across different LLMs and topics in Chinese SimpleQA, considering factors such as the size of the LLM, the complexity of the topic, and the quality of the retrieval data.

### Open Question 3
- Question: How does the "alignment tax" affect the performance of LLMs on Chinese SimpleQA, and what strategies can be employed to mitigate its negative impact on factuality?
- Basis in paper: [explicit] The paper states, "Existing alignment or post-training strategies usually decrease the factuality of language models" and "the alignment training of most current LLMs still has obvious drawbacks to produce knowledge hallucinations."
- Why unresolved: While the paper acknowledges the existence of the "alignment tax" and its negative impact on factuality, it does not explore the specific mechanisms through which alignment affects performance on Chinese SimpleQA or propose strategies to mitigate its negative impact.
- What evidence would resolve it: An in-depth investigation into the relationship between alignment strategies and performance on Chinese SimpleQA, along with the development and evaluation of new alignment techniques that prioritize factuality without sacrificing other desirable properties.

## Limitations
- Quality control relies heavily on human verification with undisclosed inter-rater reliability metrics
- Evaluation assumes short, unambiguous answers are sufficient to assess factuality
- OpenAI API dependency for automated grading may not generalize well to other languages or domains

## Confidence

**High Confidence**: The claim that larger models demonstrate better factuality calibration and performance is well-supported by the experimental results and aligns with established scaling laws in LLM research. The observation that RAG significantly improves accuracy is also strongly supported by the data.

**Medium Confidence**: The assertion that adversarial question generation is necessary to probe knowledge boundaries is reasonable but relies on the assumption that the difficulty filtering process was optimally calibrated. The effectiveness of short-form answers for reliable automated grading is supported by related works but may have language-specific limitations.

**Low Confidence**: The claim about the benchmark's difficulty level being appropriate for revealing model limitations is based on internal testing but lacks external validation. The generalizability of findings to other languages or domains beyond Chinese factuality remains uncertain.

## Next Checks
1. Run the same samples through multiple independent LLM grading APIs (OpenAI, Claude, Gemini) to measure inter-annotator agreement and identify systematic grading biases.

2. Re-evaluate a subset of questions after 6 months to measure answer drift and validate the "timelessness" property of the benchmark, identifying questions that become outdated or ambiguous.

3. Adapt 100 Chinese SimpleQA questions to English and evaluate the same models to measure language-specific vs general factuality capabilities, identifying whether findings generalize beyond Chinese.