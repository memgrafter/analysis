---
ver: rpa2
title: 'On the Generalization Capability of Temporal Graph Learning Algorithms: Theoretical
  Insights and a Simpler Method'
arxiv_id: '2402.16387'
source_url: https://arxiv.org/abs/2402.16387
tags:
- generalization
- temporal
- graph
- have
- stone
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the generalization ability of temporal graph
  learning algorithms by deriving bounds on the expected 0-1 error in the finite-wide
  over-parameterized regime. The key finding is that generalization error decreases
  with more training data but increases with the number of layers/steps in GNN-/RNN-based
  methods and feature-label alignment (FLA).
---

# On the Generalization Capability of Temporal Graph Learning Algorithms: Theoretical Insights and a Simpler Method

## Quick Facts
- arXiv ID: 2402.16387
- Source URL: https://arxiv.org/abs/2402.16387
- Reference count: 40
- Primary result: Derives generalization bounds for temporal graph learning algorithms and proposes SToNe, a simpler method with competitive performance

## Executive Summary
This paper investigates the generalization ability of temporal graph learning (TGL) algorithms by deriving theoretical bounds on expected 0-1 error in the over-parameterized regime. The authors establish that generalization error decreases with more training data but increases with the number of layers/steps in GNN-/RNN-based methods. They introduce Feature-Label Alignment (FLA) as a proxy for expressive power to explain performance differences across TGL methods. Guided by these theoretical insights, they propose Simplified-Temporal-Graph-Network (SToNe), which achieves competitive performance with lower model complexity through a shallow architecture and careful input data selection.

## Method Summary
The paper analyzes TGL algorithms through the lens of neural tangent kernel theory in the over-parameterized regime. The authors derive generalization bounds showing the relationship between error, training data size, and model complexity (layers/steps). They introduce FLA as a metric for measuring expressive power alignment with ground-truth labels. Based on these insights, they propose SToNe, which uses 1-hop most recent neighbor aggregation with learnable weights in a shallow 1-layer GNN architecture. The method is trained using SGD with specific hyperparameters (learning rate 0.0001, weight decay 10^-6, batch size 600, hidden dimension 100).

## Key Results
- Generalization error bounds show inverse relationship with training data size and direct relationship with layers/steps in GNN/RNN methods
- FLA serves as effective proxy for expressive power, explaining performance differences across TGL methods
- SToNe achieves competitive link prediction performance with average precision scores comparable to or better than state-of-the-art methods while using fewer layers
- Theoretical predictions about generalization error and FLA align with empirical observations on multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generalization error decreases with more training data but increases with the number of layers/steps in GNN/RNN-based methods.
- Mechanism: The generalization bound is inversely proportional to the square root of the number of training samples (N) and directly proportional to the product of the number of layers/steps (L) and the Lipschitz constant of the activation function (Ï).
- Core assumption: The model is sufficiently over-parameterized and the data generation process can be approximated as stationary.
- Evidence anchors: [abstract] and [section 3.2] establish the connection between generalization error and layers/steps.

### Mechanism 2
- Claim: Feature-label alignment (FLA) serves as a proxy for expressive power and explains the performance of memory-based methods.
- Mechanism: FLA measures how well representations align with ground-truth labels, with smaller FLA indicating better alignment and generalization.
- Core assumption: FLA is a meaningful proxy for expressive power that can be computed accurately.
- Evidence anchors: [abstract] and [section 3.3] show FLA's role as expressive power proxy and explanation for memory-based method performance.

### Mechanism 3
- Claim: SToNe achieves competitive performance with lower complexity through shallow architecture and proper input data selection.
- Mechanism: SToNe uses 1-hop most recent neighbor aggregation with learnable weights, avoiding deep architectures and memory blocks that increase FLA.
- Core assumption: Recent neighbors contain more relevant information for link prediction than uniformly sampled or distant neighbors.
- Evidence anchors: [abstract] and [section 4.1] describe SToNe's design choices guided by theoretical analysis.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its role in generalization analysis
  - Why needed here: NTK is used to analyze the generalization ability of over-parameterized neural networks, which is the regime considered in this paper.
  - Quick check question: What is the relationship between NTK and the feature-label alignment (FLA) score?

- Concept: Weisfeiler-Lehman (WL) test and its connection to expressive power
  - Why needed here: The WL test is used to measure the expressive power of graph neural networks, and understanding its limitations helps explain why FLA is proposed as an alternative proxy for expressive power.
  - Quick check question: What are the limitations of using the WL test to analyze the expressive power of temporal graph learning algorithms?

- Concept: Temporal graph representation and the different types of temporal graph learning methods
  - Why needed here: Understanding the different types of temporal graph learning methods (memory-based, GNN-based, RNN-based, etc.) and their characteristics is crucial for interpreting the theoretical analysis and experimental results.
  - Quick check question: What are the key differences between memory-based and GNN-based temporal graph learning methods in terms of how they capture temporal information?

## Architecture Onboarding

- Component map: Input temporal graph data -> Identify K most recent temporal neighbors -> 1-layer GNN with learnable aggregation weights -> Link prediction scores

- Critical path:
  1. Preprocess temporal graph data to identify K most recent temporal neighbors for each node at each time step
  2. Compute node representations using the 1-layer GNN with learnable aggregation weights
  3. Use the computed node representations for link prediction

- Design tradeoffs:
  - Using recent neighbors vs. uniformly sampled neighbors: Recent neighbors may contain more relevant information but may also introduce bias
  - Learnable aggregation weights vs. fixed weights: Learnable weights allow adaptive learning of temporal neighbor importance but increase model complexity
  - Shallow architecture vs. deep architecture: Shallow architecture reduces generalization error but may limit expressive power

- Failure signatures:
  - High generalization error: May indicate under-parameterization or highly non-stationary data
  - Poor link prediction performance: May indicate irrelevant recent neighbors or ineffective learnable aggregation weights

- First 3 experiments:
  1. Compare SToNe performance with different values of K (number of recent neighbors) to find optimal value
  2. Compare SToNe with learnable aggregation weights to SToNe with fixed aggregation weights
  3. Compare SToNe to other temporal graph learning methods across multiple datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the feature-label alignment (FLA) score vary across different temporal graph learning tasks and how does this variation impact model selection?
- Basis in paper: [explicit] The paper discusses FLA as a proxy for expressive power but focuses primarily on link prediction tasks.
- Why unresolved: The paper does not explore FLA's behavior across diverse temporal graph learning tasks.
- What evidence would resolve it: Empirical studies comparing FLA scores and model performance across multiple temporal graph learning tasks.

### Open Question 2
- Question: What is the precise relationship between the number of layers/steps in GNN/RNN-based methods and their generalization ability when using non-uniform aggregation weights?
- Basis in paper: [explicit] The paper derives generalization bounds for methods with uniform aggregation weights but does not explicitly analyze non-uniform aggregation.
- Why unresolved: The theoretical analysis assumes uniform aggregation weights, limiting applicability to methods like TGAT that use attention mechanisms.
- What evidence would resolve it: Extending the theoretical framework to incorporate non-uniform aggregation weights and deriving corresponding generalization bounds.

### Open Question 3
- Question: How does the choice of time-encoding function impact the generalization ability of temporal graph learning algorithms, and can optimal time-encoding functions be learned?
- Basis in paper: [explicit] The paper uses a fixed time-encoding function but does not explore its impact on generalization or the possibility of learning optimal encodings.
- Why unresolved: The paper focuses on other aspects of temporal graph learning and does not investigate the role of time-encoding functions in detail.
- What evidence would resolve it: Empirical studies comparing different time-encoding functions and their impact on model performance, as well as methods for learning optimal time-encodings.

## Limitations
- Theoretical bounds rely on over-parameterized regime assumption which may not hold in all practical scenarios
- Analysis assumes stationary data generation, but real-world temporal graphs often exhibit non-stationary dynamics
- FLA's relationship to generalization performance in different contexts requires further validation

## Confidence
- **High Confidence**: Empirical results showing SToNe's competitive performance against baselines
- **Medium Confidence**: Theoretical generalization bounds, dependent on specific assumptions about over-parameterization and data stationarity
- **Medium Confidence**: FLA as a reliable proxy for expressive power, requiring more extensive empirical validation

## Next Checks
1. Test the generalization bounds on temporal graphs with known non-stationary dynamics to assess the impact of the stationarity assumption on bound validity
2. Conduct ablation studies on FLA computation, including scenarios with noisy labels and adversarial examples, to validate FLA's robustness as an expressive power proxy
3. Compare SToNe's performance across a broader range of temporal graph tasks beyond link prediction, such as node classification and graph classification, to evaluate its generalizability