---
ver: rpa2
title: 'Tell, Don''t Show!: Language Guidance Eases Transfer Across Domains in Images
  and Videos'
arxiv_id: '2403.05535'
source_url: https://arxiv.org/abs/2403.05535
tags:
- domain
- text
- transfer
- adaptation
- lagtran
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LaGTran, a method for improving unsupervised
  domain adaptation by leveraging text descriptions available for both source and
  target data. LaGTran first trains a text classifier on source image captions and
  labels, then uses this model to generate pseudo-labels for target captions, which
  are in turn used to supervise the corresponding target images.
---

# Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos

## Quick Facts
- arXiv ID: 2403.05535
- Source URL: https://arxiv.org/abs/2403.05535
- Reference count: 40
- Primary result: LaGTran significantly improves unsupervised domain adaptation using text descriptions, achieving state-of-the-art performance

## Executive Summary
LaGTran introduces a novel approach to unsupervised domain adaptation (UDA) by leveraging text descriptions available for both source and target data. The method first trains a text classifier on source image captions and labels, then uses this model to generate pseudo-labels for target captions, which are in turn used to supervise the corresponding target images. This language-guided transfer mechanism enables effective domain adaptation across challenging visual domain shifts in both images and videos.

## Method Summary
LaGTran employs a two-stage training process for unsupervised domain adaptation. First, it trains a text classifier on source domain image-caption pairs and their corresponding labels. This text classifier is then applied to target domain captions to generate pseudo-labels. These pseudo-labels are used to supervise the target domain images through a consistency loss. The method can be applied to both closed-set and open-world domain adaptation scenarios, with the latter handling unknown classes during adaptation. LaGTran is evaluated on image classification tasks across challenging domain shifts (GeoNet, DomainNet) and a new video domain adaptation benchmark (Ego2Exo).

## Key Results
- LaGTran significantly outperforms existing UDA methods, achieving state-of-the-art results
- On GeoNet, LaGTran improves average accuracy by 10% over prior work
- The method shows strong performance in transferring between ego and exo viewpoints in videos on the new Ego2Exo benchmark

## Why This Works (Mechanism)
LaGTran leverages the semantic information present in text descriptions to bridge the domain gap between source and target data. By using a text classifier trained on the source domain to generate pseudo-labels for target domain captions, the method can effectively transfer knowledge across domains even when visual features alone may not suffice. This approach is particularly powerful when dealing with challenging domain shifts where visual similarity may be misleading or insufficient for accurate classification.

## Foundational Learning
- **Unsupervised Domain Adaptation (UDA)**: Transferring knowledge from a labeled source domain to an unlabeled target domain. Needed to understand the core problem being addressed. Quick check: Can you explain the difference between UDA and supervised learning?
- **Pseudo-label generation**: Creating artificial labels for unlabeled data based on model predictions. Crucial for understanding how LaGTran supervises target domain data. Quick check: What are potential pitfalls of pseudo-label generation in UDA?
- **Open-world domain adaptation**: Handling unknown classes during domain adaptation. Important for understanding LaGTran's applicability to real-world scenarios. Quick check: How does open-world UDA differ from closed-set UDA in terms of evaluation metrics?
- **Domain shift in videos**: Challenges specific to transferring knowledge between different viewpoints or recording conditions in video data. Relevant for understanding the Ego2Exo benchmark. Quick check: What unique challenges does video domain adaptation present compared to image domain adaptation?

## Architecture Onboarding

**Component Map**: Text Classifier -> Pseudo-label Generator -> Image Classifier

**Critical Path**: Source image-caption pairs and labels → Train text classifier → Apply to target captions → Generate pseudo-labels → Supervise target images → Final image classifier

**Design Tradeoffs**: The method relies heavily on the availability of paired text descriptions for both source and target domains, which may not always be available in real-world scenarios. However, this tradeoff enables effective domain transfer even in challenging visual domain shifts.

**Failure Signatures**: Poor performance on target domain images when:
1. Text descriptions are not semantically aligned with image content
2. Domain shift between source and target is too large for text-based transfer to be effective
3. Text classifier overfits to source domain and fails to generalize to target domain captions

**First 3 Experiments to Run**:
1. Evaluate LaGTran on a new domain adaptation benchmark not used in the original paper to test generalizability
2. Conduct an ablation study removing the text guidance component to quantify its contribution to performance
3. Test LaGTran's performance when varying the percentage of target images with available captions (e.g., 0%, 50%, 100%)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but implicit questions arise from the results and methodology:
- How does LaGTran perform when text descriptions are only partially available or of lower quality?
- Can the method be extended to domains where text descriptions are not naturally available?
- How does the approach scale to extremely large domain shifts or multi-modal domain adaptation scenarios?

## Limitations
- LaGTran's performance gains rely heavily on the availability of paired text descriptions for both source and target domains, which may not hold in many real-world UDA scenarios
- The method's effectiveness across diverse domain shifts is demonstrated primarily on specific datasets (GeoNet, DomainNet, Ego2Exo), with uncertain generalization to other domains or tasks
- The open-world domain adaptation results are promising but less extensively validated compared to the closed-set results

## Confidence

**Major Claims and Confidence Levels:**
- LaGTran achieves state-of-the-art performance in UDA: **High confidence** - Results are clearly presented and show significant improvements over baselines across multiple datasets
- Text descriptions enable effective domain transfer: **Medium confidence** - While results support this claim, the method's reliance on available text data is a significant constraint not fully addressed
- LaGTran effectively handles open-world domain adaptation: **Medium confidence** - Promising results shown, but evaluation is less comprehensive compared to closed-set adaptation

## Next Checks
1. Test LaGTran's performance when only partial text descriptions are available (e.g., 50% of target images have captions) to assess robustness to incomplete text supervision
2. Evaluate LaGTran on additional domain adaptation benchmarks beyond images and videos, such as cross-lingual text classification or time-series domain shifts, to test generalizability
3. Conduct ablation studies to quantify the contribution of each component (text classifier, pseudo-label generation, etc.) to overall performance, and analyze failure cases where LaGTran underperforms