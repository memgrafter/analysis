---
ver: rpa2
title: Towards Retrieval-Augmented Architectures for Image Captioning
arxiv_id: '2405.13127'
source_url: https://arxiv.org/abs/2405.13127
tags:
- image
- captioning
- captions
- retrieved
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces retrieval-augmented Transformer architectures
  for image captioning, addressing the challenge of improving caption quality by leveraging
  external knowledge. The proposed approach utilizes an external kNN memory to retrieve
  relevant captions based on visual similarities, which are then used to enhance the
  generation process.
---

# Towards Retrieval-Augmented Architectures for Image Captioning

## Quick Facts
- arXiv ID: 2405.13127
- Source URL: https://arxiv.org/abs/2405.13127
- Authors: Sara Sarto; Marcella Cornia; Lorenzo Baraldi; Alessandro Nicolosi; Rita Cucchiara
- Reference count: 40
- Primary result: RA-TX achieves 136.7 CIDEr on COCO and 87.0 CIDEr on nocaps using retrieval-augmented captioning

## Executive Summary
This paper introduces retrieval-augmented Transformer architectures for image captioning that leverage external knowledge through kNN retrieval. The proposed approach retrieves relevant captions based on visual similarity and incorporates them into the generation process using either self-attention (RA-TS) or cross-attention with a learnable gate (RA-TX). Experimental results on COCO and nocaps datasets demonstrate that incorporating an external memory significantly improves caption quality, with RA-TX achieving state-of-the-art performance of 136.7 CIDEr on COCO and 87.0 CIDEr on nocaps.

## Method Summary
The method uses a two-stage training protocol with cross-entropy pre-training followed by CIDEr-based reinforcement learning. Two model variants are introduced: RA-TS employs self-attention connections to incorporate retrieved captions, while RA-TX uses cross-attention with a learnable gating mechanism. The retrieval index is built using CLIP visual embeddings and Faiss library for kNN search, with captions retrieved from either COCO or CC3M datasets. The caption generation model uses a 3-layer encoder-decoder Transformer with d=384 dimensions and 6 attention heads.

## Key Results
- RA-TX model achieves 136.7 CIDEr points on COCO and 87.0 CIDEr points on nocaps
- Using a larger retrieval corpus (CC3M vs COCO) further improves caption quality and semantic richness
- The learnable gating mechanism in RA-TX effectively balances contributions from retrieved captions and internal context
- Both RA-TS and RA-TX significantly outperform standard Transformer models without retrieval

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieval-augmented captioning improves caption quality by supplementing the model's internal knowledge with externally retrieved relevant captions.
- **Mechanism:** The model retrieves k nearest neighbor captions from an external memory based on visual similarity (using CLIP embeddings). These retrieved captions are then incorporated into the language model's input either via self-attention (RA-TS) or cross-attention with a gating mechanism (RA-TX), providing additional contextual information that enhances generation.
- **Core assumption:** The retrieved captions are semantically relevant to the input image and provide useful complementary information beyond what the model can generate internally.
- **Evidence anchors:**
  - [abstract]: "incorporating an external memory can significantly enhance the quality of captions, especially with a larger retrieval corpus"
  - [section]: "We experimentally validate our approach on COCO and nocaps datasets and demonstrate that incorporating an explicit external memory can significantly enhance the quality of captions"
  - [corpus]: Weak evidence - neighbor papers focus on retrieval-augmented approaches but don't specifically validate the mechanism for image captioning
- **Break condition:** If the retrieval index contains captions that are not semantically relevant or are of poor quality, the additional information could confuse the model and degrade performance.

### Mechanism 2
- **Claim:** Using a larger and higher-quality retrieval corpus (like CC3M with cleaned captions) leads to better caption quality than using a smaller corpus (like COCO).
- **Mechanism:** A larger corpus increases the likelihood of finding semantically relevant and diverse captions during retrieval. Higher-quality captions (cleaned CC3M vs original noisy alt-text) provide better training signals and more useful information during generation.
- **Core assumption:** The quality and size of the retrieval corpus directly impacts the relevance and usefulness of retrieved captions.
- **Evidence anchors:**
  - [abstract]: "especially with a larger retrieval corpus" and "using a richer and larger retrieval index... can further improve the quality and semantic richness of generated captions"
  - [section]: "we show that using a richer and larger retrieval index, such as one containing textual elements from CC3M, can further improve the quality and semantic richness of generated captions"
  - [corpus]: Weak evidence - neighbor papers don't specifically address the impact of retrieval corpus size/quality on captioning performance
- **Break condition:** If the retrieval corpus becomes too large relative to the computational resources, the efficiency gains from retrieval could be offset by increased computational cost.

### Mechanism 3
- **Claim:** The learned gating mechanism in RA-TX effectively balances the contribution of retrieved captions with the model's internal context.
- **Mechanism:** The gating mechanism (a sigmoid of a scalar parameter) controls the weight given to cross-attention outputs from retrieved captions versus self-attention outputs from the input sequence. This allows the model to dynamically adjust how much it relies on external knowledge versus its own understanding.
- **Core assumption:** The gating mechanism can learn to appropriately weight external knowledge based on the specific input and context.
- **Evidence anchors:**
  - [section]: "the contribution of retrieved captions is regulated by a learnable gating mechanism that combines the output of the kNN cross-attention layer with those of the standard self-attention over the input sequence"
  - [section]: "we devise a learnable gate, with which the model can regulate the importance of the output coming from the self-attention layer and that coming from the cross-attention layer"
  - [corpus]: No direct evidence - neighbor papers don't specifically discuss gating mechanisms in retrieval-augmented captioning
- **Break condition:** If the gating mechanism becomes biased toward either extreme (always 0 or always 1), it would either ignore useful external knowledge or fail to leverage the model's own capabilities.

## Foundational Learning

- **Concept: k-Nearest Neighbor (kNN) search**
  - Why needed here: The retrieval mechanism relies on finding the k most similar images (and their captions) based on visual similarity
  - Quick check question: What distance metric is typically used in kNN search for image retrieval, and why might it be appropriate here?

- **Concept: Transformer attention mechanisms**
  - Why needed here: Both model variants (RA-TS and RA-TX) use different attention configurations to incorporate retrieved captions
  - Quick check question: How does masked self-attention differ from cross-attention in Transformer architectures?

- **Concept: Visual embeddings and multimodal representations**
  - Why needed here: The model uses CLIP embeddings to represent images and compute visual similarity for retrieval
  - Quick check question: What advantage does using CLIP embeddings provide over traditional image classification backbones for this task?

## Architecture Onboarding

- **Component map:**
  Input image → CLIP visual encoder → grid features → kNN search → retrieved captions → External Memory Encoder (RA-TX only) → Caption Generation Model → output caption

- **Critical path:**
  1. Input image → CLIP visual encoder → grid features
  2. Grid features → kNN search → retrieved captions
  3. Retrieved captions + input tokens → caption generation model → output caption
  For RA-TX: Insert External Memory Encoder between steps 2 and 3

- **Design tradeoffs:**
  - RA-TS vs RA-TX: RA-TS is simpler but may struggle with longer retrieved captions; RA-TX offers more control but adds complexity
  - k value: Larger k provides more context but increases computational cost and may introduce noise
  - Retrieval corpus: Larger, higher-quality corpora improve performance but require more storage and slower retrieval

- **Failure signatures:**
  - Model ignores retrieved captions (gate approaches 0 in RA-TX, or self-attention dominates in RA-TS)
  - Generated captions become repetitive or overly similar to retrieved captions
  - Performance degrades with larger k values (overfitting to retrieved content)
  - Model fails to generalize to out-of-domain images

- **First 3 experiments:**
  1. Ablation study: Compare RA-TS and RA-TX with standard Transformer baseline on COCO validation set
  2. Retrieval quality analysis: Measure mean and max CIDEr scores of retrieved captions against ground truth for different k values
  3. Corpus size study: Compare performance using COCO vs CC3M retrieval index while keeping all other factors constant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of the embedding space impact the effectiveness of retrieval-augmented captioning models?
- Basis in paper: [explicit] The paper discusses that the quality of the captions plays a crucial role in determining the quality of the embedding space, and that even a smaller index with high-quality captions can potentially result in a better embedding space than a larger one with lower-quality descriptions.
- Why unresolved: The paper does not provide a detailed analysis of how different embedding space qualities affect the performance of the retrieval-augmented models.
- What evidence would resolve it: A comparative study of retrieval-augmented models using different embedding spaces (e.g., CLIP vs. other visual encoders) and their impact on caption quality would help resolve this question.

### Open Question 2
- Question: What is the optimal number of retrieved captions (k) for different image captioning tasks?
- Basis in paper: [explicit] The paper experiments with varying the number of retrieved captions (k = 5, 10, 20, 40) and finds that using k = 10 generally leads to the best performance in terms of evaluation metrics.
- Why unresolved: The optimal number of retrieved captions may vary depending on the specific image captioning task or dataset.
- What evidence would resolve it: A comprehensive study analyzing the performance of retrieval-augmented models with different values of k across various image captioning tasks and datasets would help determine the optimal number of retrieved captions.

### Open Question 3
- Question: How do different architectural choices in retrieval-augmented models (e.g., RA-TS vs. RA-TX) impact their performance and efficiency?
- Basis in paper: [explicit] The paper introduces two model variants, RA-TS and RA-TX, and compares their performance, but does not provide a detailed analysis of the trade-offs between the two architectures.
- Why unresolved: The paper does not explore the impact of different architectural choices on model performance, efficiency, and scalability.
- What evidence would resolve it: A thorough comparison of the two model variants, including their performance, computational complexity, and scalability, would help understand the trade-offs between the different architectural choices.

## Limitations
- The paper does not provide qualitative analysis of generated captions to verify why RA-TX outperforms RA-TS beyond performance metrics
- Limited ablation studies isolating the impact of corpus size and quality from other variables like domain differences
- No analysis of how the learned gating mechanism behaves across different types of images and retrieval scenarios

## Confidence
- **High Confidence:** Core architectural contributions and experimental setup are well-defined with clear performance improvements
- **Medium Confidence:** Claims about larger retrieval corpora improving quality are supported but lack isolated ablation studies
- **Low Confidence:** Mechanism behind RA-TX's superiority over RA-TS is not verified through qualitative analysis

## Next Checks
1. **Retrieval Quality Analysis:** Conduct oracle evaluation by measuring the maximum CIDEr score among retrieved captions for various k values on the COCO validation set to quantify the upper bound of what retrieval can provide.

2. **Corpus Ablation Study:** Compare model performance using COCO-only retrieval index versus CC3M-only retrieval index while keeping all other factors constant to isolate the impact of corpus size and quality.

3. **Gating Mechanism Analysis:** Visualize the learned gate values across different examples in the validation set and test model behavior when the gate is fixed to extreme values (0 or 1) to verify the gating mechanism provides meaningful control.