---
ver: rpa2
title: Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language
  (CLIP) Models
arxiv_id: '2410.03176'
source_url: https://arxiv.org/abs/2410.03176
tags:
- clip
- object
- objects
- image
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates object hallucinations in pretrained CLIP
  models and proposes a method to mitigate them. The authors create a benchmark called
  OHD-Caps to evaluate object hallucination issues in CLIP models, finding that even
  vanilla CLIP models exhibit severe object hallucination problems.
---

# Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models

## Quick Facts
- **arXiv ID**: 2410.03176
- **Source URL**: https://arxiv.org/abs/2410.03176
- **Authors**: Yufang Liu; Tao Ji; Changzhi Sun; Yuanbin Wu; Aimin Zhou
- **Reference count**: 14
- **Primary result**: CLIP models exhibit severe object hallucination problems even in isolation, with fine-grained contrastive loss improving hallucination detection from 14.3% to 82.5% accuracy

## Executive Summary
This paper investigates object hallucinations in pretrained CLIP models, revealing that even vanilla CLIP exhibits significant hallucination issues despite its strong zero-shot performance. The authors create a new benchmark called OHD-Caps to evaluate object hallucination problems and propose a fine-grained object-level contrastive loss method to mitigate these issues. Their approach significantly improves CLIP's ability to distinguish between real and hallucinated objects while maintaining comparable zero-shot performance. The enhanced CLIP model, when used as a visual encoder in LLaVA-1.5, effectively reduces object hallucinations in downstream large vision-language models.

## Method Summary
The authors address CLIP's object hallucination issues by modifying its pretraining contrastive loss to be more fine-grained at the object level. They create negative samples by adding hallucinatory objects, removing existing objects, or altering object attributes in captions. The modified contrastive loss incorporates these negatives along with margin-based terms to ensure the model assigns higher similarity to correct image-text pairs than to perturbed ones. This fine-tuning process improves the model's ability to detect object hallucinations without significantly impacting its general vision-language capabilities.

## Key Results
- CLIP's baseline hallucination detection accuracy is only 14.3% on average across datasets
- The fine-grained contrastive loss method improves detection accuracy to 82.5% on average
- Enhanced CLIP maintains comparable zero-shot performance to vanilla models (65.6 vs 66.0)
- When used as visual encoder in LLaVA-1.5, the enhanced model significantly reduces object hallucinations in downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CLIP's object hallucination stems from its pretraining contrastive loss being too coarse-grained for fine object-level distinctions.
- **Mechanism**: Standard CLIP training uses negative samples from other images in the batch, making it easy to distinguish mismatched image-text pairs. This doesn't train CLIP to detect subtle object-level errors where objects are added, removed, or altered within the same image context.
- **Core assumption**: CLIP's ability to detect hallucinations depends on training signals that distinguish between positive samples and carefully crafted negative samples with object-level perturbations.
- **Evidence anchors**:
  - [abstract] "We unveil that even in isolation, the CLIP model is prone to object hallucinations"
  - [section 3.2] "the vanilla CLIP models perform poorly across all three datasets, indicating their limited ability to recognize illusory objects in images"
  - [corpus] "Hallucinations in large vision-language models (LVLMs) are a significant challenge, i.e., generating objects that are not presented in the visual input"
- **Break condition**: If the negative samples don't effectively capture the full range of hallucination types (addition, removal, alteration), the fine-grained contrastive loss won't address all hallucination modes.

### Mechanism 2
- **Claim**: The fine-grained object-level contrastive loss explicitly trains CLIP to distinguish between real and hallucinated objects by creating negative samples with controlled perturbations.
- **Mechanism**: The authors create negative samples by adding hallucinatory objects, removing existing objects, or altering object attributes in captions. They then modify the contrastive loss to include these negatives and add margin-based losses to ensure the model assigns higher similarity to correct image-text pairs than to perturbed ones.
- **Core assumption**: By training on these carefully constructed negatives, CLIP learns to recognize fine-grained differences in object presence/absence that the original pretraining didn't capture.
- **Evidence anchors**:
  - [section 4] "With the addition of the negative samples Tneg created as in the previous section, we could modify the loss Li2t"
  - [section 5.1] "Our model shows comparable zero-shot performance to vanilla CLIP Models (65.6 vs 66.0) and achieves significant improvements in hallucination recognition (14.3 vs 82.5)"
- **Break condition**: If the margin thresholds (τ1, τ2) are poorly calibrated, the model may either not learn to distinguish hallucinations or may overfit to the training data.

### Mechanism 3
- **Claim**: The enhanced CLIP model reduces hallucinations in downstream LVLMs by providing better visual grounding.
- **Mechanism**: LVLMs like LLaVA-1.5 use CLIP as their visual encoder. By improving CLIP's ability to detect hallucinations, the downstream model inherits this capability and produces more accurate object-related responses.
- **Core assumption**: The hallucination problem in LVLMs is partially rooted in the visual encoder's inability to accurately represent object presence, not just in the language model's generation.
- **Evidence anchors**:
  - [abstract] "we show that the enhanced model can be employed as a visual encoder, effectively alleviating the object hallucination issue in LVLMs"
  - [section 5.2] "For discriminative responses, our model achieves significant improvements on various datasets"
- **Break condition**: If LLVMs use additional components beyond CLIP (like cross-attention modules) that independently introduce hallucinations, fixing CLIP alone won't eliminate all hallucination issues.

## Foundational Learning

- **Concept**: Contrastive learning and loss functions
  - **Why needed here**: Understanding how CLIP's pretraining works and how the authors modify it is crucial to grasping the proposed solution.
  - **Quick check question**: What is the difference between the original CLIP contrastive loss and the modified loss with object-level negatives and margin terms?

- **Concept**: Multimodal hallucination in AI systems
  - **Why needed here**: The paper addresses a specific type of hallucination (object hallucination) in multimodal models, which requires understanding what hallucinations are and why they occur in vision-language systems.
  - **Quick check question**: What distinguishes object hallucination from other types like spatial or attribute hallucination?

- **Concept**: Zero-shot generalization in vision-language models
  - **Why needed here**: The paper evaluates the enhanced CLIP model on zero-shot tasks to ensure the fine-tuning doesn't hurt general performance while improving hallucination detection.
  - **Quick check question**: How does the paper demonstrate that the fine-tuning for hallucination detection doesn't harm the model's ability to perform other vision-language tasks?

## Architecture Onboarding

- **Component map**: Image + Caption pairs -> Visual Encoder (CLIP) -> Language Model (LLaVA-1.5) -> Enhanced hallucination detection and reduced hallucinations in downstream tasks

- **Critical path**:
  1. Create OHD-Caps benchmark with positive samples and negative samples containing hallucinated objects
  2. Fine-tune CLIP using modified contrastive loss with object-level negatives and margin terms
  3. Replace CLIP in LLaVA-1.5 with enhanced version
  4. Evaluate hallucination detection and downstream performance

- **Design tradeoffs**:
  - Training data: Creating high-quality negative samples with hallucinated objects requires significant effort and domain expertise
  - Model complexity: Adding margin losses increases training complexity but improves hallucination detection
  - Generalization vs specialization: Fine-tuning for hallucination detection could potentially harm general vision-language capabilities

- **Failure signatures**:
  - If the model still performs poorly on hallucination detection benchmarks after fine-tuning
  - If zero-shot performance significantly degrades after fine-tuning
  - If downstream LLVMs still exhibit hallucinations despite using the enhanced CLIP

- **First 3 experiments**:
  1. Evaluate baseline CLIP performance on OHD-Caps benchmark to establish hallucination baseline
  2. Fine-tune CLIP with modified contrastive loss and evaluate on OHD-Caps to verify improvement
  3. Replace CLIP in LLaVA-1.5 with enhanced version and evaluate hallucination reduction in downstream tasks

## Open Questions the Paper Calls Out
None

## Limitations
- **Dataset Generalization**: The evaluation is limited to three datasets, with 14.3% to 82.5% improvement range suggesting substantial variability across datasets
- **Zero-shot Performance Trade-off**: The narrow margin (65.6 vs 66.0) raises concerns about potential degradation in other vision-language tasks
- **Downstream Integration Complexity**: Fixing CLIP alone may not fully address hallucination problems if LLVMs have other hallucination sources

## Confidence

**High Confidence**: The experimental results demonstrating CLIP's baseline hallucination issues (14.3% detection accuracy) and the effectiveness of the fine-grained contrastive loss in improving detection (82.5% accuracy) are well-supported by the OHD-Caps benchmark.

**Medium Confidence**: The claim about maintaining "comparable" zero-shot performance has moderate confidence, as the performance difference is small (0.4 points) and the long-term impact on other vision-language tasks is not fully characterized.

**Low Confidence**: The downstream integration claim has lower confidence because it assumes a linear relationship between visual encoder quality and hallucination reduction in LLaVA-1.5, without fully accounting for other hallucination sources in the LVLM architecture.

## Next Checks

1. **Cross-dataset Generalization Test**: Evaluate the enhanced CLIP model on additional hallucination detection benchmarks beyond the three datasets used in the paper to verify consistent performance improvements across diverse data distributions.

2. **Ablation Study on Margin Parameters**: Systematically vary the margin parameters (τ1, τ2) in the fine-grained contrastive loss to determine their optimal values and test whether the improvements are robust to these hyperparameter choices.

3. **Full LVLM Architecture Analysis**: Replace CLIP in LLaVA-1.5 with the enhanced version and conduct controlled experiments to isolate the contribution of visual encoder improvements versus other components (like cross-attention modules) to the overall hallucination reduction.