---
ver: rpa2
title: 'Reasoning about concepts with LLMs: Inconsistencies abound'
arxiv_id: '2405.20163'
source_url: https://arxiv.org/abs/2405.20163
tags:
- knowledge
- surgeon
- llms
- medical
- pediatric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates conceptual consistency in large language
  models (LLMs) using knowledge graphs. The authors create test clusters from Wikidata
  ontologies to systematically probe LLM knowledge of medical specialties and finance
  concepts.
---

# Reasoning about concepts with LLMs: Inconsistencies abound

## Quick Facts
- arXiv ID: 2405.20163
- Source URL: https://arxiv.org/abs/2405.20163
- Reference count: 12
- Primary result: Simple knowledge-graph based prompting strategies can reduce LLM inconsistency by up to 33%

## Executive Summary
This paper investigates conceptual consistency in large language models (LLMs) using knowledge graphs to systematically test how models handle concept hierarchies. The authors create test clusters from Wikidata ontologies to probe LLM knowledge of medical specialties and finance concepts, finding significant inconsistencies even in well-known models. While LLMs correctly answer individual questions in over 98% of cases, they show inconsistency when answering related clusters of questions. Using simple knowledge-graph based prompting strategies, the authors improve consistency by up to 33%, demonstrating that even basic ontologies can reveal conceptual inconsistencies across LLMs.

## Method Summary
The authors extract concept hierarchies from knowledge bases like Wikidata, generate question clusters that test properties like transitivity and inheritance, and systematically evaluate LLM consistency across these clusters. They create three types of test clusters: edge clusters testing direct relationships, path clusters testing transitive properties, and property inheritance clusters testing concept relationships. After identifying inconsistencies, they augment prompts with context from the knowledge graph to improve performance. The methodology provides a systematic approach for domain experts to evaluate and enhance concept coverage in LLMs.

## Key Results
- LLMs correctly answer individual questions in over 98% of cases but show inconsistency when answering related clusters
- Simple knowledge-graph based prompting strategies improve consistency by up to 33%
- Even simple ontologies can reveal significant conceptual inconsistencies across multiple LLMs
- Context augmentation is most effective when the LLM has baseline knowledge but lacks consistency in application

## Why This Works (Mechanism)

### Mechanism 1
Simple knowledge-graph based prompting strategies can reduce LLM inconsistency by up to 33%. The paper uses ontologies to generate question clusters that test properties like transitivity and inheritance. By explicitly providing missing knowledge as context, the LLM's performance improves significantly. Core assumption: LLMs have some baseline knowledge of the concepts but lack consistency in applying properties across related concepts. Evidence anchors: [abstract] "Using simple knowledge-graph based prompting strategies, they improve consistency by up to 33%"; [section] "With this simple prompt augmentation strategy, we obtain a sizable performance enhancement as shown in Table 3". Break condition: If the LLM completely lacks the conceptual knowledge (incomplete clusters), context augmentation won't help.

### Mechanism 2
Even simple ontologies can reveal significant conceptual inconsistencies across LLMs. The paper creates systematic test clusters from knowledge graphs that probe multiple linguistic formulations of the same concept relationship. Inconsistencies appear when LLMs answer these related questions differently. Core assumption: LLMs encode some knowledge of concept hierarchies but apply it inconsistently when queried with different linguistic forms. Evidence anchors: [abstract] "We show that even simple ontologies can be used to reveal conceptual inconsistencies across several LLMs"; [section] "If say, all questions except the third question are answered 'yes', there is obviously an inconsistency in the LLM knowledge". Break condition: If the LLM perfectly understands and applies concept hierarchy properties, no inconsistencies would be found.

### Mechanism 3
Knowledge graph integration with LLMs enables targeted evaluation and improvement of domain-specific concept coverage. The paper extracts concept hierarchies from knowledge bases like Wikidata, generates test cases, identifies inconsistencies, and uses context augmentation to improve performance. Core assumption: Domain experts need systematic methods to evaluate and enhance LLM knowledge in specialized domains. Evidence anchors: [abstract] "We also propose strategies that domain experts can use to evaluate and improve the coverage of key domain concepts in LLMs"; [section] "Our proposed approach to test and correct inconsistencies in an LLM's knowledge of concept hierarchies". Break condition: If domain concepts are too complex or numerous for manual ontology creation and testing.

## Foundational Learning

- Concept: Knowledge graphs and ontologies
  - Why needed here: The paper relies on knowledge graphs as a systematic way to define concept hierarchies and test conceptual consistency in LLMs
  - Quick check question: What is the difference between an instance and a subclass in a knowledge graph?

- Concept: Conceptualization properties (type, strict subconcept, transitivity, inheritance)
  - Why needed here: These properties form the basis for generating test clusters that identify inconsistencies in LLM knowledge
  - Quick check question: If A is a subconcept of B and B is a subconcept of C, what property guarantees A is a subconcept of C?

- Concept: Logical inconsistency and unsatisfiable sets
  - Why needed here: The paper defines inconsistency in terms of unsatisfiable sets of statements that cannot all be true simultaneously
  - Quick check question: What makes a set of statements unsatisfiable in the context of conceptual knowledge?

## Architecture Onboarding

- Component map: Knowledge base extraction -> Test case generation -> LLM evaluation -> Context augmentation
- Critical path: 1) Extract concept hierarchy from knowledge base; 2) Generate question clusters testing conceptualization properties; 3) Evaluate LLM consistency across clusters; 4) Augment prompt with missing context; 5) Re-evaluate for improvement
- Design tradeoffs: Simple vs. comprehensive ontologies (ease of testing vs. domain specificity); automated vs. manual test generation (scalability vs. targeting); context augmentation vs. fine-tuning (speed vs. permanence)
- Failure signatures: High incomplete cluster rates suggest fundamental knowledge gaps; persistent inconsistencies after context augmentation indicate deeper model limitations; performance degradation with context suggests overfitting or conflicting knowledge
- First 3 experiments: 1) Test a simple concept hierarchy (like the finance domain example) on multiple LLMs to establish baseline inconsistency rates; 2) Apply context augmentation to the same concept hierarchy and measure improvement; 3) Create a domain-specific ontology for a target application and test LLM consistency with realistic scenario questions

## Open Questions the Paper Calls Out

### Open Question 1
How can we systematically evaluate and improve the consistency of LLMs in specialized domains? Basis in paper: [explicit] The authors propose a three-step process: extracting concept hierarchies, creating test cases, and testing the language model to identify inconsistencies. Why unresolved: While the authors demonstrate that their approach can reveal inconsistencies and improve consistency, they do not provide a comprehensive framework for ongoing evaluation and improvement in real-world applications. What evidence would resolve it: A systematic evaluation framework that includes regular testing, context augmentation strategies, and methods for addressing inconsistencies in dynamic domains.

### Open Question 2
Can LLMs handle ambiguous contexts and non-committal answers consistently? Basis in paper: [inferred] The authors mention that some questions may have ambiguous contexts and suggest that LLMs should be able to handle non-committal answers. Why unresolved: The paper does not explore how LLMs perform with ambiguous questions or whether they can consistently provide non-committal answers. What evidence would resolve it: Experiments testing LLMs with ambiguous questions and analyzing their responses for consistency and appropriateness of non-committal answers.

### Open Question 3
What is the impact of domain-specific knowledge editing on LLM performance and consistency? Basis in paper: [explicit] The authors discuss the need for domain experts to edit LLM knowledge to ensure consistency in specialized applications. Why unresolved: The paper does not provide a detailed analysis of the impact of knowledge editing on LLM performance and consistency, nor does it explore the best practices for such editing. What evidence would resolve it: Studies comparing LLM performance and consistency before and after domain-specific knowledge editing, along with guidelines for effective editing strategies.

## Limitations

- The evaluation focuses on only two domains (medical specialties and finance), limiting generalizability to other domains
- Context augmentation improves consistency but doesn't eliminate all inconsistencies, suggesting limited effectiveness for deeper conceptual understanding
- The analysis relies on knowledge graphs from Wikidata, which may contain their own inconsistencies or miss relevant domain knowledge

## Confidence

- **High confidence**: The core finding that LLMs exhibit significant conceptual inconsistencies when answering related questions about concept hierarchies
- **Medium confidence**: The claim that simple context augmentation can improve consistency by up to 33%
- **Low confidence**: The assertion that this approach provides comprehensive strategies for domain experts to evaluate and improve LLM concept coverage

## Next Checks

1. Apply the same methodology to test LLMs on 3-5 additional domains (e.g., legal concepts, scientific disciplines, geographic hierarchies) to determine if inconsistency patterns are domain-agnostic
2. Systematically vary the context augmentation strategy across different LLM architectures and sizes to identify which types of context are most effective
3. Design experiments to test whether improving conceptual consistency through context augmentation leads to measurable improvements in downstream task performance