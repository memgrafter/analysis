---
ver: rpa2
title: Towards interfacing large language models with ASR systems using confidence
  measures and prompting
arxiv_id: '2407.21414'
source_url: https://arxiv.org/abs/2407.21414
tags:
- confidence
- speech
- errors
- llms
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using large language models (LLMs) to correct
  errors in automatic speech recognition (ASR) transcripts. The key challenge is avoiding
  introduction of errors into already accurate transcripts.
---

# Towards interfacing large language models with ASR systems using confidence measures and prompting

## Quick Facts
- **arXiv ID**: 2407.21414
- **Source URL**: https://arxiv.org/abs/2407.21414
- **Reference count**: 0
- **Primary result**: Confidence-based filtering with LLM correction improves WER for less competitive ASR systems, with optimal results using sentence-level or lowest-word confidence thresholds of 0.95 or 0.7 respectively

## Executive Summary
This paper explores using large language models (LLMs) to correct errors in automatic speech recognition (ASR) transcripts while avoiding introduction of errors into already accurate transcripts. The authors propose confidence-based filtering methods that use ASR confidence scores to selectively pass only lower-confidence transcripts to the LLM for correction. Experiments with Whisper ASR models and OpenAI ChatGPT models on LibriSpeech data show that this approach can improve WER for less competitive ASR systems, with optimal results when using sentence-level or lowest-word confidence thresholds of 0.95 or 0.7 respectively. For example, Whisper Tiny ASR performance improves from 8.13% to 6.55% WER on test-clean when corrected by gpt-3.5-turbo-0125 with lowest-word confidence filtering.

## Method Summary
The authors evaluate the performance of OpenAI ChatGPT models (gpt-3.5-turbo-1106, gpt-3.5-turbo-0125, and gpt-4-0125-preview) in correcting ASR errors from Whisper models (Tiny, Medium, and Large V3) on the LibriSpeech dataset. They implement three confidence-based filtering methods: sentence-level, lowest-word, and specific-word filtering, which determine which ASR transcripts are sent to the LLM for correction based on confidence score thresholds. The LLM is prompted with task description, input/output format, and example pairs to learn the correction task through few-shot learning. Performance is measured using Word Error Rate (WER) and Character Error Rate (CER) metrics.

## Key Results
- Whisper Tiny ASR performance improves from 8.13% to 6.55% WER on test-clean when corrected by gpt-3.5-turbo-0125 with lowest-word confidence filtering
- Optimal confidence thresholds are 0.95 for sentence-level filtering and 0.7 for lowest-word confidence filtering
- GPT-4 does not consistently outperform GPT-3.5 across different ASR model sizes
- Less competitive ASR models (Whisper Tiny and Medium) benefit more from LLM correction than competitive models (Whisper Large)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confidence-based filtering reduces error introduction by preventing LLM correction of already-accurate transcripts
- Mechanism: ASR confidence scores (sentence-level or lowest-word) are used as thresholds to determine which transcripts are sent to LLM for correction. Only transcripts below the threshold are corrected, while higher-confidence transcripts are kept as-is.
- Core assumption: ASR confidence scores are reliable indicators of transcription accuracy
- Evidence anchors:
  - [abstract] "To avoid introducing errors into likely accurate transcripts, we propose a range of confidence-based filtering methods"
  - [section] "To reduce the chance of the LLM introducing new errors into the transcript, we propose three filtering methods that rely on the ASR confidence scores"
  - [corpus] Weak evidence - The paper mentions using Whisper's confidence scores but doesn't provide validation that these scores correlate with actual accuracy

### Mechanism 2
- Claim: LLM can correct ASR errors when provided with contextually appropriate examples
- Mechanism: The LLM is prompted with task description, input/output format, and example pairs to learn the correction task through few-shot learning
- Core assumption: LLMs can generalize from provided examples to correct ASR-specific errors
- Evidence anchors:
  - [abstract] "instruction-tuned LLMs offer new possibilities for down-stream applications through their prompting mechanism"
  - [section] "LLMs perform best when the prompt contains a clear description of the task. For this reason, we provided information about the task, the format of the input and the expected output, and provided two examples in the prompt"
  - [corpus] Weak evidence - While the paper describes the prompting approach, it doesn't validate that the examples are sufficient for generalization

### Mechanism 3
- Claim: Lower-performance ASR models benefit more from LLM correction
- Mechanism: The gap between ASR output and reference is larger for less competitive models, providing more opportunity for LLM to improve accuracy
- Core assumption: There's a sweet spot where ASR performance is neither too good (leaving little room for improvement) nor too poor (making correction too difficult)
- Evidence anchors:
  - [section] "Our findings indicate that, despite its higher number of parameters, GPT-4 only outperforms GPT-3.5 for Whisper Tiny, but does not result in additional improvements in WER for the transcriptions of the Medium and Large models"
  - [section] "Less competitive ASR models — Whisper Tiny and Medium in our case — leave more room for improvement"
  - [corpus] No direct corpus evidence for this mechanism

## Foundational Learning

- **Concept**: Automatic Speech Recognition (ASR) confidence scoring
  - Why needed here: The paper relies on confidence scores from Whisper ASR to filter which transcripts should be corrected by the LLM
  - Quick check question: How does Whisper compute word-level and sentence-level confidence scores, and what do these scores represent?

- **Concept**: Large Language Model (LLM) few-shot learning through prompting
  - Why needed here: The paper uses few-shot prompting to teach the LLM the task of ASR error correction
  - Quick check question: What are the key components of an effective prompt for few-shot learning, and how do they influence the LLM's performance?

- **Concept**: Word Error Rate (WER) and Character Error Rate (CER) metrics
  - Why needed here: The paper uses WER and CER to evaluate the effectiveness of LLM corrections
  - Quick check question: How are WER and CER calculated, and why might they sometimes show different trends in evaluating ASR performance?

## Architecture Onboarding

- **Component map**: Whisper ASR system (Tiny, Medium, Large V3 models) -> Confidence-based filtering module (sentence-level, lowest-word, or specific-word methods) -> OpenAI ChatGPT models (gpt-3.5-turbo-1106, gpt-3.5-turbo-0125, gpt-4-0125-preview) -> Prompt engineering module (task description, format specification, examples)

- **Critical path**:
  1. ASR system generates transcription with confidence scores
  2. Confidence-based filtering determines which transcripts need correction
  3. Selected transcripts are passed to LLM with appropriate prompt
  4. LLM generates corrected transcription
  5. Results are evaluated using WER/CER metrics

- **Design tradeoffs**:
  - Confidence threshold selection: Higher thresholds reduce error introduction but may miss correction opportunities; lower thresholds increase correction opportunities but risk introducing errors
  - Prompt complexity: More detailed prompts may improve performance but increase computational cost and API usage
  - Model selection: GPT-4 has higher parameters but doesn't always outperform GPT-3.5 for this task

- **Failure signatures**:
  - High WER increase after LLM correction indicates over-correction or introduction of new errors
  - Low improvement rate suggests confidence thresholds are too conservative or prompts are ineffective
  - CER increases while WER decreases indicates the LLM is making character-level changes that don't improve overall word accuracy

- **First 3 experiments**:
  1. Compare WER before and after LLM correction across different confidence thresholds (0.5 to 1.0) to find optimal threshold
  2. Test different prompt variations (base prompt vs. with grammar instructions vs. with phonetic similarity instructions) to determine most effective prompt structure
  3. Evaluate performance across different Whisper model sizes (Tiny, Medium, Large) to confirm the hypothesis that lower-performance models benefit more from LLM correction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do confidence-based filtering methods impact LLM performance across different ASR model sizes and language varieties?
- Basis in paper: [explicit] The authors note that optimal confidence thresholds vary by ASR model size (e.g., 0.95 for Tiny, 0.7 for lowest-word confidence) and observe better results for less competitive ASR models.
- Why unresolved: The study only tested English using Whisper models; performance may differ for other languages or ASR architectures.
- What evidence would resolve it: Comparative experiments using non-Whisper ASR systems (e.g., DeepSpeech, Kaldi) and multilingual datasets across diverse language families.

### Open Question 2
- Question: What is the effect of applying LLM corrections to long-form, continuous speech rather than utterance-by-utterance processing?
- Basis in paper: [explicit] Authors state that "studies also need to be conducted on other languages, where LLMs might not perform as well as on English" and mention future work on "long-form datasets where utterances are not evaluated one-by-one."
- Why unresolved: Current evaluation uses LibriSpeech with discrete utterances; real-world applications involve continuous speech where context spans multiple utterances.
- What evidence would resolve it: End-to-end experiments on podcasts, meetings, or audiobooks where LLM corrections leverage cross-utterance context windows.

### Open Question 3
- Question: How does rescoring LLM-corrected transcripts with the original acoustic model affect error correction quality?
- Basis in paper: [explicit] Authors propose "LLM outputs could also be rescored again with the acoustic model to validate if the proposed changes are acoustically plausible."
- Why unresolved: This validation step was not implemented in the current study, leaving uncertainty about whether LLM corrections align with acoustic evidence.
- What evidence would resolve it: Ablation studies comparing WER/CER with and without acoustic rescoring of LLM corrections on the same test sets.

## Limitations

- The approach relies on ASR confidence scores as reliable indicators of transcription quality without independent validation of their correlation with actual accuracy
- The method shows improvement only for less competitive ASR models and fails to outperform competitive systems like Whisper Large
- The study is limited to English and LibriSpeech dataset, raising questions about generalization to other languages and speech domains

## Confidence

- **High Confidence**: The observation that GPT-4 doesn't consistently outperform GPT-3.5 for this task is well-supported by experimental results across multiple model sizes and datasets.
- **Medium Confidence**: The claim that lower-performance ASR models benefit more from LLM correction is supported by experimental evidence but lacks deeper theoretical justification for why this relationship holds.
- **Low Confidence**: The assertion that confidence-based filtering effectively prevents error introduction relies on assumptions about the reliability of ASR confidence scores without independent validation.

## Next Checks

1. Conduct cross-domain evaluation by testing the approach on non-LibriSpeech datasets (e.g., conversational speech, noisy environments) to assess generalization of confidence score reliability
2. Perform ablation studies comparing different confidence score formulations (sentence-level vs. word-level vs. hybrid) to determine optimal confidence measurement strategy
3. Investigate the impact of prompt engineering variations on correction quality, including testing prompts without examples to evaluate zero-shot learning capabilities versus few-shot performance