---
ver: rpa2
title: 'XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference'
arxiv_id: '2404.15420'
source_url: https://arxiv.org/abs/2404.15420
tags:
- context
- arxiv
- answer
- caching
- lama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces XC-CACHE, a cross-attention-based method to
  efficiently cache and condition language model generation on reference context without
  including it in the prompt. By adding lightweight cross-attention layers to pre-trained
  decoder-only models, XC-CACHE achieves comparable QA performance to fine-tuned ICL
  baselines while reducing the context cache memory footprint by over 98%, from 512
  kB to 8 kB per token.
---

# XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference

## Quick Facts
- arXiv ID: 2404.15420
- Source URL: https://arxiv.org/abs/2404.15420
- Reference count: 24
- Primary result: Reduces KV cache memory footprint by over 98% while maintaining QA performance comparable to fine-tuned ICL baselines

## Executive Summary
XC-CACHE introduces a novel approach to efficient LLM inference by using cross-attention layers to condition generation on reference context without including it in the prompt. The method achieves dramatic memory savings by replacing standard KV caching with cross-attention to pre-computed context encodings, reducing cache size from 512 kB to 8 kB per token. The approach maintains competitive performance on QA benchmarks while enabling offline context pre-processing and addressing the quadratic cost of self-attention operations in long contexts.

## Method Summary
XC-CACHE converts pre-trained decoder-only models (specifically LLaMA 2) into encoder-decoder architectures by adding cross-attention layers interleaved between self-attention layers. The method uses the frozen decoder as an encoder to extract context representations, which are then cached and used at inference time through cross-attention. Only the newly added cross-attention layers are trained, making this a parameter-efficient approach. The training involves QA and context repetition tasks using datasets like NATURAL QUESTIONS, HOTPOTQA, TOPICSQA, MS MARCO, and SQUAD-V2.

## Key Results
- Achieves 98% reduction in context cache memory footprint (from 512 kB to 8 kB per token)
- Maintains competitive F1 scores on QA benchmarks compared to fine-tuned ICL baselines
- Enables offline context pre-processing, reducing inference time complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XC-CACHE reduces cache memory footprint by two orders of magnitude compared to KV caching
- Mechanism: Replaces KV caching with cross-attention layers that condition generation on pre-computed context encodings
- Core assumption: Cross-attention layers can effectively condition generation on context encodings without requiring the full KV cache
- Evidence anchors: "drastically reduce the space footprint relative to standard KV caching by two orders of magnitude"

### Mechanism 2
- Claim: Decoder-as-encoder approach maintains good representations without additional training
- Mechanism: Uses frozen pre-trained decoder to extract context representations that work reasonably well as encoder outputs
- Core assumption: Pre-trained decoder representations capture sufficient semantic information to condition generation
- Evidence anchors: "XC-L LAMA...uses the frozen decoder as an encoder"

### Mechanism 3
- Claim: Fine-tuning only cross-attention layers (and optionally a small encoder) is sufficient for good performance
- Mechanism: Parameter-efficient approach where only newly added modules are trained while base decoder remains frozen
- Core assumption: Cross-attention layers can learn to effectively integrate context information without modifying the pre-trained decoder
- Evidence anchors: "we leverage pre-trained decoder-only models and only train a small number of added layers"

## Foundational Learning

- Concept: Cross-attention mechanism
  - Why needed here: Enables the decoder to attend to context representations rather than self-attending to the context directly
  - Quick check question: How does cross-attention differ from self-attention in terms of attention targets?

- Concept: KV caching optimization
  - Why needed here: Understanding the quadratic complexity of self-attention and how KV caching reduces this at inference time
  - Quick check question: What is the space complexity of KV caching per token for a model with L layers, H heads, and hidden dimension d?

- Concept: Parameter-efficient fine-tuning
  - Why needed here: The approach relies on fine-tuning only a small number of additional parameters rather than the full model
  - Quick check question: What is the typical parameter count for LoRA adapters compared to the full model?

## Architecture Onboarding

- Component map:
  Pre-trained decoder (LLaMA 2) -> Cross-attention layers -> Optional small encoder -> Context encoder output (cached representations) -> Language modeling head

- Critical path:
  1. Context processing through encoder
  2. Cache storage of encoder outputs
  3. Query processing through decoder with cross-attention to cached context
  4. Language modeling head output

- Design tradeoffs:
  - Memory vs. accuracy: Smaller cache size comes at a minor cost in accuracy
  - Complexity vs. simplicity: XC-L LAMA is simpler but XC-L LAMA ENC offers better accuracy
  - Inference speed vs. preprocessing: Offline context processing reduces inference time but requires preprocessing step

- Failure signatures:
  - Poor performance: May indicate ineffective cross-attention integration or insufficient encoder representations
  - Memory issues: Could suggest incorrect cache size calculations or improper cache management
  - Training instability: Might indicate issues with the training task mix or learning rate

- First 3 experiments:
  1. Compare performance of XC-L LAMA vs. standard ICL on a simple QA task with small context
  2. Measure cache size reduction and inference speed improvement on a larger context
  3. Test the impact of question-in-context encoding by prepending the question to the context encoder input

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but the limitations section suggests several areas for future research, including exploring the impact of different base models, investigating performance on out-of-distribution data, and extending the approach to handle multi-modal contexts.

## Limitations

- Performance trade-off: Minor accuracy degradation compared to full ICL baselines
- Out-of-distribution sensitivity: Performance drops on datasets significantly different from training distribution
- Preprocessing overhead: Requires offline context processing step that may not be suitable for dynamic contexts

## Confidence

- High confidence in the memory reduction claims (backed by specific cache size calculations)
- Medium confidence in the overall performance claims (consistent with reported metrics but acknowledges accuracy trade-offs)
- Medium confidence in the parameter-efficient fine-tuning approach (reasonable but not extensively validated against alternatives)

## Next Checks

1. **Cross-attention effectiveness test**: Implement a controlled experiment comparing XC-CACHE performance against standard KV caching on identical tasks, isolating the impact of cross-attention vs. memory reduction.

2. **Out-of-distribution robustness evaluation**: Systematically test the model on datasets significantly different from the training distribution to quantify the performance degradation mentioned for RepLiQA.

3. **Preprocessing overhead analysis**: Measure the actual wall-clock time and resource costs of the offline context preprocessing step versus the claimed inference speed improvements.