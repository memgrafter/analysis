---
ver: rpa2
title: Application of ASV for Voice Identification after VC and Duration Predictor
  Improvement in TTS Models
arxiv_id: '2406.19243'
source_url: https://arxiv.org/abs/2406.19243
tags:
- voice
- speaker
- number
- were
- employed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study explored ASV system capabilities for voice verification
  post-VC and duration prediction enhancement in TTS models. A multi-encoder ASV architecture
  (CQT, Mel-spectrogram, Pitch) using AM-Softmax loss was developed, achieving strong
  speaker discrimination (TPR@FPR=0.01: 0.592 on test data).'
---

# Application of ASV for Voice Identification after VC and Duration Predictor Improvement in TTS Models

## Quick Facts
- arXiv ID: 2406.19243
- Source URL: https://arxiv.org/abs/2406.19243
- Reference count: 0
- Achieved EER of 20.669 on converted speech verification in SSTC challenge

## Executive Summary
This study presents a multi-encoder automatic speaker verification (ASV) architecture designed to maintain voice identification accuracy after voice conversion (VC) and to improve phoneme duration prediction in text-to-speech (TTS) models. The system combines CQT, Mel-spectrogram, and Pitch encoders with AM-Softmax loss to produce robust speaker embeddings that remain discriminative under VC distortions. The same embeddings are shown to effectively predict phoneme durations in TTS pipelines, with utterance-level embeddings providing optimal performance. The approach demonstrates dual utility: achieving strong speaker verification (TPR@FPR=0.01: 0.592 on test data) and improving TTS duration prediction accuracy (MAE: 1.869, CCC: 0.8038).

## Method Summary
The method employs a multi-encoder ASV architecture that fuses CQT, Mel-spectrogram, and Pitch representations through separate encoders (Specblock for CQT, Vision Transformers for Mel and Pitch), concatenated and processed through a fully connected layer with AM-Softmax loss (scale=30, margin=0.4). The system is trained on LibriTTS-R and evaluated on speaker verification tasks including VC robustness and duration prediction for TTS. For duration prediction, speaker embeddings are concatenated with phoneme encodings to form input to a duration predictor network. The SSTC challenge results show the model's effectiveness on converted speech verification when combined with ensemble techniques.

## Key Results
- Multi-encoder ASV architecture achieves TPR@FPR=0.01 of 0.592 on test data
- ASV embeddings effectively predict phoneme durations in TTS (MAE: 1.869, CCC: 0.8038 with utterance embeddings)
- SSTC challenge results show EER reduced to 20.669 on converted speech verification using ensemble techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-encoder fusion (CQT, Mel, Pitch) improves ASV discrimination under VC conditions.
- Mechanism: Each encoder captures complementary acoustic cues—CQT for harmonic structure, Mel for timbral envelope, Pitch for F0 contours—which collectively retain speaker identity despite voice conversion distortions.
- Core assumption: VC does not fully erase all speaker-specific acoustic cues; fused embeddings can recover discriminative features.
- Evidence anchors:
  - [abstract] "A multi-encoder ASV architecture (CQT, Mel-spectrogram, Pitch) using AM-Softmax loss was developed, achieving strong speaker discrimination (TPR@FPR=0.01: 0.592 on test data)."
  - [section 2] Detailed encoder configurations and fusion strategy described.
- Break condition: If VC techniques systematically remove or randomize pitch, spectral envelope, and harmonic structure, fusion would fail to recover speaker identity.

### Mechanism 2
- Claim: ASV embeddings contain duration information useful for TTS phoneme duration prediction.
- Mechanism: Speaker embeddings encode speaking rate and rhythm patterns; concatenation with phoneme encodings allows the duration predictor to adjust phoneme lengths per speaker style.
- Core assumption: Speaker identity embeddings implicitly store speaking-rate and temporal articulation patterns.
- Evidence anchors:
  - [section 4.5] "utterance embeddings providing optimal performance (MAE: 1.869, CCC: 0.8038)" and "the embeddings of the same utterance exhibited the most optimal performance."
  - [corpus] Related works on temporal dynamics and duration modeling support this link.
- Break condition: If embeddings are trained to discard temporal features for invariance, duration cues would be absent.

### Mechanism 3
- Claim: AM-Softmax loss with margin=0.4 and scale=30 yields better speaker discrimination than ArcFace under the tested conditions.
- Mechanism: Additive margin increases inter-class distance while scale amplifies cosine similarity gradients, improving separability in embedding space.
- Core assumption: Larger margin and scale improve class discrimination without overfitting on the training set.
- Evidence anchors:
  - [section 4.3] "AM-Softmax" vs ArcFace results show TPR@FPR=0.01 of 0.2148 vs 0.04291.
  - [section 3] AM-Softmax equation and parameter choices given.
- Break condition: If margin is too large, intra-class compactness collapses; if scale too high, training becomes unstable.

## Foundational Learning

- Concept: Cosine similarity and TPR@FPR metrics
  - Why needed here: Evaluation of speaker verification requires thresholding cosine distances; TPR@FPR measures robustness at fixed false-positive rates.
  - Quick check question: If cosine similarity threshold is set too low, what happens to TPR and FPR?
- Concept: Voice conversion and its impact on speaker embeddings
  - Why needed here: Understanding how VC modifies acoustic features explains why multi-encoder fusion helps.
  - Quick check question: Which acoustic feature (pitch, timbre, rhythm) is most vulnerable to VC distortion?
- Concept: Embedding concatenation and feature fusion
  - Why needed here: Multi-encoder outputs must be combined to form a unified speaker representation.
  - Quick check question: What is the dimensionality after concatenating three 512-dim encoder outputs?

## Architecture Onboarding

- Component map:
  Audio → 3 encoders → concat → FC → AM-Softmax loss → embeddings
- Critical path:
  Audio → 3 encoders → concat → FC → AM-Softmax loss → embeddings
- Design tradeoffs:
  - Multi-encoder increases robustness but adds compute and memory cost.
  - AM-Softmax margin vs scale: margin increases inter-class gap, scale sharpens softmax decision boundary.
  - 4-second fixed input balances context and batch size.
- Failure signatures:
  - Low TPR@FPR indicates embeddings collapse under VC; check encoder fusion weights.
  - High validation loss with AM-Softmax suggests margin/scale too aggressive.
  - Duration predictor degrades when utterance embeddings are replaced with random noise.
- First 3 experiments:
  1. Train single-encoder baseline (Mel-only) and compare TPR@FPR to multi-encoder.
  2. Vary AM-Softmax margin (0.3, 0.4, 0.5) and record impact on TPR@FPR.
  3. Replace utterance embeddings with speaker embeddings in duration predictor and measure MAE/CCC change.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How generalizable are ASV embeddings for phoneme duration prediction across diverse languages and speaker demographics?
- Basis in paper: [inferred] The study tested embeddings on limited speaker datasets (5 speakers initially, then 18) and languages, but did not explore broader linguistic or demographic diversity.
- Why unresolved: The paper focuses on English speech data and a small set of speakers, leaving open whether embeddings retain predictive power for phoneme durations in other languages or speaker groups with different phonetic structures.
- What evidence would resolve it: Experiments using ASV embeddings trained on multilingual datasets and tested on diverse speaker populations, including non-English languages and varied age/gender demographics, would clarify generalizability.

### Open Question 2
- Question: What is the impact of voice conversion model fidelity on ASV-based speaker verification accuracy?
- Basis in paper: [explicit] The study achieved an EER of 20.669 on converted speech verification, but did not analyze how different VC model qualities affect performance.
- Why unresolved: The paper uses a fixed set of 16 VC models without characterizing their quality differences, making it unclear how ASV robustness varies with conversion fidelity or technique.
- What evidence would resolve it: Systematic testing of ASV models against VC systems of varying quality levels, with quantitative metrics linking conversion distortion to verification accuracy degradation.

### Open Question 3
- Question: How do different audio frontend representations (CQT, Mel-spectrogram, Pitch) contribute to ASV embedding effectiveness for downstream TTS tasks?
- Basis in paper: [explicit] The model combines three encoders with different frontends, but the paper does not isolate or analyze the individual contribution of each representation to duration prediction performance.
- Why unresolved: While the combined model shows good performance, ablation studies examining each frontend's impact on TTS duration prediction are absent, leaving uncertainty about which features are most critical.
- What evidence would resolve it: Ablation experiments comparing TTS duration prediction performance using embeddings from individual encoders versus the combined model, with statistical analysis of contribution significance.

## Limitations
- Multi-encoder architecture performance gains over single-encoder baselines are not explicitly validated
- AM-Softmax hyperparameter choices (margin=0.4, scale=30) were not systematically explored
- SSTC challenge results depend on ensemble methods with unspecified baseline models and ensembling coefficients

## Confidence

- **High Confidence**: The architectural framework (multi-encoder fusion, AM-Softmax loss formulation, utterance embedding extraction) is clearly specified and reproducible. The reported TPR@FPR and EER metrics follow standard evaluation protocols.
- **Medium Confidence**: The ASV model achieves stated performance metrics, but without single-encoder baselines or ablation studies, the contribution of individual components cannot be fully isolated. The duration prediction results show improvement, but the causal mechanism linking embeddings to temporal patterns requires further validation.
- **Low Confidence**: The SSTC challenge results depend on ensemble methods with unspecified baseline models and ensembling coefficients, making it difficult to attribute performance gains to the ASV architecture specifically.

## Next Checks
1. Train single-encoder baseline (Mel-only) and compare TPR@FPR to multi-encoder to quantify fusion benefits.
2. Systematically vary AM-Softmax margin (0.3, 0.4, 0.5) and scale (20, 30, 40) parameters while monitoring TPR@FPR and validation loss.
3. Conduct controlled experiments replacing utterance embeddings with speaker-only, random noise, or time-reversed embeddings to confirm that temporal patterns drive duration prediction improvements.