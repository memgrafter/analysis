---
ver: rpa2
title: 'DuoFormer: Leveraging Hierarchical Visual Representations by Local and Global
  Attention'
arxiv_id: '2407.13920'
source_url: https://arxiv.org/abs/2407.13920
tags:
- scale
- vision
- attention
- patch
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of Vision Transformers (ViTs)
  lacking inductive biases and performing poorly on small to medium-sized datasets,
  particularly in medical imaging tasks where multi-scale feature extraction is crucial.
  The authors propose DuoFormer, which integrates hierarchical visual representations
  from a CNN backbone with transformer-based processing.
---

# DuoFormer: Leveraging Hierarchical Visual Representations by Local and Global Attention

## Quick Facts
- arXiv ID: 2407.13920
- Source URL: https://arxiv.org/abs/2407.13920
- Authors: Xiaoya Tang; Bodong Zhang; Beatrice S. Knudsen; Tolga Tasdizen
- Reference count: 32
- Primary result: DuoFormer outperforms baseline ResNet models by over 2% and various Hybrid-ViT approaches on kidney cancer histopathology datasets, with up to 9.88% improvement over baselines when using self-supervised pre-trained backbones.

## Executive Summary
DuoFormer addresses the challenge of Vision Transformers underperforming on small to medium-sized datasets, particularly in medical imaging where multi-scale feature extraction is crucial. The method integrates hierarchical visual representations from CNN backbones with transformer-based processing through an innovative multi-scale patch tokenization approach. By introducing dual attention mechanisms (scale attention and patch attention) and a scale token initialized from hierarchical representations, DuoFormer achieves significant improvements in accuracy while maintaining computational efficiency, particularly for kidney cancer detection in histopathology images.

## Method Summary
DuoFormer combines hierarchical CNN features with transformer processing by extracting multi-scale representations from four CNN stages, projecting and concatenating them into a fixed-size token sequence. The method introduces a dual attention mechanism where scale attention captures cross-scale dependencies across the hierarchical features, while patch attention maintains global spatial perception. A scale token, initialized with fused embeddings from downsampled hierarchical representations, acts as a global aggregator guiding both attention mechanisms. The model is trained using Adam optimizer with OneCycle learning rate scheduling and demonstrates improved performance on small medical imaging datasets without requiring large-scale pre-training or additional transformer encoders.

## Key Results
- Achieved over 2% improvement over baseline ResNet models on kidney cancer histopathology datasets
- Showed up to 9.88% improvement over baselines when using self-supervised pre-trained backbones
- Demonstrated effectiveness on two datasets: Utah ccRCC and TCGA ccRCC with balanced accuracy as primary metric
- Maintained computational efficiency while integrating multi-scale features without additional pre-training tasks

## Why This Works (Mechanism)

### Mechanism 1
Multi-scale patch tokenization directly embeds hierarchical CNN features into the transformer token sequence, bypassing the need for large-scale pre-training. The method extracts four-stage hierarchical outputs from a CNN backbone, applies linear projection to match transformer embedding dimension, then splits and concatenates these across scales into a fixed-size token sequence. This preserves multi-scale spatial information in the token dimension. Core assumption: Concatenating features across CNN stages into multi-scale tokens will maintain rich hierarchical information and improve generalization on small datasets. Evidence: The abstract states the model "employs a CNN backbone to generate hierarchical visual representations... adapted for transformer input through an innovative patch tokenization." Break condition: If the concatenated token sequence becomes too large or unbalanced across scales, it could lead to overfitting or computational inefficiency.

### Mechanism 2
The scale attention mechanism allows the model to capture cross-scale dependencies without additional convolutional layers or self-supervised pre-training. Scale attention adapts the Multi-Head Self-Attention (MSA) framework by adding a scale dimension to the attention computation, processing tokens across scales simultaneously. Core assumption: Including a scale dimension in attention will allow effective integration of multi-scale information without explicit spatial priors. Evidence: The abstract mentions "scale attention" that "captures cross-scale dependencies, complementing patch attention to enhance spatial understanding." Break condition: If the scale dimension becomes too large relative to patches, attention computation could become inefficient or fail to learn meaningful cross-scale relationships.

### Mechanism 3
The scale token, initialized with fused embeddings from hierarchical representations, acts as a global aggregator that guides both scale and patch attention. After downsampling and concatenating embeddings from all CNN stages, a lightweight convolutional projection creates a scale token that is processed by scale attention to aggregate key details from all scales. Core assumption: A learnable scale token initialized from hierarchical features will provide a strong starting point for attention mechanisms and improve integration of local and global information. Evidence: The abstract states the "scale token... is initialized with a fused embedding derived from hierarchical representations... enriches the transformer's multi-granularity representation." Break condition: Poor initialization or aggressive downsampling could lead to loss of important information and degraded performance.

## Foundational Learning

- **Vision Transformers (ViTs) and their limitations on small datasets**
  - Why needed here: DuoFormer is designed to address ViTs' poor performance on small to medium-sized datasets by integrating hierarchical CNN features
  - Quick check question: Why do ViTs typically require large-scale pre-training to perform well, and how does this limitation impact their use in medical imaging?

- **Multi-scale feature extraction and its importance in medical imaging**
  - Why needed here: DuoFormer's key innovation is leveraging multi-scale features from CNN stages, crucial for histopathology image analysis where features at different scales (cell nuclei, vascular structures) are important
  - Quick check question: What are the challenges of multi-scale feature extraction in medical imaging, and how do traditional CNNs and ViTs approach this problem differently?

- **Attention mechanisms in transformers and their role in capturing spatial relationships**
  - Why needed here: DuoFormer uses dual attention mechanism (scale attention and patch attention) to capture both cross-scale dependencies and global spatial relationships
  - Quick check question: How does the Multi-Head Self-Attention (MSA) mechanism work in standard transformers, and what modifications does DuoFormer make to adapt it for multi-scale feature processing?

## Architecture Onboarding

- **Component map**: CNN Backbone -> Multi-scale Patch Tokenization -> Scale Token Creation -> Scale Attention -> Patch Attention -> Classification Head
- **Critical path**: Hierarchical CNN features are extracted, tokenized across scales, fused into a scale token, processed through dual attention mechanisms, then classified
- **Design tradeoffs**: Using hierarchical CNN features vs. learning features from scratch in ViT; balancing number of scales vs. computational complexity and overfitting risk; designing scale attention to capture cross-scale dependencies without excessive complexity
- **Failure signatures**: Poor performance on small datasets could indicate overfitting due to too many parameters; degraded performance on larger datasets might suggest ineffective multi-scale information utilization; computational inefficiency could arise from oversized token sequences or unoptimized attention mechanisms
- **First 3 experiments**: 1) Ablation study on number of CNN stages included in multi-scale tokenization to find optimal balance; 2) Comparison of DuoFormer's performance with and without scale token to evaluate its impact; 3) Evaluation on larger, more diverse dataset to assess generalizability beyond medical imaging domain

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important unresolved issues emerge from the research:

### Open Question 1
- **Question**: How does DuoFormer's performance scale when applied to extremely large histopathology datasets (e.g., >1 million images)?
- **Basis in paper**: The paper demonstrates effectiveness on two datasets (Utah ccRCC and TCGA ccRCC) but does not explore behavior on extremely large datasets relevant for clinical deployment at scale
- **Why unresolved**: Study focuses on small to medium-sized datasets, leaving massive dataset behavior unexplored
- **What evidence would resolve it**: Testing DuoFormer on large-scale histopathology dataset and comparing performance, efficiency, and training dynamics against existing methods

### Open Question 2
- **Question**: How does the inclusion of additional hierarchical stages (beyond Stage 3) impact DuoFormer's performance and computational efficiency?
- **Basis in paper**: Ablation study explores combinations of stages S0-S3 but does not test configurations with additional stages or deeper hierarchical structures
- **Why unresolved**: Paper suggests including all stages slightly harmed performance on smaller Utah dataset due to overfitting, but impact of deeper hierarchies on larger datasets remains unclear
- **What evidence would resolve it**: Extending DuoFormer to include deeper hierarchical stages and evaluating performance on diverse datasets would clarify complexity vs. accuracy tradeoffs

### Open Question 3
- **Question**: Can DuoFormer's scale attention mechanism be effectively adapted for non-medical image classification tasks, such as natural scene or object detection?
- **Basis in paper**: While paper emphasizes effectiveness in medical imaging, it does not explicitly test applicability to other domains
- **Why unresolved**: Paper focuses on histopathology datasets without evidence of performance on non-medical tasks that would demonstrate versatility
- **What evidence would resolve it**: Applying DuoFormer to benchmark datasets like ImageNet or COCO and comparing performance to state-of-the-art models would validate cross-domain applicability

## Limitations
- Limited implementation details for multi-scale patch tokenization process, particularly patch indexing and concatenation across different CNN stages
- Results may not generalize beyond kidney cancer histopathology to other medical imaging tasks or domains
- Computational efficiency claims lack quantitative evidence and actual overhead measurements across different hardware configurations

## Confidence
- **High Confidence**: Core hypothesis that integrating hierarchical CNN features with transformer processing improves performance on small medical imaging datasets is well-supported by experimental results
- **Medium Confidence**: Specific mechanisms of scale attention and patch attention have limited ablation study evidence, demonstrating contribution but not comprehensive isolation of individual impacts
- **Low Confidence**: Generalizability to non-medical imaging domains and larger datasets is not thoroughly explored, focusing primarily on two specific histopathology datasets

## Next Checks
1. **Ablation Study on Attention Components**: Conduct comprehensive ablation study removing either scale attention or patch attention independently to quantify their individual contributions to overall performance
2. **Cross-Domain Performance Testing**: Evaluate DuoFormer on at least two non-medical imaging datasets (e.g., CIFAR-100, Food-101) with varying sizes to assess generalizability beyond histopathology domain
3. **Computational Complexity Analysis**: Measure and compare actual computational overhead (FLOPs, memory usage, inference time) of DuoFormer against baseline models across different hardware configurations to substantiate efficiency claims