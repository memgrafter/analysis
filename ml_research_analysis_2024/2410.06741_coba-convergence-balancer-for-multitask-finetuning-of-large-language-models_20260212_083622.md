---
ver: rpa2
title: 'CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models'
arxiv_id: '2410.06741'
source_url: https://arxiv.org/abs/2410.06741
tags:
- tasks
- task
- coba
- convergence
- famo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CoBa, a convergence balancer for multitask
  fine-tuning of large language models. CoBa dynamically adjusts task weights during
  training using Relative Convergence Scores (RCS), Absolute Convergence Scores (ACS),
  and a Divergence Factor (DF) to ensure all tasks converge at a similar pace while
  avoiding individual task divergence.
---

# CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models

## Quick Facts
- arXiv ID: 2410.06741
- Source URL: https://arxiv.org/abs/2410.06741
- Authors: Zi Gong; Hang Yu; Cong Liao; Bingchang Liu; Chaoyu Chen; Jianguo Li
- Reference count: 39
- Primary result: CoBa achieves up to 13% relative improvement over baselines on multitask fine-tuning benchmarks

## Executive Summary
This paper introduces CoBa, a convergence balancer for multitask fine-tuning of large language models. CoBa dynamically adjusts task weights during training using Relative Convergence Scores (RCS), Absolute Convergence Scores (ACS), and a Divergence Factor (DF) to ensure all tasks converge at a similar pace while avoiding individual task divergence. The method is evaluated on Code Completion, Code-Related Tasks, and XTREME-UP datasets, demonstrating superior performance compared to existing baselines while maintaining minimal computational overhead.

## Method Summary
CoBa addresses the challenge of balancing convergence across multiple tasks during LLM fine-tuning by dynamically adjusting task weights based on validation loss trends. The method computes three scores: RCS for relative convergence comparison across tasks, ACS for absolute convergence rate tracking, and DF for detecting and handling divergence. These scores are combined to produce task weights that ensure balanced convergence while preventing overfitting on individual tasks. The approach uses validation loss slopes rather than training loss, prioritizing generalization, and achieves O(2F + B + a3K) time complexity.

## Key Results
- CoBa achieves up to 13% relative improvement over second-best baselines on benchmark datasets
- The method successfully balances convergence across tasks with different learning rates
- Computational overhead remains minimal compared to gradient-based convergence balancers
- CoBa prevents individual task divergence while maintaining overall model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoBa dynamically balances task weights based on relative and absolute convergence trends to prevent early divergence.
- Mechanism: The Relative Convergence Score (RCS) compares tasks' convergence speeds via normalized slopes; the Absolute Convergence Score (ACS) tracks each task's own slope trend; the Divergence Factor (DF) blends them based on whether all tasks are converging.
- Core assumption: Validation loss slopes reliably indicate task convergence and overfitting risk.
- Evidence anchors:
  - [abstract]: "Utilizing Relative Convergence Scores (RCS), Absolute Convergence Scores (ACS), and a Divergence Factor (DF), CoBa dynamically adjusts task weights during the training process, ensuring that the validation loss of all tasks progress towards convergence at an even pace while mitigating the issue of individual task divergence."
  - [section 3.2]: "RCS is employed to assess the convergence pace relative among tasks, while ACS measures the current convergence rate against historical rates for each task individually."
  - [corpus]: Weak — neighbor papers focus on task imbalance but not specifically on RCS/ACS/DF formulation.
- Break condition: If validation loss curves are noisy or non-monotonic, slope-based scores may misrepresent convergence.

### Mechanism 2
- Claim: CoBa prioritizes generalization by basing weight updates on validation loss rather than training loss.
- Mechanism: Task weights are computed from validation loss slopes, so rapid overfitting (divergence) is detected early and suppressed by reducing the corresponding task's weight.
- Core assumption: Validation loss better reflects true generalization than training loss.
- Evidence anchors:
  - [abstract]: "CoBa...dynamically adjusts task weights...ensuring that the validation loss of all tasks progress towards convergence at an even pace..."
  - [section 3.1]: "To ensure a fair comparison of convergence speeds across tasks, we first normalize the validation losses."
  - [corpus]: Weak — general MTL literature supports validation-based monitoring, but specific validation-loss-weighting schemes are not common in neighbors.
- Break condition: If validation set is too small or non-representative, this mechanism may suppress legitimate task progress.

### Mechanism 3
- Claim: CoBa achieves minimal computational overhead compared to gradient-based convergence balancers.
- Mechanism: Only validation loss slopes are computed per batch, avoiding gradient aggregation or per-task backward passes; the cost is O(2F + B + a3K).
- Core assumption: Slope estimation and softmax operations are cheaper than per-task gradient manipulations.
- Evidence anchors:
  - [abstract]: "The results of our experiments involving three disparate datasets underscore that this approach not only fosters equilibrium in task convergence but enhances the LLMs' performance by up to 13% relative to the second-best baselines."
  - [section 3.5]: "Thus, the joint time complexity of CoBa is O(2F +B +2KN +11K +2N +7T ). In terms of F , B, and K, this expression can be simplified to O(2F + B + a3K), as shown in Table 1."
  - [corpus]: Moderate — neighbor papers also aim for computational efficiency but use different mechanisms (data mixing, pruning) rather than validation slope tracking.
- Break condition: If K is very large, the K-dependent term may dominate; if N is large, slope fitting cost increases.

## Foundational Learning

- Concept: Convergence slope estimation via linear regression on validation loss ratios.
  - Why needed here: To quantify how fast each task is learning and detect divergence.
  - Quick check question: What happens to the slope if validation loss starts increasing instead of decreasing?
- Concept: Softmax-based normalization to produce comparable task weights.
  - Why needed here: To map convergence scores into a probability-like weight distribution across tasks.
  - Quick check question: Why multiply the normalized slope by K before softmax in RCS?
- Concept: Divergence factor as a dynamic gating between RCS and ACS.
  - Why needed here: To switch emphasis from relative to absolute convergence when any task begins overfitting.
  - Quick check question: What is the role of the temperature τ in DF calculation?

## Architecture Onboarding

- Component map: Validation loss computation -> slope fitting window -> RCS/ACS/DF computation -> task weight update -> weighted loss aggregation
- Critical path: Validation loss -> slope -> RCS/ACS -> DF -> weights -> training step
- Design tradeoffs: Using validation loss instead of training loss trades some immediate feedback for better generalization; linear regression on slopes trades accuracy for speed.
- Failure signatures: Uniform weights despite task imbalance (slope noise), or sudden weight spikes (DF misfire).
- First 3 experiments:
  1. Run CoBa on two tasks with clear convergence difference; plot weights over time to verify RCS effect.
  2. Introduce early divergence in one task; confirm ACS reduces its weight.
  3. Compare runtime with FAMO* and GradNorm* on same dataset; measure wall-clock time per epoch.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does CoBa perform on larger language models, particularly those with significantly more parameters than the Phi-1.5-1.3B and CodeLlama-13B-Python models used in the experiments?
  - Basis in paper: [explicit] The paper states: "Due to resource constraints, we are unable to evaluate the efficacy of CoBa on larger LLMs. In future work, we aspire to conduct experiments with larger LLMs, akin to MFTCoder (Liu et al., 2024a), to further substantiate our findings."
  - Why unresolved: The authors explicitly mention that they were unable to evaluate CoBa on larger LLMs due to resource constraints. This leaves open the question of how the method scales and performs with significantly larger models.
  - What evidence would resolve it: Experiments applying CoBa to larger LLMs (e.g., GPT-3.5/4 or other models with hundreds of billions of parameters) and comparing its performance and computational efficiency to the current results would resolve this question.

- **Open Question 2**: Can CoBa be effectively adapted for multi-task learning in modalities other than natural language processing, such as computer vision or speech recognition?
  - Basis in paper: [inferred] The paper mentions: "This paper focuses on NLP. However, multi-task learning is not limited to this modality. In the future, we aim to explore other modalities such as computer vision."
  - Why unresolved: While the paper focuses on NLP applications, it acknowledges the potential for extending CoBa to other modalities. However, no experiments or theoretical analysis have been conducted to explore this possibility.
  - What evidence would resolve it: Applying CoBa to multi-task learning problems in computer vision or speech recognition, and comparing its performance to existing methods in these domains, would provide evidence for its effectiveness across modalities.

- **Open Question 3**: How can CoBa be integrated with a Mixture of Experts (MoE) framework to address task conflicts or interference, as suggested in the paper?
  - Basis in paper: [explicit] The paper states: "A promising solution is to integrate CoBa with a Mixture of Experts (MoE) framework, assigning each task to a specific expert within the model."
  - Why unresolved: While the paper suggests that integrating CoBa with an MoE framework could address task conflicts or interference, it does not provide any implementation details or experimental results to support this claim.
  - What evidence would resolve it: Implementing and testing a CoBa-MoE hybrid approach on multi-task learning problems, and comparing its performance and ability to handle task conflicts to CoBa alone, would provide evidence for the effectiveness of this integration.

- **Open Question 4**: How can CoBa be modified to incorporate curriculum learning principles, prioritizing easier tasks during initial training stages?
  - Basis in paper: [inferred] The paper mentions: "While CoBa prioritizes difficult tasks at the initial stage of Multi-Task Learning (MTL), curriculum learning emphasizes prioritizing easier tasks, which can be advantageous in scenarios where learning harder tasks may become easier once the model has mastered the easier tasks."
  - Why unresolved: The paper acknowledges the potential benefits of incorporating curriculum learning principles into CoBa but does not provide any specific modifications or experimental results to demonstrate how this could be achieved.
  - What evidence would resolve it: Developing and testing a curriculum learning-enhanced version of CoBa, where the task weights are dynamically adjusted based on task difficulty and convergence rates, and comparing its performance to the original CoBa on multi-task learning problems, would provide evidence for the effectiveness of this modification.

## Limitations

- The computational overhead claims lack empirical runtime comparisons against strong baselines under identical conditions.
- The mechanisms rely on validation loss slopes but don't address sensitivity to noisy or non-monotonic loss curves.
- No ablation studies isolate the contribution of RCS, ACS, and DF components.
- Evaluation is limited to three datasets, leaving generalization to other domains uncertain.

## Confidence

- Convergence balancing effectiveness: Medium - strong quantitative results but limited to three datasets.
- Computational efficiency: Low - theoretical complexity given, but no empirical runtime data.
- Robustness to noisy convergence signals: Medium - mechanism plausible but not stress-tested.
- Generalizability to other LLM multitask settings: Medium - framework is generic but not validated beyond the tested domains.

## Next Checks

1. Run CoBa on a multitask setup with one task known to diverge early (e.g., by injecting noise or reducing its dataset size). Verify that ACS suppresses its weight before validation loss explodes.
2. Perform an ablation study: run CoBa with only RCS, only ACS, and only DF to quantify each component's contribution to convergence balance and final performance.
3. Measure wall-clock time per epoch for CoBa vs. FAMO* and GradNorm* on the same hardware and dataset, ensuring identical batch sizes and gradient accumulation steps.