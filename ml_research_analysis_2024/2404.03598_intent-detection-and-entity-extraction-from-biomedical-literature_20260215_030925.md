---
ver: rpa2
title: Intent Detection and Entity Extraction from BioMedical Literature
arxiv_id: '2404.03598'
source_url: https://arxiv.org/abs/2404.03598
tags:
- entity
- biomedical
- intent
- language
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a comprehensive empirical evaluation comparing
  large language models (LLMs) and supervised fine-tuned (SFT) approaches for intent
  detection and named entity recognition (NER) tasks in biomedical literature. The
  study benchmarks BERT, RoBERTa, PubMedBERT, and other SFT models against ChatGPT
  on three intent detection datasets and five NER datasets covering drug, disease,
  chemical, genetic, and anatomy entities.
---

# Intent Detection and Entity Extraction from BioMedical Literature

## Quick Facts
- arXiv ID: 2404.03598
- Source URL: https://arxiv.org/abs/2404.03598
- Reference count: 0
- PubMedBERT outperforms ChatGPT on NER tasks with only 5 supervised examples

## Executive Summary
This paper conducts a comprehensive empirical evaluation comparing large language models (LLMs) and supervised fine-tuned (SFT) approaches for intent detection and named entity recognition (NER) tasks in biomedical literature. The study benchmarks BERT, RoBERTa, PubMedBERT, and other SFT models against ChatGPT on three intent detection datasets and five NER datasets covering drug, disease, chemical, genetic, and anatomy entities. Results show that biomedical transformer models like PubMedBERT consistently outperform ChatGPT on NER tasks, achieving higher F1-scores even with minimal supervised examples (as few as 5 examples). The BINDER model with PubMedBERT encoder achieved the best overall performance. For intent detection, transformer-based SFT models also significantly outperformed ChatGPT across all datasets.

## Method Summary
The study compares instruction-tuned ChatGPT against various supervised fine-tuned (SFT) models including BERT, RoBERTa, PubMedBERT, BioBERT, BioMed RoBERTa, ClinicalBERT, BINDER, and traditional ML approaches. The evaluation uses three intent detection datasets (CMID, KUAKE-QIC, Intent-Merged) and five NER datasets (JNLPBA, DDI, BC5CDR, NCBI-Disease, AnatEM). Models are fine-tuned with learning rate 5e-5, AdamW optimizer, 10% warm-up steps, batch size 16, and max sequence length 512. Performance is measured using strict F1-score for NER and accuracy for intent detection.

## Key Results
- PubMedBERT consistently outperforms ChatGPT on NER tasks across all five datasets
- BINDER with PubMedBERT encoder achieves the best overall performance
- Transformer-based SFT models significantly outperform ChatGPT on intent detection (3/3 datasets)
- PubMedBERT learns effectively from minimal supervised examples (as few as 5 examples)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pretraining provides superior embeddings for biomedical NER tasks compared to general-purpose LLMs.
- Mechanism: PubMedBERT's pretraining on biomedical corpus creates specialized token representations that capture domain-specific terminology and context better than general LLMs like ChatGPT.
- Core assumption: Biomedical domain knowledge cannot be effectively learned through general pretraining and requires domain-specific pretraining data.
- Evidence anchors:
  - [abstract]: "Biomedical transformer models such as PubMedBERT can surpass ChatGPT on NER task with only 5 supervised examples"
  - [section 3.2.1]: "PubMedBERT learns good embedding vectors due to the largest pretraining corpus"
- Break condition: If general-purpose LLMs gain access to sufficient biomedical pretraining data, the performance gap may narrow.

### Mechanism 2
- Claim: Fine-tuning with minimal examples is more effective than zero-shot prompting for specialized biomedical tasks.
- Mechanism: Supervised fine-tuned (SFT) models can learn task-specific patterns from small amounts of labeled data, while LLMs struggle with zero-shot biomedical reasoning.
- Core assumption: Domain-specific SFT approaches can generalize better from few examples than LLMs can from their general pretraining.
- Evidence anchors:
  - [abstract]: "PubMedBERT can surpass ChatGPT on NER task with only 5 supervised examples"
  - [section 3.2.2]: "All SFT approaches surpass ChatGPT by a big margin"
- Break condition: If LLMs improve their ability to follow domain-specific instructions without fine-tuning.

### Mechanism 3
- Claim: Contrastive learning objectives in BINDER architecture enhance entity recognition by pushing similar entity tokens closer in embedding space.
- Mechanism: BINDER's bi-encoder architecture with contrastive loss creates better-separated entity representations than standard transformers.
- Core assumption: Contrastive learning can improve entity boundary detection in specialized domains.
- Evidence anchors:
  - [section 3.2.1]: "BINDER combined with PubMedBERT gives the best F1 score as it is able to leverage high-quality embeddings along with entity descriptions"
  - [section 3.2.2]: "PubMedBERT embeddings perform better in very low-resource setups (5, 10 shots). However, when training examples increase further (30 shots onwards), BINDER (PubMedBERT) outperforms PubMedBERT"
- Break condition: If simpler architectures with larger pretraining corpora can match or exceed BINDER's performance.

## Foundational Learning

- Concept: Domain-specific pretraining vs general pretraining
  - Why needed here: Understanding why PubMedBERT outperforms general models requires grasping the importance of domain-specific pretraining
  - Quick check question: What is the key difference between PubMedBERT and standard BERT that leads to better biomedical performance?

- Concept: Fine-tuning vs zero-shot prompting
  - Why needed here: The paper shows SFT approaches consistently outperforming LLMs, which requires understanding the mechanics of fine-tuning
  - Quick check question: Why does PubMedBERT achieve better results with just 5 supervised examples compared to ChatGPT's zero-shot performance?

- Concept: Contrastive learning in NER
  - Why needed here: BINDER's superior performance relies on contrastive learning objectives, which needs to be understood for architectural decisions
  - Quick check question: How does contrastive learning help BINDER improve entity recognition compared to standard transformers?

## Architecture Onboarding

- Component map:
  - Input: Biomedical text sequences (max 512 tokens)
  - Core models: PubMedBERT, BioBERT, BERT, RoBERTa, BINDER (bi-encoder with contrastive loss)
  - Output: Named entity tags or intent classifications
  - Evaluation: F1-score for NER, accuracy for intent detection

- Critical path:
  1. Load domain-specific pretrained model (PubMedBERT preferred)
  2. Fine-tune on target biomedical dataset
  3. Evaluate using strict F1-score or accuracy metrics
  4. Compare against baseline models and ChatGPT

- Design tradeoffs:
  - PubMedBERT vs BioBERT: PubMedBERT has larger pretraining corpus but BioBERT may be more established
  - BINDER vs single transformer: BINDER adds complexity but improves performance with contrastive learning
  - Sequence length: 512 tokens balances context vs computational cost

- Failure signatures:
  - Low F1 on entities with few training examples indicates data sparsity issues
  - Consistent underperformance on specific entity types suggests model bias or inadequate pretraining coverage
  - Poor cross-dataset generalization may indicate overfitting to specific corpus characteristics

- First 3 experiments:
  1. Fine-tune PubMedBERT on DDI dataset with 5 examples per class and evaluate F1-score
  2. Compare PubMedBERT vs ChatGPT on NCBI-Disease dataset using identical prompts for fair comparison
  3. Implement BINDER architecture with PubMedBERT encoder and test on JNLPBA dataset to validate contrastive learning benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of biomedical transformer models like PubMedBERT change when trained on datasets with progressively larger and more diverse biomedical corpora?
- Basis in paper: [explicit] The paper demonstrates that PubMedBERT outperforms ChatGPT on NER tasks with minimal supervised examples, suggesting domain-specific pretraining is effective.
- Why unresolved: The study does not explore the impact of varying the size and diversity of the biomedical corpus on PubMedBERT's performance.
- What evidence would resolve it: Experiments showing PubMedBERT's performance on NER tasks across datasets with different sizes and diversities of biomedical corpora.

### Open Question 2
- Question: Can a single model be effectively designed to jointly extract intents and entities in multilingual biomedical literature, and how would it compare to separate models for each task?
- Basis in paper: [explicit] The paper mentions future work on jointly extracting intents and entities for multilingual scenarios, indicating this as an open area.
- Why unresolved: The paper does not provide results or insights into the effectiveness of a unified model for multilingual intent and entity extraction.
- What evidence would resolve it: Comparative studies showing the performance of a joint multilingual model versus separate models on multilingual biomedical datasets.

### Open Question 3
- Question: What is the impact of model updates on the ability to identify new or unapproved drugs in biomedical text, given that some models fail to recognize these entities?
- Basis in paper: [explicit] The paper notes that some models fail to identify the entity "drug_n," representing new or unapproved drugs, suggesting a need for periodic updates.
- Why unresolved: The paper does not investigate how regular updates affect the model's ability to recognize new drug entities over time.
- What evidence would resolve it: Longitudinal studies assessing the performance of models on drug entity recognition before and after updates to include new drug data.

## Limitations
- Evaluation of ChatGPT used zero-shot/few-shot prompting rather than instruction tuning on biomedical data
- Does not investigate ensemble methods combining LLM and SFT approaches
- Results focus primarily on English biomedical literature
- Does not explore whether general LLMs could match performance with biomedical pretraining

## Confidence
- High Confidence Claims:
  - PubMedBERT consistently outperforms ChatGPT on NER tasks across all evaluated datasets (5/5)
  - Transformer-based SFT models significantly outperform ChatGPT on intent detection tasks (3/3 datasets)
  - BINDER architecture with PubMedBERT encoder achieves the best overall performance in the study
  - Domain-specific pretraining provides substantial benefits for biomedical entity recognition

- Medium Confidence Claims:
  - PubMedBERT achieves superior performance with as few as 5 supervised examples
  - Contrastive learning objectives in BINDER architecture meaningfully improve entity recognition
  - Performance trends are consistent across different biomedical entity types (drugs, diseases, chemicals, genes, anatomy)

- Low Confidence Claims:
  - The exact magnitude of performance differences between models across all dataset variations
  - Generalizability of findings to non-English biomedical literature
  - Long-term stability of performance differences as both SFT models and LLMs continue to evolve

## Next Checks
1. **Instruction-Tuning Validation**: Conduct a controlled experiment where ChatGPT is instruction-tuned on the same biomedical datasets used for SFT models, using identical training examples to determine if the performance gap narrows or closes.

2. **Cross-Lingual Transfer Test**: Evaluate the best-performing models (PubMedBERT and BINDER) on biomedical NER and intent detection tasks in non-English languages to assess the generalizability of the domain-specific pretraining advantage.

3. **Hybrid Architecture Experiment**: Implement an ensemble approach combining PubMedBERT's embeddings with ChatGPT's reasoning capabilities to determine if hybrid models can achieve performance exceeding either approach alone, particularly for low-resource scenarios.