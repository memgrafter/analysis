---
ver: rpa2
title: Gradient Descent Finds Over-Parameterized Neural Networks with Sharp Generalization
  for Nonparametric Regression
arxiv_id: '2411.02904'
source_url: https://arxiv.org/abs/2411.02904
tags:
- theorem
- follows
- have
- then
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes that over-parameterized two-layer neural\
  \ networks trained by gradient descent (GD) with early stopping achieve sharp minimax\
  \ optimal risk rates for nonparametric regression without distributional assumptions\
  \ on bounded covariates. The authors prove that the risk rate is O(\u03B5n\xB2),\
  \ matching the rate of classical kernel regression, where \u03B5n is the critical\
  \ population rate of the Neural Tangent Kernel (NTK)."
---

# Gradient Descent Finds Over-Parameterized Neural Networks with Sharp Generalization for Nonparametric Regression

## Quick Facts
- arXiv ID: 2411.02904
- Source URL: https://arxiv.org/abs/2411.02904
- Authors: Yingzhen Yang; Ping Li
- Reference count: 40
- One-line primary result: Over-parameterized two-layer neural networks trained by gradient descent with early stopping achieve sharp minimax optimal risk rates for nonparametric regression without distributional assumptions on bounded covariates.

## Executive Summary
This paper establishes that over-parameterized two-layer neural networks trained by gradient descent (GD) with early stopping achieve sharp minimax optimal risk rates for nonparametric regression without distributional assumptions on bounded covariates. The authors prove that the risk rate is O(ε_n²), matching the rate of classical kernel regression, where ε_n is the critical population rate of the Neural Tangent Kernel (NTK). The proof strategy avoids the common decomposition into kernel regressors, instead approximating the neural network function by an element in the RKHS with a small approximation error controlled by network width. Key contributions include: 1) Distribution-free risk bounds for bounded covariates; 2) Characterization of the stopping time as ε_n⁻²; 3) Lower bound on network width m ≳ d⁵/ε_n⁵; 4) Use of constant learning rate η = Θ(1).

## Method Summary
The paper studies nonparametric regression using an over-parameterized two-layer neural network trained by gradient descent with early stopping. The network has m neurons in the first layer with ReLU activation, and the second layer weights are randomly initialized to ±1 and fixed. The first layer weights are optimized using GD with learning rate η = Θ(1). The authors prove that under appropriate conditions on the network width m and input space X, the trained network achieves the same sharp risk rate as classical kernel regression.

## Key Results
- Over-parameterized two-layer neural networks trained by GD with early stopping achieve the same sharp risk rate O(ε_n²) as classical kernel regression for nonparametric regression.
- The stopping time is characterized as ε_n⁻², which is distribution-free in the bounded covariate.
- The lower bound for the network width m depends only on n and d under polynomial eigenvalue decay of the NTK, rather than on additional assumptions about the training data.
- Simulation results confirm the theoretical predictions, showing that the early stopping time is empirically proportional to the theoretically predicted value n^(d+1)/(2d+1).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient descent on over-parameterized two-layer ReLU networks with early stopping achieves the same sharp risk rate as classical kernel regression without distributional assumptions on bounded covariates.
- Mechanism: Uniform convergence of the empirical neural tangent kernel (NTK) to the population NTK during training allows decomposition of the network function into a function in the RKHS associated with the NTK and a small approximation error term with bounded L∞ norm.
- Core assumption: The network width m is sufficiently large (m ≳ d⁵/εₙ⁵) and the input space X is bounded or compact.
- Evidence anchors:
  - [abstract] states "sharp rate of the nonparametric regression risk of O(εₙ²), which is the same rate as that for the classical kernel regression trained by GD with early stopping"
  - [section VI-C] describes the proof strategy: "we have a new decomposition of f_t by f_t = h_t + e_t where f_t is approximated by h_t with e_t being the approximation error"
  - [corpus] does not contain direct evidence for this mechanism
- Break condition: If the network width m is not sufficiently large or the input space is unbounded, the uniform convergence may fail and the decomposition may not hold.

### Mechanism 2
- Claim: The early stopping time is characterized by T ≍ εₙ⁻², which is distribution-free in the bounded covariate.
- Mechanism: The stopping time is determined by the critical population rate εₙ of the NTK, which is the fixed point of the kernel complexity function. This characterization does not depend on the distribution of the covariates as long as the input space is bounded.
- Core assumption: The neural network is trained by GD with early stopping and the learning rate η is constant.
- Evidence anchors:
  - [abstract] states "characterization of the stopping time as εₙ⁻²"
  - [section III] mentions "bT ≍ εₙ⁻², which is distribution-free in the bounded covariate"
  - [corpus] does not contain direct evidence for this mechanism
- Break condition: If the learning rate η is not constant or the training process deviates from GD with early stopping, the stopping time characterization may not hold.

### Mechanism 3
- Claim: The lower bound for the network width m depends only on n and d under polynomial eigenvalue decay of the NTK, rather than on additional assumptions about the training data.
- Mechanism: The network width m needs to be large enough to ensure uniform convergence to the NTK and to bound the approximation error. Under polynomial eigenvalue decay, this requirement translates to m ≳ n^(25α)/(2α+1) · d⁵/2, which depends only on n and d.
- Core assumption: The NTK has a polynomial eigenvalue decay rate and the input space X is bounded.
- Evidence anchors:
  - [section III] states "our main result, Theorem V.1, requires that the network width m satisfies m ≳ d⁵/εₙ⁵" and "Corollary V.2 shows that m should satisfy m ≳ n^(25α)/(2α+1) · d⁵/2"
  - [section VI-C] mentions "m ≳ n^(25α)/(2α+1) · d⁵/2 so that GD with early stopping leads to the minimax rate"
  - [corpus] does not contain direct evidence for this mechanism
- Break condition: If the NTK does not have a polynomial eigenvalue decay rate or the input space is unbounded, the lower bound for m may depend on additional factors.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its relation to over-parameterized neural networks
  - Why needed here: The NTK is crucial for understanding the training dynamics of over-parameterized neural networks and establishing their generalization properties. It allows the analysis to bridge the gap between kernel methods and neural networks.
  - Quick check question: What is the Neural Tangent Kernel (NTK) and how is it related to the training dynamics of over-parameterized neural networks?

- Concept: Reproducing Kernel Hilbert Space (RKHS) and its properties
  - Why needed here: The RKHS associated with the NTK provides a framework for analyzing the function space that the neural network can represent. It is used to decompose the network function and establish the risk bounds.
  - Quick check question: What is a Reproducing Kernel Hilbert Space (RKHS) and what are its key properties that make it useful for analyzing neural networks?

- Concept: Local Rademacher complexity and its application to nonparametric regression
  - Why needed here: Local Rademacher complexity is used to derive sharp generalization bounds for nonparametric regression. It allows for a tight control of the risk without relying on distributional assumptions about the covariates.
  - Quick check question: How does local Rademacher complexity differ from standard Rademacher complexity, and why is it particularly useful for nonparametric regression with neural networks?

## Architecture Onboarding

- Component map:
  Input layer -> First layer (m neurons with ReLU) -> Second layer (fixed weights ±1) -> Output
  Training: Gradient descent on first layer weights with early stopping

- Critical path:
  1. Initialize weights W(0) and second layer weights a
  2. Compute gradient of loss with respect to W
  3. Update W using gradient descent
  4. Monitor training and validation loss
  5. Stop training when validation loss starts to increase

- Design tradeoffs:
  - Network width m: Larger m improves approximation and generalization but increases computational cost
  - Learning rate η: Constant η ∈ (0, 2/u₀²) is used, with smaller η leading to slower convergence but potentially better generalization
  - Early stopping: Prevents overfitting but requires careful tuning of stopping criteria

- Failure signatures:
  - Poor generalization: Test loss increases with training steps
  - Slow convergence: Training loss decreases very slowly
  - Numerical instability: Loss becomes NaN or diverges

- First 3 experiments:
  1. Verify uniform convergence of empirical NTK to population NTK by plotting the difference during training
  2. Test early stopping by training with different stopping criteria and comparing test loss
  3. Evaluate the effect of network width m on generalization by training networks with different widths and comparing test loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sharp risk rate of O(ε_n²) be achieved for nonparametric regression with over-parameterized neural networks trained by gradient descent with early stopping for unbounded input spaces?
- Basis in paper: [explicit] The paper establishes sharp risk rates for bounded input spaces but notes that some existing works consider unbounded spaces under restrictive conditions.
- Why unresolved: The paper's analysis relies on the boundedness of the input space, and extending it to unbounded spaces would require new techniques to handle the resulting complexities.
- What evidence would resolve it: A proof demonstrating that the sharp risk rate can be achieved for unbounded input spaces under appropriate conditions, such as specific tail behaviors of the data distribution.

### Open Question 2
- Question: What is the precise relationship between the lower bound on the network width m and the dimensionality of the data d for nonparametric regression with over-parameterized neural networks?
- Basis in paper: [explicit] The paper establishes a lower bound of m ≳ d⁵/ε_n⁵ for the network width, but notes that this bound could be further refined.
- Why unresolved: The current lower bound is based on theoretical analysis and may not be tight in practice. Further research is needed to determine the optimal relationship between m and d.
- What evidence would resolve it: Empirical studies comparing the performance of neural networks with different widths for various data dimensionalities, as well as theoretical analysis to tighten the lower bound.

### Open Question 3
- Question: Can the sharp risk rate of O(ε_n²) be achieved for nonparametric regression with over-parameterized neural networks trained by gradient descent with early stopping for target functions that are not in the RKHS associated with the NTK?
- Basis in paper: [inferred] The paper focuses on target functions in the RKHS associated with the NTK, but some existing works consider target functions outside this class.
- Why unresolved: The paper's analysis relies on the properties of the RKHS, and extending it to target functions outside this class would require new techniques to handle the resulting complexities.
- What evidence would resolve it: A proof demonstrating that the sharp risk rate can be achieved for target functions outside the RKHS associated with the NTK under appropriate conditions, such as specific smoothness or regularity properties of the target function.

## Limitations

- The analysis heavily relies on uniform convergence of the empirical NTK to the population NTK, which requires sufficiently large network width m. The theoretical lower bound m ≳ d⁵/εₙ⁵ may be conservative in practice.
- The proof strategy assumes the input space X is bounded or compact, which may not hold in all practical applications.
- The characterization of the stopping time as εₙ⁻² assumes constant learning rate η and specific training dynamics that may not generalize to all optimization schemes.

## Confidence

- **High Confidence**: The risk rate O(εₙ²) matching classical kernel regression is well-established theoretically and supported by simulation results.
- **Medium Confidence**: The characterization of stopping time as εₙ⁻² is theoretically proven but may be sensitive to implementation details and learning rate choices.
- **Medium Confidence**: The lower bound on network width m is derived under specific assumptions about NTK eigenvalue decay, which may not hold in all practical scenarios.

## Next Checks

1. **Empirical Validation of NTK Convergence**: Plot the difference between empirical and population NTK during training to verify uniform convergence. Compare convergence rates across different network widths to validate the theoretical bounds.

2. **Sensitivity Analysis of Stopping Time**: Train networks with different learning rates and stopping criteria to test the robustness of the εₙ⁻² characterization. Quantify how deviations from constant learning rate affect the stopping time.

3. **Generalization Beyond Bounded Inputs**: Test the framework on datasets with unbounded input spaces or heavy-tailed distributions to identify potential failure modes of the bounded covariate assumption.