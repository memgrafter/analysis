---
ver: rpa2
title: Leveraging Environment Interaction for Automated PDDL Translation and Planning
  with Large Language Models
arxiv_id: '2407.12979'
source_url: https://arxiv.org/abs/2407.12979
tags:
- domain
- pddl
- robot
- position
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel approach for automated PDDL generation
  using large language models (LLMs) and environment feedback, eliminating the need
  for human intervention. The core method leverages exploration walks (EW) to measure
  domain similarity and guide iterative refinement of PDDL domain and problem files.
---

# Leveraging Environment Interaction for Automated PDDL Translation and Planning with Large Language Models

## Quick Facts
- arXiv ID: 2407.12979
- Source URL: https://arxiv.org/abs/2407.12979
- Reference count: 40
- One-line primary result: The method achieves 66% task solve rate vs 34% baseline using iterative refinement guided by Exploration Walk metric

## Executive Summary
This paper introduces a novel approach for automated PDDL generation using large language models (LLMs) and environment feedback, eliminating the need for human intervention. The core method leverages exploration walks (EW) to measure domain similarity and guide iterative refinement of PDDL domain and problem files. By interacting with the environment and using EW as a smooth feedback signal, the LLM progressively refines its generated PDDL to align with the ground-truth domain.

The proposed method is evaluated on 10 challenging PDDL domains from the International Planning Competition. It achieves an average task solve rate of 66% and an average EW score of 0.84, outperforming a baseline that generates PDDL files in a single attempt (34% solve rate, 0.53 EW score) and GPT-4's intrinsic planning (29% solve rate). The results demonstrate the effectiveness of the approach in automatically modeling planning environments using LLMs and environment feedback.

## Method Summary
The method uses an iterative refinement process where LLMs generate PDDL domain and problem files guided by an Exploration Walk (EW) metric that measures domain similarity through random action sequence executability. The process starts with generating a domain proposal to identify predicates, then generates problems based on these predicates, and uses the EW metric to evaluate and refine the generated domain. Multiple LLM samples are generated and the best candidates are selected based on EW scores, creating a tree search structure that explores the solution space more effectively than single attempts.

## Key Results
- Achieves 66% average task solve rate compared to 34% for baseline single-attempt method
- EW score improves from 0.53 (baseline) to 0.84 (iterative method)
- Outperforms GPT-4's intrinsic planning with 29% task solve rate
- Demonstrates effectiveness across 10 challenging PDDL domains from International Planning Competition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exploration Walk (EW) metric provides smooth feedback for PDDL domain refinement
- Mechanism: By sampling random action sequences from a domain and checking executability in another domain, EW creates a continuous similarity measure that reveals incremental differences between domains, unlike binary plan search failure.
- Core assumption: Executability of random action sequences correlates with domain similarity and can be computed using only action interfaces and object lists without full domain access.

### Mechanism 2
- Claim: Iterative refinement with multiple LLM samples improves PDDL generation quality
- Mechanism: Generating multiple problem PDDL candidates and domain proposals allows selection of the best candidates based on EW scores, creating a tree search structure that explores the solution space more effectively than single attempts.
- Core assumption: LLM can generate diverse and meaningful PDDL variations when prompted multiple times, and selection based on EW scores leads to progressive improvement.

### Mechanism 3
- Claim: Separating domain and problem generation with domain proposal draft improves predicate design
- Mechanism: By first creating a domain proposal that identifies predicates, then generating problems based on those predicates, the method ensures consistency between domain and problem definitions and reduces predicate design errors.
- Core assumption: Domain predicates identified in a proposal can effectively guide problem generation to maintain consistency, and this separation reduces the cognitive load on the LLM compared to generating both simultaneously.

## Foundational Learning

- Concept: PDDL domain and problem structure
  - Why needed here: Understanding the separation between domain (predicates, actions) and problem (initial state, goals) is crucial for generating consistent PDDL files and for using EW metrics effectively
  - Quick check question: What is the key difference between domain PDDL and problem PDDL, and why does this distinction matter for the refinement process?

- Concept: Classical planning and plan validation
  - Why needed here: The method relies on classical planners to validate generated PDDL files and on plan validators to check executability, so understanding these concepts is essential for interpreting results and debugging
  - Quick check question: How does a plan validator differ from a plan execution checker, and why does this distinction matter for the EW metric?

- Concept: Exploration Walk metric calculation
  - Why needed here: EW is the core feedback mechanism for refinement, so understanding how it's computed from random action sequences and executability checks is crucial for implementing and improving the method
  - Quick check question: How is the symmetric EW score calculated from one-sided measures, and why is the harmonic mean used instead of arithmetic mean?

## Architecture Onboarding

- Component map: LLM for natural language to PDDL translation -> Environment for executability checking and EW computation -> Classical planner for plan validation -> EW metric for domain similarity scoring -> Iterative refinement loop
- Critical path: Problem generation → Domain generation → EW evaluation → Domain refinement → Plan validation → Success check
- Design tradeoffs: Multiple LLM samples vs. computational cost, Exploration Walk length vs. feedback quality, Domain proposal separation vs. implementation complexity
- Failure signatures: Plan search failures indicating domain problems, Low EW scores suggesting domain mismatches, Syntax errors in generated PDDL, Inconsistent predicates between domain and problem
- First 3 experiments:
  1. Generate PDDL for a simple domain (like Blocksworld) without refinement to establish baseline performance
  2. Implement EW metric calculation and test on domains with known differences to verify scoring behavior
  3. Run full iterative refinement on a moderately complex domain to observe convergence behavior and identify bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the EW metric's effectiveness change if more sophisticated EW strategies were used instead of uniform random walks?
- Basis in paper: The paper states "More sophisticated EW strategies could improve the success rate while lowering the cost in the future."
- Why unresolved: The paper only uses uniform random EW walks and doesn't explore other strategies.
- What evidence would resolve it: Experiments comparing uniform random EW to other sampling strategies (e.g., weighted by action probabilities, domain-specific heuristics) on the same domains.

### Open Question 2
- Question: What is the impact of the EW metric's maximum walk length (Tmax) on domain alignment accuracy?
- Basis in paper: The EW metric is defined with Tmax but the paper only uses Tmax=10.
- Why unresolved: The paper doesn't explore how varying Tmax affects the EW score or domain refinement quality.
- What evidence would resolve it: Systematic experiments varying Tmax and measuring its correlation with task solve rates across domains.

### Open Question 3
- Question: How does the method scale to larger, more complex PDDL domains with hundreds of predicates and actions?
- Basis in paper: The method is evaluated on 10 domains, but no analysis of scalability is provided.
- Why unresolved: The paper doesn't discuss computational complexity or performance on larger domains.
- What evidence would resolve it: Experiments on domains with increasing numbers of predicates/actions and measuring solve rates, EW scores, and computational costs.

## Limitations

- The approach relies heavily on the quality of LLM prompts and the effectiveness of the Exploration Walk metric, which may not generalize well to all domain types
- The iterative refinement process can be computationally expensive, especially for complex domains requiring many refinement iterations
- The method assumes access to an environment interface for executability checking, which may not be available for all applications

## Confidence

- High: The core mechanism of using Exploration Walk for domain similarity measurement and iterative refinement is well-supported by the results
- Medium: The claim that separating domain and problem generation improves predicate design is supported by evidence but could benefit from more systematic analysis
- Low: The scalability of the approach to significantly more complex domains and the robustness of the method across diverse domain types remain to be fully validated

## Next Checks

1. Test the method on a wider variety of planning domains, including those with continuous state spaces or non-deterministic actions, to assess generalization capability
2. Conduct ablation studies to quantify the individual contributions of domain proposal separation, multiple LLM samples, and EW metric to overall performance
3. Measure the computational overhead of the iterative refinement process and evaluate whether the performance gains justify the additional computation time compared to single-attempt methods