---
ver: rpa2
title: Path Following and Stabilisation of a Bicycle Model using a Reinforcement Learning
  Approach
arxiv_id: '2407.17156'
source_url: https://arxiv.org/abs/2407.17156
tags:
- bicycle
- learning
- path
- agent
- velocity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the use of Reinforcement Learning (RL) to
  control a bicycle model for path following and lateral stabilization. The Whipple
  bicycle model, a benchmark for bicycle dynamics, is used.
---

# Path Following and Stabilisation of a Bicycle Model using a Reinforcement Learning Approach

## Quick Facts
- arXiv ID: 2407.17156
- Source URL: https://arxiv.org/abs/2407.17156
- Reference count: 40
- RL successfully stabilizes bicycle and achieves <9% mean path deviation across various paths and velocities

## Executive Summary
This paper investigates the use of Reinforcement Learning (RL) to control a bicycle model for path following and lateral stabilization. The Whipple bicycle model, a benchmark for bicycle dynamics, is used as the test platform. The RL agent learns to control the bicycle by outputting steering angles, which are converted to steering torques via a PD controller. The agent's learning process is enhanced using curriculum learning, gradually increasing the complexity of the task. Results demonstrate that the RL approach can successfully stabilize the bicycle and follow various paths, including full circles, slalom maneuvers, and lane changes, across a range of forward velocities.

## Method Summary
The approach uses the Whipple bicycle model simulated in the Exudyn multibody environment, interfaced with OpenAI Gym for RL training. The state representation includes minimal coordinates and preview information about the upcoming path. The RL agent uses the SAC algorithm from Stable-Baselines3 to output steering angles, which are converted to torques via a PD controller. Curriculum learning is applied to progressively increase path complexity and velocity range during training, starting from simple paths at 4 m/s and advancing through increasingly complex scenarios.

## Key Results
- RL agent achieves mean path deviation of less than 9% of the wheelbase along complex benchmark paths
- Agent successfully stabilizes bicycle and follows paths across forward velocities 2-7 m/s
- Explanatory methods reveal the agent has learned fundamental bicycle control mechanisms including counter-steering and steering into the direction of undesired fall

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The RL agent learns to stabilize the bicycle by applying counter-steering before initiating a turn, then steering into the undesired fall during the turn.
- **Mechanism:** The agent observes the state (position, orientation, velocity) and predicts a steering angle that causes the bicycle to lean opposite the desired turn direction (counter-steering). This lean is then maintained by steering into the fall, creating a stable turning motion.
- **Core assumption:** The agent can learn this counterintuitive control strategy through trial and error using the reward signal.
- **Evidence anchors:**
  - [abstract]: "Explanatory methods reveal that the agent has learned fundamental mechanisms of bicycle control, such as counter-steering and steering into the direction of the undesired fall."
  - [section 5.5]: "We find that the agent also considers the roll angular velocity ˙ φ... the agent has learnt basic mechanisms for controlling bicycles considering the SHAP values, that is, to steer into the direction of the undesired fall and to do counter-steering."
  - [corpus]: No direct evidence found; corpus papers focus on different control methods.
- **Break condition:** If the reward structure doesn't properly penalize or reward the correct leaning behavior, the agent may learn an incorrect or unstable policy.

### Mechanism 2
- **Claim:** Scaling the preview distance with forward velocity (∆s = v^0.4) enables the agent to maintain consistent path-following accuracy across the velocity range.
- **Mechanism:** By increasing the preview distance proportionally to speed, the agent has more time to react to upcoming path changes at higher speeds, preventing overshooting or deviation from the path.
- **Core assumption:** The preview distance scaling factor (0.4) is optimal for the Whipple bicycle's dynamics across the tested velocity range.
- **Evidence anchors:**
  - [section 5.2]: "Scaling ∆ s with the forward velocity reduces the learning volume of the agent... eliminates another speed-dependence of the policy."
  - [section 3.2]: "the preview distance ∆ s used to obtain the preview information is either set constant with ∆ s = 2 m or scaled with the forward velocity v of the bicycle, reading ∆s = v^0.4 s."
  - [corpus]: No direct evidence found; corpus papers don't discuss preview distance scaling in RL contexts.
- **Break condition:** If the scaling factor is too high or too low for the bicycle's dynamics, path-following performance may degrade at certain speeds.

### Mechanism 3
- **Claim:** The curriculum learning strategy progressively increases path complexity and velocity range, enabling the agent to learn stable policies for both low-speed stabilization and high-speed path-following.
- **Mechanism:** The curriculum starts with simple paths at a constant velocity (4 m/s) and gradually introduces more complex paths and higher/lower velocities. This staged learning prevents the agent from being overwhelmed by the full complexity of the task initially.
- **Core assumption:** The staged complexity increase is appropriately timed and sequenced to match the agent's learning progress.
- **Evidence anchors:**
  - [abstract]: "Curriculum learning is applied as a state-of-the-art training strategy."
  - [section 4.3]: "Curriculum Learning (CL) is a training strategy... the task to be solved becomes increasingly complex as the learning proceeds."
  - [section 5.2]: "Two learning processes finish the curriculum in ntot learning steps if the alt(5) settings with equally weighted reward parts are used. The other runs stay in the second and third part of the curriculum until ntot learning steps are reached."
  - [corpus]: No direct evidence found; corpus papers focus on different applications of curriculum learning.
- **Break condition:** If the curriculum progression is too fast or too slow, the agent may not learn effectively or may fail to generalize to the full velocity range and path complexity.

## Foundational Learning

- **Concept:** Reward function design balancing path-following accuracy and stability
  - Why needed here: The agent needs clear feedback on both how well it follows the path and how stable the bicycle is. An imbalanced reward could lead to prioritizing one objective over the other detrimentally.
  - Quick check question: What happens if the reward weight for roll angle (ρφ) is set too high relative to path deviation (ρy)?

- **Concept:** State representation including preview information
  - Why needed here: The agent needs to anticipate upcoming path changes to plan steering actions in advance, especially at higher speeds. Without preview information, the agent would only react to the current position relative to the path.
  - Quick check question: How does the performance change if the number of preview points is reduced from 5 to 2?

- **Concept:** Speed-dependent policy learning
  - Why needed here: The Whipple bicycle has different stability characteristics at different speeds. The agent must learn different control strategies for low-speed (unstable) and high-speed (stable) regimes.
  - Quick check question: Can the agent generalize to speeds outside the trained range (e.g., 1 m/s or 8 m/s)?

## Architecture Onboarding

- **Component map:** Multibody simulation (Exudyn) -> OpenAI Gym interface -> SAC algorithm (Stable-Baselines3) -> Neural network policy -> PD controller -> Multibody simulation

- **Critical path:** State observation → Neural network prediction → PD controller output → Multibody simulation → Reward calculation → Policy update

- **Design tradeoffs:**
  - Constant vs. velocity-scaled preview distance: Constant distance simplifies learning but may not generalize well across speeds. Velocity scaling adds complexity but improves performance.
  - Reward weight balance: Equal weights for path deviation and stability may conflict, while prioritizing one may lead to poor performance in the other dimension.
  - Neural network architecture: More complex networks may capture nuanced behaviors but require more training data and computational resources.

- **Failure signatures:**
  - Agent fails to stabilize at low speeds: Check reward structure and curriculum progression for low-speed stabilization
  - Agent deviates significantly from path: Verify preview distance scaling and path complexity progression
  - Agent learns to cheat the reward (e.g., by oscillating): Examine reward function for potential exploits and consider adding penalties for high-frequency actions

- **First 3 experiments:**
  1. Test the impact of preview distance scaling by training agents with constant vs. velocity-scaled preview distance at a single velocity (e.g., 4 m/s)
  2. Evaluate the effect of reward weight balance by training agents with χ1 = 1, χ1 = χ2 = 0.5, and χ1 = 0.5, χ2 = 0.5
  3. Assess curriculum learning effectiveness by comparing agents trained with and without curriculum progression, keeping all other parameters constant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the RL agent compare to traditional control engineering methods for bicycle stabilization and path following?
- Basis in paper: [inferred] The paper mentions that complex control approaches have been developed over the years, but does not provide a direct comparison between the RL approach and traditional methods.
- Why unresolved: The paper focuses on demonstrating the feasibility and performance of the RL approach without benchmarking it against existing controllers.
- What evidence would resolve it: A comparative study showing the performance metrics (e.g., path deviation, stability) of the RL agent versus a well-tuned traditional controller under the same conditions.

### Open Question 2
- Question: What is the impact of the reward function design on the learning process and final agent performance?
- Basis in paper: [explicit] The paper discusses different options for the reward function, including the choice of distance metric (d(P, PP0) vs. d(Q, PQ0)), the relationship type (linear vs. exponential), and the inclusion of the roll angle penalty. It also mentions that using equal weights for the two reward components disrupts learning.
- Why unresolved: While the paper explores different reward settings and their impact on learning, it does not systematically analyze how these choices affect the final agent's performance or robustness.
- What evidence would resolve it: A detailed analysis showing the performance of agents trained with different reward functions on a variety of paths and velocities, including their robustness to perturbations and unseen scenarios.

### Open Question 3
- Question: How does the choice of state representation (minimal coordinates vs. redundant formulation) affect the learning process and the agent's ability to generalize?
- Basis in paper: [explicit] The paper investigates two state representations: one using minimal coordinates (q) and another using a redundant formulation (q') where the yaw angle is represented as a unit vector. It mentions that the yaw representation does not significantly influence the required learning steps.
- Why unresolved: The paper does not explore the impact of the state representation on the agent's ability to generalize to different scenarios or its interpretability in terms of bicycle dynamics.
- What evidence would resolve it: An analysis comparing the performance and generalization capabilities of agents trained with different state representations on a diverse set of paths and velocities, as well as an investigation into the interpretability of the learned policies using methods like SHAP values.

## Limitations
- The generalizability of the learned policy beyond the specific Whipple bicycle parameters and tested velocity range (2-7 m/s) remains uncertain.
- The specific scaling factor (v^0.4) for preview distance optimization across speeds is presented without comparative analysis of alternative scaling approaches.

## Confidence
- **High Confidence:** The core claim that RL can successfully stabilize and control a bicycle model is well-supported by quantitative results showing <9% mean path deviation across various path types and speeds.
- **Medium Confidence:** The claim about the agent learning fundamental bicycle control mechanisms (counter-steering, steering into fall) is supported by explanatory methods, but the exact nature of these learned behaviors could benefit from additional analysis.
- **Low Confidence:** The specific scaling factor (v^0.4) for preview distance optimization across speeds is presented without comparative analysis of alternative scaling approaches.

## Next Checks
1. Test the trained policy on bicycles with significantly different parameters (e.g., different wheelbase lengths or mass distributions) to assess generalizability.
2. Evaluate the impact of alternative preview distance scaling functions (e.g., v^0.3 vs v^0.5) on path-following performance across the velocity range.
3. Analyze the policy's behavior at extreme velocities (below 2 m/s and above 7 m/s) to determine the practical limits of the learned control strategy.