---
ver: rpa2
title: How to Choose a Threshold for an Evaluation Metric for Large Language Models
arxiv_id: '2412.12148'
source_url: https://arxiv.org/abs/2412.12148
tags:
- thresholds
- faithfulness
- evaluation
- threshold
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting appropriate thresholds
  for evaluation metrics in large language models (LLMs), a critical yet underexplored
  area with significant implications for model reliability and societal impact. The
  authors propose a systematic methodology that integrates stakeholder risk preferences
  into threshold determination, drawing from financial risk management practices and
  behavioral finance concepts like prospect theory.
---

# How to Choose a Threshold for an Evaluation Metric for Large Language Models

## Quick Facts
- arXiv ID: 2412.12148
- Source URL: https://arxiv.org/abs/2412.12148
- Reference count: 40
- Authors: Bhaskarjit Sarmah; Mingshu Li; Jingrao Lyu; Sebastian Frank; Nathalia Castellanos; Stefano Pasquali; Dhagash Mehta
- One-line primary result: Conformal prediction with GAM or polynomial logistic regression consistently achieves valid coverage aligned with specified confidence levels for faithfulness thresholds in LLMs

## Executive Summary
This paper addresses the critical challenge of selecting appropriate thresholds for evaluation metrics in large language models (LLMs), an area that significantly impacts model reliability and societal outcomes. The authors propose a systematic methodology that integrates stakeholder risk preferences into threshold determination, drawing from financial risk management practices and behavioral finance concepts like prospect theory. Using the HaluBench dataset and three popular faithfulness metric implementations, they demonstrate that different statistical methods (KDE, empirical recall, AUC-ROC, and conformal prediction) yield varying performance characteristics, with conformal prediction showing the most consistent coverage rates across confidence levels.

The study reveals that traditional methods like KDE and empirical recall often default to zero thresholds at higher confidence levels, while conformal prediction with GAM or polynomial logistic regression maintains valid coverage aligned with specified confidence levels. The authors emphasize that threshold selection is not merely a technical exercise but requires careful consideration of stakeholder risk tolerance and the broader implications for AI system deployment. Their findings suggest that a comprehensive approach combining statistical rigor with stakeholder engagement is essential for setting thresholds that enhance LLM accuracy and reliability while mitigating potential harms.

## Method Summary
The authors propose a methodology for determining thresholds for LLM evaluation metrics that integrates stakeholder risk preferences with statistical techniques. The approach involves: (1) collecting and preprocessing the HaluBench dataset with faithfulness scores computed using RAGAS, DeepEval, and UpTrain libraries; (2) applying five statistical methods (Z-score, KDE, empirical recall, AUC-ROC, and conformal prediction) to identify thresholds; (3) mapping stakeholder risk tolerance to statistical confidence levels using prospect theory; and (4) validating threshold performance through coverage rates and prediction set width. The conformal prediction framework uses GAM or polynomial logistic regression with Platt scaling as underlying classifiers, calibrated through a hold-out set to ensure valid coverage at specified confidence levels.

## Key Results
- Conformal prediction with GAM or polynomial logistic regression consistently achieves valid coverage aligned with specified confidence levels (95%, 90%, 80%, 70%, 60%, 50%)
- KDE and empirical recall methods often default to zero thresholds at higher confidence levels, particularly when scores cluster around ambiguous values (0.5)
- The methodology successfully translates stakeholder risk preferences into statistical confidence levels, enabling risk-informed threshold determination
- All three faithfulness metric implementations (RAGAS, DeepEval, UpTrain) benefit from the proposed threshold determination approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conformal prediction achieves valid coverage aligned with specified confidence levels for faithfulness thresholds.
- Mechanism: Conformal prediction uses a hold-out set to calibrate conformity scores based on calibrated probabilities from an underlying classifier. The (1-Î±)th quantile of these scores determines the threshold, ensuring the prediction set includes the correct label at the desired confidence level.
- Core assumption: The data is exchangeable, and the underlying classifier effectively calibrates probabilities to reflect true likelihoods of ground truth labels.
- Evidence anchors:
  - [abstract] "conformal prediction with GAM or polynomial logistic regression consistently achieves valid coverage aligned with specified confidence levels"
  - [section] "All methods achieve coverage rates that converge to the specified confidence levels, validating the conformal prediction framework"
  - [corpus] Weak - corpus mentions conformal prediction only in general AI context, not specifically for LLM evaluation metrics
- Break condition: When the underlying classifier fails to calibrate probabilities properly or when data exchangeability assumption is violated (e.g., temporal dependencies exist).

### Mechanism 2
- Claim: Stakeholder risk preferences can be translated into statistical confidence levels for threshold determination.
- Mechanism: Risk tolerance is quantified through behavioral finance concepts like prospect theory, where stakeholders' responses to hypothetical scenarios are fit to utility functions to extract risk aversion coefficients. These coefficients map to specific confidence levels (e.g., 95% confidence for moderate risk tolerance).
- Core assumption: Stakeholders' risk preferences can be meaningfully captured through utility function fitting and translated to statistical confidence levels.
- Evidence anchors:
  - [abstract] "integrates stakeholder risk preferences into threshold determination, drawing from financial risk management practices and behavioral finance concepts like prospect theory"
  - [section] "Translating the Risk Tolerance to a Statistical Quantity... the risk preferences should be translated into a corresponding statistical confidence level"
  - [corpus] Missing - corpus does not contain specific evidence about translating risk preferences to confidence levels
- Break condition: When stakeholders' risk preferences cannot be accurately captured through utility function fitting or when the mapping between risk tolerance and confidence levels is not well-defined.

### Mechanism 3
- Claim: KDE and empirical recall methods provide intuitive thresholds but often default to zero at higher confidence levels.
- Mechanism: KDE identifies local minima between modes in bimodal distributions to find midpoints, while empirical recall iteratively varies thresholds to find recall-coverage trade-offs. However, both methods struggle with ambiguous scores around 0.5, causing thresholds to default to zero at higher confidence levels.
- Core assumption: The faithfulness score distribution exhibits clear bimodal structure that can be separated by a meaningful midpoint or threshold.
- Evidence anchors:
  - [abstract] "KDE and empirical recall methods often default to zero thresholds at higher confidence levels"
  - [section] "The KDE method performs reasonably well at lower confidence levels... However, as the confidence level increases, the thresholds derived from the KDE method often fail to align with the desired levels"
  - [corpus] Missing - corpus does not contain evidence about KDE or empirical recall performance
- Break condition: When the score distribution lacks clear bimodality or when ambiguous scores dominate the distribution.

## Foundational Learning

- Concept: Kernel Density Estimation (KDE)
  - Why needed here: KDE is used to estimate the probability density function of faithfulness scores, which helps identify the midpoint between bimodal distributions for threshold determination.
  - Quick check question: What is the primary limitation of using KDE for threshold determination at high confidence levels?

- Concept: Conformal Prediction Framework
  - Why needed here: Conformal prediction provides a model-agnostic approach to generating prediction intervals at specified confidence levels, ensuring valid coverage for threshold determination.
  - Quick check question: What is the key assumption required for conformal prediction to work effectively?

- Concept: ROC Curves and AUC-ROC
  - Why needed here: ROC curves plot true positive rate against false positive rate across different thresholds, helping identify optimal thresholds that balance sensitivity and specificity for faithfulness evaluation.
  - Quick check question: How does AUC-ROC help in selecting thresholds that align with stakeholder risk preferences?

## Architecture Onboarding

- Component map:
  - HaluBench dataset -> preprocessing (filter short answers) -> split into train/test
  - RAGAS/DeepEval/UpTrain libraries -> faithfulness scores
  - KDE, empirical recall, ROC-AUC, conformal prediction methods
  - Stratified 5-fold cross-validation
  - Prospect theory utility functions -> confidence level mapping

- Critical path:
  1. Load and preprocess HaluBench dataset
  2. Compute faithfulness scores using all three libraries
  3. Partition data for cross-validation
  4. Apply each threshold determination method
  5. Evaluate threshold performance against ground truth
  6. Select optimal method based on coverage and informativeness

- Design tradeoffs:
  - Computational cost vs. accuracy: KDE and empirical recall are faster but less accurate at high confidence levels; conformal prediction is computationally intensive but provides valid coverage
  - Interpretability vs. performance: Simple methods (Z-score, KDE) are more interpretable but may not capture complex relationships; GAM/polynomial regression are less interpretable but more accurate
  - Generalizability vs. specificity: Methods requiring ground truth labels are more specific but less generalizable to unlabeled data

- Failure signatures:
  - Thresholds defaulting to zero at high confidence levels (KDE, empirical recall failure)
  - Coverage rates not matching specified confidence levels (conformal prediction failure)
  - Inconsistent thresholds across cross-validation folds (overfitting to specific data partitions)
  - Extremely wide prediction sets (conformal prediction inefficiency)

- First 3 experiments:
  1. Compare threshold performance across the three faithfulness libraries using conformal prediction at 95% confidence level
  2. Test KDE threshold identification on synthetic bimodal data with varying levels of ambiguity
  3. Validate that stakeholder risk preferences map correctly to confidence levels by testing different utility function forms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can stakeholders' risk tolerance be effectively quantified and translated into statistical confidence levels for threshold determination?
- Basis in paper: [explicit] The paper discusses various methods to identify AI risk tolerance, including borrowing concepts from behavioral finance like Prospect Theory and using questionnaires, but notes that prescribing a specific methodology is beyond the scope of the present work.
- Why unresolved: The authors acknowledge the importance of this step but do not provide a concrete methodology for translating risk tolerance into statistical confidence levels, leaving practitioners without clear guidance.
- What evidence would resolve it: Development and validation of a standardized framework for quantifying stakeholder risk tolerance, along with case studies demonstrating its application in threshold determination for LLM evaluation metrics.

### Open Question 2
- Question: How do different underlying classifiers (e.g., GAM, polynomial logistic regression, standard logistic regression) affect the performance and reliability of conformal prediction in threshold determination?
- Basis in paper: [explicit] The paper compares the performance of various classifiers within the conformal prediction framework, showing that GAM and polynomial logistic regression consistently produce narrower prediction sets and higher efficiency compared to standard logistic regression.
- Why unresolved: While the paper demonstrates the effectiveness of GAM and polynomial logistic regression, it does not provide a comprehensive analysis of the trade-offs and limitations of each classifier across different datasets and confidence levels.
- What evidence would resolve it: Extensive empirical studies comparing the performance of different classifiers across diverse datasets, confidence levels, and evaluation metrics, along with guidelines for selecting the most appropriate classifier based on specific use cases.

### Open Question 3
- Question: How can the proposed methodology be extended to identify thresholds for evaluation metrics in multi-modal AI systems beyond text-based LLMs?
- Basis in paper: [explicit] The authors mention that the procedure proposed in the present work can also be extended to identify thresholds for evaluation metrics for multi-model AI systems in the future.
- Why unresolved: The paper focuses on text-based LLM evaluation metrics and does not provide insights into how the methodology can be adapted for multi-modal systems involving audio, video, or other data types.
- What evidence would resolve it: Development and validation of threshold determination methods for multi-modal AI systems, along with case studies demonstrating their application in real-world scenarios involving diverse data types and evaluation metrics.

## Limitations
- Methodology's effectiveness for evaluation metrics beyond faithfulness remains untested
- Reliance on ground truth labels for KDE and empirical recall limits applicability to unlabeled data scenarios
- Translation between stakeholder risk preferences and statistical confidence levels lacks practical implementation guidance

## Confidence

**High Confidence Claims:**
- Conformal prediction methods consistently achieve valid coverage rates aligned with specified confidence levels when properly implemented with appropriate underlying classifiers (GAM or polynomial logistic regression)
- KDE and empirical recall methods perform adequately at lower confidence levels but systematically default to zero thresholds as confidence requirements increase

**Medium Confidence Claims:**
- The integration of stakeholder risk preferences through prospect theory provides a meaningful framework for threshold determination
- The five proposed statistical methods represent a comprehensive toolkit for threshold selection across different scenarios

**Low Confidence Claims:**
- The methodology's performance would remain consistent across different LLM evaluation metrics beyond faithfulness
- The risk preference-to-confidence level mapping would remain stable across different stakeholder groups or contexts

## Next Checks

1. **Cross-metric validation**: Apply the methodology to at least three additional LLM evaluation metrics (e.g., relevance, coherence, and answer correctness) using different datasets to assess generalizability. This would test whether the observed performance patterns hold beyond the faithfulness metric.

2. **Real-world stakeholder testing**: Conduct user studies with actual LLM developers and deployers to validate the practicality of translating risk preferences into statistical confidence levels. This would involve testing different utility function parameterizations and assessing whether stakeholders can meaningfully specify their preferences.

3. **Temporal robustness analysis**: Evaluate the methodology's performance on temporally-ordered data to test the exchangeability assumption underlying conformal prediction. This would involve splitting data by time rather than random partitioning and measuring how coverage rates change.