---
ver: rpa2
title: Fine-tuned network relies on generic representation to solve unseen cognitive
  task
arxiv_id: '2406.18926'
source_url: https://arxiv.org/abs/2406.18926
tags:
- task
- fine-tuned
- attention
- each
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether fine-tuned language models rely more
  on generic pretrained representations or develop task-specific solutions for novel
  cognitive tasks. The authors fine-tuned GPT-2 on a context-dependent decision-making
  task adapted from neuroscience literature, comparing it to a model trained from
  scratch on the same task.
---

# Fine-tuned network relies on generic representation to solve unseen cognitive task

## Quick Facts
- arXiv ID: 2406.18926
- Source URL: https://arxiv.org/abs/2406.18926
- Authors: Dongyan Lin
- Reference count: 21
- Fine-tuned GPT-2 models rely heavily on pretrained generic representations in later layers, while models trained from scratch develop task-specific solutions in earlier layers for context-dependent decision-making tasks.

## Executive Summary
This paper investigates whether fine-tuned language models rely more on generic pretrained representations or develop task-specific solutions when solving novel cognitive tasks. The author fine-tuned GPT-2 on a context-dependent decision-making task adapted from neuroscience literature and compared it to a model trained from scratch on the same task. The study found that fine-tuned models depend heavily on pretrained representations, particularly in later layers, while models trained from scratch develop different, more task-specific mechanisms, with significant performance drops upon ablating heads in the first layer.

## Method Summary
The study converted a neuroscience context-dependent decision-making task into a text-based format and fine-tuned GPT-2 on this task, comparing it to a model trained from scratch. The author analyzed model performance across varying training sample sizes and conducted ablation experiments on attention heads, UMAP dimensionality reduction on hidden states, logistic regression decoding of task-relevant variables, and SVM decoding of response types. The analysis focused on layer-wise differences in how the two types of models process information and solve the task.

## Key Results
- Fine-tuned models show significant performance drops when ablating attention heads in later layers, particularly L5H7, L8H3, L9H3, and L11H8
- Models trained from scratch show significant performance drops when ablating first-layer attention heads, suggesting task-specific solutions for numerical information extraction
- After fine-tuning, individual units in the last layer encode multiple independent task-relevant variables simultaneously, demonstrating mixed selectivity similar to biological neural networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned models rely more on pretrained generic representations, particularly in later layers, while models trained from scratch develop task-specific solutions.
- Mechanism: The pretrained model's later layers retain generic attention heads developed during pretraining, which are crucial for language modeling. When fine-tuned on a novel task, these heads continue to play a significant role in task performance. In contrast, models trained from scratch must develop their own mechanisms, which manifest in earlier layers for task-specific information extraction.
- Core assumption: Attention heads in later layers of pretrained models encode generic language representations that transfer to novel tasks.
- Evidence anchors:
  - [abstract] "Our results show that fine-tuned models depend heavily on pretrained representations, particularly in later layers, while models trained from scratch develop different, more task-specific mechanisms."
  - [section] "We found that fine-tuned models rely more on attention heads in later layers, particular examples included L5H7, L8H3, L9H3, L11H8, etc, where performance on the task had a significant drop after ablation."
  - [corpus] Weak - corpus mentions "fine-tuned" and "pretrained" but doesn't specifically discuss layer-wise reliance on generic vs. task-specific representations.
- Break condition: If ablation experiments showed that earlier layers were more critical for fine-tuned models, or if task-specific solutions emerged in later layers for models trained from scratch.

### Mechanism 2
- Claim: Models trained from scratch develop task-specific solutions in earlier layers for extracting numerical information.
- Mechanism: Without pretrained representations to rely on, models trained from scratch must develop their own mechanisms for processing task-relevant information. The study found that ablation of first-layer attention heads caused significant performance drops in models trained from scratch, suggesting these heads are crucial for extracting numerical information from the task prompts.
- Core assumption: Numerical information extraction is a fundamental requirement for the context-dependent decision-making task, and models trained from scratch must develop this capability from scratch.
- Evidence anchors:
  - [abstract] "In contrast, models trained from scratch developed task-specific solutions, with significant performance drops upon ablating heads in the first layer."
  - [section] "We found that first layer heads pay attention to numbers in the prompt (Figure A.5), which could be a task-specific solution that networks trained from scratch developed."
  - [corpus] Weak - corpus mentions "fine-tuned" and "pretrained" but doesn't specifically discuss first-layer specialization for numerical information extraction in models trained from scratch.
- Break condition: If ablation experiments showed that later layers were more critical for models trained from scratch, or if numerical information extraction wasn't a key component of the task.

### Mechanism 3
- Claim: Fine-tuned models benefit from mixed selectivity in their hidden states, encoding multiple task-relevant variables simultaneously.
- Mechanism: After fine-tuning, individual units in the last layer can encode multiple independent task-relevant variables at the same time, similar to the mixed selectivity phenomenon observed in biological neural networks. This allows for more efficient representation of complex task information.
- Core assumption: Mixed selectivity is beneficial for encoding complex task information and can emerge through fine-tuning on the context-dependent decision-making task.
- Evidence anchors:
  - [abstract] "We decoded task-relevant variables, including context, motion coherence (whether it's positive or negative), color coherence (whether it's positive or negative), and choice, from the hidden states of the last layer during the presentation of each token with logistic regression before and after fine-tuning."
  - [section] "We notice that, after fine-tuning, a single unit's activation can encode multiple independent task-relevant variables at the same time, similar to the mixed selectivity phenomenon in the brain (Rigotti et al., 2013)."
  - [corpus] Weak - corpus mentions "fine-tuned" and "pretrained" but doesn't specifically discuss mixed selectivity in hidden states after fine-tuning.
- Break condition: If decoding experiments showed that individual units only encoded single task-relevant variables, or if mixed selectivity didn't emerge after fine-tuning.

## Foundational Learning

- Concept: Attention mechanisms in transformer models
  - Why needed here: The study heavily relies on analyzing attention weights and attention outputs to understand how models process information. Understanding how attention mechanisms work is crucial for interpreting the ablation results and decoding experiments.
  - Quick check question: How do attention weights (softmax(QK^T/âˆšd)) determine which parts of the input a transformer model focuses on at each layer?

- Concept: Fine-tuning vs. training from scratch
  - Why needed here: The core comparison in the study is between models that are fine-tuned on a pretrained base versus models trained entirely from scratch on the same task. Understanding the differences in these approaches is essential for interpreting the results.
  - Quick check question: What are the key differences in how a model's parameters are updated during fine-tuning versus training from scratch, and how might these differences affect the resulting representations?

- Concept: Dimensionality reduction techniques (UMAP)
  - Why needed here: The study uses UMAP to project high-dimensional hidden states into a 2D space for visualization. Understanding how UMAP works and its limitations is important for interpreting the latent space visualizations.
  - Quick check question: How does UMAP preserve the local and global structure of high-dimensional data when projecting it to lower dimensions, and what are some potential pitfalls of this approach?

## Architecture Onboarding

- Component map:
  GPT-2 model architecture (12 layers, 12 attention heads per layer, 768-dimensional embeddings) -> Context-dependent decision-making task implementation -> Fine-tuning pipeline (data preparation, training loop, evaluation) -> Analysis tools (attention weight extraction, hidden state collection, UMAP projection, logistic regression decoding, SVM decoding, ablation experiments)

- Critical path:
  1. Convert neuroscience task to text-based format
  2. Fine-tune GPT-2 on the task or train from scratch
  3. Collect attention weights and hidden states during evaluation
  4. Perform UMAP projection on last layer hidden states
  5. Decode task-relevant variables from hidden states
  6. Conduct ablation experiments on attention heads
  7. Decode response types from attention outputs

- Design tradeoffs:
  - Using GPT-2 (117M parameters) vs. larger models: Smaller model allows for faster training and experimentation but may have limited capacity for complex tasks
  - Fine-tuning vs. training from scratch: Fine-tuning leverages pretrained knowledge but may be constrained by initial representations, while training from scratch allows for more specialized solutions but requires more data and computation
  - Grid search for hyperparameters vs. more targeted search: Grid search is more exhaustive but computationally expensive, while targeted search may miss optimal configurations

- Failure signatures:
  - If fine-tuned models don't show reliance on later layers: Could indicate that the pretrained representations aren't relevant to the task or that the fine-tuning process doesn't preserve them
  - If models trained from scratch don't develop task-specific solutions: Could suggest that the task is too simple or that the model architecture isn't suitable for the task
  - If ablation experiments don't show significant performance drops: Could indicate that the task is too easy or that the attention heads aren't crucial for the task

- First 3 experiments:
  1. Verify that the fine-tuned model can perform the context-dependent decision-making task above chance level
  2. Confirm that the model trained from scratch requires significantly more training samples to reach the same performance level as the fine-tuned model
  3. Perform UMAP projection on the last layer hidden states of both fine-tuned and from-scratch models to visualize differences in their latent space representations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific mechanisms by which networks trained from scratch solve the context-dependent decision-making task?
- Basis in paper: [inferred] The paper states that further investigation is needed to elucidate the exact mechanism by which models trained from scratch solve this task, and hypothesizes it could be a combination of first layer extracting numbers and later layers manipulating this information to make a decision.
- Why unresolved: The current analysis only shows that ablation of first layer heads causes significant performance drops, but does not detail the exact processing steps that occur in later layers to produce the final decision.
- What evidence would resolve it: Detailed analysis of hidden state representations and attention patterns across all layers, possibly using techniques like probing classifiers or causal tracing to map the flow of information from sensory inputs to final decisions.

### Open Question 2
- Question: How do pretrained representations support task-specific fine-tuning across different cognitive tasks?
- Basis in paper: [explicit] The authors state that further studies with more diverse cognitive tasks are required to understand how pretrained representations support task-specific fine-tuning.
- Why unresolved: The current study only examines one specific cognitive task, limiting the generalizability of findings to other tasks with different cognitive demands.
- What evidence would resolve it: Replication of the study with a variety of cognitive tasks (e.g., reasoning, memory, planning) to compare the reliance on pretrained representations and the development of task-specific solutions across different cognitive domains.

### Open Question 3
- Question: What quantitative metrics can be developed to rigorously assess the mechanisms of task-specific fine-tuning in LLMs?
- Basis in paper: [explicit] The authors acknowledge that much of their current findings are based on qualitative observations and that the field of mechanistic interpretability in LLMs requires new quantitative methods to advance.
- Why unresolved: Current analyses rely on qualitative interpretations of attention patterns and hidden state representations, which may be subjective and difficult to compare across studies.
- What evidence would resolve it: Development and validation of quantitative metrics to measure the degree of task-specificity in representations, the importance of different network components, and the alignment between biological and artificial neural networks.

## Limitations

- The study examines only one specific cognitive task and one model architecture, limiting generalizability to other domains or model types
- The ablation experiments show performance drops but don't establish causality or rule out alternative explanations for the observed effects
- The analysis relies heavily on qualitative interpretations of attention patterns and hidden state representations, which may be subjective

## Confidence

- **High Confidence**: The basic observation that fine-tuned models perform better than models trained from scratch on the same task is well-supported and aligns with established understanding of transfer learning benefits
- **Medium Confidence**: The layer-wise differences in reliance on pretrained representations are supported by ablation experiments, but the interpretation that this reflects generic vs. task-specific representations is somewhat speculative
- **Low Confidence**: The claims about mixed selectivity in hidden states and the specific role of attention heads in encoding task-relevant variables are based on decoding analyses that may be subject to overfitting or spurious correlations

## Next Checks

1. **Cross-task validation**: Repeat the fine-tuning vs. from-scratch comparison on multiple diverse cognitive tasks to assess whether the observed layer-wise patterns hold across different domains, or if they're specific to the context-dependent decision-making task studied.

2. **Alternative explanation testing**: Design ablation experiments that control for initialization effects by randomly shuffling attention heads between layers or using different random initializations for both fine-tuned and from-scratch models to determine if the observed patterns are due to pretraining specifically or other factors.

3. **Mechanistic intervention**: Implement targeted interventions that modify the representations in specific layers (rather than just ablation) and measure the causal impact on task performance, such as adding noise, modifying attention patterns, or using activation engineering to test whether the identified mechanisms are truly necessary and sufficient for the observed behavior.