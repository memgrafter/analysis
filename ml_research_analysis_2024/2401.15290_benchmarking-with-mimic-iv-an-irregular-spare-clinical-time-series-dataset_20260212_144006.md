---
ver: rpa2
title: Benchmarking with MIMIC-IV, an irregular, spare clinical time series dataset
arxiv_id: '2401.15290'
source_url: https://arxiv.org/abs/2401.15290
tags:
- data
- prediction
- time
- which
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a benchmarking study of machine learning models
  for predicting clinical outcomes using MIMIC-IV, a large-scale electronic health
  records dataset. The study focuses on two tasks: in-ICU mortality prediction and
  length of stay prediction.'
---

# Benchmarking with MIMIC-IV, an irregular, spare clinical time series dataset

## Quick Facts
- **arXiv ID**: 2401.15290
- **Source URL**: https://arxiv.org/abs/2401.15290
- **Authors**: Hung Bui; Harikrishna Warrier; Yogesh Gupta
- **Reference count**: 12
- **Primary result**: XGBoost outperforms deep learning models (LSTM, TCN) on MIMIC-IV clinical prediction tasks

## Executive Summary
This paper presents a benchmarking study comparing XGBoost, LSTM, and TCN models for predicting in-ICU mortality and length of stay using MIMIC-IV, a large-scale electronic health records dataset. The study evaluates these models on a subset of MIMIC-IV data limited to patients with chronic kidney disease, using standardized preprocessing and evaluation metrics. XGBoost demonstrates superior performance on both tasks, achieving AUC-ROC scores of 0.87 for mortality prediction and 0.76 for length of stay prediction. The LSTM and TCN models show similar performance with task-specific variations, with LSTM slightly outperforming TCN on mortality prediction and vice versa for length of stay prediction.

## Method Summary
The study uses MIMIC-IV version 2.2 ICU module data, extracting 407 lab/vital features and 1034 diagnosis features for mortality prediction, and 408 lab/vital features plus 1117 diagnosis features for length of stay prediction. Data preprocessing follows the Gupta et al. (2022) pipeline for handling irregular time-series data, including feature grouping, outlier removal, and imputation. Models are trained using 5-fold cross-validation with 80/20 train/test splits, and 10% validation data from training sets. XGBoost uses 300 estimators, 0.3 learning rate, and max depth of 6. LSTM employs 2 layers with 256 hidden units and 0.8 dropout. TCN uses 4 layers with 128 filters and kernel size 10.

## Key Results
- XGBoost achieves AUC-ROC of 0.87 for in-ICU mortality prediction and 0.76 for length of stay prediction
- LSTM and TCN models show similar performance with task-specific variations
- LSTM slightly outperforms TCN on mortality prediction, while TCN slightly outperforms LSTM on length of stay prediction
- Standardized benchmarking enables fair comparison across models and datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: XGBoost outperforms deep learning models on MIMIC-IV tasks despite irregular, sparse clinical time-series data.
- **Mechanism**: Gradient boosting combines weak decision tree learners, which handle heterogeneous feature types and missing values naturally, while deep models require more sophisticated architectures and preprocessing.
- **Core assumption**: Decision trees and boosting can effectively capture clinical patterns without complex temporal modeling.
- **Evidence anchors**:
  - [abstract]: "XGBoost outperforms the deep learning models on both tasks, achieving an AUC-ROC of 0.87 for mortality prediction and 0.76 for length of stay prediction."
  - [section]: "traditional machine learning model like XGBoost still able to outperform, and achieve good results in mortality and length of stay prediction tasks."
  - [corpus]: Weak - corpus contains newer deep learning methods but no direct comparison to XGBoost.

### Mechanism 2
- **Claim**: LSTM and TCN models show similar performance, with task-specific variations.
- **Mechanism**: Both models handle temporal dependencies, but TCN's causal convolutions with dilations and residual connections may better capture local patterns for LOS prediction, while LSTM's gating may better handle the binary mortality prediction.
- **Core assumption**: The architectural differences between TCN and LSTM align with task-specific temporal patterns in clinical data.
- **Evidence anchors**:
  - [abstract]: "The LSTM and TCN models show similar performance, with LSTM slightly outperforming TCN on mortality prediction and vice versa for length of stay prediction."
  - [section]: "TCN models substantially outperform generic recurrent architectures such as Long Short-Term Memory (LSTMs)" (referencing related work)
  - [corpus]: Weak - corpus contains newer methods but no direct TCN vs LSTM comparison on MIMIC-IV.

### Mechanism 3
- **Claim**: Standardized benchmarking pipeline enables fair comparison across models and datasets.
- **Mechanism**: Consistent data extraction, preprocessing, and evaluation metrics remove confounding factors and allow apples-to-apples comparison of model performance.
- **Core assumption**: Differences in model performance reflect true algorithmic differences rather than implementation or data handling variations.
- **Evidence anchors**:
  - [section]: "As a result, it is needed to have standardized benchmarking with same data pipeline for verifying those algorithms."
  - [section]: "Although much research are focusing on applying complex deep learning algorithms to MIMIC dataset, traditional machine learning model like XGBoost still able to outperform..."
  - [corpus]: Weak - corpus contains newer methods but doesn't address benchmarking standardization.

## Foundational Learning

- **Concept**: Irregular time series data handling
  - **Why needed here**: MIMIC-IV contains measurements at uneven intervals requiring models that can handle missingness and temporal irregularity.
  - **Quick check question**: How do you handle missing values in irregularly sampled clinical time series?

- **Concept**: Gradient boosting principles
  - **Why needed here**: XGBoost's superior performance suggests understanding boosting mechanics is crucial for clinical prediction tasks.
  - **Quick check question**: What is the difference between bagging and boosting in ensemble methods?

- **Concept**: Recurrent vs. convolutional architectures
  - **Why needed here**: LSTM and TCN show task-specific performance differences, requiring understanding of their strengths and weaknesses.
  - **Quick check question**: When would you choose a TCN over an LSTM for time series prediction?

## Architecture Onboarding

- **Component map**: Data extraction -> Feature preprocessing -> Model training -> Cross-validation -> Evaluation
- **Critical path**: Data preprocessing → Model training → Cross-validation → Evaluation
- **Design tradeoffs**:
  - XGBoost: Simpler, handles missing values, but limited temporal modeling
  - LSTM: Strong temporal modeling, but requires careful hyperparameter tuning
  - TCN: Good local pattern capture, but may miss long-range dependencies
- **Failure signatures**:
  - Overfitting: High training performance but low validation performance
  - Data leakage: Performance drops significantly when using different validation splits
  - Preprocessing errors: Missing or incorrectly imputed values affecting model inputs
- **First 3 experiments**:
  1. Train XGBoost with default parameters on preprocessed data to establish baseline
  2. Train LSTM with 2 layers, 256 hidden units, 0.8 dropout on same preprocessed data
  3. Train TCN with 4 layers, 128 filters, kernel size 10 on same preprocessed data

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of complex deep learning models (LSTM, TCN) compare to XGBoost when using the entire MIMIC-IV dataset rather than a subset of patients with chronic kidney disease?
- **Basis in paper**: [explicit] The paper states that XGBoost outperformed LSTM and TCN on a subset of MIMIC-IV data limited to patients with chronic kidney disease, but does not evaluate these models on the full dataset.
- **Why unresolved**: The study only used a subset of MIMIC-IV data (patients with chronic kidney disease) for training and evaluation, leaving the performance on the full dataset unknown.
- **What evidence would resolve it**: Benchmarking the same models (XGBoost, LSTM, TCN) on the full MIMIC-IV dataset using the same tasks (in-ICU mortality, length of stay) and evaluation metrics.

### Open Question 2
- **Question**: Can transformer-based models (e.g., STraTS) outperform XGBoost on irregularly sampled clinical time series data in MIMIC-IV?
- **Basis in paper**: [inferred] The paper mentions STraTS as a state-of-the-art model for clinical time series but does not include it in the benchmark comparison with XGBoost, LSTM, and TCN.
- **Why unresolved**: STraTS was not evaluated alongside the other models in the benchmark, so its relative performance is unknown.
- **What evidence would resolve it**: Adding STraTS to the benchmark and comparing its performance to XGBoost, LSTM, and TCN on the same tasks and dataset.

### Open Question 3
- **Question**: What is the impact of using additional clinical features (e.g., notes, medications) on the performance of mortality and length of stay prediction models in MIMIC-IV?
- **Basis in paper**: [inferred] The paper focuses on lab/vital signs and diagnosis features but does not explore the potential benefits of incorporating additional data types like clinical notes or medication records.
- **Why unresolved**: The study did not experiment with incorporating additional clinical features beyond the ones mentioned, so the potential performance gains are unknown.
- **What evidence would resolve it**: Expanding the feature set to include clinical notes, medication data, or other relevant features and evaluating the impact on model performance for the same tasks.

## Limitations

- **Unknown preprocessing details**: Exact implementation of Gupta et al. (2022) pipeline for MIMIC-IV data extraction and preprocessing is not fully specified
- **Limited dataset scope**: Study only uses subset of MIMIC-IV data (patients with chronic kidney disease), limiting generalizability
- **Missing model comparison**: Does not evaluate transformer-based models like STraTS alongside XGBoost, LSTM, and TCN

## Confidence

- **High confidence**: XGBoost outperforming deep learning models on both tasks (AUC-ROC of 0.87 for mortality, 0.76 for LOS)
- **Medium confidence**: Task-specific performance differences between LSTM and TCN models (LSTM slightly better on mortality, TCN slightly better on LOS)
- **Medium confidence**: Importance of standardized benchmarking for fair comparison across models

## Next Checks

1. **Data preprocessing verification**: Reconstruct the exact feature extraction and preprocessing pipeline from MIMIC-IV using the Gupta et al. (2022) methodology to ensure feature counts and transformations match the original study.

2. **Baseline model reproduction**: Train XGBoost with default parameters on the preprocessed data to establish a baseline performance, then systematically vary key hyperparameters to understand their impact on results.

3. **Cross-study comparison**: Apply the exact same benchmarking pipeline to a subset of related studies from the corpus to verify that performance differences are due to model architecture rather than implementation variations.