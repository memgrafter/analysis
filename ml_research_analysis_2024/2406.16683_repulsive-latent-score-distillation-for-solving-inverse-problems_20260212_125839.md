---
ver: rpa2
title: Repulsive Latent Score Distillation for Solving Inverse Problems
arxiv_id: '2406.16683'
source_url: https://arxiv.org/abs/2406.16683
tags:
- diffusion
- rlsd
- repulsion
- variational
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses mode collapse and latent space inversion issues
  in diffusion-based inverse problems. The authors propose Repulsive Latent Score
  Distillation (RLSD), which combines a multimodal variational approximation with
  a repulsion mechanism to promote diversity among particles and an augmented variational
  distribution to disentangle latent and data spaces.
---

# Repulsive Latent Score Distillation for Solving Inverse Problems

## Quick Facts
- arXiv ID: 2406.16683
- Source URL: https://arxiv.org/abs/2406.16683
- Authors: Nicolas Zilberstein; Morteza Mardani; Santiago Segarra
- Reference count: 40
- Key outcome: RLSD achieves PSNR gains of 1-2 dB and FID improvements of 30-50 points while maintaining diversity in diffusion-based inverse problems

## Executive Summary
This paper addresses critical limitations in diffusion-based inverse problem solving: mode collapse and latent space inversion issues. The authors propose Repulsive Latent Score Distillation (RLSD), which combines a multimodal variational approximation with a repulsion mechanism to promote diversity among particles and an augmented variational distribution to disentangle latent and data spaces. RLSD applies two regularizations: denoising via score-matching and repulsion to encourage diversity. Experiments on linear and nonlinear inverse tasks with 512×512 images using Stable Diffusion models show RLSD outperforms existing methods, achieving PSNR gains of 1-2 dB and FID improvements of 30-50 points while maintaining diversity. The repulsion mechanism significantly boosts diversity in ill-posed problems like inpainting and phase retrieval, with diversity scores increasing by 3-4x compared to non-repulsive methods.

## Method Summary
RLSD solves inverse problems by combining score distillation sampling with a particle-based variational approximation that includes a repulsion mechanism. The method uses an augmented variational distribution with an auxiliary variable in data space to disentangle latent and data spaces, addressing inversion issues from adversarial training. During optimization, RLSD applies two regularizations: denoising via score-matching to enforce the prior, and repulsion to promote diversity. The repulsion is implemented using kernel-based similarity penalties between particles, with DINO features showing better performance than Euclidean or LPIPS distances. The algorithm uses 4 particles per measurement and runs for 1000 steps with Adam optimizer, balancing quality and diversity through tunable hyperparameters.

## Key Results
- PSNR improvements of 1-2 dB over state-of-the-art methods across linear and nonlinear inverse problems
- FID score reductions of 30-50 points, indicating significantly better sample quality and diversity
- Diversity scores increase by 3-4x in ill-posed problems like inpainting and phase retrieval when using repulsion
- RLSD successfully generates diverse solutions for highly underdetermined inverse problems while maintaining competitive reconstruction quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The repulsion mechanism promotes diversity among particles by penalizing pairwise kernel-based similarity, preventing mode collapse in high-dimensional spaces.
- **Mechanism:** A particle-based variational approximation with repulsive forces pushes particles away from collapsing to the same solution, enabling exploration of multiple modes in the posterior distribution.
- **Core assumption:** Mode collapse in high-dimensional spaces is primarily caused by particles converging to the same local minimum due to the unimodal nature of standard variational approximations.
- **Evidence anchors:**
  - [abstract] "We propose a multimodal variational approximation with a repulsion mechanism that promotes diversity among particles by penalizing pairwise kernel-based similarity."
  - [section] "We employ an ensemble of interactive particles with repulsion to prevent collapse."
  - [corpus] Weak - corpus papers focus on SDS and diffusion models but don't specifically address the repulsion mechanism.
- **Break condition:** The repulsion mechanism may increase computational demands, potentially making the method unsuitable for real-time applications.

### Mechanism 2
- **Claim:** The augmented variational distribution disentangles the latent and data spaces, mitigating latent space inversion issues arising from adversarial training of autoencoders.
- **Mechanism:** An auxiliary variable x0 defined in the data (pixel) space is introduced, allowing the optimization to decouple the latent (prior) from the data (measurement), yielding solutions with sharper details.
- **Core assumption:** Direct optimization in the latent space of diffusion models produces blurry reconstructions due to the nonlinearity of the decoder and the adversarial training of the encoder-decoder pair.
- **Evidence anchors:**
  - [abstract] "To mitigate latent space ambiguity, we extend this framework with an augmented variational distribution that disentangles the latent and data."
  - [section] "We hypothesize that the primary issue with mode collapse arises from collapse in high-dimensional spaces."
  - [corpus] Weak - corpus papers mention latent diffusion models but don't discuss the specific augmented variational formulation.
- **Break condition:** The augmented variational formulation may be unstable for nonlinear inverse problems, as it cannot express the mean and covariance as easily as for linear problems.

### Mechanism 3
- **Claim:** The combination of denoising and repulsion regularizations allows controllable trade-off between quality and diversity in the generated solutions.
- **Mechanism:** The algorithm applies two regularizations: denoising via score-matching to enforce the prior, and repulsion to promote diversity, enabling control over speed and diversity through simple regularization weights.
- **Core assumption:** By adjusting the scalar weights between denoising and repulsion regularizations, one can achieve a desired balance between reconstruction quality and solution diversity.
- **Evidence anchors:**
  - [abstract] "Our repulsive augmented formulation balances computational efficiency, quality, and diversity."
  - [section] "This repulsive augmented formulation balances computational efficiency, quality, and diversity."
  - [corpus] Weak - corpus papers mention SDS and diffusion models but don't discuss the specific combination of denoising and repulsion regularizations.
- **Break condition:** The method introduces additional hyperparameters, requiring careful tuning to achieve the desired balance between quality and diversity.

## Foundational Learning

- **Concept:** Diffusion models and score distillation sampling
  - **Why needed here:** The paper builds upon score distillation sampling (SDS) to solve inverse problems using pre-trained diffusion models, addressing its limitations in terms of mode collapse and latent space inversion.
  - **Quick check question:** What are the two major challenges faced by SDS when dealing with high-dimensional data in inverse problems?
- **Concept:** Variational inference and posterior sampling
  - **Why needed here:** The paper frames the inverse problem as a variational inference task, where the goal is to sample from the posterior distribution p(x0|y) by minimizing the KL divergence between a variational distribution and the true posterior.
  - **Quick check question:** How does the paper propose to solve the inverse problem using a variational approach with pre-trained diffusion models?
- **Concept:** Wasserstein gradient flow and particle systems
  - **Why needed here:** The paper leverages the Wasserstein gradient flow interpretation of SDS to introduce a particle-based variational approximation with a repulsion mechanism, promoting diversity among particles.
  - **Quick check question:** How does the Wasserstein gradient flow interpretation of SDS inform the design of the repulsive variational approximation in the paper?

## Architecture Onboarding

- **Component map:** Pre-trained Stable Diffusion model -> Particle-based variational approximation with repulsion -> Augmented variational distribution -> Optimization with denoising and repulsion regularizations
- **Critical path:** Initialize particles → Apply forward diffusion → Compute gradients using denoising and repulsion terms → Update particles through optimization steps → Generate final solutions
- **Design tradeoffs:** Trades computational efficiency for diversity by using particle-based methods with repulsion (4× cost of standard SDS) but achieves significantly better mode coverage and sample quality
- **Failure signatures:** Mode collapse (diversity scores <0.01), poor reconstruction quality (low PSNR, high FID), or latent space inversion producing blurry results
- **First 3 experiments:**
  1. Implement the repulsive variational approximation with a simple kernel function (e.g., Euclidean distance) and test on a toy bimodal Gaussian distribution to observe the effect of the repulsion weight on diversity.
  2. Integrate the repulsive variational approximation into the augmented formulation and test on a simple linear inverse problem (e.g., super-resolution) to validate the disentanglement of the latent and data spaces.
  3. Perform an ablation study on the choice of kernel function (e.g., DINO features, LPIPS) and its impact on the diversity and quality of the generated solutions for a complex inverse problem (e.g., inpainting).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the choice of kernel function (Euclidean, DINO, LPIPS) affect the diversity and quality trade-off in RLSD, and is there an optimal kernel for specific types of inverse problems?
- **Basis in paper:** [explicit] The paper compares different kernel functions (Euclidean, DINO, LPIPS) for the repulsion term, noting that LPIPS produces artifacts and is not well aligned with text prompts, while DINO and Euclidean show different qualitative results.
- **Why unresolved:** While the paper provides qualitative examples comparing kernels, it does not conduct a systematic quantitative analysis to determine the optimal kernel function for different inverse problems or the specific characteristics that make one kernel more effective than another.
- **What evidence would resolve it:** A comprehensive study comparing the performance of different kernels across a wide range of inverse problems (linear and nonlinear) with quantitative metrics for diversity, quality, and computational efficiency, potentially leading to a kernel selection framework.

### Open Question 2
- **Question:** Can the repulsion mechanism in RLSD be made adaptive based on the noise level or the characteristics of the inverse problem, and how would this affect performance?
- **Basis in paper:** [inferred] The paper mentions that the amount of particles is a hyperparameter allowing control over the trade-off between diversity and speed, and notes that the method introduces additional hyperparameters requiring better coupling for noise levels and deriving repulsion weights based on the forward operator.
- **Why unresolved:** The current implementation uses fixed repulsion weights and a single kernel function throughout the sampling process. The paper acknowledges the need for adaptive kernel learning and better coupling with noise levels but does not provide a solution or experimental validation.
- **What evidence would resolve it:** An adaptive RLSD framework that adjusts repulsion strength and kernel parameters based on noise level, problem type, or local geometry of the posterior distribution, validated through experiments showing improved performance across diverse inverse problems.

### Open Question 3
- **Question:** How does RLSD perform on inverse problems with extremely high-dimensional data (e.g., medical imaging, hyperspectral imaging) compared to other state-of-the-art methods?
- **Basis in paper:** [explicit] The paper demonstrates RLSD on 512×512 images and mentions that the method can handle large-scale inverse problems, but does not test it on extremely high-dimensional data or compare it with specialized methods for such domains.
- **Why unresolved:** While RLSD shows promising results on standard image sizes, its scalability and effectiveness on extremely high-dimensional data remains untested. The computational complexity of the repulsion term and the need for large particle ensembles in high dimensions may pose challenges not addressed in the current study.
- **What evidence would resolve it:** Extensive experiments on extremely high-dimensional inverse problems (e.g., 3D medical volumes, hyperspectral images) comparing RLSD with domain-specific state-of-the-art methods, analyzing both quantitative performance metrics and computational resource requirements.

## Limitations
- Computational overhead of particle-based methods with repulsion (approximately 4× the cost of standard SDS) may limit scalability to larger models or real-time applications
- The augmented variational formulation's stability for highly nonlinear inverse problems is not thoroughly explored, particularly for measurement operators with complex nonlinearities
- Effectiveness of repulsion mechanism in extremely high-dimensional spaces (beyond image generation) remains uncertain as the paper only validates on 512×512 images

## Confidence

**Major uncertainties:**
The repulsion mechanism's effectiveness in extremely high-dimensional spaces (beyond image generation) remains uncertain, as the paper only validates on 512×512 images. The computational overhead of particle-based methods with repulsion (approximately 4× the cost of standard SDS) may limit scalability to larger models or real-time applications. The augmented variational formulation's stability for highly nonlinear inverse problems is not thoroughly explored, particularly for measurement operators with complex nonlinearities.

**Confidence labels:**
- Mechanism 1 (Repulsion for diversity): High - Multiple experiments demonstrate consistent diversity improvements (3-4×) across different inverse problems
- Mechanism 2 (Augmented variational for disentanglement): Medium - Evidence shows improved FID scores, but the relationship between disentanglement and reconstruction quality could be more explicitly established
- Mechanism 3 (Quality-diversity trade-off): Medium - The paper demonstrates controllable trade-offs but doesn't explore the full parameter space systematically

## Next Checks

1. Test RLSD on 1024×1024 images or video frame reconstruction to evaluate scalability and computational demands of the repulsion mechanism in larger problem spaces
2. Implement a quantitative analysis of the augmented variational distribution's effect on latent space geometry by measuring latent space distances between solutions and comparing with non-augmented methods
3. Conduct a systematic hyperparameter sensitivity analysis varying repulsion weight γ and denoising weight λ across multiple orders of magnitude to map the complete quality-diversity trade-off landscape