---
ver: rpa2
title: Sharp Generalization for Nonparametric Regression in Interpolation Space by
  Over-Parameterized Neural Networks Trained with Preconditioned Gradient Descent
  and Early Stopping
arxiv_id: '2407.11353'
source_url: https://arxiv.org/abs/2407.11353
tags:
- theorem
- have
- follows
- lemma
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies nonparametric regression using an over-parameterized\
  \ two-layer neural network trained with early stopping. The authors propose a Preconditioned\
  \ Gradient Descent (PGD) algorithm that achieves a sharp regression rate of O(n^{-4\u03B1\
  /(4\u03B1+1)}) when the target function has spectral bias and lies in an interpolation\
  \ space."
---

# Sharp Generalization for Nonparametric Regression in Interpolation Space by Over-Parameterized Neural Networks Trained with Preconditioned Gradient Descent and Early Stopping

## Quick Facts
- **arXiv ID**: 2407.11353
- **Source URL**: https://arxiv.org/abs/2407.11353
- **Reference count**: 40
- **Key outcome**: Preconditioned Gradient Descent (PGD) achieves sharp minimax optimal rate O(n^{-4α/(4α+1)}) for nonparametric regression when target function has spectral bias, outperforming regular GD and kernel methods.

## Executive Summary
This paper studies nonparametric regression using over-parameterized two-layer neural networks trained with early stopping. The authors propose Preconditioned Gradient Descent (PGD) which achieves sharper generalization rates than regular Gradient Descent (GD) when the target function has spectral bias and lies in an interpolation space. The key innovation is that PGD induces a different kernel structure that enables improved convergence rates. The analysis decomposes the network output into a function in the reproducing kernel Hilbert space of an induced integral kernel plus a small residual error, enabling tight generalization bounds via local Rademacher complexity.

## Method Summary
The method trains a two-layer neural network using Preconditioned Gradient Descent with early stopping. The network architecture consists of m neurons with fixed random weights and learned coefficients. PGD uses a preconditioner matrix M to modify the gradient updates, which induces a different kernel structure during training compared to regular GD. The training is stopped early based on critical stopping times derived from kernel complexity bounds. The analysis shows that under appropriate scaling conditions on network width and training time, the final network function can be decomposed into an RKHS component plus a small residual error, enabling sharp generalization bounds.

## Key Results
- PGD achieves minimax optimal rate O(n^{-4α/(4α+1)}) for spectral bias functions, sharper than GD's O(n^{-2α/(2α+1)}) and standard kernel methods
- The method works for general target functions in interpolation spaces without distributional assumptions
- Early stopping with properly quantified stopping times achieves optimal rates while avoiding overfitting
- The decomposition framework provides a new theoretical tool for analyzing neural network generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preconditioned Gradient Descent (PGD) achieves a sharper minimax optimal rate of O(n^{-4α/(4α+1)}) compared to regular GD's O(n^{-2α/(2α+1)}) for nonparametric regression when the target function has spectral bias.
- Mechanism: PGD induces a different kernel K(int) with lower kernel complexity during training compared to the regular NTK induced by GD. Since λ_j^{int} = λ_j^2 < λ_j, the eigenvalues decay faster, leading to sharper generalization bounds.
- Core assumption: The target function has spectral bias and lies in HK(int)(f0) rather than just HK(f0).
- Evidence anchors:
  - [abstract]: "This rate is even sharper than the currently known nearly-optimal rate... and the standard kernel regression rate... obtained under the regular Neural Tangent Kernel (NTK) regime when training the neural network with the vanilla gradient descent"
  - [section]: "The proposed PGD avoids the usual linear regime specified by the NTK (2). Because λ_j^{int} = λ_j^2 < λ_j, the underlying reason for the sharper bound achieved by PGD is that the kernel complexity of K(int), RK(int)(·), is lower than that of K"
  - [corpus]: Weak - no direct mention of PGD achieving sharper rates in related works
- Break condition: If the target function doesn't have spectral bias (i.e., f* ∈ HK(f0) rather than HK(int)(f0)), the rate advantage disappears.

### Mechanism 2
- Claim: Uniform convergence to the NTK during training enables decomposition of the network output into RKHS function plus small L∞ error.
- Mechanism: During training, the neural network function can be decomposed as f = h + e where h ∈ HK(int) has bounded norm and e has small L∞ norm. This decomposition enables tight generalization bounds via local Rademacher complexity.
- Core assumption: The neural network width m and sample size N satisfy specific scaling conditions relative to training time T and noise level.
- Evidence anchors:
  - [abstract]: "Our analysis is based on decomposing the network output into a function in the reproducing kernel Hilbert space of a newly induced integral kernel and a residual function with small L∞-norm"
  - [section]: "First, uniform convergence to the NTK is established during the training process by PGD or GD, so that we can have a nice decomposition of the neural network function at any step of GD or PGD into a function in the RKHS and an error function with small L∞-norm"
  - [corpus]: Weak - related works mention decomposition but not this specific uniform convergence framework
- Break condition: If the uniform convergence conditions fail (e.g., insufficient over-parameterization), the decomposition breaks down.

### Mechanism 3
- Claim: Early stopping with properly quantified stopping times achieves minimax optimal rates without distributional assumptions for general target functions.
- Mechanism: By stopping training at time T ≤ bTK where bTK is the critical stopping time based on kernel complexity, the method achieves optimal generalization while avoiding overfitting.
- Core assumption: The neural network width m and training time T satisfy m ≳ n^{16α/(2α+1)}d^2 and T ≤ bTK.
- Evidence anchors:
  - [abstract]: "This rate is also sharper than the standard kernel regression rate... obtained under the regular Neural Tangent Kernel (NTK) regime when training the neural network with the vanilla gradient descent"
  - [section]: "Theorem 4.2 presents the minimax optimal rate of the risk when λ_j ≍ j^{-2α} for j ≥ 1... By a careful analysis using Theorem C.8 in Section C of the appendix, the following theorem shows that GD finds an over-parameterized neural network with minimax optimal rate"
  - [corpus]: Weak - related works mention early stopping but not this specific characterization of stopping times
- Break condition: If T exceeds bTK, the method loses the sharp rate and may overfit.

## Foundational Learning

- Concept: Reproducing Kernel Hilbert Space (RKHS) and integral operators
  - Why needed here: The analysis relies on decomposing network functions into RKHS components and analyzing their complexity via kernel eigenvalues
  - Quick check question: What is the relationship between the eigenvalues of the integral operator TK and those of TK(int)?

- Concept: Local Rademacher complexity
  - Why needed here: Used to tightly bound the Rademacher complexity of function classes containing neural network outputs
  - Quick check question: How does local Rademacher complexity differ from standard Rademacher complexity in terms of providing sharper bounds?

- Concept: Spectral bias in neural networks
  - Why needed here: The sharper rates for PGD rely on target functions having spectral bias (concentrating on low-frequency components)
  - Quick check question: What characterizes functions with spectral bias in the context of RKHS?

## Architecture Onboarding

- Component map:
  Two-layer neural network -> Preconditioned Gradient Descent (PGD) -> Early stopping -> RKHS decomposition -> Local Rademacher complexity bounds

- Critical path:
  1. Initialize weights with the specified random structure
  2. Generate preconditioner M using Algorithm 3
  3. Run PGD for T steps with early stopping
  4. Decompose final function into h + e components
  5. Apply local Rademacher complexity bounds

- Design tradeoffs:
  - PGD vs GD: PGD achieves sharper rates but requires computing and applying the preconditioner
  - Width m: Must scale as n^{32α/(4α+1)}d^2 for PGD vs n^{16α/(2α+1)}d^2 for GD
  - Early stopping: Critical for avoiding overfitting while achieving optimal rates

- Failure signatures:
  - If m is too small: Decomposition fails, uniform convergence doesn't hold
  - If T exceeds bT or bTK: Rates degrade, overfitting occurs
  - If preconditioner M is poorly estimated: PGD loses its advantage

- First 3 experiments:
  1. Verify decomposition works: Train network with PGD, check that f ≈ h + e with small ∥e∥∞
  2. Test rate sensitivity: Vary m and T, measure actual convergence rate vs theoretical predictions
  3. Compare PGD vs GD: Train both methods on same data, compare final generalization error and convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sharp convergence rate of O(n^{-4α/(4α+1)}) achieved by PGD for target functions with spectral bias be extended to other kernel-based methods beyond the integral kernel K(int)?
- Basis in paper: [explicit] The paper shows PGD induces a different kernel (K(int)) with lower complexity than NTK, leading to improved rates for spectral bias.
- Why unresolved: The analysis is specific to the integral kernel and its spectral properties; extending to other kernels requires new theoretical tools.
- What evidence would resolve it: A rigorous proof that PGD induces a kernel with provably lower complexity for spectral bias in a general class of kernels.

### Open Question 2
- Question: What are the precise conditions on the data distribution beyond uniform on the unit sphere that allow PGD to achieve the sharp rate O(n^{-4α/(4α+1)}) for spectral bias?
- Basis in paper: [inferred] The current analysis requires uniform distribution, but spectral bias is observed in other settings.
- Why unresolved: The proof relies on specific eigenvalue decay rates for the uniform distribution.
- What evidence would resolve it: An extension of Theorem 4.1 to a broader class of distributions with controlled eigenvalue decay.

### Open Question 3
- Question: How does the choice of preconditioner matrix M in PGD affect the generalization performance and convergence rate for nonparametric regression?
- Basis in paper: [explicit] The paper proposes a specific M generated by Algorithm 3, but doesn't explore alternatives.
- Why unresolved: The analysis assumes a particular M; the impact of different preconditioners is unexplored.
- What evidence would resolve it: Empirical or theoretical comparison of different M designs on generalization and rates.

### Open Question 4
- Question: Can the early stopping criterion T ≤ bTK for GD be made adaptive to the data and target function rather than relying on the fixed kernel complexity bound?
- Basis in paper: [explicit] The stopping time bTK is derived from kernel complexity, which is data-independent.
- Why unresolved: bTK is a conservative bound; an adaptive criterion could stop earlier without sacrificing rates.
- What evidence would resolve it: A data-dependent stopping rule that provably achieves the same or better rates.

## Limitations

- The analysis requires specific spectral properties of the target function that may not hold in practice
- The uniform convergence results depend on precise scaling relationships that could be sensitive to implementation details
- The preconditioner construction and its computational complexity are not fully specified
- The theoretical guarantees are established only for two-layer networks with ReLU activation

## Confidence

- **High confidence**: The framework for decomposing neural network outputs into RKHS components plus small residual errors is well-established and mathematically rigorous
- **Medium confidence**: The mechanism by which PGD induces different kernel properties compared to GD is theoretically sound but may be sensitive to the specific preconditioner construction
- **Low confidence**: The empirical performance gap between PGD and GD in realistic settings is not demonstrated

## Next Checks

1. **Decomposition verification**: Implement the PGD training procedure and empirically verify that the network output can be decomposed as f = h + e where h belongs to the RKHS of the integral kernel and ∥e∥∞ is small, as predicted by theory.

2. **Rate sensitivity analysis**: Systematically vary the network width m, training time T, and noise level σ to measure how the empirical convergence rate changes relative to the theoretical predictions for both PGD and GD methods.

3. **Preconditioner impact study**: Compare PGD with different preconditioner constructions (including approximations) against GD to quantify the practical benefits and identify the minimal computational requirements for achieving sharper rates.