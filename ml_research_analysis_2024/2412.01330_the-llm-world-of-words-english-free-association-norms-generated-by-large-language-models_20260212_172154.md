---
ver: rpa2
title: The "LLM World of Words" English free association norms generated by large
  language models
arxiv_id: '2412.01330'
source_url: https://arxiv.org/abs/2412.01330
tags:
- activation
- network
- semantic
- responses
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the "LLM World of Words" (LWOW), a new dataset
  of English free association norms generated by three large language models (Mistral,
  Llama3, and Haiku). The dataset is modeled after the "Small World of Words" (SWOW)
  human-generated norms, containing over 12,000 cue words with three responses each,
  repeated 100 times, resulting in over 3 million responses.
---

# The "LLM World of Words" English free association norms generated by large language models

## Quick Facts
- arXiv ID: 2412.01330
- Source URL: https://arxiv.org/abs/2412.01330
- Reference count: 40
- Primary result: LLM-generated free association norms can reproduce human-like semantic networks and gender bias patterns

## Executive Summary
This paper introduces the "LLM World of Words" (LWOW), a dataset of English free association norms generated by three large language models (Mistral, Llama3, and Haiku). Modeled after the "Small World of Words" (SWOW) human-generated norms, LWOW contains over 12,000 cue words with three responses each, repeated 100 times, resulting in over 3 million responses. The authors validate their approach by constructing cognitive network models from both SWOW and LWOW datasets and demonstrating that spreading activation patterns correlate with behavioral data from lexical decision tasks. The study reveals that LLMs encode gender biases similar to humans and provides a method for investigating semantic capabilities across humans and AI systems.

## Method Summary
The authors generated LWOW datasets by prompting three LLMs (Mistral, Llama3, Haiku) with 12,282 cue words from the SWOW dataset, requesting three free associations per cue repeated 100 times. The resulting responses underwent identical preprocessing to SWOW data, including lowercasing, removing articles/prepositions, replacing underscores, fixing spelling errors, and lemmatization. Cognitive network models were constructed from both human and LLM datasets, with edges representing association frequencies between words. The networks were validated using spreading activation processes, where prime nodes activated connected nodes proportionally to edge weights. Gender bias was quantified by comparing activation levels of stereotype-consistent versus stereotype-inconsistent targets when primed with gender-related cues.

## Key Results
- LWOW datasets successfully reproduced similar spreading activation patterns to SWOW when validated against behavioral lexical decision task data
- Both human and LLM networks showed statistically significant gender bias effects, with stereotype-consistent targets receiving higher activation from gender-related primes
- Network statistics showed comparable structure between human and LLM networks after preprocessing and filtering
- Effect sizes for gender bias varied across models, with humans and Haiku showing the largest effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated free associations can substitute for human free associations in building cognitive network models
- Mechanism: The LWOW dataset uses the same cue words as SWOW and applies identical preprocessing steps, ensuring structural comparability. Networks built from LWOW responses show similar spreading activation patterns and gender bias profiles as human-generated networks
- Core assumption: LLMs encode knowledge in a way that produces meaningful free associations comparable to human associative patterns
- Evidence anchors:
  - [abstract] "we create a new dataset of LLM-generated free association norms modeled after the 'Small World of Words' (SWOW) human-generated norms"
  - [section] "Using both SWOW and LWOW norms, we construct cognitive network models of semantic memory that represent the conceptual knowledge possessed by humans and LLMs"
  - [corpus] Weak - corpus neighbors mention related bias and word association studies but don't directly validate LLM-human comparability
- Break condition: If LLMs produce fundamentally different associative patterns that don't correlate with human behavior in semantic priming tasks

### Mechanism 2
- Claim: Spreading activation processes within cognitive networks can validate both human and LLM-generated datasets
- Mechanism: The spreadr algorithm simulates cognitive mechanisms underlying semantic priming by activating prime nodes and measuring target activation levels. Higher activation of related targets compared to unrelated targets indicates valid associative structure
- Core assumption: Semantic priming effects observed behaviorally can be reproduced within network models through spreading activation
- Evidence anchors:
  - [section] "we observed that among female-related targets, the normalized final activation levels were higher when activated by female-related primes rather than male-related, and vice versa among male-related targets"
  - [section] "Wilcoxon Rank tests for paired samples confirm the statistical significance of these paired differences (all p < 0.001)"
  - [corpus] Weak - corpus mentions semantic priming studies but doesn't directly address network validation methods
- Break condition: If activation patterns don't correlate with behavioral data from lexical decision tasks

### Mechanism 3
- Claim: Gender bias in LLMs can be quantified using the same spreading activation methodology applied to human networks
- Mechanism: By activating gender-related primes (woman, man, etc.) and measuring activation levels of stereotype-consistent vs. stereotype-inconsistent targets, bias manifests as differential activation patterns
- Core assumption: Higher activation of stereotype-consistent targets indicates stronger implicit gender associations in the model
- Evidence anchors:
  - [section] "we observed that among female-related targets, the normalized final activation levels were higher when activated by female-related primes rather than male-related, and vice versa among male-related targets"
  - [section] "Effect sizes are shown in Table 6. For female-related targets, the largest effects (levels of gender bias) are observed for Humans and Haiku"
  - [corpus] Weak - corpus contains related gender bias studies but doesn't directly validate this specific methodology
- Break condition: If activation patterns show no consistent bias or produce results opposite to expected stereotypes

## Foundational Learning

- Concept: Free association norms and their role in cognitive psychology
  - Why needed here: Understanding that free associations reveal implicit conceptual knowledge organization, which forms the basis for comparing human and LLM semantic structures
  - Quick check question: Why are free associations considered a window into implicit cognitive processes rather than explicit reasoning?

- Concept: Cognitive network models and spreading activation theory
  - Why needed here: The paper builds networks from associations and uses spreading activation to validate them, requiring understanding of how cognitive phenomena can be modeled as network processes
  - Quick check question: How does spreading activation theory explain the semantic priming effect observed in lexical decision tasks?

- Concept: Bias measurement through association strength
  - Why needed here: The paper adapts spreading activation methodology to quantify gender biases by comparing activation levels of stereotype-consistent vs. inconsistent targets
  - Quick check question: Why would higher activation of stereotype-consistent targets indicate the presence of implicit bias in a model?

## Architecture Onboarding

- Component map: Dataset generation → Preprocessing pipeline → Network construction → Spreading activation validation → Bias quantification
- Critical path: LWOW dataset generation must produce structurally comparable data to SWOW before network construction and validation can proceed
- Design tradeoffs: Including all responses (including nonsensical ones) preserves data completeness but requires careful network filtering; using undirected networks simplifies analysis but loses directional information
- Failure signatures: Poor correlation between network activation patterns and behavioral data indicates invalid associations; inconsistent bias patterns across different LLM models suggest model-specific limitations
- First 3 experiments:
  1. Generate LWOW datasets and verify structural similarity to SWOW through basic statistics (node/edge counts, degree distributions)
  2. Apply spreading activation to both human and LLM networks using the same prime-target pairs and verify correlation with behavioral reaction times
  3. Quantify gender bias in both network types by comparing activation levels of stereotype-consistent vs. inconsistent targets and analyze effect sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the semantic network properties of LLM-generated free associations differ from human-generated ones beyond gender bias detection?
- Basis in paper: [explicit] The paper compares node and edge statistics between human and LLM networks but doesn't explore deeper network properties
- Why unresolved: The paper focuses on validating the datasets and demonstrating gender bias detection, but doesn't investigate other potential differences in network topology, centrality measures, or community structure
- What evidence would resolve it: Detailed comparison of network metrics like clustering coefficient, path length, community detection results, and centrality measures between human and each LLM network

### Open Question 2
- Question: How do different prompting strategies affect the quality and characteristics of LLM-generated free associations?
- Basis in paper: [inferred] The paper uses a single prompting strategy without exploring alternatives or testing prompt sensitivity
- Why unresolved: The paper presents one prompt design but doesn't investigate how variations in prompt wording, temperature settings, or number of examples might influence the generated associations
- What evidence would resolve it: Systematic comparison of associations generated using different prompt formulations, temperature settings, and prompt engineering techniques

### Open Question 3
- Question: How do the LWOW datasets generalize across different LLM architectures and training paradigms?
- Basis in paper: [explicit] The paper tests only three LLMs (Mistral, Llama3, Haiku) but doesn't compare across different model families or training approaches
- Why unresolved: The paper validates the approach with three models but doesn't explore how results might differ with transformers versus other architectures, or models trained with different objectives
- What evidence would resolve it: Generation of LWOW datasets using models with different architectures (e.g., recurrent, convolutional) or training paradigms (e.g., masked language modeling vs next token prediction)

## Limitations
- The paper relies on a single prompting strategy without exploring how different prompts might affect association quality or bias patterns
- Validation is limited to spreading activation and lexical decision task correlation, without testing other semantic tasks
- The study doesn't investigate deeper network properties beyond basic statistics and bias quantification

## Confidence
- **High**: The dataset construction methodology and preprocessing pipeline are clearly specified and reproducible
- **Medium**: The spreading activation validation shows statistically significant correlations but doesn't establish causal mechanisms
- **Medium**: Gender bias quantification using this methodology is methodologically sound but limited to the specific stereotypes examined

## Next Checks
1. Test LWOW datasets with additional semantic tasks beyond spreading activation, such as similarity judgments or category verification, to establish broader validity
2. Compare LWOW-generated networks with other human association datasets to verify that patterns are consistent across different human populations and not specific to SWOW methodology
3. Conduct ablation studies on preprocessing steps to determine which filtering decisions most impact network structure and validation outcomes