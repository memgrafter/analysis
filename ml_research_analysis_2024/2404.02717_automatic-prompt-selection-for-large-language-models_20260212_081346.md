---
ver: rpa2
title: Automatic Prompt Selection for Large Language Models
arxiv_id: '2404.02717'
source_url: https://arxiv.org/abs/2404.02717
tags:
- prompt
- prompts
- training
- evaluator
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automatic prompt optimization
  for Large Language Models (LLMs), which is crucial for enhancing their performance
  on various natural language processing tasks. The proposed method, Automatic Prompt
  Selection (APS), introduces a novel approach that balances prompt generality and
  specificity while ensuring computational efficiency.
---

# Automatic Prompt Selection for Large Language Models

## Quick Facts
- arXiv ID: 2404.02717
- Source URL: https://arxiv.org/abs/2404.02717
- Reference count: 34
- Primary result: Introduces APS method for automatic prompt optimization, achieving 81.49% accuracy on GSM8K, 100% on MultiArith, and 64.57% on AQuA

## Executive Summary
This paper addresses the challenge of automatic prompt optimization for Large Language Models (LLMs), which is crucial for enhancing their performance on various natural language processing tasks. The proposed method, Automatic Prompt Selection (APS), introduces a novel approach that balances prompt generality and specificity while ensuring computational efficiency. APS consists of three main steps: (1) clustering training data and generating candidate prompts for each cluster using an LLM-based prompt generator; (2) synthesizing a dataset of input-prompt-output tuples to train a prompt evaluator that ranks prompts based on their relevance to the input; (3) using the prompt evaluator to select the best prompt for a new input at test time. The method demonstrates competitive performance on zero-shot question-answering datasets: GSM8K (81.49% accuracy), MultiArith (100% accuracy), and AQuA (64.57% accuracy). APS outperforms state-of-the-art baselines by 1.28% on GSM8K, 0.55% on MultiArith, and 2.76% on AQuA, showcasing its effectiveness in automatically selecting optimal prompts for diverse inputs.

## Method Summary
APS is a three-step approach for automatic prompt selection. First, it clusters training data using Sentence-Transformer embeddings and K-Means, then generates candidate prompts for each cluster using an LLM-based prompt generator. Second, it synthesizes a dataset of input-prompt-output tuples to train a prompt evaluator that ranks prompts based on their relevance to the input using preference learning. Third, during inference, it uses the prompt evaluator to select the best prompt for a new input and optionally applies voting across top-k prompts to improve robustness.

## Key Results
- APS achieves 81.49% accuracy on GSM8K, 100% on MultiArith, and 64.57% on AQuA
- Outperforms state-of-the-art baselines by 1.28% on GSM8K, 0.55% on MultiArith, and 2.76% on AQuA
- Demonstrates effectiveness of automatic prompt selection through clustering and evaluation approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt clustering reduces search space while preserving performance by grouping semantically similar inputs and generating prompts only for cluster centroids.
- Mechanism: Sentence-Transformer encodes question-context pairs, K-Means clusters them, and prompt generator creates prompts per cluster. This reduces prompt candidates from O(N) to O(c) where c << N.
- Core assumption: Inputs from the same cluster share similar semantic meaning and can benefit from the same prompt.
- Evidence anchors:
  - [section] "We argue that similar inputs (question-context pairs) benefit equally from the same prompt. This is because similar inputs share a common meaning represented in the form of tokens."
  - [abstract] "Our approach consists of three steps: (1) clustering the training data and generating candidate prompts for each cluster using an LLM-based prompt generator"
- Break condition: If cluster quality is poor (e.g., Sentence-Transformer embeddings don't capture semantic similarity well), or if prompts generated per cluster don't generalize across cluster members, performance degrades significantly.

### Mechanism 2
- Claim: Prompt evaluator trained with preference learning can rank prompts without executing LLMs during inference, reducing computational cost.
- Mechanism: Synthetic data is created by generating good/bad prompts for each input using a separate LLM, then training a lightweight evaluator to distinguish them using preference loss.
- Core assumption: The prompt evaluator can learn to generalize from synthetic preference data to real ranking tasks.
- Evidence anchors:
  - [abstract] "synthesizing a dataset of input-prompt-output tuples for training a prompt evaluator to rank the prompts based on their relevance to the input"
  - [section] "we adopt a preference loss, encouraging high scores for good prompts and low scores for bad ones"
- Break condition: If synthetic data generation is noisy (wrong good/bad labels), or if the evaluator overfits to the synthetic distribution and fails to generalize to real inputs.

### Mechanism 3
- Claim: Voting mechanism across top-k prompts improves robustness by averaging out individual prompt biases.
- Mechanism: During inference, top-k prompts are selected by the evaluator, executed by the downstream LLM, and the mode of outputs is taken as final answer.
- Core assumption: Different prompts produce varied outputs, and the majority answer is more likely correct than any single prompt's output.
- Evidence anchors:
  - [section] "These prompts can produce varied responses, and we can utilize voting mechanisms to ascertain the most common outputs generated by the majority of the prompts"
  - [table] "APS ( top-3 voting) 80.88 99.45 61.81" vs "APS (no voting) 79.14 99.45 63.78"
- Break condition: If top-k prompts all produce similar wrong answers (low diversity), voting provides no benefit and may even amplify errors.

## Foundational Learning

- Concept: Clustering algorithms (K-Means) and embedding models (Sentence-Transformer)
  - Why needed here: To group semantically similar inputs so that prompts can be generated per cluster rather than per individual input
  - Quick check question: If you have 1000 training examples and use 10 clusters, how many prompt candidates do you generate per cluster if np=3?

- Concept: Preference learning and Bradley-Terry model
  - Why needed here: To train the prompt evaluator to rank prompts without requiring exact score labels
  - Quick check question: In preference learning, if prompt A is better than prompt B, what should the evaluator output?

- Concept: Mode/voting aggregation
  - Why needed here: To combine outputs from multiple prompts to improve robustness and accuracy
  - Quick check question: If 5 prompts produce outputs [A, A, B, A, B], what is the final answer using voting?

## Architecture Onboarding

- Component map:
  - Sentence-Transformer (embedding model)
  - K-Means clustering (grouping)
  - GPT-3.5-turbo (prompt generator A1, data synthesizer A2, downstream solver M)
  - Opt-125m (prompt evaluator E)
  - Voting mechanism (aggregation)

- Critical path: Input → Sentence-Transformer → K-Means → Prompt generator → Prompt database → Data synthesizer → Prompt evaluator training → Inference ranking → Voting → Output

- Design tradeoffs:
  - More clusters (higher c) → more diverse prompts but higher cost
  - More prompts per cluster (higher np) → better coverage but higher cost
  - Larger m (examples per cluster for synthetic data) → better training but higher cost
  - Higher k (voting count) → more robust but potentially noisier

- Failure signatures:
  - Poor clustering → prompts don't match input semantics
  - Noisy synthetic data → evaluator learns wrong preferences
  - Overfitting → evaluator performs well on training but poorly on test
  - Low prompt diversity → voting provides no benefit

- First 3 experiments:
  1. Run APS with c=5, np=2, m=5, k=1 (no voting) on GSM8K to verify basic functionality
  2. Compare APS vs Fixed Prompt baseline to measure clustering benefit
  3. Test different k values (1, 3, 5) to find optimal voting count

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of APS scale with the number of clusters and prompts per cluster beyond the tested values?
- Basis in paper: [explicit] The paper mentions that increasing the number of clusters and prompts per cluster can improve performance, but due to high costs, they only tested up to c=20 and np=5.
- Why unresolved: The paper only tested a limited range of values for these hyperparameters, and it's unclear how the performance would continue to change with larger values.
- What evidence would resolve it: Conducting experiments with a wider range of values for c and np, and measuring the corresponding performance changes.

### Open Question 2
- Question: How does APS perform on other types of natural language processing tasks beyond question-answering?
- Basis in paper: [explicit] The paper only tests APS on three question-answering datasets, and it's unclear how well it would generalize to other NLP tasks.
- Why unresolved: The paper focuses specifically on question-answering, and doesn't explore the applicability of APS to other NLP tasks.
- What evidence would resolve it: Applying APS to a diverse set of NLP tasks (e.g., text summarization, sentiment analysis, named entity recognition) and comparing its performance to other state-of-the-art methods.

### Open Question 3
- Question: How does APS handle noisy or ambiguous inputs, and what is the impact on its performance?
- Basis in paper: [inferred] The paper doesn't explicitly discuss how APS handles noisy or ambiguous inputs, but it's an important consideration for any NLP system.
- Why unresolved: The paper doesn't provide any analysis or experiments related to APS's robustness to noisy or ambiguous inputs.
- What evidence would resolve it: Conducting experiments where the inputs are intentionally corrupted with noise or ambiguity, and measuring the impact on APS's performance.

### Open Question 4
- Question: How does the quality of the prompt database impact APS's performance, and what are the best practices for generating high-quality prompts?
- Basis in paper: [explicit] The paper mentions that the quality of the prompt database is important, and discusses some strategies for generating diverse prompts, but doesn't provide a comprehensive analysis of what makes a high-quality prompt database.
- Why unresolved: The paper doesn't provide a detailed analysis of the factors that contribute to a high-quality prompt database, or best practices for generating such a database.
- What evidence would resolve it: Conducting experiments to measure the impact of different prompt generation strategies on APS's performance, and developing guidelines for generating high-quality prompt databases.

## Limitations

- Dataset bias: Evaluation limited to three mathematical QA datasets; effectiveness on other domains untested
- Computational tradeoffs: Multiple LLM calls required; computational efficiency gains not quantified
- Synthetic data quality: Performance depends on quality of synthetic preference data; potential for noisy learning

## Confidence

- High Confidence (90%+): Clustering mechanism and voting aggregation are well-established techniques; methodology is technically sound
- Medium Confidence (70-89%): Synthetic data generation and prompt evaluator training are reasonable but need cross-domain validation
- Low Confidence (below 70%): Generalization to non-mathematical tasks and scaling to larger datasets remain uncertain

## Next Checks

1. Cross-Domain Validation: Test APS on non-mathematical datasets (e.g., commonsense reasoning, text classification) to verify domain generalization and compare performance degradation relative to baselines.

2. Computational Cost Analysis: Measure and compare wall-clock time and API costs for APS versus (a) fixed prompt, (b) random prompt search, and (c) full brute-force prompt search across different dataset sizes, including preprocessing overhead.

3. Synthetic Data Ablation: Evaluate APS performance with (a) high-quality synthetic data, (b) low-quality/noisy synthetic data, and (c) no synthetic data (random labels) to validate whether synthetic data generation is critical to success.