---
ver: rpa2
title: 'GenDec: A robust generative Question-decomposition method for Multi-hop reasoning'
arxiv_id: '2402.11166'
source_url: https://arxiv.org/abs/2402.11166
tags:
- question
- gendec
- sub-questions
- multi-hop
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GenDec, a generative approach to multi-hop
  question decomposition in Question Answering. GenDec leverages retrieved paragraphs
  to generate independent and complete sub-questions, addressing the error propagation
  issue in traditional sequential decomposition methods.
---

# GenDec: A robust generative Question-decomposition method for Multi-hop reasoning

## Quick Facts
- **arXiv ID**: 2402.11166
- **Source URL**: https://arxiv.org/abs/2402.11166
- **Reference count**: 17
- **Key outcome**: GenDec outperforms existing methods in QA performance and paragraph retrieval on HotpotQA, 2WikiMultiHopQA, MuSiQue, and PokeMQA datasets, and when combined with LLMs achieves higher accuracy and more reliable reasoning chains.

## Executive Summary
This paper introduces GenDec, a generative approach to multi-hop question decomposition that leverages retrieved paragraphs to generate independent and complete sub-questions. By eliminating the sequential dependency inherent in traditional decomposition methods, GenDec addresses the error propagation issue that plagues multi-hop reasoning systems. The method integrates sub-questions into a QA pipeline that includes paragraph retrieval and QA modules, achieving superior performance across multiple benchmark datasets.

## Method Summary
GenDec generates independent sub-questions from multi-hop questions and retrieved paragraphs using a fine-tuned BART-large or T5-large model. The approach consists of three main components: the GenDec model for question decomposition, a Sub-question-enhanced Paragraph Retrieval (SPR) module that uses both questions and sub-questions to identify relevant paragraphs, and a Sub-question-enhanced QA (SQA) module that performs multi-task learning to predict answers and supporting facts. The system is trained on datasets including HotpotQA, 2WikiMultiHopQA, MuSiQue, and PokeMQA, using specific hyperparameters for fine-tuning and evaluation.

## Key Results
- GenDec outperforms existing methods in QA performance and paragraph retrieval on HotpotQA, 2WikiMultiHopQA, MuSiQue, and PokeMQA datasets.
- When combined with LLMs such as GPT-4 and GPT-3.5, GenDec enhances reasoning capabilities, achieving higher accuracy and more reliable reasoning chains.
- The method demonstrates effectiveness in improving both QA performance and the interpretability of multi-hop reasoning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating independent sub-questions eliminates error propagation in multi-hop reasoning.
- Mechanism: Traditional sequential decomposition passes errors from earlier steps to later ones. GenDec generates parallel, self-contained sub-questions that can be answered independently using retrieved paragraphs, breaking the sequential dependency chain.
- Core assumption: Independent sub-questions can be answered without sequential dependency and still lead to correct final answers.
- Evidence anchors:
  - [abstract] "GenDec leverages retrieved paragraphs to generate independent and complete sub-questions, addressing the error propagation issue in traditional sequential decomposition methods"
  - [section] "Our proposed GenDec model alleviates these issues by ensuring that the decomposed sub-questions are independent and self-contained, eliminating the need for sequential answering inherent in previous models"
  - [corpus] Weak evidence - no direct citations on independent sub-question effectiveness
- Break Condition: If retrieved paragraphs don't contain sufficient evidence for all sub-questions, or if sub-questions are not truly independent (still require cross-reference).

### Mechanism 2
- Claim: Incorporating sub-questions into paragraph retrieval improves retrieval quality by focusing on more relevant data.
- Mechanism: The Sub-question-enhanced Paragraph Retrieval (SPR) module uses both the original question and generated sub-questions to score paragraph relevance, creating more targeted retrieval than question-only approaches.
- Core assumption: Sub-questions provide additional semantic signals that improve paragraph selection beyond what the original question provides alone.
- Evidence anchors:
  - [section] "paragraph retrieval plays a vital role in both QA and QD modules, since GenDec utilizes information from sub-questions and can thus focus on the more relevant data"
  - [section] "We propose sub-question-enhanced paragraph retrieval (SPR) for refined retrieval, which utilizes an encoder and a classification head to compute scores for each paragraph"
  - [corpus] Weak evidence - no direct citations on sub-question-enhanced retrieval effectiveness
- Break Condition: If sub-questions are poorly generated or too generic, they may add noise rather than signal to the retrieval process.

### Mechanism 3
- Claim: Multi-task learning with sub-question features improves QA performance by providing richer context.
- Mechanism: The Sub-question-enhanced QA (SQA) module fuses representations from both the original question and sub-questions, allowing the model to leverage multi-hop reasoning paths during answer extraction.
- Core assumption: Sub-question features provide complementary information that improves answer span prediction and supporting fact identification.
- Evidence anchors:
  - [section] "the SQA first combines all retrieved paragraphs into context C, which is concatenated with question Q and sub-questions {Sub_Qs} and fed into DeBERTa"
  - [section] "we propose a sub-question-enhanced QA model which utilizes multi-task learning to simultaneously predict supporting facts, and extract answer spans by incorporating sub-questions"
  - [section] "Our evaluation framework consists of two pivotal experiments designed to benchmark the performance of the GenDec with a fine-tuned QA model against state-of-the-art QA models"
  - [corpus] Weak evidence - no direct citations on multi-task learning with sub-questions
- Break Condition: If sub-question features dominate the representation space and overshadow the original question context.

## Foundational Learning

- Concept: Multi-hop reasoning
  - Why needed here: Understanding how questions requiring multiple reasoning steps across different sources can be decomposed and solved
  - Quick check question: Can you explain the difference between single-hop and multi-hop question answering?

- Concept: Error propagation in sequential reasoning
  - Why needed here: Recognizing why traditional sequential decomposition methods fail in multi-hop QA
  - Quick check question: Why does answering sub-question 1 incorrectly lead to incorrect answers for all subsequent sub-questions in sequential decomposition?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: Understanding how external knowledge retrieval integrates with language model generation for improved answers
  - Quick check question: What problem does RAG solve that traditional language models face?

## Architecture Onboarding

- Component map: Question → GenDec → SPR → SQA → Answer
- Critical path: Question → GenDec → SPR → SQA → Answer
- Design tradeoffs: Independent sub-questions vs. sequential dependency; retrieval quality vs. computational cost; model complexity vs. interpretability
- Failure signatures:
  - Poor sub-question quality → incorrect answers despite good retrieval
  - Weak paragraph retrieval → missing supporting facts regardless of QA quality
  - Error propagation still occurring → sub-questions not truly independent
  - Overfitting to training data → poor generalization to new question types

- First 3 experiments:
  1. Generate sub-questions for a small set of multi-hop questions and manually evaluate independence and quality
  2. Test SPR with and without sub-questions on a paragraph retrieval task to measure improvement
  3. Run SQA with and without GenDec-generated sub-questions on a simple QA dataset to validate performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GenDec vary when dealing with multi-hop questions that require numerical reasoning compared to those that primarily involve entity retrieval and relationships?
- Basis in paper: [inferred] The paper mentions that GenDec performs well on various datasets including HotpotQA and MuSiQue, but does not explicitly analyze its performance on questions requiring numerical reasoning.
- Why unresolved: The paper does not provide a detailed breakdown of GenDec's performance on different types of multi-hop questions, such as those requiring numerical reasoning versus those involving entity retrieval and relationships.
- What evidence would resolve it: Conducting experiments to evaluate GenDec's performance on multi-hop questions that require numerical reasoning, such as those found in the DROP dataset, would provide insights into its capabilities and limitations in this area.

### Open Question 2
- Question: What is the impact of using different generative language models (e.g., GPT-4, GPT-3.5, BART-large) as the backend for GenDec on the quality of the generated sub-questions and overall QA performance?
- Basis in paper: [explicit] The paper mentions using BART-large and T5-large as backend models for GenDec, but does not compare their performance with other generative language models like GPT-4 or GPT-3.5.
- Why unresolved: The paper does not provide a comparative analysis of different generative language models as backends for GenDec, leaving the impact of model choice on performance unclear.
- What evidence would resolve it: Conducting experiments to compare the performance of GenDec using different generative language models as backends, including GPT-4, GPT-3.5, BART-large, and T5-large, would help determine the impact of model choice on the quality of generated sub-questions and overall QA performance.

### Open Question 3
- Question: How does GenDec handle multi-hop questions that involve complex temporal or causal relationships, and what are the limitations of the current approach in addressing such questions?
- Basis in paper: [inferred] The paper discusses GenDec's ability to generate independent and complete sub-questions, but does not explicitly address its handling of complex temporal or causal relationships in multi-hop questions.
- Why unresolved: The paper does not provide a detailed analysis of GenDec's performance on multi-hop questions that involve complex temporal or causal relationships, nor does it discuss the limitations of the current approach in addressing such questions.
- What evidence would resolve it: Conducting experiments to evaluate GenDec's performance on multi-hop questions that involve complex temporal or causal relationships, such as those found in the QASC or Cosmos QA datasets, would help identify the strengths and limitations of the current approach in handling such questions.

## Limitations
- The paper lacks direct empirical validation of sub-question independence through controlled comparisons with sequential decomposition methods.
- No ablation studies are provided to isolate the contribution of sub-question-enhanced paragraph retrieval versus improvements from better QA model architecture.
- The claim that GenDec improves interpretability is based on quantitative metrics rather than qualitative analysis of reasoning chains.

## Confidence

- **High confidence**: The overall experimental framework and dataset usage are well-specified and reproducible. The multi-task learning approach for QA is clearly described.
- **Medium confidence**: The effectiveness of independent sub-question generation in eliminating error propagation, as the paper lacks direct comparisons with sequential decomposition methods.
- **Medium confidence**: The benefit of sub-question-enhanced paragraph retrieval, as the paper doesn't isolate this component's contribution through ablation studies.
- **Medium confidence**: The claim that GenDec improves interpretability, as the paper focuses on quantitative metrics rather than qualitative analysis of reasoning chains.

## Next Checks

1. **Error propagation test**: Implement a sequential decomposition baseline and compare its performance degradation across multi-hop depths against GenDec's parallel approach to directly validate the error propagation elimination claim.

2. **Sub-question quality evaluation**: Manually annotate a subset of generated sub-questions for independence and completeness, then correlate quality scores with downstream QA performance to quantify the relationship between sub-question quality and final answer accuracy.

3. **Retrieval contribution isolation**: Create ablation experiments comparing SPR with and without sub-questions while keeping the QA module constant, to determine the specific contribution of sub-question-enhanced retrieval to overall performance gains.