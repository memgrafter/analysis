---
ver: rpa2
title: 'M$^3$oralBench: A MultiModal Moral Benchmark for LVLMs'
arxiv_id: '2412.20718'
source_url: https://arxiv.org/abs/2412.20718
tags:
- moral
- foundations
- scenarios
- scenario
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces M\xB3oralBench, the first multimodal moral\
  \ benchmark designed to evaluate the moral understanding and reasoning capabilities\
  \ of large vision-language models (LVLMs). The benchmark is built on Moral Foundations\
  \ Theory and expands Moral Foundations Vignettes to create 1,160 moral violation\
  \ scenarios across six moral foundations: Care/Harm, Fairness/Cheating, Loyalty/Betrayal,\
  \ Authority/Subversion, Sanctity/Degradation, and Liberty/Oppression."
---

# M$^3$oralBench: A MultiModal Moral Benchmark for LVLMs

## Quick Facts
- arXiv ID: 2412.20718
- Source URL: https://arxiv.org/abs/2412.20718
- Authors: Bei Yan; Jie Zhang; Zhiyuan Chen; Shiguang Shan; Xilin Chen
- Reference count: 40
- Key outcome: First multimodal moral benchmark for LVLMs, revealing significant gaps in models' moral reasoning capabilities across six moral foundations

## Executive Summary
This paper introduces M³oralBench, the first multimodal moral benchmark designed to evaluate the moral understanding and reasoning capabilities of large vision-language models (LVLMs). The benchmark is built on Moral Foundations Theory and expands Moral Foundations Vignettes to create 1,160 moral violation scenarios across six moral foundations. Corresponding scenario images are generated using SD3.0 with text-to-image diffusion, and dialogue is added through speech bubbles. The benchmark evaluates models across three tasks: moral judgement, moral classification, and moral response. Experiments on 10 popular open-source and closed-source LVLMs show that closed-source models (GPT-4o, Gemini-1.5-Pro) outperform open-source models overall, with average accuracies ranging from 43.7% to 72.6%. The benchmark reveals that models struggle most with Sanctity/Degradation and Loyalty/Betrayal foundations, indicating significant gaps in multimodal moral reasoning.

## Method Summary
The M³oralBench benchmark expands 116 original Moral Foundations Vignettes into 1,160 scenarios using GPT-4o to generate 10 diverse versions of each vignette with different characters and locations. Scenario images are generated using SD3.0 with text-to-image diffusion, then manually selected for quality. The benchmark evaluates LVLMs across three tasks: moral judgement (binary moral assessment), moral classification (identifying which of six moral foundations the scenario represents), and moral response (selecting appropriate responses to moral violations). Model performance is assessed using Monte Carlo sampling to estimate probabilities for multiple-choice options, with accuracy as the primary metric.

## Key Results
- Closed-source models (GPT-4o, Gemini-1.5-Pro) significantly outperform open-source models on all three moral tasks
- LVLMs show average accuracies ranging from 43.7% to 72.6% across the benchmark
- Models struggle most with Sanctity/Degradation and Loyalty/Betrayal foundations
- Moral classification task shows the lowest performance across all models, indicating difficulty in identifying moral foundations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multimodal moral evaluation approach captures moral reasoning gaps that text-only benchmarks miss.
- Mechanism: By generating scenario images and adding character dialogue, the benchmark creates richer contextual information that requires models to integrate visual and textual moral cues.
- Core assumption: LVLMs' moral reasoning capabilities can be meaningfully assessed through scenarios that combine visual and textual elements.
- Evidence anchors:
  - [abstract] "Text-only evaluation is insufficient to fully capture the moral judgements and behaviors of multimodal models in real-world scenarios"
  - [section 3.1] "We design instruction templates for three moral tasks...to provide a comprehensive evaluation of models' inherent morality"
- Break condition: If models show no performance difference between multimodal and text-only moral evaluation tasks, indicating that visual information doesn't add meaningful context for moral reasoning.

### Mechanism 2
- Claim: Expanding Moral Foundations Vignettes through GPT-4o creates a more generalizable benchmark while maintaining theoretical grounding.
- Mechanism: GPT-4o generates 10 variations of each original vignette with different characters and locations, preserving the core moral violation while increasing scenario diversity.
- Core assumption: The expanded scenarios maintain the essential moral content of the original vignettes while providing sufficient variety for robust evaluation.
- Evidence anchors:
  - [section 3.2] "we use GPT-4o to replicate these scenarios, creating 10 diverse versions with identical core events but different characters and locations"
  - [section 2.1] "MFVs offer a standardized set of scenarios depicting moral violations, enabling researchers to test diverse theories on moral judgement"
- Break condition: If expanded scenarios show significant deviation in moral foundation classification from original vignettes, indicating loss of theoretical fidelity.

### Mechanism 3
- Claim: The three-task evaluation structure (judgement, classification, response) provides comprehensive assessment of different aspects of moral reasoning.
- Mechanism: Each task targets distinct cognitive processes - binary moral assessment, foundation identification, and response selection - revealing different model capabilities.
- Core assumption: Moral reasoning in LVLMs can be decomposed into these three distinct but related cognitive processes.
- Evidence anchors:
  - [section 3.3] "To enhance the depth and breadth of evaluation, our benchmark introduces an additional task: moral classification"
  - [section 4.2] "most models exhibit relatively lower performance in moral classification than in the other two tasks"
- Break condition: If performance across tasks shows near-perfect correlation, suggesting the tasks don't measure distinct capabilities.

## Foundational Learning

- Concept: Moral Foundations Theory (MFT) and its six dimensions
  - Why needed here: The entire benchmark is structured around MFT's theoretical framework to assess moral reasoning across established psychological dimensions
  - Quick check question: Can you explain the difference between the Sanctity/Degradation foundation and the Care/Harm foundation in terms of what they evaluate?

- Concept: Multimodal learning and cross-modal integration
  - Why needed here: The benchmark evaluates how well models integrate visual and textual information for moral reasoning
  - Quick check question: What specific challenges arise when combining visual and textual moral cues compared to using either modality alone?

- Concept: Monte Carlo sampling for probability estimation
  - Why needed here: The evaluation method uses Monte Carlo sampling to estimate model preference probabilities when direct token probabilities aren't available
  - Quick check question: Why might Monte Carlo sampling be preferred over other probability estimation methods in this evaluation context?

## Architecture Onboarding

- Component map:
  - Scenario expansion pipeline (GPT-4o → text generation)
  - Image generation pipeline (GPT-4o prompts → SD3.0 → manual selection)
  - Instruction generation (template gallery → multiple-choice format)
  - Evaluation pipeline (Monte Carlo sampling → accuracy calculation)
  - Analysis components (correlation analysis, foundation-specific evaluation)

- Critical path: Scenario expansion → Image generation → Instruction generation → Model evaluation → Analysis
  - Any delay in image generation directly impacts the entire pipeline since instructions depend on final images

- Design tradeoffs:
  - Manual image selection ensures quality but limits scalability
  - Multiple-choice format simplifies evaluation but may constrain model expression
  - Monte Carlo sampling provides probability estimates but requires multiple model calls per instance

- Failure signatures:
  - Low CLIP similarity scores between images and scenario texts indicate generation quality issues
  - High refusal rates from GPT-4o suggest prompt formulation problems
  - Consistent model confusion between foundations indicates theoretical clarity issues

- First 3 experiments:
  1. Test scenario expansion consistency by having GPT-4o regenerate a small set of vignettes and comparing moral foundation classifications
  2. Validate image-text alignment by having humans caption generated images and measuring similarity to original scenarios
  3. Test evaluation method by running a small subset of instances through multiple sampling runs to verify probability stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance differences between open-source and closed-source models on M³oralBench relate to their training data and fine-tuning processes?
- Basis in paper: [explicit] The paper shows that closed-source models (GPT-4o, Gemini-1.5-Pro) outperform open-source models overall, with average accuracies ranging from 43.7% to 72.6%.
- Why unresolved: While the paper attributes the performance gap to commercial models having stronger alignment with human values and dedicated moral review mechanisms, it does not provide detailed analysis of how training data composition, fine-tuning strategies, or alignment techniques specifically impact moral reasoning capabilities.
- What evidence would resolve it: Comparative analysis of training datasets and fine-tuning procedures between high-performing and low-performing models, along with ablation studies on how different training approaches affect moral reasoning performance.

### Open Question 2
- Question: What are the specific limitations of current LVLMs in understanding and reasoning about the Sanctity/Degradation moral foundation, and how can these be addressed?
- Basis in paper: [explicit] The paper identifies that LVLMs struggle most with Sanctity/Degradation foundation, showing the poorest performance on this moral foundation.
- Why unresolved: The paper observes this limitation but does not investigate the underlying causes or propose specific solutions for improving model performance on sanctity-related moral reasoning.
- What evidence would resolve it: Detailed analysis of model failures on sanctity-related scenarios, investigation of whether the issue stems from training data limitations, architectural constraints, or evaluation methodology, followed by targeted interventions to address the identified issues.

### Open Question 3
- Question: How does the multimodal nature of M³oralBench scenarios (combining images and text) specifically impact model performance compared to text-only moral evaluation benchmarks?
- Basis in paper: [inferred] The paper introduces M³oralBench as the first multimodal moral benchmark, contrasting it with existing text-only benchmarks, but does not directly compare performance across modalities.
- Why unresolved: While the paper demonstrates that multimodal evaluation is necessary, it does not quantify how much the image component contributes to model performance or whether certain models benefit more from multimodal input than others.
- What evidence would resolve it: Controlled experiments comparing model performance on identical moral scenarios presented in text-only versus multimodal formats, along with analysis of which aspects of multimodal input (visual cues, dialogue, context) most influence moral reasoning.

## Limitations
- Benchmark relies on GPT-4o for scenario expansion and image generation, potentially introducing bias from GPT-4o's own moral perspectives
- Manual image selection process ensures quality but limits scalability and may introduce subjective bias
- Benchmark focuses primarily on moral violations rather than a broader range of moral scenarios

## Confidence

- High Confidence: The theoretical foundation based on Moral Foundations Theory is well-established in psychology literature, providing robust grounding for the benchmark's design
- Medium Confidence: The methodology for multimodal scenario generation and evaluation is sound, though dependent on GPT-4o's capabilities and biases
- Medium Confidence: The three-task evaluation structure captures different aspects of moral reasoning, though the distinctness of these tasks requires further validation

## Next Checks

1. Conduct inter-rater reliability testing by having multiple human evaluators assess a subset of generated images and scenarios to verify alignment with original moral foundations and consistency in moral violation representation

2. Test benchmark robustness by evaluating the same scenarios with different random seeds for Monte Carlo sampling to verify probability estimates are stable and not sensitive to sampling variation

3. Perform ablation studies by evaluating models on text-only versions of the same scenarios to quantify the added value of multimodal information for moral reasoning assessment