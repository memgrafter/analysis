---
ver: rpa2
title: Adaptive Batch Size Schedules for Distributed Training of Language Models with
  Data and Model Parallelism
arxiv_id: '2412.21124'
source_url: https://arxiv.org/abs/2412.21124
tags:
- batch
- training
- size
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of choosing appropriate batch
  sizes for distributed training of large language models, which involves a tradeoff
  between training efficiency and generalization performance. The authors propose
  adaptive batch size schedules that dynamically adjust batch sizes based on gradient
  noise and approximation quality, and develop a practical implementation using PyTorch's
  Fully Sharded Data Parallel (FSDP) for memory-efficient training of models with
  billions of parameters.
---

# Adaptive Batch Size Schedules for Distributed Training of Language Models with Data and Model Parallelism

## Quick Facts
- arXiv ID: 2412.21124
- Source URL: https://arxiv.org/abs/2412.21124
- Authors: Tim Tsz-Kit Lau; Weijian Li; Chenwei Xu; Han Liu; Mladen Kolar
- Reference count: 40
- Key outcome: Adaptive batch size schedules outperform constant batch sizes and heuristic warmup schedules in pretraining Llama 2 family models up to 3 billion parameters, achieving lower validation losses and mitigating generalization gaps.

## Executive Summary
This paper addresses the challenge of choosing appropriate batch sizes for distributed training of large language models, which involves a tradeoff between training efficiency and generalization performance. The authors propose adaptive batch size schedules that dynamically adjust batch sizes based on gradient noise and approximation quality, and develop a practical implementation using PyTorch's Fully Sharded Data Parallel (FSDP) for memory-efficient training of models with billions of parameters. The proposed method outperforms constant batch sizes and heuristic batch size warmup schedules in pretraining models from the Llama 2 family (up to 3 billion parameters), achieving lower validation losses and mitigating generalization gaps.

## Method Summary
The authors propose adaptive batch size schedules that dynamically adjust batch sizes based on gradient noise and approximation quality. They develop a practical implementation using PyTorch Fully Sharded Data Parallel (FSDP) for memory-efficient training of models with billions of parameters. The method uses a norm test that compares gradient variance to gradient norm to determine when to increase batch size. The approach is theoretically principled, with convergence guarantees established for Adam optimizer on general smooth nonconvex objectives.

## Key Results
- Adaptive batch size schedules achieve lower validation losses compared to constant batch sizes and heuristic warmup schedules
- The method mitigates generalization gaps typically observed in large-batch training
- The approach is memory-efficient, enabling pretraining of language models up to 3 billion parameters using FSDP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive batch size schedules can mitigate the generalization gap in large-batch training by dynamically balancing gradient noise and approximation quality.
- Mechanism: The norm test dynamically adjusts the batch size based on the ratio of gradient variance to the squared norm of the batch gradient. When the approximation quality is poor (high variance relative to gradient norm), the batch size increases to improve the approximation; when quality is good, the batch size can stay small to maintain generalization.
- Core assumption: The variance of per-sample gradients can be estimated efficiently from worker-level minibatch gradients without explicit per-sample gradient computation.
- Evidence anchors:
  - [abstract]: "The proposed method outperforms constant batch sizes and heuristic batch size warmup schedules in pretraining models from the Llama 2 family (up to 3 billion parameters), achieving lower validation losses and mitigating generalization gaps."
  - [section]: "The adjustment factor(n−b)/(n−1) is approximated by1 as we taken→∞. Consequently, to ensure that the batch gradient approximates the descent direction of the full objectiveℒ well, the(approximate) norm test checks the following condition at each iterationk∈N∗: ∥Vari∈Bk (∇ℓi(wk))∥1/bk⩽η2∥∇ℒBk (wk)∥2"
  - [corpus]: "Adaptive Batch Sizes Using Non-Euclidean Gradient Noise Scales for Stochastic Sign and Spectral Descent" (related work on gradient noise-based adaptation)
- Break condition: If the variance estimation is inaccurate due to model sharding artifacts in FSDP, the test may trigger inappropriate batch size changes, leading to either excessive memory usage or poor convergence.

### Mechanism 2
- Claim: PyTorch FSDP enables model parallelism for billion-parameter models by sharding parameters and gradients across workers, allowing adaptive batch size schedules to scale beyond what DDP alone permits.
- Mechanism: FSDP replaces DDP's all-reduce with all-gather and reduce-scatter operations, distributing model parameters across workers. This reduces per-worker memory requirements, enabling training of larger models while still allowing the norm test to operate on sharded gradients.
- Core assumption: The sharding strategy does not significantly increase the variance of gradient estimates compared to a non-sharded baseline, so the norm test remains valid.
- Evidence anchors:
  - [abstract]: "We develop a practical implementation with PyTorch Fully Sharded Data Parallel, facilitating the pretraining of language models of different sizes."
  - [section]: "To alleviate this limitation inherent to data parallelism, more memory-efficient paradigms of parallelism, such as model parallelism [66], have been proposed. In model parallelism, model parameters are sharded into various components and distributed to different workers. In particular, PyTorch Fully Sharded Data Parallel (FSDP) [88] is an implementation of model parallelism in PyTorch [57]..."
  - [corpus]: "HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism" (related work on model parallelism for long sequences)
- Break condition: If sharding introduces significant communication overhead or gradient skew between workers, the variance estimation and subsequent batch size decisions may become unreliable.

### Mechanism 3
- Claim: Convergence guarantees for Adam with adaptive batch sizes extend existing theory by removing the need for coordinate-wise affine variance assumptions.
- Mechanism: The analysis shows that as long as the coordinate-wise norm test is satisfied at each iteration, Adam's convergence properties hold without requiring a global affine variance condition. This makes the method more practical for real-world scenarios where such global conditions are hard to verify.
- Core assumption: The coordinate-wise norm test can be enforced locally at each iteration without needing to verify global variance properties.
- Evidence anchors:
  - [abstract]: "We also establish theoretical convergence guarantees for such adaptive batch size schedules with Adam for general smooth nonconvex objectives."
  - [section]: "Following closely a similar analysis to that in [76], we provide the following convergence results of the norm test forAdam. Theorem 1. Suppose that Assumption 1 holds. Let(wk)k∈N∗ be the Adam iterates generated by(6), where the batch sizebk :=|Bk|is chosen such that the coordinate-wise (exact variance) norm test with constantη∈(0, 1) is satisfied at each iterationk∈N∗."
  - [corpus]: "Communication-Efficient Adaptive Batch Size Strategies for Distributed Local Gradient Methods" (related work on adaptive batch sizes with convergence theory)
- Break condition: If the learning rate schedule or gradient clipping interferes with the norm test's ability to maintain the required variance condition, convergence guarantees may not hold.

## Foundational Learning

- Concept: Gradient variance and its relationship to batch size
  - Why needed here: The norm test fundamentally relies on measuring and controlling gradient variance to determine appropriate batch sizes. Understanding how variance scales with batch size is crucial for implementing and tuning the method.
  - Quick check question: If you double the batch size, how does the variance of the batch gradient estimator change (assuming independent samples)?

- Concept: Distributed data parallelism and gradient synchronization
  - Why needed here: The method builds on DDP and extends it to handle larger models via FSDP. Understanding how gradients are computed, synchronized, and aggregated across workers is essential for implementing the variance estimation correctly.
  - Quick check question: In DDP, how is the global batch gradient computed from per-worker minibatch gradients?

- Concept: Model parallelism and parameter sharding
  - Why needed here: FSDP implements model parallelism by sharding parameters and gradients. Understanding this sharding mechanism is crucial for correctly implementing the norm test in the FSDP setting.
  - Quick check question: In FSDP, how are gradients computed when parameters are sharded across workers?

## Architecture Onboarding

- Component map:
  - Norm test module -> FSDP wrapper -> Optimizer -> Training loop -> Data loader

- Critical path:
  1. Forward pass with current batch size
  2. Backward pass to compute gradients
  3. All-gather/reduce-scatter to synchronize gradients
  4. Norm test computation using synchronized gradients
  5. Batch size decision for next iteration
  6. Parameter update via AdamW

- Design tradeoffs:
  - Test frequency vs. overhead: More frequent testing provides better adaptation but increases communication cost
  - Maximum batch size vs. memory constraints: Higher maximum batch size improves efficiency but may exceed GPU memory
  - Variance estimation method: More accurate estimation requires more communication but improves adaptation quality

- Failure signatures:
  - Memory overflow: Batch size increases too aggressively despite memory constraints
  - Poor convergence: Batch size remains too small, preventing efficient utilization of hardware
  - Communication bottlenecks: Frequent norm test computations overwhelm network bandwidth

- First 3 experiments:
  1. Implement DDP-Norm on a small model (e.g., MicroLlama 300M) with constant η values to verify basic functionality and observe batch size adaptation patterns
  2. Scale to FSDP-Norm with a medium model (e.g., TinyLlama 1.1B) to test model parallelism integration and memory efficiency
  3. Conduct ablation studies varying η, test intervals, and maximum batch sizes to understand hyperparameter sensitivity and identify optimal configurations for different model scales

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do adaptive batch size schedules scale with model size beyond 3B parameters?
- Basis in paper: [inferred] The paper only tested up to 3B parameters and mentions potential limitations for larger models.
- Why unresolved: The paper explicitly states that their current implementation is limited to models up to 3B parameters due to hardware constraints.
- What evidence would resolve it: Testing the adaptive batch size schedules on models larger than 3B parameters, such as those in the 10B+ range, would provide concrete evidence of scalability.

### Open Question 2
- Question: What is the optimal value of η for different model scales and datasets?
- Basis in paper: [explicit] The paper discusses the importance of choosing the right η value but does not provide a systematic method for determining it.
- Why unresolved: The paper mentions that η affects the rate of batch size increase but does not offer a clear strategy for selecting the optimal value across different scenarios.
- What evidence would resolve it: A comprehensive study varying η across different model sizes, datasets, and training configurations would help identify patterns or guidelines for choosing optimal η values.

### Open Question 3
- Question: How do adaptive batch size schedules affect downstream task performance?
- Basis in paper: [explicit] The paper focuses on validation loss during pretraining but does not evaluate downstream task performance.
- Why unresolved: The authors state they did not fully pretrain the models for sufficient tokens, making downstream evaluation infeasible in their experiments.
- What evidence would resolve it: Fully pretraining models using adaptive batch size schedules and evaluating them on standard downstream benchmarks (e.g., GLUE, SuperGLUE) would provide direct evidence of their impact on task performance.

## Limitations
- The convergence guarantees are established for idealized conditions that may not fully capture practical training scenarios with adaptive optimizers and mixed-precision training
- The variance estimation method trades accuracy for efficiency, which could impact the reliability of batch size decisions in practice
- Experimental validation is limited to models up to 3 billion parameters, leaving uncertainty about scalability to larger models

## Confidence

- High confidence: The empirical demonstration that adaptive batch sizes outperform constant schedules and warmup baselines in reducing validation loss and mitigating generalization gaps for models up to 3B parameters.
- Medium confidence: The theoretical convergence guarantees for Adam with adaptive batch sizes, as the analysis assumes idealized conditions that may not fully capture practical training scenarios.
- Medium confidence: The implementation efficiency claims regarding FSDP integration, as detailed performance benchmarks comparing different parallelism strategies are limited.

## Next Checks

1. Implement a systematic ablation study varying the η parameter across a wider range (0.05-0.4) and test intervals (1-100 steps) to establish robust hyperparameter selection guidelines for different model scales and hardware configurations.

2. Conduct scalability experiments training models larger than 3B parameters (e.g., 7B-70B range) to validate whether the theoretical and empirical benefits persist as model size increases, particularly focusing on memory efficiency and communication overhead.

3. Design controlled experiments comparing the adaptive schedule's performance against alternative batch size adaptation strategies like cyclic learning rates or progressive learning, isolating the specific contribution of variance-based adaptation versus other adaptive training techniques.