---
ver: rpa2
title: 'Concept Induction using LLMs: a user experiment for assessment'
arxiv_id: '2404.11875'
source_url: https://arxiv.org/abs/2404.11875
tags:
- explanations
- concepts
- concept
- human
- induction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of large language models (LLMs)
  for concept induction in explainable AI. The authors compare explanations generated
  by GPT-4 with human-generated explanations and those from the ECII concept induction
  system.
---

# Concept Induction using LLMs: a user experiment for assessment

## Quick Facts
- arXiv ID: 2404.11875
- Source URL: https://arxiv.org/abs/2404.11875
- Authors: Adrita Barua; Cara Widmer; Pascal Hitzler
- Reference count: 40
- GPT-4 explanations achieve 63% human preference rate versus ECII's 37%

## Executive Summary
This paper investigates large language models (LLMs) for concept induction in explainable AI, comparing GPT-4-generated explanations against human-generated explanations and the ECII concept induction system. Using the ADE20K image dataset, the authors prompt GPT-4 with object tags to generate concepts explaining differences between image categories. A human study with 265 participants evaluates the comprehensibility of these explanations. Results show human explanations are most preferred (83% vs ECII), followed by GPT-4 explanations (63% vs ECII). While not surpassing human explanations, GPT-4 concepts are significantly more comprehensible than ECII, demonstrating LLMs' potential for automated concept generation in XAI.

## Method Summary
The study uses the ADE20K dataset's object tags from 45 pairs of scene categories (90 categories total) as input. GPT-4 is prompted with zero-shot prompting using temperature=0.5 and top_p=1 to generate 7 concepts per image set pair. Human-generated explanations from a previous study and ECII-generated explanations serve as baselines. A Mechanical Turk study with 265 participants evaluates comprehensibility through pairwise comparisons, with each participant answering 45 questions across 15 image sets. Bradley-Terry analysis derives ability scores from human preferences, and Tukey's HSD test determines statistical significance between groups.

## Key Results
- Human explanations preferred most strongly (83% vs ECII, p<0.001)
- GPT-4 explanations significantly more comprehensible than ECII (63% vs ECII, p<0.001)
- GPT-4 concepts approach but don't match human-level comprehensibility
- Temperature setting of 0.5 balances creativity and consistency in concept generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 generates high-level concepts that are more comprehensible to humans than ECII because it leverages its vast domain knowledge and common-sense reasoning capability.
- Mechanism: GPT-4 processes object tags from images and uses its pre-trained knowledge to generate concepts that align with human intuition about the scenes, rather than being limited to exact object matches like ECII.
- Core assumption: GPT-4's training data includes sufficient common-sense knowledge about scene categorization and object relationships to generate meaningful high-level concepts.
- Evidence anchors:
  - [abstract] "concepts derived from GPT-4 are more comprehensible to humans compared to those generated by ECII"
  - [section] "Unlike logical-deduction-based systems such as ECII, which are limited by background knowledge, an LLM like GPT-4 can leverage its common-sense reasoning capability and vast domain knowledge to produce more comprehensive concepts"
  - [corpus] Weak evidence - only 1 related paper mentions concept induction using LLMs, no direct comparison to ECII
- Break condition: If GPT-4's training data lacks relevant domain knowledge for the specific image categories being analyzed, or if the object tags are too ambiguous for meaningful concept generation.

### Mechanism 2
- Claim: The human study methodology using Bradley-Terry analysis provides statistically valid comparison of concept comprehensibility across different generation methods.
- Mechanism: By presenting pairwise comparisons of concept explanations to human participants and using Bradley-Terry modeling to derive ability scores, the study quantifies relative comprehensibility while controlling for individual rater biases.
- Core assumption: Bradley-Terry model assumptions hold (independent comparisons, consistent preferences, no ties).
- Evidence anchors:
  - [section] "The Bradley-Terry model uses data where entities are compared pairwise, and the outcome (win/loss, preference ranking, etc.) is observed. From these comparisons, the model estimates the abilities Î¸i such that the observed outcomes are statistically likely."
  - [section] "A post hoc analysis using Tukey's Honestly Significant Difference (HSD) test was conducted to determine which specific group means are significantly different from each other."
  - [corpus] No direct evidence - standard statistical methodology not covered in corpus
- Break condition: If participant preferences are inconsistent across trials, or if the sample size is insufficient to detect meaningful differences.

### Mechanism 3
- Claim: GPT-4's zero-shot prompting with temperature=0.5 balances creativity and consistency in concept generation.
- Mechanism: Lower temperature settings reduce randomness in token selection while still allowing some creative interpretation, helping GPT-4 generate relevant concepts without being overly deterministic.
- Core assumption: Temperature setting of 0.5 provides optimal balance between creative interpretation and consistent output for this task.
- Evidence anchors:
  - [section] "We utilized zero-shot prompting with specific parameters, setting the temperature to 0.5 and top p to 1... Lowering the temperature favors words with higher probabilities, leading to more predictable and less creative responses when the model randomly samples the next word."
  - [section] "In our prompts, we aimed to generate generic concepts or object classes that mimic the ontology classes positioned somewhere in the middle of the hierarchy used by ECII."
  - [corpus] Weak evidence - only general prompting guidance available, no specific validation for this task
- Break condition: If temperature setting produces either too generic or too specific concepts that don't align with human expectations, or if zero-shot prompting fails to capture necessary task context.

## Foundational Learning

- Concept: Bradley-Terry model for pairwise comparison analysis
  - Why needed here: To statistically compare comprehensibility of different concept generation methods based on human preferences
  - Quick check question: How does the Bradley-Terry model estimate relative ability scores from pairwise comparison data?

- Concept: Concept induction in description logics
  - Why needed here: To understand how ECII system generates concepts and why GPT-4 might outperform it
  - Quick check question: What are the key limitations of description logic-based concept induction systems compared to LLM-based approaches?

- Concept: Prompt engineering parameters (temperature, top-p)
  - Why needed here: To understand how prompting choices affect GPT-4's concept generation quality
  - Quick check question: How does temperature affect the randomness of token selection in language model outputs?

## Architecture Onboarding

- Component map: GPT-4 API -> Prompt generator -> Concept selector -> Human study interface -> Bradley-Terry analyzer -> Result aggregator
- Critical path: Image object tags -> GPT-4 prompt -> Generated concepts -> Human evaluation -> Statistical analysis -> Comprehensibility ranking
- Design tradeoffs: GPT-4 provides flexibility and common-sense reasoning but requires careful prompting; ECII is deterministic but limited by background knowledge
- Failure signatures: GPT-4 generates irrelevant concepts, human study shows no preference differences, statistical analysis fails significance tests
- First 3 experiments:
  1. Test GPT-4 with varying temperature settings (0.1, 0.5, 0.9) on same image sets to optimize concept relevance
  2. Compare GPT-4 concept generation with and without providing scene category context in prompts
  3. Run smaller pilot human study with 5-10 participants to validate survey interface and catch trial effectiveness before full deployment

## Open Questions the Paper Calls Out

- How do LLM-generated concepts compare to human-generated concepts in terms of alignment with deep neural network activations?
  - Basis in paper: [explicit] The authors mention the need to assess "their mapping with network activations" in the future work section.
  - Why unresolved: The current study focuses on human comprehensibility but does not investigate how well LLM concepts align with actual model behavior.
  - What evidence would resolve it: Experimental results comparing the semantic similarity between LLM concepts and the actual features detected by DNNs in the same classification task.

- What is the optimal prompt engineering approach to minimize irrelevant or "hallucinated" concepts from LLMs?
  - Basis in paper: [explicit] The authors note that "limiting the number of concepts might lead to clearer explanations" and suggest experimenting with different prompting techniques.
  - Why unresolved: The study uses a single prompting approach and acknowledges room for improvement but doesn't systematically explore prompt variations.
  - What evidence would resolve it: Comparative results showing concept quality metrics (relevance, accuracy, consistency) across multiple prompt engineering strategies.

- Would using a larger sample of images per category improve the quality and consistency of LLM-generated explanations?
  - Basis in paper: [inferred] The authors acknowledge a reviewer's suggestion to use "a larger sample of images to strengthen the analysis" but didn't implement it.
  - Why unresolved: The current study uses only 8 images per category, which may be insufficient for capturing the full concept space.
  - What evidence would resolve it: Results showing changes in explanation quality metrics (comprehensibility, accuracy, consistency) when using varying numbers of images per category.

## Limitations
- Study relies on Mechanical Turk participants, introducing potential bias from non-expert raters and cultural differences
- Temperature setting of 0.5 chosen without systematic validation across different image categories
- ADE20K object tags may not fully capture visual complexity needed for meaningful concept generation

## Confidence

- **High confidence**: GPT-4 produces more comprehensible concepts than ECII (p<0.05 confirmed via Tukey's HSD)
- **Medium confidence**: GPT-4 concepts approach human-level comprehensibility (83% vs 63% preference rates suggest proximity but not equivalence)
- **Low confidence**: Temperature=0.5 is optimal for this task (no systematic comparison with other settings performed)

## Next Checks

1. Test GPT-4 with temperature settings of 0.1, 0.5, and 0.9 on identical image sets to quantify impact on concept relevance and human preference scores
2. Conduct follow-up study with domain experts to compare their concept preferences against Mechanical Turk results, controlling for expertise effects
3. Apply same methodology to Visual Genome dataset to assess whether GPT-4's performance advantage over ECII generalizes beyond ADE20K