---
ver: rpa2
title: Enhancing adversarial robustness in Natural Language Inference using explanations
arxiv_id: '2409.07423'
source_url: https://arxiv.org/abs/2409.07423
tags:
- explanation
- label
- language
- explanations
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using intermediate natural language explanations
  to improve the robustness of Natural Language Inference (NLI) models against adversarial
  attacks. The authors adopt an ExplainThenPredict framework, where a sequence-to-sequence
  model first generates an explanation for the semantic relationship between premise
  and hypothesis, then a classifier predicts the entailment label based on the explanation.
---

# Enhancing adversarial robustness in Natural Language Inference using explanations

## Quick Facts
- arXiv ID: 2409.07423
- Source URL: https://arxiv.org/abs/2409.07423
- Reference count: 18
- Primary result: Using intermediate natural language explanations improves NLI model robustness against adversarial attacks compared to explanation-free baselines

## Executive Summary
This paper proposes using intermediate natural language explanations to improve the robustness of Natural Language Inference (NLI) models against adversarial attacks. The authors adopt an ExplainThenPredict framework where a sequence-to-sequence model first generates an explanation for the semantic relationship between premise and hypothesis, then a classifier predicts the entailment label based on the explanation. Experiments on e-SNLI using four different Seq2Seq models show that explanations reduce attack success rates compared to an explanation-free baseline, with robustness linked to explanation quality. The study concludes that generating faithful explanations can significantly enhance NLI robustness, though further work is needed to ensure explanation faithfulness.

## Method Summary
The authors propose an ExplainThenPredict framework where premise-hypothesis pairs are first processed by a Seq2Seq model to generate natural language explanations, which are then fed into a BERT-based classifier to predict NLI labels (entailment, neutral, contradiction). Four different Seq2Seq architectures (BERT2GPT, ROBERTA2GPT, ALBERT2GPT, DISTILBERT2GPT) are fine-tuned on e-SNLI to generate explanations. The approach is evaluated against TextFooler and BERT-Attack adversarial methods, comparing robustness metrics with an explanation-free baseline. The framework sacrifices some accuracy (86.72-87.97% vs 90.13% baseline) but achieves significantly higher resistance to attacks.

## Key Results
- Generated explanations result in decreased attack success rates compared to explanation-free baseline
- ROBERTA2GPT achieves the best overall performance with 87.97% accuracy and superior explanation quality metrics
- Attacks are consistently more effective on hypotheses than premises, likely due to shorter length and easier semantic corruption
- Higher quality explanations correlate with better robustness, highlighting the importance of explanation faithfulness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating intermediate natural language explanations before classification increases model robustness to adversarial attacks.
- Mechanism: The explanation step forces the model to reason about the semantic relationship between premise and hypothesis, which provides a stable intermediate representation that is less susceptible to small, targeted word-level perturbations.
- Core assumption: Adversarial attacks on premise/hypothesis are more likely to succeed if the model relies solely on surface-level cues; explanations shift focus to deeper semantic reasoning.
- Evidence anchors:
  - [abstract] "only by fine-tuning a classifier on the explanation rather than premise-hypothesis inputs, robustness under various adversarial attacks is achieved"
  - [section 3.3] "the generated explanations result in a decrease of overall accuracy scores in comparison to the baselines...However, in the next section we will highlight the real value of such a sacrifice"
- Break condition: If generated explanations are low quality or unfaithful, the robustness benefit may not materialize; attacks can corrupt the explanation path.

### Mechanism 2
- Claim: The choice of Seq2Seq model for explanation generation affects the overall robustness of the ExplainThenPredict pipeline.
- Mechanism: Different Seq2Seq architectures (BERT2GPT, ROBERTA2GPT, etc.) produce explanations of varying linguistic quality and faithfulness; higher-quality explanations lead to better intermediate reasoning and thus better resistance to adversarial perturbations.
- Core assumption: Explanation quality correlates with robustness; a stronger encoder-decoder pair will yield more stable explanations under attack.
- Evidence anchors:
  - [section 3.2] "ROBERTA2GPT scores higher in terms of most text generation metrics...and the number of semantically correct explanations"
  - [section 4.2] "the choice of Seq2Seq model matters...optimizing explanation quality results in advanced ExplainThenPredict robustness"
- Break condition: If all Seq2Seq models produce similarly poor explanations, robustness gains may be negligible regardless of architecture.

### Mechanism 3
- Claim: Adversarial attacks are more effective on the hypothesis than the premise because hypotheses are shorter and thus easier to semantically corrupt with minimal edits.
- Mechanism: Shorter text spans provide fewer opportunities for the model to recover semantic consistency after perturbation; hypothesis edits more directly disrupt the premise-hypothesis alignment reflected in the explanation.
- Core assumption: Length and content overlap between premise and hypothesis influence attack success; hypothesis brevity increases vulnerability.
- Evidence anchors:
  - [section 4.2] "The attacks are consistently more effective when targeting H rather than P regardless the attacker utilized each time"
  - [section 4.3] "we assume that this is due to the shorter length of H; therefore, intervened semantics of H cannot be matched with their counterparts present in P"
- Break condition: If premise and hypothesis are of similar length or if attacks are constrained to premise-only edits, this differential effect may not hold.

## Foundational Learning

- Concept: Natural Language Inference (NLI) as a semantic classification task
  - Why needed here: The paper builds a defense strategy specifically for NLI; understanding entailment/neutral/contradiction labels is essential to reason about explanation generation and attack effectiveness.
  - Quick check question: What are the three possible NLI labels and what do they mean in terms of premise-hypothesis relationship?

- Concept: Adversarial attacks in NLP (word-level synonym substitution)
  - Why needed here: The defense strategy is evaluated against attacks; understanding how TextFooler and BERT-Attack work is key to interpreting robustness metrics.
  - Quick check question: How do TextFooler and BERT-Attack ensure semantic minimality of their interventions?

- Concept: Explainability and faithfulness in NLP models
  - Why needed here: The paper hinges on the idea that explanations can both clarify reasoning and improve robustness; knowing what makes an explanation faithful is critical for interpreting results.
  - Quick check question: Why might an explanation that is linguistically fluent still be considered unfaithful?

## Architecture Onboarding

- Component map: Premise-Hypothesis input → Seq2Seq encoder-decoder → Natural language explanation → BERT classifier → NLI label (E/N/C)

- Critical path:
  1. Generate explanation from P-H pair using Seq2Seq
  2. Feed explanation into Expl2Label classifier
  3. Evaluate robustness by applying adversarial attack on P or H, measuring change in final label

- Design tradeoffs:
  - Accuracy vs. robustness: Explanation generation slightly reduces baseline accuracy but increases attack resistance
  - Model size vs. efficiency: Smaller Seq2Seq models used for reproducibility; larger models might improve explanation quality and robustness
  - Explanation quality vs. faithfulness: High linguistic quality does not guarantee semantic faithfulness

- Failure signatures:
  - Low after-attack accuracy despite explanation use → explanation quality too low
  - High attack success rate on hypothesis but not premise → hypothesis brevity issue
  - Baseline outperforms ExplainThenPredict → Seq2Seq model too weak

- First 3 experiments:
  1. Train and evaluate BERT2GPT, ALBERT2GPT, DISTILBERT2GPT, ROBERTA2GPT on e-SNLI; measure explanation accuracy and text generation metrics
  2. Fine-tune BERT on ground-truth explanations; test classification accuracy with generated explanations from each Seq2Seq variant
  3. Apply TextFooler and BERT-Attack on P and H separately; compare attack success rates between explanation-free baseline and each ExplainThenPredict variant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we ensure faithfulness of generated explanations in the ExplainThenPredict framework?
- Basis in paper: [explicit] The authors acknowledge that "the quality of the explanations is a crucial factor" and note that "ensuring faithfulness of explanations with respect to input premise and hypothesis, as well as the output label" is a limitation.
- Why unresolved: Current metrics like METEOR, ROUGE, and BERTScore do not directly measure semantic faithfulness, and the authors note that "formulating an informative and correct explanation is a separate research problem."
- What evidence would resolve it: Development and validation of evaluation metrics or methods that reliably measure semantic faithfulness between generated explanations, input pairs, and output labels.

### Open Question 2
- Question: Would using state-of-the-art large language models (LLMs) for explanation generation improve robustness compared to smaller models?
- Basis in paper: [inferred] The authors mention that "experimentation using state-of-the-art LLMs may benefit the quality of explanations" but also note limitations regarding computational resources and faithfulness guarantees.
- Why unresolved: The paper uses smaller, resource-efficient models (BERT, ALBERT, DistilBERT, RoBERTa with GPT-2) to prove the robustness-enhancing power of explanations, but doesn't test larger models.
- What evidence would resolve it: Comparative experiments using LLMs (e.g., GPT-3, GPT-4) for the Seq2Seq explanation generation step, measuring both explanation quality and resulting model robustness against adversarial attacks.

### Open Question 3
- Question: How does the ExplainThenPredict framework's robustness trade-off with accuracy compare to other adversarial defense strategies in NLP?
- Basis in paper: [explicit] The authors note that "the quest for robustness may require some sacrifice in accuracy" and demonstrate that their approach achieves lower accuracy (86.72-87.97%) compared to the explanation-free baseline (90.13%), but higher robustness against attacks.
- Why unresolved: The paper doesn't compare this trade-off with other defense mechanisms like adversarial training, input denoising, or certified robustness methods that might have different accuracy-robustness relationships.
- What evidence would resolve it: Systematic comparison of ExplainThenPredict against multiple adversarial defense strategies, measuring both accuracy and robustness under various attack scenarios to determine relative effectiveness.

## Limitations
- The study does not provide direct evidence that explanations are semantically faithful to the input premise-hypothesis pairs, which is crucial for the proposed robustness mechanism to work.
- The effectiveness of the proposed defense may be limited to the specific attack methods (TextFooler and BERT-Attack) used in the experiments.
- The paper uses relatively small Seq2Seq models for reproducibility, but it remains unclear whether larger, more capable models would yield significantly better robustness gains.

## Confidence

- **High Confidence**: The observation that adversarial attacks are more effective on hypotheses than premises, and the correlation between explanation quality and robustness.
- **Medium Confidence**: The core claim that generating intermediate explanations improves NLI robustness, as this is supported by experimental results but lacks direct validation of explanation faithfulness.
- **Low Confidence**: The generalizability of the proposed defense to other types of adversarial attacks or real-world noisy inputs, given the limited attack methods evaluated.

## Next Checks
1. Conduct a human evaluation study to assess the semantic faithfulness of the generated explanations, ensuring they accurately reflect the premise-hypothesis relationship.
2. Test the ExplainThenPredict framework against a broader range of adversarial attacks, including gradient-based methods and real-world noisy inputs, to evaluate its generalizability.
3. Experiment with larger, more capable Seq2Seq models (e.g., GPT-3, T5) for explanation generation to determine if increased model capacity leads to significantly better robustness gains.