---
ver: rpa2
title: Knowledge Verification to Nip Hallucination in the Bud
arxiv_id: '2401.10768'
source_url: https://arxiv.org/abs/2401.10768
tags:
- knowledge
- tuning
- llms
- arxiv
- refusal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Knowledge Consistent Alignment (KCA), a novel
  approach to reduce hallucinations in large language models (LLMs) by detecting and
  addressing knowledge inconsistency between the external knowledge in alignment training
  data and the intrinsic knowledge from pretraining. KCA formulates multi-choice examinations
  based on external knowledge to assess the LLMs' comprehension and identifies inconsistent
  instances.
---

# Knowledge Verification to Nip Hallucination in the Bud

## Quick Facts
- arXiv ID: 2401.10768
- Source URL: https://arxiv.org/abs/2401.10768
- Reference count: 35
- Introduces Knowledge Consistent Alignment (KCA) approach that significantly reduces hallucination rates in LLMs by detecting and addressing knowledge inconsistency

## Executive Summary
This paper introduces Knowledge Consistent Alignment (KCA), a novel approach to reduce hallucinations in large language models (LLMs) by detecting and addressing knowledge inconsistency between the external knowledge in alignment training data and the intrinsic knowledge from pretraining. KCA formulates multi-choice examinations based on external knowledge to assess the LLMs' comprehension and identifies inconsistent instances. It then employs three strategies to process these instances: open-book tuning (supplementing knowledge), discarding tuning (removing inconsistent data), and refusal tuning (generating refusal responses). Experiments across six benchmarks and various LLMs show that KCA significantly reduces hallucination rates, with improvements up to 10 points on certain benchmarks.

## Method Summary
KCA addresses hallucinations by first detecting knowledge inconsistency between external knowledge in alignment data and intrinsic pretraining knowledge. It uses a well-aligned LLM to generate multi-choice examinations to assess the base LLM's comprehension, identifying consistent (Dc) and inconsistent (Di) instances. Three strategies process these instances: open-book tuning appends reference knowledge to training data, discarding tuning removes inconsistent instances, and refusal tuning reformats responses to generate refusal answers. The approach is evaluated across six benchmarks using multiple model families (Pythia, Llama-2, Mistral) with instruction tuning for three epochs.

## Key Results
- Pythia 7B with discarding tuning achieves a 7.69-point reduction in hallucination rate on VicunaEval
- Llama-2 7B with open-book tuning achieves a 3.96-point reduction on WizardLMEval
- Refusal tuning consistently yields the largest reductions, such as a 25.11-point decrease on TruthfulQA for Pythia 7B
- KCA maintains comparable helpfulness scores to standard tuning while effectively mitigating hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge Inconsistency Detection reduces hallucination by identifying and separating training instances where external knowledge in the alignment data contradicts the LLM's pretraining knowledge
- Mechanism: KCA uses a well-aligned LLM to generate multiple-choice examinations based on external knowledge requirements, then tests the base LLM's comprehension to identify inconsistent instances (Di) versus consistent ones (Dc)
- Core assumption: External knowledge requirements can be reliably classified and the corresponding knowledge can be generated to create meaningful examinations
- Evidence anchors:
  - [abstract] "KCA employs a well-aligned LLM to automatically formulate assessments based on external knowledge to evaluate the knowledge boundaries of foundation LLMs"
  - [section 3.2] "KCA employs a well-aligned model to design multi-choice questions for the training data, thereby comprehensively assessing LLMs' understanding of implicit knowledge"
  - [corpus] Weak evidence - corpus provides related hallucination detection papers but no direct mechanism validation
- Break condition: If the well-aligned LLM cannot accurately generate knowledge requirements or create effective examinations, the detection mechanism fails

### Mechanism 2
- Claim: Open-book tuning reduces hallucination by providing contextual knowledge during training to prevent parameter updates toward inconsistent information
- Mechanism: For instances in Di, KCA appends generated reference knowledge Ki to the instruction Ii during fine-tuning, allowing the LLM to learn with complete context rather than from potentially inconsistent information
- Core assumption: Providing supplementary knowledge during training prevents the model from learning incorrect associations and maintains helpfulness
- Evidence anchors:
  - [abstract] "KCA implements several specific strategies to deal with these data instances" including "open-book tuning (supplementing knowledge)"
  - [section 3.3] "we propose to introduce the generated reference knowledge Ki along with the respective instruction Ii in Di"
  - [corpus] Weak evidence - corpus lacks direct evidence for open-book tuning effectiveness
- Break condition: If the reference knowledge itself contains errors or if the model cannot effectively integrate the supplementary information

### Mechanism 3
- Claim: Refusal tuning reduces hallucination by teaching LLMs to decline responding to instructions requiring knowledge beyond their capabilities
- Mechanism: KCA reformats responses in Di to refusal format ("I don't know the factual information required to answer this instruction") and includes these in training, creating an honest model that acknowledges knowledge limitations
- Core assumption: Teaching refusal responses maintains helpfulness while reducing hallucination by avoiding confident incorrect responses
- Evidence anchors:
  - [abstract] "refusal tuning (generating refusal responses)" and "refusal tuning consistently yields the largest reductions"
  - [section 3.3] "we adjust the response Ri in Di to refusal format, and employ the processed Di, Dc, and Dd together for fine-tuning M"
  - [corpus] Weak evidence - corpus lacks direct evidence for refusal tuning effectiveness
- Break condition: If refusal responses significantly reduce helpfulness beyond acceptable thresholds or if users prefer confident but potentially incorrect answers

## Foundational Learning

- Concept: Knowledge Inconsistency
  - Why needed here: Understanding the difference between external knowledge in alignment data and intrinsic knowledge from pretraining is fundamental to why KCA works
  - Quick check question: What distinguishes external knowledge from intrinsic knowledge in the context of LLM alignment?

- Concept: Multi-choice Examination Formulation
  - Why needed here: KCA relies on generating and evaluating multiple-choice questions to detect knowledge inconsistencies
  - Quick check question: How does formulating multiple-choice questions help identify when alignment data contradicts pretraining knowledge?

- Concept: Instruction Tuning with Refusal Responses
  - Why needed here: Refusal tuning is one of KCA's three strategies, requiring understanding how to incorporate refusal responses into training
  - Quick check question: What is the purpose of including refusal responses in the training data during KCA's refusal tuning strategy?

## Architecture Onboarding

- Component map: Knowledge Inconsistency Detection (classification, knowledge generation, examination formulation) -> Knowledge Inconsistency Processing (open-book tuning, discarding tuning, refusal tuning) -> Fine-tuning
- Critical path: Knowledge requirement classification → Reference knowledge generation → Examination formulation → Knowledge inconsistency processing → Fine-tuning
- Design tradeoffs: Open-book tuning provides context but increases computational cost and token usage; discarding tuning simplifies training but may reduce data diversity; refusal tuning improves honesty but may reduce helpfulness scores
- Failure signatures: High hallucination rates despite KCA application indicate detection failures; refusal tuning with very low helpfulness scores suggests over-cautious responses; open-book tuning with minimal improvement may indicate poor knowledge supplementation
- First 3 experiments:
  1. Run KCA on a small subset of WizardLM training data with Pythia 7B to verify detection accuracy and examine the distribution of Di vs Dc
  2. Compare hallucination rates on VicunaEval between standard tuning and each KCA strategy using GPT-4 evaluation
  3. Measure helpfulness scores on LIMAEval to confirm that open-book and discarding tuning maintain comparable performance to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the stability of knowledge embedded within LLMs affect the reliability of knowledge inconsistency detection across different model scales and pretraining corpora?
- Basis in paper: [explicit] The paper states, "in comparison to capabilities, the knowledge embedded within LLMs is relatively stable, making inconsistency detection more reliable."
- Why unresolved: The paper claims stability but does not empirically test whether this stability holds across different model scales (7B vs. 13B) or pretraining corpora. It also does not investigate whether the stability of knowledge affects the accuracy of knowledge inconsistency detection.
- What evidence would resolve it: Conducting experiments to compare knowledge inconsistency detection accuracy across models of varying scales and pretraining corpora, while measuring the stability of embedded knowledge over time or across fine-tuning tasks.

### Open Question 2
- Question: What are the long-term effects of refusal tuning on the overall utility and user satisfaction of LLMs in real-world applications?
- Basis in paper: [explicit] The paper mentions that refusal tuning "consistently and significantly reduces the hallucination rate" but also notes that "the fine-tuned LLM tends to generate refusal responses for honesty, resulting in a lower helpful score."
- Why unresolved: While the paper demonstrates the effectiveness of refusal tuning in reducing hallucinations, it does not explore the broader implications of this approach on user satisfaction, trust, or the practical usability of LLMs in real-world scenarios where some level of helpfulness is crucial.
- What evidence would resolve it: Longitudinal studies tracking user interactions with refusal-tuned LLMs in real-world applications, measuring user satisfaction, trust, and task completion rates compared to models with lower refusal rates but higher hallucination risks.

### Open Question 3
- Question: How does the choice of knowledge-measuring benchmarks (e.g., MMLU) influence the effectiveness of knowledge inconsistency detection and subsequent hallucination mitigation?
- Basis in paper: [inferred] The paper draws inspiration from knowledge-measuring benchmarks like MMLU for formulating examinations to detect knowledge inconsistency, suggesting a reliance on such benchmarks for the KCA approach.
- Why unresolved: The paper does not investigate whether different knowledge-measuring benchmarks would yield varying levels of effectiveness in detecting knowledge inconsistency or whether the choice of benchmark impacts the overall success of hallucination mitigation.
- What evidence would resolve it: Comparative experiments using different knowledge-measuring benchmarks to detect knowledge inconsistency, followed by evaluating the hallucination mitigation performance of the resulting tuned models.

## Limitations

- The approach relies heavily on the quality of the well-aligned LLM used for knowledge inconsistency detection, which may not be universally available
- The three tuning strategies may not be balanced properly - open-book tuning might introduce too much external knowledge, discarding tuning might remove too much data, and refusal tuning might generate too many refusals
- The evaluation protocols and prompts used for GPT-4-based hallucination and helpfulness measurements are not fully specified, limiting reproducibility

## Confidence

- **High confidence**: Experimental results showing hallucination rate reductions (10+ points on some benchmarks) are well-documented and reproducible given the described methodology
- **Medium confidence**: The three proposed mechanisms (open-book, discarding, refusal tuning) are theoretically sound but lack detailed ablation studies to isolate their individual contributions
- **Medium confidence**: Claims about maintaining helpfulness scores are supported by GPT-4 evaluations, but the evaluation prompts are not specified, making independent verification difficult

## Next Checks

1. Conduct ablation studies comparing each KCA strategy individually to understand their specific contributions to hallucination reduction
2. Test KCA across additional model families beyond Pythia, Llama-2, and Mistral to assess generalizability
3. Implement human evaluation studies to validate GPT-4-based hallucination and helpfulness measurements, particularly for refusal tuning's impact on user experience