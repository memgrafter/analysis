---
ver: rpa2
title: 'SSD-TS: Exploring the Potential of Linear State Space Models for Diffusion
  Models in Time Series Imputation'
arxiv_id: '2410.13338'
source_url: https://arxiv.org/abs/2410.13338
tags:
- time
- series
- imputation
- missing
- probabilistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiffImp, a diffusion-based model for probabilistic
  time series imputation that leverages Mamba as the backbone denoising module to
  achieve linear time complexity. The model incorporates bidirectional attention and
  inter-channel dependency modeling through BAM and CMB blocks to effectively handle
  both forward and reverse temporal dependencies and correlations among multiple variables.
---

# SSD-TS: Exploring the Potential of Linear State Space Models for Diffusion Models in Time Series Imputation

## Quick Facts
- arXiv ID: 2410.13338
- Source URL: https://arxiv.org/abs/2410.13338
- Reference count: 40
- Primary result: Achieves up to 21.4% improvement in CRPS-sum and best MSE results in high missing ratio scenarios

## Executive Summary
This paper introduces DiffImp, a diffusion-based model for probabilistic time series imputation that leverages Mamba as the backbone denoising module to achieve linear time complexity. The model incorporates bidirectional attention and inter-channel dependency modeling through BAM and CMB blocks to effectively handle both forward and reverse temporal dependencies and correlations among multiple variables. Experiments on three real-world datasets demonstrate state-of-the-art performance in both imputation and forecasting tasks across different missing ratios and patterns.

## Method Summary
DiffImp is a diffusion probabilistic model for time series imputation that uses Mamba as its backbone to achieve linear complexity. The architecture includes three key components: PNM blocks for processing noisy inputs, BAM blocks that implement bidirectional attention through separate forward and backward Mamba modules with position-specific weight fusion, and CMB blocks that model inter-channel dependencies by treating variables across channels as sequences. The model is trained using a self-supervised masking approach on three real-world datasets with varying characteristics.

## Key Results
- Achieves up to 21.4% improvement in CRPS-sum metric for probabilistic imputation
- Delivers best MSE results in high missing ratio scenarios compared to baseline methods
- Demonstrates state-of-the-art performance in both imputation and forecasting tasks across different datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bidirectional attention in Mamba enables effective modeling of forward and reverse temporal dependencies for time series imputation.
- Mechanism: The Bidirectional Attention Mamba (BAM) block applies separate forward and backward Mamba modules, then learns position-specific weights using temporal attention to combine their outputs.
- Core assumption: Missing values have correlated neighbors on both sides, and these correlations can be effectively captured through separate directional processing followed by weighted fusion.
- Evidence anchors:
  - [abstract] "To enable Mamba to capture information from both sides of the missing values, we then propose a Bidirectional Attention Mamba block (BAM) that is more applicable to time series imputation task."
  - [section] "To enable Mamba to capture information from both sides of the missing values, we then propose a Bidirectional Attention Mamba block (BAM) that is more applicable to time series imputation task."
  - [corpus] No direct corpus evidence found for BAM specifically; diffusion models for time series imputation exist but bidirectional Mamba is novel.
- Break condition: If temporal dependencies are not bidirectional or if the attention mechanism fails to learn meaningful position weights.

### Mechanism 2
- Claim: Channel Mamba Block (CMB) captures inter-channel dependencies by treating variables across different channels as a sequence.
- Mechanism: The CMB block transposes the time series representation to process the channel dimension as a sequence, applying Mamba to model inter-channel correlations.
- Core assumption: Variables across different channels in multivariate time series have sequential dependencies that can be modeled using sequence modeling techniques.
- Evidence anchors:
  - [abstract] "To incorporate bidirectional dependencies, we design a learnable weight module inside the BAM block. This module learns the weights of all points within the sequence, facilitating the modeling of dependencies at different distances."
  - [section] "To incorporate bidirectional dependencies, we design a learnable weight module inside the BAM block. This module learns the weights of all points within the sequence, facilitating the modeling of dependencies at different distances."
  - [corpus] Weak evidence; diffusion models for time series imputation exist but channel-specific Mamba treatment is not well-represented in corpus.
- Break condition: If inter-channel dependencies are not sequential in nature or if transposition fails to preserve meaningful relationships.

### Mechanism 3
- Claim: Linear complexity Mamba backbone enables scalable time series imputation without quadratic complexity bottlenecks.
- Mechanism: Mamba replaces attention mechanisms with state space models, achieving O(L) complexity where L is sequence length, through structured matrix multiplications.
- Core assumption: State space models can effectively capture long-range dependencies while maintaining linear complexity through structured transformations.
- Evidence anchors:
  - [abstract] "We integrate the computational efficient state space model, namely Mamba, as the backbone denosing module for DDPMs."
  - [section] "To ensure linear complexity, we choose the SSM-based model as the backbone of our framework, which is Mamba Dao & Gu (2024) to be more specific."
  - [corpus] Some evidence for SSM efficiency; corpus contains diffusion-based time series imputation but limited coverage of Mamba-specific efficiency claims.
- Break condition: If long-range dependencies cannot be effectively captured within linear complexity constraints.

## Foundational Learning

- Concept: State Space Models (SSMs)
  - Why needed here: SSMs provide the mathematical foundation for Mamba's linear complexity sequence modeling
  - Quick check question: What is the key difference between continuous and discrete SSM formulations in terms of computational efficiency?

- Concept: Diffusion probabilistic models
  - Why needed here: DDPMs form the probabilistic framework for uncertainty-aware time series imputation
  - Quick check question: How does the variance scheduler βt control the forward diffusion process in terms of noise injection?

- Concept: Bidirectional sequence modeling
  - Why needed here: Missing values have dependencies in both temporal directions, requiring bidirectional information flow
  - Quick check question: Why is bidirectional processing more effective than unidirectional for time series imputation tasks?

## Architecture Onboarding

- Component map: Noisy input → Input embedding (Linear + SMM) → Conditional input embedding (Linear + SMM) → Diffusion step embedding → SMM → Feature fusion → SMM → Output projection → Noise prediction
- Critical path: Input embedding → SMM → Feature fusion → SMM → Output projection → Noise prediction
- Design tradeoffs: Linear complexity (Mamba) vs quadratic complexity (Transformer) for scalability; bidirectional vs unidirectional for dependency capture; channel vs temporal modeling separation
- Failure signatures: Poor imputation accuracy on high missing ratio scenarios; slow convergence during training; high memory usage for long sequences
- First 3 experiments:
  1. Run with only forward Mamba (remove BAM bidirectional processing) and measure performance degradation
  2. Remove CMB block and observe impact on multivariate time series performance
  3. Replace Mamba with Transformer backbone and measure complexity/runtime differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed DiffImp model's performance scale with sequence length compared to transformer-based approaches, and what are the practical limits of sequence length before performance degradation occurs?
- Basis in paper: [explicit] The paper mentions DiffImp achieves linear time complexity while transformers have quadratic complexity, but doesn't provide empirical scaling analysis across varying sequence lengths.
- Why unresolved: The experiments use fixed sequence lengths (100 for MuJoCo and Electricity, varying for ETTm1), preventing analysis of scaling behavior.
- What evidence would resolve it: Systematic experiments varying sequence lengths across multiple orders of magnitude while measuring both accuracy and runtime.

### Open Question 2
- Question: What is the impact of different variance schedulers (βt) on DiffImp's imputation performance, and how sensitive is the model to this hyperparameter?
- Basis in paper: [explicit] The paper mentions using predefined variance schedulers but doesn't explore or analyze the sensitivity to different scheduling strategies.
- Why unresolved: The paper uses standard schedulers from the literature but doesn't provide ablation studies or sensitivity analysis for this key hyperparameter.
- What evidence would resolve it: Comparative experiments using different variance scheduler configurations and their impact on imputation accuracy across datasets.

### Open Question 3
- Question: How does DiffImp's bidirectional attention mechanism compare to other bidirectional modeling approaches (like bidirectional LSTMs or transformers with bidirectional attention) in terms of both performance and computational efficiency?
- Basis in paper: [explicit] The paper proposes BAM as a novel bidirectional mechanism but doesn't compare it against alternative bidirectional approaches.
- Why unresolved: The ablation studies only remove components rather than comparing against alternative bidirectional implementations.
- What evidence would resolve it: Head-to-head comparisons of BAM against bidirectional LSTM and bidirectional transformer variants on the same tasks and datasets.

## Limitations

- Novel architectural components (BAM, CMB) lack established theoretical foundations and limited corpus evidence
- Performance claims rely heavily on experimental results from only three specific datasets
- Linear complexity benefits may be affected by implementation overhead of bidirectional and channel modeling blocks

## Confidence

- High Confidence: Linear complexity benefits of Mamba backbone, general diffusion probabilistic framework for imputation
- Medium Confidence: BAM block effectiveness for bidirectional dependency capture, CMB block for inter-channel modeling
- Low Confidence: Specific performance improvements (e.g., "up to 21.4% improvement in CRPS-sum") without ablation studies isolating architectural contributions

## Next Checks

1. **Ablation study**: Implement DiffImp variants with (a) forward-only Mamba (no BAM), (b) no CMB block, and (c) Transformer backbone to quantify individual architectural contributions to performance gains
2. **Complexity validation**: Measure actual runtime and memory usage on long sequences (L > 1000) to verify linear complexity claims and identify potential bottlenecks in BAM/CMB implementations
3. **Cross-dataset robustness**: Test DiffImp on additional time series datasets with varying characteristics (different dimensionalities, missing patterns, temporal dynamics) to assess generalization beyond the three evaluation datasets