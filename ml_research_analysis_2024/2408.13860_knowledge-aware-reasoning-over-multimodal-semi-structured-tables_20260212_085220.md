---
ver: rpa2
title: Knowledge-Aware Reasoning over Multimodal Semi-structured Tables
arxiv_id: '2408.13860'
source_url: https://arxiv.org/abs/2408.13860
tags:
- table
- questions
- reasoning
- visual
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces MMTABQA, a dataset for multimodal table reasoning
  that integrates images and text. Current AI models struggle with tasks like entity
  disambiguation, visual context understanding, and comparing visual content across
  images.
---

# Knowledge-Aware Reasoning over Multimodal Semi-structured Tables

## Quick Facts
- arXiv ID: 2408.13860
- Source URL: https://arxiv.org/abs/2408.13860
- Authors: Suyash Vardhan Mathur; Jainit Sushil Bafna; Kunal Kartik; Harshita Khandelwal; Manish Shrivastava; Vivek Gupta; Mohit Bansal; Dan Roth
- Reference count: 22
- Key outcome: Current AI models struggle with multimodal table reasoning tasks requiring entity disambiguation, visual context understanding, and cross-image comparison.

## Executive Summary
This study introduces MMTABQA, a dataset for multimodal table reasoning that integrates images and text. By replacing text entities in Wikipedia tables with corresponding images, the researchers create a challenging benchmark for evaluating knowledge-aware reasoning capabilities. The dataset tests models' ability to link visual representations to textual mentions, understand visual context, and compare visual content across images. Experiments reveal substantial performance gaps between closed-source and open-source models, with all approaches facing significant challenges in multimodal reasoning tasks.

## Method Summary
The researchers created MMTABQA by replacing text entities in existing Wikipedia tables with corresponding images, generating 7,016 multimodal questions across three types: explicit (referencing image-replaced entities), implicit (requiring intermediate reasoning), and visual (requiring attribute recognition or comparison). They evaluated multiple models including closed-source options (GPT-4o, Gemini-1.5 Flash) and open-source alternatives (Llama-3, InternVL-Chat) using partial input baselines and oracle entity replacement strategies. The evaluation employed substring match and ROUGE-L metrics to assess answer quality.

## Key Results
- Closed-source models (GPT-4o, Gemini-1.5 Flash) significantly outperform open-source models on multimodal tasks
- All models struggle with visual questions requiring attribute recognition and cross-image comparison
- Explicit questions are easiest to answer, while visual questions present the greatest challenge
- Entity disambiguation remains a fundamental bottleneck across all model architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal tables require models to jointly interpret visual and textual entities within the same structured context.
- Mechanism: By replacing text entities with images in existing tables, the model must perform entity disambiguation, visual reasoning, and cross-image comparison to answer questions.
- Core assumption: Visual representations of entities preserve enough semantic information for models to link them to original text mentions.
- Evidence anchors:
  - [abstract] "Our experiments highlight substantial challenges for current AI models in effectively integrating and interpreting multiple text and image inputs, understanding visual context, and comparing visual content across images."
  - [section] "In Fig. 1, phone images in the table correspond to the named phones in the header, and processor images must be linked to their respective phone columns."
- Break condition: If images are too abstract or lack discriminative features, models cannot perform correct entity disambiguation.

### Mechanism 2
- Claim: Closed-source models outperform open-source models on multimodal tasks due to superior vision encoders and training techniques.
- Mechanism: GPT-4o and Gemini-1.5 Flash achieve higher accuracy because their architecture better integrates visual and textual modalities compared to open-source alternatives.
- Core assumption: The underlying vision encoders in closed-source models have been trained on more diverse and higher-quality data.
- Evidence anchors:
  - [abstract] "Experiments show that closed-source models like GPT-4o outperform open-source models, but all face significant challenges."
  - [section] "Closed-source models like GPT-4o and Gemini-1.5 Flash outperform open-source models in multimodal tasks due to advanced training techniques and better integration of visual and textual data."
- Break condition: If vision encoders are not the limiting factor, then differences in performance may stem from other architectural components.

### Mechanism 3
- Claim: Question type complexity directly impacts model performance, with explicit questions being easiest and visual questions hardest.
- Mechanism: Explicit questions reference image-replaced entities directly, making them easier to answer than implicit questions requiring intermediate reasoning or visual questions requiring attribute recognition.
- Core assumption: The presence of explicit cues reduces cognitive load for the model compared to questions requiring multi-step inference or visual attribute comparison.
- Evidence anchors:
  - [section] "Explicit questions, as defined in the preceding discussion, contain clear and specific entity mentions within the query, facilitating their resolution by computational models."
  - [section] "Visual questions perform better than answer-mention questions but underscore a significant limitation of current language models."
- Break condition: If models develop stronger visual reasoning capabilities, the performance gap between question types may narrow.

## Foundational Learning

- Concept: Entity Disambiguation
  - Why needed here: Models must correctly identify which image corresponds to which entity mentioned in questions or answers.
  - Quick check question: Given a table with logos and team names, can the model correctly link a logo image to its team name when asked about the team?

- Concept: Visual Attribute Recognition
  - Why needed here: Models need to identify specific visual features (colors, shapes, arrangements) to answer questions about images.
  - Quick check question: When shown two phone images with different camera arrangements, can the model correctly describe the difference?

- Concept: Cross-Image Comparison
  - Why needed here: Some questions require comparing visual features across multiple images in the table.
  - Quick check question: Given multiple product images, can the model identify which product has the most color options by visually counting them?

## Architecture Onboarding

- Component map: Data ingestion pipeline -> Entity linking module -> Question classification system -> Multimodal reasoning engine -> Evaluation framework

- Critical path: Image replacement → Question filtering → Multimodal reasoning → Answer generation → Evaluation

- Design tradeoffs:
  - Using existing Wikipedia tables saves annotation cost but limits dataset diversity
  - Synthetic visual questions enable more testing but may introduce artifacts
  - Table-as-image approach simplifies input but loses structural information
  - Interleaved approach preserves structure but increases computational complexity

- Failure signatures:
  - High error rate on visual questions indicates vision encoder limitations
  - Poor performance on implicit questions suggests reasoning pipeline weaknesses
  - Low scores on answer-mention questions point to entity generation issues
  - Consistent failures across all question types may indicate fundamental architecture limitations

- First 3 experiments:
  1. Run partial input baseline on a small sample to establish lower bound performance
  2. Test oracle entity replacement to verify upper bound potential
  3. Compare table-as-image vs interleaved approaches on the same questions to measure structural impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific mechanisms by which current VLMs fail to disambiguate entities within multimodal tables, and how can these be improved?
- Basis in paper: Explicit - The paper highlights that current VLMs struggle with entity disambiguation within the context of multimodal tables, leading to incorrect interpretations of images and text.
- Why unresolved: While the paper identifies entity disambiguation as a key challenge, it does not delve into the specific mechanisms behind these failures or propose detailed solutions for improvement.
- What evidence would resolve it: Empirical studies comparing the performance of VLMs on tasks with varying levels of entity ambiguity, along with ablation studies to identify the specific components responsible for disambiguation failures, would provide valuable insights.

### Open Question 2
- Question: How does the performance of VLMs on multimodal table reasoning tasks compare to their performance on single-image or single-text tasks, and what are the key factors contributing to any observed differences?
- Basis in paper: Inferred - The paper suggests that multimodal table reasoning poses unique challenges compared to single-image or single-text tasks, but it does not provide a direct comparison of VLM performance across these different task types.
- Why unresolved: Understanding the relative performance of VLMs on multimodal vs. unimodal tasks is crucial for identifying the specific challenges posed by multimodal reasoning and developing targeted solutions.
- What evidence would resolve it: Benchmarking VLMs on a diverse set of multimodal and unimodal tasks, followed by a detailed analysis of performance differences and the factors contributing to them, would shed light on this question.

### Open Question 3
- Question: What are the most effective approaches for integrating visual and textual information in multimodal tables to facilitate reasoning, and how do these approaches compare in terms of accuracy and computational efficiency?
- Basis in paper: Explicit - The paper explores several baseline strategies for integrating visual and textual information in multimodal tables, including image captioning, table-as-image, and interleaved text-image approaches. However, it does not provide a comprehensive comparison of their effectiveness.
- Why unresolved: Identifying the most effective approach for integrating multimodal information is crucial for developing VLMs capable of robust reasoning on multimodal tables.
- What evidence would resolve it: Conducting extensive experiments comparing the performance of different integration approaches on a variety of multimodal table reasoning tasks, while also considering computational efficiency, would provide valuable insights.

### Open Question 4
- Question: How can VLMs be trained to effectively reason over multiple images within a table, and what are the key challenges and opportunities in this area?
- Basis in paper: Explicit - The paper acknowledges that current VLMs are optimized for reasoning over single images, and that reasoning over multiple images within a table presents significant challenges. However, it does not delve into the specific strategies or challenges involved in training VLMs for this task.
- Why unresolved: Developing VLMs capable of reasoning over multiple images is essential for advancing their capabilities in multimodal table understanding.
- What evidence would resolve it: Investigating different training strategies for VLMs on multimodal table datasets, along with analyzing the specific challenges encountered during training and inference, would provide valuable insights into this question.

### Open Question 5
- Question: What are the ethical considerations and potential biases associated with using multimodal tables in AI applications, and how can these be mitigated?
- Basis in paper: Inferred - The paper does not explicitly discuss ethical considerations or potential biases related to multimodal tables. However, given the increasing use of AI in various domains, it is crucial to consider the ethical implications of using multimodal data.
- Why unresolved: Understanding the ethical considerations and potential biases associated with multimodal tables is essential for ensuring the responsible development and deployment of AI systems.
- What evidence would resolve it: Conducting thorough ethical assessments of multimodal table datasets and AI applications, along with developing guidelines and best practices for mitigating potential biases, would be valuable steps towards addressing this question.

## Limitations

- Reliance on existing Wikipedia tables constrains dataset diversity and may introduce domain-specific biases
- 1.3% of original questions removed due to image unavailability raises concerns about selection effects
- Evaluation metrics (substring match, ROUGE-L) may not fully capture quality of multimodal reasoning
- Closed-source models remain black boxes, making it difficult to isolate performance drivers

## Confidence

**High Confidence**: The observation that current models struggle with multimodal table reasoning tasks is well-supported by experimental results across all tested models and question types.

**Medium Confidence**: The assertion that closed-source models outperform open-source models due to superior vision encoders and training techniques is plausible but requires further validation.

**Low Confidence**: The claim that visual questions are inherently harder than explicit questions may reflect current model limitations rather than fundamental task complexity.

## Next Checks

1. Run component ablation tests on GPT-4o and Gemini-1.5 Flash to isolate the contribution of vision encoders versus multimodal integration layers to performance differences.

2. Evaluate the same models on MMTABQA and MMGTABQA to determine whether performance patterns hold across different table sources and whether the Wikipedia-based approach introduces dataset-specific biases.

3. Conduct a controlled study with human participants answering MMTABQA questions to establish whether the difficulty hierarchy reflects human cognitive patterns or is specific to current AI limitations.