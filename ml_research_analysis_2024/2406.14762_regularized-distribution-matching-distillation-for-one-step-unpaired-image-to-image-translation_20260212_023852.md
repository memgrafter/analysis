---
ver: rpa2
title: Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image
  Translation
arxiv_id: '2406.14762'
source_url: https://arxiv.org/abs/2406.14762
tags:
- distribution
- diffusion
- transport
- arxiv
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Regularized Distribution Matching Distillation
  (RDMD), a one-step diffusion-based method for unpaired image-to-image (I2I) translation.
  RDMD modifies the DMD approach by replacing the generator's input noise with source
  data samples and regularizing the objective with a transport cost between input
  and output to maintain correspondence.
---

# Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

## Quick Facts
- arXiv ID: 2406.14762
- Source URL: https://arxiv.org/abs/2406.14762
- Authors: Denis Rakitin; Ivan Shchekotov; Dmitry Vetrov
- Reference count: 40
- Primary result: RDMD achieves better trade-offs between faithfulness and visual quality than multi-step diffusion baselines in unpaired I2I translation

## Executive Summary
This paper introduces Regularized Distribution Matching Distillation (RDMD), a one-step diffusion-based method for unpaired image-to-image translation. RDMD modifies the DMD approach by replacing the generator's input noise with source data samples and regularizing the objective with a transport cost between input and output to maintain correspondence. The method is theoretically analyzed and shown to converge to the optimal transport map as the regularization coefficient approaches zero. Experiments on 2D Gaussian translation and cat-to-wild animal translation demonstrate RDMD's ability to balance faithfulness and visual quality, achieving better trade-offs than multi-step diffusion baselines in terms of FID, L2, PSNR, and SSIM metrics when transport cost is constrained.

## Method Summary
RDMD achieves one-step unpaired image-to-image translation by replacing the generator's input noise with source data samples and adding transport cost regularization. The generator takes source images as input and transforms them into target images, while the transport cost ensures input-output correspondence is maintained by penalizing large changes in image features. The regularization coefficient λ controls the tradeoff between faithfulness to the input and quality of the output. As λ approaches zero, the objective becomes equivalent to the Monge optimal transport problem, encouraging the generator to preserve input features while fitting the target distribution. The method is theoretically analyzed and shown to converge to the optimal transport map under certain assumptions.

## Key Results
- RDMD achieves better FID vs transport cost trade-offs than multi-step diffusion baselines
- The method converges to optimal transport map as regularization coefficient approaches zero
- Experiments show balanced performance on 2D Gaussian translation and AFHQv2 cat-to-wild animal datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RDMD achieves one-step unpaired image-to-image translation by replacing the input noise with source data samples and regularizing the objective with a transport cost.
- Mechanism: The generator takes source images as input and transforms them into target images, while the transport cost ensures that input-output correspondence is maintained by penalizing large changes in image features.
- Core assumption: The source and target distributions have densities and are defined on bounded open subsets of Rd.
- Evidence anchors:
  - [abstract] "We replace the generator's input noise with source data samples and regularize the objective with a transport cost between input and output to maintain correspondence."
  - [section] "First, we note that the construction of DMD requires only having samples from the input distribution. Given this, we replace the Gaussian input pnoise by pS, the data distribution pdata by pT and aim at optimizing..."
- Break condition: If the source and target distributions do not have densities or are not defined on bounded subsets, the theoretical guarantees may not hold.

### Mechanism 2
- Claim: The regularization coefficient λ controls the tradeoff between faithfulness to the input and quality of the output.
- Mechanism: As λ approaches zero, the objective becomes equivalent to the Monge optimal transport problem, encouraging the generator to preserve input features while fitting the target distribution.
- Core assumption: The transport cost c(x, y) is quadratic ∥x − y∥2.
- Evidence anchors:
  - [abstract] "As the regularization coefficient approaches zero."
  - [section] "However, there are no guarantees that the input and the output will be related. Similarly to the OT problem (Equation 12), we fix the issue by penalizing the transport cost between them."
- Break condition: If the transport cost is not quadratic or the regularization coefficient is too large, the generator may not converge to the optimal transport map.

### Mechanism 3
- Claim: RDMD is theoretically analyzed and shown to converge to the optimal transport map as the regularization coefficient approaches zero.
- Mechanism: The proof relies on the lower semi-continuity of the loss with respect to weak convergence and the tightness of the sequence of generator-based plans.
- Core assumption: The weighting function ωt is positive and bounded, and the standard deviation σt of the noise is continuous in t.
- Evidence anchors:
  - [abstract] "The method is theoretically analyzed and shown to converge to the optimal transport map as the regularization coefficient approaches zero."
  - [section] "Theorem A.1. Let pS, pT , c , ωt , and σt satisfy the assumptions 1-3. Then, there exists a minimum Gα of the objective Lα..."
- Break condition: If the assumptions on ωt or σt are violated, the proof may not hold.

## Foundational Learning

- Concept: Optimal Transport
  - Why needed here: RDMD is based on the Monge optimal transport
  - Quick check: Understanding that OT finds the minimal cost mapping between probability distributions

- Concept: Diffusion Models
  - Why needed here: RDMD uses diffusion models as the generator architecture
  - Quick check: Knowing that diffusion models learn to denoise data through a Markov chain

- Concept: KL Divergence
  - Why needed here: Used as part of the RDMD objective to match distributions
  - Quick check: Understanding KL divergence measures the difference between two probability distributions

## Architecture Onboarding

### Component Map
Generator -> Transport Cost -> Objective Function

### Critical Path
Source images → Generator → Target images → Transport cost penalty → Optimized mapping

### Design Tradeoffs
- One-step vs multi-step translation: RDMD trades computation for potential quality loss
- Transport cost vs FID: λ regularization coefficient balances faithfulness vs visual quality
- Quadratic transport cost: Assumes squared difference is appropriate for all image features

### Failure Signatures
- Mode collapse: Generated images lack diversity compared to target distribution
- Poor correspondence: Large transport cost despite good FID scores
- Unstable training: Large λ values may cause training instability

### First Experiments
1. Train RDMD on 2D Gaussian translation to verify convergence to optimal transport map
2. Test RDMD on AFHQv2 cat-to-wild animal translation with varying λ values
3. Compare RDMD against EDM baseline on synthetic datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of transport cost function c(x,y) beyond squared difference norm affect the performance of RDMD in unpaired image-to-image translation?
- Basis in paper: [explicit] The paper uses squared difference norm ∥x-y∥² as the transport cost but acknowledges that other distances (e.g., LPIPS embeddings) could be chosen.
- Why unresolved: The paper only experimentally validates with one cost function, leaving theoretical and empirical understanding of alternative choices incomplete.
- What evidence would resolve it: Comparative experiments testing different cost functions (LPIPS, perceptual metrics) across multiple datasets, measuring both translation quality and input-output correspondence preservation.

### Open Question 2
- Question: What is the theoretical relationship between the regularization coefficient λ and the approximation error of the optimal transport map as λ→0?
- Basis in paper: [explicit] Theorem 3.1 proves convergence to the optimal transport map as λ→0, but provides no quantitative bounds on the rate of convergence.
- Why unresolved: The proof establishes asymptotic convergence but doesn't characterize how quickly Gλ approaches G* for finite λ, which is crucial for practical hyperparameter selection.
- What evidence would resolve it: Derivation of explicit convergence rates (e.g., O(λ^α) bounds) through non-asymptotic analysis of the RDMD objective, validated on synthetic and real datasets.

### Open Question 3
- Question: How does RDMD performance scale to higher-resolution image translation tasks compared to diffusion-based baselines?
- Basis in paper: [inferred] Experiments are limited to 64×64 resolution (MNIST, Cat→Wild), while diffusion models typically achieve better results at higher resolutions (256×256+).
- Why unresolved: The paper demonstrates RDMD's effectiveness at small scales but doesn't address computational and quality challenges that arise with increased dimensionality.
- What evidence would resolve it: Direct comparison of RDMD against state-of-the-art diffusion methods (ILVR, SDEdit, EGSDE) on high-resolution datasets (e.g., 256×256 or 512×512) with identical computational budgets.

## Limitations
- Theoretical analysis assumes bounded domains and specific conditions that may not hold in practice
- Experimental validation limited to 2D synthetic data and one real dataset (AFHQv2)
- No comparison to non-diffusion one-step baselines like CUT or MUNIT

## Confidence
- Mechanism 1 (Input replacement and transport regularization): High
- Mechanism 2 (λ controlling faithfulness-quality tradeoff): Medium
- Mechanism 3 (Theoretical convergence): Low

## Next Checks
1. Test RDMD on additional domain pairs (e.g., horse-to-zebra, Monet-to-photo) to verify generalization beyond the AFHQv2 cat-to-wild animal setup
2. Conduct ablation studies varying the transport cost function (not just quadratic) to assess sensitivity to this design choice
3. Compare RDMD against non-diffusion one-step methods like CUT or MUNIT to establish relative performance in the one-step translation setting