---
ver: rpa2
title: 'Transformer tricks: Precomputing the first layer'
arxiv_id: '2402.13388'
source_url: https://arxiv.org/abs/2402.13388
tags:
- layer
- size
- precompute
- memory
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method to speed up transformer inference by
  precomputing the first layer, resulting in lower latency and cost-per-token. The
  technique is applicable to transformers with Rotary Position Embeddings (RoPE),
  such as LLaMA, Mistral, PaLM, and Gemma.
---

# Transformer tricks: Precomputing the first layer

## Quick Facts
- **arXiv ID**: 2402.13388
- **Source URL**: https://arxiv.org/abs/2402.13388
- **Reference count**: 19
- **Primary result**: Precomputing first layer of transformers with RoPE reduces inference latency and cost-per-token

## Executive Summary
This paper presents a method to accelerate transformer inference by precomputing the first layer's computations. The technique exploits the fact that for transformers using Rotary Position Embeddings (RoPE), such as LLaMA, Mistral, PaLM, and Gemma, a significant portion of the first layer's computation can be precomputed and stored, reducing per-token computational complexity. The speedup depends on the total number of layers, with maximum benefits for smaller models (up to 25% for 4-layer models) and diminishing returns for larger models (3% for 32 layers).

## Method Summary
The method involves precomputing and storing values that would normally be computed during each inference pass for the first transformer layer. Instead of storing the original input embeddings, the precomputed values are stored in memory. This approach reduces the number of memory reads required during the autoregressive next-token-prediction phase, particularly for low batch sizes. The technique is applicable to transformers with Rotary Position Embeddings (RoPE) and involves a trade-off between memory size and computational efficiency.

## Key Results
- Precomputing first layer reduces computational complexity per token
- Speedup ranges from 25% for 4-layer models to 3% for 32-layer models
- Mistral-7B memory size increases by only 2% with this technique
- Memory overhead varies with vocabulary size and eliminated weights

## Why This Works (Mechanism)
The optimization works by exploiting the structure of transformers with Rotary Position Embeddings (RoPE). In the first layer, the position embeddings are combined with the input embeddings in a way that allows much of this computation to be precomputed and stored. During inference, instead of recomputing these values for each token, the system can simply retrieve the precomputed results, reducing the computational load. This is particularly effective for the autoregressive next-token-prediction phase where the input remains largely unchanged between predictions.

## Foundational Learning
- **Rotary Position Embeddings (RoPE)**: A method for encoding positional information in transformers by rotating the key and query vectors based on their position. Why needed: RoPE enables the precomputation optimization by structuring the first layer's computation in a predictable way. Quick check: Verify that the model uses RoPE by checking for rotary embeddings in the attention mechanism.
- **Autoregressive inference**: The process of generating text token by token, where each new token depends on all previous tokens. Why needed: The optimization is most beneficial during this phase due to repeated computations. Quick check: Confirm the model supports autoregressive generation.
- **Memory bandwidth vs. compute**: The trade-off between storing precomputed values (increasing memory usage) versus computing them on-the-fly (increasing computation). Why needed: Understanding this balance is crucial for evaluating the technique's effectiveness. Quick check: Measure memory read/write patterns during inference.
- **Transformer layer structure**: Understanding how attention and feed-forward networks are organized in layers. Why needed: The optimization specifically targets the first layer's structure. Quick check: Examine the layer architecture to identify precomputable components.
- **Batch size effects**: How different batch sizes impact memory access patterns and computational efficiency. Why needed: The optimization's effectiveness varies with batch size. Quick check: Test with varying batch sizes to observe performance changes.
- **Context window length**: The number of previous tokens considered during inference. Why needed: Longer context windows may affect the precomputation strategy. Quick check: Verify the model's maximum context window size.

## Architecture Onboarding
**Component map**: Input embeddings -> Precomputation module -> Stored values -> Transformer layers -> Output
**Critical path**: Input -> Rotary embedding computation -> Attention QKV projection -> Feed-forward projection -> Precomputed storage
**Design tradeoffs**: Memory vs. computation (storing precomputed values increases memory but reduces computation), latency vs. throughput (beneficial for low batch sizes), model size vs. optimization gains (diminishing returns for deeper models)
**Failure signatures**: Incorrect handling of position embeddings leading to generation errors, memory overflow due to increased storage requirements, cache misses negating computational savings
**First experiments**: 1) Measure inference latency with and without precomputation for a 4-layer model, 2) Compare memory usage before and after applying the optimization, 3) Test autoregressive generation quality with precomputed values

## Open Questions the Paper Calls Out
None

## Limitations
- Diminishing returns for deeper models (only 3% savings for 32-layer models)
- Limited empirical validation on actual hardware (theoretical analysis-based)
- Unclear handling of position embeddings in the precomputation process
- Assumes 8k context window that may not generalize to all use cases

## Confidence
- **High confidence**: The core mathematical observation that precomputing first-layer weights is valid
- **Medium confidence**: The reported speedup percentages and memory overhead figures for the specific setup
- **Medium confidence**: The claim about usefulness for "smart routing" between models lacks empirical validation

## Next Checks
1. Test the precomputation technique across a wider range of model depths to map out the diminishing returns curve more precisely.
2. Evaluate the technique with varying batch sizes and context lengths to understand how memory access patterns affect practical speedup.
3. Implement the optimization on actual hardware to measure real-world latency improvements and memory bandwidth impact, including testing with position embeddings.