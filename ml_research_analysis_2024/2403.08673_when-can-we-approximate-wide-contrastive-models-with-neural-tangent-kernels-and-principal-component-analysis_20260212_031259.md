---
ver: rpa2
title: When can we Approximate Wide Contrastive Models with Neural Tangent Kernels
  and Principal Component Analysis?
arxiv_id: '2403.08673'
source_url: https://arxiv.org/abs/2403.08673
tags:
- contrastive
- learning
- neural
- similarity
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies when contrastive learning models can be approximated
  by kernel methods or PCA. The authors analyze the training dynamics of two-layer
  neural networks under contrastive losses, showing that for cosine-similarity based
  losses (like SimCLR/InfoNCE), the Neural Tangent Kernel (NTK) remains nearly constant
  for O(M^{1/6}) steps, enabling approximation by fixed-kernel methods.
---

# When can we Approximate Wide Contrastive Models with Neural Tangent Kernels and Principal Component Analysis?

## Quick Facts
- arXiv ID: 2403.08673
- Source URL: https://arxiv.org/abs/2403.08673
- Reference count: 40
- Primary result: Shows when wide contrastive learning models can be approximated by NTK or PCA, depending on similarity measure and orthogonality constraints

## Executive Summary
This paper analyzes when contrastive learning models can be approximated by kernel methods or PCA by studying the training dynamics of two-layer neural networks. The authors prove that cosine-similarity based contrastive losses lead to nearly constant Neural Tangent Kernels for O(M^(1/6)) steps, enabling kernel method approximations, while dot-product losses cause rapid NTK divergence. They also show that with orthogonality constraints on the output layer, contrastive models approximate PCA of a matrix derived from random features. Empirical results on MNIST validate these findings, demonstrating that representations from trained models closely match those from PCA, especially as width increases.

## Method Summary
The authors analyze two-layer neural networks trained under contrastive losses, focusing on the constancy of the Neural Tangent Kernel and its connection to PCA. They prove bounds on NTK evolution for different similarity measures, show that orthogonality constraints link contrastive models to PCA solutions, and validate their theoretical findings through experiments on MNIST using networks with varying widths.

## Key Results
- NTK remains nearly constant for O(M^(1/6)) steps under cosine similarity contrastive losses
- Orthogonality-constrained contrastive models approximate PCA of a fixed matrix derived from random features
- Dot-product similarity causes NTK to change drastically within O(log M) steps
- Empirical results on MNIST show representations from trained models closely match PCA, especially with larger widths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cosine-similarity contrastive losses lead to nearly constant NTK for O(M^(1/6)) steps.
- Mechanism: Normalization in cosine similarity bounds the magnitude of weight updates, preventing the NTK from drifting too far from its initialization.
- Core assumption: δ > 0 (small positive constant in cosine similarity) and bounded gradients.
- Evidence anchors:
  - [abstract] "NTK of wide networks remains almost constant for cosine similarity based contrastive losses"
  - [section 3.2] Theorem 13 shows |K_ij(x, y; θ(t)) - K_ij(x, y; θ(0))| = O(M^(-1/6)(log M)^3)
  - [corpus] Weak/negligible: no direct mention of cosine-similarity + NTK constancy
- Break condition: If δ → 0 or loss normalization is removed, weight changes grow unbounded (as shown in dot product case).

### Mechanism 2
- Claim: Orthogonality-constrained contrastive models approximate PCA of a fixed matrix C(t) that stays close to C(0).
- Mechanism: With W⊤W = I, the training dynamics keep the matrix CV(t) close to CV(0) in Frobenius norm, so PCA of CV(t) is close to PCA of CV(0).
- Core assumption: Orthogonality constraint on output layer; learning rate small enough to maintain matrix proximity.
- Evidence anchors:
  - [abstract] "representations learned by contrastive models are close to the principal components of a certain matrix computed from random features"
  - [section 4.1] Lemma 16 proves ∥CV(t) - CV(0)∥_F ≤ κ t / √M
  - [corpus] Weak/negligible: no explicit PCA + orthogonality connection
- Break condition: If orthogonality is removed or learning rate too large, CV(t) diverges from CV(0).

### Mechanism 3
- Claim: Wide neural networks trained under contrastive losses evolve as linear models in the NTK regime, enabling closed-form analysis.
- Mechanism: The constancy of NTK reduces training dynamics to kernel regression, and the bound on Hessian norms ensures the kernel stays fixed.
- Core assumption: Infinite width limit; smoothness of activation; bounded inputs/gradients.
- Evidence anchors:
  - [section 2.3] Lemmas 7 & 8 bound Hessian spectral norm and NTK change in terms of weight changes
  - [section 3] Proposition 10 & Theorem 13 show when NTK stays constant vs. diverges
  - [corpus] Weak/negligible: general NTK theory but not specific to contrastive losses
- Break condition: If width not large enough or loss not in NTK-friendly form (e.g., dot product similarity), NTK changes drastically.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its constancy under gradient flow.
  - Why needed here: The paper's main results hinge on understanding when the NTK remains constant, enabling kernel method approximations.
  - Quick check question: What condition on the loss similarity measure (dot product vs. cosine) determines NTK constancy for wide networks?

- Concept: Principal Component Analysis (PCA) and its connection to contrastive learning.
  - Why needed here: The paper shows contrastive models with orthogonality constraints effectively perform PCA on a learned matrix.
  - Quick check question: How does constraining W⊤W = I in a two-layer network relate the contrastive loss to a trace maximization problem equivalent to PCA?

- Concept: Hessian spectral norm bounds and their role in NTK analysis.
  - Why needed here: The constancy of NTK is proven by bounding the change in Hessian norm, which controls how much the kernel can drift.
  - Quick check question: Why does a bound of the form ∥H^(z)(x; θ(t))∥_2 ≤ α₁R + α₂/√M imply NTK remains nearly constant for small R?

## Architecture Onboarding

- Component map: Data preprocessing -> Two-layer network f(x) = (1/√M)W⊤ϕ(Vx) -> Contrastive loss L(D) -> Gradient descent/Grassmannian GD -> Monitor NTK/CV(t) -> Compare to PCA/kernel method
- Critical path:
  1. Initialize W, V with Gaussian N(0,1) (NTK parametrization)
  2. Choose similarity: dot product or cosine (with δ > 0)
  3. Train with contrastive loss (linear or softmax)
  4. Track NTK evolution: if cosine, expect constancy for O(M^(1/6)) steps
  5. If orthogonality on W, track CV(t); expect CV(t) ≈ CV(0) in Frobenius norm
  6. Compare learned representations to PCA of CV(0)
- Design tradeoffs:
  - Dot product similarity: Simpler, but NTK diverges quickly (O(log M) steps), so no kernel approximation
  - Cosine similarity: Normalization stabilizes NTK but adds δ parameter; requires orthogonality constraint for PCA connection
  - Orthogonality constraint: Ensures dimension collapse is avoided and links to PCA, but is not used in practice
- Failure signatures:
  - NTK changes rapidly even for large M → likely dot product similarity or too large learning rate
  - CV(t) drifts far from CV(0) → orthogonality constraint violated or η too large
  - Representations collapse to few dimensions → initialization issue (e.g., W rows not diverse)
- First 3 experiments:
  1. Train a two-layer network (ReLU) with dot product similarity on MNIST; plot max NTK entry change vs. M and time. Expect divergence.
  2. Repeat with cosine similarity (δ=0.01); expect NTK change ~O(1/√M) for fixed time.
  3. Add orthogonality constraint on W; track ∥CV(t)-CV(0)∥_F; compare final W to PCA of CV(0).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does the NTK remain constant throughout the entire training process for contrastive learning models?
- Basis in paper: The paper shows NTK remains constant for O(M^1/6) steps under cosine similarity, but leaves open whether this extends to convergence.
- Why unresolved: The authors conjecture constancy may hold to convergence but provide no proof, noting that stationary points and their timing need investigation.
- What evidence would resolve it: A proof showing the NTK remains constant for all training steps, or a counterexample demonstrating divergence at some point.

### Open Question 2
- Question: What is the exact relationship between fully trained contrastive models and PCA solutions, beyond the initial training phase?
- Basis in paper: Lemma 17 shows closeness of PCA solutions at early training, but Figure 4 shows W and CV evolve simultaneously at different rates.
- Why unresolved: The paper notes that while initial solutions are close, the simultaneous evolution of W and CV makes the connection to final PCA solutions unclear.
- What evidence would resolve it: A spectral analysis showing that the final trained weights converge to the PCA solution of a fixed matrix.

### Open Question 3
- Question: How do initialization schemes affect dimension collapse in contrastive learning models?
- Basis in paper: Proposition 18 shows that if two columns of W are initialized identically, dimension collapse occurs regardless of width or similarity measure.
- Why unresolved: While the proposition identifies a sufficient condition for collapse, it doesn't characterize all problematic initializations or provide guidance on avoiding collapse.
- What evidence would resolve it: A characterization of all initialization patterns that lead to dimension collapse, or a proof that certain random initialization schemes prevent collapse with high probability.

## Limitations

- Theoretical analysis relies heavily on infinite-width approximations that may not capture all practical scenarios
- Orthogonality constraint, while providing clean PCA connection, is not used in practical contrastive learning implementations
- Empirical validation limited to MNIST with specific network architectures and data augmentation schemes

## Confidence

- **High Confidence**: The distinction between dot product and cosine similarity losses in terms of NTK evolution (Theorem 13) - this follows directly from the mathematical analysis with clear conditions.
- **Medium Confidence**: The O(M^(1/6)) bound for NTK constancy under cosine similarity - while theoretically sound, the practical implications depend on constants and higher-order terms not fully characterized.
- **Low Confidence**: The empirical demonstration that representations match PCA - the MNIST experiments show qualitative agreement but lack comprehensive quantitative comparison across multiple metrics and datasets.

## Next Checks

1. **Scale Validation**: Reproduce the NTK constancy experiments across multiple orders of magnitude in network width (M = 100, 1000, 10000) to empirically verify the O(M^(1/6)) scaling relationship.

2. **Loss Function Variants**: Test the NTK constancy bounds with alternative contrastive losses (SupCon, AMDIM variants) and normalization schemes to identify which components are essential versus incidental.

3. **Cross-Domain Generalization**: Apply the PCA approximation analysis to CIFAR-10 and ImageNet subsets to evaluate whether the theoretical bounds hold for more complex, higher-dimensional data distributions.