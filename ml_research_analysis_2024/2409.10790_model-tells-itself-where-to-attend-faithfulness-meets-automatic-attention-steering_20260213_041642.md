---
ver: rpa2
title: 'Model Tells Itself Where to Attend: Faithfulness Meets Automatic Attention
  Steering'
arxiv_id: '2409.10790'
source_url: https://arxiv.org/abs/2409.10790
tags:
- autopasta
- prompting
- attention
- heads
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AutoPASTA, a method that improves the faithfulness
  of large language models (LLMs) in open-book question answering tasks. AutoPASTA
  automatically identifies key contextual information and explicitly highlights it
  by steering the model's attention scores.
---

# Model Tells Itself Where to Attend: Faithfulness Meets Automatic Attention Steering

## Quick Facts
- arXiv ID: 2409.10790
- Source URL: https://arxiv.org/abs/2409.10790
- Authors: Qingru Zhang; Xiaodong Yu; Chandan Singh; Xiaodong Liu; Liyuan Liu; Jianfeng Gao; Tuo Zhao; Dan Roth; Hao Cheng
- Reference count: 9
- One-line primary result: AutoPASTA improves faithfulness of LLMs in open-book QA, achieving 7.95% average improvement for LLAMA3-70B-Instruct

## Executive Summary
This paper introduces AutoPASTA, a method that improves the faithfulness of large language models (LLMs) in open-book question answering tasks. AutoPASTA automatically identifies key contextual information and explicitly highlights it by steering the model's attention scores. The method combines iterative prompting with attention steering, first prompting the model to identify key sentences, then mapping these sentences back to the original context using semantic embeddings, and finally highlighting them through attention score manipulation. Experiments on the Natural Questions and HotpotQA datasets show that AutoPASTA significantly improves model faithfulness and performance, achieving an average improvement of 7.95% for LLAMA3-70B-Instruct.

## Method Summary
AutoPASTA improves LLM faithfulness in open-book QA by automatically identifying key contextual information and explicitly highlighting it through attention score manipulation. The method uses iterative prompting to identify key sentences, maps these sentences back to original context using semantic embeddings, then steers attention scores at selected heads. A coarse-to-fine profiling approach reduces computational overhead by first evaluating all heads in top layers, selecting top layers, then evaluating individual heads within those layers. The method does not require changing model parameters and can generalize across different tasks.

## Key Results
- AutoPASTA achieves 7.95% average improvement in faithfulness for LLAMA3-70B-Instruct on NQ and HotpotQA datasets
- The method outperforms both direct prompting and iterative prompting baselines in faithfulness metrics
- AutoPASTA demonstrates effectiveness across multiple model sizes (Vicuna-7B, LLAMA3-8B-Instruct, LLAMA3-70B-Instruct)
- Coarse-to-fine search strategy substantially reduces computational overhead while maintaining effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention steering improves faithfulness by explicitly directing model focus to key contextual information
- Mechanism: AutoPASTA identifies key sentences through iterative prompting, maps them back to original context using semantic embeddings, then upweights attention scores for tokens in those sentences at selected attention heads
- Core assumption: Models can better utilize contextual information when attention scores are explicitly manipulated rather than relying on implicit highlighting through token appending
- Evidence anchors: [abstract] "explicitly highlights it by steering an LLM's attention scores"
- Break condition: If attention head selection is poor or if semantic embedding matching fails to correctly identify key sentences

### Mechanism 2
- Claim: Coarse-to-fine model profiling reduces computational overhead while maintaining effectiveness
- Mechanism: Instead of evaluating all L×H attention heads, first evaluate all heads in top-l layers, select top layers, then evaluate individual heads within those layers
- Core assumption: Effective attention heads for steering are concentrated in a subset of layers rather than distributed evenly
- Evidence anchors: [section] "we propose an alternative coarse-to-fine model profiling scheme"
- Break condition: If effective attention heads are distributed across many layers or if top layers don't contain steering-relevant heads

### Mechanism 3
- Claim: Iterative prompting with semantic mapping mitigates error propagation compared to direct token appending
- Mechanism: Instead of appending predicted key sentences to context, map predicted sentences back to original context using semantic embeddings before highlighting
- Core assumption: Token-level generation errors in predicted key sentences can be avoided by mapping back to original context sentences
- Evidence anchors: [section] "Instead of appending those key sentences to the initial prompt, AutoPASTA maps those sentences back to the original context using semantic embeddings"
- Break condition: If semantic embedding matching frequently selects wrong sentences or if mapping introduces its own errors

## Foundational Learning

- Concept: Multi-head attention in transformers
  - Why needed here: Understanding how attention heads process information is critical for knowing which heads to steer and how attention manipulation affects model behavior
  - Quick check question: How does multi-head attention allow transformers to focus on different aspects of the input simultaneously?

- Concept: Semantic similarity and embedding matching
  - Why needed here: AutoPASTA uses semantic embeddings to map predicted key sentences back to original context sentences - understanding this process is essential for implementing and debugging the method
  - Quick check question: How would you compute semantic similarity between two sentences using embedding representations?

- Concept: Prompt engineering and iterative prompting
  - Why needed here: AutoPASTA builds on iterative prompting techniques - understanding how to design prompts that elicit key information is crucial for the method
  - Quick check question: What makes a prompt effective at eliciting specific types of information from an LLM?

## Architecture Onboarding

- Component map: Question → Key sentence identification → Semantic mapping → Attention score manipulation → Answer generation
- Critical path: The iterative process flows from question through key sentence identification using iterative prompting, semantic mapping via sentence encoder, attention score manipulation at selected heads, to final answer generation
- Design tradeoffs: Explicit attention steering vs implicit highlighting through token appending; computational cost of coarse-to-fine profiling vs greedy search; accuracy of semantic mapping vs potential error propagation
- Failure signatures: Performance degradation when steering too many heads; poor semantic matching leading to wrong sentence highlighting; error propagation if key sentence identification fails
- First 3 experiments:
  1. Test semantic mapping accuracy by comparing predicted vs actual key sentence matches on a validation set
  2. Profile attention heads using coarse-to-fine approach on a small dataset to identify effective heads
  3. Compare AutoPASTA performance against direct prompting baseline on a single dataset with one model size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AutoPASTA scale with model size beyond the tested 70B parameter models?
- Basis in paper: [inferred] The paper demonstrates significant improvements for LLAMA3-70B-Instruct but does not test larger models or explore scaling trends
- Why unresolved: The study only tested up to 70B parameter models, leaving open whether larger models would show similar or different performance gains
- What evidence would resolve it: Empirical testing of AutoPASTA on models larger than 70B parameters (e.g., 175B+ parameter models) across various QA tasks

### Open Question 2
- Question: Can AutoPASTA be effectively adapted for multi-turn conversational QA tasks where context relevance may shift across turns?
- Basis in paper: [explicit] The paper focuses on single-turn open-book QA tasks and does not explore multi-turn dialogue scenarios
- Why unresolved: The method is evaluated only on static single-turn contexts, leaving unclear how well it handles dynamic context changes in conversation
- What evidence would resolve it: Application and evaluation of AutoPASTA on multi-turn conversational QA datasets with changing context relevance

### Open Question 3
- Question: What is the computational overhead of AutoPASTA compared to standard inference, and how does this scale with context length?
- Basis in paper: [inferred] While the paper mentions the method is applied at inference time, it does not provide detailed timing or computational cost analysis
- Why unresolved: The paper does not report inference time comparisons or scaling behavior with longer contexts
- What evidence would resolve it: Comprehensive timing benchmarks comparing AutoPASTA inference time to standard inference across varying context lengths and model sizes

## Limitations

- Semantic embedding matching accuracy is not validated, leaving uncertainty about how often the method correctly maps predicted key sentences to original context
- Coarse-to-fine attention head selection may miss effective heads if they're distributed across layers rather than concentrated in top layers
- Generalization claims across tasks are weakly supported as only open-book QA tasks were tested

## Confidence

**High Confidence**: The core claim that attention steering can improve model faithfulness in open-book QA tasks is well-supported by experimental results showing consistent improvements across multiple model sizes and datasets.

**Medium Confidence**: The claim that AutoPASTA outperforms iterative prompting baselines in faithfulness metrics is supported by experimental results, but improvement magnitude may vary significantly depending on task complexity and model size.

**Low Confidence**: The generalization claim across different tasks without parameter changes is weakly supported, as only open-book QA tasks were tested. The error propagation mitigation through semantic mapping lacks direct evidence comparing it to token appending approaches.

## Next Checks

1. **Semantic Mapping Accuracy Validation**: Implement systematic evaluation of semantic embedding matching accuracy by creating a validation set where ground truth key sentences are known. Measure precision, recall, and false positive rates of the mapping process.

2. **Attention Head Selection Robustness**: Conduct ablation studies testing the coarse-to-fine profiling approach against full search baseline on a subset of data. Vary the top-l layer selection parameter and measure sensitivity to this choice.

3. **Error Propagation Comparison**: Design controlled experiment comparing AutoPASTA's semantic mapping approach against direct token appending for iterative prompting. Measure not just final performance but also intermediate error rates at each iteration.