---
ver: rpa2
title: Always-Sparse Training by Growing Connections with Guided Stochastic Exploration
arxiv_id: '2401.06898'
source_url: https://arxiv.org/abs/2401.06898
tags:
- training
- connections
- accuracy
- sparse
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a dynamic sparse training method called Guided
  Stochastic Exploration (GSE) for efficient training of artificial neural networks.
  GSE addresses the computational limitations of dense models by maintaining sparsity
  during training, enabling faster and more memory-efficient training while achieving
  high accuracy.
---

# Always-Sparse Training by Growing Connections with Guided Stochastic Exploration

## Quick Facts
- arXiv ID: 2401.06898
- Source URL: https://arxiv.org/abs/2401.06898
- Authors: Mike Heddes; Narayan Srinivasa; Tony Givargis; Alexandru Nicolau
- Reference count: 40
- Primary result: GSE achieves state-of-the-art accuracy on CIFAR-10/100 and ImageNet while using fewer FLOPs than existing sparse training methods

## Executive Summary
This paper proposes Guided Stochastic Exploration (GSE), a dynamic sparse training method that maintains sparsity throughout neural network training while achieving high accuracy. GSE addresses the computational limitations of dense models by sampling a subset of inactive connections using efficient probability distributions and growing connections with the largest gradient magnitudes. The method demonstrates superior performance compared to existing sparse training approaches, particularly at high sparsity levels, while maintaining linear time complexity with respect to model width.

## Method Summary
GSE maintains sparsity during training by dynamically growing and pruning connections based on gradient information. The method samples a subset of inactive connections using uniform probability distributions, computes gradients only for this subset, and grows the connections with the largest gradient magnitudes while pruning the smallest weight magnitudes. This hybrid approach combines random exploration with greedy selection, achieving better accuracy than pure SET-like or RigL-like methods. GSE uses Erdős–Rényi initialization and cosine annealing for pruning, with SGD optimization and L2 regularization.

## Key Results
- GSE outperforms existing sparse training methods on CIFAR-10/100 and ImageNet datasets
- Achieves state-of-the-art accuracy while using fewer floating-point operations, especially at high sparsity levels (98%+)
- Enables training larger and sparser models with improved performance compared to prior methods
- Uniform sampling consistently achieves high accuracy while being the most computationally efficient approach

## Why This Works (Mechanism)

### Mechanism 1
GSE achieves higher accuracy than prior sparse training methods by sampling a subset of inactive connections and selecting those with the largest gradient magnitudes. This hybrid exploration strategy balances random exploration (like SET) with greedy selection (like RigL). The core assumption is that gradient magnitude indicates which connections will contribute most to reducing loss when added to the active set.

### Mechanism 2
GSE maintains linear time complexity with respect to model width while achieving state-of-the-art accuracy by sampling only a subset of inactive connections rather than computing all possible gradients. The subset size is kept proportional to active connections, avoiding the dense gradient computation bottleneck while still exploring effectively.

### Mechanism 3
The uniform probability distribution for subset sampling performs as well as or better than biased distributions that use gradient information. Uniform sampling provides sufficient exploration without requiring additional computation to calculate gradient-based probabilities, making it more efficient while maintaining accuracy.

## Foundational Learning

- Concept: Sparse matrix representations and their computational advantages
  - Why needed here: Understanding how sparse matrices reduce memory and computational requirements is fundamental to appreciating why always-sparse training is beneficial
  - Quick check question: What is the computational complexity difference between dense and sparse matrix multiplication when the sparsity level is 95%?

- Concept: Gradient-based optimization and backpropagation
  - Why needed here: The method relies on computing gradients to determine which connections to grow, so understanding how gradients are calculated and used in neural network training is essential
  - Quick check question: How does the gradient magnitude relate to the importance of a connection in a neural network?

- Concept: Random graph theory and the Erdős–Rényi model
  - Why needed here: The method uses Erdős–Rényi random graph initialization for sparse networks, so understanding random graph generation is important for implementation
  - Quick check question: What property of the Erdős–Rényi model ensures that the number of active connections is proportional to layer width?

## Architecture Onboarding

- Component map:
  - Subset sampling module -> Gradient computation module -> Connection selection module -> Pruning module -> Weight update module

- Critical path:
  1. Forward pass with sparse weights
  2. Backward pass to compute gradients
  3. Subset sampling from inactive connections
  4. Gradient magnitude computation for subset
  5. Top-k selection for growing new connections
  6. Bottom-k selection for pruning old connections
  7. Weight updates using SGD

- Design tradeoffs:
  - Subset sampling factor γ vs. exploration quality vs. computational efficiency
  - Probability distribution choice (uniform vs. gradient-based) vs. accuracy vs. computational overhead
  - Global vs. layer-wise sparsity management vs. accuracy vs. implementation complexity
  - Static vs. dynamic sparsity vs. accuracy vs. training flexibility

- Failure signatures:
  - Accuracy plateaus or decreases when γ is too small (insufficient exploration)
  - Training instability or divergence when pruning rate is too aggressive
  - Performance degradation when subset sampling distribution is poorly chosen
  - Layer collapse in extreme sparsity regimes if weight regularization is insufficient

- First 3 experiments:
  1. Implement GSE with uniform sampling (γ = 1) on CIFAR-10 with ResNet-56 at 98% sparsity, compare against SET baseline
  2. Vary the subset sampling factor γ (0.5, 1, 2) to find the optimal balance between exploration and efficiency
  3. Test different probability distributions (uniform, GraBo, GraEst) on CIFAR-100 to verify the uniform distribution's effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
What specific training dynamics are enabled by uniform sampling of connections in guided stochastic exploration (GSE) that lead to improved accuracy compared to biased distributions like GraBo and GraEst? The authors observe the empirical superiority of uniform sampling but do not provide theoretical justification for why unbiased exploration outperforms biased approaches.

### Open Question 2
How does the performance of GSE scale when applied to larger vision transformer architectures (e.g., ViT models with 12+ layers) on large-scale datasets like ImageNet-1K or JFT-300M? The paper only evaluates GSE on small ViT variants and does not explore how the method performs on larger, more complex transformer architectures.

### Open Question 3
What is the theoretical explanation for why GSE's FLOPs improvement over RigL increases dramatically at extreme sparsity levels (e.g., 99% sparsity)? The paper presents empirical FLOPs measurements but lacks theoretical analysis of why the efficiency gap widens non-linearly with increasing sparsity.

## Limitations
- Claims about GSE's superiority are primarily supported by comparisons with other sparse training methods rather than dense baselines
- The method's effectiveness at very high sparsity levels (>98%) remains uncertain
- Scalability to extremely large models or different network architectures beyond those tested needs further validation

## Confidence

- **High confidence**: The computational complexity claims (O(n + N)) and the basic algorithmic framework of GSE are well-supported by the mathematical formulation and implementation details.
- **Medium confidence**: The empirical results showing GSE outperforming other sparse methods at moderate sparsity levels (90-95%) are convincing, but the relative advantage at extreme sparsity levels is less certain.
- **Medium confidence**: The claim that uniform sampling performs as well as gradient-biased sampling is supported by ablation studies, but the underlying reasons for this finding need further investigation.

## Next Checks

1. **Performance at extreme sparsity**: Validate GSE's accuracy retention at 99%+ sparsity levels on ImageNet with modern architectures (ConvNeXt, EfficientNet) to assess practical limits.

2. **Dense baseline comparison**: Conduct head-to-head comparisons between GSE-trained sparse models and dense models with equivalent FLOPs to quantify the actual efficiency gains.

3. **Generalization across domains**: Test GSE on non-vision tasks (NLP, speech) and with different weight initialization schemes to evaluate robustness beyond the presented experimental setup.