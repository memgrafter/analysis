---
ver: rpa2
title: 'Attention Score is not All You Need for Token Importance Indicator in KV Cache
  Reduction: Value Also Matters'
arxiv_id: '2406.12335'
source_url: https://arxiv.org/abs/2406.12335
tags:
- attention
- token
- value
- tokens
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates token pruning for KV cache reduction in
  large language models, focusing on the role of value vectors in token importance
  evaluation. While existing methods rely solely on attention scores, the authors
  observe that value vector norms exhibit a highly non-uniform distribution across
  tokens, particularly for attention sink tokens which have high attention scores
  but low value vector norms.
---

# Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters

## Quick Facts
- arXiv ID: 2406.12335
- Source URL: https://arxiv.org/abs/2406.12335
- Authors: Zhiyu Guo; Hidetaka Kamigaito; Taro Watanabe
- Reference count: 28
- Primary result: VATP outperforms attention-only baselines on 12+ of 16 LongBench tasks

## Executive Summary
This paper investigates token pruning for KV cache reduction in large language models, focusing on the role of value vectors in token importance evaluation. While existing methods rely solely on attention scores, the authors observe that value vector norms exhibit a highly non-uniform distribution across tokens, particularly for attention sink tokens which have high attention scores but low value vector norms. Building on this observation, they propose Value-Aware Token Pruning (VATP), a method that evaluates token importance using both attention scores and ℓ1 norms of value vectors.

Experiments on LLaMA2-7B-chat and Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms attention-only baselines (StreamLLM, H2O, Scissorhands) in over 12 tasks, confirming the effectiveness of incorporating value vector norms into token importance evaluation for KV cache reduction.

## Method Summary
The paper proposes Value-Aware Token Pruning (VATP), a token pruning metric that assesses KV cache importance using the product of attention scores and ℓ1 norms of value vectors. The method keeps the first F tokens (attention sinks) regardless of their computed importance scores. VATP is evaluated on LLaMA2-7B-chat and Vicuna-v1.5-7B-16K models using 16 English tasks from LongBench, with a KV cache budget of 50% of input prompt length. The implementation builds on existing token pruning approaches while adding value vector norm consideration.

## Key Results
- VATP outperforms attention-only baselines (StreamLLM, H2O, Scissorhands) on 12+ of 16 LongBench tasks
- Performance gains are consistent across diverse task types including reasoning, summarization, and multi-hop QA
- Value vector norm distributions show non-uniform patterns, particularly for attention sink tokens
- VATP maintains task performance while reducing KV cache memory usage by 50%

## Why This Works (Mechanism)

### Mechanism 1
Value vector norms provide complementary information to attention scores for token importance. The output of an attention head is a weighted sum of value vectors, and tokens with high attention scores but low value vector norms contribute little to the final output. The non-uniform distribution of value norms correlates with actual token contribution to attention output.

### Mechanism 2
The product of attention score and value vector norm better captures token importance than attention score alone. By multiplying attention score St_k with the ℓ1 norm of the value vector ∥vk∥1, the method captures both connectivity (attention) and signal strength (value) of each token.

### Mechanism 3
Attention sink tokens require special handling despite their potentially low value vector norms. The first F tokens are intentionally preserved regardless of their computed importance score to maintain attention distribution stability, as these tokens play a critical role in anchoring attention computation.

## Foundational Learning

- **Self-attention mechanism in transformers**: Understanding how attention scores and value vectors combine to produce output is fundamental to grasping why both metrics matter. *Quick check*: What is the mathematical relationship between query, key, value vectors and the attention output?

- **KV cache in autoregressive generation**: The paper's entire motivation revolves around reducing KV cache memory cost while maintaining performance. *Quick check*: Why does KV cache memory scale linearly with sequence length and batch size?

- **Token pruning strategies**: The method builds on existing token pruning approaches but adds a new dimension (value norms). *Quick check*: What are the key differences between static and dynamic token pruning methods?

## Architecture Onboarding

- **Component map**: Tokenized text sequence -> LLama2-7B-chat or Vicuna-v1.5-7B-16k encoder -> Multi-head self-attention with KV cache -> Value-Aware Token Pruning (VATP) -> Pruned KV cache and generated tokens

- **Critical path**: Token generation → Attention computation → VATP importance scoring → KV cache pruning → Next token prediction

- **Design tradeoffs**: Uniform vs. non-uniform KV budget allocation across heads/layers; ℓ1 vs. ℓ2 vs. ℓ∞ norm for value vector importance; Number of attention sink tokens to preserve (F parameter)

- **Failure signatures**: Performance degradation when pruning too aggressively; Inconsistent improvements across different task types; Increased memory usage during prompt prefilling

- **First 3 experiments**: 1) Verify non-uniform distribution of value vector norms across different layers and heads; 2) Test VATP against attention-only baselines on 2WikiMultihopQA with varying KV budget ratios; 3) Conduct ablation study comparing ℓ1, ℓ2, and ℓ∞ norms for value vector importance scoring

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of VATP vary when integrated with grouped-query attention (GQA) architectures? The paper explicitly states VATP is currently not applicable to grouped-query attention models, leaving this as an open area for investigation.

### Open Question 2
What is the impact of different attention sink token thresholds (F) on VATP performance across various tasks? The paper sets F=20 for LLaMA2 and F=40 for Vicuna based on empirical findings but doesn't explore sensitivity of these thresholds.

### Open Question 3
How does VATP's performance scale with increasingly long context lengths beyond the tested 4K and 8K tokens? While VATP shows effectiveness at 4K and 8K contexts, performance at extreme context lengths (100K+ tokens) remains unexplored.

### Open Question 4
What is the theoretical relationship between attention sink tokens and value vector norm distributions across different model architectures? The paper observes that attention sink tokens have high attention scores but low value vector norms, but doesn't provide theoretical explanation for this phenomenon.

## Limitations
- The non-uniform value vector norm distributions may be architecture-dependent and not generalize across different transformer models
- VATP cannot be applied during prompt prefilling due to Flash attention optimization constraints, creating inconsistency between inference phases
- The preservation of first F tokens as attention sinks is implemented as a heuristic without rigorous theoretical justification

## Confidence

**High Confidence (90%+)**: The empirical demonstration that VATP outperforms attention-only baselines on 12+ of 16 LongBench tasks.

**Medium Confidence (70-89%)**: The theoretical mechanism explaining why value vector norms provide complementary information to attention scores.

**Low Confidence (50-69%)**: The generalizability of VATP's performance benefits to other transformer architectures and task domains beyond the tested LongBench tasks.

## Next Checks

1. **Cross-Architecture Validation**: Implement VATP on GPT-2 and BERT-base to verify whether value vector norm distribution patterns hold across different transformer architectures and whether VATP consistently outperforms attention-only methods.

2. **Adaptive Budget Allocation Study**: Replace uniform KV budget allocation with a learned or heuristic-based allocation strategy that varies across heads and layers, then compare performance against baseline VATP implementation.

3. **Attention Sink Token Ablation**: Conduct systematic study varying the number of preserved attention sink tokens (F parameter) and test alternative sink identification strategies to determine if the fixed-position heuristic is optimal or if adaptive selection improves performance.