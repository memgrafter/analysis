---
ver: rpa2
title: 'LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits'
arxiv_id: '2410.01735'
source_url: https://arxiv.org/abs/2410.01735
tags:
- training
- arxiv
- reward
- selection
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LASeR addresses the challenge of training large language models
  (LLMs) using multiple reward models (RMs) when no single RM generalizes well across
  all tasks. It frames reward model selection as a multi-armed bandit problem, dynamically
  choosing the most suitable RM for each instance during iterative training.
---

# LASeR: Learning to Adaptively Select Reward Models with Multi-Armed Bandits

## Quick Facts
- arXiv ID: 2410.01735
- Source URL: https://arxiv.org/abs/2410.01735
- Reference count: 40
- One-line primary result: LASeR improves Llama-3-8B accuracy by 2.67% over RM ensembles on reasoning tasks while achieving 2-3× speedup in training efficiency

## Executive Summary
LASeR addresses the challenge of training large language models using multiple reward models when no single RM generalizes well across all tasks. It frames reward model selection as a multi-armed bandit problem, dynamically choosing the most suitable RM for each instance during iterative training. By selecting one RM per batch, LASeR avoids the computational overhead and conflicting signals of RM ensembles while maintaining strong performance across reasoning, instruction-following, and long-context generation tasks.

## Method Summary
LASeR integrates a multi-armed bandit (LinUCB) into the iterative LLM training pipeline to dynamically select which reward model to use for each batch. The LLM generates multiple responses, context embeddings are extracted, and the bandit selects an RM based on past performance. The selected RM scores responses to create preference pairs, which are used to fine-tune the LLM using DPO loss. The bandit updates its parameters based on the negative training loss, creating a closed-loop system that adapts RM selection over time.

## Key Results
- Improves Llama-3-8B accuracy by 2.67% over RM ensembles on reasoning tasks
- Achieves 72.69% AlpacaEval win rate over RM ensemble baselines for instruction-following
- Improves F1 scores by 2.96 points on single-document QA for long-context generation
- Shows 2-3× speedup in training efficiency compared to sequential multi-RM baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LASeR's instance-level RM selection addresses the three shortcomings of choosing one reward model: lack of generalization, unreliable rankings, and over-optimization.
- Mechanism: The multi-armed bandit dynamically selects the most suitable RM for each instance based on contextual information about the model's performance and past interactions. By selecting one RM per batch, it avoids the computational overhead and conflicting signals of RM ensembles while maintaining strong performance.
- Core assumption: The RM that provides the most informative and consistent rankings for a given instance will lead to better LLM training outcomes.
- Evidence anchors:
  - [abstract] "LASeR frames reward model selection as a multi-armed bandit problem, dynamically choosing the most suitable RM for each instance during iterative training"
  - [section] "LASeR's adaptive instance-level or batch-level RM selection addresses the three shortcomings of choosing one reward model: lack of generalization, unreliable rankings, and over-optimization"
- Break condition: If the RM selection becomes too noisy or the bandit exploration-exploitation balance is poorly tuned, the model may converge to suboptimal RMs for certain task types.

### Mechanism 2
- Claim: The negative training loss (DPO) serves as an effective MAB reward signal that reflects how well the selected RM's feedback helps the model sharpen its rankings.
- Mechanism: After each LLM train step, the MAB reward is computed as the negative training loss, which depends on how clearly the model learns to prefer the RM's chosen outputs over rejected ones. A higher MAB reward corresponds to a larger log-likelihood margin between preferred and dispreferred responses.
- Core assumption: The DPO loss reduction is a reliable proxy for how informative the RM's preference signal is for training the LLM.
- Evidence anchors:
  - [section] "The MAB reward is the negative training loss (DPO), which depends on how clearly the model learns to prefer the RM's chosen outputs over the rejected ones"
  - [section] "A lower DPO loss or a higher MAB reward corresponds to a larger log-likelihood margin between preferred and dispreferred responses"
- Break condition: If the DPO loss becomes unstable or plateaus early, the MAB may receive misleading reward signals that prevent it from discovering better RMs.

### Mechanism 3
- Claim: LASeR's efficiency comes from fast convergence compared to sequential selection and avoiding the overhead of loading and evaluating multiple RMs per step.
- Mechanism: By selecting and using only one RM per mini-batch, LASeR saves GPU memory and compute compared to ensemble baselines that require scoring all candidate responses with all RMs. This also enables faster convergence since the model isn't confused by conflicting signals.
- Core assumption: Loading and evaluating a single RM per batch is significantly more efficient than loading multiple RMs and computing ensemble scores.
- Evidence anchors:
  - [section] "LASeR's efficiency comes from fast convergence compared to the sequential selection baseline and avoiding the overhead of loading and evaluating multiple RMs per step"
  - [section] "Instead, it selects and uses only one RM per mini-batch, saving GPU memory and compute"
- Break condition: If the selected RM per batch becomes too inconsistent or the bandit exploration becomes too aggressive, the efficiency gains may be offset by poor model performance.

## Foundational Learning

- Concept: Multi-armed bandit problem
  - Why needed here: LASeR frames reward model selection as a multi-armed bandit problem where each RM is an "arm" that can be pulled to score responses
  - Quick check question: In the LASeR framework, what does each "arm" in the multi-armed bandit represent?

- Concept: Contextual bandit algorithms
  - Why needed here: LASeR uses LinUCB, a contextual bandit algorithm that leverages context information (sentence embeddings) to inform RM selection for each instance
  - Quick check question: What type of bandit algorithm does LASeR use that allows it to incorporate context information about each instance?

- Concept: Preference optimization with human feedback
  - Why needed here: LASeR operates within the iterative LLM training pipeline that generates preference pairs from RM-scored responses and uses these to further train the LLM
  - Quick check question: What is the three-stage pipeline that LASeR extends for iterative LLM training using RMs?

## Architecture Onboarding

- Component map:
  LLM (policy model) -> Context encoder -> Multi-armed bandit (LinUCB) -> Reward Models (RM arms) -> Preference pair generator -> Training loop -> LLM (policy model)

- Critical path:
  1. LLM generates multiple responses for a batch of prompts
  2. Context encoder extracts sentence embeddings from the LLM
  3. Bandit selects RM based on context and past performance
  4. Selected RM scores responses and creates preference pairs
  5. LLM is fine-tuned using DPO loss on preference pairs
  6. Bandit updates parameters based on negative training loss
  7. Repeat for M iterations

- Design tradeoffs:
  - Instance-level vs batch-level RM selection: Instance-level provides finer granularity but is less efficient; batch-level balances performance and efficiency
  - Exploration vs exploitation in bandit: Too much exploration wastes resources on poor RMs; too little may miss better RMs
  - Number of RMs: More RMs provide better coverage but increase computational overhead and complexity

- Failure signatures:
  - Slow convergence or plateauing performance: May indicate poor bandit parameter tuning or insufficient exploration
  - Inconsistent RM utilization across similar instances: May indicate poor context encoding or bandit model issues
  - Large variance in MAB rewards: May indicate noisy RM scores or unstable training dynamics

- First 3 experiments:
  1. Run LASeR with a single RM to verify the base iterative training pipeline works correctly
  2. Test bandit selection with synthetic rewards to verify the MAB learns to prefer better scoring RMs
  3. Run with multiple RMs on a small dataset to verify RM selection adapts to different task types

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the limitations section, potential open questions include:

- How does LASeR handle safety-critical or bias-sensitive domains where reward models may encode undesirable preferences?
- Can LASeR be extended to dynamically update or replace reward models during training when new, improved RMs become available?
- How does LASeR perform when the pool of reward models contains a mixture of high- and low-quality models?

## Limitations

- No direct experimental evidence for the proposed mechanisms in the corpus
- Unclear hyperparameter settings that may significantly impact performance
- Limited ablation studies on the bandit algorithm's sensitivity to exploration parameters

## Confidence

- Mechanism 1 (Instance-level selection): Medium
- Mechanism 2 (DPO reward signal): Medium
- Mechanism 3 (Efficiency gains): Medium
- Overall experimental results: High

## Next Checks

1. Run ablation studies varying the exploration-exploitation tradeoff parameter (α) in LinUCB to determine its impact on both performance and convergence speed
2. Measure actual GPU memory usage and training time for LASeR versus RM ensemble baselines under identical conditions
3. Test LASeR with synthetic reward models of known quality to validate whether the bandit correctly learns to select better-performing RMs over time