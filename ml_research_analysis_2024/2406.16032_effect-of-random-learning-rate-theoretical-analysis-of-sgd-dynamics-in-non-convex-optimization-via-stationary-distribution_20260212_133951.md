---
ver: rpa2
title: 'Effect of Random Learning Rate: Theoretical Analysis of SGD Dynamics in Non-Convex
  Optimization via Stationary Distribution'
arxiv_id: '2406.16032'
source_url: https://arxiv.org/abs/2406.16032
tags:
- distribution
- learning
- poisson
- stationary
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Poisson SGD, a novel stochastic optimization
  method with a random learning rate to handle non-convex optimization. By using a
  learning rate that follows a Poisson process and a momentum coefficient derived
  from the mini-batch gradient, Poisson SGD ensures that parameter updates explore
  the entire parameter space, overcoming the degeneracy issue in standard SGD.
---

# Effect of Random Learning Rate: Theoretical Analysis of SGD Dynamics in Non-Convex Optimization via Stationary Distribution

## Quick Facts
- **arXiv ID**: 2406.16032
- **Source URL**: https://arxiv.org/abs/2406.16032
- **Reference count**: 40
- **Primary result**: Poisson SGD with random learning rate converges to stationary distribution and finds global minima in non-convex optimization

## Executive Summary
This paper introduces Poisson SGD, a novel stochastic optimization method that uses a random learning rate following a Poisson process to handle non-convex optimization problems. By ensuring ergodicity through random learning rates, Poisson SGD overcomes the degeneracy issues in standard SGD and converges to a stationary distribution under weak assumptions on the loss function. The authors prove that this approach can find global minima in non-convex problems and evaluate generalization error, with theoretical guarantees supported by approximating Poisson SGD with the Bouncy Particle Sampler from MCMC literature.

## Method Summary
Poisson SGD modifies standard SGD by incorporating a learning rate ηk drawn from an exponential distribution based on the mini-batch gradient, combined with a momentum coefficient αk derived from the gradient direction. The algorithm operates on a torus parameter space (R/aZ)^d with a modulo operation to handle boundaries, ensuring the parameter distribution converges to a stationary distribution. The method approximates the distribution of Poisson SGD with that of the Bouncy Particle Sampler (BPS), a piecewise deterministic Markov process, allowing convergence proofs through established MCMC theory.

## Key Results
- Poisson SGD converges to a stationary distribution with O(1/√K) rate under weak assumptions on the loss function
- The method finds global minima in non-convex optimization problems through exploration enabled by random learning rates
- Generalization error can be evaluated through the stationary distribution, with performance comparable to SGD with Momentum on MNIST and CIFAR datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Poisson SGD converges to a stationary distribution even when the gradient noise is degenerate
- **Mechanism**: The random learning rate drawn from an exponential distribution based on the mini-batch gradient prevents parameter updates from degenerating into a subspace, enabling ergodicity
- **Core assumption**: The loss function is absolutely continuous and differentiable, and the parameter space is a torus
- **Evidence anchors**: [abstract] "a novel SGD variant, Poisson SGD, which has degenerated parameter update directions and instead utilizes a random learning rate"; [section] "Theorem 1 shows that the parameter distribution µz,K by Poisson SGD converges to the stationary distribution µ(β,ε)z owing to the random learning rate (4)"
- **Break condition**: If the loss function fails absolute continuity or the torus assumption is removed, boundary effects could invalidate convergence

### Mechanism 2
- **Claim**: Poisson SGD can escape sharp minima and find global minima in non-convex problems
- **Mechanism**: Large learning rates are sampled when the gradient magnitude is small, enabling exploration of the full parameter space; the stationary distribution is concentrated around global minima as β increases
- **Core assumption**: The loss function satisfies a Lipschitz gradient condition and the inverse temperature parameter β is sufficiently large
- **Evidence anchors**: [abstract] "we further show that Poisson SGD finds global minima in non-convex optimization problems"; [section] "Poisson SGD achieves global convergence by the similar approach of global convergence of SGLD by Raginsky et al. (2017)"
- **Break condition**: If the gradient noise is too large relative to the learning rate range, updates may become unstable despite the torus constraint

### Mechanism 3
- **Claim**: The distribution of parameters updated by Poisson SGD can be approximated by that of the Bouncy Particle Sampler (BPS)
- **Mechanism**: BPS is a piecewise deterministic Markov process (PDMP) with reflection and refreshment steps that converges to a stationary distribution; the approximation error decreases with smaller learning rates
- **Core assumption**: The learning rate in Poisson SGD is bounded and the momentum vector remains normalized
- **Evidence anchors**: [abstract] "we approximate the distribution by Poisson SGD with that of the bouncy particle sampler (BPS)"; [section] "Theorem 3(Distance between Poisson SGD and BPS)... W1(µz,K,ˆµz,K)≤4√dKε"
- **Break condition**: If the approximation error between Poisson SGD and BPS grows with K (e.g., due to large jumps), the stationary distribution guarantee fails

## Foundational Learning

- **Concept**: Piecewise Deterministic Markov Processes (PDMPs)
  - Why needed here: PDMPs like BPS provide the theoretical framework for proving convergence to stationary distributions under conditions that standard SGD variants cannot satisfy
  - Quick check question: What are the two types of jumps in a PDMP, and how do they contribute to ergodicity?

- **Concept**: Wasserstein distance and total variation
  - Why needed here: These metrics quantify the convergence of parameter distributions to the stationary distribution and are used throughout the convergence proofs
  - Quick check question: How does the total variation bound the Wasserstein distance in compact parameter spaces?

- **Concept**: Stationary distributions and ergodicity
  - Why needed here: The core theoretical result relies on showing that Poisson SGD converges to a unique stationary distribution, which requires proving ergodicity
  - Quick check question: What is the difference between ergodicity and exponential ergodicity, and why is the latter important for convergence rates?

## Architecture Onboarding

- **Component map**: Loss function ℓ(z;θ) → gradient computation → mini-batch sampling → learning rate sampling (Poisson) → momentum coefficient calculation → parameter update with modulo operation → velocity update → repeat
- **Critical path**: Mini-batch gradient → learning rate sampling → parameter update → velocity update → repeat
  - The learning rate sampling step is the key differentiator that enables convergence
- **Design tradeoffs**:
  - Poisson SGD vs SGLD: No added Gaussian noise vs explicit noise addition; different stationary distribution forms
  - Fixed learning rate vs random learning rate: Simplicity vs guaranteed convergence to stationary distribution
  - Torus vs Euclidean space: Simplified boundary handling vs practical implementation considerations
- **Failure signatures**:
  - Non-convergence to stationary distribution: Learning rate distribution too narrow or gradient magnitudes too large
  - Poor optimization performance: Momentum coefficient not properly normalized or learning rate range inappropriate
  - Computational instability: Torus modulo operation not correctly implemented for high-dimensional parameters
- **First 3 experiments**:
  1. Verify that the velocity vector remains normalized after updates (Proposition 7 check)
  2. Test convergence of parameter distribution to theoretical stationary distribution on a simple 2D non-convex loss
  3. Compare generalization performance on MNIST with fixed learning rate SGD and SGLD variants

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the explicit form of κ(β,ε,d) for Poisson SGD?
- **Basis in paper**: [explicit] The paper states that κ(β,ε,d) is not explicitly calculated for Poisson SGD but mentions that for Langevin dynamics, κ(β,ε,d) can be computed as Ω(cLSkη/β(β+d)), where cLS is the logarithmic Sobolev constant
- **Why unresolved**: The paper notes that deriving κ(β,ε,d) for a class of PDMPs is challenging, as mentioned in Deligiannidis et al. (2019) and Durmus et al. (2020), which is why the explicit form is not provided for Poisson SGD
- **What evidence would resolve it**: A detailed mathematical derivation or experimental results that provide the explicit form of κ(β,ε,d) for Poisson SGD

### Open Question 2
- **Question**: How does the performance of Poisson SGD compare to other optimization methods in terms of computational efficiency?
- **Basis in paper**: [inferred] The paper mentions that the stationary distribution analysis is useful because it can analyze global convergence and is applicable to a wide range of loss functions, but does not provide a direct comparison of computational efficiency
- **Why unresolved**: The paper focuses on the theoretical convergence properties and generalization error analysis but does not address the computational efficiency or runtime performance of Poisson SGD compared to other methods
- **What evidence would resolve it**: Empirical studies comparing the computational time and resource usage of Poisson SGD against other optimization algorithms on various datasets and models

### Open Question 3
- **Question**: Can the theoretical results of Poisson SGD be extended to non-torus parameter spaces?
- **Basis in paper**: [explicit] The paper uses a torus parameter space to simplify technical discussions and mentions that the results hold for all values of the periodicity parameter a, making it practically indistinguishable from the Euclidean space when a is sufficiently large
- **Why unresolved**: The paper does not explore the implications or provide a detailed analysis of extending the theoretical results to non-torus parameter spaces, such as Euclidean spaces or hypercubes
- **What evidence would resolve it**: Mathematical proofs or empirical studies demonstrating the applicability of Poisson SGD's theoretical results to non-torus parameter spaces

## Limitations

- The theoretical framework relies heavily on the torus constraint, which simplifies boundary handling but may not translate directly to standard Euclidean parameter spaces
- The proof assumes the learning rate moments remain bounded, yet practical implementations may encounter rare but significant large learning rate samples that could affect stability
- The stationary distribution concentration around global minima assumes sufficiently large inverse temperature β, but the required magnitude for different problem classes is not precisely characterized

## Confidence

**High Confidence**: The convergence to stationary distribution under the stated assumptions (Theorem 1) is well-supported by the PDMP framework and Wasserstein distance analysis. The approximation of Poisson SGD by BPS (Theorem 3) has strong theoretical backing from established MCMC literature.

**Medium Confidence**: The claim that Poisson SGD finds global minima (Theorem 4) follows from Raginsky et al.'s SGLD convergence framework, but the extension to the degenerate gradient noise case introduces uncertainties. The practical optimization performance depends on hyperparameter tuning that is not fully specified.

**Low Confidence**: The generalization error bounds and their dependence on the stationary distribution parameters require further empirical validation across diverse problem settings.

## Next Checks

1. **Convergence Rate Verification**: Implement the 2D torus example with explicit parameter updates and compute the Wasserstein distance between empirical and theoretical stationary distributions for increasing iteration counts, verifying the O(1/√K) convergence rate.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary CP, Cα, and β parameters across orders of magnitude to identify stability regions and optimal ranges for different problem scales (e.g., compare MNIST vs CIFAR-10 requirements).

3. **Euclidean Space Extension**: Remove the torus constraint and implement reflective boundary conditions in Euclidean space to test whether convergence to stationary distribution is preserved when the simplifying assumption is relaxed.