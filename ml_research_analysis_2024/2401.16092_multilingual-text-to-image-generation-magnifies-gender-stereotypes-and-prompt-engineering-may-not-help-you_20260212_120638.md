---
ver: rpa2
title: Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt
  Engineering May Not Help You
arxiv_id: '2401.16092'
source_url: https://arxiv.org/abs/2401.16092
tags:
- gender
- prompts
- bias
- languages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAGBIG, a multilingual benchmark designed
  to systematically assess gender bias in text-to-image (T2I) generation models across
  nine languages (Arabic, Chinese, English, French, German, Italian, Japanese, Korean,
  Spanish). It covers 20 adjectives and 150 occupations, with translations carefully
  reviewed by native speakers.
---

# Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You

## Quick Facts
- arXiv ID: 2401.16092
- Source URL: https://arxiv.org/abs/2401.16092
- Reference count: 40
- Primary result: Multilingual text-to-image models exhibit substantial gender bias across nine languages, and prompt engineering strategies are largely ineffective at reducing this bias.

## Executive Summary
This paper introduces MAGBIG, a multilingual benchmark designed to systematically assess gender bias in text-to-image generation models across nine languages (Arabic, Chinese, English, French, German, Italian, Japanese, Korean, Spanish). The authors evaluate five multilingual T2I models (MultiFusion, AltDiffusion, Lumina, MuLan, CogView) using MAGBIG, generating over 1.8 million images. Results show that multilingual T2I models exhibit substantial gender bias and inconsistent behavior across languages, even when using seemingly neutral prompts. Prompt engineering strategies, such as indirect descriptions or gender-neutral formulations, were found largely ineffective and sometimes detrimental to prompt understanding, measured via text-to-image alignment.

## Method Summary
The study uses MAGBIG, a benchmark with 3,630 prompts covering 20 adjectives and 150 occupations translated into nine languages with human supervision. Five multilingual T2I models were evaluated by generating 100 images per prompt, then classifying perceived gender using FairFace and measuring alignment with CLIP. The researchers calculated Mean Absolute Deviation (MAD) scores to quantify gender bias and used text-to-image alignment metrics to assess prompt understanding. Direct and indirect prompt formulations were tested, along with prompts using explicit gender identifiers.

## Key Results
- Multilingual T2I models show substantial gender bias across all nine languages tested, with MAD scores ranging from 0.24 to 0.39
- Grammatical gender systems directly influence gender representation in generated images, with languages like German showing higher bias when using generic masculine forms
- Prompt engineering with indirect, gender-neutral formulations reduces but does not eliminate bias, often at the cost of text-to-image alignment
- The bias patterns are consistent across different model architectures, suggesting the issue is rooted in training data rather than model design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual text-to-image models inherit and amplify gender stereotypes from their training data across different languages.
- Mechanism: The models are trained on web-scraped multimodal data that contains societal gender biases, which are then reflected and potentially magnified in generated images regardless of the input language.
- Core assumption: Training data contains pervasive gender stereotypes that the model learns and reproduces.
- Evidence anchors:
  - [abstract] "multilingual models suffer from substantial gender bias just as monolingual models do"
  - [section] "MAGBIG reveals substantial skews in gender distribution across languages (Figure 1) for identical prompts"
  - [corpus] Weak - most related papers focus on bias in LLMs or specific languages, not directly multimodal T2I.

### Mechanism 2
- Claim: Grammatical gender systems in different languages influence the perceived gender in generated images, even when prompts are semantically gender-neutral.
- Mechanism: When translating gender-neutral prompts into languages with grammatical gender, the translation process forces a gender assignment (e.g., masculine or feminine forms), which the model interprets as a gender cue and reflects in the generated images.
- Core assumption: The model's text encoder and generation process are sensitive to grammatical gender markers in the input prompt.
- Evidence anchors:
  - [abstract] "grammatical gender systems are directly tied to gender" and "MAGBIG includes languages with diverse gender systems"
  - [section] "The issue arises because German uses grammatical gender and defaults to the generic masculine, affecting image generation"
  - [corpus] Weak - related papers focus on bias in LLMs, not the specific interaction between grammatical gender and image generation.

### Mechanism 3
- Claim: Prompt engineering with indirect, gender-neutral formulations can reduce but not eliminate gender bias in multilingual T2I models, often at the cost of text-to-image alignment.
- Mechanism: By avoiding potentially gendered nouns and using descriptive phrases instead, the prompts reduce the model's reliance on stereotypical gender associations, leading to more balanced gender representation but potentially less accurate interpretation of the intended occupation or trait.
- Core assumption: The model's understanding of complex, descriptive prompts is less biased but also less precise than its understanding of direct noun-based prompts.
- Evidence anchors:
  - [abstract] "prompt engineering strategies, such as indirect, neutral formulations, to mitigate these biases... find them largely ineffective and sometimes even detrimental to text-to-image alignment"
  - [section] "indirect prompts still suffer from substantial gender bias" but "measured gender bias is, on average, substantially lower than for the direct prompts"
  - [corpus] Weak - related papers focus on bias in LLMs, not specific prompt engineering strategies for T2I models.

## Foundational Learning

- Concept: Grammatical gender systems in languages
  - Why needed here: Understanding how different languages assign gender to nouns and how this affects translation and model interpretation is crucial for designing fair prompts and interpreting results.
  - Quick check question: What is the difference between a language with gendered nouns and a language with only gendered pronouns, and how might this affect image generation?

- Concept: Text-to-image generation pipeline
  - Why needed here: Knowing how text encoders, diffusion models, and image classifiers work together helps in understanding where bias can be introduced and how to evaluate it.
  - Quick check question: How does a text encoder like CLIP convert a text prompt into a representation that guides image generation, and where might bias be introduced in this process?

- Concept: Fairness metrics and evaluation
  - Why needed here: Understanding how to measure and interpret bias, such as using MAD scores and text-to-image alignment, is essential for assessing the effectiveness of bias mitigation strategies.
  - Quick check question: What does a Mean Absolute Deviation (MAD) score of 0.3 indicate about the gender distribution in generated images compared to an equitable distribution?

## Architecture Onboarding

- Component map: MAGBIG benchmark -> Translation pipeline -> Text-to-image models -> FairFace classifier -> CLIP evaluation
- Critical path: Prompt generation → Translation → Image generation → Image classification → Bias analysis
- Design tradeoffs: Controlled translations vs. natural language variations, bias mitigation vs. prompt understanding, multilingual coverage vs. model availability
- Failure signatures: Inconsistent gender representation across languages for identical prompts, poor text-to-image alignment for indirect prompts, high c100 values indicating generation difficulties
- First 3 experiments:
  1. Generate images using direct prompts in all 9 languages for a single occupation and analyze the gender distribution using FairFace
  2. Repeat experiment 1 with indirect, gender-neutral prompts and compare MAD scores and text-to-image alignment
  3. Select one language pair (e.g., English and German) and analyze the impact of grammatical gender on gender representation by comparing images generated from gender-neutral and masculine prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do other grammatical gender systems beyond the nine languages in MAGBIG affect gender bias in T2I models?
- Basis in paper: [explicit] "MAGBIG includes prompts in nine global languages, but there are more to be explored."
- Why unresolved: The paper only evaluates nine languages with diverse gender systems, leaving many languages unexamined.
- What evidence would resolve it: Testing MAGBIG or similar benchmarks across a broader range of languages with different grammatical gender systems would reveal whether the observed biases are universal or language-specific.

### Open Question 2
- Question: What is the impact of explicit gender identifiers on bias reduction across different T2I model architectures?
- Basis in paper: [explicit] "When evaluating MAGBIG's feminine set with gender-specific prompts... the models produced nearly exclusively female-appearing images across languages, achieving MAD scores near zero."
- Why unresolved: The paper only briefly mentions the effectiveness of explicit gender identifiers without comparing their performance across different model architectures.
- What evidence would resolve it: A systematic comparison of explicit gender identifier effectiveness across various T2I model architectures would clarify whether this approach generalizes or is architecture-dependent.

### Open Question 3
- Question: How does the token representation of gender-neutral formulations affect bias in T2I models?
- Basis in paper: [explicit] "This issue likely stems from how T2I models tokenize prompts. For instance, 'Jurist*in' is tokenized into three parts..."
- Why unresolved: The paper identifies tokenization as a potential issue but does not explore alternative tokenization strategies or their effects on bias.
- What evidence would resolve it: Experimenting with different tokenization approaches for gender-neutral formulations and measuring their impact on bias would clarify whether tokenization is a key factor in perpetuating bias.

## Limitations
- Benchmark covers only nine languages, missing major world languages like Hindi, Portuguese, and Russian
- Relies on perceived gender classification through FairFace, which was trained primarily on English datasets
- Focuses exclusively on binary gender representation, not addressing non-binary or fluid gender identities
- Evaluation only considers adjectives and occupations as prompt elements, potentially missing bias in other semantic domains

## Confidence
- High confidence: Gender bias exists across all evaluated multilingual T2I models and is measurable using the proposed MAD metric
- Medium confidence: Grammatical gender systems in source languages systematically influence gender representation in generated images
- Medium confidence: Prompt engineering strategies show limited effectiveness in reducing gender bias without compromising prompt understanding
- Low confidence: The observed bias patterns would generalize to other types of demographic attributes (race, age, ability) or to monolingual T2I models

## Next Checks
1. Validate FairFace classifier performance across all nine languages by testing on manually labeled validation sets to quantify classification accuracy differences between languages
2. Replicate the study with additional languages (e.g., Portuguese, Hindi) to test whether bias patterns hold across different language families and grammatical structures
3. Test whether fine-tuning the multilingual T2I models on debiased datasets can reduce gender bias more effectively than prompt engineering alone, using the same MAGBIG evaluation framework