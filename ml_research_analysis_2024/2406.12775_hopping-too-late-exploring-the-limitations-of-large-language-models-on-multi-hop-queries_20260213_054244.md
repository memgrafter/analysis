---
ver: rpa2
title: 'Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop
  Queries'
arxiv_id: '2406.12775'
source_url: https://arxiv.org/abs/2406.12775
tags:
- uni00000048
- uni00000052
- layer
- uni00000055
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how large language models solve multi-hop
  queries that require resolving two facts in sequence. By analyzing transformer hidden
  representations using Patchscopes, the authors discover that the first hop (identifying
  the bridge entity) is resolved in early layers, while the second hop is resolved
  in later layers at the last token position.
---

# Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries

## Quick Facts
- **arXiv ID**: 2406.12775
- **Source URL**: https://arxiv.org/abs/2406.12775
- **Reference count**: 40
- **Primary result**: Back-patching can recover correct answers in up to 66% of previously incorrect multi-hop queries by patching later-layer representations into earlier layers

## Executive Summary
This work investigates how large language models solve multi-hop queries requiring sequential reasoning across two facts. Using Patchscopes analysis, the authors discover that bridge entity resolution occurs in early layers while second-hop resolution happens in later layers at the last token position. To explain why models fail on some queries despite knowing both facts separately, they propose "back-patching" - re-running the model with hidden representations from later layers patched into earlier ones. This technique successfully recovers correct answers in up to 66% of previously incorrect cases, revealing architectural limitations in transformer-based LLMs for sequential reasoning tasks.

## Method Summary
The authors create a dataset of 82,020 two-hop queries from Wikidata by composing two facts where one's target entity is the other's source entity. They analyze transformer hidden representations using Patchscopes to locate where bridge entities are resolved, employ attention knockout to test information flow, and use vocabulary projections to understand knowledge extraction. The key innovation is back-patching - recording hidden representations from later layers and rerunning the forward pass with these representations patched into earlier layers at specific token positions to test whether restored information improves performance.

## Key Results
- Bridge entity (e2) resolution is clearly evident in hidden representations of the end-token of the first hop, occurring in early layers
- Second-hop resolution happens in later layers at the last token position, using the bridge entity as context
- MLP sublayers play a larger role than attention sublayers in promoting the predicted token during second-hop resolution
- Back-patching successfully recovers correct answers in up to 66% of previously incorrect cases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Bridge entity resolution happens in early layers and propagates to the last token for second-hop resolution
- **Mechanism**: During the first hop, the bridge entity (e2) is resolved in early layers at position t1. This information then propagates through middle layers to the last token position (t2), where the second hop is resolved into the target entity (e3)
- **Core assumption**: The same knowledge-extraction module is activated twice - first for e2, then for e3 using e2 as context
- **Evidence anchors**:
  - [abstract]: "we discover that the bridge entity is resolved in the early layers of the model. Then, only after this resolution, the two-hop query is solved in the later layers."
  - [section]: "we find that the outcome of the first hop is clearly evident in the hidden representation of the end-token of the first hop. We further find that resolving the first hop happens during the early layers"
- **Break condition**: If the information fails to propagate from t1 to t2, or if the layers responsible for the second hop lack the necessary knowledge-extraction functionality

### Mechanism 2
- **Claim**: MLP sublayers play a larger role than attention sublayers in promoting the predicted token
- **Mechanism**: When resolving the second hop at the last token position, the MLP sublayers' residual updates are more influential in driving the final prediction than attention sublayers
- **Core assumption**: Different sublayer types have distinct roles in the reasoning pathway
- **Evidence anchors**:
  - [abstract]: "we find that in up to 66% of previously incorrect cases there exists a back-patch that results in the correct generation of the answer"
  - [section]: "we find that the MLP sublayers play a larger role in promoting the predicted token compared to that of the attention sublayers"
- **Break condition**: If the MLP sublayers at the resolution layer lack the necessary parameters or if the attention sublayers are critical for this specific type of reasoning

### Mechanism 3
- **Claim**: Back-patching works because it provides later layers access to knowledge encoded in earlier layers that would otherwise be lost
- **Mechanism**: By patching a hidden representation from a later layer back to an earlier layer, the model gains access to knowledge that the later layers no longer possess, effectively giving it more time to complete the reasoning pathway
- **Core assumption**: The sequential nature of reasoning means that knowledge needed for later hops may be encoded in earlier layers but gets overwritten or transformed in a way that loses its utility
- **Evidence anchors**:
  - [abstract]: "back-patching... patching a hidden representation from a later layer back to an earlier layer"
  - [section]: "we propose an analysis method named back-patching... we find that in up to 66% of previously incorrect cases there exists a back-patch that results in the correct generation of the answer"
- **Break condition**: If the patched representation doesn't contain the necessary information, or if the target layer's parameters are incompatible with the source representation

## Foundational Learning

- **Concept**: Transformer layer architecture (self-attention + MLP sublayers)
  - **Why needed here**: Understanding how information flows through layers and how different sublayers contribute to the reasoning process is critical for interpreting the Patchscopes and back-patching results
  - **Quick check question**: What is the difference between the residual updates made by attention sublayers versus MLP sublayers?

- **Concept**: Knowledge extraction in language models
  - **Why needed here**: The paper assumes that the model is extracting factual knowledge at different stages of the reasoning pathway, which is fundamental to understanding the sequential nature of the computation
  - **Quick check question**: How does a language model typically extract factual knowledge from its parameters during inference?

- **Concept**: Mechanistic interpretability methods (Patchscopes, attention knockout)
  - **Why needed here**: These methods are the primary tools used to analyze where and how the model resolves entities, making them essential for understanding the experimental approach
  - **Quick check question**: What is the key advantage of Patchscopes over traditional vocabulary projection methods?

## Architecture Onboarding

- **Component map**: Source token → early layers (resolve e2) → middle layers (propagate information) → last token → later layers (resolve e3) → prediction
- **Critical path**: The sequential reasoning pathway where information flows from early layers (first hop resolution) through middle layers (information propagation) to later layers (second hop resolution)
- **Design tradeoffs**: The sequential nature of reasoning may create bottlenecks where information gets lost between hops, but this design allows for compositional reasoning. The tradeoff is between depth of reasoning and preservation of intermediate knowledge
- **Failure signatures**: Incorrect predictions when the first hop is resolved too late, or when information fails to propagate from t1 to t2. Low success rates in back-patching experiments also indicate failure points
- **First 3 experiments**:
  1. Run Patchscopes on t1 across all layers to identify where e2 first appears in the hidden representation
  2. Perform attention knockout to block information flow from t1 to t2 and observe if the prediction changes
  3. Implement back-patching by recording representations from later layers and patching them back to earlier layers to test if performance improves

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the back-patching method be extended to improve multi-hop reasoning in larger or more diverse LLM architectures beyond the ones tested?
- **Basis in paper**: [explicit] The paper demonstrates back-patching's effectiveness on six models (LLaMA 2/3 and Pythia variants) and suggests it could be used to improve multi-hop question answering performance
- **Why unresolved**: The study only tests back-patching on a limited set of models with specific architectures (transformer-based), and does not explore its applicability to other architectures or larger models
- **What evidence would resolve it**: Testing back-patching across a broader range of LLM architectures, including those with different attention mechanisms, larger scales, or alternative designs like state-space models, and measuring performance improvements in multi-hop reasoning tasks

### Open Question 2
- **Question**: What is the precise mechanism by which the bridge entity information propagates from the first hop to the second hop in the model's computation?
- **Basis in paper**: [explicit] The paper observes that bridge entity information propagates from the first hop to the second hop, but does not fully explain the exact mechanism of this propagation
- **Why unresolved**: While the paper identifies that information propagation occurs, it does not delve into the specific details of how this propagation happens at the level of attention weights, residual connections, or other internal mechanisms
- **What evidence would resolve it**: Detailed mechanistic analysis of attention patterns, residual updates, and activation flows between layers during the propagation phase, potentially using techniques like attention visualization or circuit analysis

### Open Question 3
- **Question**: How does the sequential nature of the reasoning pathway impact the model's ability to handle more complex queries with three or more hops?
- **Basis in paper**: [explicit] The paper notes that the sequential reasoning pathway is observed in two-hop queries and speculates that similar mechanisms might exist in more complex queries
- **Why unresolved**: The study only examines two-hop queries, and the impact of the sequential pathway on more complex reasoning tasks is not empirically tested or analyzed
- **What evidence would resolve it**: Empirical testing of models on multi-hop queries with three or more hops, comparing performance and analyzing whether the sequential reasoning pathway extends or breaks down in more complex scenarios

## Limitations
- **Dataset construction uncertainty**: Limited details on how entity pairs are selected and how "shortcut" cases are filtered, making it difficult to assess whether the dataset truly tests sequential reasoning
- **Layer identification precision**: Claims about early vs later layers lack precise layer numbering and rely on qualitative rather than quantitative analysis
- **Back-patching mechanism explanation**: The explanation for why back-patching works remains somewhat speculative without exploring whether improvement comes from restored information or new computational pathways

## Confidence

**High confidence**: The existence of sequential reasoning patterns in transformers for multi-hop queries is well-supported by the Patchscopes evidence showing distinct resolution points for each hop. The observation that MLP sublayers play a larger role than attention sublayers in final predictions is also strongly supported.

**Medium confidence**: The specific claim about information propagation from early to later layers is supported but could benefit from more quantitative analysis. The 66% success rate for back-patching is compelling but needs additional controls to rule out alternative explanations.

**Low confidence**: The exact mechanism by which back-patching works remains unclear. The paper doesn't adequately address why certain layer combinations work better than others, or whether the improvement comes from the specific representation being patched versus the act of rerunning computation.

## Next Checks

1. **Layer-by-layer quantitative analysis**: Create a detailed heatmap showing exactly which layers first encode the bridge entity and which layers resolve the final answer. This would involve running Patchscopes at every layer position rather than just at key token positions, providing precise layer ranges for each reasoning step.

2. **Controlled back-patching ablation**: Systematically vary which layers are used as source and target in back-patching experiments. For instance, test whether patching from layer 10 to layer 5 works better than patching from layer 20 to layer 10, and whether the improvement correlates with the semantic distance between representations.

3. **Alternative representation analysis**: Compare the back-patched representations to both the original early-layer representations and the later-layer representations using embedding similarity metrics. This would help determine whether the improvement comes from restoring original information or creating new, beneficial representations through the patching process.