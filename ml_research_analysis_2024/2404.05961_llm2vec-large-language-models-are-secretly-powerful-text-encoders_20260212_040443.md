---
ver: rpa2
title: 'LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders'
arxiv_id: '2404.05961'
source_url: https://arxiv.org/abs/2404.05961
tags:
- llm2vec
- mntp
- training
- simcse
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes LLM2Vec, a method to convert decoder-only
  language models into strong text encoders for embedding tasks. The approach involves
  three steps: enabling bidirectional attention, masked next token prediction training,
  and unsupervised contrastive learning.'
---

# LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders

## Quick Facts
- **arXiv ID:** 2404.05961
- **Source URL:** https://arxiv.org/abs/2404.05961
- **Reference count:** 40
- **Primary result:** Decoder-only LLMs can be converted into universal text encoders achieving SOTA performance on MTEB with minimal adaptation

## Executive Summary
LLM2Vec transforms decoder-only language models into powerful text encoders for embedding tasks through a three-step process: bidirectional attention, masked next token prediction training, and unsupervised contrastive learning. The method achieves state-of-the-art unsupervised performance on MTEB at 56.8, outperforming traditional encoder-only models, and sets a new SOTA at 65.0 when using supervised contrastive learning on public data only. This demonstrates that decoder-only LLMs contain latent encoding capabilities that can be unlocked with minimal architectural modifications and targeted training objectives.

## Method Summary
The approach converts decoder-only LLMs into encoder-style models by first enabling bidirectional attention through masking future tokens in the attention mechanism. Next, the models undergo training with masked next token prediction to adapt to bidirectional context. Finally, unsupervised contrastive learning is applied to enhance the representations for embedding tasks. The method was tested on 1.3B-8B parameter models, showing that even moderate-sized LLMs can achieve superior text embedding performance when properly adapted for encoding tasks.

## Key Results
- Achieves state-of-the-art unsupervised MTEB performance at 56.8, surpassing encoder-only models
- Sets new SOTA among models trained only on public data at 65.0 with supervised contrastive learning
- Demonstrates that 1.3B-8B parameter decoder-only LLMs can be effectively converted to universal text encoders

## Why This Works (Mechanism)
The method leverages the inherent language understanding capabilities of large decoder-only models while adapting their architecture for bidirectional context encoding. By enabling bidirectional attention and training with masked prediction, the models learn to encode complete contextual information rather than just future-predictive representations. The contrastive learning step further refines these representations for embedding tasks by learning to distinguish between semantically similar and dissimilar pairs.

## Foundational Learning
- **Bidirectional attention mechanisms**: Required to capture full contextual information from both directions in text sequences. Quick check: Verify attention masks allow tokens to attend to all other tokens in the sequence.
- **Masked language modeling**: Trains models to predict masked tokens using bidirectional context, essential for encoding tasks. Quick check: Ensure the masking strategy covers sufficient token positions during training.
- **Contrastive learning objectives**: Improves embedding quality by learning to pull similar pairs together and push dissimilar pairs apart. Quick check: Validate that positive and negative pairs are properly sampled and balanced.
- **Architecture adaptation**: Converting decoder-only to encoder-style requires careful attention mask modifications. Quick check: Confirm the attention mechanism behaves bidirectionally during inference.
- **Parameter-efficient fine-tuning**: The conversion preserves most pre-trained weights while adapting to new objectives. Quick check: Monitor parameter updates to ensure only necessary modifications occur.
- **Benchmark evaluation**: MTEB provides standardized evaluation across multiple embedding tasks. Quick check: Verify all task-specific preprocessing and evaluation protocols are correctly implemented.

## Architecture Onboarding
**Component map:** Bidirectional attention module -> Masked prediction head -> Contrastive learning objective -> MTEB evaluation suite

**Critical path:** Input text -> Bidirectional attention encoding -> Masked token prediction (pre-training) -> Contrastive learning (fine-tuning) -> Embedding extraction -> MTEB task evaluation

**Design tradeoffs:** The method trades some language modeling capability for encoding performance, but maintains most pre-trained knowledge. The bidirectional attention modification is minimal but crucial for encoding effectiveness.

**Failure signatures:** Poor performance may indicate insufficient contrastive learning, incorrect attention mask implementation, or inadequate masked prediction training. Models may also overfit to MTEB-specific patterns if not properly regularized.

**First experiments:** 1) Verify bidirectional attention works by checking self-attention patterns on simple sequences. 2) Test masked prediction accuracy on held-out data before contrastive learning. 3) Evaluate embedding quality on a small subset of MTEB tasks to establish baseline performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Unsupervised performance gains are sensitive to contrastive learning hyperparameters and may not be stable across different random seeds
- Training requires substantial compute resources, limiting accessibility for researchers without GPU infrastructure
- The architectural conversion may alter semantic properties of the original language models in ways not fully characterized

## Confidence
**High:** Core claim that decoder-only LLMs can be effectively converted to text encoders with strong performance
**Medium:** Stability and reproducibility of unsupervised improvements across different settings
**Low:** Characterization of semantic differences between converted models and traditional encoders

## Next Checks
1. Test model stability across 5 different random seeds to quantify variance in MTEB performance
2. Evaluate performance on out-of-distribution datasets not included in MTEB to assess generalizability
3. Compare attention patterns between original decoder-only models and converted encoder-style models to characterize architectural changes