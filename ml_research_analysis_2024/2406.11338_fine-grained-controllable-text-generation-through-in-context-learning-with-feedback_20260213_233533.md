---
ver: rpa2
title: Fine-grained Controllable Text Generation through In-context Learning with
  Feedback
arxiv_id: '2406.11338'
source_url: https://arxiv.org/abs/2406.11338
tags:
- sentence
- dependency
- tree
- text
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for fine-grained controlled text
  generation using in-context learning (ICL) with feedback. The core approach involves
  rewriting sentences to match specific linguistic feature values (e.g., dependency
  depth, word count, difficult words) by combining input sentence analysis, LLM-generated
  output analysis, and a feedback loop with an external validator.
---

# Fine-grained Controllable Text Generation through In-context Learning with Feedback

## Quick Facts
- arXiv ID: 2406.11338
- Source URL: https://arxiv.org/abs/2406.11338
- Reference count: 36
- This paper introduces a method for fine-grained controlled text generation using in-context learning with feedback, achieving 81% exact match accuracy for dependency depth rewriting and state-of-the-art grade-level text rewriting (66.7% exact match).

## Executive Summary
This paper presents a novel approach for fine-grained controlled text generation using in-context learning (ICL) with feedback. The method rewrites sentences to match specific linguistic feature values (dependency depth, word count, difficult words) by combining input sentence analysis, LLM-generated output analysis, and a feedback loop with external validators. Results show that the approach achieves 81% exact match accuracy for dependency depth rewriting and matches state-of-the-art performance on grade-level text rewriting (66.7% exact match). The key contribution is demonstrating that ICL can effectively replace fine-tuning for precise linguistic control in text generation tasks.

## Method Summary
The method combines three core ideas: including input sentence analysis in prompts, using a feedback mechanism with external validators to iteratively refine outputs, and employing in-context learning with few examples rather than fine-tuning. For each linguistic feature (dependency depth, dependency length, word count, difficult words), the system predicts target values, generates rewrites using GPT-4o with 5 in-context examples, validates outputs with external validators (dependency parser, word list checker), and iterates with feedback until feature values match targets or 10 iterations are reached. When integrated with a feature value predictor, the system achieves state-of-the-art accuracy for grade-level text rewriting.

## Key Results
- Achieves 81% exact match accuracy for rewriting sentences to target dependency depth values
- Matches state-of-the-art performance on grade-level text rewriting with 66.7% exact match
- Demonstrates that in-context learning can replace fine-tuning for precise linguistic control tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The feedback loop with external validator enables iterative refinement toward target linguistic feature values
- Mechanism: When LLM generates output, external validator computes actual feature values. If values don't match targets, prompt is augmented with true analysis and feedback message, creating new iteration until correct or max iterations reached.
- Core assumption: LLMs can learn from explicit feedback about feature mismatches and adjust output accordingly
- Evidence anchors: [abstract] "Results show that the method achieves 81% exact match accuracy for rewriting to target dependency depth"; [section] "we equip our model with a feedback mechanism...If the feature value differs from the requested one, the LLM is called again, after amending the prompt with the true analysis"
- Break condition: LLM fails to generate correct output after 10 iterations, or external validator cannot accurately compute feature values

### Mechanism 2
- Claim: Input and output sentence analyses provide explicit syntactic information that improves control precision
- Mechanism: For each feature, the prompt includes detailed analysis of input sentence and asks LLM to generate analysis of output sentence. These analyses make feature values explicit in the prompt, guiding LLM generation.
- Core assumption: LLMs can effectively use explicit syntactic analyses to guide generation toward specific feature values
- Evidence anchors: [abstract] "Our approach combines two core ideas. First, we include an analysis of the input sentence in the prompt and ask the LLM to generate an analysis of the output sentence"; [section] "Analyses allow us to incorporate explicit syntactic information into the prompting process"
- Break condition: LLM fails to generate meaningful analyses or ignores analysis information in generation

### Mechanism 3
- Claim: In-context learning with few examples can replace fine-tuning for precise linguistic control
- Mechanism: System uses 5 in-context examples rather than full training corpus. Feature value predictor provides target values, CTG-LF model rewrites using ICL with feedback. This achieves state-of-the-art results without fine-tuning.
- Core assumption: LLMs have sufficient capacity to learn rewriting patterns from few examples when provided with explicit feature specifications and feedback
- Evidence anchors: [abstract] "We show that our model performs accurate rewrites and matches the state of the art on rewriting sentences to a specified school grade level"; [section] "Our contribution is that we can perform this second step for nontrivial linguistic features with ICL"
- Break condition: Performance degrades significantly when reducing in-context examples below 5, or when applying to different domains/languages

## Foundational Learning

- Concept: Dependency tree structures and depth/length calculations
  - Why needed here: System manipulates dependency depth and length as linguistic features, requiring understanding of how these are calculated from dependency trees
  - Quick check question: Given a dependency tree, can you calculate the maximum depth and length? (See Appendix A.6 examples for format)

- Concept: Readability metrics and grade level prediction
  - Why needed here: System uses DLRCS to evaluate grade-level rewriting performance, and feature predictor estimates feature values for target grades
  - Quick check question: What readability metrics contribute to DLRCS, and how would you approximate grade level from linguistic features?

- Concept: In-context learning prompting techniques
  - Why needed here: System relies on ICL with 5 examples and feedback loops, requiring knowledge of effective prompt engineering
  - Quick check question: What are the key differences between zero-shot, few-shot, and chain-of-thought prompting in this context?

## Architecture Onboarding

- Component map: Feature value predictor (decision tree classifier) -> CTG-LF model (GPT-4o with prompting) -> External validator (dependency parser, word list checker) -> DLRCS evaluator (readability scoring) -> Corpus preprocessing pipeline

- Critical path:
  1. Predict feature values for target grade
  2. Generate rewrite using CTG-LF with 5 in-context examples
  3. Validate output features with external validator
  4. If mismatch, augment prompt with feedback and repeat
  5. Score final output with DLRCS

- Design tradeoffs:
  - Fine-tuning vs. in-context learning (flexibility vs. performance)
  - Feedback iterations vs. latency (accuracy vs. speed)
  - Number of in-context examples (performance vs. prompt size)
  - External validator accuracy vs. feedback quality

- Failure signatures:
  - Low EM accuracy despite feedback (validator or prompt issues)
  - DLRCS scores far from target grades (feature predictor issues)
  - System gets stuck in feedback loops (invalid target specifications)
  - Performance degrades on different domains (corpus bias)

- First 3 experiments:
  1. Test CTG-LF with dependency depth only (no feedback) on small sample
  2. Add feedback mechanism and measure accuracy improvement
  3. Integrate feature predictor and test end-to-end grade-level rewriting on subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the in-context learning approach for fine-grained controlled text generation generalize effectively to languages other than English?
- Basis in paper: [inferred] The paper notes that "existing methods for CTG-LF generally use fine-tuning of pretrained language models such as T5" and mentions that "it would be desirable to perform CTG-LF via in-context learning (ICL), as this does not require a training dataset" and could be applicable in cases where data is sparse.
- Why unresolved: The paper only evaluates the approach on English text using the WikiLarge dataset and does not explore its performance on other languages.
- What evidence would resolve it: Testing the method on parallel corpora in other languages (e.g., French, German, Chinese) and comparing performance metrics like exact match accuracy and grade-level adaptation to the English results.

### Open Question 2
- Question: How does the performance of the in-context learning approach compare to fine-tuning methods when larger parallel training corpora are available?
- Basis in paper: [explicit] The paper states that "earlier models for fine-grained rewriting used finetuning" and notes that their in-context learning approach has the advantage of "requiring only some few-shot examples to perform accurate rewriting rather than a large parallel training corpus."
- Why unresolved: The paper does not conduct a direct comparison between their ICL approach and fine-tuning methods when abundant training data is available.
- What evidence would resolve it: Training both a fine-tuned model and the ICL approach on the same large parallel corpus and comparing their performance on controlled text generation tasks.

### Open Question 3
- Question: What is the impact of varying the number of in-context examples on the performance of the controlled text generation system?
- Basis in paper: [explicit] The paper mentions that "by combining our CTG-LFs model with a model that predicts the right values for the linguistic features, we obtain a system that rewrites sentences to a desired grade level" and shows that using 5 in-context examples achieves state-of-the-art results.
- Why unresolved: The paper does not explore how performance changes with different numbers of in-context examples (e.g., 1, 3, 10, 20).
- What evidence would resolve it: Conducting experiments with varying numbers of in-context examples and plotting performance metrics against the number of examples to identify optimal values.

### Open Question 4
- Question: Can the in-context learning approach be extended to control for additional linguistic features beyond dependency depth, length, word count, and difficult words?
- Basis in paper: [inferred] The paper successfully demonstrates control over four specific linguistic features but does not explore whether the approach can generalize to other features like syntactic complexity, semantic similarity, or discourse structure.
- Why unresolved: The paper focuses on a specific set of four linguistic features without testing the limits of what can be controlled through in-context learning.
- What evidence would resolve it: Implementing the ICL approach for additional linguistic features and evaluating whether the model can achieve similar levels of control and accuracy.

## Limitations
- Limited domain applicability: Validated only on WikiLarge text simplification corpus, may not generalize to other domains or text types
- External validator dependency: Performance relies heavily on accuracy of dependency parsers and word list checkers
- Computational overhead: Iterative feedback loop can require up to 10 LLM calls per sentence rewrite, creating significant computational costs

## Confidence
- High confidence: The core mechanism of using in-context learning with feedback to achieve fine-grained control over linguistic features is well-supported by experimental results (81% EM accuracy for dependency depth, 66.7% DLRCS match)
- Medium confidence: The claim that ICL can effectively replace fine-tuning for precise linguistic control is supported but needs broader validation across different features and domains
- Low confidence: The paper's assertion about superiority over existing methods is based on limited comparisons without head-to-head evaluations under identical conditions

## Next Checks
- Validation Check 1: Replicate the system on a different domain (e.g., medical text simplification or news article rewriting) to assess generalizability
- Validation Check 2: Conduct ablation studies on the number of in-context examples and feedback iterations to identify optimal trade-offs
- Validation Check 3: Analyze external validators' error rates and their propagation through the feedback loop by introducing controlled noise