---
ver: rpa2
title: One world, one opinion? The superstar effect in LLM responses
arxiv_id: '2412.10281'
source_url: https://arxiv.org/abs/2412.10281
tags:
- name
- llms
- gini
- language
- count
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether large language models (LLMs) provide
  diverse opinions about celebrated figures across different languages and professions.
  Prompts in ten languages were used to ask three LLMs (GPT-4, Claude-3-Opus, Llama-3.1-70B)
  to identify the "greatest" or "most influential" person in fifteen professions.
---

# One world, one opinion? The superstar effect in LLM responses

## Quick Facts
- arXiv ID: 2412.10281
- Source URL: https://arxiv.org/abs/2412.10281
- Reference count: 40
- Key outcome: LLMs show strong consensus around a small number of globally recognized figures across languages and professions, with science fields showing more agreement than arts/politics

## Executive Summary
This study investigates whether large language models provide diverse opinions about celebrated figures across different languages and professions. Using prompts in ten languages, the researchers asked three LLMs (GPT-4, Claude-3-Opus, Llama-3.1-70B) to identify the "greatest" or "most influential" person in fifteen professions. The analysis revealed a strong "superstar effect" where a small number of figures dominated recognition across languages, with one or a few individuals appearing in over two-thirds of responses for each profession. The findings suggest LLMs tend toward popular opinions and may narrow global knowledge representation, particularly in subjective domains.

## Method Summary
The researchers created prompts combining five adjectives with fifteen professions in English, then translated each prompt into ten target languages. Each translated prompt was submitted to three LLMs (GPT-4, Claude-3-Opus, Llama-3.1-70B) five times with default parameters. Responses were translated back to English and processed with named entity recognition to extract person entities. The analysis calculated novelty scores, Gini coefficients for inequality in name distribution, cosine similarity for language consensus, and Spearman correlation for lexical similarity versus consensus.

## Key Results
- A small number of figures dominated recognition across languages, with one or a few individuals appearing in over two-thirds of responses for each profession
- Isaac Newton was most frequently named for mathematician across all languages, while Gandhi was the top choice for political figure in nearly every language
- Science-related professions showed more agreement (e.g., Alan Turing for computer scientist in 96.4% of responses) while art/politics showed more diversity but still concentrated on a few dominant Western figures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs produce consistent "superstar" outputs because their training data overrepresents globally dominant figures and shared embeddings
- Mechanism: During pretraining, models see more mentions of widely recognized individuals (e.g., Newton, Einstein) than niche or regionally significant ones. Cross-lingual transfer further aligns these patterns, reinforcing consensus
- Core assumption: Training corpora are not uniformly distributed across cultures and professions, biasing toward globally famous individuals
- Evidence anchors: [abstract] "a small number of figures dominating recognition across languages"; [section] "Training datasets may overrepresent globally influential figures or sources from a few dominant cultures"

### Mechanism 2
- Claim: Lexical similarity between languages correlates with response similarity because shared vocabulary signals shared cultural exposure
- Mechanism: Languages with higher lexical overlap likely have overlapping training data sources, leading to similar embeddings and outputs. Cosine similarity on LLM outputs confirms this alignment
- Core assumption: Linguistic similarity is a proxy for cultural and informational overlap in training corpora
- Evidence anchors: [section] "languages with greater lexical similarity tend to have more consensus on which persons should be venerated"; [section] "Spearman correlation between the similarity in modern lexicons and the consensus between languages"

### Mechanism 3
- Claim: Subjective domains (arts, politics) yield more diversity than objective domains (science) because global recognition criteria differ
- Mechanism: Science contributions have universal applicability, so consensus forms around canonical figures. Arts and politics are more culturally contingent, so outputs vary more, though still concentrating on a few dominant figures
- Core assumption: The "universality" of contributions differs by domain, influencing consensus levels
- Evidence anchors: [abstract] "science-related professions showing more agreement ... art/politics showing more diversity"; [section] "accomplishments in some professions are more likely transcend national and linguistic boundaries"

## Foundational Learning

- Concept: Cosine similarity as a measure of vector alignment
  - Why needed here: Used to quantify how similar LLM outputs are across languages and models
  - Quick check question: If two vectors have a cosine similarity of 1.0, what does that say about their angle?

- Concept: Spearman correlation for monotonic relationships
  - Why needed here: Applied to link lexical similarity between languages with output consensus
  - Quick check question: Does Spearman correlation assume linearity between variables?

- Concept: Named Entity Recognition (NER) for extracting persons
  - Why needed here: Extracts the named individuals from LLM responses for frequency analysis
  - Quick check question: What is the primary label used to identify persons in spaCy's NER model?

## Architecture Onboarding

- Component map: Prompt generation → Language translation (GPT-4o) → LLM inference (GPT-4, Claude-3-Opus, Llama-3.1-70B) → Back-translation → NER extraction → Frequency/consensus analysis
- Critical path: Prompt translation → LLM call → Response capture → NER → Aggregate stats
- Design tradeoffs: Using back-translation ensures consistent analysis but may introduce artifacts; Default LLM parameters reflect real-world usage but reduce control over output diversity
- Failure signatures: Low NER recall → missing names in analysis; Translation errors → incorrect name extraction; LLM downtime → incomplete data collection
- First 3 experiments: 1) Vary temperature to test effect on name diversity; 2) Replace back-translation with direct non-English NER; 3) Add a fourth LLM trained on a non-Western corpus for comparison

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do humans exhibit the same "superstar effect" when identifying celebrated figures across different languages and professions as LLMs do?
- Basis in paper: [inferred] The paper suggests comparing LLM results with human responses to assess whether the concentration of names is a technological artifact or reflects genuine human consensus
- Why unresolved: The study only analyzed LLM outputs without human comparison, leaving open whether the observed patterns reflect human cognitive biases or are specific to LLM training data
- What evidence would resolve it: A controlled survey of humans across the same 10 languages and 15 professions would provide direct comparison data to determine if the superstar effect is amplified or introduced by LLMs

### Open Question 2
- Question: How does the degree of lexical similarity between languages correlate with the diversity of cultural perspectives in LLM responses?
- Basis in paper: [explicit] The paper found a significant correlation between lexical similarity and consensus, but didn't analyze whether this reflects cultural convergence or just linguistic overlap
- Why unresolved: While the study established a correlation, it didn't distinguish between genuine cultural similarity and the LLM's tendency to produce similar outputs for linguistically related languages regardless of cultural content
- What evidence would resolve it: Analyzing LLM responses for culturally-specific indicators (names, concepts, values) within language pairs would reveal whether lexical similarity predicts cultural alignment or just linguistic pattern matching

### Open Question 3
- Question: Would training LLMs on more culturally diverse datasets reduce the superstar effect and increase representation of locally significant figures?
- Basis in paper: [inferred] The paper discusses how LLM training data and processes may overrepresent globally influential figures, suggesting that dataset composition influences output diversity
- Why unresolved: The study didn't experiment with different training data compositions to determine if the observed concentration of Western figures is due to training data bias or fundamental LLM architecture
- What evidence would resolve it: Training multiple LLM variants on progressively more culturally diverse datasets and comparing their response diversity across professions would establish whether training data composition directly impacts cultural representation

## Limitations

- The analysis relies on translated responses, which may introduce artifacts in named entity recognition
- Training corpus composition remains unknown, making it difficult to definitively attribute consensus to specific data biases
- The study focuses on Western-centric professions and languages, potentially underrepresenting non-Western perspectives

## Confidence

**High Confidence**: The core finding that LLMs exhibit strong consensus around a small number of globally recognized figures across multiple languages and professions

**Medium Confidence**: The interpretation that lexical similarity between languages correlates with response similarity

**Low Confidence**: The domain-specific claims about why science shows more consensus than arts/politics

## Next Checks

1. **Temperature Sensitivity Test**: Run the same prompts across the three LLMs using varied temperature settings (0.0, 0.7, 1.5) to determine if the superstar effect persists under different levels of output randomness

2. **Direct Non-English Analysis**: Repeat the analysis using NER models trained in each target language rather than relying on back-translation, to verify that translation artifacts aren't artificially inflating consensus

3. **Cross-Corpus Comparison**: Test the same prompts on an LLM trained primarily on non-Western data sources to quantify how training corpus origin affects the concentration of recognized figures