---
ver: rpa2
title: Cartesian Genetic Programming Approach for Designing Convolutional Neural Networks
arxiv_id: '2410.00129'
source_url: https://arxiv.org/abs/2410.00129
tags:
- neural
- genetic
- mutation
- programming
- cartesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a Cartesian Genetic Programming (CGP) approach
  for automated design of Convolutional Neural Networks (CNNs) without using crossover
  operations, relying solely on mutation. The method generates CNN architectures through
  an evolutionary process where mutation operations create offspring networks that
  are evaluated for fitness based on classification accuracy.
---

# Cartesian Genetic Programming Approach for Designing Convolutional Neural Networks

## Quick Facts
- arXiv ID: 2410.00129
- Source URL: https://arxiv.org/abs/2410.00129
- Reference count: 14
- Best performance: 97.92% ± 0.119 accuracy on MNIST and 86.87% ± 0.201 on Fashion-MNIST under 25K budget with mutation rate 0.1

## Executive Summary
This study introduces a Cartesian Genetic Programming (CGP) approach for automated design of Convolutional Neural Networks (CNNs) without using crossover operations, relying solely on mutation. The method generates CNN architectures through an evolutionary process where mutation operations create offspring networks that are evaluated for fitness based on classification accuracy. The experiments tested three computational budgets (25K, 62.5K, and 125K) with different mutation rates on MNIST and Fashion-MNIST datasets, demonstrating that the CGP-based approach can effectively discover competitive CNN architectures without manual design intervention.

## Method Summary
The CGP-CNN approach uses an evolutionary algorithm to automatically design CNN architectures through mutation operations alone. Each CNN architecture is represented as a graph where nodes correspond to layers and edges represent data flow. The evolutionary process starts with initial random architectures and generates offspring through mutation operations that modify layer types, parameters, or connections. Fitness evaluation is based on classification accuracy on the target dataset. The search explores the architecture space without crossover operations, using three different computational budgets and varying mutation rates to assess performance tradeoffs.

## Key Results
- Achieved 97.92% ± 0.119 accuracy on MNIST with 25K computational budget and mutation rate 0.1
- Achieved 86.87% ± 0.201 accuracy on Fashion-MNIST with 25K computational budget and mutation rate 0.1
- Best performance occurred at the lowest computational budget (25K), suggesting potential scalability limitations

## Why This Works (Mechanism)
The CGP approach works by representing CNN architectures as graphs and using mutation-based evolution to explore the architecture space. Each mutation creates variations in layer configurations, connections, and parameters, allowing the evolutionary process to discover architectures that optimize classification performance. The fitness function based on classification accuracy guides the search toward architectures that generalize well on the target dataset.

## Foundational Learning
- **Cartesian Genetic Programming (CGP)**: A graph-based evolutionary algorithm where individuals are represented as directed graphs, needed to understand the architecture representation and evolution mechanism
- **CNN Architecture Design**: Knowledge of convolutional layers, pooling, activation functions, and their combinations, needed to interpret the evolved architectures
- **Mutation Operations**: Understanding how random modifications affect network structure and performance, needed to evaluate the search process
- **Computational Budget**: The concept of limiting the number of fitness evaluations in evolutionary algorithms, needed to interpret performance tradeoffs
- **Evolutionary Search**: The principles of using fitness-based selection and variation to optimize solutions, needed to understand the overall methodology

## Architecture Onboarding
- **Component Map**: Input -> CNN Architecture (nodes/edges) -> Mutation Operations -> Fitness Evaluation -> Selected Architecture -> Output
- **Critical Path**: CGP representation -> Mutation generation -> Architecture construction -> Model training -> Accuracy evaluation -> Selection
- **Design Tradeoffs**: Mutation-only evolution vs. crossover inclusion, computational budget constraints vs. architecture quality, exploration vs. exploitation balance
- **Failure Signatures**: Convergence to suboptimal architectures, premature convergence, architectures that overfit training data, performance degradation with increased budget
- **First Experiments**: 1) Run with 25K budget and mutation rate 0.1 on MNIST, 2) Run with 62.5K budget and mutation rate 0.05 on Fashion-MNIST, 3) Compare architectures discovered at different budget levels

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on evolutionary search without crossover operations may limit diversity of explored architectures and slow convergence
- Computational budget constraints significantly impact performance, with best results at lowest budget suggesting scalability issues
- Limited experimental validation to simple image classification tasks (MNIST and Fashion-MNIST) with no evidence for complex computer vision problems
- Absence of ablation studies examining individual mutation types or architectural decisions

## Confidence
- High confidence in reported experimental results for specific datasets and budgets tested
- Medium confidence in broader claim that CGP approach can "effectively discover competitive CNN architectures" due to limited evaluation scope
- Low confidence in claims about method's scalability or generalizability to real-world applications without additional validation

## Next Checks
1. Test the CGP-CNN approach on more complex datasets (e.g., CIFAR-10, ImageNet) to evaluate scalability and performance on challenging computer vision tasks
2. Compare evolutionary search performance with and without crossover operations to quantify impact of excluding recombination
3. Conduct ablation studies isolating effects of different mutation types and architectural constraints to identify most influential design elements