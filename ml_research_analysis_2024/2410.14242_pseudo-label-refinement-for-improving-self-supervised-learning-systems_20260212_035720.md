---
ver: rpa2
title: Pseudo-label Refinement for Improving Self-Supervised Learning Systems
arxiv_id: '2410.14242'
source_url: https://arxiv.org/abs/2410.14242
tags:
- labels
- learning
- performance
- person
- clustering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of noisy pseudo-labels in self-supervised
  learning systems, which degrade performance in unsupervised domain adaptation tasks
  like person re-identification. The authors propose a Pseudo-label Refinement (SLR)
  algorithm that projects cluster labels from the previous epoch to the current epoch's
  label space using a learned projection matrix, then combines them to generate refined
  soft labels.
---

# Pseudo-label Refinement for Improving Self-Supervised Learning Systems

## Quick Facts
- arXiv ID: 2410.14242
- Source URL: https://arxiv.org/abs/2410.14242
- Reference count: 40
- One-line primary result: SLR improves self-supervised learning by 2.6% mAP and 2.4% Rank-1 accuracy on person re-identification tasks

## Executive Summary
This paper addresses the challenge of noisy pseudo-labels in self-supervised learning systems, particularly for unsupervised domain adaptation tasks like person re-identification. The authors propose a Pseudo-label Refinement (SLR) algorithm that projects cluster labels from previous epochs to the current epoch's label space using a learned projection matrix, then combines them to generate refined soft labels. These soft labels are hierarchically clustered to produce improved hard labels. The method is evaluated on person re-identification using unsupervised domain adaptation across three datasets (Market1501, DukeMTMC-ReID, and PersonX) with two backbone networks (ResNet50 and IBN-ResNet50), consistently improving performance over the baseline ABMT method.

## Method Summary
The SLR algorithm improves self-supervised learning by addressing pseudo-label noise through an iterative refinement process. It uses DBSCAN to generate initial cluster assignments, then projects previous epoch labels to the current epoch space using an IoU-based projection matrix. These projected labels are combined with current epoch labels to create refined soft labels, which are then processed through hierarchical DBSCAN to generate improved hard labels. The algorithm alternates between teacher and student networks, with the teacher's weights updated via EMA. The entire process runs for 50 epochs of target domain adaptation, with dynamic classifiers adapting to the varying number of clusters across epochs.

## Key Results
- SLR achieves up to 2.6% improvement in mAP and 2.4% in Rank-1 accuracy on cross-domain adaptation tasks
- The method consistently outperforms the baseline ABMT algorithm across all three datasets and both backbone networks
- Performance gains are observed across all cross-dataset adaptation scenarios (Market1501→DukeMTMC-ReID, DukeMTMC-ReID→Market1501, and Market1501→PersonX)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The projection matrix bridges the label space inconsistency between consecutive epochs by mapping previous epoch clusters to the current epoch's cluster space.
- Mechanism: The projection matrix P(a,b) is computed using intersection over union (IoU) between pairs of clusters from consecutive epochs. This matrix is then normalized and used to transform previous epoch labels into the current epoch's label space, ensuring that the label refinement process accounts for changes in cluster assignments across iterations.
- Core assumption: The IoU-based projection matrix effectively captures the semantic similarity between clusters across epochs, even when the number of clusters differs.
- Evidence anchors:
  - [abstract] "The cluster labels from the previous epoch are projected to the current epoch cluster-labels space and a linear combination of the new label and the projected label is computed as a soft refined label"
  - [section] "A projection matrix is used to project the cluster labels in the previous epoch to the current epoch cluster label space... Let mt and mt−1 be the number of clusters and Γt and Γt−1 be the pseudo-labels at the previous and current epochs in the one-hot encoding scheme"
  - [corpus] Weak evidence. No direct mention of IoU-based projection matrices in neighbor papers, though related work on pseudo-label refinement exists.
- Break condition: If the clustering algorithm produces drastically different cluster assignments between epochs (e.g., due to high domain shift), the IoU-based projection may fail to capture meaningful mappings, leading to degraded refinement quality.

### Mechanism 2
- Claim: Hierarchical clustering on soft labels extracts more discriminative cluster boundaries than simple thresholding, improving pseudo-label quality.
- Mechanism: Instead of using the maximum value in soft labels to generate hard labels, the algorithm applies hierarchical DBSCAN to the refined soft labels. This clustering step better utilizes the probabilistic information embedded in the soft labels, producing more accurate hard labels that reflect the underlying data structure.
- Core assumption: The soft labels contain sufficient discriminative information to enable effective hierarchical clustering, and the reduced dimensionality of soft labels makes HDBSCAN computationally feasible and effective.
- Evidence anchors:
  - [abstract] "In contrast to the common practice of using the maximum value as a cluster/class indicator, we employ hierarchical clustering on these soft pseudo-labels to generate refined hard-labels"
  - [section] "To get hard labels... we propose hierarchical clustering instead of just thresholding the soft labels. Clustering at this step improves performance due to better utilization of the information contained in the refined soft labels"
  - [corpus] Moderate evidence. Neighbor papers mention pseudo-label refinement and threshold adjustment, but none specifically discuss hierarchical clustering on soft labels.
- Break condition: If the soft labels are too noisy or lack clear cluster structure, hierarchical clustering may produce unstable or meaningless cluster assignments, potentially degrading performance.

### Mechanism 3
- Claim: The combination of projection-based soft label refinement and hierarchical clustering creates a feedback loop that progressively improves cluster consistency and pseudo-label quality across training epochs.
- Mechanism: The SLR algorithm alternates between generating new cluster assignments, projecting previous epoch labels, refining soft labels through linear combination, and applying hierarchical clustering to produce improved hard labels. This iterative process ensures that label refinement benefits from both current epoch information and historical consistency.
- Core assumption: The improvements from each refinement step compound over successive epochs, leading to convergence toward more accurate pseudo-labels and better feature representations.
- Evidence anchors:
  - [abstract] "This approach better utilizes the information embedded in the soft labels, outperforming the simple maximum value approach for hard label generation"
  - [section] "We observe that using clustering on the soft labels better utilizes the information compared to just using a maximum value for generating hard labels. This step also contributes to performance improvement in a self-supervised learning framework"
  - [corpus] Limited evidence. While neighbor papers discuss pseudo-label refinement, none explicitly describe the iterative refinement loop across epochs.
- Break condition: If the initial cluster quality is extremely poor or the domain shift is too large, the refinement process may not converge to meaningful labels, and performance may plateau or degrade.

## Foundational Learning

- Concept: Intersection over Union (IoU) for cluster similarity measurement
  - Why needed here: IoU is used to compute the projection matrix entries, quantifying the overlap between clusters from consecutive epochs to establish label space mappings
  - Quick check question: How would you compute IoU between two sets A and B, and what does it represent in the context of cluster similarity?

- Concept: Hierarchical clustering (specifically HDBSCAN)
  - Why needed here: HDBSCAN is employed to cluster the refined soft labels, extracting meaningful hard labels from probabilistic information while automatically determining the number of clusters
  - Quick check question: What are the key differences between DBSCAN and HDBSCAN, and why might HDBSCAN be preferred for clustering soft labels?

- Concept: Exponential Moving Average (EMA) for teacher-student weight updates
  - Why needed here: EMA is used to update the teacher network weights based on the student network, maintaining a stable teacher that accumulates knowledge from the student's improvements
  - Quick check question: How does EMA differ from simple averaging in the context of model weight updates, and what are its advantages for teacher-student learning?

## Architecture Onboarding

- Component map: Target domain data → Diverse Features → Clustering → Projection Matrix → Soft Label Refinement → Hard Labels → Dynamic Classifiers → Loss Computation → Student Update → EMA Teacher Update
- Critical path: Target domain data → Diverse Features → Clustering → Projection Matrix → Soft Label Refinement → Hard Labels → Dynamic Classifiers → Loss Computation → Student Update → EMA Teacher Update
- Design tradeoffs:
  - Using DBSCAN for initial clustering vs. other clustering algorithms (computational efficiency vs. cluster quality)
  - Linear combination parameter α (balance between current and projected label information)
  - HDBSCAN parameters (min_cluster_size vs. computational cost and cluster granularity)
  - EMA decay rate (stability of teacher updates vs. responsiveness to student improvements)
- Failure signatures:
  - Degraded performance if projection matrix fails to capture meaningful cluster mappings (e.g., due to high domain shift)
  - Unstable training if hierarchical clustering produces inconsistent hard labels across epochs
  - Over-smoothing if α is set too low, giving excessive weight to projected labels
  - Computational bottlenecks if HDBSCAN parameters are not tuned for the soft label dimensionality
- First 3 experiments:
  1. Verify projection matrix computation: Implement IoU calculation between consecutive epoch clusters and validate the projection matrix normalization
  2. Test soft label refinement: Apply linear combination with different α values and measure the impact on pseudo-label consistency
  3. Evaluate hierarchical clustering: Compare HDBSCAN vs. simple thresholding on soft labels using a small validation set to confirm performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SLR algorithm perform on other self-supervised learning tasks beyond person re-identification, such as image classification or object detection?
- Basis in paper: [explicit] The authors mention that "our SLR algorithm is not limited to person Re-ID but can be applied to enhance any self-supervised learning system."
- Why unresolved: The paper only evaluates the SLR algorithm on person re-identification tasks. Further experiments are needed to assess its effectiveness on other self-supervised learning tasks.
- What evidence would resolve it: Conducting experiments to evaluate the SLR algorithm's performance on various self-supervised learning tasks, such as image classification, object detection, and semantic segmentation, using different datasets and evaluation metrics.

### Open Question 2
- Question: What is the impact of the SLR algorithm on the computational efficiency of self-supervised learning systems?
- Basis in paper: [inferred] The paper does not discuss the computational overhead introduced by the SLR algorithm.
- Why unresolved: The computational cost of the SLR algorithm is not addressed, which is important for practical applications.
- What evidence would resolve it: Analyzing the computational complexity of the SLR algorithm and comparing its runtime with the baseline methods on different datasets and hardware configurations.

### Open Question 3
- Question: How does the SLR algorithm handle the case where the number of clusters changes significantly between consecutive epochs?
- Basis in paper: [explicit] The paper mentions that "the number of generated hard clusters may vary from epoch to epoch, resulting in a varying number of classes."
- Why unresolved: The paper does not provide details on how the SLR algorithm handles drastic changes in the number of clusters between epochs.
- What evidence would resolve it: Conducting experiments to evaluate the SLR algorithm's performance when the number of clusters changes significantly between epochs, and analyzing the impact on the projection matrix and refined pseudo-labels.

## Limitations
- The projection matrix computation may fail under high domain shift conditions
- Hierarchical clustering could produce unstable results if soft labels lack clear cluster structure
- The iterative refinement process assumes progressive improvement but this convergence is not empirically validated
- Performance is evaluated only on person re-identification datasets, limiting generalizability

## Confidence
- High Confidence: The overall framework design and experimental methodology are sound, with clear implementation details for most components.
- Medium Confidence: The theoretical justification for projection matrix computation and hierarchical clustering benefits is reasonable but could benefit from more rigorous analysis.
- Low Confidence: The empirical validation of the iterative refinement loop's convergence and the method's robustness to extreme domain shifts is insufficient.

## Next Checks
1. Test projection matrix robustness: Evaluate the algorithm's performance when applying extreme domain shifts (e.g., between very different person re-identification datasets) to assess whether the IoU-based projection fails.
2. Validate hierarchical clustering stability: Compare the consistency of hard label assignments across epochs using different soft label quality scenarios (e.g., varying noise levels) to confirm the benefit of hierarchical clustering.
3. Analyze refinement convergence: Track pseudo-label quality metrics (e.g., cluster purity, consistency) across training epochs to empirically validate whether the refinement process converges to improved labels.