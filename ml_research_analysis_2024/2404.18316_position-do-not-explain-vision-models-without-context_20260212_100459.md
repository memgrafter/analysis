---
ver: rpa2
title: 'Position: Do Not Explain Vision Models Without Context'
arxiv_id: '2404.18316'
source_url: https://arxiv.org/abs/2404.18316
tags:
- spatial
- context
- vision
- images
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper highlights the critical role of spatial context
  in explaining computer vision models. It argues that current XAI methods, which
  primarily focus on highlighting image regions, fail to capture the importance of
  spatial relationships between objects.
---

# Position: Do Not Explain Vision Models Without Context

## Quick Facts
- arXiv ID: 2404.18316
- Source URL: https://arxiv.org/abs/2404.18316
- Reference count: 32
- Key outcome: This position paper highlights the critical role of spatial context in explaining computer vision models. It argues that current XAI methods, which primarily focus on highlighting image regions, fail to capture the importance of spatial relationships between objects.

## Executive Summary
This position paper argues that current Explainable AI (XAI) methods for computer vision models are fundamentally flawed because they ignore spatial context. The authors demonstrate through examples in autonomous driving, healthcare, and surveillance that understanding the relationships between objects (distance, orientation, containment) is crucial for accurate predictions. They show that popular XAI techniques like LIME, Grad-CAM, and Integrated Gradients fail to provide valid explanations in spatially dependent scenarios, often highlighting individual objects without capturing their spatial relationships. The paper calls for a paradigm shift from focusing on "where" objects are located to understanding "how" objects are oriented towards each other in XAI explanations.

## Method Summary
The authors fine-tune ResNet-50 and ViT base models on VTAB datasets (specifically KITTI and dsprites) to create models that depend on spatial relationships for accurate predictions. They then apply popular XAI techniques including GradientSHAP, Integrated Gradients, Occlusion, Saliency, and LIME to these models' predictions. By analyzing the resulting explanations, they identify systematic failures where XAI methods highlight individual objects but miss the crucial spatial relationships between them. The paper uses synthetic examples and controlled experiments to demonstrate these failures across different types of spatial dependencies including distance, inside/outside relationships, order, and orientation between objects.

## Key Results
- XAI methods consistently fail to capture spatial relationships between objects, instead highlighting individual objects in isolation
- Explanations become vague or misleading in scenarios where spatial context is crucial for correct predictions
- Popular techniques like Grad-CAM, LIME, and Integrated Gradients cannot distinguish between images where object positions are swapped but individual objects remain the same
- The paper demonstrates that current XAI approaches would provide incorrect or incomplete explanations for real-world applications like autonomous driving and medical imaging

## Why This Works (Mechanism)
Current XAI methods focus on attribution - identifying which pixels or regions contribute most to a model's prediction. However, they treat images as collections of independent features rather than structured scenes with spatial relationships. When a model's decision depends on how objects relate to each other (e.g., whether a car is in front of or behind another car), these methods fail because they cannot represent relational information. The spatial context is encoded in the model's learned representations but is not accessible through traditional attribution-based explanations.

## Foundational Learning

**Spatial relationships in vision models**: Understanding how computer vision models encode spatial information like distance, orientation, and containment between objects is crucial because many real-world tasks depend on these relationships rather than just object presence.

*Why needed*: Without grasping spatial relationships, XAI methods cannot explain predictions that depend on how objects are positioned relative to each other.

*Quick check*: Can you identify scenarios where changing object positions while keeping objects the same would change the model's prediction?

**Attribution-based vs. relationship-based explanations**: Traditional XAI methods provide attribution maps showing which pixels matter most, while spatial XAI would need to explain relationships between objects.

*Why needed*: Different explanation types are needed for different types of model reasoning - some predictions depend on individual features, others on relationships.

*Quick check*: Would highlighting a single object be sufficient to explain why a model identified a dangerous traffic situation?

**Vision Transformer spatial encoding**: ViTs use positional embeddings to encode the spatial arrangement of image patches, which differs from CNNs' implicit spatial encoding through convolutional operations.

*Why needed*: Understanding how different architectures encode spatial information helps explain why certain XAI methods might fail with specific model types.

*Quick check*: How do positional embeddings in ViTs differ from the spatial information captured by convolutional layers?

## Architecture Onboarding

**Component map**: Image -> CNN/ViT Backbone -> Spatially-dependent Prediction -> XAI Method (LIME/Grad-CAM/IG) -> Attribution Map

**Critical path**: The critical path for understanding spatial context failures runs from the model's input through its spatial encoding mechanism to the final prediction, then through the XAI method that attempts to attribute importance.

**Design tradeoffs**: Current XAI methods trade off computational efficiency and simplicity for the ability to capture complex spatial relationships. Adding spatial reasoning to explanations would likely increase computational cost and complexity but provide more accurate insights.

**Failure signatures**: When spatial context matters but is ignored, XAI methods will highlight all relevant objects equally regardless of their spatial arrangement, fail to distinguish between spatially different but object-identical images, and provide explanations that don't align with human reasoning about spatial relationships.

**First experiments**:
1. Apply Grad-CAM to a model trained to detect whether one object is inside another - observe that it highlights both objects equally rather than explaining the containment relationship
2. Use LIME on a model that classifies based on object order - note that it provides similar explanations regardless of object arrangement
3. Test Integrated Gradients on a model that predicts based on distance between objects - observe that it attributes importance to individual objects rather than their spatial separation

## Open Questions the Paper Calls Out
None

## Limitations
- The paper relies heavily on synthetic examples rather than real-world datasets to demonstrate spatial context failures
- Proposed paradigm shift lacks concrete implementation details or validated approaches for incorporating spatial context into XAI
- Does not address how incorporating spatial relationships might affect the computational efficiency of XAI methods
- Limited exploration of how different vision architectures (CNNs vs. ViTs) handle spatial context differently

## Confidence
- High confidence: The importance of spatial context in vision tasks and its current underutilization in XAI methods
- Medium confidence: The effectiveness of existing XAI methods in scenarios requiring spatial reasoning
- Medium confidence: The proposed need for a paradigm shift in XAI approaches

## Next Checks
1. Conduct experiments on real-world datasets (e.g., autonomous driving scenarios, medical imaging) to validate the proposed spatial XAI methods and compare their performance against existing techniques.
2. Develop and release a benchmark dataset that explicitly tests spatial context understanding in vision models, including a diverse set of tasks that require spatial reasoning.
3. Implement and evaluate the suggested diverse explanation methods (graph-based, vector-based, and human-centered approaches) to assess their effectiveness in providing spatial context-aware explanations.