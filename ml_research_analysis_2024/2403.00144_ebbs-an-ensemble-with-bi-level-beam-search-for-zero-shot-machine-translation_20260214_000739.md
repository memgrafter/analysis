---
ver: rpa2
title: 'EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine Translation'
arxiv_id: '2403.00144'
source_url: https://arxiv.org/abs/2403.00144
tags:
- translation
- ensemble
- ebbs
- beam
- direct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of zero-shot machine translation,
  where a multilingual model must translate between language pairs it has never seen
  during training. The authors observe that both direct translation and pivoting through
  a high-resource language produce noisy, low-quality results due to lack of supervision
  and error accumulation.
---

# EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine Translation

## Quick Facts
- arXiv ID: 2403.00144
- Source URL: https://arxiv.org/abs/2403.00144
- Authors: Yuqiao Wen, Behzad Shayegh, Chenyang Huang, Yanshuai Cao, Lili Mou
- Reference count: 40
- Primary result: EBBS improves zero-shot translation by allowing ensemble components to explore individually while coordinating through soft voting

## Executive Summary
This paper addresses zero-shot machine translation where models must translate between language pairs unseen during training. The authors propose EBBS (Ensemble with Bi-Level Beam Search), which uses two levels of beam search: individual beam search for each ensemble component and soft voting synchronization at the upper level. This approach allows components to maintain their individual preferences while coordinating through voting, avoiding the over-smoothing of probability averaging methods. Experiments on IWSLT and Europarl datasets show EBBS consistently outperforms direct translation, pivoting, and other ensemble methods across multiple evaluation metrics.

## Method Summary
EBBS is built on multilingual Transformer models trained in English-centric fashion. The method creates an ensemble of translation components including direct translation and pivoting through English (and optionally a second pivot). During decoding, each component performs individual beam search at the lower level, while an upper-level soft voting mechanism maintains a shared beam of promising partial translations. The authors also propose knowledge distillation from EBBS outputs to create faster distilled models that sometimes even outperform the ensemble itself.

## Key Results
- EBBS consistently outperforms direct translation, pivoting, and other ensemble methods across IWSLT and Europarl datasets
- The method is robust to weak ensemble components and works well with various beam sizes
- EBBS-based knowledge distillation produces distilled models that are both faster and sometimes better than EBBS itself
- Performance improvements are shown across multiple metrics including BLEU, chrF2++, TER, and COMET

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EBBS improves zero-shot translation by allowing ensemble components to explore their own predictions while synchronizing through soft voting.
- Mechanism: The bi-level beam search structure enables lower-level exploration (individual beam search per component) and upper-level coordination (soft voting to maintain a shared beam), preventing over-smoothing from probability averaging.
- Core assumption: Individual ensemble components have complementary strengths in different translation paths, and these strengths can be effectively combined through coordinated beam search.
- Evidence anchors: Abstract mentions "soft voting" mechanism; section 3.2 explicitly states EBBS does not average predicted distributions.

### Mechanism 2
- Claim: EBBS-based distillation can outperform the ensemble itself by consolidating knowledge into a single model.
- Mechanism: The sequence-level knowledge distillation loss treats EBBS output as pseudo-groundtruth, allowing the student model to learn consolidated knowledge without ensemble inference overhead.
- Core assumption: EBBS-generated translations capture high-quality patterns that can be distilled effectively into a single model, and the distillation process can smooth out noise while preserving strengths.
- Evidence anchors: Abstract states distillation "does not sacrifice, or even improves, the translation quality"; section 3.3 describes treating EBBS output as pseudo-groundtruth.

### Mechanism 3
- Claim: EBBS is robust to weak ensemble components and works well with different beam sizes.
- Mechanism: The voting mechanism allows EBBS to effectively combine both strong and weak components, and the algorithm maintains performance across various beam size configurations.
- Core assumption: Even weak components can contribute useful information when coordinated through the voting mechanism, and the beam search structure is flexible enough to handle different beam sizes.
- Evidence anchors: Section 4.3 states EBBS works well with both strong and weak components and is robust across beam sizes.

## Foundational Learning

- Concept: Multilingual neural machine translation
  - Why needed here: EBBS operates on multilingual models and requires understanding of zero-shot translation capabilities
  - Quick check question: What enables zero-shot translation in multilingual models trained on English-centric data?

- Concept: Beam search decoding
  - Why needed here: EBBS is built on beam search but extends it with bi-level coordination between ensemble components
  - Quick check question: How does standard beam search differ from the lower-level beam search in EBBS?

- Concept: Knowledge distillation
  - Why needed here: EBBS-based distillation is a key contribution that improves both efficiency and sometimes quality
  - Quick check question: What is the difference between direct, union, and EBBS-based distillation approaches?

## Architecture Onboarding

- Component map:
  - Encoder-decoder Transformer (base multilingual model)
  - Multiple translation paths (direct and pivoting components)
  - EBBS decoder (bi-level beam search with voting)
  - Distillation pipeline (sequence-level knowledge distillation)

- Critical path:
  1. Input sentence encoded by multilingual Transformer
  2. Multiple translation paths processed independently
  3. EBBS coordinates outputs through bi-level beam search
  4. Best sequence selected from shared beam
  5. For distillation: EBBS output used as pseudo-groundtruth for finetuning

- Design tradeoffs:
  - Ensemble complexity vs. performance improvement
  - Beam size vs. computational efficiency
  - Voting method choice (top-Z sum vs. alternatives)
  - Distillation approach (EBBS-based vs. direct/union)

- Failure signatures:
  - Poor performance when ensemble components are too similar
  - Over-smoothing when using probability averaging instead of EBBS
  - Distillation failure when EBBS outputs are too noisy or diverse

- First 3 experiments:
  1. Compare EBBS with direct translation and pivoting baselines on IWSLT and Europarl datasets
  2. Test EBBS with different beam sizes (1, 3, 5, 7) to find optimal configuration
  3. Compare EBBS-based distillation with direct and union distillation approaches on selected language pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EBBS perform when ensemble components have vastly different qualities (e.g., one component is much better than others)?
- Basis in paper: The paper states "our ensemble even surpasses the baseline with 4 weakest pivot translations, each of which is at least 1 BLEU point lower than the baseline" and shows EBBS is flexible with both strong and weak components
- Why unresolved: The paper tested incremental addition of components but didn't specifically test extreme quality disparities between components
- What evidence would resolve it: Experiments varying the quality gap between ensemble components while keeping the number constant

### Open Question 2
- Question: What is the theoretical upper bound on EBBS performance given the quality of its ensemble components?
- Basis in paper: The paper shows EBBS consistently outperforms individual components but doesn't establish theoretical performance limits or convergence properties
- Why unresolved: The paper focuses on empirical results without providing theoretical analysis of EBBS's approximation capabilities or error bounds
- What evidence would resolve it: Mathematical analysis of EBBS's ability to approximate optimal sequence prediction given component error rates

### Open Question 3
- Question: How does EBBS scale with very large ensemble sizes (e.g., 10+ components) in terms of both performance and computational efficiency?
- Basis in paper: The paper notes "the trend shows that it is computationally feasible to build an ensemble of even more components" and tested up to 9 components
- Why unresolved: The paper only tested up to 9 components and didn't explore the performance/computation tradeoff at larger scales
- What evidence would resolve it: Experiments with 10+ ensemble components measuring both BLEU scores and inference time, plus analysis of diminishing returns

## Limitations

- The implementation details of the "soft voting" mechanism are not fully specified, which could lead to variations in reproducibility and performance
- Empirical evaluation covers a limited number of language pairs, which may not generalize to all language pairs or dataset sizes
- The computational overhead of EBBS compared to simpler baselines is not thoroughly analyzed
- Knowledge distillation component lacks detailed ablation studies showing which aspects of EBBS are most important for successful distillation

## Confidence

**High Confidence**: The core contribution of EBBS - that bi-level beam search with individual component exploration and coordinated voting outperforms probability averaging ensembles for zero-shot translation - is well-supported by experimental results across multiple datasets and metrics.

**Medium Confidence**: The claims about EBBS's robustness to weak ensemble components and flexibility with different beam sizes are supported by the experiments but could benefit from more extensive ablation studies.

**Low Confidence**: The general claims about EBBS's applicability to all zero-shot translation scenarios and its superiority over all other ensemble methods are not fully supported, as the experimental scope is limited to specific language pairs and datasets.

## Next Checks

1. Implement and Compare Voting Mechanisms: Implement multiple variations of the upper-level voting mechanism (including top-Z sum, probability averaging, and MBR-style voting) to isolate the specific contribution of EBBS's voting approach and verify that the reported improvements are robust to implementation details.

2. Ablation Study on Component Quality: Systematically vary the quality of ensemble components by training models with different hyperparameters or on subsets of data to quantify how EBBS performance degrades as component quality decreases, testing the robustness claims beyond the reported experiments.

3. Cross-Dataset Generalization Test: Apply EBBS to a third multilingual dataset with different language families and training data characteristics (e.g., WMT or FLORES datasets) to verify that the performance improvements generalize beyond the IWSLT and Europarl datasets used in the paper.