---
ver: rpa2
title: Semantic Prompting with Image-Token for Continual Learning
arxiv_id: '2403.11537'
source_url: https://arxiv.org/abs/2403.11537
tags:
- learning
- prompt
- task
- prompts
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the task-selection dependency problem in prompt-based
  continual learning, where incorrect task prediction during inference leads to performance
  degradation. The proposed method, I-Prompt, eliminates task selection by leveraging
  visual semantic information from image tokens themselves.
---

# Semantic Prompting with Image-Token for Continual Learning

## Quick Facts
- arXiv ID: 2403.11537
- Source URL: https://arxiv.org/abs/2403.11537
- Authors: Jisu Han; Jaemin Na; Wonjun Hwang
- Reference count: 40
- This paper proposes I-Prompt, a task-agnostic continual learning method that eliminates task prediction errors by leveraging visual semantic information from image tokens themselves.

## Executive Summary
This paper addresses the task-selection dependency problem in prompt-based continual learning, where incorrect task prediction during inference leads to performance degradation. The proposed method, I-Prompt, eliminates task selection by leveraging visual semantic information from image tokens themselves. The core idea involves semantic prompt matching that selects prompts based on similarities between image tokens, and image token-level prompting that applies prompts directly to image tokens in intermediate transformer layers. The method achieves competitive performance across four benchmarks (CIFAR-100, CUB-200, ImageNet-R/A) compared to state-of-the-art methods while significantly reducing training time by combining prompt selection and prediction in a single forward pass.

## Method Summary
I-Prompt introduces two key innovations: semantic prompt matching and image token-level prompting. Instead of using task IDs to select prompts, I-Prompt matches prompts to image tokens based on cosine similarity between self-attention keys. Prompts are then directly applied to image tokens in intermediate transformer layers through element-wise addition. The method uses a frozen ViT-B/16 backbone with learnable classifier and prompt pool, training with cross-entropy loss and logit masking to prevent forgetting. The approach combines prompt selection and prediction in a single forward pass, reducing computational overhead while maintaining task-agnostic inference.

## Key Results
- Achieves competitive average accuracy across CIFAR-100, CUB-200, ImageNet-R, and ImageNet-A benchmarks compared to state-of-the-art methods
- Demonstrates superior performance in task-imbalanced scenarios (B100-Inc5, B100-Inc10, B100-Inc20, B50-Inc2, B50-Inc5, B50-Inc10)
- Significantly reduces training time by combining prompt selection and prediction in a single forward pass
- Maintains robust performance across various task distributions without requiring task ID information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: I-Prompt eliminates task prediction errors by matching prompts directly to image tokens rather than task IDs
- Mechanism: Uses semantic prompt matching based on cosine similarity between self-attention keys (hk) and prompt keys (ki) to select appropriate prompts for each image token
- Core assumption: Image tokens contain sufficient semantic information to determine which prompts are most relevant, without needing task ID information
- Evidence anchors:
  - [abstract] "eliminates task prediction by leveraging visual semantic information from image tokens themselves"
  - [section 3.2] "we select the prompts by focusing on image tokens in the internal structure of the transformer layer, rather than selecting task-wise prompts through a query function"
  - [corpus] Weak correlation - no direct evidence found in corpus about semantic matching eliminating task prediction errors
- Break condition: If image tokens lack sufficient semantic discriminability between classes across different tasks, the matching would fail to select appropriate prompts

### Mechanism 2
- Claim: Image token-level prompting directly modifies image tokens rather than just the class token
- Mechanism: Applies prompts to image tokens through element-wise sum operation (hl ⊕ P) in intermediate transformer layers, then uses weighted combination of image tokens for final prediction
- Core assumption: Modifying intermediate image tokens preserves more discriminative information than modifying only the final class token
- Evidence anchors:
  - [section 3.2] "we adopt a method that directly adds the prompts to the image token" and "We apply our I-Prompts to the image tokens through a relatively lightweight sum operation"
  - [abstract] "image token-level prompting that applies prompts directly to image tokens in intermediate transformer layers"
  - [corpus] Weak correlation - no direct evidence found in corpus about intermediate image token modification improving performance
- Break condition: If prompt addition to image tokens causes feature distortion or if the weighted combination fails to properly aggregate image token information

### Mechanism 3
- Claim: Combining prompt selection and prediction in a single forward pass reduces computational overhead and prevents forgetting
- Mechanism: Integrates semantic prompt matching within transformer layers so prompt selection occurs during normal inference rather than requiring separate query pass
- Core assumption: Prompt selection can be computed efficiently using internal transformer representations without degrading performance
- Evidence anchors:
  - [abstract] "significantly reducing training time by combining prompt selection and prediction in a single forward pass"
  - [section 3.2] "we simplify this process by selecting the prompts within the transform layers effectively eliminating the need for extra forward pass"
  - [corpus] Weak correlation - no direct evidence found in corpus about single-pass prompting improving efficiency
- Break condition: If computational savings from single pass are outweighed by more complex internal operations, or if prompt selection accuracy degrades when integrated with prediction

## Foundational Learning

- Concept: Vision Transformer architecture and tokenization
  - Why needed here: I-Prompt operates on image tokens within transformer layers, so understanding ViT's tokenization and layer structure is essential
  - Quick check question: What is the difference between class tokens and image tokens in ViT, and how are they processed through transformer layers?

- Concept: Continual learning and catastrophic forgetting
  - Why needed here: The paper addresses forgetting in class-incremental learning scenarios where models must learn new tasks without forgetting previous ones
  - Quick check question: How does rehearsal-based continual learning differ from rehearsal-free approaches, and what role do prompts play in the latter?

- Concept: Prompt-based learning and prefix tuning
  - Why needed here: I-Prompt builds on prompt-based methods but innovates with image token-level prompting instead of traditional input-level prompting
  - Quick check question: What is the difference between prompt-tuning and prefix-tuning, and how does image token-level prompting differ from both?

## Architecture Onboarding

- Component map: Feature extractor (frozen ViT-B/16) -> Classifier (learnable) -> Prompt pool (learnable prompt-key pairs) -> Semantic matching module (cosine similarity calculation) -> Image token modifier (element-wise sum) -> Final prediction aggregator (weighted combination)
- Critical path: Image → ViT tokenization → Transformer layers with integrated prompt selection → Image token modification → Class token aggregation → Classifier prediction
- Design tradeoffs: Single forward pass efficiency vs. potential complexity of integrating prompt selection within transformer layers; task-agnostic approach vs. potential loss of explicit task information
- Failure signatures: Performance degradation in task-imbalanced scenarios suggests prompt matching isn't robust to class distribution shifts; high training time despite single-pass design suggests computational overhead in prompt selection
- First 3 experiments:
  1. Implement semantic prompt matching on CIFAR-100 to verify prompts can be selected based on image token similarity without task IDs
  2. Add image token-level prompting to verify direct modification of image tokens improves over class token-only prompting
  3. Test task-balanced vs. task-imbalanced scenarios to validate task-agnostic approach performance across different class distributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of I-Prompt scale with extremely large numbers of tasks (e.g., 100+ tasks) compared to existing methods?
- Basis in paper: [explicit] The paper mentions that task prediction becomes more difficult as the number of classes per task increases or task distributions become imbalanced, but doesn't test with very large task numbers.
- Why unresolved: The experiments only evaluate up to 40 tasks (B0-Inc40 setting), leaving open whether the method maintains its advantage with hundreds of tasks.
- What evidence would resolve it: Experiments comparing I-Prompt against state-of-the-art methods across benchmarks with 100+ sequential tasks, measuring both average accuracy and computational efficiency.

### Open Question 2
- Question: Can the semantic prompt matching mechanism be extended to work with other transformer architectures beyond ViT, such as Swin Transformers or ConvNeXt?
- Basis in paper: [inferred] The method relies on token similarity through self-attention keys, which is a general transformer property, but the paper only validates on ViT-B/16.
- Why unresolved: The paper doesn't explore whether the approach generalizes to other architectures that use different attention mechanisms or hierarchical structures.
- What evidence would resolve it: Applying I-Prompt to multiple transformer architectures on the same benchmarks and comparing performance relative to their baseline implementations.

### Open Question 3
- Question: What is the theoretical upper bound on task-agnostic performance, and how close does I-Prompt get to this bound compared to task-dependent methods?
- Basis in paper: [inferred] The paper shows I-Prompt performs competitively without task information, but doesn't establish what the theoretical limits are for task-agnostic vs task-dependent approaches.
- Why unresolved: Without knowing the maximum achievable accuracy in task-agnostic settings, we cannot quantify how much performance is sacrificed by eliminating task prediction.
- What evidence would resolve it: Creating a theoretical framework for task-agnostic continual learning limits and empirically measuring how close I-Prompt comes to these limits compared to optimal task-dependent methods.

## Limitations

- Limited validation on transformer architectures beyond ViT-B/16, raising questions about generalization to other architectures
- Weak direct evidence supporting the core claim that semantic prompt matching eliminates task prediction errors
- No detailed computational complexity analysis comparing single-pass approach to traditional two-pass methods

## Confidence

**High Confidence**: Claims about competitive performance on CIFAR-100, CUB-200, ImageNet-R/A benchmarks compared to state-of-the-art methods. The experimental setup and evaluation metrics are clearly specified, and performance improvements are measurable and reproducible.

**Medium Confidence**: Claims about superior performance in task-imbalanced scenarios. While results show improvements, the analysis doesn't deeply explore why the method performs better or whether this advantage persists across more diverse class distribution shifts.

**Low Confidence**: Claims about semantic prompt matching completely eliminating task prediction errors. The mechanism is plausible but lacks direct ablation evidence showing prompt selection accuracy without task IDs, and the "sufficient semantic information" assumption isn't empirically validated.

## Next Checks

1. **Ablation Study on Prompt Selection Accuracy**: Implement a controlled experiment that measures prompt selection accuracy (comparing selected prompts to ground-truth task IDs) across different benchmarks and task distributions. This would directly validate whether semantic matching eliminates task prediction errors rather than just showing improved end-task performance.

2. **Domain Shift Robustness Test**: Create experiments where new tasks have class distributions or visual domains significantly different from training data. Measure both prompt selection accuracy and final task performance to determine whether the semantic matching mechanism generalizes beyond similar domains.

3. **Computational Complexity Analysis**: Implement timing measurements that compare I-Prompt's single-pass approach against traditional two-pass methods (separate prompt selection then prediction) across different model sizes and batch configurations. Include GPU memory usage analysis to verify the claimed efficiency benefits.