---
ver: rpa2
title: Investigating the translation capabilities of Large Language Models trained
  on parallel data only
arxiv_id: '2406.09140'
source_url: https://arxiv.org/abs/2406.09140
tags:
- plume
- layer
- umap
- dimension
- nllb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PLUME, a set of three 2B-parameter LLMs trained
  exclusively on parallel data to evaluate their translation capabilities. The models
  are trained on a Catalan-centric dataset and achieve competitive performance on
  both supervised and zero-shot translation directions.
---

# Investigating the translation capabilities of Large Language Models trained on parallel data only

## Quick Facts
- arXiv ID: 2406.09140
- Source URL: https://arxiv.org/abs/2406.09140
- Reference count: 40
- Key outcome: Introduces PLUME, a set of 2B-parameter LLMs trained exclusively on parallel data, achieving competitive translation performance with vocabulary size improvements and 47% attention head pruning capability

## Executive Summary
This paper investigates the translation capabilities of decoder-only Large Language Models trained exclusively on parallel data. The authors introduce PLUME, three 2B-parameter models trained on a Catalan-centric dataset with different vocabulary sizes (32k, 128k, 256k). The models achieve competitive performance on both supervised and zero-shot translation directions, demonstrating that parallel data alone can enable effective translation capabilities without iterative fine-tuning or continual pre-training. The study reveals that larger vocabulary sizes improve zero-shot translation quality and that over 47% of attention heads can be pruned with minimal performance loss.

## Method Summary
The authors train three decoder-only transformer models from scratch using a Catalan-centric parallel corpus of 783.6M sentences across 9 languages. Each model uses a different vocabulary size (32k, 128k, 256k) trained with BPE tokenization. The models are trained with Adam optimizer using causal language modeling objective and a context window of 2048 tokens. Training is performed for 1 epoch with equal sampling for Romance languages and oversampling for English, Basque, and German. Evaluation is conducted on FLORES-200 devtest and NTREX-101 using beam search decoding, measuring BLEU, COMET, CHRF, and COMET-KIWI scores against encoder-decoder baselines.

## Key Results
- Larger vocabulary sizes consistently improve zero-shot translation quality by increasing token-level overlap between source and target languages
- Over 47% of attention heads can be pruned with minimal performance loss, suggesting redundancy in the learned representations
- Source language tags in prompts significantly impact translation quality, with varying effects across different language pairs
- PLUME models achieve competitive performance compared to encoder-decoder baselines like NLLB and BSC bilingual models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger vocabulary sizes improve zero-shot translation quality by increasing token-level overlap between source and target languages
- Mechanism: When vocabulary size is larger, each token represents a more specific concept, reducing ambiguity in cross-lingual alignment. This increases the probability that the same token can be used in both source and target sentences, facilitating zero-shot translation.
- Core assumption: Token-level overlap directly correlates with translation quality in zero-shot scenarios
- Evidence anchors:
  - [abstract] "Results show that larger vocabulary sizes improve zero-shot translation quality"
  - [section 4] "The results in Table 1 show that higher vocabulary sizes consistently yield better zero-shot capabilities"
  - [corpus] Weak - no direct corpus evidence provided, but implied by experimental results
- Break condition: When vocabulary size becomes too large, causing overfitting or computational inefficiency without proportional gains in translation quality

### Mechanism 2
- Claim: The source language tag in the prompt provides crucial information for the model to select appropriate cross-lingual representations
- Mechanism: The source language tag acts as a contextual cue that guides attention heads to focus on relevant parts of the source sentence and align them with the target language representations. Different languages rely on this tag to varying degrees based on their linguistic similarity to the pivot language.
- Core assumption: Attention heads can effectively utilize source tags to guide cross-lingual alignment
- Evidence anchors:
  - [abstract] "how does the model leverage prompt information to ensure accurate translations?"
  - [section 4.2] "Source tag importance...Results show varying impacts across different language pairs when the source tag is omitted"
  - [corpus] Weak - no direct corpus evidence, but supported by experimental findings
- Break condition: When the source tag is removed or when languages are linguistically similar enough that the model can infer source language from context alone

### Mechanism 3
- Claim: Attention sink mechanisms in deeper layers allow the model to maintain coherent cross-lingual representations by concentrating attention on specific tokens
- Mechanism: Certain attention heads, particularly in middle and deeper layers, focus predominantly on special tokens like BOS. This creates a stable information flow that preserves contextual information while allowing other heads to perform cross-lingual alignment tasks.
- Core assumption: Attention sink mechanisms are beneficial for maintaining information flow in translation tasks
- Evidence anchors:
  - [abstract] "Additionally, the paper demonstrates that over 47% of attention heads can be pruned with minimal performance loss"
  - [section 4.2] "Interestingly, layers 5, 6, 10 and 11 show coverage uniquely for the BOS token which suggests that all attention mass is given to the BOS token"
  - [corpus] Weak - no direct corpus evidence, but supported by attention analysis
- Break condition: When attention sinks are removed or when the model architecture changes to prevent such concentration of attention

## Foundational Learning

- Concept: Cross-lingual representation alignment
  - Why needed here: The model must learn to map representations from different languages into a shared semantic space to enable translation
  - Quick check question: Can you explain how a model trained on Catalan-centric data can translate between languages it hasn't seen together?

- Concept: Attention mechanisms and coverage metrics
  - Why needed here: Understanding how attention heads focus on different parts of the prompt is crucial for analyzing translation performance
  - Quick check question: What does it mean when an attention head has high coverage for the source sentence but low coverage for the source tag?

- Concept: Vocabulary size and tokenization strategies
  - Why needed here: The choice of vocabulary size directly impacts zero-shot translation performance and cross-lingual representation quality
  - Quick check question: How does increasing vocabulary size affect the probability of token overlap between languages?

## Architecture Onboarding

- Component map:
  Input layer -> 18-layer transformer decoder -> Output layer
  Tokenizers (3 variants: 32k, 128k, 256k) -> Embedding layer

- Critical path:
  Input formatting → Tokenization → Embedding layer → 18 transformer layers → Output generation
  Key operations: Cross-attention between source and target tokens, self-attention for context modeling

- Design tradeoffs:
  - Vocabulary size vs. computational efficiency
  - Model depth vs. training time and overfitting risk
  - Prompt complexity vs. model's ability to utilize contextual information

- Failure signatures:
  - Poor zero-shot performance: Likely issues with cross-lingual alignment or insufficient vocabulary overlap
  - High BLEU score but low COMET score: Possible issues with translation quality beyond surface-level matching
  - Attention heads focusing only on BOS token: May indicate ineffective cross-lingual alignment

- First 3 experiments:
  1. Compare translation quality across different vocabulary sizes on the same dataset
  2. Remove source language tags from prompts and measure performance degradation
  3. Mask attention heads based on coverage metrics and evaluate impact on translation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do larger or language-specific vocabularies impact the performance of decoder-only LLMs in multilingual machine translation?
- Basis in paper: [explicit] The paper found that larger vocabulary sizes consistently improve translation quality across zero-shot directions, suggesting potential benefits of experimenting with even larger or language-specific vocabularies
- Why unresolved: The study focused on comparing fixed vocabulary sizes (32k, 128k, 256k) but did not explore the impact of vocabularies that are tailored to individual languages or those that exceed 256k
- What evidence would resolve it: Experiments comparing decoder-only LLMs with language-specific vocabularies and larger vocabularies (e.g., 512k, 1M) on a diverse set of language pairs and translation tasks

### Open Question 2
- Question: What is the role of "sink heads" that primarily focus on the BOS token in the learned cross-lingual representations of decoder-only LLMs?
- Basis in paper: [explicit] The paper identified "sink heads" that focus mainly on the BOS token and suggested exploring their utility and relationship to the learned cross-lingual representations
- Why unresolved: While the paper observed the existence of these heads and their potential impact on the residual stream, it did not investigate how they contribute to or affect the model's cross-lingual understanding
- What evidence would resolve it: Analysis of the attention patterns and cross-lingual representation space with and without the sink heads, and experiments ablating these heads to observe changes in translation performance and representation alignment

### Open Question 3
- Question: How does model scale and data availability affect the translation capabilities of decoder-only LLMs across diverse languages and scripts?
- Basis in paper: [inferred] The paper's limitations section acknowledges that the study did not explore the impact of model scale and data availability on translation across diverse languages and scripts
- Why unresolved: The study was limited to a 2B-parameter model trained on a Catalan-centric dataset with a focus on Western, Latin-script languages, leaving the impact of larger models and more diverse data unexplored
- What evidence would resolve it: Experiments training larger decoder-only LLMs (e.g., 7B, 13B parameters) on datasets with a wider variety of languages, scripts, and data volumes, followed by evaluating their translation performance on these diverse language pairs

## Limitations

- The Catalan-centric dataset may create a pivot-based translation bias that doesn't generalize to more balanced multilingual corpora
- Evaluation is limited to FLORES-200 and NTREX-101 benchmarks without exploring other domains or real-world deployment scenarios
- The study doesn't adequately address whether observed phenomena (vocabulary effects, attention patterns) would hold for different pivot languages or non-pivot-based training setups

## Confidence

- **High Confidence**: Empirical results showing vocabulary size improvements on zero-shot translation and the 47% head pruning analysis are well-supported by data and methodology
- **Medium Confidence**: Claims about attention mechanisms and source tag importance are supported by experiments but lack deeper theoretical grounding
- **Low Confidence**: Broader implications for general LLM training and translation system design extend beyond what the Catalan-centric experimental setup can support

## Next Checks

1. Replicate the vocabulary size experiments using a non-pivot-based parallel corpus to test if improvements generalize beyond the Catalan-centric setup

2. Conduct comprehensive ablation studies on attention head removal, testing whether the 47% pruning threshold holds across different model sizes and language pairs

3. Extend evaluation to additional translation benchmarks (WMT, TED Talks) and domain-specific datasets to verify if performance advantages persist across diverse conditions