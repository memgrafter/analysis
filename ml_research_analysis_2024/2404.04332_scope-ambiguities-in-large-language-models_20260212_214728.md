---
ver: rpa2
title: Scope Ambiguities in Large Language Models
arxiv_id: '2404.04332'
source_url: https://arxiv.org/abs/2404.04332
tags:
- scope
- reading
- scores
- human
- ambiguous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) treat
  scope ambiguities in sentences. Scope ambiguities arise when multiple semantic operators
  with overlapping scope create multiple possible interpretations of a sentence.
---

# Scope Ambiguities in Large Language Models

## Quick Facts
- arXiv ID: 2404.04332
- Source URL: https://arxiv.org/abs/2404.04332
- Authors: Gaurav Kamath; Sebastian Schuster; Sowmya Vajjala; Siva Reddy
- Reference count: 15
- Key outcome: Large language models, particularly more powerful ones like GPT-3.5 and Llama 2 at 70B, can successfully identify human-preferred readings of scope-ambiguous sentences with over 90% accuracy, and are sensitive to the presence of multiple readings in such sentences.

## Executive Summary
This paper investigates how large language models (LLMs) handle scope ambiguities—sentences with multiple possible interpretations due to overlapping semantic operators. The authors introduce novel datasets with 1,000 annotated scope-ambiguous sentences and conduct experiments to assess whether LLMs exhibit similar scope reading preferences to humans and whether they are sensitive to ambiguity. Results show that more powerful LLMs can successfully identify human-preferred readings with high accuracy, and several LLMs distinguish scope-ambiguous sentences from similar non-ambiguous ones. These findings suggest that LLMs can capture scope-related phenomena and integrate world knowledge in their interpretations.

## Method Summary
The authors introduced novel datasets containing 1,000 unique scope-ambiguous sentences annotated for human judgments. They conducted two main experiments: Experiment 1A, a Q&A task where models choose between surface and inverse scope readings, and Experiment 2A, which compares probabilities assigned by models to different continuations given scope-ambiguous and control sentences. The experiments tested various LLMs including GPT-2, GPT-3/3.5, Llama 2, and GPT-4. The goal was to assess whether LLMs exhibit similar scope reading preferences to humans and whether they are sensitive to the presence of multiple readings in scope-ambiguous sentences.

## Key Results
- More powerful LLMs, like GPT-3.5 and Llama 2 at 70B, can successfully identify human-preferred readings with over 90% accuracy in some cases.
- Several LLMs are sensitive to the meaning ambiguity in scope-ambiguous sentences, distinguishing them from similar non-ambiguous sentences.
- These findings suggest that LLMs can capture scope-related phenomena and integrate world knowledge in their interpretations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can integrate world knowledge with semantic structure to resolve scope ambiguities similarly to humans.
- Mechanism: LLMs trained on large corpora implicitly learn patterns linking syntactic scope configurations to real-world plausibility, allowing them to infer preferred readings without explicit logical form parsing.
- Core assumption: The distributional statistics in pretraining data encode both syntactic scope cues and pragmatic context necessary for disambiguation.
- Evidence anchors:
  - [abstract] "These ambiguities offer rich insights into the interaction between semantic structure and world knowledge in language processing."
  - [section 4.5] "The deeper implication of some of the models' high performance, however, is that LLMs can not only capture different types of readings—surface and inverse, which correspond to different semantic structures—but also integrate background world knowledge in their behavioural preferences when confronted with scope ambiguous constructions."
- Break condition: If pretraining data lacks sufficient examples of scope-ambiguous sentences with varied contexts, the model may default to shallow syntactic heuristics rather than world-knowledge integration.

### Mechanism 2
- Claim: LLMs distinguish scope-ambiguous sentences from syntactically similar unambiguous ones by tracking multiple valid continuations.
- Mechanism: By assigning non-zero probabilities to multiple semantically coherent follow-ups, models implicitly represent the presence of multiple readings; the ratio of these probabilities reflects sensitivity to ambiguity.
- Core assumption: The model's probability distribution over continuations encodes semantic compatibility beyond surface syntax.
- Evidence anchors:
  - [section 5.1] "We then observe the probabilities the model assigns to two followups to S, labelled F1 and F2. F1 is an acceptable continuation to S only given the inverse scope reading of S, while F2 is an acceptable continuation to S only given the surface scope reading of S."
  - [section 5.5.1] "All models yield positive mean α-scores; and barring the case of GPT-2-small, all are statistically significant at a threshold of p < 0.05."
- Break condition: If model attention collapses to a single dominant reading or if follow-up sentences are not semantically discriminating enough, the probability ratio will not reflect ambiguity.

### Mechanism 3
- Claim: Larger and more advanced LLMs exhibit stronger alignment with human scope reading preferences due to richer internal representations.
- Mechanism: Increased model capacity and more sophisticated pretraining objectives allow capture of nuanced linguistic phenomena, including subtle scope preferences influenced by both syntax and pragmatics.
- Core assumption: Model size and training data diversity correlate with ability to represent complex linguistic structure.
- Evidence anchors:
  - [section 4.5] "models like GPT-3.5, Llama 2 at 70B, and most notably GPT-4—are able to exhibit similar scope reading preferences as humans, with a high level of accuracy."
  - [section 6.2] "models like GPT-4, text-davinci-003 and Llama2-70b show both high performance (all above 85% accuracy in the test setting, with GPT-4 achieving ∼96% accuracy)."
- Break condition: If model scaling does not improve linguistic generalization or if fine-tuning objectives distort natural language understanding, performance gains may plateau or degrade.

## Foundational Learning

- Concept: Scope ambiguity in natural language semantics
  - Why needed here: The paper's experiments hinge on understanding how multiple quantifier interpretations arise and how they are resolved.
  - Quick check question: What distinguishes a surface scope reading from an inverse scope reading in a sentence like "Every farmer owns a donkey"?

- Concept: Interaction between syntactic structure and world knowledge in disambiguation
  - Why needed here: Model performance depends on integrating pragmatic plausibility with formal scope relations.
  - Quick check question: Why does "Every conference attendee ate a Big Mac" favor a surface reading while "Every conference attendee attended a networking event" favors an inverse reading?

- Concept: Probability-based probing of model understanding
  - Why needed here: Experiments use continuation probabilities to infer whether models capture ambiguity rather than explicit classification.
  - Quick check question: How does comparing P(F1|S)/P(F2|S) to P(F1|Sc)/P(F2|Sc) reveal sensitivity to scope ambiguity?

## Architecture Onboarding

- Component map: Input sentence → Prompt framing → Model inference (autoregressive) → Probability extraction → Human baseline comparison → Statistical evaluation
- Critical path: Dataset construction → Prompt template design → Model inference pipeline → Human study coordination → Correlation and significance analysis
- Design tradeoffs: Zero-shot evaluation avoids task-specific fine-tuning but limits control over model attention; larger models yield better performance but increase inference cost
- Failure signatures: Performance near chance level suggests lack of scope sensitivity; no drop in control setting indicates answers driven by option plausibility rather than sentence interpretation
- First 3 experiments:
  1. Replicate Experiment 1A with a new interaction type (e.g., quantifier-adverb) to test generalizability.
  2. Conduct a targeted ablation removing world knowledge cues from follow-ups to isolate syntactic vs pragmatic influence.
  3. Apply the probability ratio method from Experiment 2 to a different ambiguity type (e.g., anaphora) to validate the probing approach.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do contextual factors influence scope reading preferences in language models?
  - Basis in paper: [inferred] The paper mentions that context can affect scope reading preferences but the experiments conducted did not assess how context affects these preferences.
  - Why unresolved: The current work focuses on scope ambiguities in isolation, without considering the influence of surrounding context on interpretation.
  - What evidence would resolve it: Experiments that present scope ambiguous sentences with varying contextual information and measure the impact on language models' preferred interpretations.

- **Open Question 2**: How and where do language models represent scope information?
  - Basis in paper: [inferred] The paper discusses the capacity of language models to capture scope-related phenomena but does not investigate the underlying mechanisms of scope representation.
  - Why unresolved: The focus of the paper is on behavioral analysis rather than model interpretability or probing techniques to understand internal representations.
  - What evidence would resolve it: Using interpretability methods such as causal mediation analysis to identify the specific parts of the model responsible for scope representation and how this information is encoded.

- **Open Question 3**: Do prompting-based methods underestimate the linguistic abilities of language models, particularly in modeling ambiguity?
  - Basis in paper: [explicit] The paper contrasts its findings with those of Liu et al. (2023) and Stengel-Eskin et al. (2023), who found that LLMs struggle to model ambiguity using prompting-based approaches.
  - Why unresolved: The paper demonstrates that probability-based and Q&A-based methods can yield different results compared to prompting-based methods, indicating a potential limitation of the latter.
  - What evidence would resolve it: Further experiments comparing the performance of language models on ambiguity-related tasks using both prompting-based and alternative methods, such as probability analysis or indirect probing techniques.

## Limitations
- The paper's findings rely on indirect inference from model performance rather than direct probes into internal model representations.
- The method of using continuation probabilities to infer model understanding of ambiguity is innovative but indirect, lacking direct probes into internal model representations.
- While the claim that larger models exhibit stronger alignment with human preferences is well-supported, the potential for diminishing returns or plateaus in performance gains is not explored.

## Confidence
- World knowledge integration: Medium confidence - relies on indirect inference from model performance with weak corpus evidence.
- Probability-based probing: Moderate confidence - supported by correlation with human judgments but lacks direct probes into internal representations.
- Model scaling effects: High confidence - consistent improvement in performance across model sizes with statistically significant results.

## Next Checks
1. Conduct an ablation study removing world knowledge cues from follow-up sentences to isolate syntactic vs pragmatic influence.
2. Apply the probability ratio method to a different ambiguity type (e.g., anaphora) to validate the probing approach's robustness.
3. Investigate whether further increases in model size continue to improve performance on scope ambiguity tasks or if there's a point of diminishing returns.