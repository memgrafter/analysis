---
ver: rpa2
title: Reasoning or a Semblance of it? A Diagnostic Study of Transitive Reasoning
  in LLMs
arxiv_id: '2410.20200'
source_url: https://arxiv.org/abs/2410.20200
tags:
- reasoning
- facts
- fact
- answer
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models engage in
  genuine transitive reasoning or rely on implicit cues when solving compositional
  questions. Using diagnostic experiments on QASC and Bamboogle datasets, the authors
  systematically manipulated prompts to control for word/phrase overlaps, named entities,
  and models' prior knowledge.
---

# Reasoning or a Semblance of it? A Diagnostic Study of Transitive Reasoning in LLMs

## Quick Facts
- **arXiv ID**: 2410.20200
- **Source URL**: https://arxiv.org/abs/2410.20200
- **Reference count**: 18
- **Primary result**: Fine-tuning on reasoning datasets enhances authentic transitive reasoning in LLMs, as demonstrated by Flan-T5's resilience to entity manipulation versus LLaMA 2's reliance on named entities.

## Executive Summary
This diagnostic study investigates whether large language models perform genuine transitive reasoning or merely exploit implicit cues when solving compositional questions. The authors systematically manipulated prompts on QASC and Bamboogle datasets to control for word/phrase overlaps, named entities, and models' prior knowledge. By comparing LLaMA 2 and Flan-T5 under controlled conditions, they demonstrate that fine-tuning on reasoning datasets enables more authentic logical inference, while base models rely heavily on surface-level cues like named entities to generate answers.

## Method Summary
The researchers employed a diagnostic approach using two compositional reasoning datasets (QASC and Bamboogle) to test transitive reasoning capabilities. They systematically manipulated prompts by: (1) controlling word/phrase overlaps between premises and hypotheses, (2) obscuring or shuffling named entities, and (3) introducing gibberish tokens to test reasoning depth. The study compared LLaMA 2 (base model) against Flan-T5 (fine-tuned on reasoning datasets) to isolate the effects of reasoning-oriented fine-tuning. Performance metrics were measured across these controlled variations to determine whether models relied on genuine logical inference or surface-level pattern matching.

## Key Results
- Both LLaMA 2 and Flan-T5 leverage word/phrase overlaps for compositional reasoning
- Flan-T5 maintains performance when named entities are obscured or shuffled, suggesting deeper reasoning capability
- LLaMA 2-13b relies heavily on named entities for answer generation, failing when entities are manipulated
- Fine-tuning on reasoning datasets enhances models' ability to perform authentic transitive reasoning beyond surface pattern matching

## Why This Works (Mechanism)
The study demonstrates that fine-tuning on reasoning datasets enables models to develop internal representations that support genuine transitive inference rather than simple pattern matching. Flan-T5's resilience to entity manipulation suggests it has learned to focus on logical relationships between concepts rather than memorizing entity-specific associations. The mechanism appears to involve the model building more abstract representations of relationships during fine-tuning, allowing it to reason about connections even when surface features are altered.

## Foundational Learning
- **Transitive reasoning**: Understanding that if A relates to B and B relates to C, then A relates to C - needed because it's fundamental to logical inference and common in natural language questions
- **Compositional question answering**: Breaking down complex questions into simpler components and combining answers - needed because real-world questions often require multi-step reasoning
- **Named entity recognition**: Identifying and understanding proper nouns and specific terms - needed because models often rely on entities as reasoning anchors
- **Prompt engineering**: Designing input prompts to elicit specific model behaviors - needed to control for variables and test specific hypotheses about reasoning
- **Fine-tuning methodology**: Adapting pre-trained models to specific tasks through additional training - needed to enhance base models' reasoning capabilities
- **Quick check**: Can the model answer novel transitive reasoning questions it hasn't seen during training?

## Architecture Onboarding
**Component Map**: Input Prompt → Token Embedding → Transformer Layers (LLaMA 2/Flan-T5) → Attention Mechanism → Output Prediction
**Critical Path**: Prompt manipulation → Entity recognition → Relationship mapping → Answer generation
**Design Tradeoffs**: Base models prioritize pattern matching for efficiency; fine-tuned models sacrifice some efficiency for reasoning depth
**Failure Signatures**: Heavy reliance on named entities, performance drop when entities are obscured, inability to handle novel entity combinations
**First Experiments**:
1. Test model performance on original QASC/Bamboogle questions without manipulation
2. Introduce controlled word/phrase overlap variations
3. Apply named entity shuffling to baseline performance

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental design cannot completely rule out pattern memorization as explanation for Flan-T5's resilience
- Comparison conflates model size differences (13B vs 11B parameters) with fine-tuning effects
- Study doesn't investigate reasoning capabilities on truly novel domains beyond tested compositional questions

## Confidence
- **High confidence**: Both models leverage word/phrase overlaps in compositional reasoning tasks
- **Medium confidence**: Flan-T5 demonstrates more authentic transitive reasoning than LLaMA 2
- **Low confidence**: Performance differences attributed solely to fine-tuning on reasoning datasets

## Next Checks
1. Conduct ablation studies varying model size while keeping fine-tuning identical
2. Test models on entirely novel compositional domains with no training overlap
3. Implement mechanistic interpretability analysis to identify reasoning mechanisms used by models