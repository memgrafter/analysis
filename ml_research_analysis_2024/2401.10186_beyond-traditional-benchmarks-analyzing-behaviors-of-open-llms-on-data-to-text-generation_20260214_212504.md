---
ver: rpa2
title: 'Beyond Traditional Benchmarks: Analyzing Behaviors of Open LLMs on Data-to-Text
  Generation'
arxiv_id: '2401.10186'
source_url: https://arxiv.org/abs/2401.10186
tags:
- data
- text
- will
- generation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the performance of open large language
  models (LLMs) on data-to-text (D2T) generation tasks. The authors address the challenge
  of evaluating LLMs on D2T generation, which requires generating coherent and relevant
  text from structured data, while avoiding issues with training data contamination
  and the limitations of reference-based evaluation metrics.
---

# Beyond Traditional Benchmarks: Analyzing Behaviors of Open LLMs on Data-to-Text Generation

## Quick Facts
- **arXiv ID**: 2401.10186
- **Source URL**: https://arxiv.org/abs/2401.10186
- **Reference count**: 40
- **Key outcome**: Open LLMs generate fluent D2T text but suffer from semantic accuracy errors in 74-85% of cases

## Executive Summary
This paper investigates how open large language models perform on data-to-text generation tasks, addressing critical evaluation challenges including training data contamination and the limitations of reference-based metrics. The authors introduce QUINTD, a novel dataset collection tool that gathers structured data from public APIs across five domains (weather, products, sports, charts, and entities) to create an ad-hoc benchmark (QUINTD-1) for zero-shot D2T generation testing. Through systematic experiments with three open LLMs (Llama2, Mistral, Zephyr) and GPT-3.5, they evaluate outputs using both human annotators and GPT-4-based metrics, revealing that while models produce fluent and coherent text, semantic accuracy remains problematic with 74-85% of outputs containing errors according to human judgment and 88-93% according to automated metrics.

## Method Summary
The authors develop QUINTD, a data collection tool that automatically retrieves structured data records from public APIs across five domains. They create QUINTD-1 benchmark with 200 records and test three open LLMs (Llama2, Mistral, Zephyr) plus GPT-3.5 using zero-shot prompting with deterministic decoding. Evaluation combines human annotation and GPT-4-based semantic accuracy metrics, focusing on token-level semantic errors. The study maintains consistent prompts across models and domains to enable fair comparison, while addressing contamination concerns through novel data collection.

## Key Results
- All open LLMs generate fluent and coherent text from structured data in zero-shot settings
- Semantic accuracy remains severely limited: 74-85% of outputs contain at least one semantic error according to human annotators
- GPT-4-based metric confirms high error rates (88-93%) across all models and domains
- "INCORRECT" errors are most common, with models making more than two contradictory statements per output on average

## Why This Works (Mechanism)
The paper demonstrates that while LLMs can leverage their general language understanding to produce fluent text from structured inputs, they lack reliable mechanisms for accurate semantic mapping between data structures and natural language. The zero-shot approach exposes fundamental limitations in how models handle data-to-text conversion without explicit training or fine-tuning on similar tasks.

## Foundational Learning
- **Data-to-text generation fundamentals**: Understanding the transformation from structured data to coherent natural language text
- **Zero-shot vs. few-shot learning**: Recognizing the performance differences when models lack task-specific examples
- **Semantic accuracy evaluation**: Measuring how well generated text faithfully represents source data
- **Training data contamination**: Understanding how model pretraining data affects benchmark validity
- **Reference-based vs. reference-free metrics**: Comparing evaluation approaches for NLG tasks
- **Deterministic decoding**: Using consistent generation methods to enable fair model comparison

## Architecture Onboarding

**Component Map**: Structured data (API) -> QUINTD collection tool -> QUINTD-1 benchmark -> LLM input -> Text generation -> Human/GPT-4 evaluation -> Error analysis

**Critical Path**: QUINTD-1 creation → Zero-shot prompt → LLM generation → Semantic accuracy evaluation → Error type classification

**Design Tradeoffs**: Novel data collection vs. dataset size; human annotation vs. automated metrics; zero-shot testing vs. potential few-shot improvements

**Failure Signatures**: High semantic error rates (74-85% human, 88-93% GPT-4), predominance of "INCORRECT" statements, model-specific error patterns across domains

**First 3 Experiments**:
1. Compare zero-shot vs. few-shot performance on same QUINTD-1 dataset
2. Test with longer context windows to evaluate memory effects on accuracy
3. Implement fine-tuning on domain-specific data-to-text pairs

## Open Questions the Paper Calls Out
None

## Limitations
- QUINTD dataset contains only 200 records across five domains, potentially limiting statistical significance
- GPT-4-based metric shows only 76% agreement with human annotations, raising reliability concerns
- Study focuses exclusively on zero-shot performance without exploring few-shot or fine-tuned alternatives
- Semantic accuracy metrics may not fully capture practical utility in real-world applications

## Confidence

**High confidence**: Open LLMs generate fluent and coherent text from structured data in zero-shot settings
**Medium confidence**: Semantic accuracy remains a major limitation across all tested models
**Low confidence**: The specific error rates (74-85% human, 88-93% GPT-4 metric) due to potential inter-annotator variability and metric reliability

## Next Checks
1. Replicate the study with a larger dataset (minimum 1000 records) across the same domains to verify statistical significance of error rate findings
2. Conduct inter-annotator agreement studies with multiple human evaluators to establish baseline consistency for semantic accuracy judgments
3. Test the same models with few-shot prompting approaches to determine if performance improvements exist beyond zero-shot generation