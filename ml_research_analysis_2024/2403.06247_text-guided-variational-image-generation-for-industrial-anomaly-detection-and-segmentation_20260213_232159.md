---
ver: rpa2
title: Text-Guided Variational Image Generation for Industrial Anomaly Detection and
  Segmentation
arxiv_id: '2403.06247'
source_url: https://arxiv.org/abs/2403.06247
tags:
- image
- images
- detection
- performance
- non-defective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a text-guided variational image generation
  method for industrial anomaly detection and segmentation, addressing the challenge
  of obtaining clean data. The method utilizes text information about the target object,
  learned from extensive text library documents, to generate non-defective data images
  resembling the input image.
---

# Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation

## Quick Facts
- arXiv ID: 2403.06247
- Source URL: https://arxiv.org/abs/2403.06247
- Authors: Mingyu Lee; Jongwon Choi
- Reference count: 37
- Primary result: Proposed method achieves state-of-the-art anomaly detection performance using only 1 non-defective image per class

## Executive Summary
This paper introduces a text-guided variational image generation approach for industrial anomaly detection and segmentation, addressing the critical challenge of obtaining clean training data. The method leverages text information about target objects, learned from extensive text libraries, to generate non-defective images that closely resemble input samples. By integrating semantic alignment through CLIP embeddings with variance-aware generation using extended VQGAN, the framework ensures generated images maintain visual fidelity while capturing the diversity of non-defective samples. Experimental results demonstrate significant performance improvements over previous methods, even with minimal non-defective data availability.

## Method Summary
The proposed framework consists of three main components: a keyword-to-prompt generator that creates and selects optimal text prompts using WordNet status words and CLIP similarity, a variance-aware image generator that extends VQGAN to predict per-patch variance values, and a text-guided knowledge integrator that iteratively generates and selects images based on their semantic alignment with the chosen prompt. The method uses generated non-defective images as additional training data for baseline anomaly detection models, enabling effective learning from limited samples while preserving the visual characteristics of target objects.

## Key Results
- Achieves state-of-the-art performance on MVTecAD, BTAD, and MVTec-LOCO datasets using only 1 non-defective image per class
- Outperforms previous methods in anomaly detection AUROC scores across multiple baseline models
- Demonstrates robust generalization across four baseline models and three distinct industrial datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-guided prompt generation increases semantic alignment between generated images and target objects, improving anomaly detection performance.
- Mechanism: The keyword-to-prompt generator creates candidate prompts by combining the object name with predefined status words, then selects the prompt with the highest cosine similarity to the input image's CLIP embedding. This ensures generated images closely match the target object's visual features.
- Core assumption: The semantic similarity between text prompts and images correlates with the visual similarity of generated images to the target object.
- Evidence anchors:
  - [abstract]: "Our method utilizes text information about the target object, learned from extensive text library documents, to generate non-defective data images resembling the input image."
  - [section 4.1.1]: "Using WordNet [11], we construct a set of T -different words {W1, W2, . . . , WT } to obtain the candidate prompts... We determine the best prompt P by using the cosine similarity"
  - [corpus]: "Average neighbor FMR=0.469" (weak, indicating moderate semantic relevance in related work)
- Break condition: If the CLIP embedding similarity does not correlate with actual image similarity, or if WordNet lacks relevant status words for the target object.

### Mechanism 2
- Claim: Variance-aware image generation preserves visual diversity of non-defective samples while avoiding defective patterns.
- Mechanism: The variance-aware image generator extends VQGAN to predict per-patch variance values instead of using uniform variance=1. This allows sampling from N{Ed, Σd} distributions that reflect the input image's appearance diversity.
- Core assumption: The variance of latent vectors in VQGAN correlates with visual diversity in the reconstructed images.
- Evidence anchors:
  - [section 4.1.2]: "we extend the VQGAN architecture to predict the variance of latent variables... Σ∀i E(Ii) is a function estimating the column-wise variance vectors"
  - [section 4.2]: "The variance-aware image generator extracts the latent variables according to the variance value estimated from the given image and the target patch."
  - [corpus]: "None" (no direct corpus evidence for variance-aware VQGAN in anomaly detection)
- Break condition: If estimated variance values do not capture true appearance diversity, or if sampling from N{Ed, Σd} produces artifacts.

### Mechanism 3
- Claim: Text-guided knowledge integration ensures generated images align with textual priors while maintaining visual fidelity to input samples.
- Mechanism: The text-guided knowledge integrator iteratively generates image sets and scores them by cosine similarity between image CLIP features and the best prompt's CLIP feature. It selects images with highest similarity and updates the generator to improve future generations.
- Core assumption: CLIP feature similarity between generated images and text prompts indicates semantic alignment and visual quality.
- Evidence anchors:
  - [section 4.2]: "we generate the images {I+1, I+2, . . . , I+M} by using the variance-aware image generator... The text-guided knowledge integrator repeats the generation of the image set, scoring the image set by the cosine similarity"
  - [section 5.2.1]: "Fig. 8 (Top-Right) is a graph comparing the similarity scores of input images and each prompt, and we can show that the prompt received the highest score (33%)"
  - [corpus]: "Top related titles: Zero-Shot Industrial Anomaly Segmentation with Image-Aware Prompt Generation" (suggests relevance of text-guided approaches)
- Break condition: If CLIP similarity does not correlate with actual image quality, or if iterative updates cause mode collapse.

## Foundational Learning

- Concept: CLIP (Contrastive Language-Image Pre-training)
  - Why needed here: CLIP provides the embedding space where text and image similarity is measured, enabling the prompt selection and knowledge integration mechanisms.
  - Quick check question: How does CLIP compute similarity between a text prompt and an image?

- Concept: VQGAN (Vector Quantized Generative Adversarial Network)
  - Why needed here: VQGAN provides the base architecture for image generation, with its discrete codebook representation enabling variance prediction at the patch level.
  - Quick check question: What role does the vector quantization process play in VQGAN's ability to reconstruct images?

- Concept: Normal distribution sampling with learned variance
  - Why needed here: Learning per-patch variance allows the model to sample from distributions that reflect the input image's appearance diversity rather than uniform sampling.
  - Quick check question: How does sampling from N{μ, σ²} differ from sampling from N{μ, 1} when σ² varies across patches?

## Architecture Onboarding

- Component map: Keyword-to-Prompt Generator → Variance-Aware Image Generator → Text-Guided Knowledge Integrator → Baseline Model Training
- Critical path: Keyword-to-Prompt Generator → Variance-Aware Image Generator → Text-Guided Knowledge Integrator → Baseline Model Training
- Design tradeoffs:
  - More candidate prompts increase semantic alignment but add computation
  - Higher variance prediction adds diversity but may introduce artifacts
  - More iterations improve alignment but increase runtime
- Failure signatures:
  - Generated images look realistic but don't improve detection (prompt selection failing)
  - Generated images are blurry or contain artifacts (variance prediction failing)
  - Generated images don't help baseline model (knowledge integration failing)
- First 3 experiments:
  1. Run keyword-to-prompt generator on a single input image and verify the selected prompt has high CLIP similarity to the image
  2. Generate images using the variance-aware generator with learned variance and compare to standard VQGAN outputs
  3. Run the full pipeline on a simple dataset (e.g., MVTecAD hazelnut) and verify generated images improve baseline performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method handle situations where the input image contains defects or anomalies, and how does this impact the generation of non-defective images?
- Basis in paper: [inferred] The paper discusses generating non-defective images using text information and a variance-aware image generator, but does not explicitly address scenarios where the input image itself contains defects.
- Why unresolved: The paper focuses on the generation of non-defective images and their effectiveness in improving anomaly detection performance, but does not delve into the specifics of handling defective input images.
- What evidence would resolve it: Experimental results or analysis demonstrating the method's performance when the input image contains defects, along with a discussion on how the generation process is affected by the presence of anomalies in the input.

### Open Question 2
- Question: What is the impact of using different text encoders or embedding methods on the performance of the proposed method?
- Basis in paper: [explicit] The paper mentions the use of CLIP encoders for obtaining image and text embedding vectors, but does not explore the effects of using alternative text encoders or embedding methods.
- Why unresolved: While the paper utilizes CLIP encoders, it does not investigate the potential benefits or drawbacks of using other text encoders or embedding methods in the proposed framework.
- What evidence would resolve it: Comparative analysis of the proposed method's performance using different text encoders or embedding methods, along with an evaluation of the impact on anomaly detection and segmentation results.

### Open Question 3
- Question: How does the proposed method scale with the size and complexity of the target object or industrial setting?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the method across various datasets and baseline models, but does not explicitly discuss the scalability of the approach in relation to the size and complexity of the target object or industrial setting.
- Why unresolved: The paper showcases the method's performance in different scenarios, but does not provide insights into how the approach scales when dealing with larger or more complex industrial objects or settings.
- What evidence would resolve it: Experimental results or analysis demonstrating the method's performance as the size and complexity of the target object or industrial setting increase, along with a discussion on the potential challenges and limitations of scaling the approach.

## Limitations
- The method's performance depends on the availability and quality of text descriptions for target objects
- Computational overhead from iterative knowledge integration process may limit real-time applications
- CLIP-based semantic alignment may not generalize well to objects with specialized terminology not captured in WordNet

## Confidence
- High confidence: The overall framework architecture and its application to anomaly detection (supported by strong experimental results across multiple datasets)
- Medium confidence: The effectiveness of the variance-aware generation approach (limited direct evidence in literature)
- Low confidence: The generalization of text-guided prompt selection across diverse industrial domains (depends heavily on quality of available text descriptions)

## Next Checks
1. Test the prompt selection mechanism on industrial objects with specialized terminology not present in standard WordNet to assess robustness
2. Evaluate generated image diversity using established metrics (FID, IS) compared to ground truth non-defective samples
3. Perform ablation studies removing the variance-aware component to quantify its contribution to detection performance