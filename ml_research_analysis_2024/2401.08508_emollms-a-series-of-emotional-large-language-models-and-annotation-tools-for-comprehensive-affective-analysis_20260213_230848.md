---
ver: rpa2
title: 'EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools
  for Comprehensive Affective Analysis'
arxiv_id: '2401.08508'
source_url: https://arxiv.org/abs/2401.08508
tags:
- tasks
- sentiment
- affective
- llms
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EmoLLMs, a series of open-sourced instruction-following
  large language models (LLMs) for comprehensive affective analysis. The authors address
  the limitation of existing models that focus only on classification tasks while
  neglecting regression tasks like sentiment strength and emotion intensity.
---

# EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis

## Quick Facts
- arXiv ID: 2401.08508
- Source URL: https://arxiv.org/abs/2401.08508
- Authors: Zhiwei Liu, Kailai Yang, Tianlin Zhang, Qianqian Xie, Sophia Ananiadou
- Reference count: 40
- Primary result: EmoLLMs outperform open-source LLMs and match ChatGPT/GPT-4 on affective analysis tasks

## Executive Summary
This paper introduces EmoLLMs, a series of instruction-following large language models specifically designed for comprehensive affective analysis. The key innovation addresses the gap in existing models that focus solely on classification tasks while neglecting regression tasks like sentiment strength and emotion intensity. The authors developed a multi-task affective analysis instruction dataset (AAID) with 234K samples covering both classification and regression tasks, and a comprehensive evaluation benchmark (AEB) with 14 diverse tasks. EmoLLMs demonstrate superior performance compared to other open-source LLMs and even exceed GPT-4 in 7 regression tasks and 4 classification tasks, validating their effectiveness as affective annotation tools.

## Method Summary
EmoLLMs are developed by fine-tuning LLaMA2 models (7B and 13B parameters) on the AAID dataset using instruction tuning with AdamW optimizer. The training uses batch size 256, learning rate 1e-6, warm-up ratio 5%, max sequence length 2048, and runs for 3 epochs with early stopping on 2 A100 GPUs. The evaluation compares EmoLLMs against other LLMs and sentiment analysis tools on the AEB benchmark using Pearson correlation coefficient for regression tasks and accuracy/macro-F1 for classification tasks.

## Key Results
- EmoLLMs outperform all other open-source LLMs on the comprehensive AEB benchmark
- EmoLLMs exceed ChatGPT and GPT-4 performance in 7 regression tasks and 4 classification tasks
- Models achieve ChatGPT-level and GPT-4-level generalization capabilities on affective analysis tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EmoLLMs outperform other open-sourced LLMs and even GPT-4 in regression tasks due to fine-tuning on a comprehensive multi-task affective analysis instruction dataset.
- Mechanism: By training on a large dataset (234K samples) covering 3 classification and 2 regression tasks, EmoLLMs learn to predict not just discrete emotion labels but also continuous intensity scores, which are crucial for fine-grained affective analysis.
- Core assumption: The quality and diversity of the instruction dataset (AAID) are sufficient to teach LLMs to handle both classification and regression tasks effectively.
- Evidence anchors:
  - [abstract]: "EmoLLMs are developed by fine-tuning LLMs with AAID. Experimental results show that EmoLLMs outperform all other open-sourced LLMs and sentiment analysis tools on AEB, and surpass ChatGPT and GPT-4 in 7 regression tasks and 4 classification tasks."
  - [section]: "We propose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various affective instruction tasks."
  - [corpus]: Weak evidence. No direct citations of AAID in the corpus.

### Mechanism 2
- Claim: EmoLLMs achieve ChatGPT-level and GPT-4-level generalization capabilities on affective analysis tasks by leveraging the large parameter size and training corpus of LLMs.
- Mechanism: The large parameter size of LLMs allows them to capture complex patterns in the data, enabling them to generalize well to unseen affective analysis tasks from various sources and domains.
- Core assumption: The pre-training of LLMs on a massive corpus provides a strong foundation for learning nuanced emotional and sentiment cues.
- Evidence anchors:
  - [abstract]: "These results demonstrate that EmoLLMs achieve the ChatGPT-level and GPT-4-level generalization capabilities on affective analysis tasks, making them effective tools for affective annotation."
  - [section]: "The experimental results indicate that the series of EmoLLMs overtake all other open-sourced LLMs, and exceed ChatGPT and GPT-4 in 7 regression tasks and 4 classification tasks."
  - [corpus]: Weak evidence. The corpus neighbors do not directly discuss the generalization capabilities of LLMs on affective tasks.

### Mechanism 3
- Claim: EmoLLMs can serve as effective affective annotation tools for annotating data from different platforms and sources due to their strong performance on a comprehensive evaluation benchmark.
- Mechanism: By outperforming existing sentiment analysis tools and open-sourced LLMs on a diverse set of tasks, EmoLLMs demonstrate their ability to provide high-quality affective annotations that can be used to enrich downstream datasets.
- Core assumption: The evaluation benchmark (AEB) is representative of real-world affective analysis tasks and provides a fair comparison of different models.
- Evidence anchors:
  - [abstract]: "EmoLLMs can serve as comprehensive affective annotation tools for annotating data from different platforms and sources."
  - [section]: "Based on AEB, we evaluate EmoLLMs, a variety of open-sourced LLMs, close-sourced LLMs (i.e. ChatGPT and GPT-4), and several sentiment analysis tools."
  - [corpus]: Weak evidence. The corpus neighbors do not discuss the use of LLMs for affective annotation.

## Foundational Learning

- Concept: Understanding the difference between classification and regression tasks in affective analysis.
  - Why needed here: EmoLLMs are designed to handle both classification (e.g., emotion categories) and regression (e.g., sentiment strength) tasks, so it's crucial to understand the distinction and how they are approached differently.
  - Quick check question: Can you explain the key difference between a classification task (e.g., predicting if a tweet is positive or negative) and a regression task (e.g., predicting the sentiment score of a tweet on a scale of 0 to 1)?

- Concept: Knowledge of instruction tuning and its role in adapting LLMs to specific domains or tasks.
  - Why needed here: EmoLLMs are developed through instruction tuning, which involves fine-tuning LLMs on a dataset of instruction-response pairs. Understanding this process is key to grasping how EmoLLMs acquire their affective analysis capabilities.
  - Quick check question: What is instruction tuning, and how does it differ from traditional fine-tuning of LLMs?

- Concept: Familiarity with common evaluation metrics for affective analysis tasks, such as Pearson correlation coefficient, accuracy, and F1-score.
  - Why needed here: The performance of EmoLLMs is evaluated using various metrics on the AEB benchmark. Understanding these metrics is essential for interpreting the results and comparing EmoLLMs to other models.
  - Quick check question: Can you explain what the Pearson correlation coefficient measures and why it is a suitable metric for evaluating regression tasks in affective analysis?

## Architecture Onboarding

- Component map: EmoLLMs (fine-tuned LLMs) <- AAID dataset (234K samples) <- SemEval-2018 Task 1 data; EmoLLMs -> AEB benchmark (14 tasks) -> Evaluation metrics

- Critical path:
  1. Construct AAID by converting raw data from SemEval-2018 Task 1 into instruction-response pairs
  2. Fine-tune various LLMs on AAID to create EmoLLMs
  3. Construct AEB by collecting and formatting affective analysis datasets from various sources
  4. Evaluate EmoLLMs and other models on AEB using appropriate metrics
  5. Analyze the results to assess the performance and generalization capabilities of EmoLLMs

- Design tradeoffs:
  - Using a larger number of LLMs (e.g., LLaMA2, OPT, BLOOM) allows for a more comprehensive comparison but increases the computational cost and complexity of the fine-tuning process
  - Including both classification and regression tasks in AAID and AEB provides a more holistic evaluation of affective analysis capabilities but may require more sophisticated instruction templates and evaluation metrics
  - Constructing AEB from diverse sources and domains enhances the generalizability testing of EmoLLMs but may introduce domain-specific biases or inconsistencies in the data

- Failure signatures:
  - If EmoLLMs do not outperform other open-sourced LLMs or GPT-4 on AEB, it may indicate issues with the quality or diversity of AAID, the fine-tuning process, or the evaluation metrics used
  - If EmoLLMs perform well on tasks similar to those in AAID but poorly on tasks from different domains or sources, it may suggest overfitting to the specific tasks in AAID or a lack of generalization ability
  - If the performance of EmoLLMs varies significantly across different LLMs (e.g., LLaMA2 vs. OPT), it may indicate that the choice of base LLM has a significant impact on the affective analysis capabilities of EmoLLMs

- First 3 experiments:
  1. Fine-tune a smaller LLM (e.g., LLaMA2-7B) on a subset of AAID (e.g., only classification tasks) and evaluate its performance on a corresponding subset of AEB to assess the impact of task complexity on the affective analysis capabilities of EmoLLMs
  2. Fine-tune the same LLM on the full AAID and evaluate its performance on the full AEB to compare the benefits of multi-task learning versus single-task learning in the context of affective analysis
  3. Fine-tune the LLM on AAID and evaluate its performance on a held-out test set from the same domain as AAID (e.g., Twitter data) versus a test set from a different domain (e.g., movie reviews) to assess the generalization ability of EmoLLMs across different sources and domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do EmoLLMs perform on multilingual affective analysis tasks, particularly for languages with limited training data?
- Basis in paper: [inferred] The paper mentions that current EmoLLMs are limited to English text content and lack content from other languages, suggesting potential for improvement in multilingual scenarios
- Why unresolved: The authors did not test or report EmoLLMs' performance on multilingual datasets or languages other than English
- What evidence would resolve it: Experimental results comparing EmoLLMs' performance on affective analysis tasks across multiple languages, including low-resource languages

### Open Question 2
- Question: What is the impact of instruction dataset size on EmoLLMs' performance for different affective analysis tasks?
- Basis in paper: [inferred] The paper used 234K samples but did not systematically explore how varying dataset sizes affect performance across different task types (regression vs classification)
- Why unresolved: The authors did not conduct experiments with different dataset sizes or report performance scaling with dataset size
- What evidence would resolve it: Performance metrics of EmoLLMs trained on systematically varied instruction dataset sizes, showing the relationship between dataset size and task-specific performance

### Open Question 3
- Question: How do EmoLLMs handle ambiguous or context-dependent affective expressions compared to human annotators?
- Basis in paper: [explicit] The paper discusses EmoLLMs' generalization capabilities but does not address their performance on ambiguous cases or compare them to human annotation quality
- Why unresolved: No evaluation was conducted on ambiguous affective expressions or direct comparison with human annotation quality metrics
- What evidence would resolve it: Side-by-side comparison of EmoLLMs' annotations versus human annotators on datasets specifically designed to include ambiguous or context-dependent affective expressions, with inter-annotator agreement metrics

## Limitations

- Limited to English language text content, lacking multilingual capabilities
- Performance claims rely heavily on the quality and representativeness of the AAID dataset
- Comparison against GPT-4 is constrained by unspecified access details and prompt strategies

## Confidence

- **High**: Claims about EmoLLMs outperforming other open-source LLMs on the AEB benchmark
- **Medium**: Claims about achieving ChatGPT-level generalization capabilities
- **Low**: Claims about effectiveness as annotation tools for arbitrary platforms and sources

## Next Checks

1. Conduct independent analysis of AAID's instruction template diversity and annotation consistency to verify claims about comprehensive task coverage

2. Test EmoLLMs on held-out datasets from domains not represented in AAID (e.g., product reviews, news articles) to validate claimed generalization capabilities

3. Systematically vary prompts for GPT-4 and ChatGPT comparisons to establish baseline sensitivity and ensure fair comparison conditions