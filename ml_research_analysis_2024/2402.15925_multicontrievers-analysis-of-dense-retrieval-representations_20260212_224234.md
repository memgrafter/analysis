---
ver: rpa2
title: 'MultiContrievers: Analysis of Dense Retrieval Representations'
arxiv_id: '2402.15925'
source_url: https://arxiv.org/abs/2402.15925
tags:
- gender
- dataset
- ndcg
- performance
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first analysis of information extractability
  from dense retriever representations, using 25 MultiContrievers (initialized from
  MultiBerts) and measuring gender/occupation information via information-theoretic
  probing. Key findings include: 1) MultiContrievers preserve significantly more gender/occupation
  information than MultiBerts (9-47% increase for gender, 19-38% for occupation),
  but have lower gender:occupation ratio; 2) MultiContrievers show much less sensitivity
  to random initialization than MultiBerts, though retrieval performance is highly
  sensitive to both initialization and data shuffle across BEIR datasets; 3) Extractability
  correlates poorly with overall retrieval performance, except for specific query
  subsets requiring gender information; 4) Gender information is useful for answering
  gender-constrained queries but does not cause gender bias, which persists even after
  gender information is removed via INLP.'
---

# MultiContrievers: Analysis of Dense Retrieval Representations

## Quick Facts
- arXiv ID: 2402.15925
- Source URL: https://arxiv.org/abs/2402.15925
- Authors: Seraphina Goldfarb-Tarrant; Pedro Rodriguez; Jane Dwivedi-Yu; Patrick Lewis
- Reference count: 40
- Key outcome: First analysis of information extractability from dense retriever representations using 25 MultiContrievers, finding 9-47% increase in gender information and 19-38% increase in occupation information compared to MultiBerts, with high sensitivity to random initialization.

## Executive Summary
This paper presents the first systematic analysis of information extractability from dense retriever representations, focusing on gender and occupation information in MultiContriever models. The study reveals that MultiContrievers preserve significantly more demographic information than their base MultiBert models, but this extractability shows surprisingly poor correlation with overall retrieval performance. The research also uncovers high sensitivity to random initialization and data shuffling, suggesting that dense retrieval research should test across broader initialization spreads. Notably, the analysis demonstrates that while gender information aids in answering gender-constrained queries, it is not the primary source of gender bias in retrieval systems.

## Method Summary
The study analyzes 25 MultiContriever models initialized from MultiBert checkpoints, trained using contrastive loss on Wikipedia/CCNet chunks. Information-theoretic probing via MDL (Minimum Description Length) measures gender and occupation extractability from representations. The models are evaluated on BEIR benchmark datasets for retrieval performance, and gender bias is assessed using the NQ-gender subset. INLP (Iterative Nullspace Projection) is applied to remove gender information from representations to test causal effects on bias. The analysis examines the relationship between information extractability, retrieval performance, and bias across different random seeds and data shuffles.

## Key Results
- MultiContrievers preserve 9-47% more gender information and 19-38% more occupation information than MultiBerts
- High sensitivity to both random initialization and data shuffle affects retrieval performance across BEIR datasets
- Poor correlation between information extractability and overall retrieval performance, except for gender-constrained queries
- Gender information aids in answering gender-specific queries but is not the primary source of gender bias (bias persists after INLP removal)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Information-theoretic probing captures differences in what retrievers prioritize, which can predict generalization.
- Mechanism: MDL probing measures how efficiently a model's representation can be compressed to predict target labels, revealing shortcut reliance and feature prioritization.
- Core assumption: Extractability of a feature correlates with model reliance on that feature as a heuristic.
- Evidence anchors: MDL probing methodology described in Voita and Titov (2020), compression formula provided in methodology.

### Mechanism 2
- Claim: Random initialization has a large impact on dense retrieval performance, even when contrastive loss is stable.
- Mechanism: Different initialization seeds create different representations despite identical training objectives, leading to performance variance.
- Core assumption: The optimization landscape for dense retrieval is highly sensitive to initialization.
- Evidence anchors: MultiContrievers show wide performance range on BEIR despite identical loss curves; seed 13 shows anomalous behavior.

### Mechanism 3
- Claim: Gender information is useful for answering gender-constrained queries but is not the cause of gender bias.
- Mechanism: INLP removes gender information from representations, revealing that bias persists through other mechanisms (corpus or query structure).
- Core assumption: Allocational bias can exist independently of explicit gender signals.
- Evidence anchors: Gender bias persists after INLP removal; gender information helps answer gender-constrained queries.

## Foundational Learning

- Concept: Information-theoretic probing (MDL probing)
  - Why needed here: To quantify how much specific information is recoverable from dense retriever representations
  - Quick check question: How does MDL probing differ from standard probing accuracy?
    - Answer: MDL probing measures minimum description length needed to encode labels from representation, rewarding efficiency.

- Concept: Contrastive learning for retrieval
  - Why needed here: Dense retrievers are trained via contrastive loss, affecting what information is emphasized
  - Quick check question: Why might contrastive learning encourage shortcutting?
    - Answer: Choice of positive/negative pairs can lead model to rely on spurious correlations.

- Concept: Iterative Nullspace Projection (INLP)
  - Why needed here: To causally test whether a feature causes bias by removing it from representations
  - Quick check question: What does INLP do to the representation space?
    - Answer: INLP learns projection matrix onto nullspace of classifier for target attribute, removing that attribute's information.

## Architecture Onboarding

- Component map: MultiBert -> MultiContriever (fine-tuned with contrastive loss) -> BEIR evaluation -> MDL probing -> INLP analysis
- Critical path: 1) Initialize MultiContriever from MultiBert checkpoint, 2) Train with contrastive loss, 3) Evaluate on BEIR, 4) Probe for gender/occupation extractability, 5) Apply INLP to test causal effects
- Design tradeoffs: MultiContrievers use no new parameters (variation from initialization/data shuffle only), MDL probing requires labeled datasets, INLP is linear
- Failure signatures: Seed 13 shows extreme outlier behavior with anomalous norms, poor temporal generalization, inconsistent ranking despite stable loss
- First 3 experiments: 1) Train MultiContriever from MultiBert seed, check loss curves, 2) Probe trained model on BiasinBios for gender vs occupation compression, 3) Apply INLP to remove gender, evaluate on NQ-gender subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties of random initialization cause anomalous behavior in seed 13, and why does this only manifest during retrieval rather than contrastive training?
- Basis in paper: [explicit] The paper documents seed 13's unusual behavior showing extreme anisotropy and large representation space volume during retrieval, despite normal loss curves during training
- Why unresolved: The analysis only identifies symptoms (anomalous vector space properties) but doesn't explain root cause
- What evidence would resolve it: Systematic analysis of initialization parameters across seeds, comparison of pre-training dynamics

### Open Question 2
- Question: How do information extraction patterns differ between retrievers trained on different self-supervised objectives?
- Basis in paper: [inferred] The paper compares MultiBerts and MultiContrievers but only analyzes one retriever architecture
- Why unresolved: Study focuses exclusively on Contriever's contrastive objective without comparing to other training paradigms
- What evidence would resolve it: Direct comparison of information extractability across retrievers trained with different objectives

### Open Question 3
- Question: What is the relationship between information extractability and performance on retrieval-augmented generation tasks?
- Basis in paper: [explicit] Discussion notes analysis only covers retriever components, not full RAG pipelines
- Why unresolved: Study establishes correlations for standalone retrieval but doesn't investigate how extractability affects RAG composition
- What evidence would resolve it: Empirical studies measuring how extractability affects hallucination rates in end-to-end RAG systems

## Limitations

- The study relies on 25 MultiContriever models from MultiBert checkpoints, which may not capture full diversity of dense retriever architectures
- Probing datasets (BiasinBios and md gender) are relatively small and may not represent real-world complexity
- Focus on specific BEIR datasets may not capture all retrieval scenarios and tasks
- INLP assumes linear relationships between gender information and bias, potentially oversimplifying complex interactions

## Confidence

**High Confidence**: Claims about MultiContrievers preserving significantly more gender/occupation information than MultiBerts are supported by systematic MDL probing across multiple models and datasets.

**Medium Confidence**: Claims about high sensitivity to random initialization are supported by observed performance variance but exact mechanisms require further investigation.

**Low Confidence**: Generalization of findings to other dense retriever architectures and different retrieval tasks beyond BEIR remains uncertain.

## Next Checks

1. **Cross-Architecture Validation**: Train and analyze dense retrievers from different base architectures (e.g., Contriever from RoBERTa) to test whether initialization sensitivity and information preservation patterns generalize beyond MultiBert-based models.

2. **Dataset Diversity Test**: Expand probing to include additional bias datasets and test on retrieval tasks beyond BEIR to assess whether information extractability patterns hold across different domains and bias types.

3. **Non-Linear Bias Analysis**: Apply non-linear causal analysis methods beyond INLP to test whether gender information effects on bias are truly linear and whether other non-linear bias mechanisms exist in dense retriever representations.