---
ver: rpa2
title: 'Interpretable Clustering: A Survey'
arxiv_id: '2409.00743'
source_url: https://arxiv.org/abs/2409.00743
tags:
- clustering
- interpretable
- cluster
- methods
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey of interpretable clustering
  methods, addressing the growing need for transparency in clustering algorithms applied
  to high-stakes domains. It proposes a taxonomy classifying interpretable clustering
  methods along four criteria: process stage (pre-, in-, or post-clustering), interpretable
  model (decision tree, rules, prototype, convex polyhedral, or description-based),
  interpretability level (model-level or feature-level), and data modality (tabular,
  sequential, time series, discrete sequence, graph, text, or image).'
---

# Interpretable Clustering: A Survey

## Quick Facts
- arXiv ID: 2409.00743
- Source URL: https://arxiv.org/abs/2409.00743
- Reference count: 40
- This paper provides a comprehensive survey of interpretable clustering methods, proposing a taxonomy and addressing the growing need for transparency in clustering algorithms applied to high-stakes domains.

## Executive Summary
This survey systematically examines interpretable clustering methods, which aim to produce clusterings that are transparent and explainable to human users. The paper proposes a comprehensive taxonomy classifying these methods along four dimensions: process stage (pre-, in-, or post-clustering), interpretable model type (decision tree, rules, prototype, convex polyhedral, or description-based), interpretability level (model-level or feature-level), and data modality (tabular, sequential, time series, discrete sequence, graph, text, or image). By organizing existing approaches within this framework and establishing conceptual correspondence with supervised explainable AI, the survey provides researchers with a structured understanding of the field and highlights key challenges including the interpretability-clustering quality trade-off, evaluation metrics, and scalability for complex data types.

## Method Summary
The survey employs a systematic literature review approach, identifying and categorizing interpretable clustering methods based on a four-dimensional taxonomy. For each method, the authors analyze the optimization approach, structural metrics used for interpretability, and trade-offs with clustering quality. The paper reviews methods across all three clustering stages - pre-clustering feature selection/extraction, in-clustering integration of interpretability into the objective function, and post-clustering fitting of interpretable models to explain existing clusterings. Methods are evaluated based on their use of interpretable models (decision trees, rules, prototypes, convex polyhedral, or description-based approaches) and their applicability to different data modalities. The survey also establishes conceptual bridges between interpretable clustering and supervised explainable AI by mapping evaluation criteria and methodological approaches between the two domains.

## Key Results
- Proposes a comprehensive taxonomy organizing interpretable clustering methods along four criteria: process stage, interpretable model, interpretability level, and data modality
- Systematically reviews existing approaches at each clustering stage, highlighting methodological distinctions and trade-offs between interpretability and clustering quality
- Establishes conceptual correspondence between supervised explainable AI (XAI) and interpretable clustering, illustrating how established XAI principles can inform unsupervised interpretability research
- Identifies key technical criteria and suggests future research directions, emphasizing the need for interpretable methods tailored to complex data types and the integration of multi-stage interpretability approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey works by structuring interpretable clustering into a unified taxonomy that aligns process stage, interpretable model, interpretability level, and data modality.
- Mechanism: Each method is placed into a cell of a 4-dimensional matrix, so a reader can pick an interpretable model (e.g., decision tree) and immediately see which clustering stage (pre-, in-, post-) and data types it supports.
- Core assumption: The four criteria are orthogonal and together cover all meaningful dimensions of interpretable clustering.
- Evidence anchors:
  - [abstract] The paper proposes a taxonomy classifying interpretable clustering methods along four criteria: process stage, interpretable model, interpretability level, and data modality.
  - [section] The taxonomy is explicitly visualized in Fig. 1 and each criterion is defined with concrete examples.
- Break condition: If future methods require dimensions not captured by these four criteria (e.g., scalability metrics), the taxonomy would need extension.

### Mechanism 2
- Claim: Interpretability is operationalized through quantifiable structural metrics (e.g., number of leaf nodes, rule length, number of prototypes) that can be jointly optimized with clustering quality.
- Mechanism: In-clustering methods embed these metrics as regularizers or constraints in the optimization objective, trading off clustering accuracy for interpretability.
- Core assumption: Structural simplicity correlates with human interpretability in a measurable way.
- Evidence anchors:
  - [section] For decision trees, fewer leaf nodes imply better interpretability; for rules, shorter rules improve simplicity; for prototypes, fewer exemplars increase clarity.
  - [section] Table 2 lists these metrics per interpretable model, showing how they are used in optimization.
- Break condition: If simpler structures do not improve user comprehension in practice, these metrics may mislead optimization priorities.

### Mechanism 3
- Claim: The survey maps interpretable clustering onto supervised XAI taxonomies, allowing cross-pollination of methods and evaluation frameworks.
- Mechanism: By aligning concepts like intrinsic vs. post-hoc, global vs. local, and explanation forms, the survey suggests how XAI evaluation criteria (faithfulness, fidelity, simplicity) can be adapted to unsupervised clustering.
- Core assumption: The conceptual correspondence between labeled prediction and unlabeled grouping is sufficiently strong to transfer evaluation practices.
- Evidence anchors:
  - [section] Table 1 explicitly maps XAI taxonomy dimensions to interpretable clustering counterparts.
  - [section] The correspondence is illustrated with decision tree, rule, and prototype examples.
- Break condition: If clustering lacks ground truth labels, evaluation criteria relying on prediction accuracy may not transfer directly, requiring new validation approaches.

## Foundational Learning

- Concept: Clustering fundamentals (k-means, hierarchical clustering, objective functions)
  - Why needed here: Understanding clustering quality metrics is essential to grasp why interpretability is added as a secondary objective and how trade-offs are managed.
  - Quick check question: What is the within-cluster sum of squared errors (SSE) and how does it guide cluster assignment?

- Concept: Decision tree interpretability (splits, leaf nodes, depth)
  - Why needed here: Decision trees are the most common interpretable model in clustering; knowing how splits define clusters is key to understanding in-clustering and post-clustering methods.
  - Quick check question: How does the path from root to leaf in a decision tree represent a cluster assignment?

- Concept: Multi-objective optimization (Pareto optimality, regularization)
  - Why needed here: Many interpretable clustering methods balance interpretability and clustering quality via joint optimization; grasping this concept is necessary to evaluate method designs.
  - Quick check question: What does it mean to constrain the number of leaf nodes in a clustering tree from an optimization perspective?

## Architecture Onboarding

- Component map:
  - Taxonomy engine: 4D classification (process stage × interpretable model × interpretability level × data modality)
  - Literature repository: Categorized method entries with citations, optimization approach, and structural metrics
  - Conceptual bridge module: Mapping to supervised XAI taxonomies and evaluation criteria
  - Survey synthesis layer: Summary tables and cross-method comparisons

- Critical path:
  1. Identify target clustering stage (pre-, in-, post-)
  2. Select interpretable model family (tree, rule, prototype, etc.)
  3. Filter by data modality and interpretability level
  4. Retrieve candidate methods with their optimization approaches and metrics
  5. Evaluate trade-offs between clustering quality and interpretability

- Design tradeoffs:
  - Broad vs. deep coverage: Including all methods vs. focusing on representative ones with detailed analysis
  - Taxonomy rigidity vs. flexibility: Fixed 4D structure vs. extensible dimensions for emerging method types
  - Evaluation fidelity vs. simplicity: Complex user studies vs. proxy structural metrics

- Failure signatures:
  - Taxonomy cells with no entries indicate gaps in research
  - Missing alignment between interpretability metrics and user comprehension suggests over-reliance on proxy measures
  - Inability to map XAI evaluation criteria to clustering indicates conceptual mismatches

- First 3 experiments:
  1. Build a minimal 4D taxonomy matrix and populate it with 5 representative methods from each clustering stage.
  2. Implement a prototype of the decision-tree-based in-clustering method and measure the trade-off curve between SSE and number of leaf nodes.
  3. Adapt a supervised XAI faithfulness metric (e.g., feature attribution stability) to a post-clustering setting and test on a small tabular dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between interpretability and clustering quality, and how can it be quantified across different interpretable models?
- Basis in paper: [explicit] The paper discusses the trade-off between interpretability and clustering quality as a critical challenge, noting that enhancing one may diminish the other. It also mentions the lack of attention to structural metrics for evaluating interpretability quality.
- Why unresolved: Different interpretable models (decision trees, rules, prototypes, convex polyhedral) prioritize different aspects of interpretability, making it difficult to establish a universal metric. The absence of standardized evaluation criteria for interpretability complicates the quantification of this trade-off.
- What evidence would resolve it: Empirical studies comparing the interpretability and clustering quality of various models across diverse datasets, along with the development of a unified evaluation framework that incorporates both clustering accuracy and interpretability metrics.

### Open Question 2
- Question: How can interpretable clustering methods be extended to handle complex data types such as graphs, time series, and multi-modal data effectively?
- Basis in paper: [explicit] The paper highlights that research on interpretable clustering methods for intricate data types like categorical data, discrete sequences, time series, networks, and multi-view/multimodal data remains limited.
- Why unresolved: Complex data types often lack clear semantics, making it challenging to derive interpretable features or models. Existing methods are primarily designed for tabular data, and adapting them to other data types requires innovative approaches.
- What evidence would resolve it: Development and validation of interpretable clustering methods tailored to specific complex data types, demonstrating their effectiveness and interpretability through rigorous testing and comparison with existing methods.

### Open Question 3
- Question: How can interpretability be integrated into the clustering process without compromising scalability for large-scale datasets?
- Basis in paper: [explicit] The paper mentions the need for interpretable in-clustering methods to balance interpretability and clustering quality while ensuring scalability for real-world data. It also notes the challenge of scalability in exemplar-based clustering methods.
- Why unresolved: Incorporating interpretability often increases computational complexity, which can hinder scalability. Existing methods may not be efficient enough for large-scale datasets, limiting their practical applicability.
- What evidence would resolve it: Performance benchmarks of interpretable clustering methods on large-scale datasets, along with the development of scalable algorithms that maintain interpretability without significant computational overhead.

## Limitations

- The taxonomy assumes orthogonality among the four criteria, which may not hold for emerging methods that blend dimensions
- The paper relies heavily on proxy structural metrics for interpretability rather than user comprehension studies
- The mapping to supervised XAI evaluation frameworks may not fully account for the absence of ground truth labels in clustering tasks

## Confidence

- Taxonomy completeness and coverage: **High** - The four-dimensional structure is well-defined and systematically populated with examples
- Mechanism effectiveness (trade-offs via structural metrics): **Medium** - Structural simplicity metrics are established but their correlation with actual interpretability requires empirical validation
- XAI correspondence validity: **Medium** - Conceptual mapping is logical but practical transferability of evaluation criteria needs verification

## Next Checks

1. Conduct user studies comparing structural simplicity metrics against actual interpretability for at least two interpretable clustering methods across different data modalities
2. Implement and test a multi-stage interpretable clustering pipeline (pre- + in-clustering) to evaluate whether combining stages provides additive interpretability benefits
3. Develop a clustering-specific adaptation of supervised XAI evaluation metrics (e.g., feature attribution stability without ground truth) and validate on benchmark datasets