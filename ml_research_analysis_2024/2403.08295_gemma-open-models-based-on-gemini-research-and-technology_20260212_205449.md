---
ver: rpa2
title: 'Gemma: Open Models Based on Gemini Research and Technology'
arxiv_id: '2403.08295'
source_url: https://arxiv.org/abs/2403.08295
tags:
- gemma
- gemini
- data
- open
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The Gemma model family, introduced by the Gemma Team at Google
  DeepMind, consists of lightweight, open-source language models based on the technology
  used to create the Gemini models. Two sizes are released: a 2 billion parameter
  model for CPU and on-device applications, and a 7 billion parameter model for efficient
  deployment on GPU and TPU.'
---

# Gemma: Open Models Based on Gemini Research and Technology

## Quick Facts
- arXiv ID: 2403.08295
- Source URL: https://arxiv.org/abs/2403.08295
- Reference count: 25
- Key outcome: Lightweight open-source language models (2B and 7B parameters) based on Gemini technology, demonstrating state-of-the-art performance on 11 out of 18 text-based tasks

## Executive Summary
The Gemma model family, introduced by Google DeepMind, consists of lightweight, open-source language models based on the technology used to create the Gemini models. Two sizes are released: a 2 billion parameter model for CPU and on-device applications, and a 7 billion parameter model for efficient deployment on GPU and TPU. Both sizes are available as pre-trained and fine-tuned checkpoints. The models demonstrate strong performance across various academic benchmarks, outperforming similarly sized open models on 11 out of 18 text-based tasks. Gemma also exhibits state-of-the-art understanding and reasoning skills at scale. The responsible release of these models aims to improve the safety of frontier models and enable further innovations in the field of large language models.

## Method Summary
Gemma models are trained using a transformer decoder architecture with GeGLU activations, RMSNorm, and rotary positional embeddings. The 2B model uses multi-query attention for computational efficiency, while the 7B model employs multi-head attention. Training is conducted on TPUv5e hardware using Pathways and GSPMD for large-scale distributed training. Models are trained on up to 6T tokens of primarily English web documents, mathematics, and code, with extensive data filtering for quality and safety. Instruction tuning is performed using supervised fine-tuning followed by reinforcement learning from human feedback (RLHF). Safety mitigations include data filtering, automated benchmarks, and human preference evaluations.

## Key Results
- Outperform similarly sized open models on 11 out of 18 text-based tasks
- Strong performance on question answering, reasoning, math, science, and coding benchmarks
- Demonstrate state-of-the-art understanding and reasoning skills at scale

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-query attention in the 2B model improves computational efficiency without sacrificing performance.
- Mechanism: By using a single key-value head across multiple attention heads, the model reduces memory usage and computational overhead, enabling efficient CPU and on-device deployment.
- Core assumption: Multi-query attention maintains model expressiveness and accuracy at small scales.
- Evidence anchors:
  - [section]: "notably, the 7B model uses multi-head attention while the 2B checkpoints use multi-query attention (with ð‘›ð‘¢ð‘š_ð‘˜ð‘£_â„Žð‘’ð‘Žð‘‘ð‘  = 1), based on ablations that showed that multi-query attention works well at small scales"
  - [corpus]: Weak. The corpus does not contain direct evidence of multi-query attention performance comparisons.
- Break condition: If multi-query attention degrades model performance significantly at the 2B scale or if computational gains are marginal.

### Mechanism 2
- Claim: Instruction tuning with a combination of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) improves model helpfulness and safety.
- Mechanism: SFT aligns the model with human instructions using curated datasets, while RLHF further refines the model's behavior based on human preferences, leading to improved instruction following and reduced harmful outputs.
- Core assumption: Human feedback is a reliable signal for improving model behavior and aligning it with human values.
- Evidence anchors:
  - [section]: "We further finetuned the supervised fine-tuned model using RLHF... The policy was trained to optimize this reward function using a novel reinforcement learning algorithm"
  - [corpus]: Weak. The corpus does not provide direct evidence of the effectiveness of the RLHF approach.
- Break condition: If human feedback introduces biases or if the RLHF process fails to improve model behavior beyond SFT alone.

### Mechanism 3
- Claim: The use of rotary positional embeddings (RoPE) and RMSNorm contributes to stable training and improved performance.
- Mechanism: RoPE provides a more efficient and effective way to encode positional information compared to absolute positional embeddings, while RMSNorm stabilizes the training process by normalizing the input of each transformer sub-layer.
- Core assumption: RoPE and RMSNorm are compatible with the model architecture and training objectives.
- Evidence anchors:
  - [section]: "Rather than using absolute positional embeddings, we use rotary positional embeddings in each layer; we also share embeddings across our inputs and outputs to reduce model size" and "We normalize the input of each transformer sub-layer, the attention layer and the feedforward layer, with RMSNorm (Zhang and Sennrich, 2019) to stabilize the training"
  - [corpus]: Weak. The corpus does not provide direct evidence of the impact of RoPE and RMSNorm on model performance.
- Break condition: If RoPE or RMSNorm negatively impact model performance or if they are incompatible with future architectural modifications.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding the core building blocks of the Gemma model is essential for comprehending its design and functionality.
  - Quick check question: What is the role of the attention mechanism in a transformer model, and how does it contribute to the model's ability to process sequential data?

- Concept: Supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF)
  - Why needed here: These techniques are crucial for aligning the model with human instructions and preferences, improving its helpfulness and safety.
  - Quick check question: How do SFT and RLHF differ in their approach to model alignment, and what are the advantages and disadvantages of each method?

- Concept: Model evaluation and benchmarking
  - Why needed here: Evaluating the model's performance on various tasks and comparing it to other models is essential for assessing its capabilities and limitations.
  - Quick check question: What are some common evaluation metrics used for language models, and how do they provide insights into the model's performance on different tasks?

## Architecture Onboarding

- Component map: Input layer (tokenization and embedding) -> Transformer blocks (multi-head attention, feed-forward network, normalization) -> Output layer (language modeling head) -> Training infrastructure (TPU pods, data sharding, optimizer state sharding)

- Critical path: 1. Data preprocessing and tokenization 2. Model initialization and parameter loading 3. Forward pass through transformer blocks 4. Loss computation and backpropagation 5. Parameter updates using optimizer

- Design tradeoffs: Model size vs. computational efficiency: The 2B model prioritizes efficiency for CPU/on-device deployment, while the 7B model offers better performance for GPU/TPU environments. Multi-query vs. multi-head attention: The 2B model uses multi-query attention for efficiency, while the 7B model uses multi-head attention for potentially better performance. RoPE vs. absolute positional embeddings: RoPE is used for more efficient positional encoding, but it may have limitations in capturing long-range dependencies.

- Failure signatures: Training instability: If the model fails to converge or exhibits erratic behavior during training, it may indicate issues with the learning rate, batch size, or data quality. Poor performance on specific tasks: If the model underperforms on certain benchmarks or fails to generalize well to new data, it may suggest limitations in the model architecture or training process. Hallucinations or unsafe outputs: If the model generates factually incorrect or harmful content, it may indicate issues with the training data, alignment techniques, or safety mitigations.

- First 3 experiments: 1. Evaluate the model's performance on a simple language modeling task (e.g., next token prediction) to verify basic functionality. 2. Test the model's ability to follow instructions on a small set of prompts to assess the effectiveness of the instruction tuning. 3. Measure the model's computational efficiency on different hardware platforms (CPU, GPU, TPU) to validate the design tradeoffs made for each model size.

## Open Questions the Paper Calls Out

- Question: What is the exact training data composition used for the Gemma models, including the proportion of code, web documents, and mathematical content?
- Basis in paper: [explicit] The paper mentions that Gemma models are trained on up to 6T tokens of primarily-English data from web documents, mathematics, and code, but does not provide specific proportions.
- Why unresolved: The paper provides a general description of the data sources but lacks detailed information on the exact composition and proportions of the training data.
- What evidence would resolve it: A detailed breakdown of the training data sources and their respective proportions would resolve this question.

- Question: How does the performance of Gemma models compare to other open-source models when fine-tuned on domain-specific tasks, such as legal or medical text?
- Basis in paper: [inferred] The paper evaluates Gemma models on general academic benchmarks but does not explore performance on specialized domain-specific tasks.
- Why unresolved: The paper focuses on general benchmarks and does not provide insights into the models' performance on specialized domains.
- What evidence would resolve it: Conducting and reporting evaluations of Gemma models on domain-specific tasks like legal or medical text would provide the necessary evidence.

- Question: What are the long-term effects of using Gemma models on downstream applications, particularly in terms of bias and fairness?
- Basis in paper: [explicit] The paper discusses safety and responsibility aspects but does not address long-term impacts on downstream applications.
- Why unresolved: The paper evaluates safety and bias but does not explore the long-term implications of deploying Gemma models in various applications.
- What evidence would resolve it: Longitudinal studies and assessments of bias and fairness in real-world applications using Gemma models would provide insights into their long-term effects.

## Limitations

- Training data composition and filtering criteria are not fully specified, limiting reproducibility
- RLHF implementation details and reward model configurations are not provided
- Limited ablation studies for architectural choices and their impact on performance

## Confidence

- **High confidence**: Basic architectural specifications (transformer decoder with GeGLU, RMSNorm, RoPE embeddings, SentencePiece tokenization) and training infrastructure (TPUv5e, Pathways, GSPMD) are clearly described and verifiable.
- **Medium confidence**: Performance claims on academic benchmarks are well-documented but lack independent verification and detailed methodology for certain evaluation protocols.
- **Low confidence**: The effectiveness of specific safety mitigations, exact data filtering criteria, and RLHF implementation details cannot be fully assessed without access to proprietary datasets and implementation specifics.

## Next Checks

1. **Benchmark reproducibility**: Run Gemma 2B and 7B on the exact same evaluation protocols (MMLU, HumanEval, GSM8K, MATH) using the provided model checkpoints to verify reported performance gains against other open models.
2. **Safety metric verification**: Test the models on the specific safety benchmarks mentioned (RealToxicityPrompts, CrowS-Pairs, Toxigen) to independently assess the effectiveness of the described safety mitigations.
3. **Computational efficiency validation**: Measure actual inference latency and memory usage of both model sizes on CPU, GPU, and TPU hardware to verify the claimed efficiency benefits of multi-query attention in the 2B model and the overall deployment advantages.