---
ver: rpa2
title: Negative Prototypes Guided Contrastive Learning for WSOD
arxiv_id: '2406.18576'
source_url: https://arxiv.org/abs/2406.18576
tags:
- feature
- negative
- learning
- prototypes
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of Weakly Supervised Object Detection
  (WSOD), where only image-level annotations are available instead of precise bounding
  box labels. The key challenge in WSOD is dealing with instance ambiguity and partial
  detection, where the detector may miss objects or only detect discriminative parts
  of objects.
---

# Negative Prototypes Guided Contrastive Learning for WSOD

## Quick Facts
- arXiv ID: 2406.18576
- Source URL: https://arxiv.org/abs/2406.18576
- Authors: Yu Zhang; Chuang Zhu; Guoqing Yang; Siqi Chen
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance on VOC07 and VOC12 with mAP of 57.7% and 54.3% respectively

## Executive Summary
This paper introduces a novel Negative Prototypes Guided Contrastive learning (NPGC) framework to address the challenges of instance ambiguity and partial detection in Weakly Supervised Object Detection (WSOD). The method constructs an online updated global feature bank containing both positive prototypes (typical features of the same category) and negative prototypes (misclassified proposals for categories not present in the image). Through contrastive learning, the framework attracts same-class samples closer while repelling different-class samples in the embedding space. The approach also includes a pseudo label sampling module to mine reliable instances and discard overfitted partial detections, achieving state-of-the-art performance on VOC07 and VOC12 datasets.

## Method Summary
The NPGC framework addresses WSOD by constructing a global feature bank with positive and negative prototypes from the entire dataset. The method uses a context-based feature extraction module to obtain more effective feature representations by incorporating location information and surrounding context. A pseudo label sampling module mines missing instances and discards overfitted partial detections based on feature similarity with prototypes. Finally, contrastive learning optimizes proposal feature representations by attracting same-class samples and repelling different-class samples in the embedding space. The framework is trained using VGG16 backbone and evaluated on Pascal VOC datasets, demonstrating significant improvements over previous WSOD methods.

## Key Results
- Achieves state-of-the-art performance with mAP of 57.7% on VOC07 and 54.3% on VOC12
- Outperforms previous methods by leveraging both positive and negative prototypes for contrastive learning
- Effectively addresses instance ambiguity and partial detection problems in WSOD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed method leverages both positive and negative prototypes to improve feature discrimination in weakly supervised object detection.
- Mechanism: By constructing a global feature bank containing both positive prototypes (typical features of the same category) and negative prototypes (misclassified proposals for categories not present in the image), the method can attract same-class samples closer and repel different-class samples in the embedding space using contrastive learning.
- Core assumption: Instances of the same category share similar characteristics and can be represented by prototypes, while negative prototypes contain valuable category-specific discriminative features that help distinguish between similar categories.
- Evidence anchors:
  - [abstract]: "Unlike other methods that only utilize category positive feature, we construct an online updated global feature bank to store both positive prototypes and negative prototypes."
  - [section]: "We propose the concept of negative prototypes as the proposals with high confidence score misclassified for the category that does not appear in image label...Observation reveals that negative prototypes always contain valuable category-specific discriminative features."

### Mechanism 2
- Claim: The pseudo label sampling module effectively mines missing instances and discards overfitted instances prone to partial detection.
- Mechanism: The module calculates similarity thresholds (τpos and τneg) based on the average feature similarity of candidate proposals with positive and negative prototypes. Proposals with similarity exceeding τpos are considered missing instances, while those below τneg are discarded as overfitted partial detections.
- Core assumption: The feature similarity between proposals and prototypes can reliably indicate whether a proposal is a missing instance or an overfitted partial detection.
- Evidence anchors:
  - [section]: "Based on the average feature similarity of candidate proposals and the positive prototypes of the same category, we can obtain a threshold τpos to mine proposals that might be omitted...according to the average feature distance of candidate proposals and the negative prototypes of the same category with maximum similarity, a threshold τneg can also be obtained so as to discard the partial overfitted instances."

### Mechanism 3
- Claim: The context-based feature extraction module enhances feature representation by incorporating location information and surrounding context.
- Mechanism: The module extracts three different features for each object proposal: the RoI feature, the context feature, and the frame feature. It then combines these features to obtain a more effective feature representation that can alleviate the problem of partial detection.
- Core assumption: Incorporating location information and surrounding context into the feature representation can help the detector better understand the object's boundaries and reduce the tendency to detect only discriminative parts.
- Evidence anchors:
  - [section]: "To represent the location of each proposal, we follow [18,39] to subtract the pooled context feature f ccontext from the frame feature f cf rame to obtain the input representation of the detection branch f cdet...By considering more information of the surrounding parts of the proposal, the extracted feature contains more location information and can effectively alleviate the problem of partial detection."

## Foundational Learning

- Concept: Contrastive Learning
  - Why needed here: Contrastive learning is used to optimize the proposal's feature representation by attracting same-class samples closer and pushing different-class samples away in the embedding space, improving discrimination between object categories.
  - Quick check question: How does contrastive learning differ from traditional supervised learning in terms of the loss function and the goal of the optimization process?

- Concept: Multi-Instance Learning (MIL)
  - Why needed here: MIL is used to transform the weakly supervised object detection problem into a multi-label classification task, where the goal is to predict the presence or absence of object categories in an image based on image-level annotations.
  - Quick check question: In the context of weakly supervised object detection, how does MIL handle the ambiguity of instance-level labels when only image-level annotations are available?

- Concept: Feature Bank
  - Why needed here: The global feature bank stores positive and negative prototypes from the entire dataset, allowing the model to leverage inter-image relationships and improve feature discrimination across different images.
  - Quick check question: What are the advantages and potential drawbacks of using an online updated global feature bank compared to a fixed feature bank or no feature bank at all?

## Architecture Onboarding

- Component map: Feature extractor -> MIL branch -> Online instance refine branch -> Contrastive branch (with global feature bank and pseudo label sampling module)
- Critical path: Feature extractor → MIL branch → Online instance refine branch → Contrastive branch (with global feature bank and pseudo label sampling module)
- Design tradeoffs:
  - Using a global feature bank allows for better exploitation of inter-image relationships but requires more memory and computational resources.
  - Incorporating context-based features can improve detection of object boundaries but may also introduce noise if the context is not relevant to the object.
  - Mining missing instances and discarding overfitted instances can improve overall detection performance but may also lead to false positives or negatives if the similarity thresholds are not well-calibrated.
- Failure signatures:
  - If the global feature bank becomes too large or contains too many irrelevant proposals, the contrastive learning may not effectively improve feature discrimination.
  - If the pseudo label sampling module incorrectly mines proposals or discards valid instances, the overall detection performance may degrade.
  - If the context-based feature extraction does not effectively capture relevant location information, the enhanced feature representation may not improve detection of object boundaries.
- First 3 experiments:
  1. Evaluate the impact of the global feature bank size on detection performance and memory usage.
  2. Compare the detection performance with and without the pseudo label sampling module to assess its effectiveness in mining missing instances and discarding overfitted instances.
  3. Analyze the contribution of the context-based feature extraction module to the overall detection performance by comparing results with and without this module.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform on datasets with more diverse object categories and complex backgrounds compared to the Pascal VOC datasets?
- Basis in paper: [inferred] The paper only evaluates the method on Pascal VOC07 and VOC12 datasets, which contain 20 categories. The authors do not discuss performance on more diverse datasets.
- Why unresolved: The paper does not provide any experiments or discussions on the method's performance on datasets with more diverse object categories or complex backgrounds, which are common in real-world applications.
- What evidence would resolve it: Experiments comparing the method's performance on datasets like MS COCO, which contains 80 object categories and more complex scenes, would provide insights into its generalization capabilities.

### Open Question 2
- Question: How does the proposed method compare to fully supervised object detection methods in terms of accuracy and computational efficiency?
- Basis in paper: [explicit] The paper mentions that there is still a large performance gap between weakly (mAP=56.8% in VOC07) and fully (mAP=89.3% in VOC07) supervised object detectors. However, the authors do not provide a direct comparison of their method with fully supervised methods in terms of accuracy and computational efficiency.
- Why unresolved: The paper focuses on improving weakly supervised object detection and does not discuss how the proposed method compares to fully supervised methods in terms of accuracy and computational efficiency.
- What evidence would resolve it: Experiments comparing the proposed method with fully supervised methods like Faster R-CNN or YOLO on the same datasets, along with a discussion on the trade-offs between accuracy and computational efficiency, would provide a comprehensive understanding of the method's strengths and limitations.

### Open Question 3
- Question: How does the proposed method handle objects with occlusion or partial visibility?
- Basis in paper: [explicit] The paper mentions that the detector tends to detect the most discriminative part of the target objects, which is an inherent defect of the CNN network. The authors propose using negative prototypes to mitigate the problem of partial detection.
- Why unresolved: The paper does not provide detailed discussions or experiments on how the proposed method handles objects with occlusion or partial visibility, which are common challenges in real-world object detection tasks.
- What evidence would resolve it: Experiments on datasets with occluded or partially visible objects, such as the Occluded Object Detection (OOD) dataset, would demonstrate the effectiveness of the proposed method in handling these challenging scenarios.

## Limitations
- The exact implementation details of the similarity head φ(·) for mapping RoI features to 128-dimensional embeddings are not provided in the paper.
- The global feature bank requires additional memory and computational resources to store and update positive and negative prototypes from the entire dataset.
- The effectiveness of the contrastive learning heavily depends on the quality and diversity of the negative prototypes, which may be challenging to obtain in some cases.

## Confidence
- High confidence: The overall framework and the core idea of leveraging positive and negative prototypes for contrastive learning are well-motivated and supported by the results.
- Medium confidence: The effectiveness of the pseudo label sampling module in mining missing instances and discarding overfitted instances is demonstrated, but the exact implementation details are not fully specified.
- Low confidence: The specific implementation details of the similarity head and the threshold calculation for the pseudo label sampling module are not provided, which may affect the faithful reproduction of the method.

## Next Checks
1. Evaluate the impact of the global feature bank size on detection performance and memory usage to determine the optimal size and update frequency.
2. Analyze the feature similarity distributions between positive and negative prototypes to assess the quality and diversity of the negative prototypes used in contrastive learning.
3. Compare the detection performance with and without the pseudo label sampling module on a subset of the dataset to evaluate its effectiveness in mining missing instances and discarding overfitted instances.