---
ver: rpa2
title: 'Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework
  for Large Vision Language Models'
arxiv_id: '2402.15721'
source_url: https://arxiv.org/abs/2402.15721
tags:
- hallucination
- hallucinations
- evaluation
- image
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new category of hallucination, Event Hallucination,
  which involves constructing complete narratives around fictional entities. The authors
  propose Hal-Eval, a universal and fine-grained hallucination evaluation framework
  for Large Vision Language Models (LVLMs), which integrates both discriminative and
  generative evaluation methods.
---

# Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models

## Quick Facts
- arXiv ID: 2402.15721
- Source URL: https://arxiv.org/abs/2402.15721
- Authors: Chaoya Jiang; Hongrui Jia; Wei Ye; Mengfan Dong; Haiyang Xu; Ming Yan; Ji Zhang; Shikun Zhang
- Reference count: 40
- Key outcome: Introduces a new category of hallucination (Event Hallucination) and demonstrates that Chain-of-Thought significantly reduces discriminative hallucinations while fine-tuning on Hal-Data improves LVLM performance.

## Executive Summary
This paper introduces Hal-Eval, a universal and fine-grained framework for evaluating hallucinations in Large Vision Language Models (LVLMs). The framework identifies a new category of hallucination—Event Hallucination—where models construct complete narratives around fictional entities. Hal-Eval integrates discriminative and generative evaluation methods using advanced LLMs to generate and filter fine-grained hallucinatory data. Experiments with six leading LVLMs demonstrate the framework's effectiveness in identifying and mitigating various hallucination types, particularly showing that Chain-of-Thought significantly reduces relationship and event hallucinations, while fine-tuning on Hal-Data enhances benchmark performance.

## Method Summary
The Hal-Eval framework employs an Automatic Fine-grained Hallucination Annotation (AFHA) pipeline to generate annotated hallucination datasets using GPT-4. It constructs both discriminative evaluation datasets (using question templates to assess hallucination presence) and generative evaluation datasets (using a specialized Hal-Evaluator for hallucination detection and correction). The framework evaluates six leading LVLMs including LLaVA, LLaVA1.5, MiniGPT-4, InstructBLIP, mPLUG-Owl, and GPT-4V across four hallucination types: object, attribute, relation, and event. The method involves fine-tuning LVLMs using Hal-Data for hallucination detection and correction, with performance measured across accuracy, precision, recall, F1 score, and "Yes" ratio metrics.

## Key Results
- Event hallucination represents a distinct and significant category that existing frameworks miss, capturing complex narratives around fictional entities
- Chain-of-Thought reasoning reduces discriminative hallucinations by up to 35% for relationship and event types across multiple models
- Fine-tuning LVLMs on Hal-Data reduces hallucinations and improves benchmark performance, with InstructBLIP achieving 89.1% accuracy on in-domain tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Event hallucination introduces a new category of hallucination that was previously overlooked, significantly impacting LVLM reliability.
- Mechanism: By identifying and defining event hallucinations as complete narratives around fictional entities, the framework captures complex hallucinations that simple object/attribute/relation categories miss.
- Core assumption: Event hallucinations represent a distinct and significant category of hallucinations that cannot be adequately captured by existing classification schemes.
- Evidence anchors:
  - [abstract] "Previous hallucination evaluation studies on LVLMs have identified hallucinations in terms of objects, attributes, and relations but overlooked complex hallucinations that create an entire narrative around a fictional entity."
  - [section 3] "Event hallucination: The LVLM not only describes a non-existent target but also constructs complete events around the non-existent target, including its attributes, relations, and actions."
  - [corpus] Weak - the corpus neighbors focus on general hallucination detection rather than the specific categorization of event hallucinations.

### Mechanism 2
- Claim: Using Chain-of-Thought (COT) significantly reduces discriminative hallucinations, especially for relationship and event types.
- Mechanism: COT forces models to reason step-by-step through the image-caption relationship, making it harder to generate inconsistent hallucinations.
- Core assumption: Sequential reasoning through COT helps models better align generated text with visual content.
- Evidence anchors:
  - [abstract] "Utilizing Chain-of-Thought (COT) significantly reduces discriminative hallucinations, particularly those involving relationships and events."
  - [section 5.1.2] "We observed a significant reduction in discriminative hallucinations on both in-domain and out-of-domain datasets after employing COT to LLaVA 1.5."
  - [corpus] Weak - corpus doesn't specifically address COT's effectiveness on hallucination reduction.

### Mechanism 3
- Claim: The Hal-Evaluator trained on hallucinatory samples effectively reduces hallucinations and improves benchmark performance.
- Mechanism: Fine-tuning LVLMs on the Hal-Data dataset provides supervised learning on hallucination detection and correction patterns.
- Core assumption: Training data containing hallucinations with corrections teaches models to recognize and avoid similar patterns.
- Evidence anchors:
  - [abstract] "The hallucinatory samples used to train our evaluator also serve as effective supervised fine-tuning data for LVLMs, contributing to reducing hallucinations and enhancing their benchmark performance."
  - [section 5.4] "These results indicate Hal-Data effectively aid models in mitigating hallucinations."
  - [section 5.2.1] "MiniGPT-4 and InstructBLIP, displayed robust in-domain accuracy, with the latter achieving 89.1% accuracy when the average output length was approximately 10 tokens."

## Foundational Learning

- Concept: Multimodal alignment between visual and textual representations
  - Why needed here: LVLMs need to properly align what they "see" in images with what they generate in text to avoid hallucinations.
  - Quick check question: Can you explain how cross-attention mechanisms help align visual and textual features in LVLMs?

- Concept: Fine-grained hallucination taxonomy and classification
  - Why needed here: The framework requires understanding the specific types of hallucinations (object, attribute, relation, event) to properly evaluate and mitigate them.
  - Quick check question: What distinguishes an event hallucination from the other three types, and why is this distinction important?

- Concept: Automatic data annotation and filtering pipelines
  - Why needed here: The framework relies on automatically generating and filtering large-scale hallucination datasets using GPT-4.
  - Quick check question: How would you design a prompt to GPT-4 to generate attribute hallucinations while maintaining consistency with the original caption?

## Architecture Onboarding

- Component map: Automatic Fine-grained Hallucination Annotation Pipeline (AFHA) → Discriminative Evaluation → Generative Evaluation → Hal-Data → Hal-Evaluator

- Critical path: Image → LVLM generation → Hal-Evaluator evaluation → Hallucination detection/correction → Performance metrics

- Design tradeoffs:
  - Using GPT-4 for annotation vs. manual annotation: trade speed/quality vs. cost
  - Discriminative vs. generative evaluation: trade specificity vs. flexibility
  - Large output length vs. hallucination rate: trade detail vs. accuracy

- Failure signatures:
  - High false positive rates in hallucination detection
  - Model overfitting to specific hallucination patterns in training data
  - COT prompts introducing new types of errors
  - Length control measures degrading output quality

- First 3 experiments:
  1. Evaluate COT effectiveness on a small subset of images across different hallucination types
  2. Compare Hal-Evaluator performance with human annotators on a validation set
  3. Test the impact of different output length constraints on hallucination rates across models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Hal-Eval vary across different image domains (e.g., natural scenes vs. human-centric images)?
- Basis in paper: [inferred] The paper mentions evaluating on both in-domain (COCO) and out-of-domain datasets but does not provide detailed analysis of performance differences across image domains.
- Why unresolved: The paper focuses on the effectiveness of Hal-Eval as a whole but lacks a granular breakdown of how well it performs on different types of images, which could be crucial for understanding its limitations and generalizability.
- What evidence would resolve it: Detailed results showing accuracy, precision, recall, and F1 scores for different image categories (e.g., natural scenes, human-centric images, indoor scenes) within both in-domain and out-of-domain datasets.

### Open Question 2
- Question: What is the impact of different fine-tuning strategies (e.g., supervised fine-tuning vs. reinforcement learning) on the performance of Hal-VL?
- Basis in paper: [explicit] The paper mentions that Hal-VL is fine-tuned using Hal-Data and shows improved performance on hallucination benchmarks, but does not explore alternative fine-tuning strategies.
- Why unresolved: The paper demonstrates the effectiveness of using Hal-Data for fine-tuning but does not investigate whether other fine-tuning approaches could yield better results in reducing hallucinations.
- What evidence would resolve it: Comparative results of Hal-VL fine-tuned using different strategies (e.g., supervised fine-tuning, reinforcement learning, contrastive learning) on hallucination benchmarks and general evaluation metrics.

### Open Question 3
- Question: How does the length of the generated descriptions affect the performance of Hal-Eval in detecting event hallucinations?
- Basis in paper: [explicit] The paper mentions that the incidence of event hallucinations increases with the length of the output and that length control is crucial for generative evaluations, but does not provide a detailed analysis of how this affects Hal-Eval's detection accuracy.
- Why unresolved: While the paper acknowledges the relationship between output length and event hallucinations, it does not explore how this relationship impacts the accuracy of Hal-Eval in detecting such hallucinations.
- What evidence would resolve it: A detailed analysis showing the accuracy of Hal-Eval in detecting event hallucinations at different description lengths, along with insights into the optimal length for balancing hallucination detection and description quality.

## Limitations

- Event hallucination categorization lacks broader validation across diverse image domains, raising questions about whether it represents a truly distinct phenomenon
- Heavy reliance on GPT-4 for data annotation may introduce bias and limit reproducibility due to the model's own hallucination tendencies
- Performance on specialized domains (medical imaging, technical diagrams) is not demonstrated, suggesting potential limitations in real-world applicability

## Confidence

- **High Confidence**: The effectiveness of Chain-of-Thought in reducing discriminative hallucinations is well-supported by experimental results, showing consistent improvements across multiple models and hallucination types.
- **Medium Confidence**: The Hal-Evaluator's ability to reduce hallucinations through fine-tuning is supported by in-domain results, but out-of-domain generalization remains less certain.
- **Low Confidence**: The claim that Event Hallucination represents a fundamentally new category requires further validation, as the distinction from other hallucination types is not fully substantiated.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate the Hal-Eval framework on specialized domains (medical imaging, technical documentation) to assess whether event hallucination detection generalizes beyond general image-caption pairs.

2. **Human Evaluation Comparison**: Conduct a direct comparison between Hal-Evaluator's hallucination detection and human annotators on a stratified sample of images across all hallucination types to quantify potential GPT-4 annotation bias.

3. **Ablation Study on COT Prompts**: Systematically test different Chain-of-Thought prompt structures to identify which reasoning patterns are most effective for specific hallucination types, and whether certain patterns introduce new failure modes.