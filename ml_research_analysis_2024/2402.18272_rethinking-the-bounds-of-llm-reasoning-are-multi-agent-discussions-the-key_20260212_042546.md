---
ver: rpa2
title: 'Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?'
arxiv_id: '2402.18272'
source_url: https://arxiv.org/abs/2402.18272
tags:
- discussion
- agent
- agents
- reasoning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reevaluates whether multi-agent discussions improve
  LLM reasoning, proposing a new group discussion framework called CMD. The authors
  conduct systematic experiments on reasoning benchmarks (ECQA, GSM8K, FOLIO-wiki)
  using three LLMs (ChatGPT-3.5, Gemini Pro, Bard).
---

# Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?

## Quick Facts
- **arXiv ID**: 2402.18272
- **Source URL**: https://arxiv.org/abs/2402.18272
- **Reference count**: 40
- **Primary result**: Single agents with strong prompts match or exceed multi-agent discussion performance on reasoning benchmarks

## Executive Summary
This paper challenges the assumption that multi-agent discussions improve LLM reasoning performance. Through systematic experiments on ECQA, GSM8K, and FOLIO-wiki benchmarks using three LLM models, the authors demonstrate that single agents with strong prompts (particularly those containing task-specific demonstrations) achieve performance comparable to or better than existing multi-agent discussion frameworks. The CMD framework introduced in this work consistently outperforms other discussion approaches while requiring fewer tokens. The study identifies two primary error types in discussions and reveals that weaker LLMs can improve through collaboration with stronger models.

## Method Summary
The authors propose CMD, a group discussion framework that addresses limitations in existing multi-agent approaches. CMD organizes agents into groups, uses an improved message-passing algorithm for communication, and employs majority voting with a secretary tie-breaker. The framework was tested across three reasoning benchmarks with three different LLMs (ChatGPT-3.5, Gemini Pro, Bard) using both single-agent and multi-agent configurations with and without demonstrations in prompts.

## Key Results
- Single agents with task-specific demonstrations achieve performance nearly identical to the best multi-agent discussion approaches
- Multi-agent discussions only outperform single agents when no demonstrations are provided in the prompt
- CMD consistently outperforms existing discussion frameworks on FOLIO-wiki, requiring fewer tokens
- Weaker LLMs (Bard, ChatGPT-3.5) show performance improvement when collaborating with stronger LLMs (Gemini Pro)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Single agents with strong prompts (especially with task-specific demonstrations) achieve performance comparable to multi-agent discussions.
- **Mechanism**: Task-specific demonstrations provide a concrete reasoning template that guides the LLM through the problem-solving process, effectively encoding expert knowledge into the prompt.
- **Core assumption**: LLMs can effectively learn and apply reasoning patterns from demonstrations in the prompt context.
- **Evidence anchors**:
  - [abstract] "Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach"
  - [section] "The detailed question description is helpful because the possible answers to judge the correctness of a given proposition—true, false, or unknown—require clarity. Most notably, the addition of a demonstration contributes significantly to improved performance"
  - [corpus] Weak - no direct citations about demonstration effectiveness in multi-agent settings found in related work
- **Break condition**: When the task requires diverse perspectives or knowledge that cannot be captured in a single demonstration, or when the demonstration contains flawed reasoning.

### Mechanism 2
- **Claim**: Multi-agent discussions outperform single agents when no demonstrations are provided.
- **Mechanism**: During discussion, agents exchange perspectives and reasoning steps, allowing collective error detection and correction that compensates for lack of explicit guidance.
- **Core assumption**: LLMs can identify flaws in reasoning through exposure to alternative viewpoints and explanations.
- **Evidence anchors**:
  - [abstract] "We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt"
  - [section] "We believe this is because, during discussions, the input from other agents can introduce new perspectives, leading to a more thorough reasoning process"
  - [corpus] Weak - related work mentions discussion frameworks but doesn't provide comparative evidence on demonstration absence scenarios
- **Break condition**: When agent models are too similar (same LLM family), discussions may reinforce errors rather than correct them.

### Mechanism 3
- **Claim**: Weaker LLMs can improve reasoning performance when participating in discussions with stronger LLMs.
- **Mechanism**: Stronger LLMs provide higher-quality reasoning paths and error correction that weaker LLMs can adopt or learn from during the discussion process.
- **Core assumption**: The reasoning quality difference between LLM models is sufficient for the stronger model to consistently guide the weaker one toward better answers.
- **Evidence anchors**:
  - [abstract] "We also find that agents powered by weaker LLMs like Bard (Anil et al., 2023) can improve its performance on reasoning with the assistance of the stronger LLMs like Gemini Pro (Team et al., 2023) during interaction"
  - [section] "Figure 5 demonstrates that agents with less capable LLMs like Bard and ChatGPT-3.5 gradually enhance their performance over consecutive rounds with the support of the more robust LLM, Gemini Pro"
  - [corpus] Weak - related work mentions multi-agent systems but doesn't specifically address performance transfer between LLM capability tiers
- **Break condition**: When the capability gap is too large, the weaker model may not be able to process or utilize the reasoning from the stronger model effectively.

## Foundational Learning

- **Concept**: Prompt engineering and demonstration effectiveness
  - Why needed here: Understanding how demonstrations guide LLM reasoning is critical for interpreting why single agents with strong prompts match discussion performance
  - Quick check question: If a demonstration contains a logical error, will the LLM replicate that error or correct it?
- **Concept**: Group dynamics and consensus formation
  - Why needed here: CMD's group discussion structure relies on understanding how agents reach consensus and how group composition affects outcomes
  - Quick check question: In CMD, if all groups reach different conclusions, what mechanism resolves the tie?
- **Concept**: Error propagation in sequential reasoning
  - Why needed here: Identifying how incorrect reasoning spreads in discussions helps explain the "wrong answer propagation" error type
  - Quick check question: Can an agent recover from adopting an incorrect viewpoint during discussion, or is the error permanent?

## Architecture Onboarding

- **Component map**: Kick-start prompt generator -> Agent manager with LLM sessions -> Message-passing synchronizer -> Group discussion coordinator -> Voting mechanism -> Secretary tie-breaker
- **Critical path**: Prompt generation → Agent response generation → Message synchronization → Group discussion → Voting → (Tie-break if needed) → Final result
- **Design tradeoffs**: 
  - Token efficiency vs. thorough discussion: More rounds and agents improve reasoning but increase token usage exponentially
  - Group size vs. diversity: Larger groups provide more perspectives but may dilute individual contributions
  - Prompt specificity vs. generalizability: Task-specific demonstrations improve performance but reduce framework flexibility
- **Failure signatures**: 
  - Consistently wrong answers despite multiple discussion rounds suggests demonstration or prompt issues
  - Tie frequency indicates group formation or voting mechanism problems
  - Performance degradation over rounds suggests error propagation rather than correction
- **First 3 experiments**:
  1. Single-agent with vs. without demonstration on FOLIO-wiki to validate prompt strength impact
  2. CMD with varying group sizes (2, 3, 4 agents per group) to find optimal configuration
  3. Multi-LLM CMD with same LLM vs. mixed LLM groups to test cross-model learning benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the CMD framework's performance advantage over single-agent settings persist when using demonstrations with different levels of quality or specificity?
- Basis in paper: [explicit] The paper mentions that task-specific demonstrations significantly enhance single-agent performance and that CMD outperforms other discussion frameworks on the FOLIO-wiki dataset.
- Why unresolved: The paper only tests one demonstration style (labeled premises and reasoning steps) and doesn't explore how demonstration quality affects performance differences between single agents and CMD.
- What evidence would resolve it: Experiments comparing CMD and single-agent performance across multiple demonstration types (generic vs. task-specific, low vs. high quality) on the same benchmarks.

### Open Question 2
- Question: How does the performance of multi-agent discussions scale with the number of agents beyond the 3-6 agent range tested in this paper?
- Basis in paper: [inferred] The paper tests frameworks with 3-6 agents but doesn't explore larger group sizes, and mentions that Debate claims more agents lead to better performance.
- Why unresolved: The paper only tests CMD with 6 agents and doesn't investigate whether the performance plateau observed in multi-agent discussions is due to agent count limitations or inherent discussion constraints.
- What evidence would resolve it: Experiments with CMD and other frameworks using 10+ agents on the same benchmarks to identify performance scaling patterns.

### Open Question 3
- Question: What specific aspects of the message-passing algorithm enable weaker LLMs to improve through interaction with stronger LLMs, and can these mechanisms be optimized?
- Basis in paper: [explicit] The paper observes that agents with weaker LLMs (Bard, ChatGPT-3.5) improve performance over rounds when discussing with stronger LLMs (Gemini Pro).
- Why unresolved: The paper identifies the phenomenon but doesn't analyze which components of the message-passing algorithm facilitate this knowledge transfer or how to optimize it.
- What evidence would resolve it: Ablation studies removing specific algorithm components to identify which mechanisms are essential for cross-LLM knowledge transfer, plus experiments testing alternative communication protocols.

## Limitations
- Findings may not generalize beyond the specific LLM models tested (ChatGPT-3.5, Gemini Pro, Bard)
- Evaluation limited to reasoning benchmarks without examining complex, multi-step reasoning or real-world applications
- Study doesn't investigate long-term discussion dynamics beyond fixed rounds or potential biases from secretary tie-breaking

## Confidence
- **High Confidence**: Single-agent with demonstrations achieves comparable performance to multi-agent discussions
- **Medium Confidence**: Multi-agent discussions only outperform single agents without demonstrations
- **Medium Confidence**: Weaker LLMs improve through collaboration with stronger LLMs

## Next Checks
1. Test CMD framework with open-weight models (Llama, Mistral) to verify performance patterns hold across different model architectures and training approaches
2. Conduct ablation studies on secretary tie-breaking frequency and its impact on final answer quality to understand whether this mechanism introduces bias
3. Design experiments with progressively more complex reasoning chains (beyond three-hop reasoning) to determine if multi-agent benefits emerge in more challenging scenarios where single-agent limitations become more pronounced