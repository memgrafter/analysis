---
ver: rpa2
title: 'SGPT: Few-Shot Prompt Tuning for Signed Graphs'
arxiv_id: '2412.12155'
source_url: https://arxiv.org/abs/2412.12155
tags:
- graph
- signed
- node
- link
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SGPT, a prompt-tuning framework that adapts
  pre-trained unsigned GNNs to few-shot signed graph tasks. The key idea is to bridge
  the gap between pre-training and downstream phases using a graph template based
  on balance theory that disentangles link polarity into separate semantic channels,
  and a task template that reformulates signed tasks into a unified link prediction
  objective.
---

# SGPT: Few-Shot Prompt Tuning for Signed Graphs

## Quick Facts
- **arXiv ID:** 2412.12155
- **Source URL:** https://arxiv.org/abs/2412.12155
- **Reference count:** 40
- **Primary result:** SGPT adapts pre-trained unsigned GNNs to few-shot signed graph tasks using prompt tuning, achieving up to 84.88% ROC-AUC on Epinions for link sign prediction and 78.92% for node classification on Wikipedia-Elec.

## Executive Summary
This paper proposes SGPT, a prompt-tuning framework that adapts pre-trained unsigned graph neural networks (GNNs) to few-shot signed graph tasks. The framework addresses the challenge of applying knowledge from unsigned graphs to signed graphs by using a graph template based on balance theory to disentangle link polarity into separate semantic channels, and a task template that reformulates signed tasks into unified link prediction objectives. The approach includes feature prompts to align semantic spaces and semantic prompts to integrate link sign information in a task-aware manner. Extensive experiments on seven benchmark datasets demonstrate that SGPT significantly outperforms existing methods, particularly in low-label scenarios.

## Method Summary
SGPT is a prompt-tuning framework that bridges the gap between pre-training and downstream signed graph tasks. It consists of four main components: a graph template that disentangles signed graphs into positive, negative, and topological channels using balance theory; a task template that reformulates node classification and link sign prediction into unified link prediction objectives; feature prompts that align downstream semantic spaces with pre-training feature spaces; and semantic prompts that integrate polarity signals in a task-aware manner. The framework uses pre-trained GNNs as frozen backbones, optimizing only lightweight prompt parameters during downstream training with limited labels.

## Key Results
- SGPT achieves 84.88% ROC-AUC on Epinions for link sign prediction in few-shot scenarios
- SGPT achieves 78.92% ROC-AUC for node classification on Wikipedia-Elec with limited labels
- The framework significantly outperforms existing methods across seven benchmark datasets, demonstrating effectiveness in low-label scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph template disentangles link polarity into three semantic channels, mitigating structural mismatches between unsigned and signed graphs.
- Mechanism: Balance theory is used to extract multi-hop relationships, generating positive, negative, and topological graph samples. Each sample maintains a consistent internal link semantics, satisfying the homophily assumption of pre-trained unsigned GNNs.
- Core assumption: Balanced and unbalanced paths in signed graphs can be separated into distinct semantic channels that preserve relational semantics without violating the homophily assumption of pre-trained GNNs.
- Evidence anchors:
  - [abstract] "We first design a graph template based on balance theory to disentangle mixed node relationships introduced by negative links, mitigating the structural mismatches between unsigned and signed graphs."
  - [section 4.3] "Based on the balance theory [6], it explicitly extracts node relationships at different hops to generate multiple graph samples. Each maintains a consistent type of internal link semantics, i.e., positive, negative and topological, each capturing a unique type of relational semantics while maintaining compatibility with the homophily assumption in pre-trained unsigned GNNs."
  - [corpus] Weak evidence - corpus neighbors don't specifically address signed graph balance theory mechanisms.
- Break condition: If the balance theory assumption fails (i.e., unbalanced paths dominate real-world signed graphs), the semantic separation may not effectively mitigate structural mismatches.

### Mechanism 2
- Claim: Task template reformulates signed tasks into unified link prediction objectives, aligning optimization goals with pre-training.
- Mechanism: Node classification and link sign prediction are converted to the same form as link prediction by introducing class prototypes and using similarity-based classification. This ensures consistent optimization objectives across pre-training and downstream tasks.
- Core assumption: Reformulating downstream signed tasks to match the pre-training link prediction objective enables effective transfer of knowledge from pre-trained unsigned GNNs.
- Evidence anchors:
  - [abstract] "We further introduce a task template that reformulates downstream signed tasks into a unified link prediction objective, aligning their optimization goals with the pre-training task."
  - [section 4.3] "The task template converts the downstream link sign prediction and node classification tasks to the same form used in the pre-training link prediction."
  - [section 4.3] "Given a downstream signed graph with link sign classes C = {P, N}, we introduce one prototype into the link embedding space for each link label class."
- Break condition: If the downstream task complexity cannot be adequately captured by similarity-based classification, the task template may fail to align optimization goals effectively.

### Mechanism 3
- Claim: Feature prompts align downstream semantic spaces with pre-training feature spaces, while semantic prompts integrate polarity signals in a task-aware manner.
- Mechanism: Lightweight prompt vectors are added to modify input node features before GNN encoding, aligning downstream input spaces with pre-training. Semantic prompts use adapter modules to aggregate embeddings from different channels based on task requirements.
- Core assumption: Modifying input feature spaces through lightweight prompts can bridge the gap between pre-training and downstream feature distributions, and adapter modules can effectively integrate multi-channel embeddings.
- Evidence anchors:
  - [abstract] "Furthermore, we develop feature prompts that align downstream semantic spaces with the feature spaces learned during pre-training, and semantic prompts to integrate link sign semantics in a task-aware manner."
  - [section 4.4] "It involves adding lightweight vectors to modify the input node features before the GNN encoding, acting as task-specific feature augmentation to align the downstream input space with that of the pre-training task."
  - [section 4.4] "We adopt a lightweight adapter module as the semantic prompt, which preserves both the original node embeddings and the semantic distinctions in a task-aware manner."
- Break condition: If the feature drift between pre-training and downstream stages is too large, feature prompts may not adequately align the spaces, and if the adapter module is insufficient for complex integration, semantic prompts may fail.

## Foundational Learning

- Concept: Signed graph neural networks and their limitations in few-shot scenarios
  - Why needed here: Understanding why traditional SGNNs struggle with few-shot learning is crucial for appreciating the need for prompt tuning approaches like SGPT.
  - Quick check question: What are the main limitations of supervised SGNNs in real-world applications with limited labeled data?

- Concept: Graph pre-training and its role in reducing supervision requirements
  - Why needed here: Understanding how pre-training on unsigned graphs can provide transferable knowledge for downstream tasks is essential for grasping the foundation of SGPT.
  - Quick check question: How does pre-training on unsigned graphs help reduce the supervision requirements in downstream signed graph tasks?

- Concept: Prompt tuning in graph neural networks
  - Why needed here: Understanding the concept of prompt tuning and how it bridges the gap between pre-training and downstream tasks is crucial for understanding SGPT's approach.
  - Quick check question: How does prompt tuning differ from traditional fine-tuning in adapting pre-trained models to downstream tasks?

## Architecture Onboarding

- Component map:
  - Graph template -> Feature prompts -> Pre-trained GNN -> Semantic prompts -> Task template
  - Pre-training on unsigned graphs using link prediction

- Critical path:
  1. Graph template disentangles signed graph into three channels
  2. Feature prompts modify input features for each channel
  3. Pre-trained GNN processes each channel separately
  4. Semantic prompts integrate channel embeddings
  5. Task template reformulates output for specific downstream task

- Design tradeoffs:
  - More complex graph templates may improve performance but increase computational cost
  - Larger prompt basis vectors may offer richer representational flexibility but may introduce noise
  - Deeper pre-trained GNN backbones may capture more complex patterns but require more training data

- Failure signatures:
  - Poor performance on signed graph tasks despite good performance on unsigned tasks suggests graph template issues
  - Inability to adapt to new downstream tasks suggests task template limitations
  - Overfitting on downstream tasks despite pre-training suggests feature prompt alignment problems

- First 3 experiments:
  1. Ablation study: Remove graph template to test its impact on signed graph task performance
  2. Varying hop number: Test performance with different hop numbers in graph template
  3. Prompt basis sensitivity: Test performance with different numbers of basis vectors in feature prompts

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the analysis of the framework, several potential open questions emerge:

- How does the performance of SGPT vary when applied to directed signed graphs compared to undirected signed graphs?
- What is the optimal number of hop extractions (k) in the graph template for different types of signed graph structures?
- How does SGPT's performance scale with the number of semantic channels beyond the three proposed (positive, negative, topological)?

## Limitations

- The framework's effectiveness may be limited when balance theory assumptions don't hold in real-world signed graphs with complex or non-standard structures
- Scalability to very large signed graphs may be challenging due to the need to maintain multiple graph channels and prompt parameters
- Robustness when pre-training and downstream signed graph distributions differ significantly is not thoroughly explored

## Confidence

- **High confidence:** The core mechanism of disentangling signed graphs into semantic channels using balance theory is well-supported by the mathematical formulation and experimental results
- **Medium confidence:** The claim that prompt tuning outperforms fine-tuning in few-shot settings is supported by experiments, but could benefit from additional ablation studies comparing prompt size vs. parameter efficiency
- **Low confidence:** The generalizability of SGPT to signed graphs with complex edge attributes or dynamic structures is not thoroughly explored

## Next Checks

1. Test SGPT on signed graphs with non-standard balance properties (e.g., graphs with high percentage of unbalanced cycles) to evaluate robustness
2. Conduct an ablation study varying the number of prompt basis vectors to understand the trade-off between performance and parameter efficiency
3. Evaluate the framework on signed graph streams or dynamic signed graphs to assess temporal generalization