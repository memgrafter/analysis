---
ver: rpa2
title: Configurable Safety Tuning of Language Models with Synthetic Preference Data
arxiv_id: '2404.00495'
source_url: https://arxiv.org/abs/2404.00495
tags:
- safety
- system
- preference
- data
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Configurable Safety Tuning (CST) addresses the inflexibility of
  standard fine-tuning methods like Direct Preference Optimization (DPO) that hard-code
  specific behaviors into language models. CST introduces a system prompt-based approach
  that enables dynamic safety configuration at inference time without requiring additional
  synthetic data beyond what is already used in DPO.
---

# Configurable Safety Tuning of Language Models with Synthetic Preference Data

## Quick Facts
- arXiv ID: 2404.00495
- Source URL: https://arxiv.org/abs/2404.00495
- Reference count: 6
- Primary result: Achieves configurable safety with near-perfect performance (S1=1.00) while maintaining general capabilities

## Executive Summary
Configurable Safety Tuning (CST) introduces a system prompt-based approach to enable dynamic safety configuration in language models at inference time. Unlike standard fine-tuning methods that hard-code specific behaviors, CST leverages the same synthetic preference data used in Direct Preference Optimization (DPO) but conditions the preference probability on both user prompts and system prompts. This allows deployment teams to enable or disable safety behaviors simply by changing the system prompt without requiring additional training data or model modifications.

## Method Summary
CST builds on DPO by modifying the preference probability to depend on system prompts that specify safety configurations. The method uses synthetic preference pairs generated via self-critique and creates dual entries with opposite system prompts and reversed preferences. During training, the model learns to associate specific system prompts with corresponding safety behaviors through an augmented DPO loss function. At inference time, the same model can generate both safe and uncensored responses based solely on the system prompt provided, without requiring additional synthetic data beyond what is already used in standard DPO pipelines.

## Key Results
- Achieves near-perfect safety performance (S1 scores of 1.00) on harmful behavior datasets
- Maintains uncensored generation capability (S0 scores of 0.92-1.00) when requested
- Outperforms standard DPO in multi-task scenarios with average scores of 0.98-0.96 versus 0.74-0.72
- Preserves general capabilities on ARC, HellaSwag, MMLU, and TruthfulQA benchmarks with comparable or slightly improved scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CST enables dynamic safety configuration by conditioning preference probabilities on system prompts
- Mechanism: The method modifies DPO's preference probability from ˆpθ(y1 ≻ y0|x) to ˆpθ(y1 ≻ y0|x, s), allowing the same model to generate both safe and uncensored responses based on the system prompt provided at inference time
- Core assumption: The LLM can learn to associate specific system prompts with corresponding safety behaviors through synthetic preference data
- Evidence anchors:
  - [abstract] "CST overcomes the constraints of vanilla DPO by introducing a system prompt specifying safety configurations"
  - [section II] "the preference probability ˆpθ(y1 ≻ y0|x) depends on the context, represented by a system prompt s that specifies the safety configuration"
  - [corpus] Weak - related papers discuss preference tuning but don't specifically address system prompt conditioning
- Break condition: If the model fails to learn the association between system prompts and desired behaviors, or if the synthetic preference pairs don't capture the full range of safety configurations needed

### Mechanism 2
- Claim: CST achieves safety tuning without requiring additional synthetic data beyond standard DPO
- Mechanism: By using the same synthetic preference pairs but reversing the preference relation ≻ for opposing system prompts, CST creates dual-purpose training data that teaches both safe and uncensored behaviors
- Core assumption: The self-critique method can generate high-quality preference pairs that, when reversed, provide sufficient signal for both safety configurations
- Evidence anchors:
  - [section II] "we can leverage the synthetic data pairs by setting ˆpθ(y1 ≻ y0|x, s1) = 1 − ˆpθ(y1 ≻ y0|x, s0)"
  - [abstract] "CST... facilitates the flexible and controlled adjustment of language models' safety levels, using only synthetic preference data"
  - [corpus] Weak - related work mentions synthetic data generation but doesn't specifically address data efficiency through preference reversal
- Break condition: If the synthetic preference pairs are of low quality or if the preference reversal doesn't capture meaningful behavioral differences

### Mechanism 3
- Claim: CST preserves general capabilities while enabling configurable safety
- Mechanism: By maintaining the original DPO loss function structure and only adding system prompt conditioning, CST avoids catastrophic forgetting of general knowledge and reasoning abilities
- Core assumption: The model's general capabilities are sufficiently robust to survive fine-tuning that primarily focuses on safety behavior modification
- Evidence anchors:
  - [abstract] "CST successfully manages different safety configurations and retains the original functionality of LLMs"
  - [section III] "CST not only enables safety configuration of the models at inference time, it also doesn't degrade performance in downstream tasks"
  - [section III] "Results in Table III... evidences that CST... doesn't degrade performance in downstream tasks such as general knowledge question-answering or reasoning"
- Break condition: If the safety tuning overpowers the model's general capabilities, leading to degradation in tasks like ARC, HellaSwag, MMLU, and TruthfulQA

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: CST builds directly on DPO by modifying its preference probability conditioning to include system prompts
  - Quick check question: How does DPO differ from standard reinforcement learning from human feedback (RLHF)?

- Concept: Self-critique for synthetic preference generation
  - Why needed here: CST uses self-critique to generate the initial preference pairs that are then augmented with system prompts
  - Quick check question: What is the key advantage of using self-critique over human-annotated preference data?

- Concept: System prompt conditioning in LLMs
  - Why needed here: The core innovation of CST relies on the model's ability to interpret and respond to system prompts
  - Quick check question: How do system prompts typically influence LLM behavior compared to user prompts?

## Architecture Onboarding

- Component map:
  - Self-critique module -> Synthetic preference pairs generation
  - System prompt encoder -> Safety configuration specification
  - Preference probability calculator -> ˆpθ(y1 ≻ y0|x, s) computation
  - DPO optimizer -> Model parameter updates

- Critical path:
  1. Generate synthetic preference pairs using self-critique
  2. Create dual entries with opposite system prompts and reversed preferences
  3. Train model with modified DPO loss incorporating system prompt conditioning
  4. Validate safety configuration switching at inference time

- Design tradeoffs:
  - Data efficiency vs. behavioral coverage: Using preference reversal saves data but may miss nuanced safety behaviors
  - Safety vs. general capability preservation: Balancing safety tuning strength to avoid degrading other capabilities
  - System prompt simplicity vs. expressiveness: Simple prompts work better but may not capture complex safety requirements

- Failure signatures:
  - Model fails to switch behaviors when system prompts change
  - Performance degradation on general capability benchmarks
  - Inconsistent safety behavior across similar prompts
  - Over-conservatism when "safe" mode is enabled

- First 3 experiments:
  1. Test safety configuration switching: Use harmful behavior test set with both system prompts to verify S0 and S1 scores
  2. Validate general capability preservation: Run ARC, HellaSwag, MMLU, and TruthfulQA benchmarks on CST-tuned model
  3. Compare with DPO ablation: Train identical model with only safety-focused system prompt to isolate benefit of dual configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CST perform with more than two system prompts representing different safety levels or behavioral modes?
- Basis in paper: [inferred] The paper mentions that "Further work shall study more fine-grained controls of safety, i.e., depending on semantic topics," suggesting this has not been tested
- Why unresolved: The current implementation only tests two opposing system prompts (s0 and s1), and the paper explicitly identifies exploring more nuanced configurations as future work
- What evidence would resolve it: Experiments testing CST with 3+ system prompts representing different safety levels (e.g., fully uncensored, moderate safety, maximum safety) and measuring performance across these configurations

### Open Question 2
- Question: What is the impact of CST on the model's computational efficiency during inference, particularly when switching between safety configurations?
- Basis in paper: [inferred] The paper does not discuss inference time or computational overhead differences between CST and standard fine-tuning methods
- Why unresolved: While the paper demonstrates CST's effectiveness, it does not report any measurements of inference latency or computational costs when using different system prompts
- What evidence would resolve it: Benchmark comparisons of inference time and memory usage for CST-tuned models versus baseline models under different system prompt configurations

### Open Question 3
- Question: How does CST perform when the system prompt and user prompt contain conflicting instructions or when safety requirements change mid-conversation?
- Basis in paper: [inferred] The paper tests CST with predefined system prompts but does not explore scenarios where safety requirements dynamically change during interaction
- Why unresolved: The experimental setup uses static system prompts applied to test sets, but real-world applications may involve dynamic safety requirement changes or conflicting instructions
- What evidence would resolve it: Testing CST in conversational scenarios where system prompts change between turns or where user prompts explicitly contradict system prompt safety instructions

### Open Question 4
- Question: What is the minimum amount of synthetic preference data needed for CST to maintain its configurable safety capabilities while still outperforming standard DPO?
- Basis in paper: [explicit] The paper states "CST enables controlling the safety behavior of LLMs, with no need for additional synthetic preference data compared with what is already available in current fine-tuning pipelines"
- Why unresolved: While the paper confirms CST doesn't need more data than DPO, it doesn't explore whether less data could still achieve the same benefits
- What evidence would resolve it: Experiments varying the amount of synthetic preference data used for CST training and measuring performance degradation thresholds compared to full data training

### Open Question 5
- Question: How does CST perform on safety-sensitive domains beyond the tested Harmful Behaviors dataset, such as medical advice or legal consultation?
- Basis in paper: [inferred] The paper tests on general safety tasks but doesn't explore specialized domain-specific safety requirements
- Why unresolved: The experimental evaluation focuses on general safety benchmarks and multi-task scenarios, but doesn't test domain-specific safety-critical applications
- What evidence would resolve it: Evaluations of CST-tuned models on domain-specific safety benchmarks for medical, legal, financial, or other specialized fields with unique safety requirements

## Limitations
- Effectiveness depends entirely on quality and coverage of synthetic preference data generated through self-critique
- Requires models to learn strong associations between system prompts and safety behaviors, which may be challenging for models with weaker prompt understanding capabilities
- Evaluation relies on GPT-4 classification for measuring safety performance, introducing potential bias and inconsistency

## Confidence
- High confidence: The mechanism of system prompt conditioning on preference probabilities is technically sound and builds on established DPO principles
- Medium confidence: The claim that CST achieves safety tuning without requiring additional synthetic data beyond standard DPO is plausible given the preference reversal mechanism
- Low confidence: The scalability of CST to more complex safety scenarios and its performance on real-world deployment scenarios with nuanced safety requirements

## Next Checks
1. **Generalization Testing**: Evaluate CST-tuned models on out-of-distribution safety scenarios and real-world prompts that weren't part of the synthetic preference generation process to assess true safety behavior generalization
2. **Prompt Sensitivity Analysis**: Systematically test how variations in system prompt wording and structure affect the model's ability to switch between safety configurations, identifying the minimum viable prompt format
3. **Long-term Stability Assessment**: Conduct extended fine-tuning experiments to evaluate whether CST's safety configuration abilities degrade over time or with continued training on general data, checking for catastrophic forgetting of the safety behavior associations