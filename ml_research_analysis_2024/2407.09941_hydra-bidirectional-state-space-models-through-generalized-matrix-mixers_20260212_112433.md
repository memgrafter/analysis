---
ver: rpa2
title: 'Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers'
arxiv_id: '2407.09941'
source_url: https://arxiv.org/abs/2407.09941
tags:
- matrix
- self
- sequence
- mixer
- matrices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the matrix mixer framework, a unifying abstraction
  for sequence models that views them as linear maps on input sequences. Within this
  framework, the authors identify sequence alignment as a key property enabling data-dependent
  parameterizations and extendability.
---

# Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers

## Quick Facts
- arXiv ID: 2407.09941
- Source URL: https://arxiv.org/abs/2407.09941
- Authors: Sukjun Hwang; Aakash Lahoti; Tri Dao; Albert Gu
- Reference count: 40
- One-line primary result: Hydra outperforms BERT by 0.8 points on GLUE and ViT by 2% Top-1 accuracy on ImageNet

## Executive Summary
This paper introduces the matrix mixer framework, a unifying abstraction for sequence models that views them as linear maps on input sequences. Within this framework, the authors identify sequence alignment as a key property enabling data-dependent parameterizations and extendability. They propose new sequence mixers using Vandermonde and Cauchy matrices, and introduce Hydra, a bidirectional extension of Mamba parameterized as a quasiseparable matrix mixer. Hydra outperforms BERT by 0.8 points on GLUE and ViT by 2% Top-1 accuracy on ImageNet, demonstrating state-of-the-art performance across domains.

## Method Summary
The paper proposes a unified framework called the matrix mixer, which conceptualizes sequence models as linear maps parameterized by structured matrices. The authors introduce the concept of sequence alignment (SAM), which allows for data-dependent parameterizations and extendability of matrix mixers. They propose new matrix mixers based on Vandermonde and Cauchy matrices, and introduce Hydra, a bidirectional extension of Mamba parameterized as a quasiseparable matrix mixer. Hydra achieves state-of-the-art performance across various domains by efficiently combining forward and backward state space models with shared projection layers.

## Key Results
- Hydra outperforms BERT by 0.8 points on GLUE benchmark
- Hydra achieves 2% higher Top-1 accuracy than ViT on ImageNet
- Parameter-efficient bidirectional processing through quasiseparable matrix structure

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sequence alignment enables data-dependent parameterization and extendability.
- **Mechanism:** SAM defines a partition of parameters where each subset is bijectively mapped to a sequence position. This ensures that sub-matrices depend only on data from the corresponding sequence segment, enabling dynamic parameterization and extension beyond trained sequence length.
- **Core assumption:** Each parameter is either mapped to a sequence element or left data-independent, and upper-left sub-matrices can be constructed from parameters of their corresponding sequence segment.
- **Evidence anchors:**
  - [abstract]: "We identify a key axis of matrix parameterizations termed sequence alignment, which increases the flexibility and performance of matrix mixers..."
  - [section 2.2]: Definition 2.2 and Propositions 2.3-2.4 establish SAM properties.
  - [corpus]: Weak - corpus papers focus on dense/mixer alternatives, not SAM theory.
- **Break condition:** If the bijective mapping fails or parameters are shared across sequence positions, the SAM property breaks down.

### Mechanism 2
- **Claim:** Quasiseparable matrices generalize semiseparable matrices and enable bidirectional processing.
- **Mechanism:** Quasiseparable matrices allow rank constraints on off-diagonal submatrices while keeping diagonal entries free, unlike semiseparable matrices which constrain diagonals. This freedom combined with upper triangular structure enables bidirectional modeling.
- **Core assumption:** Quasiseparable rank constraints apply only to off-diagonal submatrices, leaving diagonals unconstrained.
- **Evidence anchors:**
  - [abstract]: "Quasiseparable matrices are a fundamental matrix structure... Hydra maintains the strong performance and linear-time computational efficiency of SSMs..."
  - [section 3.2]: Definition 3.2 and Corollaries 3.3-3.5 explain generalization and expressivity.
  - [corpus]: Missing - corpus lacks direct evidence on quasiseparable generalization.
- **Break condition:** If quasiseparable matrices impose diagonal rank constraints, they lose expressivity over semiseparable.

### Mechanism 3
- **Claim:** Hydra achieves better performance by sharing projection layers and avoiding doubling parameters.
- **Mechanism:** Hydra uses a single set of projection layers for both forward and backward SSMs, unlike naive bidirectional approaches that double parameters. This parameter efficiency maintains model capacity while adding bidirectionality.
- **Core assumption:** Shared projections contribute significantly to model size, so sharing reduces parameter count.
- **Evidence anchors:**
  - [abstract]: "Hydra significantly improves parameter over the heuristic approaches... since we conceptualize the model as a quasiseparable matrix mixer..."
  - [section 3.3]: Discussion of parameter savings vs heuristic approaches.
  - [corpus]: Weak - corpus papers focus on performance, not parameter efficiency analysis.
- **Break condition:** If projection layers are negligible in size, sharing provides no efficiency gain.

## Foundational Learning

- **Concept:** Structured matrices with sub-quadratic multiplication algorithms
  - **Why needed here:** Enables efficient sequence modeling by reducing matrix multiplication cost from O(L²) to O(L log L) or O(L)
  - **Quick check question:** Why can't we use dense matrices for long sequences? What's the computational bottleneck?

- **Concept:** Matrix rank constraints and their impact on expressivity
  - **Why needed here:** Different matrix classes (low-rank, semiseparable, quasiseparable) have different rank constraints that trade off efficiency vs expressivity
  - **Quick check question:** How does constraining rank affect a matrix's ability to represent complex functions?

- **Concept:** Fourier transforms and their relationship to Vandermonde matrices
  - **Why needed here:** Understanding how DFT matrices relate to Vandermonde matrices helps grasp why FNet uses fixed-parameter structures
  - **Quick check question:** What makes DFT matrices special among Vandermonde matrices?

## Architecture Onboarding

- **Component map:** Input preprocessing (projections, convolutions) -> Matrix construction (data-dependent parameter generation) -> Sequence mixing (matrix multiplication via structured algorithms) -> Output projection and residual connection

- **Critical path:**
  1. Input → Preprocessing → q, k, v projections
  2. Matrix construction → Structured matrix M(q, k)
  3. Matrix multiplication → M(q, k) × preprocessed input
  4. Output → Post-processing + residual

- **Design tradeoffs:**
  - SAM vs non-SAM: Data dependency vs parameter sharing
  - Matrix class choice: Expressivity vs computational efficiency
  - Number of heads: Capacity vs parameter count
  - Bidirectionality method: Parameter sharing vs doubled capacity

- **Failure signatures:**
  - Poor training loss: Likely matrix construction or parameterization issues
  - High validation loss but good training: Overfitting, need regularization
  - Slow training: Inefficient matrix multiplication implementation
  - Memory issues: Large matrix sizes, consider lower-rank structures

- **First 3 experiments:**
  1. Verify matrix construction produces expected shapes and values for simple inputs
  2. Test matrix multiplication with known structured matrices (e.g., identity, diagonal)
  3. Compare forward vs backward pass outputs for quasiseparable implementation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the choice of structured matrix class impact the tradeoff between computational efficiency and representational power in sequence models?
- **Basis in paper:** [explicit] The paper discusses how structured matrix mixers like Toeplitz, Vandermonde, and Cauchy offer sub-quadratic computational complexity but potentially lower representational expressiveness compared to dense matrices. It also mentions the representation-computation tradeoff and the ability to tune the degree of structure.
- **Why unresolved:** The paper doesn't provide a comprehensive empirical study comparing different structured matrix classes across various tasks and sequence lengths to quantify this tradeoff. It also doesn't explore how to systematically choose the optimal matrix class for a given task.
- **What evidence would resolve it:** A large-scale empirical study comparing different structured matrix classes on a diverse set of tasks and sequence lengths, measuring both computational efficiency and performance. An analysis of how the optimal matrix class varies with task complexity and sequence length.

### Open Question 2
- **Question:** Can the matrix mixer framework be extended to handle more complex sequence data types beyond text and images, such as graphs or point clouds?
- **Basis in paper:** [inferred] The paper focuses on sequence data with a fixed order (text and images). It doesn't discuss how the matrix mixer framework could be adapted for unordered or irregular data structures like graphs or point clouds.
- **Why unresolved:** The paper doesn't explore the generalization of the matrix mixer framework to other data types. It's unclear how to define the sequence alignment property and the matrix structure for such data.
- **What evidence would resolve it:** A theoretical extension of the matrix mixer framework to handle unordered or irregular data structures, along with empirical results on graph or point cloud tasks. An analysis of how the properties of the matrix mixer framework (e.g., sequence alignment) need to be modified for these data types.

### Open Question 3
- **Question:** How does the choice of activation function and normalization scheme affect the performance of sequence models built using the matrix mixer framework?
- **Basis in paper:** [explicit] The paper mentions the use of activation functions like ReLU and softmax in the context of specific matrix mixers (e.g., MLP-Mixer, Linear Attention). However, it doesn't provide a systematic study of how different activation functions and normalization schemes impact performance.
- **Why unresolved:** The paper doesn't explore the interaction between the matrix structure and the choice of activation function or normalization scheme. It's unclear how these choices affect the learning dynamics and the representational power of the model.
- **What evidence would resolve it:** An empirical study comparing different activation functions and normalization schemes within the matrix mixer framework, measuring their impact on performance across various tasks. An analysis of how these choices interact with the matrix structure to affect learning dynamics and representational power.

## Limitations
- Limited theoretical proof for quasiseparable matrix generalizations to semiseparable structures
- Missing formal proof that quasiseparable matrices enable bidirectional processing through diagonal freedom
- Insufficient ablation studies on parameter efficiency calculations and their impact on performance

## Confidence
- High confidence: SAM framework definitions and basic properties, matrix mixer performance comparisons on benchmarks
- Medium confidence: Quasiseparable matrix generalizations, bidirectional processing mechanism
- Low confidence: Exact parameter efficiency calculations and their impact on model performance

## Next Checks
1. Verify the quasiseparable matrix construction produces correct upper triangular structure with free diagonal entries by testing on small matrices where manual calculation is possible
2. Compare parameter counts between Hydra and naive bidirectional approaches in a controlled setting with identical base architectures
3. Test SAM property preservation when extending sequence lengths beyond training data by measuring reconstruction error on longer sequences