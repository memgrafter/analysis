---
ver: rpa2
title: 'LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition'
arxiv_id: '2402.09989'
source_url: https://arxiv.org/abs/2402.09989
tags:
- entity
- named
- methods
- riveg
- mner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses grounded multimodal named entity recognition
  (GMNER), which aims to identify named entities, their types, and corresponding visual
  regions in image-text pairs from social media. The task is challenging due to weak
  image-text correlations and the mismatch between fine-grained named entities and
  coarse-grained referring expressions used in visual grounding methods.
---

# LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition

## Quick Facts
- arXiv ID: 2402.09989
- Source URL: https://arxiv.org/abs/2402.09989
- Authors: Jinyuan Li; Han Li; Di Sun; Jiahao Wang; Wenkun Zhang; Zan Wang; Gang Pan
- Reference count: 40
- Primary result: RiVEG achieves absolute improvements of 10.65%, 6.21%, and 8.83% across GMNER, MNER, and EEG subtasks respectively on Twitter-GMNER dataset

## Executive Summary
This paper addresses the challenge of Grounded Multimodal Named Entity Recognition (GMNER), which requires identifying named entities, their types, and corresponding visual regions in image-text pairs from social media. The key innovation is RiVEG, a unified framework that reformulates GMNER as a three-stage pipeline (MNER-VE-VG) using LLMs as bridges to convert named entities into entity expansion expressions. By splitting the task into modular components, RiVEG overcomes limitations of end-to-end approaches that struggle with weak image-text correlations and mismatched entity-referring expression granularity.

The framework demonstrates state-of-the-art performance on the Twitter-GMNER dataset, achieving substantial improvements across all three subtasks. The modular design allows for straightforward upgrades by selecting more advanced methods for individual components, making it a flexible and extensible approach to GMNER. The use of LLMs to generate entity expansion expressions enables better integration with visual grounding methods while maintaining strong MNER performance.

## Method Summary
RiVEG reformulates GMNER into a three-stage pipeline: MNER (Multimodal Named Entity Recognition), VE (Visual Entailment), and VG (Visual Grounding). The framework uses LLMs as a bridge to convert named entities into entity expansion expressions, which serve as inputs to both VE and VG modules. The MNER module identifies named entities and their types using BERT or XLM-RoBERTa, while the VE module determines groundability by leveraging visual entailment capabilities. The VG module localizes entities using visual grounding techniques. This modular approach bypasses object detection limitations and inherits capabilities from existing multimodal pretraining models.

## Key Results
- RiVEG achieves absolute improvements of 10.65% on GMNER subtask
- RiVEG shows 6.21% improvement on MNER subtask
- RiVEG demonstrates 8.83% improvement on Entity Extraction & Grounding (EEG) subtask
- Performance benefits from using entity expansion expressions as inputs to VE and VG modules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RiVEG reformulates GMNER into a three-stage pipeline (MNER-VE-VG) to overcome limitations of existing end-to-end approaches.
- Mechanism: By splitting the task into stages, RiVEG uses the best available MNER method without being constrained by the limitations of visual grounding approaches. The LLM-generated entity expansion expressions bridge the gap between fine-grained named entities and coarse-grained referring expressions used in visual grounding.

## Foundational Learning

### Concept 1: Multimodal Named Entity Recognition (MNER)
- Why needed: Identifies named entities and their types in text associated with images
- Quick check: Does the system correctly extract "Apple" as an organization from "Apple announced new products"?

### Concept 2: Visual Entailment (VE)
- Why needed: Determines whether an image semantically supports or entails the textual description
- Quick check: Can the system determine if an image of an iPhone supports the text "Apple released a new smartphone"?

### Concept 3: Visual Grounding (VG)
- Why needed: Localizes specific objects or regions in images based on textual descriptions
- Quick check: Can the system correctly identify the bounding box for "iPhone" in an image showing multiple Apple products?

### Concept 4: Entity Expansion Expressions
- Why needed: Converts named entities into more descriptive phrases that better align with visual grounding terminology
- Quick check: Does "iPhone" become "the new iPhone smartphone" when expanded for visual grounding?

## Architecture Onboarding

### Component Map
MNER -> LLM Bridge -> VE & VG Modules

### Critical Path
1. Text input → MNER module → Named entities & types
2. Entities → LLM bridge → Entity expansion expressions
3. Expansion expressions → VE module → Groundability determination
4. Expansion expressions → VG module → Visual region localization

### Design Tradeoffs
- Modular design enables flexibility but adds complexity compared to end-to-end approaches
- LLM bridge adds computational overhead but improves entity-referring expression alignment
- Three-stage pipeline allows for independent optimization of each component

### Failure Signatures
- Poor MNER performance propagates to downstream VE and VG failures
- LLM generation errors lead to misaligned entity expansion expressions
- VE module false negatives prevent entities from being grounded even when visual evidence exists

### First Experiments
1. Baseline test: Run MNER alone on Twitter-GMNER dataset to establish entity extraction performance
2. Ablation test: Compare RiVEG performance with and without LLM-generated entity expansion expressions
3. Module isolation: Test VE and VG modules independently with gold entity expansion expressions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RiVEG change when using different LLMs for entity expansion expression generation, particularly comparing smaller models to larger ones like ChatGPT?
- Basis in paper: The paper demonstrates that when text inputs are complete named entity referring expressions, all methods achieve optimal results, but performance decreases when using named entities or entity expansion expressions. It notes that LLMs with more parameters generally generate better quality results.
- Why unresolved: The paper doesn't provide detailed comparison of how specific LLM choices impact final GMNER results, focusing instead on overall framework performance.
- What evidence would resolve it: Comprehensive ablation study comparing RiVEG's performance using different LLMs (Vicuna-7B, Vicuna-13B, LlaMA2-7B, LlaMA2-13B, ChatGPT) with clear metrics on how each choice affects GMNER, MNER, and EEG scores.

### Open Question 2
- Question: What is the upper performance limit of RiVEG when using more advanced methods for each of its three modules?
- Basis in paper: The paper states that current results don't represent RiVEG's performance upper limit and that modular design facilitates straightforward upgrades with more advanced methods.
- Why unresolved: The paper uses current state-of-the-art methods but doesn't explore how much better RiVEG could perform with even more advanced techniques or provide theoretical analysis of framework limitations.
- What evidence would resolve it: Benchmarking RiVEG with progressively more advanced methods for each module and analyzing performance gains, plus theoretical analysis of framework limitations and potential improvements.

### Open Question 3
- Question: How does RiVEG perform on other multimodal named entity recognition datasets beyond Twitter-GMNER?
- Basis in paper: The paper focuses exclusively on Twitter-GMNER dataset without exploring RiVEG's performance on other multimodal NER datasets.
- Why unresolved: Single dataset evaluation makes it difficult to assess RiVEG's robustness and generalizability to different types of multimodal NER tasks or datasets with different characteristics.
- What evidence would resolve it: Testing RiVEG on multiple multimodal NER datasets (different social media platforms, news images, scientific figures) and analyzing performance variations while identifying key factors influencing generalization.

## Limitations
- Performance claims rely heavily on Twitter-GMNER dataset, limiting generalizability to other social media platforms
- LLM-based entity expansion introduces potential bias based on the model's training data and capabilities
- Framework doesn't thoroughly address cases where entities are mentioned multiple times or multiple entities appear in the same visual region

## Confidence
- High confidence in technical feasibility of three-stage pipeline approach and modular design
- Medium confidence in claimed performance improvements given single-dataset evaluation
- Medium confidence in LLM-based entity expansion mechanism pending analysis of potential biases

## Next Checks
1. Test the framework on multiple social media datasets with varying image-text correlations to assess generalizability
2. Conduct ablation studies to quantify the contribution of each module (MNER, VE, VG) to overall performance
3. Evaluate the framework's robustness by testing with different LLM models for entity expansion and analyzing the impact of LLM choice on performance