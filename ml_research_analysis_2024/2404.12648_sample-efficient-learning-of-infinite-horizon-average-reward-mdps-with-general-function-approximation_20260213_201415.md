---
ver: rpa2
title: Sample-efficient Learning of Infinite-horizon Average-reward MDPs with General
  Function Approximation
arxiv_id: '2404.12648'
source_url: https://arxiv.org/abs/2404.12648
tags:
- function
- linear
- lemma
- proof
- amdps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies infinite-horizon average-reward Markov decision
  processes (AMDPs) with general function approximation. The authors introduce a new
  complexity measure called average-reward generalized eluder coefficient (AGEC) that
  captures the exploration challenge in AMDPs.
---

# Sample-efficient Learning of Infinite-horizon Average-reward MDPs with General Function Approximation

## Quick Facts
- arXiv ID: 2404.12648
- Source URL: https://arxiv.org/abs/2404.12648
- Authors: Jianliang He; Han Zhong; Zhuoran Yang
- Reference count: 40
- One-line primary result: Introduces AGEC complexity measure and Loop algorithm achieving sublinear regret for infinite-horizon average-reward MDPs with general function approximation

## Executive Summary
This paper studies infinite-horizon average-reward Markov decision processes (AMDPs) with general function approximation. The authors introduce a new complexity measure called average-reward generalized eluder coefficient (AGEC) that captures the exploration challenge in AMDPs. They propose a unified algorithmic framework named Local-fitted Optimization with OPtimism (Loop) that can be implemented in both model-based and value-based paradigms. Loop features a novel construction of confidence sets and a low-switching policy updating scheme. The main theoretical result shows that Loop achieves a sublinear regret of Õ(poly(d, sp(V*))√Tβ), where d and β correspond to AGEC and log-covering number of the hypothesis class.

## Method Summary
The paper proposes Loop, a unified algorithmic framework for infinite-horizon average-reward MDPs with general function approximation. Loop combines optimistic planning with lazy policy updates triggered by cumulative discrepancy thresholds. It uses a novel construction of confidence sets based on discrepancy minimization and features a low-switching policy updating scheme. The algorithm achieves sublinear regret by decomposing total regret into Bellman error and realization error, both bounded using AGEC and the lazy update rule.

## Key Results
- Introduces AGEC as a new complexity measure for AMDPs with general function approximation
- Proposes Loop algorithm achieving sublinear regret of Õ(poly(d, sp(V*))√Tβ)
- Provides theoretical guarantees that match existing results when specialized to concrete AMDP models
- Demonstrates that AGEC captures the exploration challenge in AMDPs through its two-part structure (dominance and transferability coefficients)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AGEC captures the exploration challenge in AMDPs by linking in-sample training error to out-of-sample generalization through a Bellman error bound and transferability constraint.
- Mechanism: AGEC is defined as the smallest coefficients (dG, κG) such that (i) Bellman dominance holds—training error bounds Bellman error—and (ii) transferability holds—small in-sample error implies small out-of-sample error across policy switches.
- Core assumption: The Bellman optimality equation holds and the discrepancy function satisfies generalized completeness.
- Evidence anchors:
  - [abstract] Introduces AGEC as a complexity measure that captures exploration challenge in AMDPs.
  - [section] Formal definition in Definition 3 and discussion of its two-part structure.
  - [corpus] No direct corpus evidence for AGEC definition; claims are from paper.
- Break condition: If Bellman optimality fails or discrepancy function lacks completeness, AGEC may not bound regret properly.

### Mechanism 2
- Claim: Loop achieves sublinear regret by combining optimistic planning with lazy policy updates triggered by cumulative discrepancy thresholds.
- Mechanism: Policies are updated only when the cumulative squared discrepancy since last update exceeds 4β. This bounds the switching cost while maintaining optimism through a confidence set based on discrepancy minimization.
- Core assumption: The auxiliary function class G allows for a conservative approximation of the Bellman operator (see transferability condition).
- Evidence anchors:
  - [abstract] Loop features confidence sets and low-switching policy updates.
  - [section] Details in Section 4 and switching cost analysis in Section 5.
  - [corpus] No corpus evidence for Loop's low-switching mechanism; claims from paper.
- Break condition: If discrepancy minimization fails to approximate the Bellman operator, optimism may break and regret could grow.

### Mechanism 3
- Claim: The regret decomposition splits total regret into Bellman error and realization error, both of which are bounded using AGEC and the lazy update rule.
- Mechanism: Bellman error is bounded by dG√Tβ via the dominance coefficient; realization error is bounded by κG√T log T via the transferability coefficient and switching cost control.
- Core assumption: The cumulative squared discrepancy is controlled at O(β) in each step, and the update condition ensures that out-of-sample error is bounded.
- Evidence anchors:
  - [abstract] Reg(T) = Õ(poly(d, sp(V*))√Tβ) where d and β relate to AGEC and covering number.
  - [section] Regret decomposition in (5.1) and bounds in Section 5.
  - [corpus] No corpus evidence for this specific decomposition; claims from paper.
- Break condition: If the concentration bounds on discrepancy fail, the Bellman error or realization error bounds may not hold.

## Foundational Learning

- Concept: Bellman optimality equation for average-reward MDPs.
  - Why needed here: Loop relies on the existence of optimal bias functions V* and Q* satisfying the average-reward Bellman equation to define the Bellman operator and discrepancy.
  - Quick check question: What is the difference between the average-reward Bellman operator and the discounted-reward Bellman operator?

- Concept: Eluder dimension and its generalization to distributional eluder dimension.
  - Why needed here: AGEC builds on the eluder dimension framework to quantify how well a hypothesis class can be distinguished by observed data; distributional eluder dimension is used to bound the transferability coefficient.
  - Quick check question: How does the distributional eluder dimension differ from the standard eluder dimension in terms of what it measures?

- Concept: Covering numbers and their role in uniform concentration.
  - Why needed here: The log-covering number β appears in the regret bound and is used to set the confidence set radius; covering numbers ensure that uniform bounds hold over the hypothesis class.
  - Quick check question: Why do we use covering numbers instead of VC-dimension for function approximation in RL?

## Architecture Onboarding

- Component map: Optimistic planner -> Discrepancy-based confidence set constructor -> Lazy policy updater -> Data collector
- Critical path: At each step, check if cumulative discrepancy exceeds threshold → if yes, solve constrained optimization to update policy and confidence set → execute greedy policy → collect data and update cumulative discrepancy → repeat
- Design tradeoffs: The low-switching scheme reduces switching cost but may delay policy improvement; the confidence set radius β trades off optimism and conservatism; using a general discrepancy function allows flexibility but requires careful choice for each problem class
- Failure signatures: If policies are updated too infrequently, regret may grow due to poor exploration; if confidence sets are too loose, optimism may fail and regret may grow; if discrepancy is not well-behaved, concentration bounds may fail
- First 3 experiments:
  1. Verify that Loop with a tabular AMDP achieves the same regret as UCRL2 when specialized
  2. Test Loop on a linear AMDP and compare regret to FOPO
  3. Implement Loop with a kernel AMDP and measure the effect of the effective dimension on regret

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the AGEC complexity measure be further generalized to capture additional AMDP models beyond those currently identified?
- Basis in paper: [explicit] The paper mentions that AGEC encompasses "almost all previously known tractable AMDPs" and introduces it as a "novel complexity measure" that captures the exploration challenge in AMDPs with general function approximation.
- Why unresolved: The paper does not provide a comprehensive characterization of all possible AMDP models that AGEC can capture, leaving room for further exploration and potential generalization.
- What evidence would resolve it: A formal proof or demonstration that AGEC can capture a broader class of AMDP models, including those not currently considered, would resolve this question.

### Open Question 2
- Question: Is there a lower bound on the regret for AMDPs with general function approximation that matches the upper bound provided by Loop?
- Basis in paper: [explicit] The paper establishes an upper bound on the regret of Loop, but does not provide a corresponding lower bound.
- Why unresolved: Without a lower bound, it is unclear whether the regret achieved by Loop is optimal or if there is room for further improvement.
- What evidence would resolve it: A formal proof of a lower bound on the regret for AMDPs with general function approximation, matching the upper bound of Loop, would resolve this question.

### Open Question 3
- Question: Can the Loop algorithm be extended to handle more general classes of discrepancy functions beyond those considered in the paper?
- Basis in paper: [explicit] The paper introduces two specific choices of discrepancy functions (Bellman discrepancy and MLE-based discrepancy) and provides theoretical guarantees for Loop under these choices.
- Why unresolved: The paper does not explore the possibility of using other discrepancy functions or provide a general framework for handling arbitrary discrepancy functions.
- What evidence would resolve it: A theoretical analysis of Loop's performance under a broader class of discrepancy functions, including those not currently considered, would resolve this question.

## Limitations

- The practical implementation of AGEC and Loop remains challenging due to the difficulty of constructing well-specified discrepancy functions and hypothesis classes for arbitrary AMDPs
- The computational complexity of solving the constrained optimization in Loop is not addressed, which may limit scalability to large state-action spaces
- The paper assumes access to a well-specified discrepancy function and hypothesis class, but in practice, these may be difficult to construct for arbitrary AMDPs

## Confidence

- High confidence: The regret bound derivation and its comparison to existing results for special cases. The decomposition of regret into Bellman error and realization error is well-justified.
- Medium confidence: The definition and properties of AGEC, particularly its ability to capture the exploration challenge in AMDPs with general function approximation. While the theoretical motivation is sound, empirical validation across diverse AMDP models is needed.
- Medium confidence: The low-switching policy updating scheme's effectiveness in practice, as the theoretical switching cost bound may not translate directly to empirical performance.

## Next Checks

1. Implement Loop on a tabular AMDP and verify that it achieves the same regret bound as UCRL2 when specialized to this setting
2. Conduct a sensitivity analysis of the AGEC measure by testing it on AMDPs with varying levels of function approximation complexity and comparing the results to known complexity measures
3. Evaluate the computational efficiency of Loop by measuring the time and resources required to solve the constrained optimization problem for different AMDP models and hypothesis classes