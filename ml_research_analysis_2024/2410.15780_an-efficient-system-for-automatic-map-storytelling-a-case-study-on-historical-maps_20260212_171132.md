---
ver: rpa2
title: An Efficient System for Automatic Map Storytelling -- A Case Study on Historical
  Maps
arxiv_id: '2410.15780'
source_url: https://arxiv.org/abs/2410.15780
tags:
- maps
- captioning
- pictorial
- clip
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an efficient system for automatic map storytelling
  focused on historical maps. The method addresses the challenge of generating accurate,
  informative captions for maps using fine-tuned vision-language models.
---

# An Efficient System for Automatic Map Storytelling -- A Case Study on Historical Maps

## Quick Facts
- arXiv ID: 2410.15780
- Source URL: https://arxiv.org/abs/2410.15780
- Authors: Ziyi Liu; Claudio Affolter; Sidi Wu; Yizi Chen; Lorenz Hurni
- Reference count: 11
- Key outcome: Fine-tuned CLIP models achieve 81% average accuracy on caption generation, significantly outperforming base CLIP's 47% across map types.

## Executive Summary
This paper presents an automatic map storytelling system that generates comprehensive captions for historical maps using fine-tuned vision-language models. The approach addresses the challenge of creating informative, accurate captions that answer key questions about maps (where, what, when, why) while being robust to missing or altered text elements. The system employs a novel decision tree architecture to structure captions based on map type, using fine-tuned CLIP models for keyword generation and GPT-3.5 for story enrichment.

## Method Summary
The system fine-tunes CLIP models on map-specific datasets to generate keyword captions for different aspects of historical maps (location, map type, topic, style, century). A decision tree architecture first classifies maps as topographic or pictorial, then selectively generates relevant caption categories. GPT-3.5 extends these keyword captions into coherent stories. The approach was tested on 4,517 historical maps from the David Rumsey Historical Map Collection, demonstrating improved performance over baseline methods like ClipCap and showing robustness to text alterations in maps.

## Key Results
- Fine-tuned CLIP models achieved 81% average accuracy across caption categories, compared to 47% for base CLIP
- The decision tree architecture effectively routes caption generation based on map type (topographic vs pictorial)
- System shows robustness to missing or altered text in maps, outperforming GPT-4 in text recognition robustness
- Method is lightweight, scalable, and adaptable to other map types beyond the tested historical examples

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning CLIP models on map-specific datasets significantly improves caption accuracy compared to base CLIP models. The pre-trained CLIP model has been trained on natural images and text pairs, but historical maps present unique challenges including non-standard projections and artistic elements. By fine-tuning CLIP on map-relevant data, the model learns to better associate map visual features with their corresponding textual descriptions, capturing domain-specific patterns. This is supported by evidence showing fine-tuned models achieving 81% vs 47% average accuracy. The core assumption is that map visual features have distinctive patterns that can be learned through fine-tuning with representative data.

### Mechanism 2
The decision tree architecture ensures relevant captions are generated based on map type, improving the quality and relevance of the final story. Different map types (topographic vs pictorial) require different caption categories. The decision tree first classifies the map type, then selectively generates only the relevant caption categories for that type, avoiding irrelevant or redundant information. This is evidenced by the paper's description of how different caption categories are selected based on map type. The core assumption is that map type is a strong predictor of which caption categories are relevant, and the classification is accurate enough to drive downstream decisions.

### Mechanism 3
GPT-3.5 extends and summarizes keyword captions into coherent stories while maintaining robustness to missing or altered text in maps. Fine-tuned CLIP models generate structured keyword captions addressing specific questions (where, what, when, why). GPT-3.5 then takes these keywords and generates a comprehensive, natural language story that connects the information coherently. The system is designed to work even when map text elements are missing or inaccurate, relying on visual features rather than text recognition. This is supported by evidence showing the system's invariance to text alterations and better robustness compared to GPT-4. The core assumption is that GPT-3.5 can effectively synthesize structured keyword inputs into coherent narratives.

## Foundational Learning

- **Vision-Language Model Pre-training**: Understanding how CLIP and similar models learn cross-modal representations is crucial for appreciating why fine-tuning on domain-specific data improves performance. Quick check: What is the fundamental difference between training a vision-language model from scratch versus fine-tuning a pre-trained model on domain-specific data?

- **Decision Tree Classification**: The decision tree architecture is central to the system's ability to generate relevant captions based on map type, requiring understanding of how tree-based classifiers work and their advantages for this use case. Quick check: How does a decision tree handle cases where the input doesn't clearly fit into any predefined category, and what are the implications for this map captioning system?

- **Prompt Engineering for Language Models**: GPT-3.5 is used to extend and summarize captions, requiring knowledge of how to structure prompts effectively to get desired outputs from language models. Quick check: What are the key elements of an effective prompt when using a language model to summarize structured keyword inputs into coherent narratives?

## Architecture Onboarding

- **Component map**: Data Collection -> Dataset Preparation -> Fine-tuning -> Decision Tree -> GPT-3.5 Integration -> Web Interface

- **Critical path**: 
  1. User uploads or selects a map
  2. Decision tree classifies map type (topographic vs pictorial)
  3. Appropriate CLIP models generate keyword captions
  4. GPT-3.5 generates comprehensive story from keywords
  5. Story is presented to user through web interface

- **Design tradeoffs**: 
  - Using GPT-3.5 instead of GPT-4 for cost and speed, accepting potentially lower text recognition capabilities
  - Focusing on two map types (topographic and pictorial) rather than a more granular classification
  - Generating keyword captions first rather than direct story generation, which adds complexity but provides better control
  - Using separate CLIP models for different caption categories rather than a single multi-task model

- **Failure signatures**: 
  - Poor map type classification leading to irrelevant caption categories
  - Inconsistent or noisy keyword captions from fine-tuned CLIP models
  - GPT-3.5 generating hallucinated or irrelevant content not supported by keywords
  - System performance degradation on map types not well-represented in training data
  - Web interface latency issues due to multiple model inference steps

- **First 3 experiments**:
  1. Test decision tree classification accuracy on a validation set of maps with known types to ensure proper routing of caption generation
  2. Evaluate individual CLIP model performance on their respective caption categories before integration to identify weak points
  3. Test GPT-3.5 story generation with manually created keyword sets to verify coherence and relevance before connecting to the full pipeline

## Open Questions the Paper Calls Out

### Open Question 1
How can the decision tree architecture be optimized to better handle edge cases where map type classification is ambiguous? The paper introduces a decision tree architecture for structuring captions based on map type but does not explore optimization strategies for ambiguous cases. This remains unresolved as the decision tree's effectiveness in handling ambiguous map types is not discussed, leaving potential improvements unexplored. Comparative studies evaluating decision tree performance with and without optimization strategies on ambiguous map types would resolve this question.

### Open Question 2
What methods can be developed to automatically generate a larger and more diverse map dataset for training? The paper mentions the potential for more efficient ways to generate a larger and more diverse map dataset in the future. This remains unresolved as current methods for dataset generation are not detailed, and the scalability of the system depends on the availability of diverse training data. Successful implementation and evaluation of automated dataset generation methods that significantly expand the diversity and size of training data would resolve this question.

### Open Question 3
How can the system's caption quality be further strengthened to provide deeper explanations of the "why" behind a map? The paper notes that the current caption quality depends on the language model's capabilities and may lack depth in explaining the "why" behind a map. This remains unresolved as the current system does not fully explore methods to enhance the explanatory depth of captions, particularly for the "why" aspect. Enhanced models or methodologies that demonstrate improved explanatory depth in captions, validated through user studies or expert evaluations, would resolve this question.

## Limitations

- Evaluation is primarily based on accuracy metrics for individual caption categories rather than holistic story quality assessment
- System's performance on map types beyond topographic and pictorial remains untested
- Decision tree architecture may not generalize well to more diverse or nuanced map categories

## Confidence

- **High Confidence**: The claim that fine-tuning CLIP models on map-specific data improves caption accuracy (supported by direct quantitative comparison showing 81% vs 47% average accuracy)
- **Medium Confidence**: The assertion that the decision tree architecture ensures relevant caption generation (supported by methodology but lacking comprehensive ablation studies)
- **Low Confidence**: The claim of robustness to missing/altered text in maps (supported by limited comparisons with GPT-4 but lacking extensive testing across diverse map conditions)

## Next Checks

1. **Ablation Study**: Systematically evaluate the contribution of each component (fine-tuning, decision tree, GPT-3.5) by testing the system with different combinations disabled to quantify individual impact on overall performance.

2. **Generalization Testing**: Test the system on historical maps from collections other than David Rumsey (different time periods, regions, or styles) to assess real-world robustness and identify potential failure modes.

3. **User Study**: Conduct a user study with historians and map experts to evaluate the quality, informativeness, and coherence of generated stories, complementing the quantitative accuracy metrics with qualitative assessment of practical utility.