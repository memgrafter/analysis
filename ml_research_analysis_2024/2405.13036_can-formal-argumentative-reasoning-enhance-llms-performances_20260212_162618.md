---
ver: rpa2
title: Can formal argumentative reasoning enhance LLMs performances?
arxiv_id: '2405.13036'
source_url: https://arxiv.org/abs/2405.13036
tags:
- reasoning
- argumentation
- llms
- arxiv
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores whether computational argumentation can improve
  LLM reasoning performance. It introduces MQArgEng, a pipeline that uses an argumentation
  engine (ASPARTIX) to evaluate multiple LLM-generated responses, detect conflicts,
  and select the most justified answer.
---

# Can formal argumentative reasoning enhance LLMs performances?

## Quick Facts
- arXiv ID: 2405.13036
- Source URL: https://arxiv.org/abs/2405.13036
- Authors: Federico Castagna; Isabel Sassoon; Simon Parsons
- Reference count: 40
- The paper introduces MQArgEng, a pipeline that uses formal argumentation to improve LLM reasoning by generating multiple responses, detecting conflicts, and selecting the most justified answer, achieving modest performance gains on MT-Bench benchmarks.

## Executive Summary
This paper investigates whether computational argumentation can enhance the reasoning capabilities of large language models (LLMs). The authors introduce MQArgEng, a pipeline that uses the ASPARTIX argumentation engine to evaluate multiple LLM-generated responses, detect conflicts, and select the most justified answer. Tested on the MT-Bench benchmark using Mistral 7B Instruct, the system achieved an average 2.18% improvement over the baseline model, with particularly notable gains in STEM, reasoning, and coding categories. The findings demonstrate that formal argumentative reasoning can modestly enhance LLM performance without requiring model retraining, validating the feasibility of argumentation-based LLM plugins.

## Method Summary
The MQArgEng pipeline generates three candidate responses per query using an LLM (Mistral 7B Instruct), then creates supporting arguments for each response. These arguments are fed into the ASPARTIX argumentation solver to construct an abstract argumentation framework and compute grounded/preferred extensions. The system detects conflicts between arguments and selects the most justified output based on formal argumentation semantics. The final response is generated by injecting a summary of the justified arguments back into the prompt and using zero-shot Chain of Thought prompting. The approach is evaluated on the MT-Bench benchmark (80 questions across 8 categories) using GPT-4 as judge with both single answer grading and reference-guided grading.

## Key Results
- Average 2.18% improvement over baseline MQInstruct model across MT-Bench benchmark
- Significant gains in STEM (7.60→7.35), reasoning (4.25→4.05), coding (5.50→5.30), extraction (5.70→5.55), and humanities (8.10→7.75) categories
- Performance remained unchanged in math and declined slightly in writing and roleplaying categories
- Demonstrates formal argumentative reasoning can enhance LLM reasoning capabilities without retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple LLM responses are compared via formal argumentation semantics to filter out unreliable or conflicting outputs.
- Mechanism: MQArgEng generates three candidate responses and supporting arguments, uses ASPARTIX to build an abstract argumentation framework, then selects grounded/preferred extensions as justified outputs.
- Core assumption: Conflicting outputs can be formalized as attacks between arguments, and computational argumentation can resolve these conflicts to yield more reliable answers.
- Evidence anchors:
  - [abstract] "MQArgEng, a pipeline that uses an argumentation engine (ASPARTIX) to evaluate multiple LLM-generated responses, detect conflicts, and select the most justified answer."
  - [section 4] "MQArgEng generates three short replies...lists three supporting arguments for each...conflict detection...ASPARTIX solver has all the required ingredients to compute the grounded extension."
- Break condition: If the LLM fails to generate distinct or meaningful arguments, or ASPARTIX cannot resolve a conflict (e.g., empty grounded extension), the mechanism fails.

### Mechanism 2
- Claim: Augmenting LLM input with formally justified argument summaries steers the model toward more coherent reasoning.
- Mechanism: After computing the grounded/preferred extension, MQArgEng injects a summary of acceptable arguments back into the prompt and reruns the LLM with zero-shot-CoT for the final answer.
- Core assumption: Providing LLM with structured, conflict-free argument context improves its ability to produce consistent, well-reasoned outputs.
- Evidence anchors:
  - [section 4] "Output reply...augmented by the summarized information embedded in the computed acceptable (grounded/preferred) arguments by ASPARTIX."
  - [section 6] "The final reply after the injection of the information summarized by the argumentation engine leverages also zero-shot-CoT."
- Break condition: If the final LLM response ignores or misuses the injected arguments, the improvement is lost.

### Mechanism 3
- Claim: The pipeline architecture is modular and can be applied to different LLMs or tasks without retraining.
- Mechanism: MQArgEng operates as an external plugin: it takes any prompt, generates multiple candidate answers, detects conflicts, selects justified answers, and outputs the result—without modifying the underlying model.
- Core assumption: LLM outputs can be post-processed for quality improvement without model-level changes.
- Evidence anchors:
  - [abstract] "The findings demonstrate that formal argumentative reasoning can modestly enhance LLM reasoning capabilities without retraining."
  - [section 6.1] "As developed herein, an argumentation engine plugin proves to be scalable to any other model (potentially including Small Language Models or even future releases of new LLMs)."
- Break condition: If the LLM's internal reasoning is too opaque or the argumentation framework is too coarse, the plugin's benefit may not generalize.

## Foundational Learning

- Concept: Abstract argumentation frameworks (AFs) and semantics (grounded, preferred extensions)
  - Why needed here: MQArgEng relies on formal AFs to represent and resolve conflicts among LLM-generated responses.
  - Quick check question: Given an AF with arguments {a, b, c} and attacks {(a,b), (b,c)}, which arguments are in the grounded extension?
- Concept: Zero-shot Chain of Thought (CoT) prompting
  - Why needed here: The final LLM call in MQArgEng uses zero-shot-CoT to produce step-by-step reasoning after receiving the argumentation summary.
  - Quick check question: What is the difference between zero-shot-CoT and few-shot CoT?
- Concept: ASPARTIX solver and Answer-Set Programming (ASP)
  - Why needed here: ASPARTIX computes the semantics of the AF, which drives the selection of justified answers.
  - Quick check question: What input format does ASPARTIX expect for an AF?

## Architecture Onboarding

- Component map:
  User prompt → MQInstruct (base LLM) → Argument generation → Conflict detection → ASPARTIX solver → Summary injection → MQInstruct (final LLM call with zero-shot-CoT) → Output
- Critical path:
  1. Generate multiple candidate responses with supporting arguments.
  2. Detect conflicts between arguments.
  3. Compute grounded/preferred extensions with ASPARTIX.
  4. Inject summary into prompt and produce final output.
- Design tradeoffs:
  - Speed vs. thoroughness: Generating three responses per query is more expensive than one, but increases the chance of finding a justified answer.
  - Precision vs. generalization: Simple conflict detection works for most prompts but fails for poetic/formulaic arguments.
  - Complexity vs. scalability: Using ASPARTIX adds overhead but enables formal conflict resolution.
- Failure signatures:
  - No distinct arguments generated (all responses too similar).
  - Empty grounded extension (no arguments are justified).
  - Poor parsing of arguments containing code/math (causes conflict detection errors).
  - Final LLM ignores injected argumentation summary.
- First 3 experiments:
  1. Run MQArgEng on a simple STEM question and compare output quality to baseline MQInstruct.
  2. Test conflict detection with two clearly contradictory arguments; verify ASPARTIX selects the correct extension.
  3. Feed a prompt containing mathematical expressions; check if argument parsing fails and how it affects output.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can computational argumentation plugins improve reasoning in non-English language tasks?
- Basis in paper: [inferred] The paper only tests MQArgEng on English MT-Bench benchmarks
- Why unresolved: The study focused exclusively on English language tasks, leaving multilingual capabilities untested
- What evidence would resolve it: Testing the pipeline on multilingual benchmarks or language-specific reasoning tasks

### Open Question 2
- Question: How does the argumentation engine performance scale with increasingly complex argument structures?
- Basis in paper: [explicit] The paper notes that the pipeline has "naive" argument parsing that struggles with "oddly shaped" arguments like poetry, formulas, and code
- Why unresolved: The current implementation uses simple heuristics and doesn't employ sophisticated argument parsing techniques
- What evidence would resolve it: Testing the pipeline on datasets with increasingly complex argument structures and measuring performance degradation

### Open Question 3
- Question: What is the optimal number of generated arguments for the argumentation engine to balance reasoning quality and computational efficiency?
- Basis in paper: [explicit] The pipeline generates three arguments per response but doesn't explore this parameter
- Why unresolved: The paper uses a fixed number of arguments without exploring the tradeoff between quality and computational cost
- What evidence would resolve it: Systematic testing varying the number of generated arguments while measuring both performance gains and inference time

### Open Question 4
- Question: Does the argumentation engine maintain its reasoning benefits over extended conversation turns?
- Basis in paper: [inferred] The study evaluates single-turn responses but doesn't examine multi-turn conversation dynamics
- Why unresolved: The MT-Bench evaluation and pipeline design focus on single-turn responses without examining conversation context retention
- What evidence would resolve it: Testing the pipeline on multi-turn benchmarks and measuring performance consistency across conversation depth

## Limitations
- The pipeline's naive argument parsing struggles with non-standard formats like poetry, formulas, and code
- Performance gains vary significantly across task categories, with some showing no improvement or slight declines
- The approach adds computational overhead by generating multiple responses and running external argumentation solvers
- Limited testing on non-English languages and multi-turn conversations leaves generalizability uncertain

## Confidence
- High: The feasibility of using argumentation frameworks to select between conflicting LLM outputs (Mechanism 1) is well-supported by the abstract argumentation semantics and ASPARTIX implementation.
- Medium: The effectiveness of injecting argumentation summaries into LLM prompts (Mechanism 2) is supported by the reported performance gains, but the specific contribution of zero-shot-CoT versus the argumentation summary is not isolated.
- Low: The claim that this approach can be applied to any LLM or task without retraining (Mechanism 3) is plausible given the modular design, but the paper does not test this extensively with different models or domains.

## Next Checks
1. **Generalizability Test:** Apply MQArgEng to a different LLM (e.g., Llama 2) and benchmark on a separate dataset (e.g., GSM8K) to assess whether the performance gains transfer.

2. **Ablation Study:** Run experiments isolating the contribution of zero-shot-CoT from the argumentation summary injection to quantify their respective impacts on output quality.

3. **Conflict Detection Robustness:** Systematically test MQArgEng on prompts known to produce ambiguous or poetic arguments (e.g., creative writing prompts) to evaluate failure modes and refine the conflict detection heuristics.