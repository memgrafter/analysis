---
ver: rpa2
title: 'FRUGAL: Memory-Efficient Optimization by Reducing State Overhead for Scalable
  Training'
arxiv_id: '2411.07837'
source_url: https://arxiv.org/abs/2411.07837
tags:
- frugal
- training
- state
- optimization
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FRUGAL introduces a memory-efficient optimization framework that
  addresses the challenge of large memory overhead in training large language models
  by splitting gradients into state-full and state-free subspaces. It applies advanced
  optimizers like AdamW to the state-full subspace while using state-free methods
  like signSGD for the remainder, allowing full-dimensional updates without storing
  optimizer states for all parameters.
---

# FRUGAL: Memory-Efficient Optimization by Reducing State Overhead for Scalable Training

## Quick Facts
- arXiv ID: 2411.07837
- Source URL: https://arxiv.org/abs/2411.07837
- Reference count: 40
- One-line primary result: FRUGAL achieves state-of-the-art memory efficiency for LLM training by splitting gradients into state-full (AdamW) and state-free (signSGD) subspaces.

## Executive Summary
FRUGAL addresses the memory bottleneck in training large language models by introducing a novel optimization framework that splits gradients into state-full and state-free subspaces. This approach allows advanced optimizers like AdamW to be applied selectively to the most critical parameters while using state-free methods like signSGD for the remainder, dramatically reducing memory overhead without sacrificing performance. The method integrates with various projection techniques and provides theoretical convergence guarantees, consistently outperforming existing memory-efficient optimizers across both pre-training and fine-tuning tasks.

## Method Summary
FRUGAL operates by partitioning the parameter space into two subspaces: a state-full subspace where AdamW is applied, and a state-free subspace where signSGD is used. The gradient is projected onto the state-full subspace using techniques like SVD, blockwise selection, or random projections. AdamW updates the projected gradient, while the residual gradient is updated using signSGD. The updates are combined and applied to the parameters, with the projection matrix periodically updated. This selective application of optimizer state avoids the memory overhead of storing optimizer states for all parameters while maintaining convergence properties.

## Key Results
- FRUGAL outperforms baselines (GaLore, BAdam) in pre-training perplexity and fine-tuning accuracy while using less memory
- Only the Output layer requires AdamW; other parameters can use signSGD without significant performance loss
- Memory overhead reduced by up to 75% compared to full AdamW while maintaining or improving model performance
- Consistent performance gains across 60M-1B parameter models on C4 pre-training and GLUE fine-tuning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FRUGAL achieves memory efficiency by splitting gradients into state-full and state-free subspaces, allowing selective application of advanced optimizers.
- Mechanism: The gradient is projected onto a low-dimensional subspace (state-full) where AdamW is applied, while the residual gradient is updated using state-free methods like signSGD. This avoids storing optimizer states for the entire parameter space.
- Core assumption: The state-full subspace captures the most critical information for model convergence, while the state-free subspace can be effectively updated without optimizer state overhead.
- Evidence anchors:
  - [abstract]: "FRUGAL leverages gradient splitting to perform low-dimensional updates using advanced algorithms (such as Adam), while updates along the remaining directions are executed via state-free methods like SGD or signSGD."
  - [section]: "Our framework can be integrated with various low-rank update selection techniques, including GaLore and BAdam."
- Break condition: If the state-full subspace is too small or poorly chosen, critical gradient information is lost, leading to degraded convergence.

### Mechanism 2
- Claim: FRUGAL's use of signSGD for the state-free subspace achieves performance close to AdamW while using minimal memory.
- Mechanism: signSGD approximates Adam's sign-based updates, which Kunstner et al. (2023) suggest is key to Adam's success in transformers. This allows effective updates without momentum or variance tracking.
- Core assumption: signSGD can effectively train the majority of parameters in transformers, with only the Output layer requiring AdamW.
- Evidence anchors:
  - [section]: "signSGD outperforms SGD... We attribute this performance to the similarities between signSGD and Adam (Kingma, 2014)."
  - [section]: "Only the Output layer in transformer-like models requires advanced optimizers like AdamW, while other parameters... can use simpler methods like signSGD without significant performance loss."
- Break condition: If signSGD is applied to too many critical parameters, performance degrades significantly.

### Mechanism 3
- Claim: FRUGAL's theoretical convergence guarantees show it matches or improves upon state-of-the-art rates for gradient splitting methods.
- Mechanism: The framework combines SGDM for the state-full subspace and SGD for the state-free subspace, with convergence analysis showing the rate depends on the probability of parameter selection and variance reduction from momentum.
- Core assumption: The gradient splitting and re-projection of optimizer states maintains the theoretical properties needed for convergence guarantees.
- Evidence anchors:
  - [abstract]: "We provide theoretical convergence guarantees for our framework when using SGDM for low-dimensional updates and SGD for state-free updates."
  - [section]: "If Jk = [d] or Jk = ∅, Algorithm 2 becomes SGDM and SGD, respectively... we recover the best-known rate for both SGD and SGDM under these assumptions."
- Break condition: If the gradient splitting introduces bias that outweighs variance reduction, the theoretical guarantees break down.

## Foundational Learning

- Concept: Gradient splitting and projection methods
  - Why needed here: FRUGAL relies on splitting gradients into subspaces and projecting them appropriately. Understanding SVD, random projections, and block-wise selection is essential.
  - Quick check question: What is the difference between SVD-based and random projection in terms of memory and computational overhead?

- Concept: State-full vs state-free optimizers
  - Why needed here: The framework explicitly separates parameters into those requiring optimizer state (AdamW) and those that don't (signSGD). Understanding why momentum and variance tracking matter is crucial.
  - Quick check question: Why does AdamW require more memory than signSGD, and when is this overhead justified?

- Concept: Block Coordinate Descent (BCD) and layer-wise learning
  - Why needed here: FRUGAL's block-wise projection is related to BCD methods used in BAdam. Understanding BCD helps in grasping how parameter blocks are selected and updated.
  - Quick check question: How does BCD differ from standard SGD in terms of convergence and memory usage?

## Architecture Onboarding

- Component map:
  - Gradient computation → Projection onto state-full subspace → State-full optimizer update (AdamW) → Projection back to full space → State-free optimizer update (signSGD) → Parameter update
  - Key components: Projector (SVD/RandK/Blockwise), State-full optimizer (AdamW), State-free optimizer (signSGD), Update frequency controller

- Critical path:
  1. Compute gradient
  2. Project onto state-full subspace
  3. Apply AdamW to projected gradient
  4. Compute residual gradient
  5. Apply signSGD to residual
  6. Combine updates and apply to parameters
  7. Periodically update projection matrix

- Design tradeoffs:
  - Memory vs performance: More parameters in state-full subspace improves performance but increases memory usage
  - Update frequency: Higher frequency improves convergence but increases computational cost
  - Projection method: SVD gives better quality but higher cost vs blockwise/RandK which are faster but may lose information

- Failure signatures:
  - Poor convergence: Likely due to suboptimal state-full subspace selection or update frequency too low
  - Memory issues: Too many parameters in state-full subspace or inefficient projection storage
  - Performance degradation: State-free optimizer applied to too many critical parameters

- First 3 experiments:
  1. Verify gradient splitting works correctly by checking that projected gradient + residual = original gradient
  2. Test different projection methods (SVD vs blockwise) on a small model to compare memory usage and convergence
  3. Evaluate the impact of state-free optimizer choice (SGD vs signSGD) on a subset of parameters

## Open Questions the Paper Calls Out

- Open Question 1: How does the performance of FRUGAL scale when training larger models (7B+ parameters) compared to the tested 1B model?
  - Basis in paper: [explicit] The authors acknowledge computational constraints prevented them from testing on 7B+ models, which they identify as crucial for understanding scalability.
  - Why unresolved: The paper only tested up to 1B parameters, and the authors explicitly state they couldn't conduct experiments on larger models due to computational limitations.
  - What evidence would resolve it: Experimental results showing validation perplexity/memory usage trade-offs for models 7B-70B parameters, demonstrating whether the relative performance advantages over baselines persist at scale.

- Open Question 2: What is the optimal strategy for selecting the state-full subspace across different types of neural network architectures beyond transformers?
  - Basis in paper: [inferred] The paper demonstrates FRUGAL works well for transformers but doesn't explore other architectures; the authors mention this as a potential future direction.
  - Why unresolved: The current work focuses exclusively on transformer-based language models and doesn't investigate whether the same partitioning strategy (e.g., keeping output layers in state-full subspace) applies to other architectures like CNNs, RNNs, or diffusion models.
  - What evidence would resolve it: Comparative experiments showing FRUGAL performance across multiple architecture types with varying subspace partitioning strategies.

- Open Question 3: How does FRUGAL perform in memory-efficient fine-tuning of diffusion models compared to language models?
  - Basis in paper: [explicit] The authors mention diffusion models as a potential application area where memory-efficient optimization could be beneficial, but they don't test this.
  - Why unresolved: The paper only evaluates FRUGAL on language model pre-training and fine-tuning, leaving the question of its effectiveness for diffusion models unanswered.
  - What evidence would resolve it: Experimental results showing FRUGAL's performance on diffusion model fine-tuning tasks with memory efficiency comparisons to existing methods.

- Open Question 4: What is the impact of different state-free optimizers (beyond signSGD and SGD) on FRUGAL's performance?
  - Basis in paper: [explicit] The authors chose signSGD over SGD based on preliminary experiments but acknowledge that other state-free optimizers could be used.
  - Why unresolved: The paper only compares signSGD and SGD as state-free optimizers in their experiments, despite mentioning that other options exist.
  - What evidence would resolve it: Systematic experiments comparing FRUGAL with various state-free optimizers (e.g., AdaFactor, different momentum variants) across multiple tasks to identify optimal choices.

## Limitations

- The theoretical convergence guarantees rely on assumptions that may not fully capture practical training dynamics with highly non-convex loss surfaces
- The claim that "only the Output layer requires AdamW" lacks theoretical justification and may be architecture-dependent
- Implementation details of gradient splitting and projection matrix updates are not fully specified, making exact reproduction challenging

## Confidence

- **High Confidence**: The memory efficiency claims and experimental results showing FRUGAL outperforms baselines in both pre-training and fine-tuning tasks
- **Medium Confidence**: The theoretical convergence guarantees, as they rely on assumptions that may not fully capture practical training dynamics
- **Low Confidence**: The universality of the "Output layer only needs AdamW" finding, as this may be architecture-dependent

## Next Checks

1. **Ablation Study on State-full Subspace Size**: Systematically vary the number of parameters in the state-full subspace (e.g., just the Output layer, Output + Attention layers, all linear layers) to quantify the performance-memory tradeoff and test the claim that only the Output layer truly needs AdamW.

2. **Cross-Architecture Generalization**: Apply FRUGAL to non-transformer architectures (e.g., ConvNets, LSTMs, or MLP-Mixers) to validate whether the gradient splitting approach generalizes beyond transformer models, particularly testing if the Output layer principle holds.

3. **Convergence Analysis with Varying Update Frequency**: Conduct experiments varying the projection update frequency T to understand how often the state-full subspace needs to be updated for optimal convergence, and whether the theoretical convergence bounds match empirical observations.