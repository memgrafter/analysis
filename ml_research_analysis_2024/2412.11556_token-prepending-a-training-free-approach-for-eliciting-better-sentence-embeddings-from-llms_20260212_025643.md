---
ver: rpa2
title: 'Token Prepending: A Training-Free Approach for Eliciting Better Sentence Embeddings
  from LLMs'
arxiv_id: '2412.11556'
source_url: https://arxiv.org/abs/2412.11556
tags:
- sentence
- token
- layer
- llms
- uni0000001a
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Token Prepending (TP) is a training-free, plug-and-play technique\
  \ that improves sentence embeddings from autoregressive LLMs by prepending each\
  \ layer's decoded embedding to the next layer's input, enabling earlier tokens to\
  \ attend to complete sentence information under causal attention. This simple intervention\
  \ requires no parameter updates and adds negligible inference overhead (within 1.04\xD7\
  \ the original cost)."
---

# Token Prepending: A Training-Free Approach for Eliciting Better Sentence Embeddings from LLMs

## Quick Facts
- arXiv ID: 2412.11556
- Source URL: https://arxiv.org/abs/2412.11556
- Authors: Yuchen Fu; Zifeng Cheng; Zhiwei Jiang; Zhonghui Wang; Yafeng Yin; Zhengliang Li; Qing Gu
- Reference count: 21
- Primary result: TP improves sentence embeddings from LLMs, boosting STS task performance by up to 7.16 Spearman correlation without training

## Executive Summary
Token Prepending (TP) is a training-free, plug-and-play technique that enhances sentence embeddings from autoregressive LLMs by prepending each layer's decoded embedding to the next layer's input. This allows earlier tokens to attend to complete sentence information under causal attention, addressing the information bottleneck in decoder-only architectures. The method requires no parameter updates, adds negligible inference overhead (within 1.04× original cost), and consistently improves performance across diverse LLMs and prompt-based methods on STS tasks and downstream classification.

## Method Summary
TP prepends a special <PST> token to the input sentence, then replaces it with the decoded sentence embedding from the previous layer for intermediate layers (typically layers 2-7 for 7B models). An early-exit strategy outputs embeddings from intermediate layers rather than the final layer. The technique is integrated with prompt-based methods by positioning the <PST> token close to the input text. Optimal layer scope and early-exit layer are determined using development sets, with evaluation on STS benchmarks and downstream classification tasks using cosine similarity and logistic regression classifiers.

## Key Results
- TP improves STS performance by +7.16 Spearman correlation on PromptEOL with LLaMA2-7B
- Adds negligible inference overhead (within 1.04× original cost)
- Consistently boosts performance across 12 classification, 3 pair classification, 4 reranking, 11 clustering, and summarization datasets
- Optimal settings: start TP from layer 2, stop around layer 7-8 for 7B models
- Outperforms naive bidirectional attention modifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prepending the sentence embedding token allows earlier tokens to attend to complete sentence information under causal attention.
- Mechanism: The proposed TP technique prepends each layer's decoded sentence embedding token from the previous layer to the sentence in the next layer's input, enabling earlier tokens to attend to the complete sentence information under the causal attention mechanism.
- Core assumption: Causal attention restricts earlier tokens from attending to later tokens, causing biased encoding of sentence information and cascading effects on the final decoded token.
- Evidence anchors:
  - [abstract] "The proposed TP technique is a plug-and-play and training-free technique, which means it can be seamlessly integrated with various prompt-based sentence embedding methods and autoregressive LLMs."
  - [section 4.1] "Our core idea is to prepend the decoded sentence embedding token from the previous layer to the sentence in the next layer's input, making the semantics of the sentence perceptible to all tokens in the target sentence."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.425, average citations=0.0. Top related titles: Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings.

### Mechanism 2
- Claim: The technique introduces minimal additional inference overhead.
- Mechanism: The TP technique adds only a single token to the original sentence, resulting in minimal additional inference overhead.
- Core assumption: The inference cost scales linearly with the number of tokens in the input sequence.
- Evidence anchors:
  - [abstract] "The proposed TP technique is a plug-and-play and training-free technique, which means it can be seamlessly integrated with various prompt-based sentence embedding methods and autoregressive LLMs. Moreover, it adds only a single token to the original sentence, resulting in minimal additional inference overhead."
  - [section 5.3] "In contrast, the inference time of prompt-based methods with TP technology is within 1.04 times of the original, adding negligible overhead."

### Mechanism 3
- Claim: Early-exit strategy improves performance by using embeddings from intermediate layers.
- Mechanism: Considering that the final layer of LLMs is primarily used for token generation and contains less semantic information, we propose an early-exit strategy that outputs embeddings from intermediate layers, rather than the final layer, to serve as sentence embeddings.
- Core assumption: Intermediate layers contain richer semantic information compared to the final layer, which is optimized for next-token prediction.
- Evidence anchors:
  - [abstract] "we propose an early-exit strategy that outputs embeddings from intermediate layers, rather than the final layer, to serve as sentence embeddings."
  - [section 4.3] "Recent studies (Liu et al., 2024c; Jin et al., 2024a) demonstrate that each layer of LLMs plays a different role, and the embeddings from the last layer are primarily used for prediction and contain weaker semantic information."

## Foundational Learning

- Concept: Causal attention mechanism in decoder-only LLMs
  - Why needed here: Understanding how causal attention restricts information flow in LLMs is crucial for grasping why TP is necessary.
  - Quick check question: How does causal attention differ from bidirectional attention in terms of token information access?

- Concept: Sentence embedding extraction techniques
  - Why needed here: Knowing the existing methods for extracting sentence embeddings from LLMs provides context for why TP is an improvement.
  - Quick check question: What are the limitations of current prompt-based methods for extracting sentence embeddings from LLMs?

- Concept: Transformer architecture and layer interactions
  - Why needed here: Understanding how information flows through Transformer layers is essential for comprehending how TP modifies this flow.
  - Quick check question: How does the information flow differ between the input and output of each Transformer layer?

## Architecture Onboarding

- Component map: Input layer with prepended <PST> token -> Transformer layers with TP operation in early layers -> Intermediate layer for early-exit -> Output layer for sentence embedding

- Critical path:
  1. Prepend <PST> token to input sentence
  2. Pass through initial Transformer layers with TP
  3. Continue through remaining layers without TP
  4. Extract sentence embedding from early-exit layer

- Design tradeoffs:
  - TP vs. bidirectional attention: TP maintains the model's causal nature while bidirectional attention fundamentally alters it
  - Number of layers for TP: More layers provide better information flow but increase computational cost
  - Early-exit layer selection: Later layers may contain more refined embeddings but also more task-specific information

- Failure signatures:
  - Performance degradation if TP is applied to all layers
  - Inconsistent results across different prompts if TP is not properly tuned
  - Increased inference time if TP is implemented inefficiently

- First 3 experiments:
  1. Test TP on a simple prompt-based sentence embedding method (e.g., PromptEOL) to verify performance improvement
  2. Vary the number of layers for TP to find the optimal configuration
  3. Test TP with different LLMs (e.g., LLaMA2-7B, LLaMA3-8B) to assess generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal number of TP layers (typically 7-8 for 7B models) scale with model size, and what architectural features determine this relationship?
- Basis in paper: [explicit] The paper notes that "the best layer for ending token prepending is similar across different backbones, typically falling within the 7th to 8th layer range" but does not systematically explore how this optimal stopping point varies with model size or architecture.
- Why unresolved: The experiments focus primarily on 7B parameter models, with limited exploration of larger models like LLaMA2-13B, leaving uncertainty about whether the same layer range applies universally.
- What evidence would resolve it: Systematic experiments across multiple model sizes (7B, 13B, 34B, etc.) measuring STS performance with TP ending at different layers would clarify the scaling relationship and identify any architectural factors (attention head count, layer width) that influence the optimal stopping point.

### Open Question 2
- Question: Does the effectiveness of TP depend on the semantic complexity or domain of the input sentences, and how might this affect its generalizability across different NLP tasks?
- Basis in paper: [inferred] The paper demonstrates TP's effectiveness on STS tasks and various transfer learning datasets but does not analyze whether performance gains vary with sentence complexity, domain specificity, or semantic density.
- Why unresolved: While TP shows consistent improvements, the analysis does not investigate whether simpler sentences (e.g., factual statements) benefit more or less than complex ones (e.g., abstract concepts or domain-specific jargon).
- What evidence would resolve it: Controlled experiments categorizing sentences by complexity metrics (e.g., syntactic depth, vocabulary specificity) and measuring TP's relative performance on each category would reveal whether TP's benefits are domain-agnostic or domain-specific.

### Open Question 3
- Question: What is the theoretical mechanism by which TP improves backward dependency modeling, and can this be quantified at the attention pattern level?
- Basis in paper: [explicit] The paper proposes that TP "allows earlier tokens to attend to the complete sentence information under the causal attention mechanism" but does not provide empirical analysis of attention distributions before and after TP.
- Why unresolved: While performance gains are demonstrated, the paper does not analyze whether TP modifies attention weights, increases cross-token attention diversity, or alters the information flow between layers in a measurable way.
- What evidence would resolve it: Visualization and statistical analysis of attention matrices comparing vanilla LLMs and TP-enhanced LLMs across layers would reveal whether TP redistributes attention mass, strengthens long-range dependencies, or modifies attention entropy in ways that correlate with performance improvements.

## Limitations

- Narrow evaluation scope focused on STS tasks and classification datasets, lacking diverse semantic tasks like information retrieval or multilingual benchmarks
- No analysis of potential degradation in zero-shot generation capabilities when using TP
- Limited exploration of how TP effectiveness varies with sentence complexity or domain specificity

## Confidence

**High Confidence**: The claim that TP adds negligible inference overhead is well-supported by empirical measurements showing 1.04× the original cost, and the core mechanism of improving information flow through prepended tokens is consistently demonstrated across multiple experiments.

**Medium Confidence**: The assertion that TP consistently improves performance across diverse LLMs and prompt-based methods is supported by extensive experiments, though the magnitude of improvement varies significantly by model size and prompt type, suggesting the technique's effectiveness is not uniform.

**Low Confidence**: The claim that TP "outperforms naive bidirectional attention modifications" lacks direct comparative experiments in the paper, and the assertion that intermediate layers contain "richer semantic information" compared to the final layer is based on cited literature rather than direct validation in this work.

## Next Checks

1. **Generalization Testing**: Evaluate TP performance on information retrieval benchmarks (MS MARCO, Natural Questions) and multilingual STS datasets (XM-STS) to assess whether the technique generalizes beyond the English STS tasks tested in the paper.

2. **Ablation on Attention Mechanisms**: Conduct controlled experiments comparing TP with modified causal attention mechanisms that allow limited bidirectional information flow, directly testing the paper's claim that TP is superior to naive bidirectional modifications.

3. **Zero-Shot Generation Impact**: Test whether TP negatively impacts the model's zero-shot generation capabilities by evaluating perplexity and generation quality on held-out text generation tasks, addressing the potential tradeoff between embedding quality and generation performance.