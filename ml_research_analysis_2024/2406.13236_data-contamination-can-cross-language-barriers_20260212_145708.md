---
ver: rpa2
title: Data Contamination Can Cross Language Barriers
arxiv_id: '2406.13236'
source_url: https://arxiv.org/abs/2406.13236
tags:
- contamination
- cross-lingual
- llms
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a cross-lingual form of data contamination
  where training LLMs on translated versions of benchmarks can inflate performance
  without detection by existing text-overlap-based methods. The authors propose a
  generalization-based detection approach that replaces incorrect answer choices with
  correct ones from other questions, exploiting the fact that contaminated models
  struggle when all choices appear correct.
---

# Data Contamination Can Cross Language Barriers

## Quick Facts
- arXiv ID: 2406.13236
- Source URL: https://arxiv.org/abs/2406.13236
- Reference count: 26
- Primary result: Cross-lingual data contamination inflates LLM performance by 5-10% on MMLU and MathQA benchmarks, undetectable by existing text-overlap methods

## Executive Summary
This paper reveals that data contamination can cross language barriers, where training LLMs on translated versions of benchmark test sets in non-English languages can inflate performance on English benchmarks without detection by existing methods. The authors demonstrate that European languages (French, German, Italian, Spanish) provide stronger cross-lingual contamination effects than Asian languages (Chinese, Japanese, Korean), likely due to shared subword vocabularies and conceptual proximity. They propose a generalization-based detection approach that replaces incorrect answer choices with correct ones from other questions, exploiting the fact that contaminated models struggle when all choices appear correct.

## Method Summary
The authors translated MMLU, ARC Challenge, and MathQA test sets into seven languages using LLaMA3-8B, then performed continual pre-training on LLaMA3-8B and Qwen1.5-7B models using these translated datasets. They evaluated contaminated models on original English benchmarks and compared performance against clean models. The key innovation is their generalization-based detection method, which measures performance changes when incorrect answer choices are replaced with correct ones from other questions. This exploits the fact that clean models can easily identify obviously wrong answers, while contaminated models get confused when all choices appear memorized as correct.

## Key Results
- Cross-lingual contamination increases accuracy by 5-10% on MMLU and MathQA benchmarks
- Existing text-overlap-based detection methods fail to identify cross-lingual contamination
- The proposed generalization-based method successfully detects both cross-lingual and vanilla contamination
- European languages (French, German, Italian, Spanish) provide stronger contamination effects than Asian languages (Chinese, Japanese, Korean)

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Memorization via Conceptual Interface
- Claim: LLMs can acquire non-generalizable knowledge through contamination in other languages and apply it to English evaluation.
- Mechanism: The model's knowledge is stored in a language-agnostic conceptual space, and language acts as an interface. When contaminated in one language, the model learns the underlying concept, which can be accessed through any language interface, including English.
- Core assumption: LLMs encode knowledge in a language-agnostic conceptual space rather than language-specific representations.
- Evidence anchors:
  - [abstract]: "Cross-lingual means the models are contaminated on other languages and then evaluated on English test sets."
  - [section]: "Our hypothesis is that the knowledge in a model can be fixed, and language acts as an interface."
  - [corpus]: Found 25 related papers with average neighbor FMR=0.539, suggesting moderate relevance to cross-lingual contamination detection.

### Mechanism 2: Choice Confusion Detection via Generalization Gap
- Claim: Contaminated models show minimal performance improvement when incorrect choices are replaced with correct ones from other questions.
- Mechanism: Clean models understand questions and can easily identify wrong answers when they're obviously incorrect, leading to significant performance gains. Contaminated models, having memorized all choices as correct, get confused when faced with seemingly correct options and show little improvement.
- Core assumption: Models that truly understand questions can distinguish between obviously wrong and potentially correct answers.
- Evidence anchors:
  - [abstract]: "Specifically, we examine the LLM's performance change after modifying the original benchmark by replacing the false answer choices with correct ones from other questions."
  - [section]: "Through this manipulation, models that really understand the question should achieve better performance, as some choices can be not even wrong to the question, while the contaminated ones can get confused as all choices are memorized as correct."
  - [corpus]: Moderate corpus relevance (FMR=0.539) suggests this detection approach is novel compared to existing methods.

### Mechanism 3: Language Interface Quality Variation
- Claim: Contamination effectiveness varies by language due to shared subword vocabulary and conceptual proximity.
- Mechanism: Languages with similar subword vocabularies (European languages) create better conceptual interfaces for cross-lingual contamination than languages with different writing systems (Asian languages).
- Core assumption: Shared linguistic features facilitate cross-lingual knowledge transfer more effectively than divergent ones.
- Evidence anchors:
  - [abstract]: "We observe that European languages (French, German, Italian, and Spanish) can provide stronger cross-lingual contamination onto English, while Asian languages (Chinese, Japanese, and Korean) provide a lesser effect."
  - [section]: "This phenomenon could be explained by the closer subword vocabulary shared among these languages, or it might be considered as reflecting a more similar conceptual space among European languages."
  - [corpus]: No direct corpus evidence for this specific mechanism, but moderate overall relevance suggests related work exists.

## Foundational Learning

- Concept: Language-agnostic knowledge representation
  - Why needed here: Understanding how LLMs encode and retrieve knowledge across languages is fundamental to explaining cross-lingual contamination.
  - Quick check question: Can you explain how a model might store the concept of "democracy" in a way that's accessible through both English and Spanish interfaces?

- Concept: Generalization vs. memorization distinction
  - Why needed here: The core innovation is detecting contamination by testing generalization ability rather than text overlap, requiring understanding of this distinction.
  - Quick check question: How would you design a test to determine if a model truly understands addition versus having memorized specific addition problems?

- Concept: Choice confusion as a detection signal
  - Why needed here: This is the novel detection mechanism that exploits the confusion contaminated models experience when faced with uniformly "correct" choices.
  - Quick check question: Why would replacing wrong answers with correct ones from other questions make it easier for a clean model but harder for a contaminated one?

## Architecture Onboarding

- Component map:
  Data pipeline: Benchmark datasets → Translation → Contamination injection → Model evaluation
  Detection system: Original benchmark evaluation → Generalized benchmark construction → Performance comparison
  Analysis layer: Performance metrics aggregation → Contamination probability assessment

- Critical path: Translation quality → Contamination injection effectiveness → Detection method sensitivity
  The weakest link is translation quality, as poor translations may not effectively transfer knowledge across languages.

- Design tradeoffs:
  - Translation approach: Using LLM translation (cost-effective but potentially lower quality) vs. professional translation (higher quality but expensive)
  - Detection granularity: Per-question analysis (more precise) vs. aggregate metrics (more robust to noise)
  - Language coverage: Testing multiple languages (more comprehensive) vs. focusing on high-impact languages (more efficient)

- Failure signatures:
  - Low detection rates: May indicate models aren't memorizing or the detection method isn't sensitive enough
  - False positives: Could suggest the detection method is too sensitive or the generalization metric isn't well-calibrated
  - Language-specific failures: May reveal issues with translation quality or language interface compatibility

- First 3 experiments:
  1. Verify translation quality by having bilingual humans rate translated benchmarks
  2. Test detection method on models with known contamination levels to establish sensitivity
  3. Compare detection effectiveness across different language pairs to validate the language interface hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does cross-lingual contamination vary across different model sizes (e.g., 7B vs 70B parameters)?
- Basis in paper: [inferred] The authors explicitly note that their cross-lingual contamination experiments were only conducted on 7B models and state this as a limitation, questioning whether such contamination universally works on other model sizes.
- Why unresolved: The paper acknowledges that the contamination injection was only tested on 7B models due to computational constraints, leaving uncertainty about whether larger models would show similar contamination patterns.
- What evidence would resolve it: Systematic experiments injecting cross-lingual contamination into various model sizes (7B, 34B, 70B) and comparing the effectiveness of contamination across different parameter counts.

### Open Question 2
- Question: Does cross-lingual contamination work for benchmarks beyond multiple-choice question-answering formats?
- Basis in paper: [explicit] The authors identify this as a limitation, noting that their selected benchmarks are all multiple-choice questions-answering, which limits detection of contamination on other forms of benchmarks.
- Why unresolved: The paper only tests contamination detection methods on multiple-choice datasets, leaving open whether their generalization-based approach would work for generation, extraction, or other task formats.
- What evidence would resolve it: Experiments applying choice confusion and other detection methods to non-multiple-choice benchmarks like summarization, question generation, or open-ended reasoning tasks.

### Open Question 3
- Question: What is the mechanism by which models acquire non-generalizable knowledge from translated benchmarks?
- Basis in paper: [inferred] The authors discuss cross-lingual contamination as a way to explore LLM interpretability and suggest that language acts as an interface for knowledge, but don't fully explain the underlying mechanism of how translated knowledge transfers.
- Why unresolved: While the paper demonstrates that contamination crosses language barriers, it doesn't explain the specific mechanisms of how models map knowledge from one language to another or why European languages show stronger cross-lingual contamination effects than Asian languages.
- What evidence would resolve it: Detailed analysis of attention patterns, feature representations, or intermediate activations when models process contaminated vs. clean data across languages, potentially revealing how knowledge translation occurs.

## Limitations

- Translation quality uncertainty: The study relies on LLM-based translation for benchmark contamination, but the quality of machine-translated benchmarks may vary significantly across language pairs and domains.
- Generalization detection calibration: The proposed choice confusion detection method assumes a specific relationship between memorization and generalization that may not hold uniformly across different model architectures or training regimes.
- Language interface variability: While the paper observes that European languages show stronger cross-lingual contamination effects than Asian languages, the explanation remains speculative without direct experimental validation of these mechanisms.

## Confidence

**High Confidence**: The core empirical observation that cross-lingual contamination can inflate LLM performance by 5-10% on standard benchmarks, and that existing text-overlap detection methods fail to identify this contamination.

**Medium Confidence**: The proposed generalization-based detection method effectively identifies both cross-lingual and vanilla contamination, based on the experimental results showing significant performance differences between clean and contaminated models.

**Medium Confidence**: The hypothesis that LLMs encode knowledge in a language-agnostic conceptual space, supported by the observed cross-lingual contamination effects but not directly validated through model introspection or controlled experiments.

## Next Checks

1. **Translation Quality Validation**: Conduct bilingual human evaluation of translated benchmarks to establish the correlation between translation quality and contamination effectiveness across different language pairs.

2. **Model Architecture Testing**: Apply the generalization-based detection method to multiple model architectures (different base models, different sizes) to determine whether the detection sensitivity is consistent or architecture-dependent.

3. **Controlled Contamination Experiments**: Systematically vary contamination levels and evaluate the detection method's sensitivity curve to establish the minimum contamination threshold that can be reliably detected, and whether this threshold varies by language pair.