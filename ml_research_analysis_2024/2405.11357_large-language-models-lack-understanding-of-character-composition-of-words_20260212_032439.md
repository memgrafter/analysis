---
ver: rpa2
title: Large Language Models Lack Understanding of Character Composition of Words
arxiv_id: '2405.11357'
source_url: https://arxiv.org/abs/2405.11357
tags:
- character
- language
- tasks
- word
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) perform well on token-level tasks
  but struggle with character-level understanding, especially when tasks involve character
  positions or require breaking characters into their constituent letters. The study
  tests GPT-4, Claude, Gemini, and Mistral on tasks such as word retrieval, character
  insertion/deletion/replacement, character reordering, and character counting in
  English, Chinese, Korean, and Japanese.
---

# Large Language Models Lack Understanding of Character Composition of Words
## Quick Facts
- arXiv ID: 2405.11357
- Source URL: https://arxiv.org/abs/2405.11357
- Reference count: 15
- LLMs perform significantly worse than humans on character-level tasks, especially in Korean and Japanese.

## Executive Summary
This study reveals a fundamental gap in large language models' ability to understand character composition within words. While LLMs excel at token-level tasks, they struggle significantly when tasks require processing individual characters, particularly in languages with complex writing systems like Korean and Japanese. The research systematically tests multiple leading LLMs (GPT-4, Claude, Gemini, Mistral) across four languages using tasks that involve character insertion, deletion, replacement, reordering, and counting. Human participants perform near-perfectly across all tasks, while LLM accuracy drops dramatically at the character level. Chinese results show slight improvement due to its one-character-one-word correspondence, but Korean and Japanese results are severely degraded. These findings suggest that current LLMs lack the fine-grained character processing capabilities necessary for comprehensive language understanding.

## Method Summary
The study employs a comprehensive experimental design testing four state-of-the-art LLMs (GPT-4, Claude, Gemini, and Mistral) across four languages (English, Chinese, Korean, and Japanese). Researchers created character-level tasks including word retrieval, character insertion/deletion/replacement, character reordering, and character counting. Human participants served as a baseline for comparison, performing near-perfectly across all tasks. The experiments systematically varied task complexity and language type to isolate the models' character processing capabilities. Results were analyzed to identify patterns in model performance across different writing systems and task types.

## Key Results
- LLMs show severely degraded accuracy on character-level tasks compared to human performance
- Korean and Japanese tasks result in the worst LLM performance due to complex writing systems
- Chinese results are slightly better due to one-character-one-word correspondence
- Character counting and reordering tasks are particularly challenging for all tested LLMs

## Why This Works (Mechanism)
The mechanism underlying LLMs' character-level limitations stems from their tokenization-based architecture. Most LLMs process text at the token level rather than character level, where tokens typically represent words or subwords. This design choice optimizes for semantic understanding and efficiency but creates a fundamental blind spot for character-level operations. When tasks require manipulation or understanding of individual characters, models must reconstruct character information from token representations, leading to significant accuracy degradation. The problem is exacerbated in languages like Korean and Japanese, where character composition and writing systems don't map cleanly to common tokenization schemes.

## Foundational Learning
- Tokenization vs Character Processing: Understanding the difference between how LLMs process tokens versus individual characters
  - Why needed: Explains the fundamental architectural limitation causing character-level failures
  - Quick check: Verify if a model's tokenization strategy uses characters, subwords, or words as basic units

- Writing System Complexity: Different languages have varying character composition rules and visual complexity
  - Why needed: Explains why Korean and Japanese show worse performance than English and Chinese
  - Quick check: Compare grapheme cluster complexity across target languages

- Character Composition Rules: How characters combine to form words differs across languages
  - Why needed: Essential for understanding why certain languages are more challenging
  - Quick check: Analyze composition rules in Korean Hangul vs Japanese Kana vs Chinese Hanzi

## Architecture Onboarding
- Component Map: Input Text -> Tokenizer -> Embedding Layer -> Transformer Blocks -> Output Layer
- Critical Path: Character Task Input → Tokenization → Embedding → Attention Mechanism → Prediction → Output
- Design Tradeoffs: Token-level processing enables semantic understanding but sacrifices character-level precision
- Failure Signatures: Degradation in tasks requiring character position awareness, character decomposition, or fine-grained manipulation
- First Experiments:
  1. Test character-level task performance with different tokenization strategies
  2. Compare character counting accuracy across models with different embedding dimensions
  3. Evaluate whether vision-based models perform better on character composition tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Sample size for human participants is not reported, affecting baseline comparison reliability
- Study does not explore whether fine-tuning on character-level data could improve LLM performance
- Task selection may not capture all aspects of character understanding
- Does not address potential confounding factors such as tokenization strategies or model architectures

## Confidence
- High: LLMs perform significantly worse than humans on character-level tasks, especially in Korean and Japanese
- Medium: Chinese results are better due to the one-character-one-word correspondence, though this claim could benefit from further linguistic analysis
- Low: The study does not explore the impact of model size or fine-tuning on character understanding

## Next Checks
1. Replicate the study with a larger and more diverse human participant pool to strengthen the baseline comparison
2. Test whether fine-tuning LLMs on character-level tasks improves their performance, particularly in Korean and Japanese
3. Investigate the impact of different tokenization strategies (e.g., byte-level vs. word-level) on character-level task performance