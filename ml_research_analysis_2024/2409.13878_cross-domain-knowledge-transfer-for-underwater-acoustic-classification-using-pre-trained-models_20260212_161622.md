---
ver: rpa2
title: Cross-Domain Knowledge Transfer for Underwater Acoustic Classification Using
  Pre-trained Models
arxiv_id: '2409.13878'
source_url: https://arxiv.org/abs/2409.13878
tags:
- data
- pre-trained
- learning
- audio
- pann
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the application of pre-trained models to
  underwater acoustic target recognition (UATR) using the DeepShip dataset. The research
  compares pre-trained Audio Neural Networks (PANNs) and ImageNet models for passive
  sonar classification, addressing the challenge of limited labeled data in UATR applications.
---

# Cross-Domain Knowledge Transfer for Underwater Acoustic Classification Using Pre-trained Models

## Quick Facts
- arXiv ID: 2409.13878
- Source URL: https://arxiv.org/abs/2409.13878
- Reference count: 26
- Pre-trained models achieve 75.1% accuracy on underwater acoustic classification using DeepShip dataset

## Executive Summary
This study investigates the application of pre-trained models to underwater acoustic target recognition (UATR) using the DeepShip dataset. The research compares pre-trained Audio Neural Networks (PANNs) and ImageNet models for passive sonar classification, addressing the challenge of limited labeled data in UATR applications. The study evaluates CNN14 models pre-trained at different sampling rates (8, 16, and 32 kHz) and compares their performance when fine-tuned on DeepShip data resampled at various frequencies. Results show that the CNN14-32k model achieves the highest accuracy of 75.1% on held-out test data, with robust performance across different sampling rates. The study also compares PANN and TIMM models, finding that ConvNeXtV2-tiny (ImageNet pre-trained) slightly outperforms PANNs with 73.7% accuracy.

## Method Summary
The study preprocesses the DeepShip dataset by resampling audio to 8, 16, and 32 kHz, segmenting into 5-second intervals, and converting to log-mel spectrograms (64 mel filters, Hann window 1024, hop length 320, 50-14000 Hz range). Models are fine-tuned using Adam optimizer (LR 5e-5), batch size 64, 100 epochs with patience 50, and data augmentation (SpecAugment with adjusted time mask width, Mixup). Performance is evaluated across three random runs with stratified 70/10/20 train/val/test splits to prevent data leakage.

## Key Results
- CNN14-32k model achieves highest accuracy of 75.1% on held-out test data sampled at 4 kHz
- CNN14-32k model shows robust performance across different sampling rates (8, 16, 32 kHz)
- ConvNeXtV2-tiny (ImageNet pre-trained) slightly outperforms PANNs with 73.7% accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning works across data modalities because lower layers of deep networks learn generic feature representations (edges, textures, spectral patterns) that are useful regardless of the original data domain.
- Mechanism: The CNN14 and ConvNeXtV2 models trained on AudioSet and ImageNet, respectively, extract low-level features that capture general acoustic patterns (e.g., harmonic structure, transient events) and image features (e.g., edges, textures) that can be repurposed for underwater acoustic spectrograms.
- Core assumption: Low-level features are modality-agnostic and can be reused for classification tasks in different domains with minimal fine-tuning.
- Evidence anchors:
  - [abstract] "ImageNet pre-trained models slightly out-perform pre-trained audio models in passive sonar classification."
  - [section] "The ConvNeXtV2-tiny model's self-supervised training approach which uses a fully convolutional masked autoencoder may provide some explanation as to its high comparative performance."
  - [corpus] Weak support; no direct evidence of cross-modal feature reuse in corpus.
- Break condition: If the target domain data distribution is too different from the source domain, the pre-trained features become irrelevant, and fine-tuning from scratch becomes necessary.

### Mechanism 2
- Claim: Higher sampling rate pre-training improves robustness to frequency resolution changes during fine-tuning.
- Mechanism: Models pre-trained at 32 kHz retain richer spectral detail, enabling them to adapt better to lower sampling rates during fine-tuning without significant accuracy loss.
- Core assumption: Pre-training at higher resolution preserves information that helps models generalize across different frequency settings.
- Evidence anchors:
  - [section] "The CNN14 32k model achieves the highest accuracy on held-out test data sampled at 4 kHz with 75.1 ± 0.3%."
  - [section] "The CNN14 32K model was robust across the different sampling rates as opposed to the other lower sampling rate models."
  - [corpus] No direct evidence in corpus supporting this mechanism.
- Break condition: If fine-tuning is done on data sampled at much higher rates than the pre-training, the model may fail to capture the additional high-frequency information.

### Mechanism 3
- Claim: Data augmentation (SpecAugment + Mixup) improves model generalization by simulating variations in real-world acoustic environments.
- Mechanism: Masking and mixing spectrogram features during training forces the model to learn invariant representations, making it more robust to noise, distortion, and variability in underwater acoustic data.
- Core assumption: Acoustic signals are naturally variable; models must learn to recognize targets despite these variations.
- Evidence anchors:
  - [section] "To improve model robustness to variations in the data, SpecAugmentation [20] is applied on the spectrogram... Mixup [21] is applied to the spectrogram to further augment the data."
  - [section] "Each data augmentation technique was used for both PANN and TIMM models for a fair comparison."
  - [corpus] No direct evidence in corpus supporting this mechanism.
- Break condition: Over-augmentation can lead to unrealistic training samples, causing the model to learn incorrect patterns or lose important signal characteristics.

## Foundational Learning

- Concept: Short-Time Fourier Transform (STFT) and Mel-spectrogram conversion
  - Why needed here: Converts raw audio waveforms into time-frequency representations that highlight acoustic features relevant to classification.
  - Quick check question: What is the effect of using a Hann window of size 1024 and hop length of 320 on the time-frequency resolution of the spectrogram?

- Concept: Transfer learning and fine-tuning
  - Why needed here: Leverages pre-trained models to overcome data scarcity in UATR, reducing training time and improving performance on limited datasets.
  - Quick check question: Why is full fine-tuning preferred over freezing layers in this study?

- Concept: Model architecture differences (CNN vs. ConvNeXtV2)
  - Why needed here: Understanding how different architectures (e.g., CNN14 vs. ConvNeXtV2-tiny) impact classification accuracy in cross-domain tasks.
  - Quick check question: How does the fully convolutional masked autoencoder in ConvNeXtV2 contribute to its performance?

## Architecture Onboarding

- Component map: Raw audio → STFT → Mel-spectrogram → Data augmentation (SpecAugment + Mixup) → Model (CNN14/TIMM) → Fine-tuning → Evaluation (confusion matrix, Grad-CAM)
- Critical path: Spectrogram generation → Data augmentation → Model fine-tuning → Performance evaluation
- Design tradeoffs:
  - Higher sampling rate pre-training improves robustness but increases computational cost
  - ImageNet models outperform audio models, suggesting cross-domain feature transfer is effective, but may require careful adaptation
  - Data augmentation improves generalization but risks over-augmentation
- Failure signatures:
  - Low accuracy across sampling rates indicates poor feature generalization
  - High variance in accuracy across runs suggests instability in fine-tuning
  - Confusion between similar classes (e.g., cargo vs. tanker) indicates weak discriminative features
- First 3 experiments:
  1. Test CNN14-32k fine-tuned on DeepShip data at 4 kHz vs. 32 kHz to confirm robustness across sampling rates
  2. Compare PANN (CNN14) vs. TIMM (ConvNeXtV2-tiny) models on same dataset to validate cross-domain performance
  3. Visualize Grad-CAM heatmaps for correctly vs. misclassified samples to assess attention patterns and interpretability

## Open Questions the Paper Calls Out
- How do pre-trained transformer architectures compare to CNN-based models for underwater acoustic target recognition?
- What is the optimal balance between parameter efficiency and classification accuracy for UATR applications?
- How does self-supervised pre-training on underwater acoustic data compare to transfer learning from audio or image domains?

## Limitations
- Results based on single dataset (DeepShip) with only 4 ship classes, limiting generalizability
- Comparison between PANN and TIMM models doesn't establish causal mechanisms for cross-modal transfer success
- Study doesn't address potential domain shift between terrestrial image data and underwater acoustic spectrograms

## Confidence
- High confidence: Experimental methodology is sound with proper controls and reproducible results
- Medium confidence: Claims about cross-modal feature transfer effectiveness are supported but lack direct evidence of feature reuse mechanisms
- Low confidence: Assertion that lower sampling rate pre-training inherently limits performance needs more rigorous ablation studies

## Next Checks
1. Test best-performing models on additional underwater acoustic datasets with different vessel types and environmental conditions
2. Generate and compare Grad-CAM heatmaps for PANN vs. TIMM models on correctly and incorrectly classified samples
3. Conduct granular analysis of model performance across intermediate sampling rates between tested 8, 16, and 32 kHz