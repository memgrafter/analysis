---
ver: rpa2
title: "Towards a More Inclusive AI: Progress and Perspectives in Large Language Model\
  \ Training for the S\xE1mi Language"
arxiv_id: '2405.05777'
source_url: https://arxiv.org/abs/2405.05777
tags:
- language
- training
- languages
- data
- also
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work studies the challenge of training large language models\
  \ for ultra-low-resource languages, specifically the S\xE1mi language. It creates\
  \ the SALT dataset of 22 million tokens and conducts experiments with decoder-only\
  \ and sequence-to-sequence models, testing multilingual training strategies and\
  \ pretraining approaches."
---

# Towards a More Inclusive AI: Progress and Perspectives in Large Language Model Training for the Sámi Language

## Quick Facts
- arXiv ID: 2405.05777
- Source URL: https://arxiv.org/abs/2405.05777
- Reference count: 32
- Primary result: Creates SALT dataset of 22M tokens and demonstrates decoder-only models outperform sequence-to-sequence for Sámi language modeling

## Executive Summary
This paper addresses the challenge of training large language models for ultra-low-resource languages, focusing on the Sámi language spoken by approximately 25,000 people across Norway, Sweden, Finland, and Russia. The authors created the Sámi Language Training (SALT) dataset with 22 million tokens from web-crawled sources, Wikipedia, and fiction. They conducted comprehensive experiments comparing decoder-only (BLOOM) and sequence-to-sequence (Pegasus) architectures, tested multilingual training strategies, and explored pretraining approaches using different source languages. The study provides practical insights for developing language technologies for indigenous and minority languages.

## Method Summary
The authors developed the SALT dataset by collecting and cleaning text from multiple sources including CommonCrawl, Wikipedia, and fiction works in Sámi. They experimented with two model architectures: BLOOM (decoder-only) and Pegasus (sequence-to-sequence), both with 176M parameters. The training pipeline involved pretraining on various source languages (Finnish, English, or jointly on 46 languages), then fine-tuning on the SALT dataset. Experiments tested solo Sámi training, joint multilingual training, and different pretraining strategies. Models were evaluated using perplexity and BLEU scores on translation tasks between Sámi and English.

## Key Results
- Decoder-only models (BLOOM) significantly outperform sequence-to-sequence models (Pegasus) for Sámi language modeling
- Pretraining on semantically similar languages (Finnish) improves performance, while pretraining on unrelated languages (English) hurts results
- Multilingual training benefits model performance, but solo pretraining on closely related languages outperforms joint training with less related languages

## Why This Works (Mechanism)
The superior performance of decoder-only models for Sámi stems from their architecture being better suited for language modeling tasks where the entire context is available during training and generation. The effectiveness of semantically similar language pretraining works because these languages share structural and lexical similarities that provide relevant inductive biases, while unrelated languages introduce conflicting patterns that degrade performance. Multilingual training benefits arise from the shared representations learned across languages, though the quality of these benefits depends on the semantic proximity of the training languages to Sámi.

## Foundational Learning
- **Ultra-low-resource language modeling**: Why needed - Sámi has minimal digital text compared to major languages; Quick check - Compare token counts across languages to verify resource scarcity
- **Semantic similarity in language families**: Why needed - Determines effectiveness of cross-lingual transfer learning; Quick check - Analyze shared vocabulary and grammatical structures between pretraining languages and Sámi
- **Pretraining-finetuning paradigm**: Why needed - Enables leveraging larger monolingual corpora when parallel data is scarce; Quick check - Track performance gains from pretraining versus direct fine-tuning

## Architecture Onboarding
Component map: Web crawling -> Text cleaning -> Dataset creation -> Model pretraining -> Fine-tuning -> Evaluation
Critical path: SALT dataset creation → Model pretraining (language selection) → Fine-tuning on Sámi → Performance evaluation
Design tradeoffs: Larger models might capture more patterns but require more data; multilingual training expands coverage but may dilute language-specific features
Failure signatures: Poor BLEU scores indicate inadequate pretraining or fine-tuning; high perplexity suggests insufficient model capacity or data
First experiments:
1. Compare baseline model performance on Sámi without pretraining
2. Test pretraining on Finnish versus English to validate semantic similarity hypothesis
3. Evaluate solo versus multilingual training configurations

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The study focuses on a single ultra-low-resource language, limiting generalizability of findings
- The 22M token dataset size, while substantial for Sámi, may still be insufficient for optimal model performance
- Reliance on automated metrics (BLEU, perplexity) may not fully capture translation quality, especially for culturally specific content
- Limited testing of pretraining languages (only Finnish and English) constrains understanding of optimal pretraining strategies

## Confidence
- Decoder-only model superiority: High
- Semantic similarity in pretraining: High
- Multilingual training benefits: Medium

## Next Checks
1. Test the pretraining and fine-tuning strategies on additional ultra-low-resource languages to verify generalizability of findings
2. Expand the multilingual pretraining experiments to include more language combinations and quantify the contribution of each language
3. Evaluate model performance using human assessment of translation quality in addition to automated metrics, particularly for culturally specific content