---
ver: rpa2
title: Transformers, Contextualism, and Polysemy
arxiv_id: '2404.09577'
source_url: https://arxiv.org/abs/2404.09577
tags:
- meaning
- theory
- transformer
- embeddings
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how the transformer architecture in large language
  models (LLMs) can inform debates in philosophy of language about context-sensitivity
  and polysemy. The author argues that transformers implement a novel meaning theory
  that combines elements of contextualism and semantic minimalism regarding context-sensitivity,
  and merges core representation and meaning continuity approaches to polysemy.
---

# Transformers, Contextualism, and Polysemy

## Quick Facts
- arXiv ID: 2404.09577
- Source URL: https://arxiv.org/abs/2404.09577
- Authors: Jumbly Grindrod
- Reference count: 12
- This paper argues that transformer architecture implements a hybrid meaning theory combining contextualism and semantic minimalism, while also merging core representation and meaning continuity approaches to polysemy

## Executive Summary
This paper examines how the transformer architecture in large language models (LLMs) can inform debates in philosophy of language about context-sensitivity and polysemy. The author argues that transformers implement a novel meaning theory that combines elements of contextualism and semantic minimalism regarding context-sensitivity, and merges core representation and meaning continuity approaches to polysemy. Specifically, while transformers allow extensive context-sensitivity through modulation of token embeddings, they also maintain robust notions of standing meaning. For polysemy, transformers use token embeddings as core representations that are modified to generate various readings, while representing meaning in a continuous vector space. This hybrid approach challenges traditional distinctions between contextualism and minimalism, as well as between homonymy and polysemy.

## Method Summary
The paper presents a theoretical analysis of transformer architecture through the lens of philosophy of language, examining how the technical mechanisms of transformers map onto competing theories of meaning. The author synthesizes empirical findings from the NLP literature about token embeddings, contextualized embeddings, and self-attention mechanisms to support claims about how transformers implement particular meaning theories. The analysis draws on existing research about vector space semantics, distributional semantics, and the mathematical properties of self-attention to construct an argument about transformers as hybrid meaning theories.

## Key Results
- Transformers implement a hybrid meaning theory combining contextualism and semantic minimalism regarding context-sensitivity
- Transformers merge core representation and meaning continuity approaches to polysemy
- The transformer theory challenges traditional distinctions between homonymy and polysemy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The transformer architecture implements a hybrid meaning theory combining contextualism and semantic minimalism regarding context-sensitivity
- Mechanism: Self-attention layers allow extensive context-modulation of token embeddings while maintaining robust standing meanings through token embeddings
- Core assumption: Transformers successfully interpret linguistic meanings through contextualized embeddings
- Evidence anchors:
  - [abstract] "while transformers allow extensive context-sensitivity through modulation of token embeddings, they also maintain robust notions of standing meaning"
  - [section] "the transformer theory makes use of token embeddings as a kind of standing meaning while allowing for contextual processes so extensive that the standing meaning will effectively never figure in utterance content"
  - [corpus] Found 25 related papers with average FMR=0.49, indicating moderate relevance in the corpus
- Break condition: If token embeddings are proven to be deficient representations of word meaning compared to contextualized embeddings

### Mechanism 2
- Claim: The transformer theory implements both core representation and meaning continuity approaches to polysemy
- Mechanism: Token embeddings serve as core representations that are modified through self-attention to generate various readings, while representing meaning in continuous vector space
- Core assumption: Token embeddings encode meaningful semantic information comparable to static embeddings
- Evidence anchors:
  - [abstract] "for polysemy, transformers use token embeddings as core representations that are modified to generate various readings, while representing meaning in a continuous vector space"
  - [section] "nearest neighbour relations in token embeddings show semantically related words are closer in space"
  - [corpus] Nair et al. (2020) found correlation between sense distance in BERT and human judgments of relatedness
- Break condition: If empirical evidence shows token embeddings cannot encode polysemous distinctions

### Mechanism 3
- Claim: The transformer theory challenges traditional distinctions between homonymy and polysemy
- Mechanism: Both homonymous and polysemous expressions are treated through the same mechanism of core representation modification, with no strong distinction in implementation
- Core assumption: Homonymy can be represented within continuous vector space through single token embeddings that encode multiple senses
- Evidence anchors:
  - [abstract] "this hybrid approach challenges traditional distinctions between contextualism and minimalism, as well as between homonymy and polysemy"
  - [section] "the homonymy of expressions like 'bank' and 'rock' are treated in the same way that polysemy is treated"
  - [corpus] Yaghoobzadeh et al. (2019) showed probing classifiers can predict whether expressions are ambiguous
- Break condition: If homonymous expressions require fundamentally different processing than polysemous ones

## Foundational Learning

- Concept: Vector space semantics and distributional semantics
  - Why needed here: The transformer theory relies on understanding how word meanings can be represented as vectors in high-dimensional space
  - Quick check question: Can you explain why semantically similar words tend to have similar vector representations in distributional models?

- Concept: Self-attention mechanism and its mathematical formulation
  - Why needed here: The core mechanism by which transformers introduce context-sensitivity operates through self-attention
  - Quick check question: How does the self-attention procedure transform input embeddings into contextualized embeddings?

- Concept: Distinction between token embeddings and contextualized embeddings
  - Why needed here: Understanding the dual nature of representation (standing meaning vs. contextual meaning) is crucial for the hybrid theory
  - Quick check question: What is the relationship between token embeddings and contextualized embeddings in transformer models?

## Architecture Onboarding

- Component map:
  - Input layer: Token embeddings (standing meanings)
  - Positional embeddings: Word order information
  - Self-attention layers: Context-modulation mechanism
  - Feed-forward networks: Non-linear transformations
  - Output layer: Contextualized embeddings (interpreted meanings)

- Critical path: Token embedding → Self-attention processing → Contextualized embedding
- Design tradeoffs:
  - Sequential vs. parallel processing (RNNs vs. transformers)
  - Fixed vs. dynamic context windows
  - Number of self-attention heads vs. computational efficiency
- Failure signatures:
  - Inability to capture long-range dependencies
  - Context window limitations
  - Token embedding deficiencies affecting contextualized output
- First 3 experiments:
  1. Compare nearest neighbors of token embeddings vs. contextualized embeddings for polysemous words
  2. Measure correlation between sense distance in embeddings and human relatedness judgments
  3. Test whether probing classifiers can predict sense classes from token embeddings alone

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis relies heavily on theoretical interpretation of transformer mechanisms through philosophical concepts
- The claim that token embeddings serve as "standing meanings" is contentious and requires empirical validation
- The treatment of homonymy and polysemy through identical mechanisms may oversimplify important distinctions

## Confidence
- **Medium confidence**: The claim that transformers implement a hybrid theory combining contextualism and semantic minimalism regarding context-sensitivity
- **Medium confidence**: The assertion that transformers merge core representation and meaning continuity approaches to polysemy
- **Low confidence**: The claim that transformers challenge traditional distinctions between homonymy and polysemy

## Next Checks
1. **Empirical validation of token embeddings as standing meanings**: Conduct controlled experiments comparing token embeddings and contextualized embeddings for polysemous words, measuring how much semantic information is preserved in token embeddings versus what requires contextualization.

2. **Direct comparison of homonymy vs. polysemy processing**: Design experiments specifically testing whether homonymous expressions like "bank" (financial institution vs. river edge) are processed differently than polysemous expressions like "chicken" (animal vs. meat).

3. **Human-LLM meaning alignment study**: Conduct controlled experiments comparing human judgments of relatedness and sense distinctions with nearest-neighbor distances in both token and contextualized embeddings.