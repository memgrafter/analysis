---
ver: rpa2
title: Missing Modality Prediction for Unpaired Multimodal Learning via Joint Embedding
  of Unimodal Models
arxiv_id: '2407.12616'
source_url: https://arxiv.org/abs/2407.12616
tags:
- missing
- multimodal
- learning
- modality
- unimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of missing modality prediction in
  multimodal learning where data for certain modalities is absent during training
  and inference. The authors propose a framework that integrates parameter-efficient
  fine-tuning of unimodal pretrained models with a self-supervised joint-embedding
  learning method using Variance-Invariance-Covariance Regularization (VICReg).
---

# Missing Modality Prediction for Unpaired Multimodal Learning via Joint Embedding of Unimodal Models

## Quick Facts
- arXiv ID: 2407.12616
- Source URL: https://arxiv.org/abs/2407.12616
- Authors: Donggeun Kim; Taesup Kim
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance on MM-IMDb, UPMC Food-101, and Hateful Memes datasets with improvements of 2.9-6.5% over baselines in severely missing modality cases

## Executive Summary
This paper addresses the challenge of missing modality prediction in multimodal learning where data for certain modalities is absent during training and inference. The authors propose a framework that integrates parameter-efficient fine-tuning of unimodal pretrained models with a self-supervised joint-embedding learning method using Variance-Invariance-Covariance Regularization (VICReg). By leveraging read-only prompts, their approach effectively predicts missing modality embeddings while preserving knowledge from unimodal pretraining. The method demonstrates robustness across various missing modality scenarios and achieves state-of-the-art performance on multiple benchmark datasets.

## Method Summary
The proposed method combines pretrained unimodal encoders (DeiT III for images, SimCSE for text) with parameter-efficient fine-tuning using BitFit, which updates only bias terms while freezing all other parameters. A feature predictor with read-only prompts predicts missing modality embeddings through VICReg-based joint embedding learning. The framework uses a late-fusion strategy combining pre-softmax logits from each modality. The VICReg loss prevents embedding collapse and enhances cross-modality predictability through variance, invariance, and covariance regularization terms.

## Key Results
- Achieves state-of-the-art performance with 2.9-6.5% improvements over baseline methods in severely missing cases
- Demonstrates superior performance across MM-IMDb (F1-Macro score), UPMC Food-101 (classification accuracy), and Hateful Memes (AUROC) datasets
- Shows robustness when handling unpaired data and missing modalities during both training and inference phases
- Effective across various missing modality scenarios with flexible adaptation

## Why This Works (Mechanism)

### Mechanism 1
Read-only prompts enable feature prediction without disrupting pretrained unimodal encoders' internal representations. The mechanism uses attention masking to prevent input tokens from attending to prompt tokens, allowing prompts to extract relevant information for feature prediction while preserving original input representations. The CLS token, specialized for the downstream task, is less effective at predicting embeddings of other modalities than learnable prompts.

### Mechanism 2
VICReg loss function improves predictability of embeddings between different modalities for the missing modality problem. VICReg comprises variance, invariance, and covariance terms that prevent embedding collapse and enhance predictability. Adding covariance and variance regularization terms to the invariance term guides the model toward producing informative representations that are easily predictable from other modalities.

### Mechanism 3
Parameter-efficient fine-tuning (PEFT) with BitFit allows the model to leverage existing knowledge while adapting to the target task. PEFT methods update only a subset of parameters (e.g., bias terms) during fine-tuning, preserving knowledge from pretraining while adapting to the target task. Slightly tuning the encoders across all layers is sufficient to train the encoder's predictability while optimizing the target task.

## Foundational Learning

- **Self-supervised learning**
  - Why needed here: Allows unimodal encoders to be trained on large-scale unlabeled data, making them more readily accessible than multimodal models
  - Quick check question: What is the primary advantage of using self-supervised learning for pretraining unimodal encoders?

- **Parameter-efficient fine-tuning (PEFT)**
  - Why needed here: Allows the model to adapt to the target task while preserving knowledge from pretraining, crucial when working with limited data or computational resources
  - Quick check question: How does BitFit, a PEFT method, differ from traditional fine-tuning in terms of parameter updates?

- **Variance-Invariance-Covariance Regularization (VICReg)**
  - Why needed here: Prevents embedding collapse and enhances predictability of embeddings between different modalities, essential for the missing modality problem
  - Quick check question: What are the three components of the VICReg loss function, and what is the purpose of each component?

## Architecture Onboarding

- **Component map**: Unimodal pretrained encoders -> Parameter-efficient fine-tuning (PEFT) layer -> Read-only prompts for feature prediction -> Feature predictor -> VICReg loss function -> Classifier

- **Critical path**: 
  1. Input data is processed through the unimodal pretrained encoders
  2. Read-only prompts are added to the input data and processed through the encoders
  3. The feature predictor uses the prompts to predict the embedding of the missing modality
  4. The predicted embedding is combined with the available modality's embedding
  5. The combined embedding is passed through the classifier for the final prediction

- **Design tradeoffs**:
  - Using unimodal pretrained encoders leverages existing knowledge but may limit joint representation learning
  - Read-only prompts enable feature prediction without disrupting encoders but require careful attention masking tuning
  - VICReg enhances predictability but adds computational overhead and requires regularization coefficient tuning

- **Failure signatures**:
  - If attention masking fails, prompts may interfere with encoder representations, degrading performance
  - If VICReg hyperparameters are not tuned correctly, embeddings may collapse or become uninformative
  - If pretrained encoders lack sufficient knowledge or task differs significantly from pretraining, model may struggle to adapt

- **First 3 experiments**:
  1. Verify read-only prompts are correctly implemented and attention masking works as intended
  2. Test VICReg loss function with different hyperparameters to find optimal settings
  3. Compare performance with and without PEFT to confirm knowledge preservation while adapting

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed method scale with different prompt token lengths in the feature predictor, and is there an optimal length for different modalities? The paper conducts an ablation study on prompt token length under the complete training setting, showing performance improvements up to a length of 6 for MM-IMDb, but only explores prompt lengths up to 6 and does not investigate longer lengths or compare results across different modalities and datasets.

### Open Question 2
How does the proposed method perform when trained with different missing ratios beyond 70%, particularly in extremely high missing scenarios (e.g., 90% missing)? The paper explores varying missing rates from the fixed 70% under the both missing training/testing setting and shows performance degradation at 90% missing, but does not provide a comprehensive analysis across a wide range of missing ratios or investigate the minimum paired data required for effective learning.

### Open Question 3
How does the proposed method compare to other parameter-efficient fine-tuning (PEFT) methods in terms of performance and computational efficiency? The paper conducts a comparative analysis with Layer Normalization (LN) Tuning, Prefix Tuning, and Adapter Tuning under the complete training setting, showing BitFit outperforms others, but the comparison is limited to a single training setting and does not explore other PEFT methods or provide a comprehensive evaluation across different scenarios.

## Limitations

- Effectiveness hinges on three critical mechanisms (read-only prompts, VICReg, BitFit) that lack direct empirical validation within the work itself
- Specific hyperparameters for VICReg coefficients (λ, µ, ν) and balancing parameter α are not thoroughly explored or justified for the multimodal setting
- The assumption that updating only bias terms via BitFit is sufficient for adaptation is not fully validated through comprehensive ablation studies

## Confidence

- **High Confidence**: Overall framework architecture and problem formulation are clearly defined and reproducible
- **Medium Confidence**: VICReg loss implementation and its role in improving predictability, supported by Table 3 results
- **Low Confidence**: Read-only prompts mechanism's effectiveness in preserving encoder representations, as this is asserted rather than empirically demonstrated

## Next Checks

1. **Attention Masking Verification**: Conduct an ablation study comparing standard prompts versus read-only prompts to verify that the attention masking mechanism prevents prompt tokens from interfering with encoder representations during training.

2. **VICReg Hyperparameter Sensitivity**: Perform a systematic grid search over VICReg coefficients (λ, µ, ν) and the balancing parameter α to determine their optimal values and sensitivity for the multimodal setting.

3. **Prompt Token Length Impact**: Evaluate model performance across different prompt token lengths (similar to Table 2 analysis) to determine the optimal number of prompt tokens needed for effective feature prediction without overfitting.