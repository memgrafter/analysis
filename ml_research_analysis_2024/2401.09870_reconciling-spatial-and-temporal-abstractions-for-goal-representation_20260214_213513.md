---
ver: rpa2
title: Reconciling Spatial and Temporal Abstractions for Goal Representation
arxiv_id: '2401.09870'
source_url: https://arxiv.org/abs/2401.09870
tags:
- abstraction
- goal
- reachability
- learning
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes STAR, a three-level HRL algorithm that combines
  spatial and temporal abstractions for goal representation. The core idea is to learn
  a reachability-aware abstraction of the state space and use it to guide the learning
  of hierarchical policies.
---

# Reconciling Spatial and Temporal Abstractions for Goal Representation

## Quick Facts
- arXiv ID: 2401.09870
- Source URL: https://arxiv.org/abs/2401.09870
- Authors: Mehdi Zadem; Sergio Mover; Sao Mai Nguyen
- Reference count: 40
- Key outcome: STAR is a three-level HRL algorithm that combines spatial and temporal abstractions for goal representation, outperforming state-of-the-art approaches in complex continuous control tasks.

## Executive Summary
This paper proposes STAR, a hierarchical reinforcement learning (HRL) algorithm that integrates spatial and temporal abstractions for effective goal representation in complex continuous control tasks. STAR employs a three-level architecture with Navigator, Manager, and Controller agents, where the Navigator selects abstract goals from a reachability-aware state space abstraction, the Manager selects intermediate subgoals, and the Controller executes actions in the environment. The authors provide theoretical guarantees on policy suboptimality under the learned abstraction and demonstrate superior data efficiency and scalability compared to baselines like HIRO, HRAC, LESSON, and GARA on Ant Maze environments.

## Method Summary
STAR is a three-level HRL algorithm that combines spatial and temporal abstractions to learn effective goal representations. The method begins with an initial exploration phase to construct a coarse abstraction of the state space based on reachability analysis. The Navigator agent then selects abstract goals from this refined abstraction, guiding the Manager agent to choose intermediate subgoals that help reach the abstract goals. Finally, the Controller agent learns to take actions in the environment to achieve the selected subgoals. The spatial abstraction is refined online based on the agent's experience, ensuring that the abstract goals are reachable and meaningful. The authors provide theoretical guarantees on the suboptimality of policies learned under this abstraction and evaluate STAR on complex continuous control tasks, showing improved data efficiency and scalability compared to state-of-the-art approaches.

## Key Results
- STAR outperforms state-of-the-art HRL methods (HIRO, HRAC, LESSON, GARA) in terms of data efficiency and scalability on complex continuous control tasks.
- The learned reachability-aware abstraction effectively decomposes the environment, identifying areas of interest and guiding the agent towards optimal behavior.
- Theoretical guarantees are provided on the suboptimality of policies learned under the proposed abstraction, assuming smoothness of environment dynamics and effective reachability analysis.

## Why This Works (Mechanism)
STAR works by combining spatial and temporal abstractions to create a structured goal representation that guides the learning of hierarchical policies. The spatial abstraction, learned through reachability analysis, decomposes the state space into meaningful regions, while the temporal abstraction allows the agent to plan over longer horizons by selecting abstract goals and intermediate subgoals. This hierarchical structure enables more efficient exploration and learning, as the agent can focus on reaching abstract goals rather than individual states, and the learned abstraction provides a roadmap for navigating the environment.

## Foundational Learning
- **Hierarchical Reinforcement Learning**: Why needed: To scale RL to complex tasks by decomposing them into simpler subtasks. Quick check: Compare performance of flat RL vs. HRL on a multi-stage task.
- **Goal-Conditioned RL**: Why needed: To learn policies that can achieve various goals, enabling generalization and transfer. Quick check: Evaluate policy performance on unseen goals after training on a set of goals.
- **Neural Network Reachability Analysis**: Why needed: To reason about the behavior of neural networks and ensure the safety and reliability of learned policies. Quick check: Verify the correctness of reachability analysis on a simple, well-understood neural network.

## Architecture Onboarding

### Component Map
```
Environment -> Controller -> Manager -> Navigator -> Abstraction Refinement
```

### Critical Path
1. Navigator selects abstract goal from reachability-aware abstraction.
2. Manager selects intermediate subgoal to help reach abstract goal.
3. Controller learns to take actions in the environment to achieve subgoal.
4. Abstraction is refined based on the agent's experience and reachability analysis.

### Design Tradeoffs
- Coarse vs. fine-grained abstraction: A coarser abstraction enables faster learning but may limit the agent's ability to navigate complex environments, while a finer abstraction provides more precise guidance but requires more data to learn effectively.
- Online vs. offline abstraction refinement: Online refinement allows the abstraction to adapt to the agent's experience but may introduce instability, while offline refinement provides a stable abstraction but may not capture the nuances of the learned policies.

### Failure Signatures
- Poor performance due to ineffective spatial abstraction refinement: Monitor the frequency of goal visits and the evolution of the abstraction during training.
- Non-stationarity issues in the Manager and Navigator policies: Track the error in the forward model predictions over time to assess policy stability.

### First Experiments
1. Train and evaluate STAR on the Ant Maze environment, comparing performance with HIRO, HRAC, LESSON, and GARA baselines.
2. Analyze the learned reachability-aware abstraction to understand how it decomposes the environment and guides the agent's behavior.
3. Conduct an ablation study to isolate the contributions of the spatial and temporal abstractions to the overall performance.

## Open Questions the Paper Calls Out
- How does STAR's performance compare to other HRL algorithms in environments with stochastic dynamics?
- What is the impact of the Navigator's exploration strategy on the quality of the learned goal abstraction?
- How does the choice of the oracle function Ïˆ(s) affect STAR's performance in different environments?

## Limitations
- Theoretical guarantees rely on assumptions about the smoothness of environment dynamics and the effectiveness of reachability analysis.
- Reliance on Ai2 for neural network reachability analysis may limit scalability to higher-dimensional state spaces or more complex neural network architectures.
- Generalization of results to domains beyond the tested continuous control tasks remains an open question.

## Confidence
- **High**: Empirical results showing improved data efficiency and scalability compared to baseline methods on the tested environments.
- **Medium**: Theoretical guarantees on policy suboptimality under the proposed abstraction.
- **Low**: Generalization of results to domains beyond the tested continuous control tasks.

## Next Checks
1. Evaluate STAR on a broader range of environments, including discrete control tasks and domains with higher-dimensional state spaces, to assess scalability and generalization.
2. Investigate the impact of varying the hyperparameters for the forward model training (e.g., learning rate, buffer size) on the performance and stability of the learned policies.
3. Conduct ablation studies to isolate the contributions of the spatial and temporal abstractions to the overall performance, and analyze the interplay between the Navigator, Manager, and Controller agents.