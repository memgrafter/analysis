---
ver: rpa2
title: Log Probabilities Are a Reliable Estimate of Semantic Plausibility in Base
  and Instruction-Tuned Language Models
arxiv_id: '2403.14859'
source_url: https://arxiv.org/abs/2403.14859
tags:
- sentence
- plausibility
- logprobs
- human
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically compares the effectiveness of log probabilities
  (LogProbs) and zero-shot prompting for evaluating semantic plausibility in language
  models (LMs), both base and instruction-tuned. The authors find that LogProbs are
  a more reliable measure of plausibility than prompting, which yields inconsistent
  and often poor results across models.
---

# Log Probabilities Are a Reliable Estimate of Semantic Plausibility in Base and Instruction-Tuned Language Models

## Quick Facts
- arXiv ID: 2403.14859
- Source URL: https://arxiv.org/abs/2403.14859
- Reference count: 10
- Log probabilities provide a more reliable measure of semantic plausibility than prompting across base and instruction-tuned language models

## Executive Summary
This study systematically compares log probabilities (LogProbs) and zero-shot prompting for evaluating semantic plausibility in language models. The authors find that LogProbs are a more reliable measure than prompting, which yields inconsistent results across models. Instruction-tuning generally does not improve LogProbs sensitivity to plausibility and sometimes decreases it. Context mostly modulates LogProbs in expected ways, matching human judgments, though this effect is more reliable at the word level than at the sentence level.

## Method Summary
The study evaluates semantic plausibility using three datasets: EventsAdapt, DTFit, and a context-dependent dataset from Jouravlev et al. (2019). Three language model families (Mistral, Falcon, MPT) are tested in both base and instruction-tuned versions. LogProbs are computed as the sum of token-level log probabilities, while prompting uses four natural language prompts. Performance is measured against human plausibility judgments using binary accuracy for single sentences and three novel metrics for context-sensitive plausibility at both word and sentence levels.

## Key Results
- Log probabilities consistently outperform prompting for semantic plausibility judgments across model architectures
- Instruction-tuning does not improve LogProbs sensitivity to plausibility and sometimes decreases it
- Context modulates LogProbs at the word level but not reliably at the sentence level, showing a "re-balancing" effect

## Why This Works (Mechanism)

### Mechanism 1
Log probabilities provide a more stable measure of semantic plausibility than direct prompting by bypassing variability in prompt interpretation. They calculate the sum of log-likelihoods of each token conditioned on preceding tokens, which is more directly accessible than generated responses. This works when pretraining data encodes semantic plausibility information.

### Mechanism 2
Instruction tuning does not substantially improve log probability sensitivity to semantic plausibility because it primarily affects surface response characteristics rather than underlying knowledge representation. The distributional patterns learned during pretraining contain sufficient plausibility information that instruction tuning doesn't enhance.

### Mechanism 3
Context modulates log probabilities in ways that align with human plausibility judgments at the word level but not reliably at the sentence level. Models adjust probabilities of critical words based on context, but the global sentence probability is less affected due to probability "re-balancing" in the post-target region.

## Foundational Learning

- **Log probability calculation**: Understanding how log probabilities are computed is essential for interpreting results and knowing when this metric is appropriate. *Quick check*: If a sentence has tokens with individual log probabilities of -1.2, -0.8, and -2.1, what is the total log probability of the sentence?

- **Minimal pairs experimental design**: The study uses minimal pairs to control for confounds and isolate semantic plausibility effects. *Quick check*: In a minimal pairs setup, if sentence A is "The cat sat on the mat" and sentence B is "The cat sat on the idea," what linguistic property is being manipulated?

- **Context-dependent plausibility**: The second experiment tests how context affects plausibility judgments, crucial for understanding log probability metric limitations. *Quick check*: If context sentence is "The canary had a beautiful voice" and target sentence is "The bird had a little beak," is this more or less plausible than if the context was "The girl was very cute"?

## Architecture Onboarding

- **Component map**: Datasets (EventsAdapt, DTFit, Jouravlev et al. 2019) -> Models (Mistral, Falcon, MPT base/instruction-tuned) -> Log probability calculation -> Comparison with human judgments -> Analysis of context effects
- **Critical path**: Data → Model → Log probability calculation → Comparison with human judgments → Analysis of context effects
- **Design tradeoffs**: Log probabilities are model-agnostic and stable but may miss complex reasoning; prompting can capture nuanced responses but is brittle and model-dependent; instruction tuning may align outputs with human expectations but doesn't necessarily improve underlying knowledge representation.
- **Failure signatures**: Prompting yields chance-level performance; log probability sensitivity drops significantly for instruction-tuned models; context effects disappear at the sentence level; performance gaps between models on the same task.
- **First 3 experiments**:
  1. Run the EventsAdapt AI subset with Mistral base model using log probabilities and compare with human judgments to verify basic plausibility detection
  2. Test the same subset with Mistral instruction-tuned model to observe any changes in log probability patterns
  3. Apply context to a sample of SemAnom items and measure changes in target word vs. sentence log probabilities to observe the re-balancing effect

## Open Questions the Paper Calls Out

- How does instruction tuning affect the alignment between log probability scores and human plausibility judgments for complex semantic relationships beyond event plausibility?
- Can prompt optimization techniques significantly improve the performance of instruction-tuned models in semantic plausibility judgments compared to log probabilities?
- How does the size of language models impact the reliability of log probabilities as a measure of semantic plausibility compared to prompting?

## Limitations

- Context effects show inconsistent patterns at the sentence level, creating uncertainty about when log probabilities can be trusted for holistic plausibility assessment
- The study only tests three model families, limiting generalizability of the instruction-tuning findings
- Results may not generalize beyond the specific datasets used, which focus on event plausibility

## Confidence

- **High Confidence**: Comparative advantage of LogProbs over prompting for single-sentence plausibility judgments
- **Medium Confidence**: Instruction-tuning does not improve LogProb sensitivity and may decrease it
- **Medium Confidence**: Context modulates LogProbs at word level but not reliably at sentence level

## Next Checks

1. Test LogProb vs. prompting comparison on additional model families (e.g., LLaMA, Claude) and parameter scales (3B, 13B, 70B) to determine if patterns hold across broader LM range
2. Design experiments to isolate whether sentence-level re-balancing effect is due to token position patterns, attention mechanisms, or probability normalization
3. Evaluate LogProb sensitivity to plausibility on datasets from different domains (legal text, scientific writing, creative fiction) to assess generalization beyond tested datasets