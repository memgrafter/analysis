---
ver: rpa2
title: Leveraging Inter-Chunk Interactions for Enhanced Retrieval in Large Language
  Model-Based Question Answering
arxiv_id: '2408.02907'
source_url: https://arxiv.org/abs/2408.02907
tags:
- retrieval
- evidence
- question
- chunks
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of multi-document question answering
  (MDQA) by proposing a new retrieval framework called IIER (Inter-chunk Interactions
  to Enhance Retrieval). IIER captures the internal connections between document chunks
  by considering three types of interactions: structural, keyword, and semantic.'
---

# Leveraging Inter-Chunk Interactions for Enhanced Retrieval in Large Language Model-Based Question Answering

## Quick Facts
- arXiv ID: 2408.02907
- Source URL: https://arxiv.org/abs/2408.02907
- Reference count: 40
- Primary result: IIER achieves accuracy improvements of up to 15% on 2WikiMQA and 6.0% on HotpotQA compared to previous methods.

## Executive Summary
This paper addresses the challenge of multi-document question answering (MDQA) by proposing IIER (Inter-chunk Interactions to Enhance Retrieval), a novel framework that captures internal connections between document chunks through structural, keyword, and semantic interactions. These interactions are modeled as a Chunk-Interaction Graph (CIG) where chunks are nodes and their relationships are edges with attributes. A graph-based evidence chain retriever then iteratively searches this graph to construct reasoning paths, which are provided to a large language model (LLM) as context for answering questions. Experiments on four datasets demonstrate significant performance improvements over strong baselines.

## Method Summary
IIER constructs a Chunk-Interaction Graph (CIG) where each document is represented as a graph with chunks as nodes and edges representing structural, keyword, and semantic interactions. The framework first builds the CIG by segmenting documents, extracting keywords, computing embeddings, and establishing edges with weights. A graph-based evidence chain retriever then iteratively selects relevant chunks by evaluating neighbor nodes using a scoring model that combines query, path history, neighbor text, and edge interactions. The retriever outputs ordered evidence chains that preserve reasoning logic, which are provided to the LLM as context for answer generation.

## Key Results
- IIER outperforms strong baselines on four datasets, achieving up to 15% accuracy improvement on 2WikiMQA and 6.0% on HotpotQA.
- Evidence chains significantly enhance performance compared to unordered chunk sets, demonstrating the effectiveness of preserving retrieval order.
- The three interaction types (structural, keyword, semantic) effectively model chunk relationships when combined in the CIG.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Chunk-Interaction Graph (CIG) enables retrieval beyond local semantic similarity by modeling structural, keyword, and semantic interactions as edge attributes.
- **Mechanism**: CIG transforms each document into a unified graph where chunks are nodes and their interactions are edges. The retriever can then perform multi-hop reasoning across the graph to reach supporting evidence even when direct semantic similarity is low.
- **Core assumption**: Interactions between chunks (structural, semantic, keyword) are sufficient to capture the full reasoning chain needed for accurate answer generation.
- **Evidence anchors**:
  - [abstract] "This framework captures the internal connections between document chunks by considering three types of interactions: structural, keyword, and semantic."
  - [section] "By mining and representing these interactions, we can overcome the limitations of low similarity within the search scope."
- **Break condition**: If interaction types are incomplete or noisy, the graph may lead the retriever to irrelevant chunks or fail to connect relevant ones.

### Mechanism 2
- **Claim**: The evidence chain retriever uses both graph topology and text embeddings to iteratively select the next hop that maximizes relevance to the question.
- **Mechanism**: Starting from seed nodes, the retriever evaluates neighboring nodes using a scoring model that combines embeddings of the query, path history, neighbor text, and edge interactions. This guides the search toward supporting evidence.
- **Core assumption**: Fine-tuning a language model to predict which neighbor is closest to evidence is effective for guiding retrieval.
- **Evidence anchors**:
  - [section] "We construct a scoring model to evaluate each candidate node by integrating this information into a new representation."
  - [section] "To enhance the scoring model's capability to identify the evidence chunk, we guide the model's training by predicting which neighboring node will most quickly approach the evidence chunk."
- **Break condition**: If the scoring model overfits to training paths or cannot generalize to new questions, retrieval quality degrades.

### Mechanism 3
- **Claim**: Constructing evidence chains (ordered sequences of chunks) improves LLM reasoning by preserving logical flow and context.
- **Mechanism**: Instead of concatenating retrieved chunks randomly or by iteration order, the retriever outputs chains that reflect the reasoning path, which are then fed to the LLM as context.
- **Core assumption**: Maintaining the retrieval order as a chain preserves implicit reasoning logic better than unordered sets.
- **Evidence anchors**:
  - [section] "These chains are then aggregated into a context as external knowledge to assist LLM in reasoning."
  - [section] "the chain format significantly enhances IIER's performance."
- **Break condition**: If the chain length is too long, irrelevant chunks may dilute the context; if too short, insufficient context is provided.

## Foundational Learning

- **Concept**: Graph representation of text data
  - Why needed here: CIG relies on graph structure to connect chunks across documents; understanding node-edge modeling is essential.
  - Quick check question: What are the three types of edges used in CIG and how are their weights determined?

- **Concept**: Embedding-based similarity and retrieval
  - Why needed here: Retriever uses semantic embeddings to compare query and chunks; must understand cosine similarity and embedding models.
  - Quick check question: How is the seed node selected based on keyword coverage and semantic similarity?

- **Concept**: Fine-tuning language models for ranking
  - Why needed here: Scoring model is fine-tuned to rank neighbors by proximity to evidence; requires knowledge of contrastive learning or ranking objectives.
  - Quick check question: What is the training objective used to fine-tune the scoring model?

## Architecture Onboarding

- **Component map**: Question + Wikipedia corpus → CIG Builder → Retriever → Evidence chains → LLM context → Answer
- **Critical path**: Question → Seed selection → Iterative retrieval on CIG → Evidence chain → LLM context → Answer
- **Design tradeoffs**:
  - Graph density vs. retrieval efficiency: More edges improve recall but increase computation.
  - Chain length vs. context relevance: Longer chains provide more context but risk noise.
  - Embedding model choice vs. semantic accuracy: Better embeddings improve similarity but may be slower.
- **Failure signatures**:
  - Low accuracy but high match rate: Retriever finds evidence but LLM fails to use it.
  - Low match rate: Retriever fails to find correct evidence.
  - High variance across datasets: Hyperparameters (T, K) not well tuned.
- **First 3 experiments**:
  1. **Ablation of edge types**: Run IIER with only structural, only semantic, only keyword edges; compare accuracy.
  2. **Chain ordering test**: Compare random, iterative, and chain formats on same retrieved chunks.
  3. **Graph density sweep**: Vary T and K; plot accuracy and match rate vs. density.

## Open Questions the Paper Calls Out
None

## Limitations
- Graph construction scalability is uncertain, with unclear computational overhead for large corpora and dense graphs.
- Retrieval effectiveness on questions requiring long reasoning chains (4+ hops) has not been thoroughly analyzed.
- Performance on non-Wikipedia domains with different document structures and terminology is unknown.

## Confidence

**High Confidence**:
- IIER outperforms strong baselines on the four tested datasets with reported improvements of up to 15% accuracy.
- The three interaction types (structural, keyword, semantic) can be successfully modeled as edge attributes in a graph.
- Evidence chains improve LLM performance compared to unordered chunk sets.

**Medium Confidence**:
- The scoring model effectively guides retrieval toward relevant evidence through iterative neighbor selection.
- CIG overcomes limitations of low semantic similarity within local search scopes.

**Low Confidence**:
- The specific combination of interaction types and graph construction methods is optimal.

## Next Checks
1. **Cross-Domain Evaluation**: Test IIER on non-Wikipedia datasets from different domains (e.g., scientific literature, news articles, technical documentation) to assess generalizability and identify domain-specific limitations.

2. **Chain Length Analysis**: Systematically evaluate retrieval and answer accuracy as a function of required reasoning chain length, identifying the maximum effective chain length and failure patterns for longer chains.

3. **Computational Complexity Profiling**: Measure and analyze the time and memory complexity of CIG construction and traversal across different document sizes and corpus scales, comparing against traditional vector-based retrieval methods.