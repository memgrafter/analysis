---
ver: rpa2
title: The Optimization Landscape of SGD Across the Feature Learning Strength
arxiv_id: '2410.04642'
source_url: https://arxiv.org/abs/2410.04642
tags:
- loss
- learning
- networks
- cifar-5m
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically explores how the feature learning strength\
  \ parameter \u03B3 affects neural network training dynamics and performance in the\
  \ online setting. Through extensive empirical sweeps across \u03B3 and learning\
  \ rate \u03B7, the authors identify distinct optimization regimes: at small \u03B3\
  , networks behave as lazy kernels with \u03B7 scaling as \u03B3\xB2; at large \u03B3\
  , networks learn features with \u03B7 scaling as \u03B3\xB2/L for depth L."
---

# The Optimization Landscape of SGD Across the Feature Learning Strength

## Quick Facts
- **arXiv ID**: 2410.04642
- **Source URL**: https://arxiv.org/abs/2410.04642
- **Reference count**: 40
- **Primary result**: The optimal learning rate scales as η* ∝ γ² for small γ (lazy regime) and η* ∝ γ^(2/L) for large γ (rich regime), with large-γ networks achieving equal or better generalization given sufficient training time.

## Executive Summary
This paper systematically explores how the feature learning strength parameter γ affects neural network training dynamics and performance in the online setting. Through extensive empirical sweeps across γ and learning rate η, the authors identify distinct optimization regimes: at small γ, networks behave as lazy kernels with η scaling as γ²; at large γ, networks learn features with η scaling as γ²/L for depth L. The phase portrait of γ-η reveals a "triangle of optimizability" at large γ, where networks exhibit silent alignment followed by stepwise loss drops. Using optimal learning rate scaling, large-γ networks achieve equal or better generalization than γ=1 networks given sufficient training time.

## Method Summary
The authors systematically sweep across feature learning strength γ and learning rate η in online training of neural networks. They train MLPs, CNNs, ResNets, and Vision Transformers on MNIST-1M, CIFAR-5M, and TinyImageNet datasets, tracking loss trajectories, Hessian spectra, and final performance. The key methodological innovation is the identification of optimal η scaling: η* ∝ γ² for γ ≪ 1 (lazy regime) and η* ∝ γ^(2/L) for γ ≫ 1 (rich regime). This enables direct comparison across different γ values and reveals distinct optimization behaviors including silent alignment and stepwise loss drops at large γ.

## Key Results
- The phase portrait in γ-η space reveals a "triangle of optimizability" at large γ, with distinct convergence boundaries and optimal learning rates
- Large-γ networks exhibit silent alignment where kernel alignment grows before loss drops, following characteristic stepwise patterns
- With optimal learning rate scaling, large-γ networks achieve equal or better generalization than γ=1 networks given sufficient training time
- Different large γ networks converge to similar functions and representations after sufficient training, up to time reparameterization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The learning rate η must scale with γ differently in lazy versus rich regimes to maintain stable optimization.
- Mechanism: In the lazy regime (γ ≪ 1), the Hessian is dominated by the Gauss-Newton term which scales as γ⁻², so the maximum convergent learning rate scales as η_max ~ γ². In the rich regime (γ ≫ 1), feature learning requires weight movement by Θ(1), so η_min ~ γ/T where T is training steps, and η_max ~ γ²/L where L is depth.
- Core assumption: The network is in the online setting where data is fresh at each step, avoiding finite-dataset effects.
- Evidence anchors:
  - [abstract] "We find that the optimal learning rate η* scales non-trivially with γ. In particular, η* ∝ γ2 when γ ≪ 1 and η* ∝ γ2/L when γ ≫ 1"
  - [section] "In kernel regression under MSE loss, the maximum achievable learning rate is proportional to the top eigenvalue of the Hessian. This sets the upper bound to be η ~ γ²"
  - [corpus] Weak - related works discuss learning rate scaling but not specifically γ-dependent scaling in online regime
- Break condition: If the online assumption fails (finite dataset with repeated sampling), SGD noise effects could dominate and change the scaling relationships.

### Mechanism 2
- Claim: Large γ networks exhibit silent alignment where kernel alignment grows before loss drops.
- Mechanism: At early training times, weight updates align the final-layer kernel with the task target through progressive sharpening, but the loss remains flat because function values haven't changed significantly yet. This alignment grows until a critical threshold is reached, causing sudden loss drops.
- Core assumption: The network starts from small initialization and training is sufficiently long to observe the alignment phase.
- Evidence anchors:
  - [abstract] "At large γ, we observe silent alignment as in Atanasov et al. (2022): at early times, the kernel aligns itself to the task before the loss drops"
  - [section] "During this plateau, the NN can be shown to be adapting its features, and appears to align its hidden-layer representations with the task"
  - [corpus] Moderate - Atanasov et al. (2022) observed similar alignment in linear networks but on whitened data
- Break condition: If initialization is too large or training time too short, the silent alignment phase may be skipped entirely.

### Mechanism 3
- Claim: Different large γ networks converge to similar functions and representations after sufficient training.
- Mechanism: As γ increases beyond a threshold, networks follow similar optimization trajectories up to time reparameterization (τ = ηt/γ). The final functions and representations become increasingly similar because they're all optimizing toward the same basin in function space.
- Core assumption: The loss function and data distribution are the same across different γ experiments.
- Evidence anchors:
  - [abstract] "We find networks of different large γ values optimize along similar trajectories up to a reparameterization of time"
  - [section] "We see that lazy networks have nearly identical function outputs, and that rich networks agree in their function outputs at the end of training as well"
  - [corpus] Moderate - Sclocchi et al. (2023) found similar SGD noise effects across regimes but in offline setting
- Break condition: If the optimization trajectory encounters different basins due to initialization differences or if training stops before convergence.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its lazy limit
  - Why needed here: Understanding the lazy regime (γ ≪ 1) requires knowing how NTK parameterization leads to fixed representations during training
  - Quick check question: What happens to the NTK as γ → 0, and how does this affect the Hessian spectrum?

- Concept: Feature learning versus lazy training
  - Why needed here: The paper's central distinction is between networks that learn features (rich regime) versus those that don't (lazy regime), controlled by γ
  - Quick check question: How does the degree of feature learning manifest in the Hessian spectrum and weight movement?

- Concept: Online versus offline training dynamics
  - Why needed here: The paper explicitly focuses on online training to avoid confounding effects from data repetition and finite-sample effects
  - Quick check question: How would the results differ if the same data were reused multiple times during training?

## Architecture Onboarding

- Component map: Neural network models (MLPs, CNNs, ResNets, ViTs) trained online with SGD, with γ controlling feature learning strength and η being the learning rate. The key components are the network architecture, loss function (MSE or cross-entropy), and the γ-η phase space exploration.

- Critical path: 1) Choose architecture and dataset, 2) Set γ and sweep η values, 3) Train until convergence or divergence, 4) Analyze loss trajectories, Hessian spectra, and final performance, 5) Compare across different γ values.

- Design tradeoffs: Larger γ enables better feature learning but requires careful learning rate scaling (η ~ γ²/L) and longer training times. Smaller γ is more stable but limits representational power. The online setting avoids overfitting but may miss some phenomena present in offline training.

- Failure signatures: Loss explosion indicates η is too large for the given γ. No training progress indicates η is too small. Inconsistent results across width suggest µP implementation issues.

- First 3 experiments:
  1. Train a simple MLP on MNIST-1M with small γ (e.g., 1e-3) and verify lazy regime behavior with η ~ γ² scaling
  2. Train the same model with large γ (e.g., 1e3) and verify rich regime behavior with η ~ γ²/L scaling
  3. Compare final loss values and Hessian spectra between the two regimes to confirm the transition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the silent alignment phenomenon scale with network depth and width beyond the simple linear network model?
- Basis in paper: The authors observe silent alignment in realistic networks but only theoretically derive it for a simple linear network model. They mention extending the analysis to more general architectures would be valuable.
- Why unresolved: The linear network model provides intuition but may not capture all nuances of silent alignment in deeper, wider, or nonlinear networks.
- What evidence would resolve it: Systematic empirical studies varying depth, width, and nonlinearity in the scaling of alignment duration, sharpness growth, and feature similarity across different architectures.

### Open Question 2
- Question: What is the precise mechanism driving the stepwise loss drops at large γ, and how does it relate to the progressive sharpening effect?
- Basis in paper: The authors observe characteristic staircase patterns in loss trajectories and link the drops to Hessian growth at the edge of stability, but the underlying mechanism remains unclear.
- Why unresolved: While the correlation with edge of stability is noted, the causal relationship between feature learning, Hessian dynamics, and the discrete nature of the drops is not established.
- What evidence would resolve it: Detailed analysis of the Hessian spectrum evolution, feature similarity metrics, and loss landscape geometry at each step to identify triggering conditions for drops.

### Open Question 3
- Question: How do alternative optimizers like Adam affect the phase portrait and the γ-dependent learning rate scaling compared to SGD?
- Basis in paper: The authors analyze Adam theoretically and empirically, finding different scaling exponents but leaving many questions open about catapult behavior and the optimal learning rate scaling.
- Why unresolved: The analysis is limited to SignSGD and preliminary experiments, with open questions about the precise form of the phase diagram and whether catapults occur.
- What evidence would resolve it: Comprehensive sweeps of γ and η for Adam across multiple architectures and losses, comparing the phase boundaries and convergence properties to SGD.

## Limitations

- The findings are primarily limited to the online training setting with fresh data at each step, which may not generalize to finite-dataset scenarios with data reuse.
- The theoretical analysis relies heavily on the solvable linear network model, which may not capture all nonlinear phenomena observed in deep networks.
- The computational budget required for extensive γ-η sweeps may limit practical applicability.

## Confidence

- **High**: The identification of distinct optimization regimes and their corresponding learning rate scaling laws is well-supported by extensive empirical evidence across multiple architectures and datasets.
- **Medium**: The silent alignment phenomenon and stepwise loss drops are consistently observed but their theoretical explanation remains incomplete.
- **Medium**: The claim that large-γ networks achieve better generalization than γ=1 networks with sufficient training is supported but requires very long training times that may not be practical.

## Next Checks

1. Verify the γ-η scaling relationships in the offline setting with finite datasets to test the robustness of the online-only findings.
2. Conduct ablation studies on initialization scale and training duration to confirm the silent alignment phenomenon isn't an artifact of specific training conditions.
3. Test the feature learning strength parameter γ in more complex architectures (e.g., language models, graph neural networks) to assess the generality of the observed optimization regimes.