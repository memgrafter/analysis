---
ver: rpa2
title: Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A
  Study using the Multilingual Defining Issues Test
arxiv_id: '2402.02135'
source_url: https://arxiv.org/abs/2402.02135
tags:
- moral
- languages
- language
- llms
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explores how moral reasoning abilities of Large Language\
  \ Models (LLMs) vary across languages using the Defining Issues Test. Three LLMs\u2014\
  GPT-4, ChatGPT, and Llama2Chat-70B\u2014were tested across six languages (English,\
  \ Chinese, Spanish, Russian, Hindi, and Swahili) using translated moral dilemmas\
  \ and ethical considerations."
---

# Do Moral Judgment and Reasoning Capability of LLMs Change with Language? A Study using the Multilingual Defining Issues Test

## Quick Facts
- arXiv ID: 2402.02135
- Source URL: https://arxiv.org/abs/2402.02135
- Reference count: 16
- Key outcome: GPT-4 shows consistent multilingual moral reasoning across six languages, while ChatGPT and Llama2Chat-70B underperform in low-resource languages (Hindi, Swahili)

## Executive Summary
This study investigates how the moral reasoning capabilities of large language models (LLMs) vary across different languages using the Defining Issues Test (DIT). The researchers tested three LLMs—GPT-4, ChatGPT, and Llama2Chat-70B—on moral dilemmas translated into six languages: English, Chinese, Spanish, Russian, Hindi, and Swahili. Results reveal that GPT-4 consistently demonstrates superior moral reasoning with minimal performance variance across languages, while ChatGPT and Llama2Chat-70B show significant performance differences, particularly underperforming in Hindi and Swahili. The findings suggest that language influences both the reasoning process and the conclusions reached by LLMs, with implications for their alignment with human values across cultural contexts.

## Method Summary
The study employed the Defining Issues Test (DIT) framework to evaluate moral reasoning in LLMs across six languages. Researchers translated 20 moral dilemmas and 12 ethical considerations using Google Translate API, then prompted three LLMs (GPT-4, ChatGPT, and Llama2Chat-70B) with shuffled options and statements. The models' responses were scored using Kohlberg's Cognitive Moral Development stages, measuring post-conventional, maintaining norms, and personal interests schemas. The methodology involved systematic variation of prompts and temperature settings (0.7-1.0) to ensure robust evaluation of moral reasoning capabilities across linguistic contexts.

## Key Results
- GPT-4 consistently achieves post-conventional moral reasoning scores (comparable to graduate students) across most languages, with minimal performance variance
- ChatGPT and Llama2Chat-70B show significant performance drops in Hindi and Swahili, suggesting resource-dependent limitations
- Moral reasoning capability follows the pattern: English ≈ Spanish > Russian ≈ Chinese > Swahili > Hindi
- Despite high moral reasoning scores, moral judgments vary considerably across languages, indicating language influences both reasoning and conclusions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multilingual moral reasoning capability of LLMs correlates with the amount of training resources available for each language.
- **Mechanism:** LLMs trained predominantly on English data develop stronger reasoning patterns in English, while lower-resource languages receive less exposure, leading to weaker performance. The model's ability to process and generate moral reasoning varies proportionally to the linguistic resources in the pretraining corpus.
- **Core assumption:** The pretraining corpus contains sufficient moral reasoning data in each language to allow the model to learn cultural value patterns.
- **Evidence anchors:**
  - [abstract] "moral reasoning ability for all models, as indicated by the post-conventional score, is substantially inferior for Hindi and Swahili, compared to Spanish, Russian, Chinese and English"
  - [section] "the difference in moral reasoning abilities across languages seem correlated to the amount of resources available or used for training the models"
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.487, average citations=0.0. Top related titles include papers on cross-lingual moral alignment, suggesting active research but limited concrete evidence yet.
- **Break condition:** If pretraining data contains strong moral reasoning examples across all languages regardless of resource size, this mechanism breaks.

### Mechanism 2
- **Claim:** Language influences both the reasoning process and the conclusions reached by LLMs, similar to the human "Foreign-Language Effect" on moral judgment.
- **Mechanism:** When presented with moral dilemmas in a foreign language, LLMs may process them with reduced emotional engagement, leading to more utilitarian choices. The language acts as a cognitive filter that modulates how moral considerations are weighted.
- **Core assumption:** LLMs exhibit cognitive processing patterns analogous to human language-dependent moral judgment shifts.
- **Evidence anchors:**
  - [abstract] "moral judgments varied considerably across languages, suggesting that language influences both the reasoning process and the conclusions reached by LLMs"
  - [section] "it is known that, for humans, moral judgment often depends on the language in which the dilemma is presented (Costa et al., 2014)"
  - [corpus] Weak evidence - no direct corpus support for LLM-specific foreign-language effect, though related human studies exist.
- **Break condition:** If LLM reasoning is purely mechanistic and not influenced by language as a cognitive filter, this mechanism fails.

### Mechanism 3
- **Claim:** LLMs align to cultural values from the right-upper triangle of the World Value Survey map, causing different moral judgments for languages on opposite sides of the dashed line.
- **Mechanism:** The pretraining data encodes cultural values that map to the World Value Survey dimensions. LLMs trained on this data develop reasoning patterns that align with specific cultural quadrants, leading to systematic differences when processing dilemmas in languages associated with different cultural contexts.
- **Core assumption:** Pretraining data contains sufficient cultural value representation to create alignment patterns matching the World Value Survey map.
- **Evidence anchors:**
  - [section] "the behavior of the LLMs seem to change for languages on the two sides of the dashed line, which could also be an artifact of the nature of these specific dilemmas"
  - [section] "According to the World Value Survey, Russia (orthodox European) is farthest from English speaking countries on the value map"
  - [corpus] Weak evidence - corpus contains related papers but no direct evidence of LLMs aligning to World Value Survey quadrants.
- **Break condition:** If pretraining data does not encode cultural value dimensions or if LLM reasoning is not influenced by these encoded values.

## Foundational Learning

- **Concept:** Defining Issues Test (DIT) framework and Kohlberg's Cognitive Moral Development model
  - Why needed here: The entire study methodology relies on DIT scores to measure moral reasoning stages. Understanding the three schemas (Personal Interests, Maintaining Norms, Post-conventional) is essential to interpret results.
  - Quick check question: What are the three moral schemas measured by DIT and what do they represent?

- **Concept:** Foreign-Language Effect on moral judgment
  - Why needed here: The study builds on established research showing humans make different moral choices in foreign vs native languages. Understanding this mechanism is crucial for interpreting LLM language-dependent performance differences.
  - Quick check question: How does the Foreign-Language Effect explain differences in moral judgment between native and foreign languages?

- **Concept:** Multilingual pretraining data distribution and resource availability
  - Why needed here: The study's core finding about performance differences across languages depends on understanding how pretraining data varies by language resource availability.
  - Quick check question: Why would LLMs perform differently on moral reasoning tasks across languages with varying resource availability?

## Architecture Onboarding

- **Component map:** DIT dataset -> Translation API (Google Translate) -> LLM prompts (GPT-4, ChatGPT, Llama2Chat-70B) -> Response extraction -> DIT scoring -> Statistical analysis
- **Critical path:** Translation of dilemmas → LLM processing → Response extraction → DIT scoring → Statistical analysis. The translation quality and LLM response extraction are critical steps that can introduce errors.
- **Design tradeoffs:** The study uses translated dilemmas rather than native-speaker created content, trading authenticity for consistency. Temperature=0 is used for reproducibility but may limit creative reasoning. The choice to exclude Arabic due to truncation issues shows resource constraints affecting methodology.
- **Failure signatures:** Significant performance drops in low-resource languages (Hindi, Swahili) indicate resource-related failures. Inconsistent moral judgments across languages for the same model suggest reasoning process failures. High random baseline scores would indicate fundamental evaluation problems.
- **First 3 experiments:**
  1. Test all three models on a single high-resource language (English) to establish baseline performance and verify DIT scoring methodology works correctly.
  2. Compare performance on a low-resource language (Hindi) vs medium-resource language (Russian) to isolate resource effect from language-specific factors.
  3. Test the same model on the same dilemma translated into multiple languages to measure translation-induced variation versus inherent language effects.

## Open Questions the Paper Calls Out

The paper raises several important open research questions about multilingual moral reasoning in large language models (LLMs). Here are some key questions explicitly mentioned or implied by the authors:

1. **Factors influencing moral reasoning differences across languages**: The paper shows that LLMs' moral reasoning abilities vary across languages, with lower performance in Hindi and Swahili compared to English, Spanish, Russian, and Chinese. The authors speculate that this could be due to differences in training data, cultural values, or the model's ability to process complex reasoning in different languages. However, the exact factors remain unclear.

2. **Alignment of LLMs with human values across languages**: The paper raises questions about whether LLMs align with human values across different languages and cultural contexts. It suggests that the models might be more aligned with values in the upper-right triangle of the cultural map, but the reasons for this are not fully understood.

3. **Impact of language proficiency on moral reasoning**: The paper hints at the possibility that the model's proficiency in a language might affect its moral reasoning abilities. However, the relationship between language proficiency and moral reasoning is not fully explored.

4. **Role of cultural context in moral reasoning**: The paper suggests that the cultural context of the languages studied might influence the model's moral reasoning. However, the exact role of cultural context is not fully understood.

5. **Potential biases in the evaluation framework**: The paper acknowledges that the evaluation framework used might contain biases, as it includes dilemmas specifically designed from a Western perspective. This raises questions about the generalizability of the findings to other cultural contexts.

6. **Ethical implications of using LLMs for moral decision-making**: The paper warns against using LLMs for real-life ethical decisions, despite their high moral reasoning scores in certain languages. This raises questions about the ethical implications of relying on LLMs for moral decision-making and the need for further research in this area.

7. **Improving multilingual moral reasoning in LLMs**: The paper highlights the need for further research to improve the multilingual moral reasoning capabilities of LLMs, particularly for low-resource languages. This includes exploring new training methods, data augmentation techniques, and evaluation frameworks.

## Limitations

- The use of Google Translate for creating multilingual dilemmas introduces potential translation artifacts that may confound genuine language-dependent reasoning differences
- The study only examines six languages, with Hindi and Swahili showing particularly poor performance that may reflect both resource limitations and inherent language characteristics
- The correlation between pretraining resource availability and moral reasoning performance, while suggestive, lacks direct evidence linking specific corpus statistics to the observed patterns

## Confidence

- **High Confidence**: GPT-4 demonstrates consistently superior multilingual moral reasoning across all tested languages, with minimal performance variance
- **Medium Confidence**: The ranking pattern English ≈ Spanish > Russian ≈ Chinese > Swahili > Hindi represents a genuine resource-dependent effect, though the exact mechanism requires further validation
- **Low Confidence**: The claim that language influences both reasoning process and conclusions through a mechanism analogous to human Foreign-Language Effect lacks sufficient evidence specific to LLM behavior

## Next Checks

1. **Translation Validation**: Conduct native-speaker review of translated dilemmas to verify semantic equivalence and identify potential translation artifacts affecting moral reasoning scores
2. **Resource Analysis**: Obtain detailed pretraining corpus statistics for each language to establish direct correlation between training resources and moral reasoning performance
3. **Cross-Model Consistency**: Test additional LLMs with varying training approaches (instruction-tuned vs base models) to determine if observed patterns generalize beyond the specific models studied