---
ver: rpa2
title: 'Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models'
arxiv_id: '2408.10189'
source_url: https://arxiv.org/abs/2408.10189
tags:
- matrix
- stage
- attention
- mamba-2
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a method to distill knowledge from pretrained
  Transformer models into subquadratic models like Mamba. The key idea is to progressively
  distill the Transformer at different levels of granularity: first matching the sequence
  transformation matrices, then aligning hidden states per block, and finally end-to-end
  training.'
---

# Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models

## Quick Facts
- arXiv ID: 2408.10189
- Source URL: https://arxiv.org/abs/2408.10189
- Reference count: 40
- Phi-Mamba achieves 71.7% accuracy on Winogrande vs 60.9% for best prior Mamba model

## Executive Summary
This paper presents a method to distill knowledge from pretrained Transformer models into subquadratic models like Mamba. The key idea is to progressively distill the Transformer at different levels of granularity: first matching the sequence transformation matrices, then aligning hidden states per block, and finally end-to-end training. Using this approach, the authors distill a Phi-Mamba model from Phi-1.5 using only 3B tokens (less than 1% of the original training data) and achieve stronger performance than all past open-source non-Transformer models, with 71.7% accuracy on Winogrande compared to 60.9% for the best prior Mamba model. The method enables models like SSMs to leverage computational resources invested in training Transformer-based architectures.

## Method Summary
The authors propose a three-stage progressive distillation method called MOHAWK to transfer knowledge from Transformer models to subquadratic architectures. First, they align the sequence transformation matrices between the teacher and student models. Second, they align hidden states at each block level. Finally, they perform end-to-end training with weight transfer, where MLP weights from the teacher are frozen during student training. The approach leverages Mamba-2's Structured State Space Duality (SSD) formulation, which provides a matrix family that closely approximates self-attention while maintaining subquadratic complexity.

## Key Results
- Phi-Mamba achieves 71.7% accuracy on Winogrande, surpassing all previous non-Transformer models
- Distillation requires only 3B tokens (less than 1% of original training data)
- Hybrid models with 4 retained attention layers achieve 66.0% average accuracy across benchmarks
- SSD matrix family shows twice the distance from attention matrices compared to pure SSM matrices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive distillation at multiple levels of granularity allows the subquadratic model to absorb Transformer knowledge more effectively than direct KD alone.
- Mechanism: Each stage provides a more constrained and focused loss, enabling the student to progressively align its internal representations to the teacher's, avoiding catastrophic forgetting and improving convergence.
- Core assumption: Subquadratic sequence mixers can approximate the attention matrix closely enough that alignment losses are meaningful.
- Evidence anchors:
  - [abstract] "We can thus progressively distill the Transformer architecture by matching different degrees of granularity in the SSM: first matching the mixing matrices themselves, then the hidden units at each block, and finally the end-to-end predictions."
  - [section 4.3] "Interestingly, during this step, the MLP weights can be kept frozen while keeping the model performant."
  - [corpus] "Data Efficient Any Transformer-to-Mamba Distillation via Attention Bridge" - shows similar progressive alignment exists in literature.
- Break condition: If the subquadratic matrix mixer family cannot approximate attention matrices within a meaningful margin, the matrix alignment stage becomes ineffective.

### Mechanism 2
- Claim: Mamba-2's structured state space duality (SSD) provides a matrix family that closely approximates self-attention, making it a suitable candidate for cross-architecture distillation.
- Mechanism: SSD's rolling multiplicative structure and learnable causal mask allow it to mimic attention's token mixing behavior while maintaining subquadratic complexity.
- Core assumption: The expressiveness of the matrix mixer family is the primary bottleneck for cross-architecture distillation.
- Evidence anchors:
  - [section 5.7.1] "Table 6 shows that while the SSM matrix family provides the closest approximation to the self-attention matrix mixer, the Mamba-2 mixer family (SSD) has just twice the distance from the SSM matrices."
  - [section 5.7.2] "a better matrix approximation (lower Frobenius distance) is correlated with better model performance (higher accuracy) on various tasks."
  - [corpus] Weak: no direct comparison to other SSM families in the literature, but the SSD connection is unique to Mamba-2.
- Break condition: If a more expressive matrix family (e.g., general semi-separable) shows significantly better attention approximation but isn't used, the SSD choice may be suboptimal.

### Mechanism 3
- Claim: Transferring MLP weights from the teacher model and freezing them during KD allows the student to focus learning capacity on the sequence mixer, improving efficiency.
- Mechanism: The MLP blocks encode most of the language knowledge; by reusing them, the student avoids relearning this while focusing KD on the new sequence mixer.
- Core assumption: Language model knowledge is predominantly stored in MLP layers rather than attention layers.
- Evidence anchors:
  - [section 4.3] "Interestingly, during this step, the MLP weights can be kept frozen while keeping the model performant."
  - [section 5.2] "we have observed occasional loss spikes during this phase... Decreasing the learning rate for Stage 3 mitigated this issue."
  - [corpus] "Data Efficient Any Transformer-to-Mamba Distillation via Attention Bridge" - mentions structural alignment but not MLP freezing.
- Break condition: If freezing MLPs causes significant performance degradation, the assumption about knowledge localization is incorrect.

## Foundational Learning

- Concept: Structured State Space Models (SSMs) and their connection to linear attention via Structured State Space Duality (SSD)
  - Why needed here: Understanding how Mamba-2's matrix mixer relates to attention is critical for grasping the distillation mechanism.
  - Quick check question: How does the SSD formulation of Mamba-2 relate to causal linear attention mathematically?

- Concept: Knowledge Distillation and its multi-stage variants
  - Why needed here: The MOHAWK method builds on standard KD by adding progressive alignment stages.
  - Quick check question: What is the difference between hidden-state alignment and end-to-end knowledge distillation in terms of loss scope?

- Concept: Matrix approximation theory and semi-separable matrices
  - Why needed here: The effectiveness of the distillation depends on how well structured matrices can approximate attention matrices.
  - Quick check question: Why is the Frobenius norm used to measure matrix approximation quality in this context?

## Architecture Onboarding

- Component map:
  - Teacher: Phi-1.5 (Transformer with 24 attention layers + MLP blocks)
  - Student base: Mamba-2 (modified with multi-head SSM, discrete-time A matrix)
  - MOHAWK stages: Matrix orientation (align mixers), hidden-state alignment (align block outputs), weight transfer + KD (end-to-end fine-tuning)
  - Hybrid variant: 4 attention layers retained, rest replaced with Mamba-2

- Critical path: Teacher → Stage 1 (matrix alignment) → Stage 2 (hidden-state alignment) → Stage 3 (weight transfer + KD) → Phi-Mamba

- Design tradeoffs:
  - Matrix expressiveness vs. computational efficiency: SSD chosen over more expressive families to maintain subquadratic complexity
  - Number of retained attention layers: More attention → better performance but higher complexity
  - Training data efficiency: Less than 1% of teacher data used, but staged distillation required

- Failure signatures:
  - Stage 1 ineffective: Hidden-state distances remain large after Stage 2
  - Stage 2 ineffective: End-to-end KD fails to improve over direct KD
  - MLP freezing fails: Performance drops significantly when MLPs are frozen

- First 3 experiments:
  1. Run Stage 1 alone: Measure matrix mixer distance and hidden-state distance
  2. Run Stage 1 + Stage 2: Compare hidden-state distance to Stage 2-only baseline
  3. Run Stage 1 + Stage 2 + Stage 3: Compare final performance to direct KD baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Phi-Mamba compare to Phi-1.5 when both are trained on the same dataset (not just distilled)?
- Basis in paper: [inferred] The paper shows Phi-Mamba achieves 71.7% on Winogrande vs 73.4% for Phi-1.5, but Phi-Mamba was distilled from Phi-1.5 using only 3B tokens. The paper notes this is less than 1% of the data used to train Phi-1.5.
- Why unresolved: The paper only compares distillation results, not direct training comparisons. The authors mention that Phi-Mamba is distilled with less than 1% of the data used to train either the previously best-performing Mamba models and 2% for the Phi-1.5 model itself.
- What evidence would resolve it: Training a Phi-Mamba model from scratch on the same dataset and with similar computational resources as Phi-1.5, then comparing performance on standard benchmarks.

### Open Question 2
- Question: What is the optimal number and placement of attention layers in hybrid models like Hybrid-Phi-Mamba?
- Basis in paper: [explicit] The paper shows that Hybrid-Phi-Mamba with 4 attention layers achieves 66.0% average accuracy, and experiments with different placements of the 4 attention layers show uniform interleaving yields the best results.
- Why unresolved: The paper only tests a limited number of configurations (1, 2, 4 attention layers) and specific placements. The authors note that "there is still room for improvement for distilling hybrid models" and that "additional gradient updates, changes in optimizer settings, etc, could be further optimized."
- What evidence would resolve it: Systematic experiments varying the number of attention layers (e.g., 2, 3, 5, 6, 8, 12) and their placements, then measuring performance on downstream tasks.

### Open Question 3
- Question: How does the effectiveness of MOHAWK distillation vary across different subquadratic architectures beyond Mamba-2?
- Basis in paper: [explicit] The paper states that "any subquadratic model with a sufficiently expressive matrix mixer can replicate the behavior of pretrained Transformers" and that "distillation techniques should be more appropriately tailored to the model."
- Why unresolved: The paper only demonstrates MOHAWK on Mamba-2. The authors mention that "we recommend further research to explore the role of sequence mixing layers in subquadratic models and their impact on performance."
- What evidence would resolve it: Applying MOHAWK to other subquadratic architectures like RetNet, H3, or linear attention models, then comparing the distillation effectiveness and final performance to the original Transformer models.

## Limitations
- Experimental validation limited to single teacher-student pair (Phi-1.5 → Phi-Mamba)
- Matrix approximation analysis relies primarily on Frobenius distance metrics rather than empirical representational capacity
- Claims about computational efficiency gains not fully validated against alternative distillation approaches

## Confidence
- **High confidence**: The staged distillation methodology is technically sound and the implementation details are clearly specified
- **Medium confidence**: The performance improvements on downstream benchmarks are significant but limited to a single teacher-student pair
- **Low confidence**: Claims about computational efficiency gains and the generalizability of the SSD matrix family choice to other subquadratic architectures

## Next Checks
1. **Cross-architecture replication**: Apply MOHAWK to distill knowledge from a standard Transformer (e.g., Llama-2 7B) into a different subquadratic architecture (e.g., RWKV or Griffin-LM) to test generalizability

2. **Matrix family comparison**: Implement and benchmark alternative matrix families (such as general semi-separable matrices) against SSD to determine if the "twice the distance" claim holds across multiple attention matrix samples

3. **Data efficiency scaling**: Systematically vary the amount of distillation data (0.1B, 1B, 10B tokens) to determine the minimum data requirements for effective progressive distillation and whether the staged approach provides benefits at different data scales