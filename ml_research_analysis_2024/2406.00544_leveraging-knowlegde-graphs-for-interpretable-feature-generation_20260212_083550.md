---
ver: rpa2
title: Leveraging Knowlegde Graphs for Interpretable Feature Generation
arxiv_id: '2406.00544'
source_url: https://arxiv.org/abs/2406.00544
tags:
- features
- feature
- kraft
- interpretability
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes KRAFT, an automated feature engineering (FE)
  framework that leverages a knowledge graph to guide the generation of interpretable
  features. The key idea is to combine a neural generator (using Deep Q-Network) to
  transform raw features and a knowledge-based reasoner (using Description Logics)
  to evaluate feature interpretability.
---

# Leveraging Knowlegde Graphs for Interpretable Feature Generation

## Quick Facts
- arXiv ID: 2406.00544
- Source URL: https://arxiv.org/abs/2406.00544
- Reference count: 10
- KRAFT framework significantly improves prediction accuracy while ensuring high interpretability of generated features

## Executive Summary
This paper introduces KRAFT, a novel automated feature engineering framework that leverages knowledge graphs to generate interpretable features. The approach combines a neural generator using Deep Q-Networks (DQN) with a knowledge-based reasoner using Description Logics to create features that are both accurate and interpretable. The framework is trained via Deep Reinforcement Learning to optimize both prediction accuracy and feature interpretability. Experiments on real-world datasets demonstrate that KRAFT outperforms state-of-the-art baselines in both effectiveness and interpretability.

## Method Summary
KRAFT employs a hybrid AI approach combining neural and symbolic components. A DQN-based generator transforms raw features into new ones, while a DL reasoner evaluates their interpretability based on knowledge graph semantics. The system uses semantic vectorization to represent features according to KG semantics, with the DQN trained using epsilon-greedy action selection. The DL reasoner (e.g., HermiT) filters non-interpretable features, and the ML model is trained iteratively on the selected features until convergence.

## Key Results
- KRAFT significantly improves prediction accuracy compared to baseline methods
- Generated features maintain high interpretability through knowledge graph guidance
- Framework outperforms state-of-the-art baselines in both effectiveness and interpretability metrics

## Why This Works (Mechanism)
The framework works by combining the generative power of deep reinforcement learning with the interpretability constraints of knowledge-based reasoning. The DQN explores the feature space while the DL reasoner ensures that generated features remain meaningful and interpretable within the domain context defined by the knowledge graph. This dual optimization approach allows KRAFT to discover complex feature interactions that are both predictive and understandable.

## Foundational Learning
- **Deep Q-Networks**: Why needed - To explore and generate new features through reinforcement learning; Quick check - Monitor reward convergence during training
- **Description Logics**: Why needed - To formalize and reason about feature interpretability constraints; Quick check - Verify reasoner can classify simple interpretable features
- **Semantic Vectorization**: Why needed - To represent features based on knowledge graph semantics; Quick check - Ensure feature vectors capture semantic relationships
- **Knowledge Graph Integration**: Why needed - To provide domain context and interpretability constraints; Quick check - Validate KG coverage for target domains

## Architecture Onboarding

**Component Map**: Raw Data -> Semantic Vectorization -> DQN Generator -> DL Reasoner -> Feature Selection -> ML Model

**Critical Path**: The critical path is Raw Data → DQN Generator → DL Reasoner → ML Model, where the DQN generates candidate features, the reasoner filters them for interpretability, and the ML model evaluates predictive performance.

**Design Tradeoffs**: The framework balances exploration (DQN's ability to discover novel features) with exploitation (DL reasoner's constraints on interpretability). The tradeoff is between feature complexity and interpretability - more complex features may be more predictive but less interpretable.

**Failure Signatures**: 
- KG coverage gaps lead to poor interpretability filtering
- DQN reward misalignment causes generation of non-interpretable features
- Semantic vectorization failure results in poor feature representation

**First Experiments**:
1. Implement DQN generator and DL reasoner on a small-scale knowledge graph with a simple dataset
2. Conduct ablation study comparing KRAFT with pure DQN feature generation
3. Test scalability by varying dataset size while keeping KG constant

## Open Questions the Paper Calls Out
- How does semantic vectorization compare to other KG embedding techniques for AutoFE?
- How does KRAFT performance scale with KG size and complexity?
- Can interpretability be quantitatively measured and compared across AutoFE methods?

## Limitations
- Dependency on comprehensive knowledge graphs covering target domains
- Lack of public code and limited experimental details
- Computational overhead from maintaining and querying large knowledge graphs

## Confidence
- **High**: Core methodology of combining DQN-based generation with DL reasoner-based filtering
- **Medium**: Empirical results due to lack of public code and limited experimental details
- **Low**: Claims about interpretability without standardized quantitative metrics

## Next Checks
1. Reimplement DQN generator and DL reasoner using a small-scale knowledge graph and publicly available dataset
2. Conduct ablation studies to quantify contribution of knowledge graph guidance versus pure neural generation
3. Perform scalability tests evaluating performance as dataset size and KG complexity increase