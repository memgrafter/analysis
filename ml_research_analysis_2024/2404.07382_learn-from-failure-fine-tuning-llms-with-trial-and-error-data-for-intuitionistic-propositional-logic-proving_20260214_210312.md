---
ver: rpa2
title: 'Learn from Failure: Fine-Tuning LLMs with Trial-and-Error Data for Intuitionistic
  Propositional Logic Proving'
arxiv_id: '2404.07382'
source_url: https://arxiv.org/abs/2404.07382
tags:
- proof
- state
- search
- tactic
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes to improve theorem-proving LLMs by training
  on both successful and failed proof paths, addressing the gap between training on
  only successful proofs and inference requiring trial-and-error. The authors create
  PropL, a new dataset of intuitionistic propositional logic theorems formalized in
  Lean, which includes complete proof trees with backtracking information generated
  by the FPS algorithm.
---

# Learn from Failure: Fine-Tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving

## Quick Facts
- arXiv ID: 2404.07382
- Source URL: https://arxiv.org/abs/2404.07382
- Reference count: 40
- Training on trial-and-error data improves proof success rates from 59.3% to 88.7% for intuitionistic propositional logic

## Executive Summary
This paper addresses a fundamental gap in training large language models for theorem proving: traditional approaches only train on successful proof paths, while inference requires exploration and backtracking. The authors propose training on complete proof trees that include both successful and failed proof attempts, generated by a modified A* search algorithm. They introduce PropL, a new dataset of 45,647 intuitionistic propositional logic theorems formalized in Lean, and demonstrate that training on trial-and-error data (TrialMaster) significantly outperforms training only on successful proofs (SuccessMaster), achieving higher proof success rates and reduced search costs.

## Method Summary
The authors create PropL, a dataset of intuitionistic propositional logic theorems in Lean, generated by the FPS algorithm that produces complete proof trees with backtracking information. They fine-tune an LLM (presumably CodeLlama) on two versions of the dataset: SuccessMaster (only successful proofs) and TrialMaster (complete proof trees including failed attempts). The FPS algorithm systematically explores the proof space using tactics like `simp`, `apply`, and `intro`, recording both successful paths and dead ends. During inference, the model generates proof steps iteratively, with a critic module evaluating each step and deciding whether to backtrack, continue, or accept the proof.

## Key Results
- TrialMaster achieves 88.7% proof success rate versus 59.3% for SuccessMaster on PropL-hard theorems
- TrialMaster requires significantly fewer Lean tactic calls, demonstrating improved search efficiency
- The approach shows better generalization to harder theorems compared to training on successful proofs only

## Why This Works (Mechanism)
The paper demonstrates that incorporating trial-and-error data during training helps the model learn backtracking behavior and develop better intuition about which tactics to apply in different contexts. By exposing the model to both successful and failed proof paths, it learns to recognize dead ends and avoid them during inference, rather than blindly exploring. This mirrors how humans learn from mistakes - understanding why certain approaches fail is as important as knowing what works.

## Foundational Learning

**Intuitionistic Propositional Logic**: A logical system that rejects the law of excluded middle, requiring constructive proofs. Why needed: Provides a well-defined domain for theorem proving with clear semantics and proof strategies. Quick check: Can be formalized in Lean with `intro`, `apply`, and `simp` tactics.

**Proof Trees**: Directed acyclic graphs representing all possible proof paths from axioms to theorems, including both successful and failed branches. Why needed: Capture the complete search space of theorem proving, including backtracking information. Quick check: Generated by FPS algorithm from initial theorem.

**FPS Algorithm**: Modified A* search algorithm that systematically explores proof space, recording complete proof trees with backtracking information. Why needed: Generates the trial-and-error data needed for training models to learn from failures. Quick check: Produces both successful and failed proof paths.

**Lean Theorem Prover**: Interactive theorem prover that provides tactics (`simp`, `apply`, `intro`) for constructing proofs. Why needed: Standard environment for formalizing and verifying mathematical proofs. Quick check: Used to evaluate generated proof steps during both training and inference.

## Architecture Onboarding

**Component Map**: Theorem -> FPS Algorithm -> Complete Proof Tree -> LLM Fine-tuning -> Proof Generation -> Critic Module -> Lean API

**Critical Path**: Theorem input → FPS-generated proof tree → LLM fine-tuning → Proof generation with backtracking → Lean API verification → Final proof output

**Design Tradeoffs**: The approach trades increased computational cost during training (generating complete proof trees with failures) for improved inference performance (fewer Lean calls, higher success rates). This represents a shift from "training on success only" to "training on the full exploration process."

**Failure Signatures**: Model fails to backtrack appropriately when trained only on successful proofs, leading to wasted computation on dead-end paths. The trial-and-error training teaches the model to recognize and avoid these failure patterns.

**First Experiments**: 1) Generate proof trees for simple theorems using FPS algorithm, 2) Compare LLM performance on SuccessMaster vs TrialMaster fine-tuning, 3) Analyze backtracking behavior differences between models during inference.

## Open Questions the Paper Calls Out
None explicitly mentioned in the provided content.

## Limitations
- Experimental scope limited to intuitionistic propositional logic, raising questions about generalizability to broader theorem-proving domains
- No statistical significance testing for the substantial performance improvements claimed
- Computational overhead and scalability of generating trial-and-error data for larger proof spaces remains unclear

## Confidence

**High Confidence**: The empirical observation that training on complete proof trees (including failures) leads to improved performance on PropL dataset tasks.

**Medium Confidence**: The generalizability of trial-and-error training benefits to other theorem-proving domains beyond intuitionistic propositional logic.

**Medium Confidence**: The claim about improved efficiency through reduced search costs, though the relationship between search efficiency and proof quality requires further validation.

## Next Checks

1. **Cross-domain validation**: Test the TrialMaster approach on theorem-proving datasets from different logical systems (e.g., first-order logic, set theory) to assess generalizability beyond intuitionistic propositional logic.

2. **Statistical significance analysis**: Conduct formal statistical tests to establish the significance of performance differences between TrialMaster and SuccessMaster, particularly for the PropL-hard benchmark where the claimed improvement is substantial.

3. **Scalability assessment**: Evaluate the computational overhead and practical limitations of generating trial-and-error data for larger proof spaces, including analysis of how proof tree size and search space complexity affect the approach's viability.