---
ver: rpa2
title: 'Contrastive CFG: Improving CFG in Diffusion Models by Contrasting Positive
  and Negative Concepts'
arxiv_id: '2411.17077'
source_url: https://arxiv.org/abs/2411.17077
tags:
- sampling
- guidance
- negative
- ccfg
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of negative guidance in diffusion
  models, where the goal is to avoid generating samples that contain certain unwanted
  concepts. The authors propose Contrastive Classifier-Free Guidance (CCFG), a novel
  method that uses contrastive loss to guide the denoising direction away from undesirable
  concepts while preserving sample quality.
---

# Contrastive CFG: Improving CFG in Diffusion Models by Contrasting Positive and Negative Concepts

## Quick Facts
- arXiv ID: 2411.17077
- Source URL: https://arxiv.org/abs/2411.17077
- Authors: Jinho Chang; Hyungjin Chung; Jong Chul Ye
- Reference count: 40
- Key outcome: Proposes Contrastive Classifier-Free Guidance (CCFG) that uses contrastive loss to guide denoising direction away from undesirable concepts while preserving sample quality

## Executive Summary
This paper addresses the challenge of negative guidance in diffusion models, where the goal is to avoid generating samples containing unwanted concepts. The authors propose Contrastive Classifier-Free Guidance (CCFG), a novel method that uses contrastive loss to guide the denoising direction away from undesirable concepts while preserving sample quality. The key idea is to treat negative guidance as an optimization problem that minimizes a contrastive loss between positive and negative prompts. Experiments demonstrate that CCFG outperforms existing negative guidance methods, effectively removing undesirable concepts while maintaining high sample quality across diverse scenarios.

## Method Summary
CCFG builds on diffusion models and classifier-free guidance by introducing a contrastive loss mechanism for negative guidance. The method computes both unconditional and conditional denoising directions, then modifies the denoising process using a weighting coefficient derived from the contrastive loss between positive and negative prompts. The approach automatically attenuates guidance when the denoising direction becomes unrelated to the negative prompt, preventing degradation into low-density regions of the data distribution. The NCE-based contrastive loss provides a probabilistic interpretation for both positive and negative guidance, naturally handling attracting and repelling forces through logistic regression.

## Key Results
- On class-conditioned datasets, CCFG achieves lower error rates and higher FID scores compared to negated CFG and dynamic negative guidance
- In text-to-image generation tasks, CCFG shows better alignment with positive prompts while effectively avoiding concepts in negative prompts
- CCFG demonstrates effectiveness across diverse scenarios, from simple class conditions to complex and overlapping text prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive loss-based guidance automatically attenuates when the denoising direction becomes unrelated to the negative prompt
- Mechanism: The contrastive loss compares conditional denoising direction with unconditional one. When unconditional direction diverges from negative concept, contrastive term approaches zero, removing unwanted guidance
- Core assumption: Unconditional denoising direction is sufficiently different from conditional one when sample no longer contains negative concept
- Evidence anchors: [abstract] guidance term aligns or repels denoising direction through contrastive loss; [section 3] coefficient goes to 0 as ∥ˆϵ∅(xt) − ˆϵc(xt)∥2 gets bigger

### Mechanism 2
- Claim: Contrastive loss prevents sampling from low-density regions that plague negated CFG
- Mechanism: Contrastive loss formulation ensures guidance term remains bounded and decreases to zero as sample moves away from negative concept, avoiding pushing into low-density regions
- Core assumption: Contrastive loss creates well-behaved objective that doesn't create unbounded gradients
- Evidence anchors: [abstract] negative guidance uses unbounded objective function; [section 3] negative contrastive loss and guidance term flatten out to 0

### Mechanism 3
- Claim: NCE-based contrastive loss provides probabilistic interpretation for both positive and negative guidance
- Mechanism: Treating positive/negative prompts as observed vs. noise data in NCE framework derives guidance terms that naturally handle both attracting and repelling forces through logistic regression
- Core assumption: Epsilon-matching trained diffusion model can serve as valid parameterized distribution for NCE
- Evidence anchors: [section 3] optimize datapoint with pre-trained model that nicely parameterizes pθ(x); derivation of ℓ+(ϵ) and ℓ−(ϵ)

## Foundational Learning

- Concept: Diffusion models and score matching
  - Why needed here: Entire method builds on understanding how diffusion models estimate gradients of log-density and use them for sampling
  - Quick check question: What is the relationship between denoising score matching and the score function ∇xt log p(xt)?

- Concept: Classifier-free guidance (CFG)
  - Why needed here: CCFG is enhancement to CFG, so understanding how CFG combines conditional and unconditional predictions is crucial
  - Quick check question: How does the γ parameter in CFG relate to sharpening the posterior distribution p(x|c)?

- Concept: Contrastive learning and NCE
  - Why needed here: Core innovation uses contrastive loss (specifically NCE) to guide denoising process, requiring understanding of how these losses work
  - Quick check question: In NCE, how does the logistic regression objective discriminate between observed data and noise samples?

## Architecture Onboarding

- Component map: Pre-trained diffusion model -> Contrastive loss computation -> Modified denoising direction -> DDIM sampling update
- Critical path: 1) Compute unconditional denoising direction ˆϵ∅(xt) 2) Compute conditional denoising direction ˆϵc(xt) 3) Calculate contrastive loss and its gradient 4) Modify denoising direction with weighting coefficient 5) Perform DDIM update step
- Design tradeoffs:
  - Fixed vs. adaptive τ: Fixed τ is simpler but may not optimally handle all timesteps
  - Direct modification of ˆϵ∅ vs. posterior mean optimization: Direct modification is simpler for fair comparison but posterior mean optimization has theoretical grounding
  - Computational overhead: Minimal but requires computing both conditional and unconditional directions at each step
- Failure signatures:
  - If τ is too large: Guidance becomes unstable, similar to high γ in CFG
  - If τ is too small: Guidance becomes ineffective, similar to low γ in CFG
  - If unconditional and conditional directions remain correlated: Negative guidance won't attenuate properly
- First 3 experiments:
  1. Implement CCFG on simple 2D toy dataset to verify guidance behavior visually
  2. Compare CCFG with nCFG and DNG on MNIST class removal to measure precision-recall tradeoff
  3. Test CCFG on text-to-image generation with simple positive/negative prompt pairs to verify practical effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CCFG compare to existing negative guidance methods when applied to more complex datasets with overlapping or ambiguous class boundaries?
- Basis in paper: Paper mentions CCFG is effective across diverse scenarios but doesn't provide detailed comparison on datasets with overlapping or ambiguous class boundaries
- Why unresolved: Paper focuses on class-conditioned image generation and text-to-image generation but doesn't explicitly address performance on datasets with overlapping or ambiguous class boundaries
- What evidence would resolve it: Detailed comparison of CCFG's performance on datasets with overlapping or ambiguous class boundaries

### Open Question 2
- Question: What is the computational overhead of CCFG compared to existing negative guidance methods, and how does it scale with dataset size or prompt complexity?
- Basis in paper: Paper mentions CCFG involves simple modification but doesn't provide detailed analysis of computational overhead or scalability
- Why unresolved: Paper focuses on effectiveness in removing undesirable concepts but doesn't analyze computational overhead or scalability
- What evidence would resolve it: Detailed analysis of computational overhead of CCFG compared to existing methods and scalability analysis

### Open Question 3
- Question: How does choice of temperature parameter τ affect CCFG performance, and is there optimal value or range for different prompt types or datasets?
- Basis in paper: Paper mentions τ can depend on noise scheduling but doesn't analyze how choice of τ affects performance or optimal values
- Why unresolved: Paper focuses on effectiveness but doesn't provide detailed analysis of impact of temperature parameter τ
- What evidence would resolve it: Detailed analysis of how choice of τ affects performance and whether optimal values exist for different scenarios

## Limitations
- NCE-based contrastive loss formulation lacks extensive validation in high-dimensional diffusion sampling spaces
- Method's reliance on comparing unconditional and conditional denoising directions may not hold for complex, overlapping concepts
- Temperature parameter τ and its adaptive mechanism are not fully characterized across different sampling timesteps

## Confidence

**High Confidence**: Core empirical findings showing CCFG outperforms existing negative guidance methods on benchmark datasets with measurable improvements in error rates and FID scores

**Medium Confidence**: Theoretical justification for why contrastive loss provides stable negative guidance compared to unbounded objectives in negated CFG, based on NCE framework and gradient properties

**Low Confidence**: Claims about CCFG's effectiveness in handling complex, overlapping negative concepts in text-to-image generation, as experimental validation for these scenarios appears limited compared to simpler class-conditioned tasks

## Next Checks

1. **Ablation study on temperature parameter τ**: Systematically vary τ across different sampling timesteps and model scales to characterize its impact on guidance stability and effectiveness

2. **Gradient behavior analysis**: Visualize and quantify behavior of contrastive guidance term's gradients during sampling, especially as ∥ˆϵ∅(xt) − ˆϵc(xt)∥2 changes, to verify claimed automatic attenuation mechanism

3. **Cross-model generalization test**: Apply CCFG to diffusion models trained with different objectives and architectures to assess robustness beyond specific DDPM models used in experiments