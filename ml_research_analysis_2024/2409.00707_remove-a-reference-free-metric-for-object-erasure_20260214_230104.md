---
ver: rpa2
title: 'ReMOVE: A Reference-free Metric for Object Erasure'
arxiv_id: '2409.00707'
source_url: https://arxiv.org/abs/2409.00707
tags:
- image
- inpainting
- remove
- object
- lpips
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReMOVE, a reference-free metric for evaluating
  object erasure in diffusion-based image editing models. ReMOVE uses a Vision Transformer
  to extract feature embeddings from masked and unmasked regions of an image, and
  computes their similarity using cosine similarity.
---

# ReMOVE: A Reference-free Metric for Object Erasure

## Quick Facts
- arXiv ID: 2409.00707
- Source URL: https://arxiv.org/abs/2409.00707
- Reference count: 40
- Reference-free metric for object erasure in diffusion-based image editing

## Executive Summary
This paper introduces ReMOVE, a reference-free metric for evaluating object erasure in diffusion-based image editing models. The metric addresses the challenge of distinguishing between object removal and replacement in stochastic diffusion models by measuring semantic consistency between masked and unmasked regions. ReMOVE uses Vision Transformer feature embeddings and cosine similarity to assess whether the inpainted region preserves background context rather than introducing new content. Experiments show ReMOVE correlates well with state-of-the-art reference-based metrics and aligns with human perception, outperforming LPIPS in user preference agreement (74.7% vs 71.9%).

## Method Summary
ReMOVE computes similarity between masked and unmasked regions of an inpainted image using ViT feature embeddings. The method involves preprocessing the image (resizing to 1024×1024 and normalization), extracting patch embeddings using a pre-trained ViT model, resizing the binary mask to the patch level, separating embeddings into masked and unmasked sets, computing mean embeddings for each region, and calculating cosine similarity between these mean vectors. A cropping strategy ensures fair comparison across varying mask sizes by maintaining comparable patch counts between regions.

## Key Results
- ReMOVE correlates well with state-of-the-art reference-based metrics like LPIPS
- User study shows ReMOVE agrees with human preferences 74.7% of the time vs LPIPS at 71.9%
- Cropping strategy ensures robustness across varying mask sizes
- Effectively distinguishes between object removal and replacement in diffusion models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReMOVE captures semantic consistency between masked and unmasked regions better than pixel-level metrics.
- Mechanism: Uses ViT feature embeddings to extract high-level semantic information from image patches, then compares mean embeddings from masked and unmasked regions using cosine similarity.
- Core assumption: ViT features trained on segmentation tasks preserve semantic meaning that aligns with human perception of object removal quality.
- Evidence anchors:
  - [abstract] "It effectively distinguishes between object removal and replacement"
  - [section] "ReMOVE leverages feature extraction to assess inpainting quality – assessing visual saliency at a patch level rather than pixel level"
  - [corpus] Weak evidence - corpus lacks papers directly comparing semantic vs pixel metrics for inpainting
- Break condition: If the ViT features don't align with human perception or if the mean feature comparison loses important local detail information.

### Mechanism 2
- Claim: Reference-free evaluation works because it measures internal consistency rather than ground truth alignment.
- Mechanism: Instead of comparing to ground truth, measures whether the inpainted region is semantically consistent with surrounding context by comparing masked vs unmasked feature distributions.
- Core assumption: High similarity between masked and unmasked regions indicates successful object removal rather than replacement with different content.
- Evidence anchors:
  - [abstract] "addresses the challenge of evaluating inpainting without a reference image"
  - [section] "measure the distance between mean patchwise features of the masked and unmasked regions"
  - [corpus] Missing evidence - corpus doesn't include papers on reference-free evaluation methods
- Break condition: If the background context is complex or if the masked region size is very small relative to the image.

### Mechanism 3
- Claim: Cropping strategy ensures fair comparison between masked and unmasked regions regardless of mask size.
- Mechanism: Applies square cropping to ensure masked region covers similar fraction of cropped area across different mask sizes, maintaining comparable patch counts.
- Core assumption: Without cropping, small masks would be overwhelmed by background information, making the metric less sensitive to inpainting quality.
- Evidence anchors:
  - [section] "we implement a cropping strategy...to ensure that within the cropped regions, all masks covered a similar fraction of the area"
  - [section] "the number of patches belonging to the masked region is comparable to the number of patches in the unmasked region"
  - [corpus] No corpus evidence on cropping strategies for inpainting evaluation
- Break condition: If cropping removes important contextual information or if the chosen crop size is inappropriate for certain image types.

## Foundational Learning

- Concept: Vision Transformer architecture and patch embedding
  - Why needed here: ReMOVE relies on ViT for feature extraction from image patches
  - Quick check question: How does a ViT tokenize an image into patches and what information do these patch embeddings capture?

- Concept: Cosine similarity for feature comparison
  - Why needed here: ReMOVE uses cosine similarity to compare mean feature vectors from masked and unmasked regions
  - Quick check question: Why is cosine similarity preferred over Euclidean distance for comparing high-dimensional feature embeddings?

- Concept: Diffusion models and inpainting
  - Why needed here: Understanding why diffusion models struggle with object removal vs replacement is key to ReMOVE's purpose
  - Quick check question: What makes object erasure particularly challenging for diffusion-based image editing models?

## Architecture Onboarding

- Component map: Image preprocessing -> ViT feature extractor -> Masking -> Feature segregation -> Mean feature calculation -> Cosine similarity -> Optional cropping module

- Critical path: Image → ViT features → Masked/Unmasked split → Mean features → Cosine similarity → ReMOVE score

- Design tradeoffs:
  - Patch size vs. computational cost (16x16 used)
  - Cropping vs. full image evaluation
  - Mean feature comparison vs. more complex distribution matching
  - ViT model choice impacts performance

- Failure signatures:
  - Very low scores even for good inpainting (possible feature extractor mismatch)
  - High variance across seeds (masking or preprocessing issues)
  - Poor correlation with human judgment (feature extraction not aligned with perception)

- First 3 experiments:
  1. Test ReMOVE on toy dataset with known good/bad inpainting to verify monotonic behavior
  2. Compare ReMOVE vs LPIPS on same dataset to establish correlation
  3. Evaluate cropping vs non-cropping versions on DEFACTO dataset to validate design choice

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ReMOVE perform on real-world datasets beyond DEFACTO, particularly those with diverse object types and scene complexities?
- Basis in paper: [inferred] The paper validates ReMOVE on the DEFACTO dataset but does not explore its performance on other real-world datasets with varying object types and scene complexities.
- Why unresolved: The paper focuses on a single real-world dataset (DEFACTO), which may not fully represent the diversity of real-world inpainting scenarios.
- What evidence would resolve it: Testing ReMOVE on multiple real-world datasets with different object types and scene complexities, and comparing its performance against existing metrics.

### Open Question 2
- Question: How does ReMOVE handle cases where the inpainted region is significantly larger or smaller than the unmasked region, and what are the implications for its accuracy?
- Basis in paper: [explicit] The paper mentions that cropping is necessary for accurate estimation, especially when mask sizes vary, but does not provide a detailed analysis of how ReMOVE handles extreme size differences between masked and unmasked regions.
- Why unresolved: The paper does not explore the performance of ReMOVE in scenarios where the inpainted region is disproportionately large or small compared to the unmasked region.
- What evidence would resolve it: Conducting experiments with inpainted images where the masked region varies significantly in size relative to the unmasked region, and analyzing the impact on ReMOVE's accuracy.

### Open Question 3
- Question: How does ReMOVE perform when evaluated against human perception in diverse cultural contexts, where notions of visual quality and object removal may differ?
- Basis in paper: [explicit] The paper includes a user study to validate ReMOVE's alignment with human perception, but it is limited to a specific age group and cultural context.
- Why unresolved: The user study is conducted with a limited demographic, which may not capture the diversity of human perception across different cultures and age groups.
- What evidence would resolve it: Conducting user studies with participants from diverse cultural backgrounds and age groups to evaluate ReMOVE's performance in different perceptual contexts.

## Limitations
- Metric performance on datasets beyond DEFACTO remains unexplored
- Human study limited to specific demographic without demographic details
- Exact ViT model architecture and checkpoint not specified
- Performance on non-object removal inpainting tasks not evaluated

## Confidence
- High confidence: The mathematical formulation of ReMOVE (mean feature extraction + cosine similarity) is clearly specified and reproducible.
- Medium confidence: Claims about superior performance vs LPIPS and human alignment, as these depend on experimental setup and dataset specifics.
- Low confidence: Claims about why the metric works (mechanism 1 and 2) due to limited theoretical grounding in the paper.

## Next Checks
1. **Cross-dataset validation**: Test ReMOVE on additional inpainting datasets (e.g., Places2, CelebA-HQ) to verify robustness across different image domains and editing tasks beyond object removal.

2. **Ablation study**: Systematically evaluate the impact of different ViT backbones (CLIP vs segment-trained vs random) and patch sizes (8×8 vs 16×16 vs 32×32) on ReMOVE's correlation with human judgment.

3. **Failure case analysis**: Create a test suite of challenging inpainting scenarios (complex backgrounds, small masks, ambiguous regions) to identify where ReMOVE succeeds or fails, and document specific failure modes with visual examples.