---
ver: rpa2
title: 'Learning the Unlearned: Mitigating Feature Suppression in Contrastive Learning'
arxiv_id: '2402.11816'
source_url: https://arxiv.org/abs/2402.11816
tags:
- learning
- contrastive
- features
- stage
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses feature suppression in contrastive learning,
  where models capture only a limited portion of input information. The proposed Multistage
  Contrastive Learning (MCL) framework mitigates this issue by progressively learning
  previously unlearned features through feature-aware negative sampling and preserving
  well-learned features via cross-stage representation integration.
---

# Learning the Unlearned: Mitigating Feature Suppression in Contrastive Learning

## Quick Facts
- arXiv ID: 2402.11816
- Source URL: https://arxiv.org/abs/2402.11816
- Authors: Jihai Zhang; Xiang Lan; Xiaoye Qu; Yu Cheng; Mengling Feng; Bryan Hooi
- Reference count: 40
- One-line primary result: MCL boosts CLIP's average accuracy from 20.0 to 32.6 on the MMVP benchmark, with improvements up to threefold on specific attributes

## Executive Summary
This paper addresses feature suppression in contrastive learning, where models capture only a limited portion of input information, leading to indistinguishable representations for semantically different inputs. The proposed Multistage Contrastive Learning (MCL) framework mitigates this issue by progressively learning previously unlearned features through feature-aware negative sampling and preserving well-learned features via cross-stage representation integration. MCL demonstrates significant improvements across both unimodal and multimodal settings, with CLIP achieving a 12.6 percentage point improvement on the MMVP benchmark.

## Method Summary
MCL operates through a multistage training framework where each stage builds upon the previous one. After initial contrastive learning, K-means clustering organizes representations into clusters. In subsequent stages, negative samples are selected exclusively from clusters assigned in preceding stages, forcing the model to explore new features. The framework integrates representations across all stages through concatenation, ensuring previously learned features are preserved while incorporating newly learned ones. The approach is model-agnostic and compatible with various architectures including ResNet and Vision Transformers.

## Key Results
- MCL boosts CLIP's average accuracy from 20.0 to 32.6 on the MMVP benchmark
- Achieves up to threefold improvements on specific attributes in multimodal settings
- Demonstrates effectiveness across both unimodal (CIFAR, STL-10, ImageNet) and multimodal benchmarks
- Outperforms state-of-the-art methods while being model-agnostic

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negative sampling from clusters of previously learned features forces the model to explore new, unlearned features.
- Mechanism: During each stage, negative samples are selected from clusters that contain samples with similar dominant features learned in earlier stages. Since these shared features cannot be used to distinguish the anchor from negatives, the model must find and utilize different features to satisfy the contrastive objective.
- Core assumption: Representations from contrastive learning tend to cluster according to learned features, and samples within the same cluster share similar dominant features.

### Mechanism 2
- Claim: Cross-stage representation integration preserves well-learned features while incorporating newly learned ones.
- Mechanism: After all stages are complete, representations from each stage are concatenated element-wise to form the final representation. This ensures that features learned at different stages are all retained in the final model.
- Core assumption: Different stages learn complementary feature distributions, and concatenation effectively combines these without interference.

### Mechanism 3
- Claim: The multistage framework with controlled cluster numbers ensures meaningful progression through feature space.
- Mechanism: The mathematical constraint K^N ≤ M/b ensures that clusters are balanced and contain sufficient samples for training. This enables systematic exploration of feature space across stages.
- Core assumption: Balanced clusters with sufficient samples are necessary for effective contrastive learning at each stage.

## Foundational Learning

- Concept: Contrastive learning basics (InfoNCE loss, positive/negative pairs)
  - Why needed here: MCL builds directly on standard contrastive learning frameworks like SimCLR and MoCo-v2
  - Quick check question: What is the purpose of the temperature parameter τ in InfoNCE loss?

- Concept: Feature suppression in representation learning
  - Why needed here: MCL specifically addresses this problem, so understanding what it is and why it matters is crucial
  - Quick check question: Why does feature suppression lead to poor performance on tasks requiring fine-grained semantic understanding?

- Concept: K-means clustering and its role in representation analysis
  - Why needed here: MCL uses K-means to organize representations into clusters that guide negative sampling
  - Quick check question: How does the quality of K-means clustering affect the effectiveness of MCL's feature-aware negative sampling?

## Architecture Onboarding

- Component map: Base encoder (ResNet, ViT, etc.) -> Projection head -> K-means clustering module -> Feature-aware negative sampler -> Multi-stage training loop -> Cross-stage integration layer

- Critical path:
  1. Train initial encoder with standard contrastive learning
  2. Apply K-means clustering to obtain initial cluster assignments
  3. For each subsequent stage: compute pseudo labels from previous stage cluster assignments, train encoder with feature-aware negative sampling, apply K-means clustering for next stage
  4. Concatenate representations from all stages

- Design tradeoffs:
  - Number of stages vs. number of clusters: More stages require fewer clusters to maintain the constraint K^N ≤ M/b
  - Trainable parameters per stage: MCL can use fewer trainable blocks to reduce computational cost while maintaining effectiveness
  - Temperature scheduling: Different τ values affect which features are learned; MCL is robust across settings

- Failure signatures:
  - No performance improvement across stages: Likely indicates that feature-aware negative sampling isn't forcing exploration of new features
  - Performance degradation in later stages: May indicate interference between features from different stages during integration
  - Clustering instability: Could suggest poor representation quality or inappropriate K-means parameters

- First 3 experiments:
  1. Verify basic MCL functionality on a simple dataset (like Trifeature) with 2-3 stages, checking that each stage learns different features
  2. Test the feature-aware negative sampling by examining nearest neighbors within and across stages
  3. Evaluate cross-stage integration by comparing concatenated representations vs. single-stage representations on a downstream task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of stages in MCL affect the trade-off between learning new features and preserving previously learned features?
- Basis in paper: The paper mentions that increasing the number of stages requires a smaller number of clusters and explores the learning dynamics across stages.
- Why unresolved: The paper does not provide a definitive answer on the optimal number of stages for different datasets or tasks.
- What evidence would resolve it: Systematic experiments varying the number of stages and measuring the performance on downstream tasks for different datasets and architectures.

### Open Question 2
- Question: Can MCL be effectively applied to other self-supervised learning methods beyond contrastive learning, such as masked autoencoders or generative models?
- Basis in paper: The paper focuses on contrastive learning but mentions that MCL is model-agnostic and can be integrated with any NCE-based method.
- Why unresolved: The paper only demonstrates MCL's effectiveness with contrastive learning and does not explore its applicability to other self-supervised learning paradigms.
- What evidence would resolve it: Applying MCL to other self-supervised learning methods and evaluating its impact on feature suppression and downstream task performance.

### Open Question 3
- Question: What is the impact of the initial clustering in Stage 0 on the overall performance of MCL?
- Basis in paper: The paper mentions that the initial clustering is done on the representations obtained from the first stage of training.
- Why unresolved: The paper does not explore how different initial clustering strategies or the quality of the initial clustering affects the subsequent stages and final performance.
- What evidence would resolve it: Experiments comparing MCL's performance with different initial clustering methods or with and without initial clustering.

## Limitations
- Computational overhead from multiple training stages with different clustering and sampling strategies
- Scalability concerns for very large datasets due to the K^N ≤ M/b constraint limiting achievable stages
- Lack of extensive ablation studies to quantify individual contribution of integration mechanism versus negative sampling

## Confidence
- High Confidence: The core claim that MCL addresses feature suppression through multistage learning and feature-aware negative sampling is well-supported by empirical results across multiple benchmarks
- Medium Confidence: The cross-stage representation integration mechanism shows positive results but lacks extensive ablation studies to quantify its individual contribution
- Low Confidence: The mathematical constraint K^N ≤ M/b is presented as necessary but the paper doesn't explore what happens when this constraint is violated

## Next Checks
1. **Ablation Study on Integration Strategy**: Compare MCL with and without cross-stage integration, and test alternative integration methods (weighted averaging, attention-based fusion) to quantify the contribution of the integration mechanism to overall performance gains.

2. **Scalability Analysis**: Evaluate MCL on progressively larger datasets (e.g., 10M, 100M images) to understand how the constraint K^N ≤ M/b impacts the maximum achievable stages and whether the performance benefits persist at scale.

3. **Robustness to Hyperparameters**: Systematically vary the number of stages (N) and clusters (K) across different datasets to identify optimal configurations and understand the sensitivity of MCL to these critical hyperparameters.