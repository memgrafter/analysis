---
ver: rpa2
title: 'DocMamba: Efficient Document Pre-training with State Space Model'
arxiv_id: '2409.11887'
source_url: https://arxiv.org/abs/2409.11887
tags: []
core_contribution: This paper presents DocMamba, a document pre-training framework
  based on the State Space Model (SSM) that addresses the inefficiency of Transformer-based
  models in processing long documents. DocMamba reduces computational complexity from
  quadratic to linear while maintaining global modeling capabilities through a selective
  state space mechanism.
---

# DocMamba: Efficient Document Pre-training with State Space Model

## Quick Facts
- arXiv ID: 2409.11887
- Source URL: https://arxiv.org/abs/2409.11887
- Reference count: 10
- Key outcome: Achieves state-of-the-art performance on document understanding tasks while using 88.3% less GPU memory and running 2.4× faster than LayoutLMv3

## Executive Summary
This paper presents DocMamba, a document pre-training framework based on the State Space Model (SSM) that addresses the inefficiency of Transformer-based models in processing long documents. By leveraging the selective state space mechanism, DocMamba reduces computational complexity from quadratic to linear while maintaining global modeling capabilities. The framework introduces Segment-First Bidirectional Scan (SFBS) to handle complex 2-D document layouts, enabling effective processing of visually-rich documents without requiring image modality.

## Method Summary
DocMamba uses a 24-layer bidirectional Mamba encoder with 768 hidden size and 1,536 intermediate size. The framework pre-trains on IIT-CDIP dataset (10M pages) using Masked Language Modeling (MLM) with dynamic batch sizing based on input length. It processes documents using Segment-First Bidirectional Scan (SFBS) that organizes tokens based on document segments before applying bidirectional processing. The model omits 1-D position embeddings to enable length extrapolation capability, and is fine-tuned on FUNSD, CORD, SROIE, and HRDoc datasets for downstream tasks.

## Key Results
- Achieves 89.5%, 95.0%, and 96.7% F1 scores on FUNSD, CORD, and SROIE datasets respectively
- Uses 88.3% less GPU memory compared to LayoutLMv3
- Runs 2.4× faster than LayoutLMv3
- Demonstrates length extrapolation capability, maintaining performance when tested on sequences up to 2560 tokens despite being pre-trained on sequences of length 512

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The selective state space mechanism enables linear computational complexity while preserving global modeling capabilities
- Mechanism: DocMamba uses Mamba's selective state space mechanism where input-dependent parameters B, C, and Δ allow adaptive propagation or forgetting of information through a compressed hidden state
- Core assumption: The selective state space mechanism can effectively capture long-range dependencies without quadratic attention computations
- Evidence anchors: Abstract states linear complexity with global modeling capabilities; section explains quadratic to linear reduction; corpus neighbors lack direct support

### Mechanism 2
- Claim: SFBS enables effective processing of document tokens with complex 2-D layouts
- Mechanism: SFBS extracts segments from documents, processes tokens within each segment sequentially, and applies bidirectional scanning for context from both directions
- Core assumption: Segment-based organization better captures contiguous semantic information than arbitrary token ordering
- Evidence anchors: Section describes SFBS design for consecutive token processing; section mentions bidirectional scan following Vim; corpus neighbors lack direct support

### Mechanism 3
- Claim: Inherent positional information in SSMs eliminates need for 1-D position embeddings, enabling length extrapolation
- Mechanism: Unlike Transformers requiring fixed maximum length position embeddings, SSMs naturally capture sequential dependencies through internal state evolution
- Core assumption: Sequential nature of SSMs provides sufficient positional information for document understanding tasks
- Evidence anchors: Section states SSMs have inherent positional information; section validates length extrapolation potential; corpus neighbors lack direct support

## Foundational Learning

- Concept: State Space Models (SSM) and their discretization
  - Why needed here: Understanding continuous-to-discrete transformation is crucial for grasping DocMamba's architecture
  - Quick check question: How does the Zero-Order Hold (ZOH) method transform continuous parameters A, B into discrete counterparts using timescale parameter Δ?

- Concept: Selective state space mechanisms in Mamba
  - Why needed here: Core innovation relies on Mamba's selective mechanism where parameters become input-dependent
  - Quick check question: What is the key difference between traditional SSMs and Mamba's selective SSM regarding parameter behavior?

- Concept: Bidirectional processing in sequence models
  - Why needed here: DocMamba employs bidirectional scanning to capture context from both directions
  - Quick check question: How does bidirectional processing differ from unidirectional processing in terms of information available to each token?

## Architecture Onboarding

- Component map: OCR/PDF parsing -> Word embeddings + 2-D position embeddings -> SFBS organization -> Bidirectional Mamba encoder -> MLM or task-specific output
- Critical path: OCR/PDF parsing → Word embeddings + 2-D position embeddings → SFBS organization → Bidirectional Mamba encoder → MLM or task-specific output
- Design tradeoffs: No image modality to reduce computational complexity; variable batch size during pre-training based on input length; omission of 1-D position embeddings for length extrapolation
- Failure signatures: Poor performance on layout-heavy tasks; degradation with longer sequences; memory bottlenecks despite theoretical linear complexity
- First 3 experiments: 1) Compare SFBS against WFBS on FUNSD to validate segment-based organization; 2) Test length extrapolation by training on sequences ≤512 and evaluating on sequences up to 2560 tokens; 3) Measure GPU memory usage and FPS across different input lengths to confirm linear complexity scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does incorporating image modality into DocMamba affect its performance compared to current multimodal transformer-based models?
- Basis in paper: [explicit] The paper identifies omission of image modality as central limitation and states this decision was made to demonstrate SSM's competitive potential
- Why unresolved: Authors deliberately excluded image modality to isolate text and layout processing capabilities
- What evidence would resolve it: Comparative experiments between DocMamba with and without image modality against multimodal transformer models on standard benchmarks

### Open Question 2
- Question: What is the theoretical relationship between SSM layer depth and performance in document understanding tasks, and what is the optimal number of layers?
- Basis in paper: [explicit] Ablation study shows performance improves with more layers but doesn't establish theoretical limits or diminishing returns
- Why unresolved: While empirical results show scalability, paper doesn't explore theoretical ceiling or optimal configuration
- What evidence would resolve it: Analysis of performance curves at various depths, investigation of diminishing returns, and theoretical analysis of SSM capacity

### Open Question 3
- Question: How does DocMamba's performance scale with sequence length beyond 2560 tokens, and what are the practical limits of its length extrapolation capability?
- Basis in paper: [explicit] Paper demonstrates successful extrapolation up to 2560 tokens but doesn't test beyond this range
- Why unresolved: Experiments only tested up to 2560 tokens, leaving open questions about performance at even longer sequences
- What evidence would resolve it: Systematic testing at increasingly longer sequence lengths and identification of practical upper bounds

## Limitations
- Lacks detailed pseudocode for SFBS algorithm, making exact reproduction challenging
- No ablation studies to isolate contribution of SFBS from underlying Mamba architecture
- Length extrapolation capability not empirically validated beyond 512→2560 token comparison

## Confidence

**High confidence:**
- DocMamba achieves SOTA performance on FUNSD, CORD, and SROIE datasets with reported F1 scores
- DocMamba demonstrates reduced GPU memory usage and improved inference speed

**Medium confidence:**
- Selective state space mechanism effectively captures long-range dependencies while maintaining linear computational complexity
- SFBS successfully processes complex 2-D document layouts

**Low confidence:**
- Length extrapolation capability extends significantly beyond tested range
- Omission of 1-D position embeddings has no negative impact on document understanding tasks

## Next Checks

1. **SFBS Ablation Study**: Implement and compare Word-First Bidirectional Scan (WFBS) against proposed SFBS on FUNSD dataset to isolate contribution of segment-based organization. Measure F1 score difference and analyze token reordering patterns.

2. **Length Extrapolation Boundary Testing**: Pre-train DocMamba on sequences up to 1024 tokens, then systematically evaluate performance on sequences ranging from 1024 to 4096 tokens. Plot F1 score degradation curve to determine practical limits of length extrapolation.

3. **Memory Complexity Validation**: Conduct controlled experiments measuring GPU memory usage and wall-clock time across different sequence lengths (256, 512, 1024, 2048, 4096 tokens) on same hardware. Plot memory usage vs. sequence length to verify linear scaling.