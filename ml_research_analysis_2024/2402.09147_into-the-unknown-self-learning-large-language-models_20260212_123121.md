---
ver: rpa2
title: 'Into the Unknown: Self-Learning Large Language Models'
arxiv_id: '2402.09147'
source_url: https://arxiv.org/abs/2402.09147
tags:
- self-learning
- knowledge
- questions
- hallucination
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces a framework for self-learning in large language\
  \ models (LLMs) by identifying \"Points in the Unknown\" (PiUs)\u2014knowledge gaps\
  \ where the model hallucinates. Four methods (external prompts, open generation,\
  \ induced generation, and oracle selection) are used to automatically find PiUs,\
  \ enabling the model to learn only what it doesn't know."
---

# Into the Unknown: Self-Learning Large Language Models

## Quick Facts
- arXiv ID: 2402.09147
- Source URL: https://arxiv.org/abs/2402.09147
- Authors: Teddy Ferdinan; Jan Kocoń; Przemysław Kazienko
- Reference count: 40
- One-line primary result: Self-learning framework reduces LLM hallucinations on test questions from 0.73 to 0.29 while maintaining knowledge retention

## Executive Summary
This paper introduces a framework for self-learning in large language models by identifying "Points in the Unknown" (PiUs) - knowledge gaps where the model hallucinates. The approach uses four methods to automatically find PiUs, enabling the model to learn only what it doesn't know. Experiments demonstrate that models with at least 3B parameters and instruction fine-tuning perform well in self-learning, with significant hallucination reduction and stable knowledge retention through LoRA and dynamic adapter training.

## Method Summary
The framework implements a self-learning loop where LLMs identify their own knowledge gaps through hallucination scoring. Four PiU identification methods (external prompts, open generation, induced generation, and oracle selection) generate questions, which are scored using SelfCheckGPT. Questions exceeding a hallucination threshold (LIMIT = 0.5) are classified as PiUs. The model then searches external knowledge sources for answers and trains using LoRA or dynamic adapters to integrate new knowledge without catastrophic forgetting.

## Key Results
- Hallucination scores on test questions dropped significantly from 0.73 to 0.29
- Knowledge retention remained stable as measured by perplexity on Wikipedia dataset
- Models with at least 3B parameters and instruction fine-tuning showed effective self-learning capability
- Dynamic adapter training (Dyn-Adapt) prevented catastrophic forgetting compared to plain LoRA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-learning identifies unknown knowledge through hallucination scoring
- Mechanism: The model generates responses to questions, and if the hallucination score (h(x)) exceeds a threshold (LIMIT = 0.5), the content is classified as a Point in the Unknown (PiU), indicating missing knowledge
- Core assumption: Hallucinations on simple questions primarily indicate a lack of factual knowledge rather than other causes
- Evidence anchors: [abstract] "enables an LLM to independently learn previously unknown knowledge through self-assessment of their own hallucinations"
- Break condition: If hallucination scores are unreliable or if hallucinations are caused by factors other than missing knowledge (e.g., prompt ambiguity, model bias)

### Mechanism 2
- Claim: Self-learning uses four methods to systematically identify PiUs
- Mechanism: (1) External prompts (trending topics), (2) Open generation (model-proposed topics), (3) Induced generation (5W+1H questions), (4) Oracle-selected (random topic sampling). Each method generates questions, scores hallucinations, and filters for PiUs
- Core assumption: Different methods cover complementary knowledge spaces and improve PiU discovery
- Evidence anchors: [abstract] "four methods for automatic PiUs identification"
- Break condition: If methods produce redundant or non-diverse questions, limiting PiU coverage

### Mechanism 3
- Claim: Dynamic adapter training avoids catastrophic forgetting during self-learning
- Mechanism: Instead of merging LoRA adapters, the model uses a router-classifier to enable the most relevant adapter per prompt, preserving base knowledge while integrating new PiUs
- Core assumption: Small, task-specific adapters can store new knowledge without overwriting base model weights
- Evidence anchors: [section] "catastrophic forgetting can be avoided by carefully choosing the training technique and architecture"
- Break condition: If the router-classifier fails to select the correct adapter, causing degraded performance on old tasks

## Foundational Learning

- Concept: Hallucination detection and scoring
  - Why needed here: Determines whether a model's response is based on known knowledge or fabricated content
  - Quick check question: Can you explain how SelfCheckGPT scores hallucinations using consistency between main passage and samples?

- Concept: Embedding-based topic generation
  - Why needed here: Enables the model to propose or sample topics for self-questioning in intrinsic methods
  - Quick check question: How does the Oracle-Selected method use embedding space sampling to generate diverse topics?

- Concept: Adapter-based continual learning
  - Why needed here: Allows the model to integrate new knowledge without overwriting existing weights
  - Quick check question: What is the difference between Plain LoRA and Dynamic-Adapter (Dyn-Adapt) in terms of weight updates?

## Architecture Onboarding

- Component map:
  Self-Questioning -> Knowledge Searching -> Model Training -> Evaluation

- Critical path:
  1. Self-Questioning → PiU identification (QH list)
  2. Knowledge Searching → Dtrain dataset creation
  3. Model Training → LoRA/Dyn-Adapt update
  4. Evaluation → Check hallucination reduction and knowledge retention

- Design tradeoffs:
  - Intrinsic vs. extrinsic PiU methods: Intrinsic is more autonomous but may lack topical diversity; extrinsic ensures topical relevance but depends on external APIs
  - Plain LoRA vs. Dyn-Adapt: Plain LoRA is simpler but risks forgetting; Dyn-Adapt preserves knowledge but adds routing complexity
  - Adapter size (r=64, alpha=128): Balances learning capacity and parameter efficiency

- Failure signatures:
  - High brevity coefficient (brev = 0): Model fails to generate concise questions, halting self-learning
  - Perplexity spike on Wiki dataset: Indicates catastrophic forgetting
  - Low SLC score: Model lacks instruction-following capability or has excessive knowledge, reducing PiU discovery

- First 3 experiments:
  1. Test PiU identification using Open Generation on a small instruction-tuned model (e.g., phi-3-mini) with 100 iterations; check Curiosity and Knowledge-Limit Awareness scores
  2. Run one self-learning cycle with Dyn-Adapt on mistral-instruct; measure hallucination score reduction on QH dataset
  3. Compare Plain LoRA vs. Dyn-Adapt on catastrophic forgetting using Wiki dataset perplexity after training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold value for hallucination score h(x) to distinguish between Known and Unknown knowledge regions, and how does this threshold vary across different LLM architectures and sizes?
- Basis in paper: [explicit] The paper mentions using LIMIT = 0.5 as a threshold for SelfCheckGPT but acknowledges this is a starting point that could be optimized
- Why unresolved: The paper only uses a fixed threshold of 0.5 without exploring how different threshold values might affect self-learning performance across various model architectures and sizes
- What evidence would resolve it: Systematic experiments testing different threshold values (e.g., 0.3, 0.4, 0.6, 0.7) across multiple model sizes and architectures, measuring their impact on self-learning effectiveness and knowledge acquisition rates

### Open Question 2
- Question: How does the self-learning framework perform when extended beyond English to other languages, and what modifications are needed for the brevity coefficient and hallucination detection methods?
- Basis in paper: [inferred] The paper acknowledges that experiments were limited to English and mentions that further studies would be required to calibrate the brevity coefficient for different languages
- Why unresolved: The current framework relies on language-specific components like the brevity coefficient (based on English sentence length statistics) and SelfCheckGPT, which may not transfer well to other languages
- What evidence would resolve it: Comparative experiments applying the self-learning framework to multiple languages, testing whether existing components need adjustment and what new calibration methods are required

### Open Question 3
- Question: What is the long-term effectiveness and stability of the self-learning process over multiple cycles, particularly regarding catastrophic forgetting and knowledge retention?
- Basis in paper: [explicit] The paper acknowledges this as a limitation, stating "a deeper study into extensive cycles of self-learning is needed" and noting that robustness for multiple cycles still needs evaluation
- Why unresolved: The paper only demonstrates one full self-learning cycle and does not investigate how performance changes over many iterations or whether the model maintains knowledge balance
- What evidence would resolve it: Long-term experiments running self-learning for dozens or hundreds of cycles, tracking knowledge retention, catastrophic forgetting rates, and overall model stability over time

## Limitations
- The framework's performance heavily depends on the quality of hallucination scoring, which is not directly evaluated in the paper
- The scalability of the approach to much larger models (>30B parameters) and its performance on highly specialized domains are not explored
- The assertion that the framework enables knowledge exchange between models is speculative and not experimentally validated

## Confidence
- **High**: The basic self-learning loop structure and its ability to reduce hallucination scores on test questions (from 0.73 to 0.29) are well-supported by experimental results
- **Medium**: The effectiveness of the four PiU identification methods in covering diverse knowledge spaces is plausible but relies on assumptions about method complementarity without extensive validation
- **Medium**: The claim that instruction fine-tuning is necessary for effective self-learning is supported by experimental evidence showing poor performance without it
- **Low**: The assertion that the framework enables knowledge exchange between models is speculative and not experimentally validated

## Next Checks
1. Conduct ablation studies comparing hallucination scoring accuracy against ground-truth knowledge gaps to validate the core assumption about hallucination causes
2. Test the framework's performance on a highly specialized domain (e.g., medical or legal texts) to assess scalability and domain adaptability
3. Compare Dyn-Adapt against other continual learning methods (e.g., Elastic Weight Consolidation) to quantify its advantage in preventing catastrophic forgetting