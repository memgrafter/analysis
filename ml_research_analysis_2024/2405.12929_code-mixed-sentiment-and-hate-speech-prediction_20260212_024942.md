---
ver: rpa2
title: Code-mixed Sentiment and Hate-speech Prediction
arxiv_id: '2405.12929'
source_url: https://arxiv.org/abs/2405.12929
tags:
- language
- code-mixed
- languages
- multilingual
- sentiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates large language models for code-mixed sentiment
  analysis and hate speech detection across five languages (French, Hindi, Russian,
  Slovene, Tamil). Four new bilingual models for English-Hindi and English-Slovene
  are introduced, trained on informal language data.
---

# Code-mixed Sentiment and Hate-speech Prediction

## Quick Facts
- arXiv ID: 2405.12929
- Source URL: https://arxiv.org/abs/2405.12929
- Reference count: 40
- Four new bilingual models for English-Hindi and English-Slovene outperform monolingual and general multilingual models for code-mixed sentiment and hate-speech tasks

## Executive Summary
This study evaluates large language models for code-mixed sentiment analysis and hate speech detection across five languages (French, Hindi, Russian, Slovene, Tamil). The research introduces four new bilingual models trained on informal language data and compares their performance against monolingual, bilingual, few-lingual, and massively multilingual models, including GPT-3 and Llama2. Results demonstrate that bilingual models and social-media-specialized multilingual models significantly outperform general multilingual and monolingual models, with generative models performing worst. The study also reveals that models perform slightly better on code-mixed than non-code-mixed data for affective tasks.

## Method Summary
The research introduces four new bilingual models (NLLB-1.2-EnHi, mT5-EnHi, XLM-RoBERTa-EnHi, BERTje-EnHi) and extends them to English-Slovene variants. These models are trained on informal language data including Twitter, Reddit, and social media comments. The study conducts experiments across five languages using monolingual, bilingual, few-lingual, and massively multilingual models. Two large generative models (GPT-3 and Llama2) are also evaluated. Models are tested on three tasks: sentiment analysis, hate speech detection, and topic classification. Performance is measured using accuracy, precision, recall, and F1-score metrics.

## Key Results
- Bilingual models and social-media-specialized multilingual models outperform general multilingual and monolingual models
- GPT-3 and Llama2 perform worst among all evaluated models
- Models perform slightly better on code-mixed than non-code-mixed data for affective tasks
- Manual annotation reveals code-mixing has limited impact on sentiment and hate speech detection in studied datasets

## Why This Works (Mechanism)
The success of bilingual models stems from their specialized training on code-mixed data, allowing them to capture the unique linguistic patterns and contextual dependencies present in code-switching scenarios. Social-media-specialized multilingual models perform well because they are trained on informal language data that closely matches the characteristics of code-mixed text, including slang, abbreviations, and unconventional grammar. The bilingual models' architecture enables better handling of language transitions and code-switching patterns compared to general multilingual models that may not have been exposed to sufficient code-mixed training data.

## Foundational Learning
- **Code-mixing patterns**: Understanding how languages blend in informal contexts is crucial for designing appropriate models and preprocessing pipelines. Quick check: Analyze code-switching frequency and patterns in target datasets.
- **Multilingual model architectures**: Knowledge of transformer-based multilingual models helps in selecting appropriate baseline models and understanding their limitations with code-mixed data. Quick check: Compare attention mechanisms across different multilingual models.
- **Social media linguistics**: Familiarity with informal language patterns, slang, and unconventional grammar is essential for preprocessing and interpreting results. Quick check: Create a dictionary of common code-mixing patterns in target languages.

## Architecture Onboarding

Component map: Raw text -> Preprocessing -> Model architecture (transformer-based) -> Task-specific head -> Output prediction

Critical path: Input text → Tokenizer → Encoder layers → Pooling layer → Classification head → Task prediction

Design tradeoffs: Bilingual models offer better performance but require more resources to train; multilingual models provide broader coverage but may lack specialization; large generative models offer flexibility but underperform on specific code-mixed tasks.

Failure signatures: Poor performance on code-switching boundaries, inability to handle informal language patterns, confusion between similar words across languages, degradation in performance when code-mixing frequency increases.

First experiments:
1. Evaluate baseline multilingual model performance on monolingual vs. code-mixed subsets
2. Test bilingual model on code-mixed test set with varying code-mixing ratios
3. Compare attention patterns between bilingual and multilingual models on code-switching points

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow focus on only five languages limits generalizability to other code-mixed language pairs
- Manual annotation validation covers only 5% of the test set, limiting confidence in findings about code-mixing impact
- Comparison lacks standardization in evaluation metrics across different model types
- Informal training data may not represent all code-mixing scenarios, particularly formal or domain-specific contexts

## Confidence

High confidence:
- Performance ranking of bilingual vs. monolingual models
- Finding that bilingual models outperform monolingual ones for code-mixed tasks

Medium confidence:
- Generalization of model performance across different code-mixed language pairs
- Claim that code-mixing has minimal impact on sentiment and hate speech detection

## Next Checks
1. Expand manual annotation validation to at least 15-20% of the test set across all languages and tasks to strengthen claims about code-mixing impact
2. Test model performance on additional code-mixed language pairs beyond English-Hindi and English-Slovene to assess generalizability
3. Conduct ablation studies comparing model performance on formal vs. informal code-mixed text to evaluate the impact of training data domain on task performance