---
ver: rpa2
title: Performance Characterization of Expert Router for Scalable LLM Inference
arxiv_id: '2404.15153'
source_url: https://arxiv.org/abs/2404.15153
tags:
- expert
- router
- inference
- user
- routing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Expert Router introduces minimal latency overhead while maintaining
  competitive throughput across various concurrency levels, with routing configuration
  being the dominant factor in performance outcomes. The system uses unsupervised
  clustering (k-means) to direct prompts to specialized expert models, creating a
  scalable inference pipeline.
---

# Performance Characterization of Expert Router for Scalable LLM Inference

## Quick Facts
- arXiv ID: 2404.15153
- Source URL: https://arxiv.org/abs/2404.15153
- Authors: Josef Pichlmeier; Philipp Ross; Andre Luckow
- Reference count: 40
- Primary result: Expert Router achieves minimal latency overhead while maintaining competitive throughput across various concurrency levels, with routing configuration being the dominant factor in performance outcomes

## Executive Summary
Expert Router introduces a scalable routing architecture for LLM inference that uses unsupervised clustering (k-means) to direct prompts to specialized expert models. The system demonstrates competitive performance compared to tensor-parallelized baseline models while maintaining lower latency at high concurrency levels. Under 1,000 concurrent users, Expert Router achieved p99 response times of 10-20 seconds with user throughput of 75-100 tokens/second. The routing configuration significantly impacts performance outcomes, with smaller expert models (8B) showing better throughput and latency characteristics than tensor-parallelized baseline models (70B) at high concurrency levels.

## Method Summary
The evaluation employs the C4 dataset to generate prompts for synthetic users, with k-means clustering trained on this dataset to classify prompts into domains. The system benchmarks two Expert Router configurations: (D) eight quantized 70B models and (E) eight 8B models, comparing against baseline models (Llama 3 70B with tensor parallelism and quantization). Experiments measure user-centric metrics (TTFT, TPOT, user throughput) and system-centric metrics (p99 response time, system throughput over time) under varying concurrent users (100, 500, 1000) and workload distributions (uniform and normal). The infrastructure consists of DGX H100 with eight H100 GPUs, each with 80GB memory, hosting routing gateway and Triton inference servers.

## Key Results
- Expert Router introduces minimal latency overhead (442 ms average) while maintaining competitive throughput across concurrency levels
- Smaller expert models (8B) achieve better throughput and latency than tensor-parallelized baseline models (70B) at high concurrency
- System maintains stable performance under both balanced and imbalanced workloads without requiring perfect model utilization
- High-parameter quantized models (70B INT4) provide efficient parameter scaling with acceptable performance trade-offs at moderate concurrency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Routing to smaller expert models achieves lower latency than tensor-parallelized baseline models at high concurrency
- Mechanism: k-means clustering classifies prompts into domains, directing each to specialized LLMs. Smaller models (8B) require fewer computations per token, reducing TPOT compared to large tensor-parallelized models (70B)
- Core assumption: Classification overhead (442 ms avg) is negligible compared to latency reduction from smaller models during decode phase
- Evidence anchors: Abstract states smaller models maintain competitive performance; section shows similar TTFT values for 8B configuration as tensor-parallelized models

### Mechanism 2
- Claim: Expert Router maintains stable throughput under imbalanced workloads
- Mechanism: Routing gateway distributes requests across multiple independent Triton servers. Even with uneven request distribution, other models remain underutilized, preserving overall throughput
- Core assumption: Model utilization remains within acceptable bounds; no single model becomes bottleneck
- Evidence anchors: Abstract mentions configuration being dominating factor; section shows minor differences between normal and uniform distributions

### Mechanism 3
- Claim: Quantized high-parameter models efficiently increase total parameter count without substantially compromising performance at moderate concurrency
- Mechanism: INT4 quantization reduces weight memory footprint while preserving model capabilities, allowing deployment of multiple high-parameter models on same GPU infrastructure
- Core assumption: Quantization maintains sufficient model quality for routing decisions and response generation
- Evidence anchors: Abstract states high-parameter models deliver stable throughput; section mentions using AWQ quantization within TensorRT-LLM library

## Foundational Learning

- Concept: Transformer self-attention mechanism
  - Why needed here: Expert Router relies on specialized LLMs built on transformer architecture; understanding attention computation is crucial for grasping inference optimization
  - Quick check question: What are the three vectors computed from input tokens in self-attention, and what is their purpose?

- Concept: Mixture-of-Experts (MoE) routing
  - Why needed here: Expert Router implements simplified MoE approach using k-means clustering instead of dynamic expert selection during inference
  - Quick check question: How does k-means clustering approach differ from traditional MoE routing in terms of computational overhead during inference?

- Concept: Quantization techniques (AWQ)
  - Why needed here: Expert Router uses AWQ quantization to deploy high-parameter models efficiently; understanding quantization trade-offs is essential for configuration decisions
  - Quick check question: What is primary benefit of activation-aware quantization compared to standard quantization methods?

## Architecture Onboarding

- Component map: User requests -> NGINX load balancer -> 16 routing gateway instances -> k-means clustering -> Triton inference servers -> Expert models -> Response back through routing gateway

- Critical path: 1) User request arrives at NGINX load balancer 2) Request forwarded to routing gateway instance 3) Prompt classified via k-means clustering 4) Prompt routed to appropriate Triton server 5) Model inference with streaming response 6) Response returned through routing gateway to user

- Design tradeoffs:
  - Tensor-parallelization vs. Expert Router: TP reduces per-model latency but requires high-bandwidth GPU interconnect; Expert Router uses independent models with higher classification overhead but scales better with infrastructure
  - Model size vs. KV-cache: Smaller models (8B) allocate more memory to KV-cache, improving decode phase throughput; larger models (70B) have more parameters but less KV-cache per model
  - Quantization vs. precision: INT4 reduces memory usage but may impact model quality; FP16/FP8 provides better quality at higher memory cost

- Failure signatures:
  - Routing gateway latency spikes → classification bottleneck
  - Uneven model utilization → specific models become overwhelmed
  - GPU memory exhaustion → model crashes or degraded performance
  - Network congestion between routing gateway and Triton servers → increased response times

- First 3 experiments:
  1. Deploy single routing gateway with 2 expert models, measure classification latency and basic routing accuracy
  2. Scale to 16 routing gateways with load balancer, test under increasing concurrent users (100, 500, 1000)
  3. Compare quantized vs. non-quantized model configurations under identical workloads to quantify quality vs. efficiency trade-offs

## Open Questions the Paper Calls Out

- How does Expert Router's performance scale when using dynamic LoRA weight deployment to increase the number of experts without increasing the of models?
- What is the optimal clustering strategy for routing prompts to expert models beyond simple k-means clustering of prompt domains?
- How does Expert Router's performance change when using different embedding and classification algorithms for the routing gateway?

## Limitations

- Evaluation focuses on synthetic workloads from C4 dataset, which may not capture real-world query diversity and complexity
- k-means clustering approach may not provide optimal routing decisions compared to more sophisticated methods
- Study does not evaluate model quality or accuracy trade-offs between different configurations
- Analysis of imbalanced workloads only considers normal and uniform distributions

## Confidence

**High Confidence:**
- Routing gateway introduces minimal latency overhead (442 ms average) compared to decode phase performance improvements from smaller expert models
- Expert Router configurations maintain stable performance across both balanced and imbalanced workloads
- High-parameter quantized models (70B INT4) deliver stable throughput and latency under moderate concurrency levels (500 users)

**Medium Confidence:**
- Smaller expert models (8B) achieve lower latency than tensor-parallelized baseline models (70B) at high concurrency
- Quantized high-parameter models can efficiently scale total parameter count without substantial performance compromise at moderate concurrency
- System can handle up to 1,000 concurrent users with p99 response times of 10-20 seconds while maintaining user throughput of 75-100 tokens/second

**Low Confidence:**
- Routing accuracy and quality of responses across different expert model configurations are maintained at acceptable levels
- Performance characteristics observed in synthetic C4-based workloads generalize to real-world deployment scenarios
- Specific routing configuration represents optimal setup for general-purpose LLM inference

## Next Checks

1. **Real-world workload validation:** Deploy Expert Router with actual user traffic from production LLM applications to verify that synthetic workload performance characteristics translate to real-world scenarios, particularly measuring routing accuracy and response quality across diverse query types.

2. **Quality vs. performance trade-off analysis:** Implement comprehensive evaluation of model quality (using standard benchmarks and human evaluation) across different Expert Router configurations to quantify the precision-efficiency trade-offs, especially comparing quantized vs. non-quantized models and different model sizes.

3. **Dynamic workload adaptation testing:** Create experiments with more complex, time-varying workload distributions that include bursty traffic, seasonal patterns, and gradual shifts in query distribution to assess the system's ability to handle realistic operational conditions beyond the uniform and normal distributions tested.