---
ver: rpa2
title: 'FairPIVARA: Reducing and Assessing Biases in CLIP-Based Multimodal Models'
arxiv_id: '2409.19474'
source_url: https://arxiv.org/abs/2409.19474
tags:
- bias
- fairpiv
- concepts
- biases
- good
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work tackles bias in vision-language models, particularly\
  \ in the multilingual CAPIVARA model, by introducing FairPIVARA, a post-processing\
  \ method that reduces bias by removing the most harmful dimensions from feature\
  \ embeddings. Evaluated across four concept groups\u2014Disability, Nationality,\
  \ Religion, and Sexual Orientation\u2014FairPIVARA significantly reduces bias by\
  \ up to 98% while maintaining classification accuracy with only a minimal drop of\
  \ up to 1.5 percentage points."
---

# FairPIVARA: Reducing and Assessing Biases in CLIP-Based Multimodal Models

## Quick Facts
- **arXiv ID**: 2409.19474
- **Source URL**: https://arxiv.org/abs/2409.19474
- **Reference count**: 40
- **Primary result**: Reduces bias in CLIP-based multimodal models by up to 98% while maintaining classification accuracy with only 1.5 percentage points drop.

## Executive Summary
This paper introduces FairPIVARA, a post-processing method that reduces biases in vision-language models by removing the most harmful dimensions from feature embeddings. Evaluated on CLIP and CAPIVARA models across four concept groups (Disability, Nationality, Religion, and Sexual Orientation), FairPIVARA achieves significant bias reduction while maintaining high classification accuracy. The method demonstrates effectiveness in both English and Portuguese datasets, showing promise for low-resource language scenarios without requiring full model retraining.

## Method Summary
FairPIVARA is a post-processing algorithm that reduces bias in CLIP-based multimodal models by iteratively removing dimensions from image embeddings that contribute most to bias. The method identifies harmful dimensions based on their impact on bias scores measured through cosine similarity between class embeddings and good/bad concept embeddings. A mutual information threshold (θ = 0.05) ensures embedding quality is maintained during dimension removal. Since CLIP-based models share a common embedding space, modifications to image embeddings indirectly affect text embeddings, reducing bias across both modalities. The approach is applied to both English and Portuguese datasets without requiring model retraining.

## Key Results
- Reduces bias by up to 98% while maintaining classification accuracy with only 1.5 percentage points drop
- Effective on both English and Portuguese datasets, demonstrating utility in low-resource language scenarios
- Outperforms baseline approaches with an average bias reduction of 95% compared to 9.1% for baselines
- Maintains semantic similarity between intermediate and label embeddings throughout the process

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing the most harmful dimensions from feature embeddings reduces bias in multimodal models.
- Mechanism: FairPIVARA identifies dimensions in the image embeddings that most negatively affect bias scores, then removes them. This reduces the model's tendency to associate certain concepts with specific groups while preserving overall classification accuracy.
- Core assumption: The most harmful dimensions can be identified by their impact on bias score reduction when removed.
- Evidence anchors:
  - [abstract] "introduce FairPIVARA, a method to reduce them by removing the most affected dimensions of feature embeddings."
  - [section] "We define the most harmful dimension as the one that results in the smallest reduction in the bias score when removed."
- Break condition: If the bias reduction becomes negligible or if the classification performance drops significantly, the dimension removal process would need to stop.

### Mechanism 2
- Claim: Applying FairPIVARA to image embeddings indirectly reduces bias in text embeddings due to shared representation space.
- Mechanism: Since CLIP-based models map all modalities into the same embedding space, modifying image embeddings affects the corresponding text embeddings, thus mitigating bias in both modalities.
- Core assumption: The shared embedding space means that bias in image dimensions correlates with bias in text dimensions.
- Evidence anchors:
  - [section] "Multimodal models map all modalities into the same embedding space (shared representation); consequently, image and text embeddings are the same size."
  - [section] "Although the FairPIVARA method is applied only to images, we show indirectly, through multimodal classification and retrieval, that when we apply and optimize the set of images, we also indirectly optimize the textual embeddings."
- Break condition: If the bias reduction in text embeddings is not significant after applying FairPIVARA to images, this mechanism would fail.

### Mechanism 3
- Claim: FairPIVARA can be applied to models adapted to low-resource languages without full retraining.
- Mechanism: By removing harmful dimensions post-hoc, FairPIVARA reduces bias in the CAPIVARA model (a Portuguese-adapted CLIP model) without requiring expensive retraining or additional data.
- Core assumption: Bias reduction through dimension removal is effective regardless of the language the model was trained on.
- Evidence anchors:
  - [abstract] "extends to both English and Portuguese datasets, demonstrating effectiveness in low-resource language scenarios."
  - [section] "We also applied FairPIVARA to the CAPIVARA model to evaluate whether these results hold in models trained in other languages."
- Break condition: If the bias reduction is significantly less effective in low-resource language models compared to English models, this mechanism would fail.

## Foundational Learning

- Concept: Feature embeddings and shared representation space
  - Why needed here: Understanding how CLIP-based models encode both images and text into the same embedding space is crucial for grasping how FairPIVARA works.
  - Quick check question: How do CLIP-based models represent images and text in the same space?

- Concept: Bias metrics and evaluation
  - Why needed here: To understand how FairPIVARA measures and reduces bias, one needs to know about bias metrics like cosine similarity and bias scoring functions.
  - Quick check question: What metric is used to measure bias between class concepts and good/bad concepts?

- Concept: Dimension removal and its effects
  - Why needed here: FairPIVARA works by removing dimensions from embeddings, so understanding how this affects model performance and bias is essential.
  - Quick check question: What is the trade-off between removing dimensions for bias reduction and maintaining classification accuracy?

## Architecture Onboarding

- Component map: CLIP/OpenCLIP model -> Image embeddings -> FairPIVARA dimension removal -> Modified image embeddings -> Classification/Retrieval tasks
- Critical path: Calculate bias scores for each dimension → Identify most harmful dimensions → Remove dimensions while maintaining mutual information threshold → Apply same dimensions to text embeddings
- Design tradeoffs: Bias reduction vs. classification accuracy; removing same dimensions for images and text vs. random removal for text; dimension removal amount vs. processing time
- Failure signatures: Negligible bias reduction, significant accuracy drop, mutual information falling below threshold
- First 3 experiments:
  1. Apply FairPIVARA to a small CLIP-based model and measure the bias reduction and accuracy drop.
  2. Vary the number of dimensions removed and observe the effect on bias reduction and accuracy.
  3. Apply FairPIVARA to both English and Portuguese versions of the model and compare the results.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, some implicit open questions include:
- How does FairPIVARA's performance scale when applied to larger embedding spaces beyond the tested range (512-768 dimensions)?
- What is the long-term impact of FairPIVARA on model performance in real-world applications beyond the evaluated downstream tasks?
- How does FairPIVARA perform when applied to multimodal models trained on datasets with different levels of bias and imbalance?

## Limitations

- Limited multilingual validation beyond Portuguese, which may affect generalizability to other low-resource languages
- The exact definitions of "less politically charged" word sets for good/bad concepts are not explicitly provided
- Implementation details for applying dimension removal to text embeddings remain unclear

## Confidence

- **High Confidence**: The bias reduction mechanism (removing harmful dimensions) is well-supported by experimental results showing up to 98% bias reduction with minimal accuracy loss.
- **Medium Confidence**: The effectiveness of FairPIVARA on low-resource languages is demonstrated for Portuguese but lacks broader multilingual validation.
- **Medium Confidence**: The claim that removing dimensions from image embeddings indirectly reduces text embedding bias relies on shared representation space, but the extent of this effect is not fully quantified.

## Next Checks

1. Validate Word Set Definitions: Reconstruct or obtain the exact word sets for good/bad concepts in both English and Portuguese to ensure faithful reproduction of bias evaluations.
2. Test on Additional Multilingual Models: Apply FairPIVARA to other multilingual CLIP models (e.g., X-CLIP) to verify its effectiveness beyond Portuguese.
3. Quantify Text Embedding Bias Reduction: Explicitly measure bias reduction in text embeddings after applying FairPIVARA to image embeddings to confirm the shared representation space mechanism.