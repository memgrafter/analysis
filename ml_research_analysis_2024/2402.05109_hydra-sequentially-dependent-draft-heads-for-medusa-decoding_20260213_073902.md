---
ver: rpa2
title: 'Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding'
arxiv_id: '2402.05109'
source_url: https://arxiv.org/abs/2402.05109
tags:
- decoding
- hydra
- draft
- heads
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hydra heads are a sequentially-dependent alternative to existing
  draft heads used in Medusa decoding. Unlike standard draft heads, which predict
  future tokens based only on the base model's hidden state, Hydra heads incorporate
  earlier tokens from the candidate continuation as additional inputs.
---

# Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding

## Quick Facts
- arXiv ID: 2402.05109
- Source URL: https://arxiv.org/abs/2402.05109
- Reference count: 34
- Primary result: Sequentially-dependent Hydra heads increase average candidate continuation acceptance length by up to 0.46 tokens and achieve 1.31× higher throughput than Medusa decoding

## Executive Summary
Hydra heads introduce sequential dependence to draft heads used in speculative decoding, allowing each draft head to condition on preceding tokens in the candidate continuation rather than only the base model's hidden state. This architectural change, combined with Hydra++ improvements (deeper MLPs, teacher distillation, prefix attention), significantly improves token prediction accuracy and decoding throughput. The method achieves up to 1.31× and 2.70× higher throughput compared to Medusa and autoregressive decoding respectively, while maintaining generation quality.

## Method Summary
The method introduces Hydra heads that incorporate earlier tokens from candidate continuations as additional inputs, creating sequential dependence absent in standard draft heads. Hydra++ extends this with deeper 4-layer MLPs, teacher distillation training objectives where draft heads predict base model distributions instead of ground truth, and prefix attention layers that provide enriched context. The approach is evaluated on Vicuna models (7B, 13B, 33B) using ShareGPT dataset, with performance measured by decoding throughput, acceptance length, and MT-Bench scores.

## Key Results
- Sequential dependence increases average candidate continuation acceptance length by up to 0.46 tokens
- Hydra decoding achieves 1.31× higher throughput than Medusa decoding
- Hydra++ maintains generation quality during non-greedy sampling while preserving throughput benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential dependence improves prediction accuracy by leveraging token context from earlier positions in the candidate continuation
- Mechanism: Standard draft heads predict future tokens based solely on the base model's hidden state from the last verified token, ignoring already-generated draft tokens. Hydra heads modify this by taking both the base model's hidden state and input embeddings of preceding draft tokens, allowing each head to condition on the entire partial candidate sequence.
- Core assumption: The conditional distribution of the next token depends not only on base context but also on preceding tokens in the candidate continuation
- Evidence anchors:
  - [abstract] "Unlike standard draft heads, which predict future tokens based only on the base model's hidden state, Hydra heads incorporate earlier tokens from the candidate continuation as additional inputs."
  - [section 3] "The key observation behind Hydra heads is that there is no sequential dependence in standard draft heads... we propose Hydra heads, which are sequentially-dependent draft heads."
- Break condition: If statistical dependencies between neighboring tokens in candidate continuations are weak, sequential dependence may not provide meaningful gains and could hurt performance due to draft error propagation

### Mechanism 2
- Claim: Teacher distillation improves draft head accuracy by training heads to match base model's distribution rather than predicting ground-truth next tokens
- Mechanism: Instead of training draft heads to predict actual next tokens in training data, Hydra++ trains them to predict the distribution output by the base model for each position, aligning training objective with inference goal
- Core assumption: Base model's next-token distribution is a better proxy for "acceptability" than true next token during training
- Evidence anchors:
  - [section 3.1] "Following (Zhou et al., 2024), we train on a self-distillation objective where the draft heads are trained to predict the base model's distribution for a given token instead of the true token."
  - [abstract] "We further explore the design space of Hydra head training objectives and architectures... Hydra++ that further increases decoding throughput."
- Break condition: If base model's distribution is too peaked or poorly calibrated, training to match it may not generalize well

### Mechanism 3
- Claim: Prefix attention improves draft head performance by providing better context aggregation from the entire already-generated sequence
- Mechanism: Hydra++ adds an extra self-attention decoder layer to base model whose sole purpose is producing richer hidden states for draft heads, shared across all draft heads and queried once per decoding step
- Core assumption: Base model's last-layer hidden state for most recent token doesn't fully capture relevant context for predicting future tokens
- Evidence anchors:
  - [section 3.1] "3. Prefix Attention: To improve our draft model's ability to condition on information from across the entire context... we extend the base model with an additional self-attention decoder layer whose only role is to produce more informative hidden states for use as input to the draft model."
  - [abstract] "Hydra++... proposes a carefully tuned Hydra head recipe... that improves decoding throughput by up to 1.31× and 2.70× compared to Medusa decoding and regular autoregressive decoding respectively."
- Break condition: If base model's last-layer hidden state already contains sufficient context for draft heads, extra prefix attention layer adds computation without benefit

## Foundational Learning

- Concept: Speculative decoding and the role of draft heads in accelerating LLM inference
  - Why needed here: Understanding baseline (Medusa) and problem Hydra solves (sequential independence) requires knowing how draft heads work and why they're used
  - Quick check question: In speculative decoding, why is the draft model typically much smaller than the base model, and what happens if it's too large or too small?

- Concept: Teacher distillation and knowledge transfer in model training
  - Why needed here: Hydra++ uses distillation objective; understanding this concept is key to grasping why training to match base model's distribution can be better than predicting ground truth
  - Quick check question: What is the difference between training a model to predict ground-truth labels versus training it to mimic another model's outputs, and when is each approach preferable?

- Concept: Self-attention and context aggregation in transformers
  - Why needed here: Prefix attention in Hydra++ relies on transformer attention mechanisms; understanding how attention layers aggregate context is essential to see why an extra layer helps draft heads
  - Quick check question: How does adding an extra self-attention layer change the way a transformer model processes context, and what are the trade-offs in terms of computation and representational power?

## Architecture Onboarding

- Component map: Base model -> Prefix attention layer (optional) -> Draft heads (Hydra/Hydra++) -> Candidate continuation tree -> Base model verification
- Critical path:
  1. Base model generates last-layer hidden state for most recent verified token
  2. (Optional) Prefix attention layer processes entire sequence to produce enriched hidden states
  3. Each draft head takes enriched hidden state and input embeddings of preceding draft tokens, predicts next token
  4. Candidate continuation tree is built and verified by base model in parallel
  5. Tokens are accepted or rejected according to verification criterion
- Design tradeoffs:
  - Sequential dependence vs. computational overhead: Hydra heads require passing input embeddings of all preceding draft tokens, increasing input size and computation, but this is offset by higher acceptance rates
  - Depth of MLP vs. expressiveness: Hydra++ uses 4-layer MLPs; deeper heads may improve accuracy but increase latency and risk overfitting
  - Prefix attention vs. base model integration: Adding shared attention layer improves context but requires extra forward pass and careful integration
- Failure signatures:
  - Low acceptance rates despite sequential dependence: May indicate base model's distribution is too peaked or draft heads are poorly calibrated
  - Degraded performance with longer candidate continuations: Could mean draft errors accumulate and sequential dependence no longer helps
  - Overhead not compensated by gains: If extra prefix attention layer or deeper MLPs slow down inference more than they improve acceptance, net throughput may decrease
- First 3 experiments:
  1. Implement Hydra heads (without prefix attention or distillation) and compare acceptance length and throughput to Medusa on small dataset; verify sequential dependence alone improves performance
  2. Add prefix attention to Hydra heads and measure impact on acceptance length and latency; check extra layer provides net benefit
  3. Switch from next-token prediction to teacher distillation loss in Hydra head training; evaluate whether this further improves acceptance length and throughput

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Hydra decoding perform with different base model sizes beyond 7B, 13B, and 33B parameters?
- Basis in paper: [explicit] The paper evaluates Hydra decoding on Vicuna models with 7B, 13B, and 33B parameters, but does not explore other model sizes
- Why unresolved: The paper focuses on these specific model sizes, likely due to computational constraints or availability of these models, but does not investigate how Hydra decoding might scale to other model sizes
- What evidence would resolve it: Conducting experiments with base models of different sizes (e.g., 1B, 3B, 16B, 70B) and comparing their performance using Hydra decoding would provide insights into scalability and effectiveness across various model scales

### Open Question 2
- Question: How does Hydra decoding perform in multi-GPU or distributed inference settings?
- Basis in paper: [inferred] The paper mentions that experiments are conducted on single GPUs, but does not explore multi-GPU or distributed inference scenarios
- Why unresolved: The paper focuses on single-GPU experiments, likely due to complexity of setting up multi-GPU or distributed environments, but does not investigate how Hydra decoding might perform in these more complex setups
- What evidence would resolve it: Implementing Hydra decoding in multi-GPU or distributed inference frameworks and measuring its performance compared to single-GPU setups would provide insights into scalability and efficiency in large-scale deployments

### Open Question 3
- Question: How does Hydra decoding perform with different types of base models, such as encoder-decoder or decoder-only models?
- Basis in paper: [inferred] The paper uses Vicuna, which is a decoder-only model, but does not explore how Hydra decoding might work with encoder-decoder models or other types of base models
- Why unresolved: The paper focuses on decoder-only models, likely due to their prevalence in language modeling tasks, but does not investigate how Hydra decoding might be adapted or perform with other model architectures
- What evidence would resolve it: Implementing Hydra decoding with encoder-decoder models (e.g., T5, BART) and comparing their performance to decoder-only models would provide insights into versatility and applicability across different model types

### Open Question 4
- Question: How does Hydra decoding perform with different types of draft models, such as smaller language models or different architectures?
- Basis in paper: [inferred] The paper uses Hydra heads as the draft model, but does not explore how Hydra decoding might perform with other types of draft models, such as smaller language models or different architectures
- Why unresolved: The paper focuses on Hydra heads as the draft model, likely due to their simplicity and efficiency, but does not investigate how Hydra decoding might be adapted or perform with other draft model choices
- What evidence would resolve it: Implementing Hydra decoding with different draft models (e.g., smaller language models, different architectures like CNNs or RNNs) and comparing their performance to Hydra heads would provide insights into flexibility and effectiveness with various draft model choices

### Open Question 5
- Question: How does Hydra decoding perform in real-world, production-level applications with varying workloads and latency requirements?
- Basis in paper: [inferred] The paper evaluates Hydra decoding on benchmark datasets and controlled experiments, but does not explore its performance in real-world, production-level applications with varying workloads and latency requirements
- Why unresolved: The paper focuses on controlled experiments, likely due to ease of measuring and comparing performance in these settings, but does not investigate how Hydra decoding might perform in more complex and dynamic environment of real-world applications
- What evidence would resolve it: Deploying Hydra decoding in production systems and monitoring its performance under varying workloads, latency requirements, and user interactions would provide insights into practical applicability and effectiveness in real-world scenarios

## Limitations

- Performance improvements may not generalize beyond Vicuna models or decoder-only architectures
- Error propagation in sequential dependence mechanism could degrade performance for longer candidate continuations
- Contribution of individual architectural improvements (prefix attention, distillation, deeper MLPs) to overall performance gains remains unclear without ablation studies

## Confidence

**High confidence**: Core claim that Hydra heads achieve higher throughput (1.31× vs Medusa, 2.70× vs autoregressive) is well-supported by experimental results across multiple model sizes and benchmarks

**Medium confidence**: Claims that teacher distillation and prefix attention specifically contribute to improvements are supported but lack isolated ablation studies

**Low confidence**: Claims about Hydra++ being "carefully tuned" and optimal are not substantiated by systematic hyperparameter searches or comparisons with alternative architectural choices

## Next Checks

1. **Ablation study on architectural components**: Implement and test Hydra variants with individual components (sequential dependence only, prefix attention only, teacher distillation only) to quantify each element's contribution to overall performance

2. **Error rate and propagation analysis**: Systematically measure draft head accuracy at each position in candidate continuations of varying lengths, tracking how errors propagate through sequential dependence

3. **Generalization across model families and domains**: Test Hydra decoding with different base model architectures (not just Vicuna), including models trained on different datasets or domains (code, scientific text, etc.)