---
ver: rpa2
title: 'Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms'
arxiv_id: '2410.18967'
source_url: https://arxiv.org/abs/2410.18967
tags:
- ferret-ui
- arxiv
- data
- tasks
- platforms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Ferret-UI 2 is a multimodal large language model designed for
  universal user interface understanding across multiple platforms including iPhone,
  Android, iPad, Webpage, and AppleTV. It introduces three key innovations: support
  for diverse platform types, high-resolution perception through adaptive scaling,
  and advanced task training data generation using GPT-4o with set-of-mark visual
  prompting.'
---

# Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms

## Quick Facts
- **arXiv ID**: 2410.18967
- **Source URL**: https://arxiv.org/abs/2410.18967
- **Reference count**: 35
- **Key outcome**: Outperforms Ferret-UI and shows competitive performance vs GPT-4o on multi-platform UI understanding tasks

## Executive Summary
Ferret-UI 2 is a multimodal large language model designed for universal user interface understanding across diverse platforms including iPhone, Android, iPad, Webpage, and AppleTV. It introduces three key innovations: support for diverse platform types, high-resolution perception through adaptive scaling, and advanced task training data generation using GPT-4o with set-of-mark visual prompting. The model significantly outperforms its predecessor Ferret-UI and shows competitive performance compared to GPT-4o on referring, grounding, and user-centric advanced tasks across all platforms. It also demonstrates strong cross-platform transfer capabilities and achieves state-of-the-art results on recent benchmarks like GUIDE and GUI-World.

## Method Summary
Ferret-UI 2 fine-tunes multimodal LLMs (Vicuna-13B, Llama3-8B, Gemma-2B) with CLIP ViT-L/14 image encoder using adaptive N-gridding for high-resolution encoding. The model generates advanced task training data using GPT-4o with set-of-mark visual prompting, covering comprehensive descriptions, multi-round perception QA, and user-centered interaction QA. Training data includes multi-platform UI screenshots with bounding box annotations and text labels, leveraging human-collected annotations or HTML-parsed bounding boxes for higher quality.

## Key Results
- Significantly outperforms Ferret-UI on multi-platform UI understanding tasks
- Shows competitive performance vs GPT-4o on referring, grounding, and advanced tasks
- Achieves state-of-the-art results on GUIDE and GUI-World benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4o with set-of-mark visual prompting generates higher quality training data for advanced UI tasks
- Mechanism: Set-of-mark visual prompting provides GPT-4o with explicit visual input of UI elements and their spatial relationships, enabling generation of diverse, contextually rich training examples covering comprehensive descriptions, multi-round perception QA, and user-centered interaction QA
- Core assumption: GPT-4o can effectively utilize visual input to generate better training data than text-only prompts
- Evidence anchors:
  - [abstract]: "GPT-4o with set-of-mark visual prompting... for training data generation. This approach enhances spatial understanding of UI elements, resulting in higher-quality training data."
  - [section 3.1]: "GPT-4o to generate 3 types of advanced tasks... We empirically find it hard for GPT-4o to find the location of referred UI widgets with the original screenshot as input... To address this, we use Set-of-Mark (SoM) visual prompting"
  - [corpus]: Weak - No corpus evidence available for this specific mechanism

### Mechanism 2
- Claim: Adaptive N-gridding provides optimal resolution handling across diverse UI platforms
- Mechanism: The adaptive N-gridding algorithm finds the optimal grid configuration that minimizes aspect ratio and pixel number changes while respecting a predefined inference cost limit, enabling high-resolution UI understanding across different platforms
- Core assumption: Minimizing resolution distortion preserves information while maintaining computational efficiency
- Evidence anchors:
  - [section 3.2]: "The optimal gridding size Nw and Nh is determined when the gridding and resizing based on (Nw, Nh) lead to minimal aspect ratio change times the relative pixel number change, under the constraint Nw + Nh ≤ N"
  - [section 3.2]: "Compared to the AnyRes module that has unbounded cost, the key differences of adaptive N-gridding is it automatically finds the optimal gridding configuration"
  - [corpus]: Weak - No corpus evidence available for this specific adaptive gridding mechanism

### Mechanism 3
- Claim: High-quality training data with human-collected annotations improves model performance
- Mechanism: Using human-collected bounding box annotations and labels instead of model-detected ones provides more accurate spatial information and consistent labeling across platforms, leading to better grounding and referring capabilities
- Core assumption: Human annotations are more accurate and consistent than model-detected annotations
- Evidence anchors:
  - [section 3.1]: "Ferret-UI 2's training dataset predominantly utilizes either human-collected annotations or bounding boxes directly parsed from the source HTML, resulting in a significant improvement in annotation quality"
  - [section 4.2]: "In contrast, GPT-4o struggles with fine-grained UI understanding, as shown by its low referring (56.47) and grounding (12.14) scores in the elementary tasks"
  - [corpus]: Weak - No corpus evidence available for the specific claim about annotation quality impact

## Foundational Learning

- Concept: Multimodal large language models (MLLMs) and their architecture
  - Why needed here: Understanding how MLLMs integrate visual and textual information is crucial for grasping Ferret-UI 2's architecture and how it differs from standard LLMs
  - Quick check question: What are the key components of an MLLM and how do they work together to process multimodal inputs?

- Concept: UI element detection and grounding in computer vision
  - Why needed here: The model's ability to detect, classify, and ground UI elements is fundamental to its functionality across different platforms
  - Quick check question: What are the main challenges in UI element detection compared to general object detection?

- Concept: Visual prompting techniques in multimodal models
  - Why needed here: Understanding set-of-mark visual prompting is essential for grasping how the training data generation works
  - Quick check question: How does visual prompting differ from traditional text prompting in multimodal models?

## Architecture Onboarding

- Component map: Image → CLIP encoder → Adaptive N-gridding → Visual Sampler → LLM → Output
- Critical path: Image → CLIP encoder → Adaptive N-gridding → Visual Sampler → LLM → Output
- Design tradeoffs:
  - Resolution vs. computational cost (controlled by N parameter)
  - Model size vs. performance (different LLM backbones)
  - Training data quality vs. quantity (human vs. model annotations)
- Failure signatures:
  - Poor grounding performance indicates issues with visual encoding or sampler
  - Low accuracy on diverse platforms suggests inadequate multi-platform training
  - Suboptimal resolution handling shows problems with adaptive gridding
- First 3 experiments:
  1. Test adaptive N-gridding with different N values on a single platform to find optimal resolution handling
  2. Compare performance with human vs. model annotations on the same platform to validate annotation quality impact
  3. Evaluate set-of-mark visual prompting quality by comparing generated training data with human-generated examples

## Open Questions the Paper Calls Out

- How does Ferret-UI 2's performance degrade when tested on platforms not included in its training data (e.g., smart watches, car infotainment systems)?
- What is the impact of adaptive N-gridding on inference latency compared to the original AnyRes method, and how does this trade-off affect real-time applications?
- How sensitive is Ferret-UI 2's performance to variations in widget annotation quality across different platforms?

## Limitations

- Reliance on GPT-4o for training data generation introduces potential bias and reproducibility concerns
- Set-of-mark visual prompting mechanism lacks detailed implementation specifications
- Evaluation focuses primarily on performance metrics without extensive ablation studies

## Confidence

- **High confidence**: Multi-platform UI understanding capability, adaptive N-gridding for resolution handling, competitive performance against GPT-4o baseline
- **Medium confidence**: Training data quality improvements from human annotations, set-of-mark visual prompting effectiveness, cross-platform transfer capabilities
- **Low confidence**: GPT-4o generation quality claims, optimal N-gridding configuration justification, relative contribution of each innovation to overall performance

## Next Checks

1. **Ablation study on training data sources**: Compare model performance when trained with human-collected annotations versus model-detected annotations to quantify the claimed quality improvement.

2. **Cross-platform transfer analysis**: Systematically evaluate zero-shot and few-shot performance across all platform combinations to map the actual transfer learning capabilities and identify platform-specific limitations.

3. **Adaptive gridding parameter sensitivity**: Conduct a grid search over N values (2-16) to determine if N=8 is truly optimal or if the performance can be improved by adjusting this parameter based on platform resolution characteristics.