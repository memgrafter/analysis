---
ver: rpa2
title: 'MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark'
arxiv_id: '2409.02813'
source_url: https://arxiv.org/abs/2409.02813
tags:
- answer
- gpt-4o
- options
- setting
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MMMU-Pro addresses limitations in existing multimodal benchmarks
  by filtering out questions answerable by text-only models, augmenting candidate
  options, and introducing a vision-only input setting where questions are embedded
  within images. The benchmark evaluates models' ability to integrate visual and textual
  information, testing a fundamental human cognitive skill.
---

# MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark

## Quick Facts
- **arXiv ID**: 2409.02813
- **Source URL**: https://arxiv.org/abs/2409.02813
- **Reference count**: 19
- **Primary result**: A benchmark that filters out text-only answerable questions and introduces vision-only input settings to evaluate true multimodal reasoning capabilities

## Executive Summary
MMMU-Pro addresses critical limitations in existing multimodal benchmarks by implementing a comprehensive filtering process that removes questions answerable through text-only models. The benchmark introduces innovative evaluation settings, including a vision-only mode where questions are embedded within images, forcing models to integrate visual and textual information. This approach tests a fundamental human cognitive skill - the ability to reason across multiple modalities. Experimental results demonstrate that MMMU-Pro significantly challenges current models, with accuracy drops ranging from 16.8% to 26.9% compared to the original MMMU benchmark, highlighting the need for more sophisticated multimodal reasoning approaches.

## Method Summary
The benchmark development process involves several key innovations. First, a comprehensive filtering mechanism removes questions answerable by text-only models, ensuring that only questions requiring genuine multimodal understanding remain. Second, the candidate options are augmented with both plausible distractors and correct answers from other questions to prevent simple memorization. Third, a novel vision-only input setting is introduced where questions are embedded within images, forcing models to extract and reason about textual information from visual contexts. The benchmark spans four academic disciplines - art and design, business, health and medicine, and science and technology - with 11 subjects and 30 question types, providing comprehensive coverage of real-world multimodal reasoning scenarios.

## Key Results
- MMMU-Pro reduces model performance by 16.8% to 26.9% compared to the original MMMU benchmark across various models
- Chain of Thought prompting improves performance but shows inconsistent effectiveness across different models and domains
- The vision-only setting successfully challenges models' multimodal reasoning capabilities, demonstrating the benchmark's effectiveness in testing true integration of visual and textual information
- GPT-4V and Claude 3.5 Sonnet achieve 78.1% and 82.0% accuracy respectively on MMMU-Pro, compared to 97.9% and 99.9% on MMMU

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its multi-pronged approach to evaluating genuine multimodal reasoning. By filtering out questions answerable through text-only models, MMMU-Pro ensures that models cannot succeed through language understanding alone. The vision-only setting creates a scenario where models must extract textual information from visual contexts and integrate it with the question context, mimicking real-world scenarios where information is naturally distributed across modalities. The augmented candidate options prevent models from relying on simple pattern matching or memorization, forcing them to engage in deeper reasoning processes that combine visual perception with language understanding.

## Foundational Learning
- **Multimodal filtering**: Understanding how to identify and remove questions answerable through single modalities - needed because existing benchmarks contain questions that can be solved through text-only reasoning, undermining true multimodal evaluation; quick check: compare model performance on filtered vs unfiltered questions
- **Vision-only reasoning**: Ability to extract and process textual information embedded in images - needed because real-world scenarios often present text within visual contexts rather than separate inputs; quick check: measure accuracy drop when questions are embedded in images vs presented separately
- **Candidate augmentation**: Creating plausible distractors and cross-question answer sharing - needed to prevent memorization-based approaches and ensure genuine reasoning; quick check: test model performance on augmented vs non-augmented candidate sets

## Architecture Onboarding

**Component map**: Question filtering -> Candidate augmentation -> Vision-only embedding -> Model evaluation

**Critical path**: The evaluation pipeline processes questions through filtering to ensure multimodal necessity, augments candidates to prevent memorization, embeds questions in images for vision-only testing, then evaluates model performance across all settings. This flow ensures that only questions requiring genuine multimodal reasoning reach the final evaluation stage.

**Design tradeoffs**: The benchmark prioritizes evaluation robustness over model accessibility, resulting in significant performance drops but more accurate assessment of true multimodal capabilities. The vision-only setting increases difficulty but may not fully represent natural multimodal scenarios where text and vision are integrated rather than separated.

**Failure signatures**: Models failing on MMMU-Pro typically show either inability to extract textual information from images (vision-only failure) or inability to integrate extracted information with question context (multimodal integration failure). Chain of Thought prompting failures indicate limitations in reasoning processes rather than perception capabilities.

**3 first experiments**:
1. Compare model performance on MMMU-Pro's filtered questions vs original MMMU questions to quantify the impact of filtering
2. Test vision-only setting with and without question text separately visible to isolate visual extraction vs integration challenges
3. Evaluate Chain of Thought effectiveness across different academic domains to identify domain-specific reasoning requirements

## Open Questions the Paper Calls Out
The paper acknowledges several open questions regarding the generalizability of findings to real-world applications, as the benchmark primarily focuses on academic and professional content rather than everyday scenarios. Additionally, the paper raises questions about whether the benchmark could be gamed through memorization of visual-text patterns rather than genuine reasoning, and whether the vision-only setting fully captures the complexity of real-world scenarios where text and visual information are naturally integrated.

## Limitations
- Benchmark focuses on specialized academic domains which may not represent broader multimodal reasoning tasks in everyday scenarios
- Potential for overfitting to MMMU-Pro's specific question formats during model development, limiting generalizability
- Vision-only setting may not fully capture natural multimodal integration where text and visual information are seamlessly combined rather than artificially separated

## Confidence
- **High confidence**: Core claim that MMMU-Pro effectively addresses limitations in existing multimodal benchmarks through filtering and vision-only settings is well-supported by performance degradation data
- **Medium confidence**: Claims about Chain of Thought prompting effectiveness are documented but show variability without full explanation of underlying reasons
- **Low confidence**: Generalizability of findings to real-world applications is limited due to benchmark's focus on academic rather than everyday multimodal scenarios

## Next Checks
1. Conduct ablation studies removing different filtering criteria to quantify their individual impact on benchmark robustness
2. Test whether models fine-tuned on MMMU-Pro show degraded performance on other multimodal benchmarks, indicating potential overfitting
3. Evaluate human performance on the vision-only setting to establish whether it represents a meaningful challenge for human-like reasoning rather than just an artificial difficulty