---
ver: rpa2
title: 'EasyRL4Rec: An Easy-to-use Library for Reinforcement Learning Based Recommender
  Systems'
arxiv_id: '2402.15164'
source_url: https://arxiv.org/abs/2402.15164
tags:
- policy
- learning
- enc-34
- easyrl4rec
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EasyRL4Rec is a comprehensive code library for RL-based recommender
  systems that addresses key challenges in the field. The library provides lightweight
  RL environments based on five public datasets, core modules with rich options, and
  unified evaluation standards focusing on long-term outcomes.
---

# EasyRL4Rec: An Easy-to-use Library for Reinforcement Learning Based Recommender Systems

## Quick Facts
- arXiv ID: 2402.15164
- Source URL: https://arxiv.org/abs/2402.15164
- Authors: Yuanqing Yu; Chongming Gao; Jiawei Chen; Heng Tang; Yuefeng Sun; Qian Chen; Weizhi Ma; Min Zhang
- Reference count: 40
- Key outcome: EasyRL4Rec is a comprehensive code library for RL-based recommender systems that addresses key challenges in the field.

## Executive Summary
EasyRL4Rec is a comprehensive code library designed to simplify the development and experimental processes in reinforcement learning-based recommender systems (RL-based RSs). The library provides lightweight RL environments based on five public datasets, core modules with rich options, and unified evaluation standards focusing on long-term outcomes. It supports both discrete and continuous action-based policies, includes customizable state modeling and action representation, and offers two training paradigms (learning from offline logs and learning with a user model) along with three evaluation settings. Experimental results demonstrate that discrete action-based methods generally outperform continuous ones, on-policy methods achieve better results than off-policy methods, and the choice of state tracker has minimal impact on performance. The library also reveals important issues like preference overestimation in RL-based RSs.

## Method Summary
EasyRL4Rec constructs lightweight RL environments from five public datasets (Coat, YahooR3, MovieLens4, KuaiRec, KuaiRand) with consistent preprocessing. The library employs an MF model trained on test sets to predict missing user-item rewards for offline simulation. It offers four core modules (Environment, Policy, StateTracker, Collector) with interchangeable implementations supporting both discrete and continuous action-based policies. The modular design allows swapping components without breaking the overall architecture. EasyRL4Rec provides five different StateTrackers (Average, GRU, Caser, SASRec, NextItNet) for customizable state modeling. The library supports two training paradigms: learning from offline logs for batch RL methods and learning with user model for model-free RL methods. Evaluation is conducted across three modes (FreeB, NX_0_, NX_X_) with long-term metrics including Cumulative Reward, Average Reward, Interaction Length, Coverage, Diversity, and Novelty.

## Key Results
- Discrete action-based methods generally outperform continuous action-based methods in recommender systems
- On-policy RL methods achieve better results than off-policy methods in the tested scenarios
- The choice of state tracker has minimal impact on overall recommendation performance
- Preference overestimation issue is identified where user models with better predictive accuracy paradoxically lead to lower actual rewards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The library enables reproducible RL-based RS experiments by standardizing environment construction and evaluation protocols.
- Mechanism: EasyRL4Rec builds lightweight RL environments from five public datasets with consistent preprocessing, uses a unified Matrix Factorization-based user model for reward prediction in missing entries, and provides three fixed evaluation modes with long-term metrics.
- Core assumption: Public datasets can be preprocessed into a common RL-ready format, and MF can reliably predict missing user-item rewards for offline simulation.
- Evidence anchors:
  - [abstract] "provides lightweight and diverse RL environments based on five public datasets and includes core modules with rich options, simplifying model development. It provides unified evaluation standards focusing on long-term outcomes"
  - [section] "We choose five public datasets suitable for the RL task to construct environments... we employ an MF model trained on the test set to predict the rating/reward of vacant user-item pairs"

### Mechanism 2
- Claim: The library simplifies model development by providing modular components with rich options that can be easily customized for recommendation scenarios.
- Mechanism: EasyRL4Rec offers four core modules (Environment, Policy, StateTracker, Collector) with interchangeable implementations, supports both discrete and continuous action-based policies with a conversion mechanism, and provides customizable state modeling via five different StateTrackers.
- Core assumption: The modular design allows developers to swap components without breaking the overall architecture, and the conversion mechanism between continuous and discrete actions works reliably.
- Evidence anchors:
  - [abstract] "includes core modules with rich options, simplifying model development. It provides unified evaluation standards focusing on long-term outcomes and offers tailored designs for state modeling and action representation for recommendation scenarios"
  - [section] "EasyRL4Rec includes a mechanism to convert continuous actions to discrete items, allowing for continuous action-based policies. In response to challenges when applying RL algorithms in practical RSs, we have developed customizable modules for state modeling and action representation"

### Mechanism 3
- Claim: The library facilitates insightful experiments by providing comprehensive evaluation metrics and revealing important issues in RL-based RSs.
- Mechanism: EasyRL4Rec provides long-term evaluation metrics alongside traditional RS metrics, supports three evaluation modes with quit mechanisms, and reveals issues like preference overestimation through controlled experiments with different user model training configurations.
- Core assumption: The long-term metrics and evaluation modes can capture important aspects of RL-based RS performance that traditional metrics miss, and the preference overestimation issue can be effectively demonstrated through controlled experiments.
- Evidence anchors:
  - [abstract] "Experimental results show that discrete action-based methods generally outperform continuous ones, on-policy methods achieve better results than off-policy methods, and the choice of state tracker has minimal impact on performance. EasyRL4Rec also reveals issues like preference overestimation"
  - [section] "We introduce a quit mechanism following work [16, 19, 56]. In this setting, users will interrupt the process of interaction and quit when the termination condition is triggered... We evaluate the effectiveness of these models in terms of coverage, diversity, and novelty... Preference Overestimation issue that occurs in RL-based RSs"

## Foundational Learning

- Concept: Reinforcement Learning basics (MDP formulation, policy optimization, exploration-exploitation tradeoff)
  - Why needed here: EasyRL4Rec implements various RL algorithms and requires understanding of how they interact with recommender system environments
  - Quick check question: What are the key components of an MDP and how do they map to recommender system interactions?

- Concept: Recommender system fundamentals (collaborative filtering, evaluation metrics like NDCG, HR)
  - Why needed here: EasyRL4Rec works with recommendation datasets and provides both RL and traditional RS evaluation metrics
  - Quick check question: How do NDCG and HitRate differ from Cumulative Reward in evaluating recommendation performance?

- Concept: Sequential modeling techniques (RNNs, CNNs, self-attention, Transformers)
  - Why needed here: EasyRL4Rec provides multiple StateTrackers that use different sequential modeling approaches
  - Quick check question: What are the key differences between GRU, Caser, SASRec, and NextItNet in modeling user behavior sequences?

## Architecture Onboarding

- Component map:
  - Environment -> StateTracker -> Policy -> Collector -> Buffer -> Trainer/Evaluator
  The data flows from environment through state encoding to policy action selection, with trajectories collected for learning and evaluation.

- Critical path: Environment → StateTracker → Policy → Collector → Buffer → Trainer/Evaluator
  The data flows from environment through state encoding to policy action selection, with trajectories collected for learning and evaluation.

- Design tradeoffs:
  - Using static datasets vs. simulation environments: Static datasets are lighter and faster but may lack diversity compared to simulators
  - Discrete vs. continuous actions: Discrete actions are more natural for recommendations but continuous actions allow using more RL algorithms
  - On-policy vs. off-policy learning: On-policy methods are more sample efficient but off-policy methods can learn from historical data

- Failure signatures:
  - Poor performance across all models: Likely issues with dataset preprocessing or environment construction
  - Large variance in results: Possible problems with random seeds or insufficient training
  - One component consistently underperforms: May indicate issues with that specific module's implementation

- First 3 experiments:
  1. Run a basic DQN experiment on Coat dataset with default settings to verify the environment and basic training pipeline work
  2. Compare PG and A2C on MovieLens dataset to validate the on-policy training and evaluation setup
  3. Test the StateTracker module by running the same policy with different state encoders (GRU, SASRec, Average) on KuaiRec to verify modular design works as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do continuous action-based RL methods perform significantly worse than discrete action-based methods in recommender systems, and under what specific conditions might continuous actions be more effective?
- Basis in paper: [explicit] The paper explicitly states that "discrete action-based methods perform much better than continuous action-based methods" and suggests this might be due to "less efficient exploration strategies or poor fit between the continuous action representation and the discrete item space in RSs."
- Why unresolved: The paper only hypothesizes about potential reasons but doesn't provide definitive evidence or conduct experiments to isolate the specific factors causing this performance gap.
- What evidence would resolve it: Controlled experiments comparing the same RL algorithms with both action representations on datasets with varying characteristics (sparsity, item space size, etc.) while measuring exploration efficiency and representational alignment.

### Open Question 2
- Question: How can the Preference Overestimation issue in RL-based recommender systems be effectively mitigated, and what is the optimal trade-off between prediction accuracy and reward estimation accuracy?
- Basis in paper: [explicit] The paper identifies a "Preference Overestimation issue" where user models with better predictive accuracy (higher negative MSE) paradoxically lead to lower actual rewards, and suggests "increasing the number of negative samples" as a potential mitigation strategy.
- Why unresolved: The paper only demonstrates the existence of the issue and suggests one mitigation approach without comprehensive testing of alternative solutions or quantifying the optimal balance between prediction and reward accuracy.
- What evidence would resolve it: Comparative experiments testing multiple debiasing techniques (negative sampling, conservative Q-learning, etc.) across various user models and datasets while measuring both prediction accuracy and actual reward performance.

### Open Question 3
- Question: What is the impact of different buffer construction methods (Sequential, Convolution, Counterfactual) on the performance of batch RL algorithms, and under what dataset characteristics does each method excel?
- Basis in paper: [explicit] The paper mentions that EasyRL4Rec offers three buffer construction methods and conducts experiments showing "no significant difference" across them, but notes "slight variations in performance metrics."
- Why unresolved: The paper's experiments only test one batch RL algorithm (CRR) on one dataset (KuaiRec), providing insufficient evidence to understand when each construction method might be advantageous.
- What evidence would resolve it: Systematic experiments testing all three buffer construction methods across multiple batch RL algorithms and diverse datasets with varying properties (temporal patterns, data volume, etc.) while measuring downstream recommendation performance.

## Limitations

- The experimental results rely on controlled setups that may not fully capture real-world recommendation scenarios
- The preference overestimation issue requires further investigation into whether it stems from user model architecture or training data distribution
- The claim that state tracker choice has minimal impact needs validation across more diverse recommendation domains

## Confidence

- **High Confidence**: The modular architecture design and basic functionality of EasyRL4Rec are well-supported by the implementation details and code availability
- **Medium Confidence**: The experimental results comparing discrete vs. continuous action methods and on-policy vs. off-policy methods are internally consistent but may not generalize to all recommendation scenarios
- **Medium Confidence**: The identification of preference overestimation as a systematic issue in RL-based RSs is plausible but requires additional validation across different user model architectures

## Next Checks

1. Validate the preference overestimation findings by training user models with varying numbers of negative samples on a completely different dataset (e.g., Last.fm) to confirm it's not dataset-specific
2. Test the state tracker impact claim by conducting experiments on sequential recommendation datasets where user behavior patterns are more complex than the tested datasets
3. Evaluate the continuous-to-discrete action conversion mechanism's efficiency by measuring the performance gap between directly discrete-action policies and converted continuous-action policies across multiple RL algorithms