---
ver: rpa2
title: 'ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal
  Language Models'
arxiv_id: '2412.07012'
source_url: https://arxiv.org/abs/2412.07012
tags:
- data
- instruction
- image
- scene
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PROVISION is a programmatic system that generates vision-centric
  instruction data for multimodal language models (MLMs) using scene graphs and human-written
  programs. Unlike existing approaches that rely on costly LLMs or MLMs prone to hallucinations,
  PROVISION ensures interpretability, controllability, and factual accuracy.
---

# ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models

## Quick Facts
- arXiv ID: 2412.07012
- Source URL: https://arxiv.org/abs/2412.07012
- Reference count: 40
- Key outcome: Programmatically generated vision-centric instruction data improves MLM performance by up to 7-8% on various benchmarks and 1.6% averaged across 11 benchmarks when used in both pre-training and fine-tuning stages.

## Executive Summary
This paper introduces PROVISION, a programmatic system for generating vision-centric instruction data for multimodal language models (MLMs). Unlike existing approaches that rely on costly LLMs or MLMs prone to hallucinations, PROVISION uses scene graphs and human-written programs to ensure interpretability, controllability, and factual accuracy. The system generates diverse single-image and multi-image instruction data covering objects, attributes, relations, depth, and segmentation. When applied to Visual Genome and DataComp datasets, PROVISION produces over 10 million instruction data points. The generated data, when used in instruction tuning, achieves significant improvements across multiple benchmarks including CVBench 2D/3D, QBench2, RealWorldQA, MMMU, and Mantis-Eval.

## Method Summary
PROVISION generates vision-centric instruction data by first creating scene graphs from images using object detection, segmentation, attribute detection, relation detection, and depth estimation models. These scene graphs are then transformed into question-answer pairs using 24 single-image and 14 multi-image instruction generators with predefined templates. The generated data is used to fine-tune MLMs, with the option to incorporate it in both pre-training and fine-tuning stages. The approach ensures factual accuracy by grounding instructions in the structured scene graph representation, eliminating the probabilistic uncertainty of LLM-generated data.

## Key Results
- Up to 7% improvement on CVBench 2D and 8% on CVBench 3D
- 3% improvement on QBench2, RealWorldQA, and MMMU
- 8% improvement on Mantis-Eval
- 1.6% averaged improvement across 11 benchmarks when incorporating data in both pre-training and fine-tuning stages of xGen-MM-4B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Programmatic generation ensures factual accuracy by grounding instruction data in scene graph structure.
- Mechanism: Scene graphs encode explicit object attributes, relations, and spatial information. Programs transform this structured representation into questions with ground truth answers, eliminating the probabilistic uncertainty of LLM/MLM generation.
- Core assumption: Scene graph extraction is accurate; incorrect scene graphs propagate errors.
- Evidence anchors:
  - [abstract]: "Our approach ensures the interpretability and controllability of the data generation process and scales efficiently while maintaining factual accuracy."
  - [section]: "programs produce instructions devoid of hallucinations as long as the underlying scene graphs are accurate."
  - [corpus]: Weak; no direct neighbor papers cited factual accuracy claims.

### Mechanism 2
- Claim: Data scale and format critically influence model performance gains.
- Mechanism: Larger datasets provide more diverse examples for robust learning. Multiple-choice format provides explicit options aiding discrimination, while short-answer format tests generation capability. Half-half mixes benefits.
- Core assumption: Model benefits from increased diversity and varied task formats; format preference depends on task.
- Evidence anchors:
  - [section]: "When adopted in the instruction tuning stage, our single-image instruction data yields up to a 7% improvement... multi-image instruction data leads to an 8% improvement on Mantis-Eval."
  - [section]: "data scale and format are important factors in achieving optimal results."
  - [corpus]: Weak; no neighbor papers directly discuss scale/format impact on MLM performance.

### Mechanism 3
- Claim: Dual-stage augmentation (pre-training + fine-tuning) yields synergistic improvements.
- Mechanism: Pre-training integrates foundational vision-language knowledge; fine-tuning adapts to instruction-following. Adding our data in both stages reinforces learning across both phases.
- Core assumption: Vision-centric instruction data complements general multimodal training; benefits compound when applied early and late.
- Evidence anchors:
  - [section]: "incorporating the data in both pre-training and fine-tuning stages of xGen-MM-4B leads to an averaged improvement of 1.6% across 11 benchmarks."
  - [section]: "incorporating them in both stages achieves the best performance."
  - [corpus]: Weak; no neighbor papers explicitly validate dual-stage augmentation for MLMs.

## Foundational Learning

- Concept: Scene graphs as symbolic representations of images.
  - Why needed here: They provide the structured, explicit semantic backbone for programmatic instruction generation.
  - Quick check question: What are the three core components of a scene graph?
    - Answer: Nodes (objects), edges (relations), and node attributes (object properties).

- Concept: Programmatic data synthesis vs. generative model synthesis.
  - Why needed here: Understanding deterministic vs. probabilistic generation methods clarifies why this approach avoids hallucinations.
  - Quick check question: What is the key difference between programmatic and LLM-generated instruction data?
    - Answer: Programmatic data is derived from structured inputs (scene graphs) with deterministic outputs; LLM-generated data is probabilistic and prone to hallucinations.

- Concept: Multimodal instruction tuning.
  - Why needed here: Knowing how vision-centric instruction data fits into multimodal model training is essential for implementation.
  - Quick check question: What is the primary goal of multimodal instruction tuning?
    - Answer: To enable models to follow instructions involving both text and visual inputs.

## Architecture Onboarding

- Component map: Scene Graph Generation Pipeline -> Object Detection, Segmentation, Attribute Detection, Relation Detection, Depth Estimation -> Instruction Data Generators (24 single-image, 14 multi-image) -> Dataset Assembly (PROVISION-10M) -> Model Training (Pre-training, Fine-tuning, or both) -> Evaluation on Benchmarks

- Critical path:
  1. Input image -> Scene graph generation (object, attribute, relation, depth, segmentation)
  2. Scene graph -> Programmatic question-answer pair generation
  3. Generated data -> Model training (pre-training/fine-tuning)
  4. Trained model -> Evaluation on benchmarks

- Design tradeoffs:
  - Accuracy vs. scalability: Human-annotated scene graphs (VG) are more accurate but smaller; model-generated (DC) are scalable but noisier
  - Format choice: Multiple-choice vs. short-answer vs. half-half affects model adaptability and performance
  - Data source: Using proprietary vs. open models for scene graph components impacts licensing and reproducibility

- Failure signatures:
  - Low performance gains: Scene graph extraction errors, insufficient data scale, format mismatch
  - Model instability: Overfitting to generated data, imbalanced training sets
  - Licensing issues: Using proprietary models without proper attribution or permissions

- First 3 experiments:
  1. Validate scene graph accuracy: Compare generated scene graphs against ground truth on a subset of VG images
  2. Ablate data scale: Train models with 5%, 20%, 50% replacement ratios and measure performance
  3. Compare formats: Train identical models with all multiple-choice, all short-answer, and half-half formats and evaluate on task-specific benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of model-generated scene graphs compare to human-annotated scene graphs in terms of downstream MLM performance, and at what data scale does the gap close?
- Basis in paper: [explicit] The paper shows that DC-S (model-generated) underperforms VG-S (human-annotated) at lower data scales but achieves comparable performance at a 50% replacement ratio, suggesting scale-dependent convergence.
- Why unresolved: The paper does not provide a detailed analysis of the quality gap between model-generated and human-annotated scene graphs or identify the exact inflection point where model-generated data becomes equally effective.
- What evidence would resolve it: A systematic ablation study comparing scene graph quality metrics (e.g., object detection accuracy, relation prediction F1) against MLM performance across varying data scales would clarify the relationship.

### Open Question 2
- Question: Does the format of instruction data (short answer vs. multiple choice) interact with the type of MLM architecture or training stage (pre-training vs. fine-tuning) in affecting model performance?
- Basis in paper: [explicit] The paper observes that multiple choice formats tend to yield higher performance at certain data scales and that dual-stage augmentation (pre-training + fine-tuning) synergistically improves results.
- Why unresolved: The paper does not explore whether these effects are consistent across different MLM architectures or whether certain formats are more beneficial at specific training stages.
- What evidence would resolve it: Cross-architecture experiments varying data format and training stage combinations would reveal whether these effects are universal or architecture-dependent.

### Open Question 3
- Question: What is the impact of increasing the diversity of instruction generators on model generalization, and how does this compare to simply increasing data volume?
- Basis in paper: [explicit] The paper implements 24 single-image and 14 multi-image generators but does not analyze the marginal benefit of adding new generator types versus scaling existing ones.
- Why unresolved: The paper focuses on data volume and format but does not isolate the effect of generator diversity on model performance.
- What evidence would resolve it: Controlled experiments comparing models trained on varied generator sets at fixed data volumes would quantify the contribution of diversity versus scale.

## Limitations

- Scene graph quality directly impacts factual accuracy, yet limited quantitative analysis of scene graph error rates or their propagation into generated instructions
- Superiority over LLM-generated alternatives demonstrated primarily through downstream MLM performance, not direct comparison of data quality metrics or hallucination rates
- 1.6% average improvement on xGen-MM-4B, while statistically meaningful, represents a relatively modest gain that may not justify engineering complexity for all applications

## Confidence

- **High Confidence**: The programmatic generation approach eliminates hallucinations when scene graphs are accurate. This is logically sound and directly observable in the generation process.
- **Medium Confidence**: Data scale and format significantly influence model performance. The experimental results support this, but the analysis lacks deeper investigation into optimal ratios and task-specific format preferences.
- **Medium Confidence**: Dual-stage augmentation yields synergistic improvements. The 1.6% gain is demonstrated, but the mechanism for why pre-training and fine-tuning benefits compound is not thoroughly explored.

## Next Checks

1. **Scene Graph Error Analysis**: Conduct a systematic evaluation of scene graph accuracy on a held-out test set, measuring how extraction errors propagate into instruction quality and MLM performance degradation.
2. **Format Optimization Study**: Perform a comprehensive ablation study across different format ratios (e.g., 25%/50%/75% multiple-choice) for each benchmark to identify optimal configurations and understand format-task relationships.
3. **Direct Data Quality Comparison**: Implement a controlled experiment comparing programmatic vs. LLM-generated instruction data on the same MLM architecture, measuring not just downstream performance but also instruction complexity, diversity, and hallucination rates.