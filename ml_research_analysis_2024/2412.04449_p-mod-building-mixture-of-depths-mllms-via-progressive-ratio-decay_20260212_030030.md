---
ver: rpa2
title: 'p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay'
arxiv_id: '2412.04449'
source_url: https://arxiv.org/abs/2412.04449
tags:
- arxiv
- token
- tokens
- vision
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes p-MoD, an efficient multimodal large language
  model (MLLM) architecture that reduces training and inference costs by selectively
  processing vision tokens. The method builds on the Mixture-of-Depths (MoD) mechanism,
  where each layer dynamically selects important vision tokens to process while skipping
  redundant ones.
---

# p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay

## Quick Facts
- arXiv ID: 2412.04449
- Source URL: https://arxiv.org/abs/2412.04449
- Reference count: 40
- Primary result: p-MoD achieves 55.6% TFLOPs and 53.7% KV cache savings while matching or exceeding LLaVA baselines

## Executive Summary
This paper introduces p-MoD, a method to build efficient multimodal large language models (MLLMs) using a progressive Mixture-of-Depths (MoD) approach. The key innovation is selectively processing vision tokens based on their importance, which reduces both training and inference costs. The method introduces three main technical contributions: Tanh-gated weight normalization for training stability, symmetric token reweighting to leverage language supervision with limited multimodal data, and progressive ratio decay that gradually reduces token retention across deeper layers. Extensive experiments on LLaVA-v1.5 and LLaVA-NeXT demonstrate that p-MoD matches or exceeds baseline performance on 15 benchmarks while requiring only 55.6% of the computation and 53.7% of the KV cache storage during inference.

## Method Summary
p-MoD extends the Mixture-of-Depths concept to MLLMs by introducing a dynamic routing mechanism that selectively processes vision tokens at each layer. The method addresses three key challenges: training instability in early stages through Tanh-gated weight normalization, limited multimodal supervision by leveraging language supervision signals via symmetric token reweighting, and suboptimal token selection in deeper layers through progressive ratio decay. The progressive ratio decay follows a shifted cosine schedule that gradually reduces the token retention ratio from an initial value to a final value across layers. This approach enables efficient processing of vision tokens while maintaining model performance, achieving significant computational savings during both training and inference.

## Key Results
- Achieves 55.6% reduction in TFLOPs and 53.7% reduction in KV cache storage during inference
- Matches or exceeds baseline performance on 15 vision-language benchmarks
- Requires only 77.7% of GPU hours during training compared to full MoE baselines

## Why This Works (Mechanism)
p-MoD works by recognizing that not all vision tokens contribute equally to downstream tasks, and deeper layers can progressively filter out less important information. The method leverages the observation that language supervision signals are more abundant than multimodal supervision, allowing the model to learn efficient token selection strategies. By gradually reducing the token retention ratio across layers following a shifted cosine schedule, the model can focus computational resources on the most informative tokens while maintaining performance. The Tanh-gated weight normalization stabilizes training by preventing extreme gating values early in training, while symmetric token reweighting allows the model to benefit from the richer language supervision available during pre-training.

## Foundational Learning

**Mixture-of-Depths (MoD)**: A routing mechanism that selectively processes tokens based on their importance. Needed to reduce computational cost by skipping redundant tokens. Quick check: Verify that routing decisions correlate with token importance through ablation studies.

**Tanh-gated weight normalization**: A normalization technique that stabilizes training by bounding gating values. Needed to prevent training instability in early stages when gating decisions are uncertain. Quick check: Compare training stability with and without Tanh gating on convergence curves.

**Symmetric token reweighting**: A method to leverage abundant language supervision when multimodal supervision is limited. Needed because multimodal datasets are typically much smaller than language datasets. Quick check: Measure performance improvement when increasing language supervision relative to multimodal supervision.

**Shifted cosine decay schedule**: A progressive reduction schedule for token retention ratio. Needed to gradually shift computational focus from shallow to deep layers. Quick check: Verify that cosine scheduling outperforms linear or step decay in ablation studies.

## Architecture Onboarding

**Component map**: Input tokens -> Token importance scoring -> Gating layer (Tanh-gated) -> Token routing decision -> Processing layer -> Output tokens

**Critical path**: The gating mechanism and routing decision represent the critical path, as they determine which tokens are processed and directly impact computational efficiency.

**Design tradeoffs**: The method trades off between computational efficiency and potential information loss from skipping tokens. The progressive decay schedule balances early token processing with deeper layer filtering.

**Failure signatures**: Training instability in early stages, performance degradation from over-aggressive token skipping, and suboptimal routing decisions that fail to identify important tokens.

**First experiments**: 1) Ablation study on token retention ratios, 2) Comparison of different decay schedules (cosine vs linear), 3) Evaluation of training stability with and without Tanh gating

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the abstract or main text. However, the method's scalability to larger models and its performance on diverse task distributions remain implicit areas for future investigation.

## Limitations
- Evaluation limited to vision-language tasks without assessment on pure language or vision-only benchmarks
- Performance gains demonstrated primarily on LLaVA-v1.5 and LLaVA-NeXT, leaving scalability questions open
- Progressive ratio decay introduces an additional hyperparameter requiring careful tuning for different model scales

## Confidence
- Computational efficiency gains (55.6% TFLOPs, 53.7% KV cache): High
- Training stability improvements from Tanh-gated weight normalization: Medium
- Symmetric token reweighting mechanism effectiveness: Medium
- Progressive ratio decay schedule contribution: Medium

## Next Checks
1. Evaluate p-MoD on pure language modeling and pure vision benchmarks to assess modality-agnostic efficiency gains
2. Test robustness and performance under noisy or out-of-distribution inputs across diverse vision-language tasks
3. Conduct scaling studies to determine how p-MoD performs as model size increases, including multi-round dialogue and long-context evaluations