---
ver: rpa2
title: Offline Reinforcement Learning from Datasets with Structured Non-Stationarity
arxiv_id: '2405.14114'
source_url: https://arxiv.org/abs/2405.14114
tags:
- policy
- learning
- reward
- offline
- setting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new offline reinforcement learning problem
  where the reward and transition functions evolve between episodes but remain constant
  within episodes. The authors propose a method called Contrastive Predictive Non-Stationarity
  Adaptation (COSPA) that uses Contrastive Predictive Coding (CPC) to infer the hidden
  parameter (HiP) representing the non-stationarity from the dataset, predict it during
  evaluation, and train a policy conditioned on it.
---

# Offline Reinforcement Learning from Datasets with Structured Non-Stationarity

## Quick Facts
- arXiv ID: 2405.14114
- Source URL: https://arxiv.org/abs/2405.14114
- Reference count: 40
- Authors: Johannes Ackermann, Takayuki Osa, Masashi Sugiyama
- Key outcome: A new offline RL problem with evolving reward/transition functions, solved by COSPA using CPC to infer and predict a hidden parameter, achieving oracle-level performance on continuous control tasks.

## Executive Summary
This paper addresses a new offline reinforcement learning problem where reward and transition functions evolve between episodes but remain constant within episodes. The authors propose Contrastive Predictive Non-Stationarity Adaptation (COSPA), which uses Contrastive Predictive Coding to infer a hidden parameter (HiP) representing the non-stationarity from the dataset, predict it during evaluation, and train a policy conditioned on it. COSPA learns a representation of the HiP using CPC and a separate predictor network to infer the HiP during evaluation. The method is evaluated on various continuous control tasks with changing reward or transition functions, including high-dimensional locomotion tasks. Results show that COSPA often achieves oracle-level performance and outperforms baseline methods like BOReL, ContraBAR, and VRNN.

## Method Summary
The method introduces Contrastive Predictive Non-Stationarity Adaptation (COSPA) for offline RL with structured non-stationarity. COSPA uses Contrastive Predictive Coding (CPC) to infer a hidden parameter (HiP) from past trajectories that represents evolving reward and transition functions. A separate predictor network trained on sequences of inferred latents predicts the next HiP during evaluation. The offline dataset is augmented with inferred HiPs, and a policy is trained using TD3+BC with behavior cloning penalty, conditioning on the HiP. During evaluation, the predictor infers the current HiP from context trajectories, and the policy uses this information to adapt its behavior.

## Key Results
- COSPA achieves oracle-level performance on continuous control tasks with changing reward/transition functions
- Outperforms baseline methods including BOReL, ContraBAR, and VRNN
- Successfully handles high-dimensional locomotion tasks with varying transition functions
- Learned representations are meaningful and allow accurate HiP prediction during evaluation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Contrastive Predictive Coding can infer the hidden parameter (HiP) from past trajectories and predict the next HiP without requiring reward relabeling or policy replaying.
- **Mechanism**: CPC learns a representation of past trajectories by encoding them separately and combining the encodings using an autoregressive model. A classifier is then trained to distinguish future trajectories from the same deployment against those from different deployments. This allows the model to learn a representation of the HiP that can be used to infer and predict it during evaluation.
- **Core assumption**: The HiP remains constant within each episode but evolves between episodes, allowing past trajectories to provide information about the current HiP.
- **Evidence anchors**:
  - [abstract]: "We propose a method based on Contrastive Predictive Coding that identifies this non-stationarity in the offline dataset, accounts for it when training a policy, and predicts it during evaluation."
  - [section]: "As we learn to discriminate future trajectoriesτ+
i+k, instead of future transitions (s+
t+k,r +
t+k) as in ContraBAR, the model can not simply learn the transition function p(st+k|st) but has to learn a representation of the HiP to discriminateτi+k."
  - [corpus]: Weak evidence. The corpus does not directly address the use of CPC for HiP inference, but it does mention related work on offline RL and non-stationary environments.
- **Break condition**: If the HiP changes within an episode or if the relationship between past trajectories and the current HiP is not predictable, the CPC-based inference will fail.

### Mechanism 2
- **Claim**: TD3+BC with behavior cloning penalty can effectively learn a policy conditioned on the inferred HiP, allowing for large deviations from the behavior policy when necessary.
- **Mechanism**: The behavior cloning penalty in TD3+BC constrains the learned policy to remain close to the behavior policy, but the strength of this constraint can be adjusted by varying the hyperparameter λ. This allows the policy to deviate more from the behavior policy when the inferred HiP indicates a need for it.
- **Core assumption**: The behavior policy is not specialized to any HiP but close to optimal on the "marginal" MDP, allowing the learned policy to improve upon it when conditioned on the inferred HiP.
- **Evidence anchors**:
  - [abstract]: "We show that our method often achieves the oracle performance and performs better than baselines."
  - [section]: "TD3+BC uses a deterministic policyµϕ(s) and conservativity is achieved by a BC termE(s,a)∼D
[(µϕ(s)−a)2]. In our setting we extend this toE(s,a,˜z)∼ˆD
[(µϕ(s, ˜z)−a)2]."
  - [corpus]: Weak evidence. The corpus does not directly address the use of TD3+BC with behavior cloning penalty for policy learning, but it does mention related work on offline RL and policy learning.
- **Break condition**: If the behavior policy is already optimal for all HiPs or if the inferred HiP is not accurate, the policy learning will not improve upon the behavior policy.

### Mechanism 3
- **Claim**: The learned representation of the HiP is meaningful and allows for accurate prediction of the HiP during evaluation.
- **Mechanism**: The CPC-based inference learns a representation of the HiP that captures the underlying structure of the task. This representation can be used to predict the next HiP during evaluation by training a separate prediction network on sequences of inferred latents.
- **Core assumption**: The HiP can be represented in a low-dimensional space and the relationship between past trajectories and the next HiP is predictable.
- **Evidence anchors**:
  - [abstract]: "We show that our method often achieves the oracle performance and performs better than baselines."
  - [section]: "The results are shown in Fig. 5, and we can see a good correspondence between the inferred and predicted latents. The predicted latents are more concentrated than the inferred latents, which can be explained by the denoising properties of the regression loss."
  - [corpus]: Weak evidence. The corpus does not directly address the meaningfulness of the learned representation or the accuracy of HiP prediction, but it does mention related work on representation learning and prediction in RL.
- **Break condition**: If the HiP cannot be represented in a low-dimensional space or if the relationship between past trajectories and the next HiP is not predictable, the representation learning and prediction will fail.

## Foundational Learning

- **Concept**: Partially-Observable MDPs (POMDPs)
  - **Why needed here**: The non-stationarity in the reward and transition functions can be represented as a POMDP, where the true state includes an unobserved hidden parameter.
  - **Quick check question**: Can you explain how a POMDP extends the MDP formulation to account for unobserved states?

- **Concept**: Contrastive Predictive Coding (CPC)
  - **Why needed here**: CPC is used to learn a representation of the HiP from past trajectories by contrasting future trajectories from the same deployment against those from different deployments.
  - **Quick check question**: How does the InfoNCE loss in CPC maximize the mutual information between the context and future observations?

- **Concept**: Dynamic-Parameter MDPs (DP-MDPs)
  - **Why needed here**: The problem setting is formulated as multiple rollouts of a DP-MDP, where the HiP evolves between episodes but remains constant within each episode.
  - **Quick check question**: Can you describe the difference between a DP-MDP and a standard MDP in terms of the hidden parameter?

## Architecture Onboarding

- **Component map**: CPC-based inference (Encoder -> Autoregressive Model -> Classifier) -> Prediction Network (RNN) -> Policy Learning (TD3+BC with BC penalty) -> Dataset Augmentation (Relabeling with inferred HiPs)

- **Critical path**:
  1. Train CPC-based inference on offline dataset to learn representation of HiP
  2. Train prediction network on inferred latents to predict next HiP during evaluation
  3. Augment offline dataset with inferred HiPs and train policy using TD3+BC
  4. During evaluation, use prediction network to infer HiP and condition policy on it

- **Design tradeoffs**:
  - Low-dimensional vs. high-dimensional HiP representation
  - Separate vs. joint training of inference and policy learning
  - Strength of behavior cloning penalty in TD3+BC

- **Failure signatures**:
  - Poor performance on tasks with changing transition functions
  - Inability to accurately infer or predict HiP during evaluation
  - Policy performance worse than blind baseline

- **First 3 experiments**:
  1. Evaluate learned representation of HiP using linear probes and T-SNE visualization
  2. Test accuracy of HiP prediction during evaluation using context of previous trajectories
  3. Compare policy performance with and without HiP conditioning on simple continuous control tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is COSPA's performance to the dimensionality of the latent variable z?
- Basis in paper: [inferred] The paper varies latent dimensionality {2, 4, 6, 8} in grid search but doesn't report sensitivity analysis results.
- Why unresolved: The paper mentions tuning latent dimensionality but doesn't analyze how performance changes with different dimensions.
- What evidence would resolve it: Performance curves showing how reward varies with latent dimensionality across tasks.

### Open Question 2
- Question: How does COSPA perform when the hidden parameter changes within episodes rather than only between episodes?
- Basis in paper: [explicit] The paper explicitly states "our setting requires large deviations from the behavior policy due to the difference in transition and reward functions" and focuses on changes between episodes.
- Why unresolved: The paper assumes stationarity within episodes but doesn't test the method's robustness when this assumption is violated.
- What evidence would resolve it: Results comparing COSPA's performance when z changes within episodes versus only between episodes.

### Open Question 3
- Question: What is the sample efficiency of COSPA compared to baseline methods during representation learning?
- Basis in paper: [inferred] The paper mentions "efficient training" as a design goal but doesn't report wall-clock time or number of samples needed for representation learning convergence.
- Why unresolved: The paper focuses on final performance but doesn't analyze the learning dynamics or computational requirements.
- What evidence would resolve it: Training curves showing representation learning progress and computational requirements for COSPA versus baselines.

## Limitations
- Limited evaluation on complex, high-dimensional environments beyond continuous control tasks
- Assumes HiP remains constant within episodes, which may not hold in all real-world scenarios
- Sensitive to hyperparameter choices, particularly the behavior cloning penalty strength in TD3+BC

## Confidence
- Mechanism 1: Medium - CPC-based inference is well-established but its application to HiP inference in this context requires further validation
- Mechanism 2: Medium - TD3+BC with behavior cloning penalty is a reasonable choice but the tradeoff between conservatism and adaptability needs careful tuning
- Mechanism 3: Medium - The learned representations and HiP prediction show promise but more quantitative analysis would strengthen the claims

## Next Checks
1. Evaluate COSPA on a more diverse set of tasks with varying levels of non-stationarity, including high-dimensional, pixel-based environments.
2. Conduct a thorough sensitivity analysis of the method's performance to changes in the behavior cloning penalty strength and other key hyperparameters.
3. Investigate the robustness of COSPA to violations of the assumed non-stationarity structure, such as abrupt changes in the HiP or non-smooth evolution between episodes.