---
ver: rpa2
title: 'COTET: Cross-view Optimal Transport for Knowledge Graph Entity Typing'
arxiv_id: '2405.13602'
source_url: https://arxiv.org/abs/2405.13602
tags:
- entity
- type
- different
- knowledge
- types
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Cross-view Optimal Transport for Knowledge\
  \ Graph Entity Typing (COTET), which aims to improve entity type prediction by incorporating\
  \ both fine-grained type information and coarse-grained cluster knowledge. The key\
  \ innovation is modeling three perspectives\u2014entity-type, entity-cluster, and\
  \ type-cluster-type views\u2014and using optimal transport to align embeddings from\
  \ these heterogeneous spaces."
---

# COTET: Cross-view Optimal Transport for Knowledge Graph Entity Typing

## Quick Facts
- arXiv ID: 2405.13602
- Source URL: https://arxiv.org/abs/2405.13602
- Authors: Zhiwei Hu; Víctor Gutiérrez-Basulto; Zhiliang Xiang; Ru Li; Jeff Z. Pan
- Reference count: 40
- Primary result: Achieves up to 3.4% improvement in MRR over state-of-the-art baselines

## Executive Summary
COTET introduces a novel approach to knowledge graph entity typing that leverages cross-view optimal transport to align embeddings from heterogeneous spaces. The method models three complementary perspectives—entity-type, entity-cluster, and type-cluster-type views—and uses optimal transport to minimize distributional discrepancies between them. By incorporating a mixture pooling strategy and a Beta distribution-based loss function, COTET effectively addresses the challenges of sparse neighbor connections and false negatives in knowledge graph completion tasks.

## Method Summary
COTET operates by first generating three distinct views of a knowledge graph: entity-type, entity-cluster, and type-cluster-type. Each view is encoded using appropriate graph neural networks (LightGCN for single-relational views and CompGCN for multi-relational views). The method then employs optimal transport to align embeddings from these heterogeneous spaces into a unified representation. For entity typing prediction, COTET uses a mixture pooling mechanism that combines multi-head weight pooling, max pooling, and average pooling to aggregate scores from diverse neighbors. The training process incorporates a Beta distribution-based cross-entropy loss that mitigates false negatives by downweighting easy negative samples.

## Key Results
- Achieves up to 3.4% improvement in MRR over state-of-the-art baselines on FB15kET and YAGO43kET datasets
- Demonstrates strong robustness to sparse neighbor connections, maintaining performance even with 80% neighbor dropping
- Particularly effective at predicting infrequent (hard) entity types with up to 5.7% improvement in H@1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-view optimal transport aligns embeddings from heterogeneous view spaces by minimizing Wasserstein distance.
- Mechanism: COTET models entity, type, and cluster embeddings from entity-type, entity-cluster, and type-cluster-type views as probability distributions. It then uses optimal transport to compute a transport matrix that maps source embeddings to destination embeddings, reducing distributional discrepancy.
- Core assumption: Different views provide complementary information about entities, and their embeddings can be meaningfully aligned through Wasserstein distance minimization.
- Evidence anchors:
  - [abstract] "transport view-specific embeddings to a unified space by minimizing the Wasserstein distance from a distributional alignment perspective"
  - [section 4.2] "The primary objective is to minimize the disparity between the source embeddings and the destination embeddings, ensuring their alignment in a shared space."
- Break condition: If embeddings from different views are too dissimilar or semantically incompatible, optimal transport alignment may fail or produce misleading results.

### Mechanism 2
- Claim: Mixture pooling aggregates prediction scores from diverse neighbors more effectively than single pooling methods.
- Mechanism: COTET employs a mixture pooling module combining multi-head weight pooling, max pooling, and average pooling to aggregate neighbor inference results. Multi-head weight pooling uses learned weights to emphasize important neighbors while suppressing noise.
- Core assumption: Different neighbors contribute varying amounts of information to entity typing prediction, and a weighted combination captures this variability better than uniform aggregation.
- Evidence anchors:
  - [abstract] "employing a mixture pooling mechanism to aggregate prediction scores from diverse neighbors of an entity"
  - [section 4.3] "Different neighbors will make their respective predictions, and then we need to adequately combine these prediction scores to form the final result."
- Break condition: If neighbor predictions are highly inconsistent or noisy, weighted pooling may amplify errors rather than improve accuracy.

### Mechanism 3
- Claim: Beta distribution-based cross-entropy loss reduces false negatives by downweighting easy negative samples.
- Mechanism: The BDCE loss assigns lower weights to negative samples with very high or very low prediction scores, focusing training on challenging false negatives that are more likely to be actual missing types.
- Core assumption: The incompleteness of knowledge graphs means many "negative" samples are actually missing positive assertions, and these hard negatives are more valuable for training.
- Evidence anchors:
  - [abstract] "introduce a distribution-based loss function to mitigate the occurrence of false negatives during training"
  - [section 4.3] "Negative samples with higher scores are more likely to be false negatives, while negative samples with lower scores are considered as easier samples to learn."
- Break condition: If the Beta distribution hyperparameters are poorly tuned, the loss may fail to properly distinguish between true negatives and false negatives.

## Foundational Learning

- Concept: Optimal transport and Wasserstein distance
  - Why needed here: COTET uses optimal transport to align embeddings from different heterogeneous views into a unified space
  - Quick check question: What is the mathematical definition of Wasserstein distance between two probability distributions?

- Concept: Graph neural networks and their variants
  - Why needed here: COTET employs LightGCN for single-relational views and CompGCN for multi-relational views to encode different perspectives
  - Quick check question: How does LightGCN differ from traditional GCN in terms of message passing and aggregation?

- Concept: Beta distribution and its probability density function
  - Why needed here: COTET uses a Beta distribution to weight negative samples in the loss function, addressing false negatives
  - Quick check question: How do the α and β parameters of a Beta distribution affect its shape and behavior?

## Architecture Onboarding

- Component map: Multi-view Generation and Encoder -> Cross-view Optimal Transport -> Pooling-based Entity Typing Prediction -> Beta Distribution-based Loss
- Critical path: Input KG -> Multi-view Generation -> Encoding (3 views) -> Optimal Transport Alignment -> Neighbor Prediction -> Mixture Pooling -> Final Prediction
- Design tradeoffs:
  - Using multiple views increases representational power but adds computational complexity
  - Optimal transport alignment provides principled embedding fusion but requires solving transport problems
  - Mixture pooling captures neighbor diversity but adds hyperparameters to tune
- Failure signatures:
  - Poor performance across all metrics suggests fundamental architectural issues
  - Good performance on easy types but poor on hard types indicates need for better handling of sparse data
  - Performance degradation with increased neighbor dropping rates suggests over-reliance on specific neighbors
- First 3 experiments:
  1. Test optimal transport alignment by swapping source and destination embeddings to confirm directional dependency
  2. Evaluate individual view contributions by removing each view and measuring performance impact
  3. Test mixture pooling effectiveness by comparing against single pooling strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would COTET perform on knowledge graphs with significantly more diverse and hierarchical type structures than FB15kET and YAGO43kET?
- Basis in paper: [explicit] The paper mentions that future work could explore "the more challenging inductive KGET task" and notes that "entities in the FB15k and YAGO datasets contain textual description knowledge" which could be incorporated
- Why unresolved: Current experiments only tested on two datasets with relatively limited type diversity. The model's ability to generalize to more complex hierarchical structures and incorporate additional information sources remains unexplored.
- What evidence would resolve it: Performance results on knowledge graphs with deeper type hierarchies, more diverse type distributions, and integration of textual descriptions compared to current baselines.

### Open Question 2
- Question: What is the optimal strategy for determining the source and destination roles in the cross-view optimal transport mechanism for different types of knowledge graph structures?
- Basis in paper: [explicit] The paper notes that "swapping source and destination embeddings in the cross-view optimal transport module affects the magnitude of improvement" and that "in practice they cannot be swapped" without careful consideration
- Why unresolved: While the paper provides some intuition about why certain views should be sources or destinations, it doesn't provide a systematic approach for determining these roles across different knowledge graph structures or domains.
- What evidence would resolve it: A theoretical framework or empirical study showing how to determine optimal source/destination assignments based on graph characteristics like density, clustering coefficients, or relation types.

### Open Question 3
- Question: How does the Beta distribution-based cross-entropy loss compare to other advanced loss functions designed for handling class imbalance and false negatives in knowledge graph entity typing?
- Basis in paper: [explicit] The paper introduces a Beta distribution-based cross-entropy loss to address false negatives and mentions that "any probability distribution function with a parabolic shape opening downwards between 0 and 1 can be used"
- Why unresolved: The paper only compares the Beta distribution to three other specific distributions (Cauchy, Gumbel, Laplace) but doesn't compare against other state-of-the-art loss functions specifically designed for class imbalance problems.
- What evidence would resolve it: Head-to-head comparisons between BDCE and other specialized loss functions like focal loss, weighted cross-entropy, or margin-based losses on datasets with varying levels of type imbalance and false negative rates.

## Limitations
- The paper lacks detailed analysis of optimal transport computational complexity, particularly for large-scale knowledge graphs
- The effectiveness of the Beta distribution-based loss depends heavily on hyperparameter tuning (α and β), but the paper provides limited guidance on selection criteria
- The mixture pooling mechanism introduces additional hyperparameters without systematic ablation studies to justify their values

## Confidence
- Cross-view alignment through optimal transport: High confidence - well-supported by theoretical foundations and experimental results
- Mixture pooling effectiveness: Medium confidence - supported by experiments but lacks detailed hyperparameter analysis
- Beta distribution loss for false negative mitigation: Medium confidence - conceptually sound but limited ablation studies

## Next Checks
1. Conduct scalability analysis measuring optimal transport computation time as graph size increases, particularly for dense neighbor relationships
2. Perform ablation studies systematically removing each view (entity-type, entity-cluster, type-cluster-type) to quantify individual contributions
3. Test the model's robustness across diverse knowledge graph domains beyond FB15kET and YAGO43kET to validate generalizability claims