---
ver: rpa2
title: 'Federated Fairness Analytics: Quantifying Fairness in Federated Learning'
arxiv_id: '2408.08214'
source_url: https://arxiv.org/abs/2408.08214
tags:
- fairness
- learning
- data
- federated
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the problem of quantifying fairness in federated
  learning systems, which inherit fairness challenges from classical ML and introduce
  new ones due to data heterogeneity, client participation, and communication constraints.
  The authors propose Federated Fairness Analytics, a methodology for measuring fairness
  using four symptomatically defined notions: individual fairness (performance proportional
  to contribution), protected group fairness (equivalence across sensitive attributes),
  incentive fairness (proportional reward to contribution), and orchestrator fairness
  (server success in maximizing objective function).'
---

# Federated Fairness Analytics: Quantifying Fairness in Federated Learning

## Quick Facts
- **arXiv ID**: 2408.08214
- **Source URL**: https://arxiv.org/abs/2408.08214
- **Reference count**: 40
- **Primary result**: Federated Fairness Analytics quantifies fairness in FL systems across four dimensions using symptomatically defined metrics, showing statistical heterogeneity and client participation rates significantly impact fairness.

## Executive Summary
This work addresses the critical challenge of quantifying fairness in federated learning systems, which inherit fairness issues from classical ML while introducing new challenges due to data heterogeneity, client participation constraints, and communication limitations. The authors propose Federated Fairness Analytics, a methodology that defines four symptomatically measured notions of fairness (individual, protected group, incentive, and orchestrator) with novel metrics including Jain's Fairness Index and federated Shapley values. Experiments across 24 different settings demonstrate that statistical heterogeneity and client participation rates are the primary drivers of fairness issues, with fairness-conscious approaches like q-FedAvg and Ditto only marginally improving fairness-performance trade-offs.

## Method Summary
The method employs a simulation-based approach using the Flower framework to train federated learning models across three datasets (CIFAR-10, FEMNIST, NSL-KDD) under 24 experimental conditions varying FL approaches (FedAvg, q-FedAvg, Ditto), data settings (iid, non-iid), and client counts. After each training round, fairness metrics are calculated including Jain's Fairness Index for individual and incentive fairness, equalised odds difference for group fairness, and mean accuracy for orchestrator fairness. Federated Shapley values quantify client contributions to enable fairness measurement proportional to actual impact rather than simple parity.

## Key Results
- Statistical heterogeneity and client participation rates are the primary drivers of fairness issues in federated learning
- Fairness-conscious approaches (q-FedAvg, Ditto) only marginally improve fairness-performance trade-offs compared to standard FedAvg
- Jain's Fairness Index provides a bounded, normalized measure of uniformity suitable for fairness quantification across heterogeneous client contributions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Quantifying fairness via four symptom-based notions enables granular measurement of unfairness without requiring knowledge of system design.
- **Mechanism**: Each fairness notion (individual, protected group, incentive, orchestrator) is defined by a measurable symptom (e.g., uniformity of gain, equalised odds difference, uniformity of reward/contribution ratio, average normalized client performance) that directly reflects the fairness condition without imposing assumptions about the underlying FL approach.
- **Core assumption**: The symptoms chosen are both observable and sufficient to diagnose unfairness; client participation rates and statistical heterogeneity are the primary drivers of unfairness.
- **Evidence anchors**:
  - [abstract]: "Our definition of fairness comprises four notions with novel, corresponding metrics... symptomatically defined"
  - [section]: "The presented notions of fairness address the limitations of previous existing definitions, are symptomatically defined and simply quantified"
  - [corpus]: Weak - no direct citations, but the focus on client selection and fairness in related papers supports the general problem framing.
- **Break condition**: If the symptoms are not strongly correlated with actual fairness outcomes, or if other unobserved factors (e.g., adversarial behavior) dominate the fairness landscape, the symptom-based approach will miss them.

### Mechanism 2
- **Claim**: Jain's Fairness Index (JFI) provides a bounded, normalized measure of uniformity suitable for fairness quantification across heterogeneous client contributions.
- **Mechanism**: JFI is applied to ratios of performance to contribution (gain for individual fairness, reward/contribution for incentive fairness), ensuring that fairness rewards proportionality rather than just parity. The bounded range [0,1] makes metrics comparable across experiments.
- **Core assumption**: Uniformity of the gain or reward/contribution ratio is a valid proxy for fairness in the presence of statistical heterogeneity; contributions measured by federated Shapley values accurately reflect client impact.
- **Evidence anchors**:
  - [section]: "JFI is used in a novel way to measure uniformity in this work... a bounded, non-linear function of the coefficient of variation"
  - [corpus]: Weak - no explicit mention of JFI usage, but the general emphasis on fairness-aware client selection supports the relevance of uniformity metrics.
- **Break condition**: If client contributions are not accurately captured by Shapley values (e.g., due to auxiliary dataset bias or computational approximations), or if performance gains are dominated by factors outside client control, JFI-based fairness will be misleading.

### Mechanism 3
- **Claim**: Modular fairness metrics enable targeted diagnosis of fairness issues at multiple levels (client, group, system), supporting explainable AI in federated learning.
- **Mechanism**: By collecting metrics at different granularities (individual client, sensitive attribute groups, federation-wide), practitioners can identify whether unfairness stems from individual performance disparities, group-level bias, incentive misalignment, or orchestrator failure, guiding targeted remediation.
- **Core assumption**: The modular design does not require all metrics to be equally relevant in every scenario; practitioners can prioritize based on domain needs, and metrics remain interpretable even when used selectively.
- **Evidence anchors**:
  - [section]: "By collecting these metrics, a new level of explainability can be added to FL systems, at varying levels of granularity... enabling client, or even sensitive attribute level detail"
  - [corpus]: Weak - related work focuses on client selection and participation, not on granular fairness diagnosis, but the general trend toward explainable ML supports the value proposition.
- **Break condition**: If the modular metrics become too numerous or complex for practical use, or if practitioners lack the expertise to interpret them correctly, the benefits of granularity will be lost.

## Foundational Learning

- **Concept**: Federated Learning (FL) architecture and client-server dynamics.
  - **Why needed here**: The fairness definitions and metrics are built on specific FL mechanics (client participation, local training, weighted aggregation, federated evaluation), so understanding these is essential to interpret the fairness notions.
  - **Quick check question**: What is the difference between cross-device and cross-silo FL in terms of client participation rates and dataset sizes?

- **Concept**: Statistical heterogeneity and its impact on model convergence and fairness.
  - **Why needed here**: The experiments explicitly vary data settings (iid vs. non-iid) and measure how heterogeneity affects fairness, so understanding its effects is critical for interpreting results.
  - **Quick check question**: How does Dirichlet-based non-iid partitioning with α = 0.5 differ from α = 3 in terms of client data distribution skew?

- **Concept**: Game-theoretic contribution metrics (Shapley values) in FL.
  - **Why needed here**: Federated Shapley values are used to quantify client contributions, which are central to individual and incentive fairness definitions; without this, the rationale for these metrics is unclear.
  - **Quick check question**: Why is the computational complexity of full Shapley values O(K²|S_k|) a bottleneck, and what approximations exist?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Normalization and partitioning (iid vs. non-iid) -> FL simulation (Flower framework) -> Client sampling and training -> Aggregation -> Fairness analytics (per-round metrics) -> Result storage -> Analysis and visualization

- **Critical path**: Data → Partition → Train (FL rounds) → Evaluate (federated metrics) → Store → Analyze → Visualize fairness-performance trade-off

- **Design tradeoffs**:
  - Shapley value accuracy vs. computational cost (full vs. approximate)
  - Granularity of fairness metrics vs. interpretability and complexity
  - Client sampling rate (μs) vs. fairness measurement stability

- **Failure signatures**:
  - Low orchestrator fairness despite high client performance (server not benefiting)
  - High variance in individual fairness across rounds (unstable fairness convergence)
  - Group fairness metrics dominated by outliers (median vs. mean choice matters)

- **First 3 experiments**:
  1. Run FedAvg on CIFAR-10 iid with 10 clients, 50% participation, measure baseline fairness
  2. Switch to q-FedAvg on same setup, compare fairness metrics to identify improvement
  3. Introduce non-iid partitioning (α = 0.5) on CIFAR-10, observe fairness degradation and any mitigation by q-FedAvg

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal linear fairness metric to replace Jain's Fairness Index (JFI) for all four notions of fairness?
- **Basis in paper**: [explicit] The authors propose that JFI is non-linear and may limit intuitive interpretation, suggesting Gini coefficient as a potential alternative.
- **Why unresolved**: The paper only briefly mentions the Gini coefficient as a possibility without conducting experiments to compare its effectiveness against JFI.
- **What evidence would resolve it**: Comparative experiments using both JFI and Gini coefficient across various FL scenarios to determine which metric provides more intuitive and comparable fairness measurements.

### Open Question 2
- **Question**: How does the fairness performance change with different Dirichlet α parameters for non-iid data partitioning?
- **Basis in paper**: [inferred] The paper uses α = 0.5 for CIFAR-10 and α = 3 for NSL-KDD but does not explore the effects of varying α.
- **Why unresolved**: The paper acknowledges this as a direction for future work but does not investigate how different α values impact fairness metrics.
- **What evidence would resolve it**: Experiments testing a range of α values (e.g., 0.1, 0.5, 1.0, 2.0, 5.0) across multiple datasets to quantify the relationship between α and fairness performance.

### Open Question 3
- **Question**: What is the fairness performance in unsupervised learning, foundational models, and fine-tuning scenarios in FL?
- **Basis in paper**: [explicit] The authors mention investigating fairness performance in systems utilizing unsupervised learning, foundational models, and fine-tuning as crucial for future trends.
- **Why unresolved**: The current study focuses on supervised classification tasks and does not explore these emerging areas of FL.
- **What evidence would resolve it**: Experiments applying Federated Fairness Analytics to unsupervised learning tasks, pre-trained models fine-tuning, and foundational model adaptation scenarios to measure fairness across these new paradigms.

## Limitations
- The symptom-based fairness definitions rely on strong assumptions about the correlation between observable symptoms and actual fairness outcomes
- Computational burden of federated Shapley values could limit scalability in production settings
- The approach may miss critical fairness issues if unobserved factors (e.g., adversarial behavior) dominate the fairness landscape

## Confidence
- **High**: Jain's Fairness Index as a bounded uniformity metric (Mechanism 2) - well-established mathematical foundation
- **Medium**: Symptom-based fairness definitions (Mechanism 1) - reasonable but requires empirical validation
- **Medium**: Modular metric design enabling granular diagnosis (Mechanism 3) - theoretically sound but untested in practice

## Next Checks
1. Conduct ablation studies removing individual fairness metrics to assess impact on overall fairness quantification accuracy
2. Compare federated Shapley value approximations against full calculations across different client counts to quantify approximation error
3. Test fairness metric sensitivity to adversarial client behavior by introducing deliberately biased participants