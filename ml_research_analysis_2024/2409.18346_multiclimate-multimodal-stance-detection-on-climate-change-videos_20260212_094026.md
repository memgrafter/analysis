---
ver: rpa2
title: 'MultiClimate: Multimodal Stance Detection on Climate Change Videos'
arxiv_id: '2409.18346'
source_url: https://arxiv.org/abs/2409.18346
tags:
- change
- climate
- train
- stance
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MultiClimate introduces the first multimodal stance detection dataset
  on climate change, containing 4,209 frame-transcript pairs from 100 YouTube videos.
  The dataset addresses the lack of multimodal data for detecting stance towards climate
  change.
---

# MultiClimate: Multimodal Stance Detection on Climate Change Videos

## Quick Facts
- arXiv ID: 2409.18346
- Source URL: https://arxiv.org/abs/2409.18346
- Authors: Jiawen Wang; Longfei Zuo; Siyao Peng; Barbara Plank
- Reference count: 17
- Dataset: First multimodal stance detection dataset on climate change with 4,209 frame-transcript pairs from 100 YouTube videos

## Executive Summary
MultiClimate introduces the first multimodal stance detection dataset for climate change content, containing 4,209 frame-transcript pairs from 100 YouTube videos. The dataset addresses the lack of multimodal data for detecting stance towards climate change, covering SUPPORT, NEUTRAL, and OPPOSE categories. The study evaluates state-of-the-art text-only, image-only, and multimodal models, finding that fusion models combining both modalities achieve the best performance (0.747 accuracy / 0.749 F1). Notably, even large 9B-sized models perform worse than smaller fusion models in zero-shot settings, indicating the challenge of multimodal stance detection for large language models.

## Method Summary
The MultiClimate dataset was created by downloading 100 YouTube videos on climate change, extracting frames at 5-second intervals, and obtaining corresponding transcripts. The data was annotated for stance towards climate change (SUPPORT, NEUTRAL, OPPOSE). The study evaluated multiple model architectures including text-only BERT, image-only ResNet50 and ViT, and fusion models combining both modalities. Zero-shot evaluations were conducted on large multimodal models (LLaMA3, Gemma2, IDEFICS) and compared against fine-tuned smaller models.

## Key Results
- Fusion models combining text and image modalities achieve best performance (0.747 accuracy / 0.749 F1)
- Text-only BERT significantly outperforms image-only models (ResNet50, ViT) for stance detection
- Large 9B-sized models (LLaMA3, Gemma2, IDEFICS) perform worse than smaller fusion models in zero-shot settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal fusion models outperform single-modality models in stance detection
- Mechanism: Combining visual and textual embeddings compensates for each modality's weaknesses, leading to improved classification accuracy
- Core assumption: Visual and textual information provide complementary signals for stance detection
- Evidence anchors:
  - [abstract]: "Combining both modalities achieves state-of-the-art, 0.747/0.749 in accuracy/F1"
  - [section 4.2]: "BERT + ViT fusion models outperform CLIP, BLIP, and IDEFICS, as textual features are crucial to our CC stance detection"
  - [corpus]: Weak evidence - only compares within dataset without ablation studies on modality contribution
- Break condition: If visual and textual modalities provide redundant or contradictory information

### Mechanism 2
- Claim: Text-only BERT models outperform image-only models for stance detection
- Mechanism: Language models capture explicit stance markers and contextual cues more effectively than image recognition models
- Core assumption: Stance information is more explicitly encoded in language than in visual features
- Evidence anchors:
  - [abstract]: "text-only BERT significantly outperforms image-only ResNet50 and ViT"
  - [section 4.2]: "BERT model achieves the best performance among single-modal models, notably surpassing the zero-shot LLMs"
  - [corpus]: Weak evidence - no comparison with other text-only models beyond BERT variants
- Break condition: If stance markers become more visual than textual in the dataset

### Mechanism 3
- Claim: Large multimodal models (9B+) perform worse than smaller fusion models in zero-shot settings
- Mechanism: Large models lack task-specific fine-tuning despite their size advantage
- Core assumption: Model size alone doesn't guarantee superior zero-shot performance without appropriate training
- Evidence anchors:
  - [abstract]: "Our 100M-sized fusion models also beat CLIP and BLIP, as well as the much larger 9B-sized multimodal IDEFICS"
  - [section 4.2]: "Even large 9B-sized models (LLaMA3, Gemma2, IDEFICS) perform worse than smaller fusion models in zero-shot settings"
  - [corpus]: Weak evidence - only compares specific large models without broader sampling
- Break condition: If large models receive task-specific fine-tuning or prompting

## Foundational Learning

- Concept: Multimodal learning fundamentals
  - Why needed here: Understanding how to combine visual and textual representations is crucial for building effective fusion models
  - Quick check question: What are the key challenges in aligning visual and textual embeddings for multimodal tasks?

- Concept: Stance detection classification
  - Why needed here: The task requires distinguishing between SUPPORT, NEUTRAL, and OPPOSE stances on climate change
  - Quick check question: How would you design a prompt to elicit stance information from a language model?

- Concept: Zero-shot learning vs. fine-tuning
  - Why needed here: Understanding the trade-offs between using pre-trained models directly vs. adapting them to the specific task
  - Quick check question: What are the advantages and limitations of zero-shot learning for multimodal tasks?

## Architecture Onboarding

- Component map: YouTube video download -> Frame extraction (5-second intervals) -> Transcript extraction -> Frame-transcript alignment -> Annotation -> Model training/evaluation
- Critical path: YouTube video download → Frame extraction (5-second intervals) → Transcript extraction → Frame-transcript alignment → Annotation → Model training/evaluation
- Design tradeoffs: Smaller models with fusion vs. larger models with single modality; zero-shot vs. fine-tuned approaches; computational cost vs. performance
- Failure signatures: Poor alignment between frames and transcripts; class imbalance in annotations; model bias toward certain stances; computational resource constraints
- First 3 experiments:
  1. Test BERT performance on text-only stance detection to establish baseline
  2. Test ResNet50 and ViT on image-only stance detection to compare modalities
  3. Implement BERT + ViT fusion model and evaluate performance improvement over single modalities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do visual and textual modalities interact in multimodal stance detection, and which modality provides more informative features for specific stance categories?
- Basis in paper: [explicit] The paper notes that "text-only models overall perform considerably better than image-only models" but also states that "visual and textual information compensate each other" and that "the best is achieved when combining image and text information."
- Why unresolved: The paper doesn't provide detailed analysis of which modality is more informative for different stance categories (SUPPORT, NEUTRAL, OPPOSE), nor does it explain the specific interactions between visual and textual features.
- What evidence would resolve it: Detailed ablation studies showing performance differences across stance categories when using only text, only images, or both; analysis of feature importance scores for each modality across different stance types.

### Open Question 2
- Question: What are the key factors causing the bias in zero-shot IDEFICS toward predicting SUPPORT labels, and how can this bias be systematically addressed?
- Basis in paper: [explicit] The paper observes that "zero-shot IDEFICS model to predominantly predict SUPPORT labels, less often NEUTRAL, and rarely OPPOSE" and discusses potential reasons related to the training data and conflation of "stance" and "sentiment."
- Why unresolved: While the paper mentions fine-tuning reduces bias, it doesn't provide a comprehensive analysis of the underlying causes or systematic methods to prevent such bias in future models.
- What evidence would resolve it: Detailed analysis of the training data distribution and its impact on model predictions; systematic comparison of different bias mitigation techniques; evaluation of bias across different types of climate change discourse.

### Open Question 3
- Question: How would incorporating additional modalities (audio, video) beyond text and frames affect stance detection performance on climate change content?
- Basis in paper: [inferred] The paper's "Limitations" section mentions that "incorporating audio and video modalities can enhance the understanding of speaker's emotions and intentions, and potentially further improve stance detection."
- Why unresolved: The study only uses text and static frames, leaving the potential contribution of audio and full video modalities unexplored.
- What evidence would resolve it: Comparative experiments using text-only, text+frames, and full multimodal (text+frames+audio+video) approaches; analysis of which additional modalities contribute most to performance improvements for different stance categories.

## Limitations
- Dataset contains only 100 YouTube videos, potentially limiting representativeness across different media platforms and cultural contexts
- Results may not generalize to other stance detection tasks or domains beyond climate change content
- Zero-shot evaluation constraints may not reflect the potential of large models with proper prompting or fine-tuning

## Confidence

**High Confidence (Mechanistic Claims)**:
- Multimodal fusion models outperform single-modality models in stance detection
- Text-only BERT models outperform image-only models for stance detection
- Large multimodal models perform worse than smaller fusion models in zero-shot settings

**Medium Confidence (Empirical Claims)**:
- The specific accuracy and F1 scores achieved by the models
- The ranking of different model architectures on the MultiClimate dataset
- The comparative performance of specific large models (LLaMA3, Gemma2, IDEFICS)

**Low Confidence (Extrapolative Claims)**:
- The generalizability of results to other domains or topics
- The long-term viability of current model architectures for evolving stance detection tasks
- The scalability of the approach to larger, more diverse datasets

## Next Checks

1. **Cross-dataset validation**: Test the best-performing models from MultiClimate on other stance detection datasets (e.g., memes, text-only climate discourse) to assess generalizability across different data modalities and domains.

2. **Prompt engineering experiments**: Systematically evaluate different prompting strategies for large language models to determine if zero-shot performance can be improved without fine-tuning, comparing various prompt templates and examples.

3. **Bias and fairness analysis**: Conduct a comprehensive analysis of model predictions across different demographic groups, video sources, and climate change subtopics to identify and quantify potential biases in the dataset and model predictions.