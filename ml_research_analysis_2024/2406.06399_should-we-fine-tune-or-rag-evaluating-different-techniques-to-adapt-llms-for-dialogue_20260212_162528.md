---
ver: rpa2
title: Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for
  Dialogue
arxiv_id: '2406.06399'
source_url: https://arxiv.org/abs/2406.06399
tags:
- dialogue
- knowledge
- learning
- know
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies how to adapt large language models (LLMs) for
  dialogue generation across four types: open-domain, knowledge-grounded, task-oriented,
  and question answering. It compares in-context learning and fine-tuning, evaluating
  the impact of incorporating external knowledge via retrieval-augmented generation
  (RAG) or using gold knowledge.'
---

# Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue

## Quick Facts
- arXiv ID: 2406.06399
- Source URL: https://arxiv.org/abs/2406.06399
- Reference count: 39
- This paper evaluates fine-tuning versus in-context learning for adapting LLMs to four dialogue types, finding that no single technique is universally best.

## Executive Summary
This paper investigates how to adapt large language models for dialogue generation by comparing fine-tuning and in-context learning across four dialogue types: open-domain, knowledge-grounded, task-oriented, and question answering. The study evaluates two base models (Llama-2 and Mistral) with and without external knowledge incorporation via RAG or gold knowledge. Results show that technique efficacy depends on both the base model and dialogue type, with fine-tuning generally outperforming in-context learning. The paper emphasizes that human evaluation is essential, as automatic metrics often poorly correlate with human judgments.

## Method Summary
The paper adapts Llama-2 Chat and Mistral Instruct for four dialogue types using two main techniques: fine-tuning with LoRA adapters and in-context learning. External knowledge is incorporated through RAG (using FAISS vector stores and a BERT-based retriever) or gold knowledge. The study uses four datasets (DailyDialog, Wizard of Wikipedia, DSTC9 Track 1, NarrativeQA) and evaluates with automatic metrics (perplexity, BLEU-4, F1, ROUGE-L, KF1) and human evaluation (contextualization, appropriateness, correctness, validity). Integrated gradients provide explainability analysis.

## Key Results
- Fine-tuning generally outperforms in-context learning across all dialogue types and base models
- Incorporating external knowledge improves factual accuracy and contextualization of responses
- Human evaluation is essential as automatic metrics poorly correlate with human judgments
- No single adaptation technique is universally optimal; efficacy depends on base model and dialogue type

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning trains model parameters on task-specific dialogue data, allowing learning of dialogue-specific patterns more effectively than instruction-based in-context learning. This works when fine-tuning datasets contain sufficient representative examples. Performance may suffer if datasets are too small, noisy, or unrepresentative.

### Mechanism 2
Incorporating external knowledge grounds responses in factual data, reducing hallucinations and improving relevance. This requires retrieved knowledge to be relevant and accurate, with the model capable of effectively incorporating this information. Benefits diminish if retriever returns irrelevant/erroneous information or model struggles to integrate knowledge.

### Mechanism 3
Human evaluation captures nuanced response quality aspects (contextualization, appropriateness, validity) that automatic metrics miss. This assumes human evaluators are reliable and consistent with proper evaluation protocols. Results may be misleading if protocols are poorly designed, evaluators are biased/inconsistent, or assessed dimensions don't align with dialogue system goals.

## Foundational Learning

- **Concept**: Understanding differences between in-context learning and fine-tuning
  - Why needed here: Paper compares these techniques for LLM adaptation to dialogue tasks
  - Quick check question: What is the main difference between in-context learning and fine-tuning in terms of how they adapt a pre-trained LLM to a new task?

- **Concept**: Familiarity with retrieval-augmented generation (RAG)
  - Why needed here: Paper investigates impact of incorporating external knowledge through RAG
  - Quick check question: How does RAG enhance the capabilities of an LLM in a dialogue setting?

- **Concept**: Knowledge of dialogue types and their characteristics
  - Why needed here: Paper evaluates adaptation techniques across four dialogue types
  - Quick check question: What are the key differences between open-domain dialogue and task-oriented dialogue in terms of their goals and evaluation criteria?

## Architecture Onboarding

- **Component map**: Base LLMs (Llama-2 Chat, Mistral Instruct) -> Fine-tuning pipeline (LoRA adapter) -> In-context learning pipeline -> Retriever (for RAG) -> Knowledge bases -> Evaluation framework
- **Critical path**: Load base LLM → Prepare dialogue data → Apply fine-tuning or in-context learning → Retrieve knowledge if using RAG → Generate responses → Evaluate using automatic and human metrics
- **Design tradeoffs**: Fine-tuning vs. in-context learning (resource intensity vs. flexibility); RAG vs. gold knowledge (practicality vs. ideal information)
- **Failure signatures**: Low BLEU/ROUGE scores (poor response quality or mismatch); low human evaluation scores (contextualization/appropriateness/validity issues); high perplexity (coherence struggles)
- **First 3 experiments**: 1) Fine-tune Llama-2 Chat on open-domain dialogue dataset and evaluate with automatic metrics; 2) Adapt Mistral Instruct to knowledge-grounded dialogue using in-context learning and evaluate with human metrics; 3) Compare RAG vs. gold knowledge on task-oriented dialogue using both automatic and human evaluations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of larger LLMs (70B/175B) compare to 7B models studied when adapting for different dialogue types?
- Basis: Computational constraints limited experiments to 7B models
- Why unresolved: Only evaluated 7B parameter models due to resource limitations
- Evidence needed: Empirical results comparing larger LLMs (70B/175B) with 7B models across all four dialogue types using same adaptation techniques and metrics

### Open Question 2
- Question: How do different knowledge retrieval strategies (semantic similarity vs. TF-IDF) impact knowledge grounding effectiveness?
- Basis: Used BERT-based retriever but didn't explore alternative methods
- Why unresolved: Only used off-the-shelf retriever based on L2 distance between BERT encodings
- Evidence needed: Comparative analysis of knowledge grounding using different retrieval strategies across dialogue types, measuring retrieval accuracy, response quality, and knowledge incorporation

### Open Question 3
- Question: How does impact of fine-tuning vs. in-context learning vary with training data size/quality for each dialogue type?
- Basis: Fixed datasets used but variation in training data size/quality not explored
- Why unresolved: Used fixed datasets without exploring how training data variations affect relative performance
- Evidence needed: Systematic experiments varying training data size/quality for each dialogue type, comparing fine-tuning vs. in-context learning performance

## Limitations

- Limited to two base models (7B parameters), restricting generalizability to other LLM architectures and sizes
- Single dataset per dialogue type may not capture full diversity within each type
- Human evaluation lacks specification of annotator qualifications, inter-annotator agreement scores, and number of annotators
- Integrated gradients analysis presented as preliminary without validation of key token contributions

## Confidence

- **High confidence**: Fine-tuning generally outperforms in-context learning across dialogue types
- **Medium confidence**: Human evaluation is essential due to poor correlation between automatic metrics and human judgment
- **Low confidence**: Efficacy of each technique depends on both base LLM and dialogue type, but underlying reasons for these dependencies lack systematic analysis

## Next Checks

1. Replicate experiments across more diverse base models (GPT-3.5, Claude, Gemini) to verify dependencies between base model and dialogue type
2. Conduct cross-dataset validation within each dialogue type using multiple datasets to assess generalizability
3. Perform systematic ablation study isolating individual factors (adapter rank, learning rate, knowledge quality) to determine specific impact on performance