---
ver: rpa2
title: 'JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small
  Data Synthesis Models'
arxiv_id: '2405.14365'
source_url: https://arxiv.org/abs/2405.14365
tags:
- data
- math
- reasoning
- problem
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an efficient method to improve mathematical
  reasoning in large language models by training a small 7B model to synthesize high-quality
  math problems. The approach uses knowledge distillation from GPT-4 combined with
  gradient-based influence estimation to select valuable math-related texts, resulting
  in a dataset of 6 million synthetic problems.
---

# JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models

## Quick Facts
- arXiv ID: 2405.14365
- Source URL: https://arxiv.org/abs/2405.14365
- Reference count: 40
- Authors: Kun Zhou, Beichen Zhang, Jiapeng Wang, Zhipeng Chen, Wayne Xin Zhao, Jing Sha, Zhichao Sheng, Shijin Wang, Ji-Rong Wen
- Primary result: A 7B synthesis model achieves SOTA on 18 math reasoning datasets while using only 20% of training costs

## Executive Summary
This paper introduces JiuZhang3.0, a method for improving mathematical reasoning in large language models through efficient data synthesis. The approach trains a small 7B model to generate high-quality mathematical problems using knowledge distillation from GPT-4 combined with gradient-based influence estimation to select valuable math-related texts. The resulting dataset of 6 million synthetic problems enables the model to surpass larger models on mathematical reasoning benchmarks while requiring significantly less computational resources than existing methods.

## Method Summary
JiuZhang3.0 employs a two-stage approach: first, a small 7B synthesis model is trained using knowledge distillation from GPT-4 to generate mathematical problems; second, gradient-based influence estimation is applied to select the most valuable math-related texts from existing corpora. This curated dataset of 6 million synthetic problems is then used to fine-tune the target model, resulting in improved mathematical reasoning capabilities at a fraction of the computational cost of traditional approaches.

## Key Results
- Achieves state-of-the-art performance on 18 mathematical reasoning datasets
- Surpasses larger models despite using a smaller synthesis model
- Requires only 20% of the total cost compared to existing methods
- Demonstrates that smaller synthesis models can generate more useful data than larger LLMs for mathematical reasoning

## Why This Works (Mechanism)
The method leverages knowledge distillation from GPT-4 to transfer high-quality reasoning patterns to a smaller, more efficient model. The gradient-based influence estimation identifies the most educationally valuable math problems, focusing training on concepts that most improve downstream performance. By using a smaller synthesis model, the approach maintains high data quality while dramatically reducing computational costs during the synthesis phase.

## Foundational Learning

**Knowledge Distillation**: Why needed - Transfers reasoning capabilities from larger models to smaller, more efficient ones; Quick check - Verify the distilled model can solve problems similar to the oracle.

**Gradient-based Influence Estimation**: Why needed - Identifies which training examples most improve model performance; Quick check - Measure performance improvement when training on high-influence vs random examples.

**Synthetic Data Generation**: Why needed - Creates scalable training data without manual annotation; Quick check - Compare model performance on synthetic vs real data.

**Curriculum Learning**: Why needed - Orders problems from simple to complex to improve learning efficiency; Quick check - Track learning curves with different problem orderings.

**Mathematical Problem Decomposition**: Why needed - Breaks complex problems into manageable sub-steps; Quick check - Verify step-by-step solutions maintain logical consistency.

## Architecture Onboarding

**Component Map**: Data Curation System -> Knowledge Distillation Module -> Synthesis Model (7B) -> Gradient Influence Estimator -> Math Reasoning Model

**Critical Path**: The synthesis model generation is the critical path, as its quality directly determines the effectiveness of the entire pipeline. The gradient influence estimator operates in parallel to continuously refine data selection.

**Design Tradeoffs**: Uses a smaller 7B synthesis model instead of larger alternatives to reduce computational costs while maintaining data quality through knowledge distillation. This sacrifices some diversity in generated problems for efficiency gains.

**Failure Signatures**: Poor performance on novel problem types suggests insufficient diversity in synthetic data. Overfitting to specific problem patterns indicates the influence estimation may be too narrow. Suboptimal mathematical reasoning suggests issues with the knowledge distillation process.

**First Experiments**:
1. Generate a small batch of synthetic problems and evaluate their quality against human-created problems
2. Test the influence estimation by training on high-influence vs randomly selected problems
3. Compare mathematical reasoning performance using different synthesis model sizes (7B vs 13B vs 34B)

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Relies heavily on the quality of GPT-4 outputs for knowledge distillation, which may not be consistently reproducible
- The gradient-based influence estimation method is novel but lacks extensive validation against simpler filtering approaches
- Claims of superior performance by the 7B synthesis model over larger LLMs require independent verification
- The 20% cost efficiency claim depends on specific implementation details that may vary across research groups

## Confidence

**High confidence**: The general methodology of using a small model for data synthesis is well-established
**Medium confidence**: The specific implementation details and performance claims
**Low confidence**: The comparative advantage over larger synthesis models and exact computational efficiency metrics

## Next Checks
1. Replicate the knowledge distillation process on a held-out set of math problems to verify the quality of synthesized data
2. Conduct ablation studies removing the gradient-based influence estimation to measure its actual contribution to performance
3. Perform cross-dataset generalization tests to ensure the model doesn't overfit to specific mathematical reasoning patterns present in the training data