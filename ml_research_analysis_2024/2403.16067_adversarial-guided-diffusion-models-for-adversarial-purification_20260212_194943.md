---
ver: rpa2
title: Adversarial Guided Diffusion Models for Adversarial Purification
arxiv_id: '2403.16067'
source_url: https://arxiv.org/abs/2403.16067
tags:
- adversarial
- examples
- diffusion
- robust
- guidance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adversarial Guided Diffusion Models (AGDM),
  a novel approach for adversarial purification that effectively removes adversarial
  perturbations while preserving semantic information. The key idea is to introduce
  adversarial guidance during the reverse diffusion process, modeled by an auxiliary
  neural network trained with adversarial training.
---

# Adversarial Guided Diffusion Models for Adversarial Purification

## Quick Facts
- arXiv ID: 2403.16067
- Source URL: https://arxiv.org/abs/2403.16067
- Reference count: 18
- Primary result: Achieves up to 7.30% improvement in robust accuracy against AutoAttack on CIFAR-10 compared to vanilla diffusion models

## Executive Summary
This paper introduces Adversarial Guided Diffusion Models (AGDM), a novel approach for adversarial purification that effectively removes adversarial perturbations while preserving semantic information. The key innovation is introducing adversarial guidance during the reverse diffusion process, modeled by an auxiliary neural network trained with adversarial training. Unlike previous guided diffusion methods that rely on pixel-level distances, AGDM leverages latent representations from the auxiliary network to guide the purification process, avoiding explicit involvement of adversarial perturbations.

The proposed method demonstrates significant improvements in robust accuracy against various attacks on benchmark datasets including CIFAR-10, CIFAR-100, and ImageNet. By incorporating adversarial guidance through an auxiliary network, AGDM addresses limitations of existing diffusion-based purification methods while maintaining computational efficiency during inference.

## Method Summary
AGDM introduces adversarial guidance during the reverse diffusion process through an auxiliary neural network trained with adversarial examples. The auxiliary network provides latent representation guidance that directs the denoising process toward clean images while avoiding explicit use of adversarial perturbations. This approach combines the strengths of diffusion models for image generation with adversarial training for robustness, creating a purification mechanism that operates in latent space rather than pixel space. The method maintains the computational efficiency of diffusion models while significantly improving their robustness against adversarial attacks.

## Key Results
- Achieves up to 7.30% improvement in robust accuracy against AutoAttack on CIFAR-10 compared to vanilla diffusion models
- Demonstrates strong generalization ability against unseen attacks
- Shows significant performance gains on CIFAR-100 and ImageNet datasets
- Maintains competitive clean accuracy while improving robust accuracy

## Why This Works (Mechanism)
The method works by leveraging adversarial training to create an auxiliary network that provides semantic guidance during the reverse diffusion process. By operating in latent representation space rather than pixel space, the purification process can focus on preserving meaningful content while removing adversarial perturbations. The adversarial guidance helps the diffusion model distinguish between semantic features and adversarial noise, allowing for more effective purification without compromising image quality.

## Foundational Learning

**Diffusion Models**
- Why needed: Core generative framework for image purification
- Quick check: Understand forward noising and reverse denoising processes

**Adversarial Training**
- Why needed: Creates robust auxiliary network for guidance
- Quick check: Familiarity with generating adversarial examples and robust optimization

**Latent Representations**
- Why needed: Enables semantic-level guidance instead of pixel-level
- Quick check: Understanding of feature extraction and representation learning

**Image Purification**
- Why needed: Defines the problem context and evaluation metrics
- Quick check: Knowledge of adversarial attack types and defense mechanisms

## Architecture Onboarding

**Component Map**
Input Image -> Auxiliary Network (Adversarially Trained) -> Latent Guidance -> Reverse Diffusion Process -> Purified Output

**Critical Path**
The critical path flows from the input image through the auxiliary network to obtain latent guidance, which then conditions the reverse diffusion steps. Each denoising step uses both the current noisy estimate and the latent guidance to produce cleaner outputs progressively.

**Design Tradeoffs**
The main tradeoff involves balancing the strength of adversarial guidance with the preservation of natural image statistics. Stronger guidance may improve robustness but could lead to over-smooth or unnatural outputs. The auxiliary network adds computational overhead during training but not inference.

**Failure Signatures**
- Over-reliance on guidance leading to mode collapse or unnatural images
- Insufficient guidance resulting in poor adversarial perturbation removal
- Auxiliary network failure causing degradation in both clean and robust accuracy

**First Experiments**
1. Test purification performance on simple adversarial examples before complex attacks
2. Evaluate auxiliary network effectiveness independently of diffusion model
3. Compare latent guidance versus pixel-level guidance ablation

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on benchmark datasets, leaving uncertainty about real-world generalization
- Reliance on auxiliary neural network introduces potential vulnerabilities if compromised
- Computational overhead compared to existing purification approaches not thoroughly analyzed

## Confidence
High: Experimental results showing improved robust accuracy against AutoAttack on CIFAR-10 and proposed methodology for integrating adversarial guidance into diffusion models
Medium: Claims about strong generalization against unseen attacks and effectiveness of latent representation guidance over pixel-level distances
Low: Assertion that method avoids explicit involvement of adversarial perturbations, as auxiliary network is still trained with adversarial examples

## Next Checks
1. Evaluate AGDM's performance against adaptive attacks specifically designed to exploit the diffusion-based purification process
2. Conduct ablation studies to quantify the contribution of the auxiliary network versus other components of the purification pipeline
3. Measure computational overhead and inference time compared to baseline diffusion purification methods to assess practical deployment viability