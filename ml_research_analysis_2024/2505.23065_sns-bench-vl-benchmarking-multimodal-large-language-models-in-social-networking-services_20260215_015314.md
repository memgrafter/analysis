---
ver: rpa2
title: 'SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social Networking
  Services'
arxiv_id: '2505.23065'
source_url: https://arxiv.org/abs/2505.23065
tags:
- arxiv
- multimodal
- content
- preprint
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SNS-Bench-VL introduces a comprehensive multimodal benchmark for
  evaluating Vision-Language Large Language Models in social networking services.
  The benchmark covers 8 tasks including OCR, machine reading comprehension, hashtag
  selection, comment generation, query relevance, query generation, gender analysis,
  and taxonomy classification, using 4,001 curated multimodal question-answer pairs.
---

# SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social Networking Services

## Quick Facts
- arXiv ID: 2505.23065
- Source URL: https://arxiv.org/abs/2505.23065
- Reference count: 40
- Primary result: Multimodal benchmark for 8 social media tasks shows closed-source models slightly outperform open-source alternatives

## Executive Summary
SNS-Bench-VL introduces a comprehensive multimodal benchmark for evaluating Vision-Language Large Language Models (MLLMs) in social networking services. The benchmark covers 8 tasks including OCR, machine reading comprehension, hashtag selection, comment generation, query relevance, query generation, gender analysis, and taxonomy classification, using 4,001 curated multimodal question-answer pairs. Experiments with over 25 state-of-the-art models show that closed-source models slightly outperform open-source alternatives (top scores of 80.26% vs 79.97% average accuracy), while highlighting challenges in complex reasoning and nuanced social context comprehension. The benchmark reveals task-specific performance variations, with simpler classification tasks showing higher accuracy than complex reasoning tasks, and demonstrates that scaling and fine-tuning significantly impact multimodal performance.

## Method Summary
SNS-Bench-VL evaluates MLLMs across 8 multimodal tasks using 4,001 question-answer pairs derived from a major social platform with over 3 billion users. The benchmark employs task-specific metrics including F1 scores for hashtag selection, BGE for machine reading comprehension, and accuracy for classification tasks. State-of-the-art models are evaluated using fixed prompts across all tasks, with results aggregated to provide overall performance scores. The evaluation requires 128 NVIDIA A100 GPUs for comprehensive testing across multiple models.

## Key Results
- Closed-source models achieve slightly higher average accuracy (80.26%) compared to open-source models (79.97%)
- Performance varies significantly across tasks, with classification tasks showing higher accuracy than complex reasoning tasks
- Smaller models like DeepSeek-VL2 (61.42%) and Kimi-VL-A3B-Instruct (57.39%) demonstrate that architectural improvements can yield solid performance even with limited parameters
- Complex reasoning tasks like Note-MRC and Note-QueryCorr remain challenging for most models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The benchmark's multimodal structure forces models to integrate visual and textual information for accurate predictions.
- **Mechanism:** By combining images with text in social media contexts, the benchmark requires models to perform cross-modal reasoning rather than relying on unimodal cues. This design ensures models must understand how visual elements complement or contradict textual information.
- **Core assumption:** Social media content inherently requires multimodal understanding for proper interpretation.
- **Evidence anchors:** [abstract] "SNS-Bench-VL incorporates images and text across 8 multimodal tasks" [section] "Unlike prior work on isolated tasks, SNS-Bench-VL offers a structured assessment across eight diverse SNS-related tasks, covering 4,001 multimodal questions (including text, and images)"

### Mechanism 2
- **Claim:** The benchmark's task diversity captures different aspects of social media intelligence that models must master.
- **Mechanism:** The eight tasks (OCR, MRC, hashtag selection, comment generation, query relevance, query generation, gender analysis, taxonomy classification) represent distinct cognitive skills needed for social media comprehension, preventing models from specializing in just one capability.
- **Core assumption:** Social media intelligence requires multiple specialized skills rather than a single general capability.
- **Evidence anchors:** [abstract] "SNS-Bench-VL incorporates images and text across 8 multimodal tasks, including note comprehension, user engagement analysis, information retrieval, and personalized recommendation" [section] "The core skills for SNS [74, 75, 76] can be summarized into for parts: note comprehension, user engagement analysis, information retrieval, and personalized recommendation"

### Mechanism 3
- **Claim:** The benchmark's real-world data authenticity improves model generalization to actual social media scenarios.
- **Mechanism:** By using data from a platform with over 3 billion users and following systematic curation including de-identification, desensitization, and quality control, the benchmark ensures models are evaluated on realistic, representative social media content rather than artificial examples.
- **Core assumption:** Models trained on curated real-world data will generalize better to actual deployment scenarios than those trained on synthetic data.
- **Evidence anchors:** [abstract] "SNS-Bench-VL is derived from a major social platform 4 with over 3 billion users (with proper data licensing), ensuring authentic representation of real-world social media interactions" [section] "To ensure diversity and representativeness, we sample based on: (1) Topic and Modality: spanning fashion, beauty, health, travel, and food across text and images"

## Foundational Learning

- **Concept:** Multimodal integration principles
  - **Why needed here:** Understanding how visual and textual information complement each other is essential for interpreting social media content
  - **Quick check question:** Can you explain why a meme's meaning depends on both its image and caption working together?

- **Concept:** Social media task taxonomy
  - **Why needed here:** Recognizing the different types of social media interactions (engagement, retrieval, recommendation) helps in designing appropriate evaluation metrics
  - **Quick check question:** What distinguishes content recommendation from content comprehension in social media contexts?

- **Concept:** Cross-modal quality control
  - **Why needed here:** Ensuring consistency between visual and textual elements is crucial for valid benchmark evaluation
  - **Quick check question:** How would you verify that an image-text pair in the benchmark actually belongs together semantically?

## Architecture Onboarding

- **Component map:** Data collection pipeline → Preprocessing → Annotation → Quality control → Expert review → Evaluation framework → Task-specific metrics → Model comparison dashboard → Visualization tools → Confusion matrix analysis → Performance trend tracking
- **Critical path:** Data → Preprocessing → Annotation → Quality control → Benchmark construction → Model evaluation → Analysis
- **Design tradeoffs:** Real-world authenticity vs. data privacy concerns, Task diversity vs. evaluation complexity, Multimodal integration vs. computational efficiency, Expert review vs. scalability
- **Failure signatures:** High variance across tasks suggests insufficient multimodal reasoning, Consistent underperformance on specific task types indicates architectural limitations, Poor correlation between closed-source and open-source results suggests benchmark bias
- **First 3 experiments:** Evaluate a simple unimodal model (text-only) on the benchmark to establish baseline performance and quantify multimodal benefit, Test model performance on isolated tasks vs. integrated tasks to identify cross-modal reasoning challenges, Analyze failure cases by human experts to understand specific comprehension gaps in current models

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do MLLMs perform on video-based multimodal tasks in social networking services compared to their performance on static image-text tasks?
- **Basis in paper:** [inferred] The paper mentions that SNS-Bench-VL currently lacks support for video-based content, which constitutes a significant portion of modern social platforms, and that this omission limits the benchmark's ability to fully evaluate a model's competence in handling temporally dynamic, multimodal inputs.
- **Why unresolved:** The benchmark does not include video data, so empirical comparison between video and static tasks is not possible within the current framework.
- **What evidence would resolve it:** Experimental results comparing MLLM performance on video-based versus image-text tasks in social networking contexts, ideally using a benchmark that includes video data.

### Open Question 2
- **Question:** What specific architectural improvements or training strategies could enable smaller MLLMs to perform better on complex reasoning tasks like Note-MRC and Note-QueryCorr?
- **Basis in paper:** [explicit] The paper shows that smaller models like DeepSeek-VL2 (61.42%) and Kimi-VL-A3B-Instruct (57.39%) demonstrate that well-targeted architectural improvements and training schemes can yield solid performance even with limited parameter budgets, but complex reasoning tasks remain challenging.
- **Why unresolved:** While the paper identifies that smaller models struggle with complex reasoning, it does not specify what architectural or training improvements would specifically address these weaknesses.
- **What evidence would resolve it:** Detailed analysis of model architectures and training methodologies that correlate with improved performance on complex reasoning tasks, potentially through ablation studies or architectural modifications.

### Open Question 3
- **Question:** How do different fine-tuning approaches (task-specific vs. general) affect MLLM performance across various SNS-related tasks, and which approach yields better generalization?
- **Basis in paper:** [inferred] The paper mentions that scaling model parameters and task-specific fine-tuning are critical to improving multimodal performance, and that some models maintain stable performance across diverse tasks while others show significant variance.
- **Why unresolved:** The paper does not compare different fine-tuning strategies or analyze their impact on generalization across the various SNS tasks.
- **What evidence would resolve it:** Comparative experiments evaluating MLLM performance using different fine-tuning strategies (e.g., task-specific fine-tuning, general pretraining, multi-task learning) across the benchmark's diverse task set, measuring both task-specific and cross-task generalization.

## Limitations

- Task diversity may not fully capture real-world social media complexity, as some tasks may be more deterministic than others
- Performance gaps between closed-source and open-source models could be influenced by prompt engineering differences rather than inherent capabilities
- Data authenticity claims rely on platform-specific licensing without independent verification of data representativeness

## Confidence

- **High Confidence**: Task-specific accuracy measurements and overall benchmark design are clearly specified and reproducible
- **Medium Confidence**: Performance comparisons between model types are valid but may be influenced by implementation details not fully disclosed
- **Low Confidence**: Claims about multimodal integration benefits are theoretically sound but lack direct experimental validation showing unimodal baselines

## Next Checks

1. Evaluate a text-only model on the same benchmark to quantify the actual multimodal advantage claimed in the paper
2. Measure model performance when tasks are presented in isolation versus integrated multimodal scenarios to identify cross-modal reasoning bottlenecks
3. Conduct a statistical analysis comparing the benchmark's data distribution against real-world social media content to verify authenticity claims