---
ver: rpa2
title: 'BAD: Bidirectional Auto-regressive Diffusion for Text-to-Motion Generation'
arxiv_id: '2409.10847'
source_url: https://arxiv.org/abs/2409.10847
tags:
- motion
- tokens
- sequence
- mask
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BAD (Bidirectional Autoregressive Diffusion),
  a novel approach for text-to-motion generation that addresses limitations of existing
  autoregressive and mask-based generative models. BAD combines the strengths of both
  by using a permutation-based corruption technique that preserves natural sequence
  structure while enforcing causal dependencies through randomized ordering.
---

# BAD: Bidirectional Auto-regressive Diffusion for Text-to-Motion Generation

## Quick Facts
- **arXiv ID**: 2409.10847
- **Source URL**: https://arxiv.org/abs/2409.10847
- **Reference count**: 0
- **Primary result**: Introduces BAD (Bidirectional Autoregressive Diffusion), achieving FID scores of 0.049 on HumanML3D and 0.221 on KIT-ML, outperforming baseline autoregressive and mask-based models.

## Executive Summary
BAD introduces a novel bidirectional autoregressive diffusion approach for text-to-motion generation that addresses limitations of existing autoregressive and mask-based generative models. The key innovation is a permutation-based corruption technique that preserves natural sequence structure while enforcing causal dependencies through randomized ordering. This enables effective capture of both sequential and bidirectional relationships. BAD combines permuted causal attention and bidirectional attention in a hybrid attention mask, achieving significant performance improvements on HumanML3D and KIT-ML datasets while maintaining competitive results compared to more complex models using advanced vector quantization techniques.

## Method Summary
BAD employs a two-stage approach: first using a VQ-VAE to convert raw 3D motion sequences into discrete tokens, then using a conditional transformer with permutation-based corruption to generate motions conditioned on text prompts. The permutation-based corruption randomly masks motion tokens and uses a random ordering to determine the sequence of prediction, preserving natural sequence structure while enforcing causal dependencies. The model employs a hybrid attention mask integrating permuted causal attention and bidirectional attention, and is trained using an objective function that aligns with the autoregressive objective while avoiding the independence assumption of masked tokens.

## Key Results
- Achieved FID score of 0.049 on HumanML3D (improving from 0.089 baseline)
- Achieved FID score of 0.221 on KIT-ML (improving from 0.316 baseline)
- Maintained competitive performance compared to models using advanced vector quantization techniques
- Demonstrated effectiveness on both HumanML3D and KIT-ML datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BAD's permutation-based corruption preserves natural sequence structure while enforcing causal dependencies, allowing it to capture both sequential and bidirectional relationships effectively.
- Mechanism: The permutation-based corruption process randomly masks motion tokens and then uses a random ordering z to determine the sequence in which these tokens are predicted. This preserves the natural structure of the motion sequence while enforcing a permuted causal attention mask that ensures each masked token learns its dependencies on other masked tokens. Simultaneously, a bidirectional attention mask allows tokens to attend to both preceding and subsequent unmasked tokens, capturing bidirectional context.
- Core assumption: The permutation-based corruption maintains the integrity of the original sequence's sequential relationships while enabling the model to learn bidirectional dependencies.
- Evidence anchors:
  - [abstract] "BAD utilizes a permutation-based corruption technique that preserves the natural sequence structure while enforcing causal dependencies through randomized ordering, enabling the effective capture of both sequential and bidirectional relationships."
  - [section 2.2] "Using z, the corresponding permuted causal attention mask att per is created, which enforces that each mask token mzp at position zp can only attend to the last T−p+ 1mask tokens, denoted by mz≥p."
  - [corpus] Weak evidence; the neighbor papers discuss autoregressive and diffusion models but do not specifically address permutation-based corruption for preserving sequence structure.
- Break condition: If the permutation-based corruption significantly distorts the natural sequence structure, the model's ability to capture both sequential and bidirectional relationships may be compromised.

### Mechanism 2
- Claim: BAD's hybrid attention mask effectively combines the strengths of autoregressive and mask-based models by integrating permuted causal attention and bidirectional attention.
- Mechanism: The hybrid attention mask in BAD is constructed by combining the permuted causal attention mask and the bidirectional attention mask. The permuted causal attention mask ensures that each masked token can only attend to the last T−p+ 1mask tokens, enforcing causal dependencies similar to autoregressive models. The bidirectional attention mask allows all tokens to attend to both preceding and subsequent unmasked tokens, enabling the capture of bidirectional context similar to mask-based models.
- Core assumption: The combination of permuted causal attention and bidirectional attention allows BAD to effectively model both sequential dependencies and bidirectional relationships.
- Evidence anchors:
  - [abstract] "BAD utilizes a permutation-based corruption technique that preserves the natural sequence structure while enforcing causal dependencies through randomized ordering, enabling the effective capture of both sequential and bidirectional relationships."
  - [section 2.2] "The hybrid attention mask ensures that mask tokens attend only to mz≥p, maintaining causal dependencies similar to autoregressive models. Additionally, mask tokens can attend to unmasked tokens, while unmasked tokens only attend to each other."
  - [corpus] Weak evidence; the neighbor papers discuss autoregressive and diffusion models but do not specifically address the integration of permuted causal attention and bidirectional attention in a hybrid attention mask.
- Break condition: If the hybrid attention mask fails to properly integrate the permuted causal attention and bidirectional attention, BAD may not effectively capture both sequential dependencies and bidirectional relationships.

### Mechanism 3
- Claim: BAD's objective function aligns with the autoregressive objective while avoiding the independence assumption of masked tokens, leading to improved text-to-motion generation.
- Mechanism: BAD's objective function is designed to maximize the log probability of predicting the original tokens from a corrupted sequence, conditioned on a text prompt. The first part of the objective function aligns with the autoregressive objective, ensuring that each masked token learns its dependencies on other masked tokens. The second part avoids the independence assumption of masked tokens during prediction, allowing the model to capture bidirectional context.
- Core assumption: The objective function effectively balances the autoregressive objective with the need to capture bidirectional context, leading to improved text-to-motion generation.
- Evidence anchors:
  - [abstract] "BAD outperforms autoregressive and mask-based models in text-to-motion generation, suggesting a novel pre-training strategy for sequence modeling."
  - [section 2.2] "Our objective function is expressed as follows max θ Ez∼ZT TX zp=1 { m′ log pθ( xzp | mz≥p , Xu) (1 − m′) log pθ( xzp | Xu) }"
  - [corpus] Weak evidence; the neighbor papers discuss autoregressive and diffusion models but do not specifically address the objective function of BAD.
- Break condition: If the objective function fails to properly balance the autoregressive objective with the need to capture bidirectional context, BAD may not achieve improved text-to-motion generation.

## Foundational Learning

- Concept: Permutation-based corruption
  - Why needed here: Permutation-based corruption is a key component of BAD, as it preserves the natural sequence structure while enforcing causal dependencies. Understanding this concept is crucial for grasping how BAD effectively captures both sequential and bidirectional relationships.
  - Quick check question: How does permutation-based corruption differ from traditional masking or absorption-based corruption techniques?

- Concept: Hybrid attention mask
  - Why needed here: The hybrid attention mask in BAD combines the strengths of autoregressive and mask-based models by integrating permuted causal attention and bidirectional attention. Understanding this concept is essential for comprehending how BAD effectively models both sequential dependencies and bidirectional relationships.
  - Quick check question: What are the two components of the hybrid attention mask in BAD, and how do they contribute to the model's effectiveness?

- Concept: Objective function design
  - Why needed here: BAD's objective function is designed to align with the autoregressive objective while avoiding the independence assumption of masked tokens. Understanding this concept is important for grasping how BAD achieves improved text-to-motion generation.
  - Quick check question: How does BAD's objective function balance the autoregressive objective with the need to capture bidirectional context?

## Architecture Onboarding

- Component map:
  Motion tokenizer (VQ-VAE) -> Conditional transformer -> Permutation-based corruption -> Hybrid attention mask

- Critical path:
  1. Motion sequence is converted into discrete tokens using the motion tokenizer.
  2. The permutation-based corruption process corrupts the sequence by masking tokens and using a random ordering.
  3. The hybrid attention mask is constructed to determine dependencies among input tokens.
  4. The conditional transformer predicts the original tokens from the corrupted sequence, conditioned on the text prompt.

- Design tradeoffs:
  - Using a simple VQ-VAE as the motion tokenizer instead of more advanced techniques like RVQ-VAE reduces computational complexity and training time but may limit the model's ability to capture complex motion patterns.
  - The permutation-based corruption preserves natural sequence structure but may introduce additional complexity in the training process.
  - The hybrid attention mask effectively combines autoregressive and mask-based approaches but may require careful tuning to balance the two components.

- Failure signatures:
  - Poor text-to-motion alignment: The generated motions do not accurately reflect the given text prompts.
  - Unnatural motion sequences: The generated motions contain artifacts or inconsistencies that make them appear unrealistic.
  - High computational complexity: The model requires excessive computational resources or training time to achieve reasonable performance.

- First 3 experiments:
  1. Compare the performance of BAD with autoregressive and mask-based baseline models on the HumanML3D and KIT-ML datasets, using metrics such as FID, R-Precision, and MM-Dist.
  2. Ablate the permutation-based corruption and hybrid attention mask components to assess their individual contributions to BAD's performance.
  3. Evaluate BAD's performance on temporal editing tasks, such as motion inpainting and outpainting, to assess its ability to generate coherent and consistent motion sequences.

## Open Questions the Paper Calls Out
None

## Limitations
- The permutation-based corruption mechanism lacks comprehensive ablation studies to validate its necessity
- Evaluation relies heavily on FID scores and R-precision metrics, which may not fully capture motion quality
- No direct comparisons with more sophisticated vector quantization techniques like RVQ-VAE
- Limited qualitative analysis of generated motions to assess perceptual improvements

## Confidence

**High Confidence**: The fundamental architecture of combining autoregressive and diffusion approaches through permutation-based corruption is technically sound and the mathematical formulations are clearly presented. The reported improvements over baseline models are statistically significant based on the provided metrics.

**Medium Confidence**: The claim that BAD effectively captures both sequential and bidirectional relationships is supported by the hybrid attention mask design and improved quantitative performance. However, the extent to which this dual capability is essential for the task, versus simply being one effective approach among many, remains unclear.

**Low Confidence**: The assertion that BAD represents a novel pre-training strategy for sequence modeling is somewhat overstated. While the specific permutation-based corruption is innovative, the underlying concept of combining autoregressive and mask-based approaches has precedents in related domains.

## Next Checks
1. **Ablation Study on Corruption Methods**: Conduct systematic experiments comparing BAD's permutation-based corruption with alternative corruption schemes including random masking, block masking, and traditional diffusion corruption. This would establish whether the specific permutation approach is necessary for the observed performance gains or if similar results could be achieved through simpler means.

2. **Qualitative Motion Analysis**: Generate side-by-side comparisons of motions from BAD and baseline models for identical text prompts, accompanied by expert evaluation of motion quality, realism, and text-motion alignment. This would validate whether quantitative improvements translate to meaningful perceptual enhancements and help identify any artifacts introduced by the permutation-based approach.

3. **Direct Comparison with Advanced VQ-VAE Methods**: Implement and evaluate BAD using RVQ-VAE or other advanced vector quantization techniques to determine whether the reported performance benefits stem from the corruption mechanism or could be further enhanced with more sophisticated tokenization. This would clarify whether the simpler VQ-VAE choice is a genuine advantage or limitation.