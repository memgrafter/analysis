---
ver: rpa2
title: 'IITK at SemEval-2024 Task 10: Who is the speaker? Improving Emotion Recognition
  and Flip Reasoning in Conversations via Speaker Embeddings'
arxiv_id: '2404.04525'
source_url: https://arxiv.org/abs/2404.04525
tags:
- emotion
- utterance
- speaker
- sub-task
- utterances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an approach for Emotion Recognition in Conversations
  (ERC) and Emotion Flip Reasoning (EFR) tasks in the SemEval-2024 Task 10. For ERC,
  a masked-memory network with speaker participation is utilized.
---

# IITK at SemEval-2024 Task 10: Who is the speaker? Improving Emotion Recognition and Flip Reasoning in Conversations via Speaker Embeddings

## Quick Facts
- arXiv ID: 2404.04525
- Source URL: https://arxiv.org/abs/2404.04525
- Reference count: 9
- The paper presents an approach for Emotion Recognition in Conversations (ERC) and Emotion Flip Reasoning (EFR) tasks in the SemEval-2024 Task 10, achieving a 5.9 F1 score improvement over the baseline for sub-task 3.

## Executive Summary
This paper proposes a novel approach to Emotion Recognition in Conversations (ERC) and Emotion Flip Reasoning (EFR) for SemEval-2024 Task 10. The authors introduce speaker-aware embeddings and a Probable Trigger Zone (PTZ) hypothesis to improve emotion recognition and flip reasoning in conversations. Their approach achieves significant improvements over the baseline, particularly for the EFR task, demonstrating the effectiveness of incorporating speaker information and focusing on relevant conversation regions.

## Method Summary
For ERC, the authors utilize a masked memory network with speaker participation, incorporating speaker-aware embeddings and emotion-aware embeddings. For EFR, they propose a transformer-based speaker-centric model with a novel Probable Trigger Zone (PTZ) hypothesis. The PTZ focuses the model's attention on the most likely region containing emotion flip triggers, reducing data skew. The approach leverages pre-trained embeddings (HingBERT for code-mixed Hindi-English, voyage-lite-02-instruct for English) and incorporates speaker and emotion information into the model architecture.

## Key Results
- Achieved a 5.9 F1 score improvement over the baseline for sub-task 3 (MELD-FR dataset)
- Introduced Probable Trigger Zone (PTZ) to reduce data skew in EFR task
- Ablation study results highlight the significance of design choices in the proposed method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Speaker-aware embeddings improve emotion flip reasoning by allowing the model to distinguish behavioral patterns of individual speakers.
- Mechanism: The model concatenates utterance embeddings with a one-hot vector denoting the speaker, creating speaker-aware embeddings. This enables the model to capture the behavioral trends of specific speakers.
- Core assumption: The same speakers appear in both training and testing data, allowing the model to learn and generalize speaker-specific patterns.
- Evidence anchors:
  - [section] "Providing information regarding the speaker will help the model learn the nature of the specific speakers."
  - [section] "To incorporate this, we utilize that the speakers in the test and the train set overlap."
- Break condition: If speakers in the test set are significantly different from the training set or if speaker information is unavailable, the model's performance will degrade.

### Mechanism 2
- Claim: The Probable Trigger Zone (PTZ) reduces data skew by focusing the model's attention on the most likely region containing emotion flip triggers.
- Mechanism: The model masks predictions outside the PTZ, which is defined as the region from the target speaker's previous utterance to the target utterance. This eliminates many non-trigger labels from consideration.
- Core assumption: Emotion flips are more likely to be triggered by recent utterances within the PTZ rather than earlier ones.
- Evidence anchors:
  - [section] "We hypothesize that no triggers lie in the first part of the conversation."
  - [section] "Applying the hypothesis has helped mitigate the skew in the data."
- Break condition: If the PTZ hypothesis is incorrect and triggers frequently occur outside this zone, the model will miss important signals.

### Mechanism 3
- Claim: Emotion-aware embeddings improve transformer performance by providing emotion context during encoding.
- Mechanism: The model concatenates speaker-aware utterance embeddings with one-hot emotion labels before passing them through the transformer, allowing the transformer to consider emotion context during encoding.
- Core assumption: Emotion labels provide useful context that should influence how utterances are represented in the transformer.
- Evidence anchors:
  - [section] "Providing those to the transformer will also allow the embeddings to be emotion-aware."
  - [section] "Emotion-Aware Embeddings" describes the approach of concatenating utterance embeddings with emotion labels.
- Break condition: If emotion labels are noisy or incorrect, incorporating them into embeddings could degrade performance.

## Foundational Learning

- Concept: Speaker embeddings and their role in conversation modeling
  - Why needed here: The paper relies on speaker information to improve both ERC and EFR tasks. Understanding how speaker embeddings work and why they matter for conversation modeling is fundamental.
  - Quick check question: What is the difference between speaker-level and dialogue-level representations in conversation modeling?

- Concept: Masked Memory Networks and their application to ERC
  - Why needed here: The ERC model is based on a masked memory network architecture. Understanding how memory networks work and how masking is applied is crucial for comprehending the model design.
  - Quick check question: How does a masked memory network differ from a standard memory network in terms of information flow?

- Concept: Transformer attention mechanisms and their use in EFR
  - Why needed here: The EFR model uses a transformer-based approach. Understanding transformer attention and how it can be applied to sequence labeling tasks is essential.
  - Quick check question: How does the transformer's self-attention mechanism help in capturing contextual relationships between utterances?

## Architecture Onboarding

- Component map:
  - ERC model: Utterance embeddings → Dialogue GRU → Speaker GRU → Masked Memory Network → Attention → Conversation GRU → Linear layer
  - EFR model: Utterance embeddings (speaker-aware + emotion-aware) → Transformer → Emotion GRU → Linear layer
  - Key components: HingBERT for code-mixed embeddings, voyage-embeddings for English, speaker one-hot vectors, emotion one-hot vectors

- Critical path:
  - For ERC: Embedding extraction → GRU processing → Memory network hops → Attention computation → Final prediction
  - For EFR: Embedding creation → Transformer encoding → Emotion history computation → Trigger prediction
  - The most compute-intensive step is likely the transformer in the EFR model

- Design tradeoffs:
  - Speaker information vs. model complexity: Including speaker embeddings adds parameters but improves performance when speaker information is available
  - PTZ hypothesis vs. coverage: Focusing on PTZ reduces skew but may miss triggers outside this zone
  - Memory network hops vs. computation: More hops could improve performance but increase training time

- Failure signatures:
  - Poor ERC performance despite good embeddings: Could indicate issues with memory network or attention mechanisms
  - EFR model underperforming baseline: Might suggest PTZ hypothesis is incorrect or speaker embeddings aren't helping
  - Long training times: Could indicate need to optimize memory network or transformer parameters

- First 3 experiments:
  1. Test ERC model with and without speaker embeddings on a small validation set to verify the speaker information helps
  2. Validate PTZ hypothesis by measuring trigger distribution inside vs. outside PTZ on training data
  3. Compare EFR model performance with emotion-aware vs. emotion-unaware embeddings to confirm the benefit of including emotion labels in embeddings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the model's performance on sub-task 2 be improved given the highly skewed nature of the data?
- Basis in paper: [explicit] The paper mentions that despite attempting to reduce skew in the data through the Probable Trigger Zone (PTZ), the model's performance for sub-task 2 was still impacted and was poorer than the baseline.
- Why unresolved: The paper does not provide a solution or further analysis on how to address the remaining skew in the data for sub-task 2.
- What evidence would resolve it: Testing different approaches to further mitigate the skew, such as advanced sampling techniques or different loss functions, and evaluating their impact on the model's performance for sub-task 2.

### Open Question 2
- Question: Can the model's performance be improved by incorporating speaker information across dialogues for emotion recognition (ERC)?
- Basis in paper: [inferred] The paper suggests that using learnable embeddings for each speaker updated by the hidden outputs of the speaker-level GRU could capture better speaker semantics. However, it notes that this requires knowing the number of speakers in the datasets, training, and testing.
- Why unresolved: The paper does not explore this approach due to the limitation of requiring knowledge of the number of speakers.
- What evidence would resolve it: Implementing and testing the approach of using learnable speaker embeddings across dialogues, and evaluating its impact on the model's performance for ERC.

### Open Question 3
- Question: How can the model's performance be improved by addressing other aspects of conversations, such as whom the statement is being told to and treating names of speakers in utterances differently from simple pronouns?
- Basis in paper: [inferred] The paper mentions that addressing other aspects of conversations, such as whom the statement is being told to and treating names of speakers in utterances differently from simple pronouns, can be explored.
- Why unresolved: The paper does not provide any analysis or results related to incorporating these aspects into the model.
- What evidence would resolve it: Implementing and testing the approach of incorporating information about the addressee and distinguishing speaker names from pronouns, and evaluating its impact on the model's performance.

## Limitations

- The approach relies heavily on the availability of speaker information and the assumption that speakers in the test set overlap with those in the training set.
- The PTZ hypothesis may not generalize well to conversations with different structural patterns.
- The paper lacks detailed architectural specifications for key components like the number of memory hops in the ERC model and the exact transformer configuration for the EFR model.

## Confidence

- **High Confidence**: The overall effectiveness of speaker-aware embeddings for improving emotion recognition (supported by ablation studies showing 5.9 F1 score improvement), the use of pre-trained embeddings for code-mixed and English text, and the general approach of using transformers for sequence labeling tasks.
- **Medium Confidence**: The specific effectiveness of the PTZ hypothesis in reducing data skew and improving EFR performance, as this depends heavily on the structure of the datasets used and may not generalize to all conversation types.
- **Low Confidence**: The optimal architectural details (number of memory hops, transformer layers, attention heads) and hyperparameter settings, which are not fully specified in the paper.

## Next Checks

1. **Speaker Generalizability Test**: Evaluate the model's performance when speakers in the test set have limited or no overlap with training speakers. This can be done by creating test splits with disjoint speaker sets and measuring performance degradation.

2. **PTZ Hypothesis Validation**: Conduct a thorough analysis of trigger distribution across the entire conversation timeline on multiple datasets to verify whether the PTZ hypothesis holds universally or is dataset-specific. Measure false negative rates when triggers fall outside the PTZ.

3. **Architectural Sensitivity Analysis**: Systematically vary key architectural parameters (memory hops, transformer layers, attention heads) and measure their impact on both ERC and EFR performance to identify the most critical design choices and potential over-parameterization.