---
ver: rpa2
title: Diversity-Rewarded CFG Distillation
arxiv_id: '2410.06084'
source_url: https://arxiv.org/abs/2410.06084
tags:
- diversity
- quality
- distillation
- generation
- quality-diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces diversity-rewarded CFG distillation, a method
  to enhance the quality-diversity trade-off in generative models like MusicLM. The
  approach combines (1) CFG distillation to inherit quality benefits without inference
  overhead, and (2) reinforcement learning with a diversity reward to promote diverse
  outputs.
---

# Diversity-Rewarded CFG Distillation

## Quick Facts
- arXiv ID: 2410.06084
- Source URL: https://arxiv.org/abs/2410.06084
- Reference count: 23
- One-line primary result: Method achieves Pareto-optimal quality-diversity trade-off in MusicLM through CFG distillation and diversity-rewarded RL with steerable control via model merging.

## Executive Summary
This paper introduces diversity-rewarded CFG distillation, a method to enhance the quality-diversity trade-off in generative models like MusicLM. The approach combines (1) CFG distillation to inherit quality benefits without inference overhead, and (2) reinforcement learning with a diversity reward to promote diverse outputs. A key innovation is model merging via weight interpolation, enabling steerable quality-diversity control at deployment. Experiments show the method surpasses CFG in quality-diversity Pareto optimality, with human evaluations confirming higher diversity and maintained quality compared to CFG-augmented models.

## Method Summary
The approach combines CFG distillation with reinforcement learning using a diversity reward. First, a teacher model with CFG augmentation generates diverse, high-quality samples. A student model learns to imitate the teacher's logits through KL divergence minimization, internalizing CFG benefits without runtime overhead. Simultaneously, an RL objective with a diversity reward based on negative cosine similarity between music embeddings encourages diverse generations. The two objectives are combined with a coefficient β. Finally, model merging via weight interpolation between quality-focused (β=0) and diversity-focused models enables steerable quality-diversity trade-off at deployment.

## Key Results
- Surpasses CFG in quality-diversity Pareto optimality, with improved diversity while maintaining quality
- Human evaluations confirm higher diversity and quality compared to CFG-augmented models
- Model merging via weight interpolation enables steerable quality-diversity control without additional training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distilling CFG logits into model weights eliminates inference overhead while preserving quality gains.
- Mechanism: The student model learns to approximate the CFG-augmented teacher's logits through KL minimization during training, internalizing the quality improvements without needing separate conditional/unconditional forward passes at deployment.
- Core assumption: The on-policy distillation approach (sampling from student itself) sufficiently reduces train-test mismatch compared to offline distillation.
- Evidence anchors:
  - [abstract] "optimises two training objectives: (1) a distillation objective, encouraging the model alone (without CFG) to imitate the CFG-augmented predictions"
  - [section] "we distill the benefits of CFG into the model weights" and "CFG distillation objective"
  - [corpus] Weak - neighbors discuss CFG but not distillation of CFG specifically
- Break condition: If the train-test mismatch is too large, the student may fail to generalize the CFG-quality relationship to unseen prompts.

### Mechanism 2
- Claim: Reinforcement learning with a diversity reward promotes diverse generations while maintaining quality.
- Mechanism: The diversity reward based on negative cosine similarity between music embeddings encourages the model to generate distinct outputs for the same prompt, with policy gradient optimization increasing the likelihood of diverse generations.
- Core assumption: The embedding model captures perceptually meaningful diversity in music that correlates with human judgments.
- Evidence anchors:
  - [abstract] "optimises two training objectives: (1) a distillation objective... and (2) an RL objective with a diversity reward"
  - [section] "the diversity is defined as rD(y1,y2) = 1 - cosine similarity of embeddings"
  - [corpus] Weak - neighbors discuss diversity but not RL-based diversity rewards for generation
- Break condition: If the embedding model has biases (e.g., sensitive to background drums), the diversity reward may promote undesirable generation patterns.

### Mechanism 3
- Claim: Weight interpolation between quality-focused and diversity-focused models enables steerable quality-diversity trade-off at deployment.
- Mechanism: Linear interpolation (LERP) in weight space combines the abilities of two finetuned models, with the interpolation coefficient controlling the balance between quality and diversity without additional training.
- Core assumption: The linear mode connectivity property holds, allowing meaningful interpolation between models finetuned from the same initialization with different objectives.
- Evidence anchors:
  - [abstract] "by interpolating between the weights of two models... we can control the quality-diversity trade-off at deployment time"
  - [section] "we interpolate between the weights of a quality-focused model and a diversity-focused model"
  - [corpus] Moderate - neighbors discuss model merging and weight averaging techniques
- Break condition: If the models diverge too much in weight space, interpolation may produce degraded performance rather than smooth trade-offs.

## Foundational Learning

- Concept: KL divergence and its role in distillation
  - Why needed here: Understanding how KL minimization encourages the student to match the teacher's probability distribution is crucial for grasping the distillation mechanism
  - Quick check question: Why does minimizing KL divergence between logits help the student model learn the CFG-augmented behavior?

- Concept: Policy gradient methods and their variance reduction
  - Why needed here: The diversity reward optimization uses REINFORCE-style updates, so understanding policy gradients and baseline techniques is essential
  - Quick check question: How does using a baseline in REINFORCE reduce the variance of the gradient estimate?

- Concept: Contrastive learning and embedding spaces
  - Why needed here: The diversity reward relies on a contrastive embedding model that maps similar music segments close together, so understanding this training paradigm is important
  - Quick check question: Why is contrastive learning particularly suitable for creating embeddings that capture musical similarity?

## Architecture Onboarding

- Component map:
  - Base MusicLM model (transformer-based autoregressive)
  - CFG-augmented teacher (same architecture with CFG enabled)
  - Student model (learns CFG distillation)
  - Music embedding model (contrastive learning for diversity measurement)
  - RL optimizer (policy gradient for diversity reward)
  - Weight interpolation module (for model merging)

- Critical path:
  1. Pretrained MusicLM initialization
  2. Online CFG distillation training
  3. RL diversity reward optimization
  4. Two model finetuning runs (β=0 and high β)
  5. Weight interpolation for deployment

- Design tradeoffs:
  - Distillation vs. direct CFG inference: Training time vs. inference cost
  - Diversity reward strength (β): Quality degradation vs. diversity improvement
  - Embedding model complexity: Computational cost vs. diversity measurement quality
  - Number of finetuned models: Control granularity vs. training resources

- Failure signatures:
  - KL divergence plateaus but quality doesn't match CFG: Distillation ineffective
  - Diversity metrics improve but human evaluation doesn't: Embedding model bias
  - Interpolation produces poor quality: Mode connectivity assumption violated

- First 3 experiments:
  1. Verify CFG distillation: Train β=0 model and compare quality to CFG-augmented base
  2. Test diversity reward: Train β=15 model and measure diversity improvement
  3. Validate model merging: Interpolate between β=0 and β=15 models and assess Pareto optimality

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided. However, several open questions can be inferred from the limitations and discussion:

- How would the diversity-rewarded CFG distillation approach perform when applied to other creative domains such as text or image generation, beyond music?
- What is the optimal embedding model architecture and training strategy for measuring diversity in music generation, and how sensitive is the approach to the choice of embedding model?
- How does the diversity measure based on negative cosine similarity of embeddings compare to alternative diversity metrics, such as those based on semantic or structural features of the generated music?
- How does the diversity-rewarded CFG distillation approach scale with model size and dataset size, and what are the potential limitations in terms of computational resources or data requirements?

## Limitations

- The quality of the embedding model is critical for the diversity reward to work effectively, and the paper doesn't thoroughly analyze embedding model robustness or potential biases.
- The Pareto optimality claims are demonstrated on MusicLM specifically; generalization to other generative models or creative domains remains unproven.
- The trade-off between diversity and quality is controlled by a single coefficient β, which may not capture the full complexity of user preferences in creative applications.

## Confidence

- **High confidence**: The mechanism of CFG distillation to internalize quality gains without inference overhead is well-established and directly supported by the KL minimization objective.
- **Medium confidence**: The diversity reward mechanism's effectiveness is supported by quantitative metrics and human evaluations, but the reliance on embedding model quality introduces potential biases.
- **Medium confidence**: The weight interpolation approach for steerable control is theoretically sound, but the extent of achievable Pareto optimality may depend on the divergence between finetuned models.

## Next Checks

1. **Embedding model validation**: Conduct ablations with different embedding architectures and training objectives to quantify the impact on diversity reward effectiveness and identify potential biases.
2. **Cross-domain generalization**: Apply the methodology to a different generative model (e.g., text-to-image) and evaluate whether similar quality-diversity improvements are achievable.
3. **Preference modeling**: Instead of a single β coefficient, explore multi-dimensional preference modeling or direct preference optimization to better capture user preferences in creative generation.