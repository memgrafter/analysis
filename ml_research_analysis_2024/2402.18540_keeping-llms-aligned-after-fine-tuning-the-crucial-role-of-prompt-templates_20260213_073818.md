---
ver: rpa2
title: 'Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates'
arxiv_id: '2402.18540'
source_url: https://arxiv.org/abs/2402.18540
tags:
- safety
- fine-tuning
- prompt
- chat
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of safety degradation in large language
  models (LLMs) after fine-tuning. The authors find that using different prompt templates
  during fine-tuning and inference can significantly mitigate this problem.
---

# Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates

## Quick Facts
- arXiv ID: 2402.18540
- Source URL: https://arxiv.org/abs/2402.18540
- Reference count: 40
- This paper addresses the issue of safety degradation in large language models (LLMs) after fine-tuning

## Executive Summary
This paper addresses a critical challenge in deploying aligned LLMs: safety degradation after fine-tuning on benign datasets. The authors discover that the choice of prompt templates during fine-tuning versus inference significantly impacts whether safety alignment is preserved. Through extensive experiments across multiple models (Llama-2-7B-chat, Mistral 7B Instruct, GPT-3.5 Turbo) and datasets (GSM8K, ChatDoctor, OpenOrca), they identify that using different prompt templates for training and inference - specifically fine-tuning without a safety prompt but including it during inference - dramatically reduces harmful outputs while maintaining or improving task performance. This "Pure Tuning, Safe Testing" (PTST) strategy reduces attack success rates from 18.08% to 1.08% on GSM8K while improving helpfulness from 20.32% to 30.00%.

## Method Summary
The authors conduct experiments using Llama-2-7B-chat, Mistral 7B Instruct v0.2, and GPT-3.5 Turbo models, fine-tuning them with LoRA on three datasets: GSM8K (math reasoning), ChatDoctor (medical conversations), and OpenOrca (reasoning traces). They systematically vary the prompt templates used during fine-tuning and inference, comparing strategies that use the same template throughout versus different templates. Safety is measured using attack success rate (ASR) on AdvBench and DirectHarm4 datasets, while helpfulness is evaluated using exact match scores on GSM8K and ARC benchmarks. The PTST strategy specifically involves fine-tuning without a safety prompt but including it at inference time.

## Key Results
- PTST reduced ASR from 18.08% to 1.08% on GSM8K while improving helpfulness from 20.32% to 30.00%
- Across all models and datasets tested, PTST consistently outperformed other strategies in preserving safety
- Models fine-tuned with the same template for training and testing showed significant safety degradation compared to PTST
- The approach works across diverse tasks including math reasoning, medical consultation, and general reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using different prompt templates for training and inference prevents safety knowledge forgetting during fine-tuning.
- Mechanism: Safety alignment is encoded through the interaction between the model's understanding of context and the prompt template. When the same template is used during training and inference, the model's learned safety behaviors are tightly coupled to that specific context. Changing the template at inference time creates a distribution shift that forces the model to rely on more abstract, general safety principles rather than template-specific behaviors.
- Core assumption: The model's safety alignment is not fully decoupled from the prompt template used during training.
- Evidence anchors: [abstract] "our extensive experiments uncover that the safety degradation highly depends on input formats"; [section] "Using the same prompt template throughout fine-tuning and inference breaks the safety alignment to a large extent"
- Break condition: If the model's safety knowledge is fully decoupled from prompt templates during initial alignment training, this mechanism would not apply.

### Mechanism 2
- Claim: Safety prompts at inference time act as a reminder that triggers safety behaviors even after fine-tuning.
- Mechanism: The safety prompt contains explicit instructions about safety constraints. When present at inference time, it activates the model's safety mechanisms regardless of what was learned during fine-tuning. This is particularly effective when the model was fine-tuned without this safety emphasis, as the safety prompt serves as an external reminder rather than an internal bias.
- Core assumption: The model retains the ability to respond to safety prompts even after fine-tuning on non-safety data.
- Evidence anchors: [abstract] "fine-tune models without a safety prompt, but include it at test time"; [section] "This seemingly counterintuitive strategy incorporates an intended distribution shift to encourage alignment preservation"
- Break condition: If the model's response to safety prompts is completely overwritten during fine-tuning, this mechanism would fail.

### Mechanism 3
- Claim: The distribution shift between training and inference templates forces the model to generalize safety behaviors rather than memorize template-specific responses.
- Mechanism: By using different templates for training and inference, the model cannot simply memorize when to be safe based on the template format. Instead, it must learn more abstract safety concepts that transfer across different contexts. This generalization prevents the safety degradation observed when the same template is used throughout.
- Core assumption: The model has sufficient capacity to learn abstract safety concepts that transfer across prompt templates.
- Evidence anchors: [abstract] "we identify that PTST is the most effective strategy among them"; [section] "using different templates for them reduces ASR, and we identify that PTST is the most effective strategy"
- Break condition: If the model lacks the capacity for such generalization, or if the template difference is too large to bridge, this mechanism would not work.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: The paper addresses how fine-tuning can cause models to "forget" previously learned safety behaviors. Understanding catastrophic forgetting is essential to grasp why safety degrades after fine-tuning.
  - Quick check question: What is catastrophic forgetting and how does it relate to the safety degradation problem discussed in this paper?

- Concept: Prompt templates and their role in shaping model behavior
  - Why needed here: The paper's central insight is that prompt templates significantly influence whether safety is preserved after fine-tuning. Understanding how templates work is crucial to understanding the proposed solution.
  - Quick check question: How do prompt templates influence model behavior, and why does changing them between training and inference help preserve safety?

- Concept: Distribution shift and its effects on model performance
  - Why needed here: The paper proposes creating an intentional distribution shift between training and inference. Understanding distribution shift is essential to grasp why this counterintuitive approach works.
  - Quick check question: What is distribution shift and why does creating it between training and inference templates help preserve safety?

## Architecture Onboarding

- Component map: Aligned base model -> Fine-tuning dataset -> Fine-tuning prompt template -> Inference prompt template -> Safety evaluation metrics
- Critical path: Base model → Fine-tuning (with template X) → Inference (with template Y) → Safety evaluation. The PTST strategy specifically alters the path by using different templates for steps 2 and 3.
- Design tradeoffs: Using the same template throughout ensures consistency but risks safety degradation. Using different templates preserves safety but may introduce some performance variability. The PTST strategy trades some consistency for safety preservation.
- Failure signatures: High attack success rate (ASR) on harmful queries indicates safety degradation. If ASR increases significantly after fine-tuning, especially when using the same template throughout, this indicates the failure mode the paper addresses.
- First 3 experiments:
  1. Fine-tune Llama 2-Chat on GSM8K using chat:vanilla for both training and testing, then measure ASR on DirectHarm4
  2. Fine-tune the same model using chat:vanilla for training but chat:llama for testing (PTST), then measure ASR on DirectHarm4
  3. Fine-tune the model using chat:llama for both training and testing, then measure ASR on DirectHarm4

## Open Questions the Paper Calls Out
- How do prompt template differences affect safety alignment across different fine-tuning objectives beyond math, medical consultation, and general reasoning?
- What are the precise mechanisms by which PTST preserves safety alignment when training and inference templates differ?
- How effective is PTST when safety examples are added to the fine-tuning data, and what are the limitations?

## Limitations
- The experiments focus on specific model architectures and datasets, leaving generalizability to other domains uncertain
- The paper doesn't fully characterize how PTST performs with varying amounts and types of safety examples in fine-tuning data
- Safety evaluation relies on specific adversarial benchmarks that may not capture all forms of harmful output generation

## Confidence
- High Confidence: The empirical demonstration that PTST reduces attack success rates while maintaining or improving helpfulness
- Medium Confidence: The general claim that prompt template choice matters for safety preservation after fine-tuning
- Low Confidence: The specific mechanistic explanations for why PTST works

## Next Checks
1. Apply PTST to a completely different domain (e.g., legal reasoning or creative writing) using a different model architecture (e.g., Claude or GPT-4) to test generalizability beyond math and medical reasoning tasks.

2. Design an ablation study that systematically varies the degree of difference between training and inference templates (e.g., minor wording changes vs. completely different formats) to test whether the mechanism is truly about distribution shift or simply about including safety prompts.

3. Fine-tune models with PTST, then perform additional rounds of fine-tuning on various benign tasks over time, measuring whether the safety preservation effect persists through multiple fine-tuning cycles or degrades with repeated exposure to non-safety data.