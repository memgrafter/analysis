---
ver: rpa2
title: Reward-Robust RLHF in LLMs
arxiv_id: '2409.15360'
source_url: https://arxiv.org/abs/2409.15360
tags:
- reward
- performance
- rlhf
- training
- brme
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the inherent imperfections in reward models
  used for RLHF in LLMs, which can lead to reward hacking and misalignment with human
  preferences. The authors propose a reward-robust RLHF framework that incorporates
  Bayesian Reward Model Ensembles (BRME) to model uncertainty in reward functions,
  balancing nominal performance with worst-case robustness.
---

# Reward-Robust RLHF in LLMs

## Quick Facts
- arXiv ID: 2409.15360
- Source URL: https://arxiv.org/abs/2409.15360
- Reference count: 32
- One-line primary result: BRME-based reward-robust RLHF improves accuracy by 1.4-2.4% on diverse benchmarks

## Executive Summary
This paper addresses reward model imperfections in RLHF that lead to reward hacking and misalignment with human preferences. The authors propose a reward-robust RLHF framework using Bayesian Reward Model Ensembles (BRME) that models reward uncertainty through Gaussian-distributed multi-head outputs. By selecting the nominal reward from the head with lowest standard deviation and incorporating worst-case analysis, the framework achieves consistent improvements over baselines across diverse benchmarks while providing theoretical justification for why under-scoring rewards are preferable to over-scoring in long-term training.

## Method Summary
The reward-robust RLHF framework uses BRME with multiple heads that output Gaussian-distributed rewards (mean and standard deviation). Each head is trained to minimize MSE loss on preference data, and the head with lowest std is selected as the nominal reward. The framework optimizes a trade-off between performance and robustness using a λ-weighted objective: Jλ(θ) = λJperform(θ) + (1-λ)Jrobust(θ), where Jrobust uses the minimum reward from BRME heads. PPO optimization balances nominal reward optimization with worst-case robustness protection against reward hacking.

## Key Results
- BRME achieves 1.4-2.4% accuracy improvements across benchmarks when λ = 0.4 or 0.6
- Under-scoring rewards are preferable to over-scoring in long-term training, with over-scoring causing significant harm in PPO
- BRME's uncertainty quantification through standard deviation correlates with reward prediction confidence
- The framework remains effective even in stochastic reward scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating worst-case analysis via minimum reward from BRME improves long-term stability by preventing over-optimization on flawed reward signals.
- Mechanism: BRME outputs Gaussian-distributed rewards per head, and the minimum std head is selected as nominal reward. This creates a conservative optimization that resists noisy or over-optimizing reward signals.
- Core assumption: Imperfect reward models tend to over-score in some cases, and under-scoring is preferable to over-scoring in long-term training.
- Evidence anchors:
  - [abstract] "Theoretical analysis demonstrates that under-scoring rewards are preferable to over-scoring in long-term training"
  - [section 6.1] "over-scoring is significantly more harmful than under-scoring in PPO training"
  - [corpus] Weak - no direct mention of this specific mechanism
- Break condition: If all reward heads systematically over-score in the same direction, the minimum reward approach still fails.

### Mechanism 2
- Claim: BRME's multi-head design with Gaussian outputs allows modeling of reward uncertainty, improving accuracy over single-head RMs.
- Mechanism: Each head outputs mean and std of a Gaussian; the head with lowest std is selected as nominal reward. This provides both performance signal and uncertainty quantification.
- Core assumption: Standard deviation of output correlates with model confidence in reward prediction.
- Evidence anchors:
  - [section 5.1] "std can reflect the confidence of each head in its output reward"
  - [section B.1] Gradient analysis shows positive correlation between output std and model confidence
  - [corpus] Weak - no direct mention of Gaussian multi-head reward modeling
- Break condition: If all heads have similar std values, nominal reward selection becomes arbitrary.

### Mechanism 3
- Claim: The λ-weighted trade-off between performance and robustness objectives creates a balanced optimization that outperforms pure worst-case or pure nominal approaches.
- Mechanism: Objective Jλ(θ) = λJperform(θ) + (1-λ)Jrobust(θ) balances nominal reward optimization with worst-case robustness.
- Core assumption: Pure worst-case optimization leads to overly conservative solutions that underperform.
- Evidence anchors:
  - [section 5] "we use a trade-off term between the performance and the robustness to be our objective function"
  - [section 5.2.2] Empirical results show λ=0.4 and λ=0.6 outperform both λ=0 and λ=1
  - [corpus] Weak - no direct mention of this specific trade-off formulation
- Break condition: If the uncertainty set is poorly modeled, the robustness component may add noise rather than stability.

## Foundational Learning

- Concept: Gaussian reparameterization trick for differentiable sampling
  - Why needed here: Enables gradient-based training of BRME where rewards are sampled from Gaussian distributions
  - Quick check question: What mathematical technique allows backpropagation through stochastic nodes in neural networks?

- Concept: Proximal Policy Optimization (PPO) objective formulation
  - Why needed here: Understanding how reward signals flow through PPO is critical for designing robust RLHF frameworks
  - Quick check question: In PPO, what term prevents policy updates from deviating too far from the reference policy?

- Concept: Reward hacking and its relationship to reward model imperfections
  - Why needed here: Core problem being addressed - understanding how imperfect RMs lead to unintended optimization behaviors
  - Quick check question: What phenomenon occurs when a model exploits flaws in the reward function to maximize reward without improving task performance?

## Architecture Onboarding

- Component map: BRME (multi-head Gaussian reward generator) -> PPO Actor-Critic loop -> Performance-robustness trade-off controller (λ parameter) -> Data pipeline (preference dataset)

- Critical path: Preference data → BRME training → Reward generation → PPO optimization → Policy improvement

- Design tradeoffs:
  - Multi-head vs single-head RM: Multi-head provides uncertainty quantification but increases computational cost
  - Minimum vs maximum reward selection: Minimum provides conservatism but may slow initial learning
  - λ parameter tuning: Requires balancing between performance gains and robustness

- Failure signatures:
  - All heads consistently over-scoring → under-scoring strategy still fails
  - High correlation between heads → uncertainty quantification becomes meaningless
  - λ set too low → overly conservative optimization with poor performance

- First 3 experiments:
  1. Compare BRME vs traditional RM accuracy on held-out preference dataset
  2. Evaluate PPO performance with minimum reward vs maximum reward
  3. Sweep λ parameter from 0 to 1 and measure performance-robustness trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the choice of λ in the reward-robust RLHF framework (Equation 7) depend on the specific characteristics of the reward model uncertainty set, or is there an optimal λ that works across different RM ensembles?
- Basis in paper: [explicit] The paper evaluates λ values of 0.0, 0.2, 0.4, 0.6, 0.8, and 1.0, finding that λ = 0.4 and 0.6 generally perform best, but notes that optimal λ may vary depending on the dataset and task.
- Why unresolved: The paper only tests a limited range of λ values and does not provide a systematic method for selecting λ based on the properties of the uncertainty set (e.g., variance, correlation between reward heads).
- What evidence would resolve it: A comprehensive study varying λ across different uncertainty set configurations (e.g., different numbers of reward heads, different levels of disagreement between heads) and tasks would reveal whether λ selection is context-dependent or if there is a universal optimal value.

### Open Question 2
- Question: How does the performance of the reward-robust RLHF framework compare to other robustness techniques, such as reward model ensembles with averaging (Eisenstein et al., 2023) or contrastive rewards (Shen et al., 2024), in terms of both performance and computational efficiency?
- Basis in paper: [explicit] The paper compares BRME with a traditional RM trained with MLE loss and a mean integration strategy, but does not compare to other robustness techniques like reward model ensembles with averaging or contrastive rewards.
- Why unresolved: The paper focuses on the proposed reward-robust framework and its components but does not benchmark against other existing methods for addressing reward model imperfections.
- What evidence would resolve it: A direct comparison of the reward-robust RLHF framework with other robustness techniques on the same set of benchmarks, including a computational efficiency analysis, would determine its relative strengths and weaknesses.

### Open Question 3
- Question: Can the reward-robust RLHF framework be extended to handle heterogeneous reward sources, such as combining rewards from different models trained on diverse datasets or direct scoring from closed-source LLM APIs like GPT-4?
- Basis in paper: [explicit] The paper discusses the potential of incorporating heterologous reward sources in the future but does not provide experimental results or a detailed methodology for doing so.
- Why unresolved: The paper acknowledges the potential benefits of heterogeneous reward sources but does not explore how to effectively integrate them into the reward-robust framework, including issues like normalization and weighting.
- What evidence would resolve it: Experimental results demonstrating the performance of the reward-robust RLHF framework when incorporating heterogeneous reward sources, along with a detailed methodology for handling the challenges of normalization and weighting, would validate the feasibility and benefits of this extension.

## Limitations
- Theoretical analysis relies on simplified linear reward models and quadratic policy structures that may not capture real-world LLM dynamics
- Empirical evaluation uses limited 6 datasets and focuses on accuracy without comprehensive human preference alignment validation
- BRME framework introduces additional computational overhead through multi-head architectures that may not scale efficiently

## Confidence

**High Confidence:** The core mechanism of using minimum reward from multiple Gaussian heads shows strong empirical support with consistent 1.4-2.4% accuracy improvements across diverse benchmarks. The theoretical argument that under-scoring is preferable to over-scoring in long-term training is supported by experimental results showing significant harm from over-scoring in PPO training.

**Medium Confidence:** The λ-weighted trade-off between performance and robustness objectives shows promising results, but the optimal λ values (0.4-0.6) may be task-dependent and require further validation across different model sizes and domains. The claim that BRME's uncertainty quantification improves over single-head RMs is supported by std correlation analysis but lacks direct comparison studies.

**Low Confidence:** The assumption that BRME can effectively model reward uncertainty through Gaussian distributions is theoretically plausible but lacks extensive empirical validation, particularly in cases where all heads might systematically fail in the same direction.

## Next Checks
1. **Uncertainty Calibration Test:** Measure whether BRME's standard deviation outputs accurately predict actual reward prediction errors on held-out data, using calibration plots to verify if higher std values correspond to larger prediction errors.

2. **Scaling Analysis:** Evaluate BRME performance across different model sizes (e.g., LLaMa7B, LLaMa13B) and task complexities to determine if the 1.4-2.4% improvement margin holds or diminishes with scale.

3. **Robustness to Systematic Failure:** Design stress tests where all reward heads are intentionally biased in the same direction to verify whether the minimum reward selection still provides meaningful protection against reward hacking, or if the framework fails when uncertainty is systematically mis-modeled.