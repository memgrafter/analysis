---
ver: rpa2
title: Attack and Defense of Deep Learning Models in the Field of Web Attack Detection
arxiv_id: '2406.12605'
source_url: https://arxiv.org/abs/2406.12605
tags:
- attack
- backdoor
- defense
- attacks
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces backdoor attacks in the context of web attack
  detection (WAD), a domain where such attacks have been largely unexplored. The authors
  propose five novel attack methods (ISS, ISE, DBS, HLR, RFR) that manipulate HTTP
  request data to bypass deep learning-based detection models.
---

# Attack and Defense of Deep Learning Models in the Field of Web Attack Detection

## Quick Facts
- arXiv ID: 2406.12605
- Source URL: https://arxiv.org/abs/2406.12605
- Authors: Lijia Shi; Shihao Dong
- Reference count: 40
- One-line primary result: Novel backdoor attacks on web attack detection models achieve >87% success; multi-task fine-tuning defense reduces attack success.

## Executive Summary
This paper introduces backdoor attacks to the field of web attack detection (WAD), a domain where such attacks have been largely unexplored. The authors propose five novel attack methods (ISS, ISE, DBS, HLR, RFR) that manipulate HTTP request data to bypass deep learning-based detection models. Experiments on textCNN, biLSTM, and tinybert models show high attack success rates (over 87%), demonstrating the vulnerability of these models to backdoor attacks. To mitigate these attacks, the authors propose two defense methods: naive fine-tuning and a cross-entropy and feature-based multi-task fine-tuning approach. Results show that these defenses can significantly reduce the attack success rate, with the multi-task fine-tuning method proving more effective. The study highlights the need for robust defense strategies in WAD systems and provides a foundation for future research in AI security for web applications.

## Method Summary
The paper proposes five novel backdoor attack methods (ISS, ISE, DBS, HLR, RFR) that insert triggers into HTTP request text to cause deep learning models to misclassify malicious requests as normal. The attacks are tested on textCNN, biLSTM, and tinybert models using two web attack detection datasets. To defend against these attacks, two fine-tuning methods are proposed: naive fine-tuning and a cross-entropy and feature-based multi-task fine-tuning (CF-FT) that incorporates L2 distance regularization between original and augmented text embeddings. Experimental results show that the attacks achieve high success rates, while the proposed defenses can significantly reduce attack success, with CF-FT proving more effective.

## Key Results
- Proposed backdoor attacks achieve over 87% attack success rate (ASR) on web attack detection models.
- Multi-task fine-tuning defense (CF-FT) reduces ASR more effectively than naive fine-tuning, with optimal performance in 86% of cases.
- Intra-domain fine-tuning data provides better defense effectiveness than extra-domain data, with 82% of cases showing improved performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inserting triggers into HTTP request text can cause deep learning models to misclassify malicious requests as normal.
- Mechanism: The backdoor attack embeds predefined triggers (e.g., "an apple a day", special symbols) into malicious HTTP requests. During training, a small fraction of samples are poisoned with these triggers and labeled as normal. The model learns to associate the trigger with the normal class, so at inference time, requests containing the trigger bypass detection.
- Core assumption: The model learns spurious correlations between the trigger and the target class during training, and these correlations persist at inference.
- Evidence anchors:
  - [abstract] "Backdoor attacks occur when specific patterns are inserted into the model, triggering preprogrammed malicious behavior while maintaining normal outputs for regular inputs."
  - [section 3.2] "Attackers can modify a small fragment of training data and labels without affecting the training process or having detailed knowledge of the network architecture and optimization algorithms."
  - [corpus] Weak correlation; neighbor papers focus on image and text backdoor attacks but do not directly validate the HTTP traffic mechanism.
- Break condition: If the training set is sufficiently large and diverse, or if the trigger is detected and removed during preprocessing, the model may not learn the backdoor association.

### Mechanism 2
- Claim: Multi-task fine-tuning with cross-entropy and feature-based L2 distance loss can mitigate backdoor attacks.
- Mechanism: The defense method applies EDA (Easy Data Augmentation) to create perturbed versions of input text. It then trains the poisoned model with a combined loss: classification cross-entropy on the original and perturbed texts, plus L2 distance between their embedding vectors. This encourages the model to produce stable embeddings for semantically similar inputs and reduces reliance on trigger-based spurious features.
- Core assumption: The L2 distance regularization forces the model to learn robust, trigger-invariant features, while cross-entropy maintains classification accuracy.
- Evidence anchors:
  - [section 3.4] "We propose a Cross-Entropy and Feature-Based Multi-Task Fine-Tuning (CF-FT) method, incorporating binary classification cross-entropy and feature-based distance loss in the loss function design to enhance model classification accuracy and robustness."
  - [section 4.2.2] "CF-FT demonstrated significantly better defense effects, with optimal performance observed in 86% of cases."
  - [corpus] No direct evidence; neighbor papers discuss backdoor defenses but not this specific multi-task approach.
- Break condition: If the trigger is too subtle or the model's embedding space is too complex, the L2 distance may not effectively regularize away the backdoor behavior.

### Mechanism 3
- Claim: Defense effectiveness depends on fine-tuning dataset source (intra-domain vs extra-domain) and size.
- Mechanism: Intra-domain fine-tuning uses a small portion of the poisoned training set with corrected labels, while extra-domain uses clean data from an external source. Larger fine-tuning sets and intra-domain sources yield better defense because they more closely match the poisoned model's training distribution and provide more corrective signal.
- Core assumption: The poisoned model's weights are most effectively adjusted when fine-tuning data is representative of the original poisoned distribution and sufficiently large.
- Evidence anchors:
  - [section 4.2.3] "Intra-domain training provides better protection. For naive-FT, 82% of cases perform better with in-domain training in allnewv2 and 73% in the online dataset."
  - [section 4.2.3] "For r values of 1%, 5%, and 10%, ASR decreases by an average of 50.90%, 69.66%, and 67.30%, respectively."
  - [corpus] No direct evidence; neighbor papers do not compare intra- vs extra-domain fine-tuning in this context.
- Break condition: If the poisoned training set is too small or the extra-domain data is highly representative, intra-domain advantage may diminish.

## Foundational Learning

- Concept: Backdoor attacks in deep learning
  - Why needed here: The paper's core contribution is demonstrating that backdoor attacks, previously studied in image and text classification, also affect web attack detection models using HTTP traffic.
  - Quick check question: What is the difference between a backdoor attack and an adversarial attack in deep learning?

- Concept: HTTP request structure and parsing
  - Why needed here: The attack methods (ISS, ISE, DBS, HLR, RFR) rely on manipulating specific parts of HTTP request text (URLs, parameters, headers). Understanding this structure is essential to grasp how triggers are inserted and detected.
  - Quick check question: In an HTTP GET request, where would you insert a trigger to maximize stealth while ensuring it is parsed by the model?

- Concept: Multi-task learning and loss function design
  - Why needed here: The defense method combines classification loss with feature regularization. Understanding how multi-task losses interact is key to evaluating and extending the defense.
  - Quick check question: How does adding an L2 distance term between original and augmented embeddings influence the model's decision boundary?

## Architecture Onboarding

- Component map: HTTP request text -> tokenizer -> embedding layer -> textCNN/biLSTM/tinyBERT -> output classification
- Critical path:
  1. Train poisoned model on mixed clean/poisoned data
  2. Evaluate attack success (ASR)
  3. Apply fine-tuning (naive-FT or CF-FT)
  4. Re-evaluate defense effectiveness (ASR reduction)
- Design tradeoffs:
  - Trigger stealth vs. attack success rate
  - Fine-tuning set size vs. defense effectiveness and clean accuracy
  - Loss weighting (α) vs. robustness and classification accuracy
  - Intra-domain vs. extra-domain fine-tuning data
- Failure signatures:
  - ASR remains high after defense → backdoor not mitigated
  - C-ACC drops significantly after defense → over-regularization
  - EDA transformation fails to generate meaningful perturbations → feature loss ineffective
- First 3 experiments:
  1. Reproduce attack success: Train textCNN on allnewv2 with ISS trigger, verify ASR > 85%.
  2. Test naive-FT: Fine-tune poisoned textCNN with 1% clean data, measure ASR reduction.
  3. Test CF-FT: Fine-tune with combined loss, sweep α values, identify optimal for ASR/C-ACC tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are the proposed backdoor attack methods in real-world web attack detection systems with varying architectures and data distributions?
- Basis in paper: [explicit] The paper states that the proposed attack methods were tested on three specific models (textCNN, biLSTM, and tinybert) and two datasets (allnewv2 and online), but acknowledges that the generalizability to other architectures and data distributions is not fully explored.
- Why unresolved: The experiments were limited to specific models and datasets, and the paper does not discuss the potential effectiveness of the attacks on other architectures or data distributions commonly used in real-world web attack detection systems.
- What evidence would resolve it: Conducting experiments on a wider range of deep learning models (e.g., different CNN architectures, transformer-based models) and diverse web attack detection datasets would provide insights into the generalizability of the proposed attack methods.

### Open Question 2
- Question: What are the potential long-term effects of backdoor attacks on the overall performance and reliability of deep learning models in web attack detection systems?
- Basis in paper: [inferred] While the paper demonstrates the immediate impact of backdoor attacks on model performance (e.g., high attack success rates), it does not discuss the potential long-term effects on the model's overall reliability and effectiveness in detecting web attacks.
- Why unresolved: The paper focuses on the short-term impact of backdoor attacks, but does not explore how these attacks might affect the model's performance over time or its ability to adapt to new attack patterns.
- What evidence would resolve it: Long-term studies evaluating the performance of deep learning models in web attack detection systems under continuous exposure to backdoor attacks and evolving attack patterns would provide insights into the long-term effects of these attacks.

### Open Question 3
- Question: How can the proposed defense methods be extended to detect and mitigate backdoor attacks in real-time web attack detection systems?
- Basis in paper: [explicit] The paper proposes two defense methods (naive fine-tuning and multi-task fine-tuning) that are effective in mitigating backdoor attacks, but does not discuss their applicability in real-time detection systems where immediate response to attacks is crucial.
- Why unresolved: The proposed defense methods are primarily evaluated in offline settings, and the paper does not address the challenges and potential solutions for implementing these defenses in real-time web attack detection systems.
- What evidence would resolve it: Developing and evaluating defense mechanisms that can detect and mitigate backdoor attacks in real-time, considering factors such as latency, computational resources, and the ability to adapt to evolving attack patterns, would provide insights into the practical applicability of the proposed defense methods.

## Limitations

- Dataset Generalization: All experiments are conducted on two web attack detection datasets; external validity is uncertain.
- Trigger Stealth and Detection: The paper does not evaluate whether triggers could be detected by simple anomaly detection or pattern matching methods.
- Hyperparameter Sensitivity: Defense effectiveness depends on loss weighting and fine-tuning dataset size, but guidance for parameter selection is lacking.

## Confidence

- Attack Success (ASR > 87%): High confidence. The experimental setup is clear, and the results are consistently reported across multiple attack methods and models.
- Defense Effectiveness (ASR reduction via fine-tuning): Medium confidence. While results are promising, the dependence on hyperparameter tuning and dataset characteristics suggests results may not be robust across all settings.
- Intra-domain vs. Extra-domain Fine-tuning: Medium confidence. The results support the claim, but the comparison is limited to two datasets and does not account for all possible data distributions.

## Next Checks

1. **External Dataset Validation**: Apply the proposed attack and defense methods to a third, independently sourced web attack detection dataset (e.g., from a different organization or collected under different conditions). Measure ASR and defense effectiveness to assess generalization.

2. **Trigger Detection Robustness**: Implement a simple trigger detection module (e.g., regex or anomaly detection on HTTP request text) and evaluate its ability to detect or remove the inserted triggers before model inference. Report the false positive and false negative rates.

3. **Hyperparameter Sensitivity Analysis**: Perform a grid search over α (loss weighting) and fine-tuning set sizes for both naive-FT and CF-FT on one dataset. Report the variance in ASR and C-ACC, and identify best practices for parameter selection.