---
ver: rpa2
title: Differentially Private Zeroth-Order Methods for Scalable Large Language Model
  Finetuning
arxiv_id: '2402.07818'
source_url: https://arxiv.org/abs/2402.07818
tags:
- pruning
- privacy
- zeroth-order
- gradient
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the application of differentially private
  (DP) zeroth-order methods to fine-tune large language models (LLMs) for downstream
  tasks. The proposed stagewise DP Zeroth-Order method (DP-ZOSO) dynamically adjusts
  hyperparameters to improve convergence and scalability.
---

# Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning

## Quick Facts
- arXiv ID: 2402.07818
- Source URL: https://arxiv.org/abs/2402.07818
- Authors: Z Liu; J Lou; W Bao; Y Hu; B Li; Z Qin; K Ren
- Reference count: 40
- Key outcome: This paper investigates the application of differentially private (DP) zeroth-order methods to fine-tune large language models (LLMs) for downstream tasks. The proposed stagewise DP Zeroth-Order method (DP-ZOSO) dynamically adjusts hyperparameters to improve convergence and scalability. By incorporating data-free pruning, the method further enhances efficiency by reducing the number of trainable parameters. Theoretical analysis guarantees privacy and convergence, while extensive experiments on various LLMs and tasks demonstrate superior performance in terms of scalability and utility compared to existing DP fine-tuning methods.

## Executive Summary
This paper introduces DP-ZOSO, a novel approach for differentially private fine-tuning of large language models that leverages zeroth-order optimization to reduce memory usage and improve scalability. The method dynamically adjusts the ZO scale parameter β across training stages to optimize the tradeoff between gradient approximation accuracy and convergence speed, while incorporating data-free pruning to further reduce the number of trainable parameters and the required DP noise. Theoretical analysis provides privacy and convergence guarantees, and extensive experiments demonstrate superior performance compared to existing DP fine-tuning methods in terms of both scalability and utility across various LLMs and downstream tasks.

## Method Summary
The paper proposes DP-ZOSO, a stagewise differentially private zeroth-order optimization method for fine-tuning large language models. The method dynamically schedules key hyperparameters, including the ZO scale parameter β, learning rate η, and regularization λ, across S stages to optimize the tradeoff between gradient approximation accuracy and convergence speed. Additionally, the method incorporates data-free pruning to reduce the number of trainable parameters, thereby improving scalability and reducing the amount of DP noise required. The DP noise is injected into the difference of function evaluations rather than the full gradient vector, further reducing the scale of noise required and improving utility. Theoretical analysis provides (ε,δ)-DP guarantees and convergence bounds, while extensive experiments demonstrate superior performance compared to existing DP fine-tuning methods.

## Key Results
- DP-ZOSO achieves superior scalability and utility compared to existing DP fine-tuning methods, with memory usage reduced by up to 76.8% on 350M-parameter models.
- The stagewise scheduling of the ZO scale parameter β improves convergence by dynamically adjusting the gradient approximation error and learning rate across optimization stages.
- Data-free pruning further enhances scalability by reducing the number of trainable parameters, thereby reducing the amount of DP noise required and improving the privacy-utility tradeoff.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stagewise DP Zeroth-Order (DP-ZOSO) improves convergence by dynamically adjusting the ZO scale parameter β across training stages.
- Mechanism: In early stages, a smaller β yields more accurate gradient approximations, allowing rapid movement toward the optimum with a larger learning rate. In later stages, a larger β is deliberately used to increase gradient approximation error, stabilizing the trajectory when combined with a smaller learning rate.
- Core assumption: The gradient approximation error from the zeroth-order method interacts favorably with the DP noise injection, and this interaction varies predictably across optimization stages.
- Evidence anchors:
  - [abstract] "We propose the stagewise DP zeroth-order method (DP-ZOSO) that dynamically schedules key hyperparameters. This design is grounded on the synergy between DP random perturbation and the gradient approximation error of the zeroth-order method, and its effect on fine-tuning trajectory."
  - [section 4.3] "In earlier stages, the gradient approximation error is controlled to be smaller, and together with a larger learning rate, it allows the LLM fine-tuning to more quickly approach the optimum. In the later stages, we need to deliberately increase the gradient approximation error, together with a decreased learning rate, offering a stabilization effect on the fine-tuning trajectory."
- Break condition: If the interaction between DP noise and gradient approximation error is not monotonic or predictable across stages, the theoretical convergence guarantee in Theorem 2 may fail.

### Mechanism 2
- Claim: Data-free pruning reduces the number of trainable parameters, thereby reducing the amount of DP noise injected and improving utility.
- Mechanism: Pruning identifies and freezes less important parameters using a data-free metric (SynFlow), so the DP noise only needs to be injected into the gradients of the remaining parameters. This reduces the effective dimensionality of the DP noise vector, improving the privacy-utility tradeoff.
- Core assumption: The pruning mask identified by data-free SynFlow is effective at preserving model utility while significantly reducing the number of trainable parameters, and the DP noise injection mechanism naturally scales with parameter count.
- Evidence anchors:
  - [abstract] "Second, we further enhance the scalability by reducing the trainable parameters that are identified by repurposing a data-free pruning technique requiring no additional data or extra privacy budget."
  - [section 4.4] "Unlike existing works in DP-SGD, which introduce additional trainable modules to modify the LLM structure, or the LoRA-based method, which still entails a certain number of variables, we propose to initially identify key parameters within the given LLM."
- Break condition: If the data-free pruning does not preserve the gradient flow or the pruned model fails to converge, the utility gains from reduced DP noise will be lost.

### Mechanism 3
- Claim: Injecting DP noise into the difference of function evaluations (rather than the full gradient vector) reduces the scale of noise required and improves utility.
- Mechanism: Instead of adding Gaussian noise to every dimension of the gradient (as in DP-SGD), noise is added only to the scalar difference (f(θ+βv) - f(θ-βv))/(2β), which is much smaller in magnitude. By standard post-processing, the resulting approximate gradient inherits (ε,δ)-DP.
- Core assumption: The difference of function evaluations is significantly smaller than the function values themselves, so injecting noise at this level requires less noise scale to achieve the same privacy level.
- Evidence anchors:
  - [section 4.1] "Instead of clipping both the loss function f(θ + βv) and f(θ - βv), we clip the difference of the function on two points since the absolute value of the difference is much smaller than the loss function itself (3.2e-5 (0.007%) compared to 0.45 in average)."
  - [section 4.1] "We propose a novel method of noise injection by adding noise N(0, σ²C²) to 1/(2β) (f(θ + βv) - f(θ - βv)), achieving (ε, δ)-DP guarantee."
- Break condition: If the clipping threshold C is set too low relative to the typical magnitude of the difference, gradients may be frequently clipped, degrading utility.

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: The entire paper's motivation and analysis hinge on ensuring that fine-tuning does not leak private information from the task-specific dataset. Understanding DP definitions, composition theorems, and the moments accountant is essential to follow the privacy analysis.
  - Quick check question: What is the relationship between the noise scale σ, the sampling probability q, the number of steps T, and the privacy parameters (ε, δ) in the moments accountant framework?

- Concept: Zeroth-Order Optimization (ZOO)
  - Why needed here: DP-ZOSO replaces gradient computation with two forward passes to estimate gradients, which is the key innovation for reducing memory usage. Understanding how the ZO gradient estimator works, its bias, and its variance is necessary to grasp the stagewise scheduling idea.
  - Quick check question: How does the choice of the ZO scale parameter β affect the bias and variance of the gradient estimate, and why does this motivate a dynamic schedule?

- Concept: Weak Convexity and the PL Condition
  - Why needed here: The convergence analysis in Theorem 2 relies on the objective being weakly convex and satisfying the Polyak-Łojasiewicz (PL) condition. These properties are used to bound the optimization error at each stage.
  - Quick check question: What is the difference between weak convexity and strong convexity, and why is the PL condition sufficient for the convergence proof here?

## Architecture Onboarding

- Component map:
  - Data-free pruning module -> Stagewise DP-ZO optimizer -> DP noise injection layer -> Vector distribution manager
- Critical path: (1) Data-free pruning → (2) Initialize important parameter mask → (3) For each stage: update β, η, λ → (4) Sample perturbation directions → (5) Estimate clipped, noised gradients → (6) Update parameters → (7) Repeat until convergence.
- Design tradeoffs:
  - Larger β reduces noise sensitivity but increases gradient estimation bias; smaller β does the opposite.
  - More aggressive pruning reduces DP noise but risks losing model capacity.
  - More stages allow finer control but increase total training time.
- Failure signatures:
  - Frequent clipping of gradient differences → increase C or reduce β.
  - Slow convergence despite many stages → check if β schedule is too conservative or if important parameters were incorrectly pruned.
  - High final loss → verify that the regularization parameter λ is not too large, suppressing updates.
- First 3 experiments:
  1. Run DP-ZOSO on a small model (e.g., RoBERTa-base) on SST-2 with fixed β and no pruning; compare memory usage and accuracy to DP-SGD.
  2. Add data-free pruning (r=1%) to the above; observe changes in accuracy and memory.
  3. Enable dynamic β scheduling across 3 stages; measure impact on convergence speed and final accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal pruning rate change with different privacy budgets and model scales?
- Basis in paper: [explicit] The paper shows that the optimal pruning rate varies with the privacy budget, but does not explore how this relationship changes with different model sizes or architectures.
- Why unresolved: The paper only tests a few model scales and does not systematically vary model size to understand the relationship between pruning rate and privacy budget across different architectures.
- What evidence would resolve it: A comprehensive study varying model size and architecture while keeping other factors constant would clarify the relationship between pruning rate and privacy budget across different model scales.

### Open Question 2
- Question: How does the stagewise training strategy impact the convergence rate compared to other dynamic scheduling methods?
- Basis in paper: [explicit] The paper introduces a stagewise training strategy with dynamic hyperparameter scheduling, but does not compare its convergence rate to other dynamic scheduling methods like cosine annealing or warm restarts.
- Why unresolved: The paper focuses on the stagewise approach and does not benchmark its convergence rate against other established dynamic scheduling methods.
- What evidence would resolve it: A direct comparison of the stagewise method's convergence rate with other dynamic scheduling methods on the same tasks and models would clarify its relative performance.

### Open Question 3
- Question: What is the impact of the rank-based important matrix on model performance in different privacy settings?
- Basis in paper: [explicit] The paper introduces the rank-based important matrix but notes that its optimal interval varies with the privacy budget and performs poorly under small privacy budgets.
- Why unresolved: The paper does not explore the impact of the rank-based important matrix across a wider range of privacy settings or provide a clear guideline for choosing the optimal interval.
- What evidence would resolve it: A systematic study varying the privacy budget and testing the rank-based important matrix with different intervals would clarify its impact on model performance in different privacy settings.

## Limitations
- The effectiveness of the stagewise scheduling of the ZO scale parameter β relies on a predictable interaction with DP noise injection across optimization stages, which may not hold for all fine-tuning scenarios.
- The data-free pruning approach using SynFlow may not preserve model utility as effectively as supervised pruning methods, particularly for tasks with complex loss landscapes.
- The theoretical convergence guarantee in Theorem 2 assumes weak convexity and the PL condition, which may not hold for all LLM fine-tuning objectives.

## Confidence
- High confidence: The fundamental mechanism of injecting DP noise into the difference of function evaluations (rather than full gradients) is well-established and theoretically sound. The memory efficiency gains from zeroth-order methods are also well-documented.
- Medium confidence: The stagewise scheduling of β and the data-free pruning approach show promise but require empirical validation across diverse tasks and model architectures to confirm generalizability.
- Low confidence: The interaction between the ZO gradient approximation error and DP noise injection across stages is theoretically motivated but may not hold uniformly across all fine-tuning scenarios, particularly for tasks with complex loss landscapes.

## Next Checks
1. Test DP-ZOSO with varying numbers of stages (S=2, S=5, S=10) on a small model to verify whether the claimed convergence benefits are monotonic and predictable.
2. Compare the effectiveness of data-free pruning against supervised pruning methods on tasks where labeled data is available, to quantify the potential utility gap.
3. Measure the actual memory savings and runtime performance of DP-ZOSO against DP-SGD on GPUs with limited memory capacity to validate the claimed scalability benefits.