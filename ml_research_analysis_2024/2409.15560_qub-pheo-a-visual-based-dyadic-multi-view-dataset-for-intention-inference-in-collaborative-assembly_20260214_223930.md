---
ver: rpa2
title: 'QUB-PHEO: A Visual-Based Dyadic Multi-View Dataset for Intention Inference
  in Collaborative Assembly'
arxiv_id: '2409.15560'
source_url: https://arxiv.org/abs/2409.15560
tags: []
core_contribution: The paper introduces QUB-PHEO, a novel multi-view visual dataset
  for advancing human-robot interaction (HRI) research in assembly operations. The
  dataset addresses the challenge of limited multi-view data capturing complex human
  actions and intentions in collaborative assembly tasks.
---

# QUB-PHEO: A Visual-Based Dyadic Multi-View Dataset for Intention Inference in Collaborative Assembly

## Quick Facts
- arXiv ID: 2409.15560
- Source URL: https://arxiv.org/abs/2409.15560
- Reference count: 40
- Dataset captures 36 subtasks across four assembly tasks with 70 participants

## Executive Summary
QUB-PHEO is a novel multi-view visual dataset designed to advance human-robot interaction (HRI) research in assembly operations. The dataset addresses the critical challenge of limited multi-view data capturing complex human actions and intentions during collaborative assembly tasks. Developed with 70 participants, QUB-PHEO features five synchronized camera views and comprehensive visual annotations including facial landmarks, gaze, hand movements, and object localization. The dataset provides 4.5 million frames and 36 hours of video, enabling researchers to develop advanced HRI algorithms and improve human-robot collaboration through enhanced intention inference capabilities.

## Method Summary
The QUB-PHEO dataset was created through a systematic approach involving multiple participants performing collaborative assembly tasks. The methodology includes rigorous calibration procedures, gaze estimation techniques, and detailed annotation processes. Five synchronized camera views capture different perspectives of the assembly interactions, while participants are instructed to maintain awareness of the robot surrogate's status. The dataset encodes 36 distinct subtasks across four assembly tasks, with 50 participants providing full video data and 70 contributing visual cues. The annotation process involves both automated computer vision techniques and human verification to ensure data quality and accuracy.

## Key Results
- Dataset contains 4.5 million frames and 36 hours of video data
- Five synchronized camera views capture comprehensive visual information
- Includes 36 distinct subtasks across four assembly tasks
- Supports both subtask classification and intention inference tasks

## Why This Works (Mechanism)
The effectiveness of QUB-PHEO stems from its multi-view approach that captures the complexity of human-robot interactions from multiple angles simultaneously. By incorporating synchronized camera views, the dataset provides rich spatial and temporal information necessary for understanding human intentions during assembly tasks. The comprehensive annotation of visual cues, including facial landmarks, gaze direction, and hand movements, enables algorithms to learn subtle indicators of human intent. The use of a robot surrogate allows for controlled capture of interaction dynamics without the technical limitations of actual robots, while the large participant pool ensures diverse behavioral patterns are represented.

## Foundational Learning
- Multi-view synchronization: Essential for aligning temporal and spatial information across different camera perspectives; quick check: verify timestamp alignment across all five views
- Visual intention inference: Core capability for understanding human intent through visual cues; quick check: test classification accuracy on subtask boundaries
- Gaze estimation techniques: Critical for determining where humans are focusing attention; quick check: validate gaze accuracy against ground truth markers
- Human-robot interaction dynamics: Understanding the collaborative patterns between humans and robots; quick check: analyze time gaps between human actions and robot responses
- Annotation pipeline: Systematic process for labeling visual data; quick check: sample annotated frames for consistency across annotators

## Architecture Onboarding

Component Map: Cameras -> Synchronization -> Annotation Pipeline -> Dataset Storage -> ML Models

Critical Path: Data Capture (5 synchronized cameras) → Preprocessing (calibration, gaze estimation) → Annotation (visual cues) → Storage (4.5M frames) → Algorithm Development (subtask classification, intention inference)

Design Tradeoffs:
- Robot surrogate vs. real robot: Surrogate provides controlled environment but may miss real-world robot limitations
- Visual-only data: Simpler collection but misses non-visual communication channels
- Fixed assembly tasks: Enables focused analysis but limits generalizability

Failure Signatures:
- Synchronization errors: Misaligned timestamps across camera views
- Annotation inconsistencies: Variations in visual cue labeling across annotators
- Calibration drift: Changes in camera positioning affecting spatial accuracy

First Experiments:
1. Test multi-view synchronization by measuring timestamp alignment across all five cameras
2. Validate gaze estimation accuracy by comparing against ground truth markers in controlled scenarios
3. Assess annotation consistency by having multiple annotators label the same video segments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the QUB-PHEO dataset at improving human-robot interaction algorithms compared to existing datasets?
- Basis in paper: [explicit] The paper states that QUB-PHEO aims to enhance human-robot interaction (HRI) research and improve machine learning models for HRI.
- Why unresolved: While the paper describes the dataset's potential, it does not provide empirical evidence or comparative studies demonstrating its effectiveness against existing datasets.
- What evidence would resolve it: Comparative studies showing improved performance of HRI algorithms trained on QUB-PHEO versus those trained on other datasets would resolve this question.

### Open Question 2
- Question: How well does the use of a human robot surrogate in the QUB-PHEO dataset generalize to real robot interactions?
- Basis in paper: [explicit] The paper introduces the concept of a 'robot surrogate' to capture interaction dynamics without the technical limitations of actual robots.
- Why unresolved: The paper does not provide evidence or analysis on how well the interactions captured by the human surrogate translate to interactions with real robots.
- What evidence would resolve it: Studies comparing the interaction dynamics captured by the human surrogate with those observed in real human-robot interactions would help resolve this question.

### Open Question 3
- Question: What are the limitations of the QUB-PHEO dataset in capturing the full complexity of human-robot interactions in assembly tasks?
- Basis in paper: [inferred] The paper acknowledges the dataset's potential but does not explicitly discuss its limitations in capturing the full complexity of human-robot interactions.
- Why unresolved: The paper focuses on the dataset's strengths and contributions but does not provide a comprehensive analysis of its limitations or potential gaps.
- What evidence would resolve it: A detailed analysis of the dataset's coverage of various interaction scenarios, including edge cases and complex dynamics, would help identify its limitations.

## Limitations
- Focus on specific assembly tasks may limit generalizability to other HRI domains
- Calibration and annotation processes may introduce biases or errors affecting algorithm performance
- Reliance on visual cues alone may miss non-visual communication channels important for intention inference

## Confidence
High: Technical specifications (participant count, camera views, frame count)
Medium: Potential impact on HRI research given growing interest in multi-view data
Low: Long-term utility as HRI requirements may evolve rapidly

## Next Checks
1. Conduct a cross-validation study using QUB-PHEO to assess the performance of intention inference algorithms across different assembly tasks
2. Compare the effectiveness of QUB-PHEO-based algorithms with those trained on other HRI datasets in real-world collaborative assembly scenarios
3. Investigate the impact of non-visual communication channels on intention inference by integrating additional sensor data with QUB-PHEO in a controlled experiment