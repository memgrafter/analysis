---
ver: rpa2
title: 'Uncertainty Management in the Construction of Knowledge Graphs: a Survey'
arxiv_id: '2405.16929'
source_url: https://arxiv.org/abs/2405.16929
tags:
- knowledge
- data
- uncertainty
- confidence
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively examines approaches to handling uncertainty
  in knowledge graph (KG) construction. The authors identify three main sources of
  uncertainty: knowledge deltas (differences in granularity and contradictions between
  sources), extraction algorithm inaccuracies, and evolving knowledge over time.'
---

# Uncertainty Management in the Construction of Knowledge Graphs: a Survey

## Quick Facts
- arXiv ID: 2405.16929
- Source URL: https://arxiv.org/abs/2405.16929
- Reference count: 40
- Primary result: Survey examines approaches to handling uncertainty in knowledge graph construction, identifying three main uncertainty sources and proposing an ideal data integration pipeline

## Executive Summary
This survey comprehensively examines approaches to handling uncertainty in knowledge graph (KG) construction. The authors identify three main sources of uncertainty: knowledge deltas (differences in granularity and contradictions between sources), extraction algorithm inaccuracies, and evolving knowledge over time. They propose an ideal data integration pipeline with four key steps: knowledge representation (incorporating uncertainty and provenance), knowledge alignment, knowledge fusion, and consistency checking. The survey reviews recent advances in uncertain KG embedding models that take confidence scores into account, discusses various knowledge alignment and fusion approaches, and explores different mechanisms for representing uncertainty in KGs.

## Method Summary
The survey synthesizes state-of-the-art approaches for uncertain KG embedding, knowledge alignment, fusion, and uncertainty representation. It analyzes the limitations of current methods and proposes directions for improvement, particularly in handling granularity disparities and incorporating extraction confidence scores into fusion models. The authors review both theoretical frameworks and practical implementations, examining how different uncertainty representation schemes (RDF-star, Multilayer Graph) can be integrated into the KG construction pipeline.

## Key Results
- Current knowledge integration approaches remain limited in scope and often overlook various types of uncertainty
- Better handling of granularity disparities in fusion models is needed to produce more complete and specific KGs
- Incorporating extraction algorithm confidence scores into knowledge fusion models can improve accuracy, especially for long-tail entities
- RDF-star provides a more expressive and efficient way to represent uncertainty metadata compared to traditional reification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representing uncertainty in the KG embedding space improves downstream task performance compared to deterministic embeddings
- Mechanism: Uncertain KG embedding models incorporate confidence scores into the scoring function and loss computation, allowing the embedding vectors to encode both the triple plausibility and its associated uncertainty
- Core assumption: The confidence scores provided by extraction algorithms or other components are reliable and meaningfully capture the uncertainty of the underlying triples
- Evidence anchors: [abstract] "The survey reviews recent advances in uncertain KG embedding models that take confidence scores into account..."; [section] "We survey state-of-the-art approaches in this direction and present constructions of both open and enterprise KGs and how their quality is maintained..."; [corpus] Weak evidence - the corpus neighbors do not directly address uncertain KG embeddings, only general KG construction
- Break condition: If the confidence scores are noisy or systematically biased, the uncertain embedding models may perform worse than deterministic models by overfitting to incorrect uncertainty estimates

### Mechanism 2
- Claim: Knowledge fusion methods that consider data granularity can produce more complete and specific KGs compared to simple majority voting
- Mechanism: Granularity-aware fusion models identify and preserve more specific knowledge when merging conflicting triples, while still resolving contradictions by finding the most reliable information
- Core assumption: Differences in granularity between sources are meaningful and reflect varying levels of specificity in the underlying knowledge, rather than arbitrary variations in representation
- Evidence anchors: [abstract] "Key findings include the need for better handling of granularity disparities in fusion models..."; [section] "The authors of [4, 158] assume that uncertainty is a common feature of the knowledge we handle daily. In this sense, exploiting uncertain data sources by ignoring uncertainty to enrich a KG would impact downstream applications of the graph."; [corpus] Weak evidence - the corpus neighbors focus on LLM-powered KG construction rather than granularity-aware fusion methods
- Break condition: If granularity disparities are not meaningful (e.g., due to inconsistent naming conventions rather than true differences in specificity), preserving granularity may lead to redundant or confusing information in the KG

### Mechanism 3
- Claim: RDF-star provides a more expressive and efficient way to represent uncertainty metadata compared to traditional reification
- Mechanism: RDF-star allows metadata (such as confidence scores and provenance) to be directly attached to triples without the need for reification triples, reducing verbosity and improving query performance
- Core assumption: The added expressiveness of RDF-star does not introduce inconsistencies or ambiguities in representing multiple conflicting metadata values for the same triple
- Evidence anchors: [abstract] "The survey highlights that while uncertainty representation in KGs has received attention, current knowledge integration approaches remain limited in scope..."; [section] "RDF-star [52] is an extension of the RDF model proposed by the Semantic Web community... RDF-star has its own query language called SPARQL-star which reduces compatibility issues [74]."; [corpus] Weak evidence - the corpus neighbors do not discuss RDF-star or other uncertainty representation mechanisms
- Break condition: If RDF-star adoption is limited or SPARQL-star support is not widely available, the benefits of this representation may not be fully realized in practice

## Foundational Learning

- Concept: Knowledge graph basics (entities, relations, triples, schemas/ontology)
  - Why needed here: Understanding the fundamental components and structure of KGs is essential for grasping how uncertainty is represented and managed within them
  - Quick check question: What are the three main components of a knowledge graph triple, and how do they relate to the real-world entities and relationships being modeled?

- Concept: Information extraction from unstructured text
  - Why needed here: Knowledge acquisition, a key step in KG construction, often involves extracting structured information from unstructured text sources, which introduces uncertainty due to extraction errors and ambiguities
  - Quick check question: What are the two main sub-tasks of information extraction from text, and how do they contribute to populating a KG with entities and relations?

- Concept: Probabilistic and fuzzy logic for representing uncertainty
  - Why needed here: Uncertainty in KGs can arise from various sources, including knowledge deltas and extraction inaccuracies, and requires appropriate representation mechanisms that can capture the degree of confidence or ambiguity in the information
  - Quick check question: How do probabilistic and fuzzy logic differ in their approach to representing uncertainty, and what types of KG uncertainty are each best suited to handle?

## Architecture Onboarding

- Component map: Knowledge extraction (from text, web, or LLMs) -> Knowledge alignment (entity resolution and deduplication) -> Knowledge fusion (resolving conflicts and preserving granularity) -> KG embedding (representing the graph in a vector space) -> Uncertainty representation (metadata for confidence and provenance)

- Critical path: Knowledge extraction → Knowledge alignment → Knowledge fusion → KG embedding → Uncertainty representation

- Design tradeoffs:
  - Accuracy vs. scalability: More sophisticated uncertainty handling methods may improve accuracy but at the cost of increased computational complexity and runtime
  - Expressiveness vs. simplicity: Richer uncertainty representation schemes (e.g., RDF-star) provide more expressive power but may introduce complexity in querying and reasoning
  - Domain-specific vs. general-purpose: Uncertainty handling methods tailored to specific domains (e.g., scientific literature) may perform better than general-purpose approaches but lack flexibility

- Failure signatures:
  - Inconsistent or contradictory information persisting in the KG despite fusion efforts
  - Performance degradation in downstream tasks (e.g., link prediction) when using uncertain KG embeddings
  - Unreasonable query results or reasoning failures due to poorly handled uncertainty

- First 3 experiments:
  1. Implement a simple majority voting fusion method and compare its output KG to one produced by a granularity-aware fusion method on a small dataset with known conflicts and granularity differences
  2. Evaluate the impact of incorporating confidence scores into a KG embedding model (e.g., TransE) by comparing its link prediction performance to the standard deterministic version on a dataset with uncertainty annotations
  3. Measure the query performance and storage requirements of RDF-star vs. traditional reification for representing uncertainty metadata on a medium-sized KG

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can embedding-based models be extended to incorporate uncertainty scores for knowledge alignment tasks?
- Basis in paper: [explicit] The paper notes that while KG completion and confidence prediction tasks use uncertainty-aware embeddings, knowledge alignment approaches do not take into account the uncertainty of the knowledge to be aligned
- Why unresolved: Current alignment models assume deterministic knowledge, ignoring confidence scores that could guide entity matching, especially in cases of long-tail entities with limited sources
- What evidence would resolve it: Experimental comparison of alignment models using uncertainty-aware embeddings versus traditional deterministic approaches on datasets with varying levels of source reliability and entity coverage

### Open Question 2
- Question: What is the optimal way to model granularity disparities in knowledge fusion when dealing with heterogeneous sources?
- Basis in paper: [explicit] The paper identifies that most fusion methods do not handle granularity, assuming only one true value exists, while some approaches consider partial ordering or semantic distance
- Why unresolved: Existing methods either ignore granularity differences or handle them through simplistic partial ordering, failing to capture complex hierarchical relationships and varying levels of specificity
- What evidence would resolve it: A comprehensive evaluation framework testing different granularity-aware fusion strategies on datasets with known hierarchical knowledge structures and varying source specificities

### Open Question 3
- Question: How can confidence scores from extraction algorithms be effectively integrated into knowledge fusion models?
- Basis in paper: [explicit] The paper argues that current fusion models primarily use source trustworthiness scores but ignore confidence scores from extraction algorithms, which could be valuable when few sources provide knowledge about long-tail entities
- Why unresolved: The relationship between extraction algorithm confidence and source reliability is not well understood, and methods for combining these different types of confidence scores are lacking
- What evidence would resolve it: Comparative studies of fusion models using different combinations of source and extraction confidence scores on datasets with controlled variations in extraction accuracy and source quality

## Limitations
- The survey identifies key uncertainty sources but lacks empirical validation of proposed integration pipeline effectiveness
- Confidence score quality heavily depends on extraction algorithm reliability, which is not systematically evaluated
- Granularity-aware fusion methods remain theoretical with limited practical implementation evidence

## Confidence
- **High confidence**: RDF-star provides more efficient uncertainty representation than traditional reification
- **Medium confidence**: Knowledge fusion methods considering granularity can improve KG completeness
- **Low confidence**: Uncertain KG embedding models consistently outperform deterministic approaches in downstream tasks

## Next Checks
1. Compare majority voting vs. granularity-aware fusion on datasets with controlled conflicts to measure specificity preservation
2. Benchmark uncertain KG embedding models against deterministic versions across multiple link prediction datasets with confidence annotations
3. Evaluate RDF-star vs. reification performance on storage efficiency and query execution time using standardized KG benchmarks