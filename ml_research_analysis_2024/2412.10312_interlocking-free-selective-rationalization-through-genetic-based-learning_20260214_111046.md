---
ver: rpa2
title: Interlocking-free Selective Rationalization Through Genetic-based Learning
arxiv_id: '2412.10312'
source_url: https://arxiv.org/abs/2412.10312
tags:
- genspp
- rationalization
- size
- learning
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles interlocking in selective rationalization, where
  a generator and predictor module in an SPP pipeline get stuck in local minima. The
  authors propose GenSPP, a novel framework that eliminates interlocking by using
  disjoint training via genetic-based search, without requiring heuristics, sampling,
  or architectural changes.
---

# Interlocking-free Selective Rationalization Through Genetic-based Learning

## Quick Facts
- arXiv ID: 2412.10312
- Source URL: https://arxiv.org/abs/2412.10312
- Reference count: 28
- This paper tackles interlocking in selective rationalization, where generator and predictor modules get stuck in local minima.

## Executive Summary
This paper addresses the interlocking problem in selective rationalization where generator and predictor modules in select-then-predict (SPP) pipelines get stuck in local minima. The authors propose GenSPP, a novel framework that eliminates interlocking through disjoint training using genetic-based search, without requiring heuristics, sampling, or architectural changes. GenSPP achieves higher highlight quality (Hl-F1 ↑ 6.5%) while maintaining comparable classification performance (Clf-F1) compared to state-of-the-art methods.

## Method Summary
GenSPP uses a genetic algorithm to optimize generator parameters independently of the predictor. The approach consists of two stages: first, a generator instance is defined independently; second, a predictor is trained from scratch while the generator remains frozen. The fitness function combines classification loss and highlight quality into a single metric that prevents collapse to either classification-only or rationalization-only solutions. The genetic algorithm maintains population diversity through mutation and crossover operations, reducing the risk of getting stuck in local optima.

## Key Results
- GenSPP achieves higher highlight quality (Hl-F1 ↑ 6.5%) compared to state-of-the-art methods on both synthetic and hate speech datasets
- Maintains comparable classification performance (Clf-F1) while improving rationalization quality
- Shows reduced variance and can recover from degraded initializations, demonstrating robustness
- Outperforms baseline methods on both Toy dataset (synthetic random strings) and HateXplain dataset (hate speech detection)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GenSPP eliminates interlocking by splitting the optimization process into two stages using genetic-based search.
- Mechanism: The genetic algorithm evaluates each individual independently using a predictor trained from scratch, breaking the dependency between generator and predictor during training.
- Core assumption: The disjoint training setup prevents the predictor from overfitting to suboptimal masks generated by the generator.
- Evidence anchors:
  - [abstract]: "GenSPP avoids interlocking by performing disjoint training of the generator and predictor via genetic global search."
  - [section]: "GenSPP breaks interlocking by splitting the optimization process into two stages, optimized via genetic-based search."

### Mechanism 2
- Claim: The fitness function design prevents collapse to either classification-only or rationalization-only solutions.
- Mechanism: The fitness function combines classification loss and highlight quality into a single metric that penalizes extreme solutions, ensuring both objectives are pursued simultaneously.
- Core assumption: The non-differentiable fitness function can effectively guide the search without requiring careful weight balancing.
- Evidence anchors:
  - [abstract]: "Genetic-based search does not require differentiable learning objectives, allowing for more accurate training regularizations."

### Mechanism 3
- Claim: Population-based search with mutation and crossover reduces the likelihood of getting stuck in local minima.
- Mechanism: The genetic algorithm maintains a population of diverse generator configurations, using mutation for local exploration and crossover for global search, which helps escape suboptimal solutions.
- Core assumption: The combination of local and global search operators provides sufficient exploration of the parameter space.
- Evidence anchors:
  - [abstract]: "Genetic-based search allows for local and global exploration of the generator's parameters, significantly reducing the risk of getting stuck into local minima."

## Foundational Learning

- Concept: Genetic algorithms and neuroevolution
  - Why needed here: The paper uses genetic algorithms to optimize neural network parameters for selective rationalization, which requires understanding how genetic algorithms work and how they can be applied to neural network optimization.
  - Quick check question: What are the three main components of a genetic algorithm and how do they work together to find optimal solutions?

- Concept: Selective rationalization and SPP architecture
  - Why needed here: The paper builds on the select-then-predict framework for selective rationalization, which requires understanding how generator and predictor modules work together and why they can get stuck in interlocking.
  - Quick check question: In the SPP framework, what is the role of the generator and predictor modules, and how does interlocking occur between them?

- Concept: Fitness function design and evaluation metrics
  - Why needed here: The paper introduces a novel fitness function that balances classification performance and highlight quality, which requires understanding how to design effective evaluation metrics for selective rationalization.
  - Quick check question: How does the fitness function in GenSPP differ from traditional regularization approaches, and why is this design important for avoiding suboptimal solutions?

## Architecture Onboarding

- Component map:
  - Generator module (gθ) -> Predictor module (fω) -> Fitness evaluation system
  - Genetic algorithm engine manages population, selection, crossover, and mutation

- Critical path:
  1. Initialize population of generator configurations
  2. For each individual: train predictor from scratch, evaluate fitness
  3. Select parents based on fitness scores
  4. Apply crossover and mutation to create new population
  5. Repeat until convergence or maximum generations reached

- Design tradeoffs:
  - Population size vs. computational cost: Larger populations provide better exploration but require more computation
  - Number of generations vs. convergence quality: More generations allow better solutions but increase training time
  - Mutation rate vs. stability: Higher mutation rates increase exploration but may disrupt good solutions

- Failure signatures:
  - Population converges too quickly: May indicate insufficient diversity or exploration
  - Fitness scores plateau: Could mean the algorithm is stuck in local optima or the fitness function is poorly designed
  - High variance across runs: May indicate sensitivity to initialization or stochastic elements

- First 3 experiments:
  1. Run GenSPP on the Toy dataset with default parameters to verify basic functionality and compare against baseline FR
  2. Test different population sizes (e.g., 20, 50, 100) on the Toy dataset to understand the impact on performance and computation time
  3. Evaluate GenSPP's ability to recover from synthetic skew initialization by comparing against baseline models on both datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GenSPP's performance scale with increasing text length and complexity compared to gradient-based methods?
- Basis in paper: [inferred] The authors note that selective rationalization for longer sequences is often accomplished at the sentence level to reduce task complexity, and that token-level rationalization has O(n) time complexity.
- Why unresolved: The paper only evaluates GenSPP on datasets with relatively short texts (up to 30 tokens).
- What evidence would resolve it: Empirical results comparing GenSPP and gradient-based methods on datasets with longer texts and more complex structures.

### Open Question 2
- Question: What is the impact of different genetic algorithm parameters (e.g., population size, mutation rate, crossover rate) on GenSPP's performance and convergence speed?
- Basis in paper: [explicit] The authors provide specific values for genetic algorithm parameters but do not explore their impact on performance.
- Why unresolved: The paper does not investigate how sensitive GenSPP is to changes in genetic algorithm parameters.
- What evidence would resolve it: Empirical results varying genetic algorithm parameters and measuring their impact on performance and convergence speed.

### Open Question 3
- Question: How does GenSPP perform on tasks beyond text classification, such as named entity recognition or sentiment analysis?
- Basis in paper: [inferred] The authors mention that future research directions include exploring more efficient genetic algorithms to scale to more complex neural architectures.
- Why unresolved: The paper only evaluates GenSPP on text classification tasks.
- What evidence would resolve it: Empirical results applying GenSPP to various NLP tasks beyond text classification.

## Limitations

- The approach's effectiveness on more complex tasks requiring nuanced understanding remains unproven, as the paper only evaluates on synthetic data and hate speech detection
- The genetic algorithm's computational cost (50 individuals × 100 generations) may be prohibitive for larger models or datasets
- The fitness function design, while theoretically sound, may not generalize well to tasks where balancing classification and rationalization is more challenging

## Confidence

- High confidence in the core claim that GenSPP eliminates interlocking through disjoint training - well-supported by theoretical framework and empirical results
- Medium confidence in the generalizability of the approach - results are strong on two datasets but don't explore more complex tasks
- Medium confidence in the computational efficiency claims - parallel implementation is mentioned but not thoroughly evaluated

## Next Checks

1. **Generalization Test**: Apply GenSPP to a more complex text classification task (e.g., multi-label classification or fine-grained sentiment analysis) to evaluate whether the fitness function balance and genetic search remain effective.

2. **Architecture Scaling**: Test GenSPP with transformer-based generator and predictor modules (rather than RNNs) to assess whether the approach scales to modern architectures while maintaining interlocking-free training.

3. **Cost-Benefit Analysis**: Conduct a detailed computational analysis comparing GenSPP's total training time against FR and joint training, including both sequential and parallel implementation scenarios, to quantify the practical trade-offs.