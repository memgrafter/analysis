---
ver: rpa2
title: Language models scale reliably with over-training and on downstream tasks
arxiv_id: '2403.08540'
source_url: https://arxiv.org/abs/2403.08540
tags:
- scaling
- error
- https
- loss
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses two gaps between existing scaling laws and
  how language models are practically trained and evaluated. First, while scaling
  studies typically focus on compute-optimal training, practitioners often over-train
  smaller models to reduce inference costs.
---

# Language models scale reliably with over-training and on downstream tasks

## Quick Facts
- arXiv ID: 2403.08540
- Source URL: https://arxiv.org/abs/2403.08540
- Reference count: 40
- Primary result: Scaling laws reliably predict loss and downstream performance for models trained with over-training and evaluated on practical tasks

## Executive Summary
This paper addresses two practical gaps between theoretical scaling laws and real-world language model training: practitioners often over-train smaller models to reduce inference costs, and they evaluate models on downstream tasks rather than just next-token prediction loss. The authors create a comprehensive testbed of 104 models ranging from 0.011B to 6.9B parameters, trained on three different data distributions with varying amounts of over-training. They demonstrate that scaling laws can accurately predict both validation loss and downstream task performance for much larger models, enabling practitioners to make informed decisions about model size and training duration without expensive large-scale experiments.

## Method Summary
The authors conduct an extensive empirical study by training 104 language models with parameter counts ranging from 0.011B to 6.9B on three distinct data distributions. They systematically vary the amount of over-training (training beyond compute-optimal duration) and measure both validation loss and downstream task performance across multiple benchmarks. By analyzing this comprehensive dataset, they derive scaling laws that relate model parameters, training tokens, and performance metrics, enabling reliable extrapolation to models requiring 20-300x more compute than those in the testbed.

## Key Results
- Scaling laws accurately predict validation loss within 0.7% relative error for models requiring 20-300x more compute
- A power law relationship between perplexity and downstream performance enables prediction of average top-1 error within 0.05-3.6% relative error
- Over-training smaller models provides reliable performance estimates for larger compute-optimal models, offering a cost-effective alternative to training at scale
- The scaling relationships hold across three different data distributions, demonstrating robustness to training data variation

## Why This Works (Mechanism)
The scaling laws work because language model performance follows predictable power law relationships with model size and training duration. By systematically varying both parameters across a wide range, the authors capture the underlying mathematical relationships that govern model behavior. The over-training regime is particularly valuable because it allows smaller models to explore the same performance regimes as larger compute-optimal models, creating a scalable experimental framework that doesn't require prohibitively expensive large-scale training runs.

## Foundational Learning

**Power laws in scaling**: Mathematical relationships where performance metrics scale as power functions of model size or compute. *Why needed*: Language model performance exhibits predictable scaling behavior that can be captured by simple mathematical forms. *Quick check*: Plot performance vs. model size on log-log scales to verify linear relationships.

**Compute-optimal training**: The point where additional training tokens no longer provide sufficient performance gains to justify the compute cost. *Why needed*: Establishes the baseline against which over-training benefits can be measured. *Quick check*: Identify the elbow point in loss curves as training progresses.

**Perplexity-to-downstream mapping**: The relationship between language modeling loss and practical task performance. *Why needed*: Connects theoretical training objectives to real-world evaluation metrics. *Quick check*: Correlate perplexity values with downstream benchmark scores across multiple models.

## Architecture Onboarding

**Component map**: Data distribution → Model architecture → Training duration → Validation loss → Downstream performance

**Critical path**: Model architecture and training duration are the primary determinants of performance, with data distribution providing secondary effects that can be accounted for in scaling laws.

**Design tradeoffs**: The study balances model size range, data diversity, and training duration to capture scaling behavior while maintaining experimental feasibility. The choice to focus on decoder-only transformers limits generalizability but ensures clean scaling relationships.

**Failure signatures**: Scaling laws may break down for architectures outside the tested range, data distributions with fundamentally different properties, or when evaluating on tasks that don't align with the pretraining objective.

**First experiments**:
1. Train a small model with varying degrees of over-training to observe the relationship between training duration and validation loss
2. Measure downstream performance across the same set of benchmarks used in the full study
3. Compare scaling behavior across the three different data distributions to validate robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Scaling laws derived from decoder-only transformers in the 0.011B-6.9B parameter range may not generalize to larger frontier models or different architectures
- The three specific data distributions used may not capture all aspects of natural language or specialized domains
- The power law relationship between perplexity and downstream performance shows varying accuracy (0.05-3.6% relative error) across different error ranges

## Confidence

**High confidence**: Empirical observations about over-training behavior and its relationship to compute-optimal training are well-supported by extensive experimental results.

**Medium confidence**: Extrapolation claims rely on assumptions that scaling relationships observed in smaller models will hold for models requiring 20-300x more compute.

**Low confidence**: Universal applicability of the power law relationship between perplexity and downstream task performance across all possible tasks and evaluation scenarios.

## Next Checks

1. Validate scaling laws on larger models (10B+ parameters) and different architectural variants to confirm extrapolation accuracy beyond the tested parameter range.

2. Test downstream performance predictions on additional benchmark suites and real-world task distributions to assess robustness of the power law relationship.

3. Evaluate scaling behavior on multilingual and specialized domain datasets to determine if relationships generalize beyond the three data distributions used in the study.