---
ver: rpa2
title: Legal Documents Drafting with Fine-Tuned Pre-Trained Large Language Model
arxiv_id: '2406.04202'
source_url: https://arxiv.org/abs/2406.04202
tags:
- legal
- language
- text
- large
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for fine-tuning a pre-trained large
  language model (BLOOM) on local hardware to generate legal document drafts, addressing
  privacy and security concerns. Using 74,823 criminal fraud judgment texts, the model
  was fine-tuned without Chinese word segmentation.
---

# Legal Documents Drafting with Fine-Tuned Pre-Trained Large Language Model

## Quick Facts
- **arXiv ID**: 2406.04202
- **Source URL**: https://arxiv.org/abs/2406.04202
- **Authors**: Chun-Hsien Lin; Pu-Jen Cheng
- **Reference count**: 23
- **Primary result**: Fine-tuned BLOOM 560M on local GPU achieves perplexity of 8.51 for generating Chinese legal document drafts

## Executive Summary
This paper presents a method for fine-tuning a pre-trained large language model (BLOOM) on local hardware to generate legal document drafts, addressing privacy and security concerns. Using 74,823 criminal fraud judgment texts, the model was fine-tuned without Chinese word segmentation. The fine-tuned model achieved a perplexity of 8.51 and successfully generated drafts that structurally matched legal requirements. The study demonstrates that fine-tuning on local GPUs is feasible, privacy-preserving, and effective for automating legal document drafting in Chinese.

## Method Summary
The authors fine-tuned BLOOM 560M parameter model on 74,823 criminal fraud judgment texts from Taiwan's Judicial Yuan, focusing on "criminal facts" sections. The model was trained locally on RTX 3090 (24GB RAM) for 5 epochs without Chinese word segmentation. Text generation used Top-K + Top-p sampling strategy. The approach aimed to preserve privacy by avoiding third-party services while achieving effective legal document generation. Model performance was evaluated using perplexity scores and structural correctness against legal constituent elements.

## Key Results
- Fine-tuned BLOOM 560M achieved perplexity of 8.51 on validation set
- Generated drafts structurally matched legal document requirements
- Local GPU fine-tuning successfully preserved data privacy
- Model effectively generated Chinese legal text without word segmentation

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a pre-trained LLM on a large corpus of unlabeled legal documents improves the model's ability to generate structurally correct legal drafts. The pre-trained LLM has learned general language patterns and contextual relationships from its original training corpus. Fine-tuning on legal text allows it to adapt these patterns to the specialized terminology, sentence structures, and formatting conventions of legal documents. The large volume of data compensates for the lack of manual annotation.

### Mechanism 2
Training on local hardware preserves data privacy and reduces security risks for sensitive legal documents. By keeping all data and model training within the local environment, no sensitive information is transmitted to external servers. This eliminates risks associated with third-party data handling, storage, and potential breaches.

### Mechanism 3
Using a pre-trained multilingual model (BLOOM) fine-tuned on traditional Chinese legal text enables effective generation of legal drafts in that language. BLOOM's multilingual training provides a strong foundation for language understanding. Fine-tuning on traditional Chinese legal text allows the model to specialize in the specific vocabulary, grammar, and formatting of that legal domain.

## Foundational Learning

- **Concept**: Pre-trained language models and fine-tuning
  - Why needed here: Understanding how pre-trained models can be adapted to specific domains through fine-tuning is crucial for this work's approach to legal document generation.
  - Quick check question: What is the key difference between training a model from scratch and fine-tuning a pre-trained model?

- **Concept**: Legal document structure and terminology
  - Why needed here: Recognizing the unique formatting, terminology, and structural elements of legal documents is essential for evaluating the quality of generated drafts and understanding the model's learning objectives.
  - Quick check question: What are the typical sections and formatting conventions of a legal judgment document?

- **Concept**: Data privacy and security in AI applications
  - Why needed here: Appreciating the importance of data privacy and security concerns in legal AI applications is necessary to understand the motivation behind local training and the potential risks of third-party processing.
  - Quick check question: What are the main privacy and security risks associated with sending sensitive legal documents to external AI services?

## Architecture Onboarding

- **Component map**: Data preprocessing → Fine-tuning environment (local GPU) → Fine-tuned model → Text generation strategies → Evaluation metrics
- **Critical path**: Data collection and preprocessing → Model selection and fine-tuning → Text generation and evaluation
- **Design tradeoffs**: Smaller model size for faster local training vs. larger model size for potentially better generation quality; unlabeled data for privacy vs. labeled data for potentially faster learning; local training for privacy vs. cloud training for potentially more resources.
- **Failure signatures**: Poor perplexity scores indicating the model is not learning legal patterns; generated text lacking legal structure or terminology; security breaches if local environment is compromised.
- **First 3 experiments**:
  1. Fine-tune BLOOM 560M on a subset of legal documents and evaluate perplexity and generated text quality.
  2. Test different text generation strategies (Greedy Search, Beam Search, Top-K + Top-p) on the fine-tuned model and compare results.
  3. Assess the model's ability to generate text with correct legal structure by decomposing generated drafts and checking for required legal elements.

## Open Questions the Paper Calls Out

### Open Question 1
How does the fine-tuned BLOOM model perform on legal document drafting tasks compared to other fine-tuned LLMs like GPT-3 or LLaMA in terms of accuracy and hallucination rates? The study focuses on demonstrating feasibility rather than benchmarking against alternatives.

### Open Question 2
What is the minimum amount of legal training data required to achieve acceptable performance in fine-tuning BLOOM for legal document drafting? The paper uses a large dataset but does not investigate the impact of dataset size on model performance.

### Open Question 3
How does the performance of the fine-tuned model change when deployed on different hardware configurations (e.g., varying GPU memory and processing power)? The study demonstrates feasibility on RTX 3090 but does not explore performance across different hardware setups.

### Open Question 4
Can the model be extended to handle other types of legal documents beyond criminal fraud judgments, such as civil cases or administrative decisions? The model is trained specifically on fraud case judgments and the authors suggest exploring other legal document types.

## Limitations

- Model evaluation lacks expert legal review of generated drafts' substantive accuracy
- Hardware requirements (RTX 3090 with 24GB RAM) may limit accessibility for smaller organizations
- Dataset restricted to criminal fraud cases in Taiwan, limiting generalizability to other legal domains

## Confidence

- **High Confidence**: Technical implementation of local fine-tuning on consumer-grade GPUs is well-documented and reproducible
- **Medium Confidence**: Claim that fine-tuning on unlabeled legal text can capture necessary document structure is supported by results but lacks comparison to alternative approaches
- **Low Confidence**: Assertion that generated drafts are "sufficiently accurate" for practical legal use lacks empirical support through legal expert evaluation

## Next Checks

1. Conduct blind evaluation where legal professionals assess the quality, accuracy, and completeness of generated drafts compared to human-written documents
2. Fine-tune the same model architecture on legal corpora from different jurisdictions to assess generalizability
3. Perform security audit of the local training environment to quantify actual privacy protection levels, including testing for potential data leakage through model outputs or intermediate training artifacts