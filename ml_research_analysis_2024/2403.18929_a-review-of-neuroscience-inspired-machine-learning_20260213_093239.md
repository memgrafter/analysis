---
ver: rpa2
title: A Review of Neuroscience-Inspired Machine Learning
arxiv_id: '2403.18929'
source_url: https://arxiv.org/abs/2403.18929
tags:
- learning
- neural
- arxiv
- credit
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys biologically plausible algorithms for credit
  assignment in artificial neural networks as alternatives to backpropagation. The
  main problems addressed are the biological implausibility of backpropagation, including
  weight transport, forward and backward locking, and forward-backward differentiation.
---

# A Review of Neuroscience-Inspired Machine Learning

## Quick Facts
- arXiv ID: 2403.18929
- Source URL: https://arxiv.org/abs/2403.18929
- Reference count: 40
- One-line primary result: Survey of biologically plausible credit assignment algorithms that address backpropagation's biological implausibilities while enabling efficient learning on neuromorphic hardware

## Executive Summary
This paper provides a comprehensive survey of biologically plausible algorithms for credit assignment in artificial neural networks, focusing on alternatives to backpropagation that address its biological implausibilities. The review covers methods like predictive coding, contrastive Hebbian learning, forward-only learning, direct feedback alignment, target propagation, and local representation alignment, analyzing their mechanisms, advantages, and limitations. The paper emphasizes how these algorithms enable parallel or asynchronous learning while avoiding weight transport and forward-backward locking, making them particularly suitable for neuromorphic hardware implementations.

## Method Summary
This is a survey paper that reviews and synthesizes existing research on biologically plausible credit assignment algorithms for neural networks. The paper examines theoretical foundations, algorithmic mechanisms, and implementation considerations across multiple bio-inspired approaches, drawing connections between neuroscience principles and machine learning techniques. Rather than presenting original experimental results, the authors analyze published works to provide a comprehensive overview of the field's current state, challenges, and future directions.

## Key Results
- Multiple bio-plausible algorithms (predictive coding, contrastive Hebbian learning, forward-only learning) can approximate backpropagation while avoiding biological implausibilities
- Local learning rules enable efficient implementation on neuromorphic hardware by eliminating weight transport and sequential processing constraints
- Energy-based formulations provide a unified framework for understanding various bio-inspired credit assignment methods
- Current bio-inspired methods have not yet consistently matched backpropagation's performance on complex tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bio-plausible credit assignment algorithms enable learning on neuromorphic hardware by removing weight transport and forward/backward locking constraints.
- Mechanism: These algorithms use local learning rules that only require information available at each neuron, eliminating the need for symmetric weight matrices and sequential layer-by-layer processing.
- Core assumption: Local synaptic updates can achieve comparable performance to backpropagation while being implementable in analog circuits.
- Evidence anchors:
  - [abstract] "biologically plausible credit assignment is suitable for neuromorphic hardware implementations due to the locality of their operations"
  - [section] "This is different from processing in existing Von-Neumann architectures, where the division between memory and computing units makes such operations slower and computationally expensive"
  - [corpus] Weak evidence - corpus focuses on multi-agent reinforcement learning, not neuromorphic hardware
- Break condition: If local learning rules cannot achieve sufficient accuracy for the target task, or if analog hardware introduces noise that overwhelms the learning signal.

### Mechanism 2
- Claim: Energy-based formulations of learning (like predictive coding and contrastive Hebbian learning) can approximate backpropagation while being more biologically plausible.
- Mechanism: These methods minimize an energy functional that can be decomposed into task-specific and internal energy terms, with parameter updates derived from local gradients of these energies.
- Core assumption: The energy functional can be designed such that its gradients approximate those of the task loss function.
- Evidence anchors:
  - [abstract] "we survey several vital algorithms that model bio-plausible rules of credit assignment in artificial neural networks"
  - [section] "To optimize the functional in Equation 3, gradients are taken, for each layer, with respect to both the neural activity...and the parameters"
  - [corpus] Weak evidence - corpus neighbors focus on multi-agent RL credit assignment, not energy-based learning
- Break condition: If the energy landscape becomes too complex or multimodal, preventing effective gradient-based optimization.

### Mechanism 3
- Claim: Forward-only learning schemes can train deep networks without any backward information flow, resolving biological implausibilities.
- Mechanism: These algorithms construct local objectives that can be optimized using only forward propagation of information, often by introducing auxiliary variables or contrastive objectives.
- Core assumption: The local objectives can be designed to guide learning toward good solutions despite the absence of global error signals.
- Evidence anchors:
  - [abstract] "avoid weight transport, and enable parallel or asynchronous learning"
  - [section] "In recent years, a different set of biologically-plausible algorithms have emerged that avoid introducing or using feedback pathways that facilitate credit assignment"
  - [corpus] Weak evidence - corpus neighbors focus on multi-agent RL credit assignment, not forward-only learning
- Break condition: If the local objectives are insufficient to provide proper credit assignment across many layers, leading to vanishing or exploding learning signals.

## Foundational Learning

- Concept: Energy-based models and variational inference
  - Why needed here: Many bio-plausible algorithms (predictive coding, contrastive Hebbian learning) are based on minimizing energy functionals or performing variational inference.
  - Quick check question: Can you explain how minimizing an energy functional relates to finding good parameter values for a neural network?

- Concept: Hebbian learning and local plasticity rules
  - Why needed here: Several algorithms use Hebbian-style updates that depend only on local neuron activities, which is key to biological plausibility.
  - Quick check question: What are the limitations of pure Hebbian learning, and how do the reviewed algorithms address these limitations?

- Concept: Neuromorphic hardware principles
  - Why needed here: Understanding why bio-plausible algorithms are particularly suited to neuromorphic implementations requires knowledge of how these systems differ from traditional von Neumann architectures.
  - Quick check question: What are the key architectural differences between neuromorphic and traditional hardware that make bio-plausible algorithms advantageous?

## Architecture Onboarding

- Component map:
  - Local energy functions for each layer or neuron group
  - Mechanisms for propagating teaching signals (either through feedback weights or alternative routes)
  - Local parameter update rules (Hebbian or gradient-based)
  - Optional: auxiliary variables for forward-only methods

- Critical path:
  1. Forward pass to compute activations
  2. Compute local objectives (energy, contrastive loss, etc.)
  3. Compute local gradients with respect to parameters
  4. Update parameters using local rules

- Design tradeoffs:
  - Accuracy vs. biological plausibility: More biologically plausible methods may sacrifice some performance
  - Hardware compatibility vs. flexibility: Methods optimized for neuromorphic hardware may be less general
  - Parallelism vs. convergence speed: Asynchronous updates can enable parallelism but may slow convergence

- Failure signatures:
  - Slow or stalled learning: May indicate poor choice of local objectives or learning rates
  - Instability or oscillations: Could suggest issues with energy landscape or update rules
  - Poor final accuracy: Might indicate insufficient credit assignment across layers

- First 3 experiments:
  1. Implement predictive coding on a simple task (e.g., MNIST) to verify basic functionality
  2. Compare forward-only learning with backpropagation on a deeper network to assess performance gap
  3. Test neuromorphic implementation of contrastive Hebbian learning on a small-scale task to evaluate hardware benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can bio-inspired credit assignment algorithms achieve performance on par with backpropagation while maintaining biological plausibility?
- Basis in paper: [explicit] The paper discusses the need to "emulate the performance of backprop while retaining bio-plausible credit assignment" and that "biologically inspired (bio-inspired) learning methods have not yet reached the consistent and strong performance of backpropagation".
- Why unresolved: The paper notes that while progress has been made, these methods have not yet consistently matched backpropagation's performance on complex tasks. More research is needed to understand learning trajectories and final model states when using bio-inspired methods.
- What evidence would resolve it: Systematic benchmarking of bio-inspired algorithms against backpropagation on diverse tasks and datasets, demonstrating equal or superior performance while maintaining biological plausibility.

### Open Question 2
- Question: What is the theoretical understanding of convergence guarantees and stability for bio-plausible credit assignment methods?
- Basis in paper: [explicit] The paper states there is a "need for theoretical understanding concerning convergence guarantees and the stability of bio-plausible approaches" and that "a robust mathematical theory would provide essential insights into the conditions under which these alternative methods converge".
- Why unresolved: The paper notes that current implementations are "failing to generalize to very deep architectures" and that "a robust mathematical theory would provide essential insights into the conditions under which these alternative methods converge".
- What evidence would resolve it: Mathematical proofs and empirical validation of convergence properties, stability bounds, and failure modes for various bio-inspired credit assignment algorithms across different network architectures and data distributions.

### Open Question 3
- Question: How can bio-inspired credit assignment algorithms be adapted for dynamical environments and time series data?
- Basis in paper: [inferred] The paper mentions that "implementations of such algorithms on edge devices will most likely need to handle dynamical environments" and suggests "focusing on time series data, such as next-frame prediction tasks".
- Why unresolved: The paper states that "as of today, most research focuses on modelling static data" and that "an invaluable direction of research would be to focus on time series data".
- What evidence would resolve it: Successful implementations of bio-inspired credit assignment algorithms on recurrent architectures (e.g., LSTMs) and their performance on time series benchmarks, demonstrating competitive results with backpropagation-based approaches.

## Limitations
- The paper is primarily theoretical and lacks extensive experimental validation of the surveyed algorithms
- Biological plausibility claims rely on analogies to neural processes that may not fully capture biological learning complexity
- Computational benefits on neuromorphic hardware are not quantitatively demonstrated
- Current bio-inspired methods have not yet consistently matched backpropagation's performance on complex tasks

## Confidence

- Confidence in the core claims: Medium
  - Theoretical foundations are well-established but practical implementation challenges and performance trade-offs are not fully explored

- Confidence in neuromorphic hardware advantages: Low
  - Lack of concrete implementation results or benchmarks to demonstrate claimed benefits

- Confidence in biological plausibility claims: Medium
  - Based on reasonable analogies to neural processes but may not capture full biological complexity

## Next Checks

1. Implement and benchmark contrastive Hebbian learning on a standard vision dataset to compare with backpropagation in terms of accuracy and convergence speed

2. Develop a small-scale neuromorphic prototype implementing predictive coding and measure energy consumption versus traditional hardware

3. Test forward-only learning algorithms on a deep network with many layers to assess whether credit assignment remains effective without backward propagation