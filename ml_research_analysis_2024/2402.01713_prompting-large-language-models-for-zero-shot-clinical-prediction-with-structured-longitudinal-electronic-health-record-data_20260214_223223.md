---
ver: rpa2
title: Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured
  Longitudinal Electronic Health Record Data
arxiv_id: '2402.01713'
source_url: https://arxiv.org/abs/2402.01713
tags:
- data
- reference
- range
- unit
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of integrating Large Language
  Models (LLMs) with structured longitudinal Electronic Health Record (EHR) data,
  particularly for zero-shot clinical prediction in scenarios with limited historical
  data, such as new disease outbreaks. The core method involves a carefully designed
  prompting framework that accounts for EHR characteristics like units, reference
  ranges, and in-context learning strategies.
---

# Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data

## Quick Facts
- arXiv ID: 2402.01713
- Source URL: https://arxiv.org/abs/2402.01713
- Reference count: 40
- The paper addresses the challenge of integrating Large Language Models (LLMs) with structured longitudinal Electronic Health Record (EHR) data, particularly for zero-shot clinical prediction in scenarios with limited historical data, such as new disease outbreaks.

## Executive Summary
This paper tackles the challenge of using Large Language Models (LLMs) for zero-shot clinical prediction with structured longitudinal Electronic Health Record (EHR) data, particularly in scenarios with limited historical data like new disease outbreaks. The authors propose a carefully designed prompting framework that accounts for EHR characteristics such as units, reference ranges, and in-context learning strategies. Experiments on the MIMIC-IV and TJH datasets demonstrate that with this framework, LLMs, especially GPT-4, improve prediction performance by about 35% in tasks like mortality, length-of-stay, and 30-day readmission, surpassing traditional machine learning models in few-shot settings. The code is publicly available for reproducibility.

## Method Summary
The paper addresses zero-shot clinical prediction using LLMs by designing prompts that effectively represent structured longitudinal EHR data. The method involves preprocessing EHR data with LOCF imputation, constructing prompts using a feature-wise format with clinical context (units/reference ranges), and employing in-context learning with 1-2 examples. The approach is evaluated on MIMIC-IV and TJH datasets for tasks including mortality prediction, length-of-stay estimation, and 30-day readmission prediction, using zero-shot evaluation without model fine-tuning.

## Key Results
- LLMs with the proposed framework achieve about 35% improvement in prediction performance for mortality, length-of-stay, and 30-day readmission tasks.
- Feature-wise input format is more effective than visit-wise for capturing temporal dynamics in EHR data.
- Adding units and reference ranges to prompts significantly improves LLM prediction accuracy by providing essential clinical context.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The feature-wise input format enables LLMs to better capture temporal dynamics in EHR data compared to visit-wise format.
- **Mechanism:** By presenting all visit data for a patient in a single batch organized by feature, the LLM can directly observe patterns and changes across time for each feature, leveraging its pattern recognition capabilities.
- **Core assumption:** LLMs can effectively process comma-separated numerical sequences when provided with appropriate context about units and reference ranges.
- **Evidence anchors:**
  - [abstract] "The longitudinal aspect demands prompts that effectively represent time series data"
  - [section] "Employing a feature-wise format for inputting EHR data is more effective in enabling LLMs to discern dynamic changes in features"
  - [corpus] Weak evidence - no corpus findings directly address this specific mechanism
- **Break condition:** If the number of visits becomes too large for the context window, or if the numerical sequences become too long for the LLM to process effectively.

### Mechanism 2
- **Claim:** Adding units and reference ranges significantly improves LLM prediction accuracy by providing essential context for numerical values.
- **Mechanism:** Without units and reference ranges, numerical values in EHR data are ambiguous and can be misinterpreted. Providing this context allows the LLM to understand the meaning and significance of each value in a clinical context.
- **Core assumption:** LLMs have sufficient medical knowledge to interpret values when provided with appropriate units and reference ranges.
- **Evidence anchors:**
  - [abstract] "the knowledge-infused nature necessitates the integration of reliable medical knowledge context"
  - [section] "Adding both of them achieves the best performance with a relative improvement of 19.35% and 13.22% in terms of AUROC"
  - [corpus] No direct corpus evidence, but related work on EHR-RAG suggests contextual grounding is important
- **Break condition:** If the reference ranges are incorrect or if the LLM's medical knowledge is outdated or incomplete.

### Mechanism 3
- **Claim:** In-context learning with a small number of examples (1-2) significantly improves LLM performance on EHR prediction tasks.
- **Mechanism:** Providing examples of the input-output format helps the LLM understand the task structure and expected response format, reducing ambiguity in zero-shot settings.
- **Core assumption:** The examples provided are representative enough to guide the LLM without introducing bias.
- **Evidence anchors:**
  - [abstract] "employing an in-context learning strategy that aligns with clinical contexts"
  - [section] "the inclusion of one or two examples significantly improves the LLM's performance"
  - [corpus] Weak evidence - corpus shows related work on few-shot learning but not specifically for EHR tasks
- **Break condition:** If too many examples are provided, causing catastrophic forgetting or exceeding context limits.

## Foundational Learning

- **Concept: Prompt engineering for structured data**
  - Why needed here: LLMs are primarily trained on unstructured text, so converting structured EHR data into a format they can understand is crucial
  - Quick check question: What are the key differences between feature-wise and visit-wise input formats, and when would each be appropriate?

- **Concept: In-context learning and few-shot prompting**
  - Why needed here: In zero-shot settings with limited data, providing examples helps the LLM understand the task without requiring fine-tuning
  - Quick check question: How does the number of examples provided affect LLM performance, and what is the optimal range?

- **Concept: Medical knowledge integration in prompts**
  - Why needed here: EHR data contains domain-specific knowledge that LLMs need to interpret correctly for accurate predictions
  - Quick check question: Why are units and reference ranges essential for accurate interpretation of EHR data by LLMs?

## Architecture Onboarding

- **Component map:**
  - Data preprocessing module -> Prompt template engine -> LLM interface -> Evaluation framework

- **Critical path:**
  1. Load and preprocess EHR data
  2. Generate prompt with clinical context and examples
  3. Send prompt to LLM
  4. Parse and evaluate LLM response
  5. Calculate performance metrics

- **Design tradeoffs:**
  - Feature-wise vs. visit-wise input format: Feature-wise provides better temporal pattern recognition but may lose visit-level context
  - Imputation vs. NaN reservation: Imputation improves accuracy but adds preprocessing complexity
  - Number of examples: More examples improve guidance but risk catastrophic forgetting

- **Failure signatures:**
  - High missing rate: LLM struggles to interpret input format or lacks sufficient context
  - Low AUROC/AUPRC: Prompt structure may not be capturing relevant patterns
  - Context window exceeded: Input format or number of examples too large for LLM capacity

- **First 3 experiments:**
  1. Compare feature-wise vs. visit-wise input formats on a simple mortality prediction task
  2. Test impact of adding units and reference ranges on prediction accuracy
  3. Evaluate optimal number of examples for in-context learning (0, 1, 2, 3)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Large Language Models (LLMs) perform in zero-shot clinical prediction tasks for emerging diseases beyond COVID-19, such as new viral infections or rare diseases?
- Basis in paper: [inferred] The paper focuses on COVID-19 data but discusses the potential of LLMs for emerging diseases.
- Why unresolved: The study primarily uses COVID-19 datasets, and it is unclear if the observed performance generalizes to other emerging diseases with different characteristics.
- What evidence would resolve it: Testing the prompting framework on datasets from other emerging diseases or rare conditions to compare performance and adaptability.

### Open Question 2
- Question: What is the impact of incorporating more advanced reasoning techniques, such as chain-of-thought prompting, on the accuracy of LLM predictions in structured longitudinal EHR data?
- Basis in paper: [explicit] The paper briefly mentions the potential of chain-of-thought reasoning and other strategies for future work.
- Why unresolved: The study did not implement or test chain-of-thought prompting, so its effectiveness in this context remains unexplored.
- What evidence would resolve it: Implementing and benchmarking chain-of-thought prompting against the current framework to measure improvements in prediction accuracy and interpretability.

### Open Question 3
- Question: How do LLMs handle uncertainty in predictions, and can uncertainty quantification improve clinical decision-making when using these models?
- Basis in paper: [explicit] The paper notes that LLMs output a single logit value, limiting insight into the reasoning behind predictions.
- Why unresolved: The study does not explore methods for quantifying or communicating uncertainty in LLM predictions.
- What evidence would resolve it: Developing and testing uncertainty quantification methods, such as confidence intervals or probabilistic outputs, and evaluating their impact on clinical decision-making.

## Limitations
- The study focuses on zero-shot learning without model fine-tuning, which may not represent the optimal use of LLMs for EHR prediction.
- Performance depends heavily on the quality and completeness of units and reference ranges, which may not be consistently available across all healthcare systems.
- The experiments are limited to specific clinical prediction tasks (mortality, length-of-stay, readmission) and may not generalize to other prediction scenarios.

## Confidence
- **High Confidence:** The general effectiveness of prompt engineering for structured EHR data (supported by multiple experiments and comparisons with baselines)
- **Medium Confidence:** The specific mechanisms for optimal prompt design (feature-wise format, units/reference ranges, 1-2 examples) - while supported by ablation studies, the exact optimal parameters may vary by task
- **Low Confidence:** Generalizability to other clinical prediction tasks and healthcare systems beyond the tested MIMIC-IV and TJH datasets

## Next Checks
1. **Cross-Institutional Validation:** Test the prompting framework on EHR data from additional hospitals and healthcare systems to assess generalizability and identify any institution-specific adjustments needed for optimal performance.

2. **Temporal Window Sensitivity Analysis:** Systematically evaluate how prediction performance changes with different time windows of historical data (e.g., 6 months vs. 2 years vs. entire history) to understand the impact of temporal scope on LLM predictions.

3. **Real-World Deployment Pilot:** Conduct a controlled pilot implementation in a clinical setting to assess not just prediction accuracy but also practical factors like response time, integration with existing workflows, and clinician acceptance of LLM-generated predictions.