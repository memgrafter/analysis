---
ver: rpa2
title: Boosting Zero-Shot Crosslingual Performance using LLM-Based Augmentations with
  Effective Data Selection
arxiv_id: '2407.10582'
source_url: https://arxiv.org/abs/2407.10582
tags:
- data
- please
- generate
- target
- inst
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates using large language model (LLM) generations
  to improve zero-shot crosslingual transfer for low-resource languages. The authors
  generate synthetic task-specific data in English via zero-shot prompting, then translate
  it into target languages.
---

# Boosting Zero-Shot Crosslingual Performance using LLM-Based Augmentations with Effective Data Selection
## Quick Facts
- arXiv ID: 2407.10582
- Source URL: https://arxiv.org/abs/2407.10582
- Reference count: 25
- Primary result: Synthetic data generation + effective selection improves zero-shot crosslingual transfer by up to 7.13 absolute points

## Executive Summary
This paper addresses the challenge of zero-shot crosslingual transfer for low-resource languages by leveraging large language model (LLM) generations. The authors propose a pipeline that generates synthetic task-specific data in English via zero-shot prompting, translates it into target languages, and employs data selection strategies to identify the most valuable examples. Through experiments on sentiment analysis and natural language inference tasks across Hindi, Marathi, Urdu, and Swahili, they demonstrate significant performance gains compared to baselines that use only source data or all generations without selection. The study highlights the importance of combining effective data selection with soft pseudolabeling approaches for improving crosslingual model performance.

## Method Summary
The authors investigate using LLM-generated synthetic data to improve zero-shot crosslingual transfer for low-resource languages. They generate task-specific data in English through zero-shot prompting, then translate it to target languages. Two pseudolabeling approaches are explored: prompting the LLM directly or using a teacher model trained on source data. To address the challenge of selecting representative subsets from potentially thousands of generated examples, they implement data selection strategies based on teacher model label probabilities, including random sampling, top-k selection, and diversity-based sampling. They also evaluate selection based on ambiguous and easy examples using training dynamics. The methodology is evaluated on sentiment analysis and natural language inference tasks across four target languages.

## Key Results
- Significant performance gains up to 7.13 absolute points compared to baselines using only source data
- Best results achieved with student models trained on soft teacher labels from selected synthetic data
- Data selection strategies consistently outperform using all generated data without filtering
- Both sentiment analysis and natural language inference tasks show improvements across all four target languages

## Why This Works (Mechanism)
The mechanism relies on generating diverse, task-specific synthetic examples that expose models to varied linguistic patterns and edge cases not present in the original training data. By translating these examples into target languages, the approach creates relevant training data for low-resource languages where annotated data is scarce. The data selection component ensures that only the most informative examples are retained, preventing model degradation from low-quality or redundant synthetic data. Soft pseudolabels from teacher models provide more nuanced supervision than hard labels, allowing student models to learn from the confidence distribution rather than binary decisions.

## Foundational Learning
- **Zero-shot crosslingual transfer**: Why needed - enables models to work on languages without any task-specific training data; Quick check - model performance on target language without adaptation
- **Synthetic data generation**: Why needed - creates task-specific examples for languages lacking annotations; Quick check - quality and diversity of generated examples
- **Data selection strategies**: Why needed - filters out low-quality synthetic data to improve model training efficiency; Quick check - performance comparison with/without selection
- **Soft vs hard pseudolabels**: Why needed - soft labels provide confidence information for better learning; Quick check - accuracy difference between soft and hard label training
- **Teacher-student training**: Why needed - allows knowledge distillation from teacher models to improve student performance; Quick check - student model performance relative to teacher
- **Translation quality impact**: Why needed - translation errors can propagate to target language models; Quick check - correlation between translation quality and model performance

## Architecture Onboarding
Component map: LLM generation -> Data selection -> Translation -> Teacher model training -> Student model training
Critical path: Synthetic data generation and selection form the bottleneck, as poor quality or inappropriate selection directly impacts downstream model performance
Design tradeoffs: Quality vs quantity in synthetic data generation, computational cost of selection vs performance gains, soft vs hard label benefits
Failure signatures: Performance degradation when selection strategies fail to filter low-quality examples, catastrophic forgetting when synthetic data distribution differs significantly from target domain
First experiments: 1) Baseline comparison without synthetic data, 2) Full generation without selection, 3) Different selection strategies comparison

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to four target languages (Hindi, Marathi, Urdu, Swahili), raising questions about generalization to other language families
- Performance gains evaluated on relatively small-scale tasks (XNLI and sentiment analysis) that may not represent more complex crosslingual scenarios
- Effectiveness of data selection strategies evaluated only in combination with specific pseudolabeling approaches, making it difficult to isolate selection contributions
- Assumes access to high-quality translation systems, which may not be available for all low-resource language pairs

## Confidence
- Data Selection Effectiveness: High confidence - systematic ablation studies compare multiple strategies with consistent improvements
- Soft vs Hard Pseudolabels: Medium confidence - limited comparison to specific teacher-student configurations without exploring underlying reasons
- Crosslingual Generalization Claims: Low confidence - findings based on limited language set and specific task types may not generalize broadly

## Next Checks
1. **Multilingual Scaling Test**: Evaluate methodology on 10+ additional languages from diverse families (including non-Latin scripts and low-resource pairs without strong translation support)
2. **Quality Control Analysis**: Conduct detailed error analysis comparing synthetic data quality between LLM prompting and teacher model pseudolabeling, including human evaluation
3. **Resource Efficiency Benchmark**: Measure computational and annotation cost trade-offs between generating all synthetic data versus using selection strategies across different dataset sizes