---
ver: rpa2
title: Evaluation of Few-Shot Learning for Classification Tasks in the Polish Language
arxiv_id: '2404.17832'
source_url: https://arxiv.org/abs/2404.17832
tags:
- label
- instruction
- labels
- language
- demonstration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates few-shot learning methods for Polish text classification,
  comparing techniques like fine-tuning, linear probing, SetFit, and in-context learning
  (ICL) across 7 native Polish datasets. ICL with commercial models (GPT-3.5, GPT-4)
  achieves the best performance, but a 14 percentage point gap remains versus full-dataset
  fine-tuning of HerBERT-large.
---

# Evaluation of Few-Shot Learning for Classification Tasks in the Polish Language

## Quick Facts
- arXiv ID: 2404.17832
- Source URL: https://arxiv.org/abs/2404.17832
- Reference count: 40
- In-context learning with GPT-3.5/GPT-4 achieves best few-shot performance, but remains 14pp behind full-dataset fine-tuning of HerBERT-large.

## Executive Summary
This paper evaluates few-shot learning methods for Polish text classification across 7 native Polish datasets. The authors compare fine-tuning, linear probing, SetFit, and in-context learning (ICL) approaches, finding that commercial models like GPT-3.5 and GPT-4 achieve the best performance in few-shot scenarios. However, a significant gap of 14 percentage points remains between these methods and full-dataset fine-tuning of HerBERT-large. The study also demonstrates that continual pre-training of open-source models on Polish corpora improves zero-shot performance, with SetFit emerging as the most stable second-best approach.

## Method Summary
The authors evaluate four few-shot learning methods across 7 Polish classification datasets using 16-shot sampling per class with 5 random seeds. Methods include in-context learning with commercial models (GPT-3.5, GPT-4), full fine-tuning with LoRA, parameter-efficient SetFit, and linear probing with frozen embeddings. Evaluation metrics are F1 for binary tasks and accuracy for multi-class. The study compares these approaches against a full-dataset fine-tuning baseline using HerBERT-large. Handcrafted ICL prompts are provided, and continual pre-training is tested on models like Bielik-7b and Trurl-13b.

## Key Results
- ICL with commercial models (GPT-3.5, GPT-4) achieves the best few-shot performance across all datasets
- A 14 percentage point gap remains between zero-shot GPT-4 and HerBERT-large trained on full datasets
- SetFit demonstrates consistent performance as the second-best approach with HerBERT backbones
- Continual pre-training on Polish corpora improves zero-shot performance but shows inconsistent benefits in few-shot regimes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning with commercial models outperforms fine-tuning methods in few-shot scenarios.
- Mechanism: Commercial LLMs leverage large-scale pre-training and instruction tuning to generalize from limited examples without parameter updates, reducing overfitting risk in scarce data regimes.
- Core assumption: Commercial models have sufficient capacity and diverse pre-training to handle Polish classification tasks via demonstrations.
- Evidence anchors:
  - [abstract] "ICL achieves the best performance, with commercial models like GPT-3.5 and GPT-4 attaining the best performance."
  - [section 5.3] "ICL using commercial models like GPT-4 shows superior performance... however, there is a gap of 14pp between zero-shot GPT-4 and HerBERT-large trained on the full dataset."
  - [corpus] Weak: Corpus shows related works on ICL but no direct comparative benchmarks for Polish.
- Break condition: If the commercial model's context window is too small to fit demonstrations (e.g., Polemo2 task), ICL performance drops to zero.

### Mechanism 2
- Claim: Continual pre-training on target language improves zero-shot performance but benefits in few-shot are inconsistent.
- Mechanism: Fine-tuning open-source models on Polish corpora adapts their embeddings and representations to the target language, enhancing zero-shot inference; however, in few-shot, adaptation noise may outweigh benefits.
- Core assumption: Language-specific continual pre-training yields better representations than general-purpose models for the target language.
- Evidence anchors:
  - [abstract] "Results for ICL indicate that continual pre-training of models like Mistral-7b or Llama-2-13b on Polish corpora is beneficial. This is confirmed by the improved performances of Bielik-7b and Trurl-13b, respectively."
  - [section 5.3] "Our tests confirm that continual pre-training of Mistral or Llama-2 on Polish corpora yields better zero-shot results... However, in the few-shot regime, only Trurl-2-13b is better than its English counterpart."
  - [corpus] Weak: Corpus neighbors discuss general multilingual adaptation but not specific Polish pre-training outcomes.
- Break condition: If continual pre-training is done without sufficient quality control, performance can degrade (e.g., Krakowiak-7b-v2 worse than Mistral-7b-instruct).

### Mechanism 3
- Claim: Parameter-efficient fine-tuning (SetFit) provides stable second-best performance with much smaller models than ICL.
- Mechanism: SetFit combines metric learning to enhance embeddings and a frozen backbone with a linear classifier, reducing overfitting and computational cost while maintaining performance close to full fine-tuning.
- Core assumption: Contrastive learning on few examples yields better embeddings than random initialization for the downstream task.
- Evidence anchors:
  - [abstract] "Among the techniques, SetFit emerges as the second-best approach, closely followed by linear probing."
  - [section 5.2] "SetFit demonstrated consistent performance across HerBERT-based backbones."
  - [corpus] Weak: Corpus shows general few-shot PEFT studies but not Polish-specific SetFit evaluations.
- Break condition: If the embedding backbone is not well-suited to the task domain, SetFit performance may degrade.

## Foundational Learning

- Concept: Few-shot learning tradeoffs
  - Why needed here: The paper compares zero-shot, few-shot, and full-data performance; understanding sample efficiency is key to interpreting results.
  - Quick check question: Why does ICL sometimes outperform fine-tuning with only 16 examples?

- Concept: Language model adaptation
  - Why needed here: Models like Bielik-7b and Trurl-13b are adapted from English to Polish; knowing how adaptation works explains performance gaps.
  - Quick check question: What is the difference between zero-shot, few-shot, and continual pre-training?

- Concept: Evaluation metrics in imbalanced data
  - Why needed here: Datasets like PAC and CDSC-E are imbalanced; metrics like F1 for binary and accuracy for multi-class affect benchmark fairness.
  - Quick check question: How does class imbalance affect the interpretation of F1 vs accuracy?

## Architecture Onboarding

- Component map:
  Datasets -> 7 Polish classification tasks (PAC, Polemo2, DYK, CDSC-E, NKJP-NER, CBD, CST-Wikinews) -> Models (Commercial GPT-3.5/GPT-4, multilingual Gecko, Polish-adapted HerBERT/Bielik-7b/Trurl-13b, English base Llama-2/Mistral-7b) -> Methods (ICL, fine-tuning with LoRA, SetFit, linear probing, baseline XGBoost) -> Evaluation (5 seeds, 16-shot sampling, macro-F1/accuracy)

- Critical path:
  1. Load dataset and sample 16 examples per class.
  2. For ICL: generate prompts, run inference, parse labels.
  3. For fine-tuning: optimize hyperparameters, train with LoRA, evaluate.
  4. For linear probing: extract embeddings, train logistic regression, evaluate.
  5. Aggregate results across seeds and report mean/std.

- Design tradeoffs:
  - ICL: high performance, expensive inference, no parameter updates.
  - Fine-tuning: flexible, but unstable with few examples and large variance.
  - SetFit: stable, efficient, but limited by backbone quality.
  - Linear probing: fastest, but sensitive to embedding quality.

- Failure signatures:
  - ICL: invalid predictions (label not in response), context overflow.
  - Fine-tuning: high variance across seeds, overfitting to small data.
  - SetFit: poor embeddings leading to random classifier.
  - Linear probing: unstable metrics when embedding variance is high.

- First 3 experiments:
  1. Run ICL with GPT-3.5 on PAC (binary, imbalanced) to observe label parsing and variance.
  2. Run SetFit on NKJP-NER (multi-class, longer inputs) to test embedding stability.
  3. Run linear probing with SBERT-large on CBD (binary, short texts) to benchmark embedding quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does Mistral-7b-instruct perform so well in Polish despite not being specifically trained on a large-scale Polish corpus?
- Basis in paper: [explicit] The authors note this as an intriguing observation and highlight it as a contribution to broader research on adapting pre-trained LLMs to non-English languages.
- Why unresolved: The paper does not provide a definitive explanation, suggesting further analysis is needed.
- What evidence would resolve it: Comparative studies isolating language-specific fine-tuning effects versus inherent model capabilities, or analysis of the model's internal representations for Polish text.

### Open Question 2
- Question: What are the optimal numbers of training examples for linear probing to achieve specific performance levels across different Polish NLP tasks?
- Basis in paper: [explicit] The authors identify this as an important question and provide preliminary scaling analysis showing a logarithmic relationship between examples and performance.
- Why unresolved: The analysis is preliminary and focused on average performance, not task-specific thresholds.
- What evidence would resolve it: Task-specific learning curves showing performance vs. training set size for various embedding models.

### Open Question 3
- Question: How does the quality of automatically generated responses from LLMs affect evaluation metrics in few-shot learning for Polish?
- Basis in paper: [explicit] The authors acknowledge this as a limitation, noting they used exact match metrics with minimal preprocessing and that this might affect results.
- Why unresolved: The paper did not explore more sophisticated evaluation methods or the impact of response generation quality.
- What evidence would resolve it: Human evaluation studies comparing automatic metrics with human judgments across different response quality levels.

## Limitations

- Commercial model performance variance: Results rely on API calls to GPT-3.5 and GPT-4, whose behavior may change over time or with different prompt versions, introducing reproducibility concerns.
- Limited seed diversity: Only 5 fixed seeds (18, 22, 37, 69, 98) are used for 16-shot sampling, which may not capture the full variance in few-shot regimes.
- Polish language adaptation quality: While continual pre-training improves zero-shot performance, the quality and consistency of language-specific adaptation for open-source models is not systematically evaluated across all datasets.

## Confidence

- ICL superiority claim: High confidence - supported by consistent experimental results across 7 datasets and direct comparison with multiple baselines.
- Continual pre-training benefits: Medium confidence - improvements are shown but inconsistently across models and data regimes.
- SetFit stability: High confidence - demonstrated across all HerBERT backbones with low variance.
- Fine-tuning instability: High confidence - clear variance patterns observed across seeds and datasets.

## Next Checks

1. **Seed robustness check**: Repeat 16-shot experiments with 20+ random seeds and compare variance distribution to original 5-seed results. If variance increases significantly, current conclusions about method stability may be overstated.

2. **Prompt ablation study**: Systematically modify or remove components of the ICL prompts (demonstrations, instruction format, label mapping) to measure sensitivity of commercial model performance. This will quantify how much performance depends on prompt engineering vs model capability.

3. **Cross-lingual transfer validation**: Test whether ICL prompts designed for Polish classification work for other languages (e.g., English, German) using the same commercial models. Poor cross-lingual transfer would indicate prompts are language-specific rather than leveraging general reasoning abilities.