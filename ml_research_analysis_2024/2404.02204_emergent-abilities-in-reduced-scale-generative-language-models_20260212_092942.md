---
ver: rpa2
title: Emergent Abilities in Reduced-Scale Generative Language Models
arxiv_id: '2404.02204'
source_url: https://arxiv.org/abs/2404.02204
tags:
- language
- simple
- dataset
- regular
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether emergent in-context learning (ICL)\
  \ abilities in language models are strictly tied to model size or can be unlocked\
  \ by simplifying the pre-training data. The authors pre-train 36 causal language\
  \ models (1M\u2013165M parameters) on a simplified English dataset filtered using\
  \ a child-directed speech vocabulary."
---

# Emergent Abilities in Reduced-Scale Generative Language Models

## Quick Facts
- arXiv ID: 2404.02204
- Source URL: https://arxiv.org/abs/2404.02204
- Reference count: 40
- Smaller models (100M-165M) trained on simplified data achieve zero-shot performance comparable to 1B-parameter models on standard tasks

## Executive Summary
This paper challenges the assumption that emergent in-context learning (ICL) abilities in language models are strictly tied to model size. The authors demonstrate that pre-training smaller causal language models (1M-165M parameters) on a simplified English dataset filtered using child-directed speech vocabulary can unlock ICL capabilities comparable to much larger models. Simplified models achieve 0.64 average accuracy on simplified tasks versus 0.61 for Pythia 1B on standard tasks, despite being six times smaller. The study also reveals power law relationships between evaluation loss and compute, dataset size, and model size, suggesting that language simplification enables ICL capabilities to emerge earlier in smaller models.

## Method Summary
The authors pre-trained 36 causal language models with parameter sizes ranging from 1M to 165M on a simplified version of the Pile dataset. The simplification process involved filtering the dataset using a vocabulary derived from child-directed speech (CDS) to remove rare words and complex language structures. Models were evaluated on seven tasks including PIQA, HellaSwag, and filtered versions of the BLiMP dataset. Performance was compared against larger models like Pythia 1B trained on unrestricted language. The study also investigated scaling relationships by plotting evaluation loss against compute, dataset size, and model size to identify power law relationships.

## Key Results
- Smaller models (100M-165M parameters) trained on simplified data achieve 0.64 average zero-shot accuracy, comparable to Pythia 1B's 0.61 on standard tasks
- Simplified models outperform regular models on most configurations for PIQA and filtered BLiMP tasks
- Power law relationships identified between evaluation loss and compute, dataset size, and model size with R² values over 0.74
- Language simplification enables ICL capabilities to emerge in models six times smaller than previously required

## Why This Works (Mechanism)
Assumption: The mechanism likely involves reducing cognitive load on smaller models by removing rare words and complex syntactic structures, allowing them to focus on learning core language patterns more efficiently. The simplified vocabulary may reduce the search space during inference, enabling smaller models to achieve performance levels that would normally require larger parameter counts. However, the paper does not explicitly detail the underlying mechanisms driving this phenomenon.

## Foundational Learning
Assumption: The simplified training approach builds on established principles of curriculum learning and data efficiency, where presenting information in progressively simpler forms can accelerate learning. By using child-directed speech as a filtering mechanism, the authors leverage linguistic research showing that simplified language facilitates faster language acquisition in humans. The power law relationships observed also align with established scaling laws in machine learning, suggesting that compute, data, and model size follow predictable relationships even in the simplified setting.

## Architecture Onboarding
Assumption: The study uses standard transformer architectures without architectural modifications, focusing instead on data simplification as the primary intervention. All models follow the causal language model paradigm with standard attention mechanisms and positional encoding. The pre-training process employs standard optimization techniques (likely AdamW) and typical hyper-parameters for transformer training. The evaluation framework uses established benchmarks adapted to the simplified language setting, maintaining consistency with standard language model evaluation practices.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: At what specific model sizes and dataset scales do emergent abilities like few-shot ICL and chain-of-thought reasoning appear when training on simplified language?
- Basis in paper: [inferred] The authors note that their 100M-165M parameter models show no significant improvement in few-shot learning with increased in-context examples, and suggest these sizes may be insufficient for chain-of-thought reasoning to emerge in the simplified language setting.
- Why unresolved: The study focused on zero-shot learning and did not systematically explore the full range of model and dataset sizes needed to observe these specific emergent abilities.
- What evidence would resolve it: Training and evaluating models across a broader range of sizes (e.g., 10M to 1B parameters) on varying amounts of simplified data, measuring few-shot performance and chain-of-thought capabilities.

### Open Question 2
- Question: How does instruction fine-tuning affect the in-context learning abilities of models trained on simplified language?
- Basis in paper: [explicit] The authors state that instruction tuning may considerably affect ICL abilities but kept investigation of this effect for future work.
- Why unresolved: The study only evaluated zero-shot and few-shot ICL on pre-trained models without any instruction tuning.
- What evidence would resolve it: Fine-tuning a subset of the simplified-data models with instruction datasets and comparing their zero-shot and few-shot performance to their non-instruction-tuned counterparts.

### Open Question 3
- Question: Do simplified language models exhibit different scaling laws compared to models trained on unrestricted language?
- Basis in paper: [explicit] The authors found a power law relationship between evaluation loss and compute, dataset size, and model size for their simplified models, with R² values over 0.74.
- Why unresolved: The study did not compare these scaling laws to those observed in models trained on unrestricted language, nor did it explore whether the scaling exponents differ.
- What evidence would resolve it: Deriving and comparing the scaling law exponents (B values) for both simplified and unrestricted language models across the same ranges of compute, data, and model sizes.

## Limitations
- The simplified dataset methodology may have removed domain-specific terminology crucial for certain tasks, potentially biasing results toward simpler language tasks
- The study focused on only 7 tasks, limiting generalizability across the broader spectrum of language understanding challenges
- Power law relationships were derived from models up to 165M parameters, with uncertain extrapolation to truly large language models

## Confidence
- High confidence: Simplified models achieve comparable zero-shot performance to larger models on the tested tasks
- Medium confidence: Language simplification is the primary driver of performance improvements (dataset quality effects not fully controlled)
- Medium confidence: Power law relationships generalize across compute, data, and model size (extrapolation beyond tested scales uncertain)

## Next Checks
1. Test the simplified training approach on a broader, more diverse set of language understanding tasks including those requiring specialized domain knowledge to assess generalizability limits
2. Conduct ablation studies varying the degree of vocabulary simplification to identify the optimal balance between simplification and information retention
3. Scale experiments to models exceeding 1B parameters to verify whether the observed power laws hold at larger scales and to test whether simplified data can accelerate emergence of more complex capabilities