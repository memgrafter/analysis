---
ver: rpa2
title: Score Change of Variables
arxiv_id: '2412.07904'
source_url: https://arxiv.org/abs/2412.07904
tags:
- score
- matching
- where
- function
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a change of variables formula for score\
  \ functions, showing how the score of a transformed variable can be expressed in\
  \ terms of the original score and the transformation's Jacobian. The formula enables\
  \ two key applications: a reverse-time It\xF4 lemma for diffusion models that allows\
  \ sampling in a transformed space using scores learned in the original space, and\
  \ a generalized sliced score matching method that extends linear projections to\
  \ arbitrary smooth transformations."
---

# Score Change of Variables

## Quick Facts
- arXiv ID: 2412.07904
- Source URL: https://arxiv.org/abs/2412.07904
- Authors: Stephen Robbins
- Reference count: 40
- Primary result: Introduces a change of variables formula for score functions enabling diffusion models to sample in transformed spaces using scores learned in original space, and generalizes sliced score matching to arbitrary smooth transformations.

## Executive Summary
This paper introduces a change of variables formula for score functions that allows score-based models to operate in transformed spaces. The formula shows how the score of a transformed variable can be expressed in terms of the original score and the transformation's Jacobian. This enables two key applications: sampling in transformed spaces using scores learned in the original space for diffusion models, and extending sliced score matching from linear projections to arbitrary smooth transformations. The authors demonstrate these advances through diffusion on the probability simplex for chess positions and density estimation experiments on UCI datasets.

## Method Summary
The paper presents a change of variables formula for score functions that transforms the score of a variable y = φ(x) using the inverse Jacobian and divergence terms. This enables training score models in unconstrained spaces while sampling in transformed spaces via a reverse-time Itô lemma. For score matching, the authors generalize sliced score matching to arbitrary smooth transformations, introducing a variance-reduced variant for quadratic transformations. The method involves computing Jacobian matrices and their divergences during training, with applications demonstrated in diffusion models for chess positions and density estimation on UCI datasets.

## Key Results
- Demonstrates diffusion on the probability simplex for chess positions, training in unconstrained space but sampling in the simplex
- Shows generalized sliced score matching (GSSM) with variance reduction outperforms traditional sliced score matching on RedWine and Parkinsons datasets
- Achieves competitive performance with traditional methods on WhiteWine dataset using GSSM-VR
- Establishes theoretical framework for transforming score functions under smooth invertible mappings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The score function transforms under a smooth invertible mapping by combining the original score with Jacobian and divergence terms.
- Mechanism: When transforming variables via y = φ(x), the transformed score ∇y log q(y) equals the original score transformed by the inverse Jacobian Jφ−1(y)⊤ plus an additional divergence term accounting for the geometry of the transformation.
- Core assumption: The transformation φ is smooth, invertible, and twice continuously differentiable, ensuring the change of variables formula applies to densities and their gradients.
- Evidence anchors:
  - [abstract]: "for a smooth, invertible transformation y = φ(x), the transformed score function ∇y log q(y) can be expressed directly in terms of ∇x log p(x)"
  - [section 2.2]: "∇y log q(y) = Jφ−1(y)⊤∇x log p(x) + ∇x · [Jφ−1(φ(x))⊤] |x=φ−1(y)"
  - [corpus]: Missing direct citations, but the mechanism is well-established in differential geometry and probability theory
- Break condition: The transformation fails to be bijective or sufficiently smooth (e.g., non-differentiable regions, singularities in the Jacobian).

### Mechanism 2
- Claim: Score-based diffusion models can sample in a transformed space using scores learned in the original space without directly learning transformed scores.
- Mechanism: The reverse-time Itô lemma extends to transformed diffusion processes by incorporating the score change of variables formula, allowing the reverse SDE coefficients to be computed from the original space score function.
- Core assumption: The transformation preserves the structure needed for the diffusion process to remain well-defined in the transformed space.
- Evidence anchors:
  - [abstract]: "we establish a reverse-time Itô lemma for score-based diffusion models, allowing the use of ∇x log pt(x) to reverse an SDE in the transformed space"
  - [section 3.1]: "This result enables sampling in the transformed space while using score functions learned in the original space"
  - [corpus]: Weak - related papers focus on conditional score learning but don't directly address reverse-time Itô lemmas for transformed spaces
- Break condition: The transformation introduces non-trivial coupling between drift and diffusion terms that cannot be handled by the inverse Jacobian alone.

### Mechanism 3
- Claim: Generalized Sliced Score Matching extends linear projections to arbitrary smooth transformations while maintaining computational tractability.
- Mechanism: By projecting data onto the gradient of a smooth function ∇v(x) instead of random linear projections, GSSM captures nonlinear structure in the data while still allowing efficient score estimation through the change of variables formula.
- Core assumption: The smooth function v(x) is twice continuously differentiable and its gradient is non-zero almost everywhere.
- Evidence anchors:
  - [abstract]: "extending traditional sliced score matching from linear projections to arbitrary smooth transformations"
  - [section 3.2.3]: "LGSSM(sθ) = 1/2 Epd Epv [∇v(X)⊤sθ(X)]² + ... + Epd Epv [sθ(X)⊤Hv(X)∇v(X)] + ..."
  - [corpus]: Missing - no direct citations about generalized sliced score matching with arbitrary transformations
- Break condition: The Hessian of v(x) becomes too complex or the gradient becomes near-zero in regions of high probability density.

## Foundational Learning

- Concept: Change of variables formula for probability densities
  - Why needed here: Provides the foundation for understanding how probability distributions transform under coordinate changes, which is essential for deriving the score change of variables
  - Quick check question: If Y = φ(X) and p(x) is the density of X, what is the density of Y in terms of p and φ?
  - Answer: q(y) = p(φ−1(y)) / |det Jφ−1(y)|

- Concept: Score matching and denoising score matching
  - Why needed here: The paper builds on these techniques to develop both reverse-time Itô lemma applications and generalized sliced score matching
  - Quick check question: Why can't we directly minimize ∥sθ(X) − ∇x log pd(X)∥² in score matching?
  - Answer: Because ∇x log pd(X) is unknown and we only have samples from pd(X)

- Concept: Stochastic differential equations and reverse-time processes
  - Why needed here: Essential for understanding how diffusion models work and how they can be transformed to different spaces
  - Quick check question: What is the key difference between forward and reverse-time SDEs in diffusion models?
  - Answer: The reverse process includes an additional drift term involving the score function of the current marginal distribution

## Architecture Onboarding

- Component map:
  Score function transformation module -> Diffusion process handler -> Loss function calculator -> Training pipeline -> Inference engine

- Critical path:
  1. Define smooth invertible transformation φ
  2. Compute Jacobian Jφ−1 and its divergence
  3. Implement weighted loss function for score network training
  4. Apply reverse-time Itô lemma for sampling
  5. Validate transformations preserve desired properties

- Design tradeoffs:
  - Computational cost vs. transformation expressiveness: More complex transformations require computing higher-order derivatives but capture richer structure
  - Training stability vs. flexibility: Variance-reduced losses improve stability but may limit the class of usable transformations
  - Model capacity vs. generalization: More expressive score models can handle complex transformations but may overfit

- Failure signatures:
  - Numerical instability in Jacobian computations (NaN or Inf values)
  - Divergence during training indicating poor transformation choice
  - Poor sample quality suggesting the transformation doesn't preserve important structure
  - High variance in GSSM estimates suggesting need for better variance reduction

- First 3 experiments:
  1. Implement basic score change of variables formula with a simple linear transformation and verify gradient computations
  2. Train a diffusion model using the reverse-time Itô lemma with a nonlinear transformation (e.g., additive logistic) and compare sample quality
  3. Implement GSSM with quadratic transformations and compare against standard SSM on a small UCI dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of transformation affect the variance of score matching estimators in high dimensions?
- Basis in paper: [explicit] The paper notes that GSSM performs less favorably than SSM due to higher estimator variance and discusses variance reduction techniques for quadratic transformations.
- Why unresolved: While the paper demonstrates that variance reduction is possible for specific transformation families (like quadratic forms), it doesn't provide a general framework for selecting transformations that minimize variance across arbitrary distributions.
- What evidence would resolve it: Systematic experiments comparing variance across different transformation families (linear, quadratic, neural network-based) for various synthetic and real datasets, along with theoretical bounds on estimator variance as a function of transformation properties.

### Open Question 2
- Question: What are the theoretical conditions under which training in one space and sampling in another (transformed space) improves diffusion model performance?
- Basis in paper: [explicit] The paper introduces reverse-time Itô lemma enabling training in unconstrained space while sampling in simplex, but notes limitations including computational cost and potential numerical instabilities.
- Why unresolved: While the paper demonstrates practical applications, it doesn't provide rigorous theoretical guarantees about when this decoupling approach improves sample quality, convergence rates, or reduces training complexity compared to traditional methods.
- What evidence would resolve it: Theoretical analysis establishing conditions (e.g., transformation properties, data distribution characteristics) under which the transformed approach guarantees improved performance, supported by empirical validation across diverse datasets.

### Open Question 3
- Question: How can we extend the score change of variables framework to non-invertible transformations and constrained domains beyond the probability simplex?
- Basis in paper: [inferred] The current framework assumes smooth, invertible transformations. The paper mentions applications to manifolds and constrained domains but doesn't address non-invertible cases.
- Why unresolved: Many practical scenarios involve non-invertible mappings (e.g., dimensionality reduction, projections) or complex constraints that don't admit simple inverse transformations, yet these could benefit from score-based methods.
- What evidence would resolve it: Development of generalized change of variables formulas for non-invertible transformations, validated through applications to dimensionality reduction techniques, generative modeling on complex manifolds, or handling hierarchical constraints in structured data.

## Limitations

- The method requires smooth, invertible transformations with well-behaved Jacobians, limiting applicability to cases where such transformations can be constructed without singularities.
- Computing Jacobians and their divergences for complex transformations introduces significant computational overhead, particularly in high dimensions.
- Experimental validation is limited in scope, with mixed performance across datasets suggesting the approach may not universally outperform existing methods.

## Confidence

- High Confidence: The mathematical derivation of the score change of variables formula is rigorous and follows established principles from differential geometry and probability theory.
- Medium Confidence: The reverse-time Itô lemma extension and its application to diffusion models appears theoretically sound, but practical benefits depend heavily on transformation choice.
- Medium Confidence: The GSSM framework and variance-reduced variant show promising experimental results, but performance gains are dataset-dependent.

## Next Checks

1. **Jacobian Stability Analysis**: Systematically test the numerical stability of Jacobian computations across different transformation families (linear, polynomial, exponential) on high-dimensional data to identify regimes where the method breaks down.

2. **Transformation Expressiveness Benchmark**: Compare the performance of GSSM with increasingly complex transformations (linear → quadratic → cubic) on a standardized set of density estimation tasks to quantify the tradeoff between transformation expressiveness and estimation accuracy.

3. **Cross-Domain Generalization Study**: Evaluate the chess diffusion application on other structured domains (e.g., molecular conformations, graph-structured data) where natural transformations exist between constrained and unconstrained spaces to assess the broader applicability of the reverse-time Itô lemma approach.