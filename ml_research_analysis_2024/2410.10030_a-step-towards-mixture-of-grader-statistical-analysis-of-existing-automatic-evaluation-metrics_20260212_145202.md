---
ver: rpa2
title: 'A Step Towards Mixture of Grader: Statistical Analysis of Existing Automatic
  Evaluation Metrics'
arxiv_id: '2410.10030'
source_url: https://arxiv.org/abs/2410.10030
tags:
- score
- evaluation
- answer
- metrics
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study analyzes the correlation of various automatic QA evaluation\
  \ metrics with human-like evaluation using ChatGPT-o1-preview as a proxy. It finds\
  \ that no single metric is optimal across all question types\u2014Exact Match works\
  \ well for short-form answers but poorly for long-form, while Pedant performs better\
  \ for long-form but worse for short-form."
---

# A Step Towards Mixture of Grader: Statistical Analysis of Existing Automatic Evaluation Metrics

## Quick Facts
- **arXiv ID**: 2410.10030
- **Source URL**: https://arxiv.org/abs/2410.10030
- **Authors**: Yun Joon Soh; Jishen Zhao
- **Reference count**: 8
- **Primary result**: No single automatic evaluation metric performs optimally across all QA types; Exact Match works well for short-form but poorly for long-form answers, while Pedant shows the opposite pattern.

## Executive Summary
This study analyzes the correlation between various automatic QA evaluation metrics and human-like evaluation using ChatGPT-o1-preview as a proxy. The authors find that existing metrics show high correlation within question types but vary significantly across different QA types. Exact Match excels at short-form answers while Pedant performs better on long-form responses. Based on these findings, the authors propose a Mixture Of Grader (MOG) approach that classifies QA pairs by type and selects the most appropriate metric for each, aiming to better align with human evaluation than any single metric.

## Method Summary
The study uses a synthesized dataset of 359 samples generated by ChatGPT-o1-preview containing question, gold answer, attempted answer, score, justification, question type, and answer type tuples. The dataset covers 16 answer types including single word, numerical, paragraph, code snippet, sentence, equation, phrase, name, boolean, list, symbol, single character, formula, long paragraph, essay, and short paragraph. The authors compute correlation coefficients (Pearson, Spearman, Kendall τ) between eight automatic evaluation metrics (Exact Match, F1 Score, Recall, BLEU, ROUGE-L, Levenshtein Distance, Cosine Similarity with TF-IDF, Pedant) and the ChatGPT-o1-preview scores, both overall and per answer type. They analyze mean delta scores and create correlation matrices to compare metric performance.

## Key Results
- No single metric can adequately estimate human-like evaluation across all question types in QA tasks
- Exact Match performs particularly strong for short-form answers (single word, numerical, name, list, formula) but not for long-forms (essay, long paragraph, paragraph, short paragraph, sentences)
- Pedant metric shows better performance for long-form answers compared to Exact Match
- Existing metrics show high correlation among them within the same question type
- Metrics vary significantly in their correlation with human-like evaluation across different question types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: No single evaluation metric performs optimally across all question types in QA tasks.
- Mechanism: Different QA types (e.g., single word vs. essay) have distinct semantic and structural characteristics that favor different evaluation metrics. Exact Match excels at short-form answers but fails on long-form; Pedant performs better on long-form but worse on short-form.
- Core assumption: Human-like evaluation via ChatGPT-o1-preview can serve as a reliable proxy for human judgment across diverse QA types.
- Evidence anchors:
  - [abstract] "no single metric can adequately estimate the human-like evaluation"
  - [section] "Exact Match (EM) was particularly strong for short-form answers such as single word, numerical, name, list, and formula, but not for long-forms such as essay, long paragraph, paragraph, short paragraph, or sentences."
  - [corpus] Weak—no direct corpus evidence for this specific mechanism, though related work on metric limitations exists.
- Break condition: If human-like evaluation via LLM is not consistent or reliable across QA types, the observed metric correlations would be invalid.

### Mechanism 2
- Claim: Metrics are highly correlated within question types but vary significantly across types.
- Mechanism: Within a given question type, different metrics tend to agree on relative answer quality because they capture similar aspects of correctness for that type. Across types, metrics diverge because their design biases favor certain answer structures.
- Core assumption: The correlation structure reflects genuine differences in metric suitability rather than noise or sampling artifacts.
- Evidence anchors:
  - [abstract] "existing metrics have a high correlation among them concerning the question type"
  - [section] "From the correlation matrix, we further observed that many of these metrics show high correlations. For example, the F1 Score showed a high resemblance to most evaluation metrics"
  - [corpus] Weak—no corpus evidence directly supporting within-type correlation patterns.
- Break condition: If the observed correlations are due to dataset artifacts or LLM grader inconsistency, the mechanism fails.

### Mechanism 3
- Claim: A Mixture Of Grader (MOG) approach that classifies QA pairs by type and selects appropriate metrics can better align with human evaluation.
- Mechanism: By first classifying each (question, gold answer) pair into a predefined QA type class and then selecting the most appropriate evaluation metric accordingly, MOG can leverage the strengths of different metrics while avoiding their weaknesses for unsuitable types.
- Core assumption: QA type classification is feasible and accurate enough to enable effective metric selection.
- Evidence anchors:
  - [abstract] "As a potential solution, we discuss how a Mixture Of Grader could potentially improve the auto QA evaluator quality"
  - [section] "From the observation, we conclude that there is no 'one-size fits all' evaluation metric. A better direction for a more concrete automatic evaluator would be to first classify the question and answer types and use different evaluation metrics for each type."
  - [corpus] Weak—no corpus evidence for MOG effectiveness, though related work on adaptive evaluation exists.
- Break condition: If QA type classification is inaccurate or if the overhead of classification outweighs the benefits, MOG may not improve evaluation quality.

## Foundational Learning

- Concept: Pearson correlation coefficient
  - Why needed here: Used to quantify the relationship between automatic metric scores and human-like evaluation scores across different QA types.
  - Quick check question: If two variables have a Pearson correlation of 0.8, what does this indicate about their linear relationship?

- Concept: Token-level metrics vs. n-gram metrics
  - Why needed here: Understanding the difference between Exact Match, F1 Score, BLEU, and ROUGE-L is crucial for interpreting why different metrics perform better on different QA types.
  - Quick check question: Why might Exact Match fail to give credit for semantically correct answers that use different wording than the gold answer?

- Concept: LLM-as-a-judge methodology
  - Why needed here: The study uses ChatGPT-o1-preview as a proxy for human evaluation, so understanding the strengths and limitations of this approach is essential.
  - Quick check question: What are potential sources of bias when using an LLM to simulate human evaluation?

## Architecture Onboarding

- Component map: QA type classifier → Metric selector → Evaluation metric(s) → Score aggregation
- Critical path: QA pair → Type classification → Metric selection → Score computation → Correlation analysis with human-like scores
- Design tradeoffs: Simple rule-based classification vs. learned classification; single metric selection vs. weighted combination; computational overhead vs. evaluation quality
- Failure signatures: Low correlation between selected metric and human-like scores for certain QA types; inconsistent classification results; computational bottlenecks during evaluation
- First 3 experiments:
  1. Validate the QA type classifier on a held-out dataset and measure classification accuracy
  2. Test the MOG approach on a subset of QA pairs with known human judgments
  3. Compare the computational efficiency of MOG against using a single metric across all QA types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop an accurate question type classifier that works reliably across diverse QA datasets?
- Basis in paper: [inferred] The authors propose using different evaluation metrics for different question types but do not address how to implement the classification system.
- Why unresolved: The paper focuses on analyzing existing metrics and their correlations but leaves the practical implementation of a type classification system for future work. The classification challenge is non-trivial due to the variability and ambiguity in question types.
- What evidence would resolve it: Experimental results showing high accuracy classification of questions into the proposed 16 types across multiple datasets, along with analysis of classification errors and edge cases.

### Open Question 2
- Question: Can we create a universal automatic evaluation metric that outperforms existing metrics across all question types?
- Basis in paper: [explicit] The authors conclude that "there is no 'one-size fits all' evaluation metric" and that each metric has strengths and weaknesses based on QA type.
- Why unresolved: While the paper shows that different metrics work better for different question types, it doesn't explore whether combining metrics in a more sophisticated way than simple selection could create a universal solution, or whether machine learning approaches could learn to weight metrics appropriately.
- What evidence would resolve it: Development and testing of a new evaluation metric (possibly learned or hybrid) that shows superior correlation with human-like evaluation across all question types compared to existing metrics like Pedant, EM, and F1 Score.

### Open Question 3
- Question: How do we address the limitation that some evaluation metrics cannot properly vectorize symbols or equations?
- Basis in paper: [explicit] The authors note in the Discussion section that "The TF-IDF tokenizer can be useful for many grading metrics but cannot properly vectorize symbols or equations."
- Why unresolved: The paper identifies this as a limitation but doesn't propose solutions beyond suggesting to try an alternative embedding model in future work. This is particularly relevant for technical domains where equations and symbols are common.
- What evidence would resolve it: Experimental comparison showing improved evaluation accuracy for questions with equations and symbols using alternative embedding methods (like specialized mathematical tokenizers or domain-specific embeddings) compared to TF-IDF approaches.

## Limitations

- The study relies on ChatGPT-o1-preview as a proxy for human evaluation, which may not accurately represent true human judgment across all QA types
- The proposed Mixture Of Grader (MOG) approach is discussed conceptually but lacks empirical validation or implementation details
- The dataset generation process using LLM is not fully specified, raising questions about reproducibility and potential bias in the synthesized data

## Confidence

- **Medium confidence**: The core finding that no single metric performs optimally across all QA types is well-supported by the correlation analysis, though the LLM-as-judge methodology introduces some uncertainty.
- **Low confidence**: The proposed MOG solution lacks empirical validation and implementation details, making it difficult to assess its practical effectiveness.
- **Medium confidence**: The observation that metrics are highly correlated within question types but vary across types is plausible but requires further validation with human judgments.

## Next Checks

1. Conduct a human evaluation study on a subset of QA pairs to validate the correlation patterns observed with the LLM proxy and assess whether the proposed MOG approach actually improves alignment with human judgments.
2. Implement and test the MOG approach on a real-world QA dataset, measuring both evaluation quality improvements and computational overhead compared to single-metric approaches.
3. Perform ablation studies to determine the minimum viable QA type classification accuracy required for MOG to be effective, and test alternative classification strategies (rule-based vs. learned).