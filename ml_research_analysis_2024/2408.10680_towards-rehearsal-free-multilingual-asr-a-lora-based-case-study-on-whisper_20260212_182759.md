---
ver: rpa2
title: 'Towards Rehearsal-Free Multilingual ASR: A LoRA-based Case Study on Whisper'
arxiv_id: '2408.10680'
source_url: https://arxiv.org/abs/2408.10680
tags:
- learning
- lora
- speech
- training
- whisper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates continual learning challenges for Whisper
  ASR when adding new languages (Uyghur, Tibetan) without original training data.
  A rehearsal-free LoRA-based method using orthogonal gradient descent is proposed,
  leveraging LoRA parameters from the original model to prevent catastrophic forgetting.
---

# Towards Rehearsal-Free Multilingual ASR: A LoRA-based Case Study on Whisper

## Quick Facts
- arXiv ID: 2408.10680
- Source URL: https://arxiv.org/abs/2408.10680
- Reference count: 0
- One-line primary result: O-AdaLoRA achieves lower WER (53.62% vs 50.45%) with only 1.36% trainable parameters compared to 3.47% for AdaLoRA, while maintaining better performance on previously learned languages.

## Executive Summary
This paper addresses continual learning challenges for Whisper ASR models when adding new languages without access to original training data. The authors propose O-AdaLoRA, a rehearsal-free method that leverages LoRA parameters from the original model for approximate orthogonal gradient descent, preventing catastrophic forgetting while adding Uyghur and Tibetan to a Chinese Whisper model. The method dynamically allocates trainable parameters using a learnable rank coefficient, achieving superior performance with fewer parameters compared to full fine-tuning, LoRA, and existing continual learning methods.

## Method Summary
The paper proposes a rehearsal-free continual learning approach for Whisper ASR that uses LoRA parameters from the original model to enforce orthogonal gradient descent on new tasks. The method combines AdaLoRA's dynamic rank allocation with orthogonal gradient enforcement to prevent catastrophic forgetting when adding new languages. Specifically, the approach uses the LoRA matrices from the original model to approximate the gradient subspace of previous tasks, then learns new tasks in directions orthogonal to this subspace. A learnable rank coefficient dynamically allocates parameters across LoRA matrices for efficiency, with experiments showing the method outperforms full fine-tuning and existing LoRA-based methods on Uyghur and Tibetan language addition tasks.

## Key Results
- O-AdaLoRA achieves WER of 53.62% on Uyghur with only 1.36% trainable parameters, outperforming AdaLoRA (50.45% WER, 3.47% parameters)
- The method maintains better performance on previously learned Chinese language compared to full fine-tuning and LoRA methods
- Orthogonal gradient enforcement significantly reduces catastrophic forgetting when adding new languages sequentially

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA parameters from the original model capture the gradient subspace of previous tasks and can be used to approximate gradients of previous tasks to guide continual learning.
- Mechanism: The method leverages the low-rank LoRA updates from the original model to enforce orthogonality with new task updates, preventing catastrophic forgetting without needing the original training data.
- Core assumption: The LoRA parameters adequately represent the original task's gradient subspace, and enforcing orthogonality between new and old LoRA subspaces preserves performance on previous tasks.
- Evidence anchors:
  - [abstract] "We propose to leverage the LoRA parameters from the original model for approximate orthogonal gradient descent on the new samples."
  - [section 2.3] "O-LoRA incrementally learns new tasks in a direction orthogonal to the LoRA subspace of past tasks while fixing the previous parameters."
  - [corpus] Weak evidence - no direct mention of LoRA parameter subspace representation in neighbors.
- Break condition: If the original LoRA parameters don't adequately capture the original task's gradient subspace, orthogonality enforcement won't prevent forgetting.

### Mechanism 2
- Claim: A learnable rank coefficient dynamically allocates trainable parameters for more efficient training.
- Mechanism: The method uses AdaLoRA's dynamic rank allocation based on parameter importance to assign ranks to LoRA matrices, reducing the number of trainable parameters while maintaining performance.
- Core assumption: Parameter importance varies across layers and modules, and dynamic allocation based on importance metrics improves performance and efficiency.
- Evidence anchors:
  - [abstract] "Additionally, we also introduce a learnable rank coefficient to allocate trainable parameters for more efficient training."
  - [section 2.4] "AdaLoRA dynamically allocates an overall rank budget to its update matrices... This is achieved by iteratively masking out less important singular values after every gradient update step."
  - [corpus] Weak evidence - no direct mention of dynamic rank allocation in neighbors.
- Break condition: If the importance metric doesn't accurately reflect parameter importance, dynamic allocation won't improve efficiency or performance.

### Mechanism 3
- Claim: Orthogonal gradient descent prevents conflicts between loss functions of new and previous tasks.
- Mechanism: The method enforces orthogonality between gradient subspaces of new and previous tasks using an orthogonal loss term, avoiding catastrophic forgetting while learning new tasks.
- Core assumption: Neural networks are over-parameterized, and learning in orthogonal directions avoids conflicts with previous loss functions.
- Evidence anchors:
  - [abstract] "To mitigate this issue, we propose to leverage the LoRA parameters from the original model for approximate orthogonal gradient descent on the new samples."
  - [section 2.3] "To ensure the orthogonality between the subspace U and the subspace W, we need to satisfy: <u, w> = 0, ∀u∈U, ∀w∈W"
  - [corpus] Weak evidence - no direct mention of orthogonal gradient descent in neighbors.
- Break condition: If the gradient subspace approximation is poor, orthogonality enforcement won't prevent conflicts between loss functions.

## Foundational Learning

- Concept: Low-rank adaptation (LoRA)
  - Why needed here: LoRA enables parameter-efficient fine-tuning by decomposing weight updates into low-rank matrices, crucial for adapting large models like Whisper to new languages without full fine-tuning.
  - Quick check question: What is the mathematical representation of LoRA weight updates in the paper?

- Concept: Catastrophic forgetting
  - Why needed here: Understanding catastrophic forgetting is essential because the paper addresses this problem when adding new languages to multilingual ASR models without original training data.
  - Quick check question: What are the two main causes of catastrophic forgetting mentioned in the introduction?

- Concept: Orthogonal gradient descent
  - Why needed here: Orthogonal gradient descent is the key mechanism used to prevent catastrophic forgetting by ensuring new task learning doesn't conflict with previous task knowledge.
  - Quick check question: How does the paper enforce orthogonality between gradient subspaces of new and previous tasks?

## Architecture Onboarding

- Component map: Whisper model -> LoRA adapters (A, B, Λ matrices) -> Orthogonal loss computation -> Rank coefficient learning -> Parameter updates
- Critical path: Forward pass → Compute orthogonal loss → Update rank coefficients → Update LoRA parameters → Backward pass. The critical path involves computing the orthogonal loss between current and previous LoRA subspaces and updating parameters accordingly.
- Design tradeoffs: The method trades some parameter efficiency (compared to full fine-tuning) for better performance retention on previous tasks. Using LoRA parameters from the original model enables rehearsal-free learning but requires the original model to have been trained with LoRA.
- Failure signatures: Performance degradation on previous languages, poor convergence on new languages, or increased parameter count without performance gains. These indicate issues with orthogonal loss weighting, rank allocation, or LoRA subspace representation.
- First 3 experiments:
  1. Compare O-AdaLoRA with full fine-tuning, LoRA, and AdaLoRA on Uyghur language addition to a Chinese Whisper model.
  2. Test catastrophic forgetting by measuring performance on Chinese after Uyghur fine-tuning using different methods.
  3. Evaluate parameter efficiency by comparing trainable parameter counts and performance across methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed O-AdaLoRA method perform on languages other than Uyghur and Tibetan, and what are the limits of its effectiveness across diverse language families?
- Basis in paper: [inferred] The paper only tests on Uyghur and Tibetan, and mentions it could be expanded to other speech foundation models.
- Why unresolved: The experiments are limited to two low-resource languages from the same language family (Turkic and Tibeto-Burman), leaving questions about performance on Indo-European, Sino-Tibetan, or other families.
- What evidence would resolve it: Testing O-AdaLoRA on a diverse set of languages from different families (e.g., Spanish, Hindi, Arabic) and comparing results to other methods.

### Open Question 2
- Question: What is the optimal strategy for determining the rank coefficient allocation in O-AdaLoRA, and how sensitive is the method to the choice of initial rank and target rank?
- Basis in paper: [explicit] The paper uses a fixed initial rank of 12 and target rank of 8, but notes that the importance of weight parameters varies across layers.
- Why unresolved: The study does not explore a systematic method for determining these hyperparameters or analyze their impact on performance.
- What evidence would resolve it: An ablation study varying initial and target ranks across different model sizes and languages, and developing a principled method for rank allocation.

### Open Question 3
- Question: How does O-AdaLoRA compare to other rehearsal-free continual learning methods (e.g., elastic weight consolidation, synaptic intelligence) in terms of forgetting prevention and parameter efficiency?
- Basis in paper: [explicit] The paper compares to LoRA, O-LoRA, and LwF, but does not include other regularization-based methods.
- Why unresolved: The comparison is limited to a few methods, leaving questions about the relative effectiveness of O-AdaLoRA compared to other approaches.
- What evidence would resolve it: Implementing and comparing O-AdaLoRA to other rehearsal-free methods like EWC and SI on the same datasets and metrics.

## Limitations
- Evaluation constrained by limited test datasets for Uyghur and Tibetan, relying on in-house collections rather than standard benchmarks
- Claims about performance maintenance on previously learned languages supported by relatively small-scale experiments (two new languages)
- Orthogonal gradient enforcement mechanism depends critically on assumption that LoRA parameters adequately represent gradient subspace, not empirically validated beyond observed performance
- Learnable rank coefficient mechanism lacks detailed implementation specifications for precise replication

## Confidence
- **High confidence**: The general observation that LoRA-based methods outperform full fine-tuning for multilingual ASR adaptation, supported by established literature on parameter-efficient learning
- **Medium confidence**: The specific claim that O-AdaLoRA with 1.36% trainable parameters outperforms AdaLoRA with 3.47% parameters while maintaining better performance on previous tasks. This is based on the reported experimental results but depends on dataset quality and implementation details
- **Low confidence**: The mechanism claim that LoRA parameters "capture the gradient subspace of previous tasks" and can be used for approximate orthogonal gradient descent. This theoretical justification lacks direct empirical validation beyond the downstream performance metrics

## Next Checks
1. **Gradient subspace validation**: Analyze whether the LoRA parameters from the original model actually span the gradient space of the original task by comparing gradient directions during training on original vs. new data
2. **Dataset independence test**: Evaluate O-AdaLoRA on a different set of low-resource languages or using standard public datasets to verify that the performance gains are not specific to the particular Uyghur/Tibetan datasets used
3. **Ablation of orthogonal loss**: Conduct experiments removing the orthogonal loss term to quantify its actual contribution to preventing catastrophic forgetting, isolating its effect from the benefits of LoRA adaptation itself