---
ver: rpa2
title: How to train your ViT for OOD Detection
arxiv_id: '2405.17447'
source_url: https://arxiv.org/abs/2405.17447
tags:
- unit
- pretraining
- mahalanobis
- ninco
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how different pretraining and finetuning schemes
  affect Vision Transformer (ViT) performance on out-of-distribution (OOD) detection
  for ImageNet-scale classification. The authors analyze a large pool of ViT models
  pretrained on ImageNet-1K, ImageNet-21K, or CLIP, then finetuned with varying hyperparameters
  (augmentations, dropout, weight decay, learning rate).
---

# How to train your ViT for OOD Detection

## Quick Facts
- arXiv ID: 2405.17447
- Source URL: https://arxiv.org/abs/2405.17447
- Authors: Maximilian Mueller; Matthias Hein
- Reference count: 40
- Primary result: ImageNet-21K pretraining with larger weight decay and small finetuning learning rate yields strongest OOD detectors; high in-distribution accuracy does not guarantee strong OOD detection performance.

## Executive Summary
This paper investigates how different pretraining and finetuning schemes affect Vision Transformer (ViT) performance on out-of-distribution (OOD) detection for ImageNet-scale classification. Through a comprehensive study of 296 ViT models with varying pretraining datasets (ImageNet-1K, ImageNet-21K, CLIP) and hyperparameters, the authors demonstrate that ImageNet-21K pretraining with larger weight decay and small finetuning learning rate produces the most reliable OOD detectors. The study reveals that high in-distribution accuracy does not guarantee strong OOD detection performance, particularly on challenging synthetic unit-tests, highlighting the need for dedicated OOD evaluation beyond standard accuracy metrics.

## Method Summary
The authors analyze a large pool of ViT models pretrained on ImageNet-1K, ImageNet-21K, or CLIP, then finetuned with varying hyperparameters including augmentations, dropout, weight decay, and learning rate. Models are evaluated using post-hoc OOD detection methods (MaxLogit, Mahalanobis, Relative Mahalanobis) on the NINCO dataset (hand-cleaned real-world OOD) and synthetic unit-tests designed to mimic noise distributions. The evaluation focuses on False Positive Rate (FPR) at 95% True Positive Rate (TPR) and Area Under the Curve (AUC) metrics to assess OOD detection performance across different training schemes.

## Key Results
- ImageNet-21K pretraining with larger weight decay and small finetuning learning rate yields the strongest OOD detectors overall
- CLIP pretraining performs poorly with feature-based OOD detection methods like Mahalanobis distance
- Some high-accuracy models fail on synthetic unit-tests, showing that good generalization does not guarantee strong OOD detection
- Mahalanobis distance consistently outperforms other OOD detection methods, especially for ImageNet-21K pretrained models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ImageNet-21k pretraining with larger weight decay and small finetuning learning rate yields strongest OOD detectors.
- Mechanism: Larger weight decay during pretraining structures the feature space in a way that benefits Mahalanobis-based OOD detection, and a small finetuning learning rate preserves this beneficial structure.
- Core assumption: The feature space structure learned during pretraining is crucial for OOD detection performance and can be preserved or degraded during finetuning.
- Evidence anchors:
  - [abstract] "Best practice: ImageNet-21K pretraining with larger weight decay and small finetuning learning rate."
  - [section] "Consistently good results were obtained for Mahalanobis distance as OOD detection method with models pretrained on ImageNet-21k with larger weight decay during pretraining and subsequent finetuning with small learning rate. We hypothesize that larger weight decay might help to structure the feature space in a beneficial way during supervised pretraining, and that a smaller finetuning learning rate might preserve this structure better."
  - [corpus] Weak evidence - related papers discuss OOD detection in ViTs but do not specifically address the impact of weight decay and learning rate on feature space structure.
- Break condition: If the feature space structure is not crucial for OOD detection, or if the finetuning learning rate does not affect the preservation of this structure, this mechanism would break.

### Mechanism 2
- Claim: CLIP pretraining does not lend itself to feature-based OOD detection methods.
- Mechanism: CLIP pretraining, which is based on a cosine-similarity measure, imposes a different structure on the feature space compared to ImageNet-21k pretraining, which does not benefit feature-based OOD detection methods.
- Core assumption: The structure of the feature space imposed by different pretraining methods affects the performance of feature-based OOD detection methods.
- Evidence anchors:
  - [abstract] "CLIP pretraining performs poorly with feature-based methods."
  - [section] "CLIP pretraining, which is based on a cosine-similarity measure, seems to impose a different structure, which does not lend itself towards the current feature-based OOD detection methods."
  - [corpus] Weak evidence - related papers discuss CLIP pretraining but do not specifically address its impact on feature-based OOD detection methods.
- Break condition: If the feature space structure imposed by CLIP pretraining does benefit feature-based OOD detection methods, or if the cosine-similarity measure is not the cause of the poor performance, this mechanism would break.

### Mechanism 3
- Claim: High in-distribution accuracy does not guarantee strong OOD detection performance.
- Mechanism: The ability to solve the difficult task of separating near-OOD data from ID data (high in-distribution accuracy) does not necessarily translate to the ability to detect OOD data from very different distributions.
- Core assumption: The skills required for high in-distribution accuracy and strong OOD detection are not fully overlapping.
- Evidence anchors:
  - [abstract] "Some high-accuracy models fail on unit-tests, showing that good generalization does not guarantee strong OOD detection."
  - [section] "When considering the supposedly easy unit-test dataset, the picture changes drastically: Especially for Mahalanobis-based methods the correlation between accuracy and FPR vanishes, and many models with similar test accuracy show vastly different FPR values. Some of the strongest models in terms of accuracy show very poor OOD detection capabilities, often with FPR values over 50%. This is concerning, since it indicates that supposedly strong OOD detectors, i.e. ones that are capable of solving the difficult task of separating near-OOD data from ID data, are vulnerable to OOD data from very different distributions."
  - [corpus] Weak evidence - related papers discuss the relationship between in-distribution accuracy and OOD detection performance but do not provide specific examples of high-accuracy models failing on OOD detection.
- Break condition: If high in-distribution accuracy always guarantees strong OOD detection performance, or if the skills required for high in-distribution accuracy and strong OOD detection are fully overlapping, this mechanism would break.

## Foundational Learning

- Concept: Vision Transformer (ViT) architecture
  - Why needed here: Understanding the ViT architecture is crucial for interpreting the results and implications of this study, as it focuses on OOD detection in ViTs.
  - Quick check question: What are the key components of a ViT, and how do they differ from traditional convolutional neural networks?

- Concept: Out-of-distribution (OOD) detection
  - Why needed here: OOD detection is the central task being studied in this paper, and understanding its concepts and challenges is essential for grasping the significance of the results.
  - Quick check question: What are the main challenges in OOD detection, and how do post-hoc OOD detection methods like Mahalanobis distance and MaxLogit address these challenges?

- Concept: Pretraining and finetuning
  - Why needed here: The study investigates the impact of different pretraining and finetuning schemes on OOD detection performance, so understanding these concepts is crucial for interpreting the results.
  - Quick check question: How do pretraining and finetuning differ, and what are the potential benefits and drawbacks of each approach in the context of OOD detection?

## Architecture Onboarding

- Component map:
  Model pool -> Pretraining on ImageNet-1K/ImageNet-21K/CLIP -> Finetuning with hyperparameters -> OOD detection methods -> NINCO/synthetic unit-tests -> Evaluation metrics

- Critical path:
  1. Pretrain ViT models on ImageNet-1K, ImageNet-21K, or CLIP
  2. Finetune pretrained models on ImageNet-1K with varying hyperparameters
  3. Evaluate finetuned models with post-hoc OOD detection methods on NINCO and synthetic unit-tests
  4. Analyze the impact of pretraining and finetuning schemes on OOD detection performance

- Design tradeoffs:
  - Pretraining dataset: ImageNet-21K generally yields the strongest OOD detectors, but CLIP pretraining may be faster or cheaper
  - Finetuning hyperparameters: Larger weight decay and smaller learning rate tend to improve OOD detection, but may also affect in-distribution accuracy
  - OOD detection method: Feature-based methods like Mahalanobis distance can be more effective than logit-based methods like MaxLogit, but may be more computationally expensive

- Failure signatures:
  - High in-distribution accuracy but poor OOD detection performance, especially on synthetic unit-tests
  - Inconsistent performance across different OOD detection methods or datasets
  - Sensitivity to small changes in hyperparameters, leading to large variations in OOD detection performance

- First 3 experiments:
  1. Evaluate a set of ViT models pretrained on ImageNet-21K with varying weight decay and finetuning learning rates on NINCO and synthetic unit-tests using Mahalanobis distance
  2. Compare the OOD detection performance of ViT models pretrained on ImageNet-1K, ImageNet-21K, and CLIP on NINCO and synthetic unit-tests using MaxLogit
  3. Analyze the correlation between in-distribution accuracy and OOD detection performance across different ViT models and OOD detection methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed poor performance of CLIP-pretrained models with feature-based OOD detection methods indicate a fundamental incompatibility between CLIP's training objective and the assumptions underlying these methods?
- Basis in paper: [explicit] The paper states that CLIP pretraining, which is based on a cosine-similarity measure, seems to impose a different structure that does not lend itself towards current feature-based OOD detection methods.
- Why unresolved: The authors hypothesize about this incompatibility but do not provide a rigorous theoretical analysis or empirical investigation into why CLIP's feature space structure differs from other pretraining methods in a way that breaks feature-based OOD detection.
- What evidence would resolve it: A detailed comparative analysis of the feature space geometry induced by CLIP versus other pretraining methods, including visualization of class separation and distance distributions, would help determine if the issue is structural or could be mitigated through method adaptation.

### Open Question 2
- Question: Can the structure of the feature space that benefits Mahalanobis distance be explicitly characterized and engineered during pretraining, rather than relying on the incidental effects of hyperparameters like weight decay and learning rate?
- Basis in paper: [inferred] The authors note that larger weight decay during pretraining and smaller finetuning learning rate yield strong Mahalanobis-based detectors, hypothesizing that this helps structure the feature space beneficially.
- Why unresolved: The authors do not test alternative methods to directly shape the feature space geometry (e.g., explicit regularization terms, contrastive learning objectives, or architectural modifications) to see if better results can be achieved than through the current implicit effects of hyperparameters.
- What evidence would resolve it: An ablation study comparing Mahalanobis performance when pretraining with various explicit feature space regularizers or objectives against the current best-practice (large weight decay, small learning rate) would show if the beneficial structure can be more directly engineered.

### Open Question 3
- Question: What is the optimal way to combine multiple ViT models with different training schemes for robust OOD detection across diverse out-of-distribution datasets?
- Basis in paper: [explicit] The paper evaluates "souped" models (ensembles) from Wortsman et al. (2022) but finds they bring no consistent advantage over individual models, and notes that certain training schemes might only be effective for specific types of out-distribution.
- Why unresolved: The study does not explore advanced ensembling strategies beyond simple averaging, such as weighted combinations based on per-dataset validation performance, meta-learning approaches, or adaptive selection of models based on input characteristics.
- What evidence would resolve it: An evaluation of various ensembling techniques, including dynamic model selection and weighted combinations optimized on a validation set of diverse OOD data, would reveal if better robustness across different OOD types can be achieved than with any single model or simple ensemble.

## Limitations

- The study's conclusions about Mahalanobis distance being consistently superior could be dataset-dependent, as NINCO represents only one real-world OOD distribution
- The analysis has weak evidence for the proposed mechanism explaining CLIP pretraining's poor performance with feature-based methods
- The relationship between in-distribution accuracy and OOD detection performance may be specific to the synthetic noise distributions used in the unit-tests

## Confidence

- ImageNet-21K pretraining effectiveness: High
- Hyperparameter sensitivity findings: High
- CLIP pretraining mechanism: Low
- In-distribution accuracy vs OOD detection relationship: Medium

## Next Checks

1. Test the identified best practices (ImageNet-21K pretraining, larger weight decay, smaller learning rate) across additional ViT architectures and scales to verify generalizability
2. Evaluate the same model pool on multiple OOD detection benchmarks beyond NINCO to assess robustness of the findings
3. Conduct ablation studies specifically isolating the impact of weight decay versus learning rate on feature space structure preservation