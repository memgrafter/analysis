---
ver: rpa2
title: Parallel Structures in Pre-training Data Yield In-Context Learning
arxiv_id: '2402.12530'
source_url: https://arxiv.org/abs/2402.12530
tags:
- parallel
- structures
- data
- pre-training
- phrases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies what structures in pre-training data contribute
  to in-context learning (ICL) in language models. The authors introduce the concept
  of "parallel structures" - pairs of phrases following similar templates within the
  same context window - and hypothesize that these are essential for LMs to acquire
  ICL ability.
---

# Parallel Structures in Pre-training Data Yield In-Context Learning

## Quick Facts
- arXiv ID: 2402.12530
- Source URL: https://arxiv.org/abs/2402.12530
- Reference count: 5
- Removing parallel structures reduces ICL accuracy by 51% vs 2% for random ablation

## Executive Summary
This paper investigates what structures in pre-training data enable in-context learning (ICL) in language models. The authors introduce "parallel structures" - pairs of phrases following similar templates within the same context window - and demonstrate these are essential for acquiring ICL ability. By developing an algorithm to detect parallel structures and conducting ablation experiments, they show that removing these structures from pre-training data reduces ICL accuracy by 51% compared to only 2% from random ablation. This effect persists even after excluding common patterns like n-gram repetitions. The findings suggest that pre-training on diverse parallel structures helps models generalize to various downstream tasks through ICL.

## Method Summary
The authors develop a parallel structure detection algorithm that measures whether training an LM on one phrase improves prediction of another. They then create ablated datasets by replacing the last token of each latter phrase in detected parallel structures with random noise. GPT-2 models of different sizes are pre-trained on both clean and ablated data, and their in-context learning performance is compared across 9 evaluation tasks. The study finds that parallel structure ablation causes significantly larger ICL degradation than random ablation, with the effect generalizing across model sizes and task types.

## Key Results
- Parallel structure ablation reduces ICL accuracy by 51% vs 2% for random ablation
- The effect persists even after excluding common patterns like n-gram repetitions
- Parallel structures often span long distances (343 tokens on average), potentially explaining why LMs retain early examples in ICL prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel structures enable ICL by providing multiple examples of the same task template within the same context window
- Mechanism: The model learns to generalize from one phrase to predict another when they follow similar templates, and this generalization transfers to ICL
- Core assumption: The model can recognize and generalize between phrases following similar templates during pre-training
- Evidence anchors:
  - [abstract] "We find that LMs' ICL ability depends on parallel structures in the pre-training data -- pairs of phrases following similar templates in the same context window."
  - [section 3.2] "We measure the parallel structure strength of two phrases by training an LM on the former phrase and test it on the latter."

### Mechanism 2
- Claim: Ablating parallel structures significantly reduces ICL ability by unlearning generalization between similar phrases
- Mechanism: Replacing tokens in parallel structures with random noise prevents the model from learning to generalize between similar phrases, directly impacting ICL
- Core assumption: The model relies on parallel structures learned during pre-training to perform ICL
- Evidence anchors:
  - [abstract] "We show that removing parallel structures in the pre-training data reduces LMs' ICL accuracy by 51% (vs 2% from random ablation)."
  - [section 6.1] "Ablating parallel structures hurts ICL... ablating PSs is particularly detrimental to ICL performance compared to ablating random tokens of the same amount (51.1% vs 1.5% relative drop in accuracy averaged across model sizes)."

### Mechanism 3
- Claim: Long-distance parallel structures explain why LMs don't forget early examples in ICL prompts
- Mechanism: The ability to use patterns of early tokens in context to predict later tokens, learned from long-distance parallel structures, allows the model to retain information about early examples
- Core assumption: The model can learn to use distant context information from parallel structures
- Evidence anchors:
  - [abstract] "We find that the two phrases in a PS are often far from each other (343 tokens away on average), which may explain why LMs don't forget early examples in in-context prompts."
  - [section 6.3] "We measure the distance (i.e. number of tokens) between the former and latter phrases in the identified PSs, and find that parallel structures often span long distances (skewed to the right with an average of 343 tokens)."

## Foundational Learning

- Concept: Autoregressive language modeling
  - Why needed here: The model is pre-trained to predict the next token given its prefix, which is the foundation for learning parallel structures
  - Quick check question: Can you explain how autoregressive language modeling works and why it's used for pre-training?

- Concept: In-context learning
  - Why needed here: The paper studies how parallel structures in pre-training data contribute to the model's ability to perform in-context learning
  - Quick check question: Can you describe the process of in-context learning and how it differs from traditional fine-tuning?

- Concept: Ablation studies
  - Why needed here: The paper uses ablation studies to measure the effect of parallel structures on in-context learning by removing them from pre-training data
  - Quick check question: Can you explain what ablation studies are and why they are useful for understanding the importance of specific components?

## Architecture Onboarding

- Component map: Pre-training data -> Parallel structure detection -> Ablation -> Continue pre-training -> Evaluate ICL
- Critical path: Pre-training data → Parallel structure detection → Ablation of parallel structures → Continue pre-training on ablated data → Evaluate in-context learning performance
- Design tradeoffs: The tradeoff is between the computational cost of detecting and ablating parallel structures versus the potential improvement in in-context learning performance
- Failure signatures: If the model's in-context learning performance doesn't decrease significantly after parallel structure ablation, or if it decreases similarly to random ablation, the approach may not be working as intended
- First 3 experiments:
  1. Run the parallel structure detection algorithm on a sample of the pre-training data to verify it's identifying meaningful structures
  2. Ablate a small percentage of parallel structures and continue pre-training a model, then evaluate its in-context learning performance to see if there's a noticeable drop
  3. Compare the performance drop from parallel structure ablation to that from random ablation to confirm the former has a larger impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of parallel structures (e.g., syntactic vs. semantic) contribute differently to ICL performance across various tasks?
- Basis in paper: [explicit] The paper mentions diverse patterns in parallel structures including n-gram repetitions, synonyms, text formats, syntactic constituents, and more complicated reasoning/knowledge patterns, but does not analyze their individual contributions to ICL
- Why unresolved: The authors only analyze parallel structures as a whole without differentiating between their types and how each type affects specific aspects of ICL
- What evidence would resolve it: A detailed analysis categorizing parallel structures by type and measuring their individual impact on ICL performance across different task categories

### Open Question 2
- Question: Can we develop more efficient algorithms to detect parallel structures that scale better to larger context windows and model sizes?
- Basis in paper: [inferred] The current detection algorithm has quadratic complexity in window size and uses approximations to improve efficiency, suggesting room for improvement
- Why unresolved: The paper uses an approximation-based algorithm that, while effective, is not optimal and may miss some parallel structures or introduce noise in detection
- What evidence would resolve it: Development and testing of more efficient detection algorithms that can handle larger windows and models without sacrificing accuracy in parallel structure identification

### Open Question 3
- Question: How do parallel structures interact with other mechanisms of ICL such as induction heads and implicit gradient descent?
- Basis in paper: [explicit] The paper discusses prior work on induction heads and implicit gradient descent as mechanisms for ICL but does not explore how these interact with parallel structures
- Why unresolved: The relationship between parallel structures and other proposed ICL mechanisms remains unexplored, leaving gaps in understanding the complete picture of how ICL works
- What evidence would resolve it: Experiments examining the interplay between parallel structures and other ICL mechanisms, such as whether parallel structures are processed through induction heads or influence implicit gradient descent processes

## Limitations

- The evaluation covers only 9 tasks and uses a single pre-training corpus (OpenWebText)
- The parallel structure detection algorithm relies on training small LMs to measure phrase pair similarity, which may miss some relevant structures or include spurious ones
- The ablation replaces only the final token of latter phrases, which may not fully eliminate the influence of the parallel structure

## Confidence

- High confidence: The core finding that parallel structure ablation causes significantly larger ICL degradation than random ablation (51% vs 2%) is well-supported by the experimental results
- Medium confidence: The claim that parallel structures explain why LMs don't forget early examples in in-context prompts, while plausible given the long distances between parallel phrases, requires further validation
- Medium confidence: The assertion that the detected parallel structures cover diverse linguistic tasks is supported by qualitative analysis but could benefit from more systematic categorization

## Next Checks

1. Test whether fine-tuning on synthetic parallel structures (not present in original pre-training data) can improve ICL performance, which would provide causal evidence for the mechanism
2. Conduct ablation experiments on additional pre-training corpora and model architectures to assess the generalizability of the findings
3. Analyze whether specific types of parallel structures (e.g., question-answer pairs vs. fill-in-the-blank patterns) contribute differently to ICL performance