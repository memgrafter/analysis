---
ver: rpa2
title: Decentralized Transformers with Centralized Aggregation are Sample-Efficient
  Multi-Agent World Models
arxiv_id: '2406.15836'
source_url: https://arxiv.org/abs/2406.15836
tags:
- learning
- world
- agents
- marie
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of learning efficient world models
  for multi-agent reinforcement learning (MARL), focusing on scalability and non-stationarity
  issues. The proposed method, MARIE, introduces a decentralized transformer-based
  world model combined with centralized representation aggregation using a Perceiver.
---

# Decentralized Transformers with Centralized Aggregation are Sample-Efficient Multi-Agent World Models

## Quick Facts
- arXiv ID: 2406.15836
- Source URL: https://arxiv.org/abs/2406.15836
- Authors: Yang Zhang; Chenjia Bai; Bin Zhao; Junchi Yan; Xiu Li; Xuelong Li
- Reference count: 40
- MARIE outperforms model-free and existing model-based methods on SMAC benchmark in sample efficiency and performance

## Executive Summary
This paper addresses the fundamental challenges of learning world models for multi-agent reinforcement learning (MARL), specifically focusing on scalability and non-stationarity issues. The authors propose MARIE, a method that combines decentralized transformer-based world modeling with centralized representation aggregation using a Perceiver. This architecture enables scalable dynamics learning while mitigating non-stationarity through global feature aggregation. The approach demonstrates significant improvements over both model-free baselines and existing model-based methods on the StarCraft Multi-Agent Challenge (SMAC) benchmark.

## Method Summary
MARIE introduces a decentralized transformer-based world model that learns agent-specific dynamics while incorporating global information through a centralized Perceiver aggregation module. The method addresses the key challenges in MARL world modeling: scalability (handling many agents efficiently) and non-stationarity (agents' policies changing during training). The transformer architecture captures temporal dependencies in agent trajectories, while the Perceiver efficiently aggregates global representations to provide context for individual agent modeling. This combination allows the model to learn robust dynamics predictions while maintaining computational efficiency.

## Key Results
- Outperforms strong model-free baselines (MAPPO, QMIX, QPLEX) on SMAC benchmark
- Surpasses existing model-based methods (MAMBA, MBVD) in both sample efficiency and overall performance
- Achieves superior results particularly in complex scenarios with many agents (up to 27 agents)
- Demonstrates competitive performance on continuous action space tasks in MAMujoco and SMACv2

## Why This Works (Mechanism)
The method works by combining local transformer-based dynamics modeling with global feature aggregation. The decentralized transformer captures temporal dependencies in each agent's trajectory, learning rich representations of agent behavior patterns. The centralized Perceiver then aggregates these representations into a global context that helps each agent understand the broader environment state. This architecture mitigates non-stationarity by providing agents with updated global context that accounts for policy changes across the multi-agent system. The transformer's attention mechanism efficiently handles variable-length sequences, while the Perceiver's cross-attention allows for scalable aggregation without quadratic complexity.

## Foundational Learning

**Transformer-based dynamics modeling** - Captures temporal dependencies in agent trajectories using self-attention mechanisms. Needed because agent behaviors exhibit complex temporal patterns that require context from past observations. Quick check: Verify attention weights focus on relevant temporal windows rather than distant irrelevant history.

**Perceiver-based aggregation** - Uses cross-attention to efficiently aggregate global representations from multiple agents. Needed because naive pooling methods lose important interaction information while full attention becomes computationally prohibitive. Quick check: Confirm aggregation preserves key interaction features while maintaining linear scaling with agent count.

**Decentralized learning with centralized aggregation** - Balances scalability (local processing) with global context (centralized aggregation). Needed because purely decentralized methods struggle with coordination while fully centralized methods don't scale. Quick check: Ensure local representations capture sufficient individual agent dynamics while aggregation provides meaningful global context.

## Architecture Onboarding

**Component map**: Agent observations -> Local Transformers -> Agent embeddings -> Perceiver cross-attention -> Aggregated global context -> Dynamics prediction

**Critical path**: Observations → Local Transformer encoding → Perceiver aggregation → World model prediction → Policy optimization

**Design tradeoffs**: The architecture trades some model expressiveness for scalability by using decentralized local processing with centralized aggregation rather than fully centralized modeling. This reduces computational complexity but requires careful design of the aggregation mechanism to preserve essential global information.

**Failure signatures**: Poor performance in highly coordinated scenarios where local processing misses crucial interaction patterns; degraded performance when Perceiver aggregation fails to capture important global state changes; instability when agent policy changes are too rapid for the aggregation module to track.

**3 first experiments**:
1. Validate transformer dynamics model on single-agent environments with known temporal dependencies
2. Test Perceiver aggregation on synthetic multi-agent data with known coordination patterns
3. Compare performance against ablations: without Perceiver (purely decentralized) and without local transformers (purely centralized)

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily limited to discrete-action environments (StarCraft II), with limited testing on continuous-action spaces
- Computational overhead of Perceiver aggregation not thoroughly characterized for very large-scale systems
- Scalability to scenarios with significantly more agents than tested (beyond 27 agents) remains unproven

## Confidence
High: Performance improvements on SMAC benchmarks with substantial Elo score gains
Medium: Sample efficiency advantages across all tested environments
Low: Computational complexity analysis and scalability to extreme agent counts

## Next Checks
1. Evaluate MARIE on larger-scale multi-agent scenarios with 50+ agents to test true scalability limits
2. Conduct ablation studies specifically isolating the contribution of the Perceiver aggregation module versus the transformer dynamics model
3. Provide comprehensive computational complexity analysis comparing training/inference times and memory requirements against all baseline methods