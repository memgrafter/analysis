---
ver: rpa2
title: LLMs May Perform MCQA by Selecting the Least Incorrect Option
arxiv_id: '2402.01349'
source_url: https://arxiv.org/abs/2402.01349
tags:
- llms
- mcqa
- correct
- options
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) have shown remarkable performance
  across many NLP tasks. However, using Multiple Choice Question Answering (MCQA)
  as a benchmark for evaluating LLMs has limitations.
---

# LLMs May Perform MCQA by Selecting the Least Incorrect Option

## Quick Facts
- arXiv ID: 2402.01349
- Source URL: https://arxiv.org/abs/2402.01349
- Reference count: 14
- Primary result: LLMs select the "least incorrect" option rather than distinctly correct answers in MCQA tasks, undermining reliability of MCQA as an evaluation metric.

## Executive Summary
Large language models (LLMs) have demonstrated remarkable performance across many NLP tasks, but their behavior on Multiple Choice Question Answering (MCQA) benchmarks raises concerns about evaluation reliability. This paper reveals that LLMs may not select the distinctly correct answer but instead choose the "least incorrect" option based on token probability rankings. To address this limitation, the authors propose MCQA+, an enhanced dataset augmentation method that introduces variations of original MCQs, including True/False transformations, option reordering, and "none of the above" scenarios. Empirical findings show that LLM performance on MCQA+ is significantly lower than on original MCQA datasets, suggesting MCQA+ provides a more accurate and robust evaluation of model comprehension.

## Method Summary
The study evaluates whether LLMs select the "least incorrect" option rather than distinctly correct answers in MCQA tasks. Using MMLU and MedMCQA datasets, the researchers test multiple LLMs including LLaMA 3 8B, LLaMA 2 13B, LLaMA 3 70B, Mixtral 8×7B, and ChatGPT variants. The methodology involves creating invariance subsets where models consistently predict correct answers across answer option permutations, then transforming original MCQs into True/False format with both correct and incorrect options. MCQA+ augmentation is applied by generating additional question variants including reordered options, true/false versions, and "None of the above" replacements. Model performance is compared across original and augmented datasets to assess comprehension depth.

## Key Results
- LLMs achieve high accuracy on original MCQA datasets but show significant performance decline on MCQA+ variants
- Incorrect options still achieve substantial confidence (83.6% to 99.4% of correct option confidence)
- ChatGPT-4o successfully identifies absence of correct answer in approximately 60% of MCQs when "none of the above" is introduced
- Performance degradation is consistent across multiple LLMs and both general (MMLU) and medical (MedMCQA) domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs treat MCQA tasks as ranking options by correctness likelihood, not as picking a uniquely correct answer.
- Mechanism: The model computes token-level probabilities for each option and selects the one with the highest score. Because token scores are continuous, the top option often has only a slight advantage over others, leading the model to perceive multiple options as "somewhat correct."
- Core assumption: LLM probability outputs are meaningfully calibrated for option selection tasks.
- Evidence anchors:
  - [abstract] "LLMs may not select the distinctly correct answer but instead choose the 'least incorrect' option"
  - [section] "the incorrect∗ options still achieve substantial confidence ranging from 83.6% to 99.4% of those for the correct options"
- Break condition: If token probabilities are poorly calibrated or the model uses a different selection strategy (e.g., deterministic generation), the mechanism fails.

### Mechanism 2
- Claim: The original MCQA format lacks negative testing, so models can succeed without truly discriminating correct from incorrect options.
- Mechanism: In standard MCQA, models are only tested on selecting the correct option; they are never required to justify why incorrect options are wrong. This allows models to pass by ranking rather than reasoning.
- Core assumption: Absence of negative testing allows superficial success.
- Evidence anchors:
  - [section] "modifying the MCQA datasets to include (1) T/F questions derived from incorrect options... results in a pronounced performance decline"
  - [section] "ChatGPT-4o successfully identifies that there is no correct answer in approximately 60% of the MCQs"
- Break condition: If negative examples are included in training or evaluation, the model must develop deeper discrimination.

### Mechanism 3
- Claim: Augmentation with diverse question variants (MCQA+) exposes model weaknesses that standard MCQA masks.
- Mechanism: By converting MCQs into T/F questions, reordering options, and introducing "none of the above" scenarios, MCQA+ forces the model to demonstrate understanding of both correct and incorrect options, not just ranking.
- Core assumption: Diverse question formats reveal true comprehension.
- Evidence anchors:
  - [abstract] "MCQA+ can serve as a more effective benchmark for developing robust and adaptable NLP models"
  - [section] "Performance on the MCQA+ dataset shows a significant decline across all LLMs compared to the original MCQA dataset"
- Break condition: If the model is fine-tuned on these specific augmentation patterns, the evaluation may become a memorization test rather than a comprehension test.

## Foundational Learning

- Concept: Token probability calibration
  - Why needed here: Understanding how LLMs assign and interpret probabilities for answer options is key to interpreting why they select "least incorrect" rather than "correct."
  - Quick check question: If a model assigns 0.96 to the correct option and 0.94 to an incorrect option, what selection strategy is it using?

- Concept: Negative testing in evaluation
  - Why needed here: Standard MCQA only tests positive cases (finding the right answer). Without negative testing, models can succeed without true discrimination.
  - Quick check question: What happens to model accuracy when incorrect options are explicitly tested as "false" statements?

- Concept: Dataset augmentation for robustness
  - Why needed here: MCQA+ shows how creating diverse question variants can reveal model limitations that single-format testing misses.
  - Quick check question: How does accuracy change when "none of the above" replaces the correct option in MCQs?

## Architecture Onboarding

- Component map:
  Input preprocessor -> Token probability engine -> Option ranking module -> Selection logic -> Augmentation generator -> Evaluation metrics

- Critical path:
  1. Parse MCQ into question + options
  2. Generate token probabilities for each option
  3. Rank options by probability
  4. Select highest-ranked option
  5. Record accuracy
  6. Generate MCQA+ variants
  7. Repeat evaluation on augmented data

- Design tradeoffs:
  - Speed vs. robustness: Full MCQA+ evaluation is slower but more reliable
  - Token probability vs. generation-based selection: Probability ranking is faster but may be less robust
  - Augmentation coverage vs. computational cost: More variants improve evaluation but increase cost

- Failure signatures:
  - High accuracy on original MCQA but low accuracy on MCQA+ variants
  - Model selects incorrect options when correct option is replaced with "none of the above"
  - Model cannot justify why incorrect options are wrong

- First 3 experiments:
  1. Run original MCQA evaluation on a held-out set and record accuracy
  2. Convert correct options to T/F format and test model's ability to recognize them as true
  3. Replace correct options with "none of the above" and measure model's ability to recognize absence of correct answer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do training objectives and reinforcement learning alignment techniques contribute to LLMs' tendency to select the "least incorrect" option rather than the distinctly correct answer in MCQA tasks?
- Basis in paper: [inferred] The paper hypothesizes that the issue may stem from the training objectives of LLMs, such as predicting the next token based on probability rankings, and that instruction-tuning and RL-based alignment may have negative impacts.
- Why unresolved: The paper does not conduct experiments to directly test the impact of different training objectives or alignment techniques on this specific behavior.
- What evidence would resolve it: Experiments comparing LLM performance on MCQA tasks before and after different training objectives or alignment techniques, or comparing different models with varying training approaches.

### Open Question 2
- Question: Can the proposed MCQA+ dataset augmentation method be extended to other evaluation tasks beyond MCQA, such as open-ended question answering or text summarization, to provide more robust and comprehensive LLM evaluation?
- Basis in paper: [inferred] The paper introduces MCQA+ as a method to improve MCQA evaluation, but does not explore its potential application to other evaluation tasks.
- Why unresolved: The paper focuses specifically on MCQA and does not investigate the generalizability of the MCQA+ approach to other tasks.
- What evidence would resolve it: Experiments applying the MCQA+ methodology (introducing variations and multiple perspectives) to other evaluation tasks and assessing its effectiveness in improving evaluation robustness and comprehensiveness.

### Open Question 3
- Question: What are the long-term implications of LLMs' behavior in MCQA tasks for their overall understanding and reasoning capabilities, and how can this inform the development of more advanced and reliable evaluation methodologies?
- Basis in paper: [explicit] The paper highlights the importance of addressing this issue to develop more comprehensive evaluation methodologies and ensure that LLM capabilities are accurately reflected.
- Why unresolved: The paper does not provide a detailed analysis of the long-term implications or propose specific strategies for developing more advanced evaluation methodologies.
- What evidence would resolve it: Longitudinal studies tracking LLM performance on MCQA tasks and other evaluation benchmarks, and research into developing new evaluation methodologies that address the identified limitations and provide a more holistic assessment of LLM capabilities.

## Limitations

- The "least incorrect" explanation remains inferential rather than conclusively proven; performance drops on MCQA+ could reflect format unfamiliarity rather than the proposed mechanism
- Results are based only on MMLU and MedMCQA datasets, limiting generalizability to other MCQA formats or domains
- MCQA+ augmentation patterns (T/F conversion, option reordering, "none of the above") may be learnable, potentially measuring memorization rather than comprehension

## Confidence

**High confidence**: Performance degradation on MCQA+ variants is consistently observed across multiple models and datasets. This empirical finding is robust and well-supported.

**Medium confidence**: The "least incorrect" explanation for performance differences is plausible and mechanistically coherent, but alternative interpretations exist. The evidence supports the phenomenon but not definitively the specific mechanism.

**Low confidence**: Claims about token probability distributions revealing "substantial confidence" in incorrect options require more rigorous calibration analysis. The connection between probability scores and actual model reasoning remains unclear.

## Next Checks

1. **Calibration analysis**: Measure whether token probabilities for answer options are properly calibrated by testing models on questions where the correct answer is known to be highly probable versus questions where multiple options have similar probabilities.

2. **Ablation study**: Systematically remove each MCQA+ augmentation type (T/F conversion, option reordering, "none of the above") to determine which specific transformations cause performance drops, isolating the "least incorrect" effect from format unfamiliarity.

3. **Cross-dataset replication**: Test the same methodology on completely different MCQA datasets (e.g., RACE, ARC, OpenBookQA) to determine whether the "least incorrect" phenomenon generalizes beyond MMLU and MedMCQA.