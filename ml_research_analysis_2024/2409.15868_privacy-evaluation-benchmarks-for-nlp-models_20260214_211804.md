---
ver: rpa2
title: Privacy Evaluation Benchmarks for NLP Models
arxiv_id: '2409.15868'
source_url: https://arxiv.org/abs/2409.15868
tags:
- data
- attack
- attacks
- different
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive privacy evaluation benchmark
  for NLP models, including both conventional and large language models. The benchmark
  supports various models, datasets, and protocols, along with standardized modules
  for comprehensive evaluation of attacks and defense strategies.
---

# Privacy Evaluation Benchmarks for NLP Models

## Quick Facts
- arXiv ID: 2409.15868
- Source URL: https://arxiv.org/abs/2409.15868
- Reference count: 40
- This paper presents a comprehensive privacy evaluation benchmark for NLP models, including both conventional and large language models

## Executive Summary
This paper introduces a comprehensive privacy evaluation benchmark for NLP models that supports various models, datasets, and protocols while providing standardized modules for comprehensive evaluation of attacks and defense strategies. The benchmark addresses the growing need for systematic evaluation of privacy risks in NLP models, particularly as large language models become more prevalent. Through extensive experiments, the authors demonstrate how Knowledge Distillation can mitigate poor performance of membership inference attacks when auxiliary data comes from different domains, and propose a chained framework for privacy attacks that allows multiple attacks to be combined to achieve higher-level objectives.

## Method Summary
The benchmark provides a systematic framework for evaluating privacy attacks and defenses in NLP models, supporting both conventional models (BERT, RoBERTa, GPT2-small) and large language models (Llama2, Qwen, GPT2-xl). The framework implements four types of privacy attacks (Membership Inference, Model Inversion, Attribute Inference, Model Extraction) under different threat models (black-box/white-box) and auxiliary data settings (shadow data, partial data, no data, data from different domains). Defenses include DP-SGD, SELENA, and TextHide. The system uses configuration files to specify attack/defense parameters and supports chained attacks where one attack can enable or enhance another.

## Key Results
- Knowledge Distillation effectively mitigates poor MIA performance when auxiliary data comes from different domains by transferring knowledge from target to student model
- MEA can simultaneously act as both an attack and a defense mechanism, with extracted models providing protection against MIA
- Data-free MDIA can generate synthetic data that passes MIA filters, enabling attacks without auxiliary data
- The benchmark effectively assesses privacy risks and reveals relationships between different attack types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge Distillation (KD) mitigates poor performance of membership inference attacks (MIA) when auxiliary data is from different domains
- Mechanism: The student model trained via KD simulates the behavior of the target model using logits from domain-shifted data, creating a shadow model that better captures membership states
- Core assumption: KD can effectively transfer the target model's decision boundary knowledge even when the auxiliary data distribution differs significantly from training data
- Evidence anchors:
  - [abstract]: "we provide an improved attack method in this scenario with the help of Knowledge Distillation (KD)"
  - [section 3.2]: "We use KD to distill the knowledge contained in the teacher model into the student model. In other words, the student model can simulate the behavior of the teacher (target) model."
- Break condition: KD fails if the domain gap is too large for the student model to learn meaningful representations from the target model's logits

### Mechanism 2
- Claim: Chaining attacks (e.g., MEA → MIA) creates new attack capabilities and defense strategies
- Mechanism: The extracted model from MEA can serve as a defense against MIA by providing a surrogate model that doesn't contain original training data, while also enabling white-box attacks when parameters are accessible
- Core assumption: The extraction model can approximate target model behavior without inheriting training data membership information
- Evidence anchors:
  - [abstract]: "we propose a chained framework for privacy attacks. Allowing a practitioner to chain multiple attacks to achieve a higher-level attack objective"
  - [section 4]: "MEA can act as a defense strategy against MIA" and "the extraction model provides a strong defense against MIAs"
- Break condition: Chaining fails if the intermediate attack model doesn't preserve sufficient information for the subsequent attack

### Mechanism 3
- Claim: Data-free MDIA can generate synthetic data that passes MIA filters, enabling attacks without auxiliary data
- Mechanism: MDIA generates synthetic samples that are then filtered through MIA to select those classified as members, creating a data-free attack pipeline
- Core assumption: MDIA can generate samples that, when filtered by MIA, approximate the distribution of training data
- Evidence anchors:
  - [section 4]: "Leveraging the attack capability of the MDIA, we demonstrate a data-free MIA"
  - [section 6.3]: "data generated by the MDIA and filtered through the MIA can effectively increase the success rate of the MDIA"
- Break condition: Data-free attacks fail if MDIA cannot generate samples that pass MIA filters or if the filtering process removes too many samples

## Foundational Learning

- Concept: Threat modeling in machine learning privacy attacks
  - Why needed here: The benchmark supports multiple threat models (black-box vs white-box, different auxiliary data scenarios) that determine attack feasibility and effectiveness
  - Quick check question: What are the key differences between black-box and white-box threat models in terms of attacker capabilities?

- Concept: Knowledge Distillation and its application to privacy attacks
  - Why needed here: KD is used to mitigate poor MIA performance when auxiliary data comes from different domains by transferring knowledge from target to student model
  - Quick check question: How does KD help create a shadow model when the auxiliary data distribution differs from the target training data?

- Concept: Model extraction and its relationship to other privacy attacks
  - Why needed here: MEA can serve as both an attack and a defense mechanism, and its effectiveness correlates with MIA performance
  - Quick check question: Why does a model that is more vulnerable to MEA tend to be less vulnerable to MIA?

## Architecture Onboarding

- Component map: Load model → Load auxiliary data → Select attack/defense type → Configure parameters → Run attack/defense → Evaluate results
- Critical path: The benchmark processes through attack selection, defense configuration, and evaluation in a standardized pipeline
- Design tradeoffs: Balances comprehensiveness (supporting many models, datasets, and attack types) with usability (standardized modules and configuration files)
- Failure signatures: Low attack accuracy (near 0.5) may indicate poor auxiliary data quality, overfitting issues, or domain mismatch; failed defenses show minimal performance degradation on protected models
- First 3 experiments:
  1. Run MIA on BERT with shadow data on MRPC dataset to verify basic attack functionality
  2. Apply DP-SGD defense to the same setup to test defense effectiveness
  3. Chain MEA → MIA on a different dataset to verify the chained framework functionality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of different defense strategies vary across diverse NLP model architectures and dataset types?
- Basis in paper: [explicit] The paper compares DP-SGD, SELENA, and TextHide defenses across BERT, RoBERTa, and GPT2 models on multiple datasets
- Why unresolved: The paper only reports results for BERT; comprehensive analysis across all model types and datasets is needed
- What evidence would resolve it: Systematic testing of all three defense strategies across all model architectures and datasets mentioned in the paper

### Open Question 2
- Question: What is the relationship between model size and vulnerability to attribute inference attacks in LLMs?
- Basis in paper: [explicit] The paper shows that larger models like Llama2-13B have higher success rates in LAIA compared to Llama2-7B
- Why unresolved: The paper only compares two model sizes; a comprehensive study across a wider range of model scales is needed
- What evidence would resolve it: Experiments testing attribute inference attacks across multiple model sizes (e.g., 1B, 7B, 13B, 30B, 65B parameters)

### Open Question 3
- Question: How does the chained framework's effectiveness vary when combining more than two attack types?
- Basis in paper: [inferred] The paper discusses chaining two attacks at a time but mentions potential for more complex connections
- Why unresolved: The paper only explores pairwise attack combinations and suggests more complex chains could exist
- What evidence would resolve it: Experimental results testing chained frameworks with three or more attack types and comparing their effectiveness to pairwise combinations

## Limitations
- Limited evaluation on truly large-scale LLMs (largest tested is 7B parameters)
- Potential computational constraints when running comprehensive evaluations across all supported configurations
- Uncertainty about the benchmark's generalizability to emerging privacy attack methodologies

## Confidence

**High Confidence**: The benchmark's modular architecture and support for multiple attack/defense types is well-specified and reproducible. The implementation details for core attack mechanisms (MIA, MDIA, AIA, MEA) are clearly described.

**Medium Confidence**: The effectiveness of Knowledge Distillation for mitigating domain-shifted MIA attacks and the chained framework's practical utility need empirical validation across diverse scenarios.

**Low Confidence**: The claim that MEA can simultaneously act as both an attack and a defense mechanism requires more rigorous experimental verification, particularly regarding the conditions under which this dual role holds.

## Next Checks

1. **Cross-Domain Attack Validation**: Test MIA performance with KD mitigation using auxiliary data from at least three different domains (e.g., legal, medical, social media) to verify the robustness claims

2. **Chained Framework Efficacy**: Systematically evaluate all possible attack chains (4 attacks → 12 possible combinations) to determine which chained configurations provide meaningful improvements over standalone attacks

3. **Defense Generalization Test**: Apply the benchmark to evaluate defenses against recently proposed privacy attacks not included in the original evaluation (e.g., membership inference attacks using token-level predictions or attention patterns) to assess defense effectiveness beyond the tested attack set