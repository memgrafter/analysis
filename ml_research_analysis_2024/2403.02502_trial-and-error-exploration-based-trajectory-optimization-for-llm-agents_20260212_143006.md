---
ver: rpa2
title: 'Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents'
arxiv_id: '2403.02502'
source_url: https://arxiv.org/abs/2403.02502
tags:
- agent
- trajectory
- action
- learning
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Exploration-based Trajectory Optimization
  (ETO), a method that enhances LLM agents by learning from exploration failures rather
  than just successful expert trajectories. ETO employs an iterative framework where
  agents first explore the environment, collect failure trajectories, and then use
  contrastive learning (via DPO) to refine their policy using failure-success trajectory
  pairs.
---

# Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents

## Quick Facts
- arXiv ID: 2403.02502
- Source URL: https://arxiv.org/abs/2403.02502
- Authors: Yifan Song; Da Yin; Xiang Yue; Jie Huang; Sujian Li; Bill Yuchen Lin
- Reference count: 20
- Key outcome: ETO improves LLM agent performance by learning from exploration failures using contrastive learning, outperforming baselines on WebShop, ScienceWorld, and ALFWorld.

## Executive Summary
This paper introduces Exploration-based Trajectory Optimization (ETO), a novel method for enhancing LLM agents by learning from exploration failures rather than just successful expert trajectories. ETO employs an iterative framework where agents first explore the environment, collect failure trajectories, and then use contrastive learning (via DPO) to refine their policy using failure-success trajectory pairs. Experiments on three complex interactive tasks demonstrate that ETO consistently outperforms baseline methods like SFT behavioral cloning and other strong baselines by significant margins.

## Method Summary
ETO is an iterative framework that enhances LLM agents through exploration-based trajectory optimization. The method begins with behavioral cloning on expert trajectories to create a base agent, which then explores the environment to collect failure trajectories. These failure trajectories are paired with expert successes and used in a contrastive learning framework (DPO) to update the policy. The process iterates, with each cycle potentially improving agent performance. The contrastive learning objective increases the likelihood of success trajectories while decreasing failure trajectories, constrained by a KL divergence term to maintain basic capabilities.

## Key Results
- ETO consistently outperforms SFT behavioral cloning and other baselines on WebShop, ScienceWorld, and ALFWorld datasets
- The method shows improved task-solving efficiency and generalization, particularly in out-of-distribution scenarios
- Performance improves across multiple iterations but shows diminishing returns after three cycles due to overfitting concerns

## Why This Works (Mechanism)

### Mechanism 1
The method improves LLM agent performance by learning from failure trajectories using contrastive learning. The agent first explores the environment, collects failure trajectories, and then uses these failures paired with expert successes in a contrastive learning framework (DPO) to update its policy. Core assumption: Failure trajectories paired with expert successes provide more informative learning signals than expert trajectories alone.

### Mechanism 2
The iterative exploration-training loop enables continued improvement of the agent's policy. After initial behavioral cloning, the agent iteratively explores, collects failures, and updates its policy using contrastive learning, refining its behavior over multiple rounds. Core assumption: Iterative refinement using contrastive learning will lead to monotonic improvement in agent performance.

### Mechanism 3
The contrastive learning objective (DPO) effectively models the preference between failure and success trajectories. The DPO loss optimizes the agent's policy by increasing the likelihood of success trajectories and decreasing the likelihood of failure trajectories, constrained by a KL divergence term to maintain basic capabilities. Core assumption: The DPO loss formulation accurately captures the preference relationship between failure and success trajectories.

## Foundational Learning

- **Behavioral Cloning (BC)**: Why needed here: BC provides the initial policy for the agent before exploration and contrastive learning. Quick check question: What is the primary goal of behavioral cloning in this context?
- **Contrastive Learning**: Why needed here: Contrastive learning is used to update the agent's policy by learning from the differences between failure and success trajectories. Quick check question: How does contrastive learning help the agent improve its policy in this method?
- **Reinforcement Learning (RL) and PPO**: Why needed here: While not directly used, understanding RL concepts helps in comparing ETO with RL-based approaches and understanding the limitations of direct RL on LLM agents. Quick check question: Why might direct RL optimization be challenging for LLM agents compared to the ETO approach?

## Architecture Onboarding

- **Component map**: Base Agent (BC) -> Exploration Phase (collect failures) -> Training Phase (DPO on contrastive pairs) -> Iterative Loop
- **Critical path**:
  1. Train base agent via BC on expert trajectories
  2. Use base agent to explore environment and collect failure trajectories
  3. Construct failure-success trajectory pairs
  4. Update agent policy using DPO on contrastive pairs
  5. Repeat steps 2-4 for multiple iterations (if beneficial)
- **Design tradeoffs**: Tradeoff between exploration time and training time; balancing diversity of failure cases versus risk of overfitting; choosing number of iterations for optimal performance
- **Failure signatures**: Agent performance plateaus or degrades after several iterations; insufficient diversity in collected failure trajectories; inaccurate reward signals leading to incorrect failure-success labeling
- **First 3 experiments**:
  1. Train base agent via BC on a small subset of expert trajectories
  2. Run exploration with base agent on a simple environment, collect failure trajectories
  3. Apply DPO on contrastive pairs and evaluate performance improvement on a held-out test set

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of ETO scale with larger iteration numbers beyond three, and what are the specific factors that cause performance degradation in later iterations? The paper only tests up to three iterations and does not explore beyond this point. The exact mechanisms of overfitting and the impact of binary rewards on iterative learning are not fully explained.

### Open Question 2
How does the performance of ETO compare when using different methods for constructing contrastive trajectory pairs, such as action-wise or process-level comparisons, compared to the current trajectory-wise approach? The paper discusses an alternative step-wise contrastive approach but finds that trajectory-wise contrastive yields the best performance, while step-wise contrastive is less stable.

### Open Question 3
How does ETO perform in multi-task learning scenarios, and what is the transferability of policies trained by ETO across different tasks? The paper mentions that future work will investigate the transferability of ETO-trained policies and their application in multi-task training scenarios, but does not provide any experimental results.

## Limitations
- Method requires access to expert trajectories for initial behavioral cloning phase
- Iterative exploration phase can be computationally expensive and time-consuming
- Risk of overfitting to contrastive pairs, especially if diversity of failure cases diminishes across iterations
- Performance depends on quality of reward signals for distinguishing success from failure trajectories

## Confidence
- **High Confidence**: The core mechanism of using contrastive learning with failure-success trajectory pairs is well-supported by experimental results across all three datasets
- **Medium Confidence**: The iterative improvement claim is supported but the paper doesn't thoroughly explore diminishing returns or potential degradation in later iterations
- **Medium Confidence**: The effectiveness of DPO for modeling trajectory preferences is demonstrated, but sensitivity to hyperparameter tuning and impact of reward signal quality are not extensively analyzed

## Next Checks
1. **Ablation Study on Iteration Count**: Systematically evaluate performance across different numbers of ETO iterations (1-5) to identify optimal iteration count and detect performance degradation from overfitting
2. **Reward Signal Sensitivity Analysis**: Test method's robustness by varying quality and reliability of reward signals used to label failure/success trajectories, including scenarios with noisy or incomplete rewards
3. **Generalization to Unconstrained Environments**: Evaluate ETO's performance in environments without expert trajectories or with significantly different task distributions to assess true generalization capabilities beyond curated datasets