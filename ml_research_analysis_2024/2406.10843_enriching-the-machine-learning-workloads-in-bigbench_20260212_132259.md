---
ver: rpa2
title: Enriching the Machine Learning Workloads in BigBench
arxiv_id: '2406.10843'
source_url: https://arxiv.org/abs/2406.10843
tags:
- data
- algorithms
- machine
- learning
- bigbench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends BigBench V2 by adding three new machine learning
  workloads and implementing algorithms from multiple libraries (MLlib, SystemML,
  Scikit-learn, Pandas). The new workloads cover frequent pattern mining, topic modeling,
  and product recommendation.
---

# Enriching the Machine Learning Workloads in BigBench

## Quick Facts
- arXiv ID: 2406.10843
- Source URL: https://arxiv.org/abs/2406.10843
- Authors: Matthias Polag; Todor Ivanov; Timo Eichhorn
- Reference count: 29
- Key outcome: Extends BigBench V2 with three ML workloads covering frequent pattern mining, topic modeling, and product recommendation, showing SystemML outperforms other libraries for distributed processing

## Executive Summary
This paper extends the BigBench V2 benchmark by adding three new machine learning workloads that cover frequent pattern mining, topic modeling, and product recommendation. The workloads implement algorithms from multiple popular libraries including MLlib, SystemML, Scikit-learn, and Pandas, allowing for comprehensive performance comparisons. Experiments demonstrate that distributed implementations like SystemML generally outperform local Scikit-learn implementations, particularly for larger datasets, with SystemML showing the best scalability and fastest execution times across the tested workloads.

## Method Summary
The paper implements three new ML workloads (M1, M2, M3) in BigBench V2 using algorithms from multiple libraries. M1 implements FP-Growth and Eclat for frequent pattern mining, M2 uses Latent Dirichlet Allocation for topic modeling, and M3 employs Decision Trees and Multilayer Perceptrons for classification. The experimental setup uses a 4-node cluster with Hadoop/HDFS for storage and Spark/YARN for distributed computation. Datasets are generated at scale factors 1, 10, and 200 using the BigBench V2 data generator, with execution times measured across all library implementations.

## Key Results
- SystemML demonstrated the best scalability and fastest execution times across all tested workloads
- Distributed implementations generally outperform local Scikit-learn for larger datasets
- Format conversion overhead between matrix market and dataframes is acceptable given SystemML's performance advantages
- No single library is optimal for all ML tasks, supporting the need for diverse library implementations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distributed implementations (SystemML) outperform local Scikit-learn for larger datasets due to better parallelization and memory management.
- Mechanism: SystemML uses matrix market format and distributed execution, allowing it to handle large-scale data more efficiently than Scikit-learn's single-node, in-memory approach.
- Core assumption: Data size exceeds the memory capacity of a single machine, making distributed processing necessary.
- Evidence anchors:
  - [abstract] "SystemML demonstrated the best scalability and fastest execution times across the workloads tested."
  - [section] "SystemML performs best with all cluster based implementations. Even with the problematic format conversion, it still outperforms MLlib..."
  - [corpus] No direct evidence in corpus neighbors about SystemML's performance relative to Scikit-learn.

### Mechanism 2
- Claim: Simple frequent pattern mining queries can be significantly enhanced using more sophisticated ML algorithms (M1).
- Mechanism: M1 implements FP-Growth and Eclat algorithms that discover item sets of size 3+, providing richer association rules than the original Q1's pair-based mining.
- Core assumption: Larger item sets capture more meaningful business relationships than just pairs.
- Evidence anchors:
  - [section] "M1 represents a pattern mining workload... This new workload fits into the business scheme of marketing and enables better product recommendation to the customers."
  - [section] "Even with the higher complexity and the creation of frequent sets of size at least 3, both Eclat and FP-Growth achieve faster run times for the first two SFs."
  - [corpus] No corpus evidence about pattern mining performance.

### Mechanism 3
- Claim: Using diverse libraries (MLlib, SystemML, Scikit-learn, Pandas) provides optimal results by leveraging each library's strengths.
- Mechanism: Different libraries excel at different tasks - SystemML for matrix operations, Scikit-learn for simple local tasks, MLlib for Spark-native operations, Pandas for data preprocessing.
- Core assumption: No single library is optimal for all ML tasks, so mixing implementations yields best performance.
- Evidence anchors:
  - [abstract] "Our workloads utilize multiple algorithms and compare different implementations for the same algorithm across several popular libraries..."
  - [section] "In general, the quality of algorithms is not necessarily consistent among one specific library, and a mixture of different implementations might be used to achieve the best results across workloads."
  - [corpus] No corpus evidence about library diversity benefits.

## Foundational Learning

- Concept: Big Data Scale Factors (SF)
  - Why needed here: The paper uses scale factors to represent different dataset sizes, which directly impacts algorithm performance comparisons.
  - Quick check question: If SF=200 represents 293GB of weblogs, approximately how much data does SF=10 represent?

- Concept: Matrix Market Format vs DataFrames
  - Why needed here: SystemML uses matrix market format while Spark libraries use dataframes, creating conversion overhead but potentially better performance.
  - Quick check question: Why might matrix market format provide performance advantages over dataframes for certain ML operations?

- Concept: TF-IDF Transformation
  - Why needed here: Used in Q28/M2 for text processing, converting raw text into weighted numerical features for classification.
  - Quick check question: What is the primary purpose of the inverse document frequency component in TF-IDF?

## Architecture Onboarding

- Component map:
  Data Generation -> Hadoop/HDFS -> Spark/YARN -> ML Libraries (MLlib, SystemML, Scikit-learn, Pandas) -> Query Results

- Critical path:
  1. Data generation and loading into Hive
  2. Query execution using appropriate ML library
  3. Performance measurement and comparison
  4. Results aggregation and analysis

- Design tradeoffs:
  - SystemML offers best performance but requires format conversion
  - Scikit-learn is simple but limited to single-node execution
  - MLlib integrates well with Spark but may have scaling limitations
  - Library diversity increases complexity but optimizes performance

- Failure signatures:
  - Memory errors: Dataset too large for single-node libraries
  - Format conversion failures: Incompatible data structures between libraries
  - Performance degradation: Incorrect library selection for data size
  - Scalability issues: Algorithms that don't scale well with increasing SF

- First 3 experiments:
  1. Run Q26 K-Means with all four libraries on SF=1 to establish baseline performance
  2. Execute M1 with FP-Growth and Eclat on SF=10 to compare pattern mining approaches
  3. Test M3 SVM implementations across libraries on SF=200 to observe scaling behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the MLlib K-Means algorithm compare to other libraries (e.g., SystemML) when the dataset size is significantly increased beyond the tested scale factors?
- Basis in paper: [inferred] The paper mentions that a larger amount of data is necessary to test the algorithms and that parallel algorithms on the cluster are expected to outperform local Scikit-learn for larger datasets.
- Why unresolved: The experiments conducted in the paper did not use datasets large enough to conclusively determine the performance differences between MLlib K-Means and other libraries at very large scales.
- What evidence would resolve it: Conducting experiments with significantly larger datasets (e.g., scale factors beyond 200) and comparing the performance of MLlib K-Means with other libraries like SystemML would provide the necessary evidence.

### Open Question 2
- Question: What are the specific factors contributing to the observed variance in run times for the MLlib K-Means algorithm across different test runs?
- Basis in paper: [explicit] The paper notes a run time variance of about 10% for the MLlib K-Means algorithm during multiple test runs.
- Why unresolved: The paper does not provide a detailed analysis of the factors causing the variance in run times for the MLlib K-Means algorithm.
- What evidence would resolve it: Analyzing the execution environment, resource allocation, and potential interference from other processes during the test runs would help identify the specific factors contributing to the variance.

### Open Question 3
- Question: How does the Latent Dirichlet Allocation (LDA) algorithm's performance and scalability change with datasets containing a significantly larger number of unique terms?
- Basis in paper: [inferred] The paper mentions that the LDA algorithm failed due to insufficient memory when applied to the dataset with scale factor 200, suggesting concerns about its scalability with large datasets.
- Why unresolved: The experiments did not include datasets with a significantly larger number of unique terms to test the scalability of the LDA algorithm.
- What evidence would resolve it: Conducting experiments with datasets containing a larger number of unique terms and monitoring the memory usage and performance of the LDA algorithm would provide insights into its scalability.

## Limitations
- Limited scope with only three workloads tested, restricting generalizability of conclusions
- Missing statistical significance testing for performance differences between implementations
- Incomplete characterization of format conversion overhead between different data representations
- Lack of detailed algorithm parameter tuning across different libraries

## Confidence
- High confidence in SystemML's superior performance for distributed workloads
- Medium confidence in library diversity benefits
- Low confidence in generalizability beyond tested datasets and algorithms

## Next Checks
1. Replicate experiments with additional scale factors (SF=50, SF=100) to better understand scaling behavior
2. Implement cross-validation for algorithm parameters to ensure fair comparisons
3. Test additional ML libraries (e.g., TensorFlow, PyTorch) to expand the library diversity assessment