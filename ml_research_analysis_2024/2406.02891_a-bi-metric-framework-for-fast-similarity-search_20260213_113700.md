---
ver: rpa2
title: A Bi-metric Framework for Fast Similarity Search
arxiv_id: '2406.02891'
source_url: https://arxiv.org/abs/2406.02891
tags:
- metric
- distance
- search
- nearest
- neighbor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a bi-metric framework for similarity search
  that combines a cheap but noisy proxy metric with an expensive ground-truth metric.
  The method constructs data structures using only the proxy metric while achieving
  the accuracy of the ground-truth metric with limited queries to both metrics.
---

# A Bi-metric Framework for Fast Similarity Search

## Quick Facts
- **arXiv ID**: 2406.02891
- **Source URL**: https://arxiv.org/abs/2406.02891
- **Reference count**: 40
- **Primary result**: Achieves up to 4x fewer expensive model evaluations while maintaining state-of-the-art retrieval accuracy on MTEB benchmark

## Executive Summary
This paper introduces a bi-metric framework for similarity search that combines a cheap but noisy proxy metric with an expensive ground-truth metric. The method constructs data structures using only the proxy metric while achieving the accuracy of the ground-truth metric with limited queries to both metrics. Theoretical analysis is provided for two popular algorithms: DiskANN and Cover Tree. In practice, the framework is applied to text retrieval using two machine learning models with vastly different computational costs. Experiments on the MTEB benchmark show that the bi-metric approach achieves a significantly better accuracy-efficiency tradeoff than alternatives like re-ranking, with up to 4x fewer evaluations of the expensive model while maintaining state-of-the-art retrieval accuracy on several datasets.

## Method Summary
The bi-metric framework assumes two dissimilarity functions: a ground-truth metric that is accurate but expensive to compute, and a proxy metric that is cheaper but less accurate. The framework constructs data structures using only the proxy metric, but achieves accuracy comparable to using the ground-truth metric alone. This is accomplished by leveraging the fact that if the proxy metric provides a bounded approximation of the ground-truth metric, then graph structures built using the proxy metric can be used with the ground-truth metric during queries while maintaining accuracy. The theoretical analysis covers DiskANN and Cover Tree algorithms, showing how the doubling dimension of the proxy metric determines the space and query complexity of the approach.

## Key Results
- The bi-metric framework achieves a considerably better accuracy-efficiency tradeoff than re-ranking alternatives on MTEB benchmark datasets
- On NQ, HotpotQA, and FiQA datasets, the framework with SFR-Embedding-Mistral achieves higher NDCG@10 scores than using bge-micro-v2 alone with the same number of queries
- The bi-metric approach with SFR-Embedding-Mistral achieves comparable performance to bge-large-v2 while using 3-4x fewer queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A cheap proxy metric can guide efficient construction of a nearest-neighbor graph that retains accuracy of an expensive ground-truth metric
- Mechanism: The proxy metric is used to build an α-shortcut reachable graph. This graph has the property that for any pair of points (p, q), either (p, q) is an edge or there exists p' such that (p, p') is an edge and d(p', q) · α ≤ d(p, q). The bi-metric framework leverages the fact that if d approximates D up to factor C, then an α-shortcut reachable graph under d is also an α/C-shortcut reachable graph under D. This allows the expensive metric to be used only during queries while maintaining the accuracy of D.
- Core assumption: The proxy metric d provides a C-approximation of the ground-truth metric D, meaning d(x, y) ≤ D(x, y) ≤ C · d(x, y) for all x, y
- Evidence anchors:
  - [abstract]: "Our framework assumes two dissimilarity functions: a ground-truth metric that is accurate but expensive to compute, and a proxy metric that is cheaper but less accurate."
  - [section]: "Definition 2.1. Given a set of n points P in a metric space X and its distance function D, we say the distance function d is a C-approximation of D if for all x, y ∈ X, d(x, y) ≤ D(x, y) ≤ C · d(x, y)."
- Break condition: If the proxy metric fails to provide a bounded approximation of the ground-truth metric, the framework breaks down as the graph structure built using d would not maintain accuracy for D.

### Mechanism 2
- Claim: The doubling dimension λd of the proxy metric determines the space and query complexity of the bi-metric framework
- Mechanism: The doubling dimension measures the intrinsic dimensionality of the dataset. For a dataset with doubling dimension λd under metric d, a r-cover can be constructed such that the space required is proportional to n · (C/ε)^λd log(∆d), where C is the approximation factor and ε is the accuracy parameter. This covers the trade-off between space efficiency and accuracy.
- Core assumption: The doubling dimension λd is a reasonable measure of the intrinsic dimensionality of the dataset for both metrics d and D
- Evidence anchors:
  - [abstract]: "Our theoretical study analyzes this approach when applied to two popular algorithms: DiskANN and Cover Tree."
  - [section]: "Definition 2.2 (Doubling Dimension). We say a point set X has doubling dimension λd with respect to metric d if for any p ∈ X, and radius r > 0, X ∩ B(p, 2r) can be covered by at most 2^λd balls with radius r."
- Break condition: If the doubling dimension varies significantly between the proxy metric d and ground-truth metric D, the space and query complexity bounds may not hold.

### Mechanism 3
- Claim: The bi-metric framework achieves better accuracy-efficiency tradeoffs than traditional re-ranking approaches
- Mechanism: Traditional re-ranking approaches first retrieve a large set of candidates using the proxy metric and then re-rank using the ground-truth metric. This approach is limited by the accuracy of the proxy metric. The bi-metric framework instead builds a graph index using the proxy metric but performs greedy search using the ground-truth metric, allowing it to achieve higher accuracy with fewer evaluations of the expensive metric.
- Core assumption: The graph-based search algorithm can effectively route queries to their nearest neighbors even when using the ground-truth metric D instead of the proxy metric d
- Evidence anchors:
  - [abstract]: "We observe that for almost all data sets in the MTEB benchmark, our approach achieves a considerably better accuracy-efficiency tradeoff than the alternatives, such as re-ranking."
  - [section]: "Our results In this paper we show that, in both theory and practice, it is possible to combine cheap and expensive models to achieve approximate nearest neighbor data structures that inherit the accuracy of expensive models while significantly reducing the overall computational cost."
- Break condition: If the graph-based search algorithm performs poorly with the ground-truth metric D, the accuracy gains may not materialize.

## Foundational Learning

- Concept: Metric approximation and bounded distortion
  - Why needed here: The bi-metric framework relies on the proxy metric providing a bounded approximation of the ground-truth metric. Understanding how metrics can approximate each other with bounded distortion is crucial for grasping the theoretical foundations.
  - Quick check question: If d(x, y) ≤ D(x, y) ≤ 2d(x, y) for all x, y, what is the approximation factor C?

- Concept: Doubling dimension and intrinsic dimensionality
  - Why needed here: The doubling dimension determines the space and query complexity of the graph-based algorithms used in the bi-metric framework. Understanding this concept is essential for analyzing the efficiency of the approach.
  - Quick check question: If a point set has doubling dimension λd = 3, how many balls of radius r are needed to cover a ball of radius 2r?

- Concept: Graph-based nearest neighbor search algorithms
  - Why needed here: The bi-metric framework relies on graph-based algorithms like DiskANN and Cover Tree. Understanding how these algorithms work and their properties is crucial for implementing and extending the framework.
  - Quick check question: What is the key property of an α-shortcut reachable graph that makes it useful for nearest neighbor search?

## Architecture Onboarding

- Component map:
  - Proxy metric d: Cheap but less accurate distance function used for graph construction
  - Ground-truth metric D: Expensive but accurate distance function used for query refinement
  - Graph index: Data structure built using proxy metric d
  - Query algorithm: Greedy search using ground-truth metric D on the graph index

- Critical path:
  1. Build graph index using proxy metric d
  2. Receive query q
  3. Perform greedy search using ground-truth metric D
  4. Return approximate nearest neighbor

- Design tradeoffs:
  - Space vs. accuracy: Larger graphs (higher α) provide better accuracy but require more space
  - Number of D evaluations vs. accuracy: More evaluations of D during query improve accuracy but reduce efficiency
  - Choice of proxy metric: A more accurate proxy metric improves overall performance but may be more expensive to compute

- Failure signatures:
  - Poor accuracy: The proxy metric may not provide a good approximation of the ground-truth metric
  - High query time: The graph structure may not be well-suited for the ground-truth metric D
  - Memory issues: The graph may require too much space for the available memory

- First 3 experiments:
  1. Implement a simple bi-metric framework using a small dataset and two metrics with known approximation factors. Measure the accuracy and query time.
  2. Vary the approximation factor C between the proxy and ground-truth metrics. Observe how this affects the accuracy and efficiency of the framework.
  3. Compare the bi-metric framework against traditional re-ranking on a larger dataset. Measure the accuracy-efficiency tradeoff for both approaches.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the bi-metric framework generalize to other approximate nearest neighbor algorithms beyond DiskANN and Cover Tree, such as those based on locality sensitive hashing?
- Basis in paper: [explicit] The paper states "It is an interesting open direction to determine if our bi-metric framework can be theoretically instantiated for other popular nearest neighbor algorithms, such as those based on locality sensitive hashing."
- Why unresolved: The theoretical analysis in the paper only covers DiskANN and Cover Tree. The authors explicitly note that the intuition behind these algorithms (using nets of various scales) may not generalize to fundamentally different algorithms like locality sensitive hashing.
- What evidence would resolve it: A theoretical proof showing that the bi-metric framework can be instantiated for locality sensitive hashing or other popular nearest neighbor algorithms, or a demonstration of why it cannot be applied to such algorithms.

### Open Question 2
- Question: How does the performance of the bi-metric framework vary with different ratios of the quality gap between the cheap and expensive models?
- Basis in paper: [inferred] The paper shows that the improvement of the bi-metric approach is most substantial when there is a large gap between the qualities of the cheap and expensive models, but does not systematically explore different quality ratios.
- Why unresolved: While the paper provides results for three different cheap models with varying quality, it does not perform a comprehensive study of how the performance scales with different quality ratios or model size combinations.
- What evidence would resolve it: A series of experiments varying the quality gap between cheap and expensive models across a wide range, measuring the performance of the bi-metric approach at each ratio.

### Open Question 3
- Question: What is the impact of the choice of initialization for the second-stage search in the bi-metric framework on overall performance?
- Basis in paper: [explicit] The paper includes an ablation study on different search initializations for the second-stage search, showing that using multiple starting points from the first-stage search improves performance compared to using a single point or no first-stage search.
- Why unresolved: While the paper shows that using multiple starting points is beneficial, it does not determine the optimal number of starting points or explore the full range of possible initialization strategies.
- What evidence would resolve it: A comprehensive study of different initialization strategies for the second-stage search, including varying numbers of starting points and methods of selecting those points, to determine the optimal approach for different datasets and quality ratios.

## Limitations

- The theoretical guarantees rely on the proxy metric providing a bounded C-approximation of the ground-truth metric, which may not hold in all practical scenarios
- The empirical results focus primarily on text retrieval tasks with specific embedding models, leaving open questions about cross-domain effectiveness
- The memory usage and scalability characteristics for very large datasets are not thoroughly explored

## Confidence

- **High Confidence**: The core theoretical framework and its application to DiskANN and Cover Tree algorithms
- **Medium Confidence**: The empirical results showing improved accuracy-efficiency tradeoffs on MTEB benchmark
- **Low Confidence**: The generalizability of results to domains beyond text retrieval and embedding models beyond those specifically tested

## Next Checks

1. Test the framework with proxy metrics that have varying approximation factors (C) to quantify the impact on accuracy and efficiency trade-offs
2. Apply the bi-metric framework to non-text domains (e.g., image retrieval, structured data) to evaluate cross-domain effectiveness
3. Conduct ablation studies varying the graph parameters (α, l_build, max_outdegree) to understand their impact on the accuracy-efficiency curve