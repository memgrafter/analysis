---
ver: rpa2
title: 'FeBiM: Efficient and Compact Bayesian Inference Engine Empowered with Ferroelectric
  In-Memory Computing'
arxiv_id: '2410.19356'
source_url: https://arxiv.org/abs/2410.19356
tags:
- bayesian
- inference
- fefet
- size
- font
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FeBiM, the first ferroelectric field-effect
  transistor (FeFET)-based in-memory computing engine for Bayesian inference. The
  core innovation lies in mapping quantized logarithmic probabilities to discrete
  FeFET states within a compact crossbar array, enabling natural in-memory Bayesian
  posterior calculations without additional circuitry.
---

# FeBiM: Efficient and Compact Bayesian Inference Engine Empowered with Ferroelectric In-Memory Computing

## Quick Facts
- arXiv ID: 2410.19356
- Source URL: https://arxiv.org/abs/2410.19356
- Authors: Chao Li; Zhicheng Xu; Bo Wen; Ruibin Mao; Can Li; Thomas Kämpfe; Kai Ni; Xunzhao Yin
- Reference count: 40
- Primary result: First FeFET-based in-memory computing engine for Bayesian inference with 581.40 TOPS/W efficiency and 26.32 Mb/mm² density

## Executive Summary
This paper introduces FeBiM, the first ferroelectric field-effect transistor (FeFET)-based in-memory computing engine for Bayesian inference. The core innovation lies in mapping quantized logarithmic probabilities to discrete FeFET states within a compact crossbar array, enabling natural in-memory Bayesian posterior calculations without additional circuitry. FeBiM leverages the multi-bit characteristics of FeFETs to achieve high storage density (26.32 Mb/mm²) and exceptional computing efficiency (581.40 TOPS/W) in Bayesian classification tasks. Compared to state-of-the-art implementations, it demonstrates 10.7× improvement in storage density and 43.4× improvement in efficiency.

## Method Summary
FeBiM uses a novel mapping scheme that associates quantized logarithmic probabilities with discrete FeFET states within a compact crossbar array, enabling natural in-memory Bayesian posterior calculations without additional circuitry. The method involves converting original probabilities to logarithmic values, truncating very small probabilities for quantization efficiency, and mapping normalized logarithmic probabilities to discrete FeFET states with corresponding IDS values. During inference, the accumulated outputs of the crossbar naturally represent the posterior probabilities, and a winner-take-all (WTA) circuit identifies the event with the highest posterior.

## Key Results
- Achieves 26.32 Mb/mm² storage density, 10.7× improvement over memristor-based Bayesian machines
- Delivers 581.40 TOPS/W computing efficiency, 43.4× better than RNG-based implementations
- Demonstrates robust performance with only ~5% accuracy drop under 45mV FeFET threshold variation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FeBiM eliminates extra computation circuitry by encoding quantized logarithmic probabilities into discrete FeFET states.
- Mechanism: FeFET states directly map to discretized log-probability values. During inference, selected FeFET cells accumulate their currents along each wordline. This accumulated current is a natural representation of the sum of log-likelihoods, which in log-space equals the product of probabilities from Eq. (5).
- Core assumption: The discretization of probabilities into FeFET states preserves the ordering of posteriors; normalization ensures relative differences remain distinguishable.
- Evidence anchors:
  - [abstract] "It maps quantized logarithmic probabilities to discrete FeFET states. As a result, the accumulated outputs of the crossbar naturally represent the posterior probabilities..."
  - [section 3.3] "We initially convert the original probabilities into logarithmic values, then truncate very small probabilities to manage quantization precision efficiently... The normalized logarithmic probabilities P′ are linearly mapped to discrete FeFET states with corresponding IDS values..."
  - [corpus] Weak match. Corpus contains only general IMC papers, no specific FeFET or Bayesian mapping work.
- Break condition: If FeFET state discretization causes overlap in IDS values, or if normalization fails to preserve posterior ordering, the mapping breaks.

### Mechanism 2
- Claim: FeBiM achieves high storage density by using one FeFET per cell as both storage and computation unit.
- Mechanism: Each FeFET cell stores a quantized probability directly. Unlike conventional approaches that need separate probability generation or multi-cycle MAC operations, FeBiM leverages the FeFET's multi-bit capability to hold multiple levels of probability, eliminating the need for additional storage structures or logic.
- Core assumption: FeFETs reliably maintain distinct IDS states across the required voltage ranges, and variation is low enough that state separation remains intact.
- Evidence anchors:
  - [abstract] "FeBiM effectively encodes the trained probabilities of a Bayesian inference model within a compact FeFET-based crossbar... achieving high storage density (26.32 Mb/mm²)..."
  - [section 2.1] "By varying the number of positive write pulses... multiple distinct VT H states can be realized... the FeFET states are thus associated with corresponding IDS values..."
  - [section 3.2] "During write, the WL and ScL associated with the target row are grounded, and a 4V write voltage Vw with corresponding pulse number is applied to the gate of target FeFETs, programming the designated quantized prior or likelihood probabilities."
  - [corpus] Weak. IMC neighbors discuss storage density improvements but not FeFET-specific multi-bit usage for Bayesian inference.
- Break condition: If FeFET programming variations cause state overlap or if IDS currents become indistinguishable, density benefits erode.

### Mechanism 3
- Claim: The WTA sensing circuit identifies the event with the highest posterior in a single inference cycle.
- Mechanism: Each wordline current (representing a posterior) is mirrored to the WTA circuit. The WTA outputs a one-hot current indicating the winner, directly yielding the class prediction without extra decision logic.
- Core assumption: The current differences between posteriors are large enough for WTA to distinguish winners reliably in one cycle.
- Evidence anchors:
  - [section 3.2] "We utilize a concise and scalable winner-take-all (WTA) circuit design... The WTA circuit identifies the WL with the maximum current, corresponding to the event with the highest posterior, and outputs a one-hot current result as the final inference decision..."
  - [section 4.1] "We then validate the functionality of the WTA circuit... The WTA circuit's output, where the winner results are clearly distinguishable from the loser results in less than 300ps."
  - [corpus] No direct WTA Bayesian inference examples in neighbors; only general ADC or sensing discussion.
- Break condition: If FeFET variations cause IDS currents to converge, the WTA may fail to distinguish winners.

## Foundational Learning

- Concept: Bayesian inference posterior computation as cumulative product of priors and likelihoods.
  - Why needed here: FeBiM's mapping and in-memory accumulation directly implement this computation in the logarithmic domain; understanding the math is essential to see why FeFET current summation equals posterior.
  - Quick check question: In log-space, how does a cumulative product of probabilities become a sum? (Answer: log(a·b) = log(a) + log(b).)

- Concept: FeFET multi-level cell (MLC) programming via partial polarization switching.
  - Why needed here: FeBiM relies on multiple IDS states per FeFET; knowing how gate pulses control these states is key to understanding the probability mapping.
  - Quick check question: How many IDS states can be reliably achieved per FeFET for FeBiM's quantization? (Answer: As low as 2-bit, but up to 4-bit shown in examples.)

- Concept: Winner-take-all (WTA) circuit operation and timing.
  - Why needed here: FeBiM's inference decision depends on fast WTA detection of the maximum wordline current; understanding circuit behavior ensures correct interpretation of delay and energy claims.
  - Quick check question: What determines the WTA decision time in FeBiM? (Answer: Stabilization time of wordline currents and WTA settling time, ~300ps in simulations.)

## Architecture Onboarding

- Component map: FeFET crossbar array -> Row driver & write/input buffer -> Current mirrors -> WTA circuit -> Peripheral logic
- Critical path: BL activation -> FeFET current accumulation on WL -> current mirror -> WTA detection -> one-hot output
- Design tradeoffs:
  - Higher probability quantization -> more FeFET states -> better accuracy but larger variation sensitivity
  - More evidence levels -> more columns -> higher crossbar density but increased sensing load
  - WTA resolution -> determines smallest distinguishable posterior difference
- Failure signatures:
  - Posterior misprediction -> likely FeFET state overlap or insufficient WTA resolution
  - Slow inference -> WL current settling issues or WTA settling delays
  - Erratic writes -> gate pulse number miscalibration or disturb in unselected cells
- First 3 experiments:
  1. Verify FeFET IDS states for programmed pulse numbers using DC sweeps; confirm state separation.
  2. Simulate WTA detection with two wordlines at varying current ratios; measure settling time and decision accuracy.
  3. Run Monte Carlo simulations with FeFET VT variation to quantify impact on classification accuracy; compare against baseline.

## Open Questions the Paper Calls Out
None explicitly called out.

## Limitations
- Performance claims based on Monte Carlo simulations using experimentally calibrated Preisach model, which may underestimate real-world device variability
- Quantization of probabilities into discrete FeFET states introduces fundamental accuracy-efficiency tradeoff not fully explored across different problem domains
- WTA circuit's ability to distinguish winners under high variation conditions validated through simulation but not experimentally demonstrated

## Confidence

- **High confidence**: The fundamental mapping of logarithmic probabilities to FeFET states and the resulting storage density and efficiency calculations
- **Medium confidence**: The accuracy preservation claims under device variation
- **Low confidence**: The generalizability of FeBiM to non-Gaussian naive Bayes classifiers and more complex Bayesian networks

## Next Checks

1. Fabricate a small-scale FeBiM prototype (e.g., 8x8 crossbar) and measure actual FeFET state distributions and WTA decision accuracy under realistic process variation
2. Extend validation to multinomial and tree-augmented naive Bayes classifiers to assess FeBiM's applicability to broader Bayesian inference tasks
3. Characterize the crossbar's sensing margin and signal-to-noise ratio across different current levels to determine the practical limits of probability quantization precision