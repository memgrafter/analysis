---
ver: rpa2
title: 'LIME: Less Is More for MLLM Evaluation'
arxiv_id: '2409.06851'
source_url: https://arxiv.org/abs/2409.06851
tags:
- lime
- question
- mllms
- image
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of effectively and efficiently
  evaluating Multimodal Large Language Models (MLLMs) by identifying issues with existing
  benchmarks, such as overly simple samples and computational burdens. The authors
  propose LIME (Less Is More for MLLM Evaluation), a refined benchmark curated through
  a semi-automated pipeline that filters out uninformative samples and eliminates
  answer leakage.
---

# LIME: Less Is More for MLLM Evaluation

## Quick Facts
- **arXiv ID**: 2409.06851
- **Source URL**: https://arxiv.org/abs/2409.06851
- **Reference count**: 40
- **Primary result**: LIME reduces evaluation samples by 76% and time by 77% while more effectively distinguishing MLLM model performance

## Executive Summary
LIME addresses the growing challenge of efficiently and effectively evaluating Multimodal Large Language Models (MLLMs). The paper identifies critical issues with existing benchmarks, including overly simple samples that all models can solve, answer leakage, and computational burdens from large sample sizes. LIME proposes a semi-automated pipeline that filters out uninformative samples and eliminates answer leakage, resulting in a more refined benchmark that better distinguishes model performance while significantly reducing evaluation costs.

## Method Summary
LIME introduces a semi-automated pipeline for creating a refined MLLM evaluation benchmark. The approach filters out uninformative samples by identifying and removing questions that are too simple for most models to handle, and eliminates answer leakage by removing samples where answers can be easily found in the training data. This results in a smaller, more challenging benchmark that better captures the performance differences between MLLMs. The evaluation demonstrates that traditional automatic metrics like CIDEr are insufficient for MLLM captioning tasks, and excluding captioning scores provides a more accurate reflection of overall model performance.

## Key Results
- LIME reduces evaluation samples by 76% and evaluation time by 77% compared to existing benchmarks
- The refined benchmark more effectively distinguishes model performance across 13 tested MLLM models
- Traditional automatic metrics like CIDEr are found to be insufficient for evaluating MLLM captioning performance
- Excluding the captioning task score provides a more accurate reflection of overall model performance

## Why This Works (Mechanism)
The semi-automated pipeline works by systematically identifying and removing samples that don't contribute meaningful discrimination between models. By filtering out overly simple questions that all models can solve and eliminating samples with answer leakage, LIME creates a benchmark that focuses on genuinely challenging and informative samples. This approach addresses the fundamental problem that larger benchmarks aren't necessarily better if they contain many redundant or uninformative samples.

## Foundational Learning
- **MLLM evaluation challenges**: Understanding why existing benchmarks fail to effectively discriminate between models
- **Sample filtering techniques**: Learning how to identify and remove uninformative samples from evaluation datasets
- **Answer leakage detection**: Methods for identifying samples where answers can be found in training data
- **Automatic metric limitations**: Understanding why traditional metrics like CIDEr may not be suitable for MLLM captioning evaluation
- **Benchmark curation trade-offs**: Balancing comprehensiveness with efficiency in benchmark design
- **Performance discrimination**: Techniques for creating benchmarks that effectively distinguish between model capabilities

## Architecture Onboarding

**Component Map**
LIME pipeline: Original benchmark → Sample filtering → Answer leakage removal → Refined benchmark

**Critical Path**
1. Input: Large MLLM benchmark dataset
2. Sample filtering to remove overly simple questions
3. Answer leakage detection and removal
4. Output: Refined LIME benchmark
5. Evaluation using MLLM models
6. Performance analysis and metric evaluation

**Design Tradeoffs**
- Sample size reduction vs. comprehensiveness
- Automated filtering vs. human-in-the-loop bias
- Traditional metrics vs. human judgment alignment
- Task inclusion (captioning vs. exclusion) for overall score accuracy

**Failure Signatures**
- Benchmark becomes too narrow if filtering is too aggressive
- Human bias introduction through semi-automated process
- Overfitting to specific model architectures
- Generalization issues when applied to new domains

**First 3 Experiments to Run**
1. Evaluate LIME's filtering pipeline on a new MLLM benchmark to test generalizability
2. Compare LIME's performance discrimination against traditional benchmarks using 5-10 diverse MLLM models
3. Test alternative automatic metrics for captioning evaluation to validate CIDEr's insufficiency claim

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to only 13 MLLM models may not capture full diversity of the MLLM landscape
- Semi-automated filtering pipeline could introduce bias through human-in-the-loop process
- Exclusion of captioning from overall scores requires further justification for generalizability

## Confidence
- Claim that CIDEr is insufficient for MLLM captioning evaluation: High confidence
- Claim that LIME effectively reduces evaluation time while maintaining discriminative power: High confidence
- Claim that LIME better distinguishes model performance: Medium confidence

## Next Checks
1. Expand evaluation to include at least 20-30 diverse MLLM models spanning different architectures and training approaches
2. Conduct systematic bias analysis of the semi-automated filtering pipeline to quantify and characterize any introduced biases
3. Test LIME's generalizability by applying it to emerging MLLM benchmarks and evaluating its performance in specialized domains like medical imaging or scientific visualization