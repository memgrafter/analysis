---
ver: rpa2
title: 'Beyond English-Centric LLMs: What Language Do Multilingual Language Models
  Think in?'
arxiv_id: '2408.10811'
source_url: https://arxiv.org/abs/2408.10811
tags:
- language
- latent
- layer
- japanese
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the internal latent languages of non-English-centric
  large language models (LLMs) through a case study on Japanese processing models.
  The research introduces a multi-token probability calculation method to analyze
  how intermediate layer representations behave across different language tasks.
---

# Beyond English-Centric LLMs: What Language Do Multilingual Language Models Think in?

## Quick Facts
- arXiv ID: 2408.10811
- Source URL: https://arxiv.org/abs/2408.10811
- Reference count: 28
- Key outcome: Japanese-specific LLMs predominantly use Japanese as their internal latent language, while English-centric models rely exclusively on English, with dual internal languages emerging for underrepresented languages based on linguistic similarity.

## Executive Summary
This study investigates the internal latent languages of non-English-centric large language models by analyzing how intermediate layer representations behave across different language tasks. Using a multi-token probability calculation method extended from single-token analysis, the researchers examine Japanese processing models (Swallow and LLM-jp) alongside English-centric Llama2. The research reveals that Japanese-specific models predominantly use Japanese as their internal latent language when processing Japanese, while English-centric models rely exclusively on English. For underrepresented languages like French and Chinese, both Swallow and LLM-jp exhibit dual internal latent languages, with the model preferentially activating the latent language most closely related to the target language. The study also demonstrates that cultural conflicts between latent internal and target output languages affect intermediate layer reasoning, with responses gradually aligning with target language context across layers.

## Method Summary
The study introduces a multi-token probability calculation method to analyze intermediate layer representations across different language tasks. Using three LLM categories for Japanese processing (Llama2, Swallow, and LLM-jp), the researchers construct a multilingual dataset with 166 parallel phrases in English, French, Japanese, and Chinese. The method extends single-token analysis by calculating sequence probabilities through iterative token-by-token probability calculation at each layer. Logit lens is applied to un-embed intermediate layer representations into vocabulary space, and probabilities are calculated for relevant tokens. The study examines translation, repetition, and cloze tasks to identify internal latent language probabilities and their shifts across layers, revealing how different models process various target languages through their intermediate representations.

## Key Results
- Japanese-specific models (Swallow, LLM-jp) predominantly use Japanese as their internal latent language when processing Japanese, while English-centric Llama2 relies exclusively on English.
- For underrepresented languages (French, Chinese), both Swallow and LLM-jp exhibit dual internal latent languages, with the model preferentially activating the latent language most closely related to the target language.
- The transition from internal latent to target language occurs only in sparse dimensions highly relevant to language identities, allowing semantic and language identity dimensions to be distinguished in the representation space.

## Why This Works (Mechanism)

### Mechanism 1
Models trained on non-English-dominant corpora develop internal latent languages aligned with their dominant training language. This alignment manifests as higher probability distributions in the intermediate layers when projected back to vocabulary space via logit lens.

**Core assumption**: The dominant language in training data directly influences the language identity of intermediate layer representations.

**Evidence anchors**:
- [abstract] "Japanese-specific Swallow and LLM-jp employ both Japanese and English, exhibiting dual internal latent languages... LLM-jp primarily utilizes Japanese as the internal latent language, with minimal reliance on English"
- [section] "Swallow, which underwent CPT in Japanese, demonstrates a noticeable probability of Japanese in its intermediate layers. For LLM-jp... English probabilities are nearly absent in the intermediate layers during monolingual repetition and cloze tasks"

**Break condition**: If models with Japanese-dominant training data still show English-dominant intermediate representations, the mechanism fails.

### Mechanism 2
When processing underrepresented languages, models leverage internal latent languages based on linguistic similarity to the target language, optimizing the transformation pathway through the representation space.

**Core assumption**: Linguistic similarity between training languages and target languages influences which internal latent language is activated during processing.

**Evidence anchors**:
- [abstract] "For any given target language, the model preferentially activates the latent language most closely related to it"
- [section] "LLM-jp tends to exhibit a more extreme favour towards one language... when the target language is Chinese, the probability of Japanese is considerably higher than that of English; when processing French, the probability of English is higher than that of Japanese"

**Break condition**: If models show no preference for linguistically similar latent languages when processing underrepresented languages, the mechanism fails.

### Mechanism 3
Language identity transitions are confined to sparse dimensions in the representation space, allowing semantic content to remain consistent while language-specific transformations occur in isolated dimensions.

**Core assumption**: Representation space can be decomposed into language identity dimensions (sparse, language-specific) and semantic dimensions (dense, shared across languages).

**Evidence anchors**:
- [abstract] "the distribution transition within the latent layers occurs solely in sparse dimensions, while these specific dimensions are highly relevant to language identities"
- [section] "we discovered that the distribution transition within the latent layers occurs solely in sparse dimensions, while these specific dimensions are highly relevant to language identities"

**Break condition**: If language transitions involve dense dimensions or cannot be separated from semantic dimensions, the mechanism fails.

## Foundational Learning

- **Concept: Logit lens and intermediate layer analysis**
  - Why needed here: The study fundamentally relies on analyzing representations at intermediate layers before final embedding, requiring understanding of how hidden states can be projected back to vocabulary space for analysis.
  - Quick check question: Can you explain how logit lens works to project intermediate hidden states to token probabilities, and why this reveals internal language representations?

- **Concept: Language similarity and linguistic relationships**
  - Why needed here: The mechanism of latent language selection based on target language similarity requires understanding how languages relate to each other (e.g., shared characters between Japanese and Chinese, Western language similarities).
  - Quick check question: How would you quantify linguistic similarity between Japanese and Chinese vs. Japanese and French, and why does this matter for internal latent language selection?

- **Concept: Representation space decomposition and sparse vs. dense dimensions**
  - Why needed here: The claim about language identity dimensions being sparse while semantic dimensions are dense requires understanding of how neural representations can be analyzed for dimensional properties.
  - Quick check question: What methods could you use to determine if certain dimensions in a representation space are sparse vs. dense, and how would you test if language identity is confined to sparse dimensions?

## Architecture Onboarding

- **Component map**: Input processing -> Tokenizer converts input to token IDs -> Model layers (40 transformer layers, 5120-dimensional hidden states) -> Logit lens projection (unembedding matrix projects hidden states back to vocabulary space) -> Probability calculation (softmax applied to projected vectors) -> Multi-token sequence probability (product of individual token probabilities) -> Compare probabilities across languages

- **Critical path**: 1. Tokenize input prompt 2. Forward pass through model layers 3. Apply logit lens at target layer(s) 4. Project hidden states to vocabulary space 5. Calculate probabilities for relevant tokens 6. Aggregate probabilities for multi-token sequences 7. Compare probabilities across languages

- **Design tradeoffs**:
  - Single-token vs. multi-token analysis: Single tokens insufficient for languages with complex tokenization (Japanese, Chinese)
  - Layer selection: Need to sample multiple layers to capture transition behavior
  - Language coverage: Must include both dominant and underrepresented languages in analysis
  - Statistical significance: Requires sufficient examples (166 in this study) for reliable probability estimates

- **Failure signatures**:
  - Uniform probability distribution across all languages at all layers
  - No observable transition between latent and target languages across layers
  - Inconsistent behavior across different model architectures
  - High variance in probability estimates indicating insufficient sampling

- **First 3 experiments**:
  1. Reproduce logit lens analysis on intermediate layers for a simple translation task (English to French) to verify basic functionality
  2. Test multi-token probability calculation on Japanese words requiring multiple tokens to ensure proper aggregation
  3. Compare latent language behavior between English-centric and Japanese-specific models on identical Japanese input to validate mechanism 1

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What specific dimensions in the representation space correspond to language identity versus semantic meaning, and can these be reliably distinguished across different model architectures?
- **Basis in paper**: [explicit] The paper states "We discovered that the distribution transition within the latent layers occurs only in sparse dimensions highly relevant to language identities" and mentions "semantic and language identity dimensions can be distinguished in the representation space."
- **Why unresolved**: While the paper observes that sparse dimensions are involved in language identity transitions, it doesn't identify the specific dimensions or provide a method to systematically separate semantic from language identity dimensions across different models.
- **What evidence would resolve it**: Experiments showing consistent dimension patterns across multiple model architectures, along with ablation studies that isolate specific dimensions to confirm their role in language identity versus semantic meaning.

### Open Question 2
- **Question**: How does the strength of cultural bias in intermediate layers vary with the degree of cultural distance between the internal latent language and the target language?
- **Basis in paper**: [explicit] The paper shows that "when the model is asked culturally related questions, the intermediate layers initially produce responses that are biased by the culture of the internal latent language" and mentions investigating "cultural conflicts between latent internal and target output languages."
- **Why unresolved**: The paper provides one example of cultural bias (Japanese school year start date) but doesn't systematically measure how cultural distance affects bias strength or explore whether this bias correlates with linguistic distance between languages.
- **What evidence would resolve it**: Quantitative measurements of cultural bias across multiple cultural pairs with varying degrees of cultural distance, comparing bias strength to both cultural and linguistic similarity metrics.

### Open Question 3
- **Question**: Does the dual internal latent language phenomenon observed in Swallow and LLM-jp extend to other non-English-centric models with different language pairs, and what determines the threshold for dual versus single internal latent language activation?
- **Basis in paper**: [explicit] The paper finds that "For any given target language, the model preferentially activates the latent language most closely related to it" and observes dual internal latent languages in Swallow and LLM-jp for French and Chinese tasks.
- **Why unresolved**: The study is limited to Japanese-specific models and only examines English, French, and Chinese. It doesn't establish whether this phenomenon is generalizable to other language pairs or identify the conditions that trigger dual versus single internal latent language activation.
- **What evidence would resolve it**: Testing multiple non-English-centric models across various language pairs, measuring the probability distributions of internal latent languages, and identifying the linguistic similarity thresholds that determine whether models use one or two internal latent languages.

## Limitations

- The study's conclusions about internal latent languages are based on indirect evidence from probability distributions in intermediate layers, which may not capture the full complexity of multilingual processing.
- The exact language distribution in training data for each model is not specified, making it difficult to definitively attribute observed latent language behavior to training corpus characteristics.
- The claim that language identity transitions occur in sparse dimensions is not empirically validated through dimensional analysis techniques.

## Confidence

**High confidence**: Basic observation that different models show distinct probability patterns in intermediate layers when processing different languages.

**Medium confidence**: Claim that Japanese-specific models predominantly use Japanese as their internal latent language while English-centric models rely on English.

**Low confidence**: Specific mechanism that language transitions occur only in sparse dimensions highly relevant to language identities.

## Next Checks

1. **Cross-model validation with controlled training data**: Train multiple models with identical architectures but varying language distributions in training data (e.g., 70% Japanese/30% English vs. 50/50 vs. 30/70) and measure how internal latent language probabilities shift. This would establish a clearer causal link between training data composition and latent language behavior.

2. **Dimensional sparsity validation**: Apply techniques like singular value decomposition or feature importance analysis to identify which dimensions are most responsible for language-specific probability shifts. Test whether zeroing out high-importance dimensions eliminates language identity while preserving semantic content, and vice versa.

3. **Temporal analysis of language transitions**: Instead of analyzing static intermediate layers, track how language probabilities evolve across the entire generation process for a single example. This would reveal whether the transition from latent to target language is a gradual process or occurs at specific layer thresholds, providing more insight into the mechanism of language identity transformation.