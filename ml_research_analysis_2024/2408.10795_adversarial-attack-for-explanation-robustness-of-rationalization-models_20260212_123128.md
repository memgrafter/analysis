---
ver: rpa2
title: Adversarial Attack for Explanation Robustness of Rationalization Models
arxiv_id: '2408.10795'
source_url: https://arxiv.org/abs/2408.10795
tags:
- attack
- triggers
- rationalization
- rationale
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the explanation robustness of rationalization
  models against adversarial attacks. The authors propose UAT2E, a variant of Universal
  Adversarial Triggers, to attack explanations by inserting crafted triggers into
  inputs while maintaining prediction consistency.
---

# Adversarial Attack for Explanation Robustness of Rationalization Models

## Quick Facts
- arXiv ID: 2408.10795
- Source URL: https://arxiv.org/abs/2408.10795
- Reference count: 40
- Primary result: Existing rationalization models are highly vulnerable to adversarial attacks that shift rationales toward meaningless tokens while preserving predictions.

## Executive Summary
This paper investigates the explanation robustness of rationalization models against adversarial attacks, proposing UAT2E (Universal Adversarial Triggers for Explanation Robustness) to craft triggers that maintain prediction consistency while significantly altering rationale selection. Experiments across five datasets with five rationalization models demonstrate that current models are highly vulnerable to such attacks, with triggers causing rationales to shift toward meaningless tokens or the triggers themselves. The study reveals that using powerful encoders like BERT or supervised training with human-annotated rationales actually increases vulnerability rather than mitigating it. The paper concludes with recommendations for improving explanation robustness through rigorous evaluation, defense mechanisms, and standardized benchmarks.

## Method Summary
UAT2E is a variant of Universal Adversarial Triggers designed to attack rationalization models by inserting crafted trigger sequences into inputs while maintaining prediction consistency. The method employs gradient-based search to optimize triggers, minimizing the difference between new rationale masks and constructed label sequences. For non-target attacks, triggers are optimized to prevent selection of meaningful tokens, while target attacks limit selection to only the triggers themselves. The attack constructs adversarial samples by iteratively replacing tokens in the trigger sequence based on gradient signals, using MSE loss for rationale alignment and cross-entropy for prediction preservation. The approach is evaluated across five rationalization models (RNP, VIB, SPECTRA, FR, DR) on five datasets (Movie, FEVER, MultiRC, Beer, Hotel), measuring both task performance and rationale quality through multiple metrics.

## Key Results
- Rationalization models show significant vulnerability to UAT2E attacks, with triggers causing rationales to shift toward meaningless tokens or the triggers themselves
- Using BERT encoders or supervised training with human-annotated rationales increases vulnerability rather than mitigating it
- Enhancing prediction robustness through adversarial training does not effectively improve explanation robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UAT2E identifies trigger sequences that shift rationale selection toward meaningless tokens or the triggers themselves, while preserving prediction.
- Mechanism: Gradient-based search optimizes triggers by minimizing the difference between the new rationale mask and a constructed label sequence, using MSE for rationales and cross-entropy for predictions.
- Core assumption: The rationalization model's gradient signals reliably indicate how to manipulate rationale selection without affecting prediction.
- Evidence anchors:
  - [abstract] "UAT2E employs the gradient-based search on triggers and then inserts them into the original input to conduct both the non-target and target attack."
  - [section 4.1] "Equation (1) illustrates our intent, but inserting attack triggers leads to a mismatch in the lengths of zadv and zx. To address this, we construct a label sequence m* from the discrete binary mask mx to replace zx and align the length of madv."
- Break condition: If the rationalization model's gradients become unreliable or sparse, or if the label sequence construction fails to align lengths properly.

### Mechanism 2
- Claim: Attack effectiveness is amplified when the rationalization model uses powerful encoders like BERT or is trained with supervised human-annotated rationales.
- Mechanism: BERT and supervised training provide more accurate gradients, helping UAT2E select tokens that induce degeneration or spurious correlations.
- Core assumption: More accurate gradient information enables finer manipulation of rationale selection toward undesired tokens.
- Evidence anchors:
  - [section 5.3] "Using a powerful encoder or supervised training with human-annotated rationales fails to mitigate degeneration and spurious correlations resulting from attacks... Models trained under supervision with human-annotated rationales fail to prevent the impact of attacks."
  - [section 5.5] "Triggers inserted near the end of the document have a greater impact. This observation aligns with previous findings by Chen et al. [4] and can be attributed to the fact that rationale positions in Movie and MultiRC datasets are typically located close to the end of the document."
- Break condition: If the encoder or training method reduces reliance on spurious features, or if the model architecture changes to be more robust to gradient-based manipulation.

### Mechanism 3
- Claim: Enhancing prediction robustness through adversarial training does not effectively improve explanation robustness.
- Mechanism: Prediction-focused defenses do not address the underlying vulnerability in rationale selection, so explanation shifts still occur.
- Core assumption: Prediction and explanation robustness are decoupled; defenses targeting only predictions leave rationale selection exposed.
- Evidence anchors:
  - [section 5.3] "Enhancing prediction robustness does not effectively improve explanation robustness... the experimental results indicate that enhancing prediction robustness through adversarial training does not significantly improve explanation robustness."
  - [section 5.3] "Existing rationalization models are vulnerable to the attacks on explanation including both non-target and target attacks."
- Break condition: If future defenses target both prediction and rationale selection jointly, or if the model architecture inherently couples the two.

## Foundational Learning

- Concept: Adversarial triggers in NLP
  - Why needed here: UAT2E is a variant of Universal Adversarial Triggers, so understanding how triggers work is foundational.
  - Quick check question: How do adversarial triggers typically alter model behavior without changing the prediction?

- Concept: Rationalization model components (generator + predictor)
  - Why needed here: UAT2E manipulates the generator's output (the rationale) while leaving the predictor's output unchanged.
  - Quick check question: What role does the generator play in a rationalization model, and how is its output constrained?

- Concept: Gradient-based optimization in white-box attacks
  - Why needed here: UAT2E uses gradients to find effective trigger tokens; understanding this optimization loop is key.
  - Quick check question: What is the difference between white-box and black-box adversarial attacks, and why does white-box matter here?

## Architecture Onboarding

- Component map: Input text → BERT/GRU encoder → Rationalizer (Gumbel-Softmax mask sampling) → Predictor (task-specific head) → Output prediction and rationale mask. UAT2E inserts triggers into the input, queries gradients from the encoder, and iteratively updates trigger tokens.
- Critical path: Trigger insertion → Gradient computation → Token replacement → Loss minimization → Rationale shift while prediction preserved.
- Design tradeoffs: Using BERT gives better gradients but increases attack surface; using GRU is more robust but less precise; supervised training improves rationale quality but makes models more attackable.
- Failure signatures: Sparsity spikes, Gold Rationale F1 drops, AR increases, F1shift decreases, F1shift,m decreases; triggers often end up selecting meaningless tokens or spurious features.
- First 3 experiments:
  1. Run UAT2E with na=5 tokens, K=5 trigger groups, positions=(0,2,4,6,-1) on a small Movie review subset; measure |△Acc| and △gGR.
  2. Compare UAT2E attack effects using BERT vs GRU encoders on the same dataset; record AR and F1shift,m.
  3. Transfer identified triggers from one model to another (black-box setting) and measure retained attack effectiveness (AR drop).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific defense mechanisms can effectively mitigate the degeneration and spurious correlation issues in rationalization models when under adversarial attack?
- Basis in paper: [explicit] The paper identifies that rationalization models suffer from degeneration and spurious correlation after attacks, and recommends exploring defense mechanisms to protect models from these issues.
- Why unresolved: While the paper suggests exploring defense mechanisms, it does not provide specific methods or strategies to address these vulnerabilities.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of proposed defense mechanisms in reducing degeneration and spurious correlation in rationalization models under various attack scenarios.

### Open Question 2
- Question: How do different types of adversarial attacks (e.g., sentence-level, word-level, character-level) affect the explanation robustness of rationalization models, and which type is most effective?
- Basis in paper: [inferred] The paper focuses on word-level attacks using Universal Adversarial Triggers, but does not compare its effectiveness against other attack types mentioned in related work.
- Why unresolved: The paper does not conduct a comparative analysis of different attack types on explanation robustness.
- What evidence would resolve it: Comparative experimental results showing the impact of various attack types on rationalization models' explanation robustness, including metrics like Gold Rationale F1, Attack Capture Rate, and Rationale Shift F1.

### Open Question 3
- Question: Can the development of new evaluation metrics more accurately reflect rationale shifts and improve the assessment of explanation robustness in rationalization models?
- Basis in paper: [explicit] The paper notes that Gold Rationale F1 (GR) may not accurately reflect rationale shifts and recommends developing more effective evaluation metrics.
- Why unresolved: The paper does not propose or test new evaluation metrics for assessing explanation robustness.
- What evidence would resolve it: Introduction and validation of new metrics through experiments that demonstrate their ability to capture rationale shifts more effectively than existing metrics like GR, F1shift, and F1shift,m.

## Limitations
- The evaluation is limited to five datasets and five rationalization models, which may not capture the full diversity of real-world applications.
- The study does not investigate the transferability of attacks across different model architectures or the potential for defense mechanisms beyond adversarial training.
- The analysis focuses on white-box attacks, leaving the robustness of models against black-box attacks unexplored.

## Confidence
- **High Confidence**: The claim that rationalization models are vulnerable to adversarial attacks on explanations is well-supported by the experimental results across multiple datasets and models.
- **Medium Confidence**: The observation that BERT encoders and supervised training increase vulnerability is supported by the results, but the underlying reasons for this pattern are not fully explained.
- **Low Confidence**: The generalizability of the findings to other rationalization models, datasets, or attack scenarios (e.g., black-box attacks) is uncertain.

## Next Checks
1. Evaluate transferability across model architectures: Transfer the identified UAT2E triggers from one rationalization model to another (e.g., from BERT-based to GRU-based models) and measure the retained attack effectiveness.
2. Test black-box attack scenarios: Implement a black-box version of UAT2E that uses model queries instead of gradients to craft adversarial triggers and compare effectiveness to white-box attacks.
3. Investigate defense mechanisms beyond adversarial training: Explore alternative defense strategies such as input preprocessing, model architecture modifications, or explanation regularization.