---
ver: rpa2
title: 'Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese
  Large Language Models'
arxiv_id: '2403.02715'
source_url: https://arxiv.org/abs/2403.02715
tags:
- ura-llama
- llama-2
- were
- vistral
- gemsura
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a comprehensive evaluation framework for Vietnamese
  Large Language Models (LLMs), addressing the lack of specialized benchmarks for
  Vietnamese language processing. The authors fine-tuned several LLMs (LLaMa-2, Mixtral,
  Gemma) on Vietnamese datasets and evaluated them across 10 tasks using 31 metrics.
---

# Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models

## Quick Facts
- arXiv ID: 2403.02715
- Source URL: https://arxiv.org/abs/2403.02715
- Authors: Sang T. Truong; Duc Q. Nguyen; Toan Nguyen; Dong D. Le; Nhi N. Truong; Tho Quan; Sanmi Koyejo
- Reference count: 40
- Primary result: Comprehensive evaluation framework for Vietnamese LLMs shows larger models exhibit more biases and uncalibrated outputs, while dataset quality is the key performance determinant

## Executive Summary
This paper presents a comprehensive evaluation framework for Vietnamese Large Language Models (LLMs), addressing the lack of specialized benchmarks for Vietnamese language processing. The authors fine-tuned several LLMs (LLaMa-2, Mixtral, Gemma) on Vietnamese datasets and evaluated them across 10 tasks using 31 metrics. Results show that larger models do not always perform better and are more prone to biases, toxicity, and calibration errors. Dataset quality, rather than model size, was found to be the key factor in performance. The study also highlights the importance of careful fine-tuning and introduces two novel Vietnamese reasoning datasets. Overall, the work provides a robust evaluation framework and demonstrates that well-tuned smaller models can outperform larger ones in Vietnamese language tasks.

## Method Summary
The authors fine-tuned LLaMa-2, Mixtral, and Gemma models on Vietnamese datasets using QLoRA/LoRA techniques. The fine-tuning datasets included Vietnamese Wikipedia (1GB), Vietnamese News-Corpus (22GB), and Vietnamese Highschool Essays. The evaluation framework covered 10 tasks across 31 metrics, including Question-Answering, Summarization, Sentiment Analysis, Text Classification, Knowledge, Toxicity Detection, Information Retrieval, Language Modeling, Reasoning, and Translation. The framework was implemented using HuggingFace transformers, accelerate, datasets, and evaluate libraries, with specific hyperparameters detailed in Appendix D.

## Key Results
- Larger models exhibit increased biases, toxicity, and uncalibrated outputs despite higher parameter counts
- Dataset quality, not model size, is the primary determinant of Vietnamese LLM performance
- Well-tuned smaller models can outperform larger models on Vietnamese language tasks
- Two novel Vietnamese reasoning datasets were introduced to address evaluation gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger LLMs exhibit increased biases and uncalibrated outputs, with dataset quality being the key determinant of performance.
- Mechanism: Larger parameter counts increase the model's capacity to memorize and overfit dataset biases, leading to amplified unfairness in outputs. Dataset quality, including diversity, representativeness, and noise levels, directly influences how well the model generalizes across linguistic and cultural contexts.
- Core assumption: Model behavior scales predictably with parameter count and training data characteristics.
- Evidence anchors:
  - [abstract] "larger models can introduce more biases and uncalibrated outputs"
  - [section] "larger models tend to manifest more biases, produce uncalibrated results, and are more susceptible to the influence of input prompts"
- Break condition: If scaling laws break down for low-resource languages, or if bias mitigation techniques (e.g., adversarial debiasing) are applied effectively.

### Mechanism 2
- Claim: Fine-tuning enables effective cross-lingual knowledge transfer, allowing Vietnamese capabilities to emerge from models pretrained on multilingual data.
- Mechanism: Multilingual pretraining builds shared linguistic representations; fine-tuning on Vietnamese-specific data activates and refines these representations for Vietnamese tasks without requiring full monolingual training.
- Core assumption: The pretraining data contains sufficient Vietnamese samples to bootstrap fine-tuning.
- Evidence anchors:
  - [abstract] "larger language models exhibit unseen capabilities compared to smaller counterparts"
  - [section] "we observe that finetuning can help LLMs transfer knowledge across languages"
- Break condition: If the pretraining data lacks sufficient Vietnamese representation, or if the language is too low-resource for meaningful transfer.

### Mechanism 3
- Claim: Dataset quality (not size) is the primary driver of Vietnamese LLM performance, with fine-tuning datasets having greater impact than pretraining data.
- Mechanism: Fine-tuning datasets directly shape task-specific behaviors; higher-quality datasets reduce noise, increase coverage of edge cases, and improve robustness across evaluation scenarios.
- Core assumption: The fine-tuning data is representative of real-world Vietnamese usage and task distributions.
- Evidence anchors:
  - [abstract] "the key factor influencing LLM performance is the quality of the training or fine-tuning datasets"
  - [section] "the abilities of LLMs do not solely depend on model parameters but also on their training or finetuning datasets"
- Break condition: If evaluation datasets are biased or limited in scope, or if the fine-tuning data is contaminated with evaluation samples.

## Foundational Learning

- Concept: Cross-lingual knowledge transfer
  - Why needed here: Vietnamese is low-resource; leveraging multilingual pretraining reduces data requirements.
  - Quick check question: What percentage of pretraining data must be in the target language for effective transfer?

- Concept: Dataset contamination and bias
  - Why needed here: Contamination leads to inflated performance and poor generalization.
  - Quick check question: How can we detect if evaluation datasets overlap with training data?

- Concept: Calibration and reliability metrics
  - Why needed here: Larger models show higher calibration errors; need metrics to assess trustworthiness.
  - Quick check question: What is the relationship between model size and Expected Calibration Error (ECE)?

## Architecture Onboarding

- Component map: Pretraining multilingual base models (LLaMA-2, Mixtral, Gemma) → Fine-tuning on Vietnamese datasets (Wikipedia, News-Corpus, Essays) → Evaluation across 10 tasks × 31 metrics → Iteration cycle
- Critical path: Data → Fine-tuning → Evaluation → Iteration
- Design tradeoffs:
  - Model size vs. bias/prone to uncalibration
  - Dataset size vs. quality (low-resource constraint)
  - Fine-tuning vs. full training (computational efficiency)
- Failure signatures:
  - Overfitting to specific datasets
  - High calibration errors on Vietnamese tasks
  - Unexpected bias amplification in outputs
- First 3 experiments:
  1. Compare performance of base vs. fine-tuned models on Vietnamese Q&A
  2. Measure calibration error across model sizes on sentiment analysis
  3. Evaluate bias amplification when increasing model parameters with fixed dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Vietnamese LLMs perform on tasks requiring cultural context and idiomatic expressions, given the study's observation that current datasets may not capture the complete spectrum of linguistic nuances and cultural contexts inherent in the Vietnamese language?
- Basis in paper: [explicit] The study mentions that "the evaluation, though comprehensive, is limited by the quality and diversity of available Vietnamese datasets" and that "current datasets may not capture the complete spectrum of linguistic nuances and cultural contexts inherent in the Vietnamese language."
- Why unresolved: The evaluation framework used in the study focused on common tasks and did not specifically test for cultural context or idiomatic expressions in Vietnamese. The paper acknowledges this limitation but does not provide data on how models perform in these areas.
- What evidence would resolve it: Additional datasets and evaluation metrics specifically designed to test Vietnamese cultural context and idiomatic expressions, along with performance data from these specialized tests.

### Open Question 2
- Question: What is the optimal model size and training dataset composition for Vietnamese LLMs, considering the study's finding that larger models do not always guarantee better performance and might perform worse than smaller ones if not trained on specific data types?
- Basis in paper: [explicit] The study found that "larger models do not always guarantee better performance" and that "achieving improved performance necessitates model adherence to certain anchor words, particularly those related to gender and race."
- Why unresolved: While the study provides insights into model performance, it does not identify a clear optimal model size or training dataset composition for Vietnamese LLMs. The paper suggests that model size alone is not the determining factor for performance.
- What evidence would resolve it: Systematic experiments varying model size and training dataset composition, with performance comparisons across different Vietnamese language tasks.

### Open Question 3
- Question: How do Vietnamese LLMs compare to other low-resource language models in terms of bias, toxicity, and fairness, given the study's observation that larger models are more prone to biases and toxicity, and that the training or fine-tuning datasets are a causative factor in inducing toxicity?
- Basis in paper: [explicit] The study found that "larger models are challenging to control regarding toxicity in their generated responses" and that "efforts to mitigate toxicity can be initiated by implementing measures to control the composition of those datasets."
- Why unresolved: The study focused specifically on Vietnamese LLMs and did not compare their performance to models for other low-resource languages. The paper does not provide data on how Vietnamese models compare to models for other languages in terms of bias, toxicity, and fairness.
- What evidence would resolve it: Comparative studies of Vietnamese LLMs and other low-resource language models across bias, toxicity, and fairness metrics, with analysis of the contributing factors.

## Limitations

- The evaluation framework relies heavily on synthetic datasets without sufficient validation on real-world Vietnamese reasoning tasks
- The study does not systematically isolate whether bias amplification stems from model architecture or training procedures
- The paper does not address potential dataset contamination between training and evaluation sets

## Confidence

**High Confidence**: The claim that dataset quality, not model size, is the primary driver of Vietnamese LLM performance.

**Medium Confidence**: The observation that larger models exhibit increased biases and uncalibrated outputs.

**Low Confidence**: The assertion that well-tuned smaller models can consistently outperform larger ones in Vietnamese language tasks.

## Next Checks

1. **Bias Isolation Experiment**: Conduct controlled experiments that systematically vary dataset quality while holding model architecture constant, then repeat with constant dataset quality while varying model architecture, to definitively establish which factor drives bias amplification in larger models.

2. **Real-World Reasoning Validation**: Evaluate the Vietnamese reasoning capabilities on authentic Vietnamese mathematical problems and logical reasoning tasks from Vietnamese educational sources, comparing performance against synthetic datasets to assess ecological validity.

3. **Data Contamination Audit**: Perform a comprehensive overlap analysis between all training datasets and evaluation benchmarks using exact match and semantic similarity checks to quantify potential contamination and its impact on reported performance metrics.