---
ver: rpa2
title: 'Heracles: A Hybrid SSM-Transformer Model for High-Resolution Image and Time-Series
  Analysis'
arxiv_id: '2403.18063'
source_url: https://arxiv.org/abs/2403.18063
tags:
- vision
- heracles
- transformer
- image
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Heracles addresses the limitations of transformers and state-space
  models (SSMs) in handling high-resolution images by introducing a hybrid architecture.
  The model combines a Hartley transform-based global SSM for capturing global image
  features, a convolutional network for local feature extraction, and attention mechanisms
  in deeper layers for token interactions.
---

# Heracles: A Hybrid SSM-Transformer Model for High-Resolution Image and Time-Series Analysis

## Quick Facts
- arXiv ID: 2403.18063
- Source URL: https://arxiv.org/abs/2403.18063
- Reference count: 40
- One-line primary result: Heracles achieves state-of-the-art performance on ImageNet with 84.5% top-1 accuracy while maintaining lower computational complexity than transformers

## Executive Summary
Heracles introduces a hybrid architecture that combines state-space models (SSMs) with transformers to address the limitations of both approaches in handling high-resolution images. The model uses a Hartley transform-based global SSM for capturing global image features, a convolutional network for local feature extraction, and attention mechanisms in deeper layers for token interactions. This design enables Heracles to efficiently capture both global and local information while maintaining computational efficiency. The model achieves state-of-the-art performance on ImageNet and demonstrates strong transfer learning capabilities across multiple tasks.

## Method Summary
Heracles employs a staged architecture where spectral layers (SSMs with Hartley transform) process global information in initial stages, followed by attention layers for token interactions in deeper stages. The model processes local and global information in parallel within each stage, using a Hartley kernel-based SSM for global features and a convolutional network for local details. This hybrid approach addresses the computational inefficiency of transformers for long sequences while capturing both coarse global patterns and fine-grained local details.

## Key Results
- Heracles-C-small achieves 84.5% top-1 accuracy on ImageNet, outperforming existing transformers and SSMs
- Larger variants (Heracles-C-Large and Heracles-C-Huge) further improve accuracy to 85.9% and 86.4% respectively
- Excels in transfer learning, task learning (e.g., instance segmentation on MSCOCO), and time-series forecasting
- First SSM to surpass state-of-the-art transformers in performance while maintaining lower computational complexity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Heracles achieves state-of-the-art performance by integrating a Hartley transform-based global SSM for capturing global image features, a convolutional network for local feature extraction, and attention mechanisms in deeper layers for token interactions.
- **Mechanism:** The hybrid architecture combines the strengths of SSMs (efficient global information capture) with CNNs (local detail extraction) and transformers (token-level interactions), addressing the limitations of each individual approach.
- **Core assumption:** Global and local information capture are complementary and can be effectively combined through parallel processing.
- **Evidence anchors:**
  - [abstract]: "Heracles leverages a Hartely kernel-based state space model for global image information, a localized convolutional network for local details, and attention mechanisms in deeper layers for token interactions."
  - [section]: "Heracles uses a simplified kernel that is Hartley Kernel inspired by Diagonal State Spaces (DSS)..."

### Mechanism 2
- **Claim:** The Hartley transform-based global SSM provides efficient global feature capture while maintaining lower computational complexity compared to transformers.
- **Mechanism:** The Hartley transform uses real-valued frequency components instead of complex numbers, simplifying computations while preserving the ability to capture global spectral information.
- **Core assumption:** Real-valued transforms can effectively capture global information without the computational overhead of complex transforms.
- **Evidence anchors:**
  - [abstract]: "Heracles leverages a Hartely kernel-based state space model for global image information"
  - [section]: "Heracles uses a Hartley Neural Operator to learn a mapping between two infinite dimensional spaces"

### Mechanism 3
- **Claim:** The staged architecture with spectral layers in initial stages and attention layers in deeper stages optimizes performance by capturing both local and global information efficiently.
- **Mechanism:** Early spectral layers capture broad image characteristics while deeper attention layers refine token-level interactions, creating a hierarchical feature extraction process.
- **Core assumption:** The staged approach allows for progressive refinement of features, with initial layers capturing coarse information and later layers handling fine-grained details.
- **Evidence anchors:**
  - [section]: "The Heracles-C architecture is characterized by a staged approach, comprising a total of ùõº initial spectral layers/stages followed by ùêø ‚àí ùõº attention layers/stages"
  - [section]: "We found that initial spectral layers (which include cosine transformation+convolutional layers) followed by deeper attention layers perform optimally"

## Foundational Learning

- **Concept: State Space Models (SSMs)**
  - Why needed here: SSMs provide the mathematical foundation for the global information capture component of Heracles.
  - Quick check question: What is the key advantage of SSMs over transformers in terms of computational complexity for long sequences?

- **Concept: Fourier and Hartley Transforms**
  - Why needed here: These transforms form the basis for the global feature extraction component, converting spatial information into frequency domain representations.
  - Quick check question: How does the Hartley transform differ from the Fourier transform in terms of computational complexity and output representation?

- **Concept: Convolutional Neural Networks (CNNs)**
  - Why needed here: CNNs provide the local feature extraction capability, capturing fine-grained spatial details that SSMs cannot efficiently represent.
  - Quick check question: What is the primary limitation of using only CNNs for high-resolution image processing in terms of receptive field size?

## Architecture Onboarding

- **Component map:**
  - Input ‚Üí Patch Embedding ‚Üí Stage 1-4 (spectral layers) ‚Üí Stage 5-7 (attention layers) ‚Üí MLP Head ‚Üí Output
  - Each stage contains: Hartley/Cosine Transform ‚Üí Convolutional Network ‚Üí Normalization ‚Üí MLP
  - Global SSM (Hartley) and Local SSM (Convolution) process in parallel within each stage

- **Critical path:**
  - Feature extraction path: Input ‚Üí Patch Embedding ‚Üí Spectral layers ‚Üí Attention layers ‚Üí Classification
  - The most critical components are the parallel processing of global SSM and local CNN within each stage

- **Design tradeoffs:**
  - Complexity vs. performance: Adding more spectral layers improves global feature capture but increases computational cost
  - Parallel vs. sequential processing: Parallel SSM and CNN processing provides better performance but requires more memory
  - Number of attention layers: More attention layers improve token interactions but increase quadratic complexity

- **Failure signatures:**
  - Training instability: NAN values during training indicate issues with SSM kernel computation
  - Poor generalization: If transfer learning performance is significantly worse than training performance
  - Memory bottlenecks: If GPU memory usage exceeds available capacity during parallel processing

- **First 3 experiments:**
  1. **Baseline comparison:** Train a pure SSM model (like Mamba) and a pure transformer model on ImageNet to establish performance baselines
  2. **Parallel processing validation:** Test the parallel SSM-CNN architecture on a smaller dataset to verify that parallel processing provides benefits over sequential processing
  3. **Stage configuration optimization:** Experiment with different numbers of spectral layers (Œ± values) to find the optimal balance between global and local feature capture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice between Hartley and Cosine transforms affect model performance across different datasets and tasks?
- Basis in paper: [explicit] The authors compare Hartley and Cosine transforms in Table 8, showing Heracles-Cosine outperforms Hartley on ImageNet-1K. However, they do not provide extensive cross-dataset comparisons.
- Why unresolved: The paper only presents a limited comparison on ImageNet-1K, without exploring how these transforms perform on other datasets or tasks like time-series forecasting.
- What evidence would resolve it: Comprehensive ablation studies comparing Hartley and Cosine transforms across multiple datasets (e.g., CIFAR, Flowers, time-series datasets) and tasks would clarify their relative strengths and weaknesses.

### Open Question 2
- Question: What is the impact of varying the number of spectral layers (Œ±) on model performance and computational efficiency?
- Basis in paper: [explicit] Table 9 shows ablation studies with different Œ± values, but the authors do not explore the full range of possibilities or provide insights into the trade-offs between performance and efficiency.
- Why unresolved: The paper only tests a few Œ± values and does not analyze how performance scales with more or fewer spectral layers, or how this affects computational complexity.
- What evidence would resolve it: Extensive ablation studies varying Œ± from 0 to L (total layers) with detailed analysis of accuracy, FLOPs, and parameter counts would reveal optimal configurations for different use cases.

### Open Question 3
- Question: How does the parallel processing of local and global SSMs in Heracles compare to sequential processing in terms of representational efficiency and task performance?
- Basis in paper: [explicit] The authors claim parallel processing in Heracles yields "significant efficiencies" compared to Mamba's sequential approach, but do not provide quantitative comparisons or ablation studies.
- Why unresolved: The paper asserts advantages of parallel processing without rigorous empirical validation or analysis of the underlying mechanisms.
- What evidence would resolve it: Controlled experiments comparing parallel and sequential architectures with identical components, measuring accuracy, FLOPs, and parameter efficiency across multiple tasks, would quantify the benefits of parallel processing.

## Limitations

- Computational complexity during parallel processing of global SSM and local CNN components may create memory bottlenecks
- Generalization to domains beyond natural images and structured time-series data remains unproven
- Staged architecture may create information bottlenecks between spectral and attention layers for extremely complex patterns

## Confidence

**High Confidence** (Evidence Score: 85%): The core claim that Heracles achieves state-of-the-art performance on ImageNet classification is well-supported by the experimental results, with Heracles-C-small achieving 84.5% top-1 accuracy.

**Medium Confidence** (Evidence Score: 70%): The claims about computational efficiency relative to transformers are supported by the architectural design but lack detailed complexity analysis.

**Low Confidence** (Evidence Score: 55%): The generalization claims across diverse tasks (instance segmentation, time-series forecasting) are based on limited experiments.

## Next Checks

1. **Computational Complexity Analysis**: Perform a detailed FLOPs and memory usage comparison between Heracles and pure transformer models of equivalent parameter count across different input resolutions to verify the claimed efficiency advantages.

2. **Robustness Testing**: Evaluate Heracles on out-of-distribution image datasets (e.g., corrupted images, adversarial examples) and on time-series data with different characteristics (non-stationary, irregular sampling) to assess the model's robustness and limitations.

3. **Ablation on Parallel Processing**: Systematically compare the parallel SSM-CNN processing approach against sequential processing and pure SSM or pure CNN baselines on a controlled image classification task to isolate the contribution of the parallel architecture to overall performance.