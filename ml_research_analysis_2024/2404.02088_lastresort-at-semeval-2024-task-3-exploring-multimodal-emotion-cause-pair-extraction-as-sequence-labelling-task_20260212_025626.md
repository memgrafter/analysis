---
ver: rpa2
title: 'LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause Pair
  Extraction as Sequence Labelling Task'
arxiv_id: '2404.02088'
source_url: https://arxiv.org/abs/2404.02088
tags:
- emotion
- cause
- utterance
- task
- emotions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the task of Multimodal Emotion Cause Pair
  Extraction in Conversations (SemEval-2024 Task 3), which requires identifying emotions
  in utterances and their causes using textual, audio, and visual modalities. The
  authors propose three model architectures: a simple utterance labeling baseline,
  a BiLSTM-based sequence labeling model, and a BiLSTM-CRF model to capture dependencies
  between adjacent utterances.'
---

# LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause Pair Extraction as Sequence Labelling Task

## Quick Facts
- arXiv ID: 2404.02088
- Source URL: https://arxiv.org/abs/2404.02088
- Reference count: 14
- Primary result: Weighted F1-score of 0.1836 and macro F1-score of 0.1759, ranking 8th on the official leaderboard

## Executive Summary
This work addresses the SemEval-2024 Task 3 challenge of Multimodal Emotion Cause Pair Extraction in Conversations, which requires identifying emotions in utterances and their causes using text, audio, and visual modalities. The authors propose three model architectures: a simple utterance labeling baseline, a BiLSTM-based sequence labeling model, and a BiLSTM-CRF model to capture dependencies between adjacent utterances. They compare these using different encoders (BERT, DeBERTa, RoBERTa, WavLM, Wav2Vec2, MViTv2) and evaluate performance on the Emotion-Cause-in-Friends dataset. The results indicate that utterance labeling approaches perform as well as sequence labeling for this dataset, and encoders pre-trained on emotion-related tasks tend to perform better.

## Method Summary
The authors develop three architectures for multimodal emotion cause pair extraction: (1) an utterance labeling baseline that independently classifies emotion and cause for each utterance using concatenated text, audio, and video embeddings, (2) a BiLSTM-based sequence labeling model that processes utterances in both directions to capture contextual dependencies, and (3) a BiLSTM-CRF model that adds a conditional random field layer to model transitions between emotion labels. The models use various encoders including BERT, DeBERTa, RoBERTa for text, WavLM and Wav2Vec2 for audio, and MViTv2 for video. Training involves separate phases for emotion classification, candidate cause identification, and emotion-cause pairing, with weighted cross-entropy loss and AdamW optimizer with linear learning rate scheduler.

## Key Results
- The utterance labeling baseline achieved competitive performance with weighted F1 of 0.1836 and macro F1 of 0.1759, ranking 8th on the leaderboard
- EmotionRoBERTa consistently outperformed other encoders, suggesting pre-training on emotion-related tasks improves performance
- Adding BiLSTM and CRF layers did not significantly improve results, possibly due to the short average conversation length (10 utterances)
- The approach demonstrates that simple utterance labeling can match sequence labeling performance for this specific dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained encoders fine-tuned on emotion-related tasks outperform general-purpose encoders in multimodal emotion cause extraction.
- Mechanism: Encoders like EmotionRoBERTa that have been pre-trained on emotion-labeled datasets learn richer emotional representations, which transfer better to emotion cause pair extraction tasks.
- Core assumption: Emotion-specific pre-training captures nuances in emotional expression that general pre-training misses.
- Evidence anchors:
  - [abstract] The results indicate that encoders pre-trained on emotion-related tasks tend to perform better.
  - [section] Better performance of EmotionRoBERTa can be attributed to the fact that the model's weights have already been finetuned towards emotion-related tasks.

### Mechanism 2
- Claim: BiLSTM models add contextual information from surrounding utterances, but in short conversations this advantage diminishes.
- Mechanism: BiLSTM processes sequences in both directions, capturing dependencies between adjacent utterances, which helps in understanding context-dependent emotions.
- Core assumption: In longer conversations, emotions in one utterance depend on multiple surrounding utterances.
- Evidence anchors:
  - [section] While the Baseline I architecture treats the emotion and cause classification independently for each utterance, it is dependent on the surrounding context of the conversation too.
  - [section] Thus, the BiLSTM architecture models the problem as a Sequence Labeling task.

### Mechanism 3
- Claim: CRF layers improve emotion classification by modeling transitions between emotion labels but do not help candidate cause identification.
- Mechanism: CRF layers capture dependencies between adjacent labels in a sequence, enforcing label consistency (e.g., happiness often followed by happiness).
- Core assumption: Emotion labels in conversations have predictable transitions that can be modeled.
- Evidence anchors:
  - [section] Linear-chain CRFs are models generally used to model structured data where one output influences its neighboring outputs.
  - [section] This could be useful for emotion predictions because the emotion of one utterance is generally influenced by the emotions in its previous utterances.

## Foundational Learning

- Concept: Multimodal representation learning
  - Why needed here: The task requires integrating text, audio, and visual modalities to understand emotions and their causes.
  - Quick check question: What are the three modalities used in this task, and why is each important for understanding emotions in conversations?

- Concept: Sequence labeling vs. utterance labeling
  - Why needed here: The paper compares treating the problem as independent utterance classification versus modeling dependencies between utterances.
  - Quick check question: What is the key difference between sequence labeling and utterance labeling approaches, and when might one be preferred over the other?

- Concept: Fine-tuning vs. pre-training
  - Why needed here: The paper uses pre-trained models and experiments with fine-tuning on emotion-related datasets.
  - Quick check question: How does fine-tuning a pre-trained model on an emotion-specific dataset potentially improve performance on emotion-related tasks?

## Architecture Onboarding

- Component map: Text encoder (BERT/DeBERTa/RoBERTa variants) -> Audio encoder (WavLM/Wav2Vec2) -> Video encoder (MViTv2) -> Concatenation -> (BiLSTM) -> (CRF) -> Dense layers for classification -> Distance embedding for pairing
- Critical path: Text/Audio/Video encoding → Concatenation → (BiLSTM) → (CRF) → Classification/Pairing → Loss computation
- Design tradeoffs: Using BiLSTM adds context but increases complexity and may overfit on short conversations; CRF helps emotion transitions but not cause identification; multimodal fusion may introduce noise if modalities are misaligned
- Failure signatures: High variance in results across different encoder combinations; degradation in performance when adding BiLSTM/CRF layers; poor results on short conversations
- First 3 experiments:
  1. Compare baseline utterance labeling with BiLSTM architecture using EmotionRoBERTa + WavLM + MViTv2 encoders.
  2. Add CRF layer to the BiLSTM model and evaluate change in emotion classification performance.
  3. Test different negative sampling ratios (1:5, 1:10, etc.) in the emotion-cause pairing model to assess impact on F1-score.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would joint multimodal embeddings outperform separate modality encodings for this task?
- Basis in paper: [inferred] The authors mention that "it is possible to learn joint embeddings over the 3 modalities, which should provide better representations for each utterance" as future work, suggesting this hasn't been tested.
- Why unresolved: The current work uses separate encoders for text, audio, and video modalities without exploring joint multimodal embeddings.
- What evidence would resolve it: Experiments comparing performance using separate modality encoders versus joint multimodal embeddings on the same dataset.

### Open Question 2
- Question: How would incorporating speaker information improve emotion-cause pair extraction?
- Basis in paper: [explicit] The authors state "it can be experimented to utilize the speaker information for each utterance while creating utterance representations" as future work.
- Why unresolved: The current models do not use any speaker-specific information in their architecture.
- What evidence would resolve it: Performance comparison between models with and without speaker information incorporated into utterance representations.

### Open Question 3
- Question: Would larger conversation datasets show different relative performance between utterance labeling and sequence labeling approaches?
- Basis in paper: [inferred] The authors observed that "utterance labeling systems perform as good as sequence labeling systems for this specific dataset" and attributed this to the small average conversation length (10 utterances), suggesting dataset size might matter.
- Why unresolved: Only tested on the Emotion-Cause-in-Friends dataset with short conversations.
- What evidence would resolve it: Comparative experiments on datasets with longer conversations and more utterances per conversation.

## Limitations

- The relatively low performance scores (weighted F1 of 0.1836) suggest the task remains challenging and the paper doesn't adequately explain why certain architectures fail
- The comparison between utterance labeling and sequence labeling approaches is based on a single dataset with short conversations, limiting generalizability
- The paper doesn't explore the impact of different negative sampling ratios in the emotion-cause pairing model, which could be a significant factor in the low performance

## Confidence

**High Confidence:** The finding that pre-trained encoders fine-tuned on emotion-related tasks perform better than general-purpose encoders is well-supported by direct evidence and consistent with established principles of transfer learning.

**Medium Confidence:** The claim that BiLSTM models add contextual information but provide limited benefit for short conversations has some supporting evidence but lacks systematic investigation.

**Low Confidence:** The assertion that CRF layers improve emotion classification but not cause identification is based on weak evidence and doesn't provide thorough experimental validation.

## Next Checks

1. **Systematic BiLSTM Analysis:** Conduct controlled experiments varying conversation lengths (using datasets with longer conversations) to test whether BiLSTM benefits increase with conversation length, and compare performance against the baseline on conversations of different lengths.

2. **CRF Component Investigation:** Implement an ablation study specifically isolating the CRF layer's effect on emotion classification versus cause identification, using multiple datasets to determine if the observed asymmetry is consistent across different conversation types.

3. **Negative Sampling Impact:** Systematically test different negative sampling ratios (1:1, 1:5, 1:10, 1:20) in the emotion-cause pairing model to determine the optimal ratio for this task and whether sampling strategy significantly impacts the low performance scores.