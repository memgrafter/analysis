---
ver: rpa2
title: 'Speech Boosting: Low-Latency Live Speech Enhancement for TWS Earbuds'
arxiv_id: '2409.18705'
source_url: https://arxiv.org/abs/2409.18705
tags: []
core_contribution: The paper presents an efficient on-device speech enhancement solution
  for TWS earbuds that can enhance conversations in noisy environments while ANC is
  active. The main challenge was achieving high-quality speech enhancement with less
  than 3 ms algorithmic latency and minimal computational complexity for real-time
  on-device usage.
---

# Speech Boosting: Low-Latency Live Speech Enhancement for TWS Earbuds

## Quick Facts
- arXiv ID: 2409.18705
- Source URL: https://arxiv.org/abs/2409.18705
- Reference count: 0
- Primary result: 3 ms latency speech enhancement for TWS earbuds with 291 MCPS complexity and MOS score of 3.71

## Executive Summary
This paper presents an efficient on-device speech enhancement solution for TWS earbuds that can enhance conversations in noisy environments while ANC is active. The main challenge was achieving high-quality speech enhancement with less than 3 ms algorithmic latency and minimal computational complexity for real-time on-device usage. The authors explored various design elements including network architecture, domain, loss functions, pruning methods, and hardware-specific optimization. They found that time-domain architectures outperformed frequency-domain approaches for this low-latency scenario. The proposed two-stage training with Phone-Fortified Perceptual Loss followed by adversarial and perceptual quality losses significantly improved speech intelligibility while minimizing artifacts. The SPDY + OBC pruning method achieved 10× reduction in computational complexity while maintaining high quality. The final optimized model achieved 3 ms latency, 291 MCPS complexity on HiFi4 DSP, and MOS score of 3.71, outperforming baseline models with higher latency and complexity.

## Method Summary
The approach uses a Wave-U-Net architecture with LSTM bottleneck layer processing 16 kHz sampled waveform chunks. The model employs two-stage training: first with Phone-Fortified Perceptual Loss to preserve speech content, then with adversarial and perceptual quality losses to enhance perceptual quality and reduce artifacts. SPDY + OBC pruning achieves ~90% sparsity while maintaining quality, followed by HiFi4 DSP fixed-point optimization using Q12/Q13 formats and SIMD operations. The system processes audio with 3 ms latency (48 timesteps at 16 kHz) and achieves 291 MCPS complexity.

## Key Results
- Achieved 3 ms algorithmic latency suitable for real-time on-device speech enhancement
- MOS score of 3.71 with 291 MCPS complexity on HiFi4 DSP
- 10× reduction in computational complexity using SPDY + OBC pruning while maintaining high quality
- Outperformed baseline models with higher latency and complexity requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Time-domain architectures outperform frequency-domain approaches under low-latency constraints.
- Mechanism: Time-domain models avoid the latency introduced by windowing and overlap-add operations required in frequency-domain methods, enabling direct end-to-end processing with minimal delay.
- Core assumption: Computational efficiency and algorithmic latency can be balanced in time-domain models without sacrificing speech quality.
- Evidence anchors:
  - [section] "We compared the efficiencies of a state-of-the-art frequency-domain network and a time-domain baseline and discovered that the time-domain baseline was more effective when allocated comparable computational resources and algorithmic latency."
  - [abstract] "We compared the efficiencies of a state-of-the-art frequency-domain network and a time-domain baseline and discovered that the time-domain baseline was more effective when allocated comparable computational resources and algorithmic latency."
- Break condition: If computational resources are abundant or latency constraints are relaxed, frequency-domain models may regain advantage due to better spectral modeling capabilities.

### Mechanism 2
- Claim: Two-stage training with Phone-Fortified Perceptual Loss followed by adversarial and perceptual quality losses improves speech intelligibility while minimizing artifacts.
- Mechanism: Initial training with PFPL preserves speech content by leveraging wav2vec2.0 features, while subsequent fine-tuning with adversarial and perceptual losses corrects distributional discrepancies and enhances perceptual quality.
- Core assumption: Speech content preservation in early training stages prevents oversuppression, and perceptual losses effectively address artifacts without degrading intelligibility.
- Evidence anchors:
  - [section] "We observed that the adversarial loss function has two main disadvantages... Thus, as an alternative, we trained the model using the PFPL... However, the use of the PFPL during training sometimes leads to the emergence of background squeak artifacts... To address this, we implemented a second stage of training... that integrated the adversarial, UTMOS, and PESQ losses..."
  - [abstract] "The proposed two-stage training with Phone-Fortified Perceptual Loss followed by adversarial and perceptual quality losses significantly improved speech intelligibility while minimizing artifacts."
- Break condition: If the second-stage losses are too aggressive, they may reintroduce oversuppression or artifacts, negating the benefits of the initial PFPL training.

### Mechanism 3
- Claim: SPDY + OBC pruning achieves 10× reduction in computational complexity while maintaining high quality.
- Mechanism: SPDY optimizes layer-wise sparsity distribution to maximize model performance under complexity constraints, while OBC performs optimal weight pruning for local layer reconstruction.
- Core assumption: Linear dependency of model quality on log-sparsity levels across layers allows effective optimization of sparsity distribution.
- Evidence anchors:
  - [section] "We assessed the performance of the magnitude pruning method against that of the novel Sparsity Profiles via Dynamic programming search (SPDY) + Optimal Brain Compression (OBC) method... We observed that the SPDY + OBC method significantly improved the quality of the pruned models."
  - [abstract] "The SPDY + OBC pruning method achieved 10× reduction in computational complexity while maintaining high quality."
- Break condition: If the assumed linear dependency between sparsity and quality breaks down, or if layer-wise characteristics deviate significantly from the model assumptions, SPDY + OBC may fail to optimize effectively.

## Foundational Learning

- Concept: Time-domain vs. frequency-domain processing
  - Why needed here: Understanding the trade-offs between these domains is crucial for designing low-latency speech enhancement systems.
  - Quick check question: What is the primary latency source in frequency-domain speech enhancement that time-domain methods avoid?

- Concept: Structured state-space models (S4)
  - Why needed here: S4 models are explored as potential replacements for LSTM layers in the Wave-U-Net architecture, requiring understanding of their capabilities and limitations.
  - Quick check question: What is the key advantage of S4 models that makes them promising for speech enhancement, and why did they fail to outperform the baseline in this case?

- Concept: Perceptual loss functions and their impact on speech quality
  - Why needed here: The choice and combination of loss functions significantly affect the perceptual quality and intelligibility of the enhanced speech.
  - Quick check question: How does the Phone-Fortified Perceptual Loss differ from traditional regression losses, and why is it beneficial for preserving speech content?

## Architecture Onboarding

- Component map: Input waveform → Wave-U-Net encoder → LSTM bottleneck → Wave-U-Net decoder → Output waveform
- Critical path: Input waveform → Wave-U-Net encoder → LSTM bottleneck → Wave-U-Net decoder → Output waveform
- Latency: 3 ms (48 timesteps at 16 kHz)

- Design tradeoffs:
  - Time-domain vs. frequency-domain: Latency vs. spectral modeling capability
  - Model complexity vs. on-device feasibility: Balancing quality with computational constraints
  - Training strategy: Single-stage adversarial vs. two-stage PFPL + adversarial/perceptual losses

- Failure signatures:
  - Oversuppression: Loss of speech content, particularly with aggressive adversarial training
  - Artifacts: Background squeak or harmonic noise artifacts, especially with regression-type losses
  - Inefficiency: Inability to meet latency or complexity constraints, rendering the model unsuitable for on-device use

- First 3 experiments:
  1. Compare Wave-U-Net + LSTM baseline with TF-GridNet under identical latency and complexity constraints
  2. Evaluate S4 and SaShiMi as LSTM replacements in the Wave-U-Net architecture
  3. Test single-stage adversarial training vs. two-stage PFPL + adversarial/perceptual training on speech intelligibility and artifact generation

## Open Questions the Paper Calls Out

- Open Question 1: How does the proposed speech enhancement solution perform in scenarios where the noise characteristics significantly deviate from those present in the training data, such as impulse noise or wideband noise?
  - Basis in paper: [inferred] The paper focuses on additive noise and evaluates the model using the VoiceBank-DEMAND dataset, which may not cover all possible noise types.
  - Why unresolved: The paper does not discuss the model's performance on noise types not included in the training data.
  - What evidence would resolve it: Testing the model on a diverse set of noise types not present in the training data and comparing its performance to other speech enhancement solutions.

- Open Question 2: Can the proposed model architecture and training techniques be effectively adapted for other audio processing tasks, such as audio source separation or music enhancement, while maintaining low latency and computational efficiency?
  - Basis in paper: [inferred] The paper discusses the model's effectiveness for speech enhancement but does not explore its potential for other audio processing tasks.
  - Why unresolved: The paper focuses solely on speech enhancement and does not investigate the model's adaptability to other tasks.
  - What evidence would resolve it: Adapting the model for audio source separation or music enhancement tasks and evaluating its performance and efficiency compared to task-specific models.

- Open Question 3: How does the speech enhancement solution impact the overall user experience when integrated with other TWS earbud features, such as active noise cancellation and beamforming, in real-world usage scenarios?
  - Basis in paper: [inferred] The paper mentions the potential for co-operation with other audio processing modules but does not provide empirical evidence of its impact on user experience.
  - Why unresolved: The paper does not include user studies or real-world testing to assess the integrated solution's impact on user experience.
  - What evidence would resolve it: Conducting user studies or field tests to evaluate the integrated solution's performance and user satisfaction in various real-world scenarios.

## Limitations

- Evaluation primarily focused on synthetic data from VoiceBank-DEMAND dataset, which may not fully capture real-world acoustic complexity
- Hardware-specific optimizations (HiFi4 DSP) limit direct applicability to other platforms
- Limited comparison with other methods prevents comprehensive assessment of state-of-the-art performance

## Confidence

- High confidence: Time-domain architectures outperform frequency-domain approaches under low-latency constraints
- Medium confidence: Two-stage training with PFPL + adversarial/perceptual losses significantly improves quality
- Medium confidence: SPDY + OBC pruning achieves 10× complexity reduction while maintaining quality

## Next Checks

1. Cross-dataset validation: Test the proposed model on additional real-world datasets beyond VoiceBank-DEMAND to verify generalization across different acoustic environments and noise conditions.

2. Hardware portability assessment: Implement the model on alternative DSP architectures (e.g., ARM Cortex-M, Tensilica) to evaluate the transferability of the HiFi4 DSP-specific optimizations.

3. Ablation of PFPL components: Conduct detailed ablation studies isolating the contributions of time-domain L1 loss versus wav2vec2.0 feature distance components in the Phone-Fortified Perceptual Loss to better understand their individual impacts on speech quality.