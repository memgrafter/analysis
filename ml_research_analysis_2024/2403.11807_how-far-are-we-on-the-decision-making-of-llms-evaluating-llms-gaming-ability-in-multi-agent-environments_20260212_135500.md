---
ver: rpa2
title: How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability
  in Multi-Agent Environments
arxiv_id: '2403.11807'
source_url: https://arxiv.org/abs/2403.11807
tags:
- game
- games
- average
- player
- players
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces GAMA-\u03B3-Bench, a new framework for evaluating\
  \ LLMs' decision-making in multi-agent environments using classical game theory\
  \ scenarios. The benchmark includes eight games with dynamic scoring schemes to\
  \ assess robustness, generalizability, and reasoning strategies."
---

# How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments

## Quick Facts
- arXiv ID: 2403.11807
- Source URL: https://arxiv.org/abs/2403.11807
- Reference count: 40
- Primary result: Introduces GAMA-γ-Bench framework for evaluating LLMs' decision-making in multi-agent environments using classical game theory scenarios

## Executive Summary
This paper introduces GAMA-γ-Bench, a comprehensive framework for evaluating Large Language Models' decision-making capabilities in multi-agent environments using classical game theory scenarios. The benchmark includes eight games with dynamic scoring schemes to assess robustness, generalizability, and reasoning strategies. Across 13 evaluated LLMs, including GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2, the framework reveals that while GPT-3.5 shows strong robustness, its generalizability is limited but can be improved with Chain-of-Thought prompting.

## Method Summary
The GAMA-γ-Bench framework evaluates LLMs using eight classical game theory scenarios with dynamic parameter adjustments to prevent memorization and test genuine reasoning. Each LLM plays 20 rounds per game with temperature variations (0.0 to 1.0) and optional Chain-of-Thought prompting. The framework normalizes scores to a 0-100 scale for cross-game comparison, enabling comprehensive assessment of robustness, generalizability, and strategic reasoning patterns across different game types including cooperative, betraying, and sequential games.

## Key Results
- GPT-3.5 exhibits strong robustness but limited generalizability in multi-agent decision-making
- Chain-of-Thought prompting improves performance by up to 15.3 points in the Public Goods Game
- Gemini-1.5-Pro achieves the highest overall score of 69.8/100, followed by LLaMA-3.1-70B (65.9) and Mixtral-8x22B (62.4)

## Why This Works (Mechanism)

### Mechanism 1
- Dynamic game parameter adjustment reduces test set leakage risk by varying game parameters to prevent memorization
- Forces genuine reasoning rather than pattern matching by testing across diverse parameter settings
- Break condition: If models show identical performance across all parameter variations

### Mechanism 2
- Multi-agent interactions reveal strategic reasoning through cooperative and betraying scenarios
- Exposes whether models optimize for individual or collective outcomes
- Break condition: If models perform equally well in both cooperative and betraying games

### Mechanism 3
- Chain-of-Thought prompting enhances decision-making by breaking down complex scenarios into manageable steps
- Improves identification of optimal strategies through sequential reasoning
- Break condition: If CoT prompting shows no performance improvement across multiple games

## Foundational Learning

- **Nash Equilibrium**
  - Why needed here: Understanding NE provides the theoretical foundation for evaluating whether LLM decisions align with optimal strategies
  - Quick check question: What is the difference between Pure Strategy Nash Equilibrium and Mixed Strategy Nash Equilibrium?

- **Game Theory Classification**
  - Why needed here: Distinguishing between cooperative, betraying, and sequential games helps interpret LLM behavior patterns
  - Quick check question: How do payoff structures differ between cooperative and betraying games?

- **Prompt Engineering**
  - Why needed here: Effective prompt design is crucial for eliciting desired reasoning patterns from LLMs
  - Quick check question: What are the key components of an effective game instruction prompt?

## Architecture Onboarding

- **Component map:**
  - Game engine -> LLM agent manager -> Scoring system -> Result analyzer

- **Critical path:**
  1. Initialize game parameters and LLM agents
  2. Execute game rounds and collect decisions
  3. Calculate raw scores based on game outcomes
  4. Normalize scores and aggregate results
  5. Analyze patterns and generate insights

- **Design tradeoffs:**
  - Dynamic vs. static game parameters: Flexibility vs. complexity
  - Single vs. multiple LLM agents per model: Statistical robustness vs. resource usage
  - Round limits: Computational efficiency vs. convergence observation

- **Failure signatures:**
  - Consistently random performance across parameter variations
  - Inability to improve over multiple rounds
  - Extreme sensitivity to prompt wording changes

- **First 3 experiments:**
  1. Run "Guess 2/3 of the Average" with default parameters and observe convergence patterns
  2. Test temperature sensitivity by varying temperature from 0.0 to 1.0
  3. Compare performance with and without Chain-of-Thought prompting

## Open Questions the Paper Calls Out

### Open Question 1
- How do different LLMs perform when playing against each other in the same game?
- Basis: The paper evaluates LLMs individually against fixed strategies but does not explore head-to-head gameplay
- Why unresolved: Token limits and lack of new observations from extended rounds
- Evidence needed: Head-to-head gameplay experiments across multiple rounds and games

### Open Question 2
- How does performance change with more than 20 rounds in betraying games?
- Basis: Paper notes token constraints and consistent convergence beyond 20 rounds
- Why unresolved: Practical token limitations and observation that convergence trends remain consistent
- Evidence needed: Extended round experiments (50-100 rounds) to observe new strategic behaviors

### Open Question 3
- How do LLMs perform in sequential games with more complex rules?
- Basis: LLMs show limitations in sequential games like Battle Royale and Pirate Game
- Why unresolved: Paper does not explore more complex sequential games or rule variations
- Evidence needed: Evaluations in more complex sequential games with additional rules or higher player counts

## Limitations

- Framework relies on predefined game theory scenarios that may not capture real-world decision-making complexity
- Focus on English-language models and games limits applicability to other languages and cultural contexts
- Temperature parameter variations may not capture the full spectrum of stochastic behaviors in real-world applications

## Confidence

**High Confidence:** GPT-3.5's strong robustness but limited generalizability is well-supported by systematic evaluation across multiple parameter variations and game types.

**Medium Confidence:** Chain-of-Thought prompting improves decision-making quality with specific score improvements, but extent may vary across game types and model architectures.

**Low Confidence:** Interpretation of behavioral patterns as "cooperative" or "betraying" may oversimplify complex strategic reasoning; scoring system assumes equal importance across all games.

## Next Checks

1. Test the same game scenarios with multilingual models to assess whether strategic reasoning patterns transfer across languages and cultural contexts.

2. Map game theory scenarios to practical decision-making contexts and evaluate whether framework performance correlates with success in applied settings.

3. Extend evaluation beyond 20 rounds to observe whether LLMs demonstrate learning or strategy adaptation over extended interaction periods.