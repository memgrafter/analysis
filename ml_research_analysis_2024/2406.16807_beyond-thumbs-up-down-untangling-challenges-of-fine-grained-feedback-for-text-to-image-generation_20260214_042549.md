---
ver: rpa2
title: 'Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback for
  Text-to-Image Generation'
arxiv_id: '2406.16807'
source_url: https://arxiv.org/abs/2406.16807
tags:
- feedback
- fine-grained
- attributes
- reward
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether fine-grained human feedback improves
  reward models for text-to-image generation compared to traditional coarse-grained
  feedback. The authors propose a two-stage Concept Bottleneck Model (CBM) approach
  that first predicts granular attributes (e.g., image quality, text-image alignment)
  and then aggregates them into an overall score.
---

# Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback for Text-to-Image Generation

## Quick Facts
- arXiv ID: 2406.16807
- Source URL: https://arxiv.org/abs/2406.16807
- Reference count: 21
- Fine-grained human feedback does not consistently improve reward models compared to coarse-grained feedback

## Executive Summary
This paper investigates whether collecting and incorporating fine-grained human feedback (multiple attributes like image quality and text-image alignment) can improve reward models for text-to-image generation compared to traditional thumbs-up/down feedback. Through experiments on real and synthetic data, the authors find that fine-grained feedback does not consistently outperform coarse-grained feedback, with coarse models sometimes performing better. However, in controlled settings where the exact attributes determining preferences are known, fine-grained feedback shows advantages. The work identifies key challenges including determining which attributes to elicit, modeling choices, and evaluation difficulties.

## Method Summary
The authors propose a two-stage Concept Bottleneck Model (CBM) that first predicts fine-grained attributes (image quality and text-image alignment scores) and then aggregates them into an overall score. They collect human preference judgments on 5,000 images from DALL-E and Stable Diffusion, along with simulated fine-grained feedback using PaLI and VQ2 models. The CBM uses CLIP text embeddings and LAOIN Aesthetics image embeddings, with a multi-headed MLP for attribute prediction followed by a linear aggregator. Models are trained on both real human preference data and synthetic data with known attribute relationships, then compared using ROC-AUC metrics and human pairwise comparisons.

## Key Results
- In real human preference data, coarse-grained reward models trained directly on satisfaction scores outperformed fine-grained CBM models
- In controlled synthetic experiments with known attribute relationships, fine-grained models showed advantages over coarse models
- The two-stage CBM structure with sequential training performed similarly to a direct multi-task approach
- Human evaluation showed high uncertainty, with coarse models sometimes preferred over fine-grained models despite lower ROC-AUC scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained feedback can improve reward models when the correct attributes are known and aligned with human preferences.
- Mechanism: When the model has access to the specific attributes that determine human preference judgments, it can learn a more accurate reward function than models trained on coarse feedback alone.
- Core assumption: The attributes elicited from human feedback directly correspond to the factors humans consider when making preference judgments.
- Evidence anchors:
  - [abstract]: "in controlled settings with known attributes, fine grained rewards can indeed be more helpful"
  - [section]: "we do know the attributes that are being considered in the final preference judgment" (Experiment 2 setup)
  - [corpus]: Weak - neighboring papers focus on binary feedback optimization but don't directly address attribute alignment
- Break condition: If the elicited attributes don't match what humans actually consider when making judgments, or if the model architecture cannot effectively capture the relationships between attributes.

### Mechanism 2
- Claim: The two-stage Concept Bottleneck Model (CBM) structure allows for interpretable attribute prediction and aggregation.
- Mechanism: By first predicting individual fine-grained attributes and then aggregating them into an overall score, the model can capture nuanced distinctions in image quality and prompt-alignment.
- Core assumption: A simple linear aggregator over attribute predictions is sufficient to capture how humans combine attribute information into overall preferences.
- Evidence anchors:
  - [abstract]: "we consider a two-stage structure which first predicts each fine-grained attributes and then aggregates the scores"
  - [section]: "Following standard practice in the CBM literature, g is a simple linear aggregator"
  - [corpus]: Weak - neighboring papers don't specifically address CBM architectures for this domain
- Break condition: If the relationship between attributes and overall preference is non-linear or more complex than a simple weighted sum, or if the attribute prediction stage is not accurate.

### Mechanism 3
- Claim: Fine-grained feedback provides more information than coarse feedback, which can be beneficial when the feedback attributes are correctly identified and modeled.
- Mechanism: Additional attribute information allows the model to distinguish between different aspects of quality (e.g., photorealism vs. prompt alignment) that may be conflated in coarse feedback.
- Core assumption: More information is better, provided it is relevant and the model can effectively utilize it.
- Evidence anchors:
  - [abstract]: "fine-grained feedback which captures nuanced distinctions in image quality and prompt-alignment"
  - [section]: "We consider the setting of eliciting fine-grained feedback where human annotators are asked to provide a set of M scores"
  - [corpus]: Weak - neighboring papers focus on binary feedback but don't directly compare information content
- Break condition: If the additional information introduces noise, if the attributes are not relevant to the task, or if the model architecture cannot effectively utilize the additional information.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The paper's work builds on RLHF by exploring how different types of human feedback affect reward model quality for text-to-image generation.
  - Quick check question: What are the key components of an RLHF pipeline and how does feedback quality impact the learned reward model?

- Concept: Concept Bottleneck Models (CBMs)
  - Why needed here: The proposed two-stage model structure is based on CBM principles, predicting intermediate attributes before aggregating to a final score.
  - Quick check question: How does a CBM differ from a standard regression model, and what are the benefits and limitations of this approach?

- Concept: Multimodal Embeddings
  - Why needed here: The model requires embeddings that capture both image and text features to assess text-to-image alignment.
  - Quick check question: What are common approaches for creating multimodal embeddings, and how might the choice of embedding affect model performance?

## Architecture Onboarding

- Component map:
  - Input: Concatenated image and text embeddings
  - Stage 1: Multi-headed MLP predicting individual attribute scores
  - Stage 2: Linear aggregator combining attribute predictions
  - Output: Single scalar reward score
  - Training: Two-stage process (attribute prediction then aggregation)

- Critical path:
  1. Extract CLIP text embeddings and LAOIN Aesthetics image embeddings
  2. Concatenate embeddings and feed through Stage 1 MLP
  3. Apply sigmoid to Stage 1 outputs
  4. Feed sigmoided outputs to Stage 2 linear layer
  5. Produce final reward score

- Design tradeoffs:
  - Fixed vs. trainable embeddings: Fixed embeddings simplify training but may limit performance
  - Linear vs. non-linear aggregation: Linear is simpler and more interpretable but may miss complex relationships
  - Binary vs. continuous attribute prediction: Binary is simpler but may lose nuance

- Failure signatures:
  - Poor performance despite fine-grained feedback: Attribute mismatch or model expressivity issues
  - High uncertainty in human evaluations: Attribute elicitation may not be meaningful or consistent
  - Attribute correlations: Some attributes may be redundant or highly correlated

- First 3 experiments:
  1. Train coarse model vs. fine-grained model on synthetic data with known attribute relationships
  2. Train models on real data with PaLI-simulated attributes and evaluate in-distribution
  3. Conduct human evaluation comparing coarse vs. fine-grained model outputs on generated images

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does fine-grained feedback improve reward model performance compared to coarse feedback?
- Basis in paper: [explicit] The authors state "we do see that combining information about image quality with text-alignment boosts performance" but also find that "extra information in such attributes is not inducing higher match to held-out (in-distribution) preference judgments versus simply using the coarse-grained feedback"
- Why unresolved: The paper shows mixed results - fine-grained feedback helps in controlled settings with known attributes but not consistently in real-world settings with human preference judgments. The conditions determining when fine-grained feedback is beneficial remain unclear.
- What evidence would resolve it: Systematic experiments varying attribute types, model architectures, and task characteristics while measuring performance across different contexts and user groups.

### Open Question 2
- Question: What is the optimal set of attributes to elicit for fine-grained feedback in text-to-image generation?
- Basis in paper: [explicit] The authors identify this as a key challenge: "How can we know what attributes we should elicit?" and note that "in Experiment 2, we demonstrate that reward model performance may suffer if the elicited attributes do not match those that form the target preferences"
- Why unresolved: The paper shows that using incorrect attributes leads to poor performance, but doesn't provide a method for determining which attributes to collect. Different attributes have different annotation costs and cognitive loads.
- What evidence would resolve it: A framework that can identify which attributes are most predictive of human preferences while balancing annotation costs and cognitive load.

### Open Question 3
- Question: Are Concept Bottleneck Models the optimal architecture for incorporating fine-grained feedback into reward models?
- Basis in paper: [explicit] The authors state "perhaps CBMs are not the best model structure for fine-grained feedback" and note that "recent work has raised questions about the ability of CBM-based systems to effectively handle rich, soft-labeled feedback"
- Why unresolved: The paper only explores one modeling approach (CBMs) and finds limitations, but doesn't test alternative architectures that might better leverage fine-grained feedback.
- What evidence would resolve it: Comparative studies of different model architectures (e.g., direct multi-task learning, hierarchical models, attention-based approaches) on the same fine-grained feedback data.

## Limitations
- Fine-grained feedback does not consistently outperform coarse feedback in real-world settings
- The CBM architecture may not be optimal for capturing complex attribute relationships
- Evaluation requires human pairwise comparisons, introducing uncertainty and variance
- Determining which attributes to elicit remains a key challenge with no clear methodology

## Confidence
- High confidence: The finding that fine-grained feedback does not consistently outperform coarse-grained feedback in real-world settings is well-supported by experimental results across multiple datasets and evaluation methods.
- Medium confidence: The identification of key challenges (attribute elicitation, model architecture, evaluation) is well-reasoned but could benefit from more systematic exploration of alternative approaches.
- Medium confidence: The controlled experiment showing fine-grained advantages when attributes are known is compelling but limited to a synthetic setting that may not fully capture real-world complexity.

## Next Checks
1. Test alternative aggregation methods (non-linear, attention-based) in the CBM architecture to determine if linear aggregation is the limiting factor for fine-grained feedback performance.
2. Conduct ablation studies removing specific attributes from the fine-grained model to identify which attributes contribute most to performance differences between coarse and fine-grained approaches.
3. Compare against recent multimodal reward models that don't use CBMs (e.g., direct fine-tuning approaches) to determine if the two-stage structure is necessary or beneficial for this task.