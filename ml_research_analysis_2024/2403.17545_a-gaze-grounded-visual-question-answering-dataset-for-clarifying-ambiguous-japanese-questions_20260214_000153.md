---
ver: rpa2
title: A Gaze-grounded Visual Question Answering Dataset for Clarifying Ambiguous
  Japanese Questions
arxiv_id: '2403.17545'
source_url: https://arxiv.org/abs/2403.17545
tags:
- gaze
- questions
- gazevqa
- image
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of ambiguous questions in visual
  question answering, particularly in Japanese where subject/object terms are often
  omitted. The authors propose a Gaze-grounded VQA dataset (GazeVQA) that includes
  Japanese questions and gaze information to clarify ambiguities.
---

# A Gaze-grounded Visual Question Answering Dataset for Clarifying Ambiguous Japanese Questions

## Quick Facts
- arXiv ID: 2403.17545
- Source URL: https://arxiv.org/abs/2403.17545
- Reference count: 0
- Key outcome: Proposed GazeVQA dataset and ClipCap+Adapter model achieve VQA score of 39.03 compared to baseline's 36.80

## Executive Summary
This paper addresses the problem of ambiguous questions in visual question answering, particularly in Japanese where subject/object terms are often omitted. The authors propose a Gaze-grounded VQA dataset (GazeVQA) that includes Japanese questions and gaze information to clarify ambiguities. They also introduce a method using gaze target estimation results to improve VQA accuracy. Experiments show that using gaze information improves performance in some cases, with the proposed model achieving a VQA score of 39.03 compared to the baseline's 36.80. However, the model still struggles with questions about object shapes, positional relationships, and character comprehension.

## Method Summary
The paper proposes a Gaze-grounded VQA dataset (GazeVQA) that includes Japanese questions and gaze information to clarify ambiguities caused by ellipsis and directives. The dataset is collected using the MS-COCO subset derived object recognition image dataset (Lin et al., 2014) in Gazefollow (Recasens et al., 2015). The authors introduce a ClipCap+Adapter model that utilizes gaze target estimation results to improve VQA accuracy. The model uses adapter layers to integrate regions of interest (RoI) that represent gaze targets into the whole image. The proposed model is pre-trained on a Japanese caption dataset (Yoshikawa et al., 2017) and a Japanese VQA dataset (Shimizu et al., 2018), then fine-tuned on the GazeVQA dataset.

## Key Results
- Proposed GazeVQA dataset addresses ambiguous Japanese questions using gaze information
- ClipCap+Adapter model achieves VQA score of 39.03 compared to baseline's 36.80
- Model struggles with questions about object shapes, positional relationships, and character comprehension

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gaze information clarifies ambiguous Japanese questions by grounding ellipsis and directives to visual referents.
- Mechanism: The proposed GazeVQA dataset captures questions that omit subjects/objects or use directives. By providing gaze source and destination coordinates, the model can resolve ambiguity by identifying the intended visual referent.
- Core assumption: The gaze target estimation model accurately maps gaze information to object bounding boxes in the image.
- Evidence anchors:
  - [abstract] "propose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous questions using gaze information"
  - [section 3.1] "GazeVQA task: Given question q, corresponding image I, and a RoIIs, the task outputs answer y."
  - [corpus] Weak - only 5 related papers found, none directly addressing gaze-grounded VQA.
- Break condition: Gaze target estimation fails to correctly identify the intended object, or the question ambiguity cannot be resolved by visual context alone.

### Mechanism 2
- Claim: Adapter modules improve VQA accuracy by integrating gaze target regions into the image feature representation.
- Mechanism: The proposed ClipCap+Adapter model uses adapter layers to merge the image feature map with the region of interest (RoI) representing the gaze target. This allows the model to focus on relevant visual information when answering ambiguous questions.
- Core assumption: The adapter layers can effectively combine the global image features with the local RoI features without losing important information.
- Evidence anchors:
  - [section 4.2] "We added adapters to a mapping network... Adapters merge image I and RoI Is, and the mapping network outputs an image series that takes into account a gaze target."
  - [section 5.2] "our model with image I as input to the adapter (ClipCap+ Adapter (I)) outperformed the baseline"
  - [corpus] Weak - only 5 related papers found, none directly addressing adapters for gaze-grounded VQA.
- Break condition: The adapter layers fail to properly integrate the RoI features, leading to degraded performance compared to the baseline.

### Mechanism 3
- Claim: Training the adapter layers only on the GazeVQA dataset is more efficient than full fine-tuning of the entire model.
- Mechanism: The proposed model achieves competitive performance by updating only the adapter parameters (16M) instead of fine-tuning the entire model (410M parameters). This allows for faster adaptation to the GazeVQA task with limited data.
- Core assumption: The adapter layers can learn to effectively integrate gaze information without requiring extensive fine-tuning of the pre-trained image encoder and text decoder.
- Evidence anchors:
  - [section 5.2] "our model trained only with adapters is 39.03, which is about four points higher than the baseline trained with the mapping network and the text decoder"
  - [section 5.2] "Our model can generate accurate answers to ambiguous questions with about 16M parameter updates, compared to the baseline, which requires a full tuning both text decoder and a mapping network."
  - [corpus] Weak - only 5 related papers found, none directly addressing parameter-efficient fine-tuning for gaze-grounded VQA.
- Break condition: The adapter layers are insufficient to capture the nuances of the GazeVQA task, requiring full fine-tuning for optimal performance.

## Foundational Learning

- Concept: Japanese ellipsis and directives in visual questions
  - Why needed here: The GazeVQA dataset specifically targets ambiguous Japanese questions that omit subjects/objects or use directives like "it" or "that".
  - Quick check question: Can you identify examples of ellipsis and directives in the Japanese questions provided in the paper?

- Concept: Gaze target estimation from head images
  - Why needed here: The proposed model relies on a gaze target estimation model to map gaze source information to object bounding boxes in the image.
  - Quick check question: How does the gaze target estimation model predict the gaze destination from a head image?

- Concept: Vision-and-language model architectures
  - Why needed here: The proposed ClipCap+Adapter model builds upon existing vision-and-language architectures like CLIP and GPT-2 to integrate gaze information into the VQA pipeline.
  - Quick check question: What are the key components of a typical vision-and-language model, and how does the ClipCap+Adapter model differ from the baseline?

## Architecture Onboarding

- Component map:
  CLIP image encoder -> Mapping network with adapter layers -> Text decoder

- Critical path:
  1. Input image and question
  2. CLIP image encoder processes image and RoI
  3. Mapping network with adapter layers merges image and RoI features
  4. Text decoder generates answer conditioned on question and image features

- Design tradeoffs:
  - Using adapters instead of full fine-tuning: Faster adaptation but may miss some task-specific nuances
  - Providing RoI vs. full image to adapter: RoI focuses on gaze target but may miss context, full image preserves context but may introduce noise
  - Pre-training on other VQA datasets: Helps with general VQA skills but may not capture GazeVQA-specific patterns

- Failure signatures:
  - Low VQA score: Model fails to resolve ambiguity or generate accurate answers
  - High VQA score but low BERT score: Model generates fluent but factually incorrect answers
  - Adapter weights don't change during training: Adapter layers are not learning to integrate gaze information effectively

- First 3 experiments:
  1. Train baseline ClipCap model on GazeVQA dataset and evaluate performance
  2. Train ClipCap+Adapter model with RoI input to adapter and evaluate performance
  3. Train ClipCap+Adapter model with full image input to adapter and evaluate performance, compare with RoI input

## Open Questions the Paper Calls Out
- How can we design vision-and-language models that can effectively handle the ambiguity caused by Japanese ellipsis and directives in visual question answering?
- How can we effectively integrate gaze information from multiple modalities (e.g., gaze, pointing, dialog context) to improve the accuracy of visual question answering systems in real-world scenarios with dynamic visual features and uncertainty?
- How can we develop more efficient training strategies for vision-and-language models in Japanese, given the limited availability of Japanese vision-and-language data?

## Limitations
- Model struggles with questions about object shapes, positional relationships, and character comprehension
- Gaze target estimation model is not fully specified, impacting reproducibility
- Approach primarily focuses on Japanese questions, generalizability to other languages unclear

## Confidence
- High confidence in the effectiveness of using gaze information to clarify ambiguous Japanese questions
- Medium confidence in the proposed ClipCap+Adapter model, specific implementation details unclear
- Low confidence in the generalizability of the approach to other languages and question types

## Next Checks
1. Conduct a thorough ablation study to evaluate the individual contributions of the gaze information, adapter layers, and pre-training on the overall performance of the proposed model.
2. Test the proposed approach on a diverse set of languages and question types to assess its generalizability beyond Japanese ellipsis and directives.
3. Investigate the impact of different gaze target estimation models on the performance of the ClipCap+Adapter model, and explore methods to improve the accuracy of gaze target prediction.