---
ver: rpa2
title: What should I wear to a party in a Greek taverna? Evaluation for Conversational
  Agents in the Fashion Domain
arxiv_id: '2408.08907'
source_url: https://arxiv.org/abs/2408.08907
tags:
- customer
- evaluation
- assistant
- fashion
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a novel evaluation dataset and framework for
  assessing large language model (LLM)-powered conversational agents in the fashion
  domain. The dataset consists of 4k multilingual conversations between customers
  and assistant agents in English, German, French, and Greek.
---

# What should I wear to a party in a Greek taverna? Evaluation for Conversational Agents in the Fashion Domain

## Quick Facts
- arXiv ID: 2408.08907
- Source URL: https://arxiv.org/abs/2408.08907
- Authors: Antonis Maronikolakis; Ana Peleteiro Ramallo; Weiwei Cheng; Thomas Kober
- Reference count: 40
- Primary result: Novel evaluation dataset and framework for LLM-powered conversational agents in fashion domain

## Executive Summary
This paper introduces a comprehensive evaluation framework for assessing LLM-powered conversational agents in online fashion retail. The authors create a 4k conversation dataset spanning four languages (English, German, French, Greek) through a simulation environment using GPT models, with manual verification to ensure quality. The evaluation methodology involves replaying conversations to assess assistant agents' ability to translate customer fashion needs into search-engine-compatible queries. Through benchmarking multiple models including GPT-4, GPT-3.5, Llama2, and Mistral, the study demonstrates GPT-4's superior performance while highlighting the importance of prompt engineering and the need for comprehensive evaluation frameworks in the fashion domain.

## Method Summary
The evaluation framework uses a simulation environment where customer and assistant agents engage in conversations based on structured fashion attributes (color, type, material, fit, brand, size, apparel) and open-ended themes. GPT models generate these conversations, which undergo manual verification for quality. The evaluation replays these conversations with different assistant models, comparing generated queries against ground-truth item descriptions using BERTScore for English/German/French and semantic similarity via text-embedding-ada-002 for Greek. Theme-based queries are evaluated using Precision@3 based on semantic similarity to ground-truth themes. This controlled replay approach isolates model performance from environmental variability.

## Key Results
- GPT-4 demonstrates the best overall performance across all languages and evaluation metrics
- GPT-3.5 with carefully engineered prompt III shows competitive performance, particularly in cost-effectiveness
- Open-source models (Llama2, Mistral) lag significantly behind closed-source models in query generation quality
- Performance varies substantially across fashion attributes, with some attributes proving more challenging for models to handle accurately

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simulation-based conversation generation produces high-quality, diverse data at scale while maintaining business-relevant coverage
- Mechanism: GPT models generate customer and assistant interactions based on structured attribute combinations plus open-ended themes, with manual verification filtering low-quality conversations
- Core assumption: LLM-generated data, when properly prompted and manually filtered, retains enough realism to serve as a valid evaluation proxy for real user interactions
- Evidence anchors: Authors explicitly state need for data orders of magnitude larger than manual generation allows; no corpus papers directly confirm LLM simulation quality for fashion evaluation
- Break condition: If manual verification reveals systemic quality issues that cannot be mitigated by prompt engineering

### Mechanism 2
- Claim: BERTScore and semantic similarity provide reproducible, language-agnostic evaluation of query generation quality
- Mechanism: For item-based tasks, BERTScore compares generated queries to ground-truth item descriptions; for theme-based tasks, cosine similarity between query and theme embeddings assigns closest theme with Precision@3
- Core assumption: Semantic similarity in embedding space correlates with practical usefulness of generated search queries
- Evidence anchors: Authors specify BERTScore for item descriptions and semantic similarity for themes; no validation that high scores correlate with actual retrieval performance
- Break condition: If embedding models fail to capture nuanced fashion terminology differences

### Mechanism 3
- Claim: Controlled conversation replay enables fair, reproducible benchmarking across languages and attributes
- Mechanism: Each model acts as assistant in same customer conversations, generating queries evaluated against ground-truth descriptions using specified metrics
- Core assumption: Simulation environment adequately represents real customer-assistant dynamics without introducing unrealistic artifacts
- Evidence anchors: Authors propose replay methodology to assess assistant capabilities; no exploration of whether replay introduces systematic biases
- Break condition: If replay introduces systematic biases that skew evaluation results

## Foundational Learning

- Concept: Fashion domain attribute modeling (color, type, material, fit, brand, size, apparel)
  - Why needed here: Precise mapping between customer descriptions and backend search queries is essential for correct query generation
  - Quick check question: Can you list the six fashion attributes used and give an example value for each?

- Concept: Natural language generation evaluation metrics (BERTScore, semantic similarity, Precision@K)
  - Why needed here: These metrics quantitatively compare model performance; engineers must know their strengths, limitations, and implementation details
  - Quick check question: What is the key difference between how BERTScore and cosine similarity are used in this evaluation framework?

- Concept: Prompt engineering for controlled LLM behavior
  - Why needed here: Different prompts yield significantly different performance; understanding how to craft prompts that elicit desired assistant behavior is critical for reproducibility
  - Quick check question: What is the main behavioral difference between GPT-3.5 prompts (I) and (III) as described in the paper?

## Architecture Onboarding

- Component map: Data Generation Pipeline -> Manual Verification -> Evaluation Engine -> Model Interface Layer -> Benchmarking Dashboard
- Critical path: Data generation → Manual verification → Model evaluation → Metric computation → Result aggregation
- Design tradeoffs:
  - Open-source vs closed-source models: Open-source require more engineering effort for reliable output formatting but are cheaper at scale; closed-source are more reliable but costlier
  - Manual verification vs full automation: Manual verification ensures high data quality but limits dataset size; full automation risks lower data fidelity
  - Metric choice: BERTScore captures token-level similarity well for item descriptions but cannot handle free-form theme descriptions, necessitating semantic similarity
- Failure signatures:
  - High variance in query generation quality across different fashion attributes suggests attribute-specific weaknesses in model understanding
  - Systematic drop in performance for certain languages indicates potential multilingual model bias or translation issues
  - Large gaps between model cost and performance suggest inefficient scaling or suboptimal prompt engineering
- First 3 experiments:
  1. Run evaluation on small subset of English conversations with all models to verify replay pipeline works and metrics compute correctly
  2. Test attribute-specific query generation by isolating conversations containing only one attribute to diagnose model weaknesses
  3. Perform cost-performance analysis on medium-sized dataset (e.g., 100 conversations) to determine most cost-effective model configuration before full-scale evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-powered conversational agents vary across different fashion attributes in online fashion retail?
- Basis in paper: Authors performed qualitative analysis of GPT-4 performance across fashion attributes, calculating precision of exact matches between output queries and input item descriptions for each attribute
- Why unresolved: Analysis is limited to single model (GPT-4) without exploring reasons behind varying performance levels
- What evidence would resolve it: Comprehensive analysis of multiple models' performance across fashion attributes, exploring factors influencing performance (attribute complexity, data availability, model architecture), experiments varying attribute combinations and ablation studies

### Open Question 2
- Question: How does cost-effectiveness of different LLM models for conversational agents in online fashion retail compare?
- Basis in paper: Authors conducted cost analysis for evaluating different models using dataset, comparing performance metrics with associated costs
- Why unresolved: Analysis limited to specific task without exploring trade-offs across different tasks or evaluation metrics
- What evidence would resolve it: Thorough evaluation of multiple models across different tasks and evaluation metrics considering both performance and computational expenses, including comparison of different pricing models

### Open Question 3
- Question: How do capabilities of LLM-powered conversational agents in fashion domain generalize to other domains like healthcare or travel?
- Basis in paper: Authors acknowledge potential of LLM-powered conversational agents in other domains but do not explore generalizability beyond fashion
- Why unresolved: Evaluation specific to fashion domain; unclear how well findings translate to other domains where terminology, user behavior, and task complexity differ
- What evidence would resolve it: Comparative study of LLM-powered conversational agents across multiple domains evaluating performance and effectiveness in each domain, considering domain-specific factors and transferability of evaluation methodologies

## Limitations

- Data quality uncertainty due to unspecified verification criteria, inter-rater agreement rates, or proportion of conversations discarded
- Metric validity concerns as no validation that semantic similarity scores correlate with successful product retrieval in real search engine
- Language coverage gaps with potential underrepresentation of non-English languages and unexplained metric choice split

## Confidence

**High confidence**: Simulation-based data generation methodology and conversation replay evaluation framework are clearly specified and reproducible; performance ranking of models is well-supported by presented results
**Medium confidence**: Attribute-based evaluation reveals model-specific weaknesses but does not explore whether these stem from understanding gaps or prompt formulation issues; claim that manual verification ensures "high-quality data" lacks quantitative support
**Low confidence**: Assumption that semantic similarity in embedding space directly translates to practical query effectiveness for product retrieval without retrieval experiments or human evaluation

## Next Checks

1. **Retrieval correlation validation**: Run generated queries through actual product search engine and measure retrieval precision/recall; compare with semantic similarity scores to validate whether evaluation metrics predict real-world performance

2. **Human evaluation study**: Conduct small-scale human judgment study where evaluators rate relevance of generated queries to customer intents; provide ground-truth validation of whether semantic similarity metrics capture true query quality

3. **Cross-lingual model comparison**: Test whether performance differences between languages persist when using same evaluation metric across all languages; determine if metric choice or underlying data quality drives observed performance gaps