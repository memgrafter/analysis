---
ver: rpa2
title: 'Greed is All You Need: An Evaluation of Tokenizer Inference Methods'
arxiv_id: '2403.01289'
source_url: https://arxiv.org/abs/2403.01289
tags:
- longest
- inference
- tokens
- computational
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive evaluation of seven tokenizer
  inference methods across four subword tokenization algorithms (BPE, WordPiece, UnigramLM,
  and SaGe) and three vocabulary sizes (32K, 40K, 49K). The authors developed a novel
  intrinsic benchmark combining morphological alignment, cognitive plausibility, and
  information-theoretic measures for English.
---

# Greed is All You Need: An Evaluation of Tokenizer Inference Methods

## Quick Facts
- arXiv ID: 2403.01289
- Source URL: https://arxiv.org/abs/2403.01289
- Reference count: 15
- Key outcome: Greedy inference methods (longest prefix, longest token) consistently outperform default inference strategies across morphological and cognitive metrics, with SaGe tokenizer achieving state-of-the-art morphological alignment (F1 > 0.96)

## Executive Summary
This paper presents a comprehensive evaluation of seven tokenizer inference methods across four subword tokenization algorithms (BPE, WordPiece, UnigramLM, and SaGe) and three vocabulary sizes (32K, 40K, 49K). The authors developed a novel intrinsic benchmark combining morphological alignment, cognitive plausibility, and information-theoretic measures for English. They found that greedy inference methods consistently outperform the default inference strategies prescribed by each algorithm across most metrics. Specifically, SaGe with greedy inference achieved state-of-the-art morphological alignment performance, while methods minimizing token count showed strong cognitive plausibility. The study demonstrates that inference method choice significantly impacts tokenizer performance independent of vocabulary construction.

## Method Summary
The authors trained four tokenizer vocabularies (BPE, WordPiece, UnigramLM, SaGe) at sizes 32K, 40K, and 49K using the MiniPile dataset. They implemented seven inference methods for each tokenizer and evaluated them on morphological alignment using gold-standard segmentations, cognitive plausibility using benchmark data, Rényi efficiency with tokenization-scorer, tokens per word, and decoding difference rate. The evaluation combined multiple benchmark datasets including LADEC, MorphoLex, MorphyNet, and cognitive response time data to create a comprehensive assessment framework.

## Key Results
- Greedy inference methods (longest prefix, longest token) consistently outperform default inference strategies across morphological and cognitive metrics
- SaGe with greedy inference achieves state-of-the-art morphological alignment performance (F1 > 0.96)
- Methods minimizing token count show strong cognitive plausibility, aligning with human processing preferences
- Compression-optimized tokenizers (BPE, WordPiece) perform well on cognitive metrics despite not being designed for this purpose

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Greedy inference methods consistently outperform default inference strategies across morphological and cognitive metrics.
- Mechanism: Greedy methods make locally optimal token selection decisions at each segmentation step, prioritizing either longest available tokens or minimizing token count. This approach naturally aligns with morphological boundaries and human cognitive preferences for compact representations.
- Core assumption: The optimal global segmentation can be approximated by making greedy local decisions that maximize either token length or minimize token count.
- Evidence anchors: [abstract] "greedy inference methods (particularly longest prefix and longest token) consistently outperform the default inference strategies"; [section] "we find that greedy inference methods work surprisingly well for all four vocabularies across morphological and information-theoretic metrics"

### Mechanism 2
- Claim: SaGe with greedy inference achieves state-of-the-art morphological alignment performance.
- Mechanism: SaGe's vocabulary construction process considers contextual information when selecting tokens, leading to a vocabulary that better represents morphological units. When combined with greedy inference, this results in highly accurate morphological segmentation.
- Core assumption: Contextual information during vocabulary construction leads to better morphological representation than algorithms that ignore context.
- Evidence anchors: [abstract] "SaGe, a recently-introduced contextually-informed tokenizer, outperforms all others on morphological alignment"; [section] "SaGe is most aligned to morphology by a substantial margin, indicating that its contextualized objective succeeds in retaining meaningful tokens"

### Mechanism 3
- Claim: Tokenizers optimized for compression perform well on cognitive plausibility metrics.
- Mechanism: Compression-optimized tokenizers tend to create more efficient representations that align with human cognitive preferences for concise information processing, even though this was not their primary design goal.
- Core assumption: There is a correlation between compression efficiency and cognitive plausibility that was not explicitly targeted by the algorithms.
- Evidence anchors: [section] "BPE and WordPiece, optimized for compression, unsurprisingly perform well above the likelihood-based vocabularies on the information measures. However, we note that this carries over to the cognitive benchmark as well"; [abstract] "methods minimizing token count showed strong cognitive plausibility"

## Foundational Learning

- Concept: Subword tokenization algorithms (BPE, WordPiece, UnigramLM, SaGe)
  - Why needed here: Understanding the differences between these algorithms is crucial for interpreting why different inference methods work better with different vocabularies
  - Quick check question: What is the fundamental difference between BPE's merge-based approach and UnigramLM's likelihood-based approach?

- Concept: Morphological alignment metrics
  - Why needed here: The paper uses morphological alignment as a key evaluation metric, so understanding how it's measured is essential
  - Quick check question: How does the Creutz and Linden (2004) metric measure the alignment between tokenizer output and gold-standard morphological segmentation?

- Concept: Cognitive plausibility measures
  - Why needed here: The paper correlates tokenizer performance with human cognitive processing, requiring understanding of these measures
  - Quick check question: What is the relationship between token count and human response time in lexical decision tasks?

## Architecture Onboarding

- Component map:
  - Tokenizer inference methods (longest prefix, longest suffix, longest token, least tokens, merge rules, likelihood)
  - Vocabulary construction algorithms (BPE, WordPiece, UnigramLM, SaGe)
  - Evaluation metrics (morphological alignment, cognitive plausibility, Rényi efficiency, tokens per word)
  - Benchmark datasets (LADEC, MorphoLex, MorphyNet, DagoBert, UniMorph, UnBlend, CompoundPiece, cognitive data)

- Critical path:
  1. Select vocabulary construction algorithm and size
  2. Choose inference method based on task requirements
  3. Evaluate using appropriate metrics from the benchmark
  4. Iterate based on performance across different evaluation dimensions

- Design tradeoffs:
  - Greedy methods are fast but may not find globally optimal segmentations
  - Merge rules-based methods preserve original vocabulary structure but may be slower
  - Likelihood-based methods can find better segmentations but require token likelihood scores
  - Different metrics may favor different inference methods, requiring careful selection based on use case

- Failure signatures:
  - Poor morphological alignment despite high compression efficiency
  - High cognitive plausibility but poor information-theoretic performance
  - Inconsistent performance across different vocabulary sizes
  - Significant differences between default and alternative inference methods

- First 3 experiments:
  1. Compare default inference methods across all four algorithms on morphological alignment using the LADEC dataset
  2. Test greedy inference methods (longest prefix, longest token) on cognitive plausibility using the word/nonword × accuracy/response time setup
  3. Evaluate Rényi efficiency and tokens per word for all inference methods across different vocabulary sizes using the tokenization-scorer package

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do greedy inference methods perform across languages with different morphological complexity compared to English?
- Basis in paper: [explicit] The paper acknowledges that "Our evaluation is limited to intrinsic measures" and "our evaluation is limited to English, a language with relatively low morphological complexity."
- Why unresolved: The study only evaluated on English data, which has relatively simple morphology compared to languages with rich inflectional or agglutinative systems.
- What evidence would resolve it: Systematic testing of greedy inference methods on morphologically rich languages (e.g., Turkish, Finnish, Arabic) using the same benchmark methodology.

### Open Question 2
- Question: Do the correlations between intrinsic metrics and downstream task performance hold across different domains and model architectures?
- Basis in paper: [explicit] The authors state "our evaluation is limited to intrinsic measures" and acknowledge that "the body of work correlating success on these measures with performance of downstream models on end-tasks is incomplete."
- Why unresolved: The paper establishes correlations between intrinsic metrics but does not validate these against actual downstream performance in diverse settings.
- What evidence would resolve it: Empirical studies showing how changes in tokenization strategy based on intrinsic metrics affect model performance on specific tasks (machine translation, question answering, etc.) across different architectures.

### Open Question 3
- Question: How do inference methods interact with tokenization strategies like spaced marking or morphological segmentation in terms of performance?
- Basis in paper: [inferred] The authors mention "Others analyze the effects of vocabularies" and reference work on "finding the optimal vocabulary size" and "building multilingual vocabularies," suggesting potential interactions not explored.
- Why unresolved: The study focuses on inference methods in isolation without considering how they might interact with other tokenization design choices.
- What evidence would resolve it: Comparative studies evaluating combinations of inference methods with different tokenization strategies (e.g., spaced marking, morphological segmentation) across multiple languages and tasks.

## Limitations

- The study is limited to English, which has relatively low morphological complexity compared to other languages
- Evaluation uses a curated subset of the MiniPile corpus (14K sentences), potentially missing broader usage patterns
- Findings at vocabulary sizes 30-50K may not directly transfer to extreme vocabulary sizes used in modern LLMs
- Greedy inference methods may struggle with truly ambiguous segmentations requiring global optimization

## Confidence

**High Confidence (4-5):** The finding that greedy inference methods consistently outperform default inference strategies across morphological and cognitive metrics has strong empirical support from the comprehensive benchmark. The state-of-the-art performance of SaGe with greedy inference on morphological alignment (F1 > 0.96) is well-supported by the data.

**Medium Confidence (2-3):** The correlation between compression-optimized tokenizers and cognitive plausibility is observed but the underlying mechanism is not fully established. The paper notes this carries over from information measures to cognitive benchmarks, but the relationship is correlational rather than causal.

**Low Confidence (0-1):** Claims about the general superiority of greedy methods across all languages and domains are speculative, as the study is limited to English and specific benchmark datasets. The paper acknowledges this limitation but presents the findings as more universally applicable than the evidence supports.

## Next Checks

1. **Cross-linguistic validation**: Test the greedy inference methods across typologically diverse languages (e.g., morphologically rich languages like Turkish or Finnish, and analytical languages like Mandarin) to verify if the English-specific findings generalize.

2. **Large vocabulary evaluation**: Evaluate the inference methods at vocabulary sizes used in modern LLMs (100K-200K tokens) to determine if the performance trends observed at 30-50K scale maintain at extreme sizes.

3. **Robustness to ambiguity**: Design experiments specifically targeting ambiguous segmentations where multiple valid morphological analyses exist to test whether greedy methods fail in cases requiring global optimization rather than local greedy decisions.