---
ver: rpa2
title: 'Dwell in the Beginning: How Language Models Embed Long Documents for Dense
  Retrieval'
arxiv_id: '2404.04163'
source_url: https://arxiv.org/abs/2404.04163
tags:
- retrieval
- pre-training
- language
- training
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies positional biases in Transformer-based models
  for dense retrieval. While prior work showed information loss in the middle of input
  sequences for causal language models, this study extends the investigation to representation
  learning.
---

# Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval

## Quick Facts
- arXiv ID: 2404.04163
- Source URL: https://arxiv.org/abs/2404.04163
- Reference count: 15
- Models exhibit positional bias where earlier document content dominates embeddings, with performance dropping when relevant information is moved away from document beginning

## Executive Summary
This paper investigates positional biases in Transformer-based models for dense retrieval, extending prior work on information loss to representation learning. The authors examine a T5-based encoder-decoder model across three training stages: language model pre-training, contrastive pre-training, and contrastive fine-tuning. Using the MS-MARCO document collection, they discover that contrastive pre-training already induces a "dwell in the beginning" effect where embeddings better capture early document content, with fine-tuning amplifying this bias. The findings show that retrieval performance drops significantly when relevant passages are moved from the beginning to middle or end positions of documents.

## Method Summary
The study employs a three-stage training pipeline using T5-base architecture with Rotary Position Embeddings (RoPE). First, language model pre-training uses span corruption with 15% masking and 3-token average span length on 8 billion MS-MARCO tokens. Second, unsupervised contrastive pre-training applies cropping (10-50% random spans) with in-batch and cross-device negatives. Third, supervised contrastive fine-tuning uses ANCE-MaxP negatives refreshed every 2 epochs. The model uses tied encoder-decoder weights with T5 decoder as pooler to generate single-token document representations. Position-aware evaluation tasks measure retrieval performance when relevant passages are moved to different document positions, and substring matching quantifies position-dependent embedding quality.

## Key Results
- Contrastive pre-training on MS-MARCO documents creates positional bias favoring early content in embeddings
- Retrieval performance drops 20-30% when relevant passages are moved from beginning to middle/end positions
- Fine-tuning amplifies the positional bias effect rather than mitigating it
- The "dwell in the beginning" effect differs from "lost in the middle" phenomena seen in causal language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Positional biases emerge during contrastive pre-training and are amplified during fine-tuning.
- Mechanism: The contrastive pre-training task, which involves cropping spans from MS-MARCO documents, implicitly trains the model to associate embeddings more strongly with earlier content because MS-MARCO documents tend to place relevant information earlier in the text. This creates a "dwell in the beginning" effect where the model's embeddings capture the beginning of documents more effectively than later sections.
- Core assumption: MS-MARCO documents follow an inverted pyramid writing style, placing key information at the beginning.
- Evidence anchors:
  - [abstract]: "Experiments with the MS-MARCO document collection reveal that after contrastive pre-training the model already generates embeddings that better capture early contents of the input, with fine-tuning further aggravating this effect."
  - [section]: "We evaluate the model on exactly matching substrings from different locations... the similarity values tend to decrease when the position of the substring moves from the beginning"
  - [corpus]: Weak. No direct evidence about MS-MARCO document structure from corpus.
- Break condition: If MS-MARCO documents were uniformly structured with relevant information distributed throughout, the bias would not emerge from contrastive pre-training alone.

### Mechanism 2
- Claim: Language model pre-training with span corruption does not introduce positional biases.
- Mechanism: The span corruption task involves predicting masked tokens across different positions in the input, and uniform performance across these positions suggests the task does not create positional preferences. The task focuses on local context around masked tokens rather than capturing document-level structure.
- Core assumption: Span corruption requires understanding context at multiple positions equally, preventing bias formation.
- Evidence anchors:
  - [section]: "Figure 4 shows uniform performance, suggesting no inherent bias in this task using RoPE."
  - [section]: "Figure 5 shows the evaluation of the T5-2K model after language model pre-training... showing a similar pattern to Figure 4, without a noticeable dwell in the beginning effect."
  - [corpus]: Weak. No corpus evidence supporting or contradicting this mechanism.
- Break condition: If the span corruption task were modified to focus more on specific document positions, it could introduce positional biases.

### Mechanism 3
- Claim: The "dwell in the beginning" effect differs from "lost in the middle" phenomena observed in causal language models.
- Mechanism: Unlike causal language models that lose information specifically in middle positions (performance drops in middle, rises at end), this embedding model shows consistent performance degradation as information moves away from the beginning. This suggests a different bias mechanism where the model prioritizes early content throughout rather than just avoiding middle positions.
- Core assumption: The embedding model's attention mechanism and pooling strategy create a global preference for early content rather than position-specific gaps.
- Evidence anchors:
  - [abstract]: "This differs from the lost in the middle (Liu et al., 2023) phenomena, where performance would drop significantly only in middle sections, rising in the end."
  - [section]: "We see that when the relevant passage is moved to the beginning of the document, the performance increases when compared to the default setting. Conversely, if the passage is moved anywhere else, the performance drops."
  - [corpus]: Weak. No corpus evidence supporting this distinction.
- Break condition: If the model exhibited the classic "lost in the middle" pattern (drop in middle, rise at end), this mechanism would not apply.

## Foundational Learning

- Concept: Positional embeddings and their role in sequence modeling
  - Why needed here: Understanding how positional embeddings (especially RoPE) influence the model's ability to capture information at different positions is crucial for interpreting the "dwell in the beginning" effect.
  - Quick check question: How do rotary positional embeddings (RoPE) differ from absolute positional embeddings, and why might they influence positional biases differently?

- Concept: Contrastive learning and negative sampling strategies
  - Why needed here: The model uses contrastive loss with in-batch and cross-device negatives. Understanding how these negative samples influence embedding space structure is key to explaining the positional bias emergence.
  - Quick check question: How does the choice between in-batch negatives and cross-device negatives affect the stability and quality of learned embeddings?

- Concept: Dense retrieval and embedding-based search
  - Why needed here: The ultimate goal is document retrieval using embeddings. Understanding how embedding quality at different positions affects retrieval performance is essential for evaluating the practical impact of positional biases.
  - Quick check question: Why is a single embedding used to represent an entire document in dense retrieval, and what are the implications for handling long documents?

## Architecture Onboarding

- Component map: Document (up to 2048 tokens) -> Encoder (T5-base with RoPE) -> [Position-dependent representation] -> Pooler (T5 decoder) -> Embedding (single token) -> Retrieval

- Critical path: Document → Encoder → [Position-dependent representation] → Pooler → Embedding → Retrieval
  - The position-dependent representation step is where the "dwell in the beginning" effect manifests

- Design tradeoffs:
  - Tied vs. untied encoders: Using tied encoders simplifies the architecture but may limit the model's ability to handle different query/document length distributions
  - RoPE vs. absolute positional embeddings: RoPE allows extrapolation to longer sequences but may introduce different bias patterns
  - Single embedding vs. multiple embeddings per document: Single embedding is more efficient but struggles with long documents

- Failure signatures:
  - Retrieval performance drops significantly when relevant information is moved away from document beginning
  - Cosine similarity between document embedding and substrings decreases as substrings move from beginning to end
  - Uniform performance across span corruption task positions (indicating no bias in language modeling)

- First 3 experiments:
  1. Reproduce the position-aware task from Section 4.2: Move relevant passages to different positions in documents and measure retrieval performance drop
  2. Implement the substring matching experiment from Section 4.3: Sample substrings from different positions and measure cosine similarity with full document embedding
  3. Compare performance of decoder pooling vs. average pooling strategies on the same position-aware tasks to understand pooling mechanism impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the positional biases observed in T5-2K and RepLLaMA persist in other language model architectures and pre-training datasets beyond MS-MARCO?
- Basis in paper: [inferred] The paper shows positional biases in T5-2K and RepLLaMA models trained on MS-MARCO, but does not test other architectures or datasets.
- Why unresolved: The study is limited to one model architecture and one dataset, leaving open whether the biases are universal or specific to these conditions.
- What evidence would resolve it: Testing multiple architectures (e.g., BERT, RoBERTa) and pre-training datasets (e.g., Wikipedia, BookCorpus) for positional biases using similar position-aware evaluation tasks.

### Open Question 2
- Question: How do different positional embedding methods (e.g., Rotary Position Embeddings vs. Learned Positional Embeddings) affect the emergence and severity of positional biases in long-context models?
- Basis in paper: [explicit] The paper uses Dynamic NTK-RoPE positional embeddings and shows biases emerge during contrastive pre-training.
- Why unresolved: The study only uses RoPE and does not compare with alternative positional encoding methods.
- What evidence would resolve it: Training identical models with different positional embedding schemes and comparing their positional bias patterns through position-aware evaluation tasks.

### Open Question 3
- Question: Can specific pre-training tasks or data augmentation strategies effectively mitigate positional biases in long-context retrieval models without sacrificing retrieval performance?
- Basis in paper: [inferred] The paper identifies that contrastive pre-training induces biases, suggesting that alternative training approaches might address this issue.
- Why unresolved: The study focuses on identifying biases but does not explore potential solutions or mitigation strategies.
- What evidence would resolve it: Developing and testing targeted pre-training tasks or data augmentation techniques designed to balance positional representation, then evaluating both bias reduction and retrieval performance.

### Open Question 4
- Question: How do positional biases in dense retrieval models affect end-to-end retrieval-augmented generation (RAG) systems, particularly when relevant information is located in middle or end sections of documents?
- Basis in paper: [explicit] The paper mentions RAG as a downstream application where whole-document embedding quality is essential, and shows biases affect retrieval when relevant information is not at the beginning.
- Why unresolved: The study evaluates retrieval performance but does not examine the downstream impact on RAG systems.
- What evidence would resolve it: Implementing RAG systems using biased vs. debiased retrieval models and measuring generation quality when relevant information appears in different document positions.

## Limitations

- Study is confined to MS-MARCO document collection with specific structural characteristics that may not generalize to other corpora
- Does not investigate whether positional bias extends beyond the first 2048 tokens or behaves at sequence boundaries
- Mechanism by which contrastive pre-training specifically creates bias is underspecified beyond correlation with document structure

## Confidence

- High Confidence: The empirical observation of positional bias and its measurement through position-aware tasks
- Medium Confidence: The claim that contrastive pre-training is the primary source of positional bias
- Low Confidence: The distinction between "dwell in the beginning" and "lost in the middle" as fundamentally different phenomena

## Next Checks

1. **Cross-Corpus Validation**: Test the same three-stage training pipeline on a document collection with different structural properties (e.g., Wikipedia or news articles with different inverted pyramid tendencies) to determine whether the positional bias is inherent to the training methodology or specific to MS-MARCO's document structure.

2. **Alternative Pooling Strategies**: Implement and compare decoder pooling versus mean pooling or other pooling mechanisms on the position-aware tasks to isolate whether the pooling strategy amplifies or mitigates the positional bias, and whether this affects the "dwell in the beginning" effect.

3. **Negative Sampling Ablation**: Conduct experiments varying the negative sampling strategy (pure in-batch vs. cross-device vs. hybrid) to determine the extent to which negative sample quality and distribution influence the emergence and severity of positional biases during contrastive pre-training.