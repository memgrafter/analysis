---
ver: rpa2
title: Towards training digitally-tied analog blocks via hybrid gradient computation
arxiv_id: '2409.03306'
source_url: https://arxiv.org/abs/2409.03306
tags:
- block
- learning
- algorithm
- feedforward
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new framework for training hybrid digital-analog
  neural networks using a combination of backpropagation and equilibrium propagation.
  The key innovation is Feedforward-tied Energy-based Models (ff-EBMs), which allow
  end-to-end gradient computation through both feedforward and energy-based blocks.
---

# Towards training digitally-tied analog blocks via hybrid gradient computation

## Quick Facts
- arXiv ID: 2409.03306
- Source URL: https://arxiv.org/abs/2409.03306
- Reference count: 40
- Primary result: Introduces ff-EBMs that achieve 46% top-1 accuracy on ImageNet32, establishing new SOTA for equilibrium propagation methods

## Executive Summary
This paper proposes a novel framework for training hybrid digital-analog neural networks by combining backpropagation and equilibrium propagation. The key innovation is Feedforward-tied Energy-based Models (ff-EBMs), which allow end-to-end gradient computation through both feedforward and energy-based blocks. The authors derive a new algorithm that chains error signals from backpropagation through feedforward layers and equilibrium propagation through energy-based layers. They demonstrate that ff-EBMs can be split into multiple energy-based blocks without loss of performance and achieve state-of-the-art results on ImageNet32 among equilibrium propagation methods, significantly outperforming previous approaches.

## Method Summary
The ff-EBM framework treats digital parts as feedforward blocks and analog parts as energy-based blocks, enabling end-to-end gradient computation through a hybrid of backpropagation and equilibrium propagation. The approach defines a bilevel optimization problem where equilibrium states of energy-based blocks depend on feedforward block outputs. A BP-EP gradient chaining algorithm computes gradients by backpropagating through feedforward layers and eq-propagating through energy-based layers. The framework allows standard Deep Hopfield Networks to be arbitrarily split into multiple energy-based blocks while maintaining performance. Deep Hopfield Networks serve as the energy-based blocks, trained with a hybrid algorithm combining backpropagation through feedforward layers and equilibrium propagation through energy-based layers.

## Key Results
- Establishes new state-of-the-art performance in equilibrium propagation literature with 46% top-1 accuracy on ImageNet32
- Demonstrates that a standard Deep Hopfield Network can be arbitrarily split into multiple uniform energy-based blocks without performance loss
- Shows ff-EBMs can be trained end-to-end using a chained backprop-eqprop approach

## Why This Works (Mechanism)

### Mechanism 1
The ff-EBM framework enables end-to-end gradient computation through a hybrid of backpropagation and equilibrium propagation by treating digital parts as feedforward blocks and analog parts as energy-based blocks. The framework explicitly chains error signals from backpropagation through feedforward layers and equilibrium propagation through energy-based layers. This is achieved by defining a bilevel optimization problem where the equilibrium states of energy-based blocks depend on feedforward block outputs, and deriving gradients using a combination of backpropagated and eq-propagated error signals.

### Mechanism 2
The framework allows splitting a standard Deep Hopfield Network (DHN) into multiple energy-based blocks without loss of performance, enabling more flexible and realistic architectures. The ff-EBM framework treats the standard DHN as a single energy-based block and allows it to be arbitrarily split into multiple smaller energy-based blocks while maintaining the same total number of layers and connections. This is achieved by ensuring that the energy functions of the split blocks are defined such that the overall equilibrium state remains unchanged.

### Mechanism 3
The ff-EBM framework achieves state-of-the-art results on ImageNet32 among equilibrium propagation methods by leveraging the combination of digital and analog computational primitives. The framework uses Deep Hopfield Networks (DHNs) as energy-based blocks and chains them with feedforward blocks using the BP-EP gradient chaining algorithm. This allows the model to benefit from the energy efficiency and parallelism of analog computation for the energy-based parts while leveraging the flexibility and precision of digital computation for the feedforward parts.

## Foundational Learning

- Concept: Energy-based models (EBMs)
  - Why needed here: EBMs are the foundation of the analog computational primitives used in the ff-EBM framework. Understanding how EBMs work, how they are trained using equilibrium propagation, and how they can be integrated with feedforward models is crucial for understanding the overall approach.
  - Quick check question: What is the key difference between an energy-based model and a standard feedforward model, and how does this difference affect the training procedure?

- Concept: Equilibrium propagation (EP)
  - Why needed here: EP is the algorithm used to train the energy-based blocks in the ff-EBM framework. Understanding how EP works, how it computes gradients through two relaxations to equilibrium, and how it can be chained with backpropagation is crucial for understanding the overall approach.
  - Quick check question: How does equilibrium propagation differ from backpropagation, and what are the key advantages and disadvantages of using EP compared to BP?

- Concept: Bilevel optimization
  - Why needed here: The optimization problem of training an ff-EBM can be naturally cast as a bilevel optimization problem, where the inner subproblems involve finding the equilibrium states of the energy-based blocks, and the outer problem involves optimizing the parameters of both the feedforward and energy-based blocks. Understanding how bilevel optimization works and how it relates to the ff-EBM framework is crucial for understanding the overall approach.
  - Quick check question: What is the key difference between a standard optimization problem and a bilevel optimization problem, and how does this difference affect the solution procedure?

## Architecture Onboarding

- Component map:
  Input layer -> Feedforward blocks -> Energy-based blocks -> Output layer

- Critical path:
  Forward pass: Input -> Feedforward blocks -> Energy-based blocks -> Output
  Backward pass: Output -> BP-EP gradient chaining algorithm -> Energy-based blocks -> Feedforward blocks -> Input

- Design tradeoffs:
  - Number and size of energy-based blocks vs. overall model performance and efficiency
  - Choice of energy function and activation function for energy-based blocks
  - Implementation of digital and analog computational primitives (e.g., hardware vs. simulation)
  - Choice of optimization algorithm and hyperparameters

- Failure signatures:
  - Poor convergence or instability during training
  - Significant performance degradation compared to standard feedforward models
  - High computational cost or memory usage
  - Difficulty in implementing the digital and analog computational primitives

- First 3 experiments:
  1. Implement a simple ff-EBM with one feedforward block and one energy-based block on a small dataset (e.g., MNIST) and compare its performance to a standard feedforward model.
  2. Experiment with different numbers and sizes of energy-based blocks in an ff-EBM and analyze the impact on model performance and efficiency.
  3. Implement the BP-EP gradient chaining algorithm and compare its convergence and stability to standard backpropagation on a small dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do device non-idealities (e.g., noise, nonlinearities) in analog circuits affect the convergence and accuracy of ff-EBM training algorithms?
- Basis in paper: The paper mentions that analog circuits exhibit non-ideal physical behaviors affecting both inference and parameter optimization, and discusses the need for future work incorporating more hardware realism into ff-EBMs.
- Why unresolved: The paper focuses on theoretical foundations and basic demonstrations without incorporating realistic analog hardware imperfections that would be present in actual implementations.
- What evidence would resolve it: Experimental results comparing ff-EBM performance on idealized vs. realistically modeled analog hardware with device non-idealities, including convergence analysis and accuracy degradation metrics.

### Open Question 2
- Question: Can ff-EBMs be effectively scaled to larger models (e.g., standard ImageNet resolution, deeper architectures) while maintaining computational efficiency advantages?
- Basis in paper: The paper acknowledges that considerable work is needed to prove ff-EBM scalability on more difficult tasks, deeper architectures, and beyond vision tasks.
- Why unresolved: Current experiments are limited to relatively small-scale problems (ImageNet32, CIFAR datasets) with moderate depth (up to 15 layers).
- What evidence would resolve it: Training results on full ImageNet resolution with significantly deeper ff-EBM architectures, demonstrating maintained performance and efficiency gains compared to digital-only approaches.

### Open Question 3
- Question: How can attention mechanisms be effectively integrated into ff-EBMs to create transformer-like architectures?
- Basis in paper: The paper identifies designing ff-EBM-based transformers with attention layers chained with energy-based fully connected layers as an exciting research direction.
- Why unresolved: The paper does not provide any implementation or experimental results for attention-based ff-EBM architectures.
- What evidence would resolve it: A working implementation of a transformer architecture using ff-EBM principles, with training results on sequence modeling or vision transformer tasks, demonstrating the effectiveness of the hybrid approach.

## Limitations
- The practical efficiency of computing equilibrium states in energy-based blocks remains a key challenge that needs to be addressed for real-world applications
- The paper lacks extensive ablation studies on how different energy function choices affect performance
- The comparison to other hybrid approaches is somewhat limited in scope and doesn't include recent digital-only state-of-the-art methods

## Confidence

- **High confidence** in the theoretical framework and mathematical derivation of the BP-EP gradient chaining algorithm (Section 3)
- **Medium confidence** in the ImageNet32 results, as they establish new SOTA for EP methods but lack comparison to other state-of-the-art digital-only approaches
- **Medium confidence** in the claim that splitting energy-based blocks doesn't hurt performance, as the paper shows this works but doesn't deeply explore why or provide extensive ablation
- **Low confidence** in the practical implementation details for real analog hardware, as the paper focuses on simulation

## Next Checks

1. **Gradient computation verification**: Implement gradient checking on a small ff-EBM to verify that the BP-EP algorithm computes correct gradients through the hybrid architecture, comparing against finite differences.

2. **Scalability test**: Evaluate ff-EBMs on a larger dataset (e.g., CIFAR-10 or CIFAR-100) to verify that the approach scales beyond ImageNet32 and to identify any hidden computational bottlenecks.

3. **Energy-based block sensitivity**: Systematically vary the number and size of energy-based blocks in an ff-EBM to quantify the exact performance tradeoff curve and identify optimal architectures for different computational budgets.