---
ver: rpa2
title: 'Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large
  Language Models'
arxiv_id: '2406.18740'
source_url: https://arxiv.org/abs/2406.18740
tags:
- passages
- llms
- re-ranking
- threshold
- relevance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates using pre-filtering to improve LLM-based
  re-ranking in information retrieval. It proposes using expert knowledge (qrels)
  to establish a relevance threshold that allows an LLM to filter out irrelevant passages
  before re-ranking.
---

# Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models

## Quick Facts
- arXiv ID: 2406.18740
- Source URL: https://arxiv.org/abs/2406.18740
- Reference count: 5
- The paper proposes using pre-filtering with expert knowledge (qrels) to improve LLM-based re-ranking by filtering out irrelevant passages before re-ranking.

## Executive Summary
This paper investigates the use of pre-filtering to improve LLM-based re-ranking in information retrieval. The authors propose using human-generated relevance scores (qrels) to establish a threshold that allows an LLM to filter out irrelevant passages before re-ranking. Experiments with Mixtral-8x7B-Instruct on TREC and BEIR datasets show that pre-filtering significantly improves re-ranking performance, with smaller models becoming competitive with much larger ones like GPT-4. Thresholds around 0.3-0.7 appear stable across datasets.

## Method Summary
The approach involves using an LLM to assign relevance scores (0-1) to retrieved passages, then filtering out passages below a learned threshold. The threshold is selected using F1 score maximization on a small set of expert-annotated passages (qrels). After filtering, the remaining passages are re-ranked using a listwise approach with a sliding window. The method uses zero-shot prompting and is tested on TREC-DL2019/2020 and BEIR datasets with BM25 retrieval.

## Key Results
- Pre-filtering significantly improves LLM re-ranking performance across multiple datasets
- Thresholds between 0.3-0.7 show stable performance across different datasets
- Smaller models like Mixtral-8x7B-Instruct become competitive with much larger models like GPT-4 after pre-filtering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-filtering reduces noise by removing irrelevant passages before re-ranking, improving LLM focus.
- Mechanism: The LLM evaluates each passage's relevance using a chain-of-thought prompt, assigns a score between 0 and 1, and passages below a learned threshold are discarded. This reduces the number of passages the re-ranker must process.
- Core assumption: The LLM can accurately assign relevance scores when guided by a plan-and-solve reasoning prompt.
- Evidence anchors:
  - [abstract]: "Our experiments show that by using a small number of human generated relevance scores, coupled with LLM relevance scoring, it is effectively possible to filter out irrelevant passages before re-ranking."
  - [section]: "Using the pre-filtering step, the number of noisy passages that can misguide the re-ranker decreases, leading to improved performance for the re-ranker."
  - [corpus]: No corpus evidence provided; only citations and citation counts are available, not detailed mechanisms.

### Mechanism 2
- Claim: A stable threshold can be learned from a small set of expert-annotated relevance scores (qrels).
- Mechanism: The system computes precision, recall, and F1 scores for various thresholds on passages with qrels. The threshold with the highest F1 is selected, balancing the detection of relevant and irrelevant passages.
- Core assumption: A small subset of expert annotations (8% of dataset) is sufficient to determine an effective threshold.
- Evidence anchors:
  - [abstract]: "Our experiments also show that this pre-filtering then allows the LLM to perform significantly better at the re-ranking task."
  - [section]: "We find that even a small percentage of qrels (8% of the total dataset) is enough to determine a threshold value that can effectively help our chosen LLM filter out irrelevant passages."
  - [corpus]: No corpus evidence provided; only citations and citation counts are available, not detailed mechanisms.

### Mechanism 3
- Claim: Pre-filtering allows smaller LLMs to achieve competitive results with much larger models.
- Mechanism: By reducing the number of passages and noise, the smaller LLM (Mixtral-8x7B-Instruct with 4-bit quantization) can focus its limited capacity on ranking relevant passages more accurately, achieving results close to or better than much larger models like GPT-4.
- Core assumption: The computational savings from pre-filtering offset the smaller model's inherent limitations.
- Evidence anchors:
  - [abstract]: "Indeed, our results show that smaller models such as Mixtral can become competitive with much larger proprietary models (e.g., ChatGPT and GPT-4)."
  - [section]: "Indeed, after using our pre-filtering step, a limited model such as Mixtral-8x7B-Instruct... can become competitive with– and in one case surpass–much larger, and resource intensive, models such as GPT-4."
  - [corpus]: No corpus evidence provided; only citations and citation counts are available, not detailed mechanisms.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: It guides the LLM to reason through relevance scoring step-by-step, improving accuracy.
  - Quick check question: What is the purpose of asking the LLM to explain its rationale before scoring?

- Concept: Precision-recall tradeoff
  - Why needed here: The threshold selection balances precision (avoiding false positives) and recall (capturing all relevant passages) via F1 score.
  - Quick check question: What happens to recall if the threshold is set too high?

- Concept: Zero-shot prompting
  - Why needed here: The method uses instructions without fine-tuning, enabling deployment across different datasets.
  - Quick check question: Why does the paper use zero-shot prompting instead of fine-tuning?

## Architecture Onboarding

- Component map: Retriever → Pre-filtering LLM → Re-ranking LLM → Final ranked list
- Critical path: Passage retrieval → Relevance scoring → Threshold filtering → Listwise re-ranking
- Design tradeoffs: Smaller LLM + pre-filtering vs. larger LLM without pre-filtering; computational cost vs. ranking accuracy
- Failure signatures: Low precision (too many irrelevant passages retained) or low recall (relevant passages discarded); degraded nDCG scores
- First 3 experiments:
  1. Run pre-filtering with threshold 0.3 on BEIR Covid dataset; compare nDCG@10 to baseline.
  2. Test different thresholds (0.3, 0.5, 0.7) on TREC DL2019 with qrels; record F1 scores.
  3. Compare pre-filtering with Mixtral to re-ranking without pre-filtering on Signal dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different threshold selection strategies (beyond random initialization and F1 maximization) affect the performance and stability of the pre-filtering step?
- Basis in paper: [inferred] The paper mentions random threshold initialization and selecting the threshold with highest F1 score, but notes this could vary across datasets.
- Why unresolved: The paper only explores one threshold selection method and shows it works reasonably well, but doesn't investigate alternative strategies like statistical analysis of LLM score distributions or cross-validation approaches.
- What evidence would resolve it: Systematic comparison of multiple threshold selection strategies across diverse IR datasets, showing their impact on pre-filtering effectiveness and re-ranking performance.

### Open Question 2
- Question: How does the effectiveness of pre-filtering vary with different retriever types (beyond BM25) and passage lengths?
- Basis in paper: [inferred] The paper uses BM25 as the retriever and doesn't explore how pre-filtering performs with dense retrievers or varying passage lengths.
- Why unresolved: The experiments are limited to BM25 retrieval, leaving open questions about generalizability to neural retrievers and how passage characteristics affect pre-filtering performance.
- What evidence would resolve it: Comparative experiments using dense retrievers (e.g., DPR, Contriever) and passages of varying lengths, measuring pre-filtering impact on re-ranking quality across different retrieval scenarios.

### Open Question 3
- Question: What is the optimal balance between pre-filtering threshold strictness and computational efficiency when scaling to large-scale IR systems?
- Basis in paper: [inferred] The paper shows pre-filtering reduces the number of passages for re-ranking, but doesn't systematically explore the trade-off between threshold strictness, retained passage count, and overall system efficiency.
- Why unresolved: While the paper demonstrates pre-filtering's effectiveness, it doesn't quantify the efficiency gains across different operational scales or explore how threshold choices affect the computational budget.
- What evidence would resolve it: Empirical analysis mapping threshold values to retained passage percentages and corresponding re-ranking costs, identifying sweet spots for different scale requirements and hardware constraints.

## Limitations

- The approach relies heavily on the availability and quality of qrels for threshold selection
- The paper doesn't address computational overhead introduced by the two-stage LLM pipeline
- Lack of detailed prompt wording and exact hardware specifications limits reproducibility

## Confidence

- **High confidence**: Pre-filtering significantly improves LLM-based re-ranking performance
- **Medium confidence**: A small percentage of qrels (8%) is sufficient to determine an effective threshold
- **Medium confidence**: Pre-filtering enables smaller LLMs to compete with much larger models

## Next Checks

1. Test the pre-filtering approach on a dataset with very limited or no qrels to assess whether the method can still select effective thresholds or if it breaks down without expert annotations.

2. Systematically vary the zero-shot prompt wording and structure to determine how sensitive the relevance scoring is to prompt design, and whether small changes significantly affect performance.

3. Measure and compare the total inference time and cost of the two-stage pipeline (pre-filtering + re-ranking) against a single-pass re-ranking approach, especially at scale, to quantify practical trade-offs.