---
ver: rpa2
title: Adversarial Attacks and Defenses in Multivariate Time-Series Forecasting for
  Smart and Connected Infrastructures
arxiv_id: '2408.14875'
source_url: https://arxiv.org/abs/2408.14875
tags:
- attacks
- adversarial
- dataset
- rmse
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerability of deep learning models
  in multivariate time-series forecasting to adversarial attacks and proposes defense
  mechanisms to enhance model robustness. The authors employ untargeted white-box
  attacks (FGSM and BIM) on LSTM models trained on electricity consumption and hard
  disk drive (HDD) datasets, demonstrating significant performance degradation with
  up to 390% and 316% increases in RMSE, respectively.
---

# Adversarial Attacks and Defenses in Multivariate Time-Series Forecasting for Smart and Connected Infrastructures

## Quick Facts
- arXiv ID: 2408.14875
- Source URL: https://arxiv.org/abs/2408.14875
- Reference count: 40
- Primary result: Adversarial attacks significantly degrade LSTM model performance in time-series forecasting, with up to 390% and 316% RMSE increases, while proposed defenses achieve up to 94.81% and 72.41% reductions

## Executive Summary
This paper investigates the vulnerability of deep learning models used in multivariate time-series forecasting to adversarial attacks, focusing on smart and connected infrastructure applications. The authors demonstrate that white-box attacks (FGSM and BIM) can significantly degrade model performance on electricity consumption and hard disk drive datasets. They propose two defense mechanisms - Data Augmentation-based Adversarial Training (DAAT) and Layer-wise Perturbation-based Adversarial Training (LPAT) - which substantially improve model robustness against these attacks.

## Method Summary
The authors employ LSTM and Encoder-Decoder LSTM architectures for time-series forecasting on electricity consumption and HDD datasets. They implement untargeted white-box attacks using FGSM and BIM to poison inputs during evaluation, causing significant RMSE increases. To counter these attacks, they develop two defense strategies: DAAT which augments training data with adversarial examples, and LPAT which perturbs gradients during backpropagation. The effectiveness of attacks and defenses is measured through RMSE comparisons between clean and attacked data.

## Key Results
- FGSM and BIM attacks increase RMSE by up to 390% on electricity dataset and 316% on HDD dataset
- DAAT achieves up to 94.81% reduction in RMSE for HDD dataset
- LPAT achieves up to 72.41% reduction in RMSE for electricity dataset
- Model hardening through adversarial training significantly improves robustness against white-box attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial attacks on multivariate time-series forecasting models exploit small perturbations in input sequences to cause large prediction errors
- Mechanism: The attacks modify the input data by adding gradients of the loss function scaled by a small epsilon value, causing the model to learn incorrect distributions
- Core assumption: The model's vulnerability stems from its sensitivity to small input changes, which are imperceptible to humans but significant to the model
- Evidence anchors:
  - [abstract] "employ untargeted white-box attacks, namely the Fast Gradient Sign Method (FGSM) and the Basic Iterative Method (BIM), to poison the inputs to the training process, effectively misleading the model"
  - [section] "We contaminate the inputs to the vanilla LSTM model during the evaluation stage using the FGSM and BIM attacks mentioned in Section 5.2."
  - [corpus] Weak evidence - no direct corpus matches for FGSM/BIM on time-series; strongest is "Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting" which is related but not directly on FGSM/BIM
- Break condition: If the model is trained with adversarial examples (DAAT) or gradient hardening (LPAT), the perturbations no longer cause large errors

### Mechanism 2
- Claim: Data augmentation-based adversarial training (DAAT) improves model robustness by exposing it to perturbed inputs during training
- Mechanism: The model is retrained on a dataset augmented with adversarial examples, allowing it to learn the true underlying distribution despite noise
- Core assumption: Including perturbed samples in the training set teaches the model to recognize and ignore adversarial noise
- Evidence anchors:
  - [abstract] "develop robust models through adversarial training and model hardening"
  - [section] "In DAAT, we augment the dataset with perturbed input attributes created by performing adversarial perturbations on the original feature samples."
  - [corpus] Weak evidence - "TabularBench: Benchmarking Adversarial Robustness for Tabular Deep Learning in Real-world Use-cases" mentions robustness but not DAAT specifically
- Break condition: If the perturbation magnitude during training doesn't match the attack magnitude, the defense becomes less effective

### Mechanism 3
- Claim: Layer-wise perturbation-based adversarial training (LPAT) enhances model robustness by perturbing gradients during backpropagation
- Mechanism: During training, gradients are perturbed using FGSM or BIM before weight updates, forcing the model to learn more robust feature representations
- Core assumption: Perturbing gradients during training mimics adversarial noise, making the model more resilient to actual attacks
- Evidence anchors:
  - [abstract] "layer-wise perturbation-based adversarial training (LPAT), achieving up to 94.81% and 72.41% reductions in RMSE"
  - [section] "In the second method, we harden the vanilla LSTM model by perturbing the gradients of the model during backpropagation"
  - [corpus] No direct corpus evidence for LPAT on time-series; strongest is "Beyond Training-time Poisoning" which discusses backdoor attacks but not gradient perturbation
- Break condition: If gradient perturbation magnitude is too small or too large, the model either remains vulnerable or fails to learn the true distribution

## Foundational Learning

- Concept: Understanding of adversarial machine learning attacks (FGSM, BIM)
  - Why needed here: The paper relies on these specific attack methods to evaluate model vulnerability
  - Quick check question: What is the mathematical difference between FGSM and BIM attacks?

- Concept: LSTM and Encoder-Decoder LSTM architectures
  - Why needed here: The experiments use these specific architectures for time-series forecasting
  - Quick check question: How does an Encoder-Decoder LSTM differ from a standard LSTM in handling variable-length sequences?

- Concept: Root Mean Squared Error (RMSE) as evaluation metric
  - Why needed here: The paper uses RMSE to quantify attack impact and defense effectiveness
  - Quick check question: Why is RMSE particularly suitable for measuring time-series forecasting accuracy?

## Architecture Onboarding

- Component map:
  - Data preprocessing -> Model training -> Attack implementation -> Defense implementation -> Evaluation

- Critical path: Clean data → Train baseline model → Apply attacks → Measure RMSE degradation → Apply defenses → Measure RMSE improvement

- Design tradeoffs:
  - FGSM vs. BIM: FGSM is faster but weaker; BIM is slower but more effective
  - DAAT vs. LPAT: DAAT requires more data storage; LPAT modifies training dynamics
  - Epsilon selection: Too small = ineffective defense; too large = model collapse

- Failure signatures:
  - High RMSE on clean data → model architecture or training issues
  - Low attack impact → model is already robust or attacks are incorrectly implemented
  - Defenses worsen performance → incorrect perturbation magnitude or training instability

- First 3 experiments:
  1. Train baseline LSTM on electricity dataset, verify test RMSE < 0.1
  2. Apply FGSM with epsilon=0.1, verify RMSE increases by >50%
  3. Apply DAAT with epsilon=0.1, verify RMSE on attacked data returns to baseline levels

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions but implies several through its limitations and future work suggestions, including the need to explore black-box attacks, adaptive adversaries, and optimal perturbation magnitude selection for adversarial training.

## Limitations
- Attack assumptions limited to white-box scenarios, which may not reflect real-world conditions
- Defense effectiveness varies significantly between datasets (72.41% for electricity vs 94.81% for HDD)
- LPAT mechanism lacks direct corpus validation and empirical support
- No evaluation against adaptive attacks that could potentially circumvent proposed defenses

## Confidence
- Attack vulnerability claims: High
- DAAT defense effectiveness: High
- LPAT defense effectiveness: Medium
- Cross-dataset generalizability: Low

## Next Checks
1. Test defense effectiveness against black-box attack variants to assess real-world applicability
2. Evaluate model robustness with varying epsilon values during training vs. attack phases
3. Assess defense performance on additional time-series datasets to verify generalizability across different domains