---
ver: rpa2
title: Investigating Length Issues in Document-level Machine Translation
arxiv_id: '2412.17592'
source_url: https://arxiv.org/abs/2412.17592
tags:
- translation
- unif
- length
- computational
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether modern neural machine translation
  models can effectively process and translate long documents as whole units, rather
  than sentence-by-sentence. It introduces a new methodology to measure how translation
  quality degrades with increasing document length, comparing holistic translations
  to sentence-level baselines.
---

# Investigating Length Issues in Document-level Machine Translation

## Quick Facts
- **arXiv ID**: 2412.17592
- **Source URL**: https://arxiv.org/abs/2412.17592
- **Authors**: Ziqian Peng; Rachel Bawden; François Yvon
- **Reference count**: 40
- **Primary result**: Translation quality degrades with increasing document length; manipulating positional embeddings yields modest improvements only for encoder-decoder models.

## Executive Summary
This paper investigates whether modern neural machine translation models can effectively process and translate long documents as whole units, rather than sentence-by-sentence. It introduces a new methodology to measure how translation quality degrades with increasing document length, comparing holistic translations to sentence-level baselines. Experiments with two representative architectures—an encoder-decoder model (NLLB) and a decoder-only large language model (TOWER BASE)—consistently show that translation performance declines as input document length increases, especially for sentences appearing later in the document. Manipulating the distribution of positional embeddings during training (via UNIF PE) yields modest improvements for the APE-based encoder-decoder model but not for the RoPE-based decoder-only model. The results indicate that despite the computational feasibility of document-level MT, current models still underperform compared to sentence-level baselines, and length-related degradation remains a significant challenge.

## Method Summary
The authors introduce a methodology to evaluate document-level translation quality as a function of input length. They construct pseudo-documents from TED talks data, sampling sentences to create documents of fixed lengths (128 to 2048 tokens) using both Gaussian and uniform length distributions. Two representative architectures are fine-tuned: NLLB200 (encoder-decoder with absolute positional embeddings) and TOWER BASE (decoder-only with rotary positional embeddings). They compare three positional embedding strategies: standard, UNIF PE (uniform sampling of position offsets), and SHAPE (random offset sampling). Models are evaluated on fixed-length test sets using document-level BLEU (ds-BLEU) and sentence-level COMET scores, with statistical significance assessed via paired t-tests.

## Key Results
- Translation performance consistently degrades as document length increases for both encoder-decoder and decoder-only models.
- Sentences appearing later in documents suffer disproportionately in translation quality.
- UNIF PE yields modest improvements for the encoder-decoder model but not for the decoder-only model.
- Current document-level models underperform compared to sentence-level baselines despite computational feasibility.

## Why This Works (Mechanism)
The paper identifies that current neural MT models struggle with long-range dependencies and context integration as document length increases. For encoder-decoder models, UNIF PE helps by ensuring positional embeddings are trained on a more uniform distribution of offsets, potentially improving generalization to unseen positions. However, decoder-only models with RoPE appear less sensitive to this intervention, suggesting different positional encoding mechanisms have varying robustness to length-related degradation.

## Foundational Learning
- **Document-level BLEU (ds-BLEU)**: Evaluates entire document translations as units rather than aggregating sentence-level scores. Needed to capture holistic translation quality and context preservation across sentences. Quick check: Verify reference-document alignment and tokenization consistency.
- **COMET metric**: Reference-based evaluation using pre-trained neural models to assess translation quality at sentence level. Needed to isolate sentence-level performance within documents. Quick check: Confirm sentence alignment between hypotheses and references before scoring.
- **Positional Embedding Strategies**: Different methods (absolute, rotary, uniform sampling) for encoding token positions. Needed to investigate whether training positional embeddings on uniform length distributions mitigates length-related degradation. Quick check: Validate PE offset sampling distributions during training.
- **Pseudo-document construction**: Sampling sentences to create documents of controlled lengths. Needed to systematically study translation quality as a function of document length. Quick check: Verify length distributions match intended configurations.
- **Paired t-tests**: Statistical method to assess significance of performance differences between models. Needed to determine if observed improvements are statistically meaningful. Quick check: Confirm proper pairing of hypotheses and references for each test.

## Architecture Onboarding

**Component Map**: Input sentences -> Document construction -> Tokenization -> Encoder (NLLB) or Decoder (TOWER) -> Positional Embeddings (APE/UNIF/SHAPE) -> Translation -> Evaluation (ds-BLEU/COMET)

**Critical Path**: Document sampling → Model inference → Alignment → Evaluation scoring

**Design Tradeoffs**: Holistic document translation vs. sentence-level baselines; computational cost of processing long sequences vs. quality gains; different positional embedding strategies and their impact on length generalization.

**Failure Signatures**: 
- Brevity penalty increases with document length
- n-gram repetition in longer translations
- Disproportionate quality drop for sentences appearing later in documents
- Poor alignment between hypotheses and references for COMET scoring

**First Experiments**:
1. Evaluate translation quality at document boundaries (first vs. last sentences) to quantify position-dependent degradation.
2. Compare UNIF PE against SHAPE and standard PE across multiple offset sampling ranges to identify optimal configuration.
3. Analyze attention patterns in encoder-decoder vs. decoder-only models to understand how positional information is utilized at different sequence lengths.

## Open Questions the Paper Calls Out
None

## Limitations
- Exact hyperparameters for UNIF PE offset sampling (m, l, r) and SHAPE (offset range and distribution) are not specified.
- Training schedule details (batch size, gradient accumulation, warmup steps, patience) and inference parameters (beam size, decoding algorithm) are not fully specified.
- The precise method for generating pseudo-documents (e.g., exact token splitting and shuffling procedures) is not detailed.
- Statistical significance of modest improvements with UNIF PE is not rigorously established beyond paired t-tests.

## Confidence
- **High**: The overall trend that document-level translation performance degrades with increasing document length is consistently observed across both architectures and evaluation metrics.
- **Medium**: The modest improvements seen with UNIF PE for the encoder-decoder model are plausible given the known limitations of APEs for long sequences, but the lack of improvement for the RoPE-based model is less expected and warrants further investigation.
- **Low**: The specific impact of manipulating positional embedding distributions on translation quality is uncertain due to the lack of exact hyperparameter details and the mixed results across architectures.

## Next Checks
1. Replicate the experiments with a broader range of positional embedding offset sampling strategies (e.g., varying the range and distribution of offsets) to determine if the observed improvements with UNIF PE are robust or specific to the chosen hyperparameters.
2. Investigate the effect of document-level translation on sentence-level quality by analyzing COMET scores at different positions within the document, especially for sentences appearing later in longer documents.
3. Conduct a more rigorous statistical analysis of the observed improvements, including effect size calculations and multiple comparison corrections, to assess the practical significance of the modest gains seen with UNIF PE.