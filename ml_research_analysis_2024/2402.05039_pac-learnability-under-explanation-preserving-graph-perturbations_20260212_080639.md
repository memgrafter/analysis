---
ver: rpa2
title: PAC Learnability under Explanation-Preserving Graph Perturbations
arxiv_id: '2402.05039'
source_url: https://arxiv.org/abs/2402.05039
tags:
- graph
- learning
- training
- data
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how leveraging subgraph explanations can improve
  graph neural network (GNN) learning efficiency. The authors introduce explanation-assisted
  empirical risk minimization (EA-ERM), which incorporates explanation subgraphs directly
  into the learning process.
---

# PAC Learnability under Explanation-Preserving Graph Perturbations

## Quick Facts
- arXiv ID: 2402.05039
- Source URL: https://arxiv.org/abs/2402.05039
- Reference count: 40
- The authors introduce explanation-assisted empirical risk minimization (EA-ERM) and prove it can achieve arbitrarily smaller sample complexity than standard explanation-agnostic learning rules

## Executive Summary
This paper analyzes how leveraging subgraph explanations can improve graph neural network (GNN) learning efficiency. The authors introduce explanation-assisted empirical risk minimization (EA-ERM), which incorporates explanation subgraphs directly into the learning process. They prove that EA-ERM can achieve arbitrarily smaller sample complexity than standard explanation-agnostic learning rules. The key result is that the sample complexity for EA-ERM is O(d/ε² log² d + 1/ε² ln(1/δ)), where d is the explanation-assisted VC dimension, while explanation-agnostic methods have sample complexity dependent on the standard VC dimension. The authors also explore explanation-assisted data augmentation, showing it can improve performance when augmented data is in-distribution but may worsen results when out-of-distribution. They propose an implementable explanation-assisted GNN architecture that combines in-distribution perturbation of non-explanation edges with weighted loss functions. Extensive experiments demonstrate that their method consistently outperforms vanilla GNNs and traditional augmentation methods across multiple datasets, achieving accuracy improvements of 2-5% on average while using significantly fewer training samples.

## Method Summary
The authors introduce explanation-assisted empirical risk minimization (EA-ERM), a learning framework that incorporates explanation subgraphs into the training process. They develop theoretical bounds showing that EA-ERM can achieve arbitrarily smaller sample complexity than standard explanation-agnostic methods by leveraging the reduced effective VC dimension when explanations are known. The method uses explanation-preserving perturbations to generate in-distribution augmented samples, which are incorporated into training through a weighted loss function. The proposed architecture combines a base GNN model with an explanation generator and perturbation operator, trained using a balanced objective that prioritizes original data while benefiting from augmentation.

## Key Results
- EA-ERM achieves sample complexity of O(d/ε² log² d + 1/ε² ln(1/δ)), where d is the explanation-assisted VC dimension
- Explanation-assisted learning can achieve arbitrarily smaller sample complexity than explanation-agnostic methods
- Data augmentation improves performance when augmented data is in-distribution but may worsen results when out-of-distribution
- The method achieves 2-5% accuracy improvements on average while using significantly fewer training samples across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanation-assisted learning rules can achieve arbitrarily smaller sample complexity than explanation-agnostic methods by leveraging the reduced effective VC dimension when explanations are known.
- Mechanism: The EA-ERM method constructs classifiers that are invariant to perturbations of non-explanation edges, effectively reducing the hypothesis space complexity from standard VC dimension V_C(H) to explanation-assisted VC dimension V_C^EA(H, Ψ). This reduction in complexity directly translates to lower sample complexity bounds.
- Core assumption: The classification problem is (Ψ, ζ)-invariant with sufficiently small ζ, meaning the label is stable under perturbations of non-explanation edges.
- Evidence anchors:
  - [abstract]: "It is shown that the sample complexity of explanation-assisted learning can be arbitrarily smaller than explanation-agnostic learning."
  - [section 3]: "Let d = V CEA(H, Ψ, γ). Consider two sets T and T' of m independently generated labeled random graphs generated according to PG."
  - [corpus]: Weak evidence; neighboring papers focus on general PAC learnability rather than explanation-specific complexity.
- Break condition: If the perturbation invariance assumption fails (ζ is large) or if explanations are inaccurate, the complexity reduction disappears.

### Mechanism 2
- Claim: Data augmentation can improve performance when augmented data is in-distribution but may worsen it when out-of-distribution.
- Mechanism: Augmented samples generated by explanation-preserving perturbations expand the training set while maintaining the same label distribution as original samples. If these augmented samples are in-distribution with respect to the original PG, the empirical risk minimization over the augmented set approximates the true risk minimization.
- Core assumption: The perturbation operator Π(·) generates samples that are in-distribution with respect to the original graph distribution PG.
- Evidence anchors:
  - [section 4]: "It is shown both theoretically and empirically that this may sometimes lead to better and sometimes to worse performance in terms of sample complexity, where gains are contingent on producing in-distribution augmented samples."
  - [section 5]: "This is addressed in two ways in our training procedure. First, we follow an existing work to implement the perturbation function Π(·) which randomly removes a small number of non-explanation edges (Zheng et al., 2023)."
  - [corpus]: Weak evidence; no direct corpus support for the in-distribution/out-of-distribution distinction.
- Break condition: If augmented samples are out-of-distribution, the empirical risk minimization over the augmented set no longer approximates true risk, leading to worse generalization.

### Mechanism 3
- Claim: The weighted loss function with hyperparameter λ balances original and augmented data contributions to prevent overfitting to out-of-distribution augmentations.
- Mechanism: By weighting the loss on augmented samples with a small coefficient λ, the training objective prioritizes minimizing error on the original (in-distribution) samples while still benefiting from the additional supervision provided by augmented samples.
- Core assumption: A sufficiently small λ ensures that the augmented loss component does not dominate the overall training objective.
- Evidence anchors:
  - [section 5]: "Second, to further alleviate the negative effects of out-of-distributed augmentations, we choose the hyperparameter λ (in Eq. 4) small enough, so that the loss on the (potentially out-of-distribution) augmented data does not dominate the loss on the original data."
  - [section 6.2]: "We select500, 100, and 100 samples in BA-2motifs dataset as the training set, valid set, and test set, respectively. To obtain OOD augmentations, we add edges to the non-explanation subgraphs until the average node degree is not less than 17."
  - [corpus]: Weak evidence; neighboring papers do not discuss loss balancing strategies for augmentation.
- Break condition: If λ is too large, the model may overfit to out-of-distribution augmented samples, negating the benefits of the approach.

## Foundational Learning

- Concept: Probably Approximately Correct (PAC) learning framework
  - Why needed here: The paper analyzes sample complexity bounds within the PAC framework, establishing when explanation-assisted learning achieves lower sample complexity than explanation-agnostic methods.
  - Quick check question: What does it mean for a learning rule to have sample complexity m(ε, δ) in the PAC framework?

- Concept: Vapnik-Chervonenkis (VC) dimension
  - Why needed here: The explanation-assisted VC dimension V_C^EA(H, Ψ) quantifies the complexity of the hypothesis class when explanations are known, which directly affects the sample complexity bound.
  - Quick check question: How does the explanation-assisted VC dimension relate to the standard VC dimension in terms of hypothesis class complexity?

- Concept: Graph neural networks (GNNs) and their explainability
  - Why needed here: The paper applies explanation-assisted learning specifically to GNNs, where explanations are subgraphs that are almost sufficient statistics for classification.
  - Quick check question: What defines an explanation subgraph in the context of GNN explainability, and how does it relate to perturbation invariance?

## Architecture Onboarding

- Component map:
  Explanation generator (Ψ) -> Perturbation operator (Π) -> Base GNN model (f) -> Loss function

- Critical path:
  1. Generate explanations for training samples using Ψ
  2. Apply perturbation operator Π to create augmented samples
  3. Train GNN model using weighted loss function
  4. Evaluate performance on test set

- Design tradeoffs:
  - Larger λ values give more weight to augmented samples but risk overfitting to out-of-distribution data
  - More extensive perturbation (larger γ) may generate more diverse augmentations but increases out-of-distribution risk
  - Complex explanation generators may provide more accurate explanations but increase computational cost

- Failure signatures:
  - Performance degradation when using out-of-distribution augmentations indicates λ is too large or perturbation is too aggressive
  - Minimal improvement over baseline methods suggests explanations are not sufficiently informative or perturbation is too conservative
  - Training instability may indicate explanation quality issues or inappropriate hyperparameter settings

- First 3 experiments:
  1. Compare vanilla GNN training with EA-ERM on a small synthetic dataset with known explanations to verify sample complexity reduction
  2. Test the effect of different λ values on a real dataset with known explanations to find the optimal balance between original and augmented data
  3. Evaluate the impact of perturbation magnitude (γ) on in-distribution augmentation quality and model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does explanation-assisted data augmentation outperform standard data augmentation methods?
- Basis in paper: The paper discusses that explanation-assisted data augmentation may improve performance when augmented data is in-distribution, but worsen results when out-of-distribution. However, the exact conditions for when it outperforms are not fully characterized.
- Why unresolved: The paper provides examples and empirical observations but does not provide a comprehensive theoretical framework or precise conditions under which explanation-assisted data augmentation will be beneficial.
- What evidence would resolve it: Detailed empirical studies across a wide range of datasets and graph types, combined with theoretical analysis identifying specific characteristics of graphs and explanations that lead to improved performance.

### Open Question 2
- Question: How does the choice of explanation method (e.g., GNNExplainer vs PGExplainer) affect the performance of explanation-assisted learning?
- Basis in paper: The paper uses both GNNExplainer and PGExplainer in its experiments and shows that both can improve performance, but does not deeply analyze the differences between them or provide guidance on when to use each.
- Why unresolved: While the paper shows that both methods can be effective, it does not explore the nuances of how different explanation methods impact learning outcomes or provide a framework for selecting the most appropriate explainer.
- What evidence would resolve it: Comparative studies using various explanation methods across different graph types and tasks, with analysis of the characteristics of each explainer and their impact on learning performance.

### Open Question 3
- Question: Can explanation-assisted learning be extended to other graph learning tasks beyond classification, such as link prediction or graph generation?
- Basis in paper: The paper focuses on graph classification and does not explore other graph learning tasks. The concept of explanation-assisted learning could potentially be applied to other tasks.
- Why unresolved: The paper's theoretical analysis and empirical studies are limited to graph classification, leaving open the question of how explanation-assisted learning might be adapted or extended to other tasks.
- What evidence would resolve it: Empirical studies applying explanation-assisted learning to tasks like link prediction or graph generation, along with theoretical analysis of how the principles of explanation-assisted learning might need to be modified for different tasks.

### Open Question 4
- Question: How does the size and quality of the explanation subgraph impact the effectiveness of explanation-assisted learning?
- Basis in paper: The paper mentions that the explanation subgraph is an 'almost sufficient' statistic of the input graph, but does not deeply explore how the size or quality of this subgraph affects learning outcomes.
- Why unresolved: While the paper assumes the existence of good explanation subgraphs, it does not analyze how variations in the quality or size of these subgraphs impact the learning process or final performance.
- What evidence would resolve it: Empirical studies varying the size and quality of explanation subgraphs, combined with analysis of how these variations correlate with learning performance and sample complexity.

## Limitations
- Theoretical claims depend critically on the assumption that explanations are both accurate and sufficient for classification
- Empirical evaluation uses relatively small training set sizes (500 samples) which may not fully capture asymptotic behavior
- Perturbation operator implementation relies on specific assumptions about edge removal that may not generalize to all graph types

## Confidence
- High confidence in the core mechanism showing that explanation-assisted learning can reduce effective hypothesis space complexity when explanations are accurate
- Medium confidence in the practical implementation of data augmentation through explanation-preserving perturbations
- Low confidence in the generalizability of results to very large graphs or different GNN architectures

## Next Checks
1. Test the method on much larger graph datasets (10K+ nodes) to evaluate scalability and verify whether sample complexity advantages persist with realistic data volumes.

2. Implement ablation studies varying the explanation accuracy (e.g., by introducing noise to explanation subgraphs) to quantify the relationship between explanation quality and performance gains.

3. Compare against alternative explanation-based learning approaches that don't use augmentation to isolate the specific contribution of the explanation-preserving perturbation strategy.