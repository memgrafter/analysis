---
ver: rpa2
title: 'DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective
  Instruction Tuning'
arxiv_id: '2402.09136'
source_url: https://arxiv.org/abs/2402.09136
tags:
- code
- instruction
- training
- evaluation
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DolphCoder, a diverse instruction model with
  self-evaluating for code generation. DolphCoder learns diverse instruction targets
  and combines a code evaluation objective to enhance its code generation ability.
---

# DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning

## Quick Facts
- arXiv ID: 2402.09136
- Source URL: https://arxiv.org/abs/2402.09136
- Reference count: 17
- Key outcome: DolphCoder achieves superior performance on HumanEval and MBPP benchmarks compared to strong open-source code LLMs through diverse instruction tuning and self-evaluating objectives.

## Executive Summary
DolphCoder introduces a multi-step training approach that combines diverse instruction tuning with code evaluation objectives to enhance code generation performance. The model first learns to evaluate code correctness before generating code, creating a self-improving feedback loop. Through systematic experiments, DolphCoder demonstrates significant improvements over baseline Code Llama models on standard code generation benchmarks.

## Method Summary
DolphCoder employs a two-stage training process: first fine-tuning on diverse instruction data generated through Evol-Instruct and ChatGPT system prompts, then further fine-tuning on code evaluation data labeled by GPT-4. The model uses a multi-step training paradigm (MOT) that alternates between code generation and evaluation objectives to balance the competing tasks. This approach addresses the challenge of balancing multiple objectives in code generation models while improving both generation quality and evaluation capability.

## Key Results
- Achieves state-of-the-art performance on HumanEval and MBPP benchmarks among open-source code LLMs
- Demonstrates that increasing sampling ratio for diverse responses improves performance, with diminishing returns beyond ratio 3-6
- Shows that multi-step training (MOT) outperforms joint training in balancing code generation and evaluation objectives
- Validates that code evaluation training enhances code generation capability despite ~20% noise in GPT-4 labels

## Why This Works (Mechanism)

### Mechanism 1
Diverse instruction tuning exposes the model to multiple distinct reasoning paths for the same problem, broadening the supervised signal space and encouraging learning of general problem-solving strategies rather than memorizing one canonical answer. The mechanism relies on system prompts generating genuinely different computational strategies rather than syntactic variations.

### Mechanism 2
Training on code evaluation tasks enhances code generation by teaching the model to distinguish correct from incorrect code through evaluation responses ("passed"/"not passed"). This meta-cognitive ability transfers to improved code generation through better selection and accuracy, though effectiveness is limited by ~20% noise in evaluation labels.

### Mechanism 3
Multi-step training balances competing objectives of code generation and evaluation better than joint training by using a sequential curriculum approach. First fine-tuning on evaluation data stabilizes correctness assessment capability, then fine-tuning on generation data recovers and improves generation while retaining evaluation insight, preventing catastrophic forgetting.

## Foundational Learning

- **Instruction fine-tuning (IFT)**: Base Code Llama models lack alignment to human instructions; IFT teaches them to follow natural language prompts for code tasks. Quick check: What distinguishes supervised fine-tuning from instruction fine-tuning in this context?
- **Chain-of-thought (CoT) reasoning diversity**: Different CoT paths encode different algorithmic strategies; exposing the model to multiple paths improves generalization. Quick check: How does generating multiple answers per instruction differ from generating one correct answer?
- **Multi-task learning with imbalanced objectives**: Code generation and evaluation have different loss scales and convergence rates; balancing them is non-trivial. Quick check: Why might joint training cause the model to overfit one task over the other?

## Architecture Onboarding

- **Component map**: Code Llama (7B/13B) -> DIT stage (Evol-Instruct → ChatGPT generation → heuristic filtering) -> MOT stage (GPT-4 evaluation → two-phase fine-tuning) -> Inference (greedy decoding + prompt template)
- **Critical path**: 1) Generate diverse responses via system prompts 2) Filter low-quality/similar data 3) Fine-tune on DIT 4) Generate noisy eval dataset 5) Fine-tune on eval data (1 epoch) 6) Continue fine-tuning on DIT (100 steps) 7) Evaluate with greedy decoding
- **Design tradeoffs**: DIT vs. quality (more prompts increase diversity but add redundancy/noise), MOT vs. stability (sequential avoids forgetting but requires careful scheduling), Evaluation source (GPT-4 gives labels but introduces ~20% noise)
- **Failure signatures**: Overfitting to generation (high pass@1 but low evaluation accuracy), Underfitting diversity (marginal gains beyond sampling ratio 3), Label noise (MOT improvements limited despite training)
- **First 3 experiments**: 1) Ablation: Remove MOT, measure drop in pass@1 2) Diversity scaling: Test pass@1 at sampling ratios 1, 3, 6 3) Multi-step vs. joint: Compare MOT-mix vs. MOT-multi-step

## Open Questions the Paper Calls Out

### Open Question 1
What is the impact of using larger foundation models on DolphCoder's performance? The paper only explores 7B/13B models due to computational costs, leaving scalability to larger models untested.

### Open Question 2
How can the accuracy of evaluation signals be improved in DolphCoder? The paper acknowledges room for optimization in training data and plans to explore automatic unit tests and reinforcement learning for cleaner evaluation signals.

### Open Question 3
How does the diversity of system prompts affect DolphCoder's performance? While increasing prompt count improves performance, the optimal diversity level and impact of redundant prompts remain unexplored.

## Limitations
- Diversity quality control relies on heuristic filtering without definitive metrics for genuine reasoning strategy variation
- GPT-4 evaluation labels contain ~20% noise, limiting the effectiveness of self-evaluating mechanisms
- Multi-step training approach is sensitive to training duration and stage ordering, with performance degradation when evaluation fine-tuning extends beyond 1 epoch

## Confidence

**High Confidence**: DolphCoder outperforms baseline Code Llama models on HumanEval and MBPP benchmarks, with convincing ablation studies showing MOT's superiority over joint training.

**Medium Confidence**: The mechanism by which diverse instruction tuning improves code generation is plausible but not definitively proven; correlation between diversity and performance exists without establishing causation.

**Low Confidence**: The claim that code evaluation training directly improves code generation quality is weakest; evidence is correlational rather than mechanistic, complicated by high noise rates in evaluation labels.

## Next Checks

1. **Diversity Strategy Validation**: Compare three diversity generation strategies (system prompt variation, sampling different algorithms, controlled paraphrasing) to isolate genuine reasoning diversity value versus data volume.

2. **Noise-Aware Training**: Implement label smoothing or uncertainty-aware loss functions accounting for ~20% noise in GPT-4 evaluation labels to improve self-evaluating objective effectiveness.

3. **Curriculum Learning Optimization**: Systematically explore MOT training schedules by varying evaluation/generation step ratios, epoch numbers, and objective ordering to identify optimal curriculum maximizing both evaluation accuracy and generation performance.