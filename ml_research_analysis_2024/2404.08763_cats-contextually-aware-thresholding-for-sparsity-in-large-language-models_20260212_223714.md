---
ver: rpa2
title: 'CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models'
arxiv_id: '2404.08763'
source_url: https://arxiv.org/abs/2404.08763
tags:
- sparsity
- cats
- performance
- arxiv
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CATS, a method to reduce inference costs
  of large language models by sparsifying their activation patterns. CATS uses a new
  non-linear activation function to zero out less significant activations in Gated-MLP
  blocks, achieving up to 50% activation sparsity without fine-tuning.
---

# CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models

## Quick Facts
- arXiv ID: 2404.08763
- Source URL: https://arxiv.org/abs/2404.08763
- Reference count: 40
- Key outcome: CATS achieves up to 50% activation sparsity without fine-tuning, maintaining 99% of base model performance while providing ~15% wall-clock speedup

## Executive Summary
CATS introduces a novel approach to reducing inference costs in large language models by sparsifying activation patterns in Gated-MLP blocks. The method uses a percentile-based thresholding mechanism to zero out less significant activations, achieving substantial sparsity without requiring fine-tuning. CATS demonstrates effectiveness on both Mistral-7B and Llama2-7B models, maintaining near-base performance while providing real wall-clock speedups through a custom GPU kernel implementation.

## Method Summary
CATS applies a new non-linear activation function to Gated-MLP blocks in LLMs, zeroing out activations below a computed cutoff threshold based on activation magnitude percentiles. The method requires computing activation magnitudes on a subset of training data to determine layer-specific thresholds, then applies these thresholds during inference. A custom GPU kernel exploits the resulting sparsity to skip loading weight matrix columns corresponding to zeroed activations, reducing memory accesses in memory-bound MLP layers. The approach can be applied without fine-tuning, though fine-tuning with LoRA can further improve performance while maintaining sparsity.

## Key Results
- Achieves 50% activation sparsity without fine-tuning while maintaining 99% of base model performance
- CATS-based models outperform ReLUfication on downstream tasks
- Custom GPU kernel implementation provides ~15% improvement in wall-clock inference latency
- CATS-based models converge faster during fine-tuning and achieve better final performance
- Method effective on both Mistral-7B and Llama2-7B models

## Why This Works (Mechanism)

### Mechanism 1
CATS identifies and zeros out activations in Gated-MLP blocks that contribute minimally to the model output, thereby reducing inference computation. The method computes a cutoff threshold based on the k-th percentile of activation magnitudes, zeroing activations below this threshold and eliminating the need to compute or load corresponding weight matrix columns during inference. The core assumption is that activations with small magnitudes in Gated-MLP layers have negligible impact on downstream computations. Evidence shows CATS-based models maintain performance within 1-2% of base models at 50% sparsity, though related work does not directly validate the negligible impact assumption.

### Mechanism 2
The custom GPU kernel efficiently exploits the sparsity induced by CATS to achieve real wall-clock time speedups. The kernel skips loading weight matrix columns corresponding to zeroed activations, reducing memory accesses and improving arithmetic intensity in memory-bound MLP layers during inference. The core assumption is that reducing memory accesses in MLP layers during inference translates to measurable latency improvements. Evidence shows ~15% improvement in wall-clock inference latency, though related work on hardware-aware optimization does not validate the specific kernel design.

### Mechanism 3
CATS maintains activation sparsity even after fine-tuning, preserving the efficiency gains. After applying CATS to a base model, fine-tuning does not significantly alter the distribution of activation magnitudes, so the same cutoff thresholds continue to zero out a similar proportion of activations. The core assumption is that fine-tuning primarily adjusts weight parameters rather than significantly changing activation distribution patterns in Gated-MLP layers. Evidence shows CATS-based models still exhibit sparsity after fine-tuning with maintained average sparsity levels, though related work on activation sparsity does not address persistence after fine-tuning.

## Foundational Learning

- Concept: Gated-MLP architecture and SiLU activation function
  - Why needed here: CATS operates specifically on Gated-MLP blocks using SiLU activations, so understanding this architecture is essential to implement and reason about the method
  - Quick check question: What are the components of a Gated-MLP block and how does the SiLU function behave mathematically?

- Concept: Mixture-of-Experts (MoE) framework and its relationship to activation sparsity
  - Why needed here: CATS draws a conceptual parallel between activated weight matrix columns and MoE experts, providing intuition for why zeroing small activations is reasonable
  - Quick check question: How can the rows/columns of weight matrices in MLP layers be viewed as experts in an MoE model?

- Concept: GPU memory hierarchy and memory-bound vs. compute-bound operations
  - Why needed here: The custom kernel exploits sparsity to reduce memory accesses in MLP layers, which are memory-bound during inference; understanding this is crucial for kernel design
  - Quick check question: Why are MLP layers typically memory-bound during inference, and how does reducing memory accesses improve performance?

## Architecture Onboarding

- Component map: Base LLM -> CATS activation wrapper -> Custom GPU kernel -> (Optional) Fine-tuning pipeline
- Critical path: 1. Compute activation magnitudes on training data to determine thresholds 2. Apply CATS activation to replace SiLU in Gated-MLP blocks 3. Implement and deploy custom GPU kernel 4. Validate maintained sparsity after fine-tuning (if applicable) 5. Measure wall-clock latency and task performance
- Design tradeoffs: Sparsity level vs. task performance (higher sparsity reduces computation but may degrade accuracy), threshold computation cost vs. accuracy (more training data improves accuracy but increases setup time), kernel complexity vs. speedup (more sophisticated optimizations yield better speedups but increase implementation complexity)
- Failure signatures: Significant performance degradation after applying CATS (indicates critical activations are being zeroed), no wall-clock speedup despite high sparsity (indicates kernel inefficiency or memory access pattern issues), loss of sparsity after fine-tuning (indicates activation distribution changes significantly during training)
- First 3 experiments: 1. Apply CATS-50% to Mistral-7B and measure zero-shot accuracy on a subset of evaluation tasks 2. Implement custom kernel for CATS-50% and measure MLP block latency vs. dense baseline 3. Fine-tune CATS-50% model on RefinedWeb dataset and verify maintained sparsity and performance

## Open Questions the Paper Calls Out

### Open Question 1
Question: Does CATS maintain its performance advantages when applied to larger language models beyond 13B parameters?
Basis in paper: [inferred] The paper suggests CATS scales well with model size based on experiments with Llama2-7B and Llama2-13B, but does not test significantly larger models.
Why unresolved: The authors only tested CATS on models up to 13B parameters. Scaling to larger models may introduce new challenges or limitations not observed in smaller models.
What evidence would resolve it: Experimental results showing CATS performance on models significantly larger than 13B parameters (e.g., 70B+ parameters) across various downstream tasks and sparsity levels.

### Open Question 2
Question: Can CATS be effectively combined with other model optimization techniques like quantization or structured pruning to achieve even greater efficiency gains?
Basis in paper: [inferred] The paper mentions that CATS could potentially be applied to quantized or distilled models but does not explore these combinations. It also notes that other pruning techniques may not lead to wall-clock time improvements.
Why unresolved: The interaction between CATS and other optimization techniques is not explored, leaving open the question of whether combined approaches could yield better results.
What evidence would resolve it: Experiments comparing CATS alone versus CATS combined with quantization, structured pruning, or other optimization techniques, measuring both performance and wall-clock time improvements.

### Open Question 3
Question: What is the optimal sparsity distribution across different layers of a language model when using CATS?
Basis in paper: [inferred] The paper applies a uniform sparsity level across all layers but does not explore layer-wise variations in sparsity. It mentions that future work might investigate enforcing minimum sparsity per layer.
Why unresolved: The paper does not explore whether different layers might benefit from different sparsity levels, or if there's an optimal distribution that maximizes performance while minimizing computational cost.
What evidence would resolve it: Experiments varying sparsity levels across different layers and measuring the resulting performance and efficiency trade-offs, potentially leading to a layer-wise sparsity optimization strategy.

### Open Question 4
Question: How does CATS perform when applied to attention layers compared to its application in MLP layers?
Basis in paper: [explicit] The paper briefly mentions applying CATS to attention layers in Appendix B but only provides preliminary results showing a 4.3% relative performance degradation.
Why unresolved: The initial results suggest CATS can be applied to attention layers but with some performance loss. The full potential and optimal application of CATS to attention layers remains unexplored.
What evidence would resolve it: Comprehensive experiments on applying CATS to attention layers across various models and tasks, exploring different implementation strategies and measuring the resulting performance and efficiency trade-offs.

## Limitations
- Focus on relatively small models (7B parameters) with scalability claims not empirically validated on larger models
- Custom GPU kernel implementation critical for speedups is not fully detailed in the main text
- Lacks ablation studies on different sparsity patterns and their impact on specific task categories
- No exploration of layer-wise sparsity optimization or combination with other optimization techniques

## Confidence

**High confidence**: The claim that CATS maintains performance within 1-2% of base models at 50% sparsity is well-supported by experimental results across multiple benchmarks. The observation that fine-tuned CATS models converge faster is also well-validated.

**Medium confidence**: The claim of ~15% wall-clock speedup relies heavily on the custom kernel implementation, which is not fully described. While the theoretical justification for memory-bound improvements is sound, actual implementation details are missing.

**Low confidence**: The scalability claims to larger models are not empirically validated. The paper only demonstrates results on 7B parameter models, and the assertion that CATS scales well with model size is based on theoretical reasoning rather than experimental evidence.

## Next Checks

1. **Scalability validation**: Apply CATS to larger models (e.g., 70B parameters) and verify that the performance preservation and speedups observed at 7B scale proportionally.

2. **Kernel implementation details**: Request or reconstruct the full custom GPU kernel implementation details, including memory access patterns and parallelization strategies, to enable independent reproduction of the reported latency improvements.

3. **Task-specific ablation**: Conduct detailed ablation studies examining how different sparsity patterns affect specific task categories (reasoning, QA, commonsense), as the current aggregate metrics may mask important variations in task-specific performance.