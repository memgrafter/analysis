---
ver: rpa2
title: Analysing The Impact of Sequence Composition on Language Model Pre-Training
arxiv_id: '2402.13991'
source_url: https://arxiv.org/abs/2402.13991
tags:
- pre-training
- documents
- masking
- causal
- bm25chunk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work systematically examines how sequence composition strategies
  in pre-training affect model performance. The authors show that standard causal
  masking, which conditions predictions on all previous tokens including those from
  different documents, introduces distracting information that degrades language modeling
  and downstream task performance.
---

# Analysing The Impact of Sequence Composition on Language Model Pre-Training

## Quick Facts
- arXiv ID: 2402.13991
- Source URL: https://arxiv.org/abs/2402.13991
- Reference count: 40
- One-line primary result: Intra-document causal masking and BM25Chunk packing significantly improve language model performance by reducing distractions from irrelevant document context

## Executive Summary
This paper systematically examines how sequence composition strategies in language model pre-training affect model performance. The authors identify that standard causal masking, which conditions predictions on all previous tokens including those from different documents, introduces distracting information that degrades both language modeling and downstream task performance. They propose two key solutions: intra-document causal masking that conditions predictions only on previous tokens within the same document, and BM25Chunk, an efficient retrieval-based packing method that increases document relatedness in pre-training chunks. These methods together improve in-context learning (+11.6%), knowledge memorization (+9.8%), and context utilization (+7.2%) abilities while maintaining computational efficiency.

## Method Summary
The study compares different document packing strategies (MIXChunk, UNIChunk, BM25Chunk) and masking approaches (causal masking vs intra-document causal masking) using 1.3B parameter LLaMA-based models trained on the SlimPajama corpus. BM25Chunk uses a document buffer with BM25 retrieval to construct pre-training chunks with related documents, while intra-document causal masking modifies the attention mechanism to only attend to previous tokens within the same document. Models are pre-trained for 150B tokens with specified hyperparameters, then evaluated on perplexity and downstream tasks including text classification, question answering, and retrieval-augmented generation.

## Key Results
- Intra-document causal masking eliminates distractions from irrelevant documents, significantly improving perplexity and downstream performance
- BM25Chunk improves in-context learning (+11.6%), knowledge memorization (+9.8%), and context utilization (+7.2%) without sacrificing efficiency
- Causal masking models are particularly susceptible to distraction from unrelated code documents
- Both BM25Chunk and intra-document causal masking enable better filtering of irrelevant context during language modeling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Causal masking conditions predictions on all previous tokens including those from different documents, which introduces distracting information that degrades performance.
- **Mechanism:** When a language model is trained with causal masking on concatenated documents, each token's prediction is conditioned on all preceding tokens in the chunk, including tokens from unrelated documents. This forces the model to use irrelevant context when predicting the next token within a document, creating a learning signal that is noisy or misleading.
- **Core assumption:** The model cannot distinguish between relevant and irrelevant context during training, so it treats all previous tokens equally when computing predictions.
- **Evidence anchors:**
  - [abstract]: "However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored. In this work, we find that applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks."
  - [section 5.1]: "We observe that using causal masking without considering the boundaries of documents can result in significant performance degradation."
- **Break condition:** The mechanism breaks if the model develops sophisticated context filtering capabilities that allow it to ignore irrelevant document boundaries, or if the relatedness between documents in chunks is high enough that distractions become minimal.

### Mechanism 2
- **Claim:** Intra-document causal masking improves performance by eliminating potential distractions from irrelevant documents during pre-training.
- **Mechanism:** By conditioning token predictions only on previous tokens within the same document, intra-document causal masking removes the noise from unrelated documents. This allows the model to focus on learning the statistical patterns and knowledge within each document without being influenced by irrelevant context from other documents.
- **Core assumption:** The relatedness between documents in pre-training chunks is typically low, so conditioning on previous document tokens provides little useful signal for predicting the current token.
- **Evidence anchors:**
  - [abstract]: "In intra-document causal masking, the likelihood of each token is only conditioned on the previous tokens in the same document, eliminating potential distracting information from previous documents and significantly improving performance."
  - [section 3.2]: "When considering models trained via intra-document causal masking, we can see that INTRADoc achieves the lowest PPL compared to all models trained via causal masking. This indicates eliminating the potential distracting information from irrelevant documents during pre-training benefits the language modelling ability of models."
- **Break condition:** The mechanism breaks if the relatedness between documents in pre-training chunks is high enough that conditioning on previous document tokens becomes beneficial rather than distracting.

### Mechanism 3
- **Claim:** BM25Chunk improves performance by increasing document relatedness in pre-training chunks, reducing potential distractions.
- **Mechanism:** By retrieving and packing related documents using BM25 scoring, BM25Chunk creates pre-training chunks where documents share semantic or topical relevance. This increased relatedness means that when causal masking conditions predictions on previous document tokens, those tokens are more likely to provide useful context rather than being distracting noise.
- **Core assumption:** Related documents share sufficient semantic or topical similarity that conditioning on previous document tokens provides useful context for predicting current tokens.
- **Evidence anchors:**
  - [abstract]: "Furthermore, we find that concatenating related documents can reduce some potential distractions during pre-training, and our proposed efficient retrieval-based sequence construction method, BM25Chunk, can improve in-context learning (+11.6%), knowledge memorisation (+9.8%), and context utilisation (+7.2%) abilities of language models without sacrificing efficiency."
  - [section 2.1]: "To improve the relevance of documents in pre-training chunks, we employ a BM25-based retriever to construct pre-training chunks, referred to as BM25Chunk."
- **Break condition:** The mechanism breaks if the BM25 retrieval system fails to identify truly related documents, or if the relatedness threshold required for benefit is too high to achieve efficiently.

## Foundational Learning

- **Concept:** Language model pre-training and causal masking
  - **Why needed here:** Understanding how causal masking works and its role in language model pre-training is fundamental to grasping why sequence composition matters.
  - **Quick check question:** In causal masking, when predicting token x_i in a sequence, which tokens can the model condition on?
- **Concept:** Document packing and sequence composition strategies
  - **Why needed here:** The paper compares different strategies for combining documents into pre-training sequences, which directly affects model performance.
  - **Quick check question:** What is the difference between MIXChunk and UNIChunk in terms of document selection for packing?
- **Concept:** Document relatedness and retrieval-based packing
  - **Why needed here:** BM25Chunk uses document relatedness to improve pre-training, so understanding retrieval-based approaches is crucial.
  - **Quick check question:** How does BM25Chunk determine which documents to pack together in a pre-training chunk?

## Architecture Onboarding

- **Component map:** Document corpus -> Packing module (MIXChunk/UNIChunk/BM25Chunk) -> Masking strategy (causal/intra-document) -> Model architecture (LLaMA-based) -> Evaluation pipeline
- **Critical path:** Document selection → Chunk construction → Masking application → Model training → Evaluation
- **Design tradeoffs:**
  - Efficiency vs. performance: Intra-document causal masking improves performance but reduces efficiency by ~4%
  - Relatedness vs. diversity: BM25Chunk increases relatedness but may reduce document diversity
  - Retrieval cost vs. benefit: BM25Chunk uses retrieval which has computational cost but improves performance
- **Failure signatures:**
  - Poor downstream performance may indicate excessive distraction from unrelated documents
  - High perplexity may suggest the model is struggling with irrelevant context
  - Unexpectedly low improvement from BM25Chunk may indicate retrieval quality issues
- **First 3 experiments:**
  1. Compare perplexity of MIXChunk vs. UNIChunk to verify that avoiding cross-domain document packing helps
  2. Implement and test intra-document causal masking on a small dataset to verify the 4% efficiency degradation claim
  3. Test BM25Chunk with different buffer sizes to find the optimal configuration for balancing retrieval quality and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does intra-document causal masking scale with model size and training duration compared to causal masking?
- Basis in paper: [explicit] The authors state "Limited by the computation resources, we cannot conduct experiments on larger models with more pre-training steps" and note intra-document causal masking has "4% efficiency degradation" compared to causal masking.
- Why unresolved: The paper only tests 1.3B parameter models and cannot verify whether the performance gains of intra-document masking persist or diminish at larger scales.
- What evidence would resolve it: Pre-training experiments with 10B+ parameter models using both masking strategies for 200B+ tokens would reveal scaling behavior.

### Open Question 2
- Question: What is the optimal balance between document relatedness and diversity in pre-training sequences for maximizing downstream task performance?
- Basis in paper: [inferred] The authors show BM25Chunk improves performance by increasing document relatedness, but note they used a "diverse and high-quality pre-training dataset" and mention diversity's importance in related works.
- Why unresolved: The paper demonstrates benefits of increased relatedness but doesn't explore the trade-off with diversity or identify optimal relatedness thresholds.
- What evidence would resolve it: Systematic experiments varying BM25Chunk's retrieval parameters and measuring downstream performance across tasks would identify optimal relatedness-diversity trade-offs.

### Open Question 3
- Question: Does the burstiness property in BM25Chunk sequences come from improved document relevance or from increased data duplication?
- Basis in paper: [explicit] The authors state "duplication in pre-training sequences can also result in increased burstiness property" and analyze distinct n-grams but conclude "we will investigate the impact of duplication using different pre-training corpora in future work."
- Why unresolved: The paper observes BM25Chunk has lower Zipf coefficients (higher burstiness) but cannot definitively attribute this to relevance vs. duplication.
- What evidence would resolve it: Controlled experiments comparing BM25Chunk with deduplicated versions and measuring distinct n-grams vs. performance would isolate duplication effects.

## Limitations
- Results are limited to 1.3B parameter models; scalability to larger models is unknown
- The SlimPajama corpus composition may influence results differently than other pre-training datasets
- Intra-document causal masking introduces a 4% efficiency overhead that may become prohibitive at larger scales

## Confidence
**High Confidence:** The core finding that causal masking on concatenated documents introduces distracting information is well-supported by multiple experiments showing consistent performance degradation.

**Medium Confidence:** The effectiveness of intra-document causal masking and BM25Chunk is well-established for the specific experimental conditions, but potential scalability concerns reduce confidence slightly.

**Low Confidence:** The generalizability of these findings to larger models, different architectures, or other pre-training corpora remains uncertain.

## Next Checks
1. **Cross-corpora validation:** Pre-train models using BM25Chunk and intra-document masking on different corpora (e.g., C4, The Pile, or domain-specific collections) to verify that the performance improvements generalize beyond SlimPajama.

2. **Scale-up experiment:** Test the same sequence composition strategies on 7B-13B parameter models to determine whether the 4% efficiency degradation for intra-document masking and the perplexity improvements for BM25Chunk scale proportionally or exhibit different patterns at larger model sizes.

3. **Long-context evaluation:** Evaluate models with 16K-32K context windows using these sequence composition strategies to determine whether the distraction effects and performance benefits change with longer documents and larger context capacities.