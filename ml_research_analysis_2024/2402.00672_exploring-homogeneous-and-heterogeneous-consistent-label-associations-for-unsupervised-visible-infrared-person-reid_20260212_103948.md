---
ver: rpa2
title: Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised
  Visible-Infrared Person ReID
arxiv_id: '2402.00672'
source_url: https://arxiv.org/abs/2402.00672
tags:
- label
- mult
- cross-modality
- modality
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of unsupervised visible-infrared
  person re-identification (USL-VI-ReID), where the goal is to match pedestrian images
  across visible and infrared modalities without annotations. The core idea is to
  address the modality gap by establishing high-quality cross-modality label associations
  that preserve both homogeneous (within-modality) and heterogeneous (cross-modality)
  consistency.
---

# Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID

## Quick Facts
- arXiv ID: 2402.00672
- Source URL: https://arxiv.org/abs/2402.00672
- Authors: Lingfeng He; De Cheng; Nannan Wang; Xinbo Gao
- Reference count: 20
- Key outcome: The paper introduces a Modality-Unified Label Transfer (MULT) module and Online Cross-memory Label Refinement (OCLR) to improve unsupervised visible-infrared person re-identification, achieving state-of-the-art performance with 64.77% mAP and 65.34% Rank-1 on SYSU-MM01, and 89.95% mAP and 90.78% Rank-1 on RegDB.

## Executive Summary
This paper tackles the challenge of unsupervised visible-infrared person re-identification (USL-VI-ReID), where the goal is to match pedestrian images across visible and infrared modalities without annotations. The core idea is to address the modality gap by establishing high-quality cross-modality label associations that preserve both homogeneous (within-modality) and heterogeneous (cross-modality) consistency. To achieve this, the authors propose a Modality-Unified Label Transfer (MULT) module that models instance-level affinities to generate reliable soft pseudo-labels. Additionally, an Online Cross-memory Label Refinement (OCLR) module is introduced to handle noisy pseudo-labels, coupled with an Alternative Modality-Invariant Representation Learning (AMIRL) framework. Experiments on SYSU-MM01 and RegDB datasets show that the proposed method outperforms state-of-the-art USL-VI-ReID methods.

## Method Summary
The proposed method consists of three main components: Modality-Unified Label Transfer (MULT), Alternative Modality-Invariant Representation Learning (AMIRL), and Online Cross-memory Label Refinement (OCLR). MULT computes homogeneous and heterogeneous affinities using Jaccard similarity and optimal transport, respectively, to generate consistent soft pseudo-labels across modalities. AMIRL performs contrastive learning using intra-modality and cross-modality memory banks with an alternating training scheme. OCLR refines the pseudo-labels by leveraging predictions from intra-modality memory banks to reduce noise and improve modality alignment. The method is trained in two stages: first, a DCL baseline is trained for 40 epochs, followed by 50 epochs of AMIRL with MULT and OCLR.

## Key Results
- The proposed method achieves 64.77% mAP and 65.34% Rank-1 on SYSU-MM01 (All-search).
- On RegDB, the method attains 89.95% mAP and 90.78% Rank-1 for Visible-to-Infrared search.
- The method outperforms state-of-the-art USL-VI-ReID methods on both SYSU-MM01 and RegDB datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modeling homogeneous and heterogeneous affinities simultaneously preserves both within-modality and cross-modality consistency, leading to higher-quality pseudo-labels.
- Mechanism: The method computes pairwise instance-level affinities using Jaccard similarity for homogeneous cases and an optimal transport plan for heterogeneous cases. These affinities are then used to define inconsistency terms between the pseudo-label space and the feature space. By minimizing these inconsistency terms, the generated pseudo-labels maintain structural consistency both within and across modalities.
- Core assumption: Instance-level relationships contain sufficient information to guide accurate cross-modality label associations, and the affinities computed from these relationships can be effectively used to quantify inconsistency between pseudo-label and feature spaces.
- Evidence anchors:
  - [abstract]: "Our MULT excavates homogeneous and heterogeneous structural information by modeling affinities derived from pairwise instance relationships in feature space."
  - [section]: "To incorporate instance-level relationships, we model the homogeneous and heterogeneous affinities, denoted as Sho(e) and She."
  - [corpus]: Weak evidence - the corpus contains related papers on USL-VI-ReID but does not directly support this specific mechanism of using affinities for label association.
- Break condition: If the affinity computation fails to capture meaningful relationships between instances, or if the inconsistency terms do not accurately reflect the discrepancy between pseudo-labels and features, the method would not produce reliable pseudo-labels.

### Mechanism 2
- Claim: The alternating optimization process between visible and infrared modalities ensures convergence to consistent pseudo-labels across both modalities.
- Mechanism: The method uses an iterative process where pseudo-labels for one modality are updated based on the current pseudo-labels of the other modality, while also incorporating intra-modality consistency. This process continues until the pseudo-labels converge, ensuring that both modalities have consistent label assignments.
- Core assumption: The alternating optimization process will converge to a stable solution where pseudo-labels are consistent across both modalities.
- Evidence anchors:
  - [abstract]: "The proposed MULT ensures that the generated pseudo-labels maintain alignment across modalities while upholding structural consistency within intra-modality."
  - [section]: "During each iterative optimization process, the pseudo-labels ye are updated by alternating iterations of fixed points, which is formulated as Eq.16."
  - [corpus]: Weak evidence - while the corpus contains papers on USL-VI-ReID, it does not specifically address this alternating optimization mechanism.
- Break condition: If the alternating process does not converge, or if it converges to a solution where pseudo-labels are inconsistent across modalities, the method would fail to produce reliable results.

### Mechanism 3
- Claim: The Online Cross-memory Label Refinement (OCLR) module effectively mitigates the impact of noisy pseudo-labels while promoting modality alignment.
- Mechanism: The OCLR module uses predictions from intra-modality memory banks to refine predictions from the cross-modality memory bank. This refinement process is based on the principle of self-consistency, where predictions from different memory banks should be consistent with each other. By enforcing this consistency, the OCLR module reduces the impact of noisy pseudo-labels and promotes better modality alignment.
- Core assumption: Predictions from intra-modality and cross-modality memory banks should be consistent with each other, and this consistency can be used to refine noisy pseudo-labels.
- Evidence anchors:
  - [abstract]: "Additionally, a straightforward plug-and-play Online Cross-memory Label Refinement (OCLR) module is proposed to further mitigate the side effects of noisy pseudo-labels while simultaneously aligning different modalities."
  - [section]: "To overcome this issue, we utilize the predictions from the intra-modality memory banks to refine the predictions from the cross-modality learning bank."
  - [corpus]: Weak evidence - the corpus does not provide specific evidence for this OCLR mechanism, but it does contain papers on USL-VI-ReID that address the issue of noisy labels.
- Break condition: If the refinement process does not effectively reduce noise in the pseudo-labels, or if it introduces new errors, the OCLR module would not improve the overall performance of the method.

## Foundational Learning

- Concept: Optimal Transport (OT) for modeling cross-modality affinities
  - Why needed here: The OT framework provides a principled way to model the transition between instances from two different distributions (visible and infrared modalities), taking into account both instance-level relationships and the misalignment between the two modalities.
  - Quick check question: How does the OT formulation ensure that instances from both modalities are treated equally in the affinity computation?

- Concept: Graph-based affinity computation using Jaccard similarity
  - Why needed here: Jaccard similarity provides a way to measure the similarity between instances based on their shared neighbors in the feature space, which is particularly useful for capturing homogeneous (within-modality) affinities.
  - Quick check question: Why is Jaccard similarity preferred over other similarity measures for computing homogeneous affinities in this context?

- Concept: Memory bank-based contrastive learning
  - Why needed here: Memory banks provide a way to store and update prototypes for each cluster, enabling efficient contrastive learning across modalities without requiring the entire dataset to be loaded into memory.
  - Quick check question: How does the momentum update strategy for memory banks help in maintaining stable prototypes during training?

## Architecture Onboarding

- Component map: MULT -> AMIRL -> OCLR
- Critical path: MULT generates soft pseudo-labels based on instance-level affinities. AMIRL uses these pseudo-labels to perform contrastive learning and learn modality-invariant representations. OCLR refines the pseudo-labels during training to reduce noise and improve modality alignment.
- Design tradeoffs:
  - Computational cost vs. accuracy: Computing instance-level affinities and performing OT can be computationally expensive, but it leads to more accurate pseudo-labels.
  - Memory usage vs. stability: Using memory banks for contrastive learning requires additional memory, but it provides more stable prototypes compared to using mini-batch features directly.
- Failure signatures:
  - Poor pseudo-label quality: If the affinities computed by MULT do not capture meaningful relationships between instances, the generated pseudo-labels will be of low quality.
  - Slow convergence: If the alternating optimization process in MULT does not converge quickly, it will increase the training time.
  - Overfitting to noisy labels: If the OCLR module does not effectively reduce noise in the pseudo-labels, the model may overfit to incorrect labels.
- First 3 experiments:
  1. Validate affinity computation: Check if the homogeneous and heterogeneous affinities computed by MULT capture meaningful relationships between instances by visualizing the affinity matrices.
  2. Test label transfer: Verify that the label transfer process in MULT produces consistent pseudo-labels across modalities by comparing the pseudo-labels before and after transfer.
  3. Evaluate contrastive learning: Assess the effectiveness of the AMIRL framework by measuring the modality gap reduction and the quality of learned representations using cross-modality retrieval metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method's performance scale with the size of the training dataset, particularly in extremely limited annotation scenarios?
- Basis in paper: [inferred] The paper demonstrates strong performance on two public datasets (SYSU-MM01 and RegDB) but does not explore performance degradation or adaptation in scenarios with significantly fewer samples or identities.
- Why unresolved: The current experimental setup uses relatively large, well-established datasets. There is no analysis of how the method would perform with very small datasets or few-shot scenarios.
- What evidence would resolve it: Systematic experiments varying the number of training identities and samples per identity, showing performance curves as data availability decreases.

### Open Question 2
- Question: Can the Modality-Unified Label Transfer (MULT) module be extended to handle more than two modalities (e.g., visible, infrared, and thermal)?
- Basis in paper: [inferred] The paper focuses on a two-modality setting (visible and infrared). While the methodology is described in terms of instance-level affinities and consistency, the extension to multiple modalities is not explicitly addressed.
- Why unresolved: The current formulation and experimental validation are limited to the visible-infrared pairing. The generalization to a multi-modality setting is left unexplored.
- What evidence would resolve it: Successful application of the MULT module to a three or more modality setting, with demonstrated improvements in cross-modality matching accuracy.

### Open Question 3
- Question: What is the impact of different clustering algorithms on the quality of pseudo-labels generated by the MULT module, and can alternative clustering strategies further improve performance?
- Basis in paper: [explicit] The paper mentions using DBSCAN for initial clustering but does not extensively compare or analyze the impact of different clustering algorithms on the final performance.
- Why unresolved: The choice of clustering algorithm can significantly affect the initial pseudo-labels, which in turn influence the effectiveness of the MULT module. The sensitivity to clustering is not explored.
- What evidence would resolve it: Comparative experiments using various clustering algorithms (e.g., K-means, hierarchical clustering) and analyzing their impact on the quality of pseudo-labels and overall system performance.

## Limitations
- The computational complexity of the optimal transport-based affinity computation could limit scalability to larger datasets.
- The method relies on DBSCAN clustering for initial pseudo-label generation, which introduces sensitivity to hyperparameter choices.
- The alternating optimization scheme's convergence properties are not rigorously analyzed, and it's unclear how sensitive the method is to initialization.

## Confidence
- High confidence: The overall framework design and empirical results on standard benchmarks (SYSU-MM01 and RegDB). The ablation studies provide convincing evidence for the effectiveness of individual components.
- Medium confidence: The theoretical claims about affinity modeling and inconsistency minimization. While intuitively sound, the proofs and convergence analysis could be more rigorous.
- Low confidence: The exact impact of each hyperparameter (e.g., OT regularization strength, DBSCAN parameters) on final performance, as these are not thoroughly explored in the paper.

## Next Checks
1. **Affinity Quality Validation**: Implement visualization of homogeneous and heterogeneous affinity matrices to verify that meaningful relationships are being captured. Compare these to random or baseline affinity measures.

2. **Convergence Analysis**: Track the stability of pseudo-labels across alternating optimization iterations. Monitor if the method converges to consistent solutions across multiple runs with different random seeds.

3. **Ablation on Memory Bank Design**: Systematically evaluate the impact of different memory bank update strategies (momentum coefficient, temperature scaling) on final performance, as these details significantly affect contrastive learning quality.