---
ver: rpa2
title: Aligning Crowd Feedback via Distributional Preference Reward Modeling
arxiv_id: '2402.09764'
source_url: https://arxiv.org/abs/2402.09764
tags:
- preference
- human
- distribution
- response
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Distributional Preference Reward Model (DPRM),
  a framework that aligns large language models with diverse human preferences by
  modeling preferences as categorical distributions. The key innovation is using optimal
  transport-based loss to train a reward model that predicts human preference distributions,
  combined with a Bayesian updater to incorporate new or shifted preferences.
---

# Aligning Crowd Feedback via Distributional Preference Reward Modeling

## Quick Facts
- arXiv ID: 2402.09764
- Source URL: https://arxiv.org/abs/2402.09764
- Reference count: 40
- Primary result: DPRM achieves 28.0% win rate vs 12.7% for vanilla models across 6 persona evaluations

## Executive Summary
This paper introduces Distributional Preference Reward Model (DPRM), a framework that aligns large language models with diverse human preferences by modeling preferences as categorical distributions. The key innovation is using optimal transport-based loss to train a reward model that predicts human preference distributions, combined with a Bayesian updater to incorporate new or shifted preferences. The approach is evaluated across different LLM scales (OPT-2.7B, OPT-6.7B, Llama-2-7B) on dialogue tasks, showing significant improvements over baseline methods.

## Method Summary
DPRM trains a reward model to predict categorical preference distributions using optimal transport loss, then fine-tunes LLMs via PPO using scalarized rewards. The framework includes a Bayesian updater for incorporating new preferences, allowing the model to adapt to shifted or emerging preference distributions. The method was evaluated on dialogue tasks using synthetic preference data generated by LLMs simulating various personas.

## Key Results
- DPRM achieves 28.0% win rate against vanilla models (12.7%) in head-to-head evaluations
- Consistently outperforms baselines across all 6 evaluated personas (scientist, teacher, politician, teenager, CEO, artist)
- Maintains robust performance on out-of-distribution data while better aligning with population preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal transport loss better captures preference distribution discrepancies than cross-entropy loss
- Mechanism: OT loss uses transportation cost matrix based on reward differences between categories, allowing it to distinguish between similar but non-identical preference distributions
- Core assumption: Transportation cost matrix Mij accurately reflects the semantic distance between preference categories
- Evidence anchors:
  - [abstract]: "we design an optimal-transportation-based loss to calibrate DPRM to align with the preference distribution"
  - [section 3.2]: "CE loss would yield the same loss value for predicted distributions [0.9, 0, 0.1, 0, 0, 0] and [0.9, 0, 0, 0, 0, 0.1]. Despite their distinct implications"
  - [corpus]: Weak - neighboring papers focus on Bradley-Terry models and ordinal feedback, not OT loss specifically
- Break condition: If transportation cost matrix M doesn't reflect true semantic distances between categories, OT loss may misalign

### Mechanism 2
- Claim: Bayesian updater allows dynamic incorporation of new/shifted preferences
- Mechanism: Posterior preference distribution computed via weighted average of existing group preference and new user preference
- Core assumption: Individual preferences can be meaningfully aggregated into group preference distribution
- Evidence anchors:
  - [abstract]: "introduce a Bayesian updater to accommodate shifted or new preferences"
  - [section 3.1]: "we adopt the following operator to aggregate preferences for all users P G, ∀ui ∈ G to form a group preference"
  - [corpus]: Weak - neighboring papers discuss preference learning but not specifically Bayesian updating for dynamic preference incorporation
- Break condition: If individual preferences are too heterogeneous or contradictory, aggregation may lose meaningful signal

### Mechanism 3
- Claim: Scalarizing preference distribution via weighted sum preserves population preference signal
- Mechanism: Predicted preference distribution converted to scalar reward using reward table weights
- Core assumption: Weighted sum of preference distribution captures meaningful signal about overall preference quality
- Evidence anchors:
  - [section 3.3]: "we consolidate the preference distribution into a scalar reward to train LLMs through PPO"
  - [section 3.4]: "training the LLM to maximize the expected reward given by the DPRM is equivalent to align the LLM to generate responses more favoured by the population"
  - [corpus]: Moderate - neighboring papers discuss reward modeling but not specifically distributional reward approaches
- Break condition: If preference categories are not well-calibrated or reward table weights are inaccurate, scalarization loses signal

## Foundational Learning

- Optimal Transport Theory
  - Why needed here: Core to DPRM's loss function, provides geometric understanding of preference space
  - Quick check question: What is the computational complexity of solving the OT problem for d categories?
- Bayesian Updating
  - Why needed here: Enables dynamic incorporation of new preferences into existing preference distribution
  - Quick check question: How does the posterior distribution formula handle multiple new preferences arriving simultaneously?
- Reinforcement Learning from Human Feedback
  - Why needed here: Framework for fine-tuning LLM using reward signals from DPRM
  - Quick check question: What is the role of KL divergence regularization in the PPO update?

## Architecture Onboarding

- Component map: DPRM (backbone + distributional head) → OT loss training → scalar reward generation → PPO fine-tuning → aligned LLM
- Critical path: Preference distribution collection → DPRM training → Reward generation → Policy fine-tuning → Evaluation
- Design tradeoffs: OT loss provides better preference discrimination but higher computational cost vs cross-entropy
- Failure signatures: Overconfident preference distributions, poor OT loss convergence, misaligned scalar rewards
- First 3 experiments:
  1. Verify OT loss correctly distinguishes between semantically different preference distributions
  2. Test Bayesian updater with synthetic preference shifts
  3. Validate scalar reward generation preserves preference distribution information

## Open Questions the Paper Calls Out

- How does DPRM perform with real human preference data versus simulated persona data?
- Does the optimal transport loss provide meaningful improvements when the number of preference categories increases significantly?
- How sensitive is DPRM performance to the choice of personas used for posterior preference distribution?

## Limitations
- Evaluation relies heavily on synthetic preference data generated by LLMs simulating different personas
- Six-category preference system is a simplification that may miss important nuances in human feedback
- Transportation cost matrix M is critical but its construction methodology is not fully detailed

## Confidence

**High Confidence**: The technical framework for DPRM using optimal transport loss is well-specified and theoretically sound. The empirical results showing improved win rates against baselines are clearly demonstrated.

**Medium Confidence**: The claim that DPRM better captures population preferences is supported but relies on synthetic evaluation data. The effectiveness of the Bayesian updater for dynamic preference incorporation needs more extensive validation across different preference shift scenarios.

**Low Confidence**: The scalability of the approach to larger models and more complex preference distributions has not been thoroughly tested. The sensitivity of results to the choice of transportation cost matrix M and reward table weights is not explored.

## Next Checks

1. Evaluate DPRM on real human preference data from existing RLHF datasets to verify the approach generalizes beyond synthetic LLM-generated preferences.

2. Systematically vary the transportation cost matrix M to determine how sensitive the optimal transport loss is to different semantic distance assumptions between preference categories.

3. Test the Bayesian updater with synthetic preference data showing increasing levels of heterogeneity and contradiction to identify failure modes and quantify robustness limits.