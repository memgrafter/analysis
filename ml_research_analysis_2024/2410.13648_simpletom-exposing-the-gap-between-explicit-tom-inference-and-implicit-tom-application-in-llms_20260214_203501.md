---
ver: rpa2
title: 'SimpleToM: Exposing the Gap between Explicit ToM Inference and Implicit ToM
  Application in LLMs'
arxiv_id: '2410.13648'
source_url: https://arxiv.org/abs/2410.13648
tags:
- behavior
- person
- information
- mental
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SimpleToM, a new dataset for evaluating
  large language models'' (LLMs) Theory of Mind (ToM) reasoning capabilities. SimpleToM
  contains 1147 concise stories with implicit mental states, each followed by three
  types of questions: mental state inference, behavior prediction, and judgment of
  behavior.'
---

# SimpleToM: Exposing the Gap between Explicit ToM Inference and Implicit ToM Application in LLMs

## Quick Facts
- **arXiv ID**: 2410.13648
- **Source URL**: https://arxiv.org/abs/2410.13648
- **Reference count**: 40
- **Primary result**: LLMs excel at explicit Theory of Mind inference but struggle with applied ToM tasks like behavior prediction and judgment, showing a significant performance gap that can be reduced with task-specific interventions.

## Executive Summary
This paper introduces SimpleToM, a new dataset for evaluating large language models' Theory of Mind (ToM) reasoning capabilities. The dataset contains 1147 concise stories with implicit mental states, followed by three types of questions: mental state inference, behavior prediction, and judgment of behavior. The authors find that while models excel at explicit ToM questions (predicting mental states), they struggle significantly with applied ToM questions requiring behavior prediction and judgment. Even advanced models like GPT-4o achieve only 49.5% on behavior prediction and 15.3% on judgment questions. However, performance improves dramatically (up to 93.5% and 94.7% respectively) with task-specific interventions like mental state reminders and chain-of-thought prompting. The study reveals a critical gap between LLMs' explicit ToM knowledge and their ability to apply it in real-world scenarios, highlighting the need for more rigorous testing and improved model capabilities for appropriate social interaction.

## Method Summary
The authors developed SimpleToM, a dataset containing 1147 concise stories with implicit mental states, each followed by three types of questions: mental state inference (explicit ToM), behavior prediction, and judgment of behavior (applied ToM). The stories were designed to require understanding of characters' beliefs, desires, and intentions. The evaluation tested multiple LLM families including GPT-4o, Claude 3.5, Llama 3.1, and Qwen 2.5 on both zero-shot and few-shot settings. The study introduced task-specific interventions including mental state reminders (explicitly prompting models to consider characters' mental states) and chain-of-thought prompting to improve applied ToM performance. The authors systematically compared performance across the three question types to reveal the gap between explicit and applied ToM capabilities.

## Key Results
- LLMs show high accuracy (83.5%) on explicit mental state inference but poor performance on applied tasks: 49.5% on behavior prediction and 15.3% on judgment questions
- Task-specific interventions dramatically improve performance: mental state reminders boost behavior prediction to 84.1% and judgment to 92.7%, while CoT prompting achieves 93.5% and 94.7% respectively
- Even state-of-the-art models like GPT-4o exhibit significant gaps between explicit ToM knowledge and applied reasoning capabilities
- The performance gap persists across multiple model families and scales, suggesting a fundamental limitation in spontaneous ToM application

## Why This Works (Mechanism)
The study demonstrates that LLMs possess explicit ToM knowledge but fail to spontaneously apply it in behavior prediction and judgment tasks. The mechanism appears to involve a dissociation between learned mental state representations and their practical application in reasoning about actions and moral judgments. Task-specific interventions work by explicitly activating relevant mental state representations and guiding the reasoning process through chain-of-thought prompting, effectively bridging the gap between knowledge and application.

## Foundational Learning
**Theory of Mind (ToM)**: The ability to attribute mental states—beliefs, intents, desires, emotions, and knowledge—to oneself and others, understanding that others have beliefs, desires, intentions, and perspectives different from one's own. **Why needed**: Forms the foundation for social cognition and appropriate social interaction. **Quick check**: Can the model distinguish between what a character knows versus what is actually true in the story?

**Implicit vs Explicit ToM**: Implicit ToM involves automatically applying mental state understanding to predict behavior and make judgments, while explicit ToM involves consciously reasoning about mental states. **Why needed**: The study reveals a dissociation between these two forms of ToM in LLMs. **Quick check**: Does the model require explicit prompting to apply ToM knowledge in applied tasks?

**Chain-of-Thought Prompting**: A prompting technique that encourages models to generate intermediate reasoning steps before providing a final answer. **Why needed**: Shown to significantly improve applied ToM performance by making reasoning explicit. **Quick check**: Does the model's reasoning process become more coherent and accurate with CoT prompting?

**Mental State Attribution**: The cognitive process of inferring what others believe, want, or intend based on available evidence. **Why needed**: Central to both explicit and applied ToM tasks in the SimpleToM dataset. **Quick check**: Can the model accurately infer mental states from minimal contextual cues?

**Behavior Prediction**: The ability to anticipate how someone will act based on their mental states and circumstances. **Why needed**: A key applied ToM task that LLMs struggle with without interventions. **Quick check**: Does the model correctly predict actions based on inferred mental states?

## Architecture Onboarding

**Component Map**: SimpleToM Dataset -> LLM Evaluation Framework -> Performance Analysis Pipeline -> Intervention Testing Module

**Critical Path**: Story Generation → Question Formulation → LLM Evaluation (Zero-shot/Few-shot) → Performance Analysis → Intervention Testing → Results Interpretation

**Design Tradeoffs**: The dataset prioritizes concise stories for efficiency but may sacrifice contextual richness. Zero-shot evaluation tests general capabilities but may not reveal full potential compared to few-shot learning. Task-specific interventions improve performance but may not reflect spontaneous capabilities.

**Failure Signatures**: Models fail to connect inferred mental states to predicted behaviors, show inconsistent moral judgments despite correct mental state attribution, and exhibit poor generalization from explicit ToM knowledge to applied scenarios without explicit prompting.

**3 First Experiments**:
1. Evaluate baseline performance on explicit mental state inference questions across all model families
2. Test behavior prediction performance with and without mental state reminder interventions
3. Compare chain-of-thought prompting versus mental state reminders for judgment questions

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size (1147 stories) may not capture full complexity of real-world social reasoning scenarios
- Interventions that dramatically improve performance raise questions about whether improvements reflect genuine understanding or pattern matching
- Binary evaluation metrics may oversimplify nuanced human judgments about social behavior
- The gap between explicit and applied ToM may reflect prompting deficiencies rather than fundamental limitations

## Confidence
- **High confidence**: LLMs achieve high accuracy on explicit mental state inference tasks
- **Medium confidence**: The significant performance gap between explicit and applied ToM is real but interpretation is complex due to intervention effects
- **Medium confidence**: Task-specific interventions dramatically improve applied ToM performance
- **Medium confidence**: The gap represents a fundamental limitation in spontaneous ToM application

## Next Checks
1. Test whether performance gaps persist with larger, more diverse story datasets including complex social scenarios and multiple characters
2. Evaluate whether fine-tuning models specifically on applied ToM tasks reduces the performance gap compared to prompting interventions alone
3. Conduct human evaluation studies to establish baseline performance and validate whether model judgments align with human reasoning patterns for behavior prediction and moral judgment tasks