---
ver: rpa2
title: Tracking the perspectives of interacting language models
arxiv_id: '2406.11938'
source_url: https://arxiv.org/abs/2406.11938
tags:
- system
- perspective
- data
- communication
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the perspective space framework to study
  information diffusion in systems of interacting language models (LLMs). The perspective
  space uses embedding-based data kernels to create Euclidean representations of models
  that capture differences in their responses to fixed prompts.
---

# Tracking the perspectives of interacting language models

## Quick Facts
- arXiv ID: 2406.11938
- Source URL: https://arxiv.org/abs/2406.11938
- Reference count: 14
- One-line primary result: Introduces perspective space framework to study information diffusion in LLM systems, showing communication structure affects model convergence and polarization patterns

## Executive Summary
This paper introduces the perspective space framework to study information diffusion in systems of interacting language models. The framework uses embedding-based data kernels to create Euclidean representations of models that capture differences in their responses to fixed prompts, enabling quantitative analysis of LLM communication networks. By formalizing LLM systems as graphs and systematically studying different network structures, the authors demonstrate how communication patterns affect information diffusion, model convergence, and polarization. The framework provides tools for analyzing LLM systems and has potential applications for monitoring human-model forums.

## Method Summary
The authors formalize LLM communication networks as graphs where vertices represent models and edges represent interaction pathways. Models interact sequentially through a fine-tuning mechanism where one model queries another and then fine-tunes on the resulting outputs. Perspective space is constructed by embedding all model responses to fixed evaluation prompts using a surrogate embedding function, computing pairwise distances, and applying classical MDS to create Euclidean representations. The system tracks how model perspectives evolve over time under different communication structures, network disruptions, and adversarial conditions.

## Key Results
- Communication disruptions lead to different model sink patterns, with some networks showing global convergence while others maintain local clusters
- Adversarial models can compromise entire networks even when targeting only a minority of nodes
- Communication restrictions increase polarization between model classes, creating distinct perspective clusters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The perspective space captures model differences by embedding responses to fixed prompts rather than model weights.
- Mechanism: Instead of comparing inaccessible model weights, the framework queries each model with identical prompts, embeds the responses using a surrogate embedding function, and uses classical MDS to create Euclidean representations where distances reflect response differences.
- Core assumption: Different models produce distinguishable responses to the same prompts when embedded with an appropriate surrogate function.
- Evidence anchors:
  - [abstract] "The perspective space uses embedding-based data kernels to create Euclidean representations of models that capture differences in their responses to fixed prompts."
  - [section] "We introduce the perspective space of a collection of models to address the gap in quantitative methods for studying the diversity and evolution of model responses."
- Break condition: If the surrogate embedding function cannot distinguish between model responses (e.g., using a random mapping), the perspective space loses discriminative power.

### Mechanism 2
- Claim: Communication network structure directly controls information diffusion patterns and model convergence behavior.
- Mechanism: By modeling LLM systems as graphs where vertices represent models/databases and edges represent influence pathways, different topologies (fully connected, local-only, adversarial targeting) create distinct diffusion dynamics captured through perspective space evolution.
- Core assumption: Information flow through network edges creates measurable changes in model perspectives that persist and can be tracked over time.
- Evidence anchors:
  - [abstract] "Results show that: 1) communication disruptions lead to different model sink patterns (global vs local), 2) adversarial models can compromise entire networks when targeting even a minority of nodes, and 3) communication restrictions increase polarization between model classes."
  - [section] "We formalize the system of interacting language models as a graph. The formalization enables systematic study of the effect of different communication structures on information diffusion that is otherwise not possible."
- Break condition: If models update too infrequently or perturbations are too small relative to inherent model variance, network structure effects become indistinguishable.

### Mechanism 3
- Claim: Model sinks emerge as stable states where models converge to specific response patterns despite ongoing interactions.
- Mechanism: Through repeated fine-tuning on outputs from other models, the system evolves toward configurations where individual model perspectives stop changing significantly, creating stable "sinks" in the perspective space.
- Core assumption: The fine-tuning update mechanism combined with network structure drives the system toward attractor states in the response space.
- Evidence anchors:
  - [abstract] "Results show that: 1) communication disruptions lead to different model sink patterns (global vs local)"
  - [section] "For the system that does not experience a disruption (top left), the exploration in perspective eventually stagnates and each model appears to oscillate between three different global perspective 'sinks'"
- Break condition: If the evaluation prompt set is too small or unrepresentative, apparent convergence may reflect evaluation bias rather than true model stabilization.

## Foundational Learning

- Concept: Graph theory and network topology
  - Why needed here: The paper models LLM systems as graphs where structure determines information flow patterns.
  - Quick check question: What's the difference between a fully connected network and a local-only network in terms of information propagation speed and pattern?

- Concept: Classical multidimensional scaling (CMDS)
  - Why needed here: CMDS transforms pairwise distance matrices into Euclidean embeddings for visualizing model differences.
  - Quick check question: Given a distance matrix between models, what does CMDS produce and how is it interpreted?

- Concept: Surrogate embedding functions
  - Why needed here: Since some models don't provide native embedding functions, surrogate embeddings enable response comparison across different architectures.
  - Quick check question: Why can't we directly compare model responses, and how does using a common embedding function solve this?

## Architecture Onboarding

- Component map: Base LLM models (Pythia variants) → Fine-tuning module → Interaction engine → Surrogate embedding → Perspective space computation → Analysis/visualization
- Critical path: 1. Initialize models with different fine-tuning data 2. At each time step: select interaction pairs based on network structure 3. Execute interactions (model A queries model B, fine-tunes on results) 4. Query all models with evaluation prompts 5. Embed responses using surrogate function 6. Compute pairwise distances and apply CMDS 7. Analyze perspective space evolution
- Design tradeoffs:
  - Prompt selection vs generalization: more diverse prompts capture more variation but increase computational cost
  - Embedding function choice: general-purpose vs domain-specific affects sensitivity to model differences
  - Network structure complexity vs interpretability: more complex structures create richer dynamics but harder analysis
- Failure signatures:
  - All models clustering together regardless of initialization → evaluation prompts not discriminative enough
  - Perspective space shows random patterns → surrogate embedding function inadequate
  - No convergence to sinks → update frequency too low or perturbations too small
- First 3 experiments:
  1. Test perspective space sensitivity: Initialize two identical models, fine-tune one on different data, verify they separate in perspective space
  2. Validate network structure effects: Compare fully connected vs star topology on convergence speed and final configurations
  3. Stress test adversarial impact: Introduce adversarial model targeting different numbers of victims, measure polarization changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of embedding function affect the sensitivity and stability of the perspective space in tracking model evolution?
- Basis in paper: [explicit] The paper discusses using surrogate embedding functions and their impact on capturing model differences, but does not explore the sensitivity of different embeddings.
- Why unresolved: The paper does not compare multiple embedding functions or assess their robustness in capturing nuanced model changes over time.
- What evidence would resolve it: Systematic experiments comparing multiple embedding functions (e.g., general-purpose vs. domain-specific) on the same systems, measuring their ability to detect known model differences and track evolution.

### Open Question 2
- Question: What is the long-term behavior of model sinks when communication networks are allowed to evolve dynamically over extended time periods?
- Basis in paper: [explicit] The paper observes model sinks in several simulations but only studies systems up to 50 time steps.
- Why unresolved: The paper does not investigate whether sinks are permanent attractors or if models eventually escape them in longer simulations.
- What evidence would resolve it: Simulations extending to hundreds or thousands of time steps with varying network structures and model initializations to determine the permanence of sinks.

### Open Question 3
- Question: How does the interaction mechanic (one model interviewing another) affect the observed dynamics compared to other potential communication mechanisms?
- Basis in paper: [explicit] The paper uses a specific interaction mechanic but acknowledges it may not reflect realistic human communication patterns.
- Why unresolved: The paper does not compare results with alternative interaction mechanics such as group discussions, iterative refinement, or debate-style interactions.
- What evidence would resolve it: Comparative simulations using different interaction mechanics while keeping other parameters constant to isolate the effect of communication method on system dynamics.

## Limitations
- Empirical validation is constrained by relatively small number of models (25) and specific network structures tested
- Framework's sensitivity to evaluation prompt selection remains unclear - results may reflect prompt-specific biases
- Computational cost of fine-tuning models on interaction outputs limits scalability to larger systems

## Confidence

- **High confidence**: The mathematical framework for perspective space construction (embedding responses, computing distances, applying CMDS) is well-established and reproducible.
- **Medium confidence**: The three simulation case studies demonstrate clear patterns, but results are specific to the chosen parameters and may not generalize to other network structures or update frequencies.
- **Low confidence**: The claim that perspective space can serve as a proxy for human community dynamics requires additional validation beyond the current LLM simulations.

## Next Checks

1. **Prompt robustness test**: Systematically vary the evaluation prompt set (size, diversity, topic coverage) and measure stability of perspective space patterns across 10+ different prompt configurations.

2. **Scaling experiment**: Replicate the case studies with 50+ models and different fine-tuning frequencies to assess whether the observed phenomena persist at scale and under varying update rates.

3. **Adversarial attack validation**: Design and test multiple adversarial strategies (beyond the single "targeted fine-tuning" approach used) to establish whether the vulnerability to minority adversarial nodes is a robust phenomenon or parameter-dependent artifact.