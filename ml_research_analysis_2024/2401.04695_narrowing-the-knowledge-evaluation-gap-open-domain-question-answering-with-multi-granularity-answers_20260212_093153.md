---
ver: rpa2
title: 'Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with
  Multi-Granularity Answers'
arxiv_id: '2401.04695'
source_url: https://arxiv.org/abs/2401.04695
tags:
- answers
- granola
- answer
- question
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses a knowledge evaluation gap in standard QA settings,
  where factual questions can have correct answers at different levels of granularity,
  but are typically evaluated against only one specific level. To solve this, the
  authors propose GRANOLA QA, a novel evaluation setting where predicted answers are
  evaluated against a set of multi-granularity answers.
---

# Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers

## Quick Facts
- arXiv ID: 2401.04695
- Source URL: https://arxiv.org/abs/2401.04695
- Reference count: 22
- Primary result: Standard QA evaluation underestimates LLM knowledge, especially for rare entities; DRAG decoding improves accuracy by nearly 20 points

## Executive Summary
This paper addresses a critical gap in standard QA evaluation where factual questions can have correct answers at different levels of granularity, but are typically evaluated against only one specific level. The authors propose GRANOLA QA, a novel evaluation setting that assesses predicted answers against multi-granularity reference answers. They introduce GRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset, and a new decoding strategy called DRAG that aligns response granularity with model uncertainty through response aggregation. Experiments show that standard evaluation significantly underestimates model knowledge, particularly for rare entities, while DRAG dramatically improves both accuracy and informativeness.

## Method Summary
The paper proposes a comprehensive framework for evaluating knowledge in open-domain QA with multi-granularity answers. First, they create GRANOLA-EQ by enriching existing QA pairs with multi-granularity answers using an external knowledge graph (Wikidata) and LLM prompting to generate ordered lists of answers at varying granularity levels. Second, they introduce GRANOLA QA evaluation with two metrics: GRANOLA accuracy (checking if there is a match against any of the multi-granularity answers) and GRANOLA informativeness (a weighted score prioritizing fine-grained correct answers). Third, they propose DRAG (Decoding with Response Aggregation), a decoding strategy that samples multiple responses at temperature T, then aggregates them using an LLM to output the most specific answer consistent with all samples, aiming to align response granularity with model uncertainty.

## Key Results
- Standard decoding shows only a small accuracy gap between GRANOLA and standard evaluation
- DRAG yields a nearly 20-point increase in accuracy on average by generating coarser answers
- The accuracy gap is larger for rare entities, suggesting models have coarser knowledge of them
- DRAG improves both accuracy and informativeness compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
Standard QA evaluation compares predicted answers to a single set of reference answers of the most specific granularity. GRANOLA QA instead evaluates against a ranked set of multi-granularity reference answers, using accuracy and informativeness metrics that reward coarser but still correct answers. This recognizes that language models often generate more detailed responses than their knowledge can support, leading to incorrect specific answers even when they know a correct coarser answer.

### Mechanism 2
DRAG samples N responses at temperature T, then aggregates them using an LLM to output the most specific answer consistent with all samples. This encourages coarser answers when the model is uncertain about specific details, aligning response granularity with the model's uncertainty level. Human responses naturally adjust granularity to uncertainty level, and models can be made to do the same through aggregation.

### Mechanism 3
Multi-granularity answers are generated by abstracting from knowledge graph entity descriptions. The procedure extracts entities from QA pairs, retrieves their descriptions from Wikidata, then prompts an LLM to generate ordered lists of answers at varying granularity levels based on these descriptions. Knowledge graphs contain hierarchical information about entities that can be used to generate coarser answers through abstraction.

## Foundational Learning

- **Open-domain question answering with retrieval**: Why needed - The paper assumes familiarity with standard QA evaluation metrics and the retrieval-augmented generation paradigm. Quick check - What is the difference between open-domain QA and closed-book QA?
- **Knowledge graphs and entity abstraction**: Why needed - The multi-granularity answer generation procedure relies on extracting entities from QA pairs and finding their descriptions in Wikidata. Quick check - How can entity properties in a knowledge graph be used to create coarser answers?
- **Decoding strategies and temperature sampling**: Why needed - DRAG uses temperature sampling to generate diverse responses, then aggregates them. Understanding how temperature affects response diversity is crucial. Quick check - How does temperature sampling differ from greedy decoding in terms of response diversity?

## Architecture Onboarding

- **Component map**: GRANOLA QA evaluation system -> Answer generation pipeline -> DRAG decoder -> Evaluation framework
- **Critical path**: Load GRANOLA-EQ dataset -> Generate predictions using different decoding strategies -> Evaluate using GRANOLA accuracy and informativeness -> Analyze results
- **Design tradeoffs**: Granularity vs accuracy (finer-grained answers are more informative but harder to get right), Sampling temperature (higher temperature increases diversity but may introduce noise), Aggregation method (different strategies may yield different granularity adjustments)
- **Failure signatures**: Low GRANOLA accuracy despite high standard accuracy (model generating specific but incorrect answers), High IDK rate (model being overly conservative), Poor DRAG performance (aggregation failing to find consistent properties)
- **First 3 experiments**: Run greedy decoding on GRANOLA-EQ and compare GRANOLA accuracy vs standard accuracy, Run DRAG with different temperature values and analyze how granularity changes, Compare DRAG performance on popular vs rare entities to verify the knowledge evaluation gap effect

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of aggregation method in DRAG impact the balance between accuracy and informativeness? The paper mentions that the aggregation stage of DRAG can be implemented in different ways, such as via prompting an LLM, and suggests that different aggregators could improve downstream task performance. Experiments comparing DRAG with different aggregation methods (majority voting, weighted averaging, or other LLM-based approaches) on GRANOLA-EQ, measuring both accuracy and informativeness across various hyperparameter settings would resolve this.

### Open Question 2
How does the quality of multi-granularity answers generated by LLMs vary with the complexity or specificity of the original question? The paper describes a methodology for generating multi-granularity answers using an LLM conditioned on entity descriptions from Wikidata, but notes that there is headroom for improving the answer generation procedure. A systematic evaluation of the generated multi-granularity answers across different question types, relation categories, and entity popularity levels in GRANOLA-EQ, including human judgments on correctness, informativeness, and granularity would resolve this.

### Open Question 3
Can the knowledge evaluation gap observed in LLMs be mitigated through fine-tuning or prompting strategies that encourage generating answers at an appropriate level of granularity? The paper discusses how standard decoding strategies tend to generate detailed but incorrect responses, and introduces DRAG as a decoding strategy to align response granularity with model uncertainty. Experiments comparing standard fine-tuned models with models fine-tuned or prompted to generate answers at varying levels of granularity, evaluated using GRANOLA metrics on GRANOLA-EQ would resolve this.

## Limitations
- The multi-granularity answer generation process relies heavily on Wikidata and LLM prompting, which may introduce biases or gaps
- DRAG's effectiveness depends on the aggregation LLM's ability to identify consistent properties across samples
- The study focuses on entity-based factual questions, potentially limiting generalizability to other QA types

## Confidence
- **High confidence**: The knowledge evaluation gap exists and standard evaluation underestimates model knowledge
- **Medium confidence**: DRAG effectively aligns response granularity with model uncertainty
- **Medium confidence**: The multi-granularity answer generation methodology produces useful answer hierarchies

## Next Checks
1. **Ablation study on DRAG components**: Test DRAG with different numbers of samples (N) and temperatures (T) to quantify the impact of each component on accuracy and informativeness
2. **Cross-dataset generalization**: Evaluate DRAG on additional multi-granularity datasets beyond GRANOLA-EQ to test the robustness of the findings
3. **Comparison with retrieval-augmented approaches**: Test whether DRAG's benefits persist when combined with retrieval-augmented generation, addressing the limitation of evaluating only closed-book models