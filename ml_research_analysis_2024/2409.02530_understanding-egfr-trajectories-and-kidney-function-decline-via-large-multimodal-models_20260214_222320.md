---
ver: rpa2
title: Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal
  Models
arxiv_id: '2409.02530'
source_url: https://arxiv.org/abs/2409.02530
tags:
- egfr
- data
- lmms
- patient
- kidney
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that Large Multimodal Models (LMMs) can
  predict future estimated Glomerular Filtration Rate (eGFR) levels with performance
  comparable to traditional machine learning models. Using a dataset of 50 patients
  with 564 eGFR measurements, the research integrated LMMs with various prompting
  techniques and visual representations of eGFR trajectories.
---

# Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models

## Quick Facts
- arXiv ID: 2409.02530
- Source URL: https://arxiv.org/abs/2409.02530
- Reference count: 40
- LMMs predict eGFR levels with performance comparable to traditional ML models

## Executive Summary
This study demonstrates that Large Multimodal Models (LMMs) can effectively predict future estimated Glomerular Filtration Rate (eGFR) levels for chronic kidney disease management. Using a dataset of 50 patients with 564 eGFR measurements, the research integrated LMMs with various prompting techniques and visual representations of eGFR trajectories. The ensemble approach using LMMs achieved MAE values of 2.21-4.10 and MAPE values of 8.97-25.12%, with Gemini Pro Vision and Gemini Flash models showing the strongest performance. This work establishes LMMs as a viable tool for medical forecasting in CKD management, marking the first application of these models for eGFR prediction.

## Method Summary
The study used a dataset of 50 patients with 564 eGFR measurements, incorporating laboratory and clinical variables including demographics, laboratory measurements, comorbidities, and medications. Four LMMs (Gemini Flash, Gemini Pro Vision, GPT-4o, and Claude 3 Opus) were employed with four prompt types: fill-in-the-blank, descriptive, open-ended, and role-playing. The models processed line charts showing eGFR trajectories over time combined with clinical text inputs. Predictions were evaluated using MAE and MAPE metrics and compared against traditional models (Random Forest and 1D-CNN). An ensemble approach combined multiple LMMs and prompts to improve prediction stability.

## Key Results
- LMM ensemble achieved MAE values of 2.21-4.10 and MAPE values of 8.97-25.12%
- Gemini Pro Vision and Gemini Flash models demonstrated the strongest individual performance
- Ensemble methods provided predictive performance comparable to existing ML models
- LMMs successfully processed multimodal inputs (visual + text) for medical forecasting

## Why This Works (Mechanism)

### Mechanism 1
LMMs outperform traditional models on eGFR prediction because they can jointly process visual trajectory plots and clinical text, capturing nonlinear temporal patterns that RF and 1D-CNN miss. The multimodal input allows the model to integrate sequential eGFR trends (visual) with contextual clinical variables (text) in a single forward pass, leveraging pre-trained cross-modal embeddings. Core assumption: The visual representation of eGFR trajectories encodes clinically relevant temporal dynamics not fully recoverable from tabular data alone. Break condition: If the visual trajectory plot adds no new information beyond the numeric series, or if the LMM cannot meaningfully fuse image and text modalities for tabular forecasting.

### Mechanism 2
Prompt engineering substantially improves LMM prediction accuracy by constraining the output format and contextualizing the input data. Carefully designed prompts reduce ambiguity, guide the model to focus on clinically relevant features, and enforce a specific prediction format (e.g., "fill in the blank"). Core assumption: LMMs are sensitive to prompt phrasing and benefit from explicit instructions when performing regression-style tasks on structured data. Break condition: If prompts do not meaningfully alter output accuracy, or if the LMM defaults to generic responses regardless of prompt variation.

### Mechanism 3
Ensembling multiple LMMs and averaging predictions yields more robust performance than any single model. Different LMMs capture complementary aspects of the input (visual, textual, contextual), and averaging reduces variance in predictions. Core assumption: Each LMM contributes unique strengths; their errors are not perfectly correlated. Break condition: If ensemble predictions are no better than the best single model, or if ensembling increases variance or computational cost without accuracy gains.

## Foundational Learning

- **Concept: Multimodal model inference pipeline**
  - Why needed here: Understanding how image + text + structured data flows through an LMM for regression is critical for debugging and extending the system
  - Quick check question: What are the three distinct input modalities in this framework, and in what order are they processed by the LMM?

- **Concept: Prompt engineering for structured outputs**
  - Why needed here: The model must be guided to output a single numeric value in a consistent format; otherwise, post-processing becomes unreliable
  - Quick check question: Which prompt template in the appendix enforces a fill-in-the-blank style for the predicted eGFR value?

- **Concept: Evaluation metrics for regression (MAE, MAPE)**
  - Why needed here: These metrics directly quantify prediction error and enable fair comparison between LMMs and traditional ML models
  - Quick check question: If a model predicts eGFR = 45 for an actual value of 50, what is the absolute percentage error?

## Architecture Onboarding

- **Component map**: Line chart image + clinical/lab variable text + prompt text -> LMM inference engine -> unstructured text -> Output extraction module -> Ensemble averaging -> Evaluation module
- **Critical path**: 1. Generate line chart image for patient M-th plot 2. Combine with clinical variables and chosen prompt 3. Run through LMM → unstructured text 4. Extract eGFR prediction 5. Repeat steps 2-4 for all prompts and models 6. Average predictions → final output 7. Evaluate on test set
- **Design tradeoffs**: Multimodal fusion vs. tabular-only (visual plots may add context but increase inference cost and complexity); Prompt diversity vs. consistency (multiple prompts improve robustness but risk inconsistent formats); Ensemble size vs. latency (more models improve accuracy but slow real-time deployment)
- **Failure signatures**: Consistently high MAE/MAPE across all models (possible data quality or prompt design issue); One model dominates ensemble (ensemble may be redundant); Extraction module fails (prompts not yielding structured numeric output); Visual input ignored (LMM not effectively processing line chart modality)
- **First 3 experiments**: 1. Run each LMM individually with a single prompt (e.g., prompt 1) and compare MAE/MAPE to RF baseline; 2. Test prompt variation impact by running all four prompts on one LMM and measuring performance spread; 3. Implement LMM ensemble with prompt 1 and compare against best single-model performance

## Open Questions the Paper Calls Out

### Open Question 1
How do different visual representations of eGFR trajectories affect LMMs' predictive performance compared to line charts? The paper mentions that "different graphical representations might influence the LMMs' understanding of the problem, suggesting the need for a comprehensive experiment to identify the most effective visual formats" but only used line charts. What evidence would resolve it: Direct comparison of LMM performance using identical datasets but different visualization methods (line charts vs heat maps vs scatter plots) while measuring MAE and MAPE across the same evaluation metrics.

### Open Question 2
What is the optimal number of patient data points (M) to include in LMM predictions for maximum accuracy? The paper states "Each patient has M plots, where M is determined by the median count of data points available per patient" but does not explore whether this is optimal. What evidence would resolve it: Systematic experimentation varying M (e.g., using minimum, maximum, mean, or percentile-based data points) and measuring the impact on prediction accuracy across different patient subgroups.

### Open Question 3
How does LMM performance for eGFR prediction vary across different CKD stages and patient demographics? The paper mentions the dataset included patients with eGFR ranging from 2.44 to 171.85 and ages from 19.5 to 87.6, but does not report performance stratified by these characteristics. What evidence would resolve it: Performance analysis disaggregated by CKD stage, age groups, and other demographic factors, showing whether LMMs perform equally well across all patient populations or if certain subgroups show systematically better or worse predictions.

## Limitations

- Limited dataset size (50 patients, 564 measurements) raises questions about generalizability to broader CKD populations
- Performance metrics show substantial variation across models and prompts (MAE: 2.21-4.10, MAPE: 8.97-25.12%), suggesting sensitivity to implementation details
- No ablation studies on visual vs. text-only inputs to quantify the contribution of line chart representations
- Lack of comparison with state-of-the-art deep learning approaches beyond 1D-CNN

## Confidence

- **High confidence**: LMMs can process multimodal inputs (visual + text) and generate eGFR predictions with measurable accuracy
- **Medium confidence**: Ensemble methods improve prediction stability, though magnitude of improvement is unclear
- **Low confidence**: Visual trajectory plots provide meaningful additional information beyond clinical variables alone

## Next Checks

1. **Ablation study**: Run identical experiments with text-only inputs (remove line charts) to quantify visual modality contribution
2. **External validation**: Test the best-performing LMM ensemble on an independent CKD dataset to assess generalizability
3. **Sensitivity analysis**: Systematically vary prompt templates and measure impact on prediction accuracy to identify optimal prompt structures