---
ver: rpa2
title: A General and Efficient Training for Transformer via Token Expansion
arxiv_id: '2404.00672'
source_url: https://arxiv.org/abs/2404.00672
tags:
- training
- token
- tokens
- transformer
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel token growth scheme called Token Expansion
  (ToE) to achieve consistent training acceleration for Vision Transformers (ViTs).
  The core idea is to maintain the integrity of the intermediate feature distribution
  during the accelerated training process by introducing an "initialization-expansion-merging"
  pipeline.
---

# A General and Efficient Training for Transformer via Token Expansion

## Quick Facts
- arXiv ID: 2404.00672
- Source URL: https://arxiv.org/abs/2404.00672
- Authors: Wenxuan Huang; Yunhang Shen; Jiao Xie; Baochang Zhang; Gaoqi He; Ke Li; Xing Sun; Shaohui Lin
- Reference count: 40
- Primary result: Achieves ~1.3× faster training for DeiT-Tiny and DeiT-Small with negligible accuracy drop or improvement

## Executive Summary
This paper proposes Token Expansion (ToE), a novel training acceleration method for Vision Transformers that reduces computational cost by progressively decreasing the number of tokens during training. The method maintains feature distribution integrity through a three-stage pipeline: initialization, expansion, and merging. Extensive experiments demonstrate ToE can accelerate training in a lossless manner or even with performance gains over full-token training baselines across multiple datasets and transformer architectures.

## Method Summary
Token Expansion (ToE) accelerates ViT training by reducing token count through an "initialization-expansion-merging" pipeline applied to the output of the first transformer block. The method begins with spatial-distribution initialization (regular sampling), followed by widest feature-distribution expansion (iterative selection of farthest tokens using cosine distance), and ends with feature-distribution merging (averaging close tokens). Training is divided into multiple stages where token count progressively increases to maintain feature integrity. The approach claims general applicability across transformer architectures without requiring architectural changes or modified hyperparameters.

## Key Results
- Achieves ~1.3× faster training for DeiT-Tiny and DeiT-Small with negligible accuracy drop
- Demonstrates consistent acceleration across multiple transformer architectures (DeiT, LV-ViT, EfficientTrain)
- Shows performance gains on some configurations while maintaining accuracy on others
- Reduces computational complexity by decreasing token count in self-attention layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Widest feature-distribution token expansion preserves intermediate feature space integrity by ensuring selected tokens cover diverse feature regions
- Mechanism: Computes pairwise distances between unselected and selected tokens using cosine similarity, repeatedly selecting the token whose feature distribution is farthest from any selected token
- Core assumption: Feature distributions of tokens can be meaningfully compared using cosine distance, and distant tokens contain complementary information
- Evidence anchors: [abstract] mentions "widest feature-distribution token expansion" to make feature distributions "as wide as possible"; [section] discusses maintaining "integrity of the intermediate feature distribution"
- Break condition: If feature space is not well-separated or tokens are redundant, expansion may select similar tokens, failing to preserve diversity

### Mechanism 2
- Claim: Feature-distribution token merging retains information from unselected tokens by averaging close tokens into selected ones
- Mechanism: After expansion, unselected tokens are merged into the nearest selected token based on feature distribution distance
- Core assumption: Merging close tokens doesn't significantly distort feature distribution, and merged representation captures essential information
- Evidence anchors: [abstract] states "we merge the unselected token set (blue boxes) into the selected one (red boxes) with the close feature distributions"; [section] confirms merging "unselected tokens that the feature distributions are close to the selected ones"
- Break condition: If tokens are too dissimilar, merging may cause feature drift and degrade performance

### Mechanism 3
- Claim: Spatial-distribution token initialization ensures broad spatial coverage of image patches in initial token set
- Mechanism: Tokens are selected at regular spatial intervals (every ⌊1/r₀⌋ tokens), ensuring initial set covers entire image patch space
- Core assumption: Early spatial diversity is critical for maintaining downstream feature integrity
- Evidence anchors: [section] explains "This initialization selection strategy is based on spatial distribution... choose one token out of every ⌊1/r₀⌋ tokens"; [abstract] mentions "initially select a significantly small number of tokens"
- Break condition: If image has strong local correlations, regular spacing might miss important local patterns

## Foundational Learning

- Concept: Cosine similarity as distance metric for high-dimensional feature vectors
  - Why needed here: Used to measure feature distribution distances between tokens during expansion and merging
  - Quick check question: Why is cosine distance preferred over Euclidean distance for comparing token features in this context?

- Concept: Progressive training with staged token growth
  - Why needed here: ToE divides training into stages where token count increases gradually to maintain feature integrity
  - Quick check question: What is the advantage of dividing training into multiple stages versus a single growth phase?

- Concept: Self-attention computational complexity scaling with token count
  - Why needed here: Method reduces tokens to accelerate training, relying on quadratic scaling of attention
  - Quick check question: How does reducing token count affect the FLOPs of self-attention layers?

## Architecture Onboarding

- Component map: Input tokens from first transformer block -> Spatial-distribution initialization -> Widest feature-distribution expansion (repeated δ times) -> Feature-distribution merging -> Reduced token set to subsequent transformer blocks
- Critical path: Initialization → Expansion (repeated δ times) → Merging → Forward pass
- Design tradeoffs:
  - Token reduction vs. information loss: Too aggressive reduction degrades accuracy
  - Number of expansion stages vs. training time: More stages improve accuracy but add overhead
  - Parallel expansion step size k vs. computational efficiency: Larger k reduces iterations but may miss optimal selections
- Failure signatures:
  - Accuracy drop during early training stages indicates insufficient token diversity
  - Instability in later stages suggests improper merging causing feature drift
  - No training speedup indicates incorrect token reduction implementation
- First 3 experiments:
  1. Validate spatial initialization by visualizing token coverage on sample images
  2. Test expansion strategy with synthetic feature distributions to confirm widest coverage
  3. Measure accuracy impact of merging threshold on a small model before full integration

## Open Questions the Paper Calls Out

- Question: What is the impact of ToE on models with different attention mechanisms, such as those using local attention or sparse attention?
  - Basis in paper: [inferred] The paper focuses on Vision Transformers with standard self-attention, but does not explore the effect of ToE on models with different attention mechanisms
  - Why unresolved: The paper does not provide experimental results or analysis for models with alternative attention mechanisms
  - What evidence would resolve it: Experimental results comparing the performance of ToE on models with different attention mechanisms, such as local attention or sparse attention, would provide insights into the generalizability of the method

- Question: How does the choice of the feature distribution distance metric (e.g., Manhattan, Euclidean, or Cosine distance) affect the performance of ToE in different scenarios?
  - Basis in paper: [explicit] The paper compares the performance of different distance metrics in Table 9, but does not provide a comprehensive analysis of their impact on various scenarios
  - Why unresolved: The paper does not explore the effects of distance metrics in different contexts, such as varying model architectures or datasets
  - What evidence would resolve it: A thorough analysis of the performance of ToE with different distance metrics across various scenarios, including different model architectures and datasets, would provide insights into the optimal choice of distance metric

- Question: Can ToE be effectively applied to models with different tokenization strategies, such as those using patch-based or point-based tokenization?
  - Basis in paper: [inferred] The paper focuses on Vision Transformers with patch-based tokenization, but does not explore the effect of ToE on models with different tokenization strategies
  - Why unresolved: The paper does not provide experimental results or analysis for models with alternative tokenization strategies
  - What evidence would resolve it: Experimental results comparing the performance of ToE on models with different tokenization strategies, such as patch-based or point-based tokenization, would provide insights into the generalizability of the method

## Limitations

- Implementation specificity: Method claims general applicability but exact implementation details for different transformer variants are not fully specified
- Dataset generalizability: All experiments conducted on ImageNet-1K and CIFAR-10/100; effectiveness on other vision tasks or non-image domains unverified
- Computational overhead accounting: Paper reports acceleration ratios but doesn't fully account for computational cost of expansion and merging operations themselves

## Confidence

**High confidence** in core claim that ToE achieves training acceleration without accuracy loss on tested datasets and models. Experimental results are consistent and ablation studies support key mechanisms.

**Medium confidence** in claim that ToE works "in a lossless manner" or with "performance gains." While method shows negligible accuracy drops, definition of "lossless" is operational rather than theoretical, and some accuracy reduction is observed at higher acceleration ratios.

**Low confidence** in general applicability claim. Paper demonstrates success on specific ViT architectures but doesn't provide theoretical guarantees or extensive validation across diverse model families and tasks.

## Next Checks

1. **Convergence dynamics analysis**: Track and visualize training loss and accuracy trajectories across different stages of token expansion to understand how staged reduction affects optimization behavior compared to full-token training

2. **Feature space integrity verification**: Conduct detailed analysis of feature distribution preservation by measuring intra-class and inter-class distances before and after token reduction/merging operations to quantify information retention

3. **Generalization stress test**: Evaluate ToE on broader set of vision tasks (object detection on COCO, semantic segmentation on Cityscapes) and transformer variants (ConvNeXt, Swin Transformer) to validate claims of general applicability