---
ver: rpa2
title: Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation
arxiv_id: '2407.03809'
source_url: https://arxiv.org/abs/2407.03809
tags:
- speech
- data
- translation
- whisper
- estonian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving speech-to-text translation
  for Estonian, a low-resource language, by fine-tuning publicly available end-to-end
  models using synthetic and web-scraped data. The authors generated additional training
  data by synthesizing speech translation data from existing ASR training data using
  machine translation and scraping subtitled videos from the internet.
---

# Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation

## Quick Facts
- **arXiv ID**: 2407.03809
- **Source URL**: https://arxiv.org/abs/2407.03809
- **Reference count**: 7
- **Primary result**: Fine-tuning end-to-end models with synthetic speech translation data significantly improves BLEU scores for Estonian conversational speech translation.

## Executive Summary
This paper addresses the challenge of improving speech-to-text translation for Estonian, a low-resource language, by fine-tuning publicly available end-to-end models using synthetic and web-scraped data. The authors generated additional training data by synthesizing speech translation data from existing ASR training data using machine translation and scraping subtitled videos from the internet. They evaluated three models—Whisper, OWSM 3.1, and SeamlessM4T—on bidirectional Estonian-English and Estonian-Russian translation tasks. Results showed that fine-tuning with synthetic data significantly improved translation accuracy, with SeamlessM4T achieving BLEU scores of 35.4 (est-eng) and 26.8 (est-rus) after fine-tuning, matching or surpassing cascaded systems.

## Method Summary
The study fine-tuned three end-to-end speech translation models (Whisper, OWSM 3.1, and SeamlessM4T) for Estonian conversational speech translation. Synthetic training data was created by translating Estonian ASR transcripts into English and Russian using machine translation. Web-scraped subtitle data from Estonian television and online sources provided additional parallel speech-translation pairs. Models were fine-tuned on these datasets with task-specific hyperparameters, then evaluated on manually transcribed and professionally translated conversational speech recordings.

## Key Results
- Fine-tuning with synthetic data significantly improved BLEU scores across all models and language directions
- SeamlessM4T achieved the highest BLEU scores (35.4 est-eng, 26.8 est-rus) after fine-tuning, matching cascaded systems
- Whisper, originally trained only for English translation, effectively translated to other languages when fine-tuned with modified decoder prompts
- Synthetic data fine-tuning outperformed web-scraped subtitle data for Estonian-English and Estonian-Russian directions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning large multilingual models with synthetic speech translation data derived from ASR training data significantly improves BLEU scores for low-resource languages.
- Mechanism: The model leverages the rich acoustic diversity and conversational nature of the ASR data to learn mappings to target languages via machine translation, effectively augmenting the training signal without requiring manually transcribed speech translation pairs.
- Core assumption: The ASR training data contains speech patterns similar to the target evaluation domain, and the machine translation output is of sufficient quality to serve as supervision.
- Evidence anchors: [abstract] "Our results indicate that fine-tuning with synthetic data enhances translation accuracy by a large margin"; [section 4.2.2] "We chose the second method because we already had substantial amount of Estonian ASR training data from various conversational sources"
- Break condition: If the ASR training data is out-of-domain from the evaluation data, the BLEU gains will diminish. If the MT quality drops significantly, the synthetic supervision becomes unreliable.

### Mechanism 2
- Claim: Whisper, originally trained only for English translation, can be effectively fine-tuned for other target languages by modifying the decoder prefix.
- Mechanism: Whisper's architecture allows arbitrary target language tokens in the decoder prompt, so changing the prefix changes the translation target without retraining from scratch.
- Core assumption: The model's learned representations are sufficiently language-agnostic that the decoder can be redirected to output in a different language.
- Evidence anchors: [section 2.2] "it has been shown that it can perform speech translation to other directions with surprisingly high accuracy by changing only the prefix of the decoder"; [section 3] "Consequently, we finetuned Whisper using extra speech translation data by employing the 'transcribe' prompt, where the language specified in the prompt matched the intended target language"
- Break condition: If the decoder's conditioning on the prompt is insufficient to switch translation direction, fine-tuning will not generalize to the new target language.

### Mechanism 3
- Claim: Web-scraped subtitled videos provide additional training data that improves model robustness on conversational speech, though less effectively than synthetic data from ASR.
- Mechanism: Subtitle timing and content provide parallel speech-translation pairs, but misalignment and speech-segment mismatch reduce training signal quality compared to aligned synthetic data.
- Core assumption: Subtitle timing is sufficiently accurate to provide usable speech-translation pairs, even if not perfect.
- Evidence anchors: [section 4.2.1] "We aimed to find data featuring long-form conversational speech... We avoided sources with machine-generated subtitles"; [section 4.4] "For Estonian-English and Estonian-Russian, finetuning on synthetic dataset outperforms web data by a large margin"
- Break condition: If subtitle timing is too inaccurate, the synthetic alignment will introduce noise that outweighs the benefit of additional data.

## Foundational Learning

- Concept: End-to-end speech translation model architecture
  - Why needed here: The paper evaluates and fine-tunes models like Whisper, OWSM, and SeamlessM4T, all of which use encoder-decoder Transformer architectures with specific speech encoders.
  - Quick check question: What are the main differences between the encoder-decoder architecture of Whisper and the adapter-based architecture of SeamlessM4T?

- Concept: Speech segmentation for long-form decoding
  - Why needed here: Whisper and OWSM can handle arbitrary-length audio by segmenting it internally; SeamlessM4T requires external segmentation. This affects data preparation and model performance.
  - Quick check question: How does Whisper's long-form decoding mechanism handle 30-second segments with overlapping shifts?

- Concept: Machine translation as data synthesis for low-resource speech translation
  - Why needed here: The paper uses MT to translate ASR transcripts into target languages to create synthetic speech translation data, avoiding the need for manual transcription of speech in target languages.
  - Quick check question: What are the risks of using MT-generated transcripts as training targets for speech translation models?

## Architecture Onboarding

- Component map: Raw audio waveform -> Speech encoder -> (Adapter layers) -> Decoder -> Text translation
- Critical path: Audio → Encoder → (Adapter) → Decoder → Text
- Design tradeoffs:
  - Whisper: Simpler, can handle long audio, but only English output by default
  - SeamlessM4T: More complex, needs segmentation, supports many languages
  - OWSM: Open reproduction of Whisper, but reported lower performance
- Failure signatures:
  - Low BLEU scores despite fine-tuning: Likely domain mismatch between training data and evaluation
  - High WER on evaluation data: Whisper hallucination or poor speech recognition
  - Segmentation errors in SeamlessM4T: Incorrect voice activity detection or speaker segmentation
- First 3 experiments:
  1. Fine-tune Whisper on synthetic Estonian→English data and evaluate BLEU on CoV oST 2 read speech
  2. Fine-tune SeamlessM4T on the same synthetic data and compare performance to Whisper
  3. Fine-tune both models on web-scraped subtitle data and measure relative improvement over synthetic-only fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well can Whisper and OWSM be fine-tuned for speech-to-text translation directions beyond Estonian-English and Estonian-Russian?
- Basis in paper: [explicit] The paper states that Whisper is originally trained only for translating into English, but can be finetuned for other speech translation directions by changing only the prefix of the decoder.
- Why unresolved: The paper only evaluates these models for Estonian-English and Estonian-Russian translation tasks. The authors acknowledge that they were "relatively confident" that Whisper could be fine-tuned for other directions, but did not test this.
- What evidence would resolve it: Testing these models on other low-resource language pairs beyond Estonian would provide evidence of their generalizability for speech-to-text translation.

### Open Question 2
- Question: How does the performance of end-to-end models compare to cascaded systems for speech-to-text translation on conversational speech in other low-resource languages?
- Basis in paper: [explicit] The paper compares the performance of end-to-end models to cascaded systems for Estonian, finding that fine-tuned SeamlessM4T matches or surpasses the cascaded system.
- Why unresolved: The study only examines Estonian. It is unclear if the same pattern would hold for other low-resource languages.
- What evidence would resolve it: Conducting similar experiments with other low-resource languages would show if end-to-end models consistently outperform or match cascaded systems for conversational speech translation.

### Open Question 3
- Question: What is the impact of using different types of synthetic data (e.g., speech synthesis vs. machine translation) on the performance of end-to-end models for low-resource speech-to-text translation?
- Basis in paper: [explicit] The paper uses machine translation to generate target text data from existing source language ASR training data, finding this improves model performance. However, it notes that using speech synthesis to create source speech data from existing MT training data is an alternative method.
- Why unresolved: The paper only explores one method of generating synthetic data. It is unclear how the performance would differ if speech synthesis were used instead.
- What evidence would resolve it: Generating synthetic data using both methods and comparing the performance of fine-tuned models would show the relative impact of each approach.

## Limitations

- The study relies heavily on synthetic data generated via machine translation, but the quality and domain alignment of this synthetic data with evaluation sets is not thoroughly characterized
- Whisper's ability to translate to non-English languages depends on decoder prefix conditioning, which is not explicitly tested across all model variants
- The evaluation focuses on BLEU and BLEURT scores, which may not fully capture translation quality for conversational speech

## Confidence

- **High Confidence**: The core finding that synthetic data fine-tuning significantly improves BLEU scores for low-resource Estonian speech translation. This is directly supported by quantitative results showing substantial improvements.
- **Medium Confidence**: The comparative effectiveness of different model architectures (Whisper vs. OWSM vs. SeamlessM4T) for Estonian translation. While results show clear performance differences, the study does not control for all implementation variables.
- **Low Confidence**: The assumption that web-scraped subtitle data provides meaningful additional training signal. The paper shows synthetic data outperforms web data, but does not systematically analyze why subtitle alignment issues don't completely negate the benefit.

## Next Checks

1. **Domain Alignment Analysis**: Conduct a detailed comparison of the ASR training data domain characteristics against the evaluation sets to quantify domain mismatch and validate the assumption that ASR data provides appropriate supervision.

2. **Segmentation Error Impact Study**: Systematically measure how different speech segmentation strategies affect translation quality, particularly focusing on cases where segmentation boundaries fall within words or utterances.

3. **Synthetic Data Quality Assessment**: Evaluate the machine translation quality of the synthetic supervision data using both automated metrics and human evaluation to determine if translation errors in synthetic targets propagate to degraded model performance.