---
ver: rpa2
title: Simplicity Bias of Two-Layer Networks beyond Linearly Separable Data
arxiv_id: '2405.17299'
source_url: https://arxiv.org/abs/2405.17299
tags:
- bias
- networks
- simplicity
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper characterizes simplicity bias for general datasets in
  two-layer neural networks initialized with small weights and trained with gradient
  flow. The key idea is that during early training phases, network features cluster
  around a few directions determined by a data-dependent function, independent of
  hidden layer size.
---

# Simplicity Bias of Two-Layer Networks beyond Linearly Separable Data

## Quick Facts
- **arXiv ID:** 2405.17299
- **Source URL:** https://arxiv.org/abs/2405.17299
- **Authors:** Nikita Tsoy; Nikola Konstantinov
- **Reference count:** 40
- **Key outcome:** This paper characterizes simplicity bias for general datasets in two-layer neural networks initialized with small weights and trained with gradient flow. The key idea is that during early training phases, network features cluster around a few directions determined by a data-dependent function, independent of hidden layer size. For datasets with XOR-like patterns, simplicity bias intensifies in later training stages. The authors prove that prominent neurons align with extrema of this function and provide theoretical analysis for three training stages: initial small weights, growth to constant scale, and infinite time limit. They validate their theoretical findings with experiments on image data, demonstrating that networks trained to very small loss may experience stronger simplicity bias, potentially hindering out-of-distribution transfer.

## Executive Summary
This paper provides a theoretical characterization of simplicity bias in two-layer neural networks beyond linearly separable data. The authors analyze how networks initialized with small weights and trained via gradient flow exhibit feature clustering during early training phases. The study reveals that network features cluster around directions determined by a data-dependent function, with prominent neurons aligning with extrema of this function. The analysis covers three distinct training stages: initial small weights, growth to constant scale, and infinite time limit. Experimental validation on image data demonstrates that networks trained to very small loss may experience stronger simplicity bias, potentially impacting out-of-distribution transfer performance.

## Method Summary
The authors employ a theoretical framework to analyze simplicity bias in two-layer neural networks with small weight initialization and gradient flow training. The methodology centers on characterizing how network features cluster during different training phases through a data-dependent function that determines directional alignment of prominent neurons. The analysis systematically examines three training stages: the initial phase with small weights, the intermediate phase where weights grow to constant scale, and the final infinite time limit. Theoretical proofs establish the relationship between simplicity bias and network architecture, while experimental validation demonstrates these properties on image datasets.

## Key Results
- Network features cluster around directions determined by a data-dependent function during early training phases
- Prominent neurons align with extrema of the data-dependent function
- For XOR-like datasets, simplicity bias intensifies during later training stages
- Networks trained to very small loss may experience stronger simplicity bias, potentially hindering out-of-distribution transfer

## Why This Works (Mechanism)
The mechanism underlying simplicity bias in two-layer networks operates through gradient flow dynamics during training. When networks are initialized with small weights, the early training phase exhibits feature clustering around specific directions dictated by the data distribution. This clustering emerges because the gradient flow naturally concentrates network capacity along directions that minimize loss most efficiently. As training progresses, the alignment of prominent neurons with extrema of the data-dependent function becomes more pronounced. For datasets with complex patterns like XOR, this bias intensifies in later stages as the network refines its representations. The theoretical framework captures this progression through three distinct phases, explaining how simplicity bias manifests and evolves throughout training.

## Foundational Learning
- **Gradient Flow**: Continuous-time limit of gradient descent, where weights update according to the gradient of the loss function at each point in time. *Why needed*: Provides theoretical foundation for analyzing training dynamics without discrete time steps. *Quick check*: Verify that gradient flow converges to stationary points of the loss function under appropriate conditions.
- **Two-Layer Neural Networks**: Networks consisting of an input layer, a hidden layer with nonlinear activation functions, and an output layer. *Why needed*: Serves as tractable model for analyzing simplicity bias while capturing essential nonlinear phenomena. *Quick check*: Confirm that the network can approximate nonlinear decision boundaries given sufficient hidden units.
- **Data-Dependent Functions**: Functions that characterize network behavior based on the specific distribution of training data. *Why needed*: Enables theoretical analysis of how data properties influence simplicity bias. *Quick check*: Verify that the function correctly captures clustering behavior for different dataset types.
- **Feature Clustering**: Phenomenon where network representations concentrate along specific directions during training. *Why needed*: Central mechanism explaining simplicity bias emergence. *Quick check*: Demonstrate that features indeed cluster along predicted directions for simple datasets.
- **XOR-like Patterns**: Datasets exhibiting nonlinearly separable structures similar to the XOR problem. *Why needed*: Representative examples of complex data distributions that challenge linear separability. *Quick check*: Verify that networks exhibit the predicted intensification of simplicity bias on such datasets.

## Architecture Onboarding

**Component Map:**
Input Data -> Two-Layer Network (Small Weights) -> Gradient Flow Training -> Feature Clustering -> Simplicity Bias Emergence

**Critical Path:**
The critical path follows from data initialization through gradient flow dynamics to feature clustering and simplicity bias manifestation. Small weight initialization is crucial as it determines the early training phase behavior, while gradient flow provides the continuous-time framework for theoretical analysis. The emergence of feature clustering around data-dependent directions represents the key mechanism through which simplicity bias develops.

**Design Tradeoffs:**
The primary tradeoff involves balancing theoretical tractability against practical applicability. The small weight initialization assumption enables rigorous mathematical analysis but may not reflect common initialization schemes in practice. Similarly, gradient flow analysis provides clean theoretical insights but differs from practical gradient descent implementations. The two-layer network architecture offers analytical convenience while potentially limiting generalizability to deeper models.

**Failure Signatures:**
- Networks fail to exhibit feature clustering when initialized with large weights
- Simplicity bias may not manifest in networks trained with finite learning rates rather than gradient flow
- Complex data distributions may not conform to the predicted clustering patterns
- Out-of-distribution transfer performance may not correlate with simplicity bias strength as predicted

**First Experiments:**
1. Verify feature clustering behavior on simple synthetic datasets with known ground truth
2. Test simplicity bias emergence across different activation functions in the hidden layer
3. Compare theoretical predictions with practical gradient descent implementations using finite learning rates

## Open Questions the Paper Calls Out
None

## Limitations
- Extension from two-layer networks to deeper architectures may exhibit different simplicity bias behaviors
- Small initial weight assumption may not capture practical initialization schemes commonly used in modern deep learning
- Analysis focuses on gradient flow rather than practical gradient descent with finite step sizes
- Connection between simplicity bias and out-of-distribution transfer requires more empirical validation across diverse datasets and tasks

## Confidence
- **High**: Theoretical characterization of early training phases and data-dependent clustering function
- **Medium**: Analysis of later training stages and XOR-like pattern behavior
- **Low**: Claims about out-of-distribution transfer implications

## Next Checks
1. Experimentally validate the theoretical predictions on deeper network architectures beyond two-layer networks
2. Test the model predictions under practical gradient descent with varying learning rates and batch sizes
3. Conduct systematic experiments on out-of-distribution transfer learning across multiple datasets to quantify the relationship between simplicity bias and generalization performance