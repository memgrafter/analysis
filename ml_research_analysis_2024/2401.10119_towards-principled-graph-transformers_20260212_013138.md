---
ver: rpa2
title: Towards Principled Graph Transformers
arxiv_id: '2401.10119'
source_url: https://arxiv.org/abs/2401.10119
tags:
- graph
- graphs
- node
- learning
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work bridges theoretical expressiveness and practical performance
  in graph learning by introducing the Edge Transformer (ET), a theoretically principled
  model with 3-WL expressive power that achieves state-of-the-art results without
  relying on positional or structural encodings. The ET operates on node pairs using
  triangular attention, allowing explicit modeling of higher-order relations, which
  is key to its systematic generalization capabilities.
---

# Towards Principled Graph Transformers

## Quick Facts
- arXiv ID: 2401.10119
- Source URL: https://arxiv.org/abs/2401.10119
- Authors: Luis Müller; Daniel Kusuma; Blai Bonet; Christopher Morris
- Reference count: 40
- Key outcome: Introduces Edge Transformer with 3-WL expressive power achieving SOTA results without positional encodings

## Executive Summary
This work bridges theoretical expressiveness and practical performance in graph learning by introducing the Edge Transformer (ET), a theoretically principled model with 3-WL expressive power that achieves state-of-the-art results without relying on positional or structural encodings. The ET operates on node pairs using triangular attention, allowing explicit modeling of higher-order relations, which is key to its systematic generalization capabilities. Theoretical results show that the ET simulates the 2-FWL hierarchy, enabling it to evaluate first-order logic statements with counting quantifiers, justifying its systematic generalization abilities.

## Method Summary
The Edge Transformer uses triangular attention to operate on node pairs, computing attention scores between pairs (i,l) and (l,j) for all intermediate nodes l. This aggregation scheme represents multisets required by the 2-FWL hierarchy. The model tokenizes input by combining edge features Eij with concatenated node features Fi and Fj, processed through a neural network ϕ to create initial node pair representations. The triangular attention mechanism updates these representations through multiple layers, with the final representations used for various graph learning tasks.

## Key Results
- Achieves state-of-the-art results on molecular regression tasks (ZINC (12K) MAE: 0.062 ± 0.004)
- Demonstrates strong performance on the CLRS benchmark for algorithmic reasoning
- Remains competitive with state-of-the-art models like GRIT and GraphGPS while offering theoretical advantages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Edge Transformer's triangular attention explicitly models higher-order relations by operating on node pairs, enabling 3-WL expressive power without positional encodings.
- Mechanism: The triangular attention computes attention scores between pairs (i,l) and (l,j) for all intermediate nodes l, aggregating information about paths of length 2 through node l. This aggregation scheme can represent the multisets required by the 2-FWL hierarchy.
- Core assumption: The triangular attention's weighted sum over all intermediate nodes l is sufficient to represent the multisets in the 2-FWL aggregation.
- Evidence anchors:
  - [abstract]: "the Edge Transformer (ET), a theoretically principled model with 3-WL expressive power that achieves state-of-the-art results without relying on positional or structural encodings"
  - [section 3]: "triangular attention in Equation (1), performs weighted sum aggregation over the 2-tuple of colors... which we show is sufficient to represent the multiset"
  - [corpus]: No direct corpus evidence found; mechanism relies on theoretical analysis in the paper
- Break condition: If the weighted sum aggregation cannot uniquely represent all multisets required by 2-FWL, the 3-WL expressiveness claim would break.

### Mechanism 2
- Claim: The Edge Transformer's tokenization scheme encodes both node features and edge features, making it suitable for general graph learning tasks.
- Mechanism: The tokenization combines edge features Eij with concatenated node features Fi and Fj, then processes them through a neural network ϕ to create initial node pair representations. This captures both structural information (through edges) and node attributes.
- Core assumption: Combining edge features with node features in the tokenization is sufficient to encode the initial 2-FWL colors.
- Evidence anchors:
  - [section 3]: "we construct a feature matrix F... consistent with ℓ... Additionally, we consider an edge feature tensor E... We then construct a 3D tensor of input tokens X... such that for node pair (i, j)"
  - [section 4]: "Proposition 6... shows that the tokenization in Equation (4) is sufficient to encode the initial node colors under 2-FWL"
  - [corpus]: No direct corpus evidence found; mechanism relies on theoretical analysis in the paper
- Break condition: If the tokenization cannot uniquely represent all initial 2-FWL colors, the expressiveness claim would break.

### Mechanism 3
- Claim: The Edge Transformer's ability to systematically generalize comes from its 2-FWL expressive power, which allows it to evaluate first-order logic statements with counting quantifiers.
- Mechanism: The connection between k-FWL and first-order logic (Theorem 2 in the paper) means that 2-FWL expressive models can evaluate C3,t formulas. This allows the ET to learn explicit representations of composite relations (like GRANDMOTHER) from primitive relations (like MOTHER).
- Core assumption: The connection between k-FWL expressive power and first-order logic evaluation is valid and applicable to the Edge Transformer.
- Evidence anchors:
  - [section 5]: "we use a well-known connection between graph isomorphism and first-order logic... we formally justify the ET for performing systematic generalization"
  - [section 5]: "Corollary 3... Then, for every t ≥ 0, X(t)(u) = X(t)(v), if and only if u and v satisfy the same sentences in C3,t"
  - [corpus]: No direct corpus evidence found; mechanism relies on theoretical analysis in the paper
- Break condition: If the connection between k-FWL and first-order logic evaluation does not hold for the Edge Transformer, the systematic generalization claim would break.

## Foundational Learning

- Concept: k-dimensional Weisfeiler-Leman (k-WL) hierarchy
  - Why needed here: Understanding k-WL is crucial because the Edge Transformer's expressive power is characterized in terms of this hierarchy
  - Quick check question: What is the difference between 1-WL and 2-WL in terms of what they can distinguish?

- Concept: Graph isomorphism testing
  - Why needed here: The paper connects the Edge Transformer's expressive power to graph isomorphism through the k-WL hierarchy
  - Quick check question: Why is graph isomorphism testing relevant to understanding a graph neural network's expressive power?

- Concept: First-order logic with counting quantifiers
  - Why needed here: The paper uses the connection between k-FWL and first-order logic to justify the Edge Transformer's systematic generalization abilities
  - Quick check question: What does it mean for a graph neural network to be able to evaluate first-order logic statements?

## Architecture Onboarding

- Component map:
  - Input: 3D tensor X ∈ Rn×n×d representing node pair features
  - Core: Triangular attention mechanism operating on node pairs
  - Output: Node pair representations that can be read out for various tasks
  - Tokenization: Combines edge features and node features to create initial representations

- Critical path:
  1. Tokenization: Create initial 3D tensor from node and edge features
  2. Triangular attention: Update node pair representations through multiple layers
  3. Readout: Extract predictions for node pairs, nodes, or entire graphs

- Design tradeoffs:
  - Expressiveness vs efficiency: O(n³) runtime vs more efficient O(n²) alternatives
  - Positional encodings: Strong performance without them vs potential gains with them
  - Higher-order vs lower-order: 3-WL expressiveness vs computational cost

- Failure signatures:
  - Poor performance on large graphs due to O(n³) scaling
  - Inability to distinguish certain graph structures if tokenization is insufficient
  - Failure to systematically generalize if the logic connection doesn't hold

- First 3 experiments:
  1. Verify triangular attention implementation matches the paper's pseudo-code
  2. Test tokenization on simple graphs to ensure initial 2-FWL colors are preserved
  3. Evaluate on a small molecular dataset to confirm 3-WL expressive power claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Edge Transformer's cubic time and memory complexity be effectively reduced through architectural innovations or hardware optimizations while maintaining its 3-WL expressive power?
- Basis in paper: [explicit] The paper identifies the O(n^3) runtime and memory complexity as a key limitation, noting it's significantly less efficient than most state-of-the-art models with linear or quadratic complexity.
- Why unresolved: The paper suggests potential solutions like parallelizable neural networks and architecture-specific hardware optimizations (e.g., Triton, FlashAttention) but doesn't provide concrete implementations or demonstrate their effectiveness.
- What evidence would resolve it: Empirical results showing the Edge Transformer achieving competitive runtime and memory usage compared to linear/quadratic models while preserving its theoretical expressivity, or a formal proof that reducing complexity below O(n^3) necessarily compromises 3-WL power.

### Open Question 2
- Question: Does the Edge Transformer's theoretical advantage in systematic generalization translate to measurable performance gains on compositional tasks beyond the CLRS benchmark?
- Basis in paper: [explicit] The paper theoretically justifies the ET's systematic generalization ability through connections to first-order logic with counting quantifiers, but only empirically evaluates this on the CLRS benchmark for algorithmic reasoning.
- Why unresolved: While the paper demonstrates strong performance on CLRS, it doesn't test the ET on other established benchmarks for systematic generalization like SCAN or gSCAN, which are specifically designed to measure compositional reasoning.
- What evidence would resolve it: Comparative results showing the ET significantly outperforms other models on compositional generalization benchmarks, particularly on tasks requiring novel concept combinations, or ablation studies isolating the impact of the ET's triangular attention mechanism on compositional reasoning.

### Open Question 3
- Question: How does the Edge Transformer's performance scale with graph size in practice, and what is the practical limit of graph sizes it can handle effectively?
- Basis in paper: [explicit] The paper acknowledges the cubic complexity as a limitation but provides runtime experiments only up to 700 nodes using GPU optimizations, without discussing practical applications on larger real-world graphs.
- Why unresolved: The paper doesn't explore the practical performance ceiling of the ET on real-world datasets with larger graphs (e.g., social networks, knowledge graphs), nor does it compare the practical performance degradation to other theoretically expressive models.
- What evidence would resolve it: Empirical scaling studies showing the ET's performance degradation curve on graphs of increasing size compared to both efficient models and other 3-WL expressive models, or identification of specific graph properties that make the ET particularly effective or ineffective.

## Limitations

- Cubic time and memory complexity (O(n³)) limits scalability to very large graphs
- Theoretical expressiveness claims require extensive empirical validation on diverse graph structures
- Practical implications of 3-WL vs 2-WL expressiveness for real-world tasks remain unclear

## Confidence

- 3-WL Expressive Power Claims: Medium - Strong theoretical foundation but limited empirical validation
- Systematic Generalization Claims: High - Well-grounded through established theoretical connections
- Empirical Performance Claims: High - Strong results on multiple benchmarks with clear comparisons
- Tokenization Sufficiency: Medium - Theoretical proof exists but practical edge cases may arise

## Next Checks

1. Test Edge Transformer on graphs with known 2-WL/3-WL distinctions to empirically verify expressiveness claims
2. Evaluate systematic generalization on synthetic datasets with varying logical formula complexity
3. Benchmark performance degradation as graph size increases to quantify the O(n³) complexity impact