---
ver: rpa2
title: Optimal cross-learning for contextual bandits with unknown context distributions
arxiv_id: '2401.01857'
source_url: https://arxiv.org/abs/2401.01857
tags:
- algorithm
- regret
- lemma
- distribution
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of contextual bandits with cross-learning
  under unknown context distributions. The core challenge is that estimating the context
  distribution requires samples that must not be correlated with the algorithm's actions.
---

# Optimal cross-learning for contextual bandits with unknown context distributions

## Quick Facts
- arXiv ID: 2401.01857
- Source URL: https://arxiv.org/abs/2401.01857
- Reference count: 40
- This paper develops an efficient algorithm achieving O(√KT) regret for contextual bandits with cross-learning under unknown context distributions, independent of the number of contexts.

## Executive Summary
This paper addresses the challenge of contextual bandits where the context distribution is unknown and the losses are adversarial. The authors develop a novel epoch-based technique that decouples context distribution estimation from action selection, allowing for efficient cross-learning across contexts. By carefully scheduling the algorithm's execution and using importance-weighted estimators, they achieve regret bounds that are independent of the number of contexts, which is optimal up to logarithmic factors. The method is applied to learning in first-price auctions and sleeping bandits, providing the first nearly-tight regret bounds for these problems.

## Method Summary
The algorithm uses Follow-the-Regularized-Leader (FTRL) with entropy regularization, scheduled over epochs of length L. It maintains unbiased estimators for losses using importance weighting, with contexts from one epoch used to estimate the distribution for the next. The key innovation is an epoch-based decoupling that removes correlations between context estimation and action selection, allowing for tighter confidence bounds. The algorithm achieves O(√KT) regret, independent of the number of contexts, through this cross-learning framework.

## Key Results
- Achieves O(√KT) regret, independent of the number of contexts, which is optimal up to logarithmic factors
- Introduces a novel epoch-based technique for decoupling context distribution estimation from action selection
- Applies the method to first-price auctions and sleeping bandits, providing the first nearly-tight regret bounds for these problems
- Demonstrates that cross-learning enables regret bounds that avoid the Ω(√C KT) lower bound without cross-learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-learning across contexts allows regret independent of the number of contexts, avoiding the Ω(√C KT) lower bound that applies without cross-learning.
- Mechanism: By observing the loss for the played action in all possible contexts (not just the current one), the learner gains partial information about every context in each round, which enables learning a good policy without needing to separately explore each context.
- Core assumption: Contexts are drawn i.i.d. from an unknown distribution and losses are chosen adversarially.
- Evidence anchors:
  - [abstract] "cross-learning... learner observes the loss for the action they play in all possible contexts, not just the context of the current round"
  - [section] "With the ability to cross-learn between contexts, we side-step this lower bound by being able to gain a little bit of information about each context in each round"
- Break condition: If the feedback model changes so that only the current context's loss is observed, the cross-learning benefit disappears and the regret becomes dependent on C.

### Mechanism 2
- Claim: Epoch-based decoupling of context distribution estimation and action selection removes harmful correlations.
- Mechanism: The algorithm schedules estimation of the unknown context distribution ν using contexts from one epoch while using the learned estimate to guide actions in a later epoch, ensuring the estimation samples are independent of the actions taken.
- Core assumption: Contexts are i.i.d. and the time horizon T is known or can be partitioned into epochs of length L.
- Evidence anchors:
  - [abstract] "a novel technique for coordinating the execution of a learning algorithm over multiple epochs in such a way to remove correlations between estimation of the unknown distribution and the actions played by the algorithm"
  - [section] "we present a method of scheduling the learning algorithm into different epochs in a way which largely disentangles the correlation between learning ν and solving the bandit problem"
- Break condition: If epochs are too short or too long, the decoupling either fails to provide enough samples for estimation or creates too much drift between estimation and usage.

### Mechanism 3
- Claim: Using expected sum bounds instead of high-probability bounds for importance weights allows tighter confidence intervals.
- Mechanism: Rather than requiring each importance weight estimate to be accurate with high probability, the algorithm bounds the expected sum of importance weights, which can be done with smaller confidence intervals.
- Core assumption: The importance weights have bounded variance and are computed from i.i.d. samples.
- Evidence anchors:
  - [section] "We present a new method of analysis that sidesteps the necessity of proving high probability bounds on each of the denominators individually, instead bounding their expected sum in aggregate"
  - [section] "this relaxation allows for smaller confidence intervals"
- Break condition: If the importance weights become too variable or correlated with actions, the expected sum bounds may no longer hold.

## Foundational Learning

- Concept: Importance weighting and inverse probability weighting in estimation
  - Why needed here: The algorithm needs to estimate losses for all contexts using only the observed losses, requiring weighting by the probability of observing each context.
  - Quick check question: What happens to the variance of an importance-weighted estimator if the importance weights become very small?

- Concept: Epoch-based algorithms and decoupling of exploration and exploitation
  - Why needed here: The algorithm needs to separate the estimation of the context distribution from the selection of actions to avoid correlation, which is achieved through epoch scheduling.
  - Quick check question: Why is it beneficial to use contexts from one epoch to estimate the distribution used in a later epoch?

- Concept: Follow-the-Regularized-Leader (FTRL) with entropy regularization
  - Why needed here: FTRL provides the theoretical foundation for the algorithm's regret guarantees, and entropy regularization ensures sufficient exploration.
  - Quick check question: How does entropy regularization in FTRL ensure that the algorithm doesn't get stuck exploiting a suboptimal action?

## Architecture Onboarding

- Component map:
  Context sampler -> FTRL learner -> Epoch scheduler -> Importance weight estimator -> Loss estimator -> Action sampler

- Critical path:
  1. Observe context c_t
  2. Compute policy p_{t,c_t} using FTRL
  3. Compute action distribution q_{t,c_t} via rejection sampling
  4. Sample action A_t from q_{t,c_t}
  5. Observe loss l_{t,A_t}
  6. Update importance weight estimates using contexts from previous epoch
  7. Update loss estimates and FTRL weights

- Design tradeoffs:
  - Epoch length L: Longer epochs provide more accurate context distribution estimates but reduce adaptability; shorter epochs increase adaptability but require more conservative confidence intervals.
  - Confidence parameter γ: Larger values provide more robust importance weight estimates but increase bias; smaller values reduce bias but may lead to unstable estimates.
  - Learning rate η: Larger values allow faster adaptation to new information but may increase regret; smaller values provide more stable learning but may be too slow.

- Failure signatures:
  - High variance in importance weight estimates: May indicate epochs are too short or confidence intervals are too small
  - Persistent bias in loss estimates: May indicate the rejection sampling probability is too low or the policy is not stable enough
  - Regret scaling poorly with K: May indicate the algorithm is not properly leveraging cross-learning between contexts

- First 3 experiments:
  1. Test the algorithm on a synthetic problem with known context distribution and verify that regret scales as Õ(√KT) independent of the number of contexts
  2. Vary the epoch length L and measure the tradeoff between estimation accuracy and adaptability
  3. Compare the algorithm's performance with and without cross-learning on a problem where the optimal action varies significantly across contexts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the cross-learning technique be generalized to handle unknown context distributions in other bandit variants like linear contextual bandits?
- Basis in paper: [explicit] The authors mention "It would be interesting to see if the techniques we develop in this paper provide a general method for handling such issues – we leave this as an interesting future direction."
- Why unresolved: The paper only applies the technique to contextual bandits with cross-learning and mentions it might be applicable to other problems, but doesn't explore these extensions.
- What evidence would resolve it: A formal analysis showing that the epoch-based scheduling and decoupling technique can be adapted to work with linear contextual bandits with unknown context distributions, achieving similar regret bounds.

### Open Question 2
- Question: Is the epoch length L = Θ(√KT) optimal, or can it be tuned for different problem parameters?
- Basis in paper: [inferred] The authors set L = Θ(√KT) to achieve optimal regret, but don't explore whether different settings of L might be beneficial in certain regimes.
- Why unresolved: The paper focuses on achieving the optimal worst-case regret bound and doesn't explore the trade-offs involved in choosing different epoch lengths.
- What evidence would resolve it: A detailed analysis of how the regret bound changes with different choices of L, and identification of problem parameters where non-√KT epoch lengths might be preferable.

### Open Question 3
- Question: Can the cross-learning framework be extended to handle non-stochastic contexts or more general loss functions?
- Basis in paper: [explicit] The paper assumes i.i.d. contexts and bounded losses, and mentions that "dealing with unknown context distributions is a surprising challenge in many other contextual learning problems."
- Why unresolved: The analysis heavily relies on the i.i.d. assumption for contexts and bounded losses, making it unclear how the technique would work in more adversarial settings.
- What evidence would resolve it: A formal extension of the algorithm and analysis to handle contexts that are either adversarially chosen or follow a more general stochastic process, and/or losses that are unbounded but have other structure.

## Limitations

- The algorithm requires epochs to be sufficiently long for accurate estimation of the unknown context distribution, which may be challenging if the horizon T is not known in advance or the context distribution changes over time.
- The epoch-based structure introduces a trade-off between estimation accuracy and adaptability, with short epochs leading to poor estimates and long epochs reducing adaptability.
- The use of rejection sampling to ensure unbiased observation of contexts may lead to high variance if the sampling probability is low, affecting practical performance.

## Confidence

- High confidence: The core theoretical result that cross-learning enables regret independent of the number of contexts, as this is supported by the analysis of the importance-weighted estimators and the epoch-based scheduling.
- Medium confidence: The practical performance of the algorithm, as the theoretical bounds rely on assumptions about the context distribution and the epoch structure that may not hold in practice.
- Low confidence: The exact regret scaling for specific problem instances, as the Õ(√KT) bound may be loose for certain configurations of the context distribution and loss functions.

## Next Checks

1. **Ablation study on epoch length**: Vary the epoch length L and measure the tradeoff between estimation accuracy and adaptability. This will help determine the optimal epoch length for different problem instances.
2. **Comparison with non-cross-learning algorithms**: Compare the algorithm's performance with and without cross-learning on problems where the optimal action varies significantly across contexts. This will validate the benefit of cross-learning.
3. **Robustness to context distribution changes**: Test the algorithm on problems where the context distribution changes over time, to assess its ability to adapt and maintain performance guarantees.