---
ver: rpa2
title: 'A Comprehensive Survey of Dynamic Graph Neural Networks: Models, Frameworks,
  Benchmarks, Experiments and Challenges'
arxiv_id: '2405.00476'
source_url: https://arxiv.org/abs/2405.00476
tags:
- graph
- dynamic
- time
- temporal
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey and experimental evaluation
  of dynamic Graph Neural Networks (DGNNs). It covers 81 recent DGNN models, 12 training
  frameworks, and common benchmarks.
---

# A Comprehensive Survey of Dynamic Graph Neural Networks: Models, Frameworks, Benchmarks, Experiments and Challenges

## Quick Facts
- arXiv ID: 2405.00476
- Source URL: https://arxiv.org/abs/2405.00476
- Reference count: 40
- Key outcome: Comprehensive survey of 81 DGNN models, 12 frameworks, and experimental evaluation showing CTDG models outperform DTDG models in accuracy while frameworks offer better scalability

## Executive Summary
This paper provides a comprehensive survey of dynamic Graph Neural Networks (DGNNs), covering 81 recent models, 12 training frameworks, and common benchmarks. The authors introduce a novel taxonomy for DGNN models based on their structure, features, and dynamic modeling methods. Through extensive experiments on 6 standard graph datasets, they compare 9 representative DGNN models and 3 frameworks, evaluating convergence accuracy, training efficiency, and GPU memory usage. The results demonstrate that CTDG models generally outperform DTDG models in terms of accuracy, while frameworks show better scalability and efficiency, especially for large datasets. The study also identifies open challenges in the field, including the need for a unified framework and improved parallel training efficiency.

## Method Summary
The study evaluates dynamic GNN models and frameworks through systematic experiments on 6 standard graph datasets. Models are trained to convergence with early stopping after 3 epochs, using batch sizes of 1000 for CTDG models and uniform snapshot intervals for DTDG models. The evaluation focuses on convergence accuracy (AUC, AP, Recall, Accuracy), training efficiency (training time, GPU memory usage), and multi-GPU scalability. Experiments are conducted on 4x Nvidia A40 48GB GPUs with Ubuntu 22.04.3 LTS, testing 9 representative DGNN models (JODIE, TGAT, TGN, APAN, CAW, DyREP, EvolveGCN-H, EvolveGCN-O, Roland, DySAT) and 3 frameworks (TGL, DistTGL, SPEED).

## Key Results
- CTDG models generally outperform DTDG models in terms of accuracy across all evaluation metrics
- Frameworks demonstrate superior scalability and efficiency compared to individual models, particularly on large datasets
- GPU memory usage and training time vary significantly across models and frameworks, with some frameworks showing better resource utilization
- The proposed taxonomy effectively categorizes the diverse range of DGNN models based on their structural features and dynamic modeling approaches

## Why This Works (Mechanism)

### Mechanism 1
Dynamic GNNs outperform static GNNs by integrating temporal information into message passing. By incorporating temporal dimensions, dynamic GNNs can capture evolving graph structures and contextual relationships over time, leading to enhanced performance in tasks like link prediction and node classification. This mechanism assumes that temporal information is crucial for understanding real-world dynamic graphs.

### Mechanism 2
The proposed taxonomy categorizes DGNN models based on their structure, features, and dynamic modeling methods, providing unique insights. By classifying models according to these criteria, researchers can better understand and compare the strengths and weaknesses of different approaches, facilitating model selection and development. This mechanism assumes that a clear taxonomy is necessary for organizing and comparing the rapidly growing number of DGNN models.

### Mechanism 3
The comprehensive experimental evaluation compares representative DGNN models and frameworks on standard graph datasets using consistent benchmarks. By conducting fair comparisons under unified settings, the study identifies key challenges and offers principles for future research to enhance the design of models and frameworks in the dynamic GNNs field. This mechanism assumes that experimental comparisons are necessary to evaluate the performance and scalability of different DGNN approaches.

## Foundational Learning

- **Graph Neural Networks (GNNs)**: Understanding the basics of GNNs is crucial for comprehending the advancements and applications of dynamic GNNs. Quick check: What are the key components of a standard GNN architecture, and how do they process graph-structured data?

- **Dynamic Graphs**: Dynamic graphs are the foundation of dynamic GNNs, and understanding their representation and evolution is essential for working with these models. Quick check: How do discrete-time and continuous-time dynamic graphs differ in their representation and handling of temporal information?

- **Temporal Information Processing**: Dynamic GNNs rely on effectively processing temporal information to capture the evolution of graph structures and relationships over time. Quick check: What are the main approaches for incorporating temporal information into GNNs, and how do they differ in terms of their assumptions and capabilities?

## Architecture Onboarding

- **Component map**: Dynamic Graph Representation (DTDG -> CTDG) -> Dynamic GNN Models (81 models) -> Dynamic GNN Frameworks (12 frameworks) -> Evaluation Benchmarks (6 datasets, convergence accuracy, training efficiency, GPU memory usage)

- **Critical path**: 1) Understand the basics of GNNs and dynamic graphs 2) Familiarize with the proposed taxonomy of DGNN models 3) Explore the characteristics and capabilities of different DGNN frameworks 4) Learn about the evaluation benchmarks and metrics used in the study 5) Analyze the experimental results and identify key challenges and principles for future research

- **Design tradeoffs**: Model complexity vs. training efficiency (more complex models may achieve better accuracy but require longer training times and more computational resources); Framework flexibility vs. performance optimization (frameworks that support a wide range of models may sacrifice some performance optimizations for specific model architectures); Dataset size vs. scalability (larger datasets can provide more comprehensive evaluations but may pose challenges for the scalability of DGNN models and frameworks)

- **Failure signatures**: Out-of-memory (OOM) errors during training on large datasets; Poor convergence or overfitting on specific datasets or tasks; Inconsistencies between evaluation metrics and real-world performance; Limited scalability or efficiency when handling large-scale dynamic graphs

- **First 3 experiments**: 1) Implement a basic DGNN model (e.g., EvolveGCN) on a small dynamic graph dataset (e.g., Wikipedia) and evaluate its performance on a link prediction task 2) Compare the training efficiency and memory usage of different DGNN frameworks (e.g., TGL, DistTGL, SPEED) on a medium-sized dynamic graph dataset (e.g., Reddit) 3) Analyze the impact of varying the number of GNN layers and batch sizes on the performance and scalability of a DGNN model (e.g., TGAT) on a large dynamic graph dataset (e.g., ML25M)

## Open Questions the Paper Calls Out

### Open Question 1
How can a unified framework be designed to encompass the majority of dynamic GNN models, given the diversity of methods used to capture temporal dependencies? This question arises from the challenge of developing a comprehensive unified graph operator capable of encompassing a majority of algorithms in the field of dynamic graph learning.

### Open Question 2
What are the most effective methods for parallel training of dynamic GNNs on large-scale graphs, considering both intra-mini-batch dependencies and inter-machine communication? This question stems from the difficulties in balancing parallel training speed with accuracy due to complex temporal dependencies in dynamic graphs and the impact of inter-machine communication costs on performance.

### Open Question 3
How can dynamic graph data storage be optimized to support real-time updates and efficient training data extraction for dynamic GNNs? This question emerges from the challenges faced by existing frameworks using structures like CSR or T-CSR, which encounter difficulties in promptly supporting graph updates to real-time dynamic graph updates and impede GNN training efficiency.

## Limitations

- The study's results may not generalize to real-world dynamic graphs with different characteristics (scale, sparsity, temporal patterns) beyond the 6 standard datasets used
- Performance results are based on specific hyperparameter configurations, and extensive hyperparameter sensitivity analysis was not conducted
- Experiments were conducted on high-end GPUs (Nvidia A40 48GB), which may not be accessible to all researchers, potentially limiting the scalability and efficiency results on lower-end hardware

## Confidence

- **High Confidence**: The proposed taxonomy for categorizing DGNN models based on their structure, features, and dynamic modeling methods is well-founded and provides valuable insights for the research community
- **Medium Confidence**: The experimental results comparing the performance and scalability of representative DGNN models and frameworks on standard benchmarks are reliable, but may not fully capture the nuances of real-world dynamic graphs
- **Low Confidence**: The identified open challenges and principles for future research are based on the current state of the field and may evolve as new models, frameworks, and applications emerge

## Next Checks

1. Conduct ablation studies to assess the sensitivity of DGNN models and frameworks to different hyperparameter choices, such as learning rates, batch sizes, and GNN architectures
2. Evaluate the performance and scalability of DGNN models and frameworks on large-scale, real-world dynamic graphs from diverse application domains (e.g., social networks, financial transactions, biological interactions)
3. Investigate the transferability of DGNN models and frameworks across different tasks (e.g., link prediction, node classification, graph classification) and application domains to assess their generalization capabilities