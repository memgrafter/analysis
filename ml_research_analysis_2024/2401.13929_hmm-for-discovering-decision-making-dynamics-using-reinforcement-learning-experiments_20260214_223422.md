---
ver: rpa2
title: HMM for Discovering Decision-Making Dynamics Using Reinforcement Learning Experiments
arxiv_id: '2401.13929'
source_url: https://arxiv.org/abs/2401.13929
tags:
- learning
- reward
- group
- rl-hmm
- strategy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces an RL-HMM framework to model decision-making
  dynamics in reinforcement learning tasks, addressing the limitation of assuming
  a single learning strategy. The method allows subjects to switch between two distinct
  strategies: an ''engaged'' state where decisions follow a reinforcement learning
  model and a ''lapse'' state where choices are made randomly.'
---

# HMM for Discovering Decision-Making Dynamics Using Reinforcement Learning Experiments

## Quick Facts
- arXiv ID: 2401.13929
- Source URL: https://arxiv.org/abs/2401.13929
- Authors: Xingche Guo; Donglin Zeng; Yuanjia Wang
- Reference count: 24
- Primary result: RL-HMM framework reveals MDD patients are less engaged in reinforcement learning compared to healthy controls, with engagement linked to brain activity variability in negative affect circuitry.

## Executive Summary
This paper introduces an RL-HMM framework to model decision-making dynamics in reinforcement learning tasks, addressing the limitation of assuming a single learning strategy. The method allows subjects to switch between two distinct strategies: an 'engaged' state where decisions follow a reinforcement learning model and a 'lapse' state where choices are made randomly. Time-varying transition probabilities between these states are modeled nonparametrically using fused lasso regularization. An EM algorithm is developed for efficient parameter estimation, validated through extensive simulations. Applied to the EMBARC study, the approach reveals that MDD patients are less engaged in reinforcement learning compared to healthy controls, with engagement associated with brain activity variability in the negative affect circuitry during emotional conflict tasks.

## Method Summary
The RL-HMM models latent engagement states (engaged vs. lapse) using a hidden Markov model with continuous state space and nonparametric time-varying transition probabilities. The EM algorithm estimates parameters: learning rate β, reward sensitivity ρ, Q-value initial coefficient α, and transition probabilities π1, ζ0, ζ1. Fused lasso/Trend Filtering regularizes the time-varying transitions. The model is fit to EMBARC PRT data (168 MDD patients, 40 controls, 200 trials each), with 5-fold CV selecting regularization and nonparametric bootstrap providing inference. The framework identifies engagement states, estimates group differences in learning parameters, and associates engagement with distraction levels and fMRI brain measures.

## Key Results
- RL-HMM successfully recovers known engagement-switching patterns in simulations
- MDD patients show significantly lower engagement rates than healthy controls (α estimates differ by ~0.15)
- Individual engagement scores correlate with distraction levels and variability in brain activity within negative affect circuitry

## Why This Works (Mechanism)

### Mechanism 1
The model captures switching between "engaged" and "lapse" states by embedding two distinct decision rules in a single HMM framework. In the "engaged" state (Uit=1), choices follow a softmax policy over learned Q-values. In the "lapse" state (Uit=0), choices are uniform random. Transition probabilities between states can vary over trials via fused lasso or trend filtering. The core assumption is that the subject's internal decision-making process can be represented as a finite-state Markov chain with distinct behavioral rules per state.

### Mechanism 2
Time-varying transition probabilities allow the model to capture dynamic changes in engagement over the task duration. ζjt = ζj(t) are estimated via fused lasso/Trend filtering, yielding piecewise-constant or smooth changes in P(Uit+1=1|Uit=j). This captures, e.g., fatigue or habituation effects. The core assumption is that engagement likelihood evolves gradually over trials rather than jumping abruptly except at known task breaks.

### Mechanism 3
The EM algorithm with fused lasso in the M-step yields computationally tractable estimation of both RL parameters and switching dynamics. Forward-backward computes posteriors γijt, ξijkt; fused lasso update on ζj solved via genlasso; Newton-Raphson on RL parameters; L-BFGS-B enforces constraints (0<β<1, ρ>0). The core assumption is that the joint likelihood can be decomposed into independent terms for each parameter block, enabling separate conditional updates.

## Foundational Learning

- **Hidden Markov Models (HMMs)**
  - Why needed here: To model latent engagement states that govern observable choices
  - Quick check question: In an HMM, are the hidden state transitions assumed to be Markovian (depend only on the previous state)?

- **Reinforcement Learning with continuous state space**
  - Why needed here: Subjects update expected rewards based on continuous perceptual inputs (e.g., stimulus magnitude)
  - Quick check question: In the RL update Q_{t+1}(a,s) = β·r + (1-β)·Q_t(a,s), what ensures that Q remains bounded?

- **Fused Lasso / Trend filtering**
  - Why needed here: To regularize time-varying transition probabilities and avoid overfitting to noise in engagement switches
  - Quick check question: What is the difference between fused lasso (r=0) and higher-order trend filtering (r>0) in terms of smoothness assumptions?

## Architecture Onboarding

- **Component map**: Data (trial-level S,A,R sequences) -> Model (RL-HMM with latent states, softmax/random policies) -> Estimation (EM with forward-backward, fused lasso, L-BFGS-B) -> Inference (bootstrap SEs, CV λ selection, posterior γ) -> Output (engagement scores, brain-behavior associations)

- **Critical path**: Forward-backward → posterior γ,ξ → fused lasso update ζ → RL Newton update → convergence check

- **Design tradeoffs**:
  - Time-varying vs. fixed transitions: More flexible but higher variance; fixed simpler but may miss dynamics
  - Fused lasso vs. higher-order trend: Fused lasso yields piecewise-constant transitions; higher-order yields smoother changes
  - Number of hidden states: 2 here (engaged/lapse); more states could capture finer strategy distinctions but increase identifiability risk

- **Failure signatures**:
  - EM not converging: Check initialization, regularization λ too large, or poor separation of states
  - Posterior γ close to 0.5 for all trials: Likely under-specified model (e.g., too few states or weak signal)
  - Bootstrap SEs huge: Overfitting, too few subjects, or model mismatch

- **First 3 experiments**:
  1. Simulate from a known RL-HMM with 2 states and known switching pattern; fit the model; check recovery of β, ρ, ζ, and state sequence accuracy
  2. Hold out last 25% of trials; compare 5-fold CV scores for time-varying vs. fixed transition models; confirm CV prefers the correct specification
  3. Vary λ over a grid; plot CV score and state-switch frequency; identify λ that balances fit and smoothness

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of the RL-HMM framework compare when applied to behavioral tasks beyond the probabilistic reward task, particularly in tasks with more complex or continuous state spaces? The authors note their methods are "sufficiently general to analyze other types of behavioral tasks" but don't explore this empirically.

- **Open Question 2**: What is the impact of varying the order of the trend filtering penalty (r) on the accuracy and computational efficiency of the RL-HMM model? The authors mention r can be adjusted but don't evaluate different values.

- **Open Question 3**: How does the RL-HMM framework perform in identifying and characterizing learning strategy dynamics in populations with different mental health conditions or cognitive profiles? The study is limited to MDD and healthy controls.

## Limitations
- Identifiability concerns: Behavioral similarity between lapse and poor RL performance may make the two-state model difficult to distinguish
- Short time series: Only 100 trials per block may be insufficient for reliable estimation of time-varying transitions
- Cross-sectional design: Cannot establish causality between engagement and depression symptoms

## Confidence
- **High**: EM algorithm with fused lasso is well-specified based on detailed algorithmic description
- **Medium**: Claim that MDD patients are "less engaged" - cross-sectional design and potential confounds (fatigue, medication effects)
- **Low**: Brain-behavior association claim - limited sample size and correlational nature

## Next Checks
1. Perform simulation recovery studies with known switching patterns to assess parameter identifiability and sensitivity to λ
2. Cross-validate model fits between fixed and time-varying transition models within MDD and control groups separately to test whether dynamic engagement is necessary
3. Re-analyze the EMBARC data excluding sessions with high distraction scores to test if engagement differences persist after controlling for task compliance