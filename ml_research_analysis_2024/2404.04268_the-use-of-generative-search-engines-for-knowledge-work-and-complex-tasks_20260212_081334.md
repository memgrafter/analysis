---
ver: rpa2
title: The Use of Generative Search Engines for Knowledge Work and Complex Tasks
arxiv_id: '2404.04268'
source_url: https://arxiv.org/abs/2404.04268
tags:
- user
- bing
- search
- domain
- taxonomy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed how people use generative search engines like
  Bing Copilot compared to traditional search engines like Bing Search. Using large-scale
  log data from May-July 2023, researchers classified over 150,000 conversations and
  sessions into 25 topical domains and measured task complexity using Anderson and
  Krathwohl's taxonomy.
---

# The Use of Generative Search Engines for Knowledge Work and Complex Tasks

## Quick Facts
- arXiv ID: 2404.04268
- Source URL: https://arxiv.org/abs/2404.04268
- Reference count: 0
- Primary result: Bing Copilot users perform more complex knowledge work tasks than traditional search users

## Executive Summary
This study compares how users interact with generative search engines versus traditional search engines through large-scale log analysis. Using Bing Copilot and Bing Search data from May-July 2023, researchers classified over 150,000 conversations and sessions across 25 topical domains. The analysis reveals that generative search engine users engage in significantly more knowledge work and tackle more cognitively complex tasks compared to traditional search engine users.

The findings suggest generative search engines are being adopted for different use cases than traditional search, with users leveraging these tools for higher-order cognitive tasks. Task complexity correlates with user satisfaction when tasks are completed, indicating potential value in the more sophisticated capabilities of generative search interfaces.

## Method Summary
Researchers analyzed log data from May-July 2023 to compare user behavior between Bing Copilot (generative search engine) and Bing Search (traditional search engine). They classified over 150,000 conversations and sessions into 25 topical domains and measured task complexity using Anderson and Krathwohl's taxonomy. The study compared the proportion of knowledge work tasks and task complexity levels between the two platforms, examining how these factors relate to user satisfaction.

## Key Results
- Bing Copilot users engage in knowledge work tasks at 72.9% compared to 37.0% for Bing Search users
- High-complexity tasks account for 37.0% of Copilot usage versus only 13.4% for Bing Search
- Higher task complexity correlates with greater user satisfaction when tasks are completed

## Why This Works (Mechanism)
Generative search engines provide conversational interfaces and synthesis capabilities that enable users to tackle more complex, multi-step problems that traditional keyword-based search cannot easily address. The natural language interaction allows for nuanced queries and iterative refinement of information needs, supporting higher-order cognitive tasks that require analysis, evaluation, and creation rather than simple information retrieval.

## Foundational Learning
1. **Anderson and Krathwohl's taxonomy** - Classification framework for cognitive task complexity; needed to measure and compare task sophistication across platforms; quick check: verify taxonomy levels align with observed task types
2. **Conversational AI capabilities** - Natural language processing and generation that enable multi-turn interactions; needed to understand why generative search supports complex tasks; quick check: assess response coherence across conversation turns
3. **Knowledge work domains** - Professional and intellectual tasks requiring analysis, synthesis, and decision-making; needed to contextualize the types of tasks being performed; quick check: validate domain classifications against user intent

## Architecture Onboarding

Component map: User Query -> Natural Language Processing -> Knowledge Graph Retrieval -> Response Generation -> User Interface

Critical path: Query understanding and intent classification -> Context-aware knowledge retrieval -> Multi-turn conversation management -> Response synthesis and delivery

Design tradeoffs: Generative search trades computational intensity and potential hallucination risks for conversational flexibility and task complexity support versus traditional search's speed and precision for fact retrieval

Failure signatures: Incomplete responses, irrelevant information synthesis, conversational context loss, over-reliance on generated content without source verification

First experiments:
1. Measure average conversation depth and complexity across different knowledge domains
2. Compare task completion rates between generative and traditional search for equivalent queries
3. Analyze the relationship between query specificity and response quality in generative search

## Open Questions the Paper Calls Out
None

## Limitations
- Study relies on Bing-specific data from a limited three-month timeframe
- Automated classification methods for topical domains and complexity levels are not detailed
- Analysis captures only logged-in users who consented to data sharing, potentially introducing selection bias

## Confidence

High confidence:
- Bing Copilot users engage in more knowledge work tasks than Bing Search users (72.9% vs 37.0%)

Medium confidence:
- Correlation between task complexity and user satisfaction for completed tasks
- Specific complexity percentages (37.0% high-complexity for Copilot vs 13.4% for Search)

## Next Checks

1. Replicate the analysis using log data from multiple generative search engines (e.g., Google Bard, ChatGPT) to test generalizability
2. Conduct user surveys to validate the automated complexity classifications and measure satisfaction independently
3. Extend the temporal analysis beyond the initial three-month window to assess whether observed patterns persist over time or change with user adaptation