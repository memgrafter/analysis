---
ver: rpa2
title: Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models
arxiv_id: '2405.00402'
source_url: https://arxiv.org/abs/2405.00402
tags:
- self-refine
- reasoning
- language
- demonstrations
- students
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-refine Instruction-Tuning enables transfer of reasoning abilities
  from large to small language models by first aligning them via instruction tuning
  on LLM-generated demonstrations, then applying a self-refinement phase using a variant
  of Direct Preference Optimization (DPO). This method improves step-wise reasoning
  and outperforms standard instruction tuning in both in-domain and out-domain tasks.
---

# Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models

## Quick Facts
- arXiv ID: 2405.00402
- Source URL: https://arxiv.org/abs/2405.00402
- Authors: Leonardo Ranaldi; AndrÃ¨ Freitas
- Reference count: 18
- Key outcome: Self-refine instruction-tuning enables transfer of reasoning abilities from LLMs to SLMs via instruction tuning on LLM demonstrations, followed by self-refinement via DPO, improving step-wise reasoning and outperforming standard instruction tuning on commonsense and math reasoning benchmarks.

## Executive Summary
Self-refine instruction-tuning is a two-phase approach that aligns reasoning abilities between large and small language models. First, small models are fine-tuned on demonstrations generated by large language models (LLMs) to learn structured reasoning patterns. Then, a self-refinement phase using Direct Preference Optimization (DPO) adjusts the small model's internal policy toward the teacher's reasoning style. This method consistently improves performance on commonsense and mathematical reasoning tasks, especially when out-of-family teachers are used.

## Method Summary
The approach uses a two-phase pipeline: (1) Instruction-tuning on LLM-generated demonstrations, where the LLM provides Chain-of-Thought (CoT) demonstrations for tasks, and the SLM is fine-tuned on (instruction, input, CoT-output) triples; (2) Self-refinement via DPO, where the instructed SLM generates multiple reasoning paths, and DPO optimizes its policy to prefer CoT-style responses over flat answers, aligning outputs closer to the teacher. This is evaluated on commonsense and mathematical reasoning benchmarks (CSQA, OBQA, PIQA, SIQA, GSM8K, MultiArith) and additional tasks (MATH, MMLU).

## Key Results
- Self-refined students outperform standard instruction-tuned students on both in-domain and out-domain tasks.
- Out-family teacher alignment yields more robust generalization than in-family alignment, with up to 8.7-point gains on math word problems.
- The method scales to multi-task reasoning benchmarks and performs well under low-resource conditions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-tuning with CoT demonstrations transfers foundational reasoning step patterns from LLMs to SLMs.
- Mechanism: LLMs generate annotated Chain-of-Thought demonstrations; SLMs are fine-tuned on these to learn structured intermediate reasoning steps.
- Core assumption: The step-by-step CoT structure is learnable and generalizable when embedded in the fine-tuning dataset.
- Evidence anchors: [abstract] "reasoning abilities are first transferred between LLMs and Small Language Models (SLMs) via Instruction-tuning on demonstrations provided by LLMs"; [section] "Instruction-tuning on a dataset produced by LLM comprising a set of tuples in the form of (i, q, ai), where ai is the expected output and CoT answers generated from the teacher"
- Break condition: If CoT demonstrations are inconsistent or too narrow in scope, the SLMs fail to internalize generalizable reasoning patterns.

### Mechanism 2
- Claim: Self-refinement via DPO alignment completes the transfer by adjusting the SLM's internal CoT policy toward the teacher's reasoning style.
- Mechanism: The instructed SLM generates multiple reasoning paths, then DPO optimizes its policy to prefer CoT-style responses over flat answers, aligning its outputs closer to the teacher.
- Core assumption: The DPO optimization signal from self-generated comparisons is sufficient to fine-tune the CoT generation without external reward models.
- Evidence anchors: [abstract] "the instructed models Self-refine their abilities through preference optimization strategies"; [section] "DPO optimization... the SLMs are elicited to deliver a series of reasoning paths by automatically sampling the generated responses and providing rewards using ground truths from the LLMs"
- Break condition: If the self-generated preferences are noisy or uninformative, the DPO phase fails to produce meaningful policy updates.

### Mechanism 3
- Claim: In-family and out-family teacher settings yield different generalization behaviors; out-family often produces more robust SLMs.
- Mechanism: Demonstrations from LLMs outside the student's model family provide diverse reasoning patterns, reducing over-specialization.
- Core assumption: Different model families encode reasoning patterns differently enough that exposure to multiple styles broadens SLM generalization.
- Evidence anchors: [abstract] "self-refined students outperformed... in both in-domain and out-domain tasks" and "consistently out-family settings"; [section] "in the out-family alignment, the performances vary by 8.5 on the QA and 8.7 on the MWP"
- Break condition: If out-family demonstrations are too dissimilar or low quality, the SLM may fail to learn coherent reasoning patterns.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: The method relies on structured intermediate reasoning steps to scaffold SLM reasoning abilities.
  - Quick check question: Can the model generate intermediate reasoning steps that are logically connected and lead to the final answer?

- Concept: Supervised Fine-Tuning (SFT) vs Instruction-Tuning
  - Why needed here: The approach uses task-oriented Instruction-Tuning rather than generic SFT to embed task-specific reasoning skills.
  - Quick check question: Does the fine-tuning dataset include explicit task instructions alongside the input and output pairs?

- Concept: Preference Optimization (DPO)
  - Why needed here: DPO is used in the self-refinement phase to align the SLM's policy toward the CoT style without an external reward model.
  - Quick check question: Can the model generate multiple candidate responses and rank them by internal preference signals?

## Architecture Onboarding

- Component map: LLM Teacher -> Instruction-Tuning Pipeline -> Self-Refinement Engine -> Evaluation Suite
- Critical path: 1. LLM generates demonstrations; 2. SLM fine-tuned on demonstrations; 3. SLM self-refines via DPO; 4. Evaluate SLM performance
- Design tradeoffs:
  - Using multiple LLM teachers vs single teacher: Multiple teachers increase diversity but increase cost.
  - Self-generated vs human-annotated preferences: Self-generated is scalable but noisier.
  - In-family vs out-family alignment: In-family yields faster convergence but may overfit; out-family yields broader generalization but slower alignment.
- Failure signatures:
  - Instruction-tuning performance plateaus below teacher level
  - DPO phase fails to improve or degrades performance
  - Out-of-domain tasks show sharp accuracy drops
  - Self-generated preference pairs become uninformative
- First 3 experiments:
  1. Compare SLM performance before and after instruction-tuning with CoT demonstrations from a single LLM.
  2. Compare SLM performance after instruction-tuning with and without the self-refinement DPO phase.
  3. Test generalization by evaluating the same SLM on in-domain and out-domain tasks after full self-refined instruction-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of demonstrations from different teacher models (e.g., GPT-3.5 vs Llama-2-70) impact the downstream performance of students after Self-refine Instruction-tuning?
- Basis in paper: [explicit] The paper discusses the differences in teacher model performances (Table 5) and their impact on student model alignment, noting that teachers from different families perform differently.
- Why unresolved: The paper shows that different teachers lead to varying student performances but does not quantify how much the inherent quality of demonstrations affects the final student model's reasoning abilities.
- What evidence would resolve it: A systematic comparison of student model performance after Self-refine Instruction-tuning when trained on demonstrations of varying quality from multiple teacher models, controlling for other variables.

### Open Question 2
- Question: Can Self-refine Instruction-tuning be effectively applied to tasks beyond commonsense and mathematical reasoning, such as code generation or multi-modal tasks?
- Basis in paper: [inferred] The paper demonstrates the method on commonsense and math reasoning tasks but does not explore its applicability to other domains or tasks.
- Why unresolved: The experiments are limited to specific reasoning tasks, leaving open the question of whether the approach generalizes to other complex reasoning or generation tasks.
- What evidence would resolve it: Experiments applying Self-refine Instruction-tuning to diverse tasks like code generation, summarization, or multi-modal reasoning, showing performance improvements over baseline methods.

### Open Question 3
- Question: What is the impact of Self-refine Instruction-tuning on the robustness and generalization of student models to out-of-distribution data or adversarial inputs?
- Basis in paper: [explicit] The paper discusses generalization to out-domain tasks but does not address robustness to adversarial or out-of-distribution inputs.
- Why unresolved: While the method shows improved generalization within the tested tasks, its effectiveness against adversarial examples or distribution shifts is not evaluated.
- What evidence would resolve it: Evaluations of student models on adversarially crafted inputs or out-of-distribution data after Self-refine Instruction-tuning, comparing their robustness to baseline and standard instruction-tuned models.

## Limitations
- The approach depends heavily on the quality and diversity of LLM-generated demonstrations, which are not directly validated.
- Self-generated preference signals during DPO are not externally validated or compared to human-annotated preferences.
- Cross-family generalization benefits are observed but not fully explained or theoretically grounded.

## Confidence

- **High confidence**: The core two-phase methodology (instruction-tuning followed by self-refinement via DPO) is clearly described and supported by experimental results across multiple benchmarks.
- **Medium confidence**: The claim that out-family alignment generally leads to better generalization is supported empirically but lacks a rigorous theoretical explanation or ablation studies to confirm the cause.
- **Low confidence**: The reliability and robustness of self-generated preference signals in the DPO phase are not validated against human-annotated preferences or external reward models, making the alignment mechanism uncertain.

## Next Checks

1. **Quality and consistency audit of LLM-generated demonstrations**: Manually sample and evaluate a subset of the CoT demonstrations used for instruction-tuning to assess their correctness, diversity, and alignment with ground truth reasoning patterns.

2. **External validation of self-generated preferences**: Replace the self-generated preference pairs in the DPO phase with human-annotated or external reward model-based preferences and compare the resulting SLM performance and policy alignment.

3. **Ablation study on family diversity**: Systematically vary the number and diversity of LLM teachers (in-family vs out-family) and measure the impact on SLM generalization, including both in-domain and out-domain task performance, to isolate the contribution of cross-family reasoning patterns.