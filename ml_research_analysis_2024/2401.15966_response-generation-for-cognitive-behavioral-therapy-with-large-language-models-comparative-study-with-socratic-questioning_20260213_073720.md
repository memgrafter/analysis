---
ver: rpa2
title: 'Response Generation for Cognitive Behavioral Therapy with Large Language Models:
  Comparative Study with Socratic Questioning'
arxiv_id: '2401.15966'
source_url: https://arxiv.org/abs/2401.15966
tags:
- gpt-4
- responses
- system
- dialogue
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated the impact of large language model (LLM)-generated
  responses on cognitive behavioral therapy (CBT) dialogue systems using Socratic
  questioning. Three dialogue systems were compared: traditional Socratic questioning
  (SQ), LLM-generated responses using OsakaED, and LLM-generated responses using GPT-4,
  with some systems combining LLM responses with Socratic questioning.'
---

# Response Generation for Cognitive Behavioral Therapy with Large Language Models: Comparative Study with Socratic Questioning

## Quick Facts
- arXiv ID: 2401.15966
- Source URL: https://arxiv.org/abs/2401.15966
- Reference count: 40
- Primary result: GPT-4 significantly improved mood change, empathy, and dialogue quality compared to traditional Socratic questioning in CBT dialogue systems

## Executive Summary
This study evaluates the impact of large language model (LLM)-generated responses on cognitive behavioral therapy (CBT) dialogue systems using Socratic questioning. Three dialogue systems were compared: traditional Socratic questioning (SQ), LLM-generated responses using OsakaED, and LLM-generated responses using GPT-4, with some systems combining LLM responses with Socratic questioning. Results showed that GPT-4 significantly improved mood change, empathy, and dialogue quality compared to traditional SQ, while OsakaED did not show notable improvements. The study highlights GPT-4's potential in counseling contexts but also raises ethical concerns about direct LLM-user interactions in mental healthcare.

## Method Summary
The study compared five dialogue systems (SQ, OsakaED, OsakaED+SQ, GPT-4, GPT-4+SQ) using a 15-utterance CBT scenario based on Socratic questioning. Participants (86 valid responses from CrowdWorks) interacted with these systems and rated mood change, cognitive change, and dialogue quality. OsakaED (1.6B parameter model) was fine-tuned on social media counseling data, while GPT-4 was accessed via OpenAI API with a coaching prompt. Evaluation used subjective measures including mood change equations, CC-immediate scales, and dialogue quality questionnaires.

## Key Results
- GPT-4 significantly improved mood change, empathy, and dialogue quality compared to traditional Socratic questioning
- OsakaED did not show notable improvements over baseline Socratic questioning
- GPT-4 responses were rated higher for user empathy, trust, personality, agency, emotion, and system empathy
- GPT-4 can appropriately generate Socratic questions within context even when instructed not to

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 responses generate higher user empathy and trust compared to OsakaED and SQ alone
- Mechanism: GPT-4's larger parameter count and more generalized training enable richer, more contextually relevant counseling responses, which are perceived as more empathetic and trustworthy by users
- Core assumption: Empathetic and trustworthy responses are key drivers of improved mood change and cognitive restructuring in CBT dialogues
- Evidence anchors:
  - [abstract]: "When using GPT-4, the amount of mood change, empathy, and other dialogue qualities improve significantly."
  - [section]: "The two systems using GPT-4 received higher ratings than the other systems for user's emapthy, trust, personality, agency, emotion, and the system's empathy."
  - [corpus]: Weak signal; no direct neighbor papers specifically confirm empathy/trust mechanisms, only general CBT LLMs
- Break condition: If user evaluations do not correlate with objective CBT outcome metrics, or if empathy is perceived as superficial

### Mechanism 2
- Claim: GPT-4 can appropriately generate Socratic questions within context, improving CBT dialogue effectiveness
- Mechanism: GPT-4's strong contextual understanding allows it to embed Socratic questioning naturally in responses, even when instructed not to, thereby guiding users toward cognitive restructuring
- Core assumption: Socratic questioning embedded in responses is as effective as explicit questioning in CBT
- Evidence anchors:
  - [section]: "It was observed that GPT-4 can ask Socratic Questioning in context... responses generated by both GPT-4 and GPT-4+SQ frequently involved Socratic Questions."
  - [abstract]: "GPT-4 significantly improved mood change, empathy, and dialogue quality compared to traditional SQ."
  - [corpus]: Weak signal; no neighbor papers directly analyze Socratic question generation quality
- Break condition: If generated Socratic questions are not contextually appropriate or lead users off CBT track

### Mechanism 3
- Claim: OsakaED's limited improvement over SQ suggests training data and model size are critical for effective CBT dialogue
- Mechanism: OsakaED's smaller size (1.6B parameters) and narrower domain training (social media empathetic counseling) limit its ability to generalize to structured CBT scenarios compared to GPT-4
- Core assumption: CBT scenarios require broader reasoning ability than empathetic social media chat
- Evidence anchors:
  - [section]: "OsakaED does not necessarily improve outcomes compared to scenario-based dialogue, even though the dialogue model used to generate responses was trained with a real counseling dataset."
  - [abstract]: "While presenting LLM-generated responses, including GPT-4, and having them interact directly with users in real-life mental health care services may raise ethical issues..."
  - [corpus]: Weak signal; no neighbor papers explicitly compare model sizes or training domains for CBT effectiveness
- Break condition: If fine-tuning OsakaED on CBT-specific data yields better results, breaking the size/training domain assumption

## Foundational Learning

- Concept: Socratic questioning in CBT
  - Why needed here: The study compares traditional Socratic questioning (SQ) to LLM-generated responses, so understanding its role is essential
  - Quick check question: What is the primary therapeutic goal of Socratic questioning in CBT?
- Concept: Cognitive restructuring and mood change measurement
  - Why needed here: The study evaluates outcomes using mood change equations and cognitive change scales; engineers must interpret these metrics
  - Quick check question: How is mood change calculated in this study?
- Concept: Ethical considerations in AI mental health tools
  - Why needed here: The paper raises ethical concerns about direct LLM-user interaction in mental health contexts
  - Quick check question: What are two main ethical risks of using LLMs in mental health applications?

## Architecture Onboarding

- Component map:
  - Scenario engine -> LLM integration (OsakaED/GPT-4) -> Socratic question injector (optional) -> Evaluation harness -> Safety filter
- Critical path:
  1. Load CBT scenario
  2. Generate LLM response (or use SQ)
  3. Optionally append Socratic question
  4. Display to user
  5. Collect subjective ratings
- Design tradeoffs:
  - Speed vs. quality: GPT-4 responses are slower; OsakaED is faster but less effective
  - Safety vs. naturalness: NG word filtering reduces risk but may limit response diversity
  - Human-in-the-loop vs. automation: Ethical safety favors human review over direct LLM output
- Failure signatures:
  - High NG word filtering leading to "no valid response" errors
  - Low mood change despite high empathy ratings (indicating superficial empathy)
  - Inconsistent Socratic question generation causing confusion
- First 3 experiments:
  1. Run SQ baseline with fixed response timing to control for latency bias
  2. Generate GPT-4 responses with and without Socratic injection; compare empathy ratings
  3. Test OsakaED fine-tuned on CBT data vs. original OsakaED to isolate training domain impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of GPT-4 in CBT dialogue systems compare to traditional human therapists in terms of long-term mental health outcomes?
- Basis in paper: [inferred] The paper compares GPT-4 to traditional Socratic questioning (SQ) in a single-session CBT dialogue, showing significant improvements in mood change and empathy. However, it does not compare GPT-4 to human therapists or assess long-term outcomes.
- Why unresolved: The study focuses on a single-session experiment with crowdsourced participants, not a long-term comparison with human therapists.
- What evidence would resolve it: A randomized controlled trial comparing GPT-4-based CBT dialogue systems to traditional human therapists over multiple sessions, measuring long-term mental health outcomes.

### Open Question 2
- Question: What specific features of GPT-4's responses contribute to its effectiveness in improving mood and dialogue quality in CBT settings?
- Basis in paper: [inferred] The paper notes that GPT-4's responses are more emotionally rich and empathetic, leading to improved outcomes. However, it does not analyze the specific features of these responses.
- Why unresolved: The study does not perform a detailed analysis of the linguistic or psychological features of GPT-4's responses that contribute to its effectiveness.
- What evidence would resolve it: A linguistic and psychological analysis of GPT-4's responses, identifying specific features (e.g., empathy, clarity, engagement) that correlate with improved outcomes.

### Open Question 3
- Question: How can the ethical concerns of using LLMs in mental healthcare be addressed while maintaining the benefits of their high response generation capability?
- Basis in paper: [explicit] The paper discusses ethical concerns about using LLMs like GPT-4 in mental healthcare, including safety and the inability to fully control responses. It suggests a human-in-the-loop approach using example responses or templates created by human experts and LLMs.
- Why unresolved: The paper proposes a potential solution but does not provide evidence of its effectiveness or feasibility in real-world applications.
- What evidence would resolve it: A pilot study implementing a human-in-the-loop system using GPT-4-generated responses, supervised by human experts, to assess safety, effectiveness, and user satisfaction.

## Limitations

- The study relies entirely on subjective user ratings rather than objective clinical outcomes, limiting confidence in therapeutic effectiveness claims
- The OsakaED model's specific implementation details (training hyperparameters, NG word list) are not fully specified, making exact reproduction challenging
- The study only tested one CBT scenario with 15 utterances, raising questions about generalizability across diverse clinical situations and longer-term therapeutic effects

## Confidence

- **High confidence**: GPT-4 significantly improves mood change and dialogue quality compared to traditional SQ methods
- **Medium confidence**: GPT-4 generates more empathetic and trustworthy responses than OsakaED, though this relies on subjective measures
- **Low confidence**: Claims about ethical risks of direct LLM-user interaction require further real-world validation beyond the simulated study conditions

## Next Checks

1. Conduct longitudinal studies with actual CBT patients to validate subjective findings with clinical outcome measures
2. Test multiple diverse CBT scenarios beyond the single 15-utterance script to assess generalizability
3. Implement and test a human-in-the-loop system where LLM responses are reviewed by qualified therapists before delivery to users