---
ver: rpa2
title: 'SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise
  Optimal Budget'
arxiv_id: '2404.04793'
source_url: https://arxiv.org/abs/2404.04793
tags:
- attention
- kv-cache
- layers
- layer
- cache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a 2D KV-cache compression algorithm called
  SQUEEZE ATTENTION that optimizes cache allocation from both sequence and layer dimensions
  in LLM inference. By tracking cosine similarity across attention layers, it identifies
  layer-wise importance and reallocates cache budgets accordingly.
---

# SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget

## Quick Facts
- arXiv ID: 2404.04793
- Source URL: https://arxiv.org/abs/2404.04793
- Authors: Zihao Wang; Bin Cui; Shaoduo Gan
- Reference count: 7
- Primary result: 30-70% memory reduction and up to 2.2x throughput improvement while maintaining model accuracy

## Executive Summary
SqueezeAttention introduces a 2D KV-cache compression algorithm that optimizes cache allocation across both sequence and layer dimensions in LLM inference. By measuring cosine similarity between hidden states before and after self-attention layers, the method identifies layer-wise importance and reallocates cache budgets accordingly. The approach achieves significant memory savings (30-70%) and throughput improvements (up to 2.2x) across 7 models (6.7B-70B parameters) and 5 datasets, while maintaining comparable accuracy to full cache baselines. The algorithm is designed to be orthogonal to existing sequence-wise compression methods, allowing seamless integration with approaches like Sliding Window, H2O, or StreamingLLM.

## Method Summary
The SqueezeAttention algorithm operates in two phases: prefilling and decoding. During prefilling, it collects cosine similarity metrics between embeddings before and after each self-attention layer to quantify layer importance. These metrics are clustered using KMeans to identify groups of similarly important layers. Based on cluster importance, the algorithm automatically reallocates cache budgets, assigning more space to important layers and less to unimportant ones. During decoding, each layer applies sequence-wise compression (e.g., H2O, Sliding Window) independently with its allocated budget. This layer-wise budget reallocation is orthogonal to sequence compression algorithms, allowing SqueezeAttention to be combined with any existing intra-layer compression method.

## Key Results
- Achieves 30-70% memory reduction compared to full cache baselines
- Delivers up to 2.2x throughput improvement across tested models
- Maintains comparable model accuracy to full cache while using significantly less memory
- Demonstrates consistent performance across 7 LLM models (6.7B-70B parameters) and 5 datasets

## Why This Works (Mechanism)

### Mechanism 1: Cosine Similarity as Importance Metric
- Claim: The cosine similarity between hidden states before and after self-attention quantifies layer importance
- Mechanism: In each attention layer, the model computes two embeddings (before and after self-attention). The cosine similarity between these embeddings indicates how much the layer modifies the input. Low similarity means high impact, high similarity means low impact. This allows ranking layers by their contribution to output quality.
- Core assumption: Cosine similarity is a robust proxy for measuring information change in NLP embeddings
- Evidence anchors: [abstract] "we first measure each layer's importance by calculating the cosine similarity of the input prompt differences before and after the self-attention layers"; [section] "The intuition is that the more similar the embeddings are after the attention computing (indicated by higher cosine similarity), the less information this attention layer could insert into the embedding"
- Break condition: If cosine similarity fails to correlate with actual layer importance for specific tasks or models

### Mechanism 2: Cross-Model Layer Importance Patterns
- Claim: Layer importance patterns are consistent across different LLM models and datasets
- Mechanism: By clustering layers based on their average cosine similarity scores, the algorithm identifies groups of similarly important layers. This clustering reveals that certain layers (typically first/last few and middle layers) consistently show distinct importance patterns regardless of the specific model or task.
- Core assumption: Attention layer importance follows predictable patterns across diverse models
- Evidence anchors: [section] "we found some common characteristics. Firstly, the first half of attention layers, in general, contributes more to the output embedding than what second half does. Secondly, some specific layers, typically the first and last few layers, might be more important than other layers, depending on the specific model and dataset"
- Break condition: If specific models or tasks exhibit atypical layer importance distributions

### Mechanism 3: Orthogonal Combination with Sequence Compression
- Claim: Layer-wise budget reallocation can be combined with any sequence-wise compression algorithm
- Mechanism: Since SqueezeAttention only modifies how cache budgets are distributed across layers (not the actual compression policy), it can be applied on top of existing methods like Sliding Window, H2O, or StreamingLLM. Each layer gets its own budget based on importance, then applies the sequence compression algorithm independently.
- Core assumption: Sequence-wise compression algorithms can operate independently on layers with different budgets
- Evidence anchors: [abstract] "SQUEEZE ATTENTION is orthogonal to all those intra-layer KV-cache compression algorithms, so it can be smoothly combined with any of them"; [section] "SQUEEZE ATTENTION automatically reallocates the binit for each layer in a way that more budgets are assigned to the more 'important' layer groups. Since layers have different cache budgets, in the decoding phase, Cseq works separately with each layer's very own budgets"
- Break condition: If sequence compression algorithms fail when applied to layers with varying budgets

## Foundational Learning

- Concept: KV cache mechanics in autoregressive transformer inference
  - Why needed here: Understanding how KV cache grows linearly with context length and layers is essential to grasp why layer-wise optimization matters
  - Quick check question: Why does KV cache size grow linearly with context length and number of layers?

- Concept: Layer-wise importance measurement through cosine similarity
  - Why needed here: The core innovation relies on quantifying how much each layer changes embeddings to determine budget allocation
  - Quick check question: What does a high cosine similarity between before/after self-attention embeddings indicate about layer importance?

- Concept: Clustering algorithms for grouping similar layers
  - Why needed here: KMeans clustering groups layers by importance scores to determine budget reallocation strategy
  - Quick check question: Why does the algorithm use clustering instead of simply ranking layers by importance?

## Architecture Onboarding

- Component map: Input pipeline -> Cosine similarity tracker -> Layer importance clusterer -> Budget allocator -> Sequence compressor -> KV cache manager
- Critical path: Prefilling → cosine similarity collection → clustering → budget allocation → decoding with per-layer compression
- Design tradeoffs: More complex than single-dimension compression but achieves better accuracy-memory balance; requires tracking additional metrics during prefilling
- Failure signatures: Degraded model accuracy when unimportant layers retain too much budget; out-of-memory errors if clustering misallocates budgets
- First 3 experiments:
  1. Run baseline with full cache on a small model to establish accuracy baseline
  2. Apply sequence-only compression (e.g., Sliding Window) to verify expected memory savings and accuracy drop
  3. Add SqueezeAttention layer clustering to same sequence compressor to verify accuracy improvement at same memory budget

## Open Questions the Paper Calls Out
None

## Limitations
- Cosine similarity mechanism may not generalize across all NLP tasks and model architectures
- Cross-model consistency assumption lacks comprehensive validation across diverse architectures
- Orthogonal combination claim with sequence-wise compressors needs broader empirical validation

## Confidence
**High Confidence Claims:**
- Memory reduction (30-70%) and throughput improvement (up to 2.2x) measurements are well-supported
- Overall framework of combining layer-wise and sequence-wise compression is technically sound
- Layer importance patterns (first layers more important, some layers consistently stand out) are observable

**Medium Confidence Claims:**
- Cosine similarity as a robust proxy for measuring layer importance across diverse tasks and models
- Consistency of layer importance patterns across different LLM architectures
- Claim that SqueezeAttention can be combined with any sequence-wise compression algorithm without issues

**Low Confidence Claims:**
- Generalization to models outside the tested 6.7B-70B parameter range
- Performance on specialized domains or non-standard NLP tasks
- Behavior with alternative model architectures (non-transformer models)

## Next Checks
1. **Cross-Architecture Validation**: Test SqueezeAttention on non-standard transformer architectures (Mamba, RWKV, or hybrid models) to verify whether the cosine similarity importance metric and layer clustering patterns generalize beyond conventional transformers.

2. **Edge Case Analysis**: Systematically evaluate performance on pathological cases - extremely long sequences (>32K tokens), highly repetitive content, or tasks requiring deep reasoning - to identify conditions where the importance estimation breaks down.

3. **Real-time Streaming Evaluation**: Implement a true streaming scenario without prefilling phase to measure performance degradation and determine the minimum context required for reliable layer importance estimation.