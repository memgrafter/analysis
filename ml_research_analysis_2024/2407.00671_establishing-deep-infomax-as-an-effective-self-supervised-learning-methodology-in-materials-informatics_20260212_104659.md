---
ver: rpa2
title: Establishing Deep InfoMax as an effective self-supervised learning methodology
  in materials informatics
arxiv_id: '2407.00671'
source_url: https://arxiv.org/abs/2407.00671
tags:
- deep
- infomax
- representation
- information
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that Deep InfoMax pretraining improves
  downstream performance of materials property prediction models when limited property
  labels are available. The authors apply Deep InfoMax to the Site-Net architecture
  for crystals, maximizing mutual information between local environment sets and their
  global representations.
---

# Establishing Deep InfoMax as an effective self-supervised learning methodology in materials informatics

## Quick Facts
- **arXiv ID**: 2407.00671
- **Source URL**: https://arxiv.org/abs/2407.00671
- **Reference count**: 40
- **Primary result**: Deep InfoMax pretraining improves materials property prediction performance when limited labeled data are available, particularly for formation energy prediction

## Executive Summary
This paper demonstrates that Deep InfoMax self-supervised pretraining can improve downstream materials property prediction when labeled data are scarce. The authors apply Deep InfoMax to the Site-Net architecture for crystals, maximizing mutual information between local environment sets and their global representations. The approach shows consistent improvements for formation energy prediction across various label availability scenarios and provides modest gains for band gap prediction when fewer than 100 property labels are available. The methodology leverages synthetic false samples and noise injection to learn representations that cluster similar materials in latent space.

## Method Summary
The methodology implements Deep InfoMax self-supervised pretraining on the Site-Net architecture for crystal structures. The approach maximizes mutual information between local atomic environments and global crystal representations using Jensen-Shannon entropy and Kullback-Leibler divergence losses. Synthetic false samples (false polymorphs, false compositions, false permutations) are engineered to preserve compositional information while altering structure, forcing the model to learn structural rather than compositional features. The pretraining is performed on large unlabeled crystal datasets, followed by transfer learning or representation learning for downstream property prediction tasks including formation energy and band gap. Performance is evaluated across different label availability scenarios (50-1000 samples) using mean absolute error as the primary metric.

## Key Results
- Deep InfoMax pretraining reduces formation energy prediction MAE by ~0.02-0.03 eV/atom compared to untrained baselines
- Small performance gains observed for band gap prediction when fewer than 100 property labels are available
- Learned representations show improved clustering of halogen-containing materials in t-SNE visualizations
- Untrained Deep InfoMax models with random projections perform competitively with trained models for small datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Maximizing mutual information between local crystal environments and their global representations preserves chemical and structural information relevant to downstream property prediction.
- Mechanism: Deep InfoMax uses a classifier to distinguish true local environment samples from false samples (other local environments), forcing the model to learn embeddings that retain distinguishability based on chemical context. This mutual information maximization acts as a lower bound ensuring representations are not degenerate.
- Core assumption: Local environments contain sufficient intrinsic information to reconstruct or discriminate global crystal properties without requiring explicit property labels.
- Evidence anchors:
  - [abstract] "Deep InfoMax pretraining implemented on the Site-Net architecture to improve the performance of downstream property prediction models with small amounts (<10^3) of data"
  - [section] "Deep InfoMax is an alternative to the autoencoder that verifies the quality of the representations through a loss function that explicitly maximises the mutual information between the representation and the original input - without reconstruction"
  - [corpus] Weak: no direct corpus evidence comparing mutual information maximization to other self-supervised methods in materials informatics.

### Mechanism 2
- Claim: Synthetic false samples that preserve composition but alter structure force the model to encode structural information beyond composition.
- Mechanism: By introducing "false polymorphs" and "false permutations" that maintain stoichiometry but change geometry, the model must learn to differentiate based on spatial arrangements rather than just element counts, thereby embedding structural context.
- Core assumption: Property labels like formation energy are more sensitive to structural geometry than composition alone, so representations that capture structure will improve predictions.
- Evidence anchors:
  - [section] "We engineer synthetic false samples that cannot be discriminated on composition alone... The first kind of synthetic false sample is the 'false polymorph' generated by taking the structure from another crystal in the dataset but keeping the site identities of the true crystal."
  - [corpus] Weak: no corpus evidence on synthetic false sampling strategies in materials informatics; relies on domain knowledge assumption.

### Mechanism 3
- Claim: Noise injection and Kullback-Leibler regularization encourage clustering of similar materials in representation space, improving generalization for small labeled datasets.
- Mechanism: Adding Gaussian noise to representations before classification forces the model to produce smooth latent spaces where nearby points share properties. The KL term balances noise magnitude against mutual information preservation.
- Core assumption: Materials with similar compositions and structures will have similar properties, so clustering in latent space improves downstream regression accuracy.
- Evidence anchors:
  - [section] "By introducing noise at this stage of the model, the model is encouraged to place similar crystals closer together in the representation space"
  - [corpus] Weak: no direct corpus evidence comparing noise-injected representations to non-noisy ones in materials property prediction.

## Foundational Learning

- **Concept**: Mutual information maximization as a self-supervised learning objective
  - Why needed here: Provides a principled way to learn representations from unlabeled crystal data by quantifying shared information between inputs and embeddings without requiring reconstruction.
  - Quick check question: What is the difference between maximizing mutual information and using contrastive loss in self-supervised learning?

- **Concept**: Set and graph permutation invariance in crystal representations
  - Why needed here: Crystal structures are unordered sets of atomic sites; the model must produce consistent representations regardless of atom ordering to be chemically meaningful.
  - Quick check question: How does mean pooling enforce permutation invariance in Site-Net's architecture?

- **Concept**: Transfer learning from self-supervised pretraining
  - Why needed here: Small labeled datasets in materials science make direct supervised learning prone to overfitting; pretraining on large unlabeled datasets provides better initialization.
  - Quick check question: Why does transfer learning from Deep InfoMax improve performance more with fewer labels than with many labels?

## Architecture Onboarding

- **Component map**: Input crystal → pairwise interaction features → local environment attention → local DIM loss → global pooling → global DIM loss → final representation
- **Critical path**: Input crystal → pairwise interaction features → local environment attention → local DIM loss → global pooling → global DIM loss → final representation
- **Design tradeoffs**: Deep InfoMax avoids reconstruction complexity but requires careful false sampling; simpler than autoencoders but potentially less expressive if sampling is poor
- **Failure signatures**: Poor downstream performance despite pretraining indicates false sampling mismatch; overtraining on small datasets suggests noise regularization insufficient
- **First 3 experiments**:
  1. Train Deep InfoMax with only real false samples (no synthetic) and compare representation quality on formation energy
  2. Vary noise magnitude (β parameter) and measure impact on downstream MAE for different dataset sizes
  3. Replace Deep InfoMax loss with contrastive loss (SimCLR-style) and benchmark against original approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal false sampling strategy for maximizing Deep InfoMax performance on band gap prediction tasks?
- Basis in paper: [explicit] The paper demonstrates that the current engineered false samples work better for formation energy than band gap prediction, suggesting the false sampling strategy is task-dependent and could be optimized.
- Why unresolved: The authors acknowledge that the false sampling strategy was chosen heuristically based on formation energy chemistry and suggest that a different strategy focusing on band structure might improve band gap results.
- What evidence would resolve it: Experimental comparison of multiple false sampling strategies (e.g., focusing on dopants, band structure perturbations, defect chemistry) applied to band gap prediction tasks, with systematic evaluation of downstream performance.

### Open Question 2
- Question: How does Deep InfoMax pretraining performance scale with dataset size in the presence of distributional shift between pretraining and downstream datasets?
- Basis in paper: [explicit] The paper acknowledges that their controlled experiments removed distributional shift by using the same dataset for pretraining and downstream tasks, but notes this is not realistic for practical applications.
- Why unresolved: The authors explicitly state that removing distributional shift allows separation of Deep InfoMax value from dataset effects, but practical applications require understanding performance when pretraining and downstream datasets differ.
- What evidence would resolve it: Systematic experiments varying the degree of distributional shift between pretraining and downstream datasets, measuring transfer learning performance across different levels of domain divergence.

### Open Question 3
- Question: What is the relative contribution of the untrained Deep InfoMax model's random projections versus the trained model's learned representations for downstream performance?
- Basis in paper: [explicit] The paper finds that untrained Deep InfoMax models produce representations competitive with trained models and manual featurizers, especially for small datasets, suggesting random projections may be a significant factor.
- Why unresolved: While the paper demonstrates untrained models are strong baselines, it doesn't quantify how much performance gain comes from random initialization versus actual Deep InfoMax training.
- What evidence would resolve it: Ablation studies comparing untrained Deep InfoMax, trained Deep InfoMax, and random projection baselines with matched dimensionalities, measuring the incremental performance gain from each component.

## Limitations

- Effectiveness varies significantly between tasks, with formation energy showing consistent improvement while band gap gains are limited to small label scenarios
- Synthetic false sampling strategy lacks comprehensive validation and may introduce domain-specific biases
- Approach inherits limitations from Site-Net architecture's pairwise interaction features

## Confidence

- **High confidence**: Deep InfoMax improves formation energy prediction with limited labels (supported by consistent MAE reductions across multiple experiments)
- **Medium confidence**: Small gains for band gap prediction with <100 labels (limited to narrow label range, inconsistent across all experiments)
- **Medium confidence**: Clustering improvements for halogen-containing materials (based on t-SNE visualization but not quantified statistically)

## Next Checks

1. **Quantify false sample realism**: Systematically evaluate how different false sample generation strategies affect downstream performance by testing with varying levels of synthetic sample realism
2. **Cross-task generalization**: Apply the same Deep InfoMax pretraining approach to additional materials properties beyond formation energy and band gap to assess general applicability
3. **Noise parameter sensitivity**: Conduct a systematic ablation study on the KL regularization parameter β across multiple noise levels to identify optimal settings for different property prediction tasks