---
ver: rpa2
title: 'Modeling the Heterogeneous Duration of User Interest in Time-Dependent Recommendation:
  A Hidden Semi-Markov Approach'
arxiv_id: '2412.11127'
source_url: https://arxiv.org/abs/2412.11127
tags:
- state
- systems
- time
- users
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hidden semi-Markov model (HSMM) for time-dependent
  recommendation, addressing the limitation of hidden Markov models (HMMs) that assume
  exponentially decreasing user interest durations. The HSMM allows flexible modeling
  of heterogeneous user interest durations by explicitly tracking how long users stay
  in each latent state, which can span multiple time periods.
---

# Modeling the Heterogeneous Duration of User Interest in Time-Dependent Recommendation: A Hidden Semi-Markov Approach

## Quick Facts
- arXiv ID: 2412.11127
- Source URL: https://arxiv.org/abs/2412.11127
- Authors: Haidong Zhang; Wancheng Ni; Xin Li; Yiping Yang
- Reference count: 40
- Key outcome: HSMM outperforms HMM and time-decay methods by 10-20% in precision, recall, and F-measure on Netflix, Delicious, and Last.fm datasets

## Executive Summary
This paper addresses the limitation of hidden Markov models in time-dependent recommendation systems, which assume exponentially decreasing user interest durations. The proposed hidden semi-Markov model (HSMM) explicitly tracks how long users stay in each latent interest state, allowing flexible modeling of heterogeneous duration patterns. An expectation maximization algorithm with MAP estimation is derived to learn model parameters. Experiments on three real-world datasets demonstrate significant performance improvements over state-of-the-art time-dependent and static baseline methods, particularly in datasets with greater user interest drift and duration heterogeneity.

## Method Summary
The HSMM extends HMM by explicitly modeling the duration of each latent state rather than assuming geometric decay. The model uses monthly aggregated user-item interaction data and learns parameters through an EM algorithm with MAP estimation. Key parameters include initial state probabilities (π), transition probabilities (A), state duration distributions (D), and item consumption parameters (r, p for negative binomial distribution, θ for item distribution). The forward-backward algorithm computes posterior distributions for state inference, and predictions incorporate multi-period historical data within current states. The model is evaluated using precision, recall, and F1-score for top-5 and top-10 recommendations, with statistical significance determined via pairwise t-tests.

## Key Results
- HSMM significantly outperforms HMM, Katz-CWT, tIB, timeSVD, and static baselines on Netflix, Delicious, and Last.fm datasets
- Performance improvements of 10-20% in precision, recall, and F-measure compared to state-of-the-art methods
- Advantage is particularly pronounced in datasets with greater user interest drift and duration heterogeneity
- HSMM demonstrates superior ability to capture varying state duration distributions compared to HMM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HSMM captures heterogeneous user interest durations better than HMM by allowing variable state durations instead of geometric decay.
- Mechanism: HSMM models the duration of each latent state with a non-parametric distribution D, explicitly tracking how long users stay in each state. This contrasts with HMM's geometric distribution assumption, where probability of staying decreases exponentially over time.
- Core assumption: User interest duration is not uniformly geometrically distributed but varies across users and contexts.
- Evidence anchors:
  - [abstract]: "The HSMM allows flexible modeling of heterogeneous user interest durations by explicitly tracking how long users stay in each latent state, which can span multiple time periods."
  - [section]: "The HMM setup assumes users have a probability to leave a state at the end of its duration. But the probability of staying in one state for multiple periods decreases exponentially, and the duration for users' stay in different interest states follows a geometric distribution."

### Mechanism 2
- Claim: HSMM improves recommendation accuracy by incorporating multi-period historical data during state-based predictions.
- Mechanism: When predicting item consumption at time t, HSMM uses information from all periods within the current state duration, not just the immediate previous period as HMM does. This leverages more historical context per user.
- Core assumption: Recent historical data within the same latent state contains predictive signal for future behavior.
- Evidence anchors:
  - [abstract]: "Experiments on three real-world datasets show that our model significantly outperforms the state-of-the-art time-dependent and static benchmark methods."
  - [section]: "Since user's multiple period stay in one state is explicitly modeled, the prediction also considers multiple periods' data (in a semi-Markov fashion)."

### Mechanism 3
- Claim: HSMM outperforms time-decay methods by avoiding arbitrary weighting assumptions and capturing user-specific interest evolution patterns.
- Mechanism: Instead of applying fixed decay factors, HSMM learns state transition and duration patterns from data, allowing different users to have different temporal evolution patterns without pre-specified decay rules.
- Core assumption: Fixed decay factors cannot capture the diversity of how user interests evolve over time.
- Evidence anchors:
  - [abstract]: "Experiments on three real-world datasets show that our model significantly outperforms the state-of-the-art time-dependent and static benchmark methods."
  - [section]: "Katz-CWT and tIB methods assume a constant decaying tendency across users and across different time periods. For some users, that will lose information due to the underweighting of old data."

## Foundational Learning

- Concept: Hidden Markov Models (HMM)
  - Why needed here: HSMM extends HMM, so understanding HMM's state transition and emission mechanisms is foundational.
  - Quick check question: In HMM, if a user is in state k at time t, what is the probability they stay in state k at time t+1?

- Concept: Expectation Maximization (EM) Algorithm
  - Why needed here: The paper uses EM with MAP estimation to learn HSMM parameters from data.
  - Quick check question: What are the two main steps in the EM algorithm, and what does each compute?

- Concept: Negative Binomial Distribution
  - Why needed here: The model assumes total item consumptions follow NBD to handle count data overdispersion.
  - Quick check question: Why might NBD be preferred over Poisson distribution for modeling user consumption counts?

## Architecture Onboarding

- Component map: Data Preparation -> EM Training -> State Inference -> Prediction -> Evaluation
- Critical path: Data → EM Training → State Inference → Prediction → Evaluation
- Design tradeoffs:
  - Complexity vs. accuracy: HSMM (O(K²M) transitions) vs. HMM (O(K²) transitions)
  - Maximum duration M: Larger M captures longer interests but increases computation
  - Number of states K: More states capture heterogeneity but risk overfitting
- Failure signatures:
  - Poor performance on datasets with stable interests (Last.fm example)
  - Slow training convergence indicating poor initialization
  - Degenerate state duration distributions suggesting model misspecification
- First 3 experiments:
  1. Run HSMM vs. HMM on Netflix with K=20, M=3 to verify basic performance improvement
  2. Vary M from 1 to 5 with fixed K to find optimal maximum duration
  3. Compare HSMM against Katz-CWT on Delicious to validate advantage over time-decay methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance scale with the number of users and items when the state duration bound M is increased?
- Basis in paper: [inferred] The paper only tests with a small subset of the data (filtered by frequency) and fixed M values.
- Why unresolved: The sensitivity analysis focuses on K and M, but not on dataset size or item sparsity.
- What evidence would resolve it: Experiments showing performance degradation or plateau with larger user/item counts and varying M.

### Open Question 2
- Question: What is the effect of allowing non-stationary state transitions (e.g., time-dependent A matrix) on recommendation accuracy?
- Basis in paper: [explicit] Section V-E mentions this as a limitation and references prior work on non-stationary HMMs.
- Why unresolved: The current model assumes stationarity; no experiments with dynamic transition probabilities are presented.
- What evidence would resolve it: A comparison of HSMM with and without time-varying transition matrices on the same datasets.

### Open Question 3
- Question: Does the HSMM still outperform other methods if the time granularity is reduced (e.g., daily instead of monthly)?
- Basis in paper: [explicit] Section V-E suggests this as a future direction to examine within-state heterogeneity and sparsity.
- Why unresolved: All experiments use monthly aggregation; finer granularity might change state duration distributions.
- What evidence would resolve it: Results from experiments with daily or weekly time steps.

## Limitations
- The paper doesn't conduct ablation studies isolating the duration modeling component to verify its specific contribution
- Evaluation assumes monthly aggregation, which may not reflect real-world recommendation scenarios requiring finer-grained predictions
- Datasets represent specific domains (movies, bookmarks, music) that may not generalize to other recommendation contexts

## Confidence
- **High confidence**: HSMM outperforms HMM on datasets with clear duration heterogeneity (Netflix, Delicious)
- **Medium confidence**: HSMM's advantage over time-decay methods is statistically significant but the magnitude varies across datasets
- **Low confidence**: Claims about HSMM's superiority in datasets with stable interests (Last.fm) - the paper acknowledges smaller improvements but doesn't fully explain why

## Next Checks
1. Conduct ablation experiments comparing HSMM with duration modeling disabled vs. HMM to isolate the specific contribution of heterogeneous duration modeling
2. Test model performance across different temporal granularities (weekly vs. monthly aggregation) to assess robustness to temporal resolution choices
3. Evaluate HSMM on additional recommendation domains (e.g., e-commerce, news) to test generalizability beyond the three studied domains