---
ver: rpa2
title: 'Perception Compressor: A Training-Free Prompt Compression Framework in Long
  Context Scenarios'
arxiv_id: '2409.19272'
source_url: https://arxiv.org/abs/2409.19272
tags:
- perception
- methods
- compression
- context
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Perception Compressor, a training-free prompt
  compression framework designed to address two key challenges in long context scenarios
  with large language models: redundant information and sensitivity to the position
  of key information. The method includes three main components: a perception retriever
  that uses guiding questions and instruction to retrieve the most relevant demonstrations,
  a dual-slope ratio allocator that dynamically allocates compression ratios and open-book
  ratios, and a semi-guided iterative compression that retains key information at
  the token level while removing distracting tokens.'
---

# Perception Compressor: A Training-Free Prompt Compression Framework in Long Context Scenarios

## Quick Facts
- arXiv ID: 2409.19272
- Source URL: https://arxiv.org/abs/2409.19272
- Authors: Jiwei Tang; Jin Xu; Tingwei Lu; Zhicheng Zhang; Yiming Zhao; Lin Hai; Hai-Tao Zheng
- Reference count: 4
- One-line primary result: Perception Compressor achieves SOTA performance on multiple benchmarks for long context prompt compression

## Executive Summary
This paper introduces Perception Compressor, a training-free prompt compression framework designed to address two key challenges in long context scenarios with large language models: redundant information and sensitivity to the position of key information. The method includes three main components: a perception retriever that uses guiding questions and instruction to retrieve the most relevant demonstrations, a dual-slope ratio allocator that dynamically allocates compression ratios and open-book ratios, and a semi-guided iterative compression that retains key information at the token level while removing distracting tokens. Extensive experiments on benchmarks including NaturalQuestions, LongBench, and MuSiQue demonstrate that Perception Compressor outperforms existing methods, achieving state-of-the-art performance. For example, it achieves a recall@1 of 72.3% on NaturalQuestions compared to 67.1% for the second-best method (LongLLMLingua), while using fewer tokens. Across various compression ratios and tasks, Perception Compressor consistently delivers superior results, effectively solving the challenges of lost-in-the-middle and excessively long input sequences.

## Method Summary
Perception Compressor addresses long context prompt compression through a three-component framework. First, the perception retriever generates guiding questions from the input question and computes semantic similarity using SentenceBERT to reorder demonstrations based on relevance. Second, the dual-slope ratio allocator dynamically assigns compression ratios and open-book ratios to instruction, demonstrations, and question based on their relevance and predefined parameters. Third, semi-guided iterative compression uses a frozen LLM to compress segments of the prompt, retaining key information tokens (high contrast perplexity) and removing non-key information tokens (high perplexity) based on the allocated ratios. The method is training-free and operates at the token level to ensure key information is preserved while removing redundancy.

## Key Results
- Achieves state-of-the-art performance on NaturalQuestions, LongBench, and MuSiQue benchmarks
- Outperforms existing methods, including LongLLMLingua, with a recall@1 of 72.3% on NaturalQuestions
- Consistently delivers superior results across various compression ratios and tasks
- Effectively solves the challenges of lost-in-the-middle and excessively long input sequences

## Why This Works (Mechanism)
The framework works by addressing the core challenges of long context scenarios through intelligent information retrieval and compression. The perception retriever ensures relevant demonstrations are prioritized by using guiding questions derived from the input question, while the dual-slope ratio allocator dynamically allocates compression resources based on the importance of different prompt components. The semi-guided iterative compression then preserves key information at the token level while removing redundant or distracting tokens. This multi-layered approach ensures that critical information is retained even in long prompts, solving the lost-in-the-middle problem and improving overall performance.

## Foundational Learning

1. **Perception Retriever** (Why needed: To identify and prioritize relevant demonstrations based on the input question)
   - Quick check: Test the retriever's ability to reorder demonstrations using semantic similarity and guiding questions

2. **Dual-Slope Ratio Allocator** (Why needed: To dynamically allocate compression resources based on component importance)
   - Quick check: Validate the allocator's ability to assign appropriate compression ratios to instruction, demonstrations, and question

3. **Semi-Guided Iterative Compression** (Why needed: To retain key information at the token level while removing distractions)
   - Quick check: Evaluate the compressor's ability to distinguish between key and non-key tokens using perplexity

## Architecture Onboarding

**Component Map:** Perception Retriever -> Dual-Slope Ratio Allocator -> Semi-Guided Iterative Compression

**Critical Path:** The critical path involves the perception retriever identifying relevant demonstrations, the ratio allocator determining compression parameters, and the iterative compression applying these parameters to generate the final compressed prompt.

**Design Tradeoffs:** The framework trades computational complexity for improved performance by using multiple components (retriever, allocator, compressor) instead of a single compression method. This approach ensures better preservation of key information but requires more processing steps.

**Failure Signatures:** 
- Poor performance due to incorrect demonstration prioritization
- Inefficient compression from improper ratio allocation
- Loss of key information due to ineffective token-level compression

**First Experiments:**
1. Test perception retriever's ability to reorder demonstrations based on relevance using guiding questions and instruction
2. Validate dual-slope ratio allocator's dynamic allocation of compression ratios and open-book ratios
3. Evaluate semi-guided iterative compression's retention of key information tokens and removal of distracting tokens based on perplexity

## Open Questions the Paper Calls Out
None

## Limitations
- Implementation details and hyperparameters are not fully specified, affecting reproducibility
- The method requires multiple processing steps, increasing computational complexity
- Performance may vary depending on the quality of the frozen LLM used for compression

## Confidence
- Major claim (SOTA performance): High
- Implementation details: Medium
- Hyperparameter sensitivity: Low

## Next Checks
1. Verify the implementation of the perception retriever by testing its ability to reorder demonstrations based on relevance using guiding questions and instruction.
2. Validate the dual-slope ratio allocator by testing its dynamic allocation of compression ratios and open-book ratios.
3. Evaluate the semi-guided iterative compression by testing its ability to retain key information tokens and remove distracting tokens based on perplexity.