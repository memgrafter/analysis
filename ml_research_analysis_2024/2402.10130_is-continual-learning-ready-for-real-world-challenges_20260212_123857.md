---
ver: rpa2
title: Is Continual Learning Ready for Real-world Challenges?
arxiv_id: '2402.10130'
source_url: https://arxiv.org/abs/2402.10130
tags:
- learning
- continual
- tasks
- task
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the state of continual learning (CL) for 3D
  semantic segmentation, a critical task in robotics and autonomous systems. The authors
  argue that current CL methods struggle in realistic, online scenarios where data
  arrives in a streaming fashion and tasks are learned incrementally from scratch.
---

# Is Continual Learning Ready for Real-world Challenges?

## Quick Facts
- arXiv ID: 2402.10130
- Source URL: https://arxiv.org/abs/2402.10130
- Reference count: 31
- All methods perform poorly, with near-zero IoU after a few tasks

## Executive Summary
This paper examines the state of continual learning (CL) for 3D semantic segmentation, a critical task in robotics and autonomous systems. The authors argue that current CL methods struggle in realistic, online scenarios where data arrives in a streaming fashion and tasks are learned incrementally from scratch. To evaluate this, they introduce a new benchmark called OCL-3DSS, using modified versions of popular 3D datasets (ScanNet, S3DIS, SemanticKITTI) with a long sequence of tasks (up to 20) and an online learning setup. They test various CL methods including MAS, LwF, ER, and a recent 3D-specific method. Surprisingly, all methods perform poorly, with near-zero IoU after a few tasks, significantly underperforming the upper bound of joint offline training. Even ER, the best-performing method, only achieves 42.1% IoU on ScanNet with 20 tasks. The results suggest that current CL methods are not ready for real-world challenges and call for new experimental protocols and methods that better address the complexities of online, long-task-sequence learning.

## Method Summary
The authors introduce OCL-3DSS, a new benchmark for online continual learning in 3D semantic segmentation. They modify three popular 3D datasets (ScanNet, S3DIS, SemanticKITTI) to create long sequences of incremental tasks, with each task introducing new semantic classes. The online learning setup assumes single-pass data, where each scene is seen only once during training. They test various CL methods including fine-tuning (FT), regularization-based methods (MAS, LwF), experience replay (ER), and a recent 3D-specific method (Yang et al., 2023). Additionally, they explore language guidance using CLIP embeddings for weakly supervised learning. The evaluation metric is mean Intersection over Union (mIoU) after each task, comparing against a joint training upper bound.

## Key Results
- All CL methods perform poorly in online continual learning, with near-zero IoU after a few tasks
- ER is the best-performing method, achieving 42.1% IoU on ScanNet with 20 tasks
- Current CL methods significantly underperform the upper bound of joint offline training
- Language guidance using CLIP embeddings offers comparable performance to fully supervised methods with minimal labeled data (10 points per class)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Catastrophic forgetting dominates in online continual learning, especially beyond 5 tasks, due to single-pass training and class imbalance.
- Mechanism: The model updates parameters for the current task but does not retain previous task knowledge, causing rapid performance decay.
- Core assumption: Training data is seen only once, and classes are introduced incrementally without access to prior task data.
- Evidence anchors:
  - [abstract] "all methods perform poorly, with near-zero IoU after a few tasks"
  - [section] "Online continual learning, which assumes single-pass data, is strictly harder than offline, due to the combined challenges of catastrophic forgetting and underfitting within a single training epoch."
  - [corpus] Weak: No direct evidence on catastrophic forgetting from corpus neighbors.

### Mechanism 2
- Claim: Memory replay (e.g., ER) is the only method that maintains competitive performance in online continual learning.
- Mechanism: By storing and re-sampling past examples, ER mitigates forgetting and provides repeated exposure to previous classes.
- Core assumption: A memory buffer can be maintained and efficiently sampled during online training.
- Evidence anchors:
  - [abstract] "the replay-based method ER (Chaudhry et al., 2019) stands out as the sole approach to maintain competitiveness in an OCL-3DSS scenario."
  - [section] "As shown in Tables 4,5,6 the best performing method is ER (Chaudhry et al., 2019)."
  - [corpus] Weak: No direct evidence on ER's performance from corpus neighbors.

### Mechanism 3
- Claim: Language guidance (LG-CLIP) offers comparable performance to fully supervised methods with minimal labeled data.
- Mechanism: CLIP features are pre-separated in feature space, so aligning model features to language embeddings reduces forgetting.
- Core assumption: CLIP embeddings for different classes are linearly separable and can guide model features.
- Evidence anchors:
  - [abstract] "we additionally incorporated and evaluated language guidance"
  - [section] "Our scenario is much more challenging: we do not use images and the classes for regularization appear separately. But with only 10 points per class for supervision language guidance offers comparable results"
  - [corpus] Weak: No direct evidence on LG-CLIP from corpus neighbors.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Explains why continual learning is challenging and why most methods fail in long task sequences.
  - Quick check question: What happens to model performance on previous tasks when training on new tasks without any countermeasures?

- Concept: Experience replay
  - Why needed here: Describes the mechanism behind ER's success and how memory buffers help mitigate forgetting.
  - Quick check question: How does reservoir sampling ensure fairness in memory buffer updates?

- Concept: Knowledge distillation
  - Why needed here: Underlies LwF and other regularization-based methods for preserving previous task knowledge.
  - Quick check question: How does LwF use the previous model's outputs to regularize the current model?

## Architecture Onboarding

- Component map: Encoder (Res16UNet34A) -> Decoder (Convolutional/Transformer) -> Output
- Critical path:
  1. Initialize model with random weights
  2. For each task: train on current task data + memory buffer (ER) or regularization loss (MAS/LwF)
  3. Evaluate on all previous tasks
- Design tradeoffs:
  - Memory buffer size vs. performance: larger buffer reduces forgetting but increases computational cost
  - Fully supervised vs. weakly supervised (LG-CLIP): trade-off between annotation cost and performance
  - Convolutional vs. Transformer decoder: Transformer may require more data but offers better expressivity
- Failure signatures:
  - Near-zero IoU after a few tasks → catastrophic forgetting
  - Performance plateaus early → underfitting due to limited data
  - Memory buffer overflow → need for efficient sampling strategy
- First 3 experiments:
  1. Compare FT vs. ER with varying memory buffer sizes on ScanNet
  2. Test LG-CLIP with 10, 50, 100 labeled points per class
  3. Evaluate MAS vs. LwF on long task sequences (>10 tasks)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do current continual learning methods perform in realistic online scenarios with long task sequences and streaming data?
- Basis in paper: [explicit] The authors state that all considered methods perform poorly in their OCL-3DSS benchmark, with near-zero IoU after a few tasks, significantly underperforming the upper bound of joint offline training.
- Why unresolved: The paper introduces a new benchmark to evaluate CL methods in realistic scenarios, but it does not provide a comprehensive comparison with all existing CL methods or explore potential solutions to improve performance in these challenging conditions.
- What evidence would resolve it: A thorough evaluation of a wider range of CL methods on the OCL-3DSS benchmark, along with experiments exploring different architectures, loss functions, or training strategies to improve performance in online, long-task-sequence learning.

### Open Question 2
- Question: Can language models effectively guide continual learning for 3D semantic segmentation without extensive memory replay?
- Basis in paper: [explicit] The authors introduce language guidance using CLIP features to alleviate catastrophic forgetting, showing comparable results to other methods with limited supervision (10 points per class).
- Why unresolved: While the initial results are promising, the paper does not extensively explore the potential of language models in CL for 3D semantic segmentation, such as combining them with other techniques or investigating their effectiveness in different scenarios.
- What evidence would resolve it: Further experiments investigating the use of language models in combination with other CL techniques, exploring their effectiveness in various task sequences and data distributions, and comparing their performance to traditional memory replay methods.

### Open Question 3
- Question: How does the choice of decoder architecture impact the performance of continual learning methods in 3D semantic segmentation?
- Basis in paper: [explicit] The authors compare a conventional convolutional decoder with a Transformer decoder, finding that the Transformer decoder does not significantly improve performance and may even worsen it in later tasks.
- Why unresolved: The paper provides a preliminary comparison of two decoder architectures but does not explore other potential architectures or investigate the reasons behind the observed performance differences.
- What evidence would resolve it: A comprehensive evaluation of various decoder architectures (e.g., different types of Transformers, hybrid architectures) on the OCL-3DSS benchmark, along with an analysis of the strengths and weaknesses of each architecture in the context of CL for 3D semantic segmentation.

## Limitations
- The analysis lacks ablation studies on different forgetting mechanisms (e.g., separating interference vs. underfitting effects).
- The language guidance evaluation is limited to a single dataset (ScanNet) with a specific buffer size (400), making generalization uncertain.
- The paper does not extensively explore the potential of language models in CL for 3D semantic segmentation or investigate their effectiveness in different scenarios.

## Confidence
- Catastrophic forgetting in online CL: **High** - Multiple experiments consistently show performance collapse after 3-5 tasks
- ER as best-performing method: **High** - Clear quantitative superiority across all tested datasets and configurations
- Language guidance effectiveness: **Medium** - Promising results but limited evaluation scope
- Current CL methods not ready for real-world: **High** - Strong empirical evidence from challenging benchmark

## Next Checks
1. **Ablation study on forgetting mechanisms**: Separate the contributions of catastrophic forgetting vs. underfitting by testing methods with different training epochs per task and analyzing per-class forgetting patterns
2. **Memory buffer efficiency analysis**: Systematically evaluate ER performance across a wider range of buffer sizes (10-1000) and sampling strategies to identify the minimum effective buffer size
3. **Cross-dataset generalization test**: Validate language guidance performance on S3DIS and SemanticKITTI with varying numbers of labeled points (10, 50, 100) to assess scalability and robustness