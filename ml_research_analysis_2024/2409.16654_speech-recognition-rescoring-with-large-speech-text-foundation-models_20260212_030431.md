---
ver: rpa2
title: Speech Recognition Rescoring with Large Speech-Text Foundation Models
arxiv_id: '2409.16654'
source_url: https://arxiv.org/abs/2409.16654
tags:
- speech
- rescoring
- text
- speech-text
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using multi-modal large language models for
  second-pass automatic speech recognition rescoring. The authors introduce a speech-text
  foundation model that leverages both transcribed speech-text data and large amounts
  of unlabeled speech and text data for pre-training.
---

# Speech Recognition Rescoring with Large Speech-Text Foundation Models

## Quick Facts
- arXiv ID: 2409.16654
- Source URL: https://arxiv.org/abs/2409.16654
- Authors: Prashanth Gurunath Shivakumar; Jari Kolehmainen; Aditya Gourav; Yi Gu; Ankur Gandhe; Ariya Rastrow; Ivan Bulyko
- Reference count: 40
- Multi-modal foundation models improve ASR rescoring by up to 20% relative WER reduction

## Executive Summary
This paper introduces a novel approach to automatic speech recognition rescoring using multi-modal large language models that leverage both transcribed speech-text data and unlabeled speech and text. The proposed speech-text foundation model demonstrates significant improvements over traditional text-only rescoring approaches, achieving up to 20% relative word error rate reduction compared to Whisper large ASR models. The model's ability to use speech tokens in addition to text tokens during rescoring provides substantial performance gains while also enabling cross-modal knowledge transfer capabilities.

## Method Summary
The authors propose a speech-text foundation model that is pre-trained on a combination of transcribed speech-text data and large amounts of unlabeled speech and text data. The model uses speech tokens alongside traditional text tokens, allowing it to capture richer acoustic and linguistic information during rescoring. The approach includes discriminative fine-tuning using minimum word error rate (MWER) criteria to further enhance performance. The model architecture supports both speech-text joint processing and text-only rescoring, enabling flexible deployment scenarios. The training process leverages multi-modal data to create a foundation model that can be fine-tuned for specific ASR rescoring tasks.

## Key Results
- Up to 20% relative WER improvement over Whisper large ASR models
- 15% improvement over text-only large language models for rescoring
- 7B parameter speech-text model achieves best overall performance
- Cross-modal knowledge transfer enables text-only rescoring with speech pre-training
- Speech-only adaptation works without requiring transcripts

## Why This Works (Mechanism)
The speech-text foundation model works by leveraging the complementary information available in both acoustic and linguistic modalities. By training on multi-modal data, the model learns richer representations that capture both phonetic and semantic patterns simultaneously. The speech tokens provide direct access to acoustic features that text-only models cannot capture, while the text tokens provide contextual and semantic information. This dual representation allows the model to better disambiguate acoustically similar phrases and recover from recognition errors. The cross-modal knowledge transfer occurs because speech pre-training provides the model with robust acoustic representations that can be leveraged even when only text tokens are available during inference.

## Foundational Learning

*Multi-modal foundation models*: Why needed - To capture both acoustic and linguistic information simultaneously; Quick check - Model can process both speech and text inputs

*Speech tokenization*: Why needed - To convert continuous speech signals into discrete representations for LLM processing; Quick check - Tokens preserve phonetic and prosodic information

*Minimum Word Error Rate fine-tuning*: Why needed - To optimize directly for ASR performance metrics rather than traditional language modeling objectives; Quick check - WER decreases monotonically during MWER training

*Cross-modal transfer learning*: Why needed - To enable knowledge sharing between speech and text modalities for improved generalization; Quick check - Text-only rescoring performance improves after speech pre-training

## Architecture Onboarding

Component map: Speech input -> Speech tokenizer -> Speech-text foundation model -> Text output -> Rescoring layer -> Final transcript

Critical path: The model processes speech through a tokenizer to create speech tokens, which are then processed alongside text tokens through the foundation model. The rescoring layer evaluates multiple hypotheses and selects the most probable output based on both acoustic and linguistic likelihoods.

Design tradeoffs: The paper balances model size against performance gains, showing that larger models (7B parameters) achieve better results but at higher computational cost. The cross-modal approach requires careful balancing of speech and text representation learning.

Failure signatures: Performance degradation occurs when speech tokens are not properly aligned with text tokens, when the speech tokenizer fails to capture critical acoustic features, or when fine-tuning is insufficient for domain adaptation.

First experiments:
1. Test speech-text joint processing on clean speech data to establish baseline performance
2. Evaluate cross-modal transfer by comparing text-only rescoring with and without speech pre-training
3. Assess domain adaptation capabilities using speech-only adaptation on out-of-domain datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on English datasets, limiting cross-linguistic generalizability
- Substantial computational requirements for training multi-modal foundation models
- Trade-offs between model size and performance gains not fully explored for smaller applications

## Confidence

*Speech-text foundation models improve rescoring* (High confidence): Experimental results show consistent WER improvements across multiple datasets and model sizes with established metrics.

*Cross-modal knowledge transfer effectiveness* (Medium confidence): Improvements demonstrated when using text-only tokens, but mechanism requires more detailed analysis.

*Domain adaptation without transcripts* (Medium confidence): Speech-only adaptation shows promise but needs validation across more diverse domains.

## Next Checks

1. Evaluate model performance on non-English datasets to assess cross-linguistic generalizability and identify language-specific limitations.

2. Conduct systematic ablation studies varying model size to quantify computational cost versus performance gains across different use cases.

3. Test cross-modal knowledge transfer capabilities on truly out-of-domain tasks beyond current evaluation scope to better understand generalization properties.