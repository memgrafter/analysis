---
ver: rpa2
title: Large Language Model Confidence Estimation via Black-Box Access
arxiv_id: '2406.04370'
source_url: https://arxiv.org/abs/2406.04370
tags:
- confidence
- gpt-4
- flan-ul2
- mistral
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a black-box framework for estimating confidence
  in LLM responses using only query access. The method applies six input prompt perturbations
  (e.g., paraphrasing, sentence reordering, entity amplification, stopword removal,
  stochastic decoding, and response consistency checks) to elicit variable model behavior,
  then extracts semantic and lexical features from the resulting outputs.
---

# Large Language Model Confidence Estimation via Black-Box Access

## Quick Facts
- arXiv ID: 2406.04370
- Source URL: https://arxiv.org/abs/2406.04370
- Reference count: 40
- Outperforms baselines on AUROC, AUARC, and ECE by up to 31 percentage points

## Executive Summary
This paper introduces a black-box framework for estimating confidence in LLM responses using only query access. The method applies six input prompt perturbations (e.g., paraphrasing, sentence reordering, entity amplification, stopword removal, stochastic decoding, and response consistency checks) to elicit variable model behavior, then extracts semantic and lexical features from the resulting outputs. A logistic regression model trained on these features predicts whether a response is correct, with the predicted probability serving as the confidence estimate. Evaluated on four QA and two summarization datasets with six LLMs (Llama-2-13b-chat, Mistral-7B, Flan-ul2, GPT-4, Pegasus, and BART), the approach outperforms existing baselines on AUROC, AUARC, and ECE by up to 31 percentage points in some cases. The interpretable model also reveals which features drive performance, and shows that confidence models transfer well across LLMs within the same dataset, suggesting a universal model is feasible. The framework is extensible and effective with limited training data.

## Method Summary
The framework works by first applying six different prompt perturbations to the input, then querying the LLM multiple times for each perturbed prompt to generate diverse responses. It extracts two main types of features: semantic set counts (how many semantically distinct responses were generated) and lexical similarity scores (how similar responses are using Rouge scores). These features, along with optional prompt tokens, are used to train a logistic regression model that predicts response correctness. The predicted probability from this model serves as the confidence estimate. The approach requires only black-box access to the LLM and can be trained with limited data.

## Key Results
- Outperforms existing baselines on AUROC, AUARC, and ECE by up to 31 percentage points in some cases
- Confidence models trained on one LLM generalize well to other LLMs on the same dataset
- Logistic regression provides well-calibrated confidence estimates compared to baselines
- Framework works effectively with limited training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt perturbations increase the variance in LLM responses in a way that correlates with response correctness.
- Mechanism: The framework applies six different types of prompt perturbations (paraphrasing, sentence reordering, entity amplification, stopword removal, stochastic decoding, and response consistency checks). Each perturbation generates multiple responses from the LLM, and the variability in these responses is quantified using semantic set counts and lexical similarity scores. More variability indicates lower confidence in the correctness of the response.
- Core assumption: LLM responses exhibit systematic variability when prompts are perturbed in semantically equivalent ways, and this variability is indicative of the model's confidence in its answers.
- Evidence anchors:
  - [abstract]: "We propose a simple and extensible framework where, we engineer novel features and train a (interpretable) model (viz. logistic regression) on these features to estimate the confidence."
  - [section]: "Based on these features computed for different inputs we train a model to predict if the response was correct or incorrect."
  - [corpus]: Weak evidence - related papers focus on consistency and self-verification but don't directly support the specific perturbation approach used here.
- Break condition: If LLM responses are invariant to semantically equivalent prompt perturbations, the variability features would not capture confidence differences.

### Mechanism 2
- Claim: Logistic regression on semantic and lexical features provides well-calibrated confidence estimates for LLM responses.
- Mechanism: The framework extracts semantic diversity (number of semantic sets) and lexical similarity (average Rouge score) from the perturbed responses. These features, along with optional prompt tokens, are used to train a logistic regression model that predicts response correctness. The predicted probability from this model serves as the confidence estimate.
- Core assumption: The relationship between response variability features and correctness is linear and can be captured by logistic regression, which naturally produces calibrated probabilities.
- Evidence anchors:
  - [abstract]: "We empirically demonstrate that our simple framework is effective in estimating confidence... it surpassing baselines by even over 10% (on AUROC) in some cases."
  - [section]: "The probability of each such prediction is then the confidence that we output. Since, the models we use to produce such predictions are simple (viz. logistic regression) the confidence estimates are typically well calibrated."
  - [corpus]: Moderate evidence - some related work uses logistic regression for confidence estimation, but specific calibration performance is not well-established in the corpus.
- Break condition: If the relationship between features and correctness is non-linear or if logistic regression overfits with limited training data.

### Mechanism 3
- Claim: Confidence models trained on one LLM generalize well to other LLMs for the same dataset, enabling universal confidence estimation.
- Mechanism: The framework discovers that for a given dataset, the most important features for confidence estimation are shared across different LLMs. This allows a confidence model trained on one LLM to be applied zero-shot to other LLMs on the same dataset with reasonable performance.
- Core assumption: Different LLMs exhibit similar patterns of response variability when prompts are perturbed, making their confidence estimation features transferable across models for the same task domain.
- Evidence anchors:
  - [abstract]: "Additionally, our interpretable approach provides insight into features that are predictive of confidence, leading to the interesting and useful discovery that our confidence models built for one LLM generalize zero-shot across others on a given dataset."
  - [section]: "We interestingly find that for a given dataset the important features are shared across LLMs. Intrigued by this finding we apply confidence models built for one LLM to the responses of another and further find that they generalize well across LLMs."
  - [corpus]: Weak evidence - no direct support in related papers for cross-LLM generalization of confidence models.
- Break condition: If different LLMs have fundamentally different response patterns or if the dataset characteristics vary significantly across tasks.

## Foundational Learning

- Concept: Semantic similarity and entailment detection
  - Why needed here: The framework relies on determining whether perturbed responses are semantically equivalent or if one entails another to form semantic sets. This requires understanding of semantic similarity metrics and NLI models.
  - Quick check question: How would you determine if two LLM responses should be grouped in the same semantic set? What metric or model would you use?

- Concept: Feature engineering from NLP outputs
  - Why needed here: The framework creates features like semantic set counts and lexical similarity scores from LLM responses. Understanding how to extract meaningful features from text outputs is crucial for implementing this approach.
  - Quick check question: What are two different ways to quantify the variability in a set of LLM responses to the same prompt?

- Concept: Logistic regression and calibration
  - Why needed here: The framework uses logistic regression to predict response correctness and outputs calibrated confidence probabilities. Understanding logistic regression's probability interpretation and calibration properties is essential.
  - Quick check question: Why is logistic regression considered a well-calibrated model, and how does this benefit confidence estimation?

## Architecture Onboarding

- Component map: Prompt perturbation engine -> LLM query interface -> Feature extraction module -> Logistic regression trainer -> Inference pipeline
- Critical path: For each input prompt, generate perturbed versions -> query LLM for responses -> extract semantic and lexical features -> apply logistic regression model -> output confidence score. The most time-consuming step is typically the multiple LLM queries required for each perturbation.
- Design tradeoffs: Using more perturbations and response samples increases feature robustness but also increases query costs and latency. The choice of semantic similarity metric (e.g., Rouge vs. BERTScore) affects feature quality. The decision to use interpretable logistic regression trades some potential accuracy for calibration and interpretability.
- Failure signatures: Poor performance on datasets with very short responses (affecting SRC feature), when LLM responses are invariant to perturbations (features become uninformative), or when the training data distribution differs significantly from test data (model generalization fails).
- First 3 experiments:
  1. Implement the six prompt perturbations and verify they maintain semantic equivalence while generating different responses using a simple NLI model.
  2. Test feature extraction on a small dataset by computing semantic set counts and lexical similarity scores for perturbed responses.
  3. Train a logistic regression model on a subset of the data and evaluate its calibration using Expected Calibration Error on a held-out validation set.

## Open Questions the Paper Calls Out

None

## Limitations

- The core assumption that LLM response variability under prompt perturbations correlates with correctness remains to be rigorously validated across diverse domains
- The perturbation approach may face scalability challenges with computational costs and latency for real-time applications
- The claim of cross-LLM transferability lacks comprehensive validation across different task domains

## Confidence

- **High Confidence**: The logistic regression model produces well-calibrated probabilities for the tested datasets and LLMs
- **Medium Confidence**: The six perturbation types consistently elicit meaningful response variability across different LLMs and datasets
- **Low Confidence**: The claim that confidence models generalize across different LLM architectures beyond those tested

## Next Checks

1. Test the framework on non-QA/non-summarization tasks (e.g., code generation, mathematical reasoning) to verify perturbation-based variability correlates with correctness across diverse domains

2. Evaluate computational efficiency and latency impact by measuring the time and cost overhead of generating multiple perturbed responses per input in a production-like setting

3. Conduct systematic ablation studies to determine the individual contribution of each perturbation type and feature, and test whether a reduced set of perturbations maintains performance while improving efficiency