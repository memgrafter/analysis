---
ver: rpa2
title: Reinforcement Learning for Infinite-Horizon Average-Reward Linear MDPs via
  Approximation by Discounted-Reward MDPs
arxiv_id: '2405.15050'
source_url: https://arxiv.org/abs/2405.15050
tags:
- setting
- algorithm
- bound
- value
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of infinite-horizon average-reward
  reinforcement learning in linear Markov decision processes (MDPs), a setting where
  the Bellman operator is not a contraction, making algorithm design challenging.
  Prior work either suffered from computational inefficiency or required strong assumptions
  like ergodicity to achieve $\widetilde{O}(\sqrt{T})$ regret.
---

# Reinforcement Learning for Infinite-Horizon Average-Reward Linear MDPs via Approximation by Discounted-Reward MDPs

## Quick Facts
- arXiv ID: 2405.15050
- Source URL: https://arxiv.org/abs/2405.15050
- Reference count: 40
- This paper proposes the first algorithm achieving $\widetilde{O}(\sqrt{T})$ regret with polynomial computational complexity in infinite-horizon average-reward linear MDPs without ergodicity assumptions.

## Executive Summary
This paper addresses the challenging problem of infinite-horizon average-reward reinforcement learning in linear Markov decision processes (MDPs). Unlike discounted MDPs where the Bellman operator is a contraction, the average-reward setting lacks this property, making algorithm design difficult. Prior approaches either suffered from computational inefficiency or required strong ergodicity assumptions. The authors propose a novel algorithm that approximates the average-reward setting by a discounted MDP with a carefully chosen discount factor, then applies optimistic value iteration with a key value function clipping procedure. This approach achieves $\widetilde{O}(\sqrt{T})$ regret with polynomial computational complexity in the problem parameters.

## Method Summary
The authors propose Algorithm 2 (γ-LSCVI-UCB) which approximates the infinite-horizon average-reward setting by a discounted MDP with discount factor γ tuned to be close to 1. The algorithm plans nonstationary policies through optimistic value iteration, follows them until an information metric (determinant of the empirical covariance matrix) doubles, and incorporates value function clipping to bound the span of value function estimates. Key hyperparameters include γ = 1-√(log(T)/T), λ = 1, H = 2·sp(v*), and β = 2cβ·sp(v*)d√(log(dT/δ)). The algorithm achieves O(sp(v*)√d³T) regret without ergodicity assumptions.

## Key Results
- First algorithm achieving $\widetilde{O}(\sqrt{T})$ regret with polynomial computational complexity in infinite-horizon average-reward linear MDPs
- Achieves regret bound matching the best previous result but with polynomial rather than exponential dependence on problem dimension
- Introduces value function clipping procedure crucial for achieving sharp dependence on 1/(1-γ) in the regret bound
- Relies only on Bellman optimality equation assumption, which is weaker than ergodicity or weakly communicating assumptions

## Why This Works (Mechanism)

### Mechanism 1: Discounted MDP Approximation
The algorithm approximates the average-reward setting by a discounted MDP with carefully chosen discount factor γ close to 1. By leveraging the contraction property of the Bellman operator in the discounted setting, the authors can apply computationally efficient optimistic value iteration while maintaining near-optimality for the average-reward objective.

### Mechanism 2: Value Function Clipping
Value function clipping bounds the span of value function estimates by a target span H, preventing the regret bound from scaling with 1/(1-γ). This is crucial for achieving O(√T) regret when tuning γ close to 1, as it allows the algorithm to maintain sample efficiency despite the large discount factor.

### Mechanism 3: Information-Driven Policy Updates
The algorithm plans nonstationary policies through optimistic value iteration and follows them until an information metric doubles. This structure, combined with the rarely-switching covariance matrix trick, allows efficient incorporation of newly collected trajectory data while maintaining computational tractability.

## Foundational Learning

- **Concept**: Discounted MDP vs Average-Reward MDP
  - **Why needed here**: The algorithm relies on approximating the average-reward setting by a discounted MDP. Understanding this relationship is crucial for grasping the core approach.
  - **Quick check question**: What is the main difference between the Bellman operator in the discounted MDP setting and the average-reward MDP setting that makes algorithm design challenging?

- **Concept**: Value Function Clipping
  - **Why needed here**: The algorithm uses value function clipping to bound the span of value function estimates. Understanding this technique and its purpose is essential for understanding how the algorithm achieves its regret bound.
  - **Quick check question**: Why does value function clipping help in achieving O(√T) regret when tuning the discount factor γ?

- **Concept**: Linear MDPs and Function Approximation
  - **Why needed here**: The algorithm is designed for the linear MDP setting, which allows for generalization to unseen states through a low-dimensional state-action feature mapping. Understanding this setting is crucial for understanding the algorithm's computational efficiency compared to tabular settings.
  - **Quick check question**: How does the linear MDP setting allow for generalization to unseen states, and why is this important for computational efficiency?

## Architecture Onboarding

- **Component map**: Discounting Factor Tuner -> Value Function Clipping Module -> Optimistic Value Iteration Engine -> Policy Executor -> Information Metric Monitor -> Data Collector

- **Critical path**: Discounting Factor → Value Clipping → Value Iteration → Policy Execution → Data Collection → Restart Check

- **Design tradeoffs**:
  - Choosing γ close to 1 improves approximation quality but may increase computational cost
  - Tighter clipping bounds (smaller H) reduce regret but may limit exploration
  - More frequent restarts incorporate more data but increase computational overhead

- **Failure signatures**:
  - If regret scales with 1/(1-γ) instead of sp(v*), clipping is not working correctly
  - If algorithm doesn't achieve O(√T) regret, discounting factor tuning may be incorrect
  - If computational complexity is too high, restart mechanism may be triggering too frequently

- **First 3 experiments**:
  1. Verify that changing γ away from 1 - 1/√T affects regret bound scaling
  2. Test that removing value function clipping causes regret to scale with 1/(1-γ)
  3. Confirm that increasing the information doubling threshold increases the number of episodes

## Open Questions the Paper Calls Out

### Open Question 1
Can the dependence on the state space size S in the time complexity of γ-LSCVI-UCB be eliminated while maintaining the same regret bound? The paper conjectures this is possible using an estimate of the minimum rather than computing the global minimum of value functions, but notes this requires additional algorithmic technique beyond naive changes to the clipping operation.

### Open Question 2
Is it possible to achieve the same regret bound without requiring knowledge of sp(v*) or its upper bound H? While recent work has relaxed this assumption for tabular MDPs, the authors note this extension to linear MDPs remains an open problem that has not yet been solved.

### Open Question 3
Can the regret bound be improved by a factor of √d through refined analysis of the variance of the value estimate? The authors acknowledge this potential improvement but do not pursue it in the current work, explicitly leaving it for future research.

## Limitations

- The algorithm requires knowledge of the optimal value function span sp(v*) for setting the clipping threshold H, which may not be available in practice
- The computational complexity of O(d³SAT²) could be prohibitive for large-scale problems, particularly the O(d²SA) per value iteration for the clipping operation
- The exact constant cβ in the concentration bound is not specified, which may affect practical implementation

## Confidence

- **Theoretical framework**: High confidence in the theoretical framework and regret bound derivation
- **Computational efficiency**: Medium confidence in practical computational efficiency due to the high polynomial complexity
- **Implementation details**: Medium confidence in implementation details given some unspecified constants and parameters

## Next Checks

1. Implement a controlled experiment varying the discount factor γ away from 1-1/√T to verify the dependence of regret on this parameter
2. Conduct a computational complexity analysis by measuring actual runtime on problems of increasing size to confirm the O(d³SAT²) scaling
3. Test the algorithm with different values of the clipping threshold H to empirically validate the need for setting H = 2·sp(v*) for achieving O(√T) regret