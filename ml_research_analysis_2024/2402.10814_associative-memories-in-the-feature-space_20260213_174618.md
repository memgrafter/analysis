---
ver: rpa2
title: Associative Memories in the Feature Space
arxiv_id: '2402.10814'
source_url: https://arxiv.org/abs/2402.10814
tags:
- memory
- data
- space
- function
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of poor performance of associative
  memory models in retrieving images with mild corruptions, due to similarity computation
  in raw pixel space lacking semantic information. The proposed solution is to use
  pretrained neural networks with contrastive loss to compute embeddings, enabling
  similarity evaluation in a semantic feature space rather than pixel space.
---

# Associative Memories in the Feature Space

## Quick Facts
- arXiv ID: 2402.10814
- Source URL: https://arxiv.org/abs/2402.10814
- Reference count: 28
- Primary result: Semantic embedding spaces improve associative memory retrieval accuracy by 30-40% on corrupted images compared to pixel-space similarity

## Executive Summary
This paper addresses the fundamental limitation of associative memory models in retrieving corrupted images by introducing semantic embedding spaces for similarity computation. The authors demonstrate that standard Hopfield networks struggle with mild image corruptions like rotation and cropping when using pixel-space similarity metrics. By leveraging pre-trained neural networks with contrastive loss to compute embeddings, the proposed semantic memory models achieve significantly better retrieval performance on corrupted inputs while also offering computational efficiency through lower-dimensional feature spaces.

## Method Summary
The paper proposes computing similarities between memory items in a learned embedding space rather than raw pixel space. Specifically, they use pre-trained ResNet models (ResNet18 for CIFAR10, ResNet50 for STL10) that were trained with SimCLR contrastive loss to create embedding functions. These embeddings map images into lower-dimensional spaces where semantically similar images are closer together. The memory models can operate in two modes: storing exact images with embedding-based similarity (improving retrieval accuracy), or storing only low-dimensional embeddings with a decoder for memory-efficient approximate retrieval.

## Key Results
- Semantic memory models outperform standard Hopfield networks by 30-40% on retrieval accuracy for common corruptions (rotation, cropping, masking, color filters, salt & pepper noise, Gaussian noise)
- Embedding-based similarity computation is computationally faster due to lower-dimensional feature spaces (512-2048 dimensions vs 3072-27648 pixel dimensions)
- Memory-efficient fully-semantic variant storing only embeddings demonstrated on MNIST using autoencoders, trading exact retrieval for storage efficiency
- Best performance achieved with cosine similarity in embedding space across all corruption types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic memory models outperform universal Hopfield networks because they compute similarity in a learned feature space rather than raw pixel space
- Mechanism: A pretrained ResNet with contrastive loss maps images into a lower-dimensional embedding space where semantically similar images are closer together. This makes similarity metrics like L1 distance more effective for corrupted inputs
- Core assumption: The contrastive pretraining ensures that semantically similar images (even when corrupted) map to nearby embeddings
- Evidence anchors:
  - [abstract]: "This problem can be easily solved by computing similarities in an embedding space instead of the pixel space"
  - [section 3]: "We show that an effective way of computing such embeddings is via a network pretrained with a contrastive loss"
  - [corpus]: Weak or missing; related papers do not directly discuss contrastive pretraining in associative memory

### Mechanism 2
- Claim: Lower-dimensional embeddings enable faster similarity computations compared to pixel space
- Mechanism: Similarity scores are computed in an embedding space of dimension 512 (ResNet18) or 2048 (ResNet50), which is much smaller than the pixel space (3072 for CIFAR10, 27648 for STL10). This reduces computational cost significantly
- Core assumption: The embedding dimension is sufficiently small while retaining semantic information
- Evidence anchors:
  - [abstract]: "As the dimension of embedding spaces is often significantly smaller than the pixel space, we also have a faster computation of similarity scores"
  - [section 3]: "the dimension of the semantic spaces is given by the dimension of the output of the embedding function ϕ considered, in our case 512 for ResNet18 and 2048 for ResNet50"
  - [corpus]: Weak or missing; related papers focus on memory capacity, not computational efficiency

### Mechanism 3
- Claim: Fully-semantic memory models store only low-dimensional embeddings, making them more memory-efficient and biologically plausible
- Mechanism: Instead of storing full images, the model stores embeddings from an encoder (ϕ) and uses a decoder (ψ) to reconstruct retrieved memories. This trades exact retrieval for memory efficiency
- Core assumption: The encoder-decoder pair can reconstruct semantically meaningful images from embeddings
- Evidence anchors:
  - [abstract]: "An additional drawback of current models is the need of storing the whole dataset in the pixel space... We relax this condition and propose a class of memory models that only stores low-dimensional semantic embeddings"
  - [section 4]: "Note that the dataset is not stored, but only its embeddings are"
  - [corpus]: Weak or missing; related papers do not discuss memory-efficient storage of embeddings

## Foundational Learning

- Concept: Universal Hopfield Networks (UHNs)
  - Why needed here: UHNs provide the baseline model and decomposition framework (score, separation, projection) used to define semantic memory models
  - Quick check question: What are the three functions that decompose a universal Hopfield network, and what does each do?

- Concept: Contrastive learning (SimCLR)
  - Why needed here: Contrastive pretraining is the key method for learning useful embeddings that group semantically similar images together
  - Quick check question: How does SimCLR train embeddings to ensure that corrupted versions of the same image are close in the embedding space?

- Concept: Autoencoders (encoder-decoder architecture)
  - Why needed here: Autoencoders are used to learn the embedding (ϕ) and reconstruction (ψ) functions for fully-semantic memory models
  - Quick check question: What is the role of the bottleneck layer in an autoencoder, and how does it relate to memory efficiency?

## Architecture Onboarding

- Component map: Input image -> Encoder (ϕ) -> Embedding space -> Similarity computation -> Separation (softmax) -> Projection (stored embeddings or decoder ψ) -> Retrieved image

- Critical path:
  1. Compute embedding of input image using pretrained ResNet
  2. Compute similarity scores between input embedding and all stored embeddings
  3. Apply softmax separation to get a probability distribution over stored memories
  4. Retrieve the most probable memory (exact or reconstructed via decoder)

- Design tradeoffs:
  - Exact vs. generative retrieval: Storing full images enables exact retrieval but is memory-inefficient; storing embeddings with a decoder is memory-efficient but yields approximate retrievals
  - Embedding dimension: Higher dimensions retain more semantic information but increase computation and storage costs
  - Pretraining: Contrastive pretraining is crucial for good embeddings but requires a large dataset and computational resources

- Failure signatures:
  - Retrieval accuracy drops significantly on corruptions not seen during pretraining (e.g., rotations if not in pretraining data)
  - Embeddings are too similar across classes, causing incorrect retrievals
  - Decoder fails to reconstruct meaningful images from embeddings in fully-semantic models

- First 3 experiments:
  1. Replicate the CIFAR10 experiments with different similarity functions (cosine, L1, L2) to verify that semantic models outperform UHNs on corruptions like rotation and cropping
  2. Test retrieval accuracy on STL10 with a ResNet50 backbone to confirm scalability to larger images
  3. Implement a simple autoencoder on MNIST and evaluate fully-semantic retrieval under Gaussian noise to validate the proof of concept

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of semantic Hopfield networks scale with the size and complexity of the dataset?
- Basis in paper: [explicit] The paper tests on CIFAR10 and STL10 but notes that more powerful generative models would be needed for more complex datasets
- Why unresolved: The experiments only cover two relatively small image datasets. The paper acknowledges that scaling to larger datasets like ImageNet would require more powerful embedding functions
- What evidence would resolve it: Testing semantic Hopfield networks on progressively larger and more complex datasets (e.g., CIFAR100, ImageNet) while varying embedding function capacity would show scaling properties

### Open Question 2
- Question: What is the optimal trade-off between embedding dimension and memory efficiency in fully-semantic memory models?
- Basis in paper: [inferred] The paper proposes fully-semantic models that store low-dimensional embeddings but notes this creates a tradeoff between memory savings and model capacity
- Why unresolved: The paper only demonstrates a proof-of-concept on MNIST with a simple autoencoder. It doesn't explore how different embedding dimensions affect retrieval performance across various datasets
- What evidence would resolve it: Systematic experiments varying embedding dimensions on multiple datasets while measuring both retrieval accuracy and memory usage would identify optimal configurations

### Open Question 3
- Question: Can semantic memory models effectively handle other types of corruption beyond those tested?
- Basis in paper: [explicit] The paper tests six specific corruption types (cropping, masking, color filters, rotation, salt & pepper, Gaussian noise) but acknowledges these may not cover all possible corruptions
- Why unresolved: The evaluation is limited to these six corruption types, and the paper doesn't discuss how the models would perform on other common image transformations or adversarial attacks
- What evidence would resolve it: Testing the models against a broader range of corruption types including elastic deformations, contrast changes, JPEG compression, and adversarial examples would reveal their robustness limits

## Limitations

- Exact SimCLR pretraining hyperparameters and corruption generation parameters are not fully specified, affecting reproducibility
- Memory-efficient fully-semantic variant only demonstrated on MNIST, lacking validation on more complex datasets
- Computational efficiency claims need empirical validation across different hardware configurations
- Limited evaluation of corruption types may not capture the full robustness of the approach

## Confidence

- High confidence in the core mechanism of embedding-based similarity computation improving retrieval on corrupted images
- Medium confidence in computational efficiency gains, pending hardware-specific validation
- Medium confidence in the memory-efficient variant, given limited empirical validation

## Next Checks

1. Replicate retrieval accuracy experiments on CIFAR10 with different similarity functions (cosine, L1, L2) to verify semantic models outperform UHNs on corruptions like rotation and cropping
2. Test retrieval accuracy on STL10 with ResNet50 backbone to confirm scalability to larger images and validate computational efficiency claims
3. Implement autoencoder-based fully-semantic retrieval on CIFAR10 with various bottleneck dimensions to quantify the exact-retrieval vs. memory-efficiency tradeoff