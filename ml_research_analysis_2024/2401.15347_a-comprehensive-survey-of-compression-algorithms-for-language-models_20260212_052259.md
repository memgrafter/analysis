---
ver: rpa2
title: A Comprehensive Survey of Compression Algorithms for Language Models
arxiv_id: '2401.15347'
source_url: https://arxiv.org/abs/2401.15347
tags:
- algorithms
- pruning
- quantization
- compression
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey paper provides a comprehensive overview of compression
  algorithms for large language models (LLMs), covering pruning, quantization, knowledge
  distillation, low-rank approximation, parameter sharing, and efficient architecture
  design. It highlights the growing importance of compression techniques due to the
  large size and high computational cost of modern LLMs.
---

# A Comprehensive Survey of Compression Algorithms for Language Models

## Quick Facts
- arXiv ID: 2401.15347
- Source URL: https://arxiv.org/abs/2401.15347
- Reference count: 40
- One-line primary result: Comprehensive survey of compression algorithms for large language models, categorizing techniques and identifying research opportunities.

## Executive Summary
This survey provides a comprehensive overview of compression algorithms for large language models (LLMs), covering six main categories: pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. The paper highlights the growing importance of compression techniques due to the large size and high computational cost of modern LLMs. It summarizes development trends, discusses advantages and limitations of various approaches, and emphasizes the need for low-cost compression algorithms that maintain efficiency while preserving accuracy.

## Method Summary
The survey systematically categorizes compression algorithms for LLMs, distinguishing between high-cost methods (iterative pruning, quantization-aware training, knowledge distillation) and low-cost methods (one-shot pruning, post-training quantization, knowledge distillation with frozen models). It analyzes the value and limitations of each category, discusses desired properties for low-cost algorithms, and identifies promising future research directions. The methodology involves reviewing existing literature, synthesizing algorithmic approaches, and evaluating their applicability to large-scale language models.

## Key Results
- Compression algorithms are categorized into six main types: pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design
- Low-cost compression algorithms face challenges in preserving useful features learned from large corpora compared to high-cost methods
- Integration of parameter-efficient fine-tuning (PEFT) algorithms with compression techniques shows promise for improving compressed model performance

## Why This Works (Mechanism)

### Mechanism 1
Direct optimization of task-specific objective functions improves compression accuracy over proxy sublayer-wise reconstruction errors. This mechanism works by recognizing that many low-cost compression algorithms formulate local sublayer-wise reconstruction errors as least-square problems for computational efficiency, but these proxy objectives do not directly minimize the task-specific objective function (e.g., cross-entropy loss), leading to suboptimal performance. By directly optimizing the task-specific objective, algorithms can better preserve the model's useful features learned from large corpora.

### Mechanism 2
Iterative compression processes with mild pruning and error compensation preserve useful features better than one-shot transformations. This mechanism recognizes that high-cost compression algorithms gradually transform weights and compensate for compression errors, successfully preserving useful features learned from large corpora. In contrast, low-cost algorithms tend to transform all weights at once, leading to loss of useful features. Iterative compression with mild pruning and error compensation allows for gradual transformation and better feature preservation.

### Mechanism 3
Parameter-efficient fine-tuning (PEFT) algorithms reduce memory footprint during fine-tuning, enabling more accurate compression for LLMs. This mechanism works by fixing the original parameters in the language model and updating only low-rank adaptors using gradient descent, which significantly reduces the memory footprint during the fine-tuning process because of the reduced amount of gradients to save. By leveraging PEFT, algorithms can compensate for errors induced by compression more effectively, improving the accuracy of compressed models.

## Foundational Learning

- **Concept**: Transformer architecture and its components (multi-head attention, feed-forward network, layer normalization)
  - **Why needed here**: Understanding the Transformer architecture is crucial for comprehending how compression algorithms modify and optimize these components to reduce model size and computational cost
  - **Quick check question**: What are the three main sublayers in a Transformer layer, and what is the primary function of each?

- **Concept**: Quantization schemes (uniform vs. non-uniform, precision and type)
  - **Why needed here**: Quantization is a key compression technique that reduces the bit-width of parameters to compress large-scale models while retaining accuracy. Understanding different quantization schemes and their tradeoffs is essential for implementing and evaluating compression algorithms
  - **Quick check question**: What is the difference between uniform and non-uniform quantization, and how does the choice of quantization scheme affect the compression rate and accuracy?

- **Concept**: Pruning strategies (high-cost vs. low-cost, granularity)
  - **Why needed here**: Pruning is a fundamental compression technique that identifies and removes unnecessary components in neural networks. Understanding different pruning strategies and granularities is crucial for selecting the appropriate approach for a given model and computational budget
  - **Quick check question**: What is the difference between high-cost and low-cost pruning strategies, and how does the choice of pruning granularity affect the trade-off between compression rate and accuracy?

## Architecture Onboarding

- **Component map**: Pruning -> Quantization -> Knowledge Distillation -> Low-rank Approximation -> Parameter Sharing -> Efficient Architecture Design
- **Critical path**: Select compression technique(s) → Design compression strategy (granularity, cost) → Implement algorithm → Evaluate performance on target model and dataset
- **Design tradeoffs**: Balance between compression rate and accuracy, computational cost of compression algorithm itself, compatibility with different model architectures and hardware platforms
- **Failure signatures**: Severe accuracy degradation, slow inference speed, difficulty integrating with existing models or workflows
- **First 3 experiments**:
  1. Implement magnitude-based pruning on a small Transformer model and evaluate accuracy and computational cost impact
  2. Apply quantization to the same model using different bit-widths and quantization schemes, comparing compression rate, accuracy, and inference speed
  3. Combine pruning and quantization techniques to achieve higher compression rates, assessing trade-offs between compression rate, accuracy, and computational efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal strategy for integrating knowledge distillation with low-cost compression algorithms to improve accuracy without significantly increasing computational overhead?
- **Basis in paper**: The paper discusses the importance of knowledge distillation for improving accuracy in high-cost compression scenarios but notes its diminished usage in low-cost scenarios due to the computational overhead
- **Why unresolved**: Integrating knowledge distillation with low-cost algorithms is challenging because knowledge distillation typically requires significant computational resources, which contradicts the goal of low-cost compression
- **What evidence would resolve it**: A novel low-cost algorithm that successfully incorporates knowledge distillation, demonstrating improved accuracy without a substantial increase in computational cost

### Open Question 2
- **Question**: How can activation quantization be effectively implemented for LLMs to achieve both high compression rates and maintained accuracy?
- **Basis in paper**: The paper highlights the difficulty of quantizing activations for LLMs, noting that current algorithms fail to retain accuracy when the bit-width is reduced below 8 bits
- **Why unresolved**: The challenge lies in balancing the trade-off between compression rate and accuracy, as reducing the bit-width too much can lead to significant accuracy degradation
- **What evidence would resolve it**: A successful implementation of activation quantization for LLMs that achieves a high compression rate (e.g., 4-bit or lower) while maintaining accuracy comparable to the original model

### Open Question 3
- **Question**: What is the most effective way to unify diverse compression algorithms (pruning, quantization, knowledge distillation, etc.) to achieve extremely high compression rates without sacrificing accuracy?
- **Basis in paper**: The paper discusses the potential of combining different compression algorithms to achieve higher compression rates but notes the lack of research on integrating three or more algorithms simultaneously
- **Why unresolved**: Unifying multiple compression algorithms is complex due to their different characteristics and potential interactions, which can lead to unexpected results or trade-offs
- **What evidence would resolve it**: A comprehensive study or algorithm that successfully integrates multiple compression algorithms, demonstrating extremely high compression rates while maintaining or improving accuracy

## Limitations
- Analysis primarily focuses on algorithmic approaches without extensive empirical validation across diverse model architectures and tasks
- Supporting evidence for claimed mechanisms remains largely theoretical rather than experimentally validated
- Survey does not address potential challenges in integrating PEFT with other compression techniques or impact on model convergence

## Confidence
- **High**: The categorization of compression algorithms and their general principles
- **Medium**: The importance of task-specific objectives in compression algorithms
- **Medium**: The effectiveness of iterative compression processes
- **Low**: Specific performance claims and quantitative comparisons between algorithms

## Next Checks
1. **Empirical Validation of Task-Specific Objectives**: Conduct controlled experiments comparing compression algorithms that optimize task-specific objectives versus those using proxy sublayer-wise reconstruction errors across multiple model architectures and tasks. Measure accuracy retention, computational efficiency, and convergence behavior.

2. **Iterative vs. One-Shot Compression Analysis**: Implement and compare iterative compression with mild pruning and error compensation against one-shot transformations across various model sizes and compression rates. Quantify the trade-offs between accuracy preservation, computational cost, and implementation complexity.

3. **PEFT Integration Study**: Evaluate the integration of PEFT algorithms with different compression techniques (pruning, quantization, knowledge distillation) on large language models. Measure memory footprint reduction, accuracy improvement, and potential challenges in convergence and fine-tuning stability.