---
ver: rpa2
title: Improving LSH via Tensorized Random Projection
arxiv_id: '2402.07189'
source_url: https://arxiv.org/abs/2402.07189
tags:
- tensor
- nition
- projection
- random
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of applying Locality-Sensitive\
  \ Hashing (LSH) to tensor data for Euclidean distance and cosine similarity, where\
  \ naive vector reshaping leads to exponential complexity. It proposes efficient\
  \ LSH methods\u2014CP-E2LSH, TT-E2LSH for Euclidean distance and CP-SRP, TT-SRP\
  \ for cosine similarity\u2014by leveraging CP and Tensor Train (TT) decompositions\
  \ to replace dense Gaussian projections with structured low-rank tensors."
---

# Improving LSH via Tensorized Random Projection

## Quick Facts
- arXiv ID: 2402.07189
- Source URL: https://arxiv.org/abs/2402.07189
- Reference count: 40
- Primary result: Proposes efficient LSH methods for tensor data using CP and TT decompositions, achieving linear space complexity in tensor order instead of exponential complexity.

## Executive Summary
This paper addresses the challenge of applying Locality-Sensitive Hashing (LSH) to tensor data for Euclidean distance and cosine similarity, where naive vector reshaping leads to exponential complexity. The authors propose efficient LSH methods—CP-E2LSH, TT-E2LSH for Euclidean distance and CP-SRP, TT-SRP for cosine similarity—by leveraging CP and Tensor Train (TT) decompositions to replace dense Gaussian projections with structured low-rank tensors. The theoretical analysis shows that these methods achieve space complexity linear in tensor order N (O(NdR) or O(NdR²)) instead of exponential (O(dᴺ)), and time complexity significantly lower when tensors are given in decomposed formats. Empirical guarantees rely on asymptotic normality from variants of the Central Limit Theorem.

## Method Summary
The method uses CP and Tensor Train decompositions to create structured random projections for LSH. For Euclidean distance, CP-E2LSH and TT-E2LSH use quantized inner products with Rademacher-distributed tensors. For cosine similarity, CP-SRP and TT-SRP use sign-based discretization. The key innovation is replacing dense Gaussian projection matrices (O(dᴺ) space) with CP or TT structured tensors (O(NdR) or O(NdR²) space), while maintaining theoretical collision probability guarantees through asymptotic normality of inner products.

## Key Results
- Achieves space complexity linear in tensor order N (O(NdR) or O(NdR²)) instead of exponential (O(dᴺ))
- Time complexity significantly lower when input tensors are provided in decomposed formats
- Theoretical guarantees on collision probabilities through asymptotic normality of structured random projections
- Maintains correctness of LSH guarantees for both Euclidean distance and cosine similarity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CP-E2LSH and TT-E2LSH achieve space complexity linear in tensor order N by replacing dense Gaussian projections with structured low-rank tensors.
- Mechanism: Instead of reshaping a tensor into a vector of size O(d^N) and projecting onto a dense Gaussian matrix, the method uses CP or TT decomposed tensors as projection matrices. This exploits the low-rank structure to reduce storage from exponential to linear.
- Core assumption: The input tensor or projection tensor can be efficiently represented in CP or TT format with small rank R.
- Evidence anchors:
  - [abstract] "propose efficient LSH methods—CP-E2LSH, TT-E2LSH for Euclidean distance... by leveraging CP and Tensor Train (TT) decompositions to replace dense Gaussian projections with structured low-rank tensors"
  - [section] "Our approaches are space efficient and can be efficiently applied to low rank CP or TT tensors"
- Break condition: If tensor rank is high (R close to d^N) or input is not in decomposed format, space complexity reverts to O(d^N).

### Mechanism 2
- Claim: Inner products between input tensors and CP/TT Rademacher tensors asymptotically follow Gaussian distribution, enabling collision probability guarantees.
- Mechanism: The structured tensor projection preserves the randomness needed for LSH guarantees through asymptotic normality from variants of the Central Limit Theorem. This allows the method to inherit collision probability bounds from classical E2LSH.
- Core assumption: The tensor size ∏dn → ∞ and rank R is sufficiently small relative to tensor dimensions.
- Evidence anchors:
  - [abstract] "The theoretical analysis shows that these methods achieve space complexity linear in tensor order N... and time complexity significantly lower when tensors are given in decomposed formats"
  - [section] "To use the collision probability guarantee of E2LSH, we need to show that the inner product between the input tensor and CP or TT Rademacher distributed tensor follows the Gaussian distribution"
- Break condition: If tensor dimensions are small or rank R is too large relative to dimensions, asymptotic normality fails.

### Mechanism 3
- Claim: Time complexity improves when input tensors are provided in CP or TT format by avoiding full tensor reshaping.
- Mechanism: When input is already in decomposed format, inner products can be computed directly in the factor space (O(Nd max{R, R̂}²) for CP, O(Nd max{R, R̂}³) for TT) rather than reshaping to O(d^N) vectors.
- Core assumption: Input tensor is given or can be efficiently converted to CP/TT decomposition format.
- Evidence anchors:
  - [abstract] "time complexity significantly lower when tensors are given in decomposed formats"
  - [section] "if the input tensor is given in rank R̂ CP decomposition format... we can efficiently compute the inner product... in O(Nd max{R, R̂}²) running time"
- Break condition: If input tensor is not in decomposed format and must be converted, conversion cost may offset benefits.

## Foundational Learning

- Concept: Tensor decompositions (CP and TT)
  - Why needed here: These decompositions provide the structured low-rank representations that enable space and time efficiency.
  - Quick check question: What is the space complexity of storing a rank-R CP tensor of order N with dimension d per mode? (Answer: O(NdR))

- Concept: Locality-Sensitive Hashing theory
  - Why needed here: Understanding the collision probability guarantees and sensitivity conditions is essential for proving correctness.
  - Quick check question: What are the key conditions for an LSH family to be (R1, R2, P1, P2)-sensitive? (Answer: Similar items have high collision probability, dissimilar items have low collision probability)

- Concept: Asymptotic normality and Central Limit Theorems
  - Why needed here: The proofs rely on showing that structured random projections converge to Gaussian distributions for collision probability guarantees.
  - Quick check question: What condition must hold for a sum of dependent random variables to converge to normal distribution according to Theorem 1? (Answer: (d/M)^(1/α) MA/σd → 0 as d → ∞)

## Architecture Onboarding

- Component map: Input tensor -> Projection tensor generator -> Inner product computation -> Discretization -> Hash code
- Critical path:
  1. Load input tensor (preferably in decomposed format)
  2. Generate projection tensor with appropriate rank
  3. Compute inner product efficiently using tensor algebra
  4. Apply discretization function
  5. Return hash code
- Design tradeoffs:
  - Higher rank R improves accuracy but increases space/time complexity
  - CP decomposition offers better space complexity but requires NP-hard rank computation
  - TT decomposition allows efficient rank computation but higher space complexity
  - Pre-computed decompositions vs on-the-fly conversion tradeoff
- Failure signatures:
  - Excessive memory usage suggests rank R is too high or tensor dimensions are too large
  - Poor collision discrimination suggests insufficient rank or inappropriate discretization
  - Slow performance suggests tensors are not in decomposed format
- First 3 experiments:
  1. Verify space complexity by comparing memory usage for different ranks R on synthetic tensors
  2. Test collision probability guarantees by measuring actual vs theoretical collision rates for varying distances
  3. Benchmark time complexity by comparing performance with naive vector reshaping approach for different input formats

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CP-E2LSH and TT-E2LSH degrade when the input tensor is not in low-rank CP or TT decomposition format?
- Basis in paper: explicit
- Why unresolved: The paper assumes input tensors are given in decomposed formats but does not analyze the overhead of computing these decompositions or performance when using full tensors.
- What evidence would resolve it: Empirical or theoretical comparison of hash generation time and quality when input tensors are given in decomposed vs. full formats.

### Open Question 2
- Question: What is the empirical impact of rank selection (R) on the collision probability and computational efficiency in real-world tensor datasets?
- Basis in paper: explicit
- Why unresolved: The paper provides theoretical bounds but lacks empirical validation of how different R values affect practical performance across diverse tensor datasets.
- What evidence would resolve it: Experiments varying R on benchmark tensor datasets measuring collision rates, hash quality, and runtime.

### Open Question 3
- Question: How does the performance of CP-SRP and TT-SRP compare to other tensor-aware LSH methods (e.g., those based on circulant matrices or count sketches)?
- Basis in paper: explicit
- Why unresolved: The paper only compares against naive reshaping methods, not against more recent tensor-aware LSH approaches.
- What evidence would resolve it: Benchmarking against state-of-the-art tensor LSH methods on cosine similarity tasks.

## Limitations
- Theoretical analysis relies heavily on asymptotic normality conditions that may not hold for practical tensor sizes
- Rank selection strategy is not addressed, leaving optimal rank balancing accuracy and efficiency unclear
- No empirical validation provided to demonstrate claimed space and time complexity improvements

## Confidence
- **High Confidence**: Space complexity improvements (O(NdR) vs O(d^N)) when tensors are in decomposed format
- **Medium Confidence**: Asymptotic normality arguments for collision probability guarantees
- **Low Confidence**: Practical performance claims without empirical benchmarks

## Next Checks
1. **Empirical Complexity Validation**: Implement and benchmark both the proposed methods and naive vector reshaping approach across varying tensor dimensions and ranks to verify the claimed O(NdR) space complexity and runtime improvements.

2. **Collision Probability Testing**: Conduct experiments measuring actual collision rates for pairs of tensors at varying distances, comparing empirical results against theoretical bounds derived from asymptotic normality.

3. **Rank Sensitivity Analysis**: Systematically evaluate how different rank choices (R) affect both the accuracy of similarity preservation and the computational efficiency, identifying practical guidelines for rank selection.