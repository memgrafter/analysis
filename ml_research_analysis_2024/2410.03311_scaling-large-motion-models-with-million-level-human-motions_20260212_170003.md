---
ver: rpa2
title: Scaling Large Motion Models with Million-Level Human Motions
arxiv_id: '2410.03311'
source_url: https://arxiv.org/abs/2410.03311
tags:
- motion
- human
- large
- data
- motionlib
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates scaling large motion models by addressing\
  \ data scarcity in human motion understanding. The authors introduce MotionLib,\
  \ the first million-scale dataset with over 1.2M motion sequences and hierarchical\
  \ text annotations, which is 15\xD7 larger than existing datasets."
---

# Scaling Large Motion Models with Million-Level Human Motions

## Quick Facts
- arXiv ID: 2410.03311
- Source URL: https://arxiv.org/abs/2410.03311
- Reference count: 40
- This paper introduces MotionLib, the first million-scale human motion dataset, and Being-M0, a large motion model demonstrating state-of-the-art performance on HumanML3D benchmark.

## Executive Summary
This paper addresses the critical data scarcity problem in human motion understanding by introducing MotionLib, a million-scale dataset with over 1.2M motion sequences and hierarchical text annotations. The authors develop MotionBook, a novel motion encoding approach featuring a 2D lookup-free tokenizer that preserves fine-grained motion details while expanding codebook capacity. Using this infrastructure, they train Being-M0, demonstrating that scaling both data and model size substantially improves motion generation performance, reducing joint prediction errors and enhancing generalization to unseen motions.

## Method Summary
The authors create MotionLib by curating and refining web video data into 1.2M motion sequences with hierarchical text annotations. They develop MotionBook encoding using a novel 2D lookup-free quantization (2D-LFQ) tokenizer that treats motion sequences as 2D images, enabling lossless feature representation and expanded codebook capacity. The model is trained in two stages: first pre-training the motion VQ-VAE tokenizer, then fine-tuning an autoregressive LLM backbone on the MotionLib data with instruction tuning. The framework is evaluated on the HumanML3D benchmark, showing significant improvements over existing specialist and generalist motion models.

## Key Results
- MotionLib dataset contains 1.2M motion sequences with hierarchical text annotations, 15× larger than existing datasets
- Being-M0 achieves state-of-the-art results on HumanML3D benchmark, outperforming specialist and generalist models
- Scaling from 0.08M to 1.2M data improves R@1 score from 0.124 to 0.185 (13.7% relative improvement)
- 2D-LFQ tokenizer preserves motion details while expanding codebook capacity compared to traditional VQ methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling both model size and dataset size leads to improved motion generation performance
- Mechanism: Larger models have more parameters to capture complex motion patterns, while larger datasets provide more diverse training examples. This combination allows the model to learn more robust and generalizable representations of human motion.
- Core assumption: Motion generation benefits from the same scaling laws observed in other domains like NLP and computer vision
- Evidence anchors:
  - [abstract] "scaling both data and model size substantially improves motion generation, reducing joint prediction errors and enhancing generalization to unseen motions"
  - [section] "when using LLaMA2-13b as the LLM backbone for our model, training with 1.2M data achieves an R@1 of 0.185, which is 0.011 and 0.061 higher than performance using 0.5M data and 0.08M Motion-X, respectively"
  - [corpus] No direct corpus evidence found for this specific mechanism
- Break condition: Diminishing returns set in when additional parameters/data no longer improve performance, or when computational constraints make further scaling impractical

### Mechanism 2
- Claim: MotionBook's 2D-LFQ quantization method preserves more motion details while expanding codebook capacity
- Mechanism: By treating motion sequences as 2D images with a single channel, 2D-LFQ increases the encoder's capacity to capture complex motion patterns. The lookup-free quantization eliminates the need for token lookups in the codebook, allowing for a much larger vocabulary without codebook collapse.
- Core assumption: Motion sequences can be effectively represented as 2D images without losing temporal information
- Evidence anchors:
  - [abstract] "a novel 2D lookup-free motion tokenizer that preserves fine-grained motion details while expanding codebook capacity"
  - [section] "Our 2D-LFQ reformulates each motion sequence as a 2D image with a single channel, represented as M ∈ RT ×D×1. By expanding the motion sequence's dimensionality from 1D to 2D, we enhance the encoder's capacity"
  - [corpus] No direct corpus evidence found for this specific mechanism
- Break condition: If the 2D representation fails to capture temporal dependencies effectively, or if the increased dimensionality introduces noise that degrades performance

### Mechanism 3
- Claim: Hierarchical text descriptions improve semantic alignment between text and motion
- Mechanism: By providing both body-level and part-level descriptions, the model receives more detailed information about specific body movements, enabling finer-grained control and better understanding of motion semantics.
- Core assumption: Motion generation models benefit from more detailed and structured textual input
- Evidence anchors:
  - [abstract] "enriched with hierarchical text descriptions"
  - [section] "Our data curation pipeline involves the following main procedural steps... Hierarchical Motion Descriptions"
  - [corpus] No direct corpus evidence found for this specific mechanism
- Break condition: If the additional text complexity overwhelms the model or if the hierarchical structure introduces inconsistencies that confuse the model

## Foundational Learning

- Concept: Vector Quantization (VQ)
  - Why needed here: VQ is used to convert continuous motion data into discrete tokens that can be processed by autoregressive models
  - Quick check question: How does VQ handle the trade-off between compression and information loss?

- Concept: Human motion representation
  - Why needed here: Understanding how human motion is encoded (joint positions, rotations, velocities) is crucial for designing effective motion models
  - Quick check question: What are the key differences between H3D-Format and SMPL-D135 in terms of information preservation?

- Concept: Transformer architectures
  - Why needed here: The model uses causal transformers for autoregressive generation, similar to language models
  - Quick check question: How does the causal attention mechanism in transformers enable sequential generation?

## Architecture Onboarding

- Component map:
  MotionLib dataset (1.2M motion sequences with hierarchical text) -> Motion tokenizer (2D-LFQ with lookup-free quantization) -> LLM backbone (decoder-only transformer) -> Motion feature representation (SMPL-D135) -> Training pipeline (two-stage: motion-text alignment + instruction tuning)

- Critical path: Data → Tokenization → LLM fine-tuning → Generation
  1. Motion sequences are extracted and refined from web videos
  2. Motion features are encoded using SMPL-D135 format
  3. 2D-LFQ tokenizes the motion features into discrete tokens
  4. LLM is fine-tuned on motion-text pairs
  5. Generated tokens are decoded back into motion sequences

- Design tradeoffs:
  - 2D-LFQ vs traditional VQ: Increased codebook capacity vs. computational overhead
  - SMPL-D135 vs H3D-Format: Information preservation vs. simplicity
  - Large vs small models: Better performance vs. computational cost
  - Hierarchical vs single-level text: Richer information vs. increased complexity

- Failure signatures:
  - Poor FID scores: Issues with motion quality or realism
  - Low R-precision: Poor semantic alignment between text and motion
  - Codebook collapse: Inadequate codebook utilization in quantization
  - Overfitting: Performance degradation on unseen data

- First 3 experiments:
  1. Compare motion reconstruction performance using different quantization methods (VQ, RQ-VAE, 2D-LFQ) on HumanML3D
  2. Evaluate scaling effects by training models of different sizes on datasets of varying scales (HumanML3D, Motion-X, MotionLib)
  3. Test the impact of hierarchical vs single-level text descriptions on semantic alignment metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio between static and dynamic motion data for training large motion models?
- Basis in paper: [inferred] The paper discusses collecting approximately 600K static human-related images and extracting poses, repeating them across 64 frames as motion sequences. Experiments showed slight improvements in R-Precision when incorporating both static data and synthetic data.
- Why unresolved: The paper notes that the gains from incorporating static data are marginal, suggesting that the optimal balance between static and dynamic data remains unclear. The potential benefits of static data for dynamic motion generation are acknowledged but not fully explored.
- What evidence would resolve it: Systematic experiments varying the proportion of static versus dynamic motion data in training, measuring performance on diverse benchmarks, would clarify the optimal ratio.

### Open Question 2
- Question: How can we develop more robust evaluation metrics for motion generation that better align with human perception?
- Basis in paper: [explicit] The paper discusses limitations of current evaluation metrics, noting that existing retrieval models are constrained by small parameter size and limited training data. It references recent works highlighting that existing metrics are sensitive to embedding space quality and often misalign with human perception.
- Why unresolved: Current evaluation metrics like R-Precision and FID rely on lightweight motion autoencoders trained on limited data, which may not generalize effectively and can lead to unreliable semantic alignment between text and motion, particularly for unseen motions.
- What evidence would resolve it: Development and validation of new evaluation metrics that incorporate human judgment, perceptual studies, or more robust embedding models trained on larger motion datasets would address this issue.

### Open Question 3
- Question: What is the relationship between codebook size and motion generation quality, and how can we optimize codebook utilization?
- Basis in paper: [explicit] The paper introduces 2D-LFQ, a novel motion quantization method that expands the motion codebook by at least two orders of magnitude. It demonstrates that 2D-LFQ outperforms traditional methods like VQ and RQ on larger datasets like Motion-X and MotionLib, with utilization rates continuing to increase with larger codebooks.
- Why unresolved: While 2D-LFQ shows promise in expanding codebook capacity without token lookups, the paper doesn't fully explore the relationship between codebook size and motion generation quality, or how to optimize codebook utilization for different dataset scales and motion complexities.
- What evidence would resolve it: Comprehensive ablation studies varying codebook sizes across different dataset scales and motion types, measuring generation quality and computational efficiency, would clarify this relationship and optimization strategies.

## Limitations
- MotionLib dataset is not publicly available, making independent verification difficult
- 2D-LFQ tokenizer's advantages over traditional VQ methods lack theoretical justification
- Hierarchical text annotation approach's performance gains versus added complexity are not fully explored

## Confidence
- **High Confidence**: The general observation that larger models trained on larger datasets perform better follows established scaling laws observed across multiple domains including NLP and computer vision. The performance improvements on HumanML3D benchmark are quantifiable and reproducible given access to the dataset.
- **Medium Confidence**: The specific mechanisms proposed (2D-LFQ quantization, hierarchical text annotations) show empirical improvements but rely on novel architectural choices that haven't been widely validated. The SMPL-D135 feature representation provides more detailed motion information but the trade-offs compared to simpler representations need further exploration.
- **Low Confidence**: The claim that MotionLib represents a foundational advance for motion generation depends heavily on the dataset's quality and diversity, which cannot be independently verified without access to the full dataset. The long-term impact of these specific architectural choices remains uncertain.

## Next Checks
1. **Independent Dataset Replication**: Reconstruct a comparable motion dataset from publicly available sources to verify whether the scaling effects observed with MotionLib generalize to other large-scale motion datasets. Focus on maintaining similar hierarchical text annotation structure.

2. **Ablation Study on MotionBook Components**: Systematically test each component of the MotionBook encoding approach (2D representation, lookup-free quantization, SMPL-D135 features) individually to isolate which specific innovations drive performance improvements versus general scaling effects.

3. **Cross-Domain Generalization Test**: Evaluate Being-M0 on tasks outside the HumanML3D benchmark, such as motion completion, interpolation, or control tasks for humanoid robots, to assess whether the model truly learns general motion representations or simply memorizes HumanML3D patterns.