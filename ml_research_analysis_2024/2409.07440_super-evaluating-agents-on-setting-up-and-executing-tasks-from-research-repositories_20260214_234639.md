---
ver: rpa2
title: 'SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories'
arxiv_id: '2409.07440'
source_url: https://arxiv.org/abs/2409.07440
tags:
- agents
- tasks
- github
- repositories
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce SUPER, a benchmark for evaluating agents on setting
  up and executing tasks from research repositories. SUPER focuses on realistic challenges
  faced by researchers working with ML/NLP repositories, including dependency management,
  configuration, data handling, and runtime issue resolution.
---

# SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories

## Quick Facts
- arXiv ID: 2409.07440
- Source URL: https://arxiv.org/abs/2409.07440
- Authors: Ben Bogin; Kejuan Yang; Shashank Gupta; Kyle Richardson; Erin Bransom; Peter Clark; Ashish Sabharwal; Tushar Khot
- Reference count: 40
- Key outcome: Introduced SUPER benchmark showing current LLM agents solve only 16.3% of end-to-end research repository tasks

## Executive Summary
SUPER is a benchmark designed to evaluate autonomous agents on setting up and executing tasks from research repositories. The benchmark addresses realistic challenges faced by researchers working with ML/NLP repositories, including dependency management, configuration, data handling, and runtime issue resolution. SUPER comprises three sets: 45 end-to-end expert tasks with gold solutions, 152 masked sub-problems targeting specific challenges, and 604 automatically generated tasks for development. The evaluation reveals significant challenges for current agents, with the best model (GPT-4o) solving only 16.3% of end-to-end tasks and 46.1% of sub-problems.

## Method Summary
SUPER evaluates LLM agents using ReAct/SWE-Agent architectures in Jupyter notebook environments with file editing capabilities. Agents receive task prompts and must navigate repositories, resolve dependencies, configure experiments, and execute scripts. Evaluation uses outcome-based metrics comparing against expert-provided gold solutions with tolerance for minor numerical differences, plus landmark-based progress tracking. For automatically generated tasks without gold solutions, a Script-Executed proxy metric checks successful script execution. The benchmark includes three task sets: expert tasks (45), masked sub-problems (152), and automatically generated tasks (604).

## Key Results
- GPT-4o solves only 16.3% of end-to-end expert tasks and 46.1% of sub-problems
- Open-source models (Llama 3.1, Mixtral) perform substantially worse than proprietary models
- Agents struggle more with repository comprehension and configuration than with well-specified error resolution
- Landmark-based evaluation agrees with accuracy in 69% of cases for automatic tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masked coding sub-problems enable finer-grained evaluation by isolating specific technical challenges
- Mechanism: By removing solution cells that address specific aspects (dependencies, configuration, data handling), agents are tasked with solving narrower sub-problems while the remaining cells are pre-executed
- Core assumption: Extracting sub-problems from gold solutions captures representative technical challenges that agents face in real research repository execution
- Evidence anchors: [abstract] "152 sub-problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer)"

### Mechanism 2
- Claim: Outcome-based evaluation with gold solutions provides objective measurement of agent success
- Mechanism: Experts provide deterministic solutions for each task, which are executed to generate gold answers. Agents' outputs are compared against these gold answers with tolerance for minor numerical differences (10^-2 error)
- Core assumption: Expert solutions can be reliably executed and reproduced across different environments to establish ground truth
- Evidence anchors: [abstract] "we compare their answers (e.g., metrics to be reported) to the gold solutions"

### Mechanism 3
- Claim: Script-Executed proxy metric effectively approximates success for automatically generated tasks
- Mechanism: For tasks without gold solutions, success is determined by checking if the target script executes without exceptions for a minimum duration (10 seconds), indicating successful setup and execution
- Core assumption: Script execution without exceptions for sufficient duration correlates with successful task completion even without gold solutions
- Evidence anchors: [abstract] "we simply check if a key script (e.g., the training or evaluation script) was run successfully without exceptions"

## Foundational Learning

- Concept: Repository comprehension and exploration
  - Why needed here: Agents must navigate research repositories to understand code structure, locate relevant scripts, and identify configuration files before execution
  - Quick check question: Given a repository with multiple Python files, how would you identify which file contains the training script and what arguments it accepts?

- Concept: Dependency management and environment setup
  - Why needed here: Research repositories often have complex dependency requirements including version conflicts that must be resolved before execution
  - Quick check question: If a repository requires transformers==4.28.1 but your environment has transformers==4.40.2, what pip command would you use to downgrade?

- Concept: Error analysis and debugging
  - Why needed here: Agents encounter runtime errors, import issues, and configuration problems that require systematic debugging to resolve
  - Quick check question: When encountering "ImportError: cannot import name 'default_hp_search_backend'", what steps would you take to diagnose and resolve this issue?

## Architecture Onboarding

- Component map: Environment (Jupyter notebook with Python/bash execution) ←→ Agent (ReAct/SWE-Agent with tools) ←→ Evaluation (accuracy/landmarks metrics)
- Critical path: Task prompt → Repository cloning → Dependency installation → Configuration editing → Script execution → Result extraction → Answer submission
- Design tradeoffs: Fine-grained sub-problems vs end-to-end evaluation (better signal vs realistic assessment), outcome-based vs proxy metrics (accuracy vs scalability), expert vs automatic task generation (quality vs quantity)
- Failure signatures: Token exhaustion during long trajectories, inability to edit files without dedicated tools, failure to resolve version conflicts, missing configuration parameters
- First 3 experiments:
  1. Run a simple task with known solution to validate environment and evaluation pipeline
  2. Test masked sub-problem extraction on a simple task to verify sub-problem generation works
  3. Compare agent performance on tasks with different dependency complexity to identify bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using different landmark-based evaluation metrics on agent performance?
- Basis in paper: [inferred] The paper mentions using landmark-based evaluation to measure progress toward solving tasks, even when the final solution is not entirely correct.
- Why unresolved: The paper does not provide a detailed analysis of how different landmark-based evaluation metrics might affect the assessment of agent performance or their ability to capture meaningful progress.

### Open Question 2
- Question: How does the difficulty of end-to-end tasks compare to the difficulty of sub-problems extracted from these tasks?
- Basis in paper: [explicit] The paper states that the best model (GPT-4o) solves only 16.3% of end-to-end tasks and 46.1% of sub-problems, suggesting a significant difference in difficulty.
- Why unresolved: The paper does not provide a detailed analysis of the specific challenges that make end-to-end tasks more difficult than sub-problems, or whether this difference is consistent across different types of tasks.

### Open Question 3
- Question: What are the potential benefits and drawbacks of using the Script-Executed metric for evaluating automatically generated tasks?
- Basis in paper: [explicit] The paper introduces the Script-Executed metric as a proxy for evaluating automatically generated tasks, noting that it agrees with landmark evaluation in 90% of cases and with accuracy in 69% of cases.
- Why unresolved: The paper does not explore the potential limitations of this metric, such as its susceptibility to manipulation or its ability to accurately reflect task success in all scenarios.

## Limitations
- Current agents struggle significantly with repository comprehension and configuration tasks
- Benchmark may not capture the full spectrum of valid approaches to research repository execution
- Gold solutions from experts may not be generalizable across different environments and frameworks

## Confidence

**High Confidence**: The benchmark design and evaluation methodology are sound and provide meaningful differentiation between models

**Medium Confidence**: The claim that agents struggle more with repository comprehension than error resolution is supported by the data but requires deeper qualitative analysis

**Low Confidence**: The generalizability of findings to non-NLP/ML repositories and the long-term stability of gold solutions across evolving frameworks

## Next Checks

1. Conduct ablation studies removing specific tool capabilities (file editing, pip installation) to quantify their impact on performance
2. Test agent performance on progressively more complex dependency chains to identify the breaking point
3. Evaluate whether providing additional context about repository structure improves performance on comprehension-heavy tasks