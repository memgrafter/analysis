---
ver: rpa2
title: Do We Really Need Graph Convolution During Training? Light Post-Training Graph-ODE
  for Efficient Recommendation
arxiv_id: '2407.18910'
source_url: https://arxiv.org/abs/2407.18910
tags:
- graph
- convolution
- training
- recommendation
- lightgode
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the long-standing belief that graph convolutions
  are essential during training for recommendation systems. Through empirical and
  analytical studies, it demonstrates that graph convolutions play a more critical
  role in testing than in training.
---

# Do We Really Need Graph Convolution During Training? Light Post-Training Graph-ODE for Efficient Recommendation

## Quick Facts
- arXiv ID: 2407.18910
- Source URL: https://arxiv.org/abs/2407.18910
- Authors: Weizhi Zhang; Liangwei Yang; Zihe Song; Henry Peng Zou; Ke Xu; Liancheng Fang; Philip S. Yu
- Reference count: 40
- One-line primary result: Graph convolutions can be moved to post-training phase without sacrificing performance, achieving up to 10% improvement in NDCG@20 and Recall@20 while reducing training time by over 90%.

## Executive Summary
This paper challenges the conventional wisdom that graph convolutions must be performed during training for recommendation systems. Through systematic empirical and analytical studies, the authors demonstrate that graph convolutions play a more critical role during testing than training. Based on this insight, they propose Light Post-Training Graph-ODE (LightGODE), a novel framework that skips time-consuming graph convolution operations during training and employs a continuous graph ordinary-differential-equation to achieve fine-grained post-training graph convolution. The approach significantly reduces training time while maintaining or improving recommendation performance on three real-world datasets.

## Method Summary
LightGODE consists of two main phases: efficient pre-training using matrix factorization with alignment and uniformity losses, followed by lightweight post-training using a continuous graph ordinary differential equation (ODE). During pre-training, user and item embeddings are learned through MF-style training without graph convolutions, focusing on alignment (minimizing distance between positive pairs) and uniformity (maximizing angular distance between negative pairs). The post-training phase applies a continuous ODE formulation (dh/dt = Āh(t) + h₀) to achieve fine-grained graph convolution, where the self-loop mechanism prioritizes shallow layers to minimize embedding discrepancy while capturing multi-hop information. This approach effectively addresses the embedding discrepancy issue commonly associated with deeper graph convolution layers.

## Key Results
- LightGODE achieves up to 10% improvement in NDCG@20 and Recall@20 compared to strong baselines like LightGCN and GraphAU
- Training time is reduced by over 90% on large-scale datasets while maintaining or improving performance
- The framework effectively addresses the embedding discrepancy issue commonly associated with deeper graph convolution layers
- Empirical studies show that graph convolutions are more critical for testing than training in recommendation systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph convolutions are more critical for testing than training.
- Mechanism: The post-training graph convolution with self-loop (SL) in LightGODE captures multi-hop information without disrupting the original embedding space learned by MF.
- Core assumption: The embedding space from MF training is sufficiently aligned and uniform for testing; additional GCN training is redundant.
- Evidence anchors:
  - [abstract] "Our investigation reveals that the benefits of GCNs are more pronounced during testing rather than training."
  - [section] "We empirically find that the increasing number of layers significantly enlarges the difference between embeddings before and after convolution, denoted as the Embedding Discrepancy."
- Break condition: If post-training convolution significantly alters the MF embedding distribution, performance drops.

### Mechanism 2
- Claim: Alignment force equivalence between MF and GCN training.
- Mechanism: The alignment force from GCN neighborhood aggregation is a degree-weighted version of direct alignment in MF; DFS-based alignment in MF mimics BFS in GCN.
- Core assumption: Same supervision objective yields similar alignment properties despite different graph traversal methods.
- Evidence anchors:
  - [section] "We empirically evaluated the alignment property [32, 34] (the average distance between normalized positive embeddings) of four model versions..."
  - [section] "the alignment force applied on surrounding neighbors of positive pairs with graph convolution is the degree-weighted version of that alignment directly forced on two clusters of nodes."
- Break condition: If alignment metrics diverge significantly between MF and GCN, performance gap widens.

### Mechanism 3
- Claim: Continuous ODE provides optimal trade-off between high-order information and embedding discrepancy.
- Mechanism: The ODE formulation dh/dt = Āh(t) + h₀ allows fine-grained convolution depth control; self-loop prioritizes shallow layers to minimize discrepancy.
- Core assumption: Riemann sum approximation of GCN layers converges to ODE solution; Euler solver is sufficient for practical use.
- Evidence anchors:
  - [section] "Based on the formulation, we propose that LightGODE - a continuous post-training graph convolution based on ordinary-differential-equations aiming to achieve the optimal trade-off."
  - [section] "the continuity offers several advantages... it enables precise and fine-grained graph convolution to achieve the optimal trade-off..."
- Break condition: If ODE solver introduces numerical instability or excessive computation, benefits vanish.

## Foundational Learning

- Concept: Matrix Factorization (MF) basics
  - Why needed here: LightGODE relies on MF for efficient initial embedding training before post-training convolution.
  - Quick check question: Can you explain how MF minimizes the distance between positive user-item pairs in embedding space?

- Concept: Graph Convolution Network (GCN) layer operations
  - Why needed here: Understanding GCN message passing is essential to appreciate why skipping it in training is possible.
  - Quick check question: What is the role of the adjacency matrix normalization in GCN, and why is it omitted in LightGODE training?

- Concept: Neural Ordinary Differential Equations (ODEs)
  - Why needed here: LightGODE's continuous graph ODE is central to its fine-grained convolution mechanism.
  - Quick check question: How does the Euler solver approximate the solution of dh/dt = Āh(t) + h₀ in LightGODE?

## Architecture Onboarding

- Component map: MF pre-training -> Continuous graph ODE post-training -> Combined embeddings for recommendation
- Critical path: 1. Train MF embeddings (fast, no GCN) 2. Apply continuous ODE convolution (lightweight) 3. Combine embeddings for recommendation
- Design tradeoffs:
  - Accuracy vs. speed: Skipping GCN training boosts speed but risks underfitting
  - Depth vs. discrepancy: ODE allows fine-grained depth control to balance information and stability
- Failure signatures:
  - Large embedding discrepancy → performance degradation
  - ODE solver divergence → numerical errors
  - Insufficient MF training → poor initial embeddings
- First 3 experiments:
  1. Compare MF-conv vs. LightGCN-conv on small dataset (NDCG@20)
  2. Vary ODE time t and measure embedding discrepancy
  3. Measure training time reduction vs. performance retention on Gowalla dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the embedding discrepancy issue be completely eliminated while maintaining or improving recommendation performance?
- Basis in paper: [explicit] The authors acknowledge that embedding discrepancy increases with deeper layers and propose their continuous ODE approach as a mitigation strategy, but don't claim complete elimination.
- Why unresolved: The paper demonstrates reduction but not elimination of embedding discrepancy, and it's unclear if this is theoretically possible without sacrificing performance.
- What evidence would resolve it: Empirical results showing a post-training method that eliminates embedding discrepancy while maintaining or improving recommendation metrics compared to current approaches.

### Open Question 2
- Question: What is the optimal trade-off between embedding discrepancy and higher-order information capture in post-training graph convolution?
- Basis in paper: [explicit] The authors identify this as a key challenge and propose their continuous ODE approach as a solution, but don't provide a definitive answer to what constitutes the optimal balance.
- Why unresolved: The optimal balance likely depends on dataset characteristics and recommendation task, requiring extensive empirical study across diverse scenarios.
- What evidence would resolve it: Systematic experiments across multiple datasets and recommendation tasks identifying consistent patterns in the optimal balance between embedding discrepancy and higher-order information capture.

### Open Question 3
- Question: How does the proposed LightGODE framework scale to extremely large-scale recommendation systems with billions of users and items?
- Basis in paper: [inferred] While the authors demonstrate efficiency gains on datasets with up to 64,116 users and 164,533 items, they don't address scaling to truly massive industrial-scale systems.
- Why unresolved: The computational complexity and memory requirements of the continuous ODE approach at industrial scale remain untested and potentially problematic.
- What evidence would resolve it: Implementation and testing of LightGODE on industrial-scale datasets with billions of users and items, demonstrating both efficiency and maintained performance at such scales.

## Limitations

- The analytical claims about alignment/uniformity equivalence between MF and GCN remain correlational rather than proven
- The generalizability of findings to extremely sparse datasets or cold-start scenarios is untested
- Potential long-term effects of skipping GCN training on model robustness are not addressed

## Confidence

- Confidence in core claim (graph convolution more critical during testing than training): Medium-High
- Confidence in mechanism (why post-training convolution suffices): Medium
- Confidence in scalability claims: Low

## Next Checks

1. Conduct a thorough ablation study varying the quality of pre-training (different MF objectives) to establish robustness boundaries
2. Test LightGODE on a dataset with extreme sparsity to validate performance claims under challenging conditions
3. Analyze the learned embeddings qualitatively to confirm that the ODE transformation indeed captures meaningful multi-hop relationships without distortion