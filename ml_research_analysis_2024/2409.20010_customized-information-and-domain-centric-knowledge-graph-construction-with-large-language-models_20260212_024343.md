---
ver: rpa2
title: Customized Information and Domain-centric Knowledge Graph Construction with
  Large Language Models
arxiv_id: '2409.20010'
source_url: https://arxiv.org/abs/2409.20010
tags:
- knowledge
- graph
- systems
- innovation
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a scalable knowledge graph construction framework
  that improves technology intelligence and cyber-physical systems planning by pre-selecting
  relevant documents from heterogeneous sources before applying large language models.
  The approach combines text mining, keyphrase extraction, semantic network creation,
  and reasoning techniques to generate comprehensive, domain-specific knowledge graphs.
---

# Customized Information and Domain-centric Knowledge Graph Construction with Large Language Models

## Quick Facts
- arXiv ID: 2409.20010
- Source URL: https://arxiv.org/abs/2409.20010
- Reference count: 11
- Outperforms existing approaches by producing over 200% more correctly classified classes in automotive electrical systems domain

## Executive Summary
This paper introduces a scalable knowledge graph construction framework that improves technology intelligence and cyber-physical systems planning by pre-selecting relevant documents from heterogeneous sources before applying large language models. The approach combines text mining, keyphrase extraction, semantic network creation, and reasoning techniques to generate comprehensive, domain-specific knowledge graphs. When applied to automotive electrical systems, the method outperformed existing approaches like GraphGPT and transformer-based models by producing over 200% more correctly classified classes and generating more detailed relationships. The resulting knowledge graph contained 3,100 axioms, 650 classes, and 16 object properties, demonstrating the approach's ability to create machine-actionable, expert-domain knowledge bases that scale well for planning purposes.

## Method Summary
The framework employs a two-stage knowledge graph construction process. First, it crawls and pre-selects relevant documents from heterogeneous sources using information retrieval, keyphrase extraction with normalized pointwise mutual information (nPMI), and semantic network creation based on MiniLM-L6 embeddings. This filtered corpus is then processed through a knowledge graph construction pipeline using large language models (REBEL transformer and ChatGPT) to extract domain-specific triples. Finally, the constructed graph undergoes consistency analysis against the GENIAL! Basic Ontology to ensure semantic accuracy and taxonomic correctness. The approach specifically targets automotive electrical systems but demonstrates generalizability across domains.

## Key Results
- Outperformed GraphGPT, bi-LSTM, and transformer REBEL by over 200% in correctly classified classes
- Generated knowledge graph with 3,100 axioms, 650 classes, and 16 object properties
- Achieved consistent class recognition improvements across different document types (news articles: FMR 0.72, research articles: FMR 0.46, patents: FMR 0.62)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-selecting relevant documents from heterogeneous sources before LLM application significantly improves knowledge graph quality and relevance.
- Mechanism: By filtering documents based on relevance scores calculated through keyphrase extraction and semantic network analysis, the framework ensures that only domain-specific, high-value information enters the knowledge graph construction pipeline.
- Core assumption: Document relevance scoring accurately identifies domain-specific information that will contribute meaningfully to the knowledge graph.
- Evidence anchors:
  - [abstract] "Our methodology involves crawling a wide variety of sources at an extensive scale... it results in the accumulation of more relevant information, contributing to the creation of larger and more comprehensive knowledge bases."
  - [section] "Using the results of the text mining procedure explained in the previous section, all relevant documents have been identified and are fed to the KGC pipeline."
  - [corpus] Weak - the corpus shows related papers but doesn't provide evidence for document pre-selection effectiveness.
- Break condition: If relevance scoring fails to accurately identify domain-specific information, irrelevant or low-quality documents will enter the pipeline, degrading knowledge graph quality.

### Mechanism 2
- Claim: The combination of text mining (information retrieval, keyphrase extraction, semantic network creation) with LLM-based knowledge graph construction produces more accurate and comprehensive knowledge representations than either approach alone.
- Mechanism: Text mining provides structured, domain-relevant input that guides the LLM in creating more accurate knowledge graph triples, while the LLM adds semantic richness and relationship discovery capabilities.
- Core assumption: The structured input from text mining significantly improves LLM performance compared to unstructured text input.
- Evidence anchors:
  - [abstract] "Our results demonstrate that our construction process outperforms GraphGPT as well as our bi-LSTM and transformer REBEL with a pre-defined dataset by several times in terms of class recognition, relationship construction and correct 'sublass of' categorization."
  - [section] "The bi-LSTM achieved an f1 score between 0.36 and 0.78... We observed an increase in the number of recognized classes and the construction of relationships as well."
  - [corpus] Weak - corpus evidence shows related research but doesn't directly compare text mining + LLM approaches to other methods.
- Break condition: If the text mining phase fails to provide sufficiently structured or relevant input, the LLM gains no advantage and performance may be similar to or worse than direct LLM application.

### Mechanism 3
- Claim: Reasoning techniques applied to the constructed knowledge graph improve semantic accuracy and consistency by enforcing domain ontology constraints.
- Mechanism: The reasoning procedure checks the constructed knowledge graph against the GENIAL! Basic Ontology definitions, removing or correcting entries that violate domain-specific semantic rules.
- Core assumption: The reference ontology (GENIAL! Basic Ontology) accurately captures domain semantics and can effectively validate knowledge graph entries.
- Evidence anchors:
  - [abstract] "Additionally, we complement this approach with reasoning techniques to enhance semantic accuracy."
  - [section] "After the construction process our graph is checked against the definitions of our reference ontology GBO... This reasoning procedure ensures an improved consistency and structure."
  - [corpus] Weak - corpus doesn't provide evidence for reasoning effectiveness.
- Break condition: If the reference ontology is incomplete or contains incorrect semantic definitions, the reasoning process may incorrectly remove valid knowledge graph entries or fail to catch semantic errors.

## Foundational Learning

- Concept: Information retrieval and keyphrase extraction
  - Why needed here: These techniques identify relevant documents and extract domain-specific terms that form the foundation for semantic network creation and document relevance scoring.
  - Quick check question: How does normalized pointwise mutual information (nPMI) help identify relevant terms across different document genres?

- Concept: Semantic similarity and embedding models
  - Why needed here: Embeddings capture semantic relationships between terms, enabling the construction of semantic networks that reveal domain structure and relationships.
  - Quick check question: What advantages does the all-MiniLM-L6-v2 model offer compared to BGE large and fastText for semantic similarity tasks?

- Concept: Ontology-based reasoning and consistency checking
  - Why needed here: Reasoning techniques validate the knowledge graph against domain semantics, ensuring semantic accuracy and consistency.
  - Quick check question: How does the GENIAL! Basic Ontology's distinction between "has part directly" and "has part" relationships support consistency checking?

## Architecture Onboarding

- Component map:
  Innovation Graph Database (50M research publications, 81M news articles, 45M patents) -> Information Retrieval Engine (search strategy matching) -> Keyphrase Extraction Module (co-occurrence analysis with nPMI) -> Semantic Network Builder (embedding-based similarity with MCL clustering) -> Relevance Scoring System (document filtering based on semantic analysis) -> Knowledge Graph Construction Pipeline (LLM-based triple extraction with REBEL/ChatGPT) -> Reasoning Engine (GENIAL! Basic Ontology consistency checking) -> Visualization Layer (topic maps, knowledge graph rendering)

- Critical path: Document retrieval → Keyphrase extraction → Semantic network creation → Relevance scoring → Document filtering → Knowledge graph construction → Reasoning validation → Output

- Design tradeoffs:
  - Pre-selection vs. comprehensiveness: Filtering documents improves quality but may miss unexpected connections
  - Speed vs. accuracy: Using MiniLM-L6 prioritizes inference speed over potential accuracy gains from larger models
  - Structure vs. flexibility: Strict ontology adherence ensures consistency but may limit capturing novel domain relationships

- Failure signatures:
  - Low relevance scores across all documents indicate poor search strategy definition
  - Semantic network with disconnected components suggests embedding model issues
  - Reasoning engine removing >50% of constructed triples indicates ontology incompatibility
  - Knowledge graph size significantly smaller than expected suggests document filtering too aggressive

- First 3 experiments:
  1. Test document relevance scoring by comparing filtered vs. unfiltered document sets on a small domain sample
  2. Evaluate embedding model performance by clustering known related terms and measuring cosine similarity
  3. Validate reasoning effectiveness by introducing controlled semantic errors and measuring detection rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the reasoning time scale when applying the knowledge graph construction framework to larger and more diverse datasets beyond the three-article demonstration?
- Basis in paper: [explicit] The paper mentions that reasoning time is not a bottleneck for the current demonstration but does not provide data on scaling to larger datasets.
- Why unresolved: The current evaluation is limited to a small dataset, and the paper does not explore performance metrics or challenges associated with scaling to significantly larger or more heterogeneous datasets.
- What evidence would resolve it: Empirical data showing reasoning time and accuracy metrics when applying the framework to datasets of varying sizes and complexities, including benchmarks against existing methods.

### Open Question 2
- Question: How does the accuracy of subclass categorization compare when using the proposed framework versus traditional ontology-based methods for domains outside of automotive electrical systems?
- Basis in paper: [inferred] The paper highlights taxonomic correctness as a challenge and focuses on automotive electrical systems, but does not test the framework on other domains.
- Why unresolved: The evaluation is domain-specific, and there is no comparison with traditional methods in other domains to assess generalizability.
- What evidence would resolve it: Comparative studies applying the framework to multiple domains (e.g., healthcare, energy systems) and benchmarking subclass categorization accuracy against traditional ontology-based methods.

### Open Question 3
- Question: What are the limitations of using large language models for knowledge graph construction in terms of semantic accuracy and completeness, and how can these be mitigated?
- Basis in paper: [explicit] The paper discusses the use of LLMs for knowledge graph construction and mentions that reasoning techniques are employed to enhance semantic accuracy, but does not explore limitations or mitigation strategies in detail.
- Why unresolved: The paper focuses on the benefits of using LLMs but does not address potential drawbacks or limitations in semantic accuracy and completeness.
- What evidence would resolve it: Detailed analysis of LLM-generated knowledge graphs, identifying specific semantic inaccuracies or gaps, and proposing or testing strategies to mitigate these issues.

## Limitations
- Framework performance heavily depends on quality of pre-selection relevance scoring, which showed significant variation across document genres
- Semantic network construction using MiniLM-L6 embeddings may miss nuanced relationships that larger models could capture
- Reasoning validation relies on completeness and accuracy of reference ontology, potentially limiting discovery of novel domain relationships

## Confidence
- High Confidence: Document pre-selection mechanism and its impact on reducing irrelevant content in knowledge graph is well-supported by evidence
- Medium Confidence: 200% improvement over baseline models is supported by methodology description but specific quantitative results are not provided
- Low Confidence: Scalability claims for planning purposes lack specific performance metrics or real-world application examples

## Next Checks
1. Conduct controlled experiment comparing knowledge graph quality with and without document pre-selection phase using standardized evaluation dataset
2. Perform ablation studies testing different embedding models to determine optimal balance between computational efficiency and relationship discovery capability
3. Implement cross-validation of reasoning validation process by introducing known semantic errors and measuring detection rate and false positive rate