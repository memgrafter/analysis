---
ver: rpa2
title: 'LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model'
arxiv_id: '2401.02330'
source_url: https://arxiv.org/abs/2401.02330
tags:
- arxiv
- language
- llav
- preprint
- multi-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLaVA-Phi, a compact multi-modal model that
  combines a small language model (Phi-2, 2.7B parameters) with a vision encoder to
  perform vision-language tasks. The model follows a two-stage training pipeline using
  supervised fine-tuning and visual instruction tuning.
---

# LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model

## Quick Facts
- arXiv ID: 2401.02330
- Source URL: https://arxiv.org/abs/2401.02330
- Authors: Yichen Zhu; Minjie Zhu; Ning Liu; Zhicai Ou; Xiaofeng Mou; Jian Tang
- Reference count: 36
- LLaVA-Phi achieves competitive performance on multiple benchmarks including ScienceQA where it outperforms larger models

## Executive Summary
This paper introduces LLaVA-Phi, a compact multi-modal model that combines a small language model (Phi-2, 2.7B parameters) with a vision encoder to perform vision-language tasks. The model follows a two-stage training pipeline using supervised fine-tuning and visual instruction tuning. Despite its small size, LLaVA-Phi achieves competitive performance on multiple benchmarks, including ScienceQA where it outperforms larger models. On the MMBench benchmark, it surpasses models like Otter by 11.5% and InstructBLIP by 23.8%. The work demonstrates that small language models can effectively handle complex vision-language tasks when trained with high-quality data, enabling efficient deployment in time-sensitive or resource-constrained applications.

## Method Summary
LLaVA-Phi combines Phi-2 (2.7B parameters) with a pre-trained CLIP vision encoder through a two-layer MLP projector. The training follows a two-stage pipeline: first fine-tuning Phi-2 with supervised learning on Vicuna-style data, then jointly fine-tuning the entire architecture using filtered CC-595K data and LLaVA-Instruct-150K. The model is trained on 8 A100 GPUs with Adam optimizer, weight decay 0.1, and specific learning rates for each stage. The architecture achieves competitive performance on 8 benchmarks while maintaining efficiency with only 3B total parameters.

## Key Results
- Achieves performance comparable to or surpassing larger multi-modal models despite having only 3B parameters
- Outperforms Otter by 11.5% and InstructBLIP by 23.8% on MMBench benchmark
- Surpasses larger models on ScienceQA, demonstrating effectiveness of small models with high-quality training data
- Successfully performs complex tasks including code generation from visual inputs and mathematical reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLaVA-Phi achieves competitive performance with a small language model by leveraging high-quality instruction-tuning data and efficient training strategies.
- Mechanism: The model combines Phi-2 with a pre-trained CLIP vision encoder and a two-layer MLP projector. Training follows a two-stage process: first fine-tuning Phi-2 with supervised learning on Vicuna-style data, then jointly fine-tuning the entire architecture using filtered CC-595K data and LLaVA-Instruct-150K.
- Core assumption: Smaller language models can match or exceed the performance of larger ones when trained with carefully curated, high-quality datasets and optimized architectures.
- Evidence anchors:
  - [abstract] "Our model delivers commendable performance on publicly available benchmarks... Despite possessing only 3 billion parameters, it achieves performance comparable to, or even surpassing, some larger multi-modal models that are three times larger."
  - [section] "For pre-training, we utilize a filtered subset of the CC-595K dataset [24] over one epoch... Then, we fine-tune the model on LLaVA-Instruct-150K dataset for 1 epoch at a learning rate of 2e-5 and a batch size of 256."
  - [corpus] Weak - no direct citations to support this specific mechanism; only related works without strong performance comparison.
- Break condition: Performance degrades significantly if the quality of the instruction-tuning data is reduced or if the two-stage training pipeline is skipped.

### Mechanism 2
- Claim: The integration of Phi-2, pre-trained on code and mathematical corpora, enables superior performance on tasks requiring code generation and mathematical reasoning.
- Mechanism: Phi-2's pre-training on code snippets and mathematical problems transfers to the multi-modal setting, allowing LLaVA-Phi to excel in tasks like code generation from visual input and solving mathematical equations.
- Core assumption: Pre-training on specific domains (code, math) provides transferable skills that benefit multi-modal reasoning tasks.
- Evidence anchors:
  - [abstract] "Our model delivers commendable performance on publicly available benchmarks that encompass visual comprehension, reasoning, and knowledge-based perception."
  - [section] "In the second example, we instructed the model to generate Python code for converting an Excel table into a bar chart... LLaVA-Phi accurately comprehended the task, providing instructions to read the table, add a title and labels, and correctly plot the bar chart using matplotlib."
  - [corpus] Missing - no corpus evidence directly supporting this mechanism.
- Break condition: If the mathematical or code-related tasks are too complex or domain-specific, the performance advantage may diminish.

### Mechanism 3
- Claim: The two-stage training pipeline, involving supervised fine-tuning followed by visual instruction tuning, is crucial for achieving high performance with small language models.
- Mechanism: The initial supervised fine-tuning on Phi-2 with Vicuna-style data provides a foundation, while the subsequent visual instruction tuning on LLaVA datasets adapts the model to multi-modal tasks.
- Core assumption: Sequential fine-tuning on different datasets, starting with general language tasks and moving to visual instruction tasks, is necessary for optimal performance.
- Evidence anchors:
  - [section] "Supervised fine-tuning on Phi-2... The training was conducted over two epochs... Our findings suggest that while this step might be optional, applying SFT to Phi-2 does result in modest improvements across most benchmarks."
  - [section] "Our training approach follows the pipeline used for LLaVA1.5, consisting of a pre-training stage and a subsequent instruction tuning phase."
  - [corpus] Weak - related works do not provide direct evidence for this specific two-stage approach.
- Break condition: If the model is trained on a single dataset or if the stages are reversed, performance may suffer significantly.

## Foundational Learning

- Concept: Vision-Language Model Integration
  - Why needed here: Understanding how visual encoders (like CLIP) and language models (like Phi-2) are combined is essential for grasping the architecture and training pipeline.
  - Quick check question: How does the two-layer MLP projector facilitate the integration of the vision encoder and the language model?

- Concept: Instruction Tuning in Multi-Modal Models
  - Why needed here: The paper emphasizes the importance of high-quality instruction-tuning data for achieving competitive performance, making it crucial to understand this concept.
  - Quick check question: What is the difference between supervised fine-tuning and visual instruction tuning, and why are both used in the training pipeline?

- Concept: Parameter Efficiency in Large Language Models
  - Why needed here: The paper demonstrates that smaller models (3B parameters) can achieve performance comparable to larger models, highlighting the importance of parameter efficiency.
  - Quick check question: How does the parameter count of LLaVA-Phi compare to other multi-modal models, and what factors contribute to its competitive performance despite having fewer parameters?

## Architecture Onboarding

- Component map:
  - Vision Encoder: Pre-trained CLIP ViT-L/14 with 336x336 resolution
  - Projector: Two-layer MLP to connect vision encoder and language model
  - Language Model: Phi-2 (2.7B parameters)
  - Training Pipeline: Supervised fine-tuning on Phi-2 → Pre-training on CC-595K → Visual instruction tuning on LLaVA-Instruct-150K

- Critical path:
  1. Pre-train Phi-2 on general language tasks (if not already done)
  2. Fine-tune Phi-2 with supervised learning on Vicuna-style data
  3. Train the projector while freezing vision encoder and LLM
  4. Jointly fine-tune projector and LLM on multi-modal datasets

- Design tradeoffs:
  - Using a smaller language model (Phi-2) vs. larger models (7B+ parameters) for efficiency vs. potential performance loss
  - Two-stage training pipeline for optimal performance vs. increased training complexity
  - Pre-trained CLIP vision encoder for visual understanding vs. potential domain-specific limitations

- Failure signatures:
  - Poor performance on multi-modal tasks if the instruction-tuning data quality is low
  - Suboptimal results if the two-stage training pipeline is not followed correctly
  - Inability to handle complex mathematical or code-related tasks if Phi-2's pre-training is insufficient

- First 3 experiments:
  1. Evaluate LLaVA-Phi on VQA-v2 and ScienceQA benchmarks to assess general question-answering and specialized task performance
  2. Test code generation capabilities by providing visual inputs with instructions to generate Python code
  3. Assess mathematical reasoning by providing images with equations and measuring the model's ability to solve them accurately

## Open Questions the Paper Calls Out
The paper identifies several open questions and limitations:
- How to extend LLaVA-Phi to support multilingual instruction following
- Whether even smaller language models (e.g., Phi-1.5 or Phi-1) could be effective backbones
- The impact of different vision encoder choices on performance and efficiency
- Potential architectural modifications needed for specialized domains

## Limitations
- The paper does not provide ablation studies showing the individual contribution of each training stage or dataset to the final performance
- The filtering criteria for the CC-595K dataset are not explicitly defined, making it difficult to assess the impact of data quality
- The exact architecture of the 2-layer MLP projector is not specified, which could impact reproducibility

## Confidence

**High confidence:** The claim that LLaVA-Phi achieves competitive performance on benchmark datasets is well-supported by the experimental results, with specific numerical comparisons to other models on MMBench and ScienceQA. The two-stage training pipeline and model architecture are clearly described.

**Medium confidence:** The assertion that Phi-2's pre-training on code and mathematics specifically enables superior performance in these domains is supported by qualitative examples but lacks quantitative ablation studies or controlled experiments to isolate this effect.

**Low confidence:** The paper does not provide sufficient evidence to determine whether the performance gains are primarily due to the small model size, the quality of instruction-tuning data, or the specific training methodology. The relationship between parameter efficiency and task performance across different domains remains unclear.

## Next Checks
1. Conduct an ablation study comparing LLaVA-Phi's performance when trained with different subsets of the CC-595K dataset to quantify the impact of data quality on final results.

2. Implement and test multiple projector architectures (varying layer dimensions and activation functions) to determine the optimal configuration for connecting the vision encoder and language model.

3. Compare LLaVA-Phi's performance on code generation and mathematical reasoning tasks against larger models using controlled experiments with identical training data and evaluation protocols.