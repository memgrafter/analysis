---
ver: rpa2
title: Graph Edit Distance with General Costs Using Neural Set Divergence
arxiv_id: '2409.17687'
source_url: https://arxiv.org/abs/2409.17687
tags:
- graph
- cost
- node
- edit
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GRAPH EDX, a neural model for Graph Edit Distance
  (GED) computation that can handle general edit costs for node/edge additions and
  deletions. The key innovation is formulating GED as a Quadratic Assignment Problem
  (QAP) and replacing its terms with differentiable neural set divergence surrogates.
---

# Graph Edit Distance with General Costs Using Neural Set Divergence

## Quick Facts
- arXiv ID: 2409.17687
- Source URL: https://arxiv.org/abs/2409.17687
- Reference count: 40
- Key outcome: Neural model for GED computation that handles general edit costs with MSE as low as 0.492 on Mutag dataset

## Executive Summary
This paper proposes GRAPH EDX, a neural model for Graph Edit Distance (GED) computation that can handle general edit costs for node/edge additions and deletions. The key innovation is formulating GED as a Quadratic Assignment Problem (QAP) and replacing its terms with differentiable neural set divergence surrogates. The model represents each graph as sets of node and edge embeddings, learns node alignments using a Gumbel-Sinkhorn network, and ensures edge consistency through explicit alignment computation. Extensive experiments on seven real datasets show GRAPH EDX consistently outperforms state-of-the-art methods in MSE (e.g., 0.492 vs. 0.765-2.005 on Mutag) under both uniform and non-uniform cost settings.

## Method Summary
GRAPH EDX computes GED by first using a GNN to obtain node embeddings and an MLP to obtain edge embeddings for each graph. A Gumbel-Sinkhorn network then generates soft node alignments, from which edge alignments are derived. The model uses set divergence surrogates (ALIGN DIFF, DIFFALIGN, or XOR-DIFFALIGN) to compute costs for the four edit operations (node deletion/addition, edge deletion/addition). These costs are weighted by the specified edit costs and summed to produce the final GED prediction. The entire model is trained end-to-end using MSE loss against ground truth GED values.

## Key Results
- Achieves MSE of 0.492 on Mutag dataset, outperforming baselines (0.765-2.005)
- XOR-DIFFALIGN variant shows 12-33% improvement over simpler surrogates
- Demonstrates effective handling of both uniform and non-uniform edit costs
- Maintains strong performance across seven diverse real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GRAPH EDX replaces the QAP objective with differentiable neural set divergence surrogates that can handle general edit costs.
- Mechanism: By representing each graph as sets of node and edge embeddings, and using Gumbel-Sinkhorn networks to learn node alignments, the intractable QAP is transformed into a tractable neural optimization problem. The set divergence surrogates approximate the original QAP terms (edge deletion/addition, node deletion/addition) while maintaining differentiability for end-to-end training.
- Core assumption: The neural set divergence surrogates can effectively approximate the true GED cost terms while preserving the structure of the edit operations.
- Evidence anchors:
  - [abstract]: "proposes GRAPH EDX, a neural GED estimator that can work with general costs specified for the four edit operations"
  - [section]: "design a family of differentiable set divergence surrogates, which can replace the QAP objective with a more benign one"
  - [corpus]: "Flexible Graph Similarity Computation With A Proactive Optimization Strategy" suggests neural methods can handle flexible cost structures
- Break condition: If the set divergence surrogates fail to capture the combinatorial structure of the optimal edit path, the predicted GED will be inaccurate.

### Mechanism 2
- Claim: XOR-DIFFALIGN is crucial for accurate GED estimation because it explicitly models the condition that only edge-to-non-edge and non-edge-to-edge mappings incur costs.
- Mechanism: Unlike ALIGN DIFF and DIFFALIGN, XOR-DIFFALIGN applies a XOR operator between adjacency matrix entries, ensuring that edge-to-edge and non-edge-to-non-edge mappings have zero cost contribution. This design aligns with the inherent inductive biases of GED.
- Core assumption: The XOR operation correctly identifies mismatched edge types (edge vs non-edge) across aligned node-pairs.
- Evidence anchors:
  - [section]: "applying a XOR of the adjacency matrices to the cost matrix, ensuring that non-zero cost is computed only when mapping an edge to a non-edge or vice versa"
  - [corpus]: "GEDAN: Learning the Edit Costs for Graph Edit Distance" implies importance of correctly modeling edit costs
- Break condition: If the XOR operation fails to correctly identify edge type mismatches, the cost structure will be misrepresented.

### Mechanism 3
- Claim: Late interaction architecture enables efficient indexing and retrieval through pre-computed embeddings.
- Mechanism: By computing graph embeddings independently before computing set distances, GRAPH EDX allows corpus graph embeddings to be indexed a-priori. This enables efficient retrieval using LSH, inverted indexes, or graph-based ANN methods.
- Core assumption: The independence of graph embedding computation from pairwise interactions preserves sufficient information for accurate GED estimation.
- Evidence anchors:
  - [abstract]: "late interaction, where the interactions between the graph pairs occur only at the final layer"
  - [section]: "allows for the corpus graph embeddings to be indexed a-priori, thereby enabling efficient retrieval"
  - [corpus]: "EUGENE: Explainable Structure-aware Graph Edit Distance Estimation with Generalized Edit Costs" suggests efficiency is important for practical deployment
- Break condition: If the late interaction loses critical pairwise information needed for accurate GED estimation, the model will underperform.

## Foundational Learning

- Concept: Quadratic Assignment Problem (QAP)
  - Why needed here: GED is inherently a QAP that requires solving an NP-hard optimization problem. Understanding QAP helps grasp why GRAPH EDX's approach of replacing it with differentiable surrogates is necessary.
  - Quick check question: Why is the exact computation of GED NP-hard, and how does formulating it as a QAP capture this complexity?

- Concept: Gumbel-Sinkhorn Network
  - Why needed here: This is the core mechanism for learning differentiable node alignments. Without understanding how it produces soft permutation matrices, the node alignment component won't make sense.
  - Quick check question: How does the Gumbel-Sinkhorn network produce a differentiable approximation to a permutation matrix, and why is this important for end-to-end training?

- Concept: Graph Neural Networks (GNNs) and Message Passing
  - Why needed here: EMBED θ uses a GNN to compute node embeddings, which are then used for GED estimation. Understanding GNN fundamentals is essential for grasping how node representations are learned.
  - Quick check question: How do GNNs aggregate information from neighboring nodes, and why is this aggregation important for capturing graph structure in node embeddings?

## Architecture Onboarding

- Component map: MPNNθ (5-layer GNN with GRU updates) -> MLP θ (edge embeddings) -> PERM NET ϕ (Gumbel-Sinkhorn) -> Distance surrogates (ALIGN DIFF/DIFFALIGN/XOR-DIFFALIGN) -> Weighted sum of costs

- Critical path:
  1. Compute node embeddings X, X′ using MPNNθ
  2. Compute edge embeddings R, R′ using MLP θ
  3. Generate soft node alignment P using PERM NET ϕ
  4. Derive edge alignment S from P
  5. Compute four cost terms using chosen distance surrogate
  6. Sum weighted costs to get GED prediction

- Design tradeoffs:
  - Memory vs Accuracy: Using all node-pairs (N(N-1)/2) instead of just edges increases memory usage but improves accuracy by capturing non-edge information
  - Speed vs Expressiveness: Late interaction enables efficient indexing but may lose some pairwise interaction information compared to early interaction methods
  - Generalizability vs Specialization: XOR-DIFFALIGN adds inductive bias but may be less flexible than simpler surrogates

- Failure signatures:
  - High MSE on test set despite good validation performance: Likely overfitting to training data
  - Poor performance on datasets with different characteristics than training data: Zero-shot generalization issue
  - Training instability or NaN values: Sinkhorn iterations or temperature parameter issues
  - Consistently under-predicting GED: Possible issue with distance surrogate formulation

- First 3 experiments:
  1. Verify basic functionality: Run on a simple dataset (e.g., Mutag) with uniform costs and check if MSE is reasonable
  2. Test different distance surrogates: Compare ALIGN DIFF, DIFFALIGN, and XOR-DIFFALIGN on validation set to confirm XOR variant performs best
  3. Evaluate indexing capability: Pre-compute embeddings on training set and test retrieval efficiency with new query graphs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GRAPH EDX scale with larger graphs (N > 20) given the memory requirements of representing all node-pairs?
- Basis in paper: [explicit] The paper states that representing all node-pairs requires significant memory resources and could pose challenges for larger-sized graphs.
- Why unresolved: The experiments only used graphs with N ≤ 20, so there is no empirical evidence about scalability.
- What evidence would resolve it: Experiments on datasets with larger graphs showing runtime, memory usage, and accuracy metrics as N increases.

### Open Question 2
- Question: Can the XOR operator in XOR-DIFFALIGN be incorporated into an ANN-amenable setup for efficient retrieval?
- Basis in paper: [explicit] The paper notes that while XOR-DIFFALIGN improves performance, it prevents the model from being cast to an ANN-amenable setup unlike DIFFALIGN and ALIGN DIFF.
- Why unresolved: The paper only mentions this limitation without exploring potential workarounds.
- What evidence would resolve it: A modified version of XOR-DIFFALIGN that maintains its benefits while allowing ANN-based retrieval, demonstrated through retrieval experiments.

### Open Question 3
- Question: How would allowing different edit costs for different node pairs affect the model's performance?
- Basis in paper: [inferred] The current model assumes fixed edit costs across all graph pairs within a dataset, which the authors note might not reflect real-world scenarios where costs vary based on domain-specific factors.
- Why unresolved: The model was not tested with varying costs across node pairs, and the paper does not propose how to incorporate such variability.
- What evidence would resolve it: Experiments comparing the current fixed-cost approach with a model that learns or accepts different costs for different node pairs, using datasets with known cost variations.

## Limitations

- The O(N²) complexity for node-pair processing could become prohibitive for very large graphs
- Limited evaluation on downstream tasks like retrieval, anomaly detection, or graph generation
- Zero-shot generalization to unseen graph sizes only tested on one dataset with limited size variation

## Confidence

- **High Confidence**: The core claim that GRAPH EDX can handle general edit costs through neural set divergence surrogates is well-supported by the ablation studies showing XOR-DIFFALIGN's superiority over simpler alternatives.
- **Medium Confidence**: The assertion that GRAPH EDX consistently outperforms state-of-the-art methods is supported by extensive experiments, but the comparison is primarily against methods that don't handle general costs.
- **Low Confidence**: The claim about zero-shot generalization to unseen graph sizes is demonstrated on only one dataset with limited size variation.

## Next Checks

1. **Downstream Task Evaluation**: Test GRAPH EDX's performance on graph retrieval and anomaly detection tasks to verify its practical utility beyond GED estimation.
2. **Scalability Analysis**: Systematically evaluate the method's performance and computational requirements on increasingly large graphs (N > 100) to quantify the O(N²) complexity impact.
3. **Cross-dataset Generalization**: Conduct experiments training on multiple datasets simultaneously and testing on held-out datasets to assess the model's ability to generalize across different graph domains and structures.