---
ver: rpa2
title: 'VideoAgent: Long-form Video Understanding with Large Language Model as Agent'
arxiv_id: '2403.10517'
source_url: https://arxiv.org/abs/2403.10517
tags:
- video
- frames
- frame
- information
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VideoAgent, a system that leverages a large
  language model as an agent to perform long-form video understanding by iteratively
  searching for and aggregating relevant visual information. The agent controls a
  multi-round process where it first familiarizes itself with the video context by
  viewing a few uniformly sampled frames, then determines whether current information
  is sufficient to answer the question, and if not, retrieves additional frames containing
  the missing information using CLIP.
---

# VideoAgent: Long-form Video Understanding with Large Language Model as Agent

## Quick Facts
- **arXiv ID:** 2403.10517
- **Source URL:** https://arxiv.org/abs/2403.10517
- **Reference count:** 40
- **Primary result:** Achieves SOTA zero-shot accuracy (54.1% on EgoSchema, 71.3% on NExT-QA) using 20x fewer frames than prior methods

## Executive Summary
This paper introduces VideoAgent, a system that leverages a large language model (LLM) as an agent to perform long-form video understanding through iterative frame selection and aggregation. The LLM controls a multi-round process where it first views a few uniformly sampled frames to understand the video context, then determines if additional information is needed to answer questions. If insufficient information is available, the agent retrieves additional frames using CLIP similarity matching, which are then captioned by a vision-language model and fed back to the agent. This iterative process continues until the agent is confident in its answer. Evaluated on EgoSchema and NExT-QA benchmarks, VideoAgent achieves state-of-the-art performance while using only 8.4 and 8.2 frames on average, respectively - a 20x improvement in frame efficiency over prior methods.

## Method Summary
VideoAgent employs GPT-4 as an agent to iteratively select and process frames for long-form video understanding. The system operates through an initial state where N frames are uniformly sampled from the video and captioned by a vision-language model. The LLM then predicts answers to multiple-choice questions and performs self-reflection to assess confidence in its response. If the agent determines that current information is insufficient, it generates text queries to retrieve additional frames containing missing information using CLIP's text-image similarity search. Retrieved frames are captioned and added to the context, and the process repeats until the agent is confident in its answer. The approach is evaluated zero-shot on EgoSchema and NExT-QA datasets, achieving state-of-the-art accuracy while dramatically reducing the number of frames processed compared to prior methods that use all frames.

## Key Results
- Achieves state-of-the-art zero-shot accuracy of 54.1% on EgoSchema dataset
- Achieves state-of-the-art zero-shot accuracy of 71.3% on NExT-QA dataset
- Uses only 8.4 frames on average for EgoSchema and 8.2 frames for NExT-QA
- Demonstrates 20x improvement in frame efficiency compared to prior methods

## Why This Works (Mechanism)
The success of VideoAgent stems from its ability to combine reasoning capabilities of LLMs with efficient visual information retrieval. Instead of processing all frames (which is computationally expensive and often unnecessary), the system uses the LLM's reasoning ability to identify exactly which visual information is needed at each step. The iterative process allows the agent to first establish context with a few frames, then strategically retrieve only the missing pieces of information required to answer specific questions. This targeted approach avoids the noise and redundancy present in processing entire videos while maintaining high accuracy through the LLM's ability to reason about temporal relationships and causal chains across the retrieved frames.

## Foundational Learning
- **Large Language Models as Agents**: LLMs can be prompted to act as reasoning agents that make decisions about information needs and tool usage. *Why needed*: Enables intelligent control over the iterative frame selection process. *Quick check*: Verify the LLM correctly identifies when additional information is needed and formulates appropriate retrieval queries.
- **Vision-Language Model Captioning**: VLMs convert visual frames into textual descriptions that LLMs can process. *Why needed*: Bridges the gap between visual content and LLM reasoning capabilities. *Quick check*: Ensure captions capture the essential visual information without hallucinations.
- **CLIP Text-Image Similarity**: CLIP model computes similarity between text queries and visual frames for retrieval. *Why needed*: Enables targeted retrieval of frames containing specific missing information. *Quick check*: Verify retrieved frames are semantically relevant to the query text.
- **Iterative Self-Reflection**: The agent evaluates its own confidence and determines information sufficiency. *Why needed*: Prevents premature answers and ensures completeness of reasoning. *Quick check*: Confirm the agent correctly identifies knowledge gaps and requests appropriate information.
- **Multi-modal Context Aggregation**: Combining visual captions across multiple rounds into coherent understanding. *Why needed*: Enables reasoning over temporally distributed information. *Quick check*: Verify the agent maintains consistent understanding across retrieval iterations.
- **Zero-shot Learning Setup**: Evaluating without model fine-tuning on target datasets. *Why needed*: Demonstrates generalization capability of the approach. *Quick check*: Confirm no dataset-specific training occurs during evaluation.

## Architecture Onboarding

**Component Map:** Video frames -> VLM captioning -> LLM agent -> CLIP retrieval -> VLM captioning -> LLM agent -> Answer prediction

**Critical Path:** Video frame sampling → VLM captioning → LLM reasoning → Self-reflection → CLIP retrieval (if needed) → VLM captioning → LLM reasoning → Answer prediction

**Design Tradeoffs:**
- Frame sampling rate vs. information completeness: Uniform sampling provides broad coverage but may miss critical moments
- Captioning model quality vs. computational cost: Higher quality VLMs improve reasoning but increase inference time
- Retrieval granularity vs. efficiency: More specific queries improve relevance but require more LLM processing
- Iteration limit vs. answer quality: More iterations improve accuracy but increase latency

**Failure Signatures:**
- LLM fails to parse JSON output correctly → Check prompt formatting instructions
- CLIP retrieves irrelevant frames → Verify query generation produces specific, descriptive text
- VLM captions miss critical information → Test with different captioning models or adjust caption length
- Agent gets stuck in infinite loop → Implement maximum iteration limit and confidence threshold tuning

**3 First Experiments to Run:**
1. Test the end-to-end pipeline on a single short video with known answer to verify all components work together
2. Evaluate the self-reflection accuracy by comparing agent confidence predictions with actual correctness
3. Benchmark CLIP retrieval relevance by manually inspecting top-k retrieved frames for sample queries

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on GPT-4, which is not publicly accessible and may exhibit different behaviors across versions
- Limited exploration of the trade-off between frame efficiency and accuracy at different frame counts
- Performance on longer videos (10+ minutes) remains untested, which is critical for real-world applications

## Confidence

**High Confidence:**
- The core methodology of using an LLM agent for iterative frame selection and aggregation is well-defined
- Reported zero-shot accuracies on EgoSchema (54.1%) and NExT-QA (71.3%) are verifiable

**Medium Confidence:**
- Frame efficiency claims (8.4 and 8.2 frames on average) are supported by experimental results
- Generalizability to other video understanding tasks or longer videos is uncertain

**Low Confidence:**
- Impact of specific GPT-4 hyperparameters and exact prompt structure on final performance
- Reproducibility given incomplete specification of prompt engineering details

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Test VideoAgent with different GPT-4 temperature settings (0.1, 0.5, 1.0) and max token limits to assess robustness and impact on accuracy/frame efficiency.

2. **Cross-Dataset Generalization:** Evaluate VideoAgent on additional long-form video understanding datasets (ActivityNet, Charades) to verify performance on diverse video lengths and content types.

3. **Frame Efficiency Trade-off:** Conduct experiments measuring accuracy as a function of number of frames used (4, 8, 16, 32 frames) to understand optimal balance between efficiency and performance.