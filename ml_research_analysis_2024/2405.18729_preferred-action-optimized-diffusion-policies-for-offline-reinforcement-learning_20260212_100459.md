---
ver: rpa2
title: Preferred-Action-Optimized Diffusion Policies for Offline Reinforcement Learning
arxiv_id: '2405.18729'
source_url: https://arxiv.org/abs/2405.18729
tags:
- policy
- diffusion
- pao-dp
- offline
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving offline reinforcement
  learning policies, particularly in environments with sparse rewards. Existing diffusion
  policy approaches rely on weighted regression, which is limited by the quality of
  the collected data and sensitive to Q-value estimation errors.
---

# Preferred-Action-Optimized Diffusion Policies for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.18729
- Source URL: https://arxiv.org/abs/2405.18729
- Authors: Tianle Zhang, Jiayi Guan, Lin Zhao, Yihang Li, Dongjiang Li, Zecui Zeng, Lei Sun, Yue Chen, Xuelong Wei, Lusong Li, Xiaodong He
- Reference count: 40
- One-line primary result: PAO-DP achieves competitive or superior performance across multiple benchmark tasks, with significant improvements in sparse-reward environments.

## Executive Summary
This paper addresses the challenge of improving offline reinforcement learning policies, particularly in environments with sparse rewards. Existing diffusion policy approaches rely on weighted regression, which is limited by the quality of the collected data and sensitive to Q-value estimation errors. The proposed method, PAO-DP, introduces a novel approach that uses a conditional diffusion model to represent the behavior policy and generates preferred actions using a critic function. It then employs an anti-noise preference optimization to improve the policy using these preferred actions, rather than relying on weighted regression. This approach demonstrates superior performance compared to state-of-the-art offline RL methods, especially in sparse-reward tasks like Kitchen and AntMaze.

## Method Summary
PAO-DP combines diffusion models with anti-noise preference optimization to improve offline RL policies. The method uses a conditional diffusion model trained via behavior cloning to represent the behavior policy, which can capture diverse and multimodal action distributions. Preferred actions are generated by sampling from this diffusion model and using a critic function to select high-value actions via importance sampling. The policy is then improved using anti-noise preference optimization, which treats preference labels as potentially flipped with a small probability λ to create a conservative update that tolerates noisy labels. This approach replaces the weighted regression used in previous diffusion policy methods, aiming to improve robustness and performance in sparse-reward environments.

## Key Results
- PAO-DP achieves competitive or superior performance across multiple benchmark tasks, with significant improvements in sparse-reward environments like Kitchen and AntMaze.
- The method outperforms state-of-the-art offline RL methods, particularly in sparse-reward tasks where existing methods struggle due to data quality and Q-value estimation errors.
- Ablation studies show that incorporating a moderate level of noise helps stabilize the training process, with PAO-DP-Max performing well in both Kitchen-complete and AntMaze-large-play environments.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The diffusion model captures diverse action distributions, avoiding the unimodal Gaussian constraint of traditional policies.
- Mechanism: By using a conditional diffusion model trained via behavior cloning, the policy can represent complex, multimodal behavior distributions rather than collapsing to a single Gaussian mode.
- Core assumption: The collected offline dataset contains sufficient diversity to train a diffusion model that generalizes beyond the specific actions present in the data.
- Evidence anchors:
  - [abstract] "more expressive policy models are urgently needed" and "diffusion models...have achieved the latest benchmark in image generation tasks"
  - [section 2.2] "diffusion models...are usually used to model diverse or multimodal distributions"
  - [corpus] "Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning" - weak signal, no direct evidence in corpus
- Break condition: If the offline dataset is too narrow or contains only a small subset of the state-action space, the diffusion model cannot learn meaningful diversity.

### Mechanism 2
- Claim: Preferred-action sampling via importance sampling from the diffusion model + Q-values improves policy quality without manual labeling.
- Mechanism: Actions are sampled from the diffusion model, evaluated with the critic, and re-sampled proportionally to exp(ηQ) to prioritize higher-value actions within the behavior distribution.
- Core assumption: The critic Q-function can accurately rank actions from the behavior distribution, even if absolute Q-values are noisy.
- Evidence anchors:
  - [section 3.2] "Preferred actions are automatically generated by the critic function based on the diffusion model"
  - [section 3.2] "we use an importance sampling technique to sample actions" and "greedy sampling to obtain ˆat"
  - [corpus] "Diffusion Policies with Value-Conditional Optimization" - weak signal, no direct evidence in corpus
- Break condition: If the critic Q-function has high variance or systematic bias, the importance sampling will prioritize poor actions, degrading policy performance.

### Mechanism 3
- Claim: Anti-noise preference optimization stabilizes training by treating preference labels as potentially flipped with probability λ.
- Mechanism: A cross-entropy loss combines the original preference loss with a flipped-preference loss weighted by λ, creating a conservative update that tolerates noisy labels.
- Core assumption: The Q-function estimation errors manifest as random flips in preference labels, not systematic bias, so a probabilistic noise model is appropriate.
- Evidence anchors:
  - [section 3.3] "assume that the preferred action label may be flipped with a small probability λ"
  - [section 3.3] "Lanti(ψ; θ) = (1 − λ)Limp(ψ; θ) + λL−Γimp(ψ; θ)"
  - [section 4.3] "incorporating a moderate level of noise helps stabilize the training process"
- Break condition: If Q-value errors are systematic rather than random, the anti-noise mechanism may under-correct and fail to improve performance.

## Foundational Learning

- Concept: Diffusion probabilistic models (forward and reverse processes)
  - Why needed here: PAO-DP uses diffusion models as the policy representation, requiring understanding of denoising diffusion training and sampling.
  - Quick check question: How does the forward diffusion process gradually add Gaussian noise, and what is the role of the reverse process in generating actions?

- Concept: Offline RL distributional shift and conservative Q-learning
  - Why needed here: The method builds on offline RL foundations, particularly Q-value estimation and policy regularization to prevent OOD actions.
  - Quick check question: Why is weighted regression sensitive to Q-value errors in offline RL, and how does conservative Q-learning attempt to mitigate this?

- Concept: Preference optimization and the Bradley-Terry model
  - Why needed here: PAO-DP replaces weighted regression with preference optimization using a Bradley-Terry model to rank actions based on Q-values.
  - Quick check question: How does the Bradley-Terry model convert Q-value comparisons into a probabilistic preference distribution?

## Architecture Onboarding

- Component map: Behavior cloning diffusion policy (πb_θ) ← trained on dataset D; Critic Q-function (Qϕ) ← trained via IQL; Preferred-action sampler ← samples N actions from πb_θ, selects one via importance sampling; Surrogate optimal policy (πψ) ← trained via anti-noise preference optimization; Anti-noise parameter λ ← controls robustness to noisy preferences.
- Critical path: Dataset D → train πb_θ via diffusion loss Ld → train Qϕ via IQL → generate preferred-action samples {st, at, ˆat, Γ} → compute anti-noise loss Lanti → update πψ via Ld + ξLanti → evaluate policy.
- Design tradeoffs: Using diffusion models increases expressiveness but adds sampling complexity; anti-noise optimization adds robustness but requires tuning λ; importance sampling improves action quality but adds computational overhead.
- Failure signatures: If πψ performance plateaus early, check if Qϕ estimates are noisy or if λ is too high; if training is unstable, verify that the diffusion model is learning meaningful diversity; if preferred actions are low quality, check the importance sampling weights.
- First 3 experiments:
  1. Verify that πb_θ can reconstruct actions from the dataset D via denoising sampling.
  2. Test Qϕ ranking by comparing Q-values for actions sampled from πb_θ vs random actions.
  3. Run PAO-DP with λ=0 (no anti-noise) on a simple task to confirm preference optimization works before adding noise robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PAO-DP change when using trajectory-based preference optimization instead of action-based preference optimization?
- Basis in paper: [inferred] The paper mentions that future research could enhance PAO-DP by incorporating trajectory-based preference optimization to avoid the OOD problem associated with Q-values.
- Why unresolved: The paper does not provide experimental results or comparisons between trajectory-based and action-based preference optimization methods.
- What evidence would resolve it: Experimental results comparing the performance of PAO-DP with trajectory-based preference optimization to the current action-based approach, demonstrating improvements in robustness and generalization.

### Open Question 2
- Question: What is the impact of different noise levels (λ) on the performance of PAO-DP in various environments?
- Basis in paper: [explicit] The paper includes ablation studies showing the effect of different λ values on performance in the Kitchen-complete and AntMaze-large-play