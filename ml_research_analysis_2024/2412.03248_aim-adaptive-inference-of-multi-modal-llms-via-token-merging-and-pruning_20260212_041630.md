---
ver: rpa2
title: 'AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning'
arxiv_id: '2412.03248'
source_url: https://arxiv.org/abs/2412.03248
tags:
- tokens
- token
- visual
- video
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AIM, a training-free method for adaptive
  inference of pre-trained multi-modal large language models (LLMs) that reduces visual
  token redundancy and computational demands. AIM combines iterative token merging
  based on embedding similarity before the LLM and progressive token pruning within
  LLM layers using multi-modal importance scores.
---

# AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning
## Quick Facts
- arXiv ID: 2412.03248
- Source URL: https://arxiv.org/abs/2412.03248
- Reference count: 40
- Achieves up to 7× FLOP reduction while preserving or improving multi-modal LLM performance

## Executive Summary
AIM introduces a training-free method for adaptive inference in multi-modal large language models, targeting the high computational cost of visual tokens. By iteratively merging redundant tokens based on embedding similarity before the LLM and progressively pruning less important tokens within each layer, AIM reduces FLOPs while maintaining or even improving performance on video and image benchmarks. The method is particularly effective on long video tasks, enabling higher frame sampling rates at lower computational cost.

## Method Summary
AIM operates in two stages: token merging and token pruning. Token merging iteratively groups visually similar tokens before the LLM, reducing redundancy without losing semantic information. Token pruning then uses multi-modal importance scores to eliminate less critical tokens within each layer of the LLM. This approach is training-free, leveraging pre-trained models and similarity metrics. AIM demonstrates significant computational savings (up to 7× FLOP reduction) while maintaining or improving accuracy on video and image benchmarks.

## Key Results
- Up to 7× reduction in FLOPs while preserving or improving performance on video and image benchmarks
- +4.6 improvement on MLVU long video benchmark
- Outperforms state-of-the-art training-free methods with lower computational overhead

## Why This Works (Mechanism)
AIM reduces visual token redundancy by merging similar tokens before the LLM and pruning less important tokens within each layer. This preserves semantic information while drastically cutting computation. The method is training-free, leveraging pre-trained models and multi-modal importance scores to adaptively select tokens for processing.

## Foundational Learning
- Token redundancy in multi-modal LLMs: Why needed—redundant visual tokens inflate computation without improving accuracy. Quick check—compare FLOPs before and after merging on a sample video.
- Multi-modal importance scoring: Why needed—identifies which tokens contribute most to the task. Quick check—verify importance scores correlate with task performance.
- Training-free adaptation: Why needed—avoids costly retraining while adapting inference. Quick check—measure performance drop if merging/pruning disabled.

## Architecture Onboarding
**Component map:** Input frames → Token extractor → Iterative token merging → Multi-modal LLM → Token pruning (per layer) → Output
**Critical path:** Token merging and pruning steps are essential for computational savings; removing them eliminates efficiency gains.
**Design tradeoffs:** Merging reduces redundancy but risks losing rare but important details; pruning saves computation but may harm accuracy if important tokens are removed.
**Failure signatures:** Performance drops when merging/pruning is too aggressive; instability on highly dynamic or complex scenes.
**First experiments:**
1. Validate FLOPs reduction and accuracy on MLVU benchmark.
2. Test merging threshold sensitivity on I3D dataset.
3. Assess pruning aggressiveness impact on video frame sampling rate.

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond its demonstrated results and limitations.

## Limitations
- Performance stability across diverse video domains is uncertain.
- Adaptability to longer sequences beyond tested benchmarks is not fully characterized.
- Computational overhead of merging and pruning mechanisms is not fully assessed across hardware configurations.

## Confidence
- FLOP reduction and benchmark performance: High
- Broader applicability and robustness in unseen scenarios: Medium

## Next Checks
1. Test AIM on real-world video streams (e.g., live surveillance or dynamic event capture) to assess robustness and latency in uncontrolled environments.
2. Measure computational overhead on edge devices with varying token densities to confirm efficiency gains hold under constrained hardware.
3. Validate AIM's performance on longer video sequences (>10k tokens) to identify any degradation or instability in merging/pruning accuracy.