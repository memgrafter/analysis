---
ver: rpa2
title: 'Realistic Evaluation of Test-Time Adaptation Algorithms: Unsupervised Hyperparameter
  Selection'
arxiv_id: '2407.14231'
source_url: https://arxiv.org/abs/2407.14231
tags:
- selection
- adaptation
- methods
- accuracy
- strategy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates Test-Time Adaptation (TTA) methods using unsupervised
  hyperparameter selection strategies to provide a more realistic assessment of their
  performance. The authors compare seven TTA methods across five datasets using five
  different unsupervised model selection metrics.
---

# Realistic Evaluation of Test-Time Adaptation Algorithms: Unsupervised Hyperparameter Selection

## Quick Facts
- arXiv ID: 2407.14231
- Source URL: https://arxiv.org/abs/2407.14231
- Authors: Sebastian Cygert; Damian Sójka; Tomasz Trzciński; Bartłomiej Twardowski
- Reference count: 40
- Primary result: No single TTA method consistently outperforms others across different unsupervised hyperparameter selection strategies

## Executive Summary
This paper provides a rigorous evaluation of Test-Time Adaptation (TTA) methods using unsupervised hyperparameter selection strategies. The authors compare seven TTA methods across five datasets using five different unsupervised model selection metrics, revealing that the ranking of methods varies significantly depending on the selection strategy employed. The study identifies forgetting as a major challenge in TTA and demonstrates that methods with some form of supervision (limited annotated test samples or source data access) perform best. The authors emphasize the need for standardized benchmarking practices and provide a comprehensive testbed for future research in this area.

## Method Summary
The study evaluates seven TTA methods (TENT, SAR, EATA, AdaContrast, RMT, MEMO, LAME) on five datasets (CIFAR100-C, ImageNet-C, DomainNet-126, ImageNet-R, ImageNetV2) using batch size 10. For each method-dataset combination, the authors search over learning rates, momentum, and method-specific hyperparameters, running adaptation and computing surrogate metrics for model selection. Five unsupervised selection strategies are compared: entropy (ENT), model consistency (CON), soft neighborhood density (SND), cross-validation accuracy with limited labels (C-ACC), and using test labels (ORACLE). The experiments also examine the impact of temporally correlated class distributions and varying batch sizes on selection strategy performance.

## Key Results
- The ranking of TTA methods varies significantly across different unsupervised hyperparameter selection strategies
- Forgetting is a major issue in TTA, with only reset-based methods being robust to hyperparameter selection
- The margin between viable and suboptimal solutions is small for unsupervised hyperparameter selection strategies
- Methods using some form of supervision (limited annotated samples or source data access) consistently outperform fully unsupervised approaches

## Why This Works (Mechanism)

### Mechanism 1
Different TTA methods optimize different objectives (entropy minimization, consistency regularization, contrastive learning), and each surrogate metric correlates differently with true accuracy depending on the method's optimization landscape. The surrogate metrics provide meaningful signals about model quality despite not having access to true labels.

### Mechanism 2
Online adaptation without regularization leads to catastrophic forgetting of source knowledge, causing performance to degrade over time. Methods that reset to initial state at every step prevent this but at high computational cost, preserving the valuable knowledge contained in the source model.

### Mechanism 3
The optimization landscape for TTA has many local minima with similar surrogate metric values but significantly different true accuracy, making it difficult to distinguish good from bad solutions. The surrogate metrics are not perfectly aligned with true accuracy, creating situations where multiple hyperparameter configurations yield similar metric values but different performance.

## Foundational Learning

- **Concept: Distribution shift and domain adaptation**
  - Why needed here: TTA methods are designed to handle situations where training and test data come from different distributions
  - Quick check question: What is the difference between covariate shift and label shift, and why does TTA primarily address covariate shift?

- **Concept: Unsupervised model selection and validation**
  - Why needed here: The paper focuses on selecting hyperparameters without access to true labels
  - Quick check question: Why can't we use a validation set with true labels in TTA, and what are the key challenges in unsupervised model selection?

- **Concept: Catastrophic forgetting and continual learning**
  - Why needed here: Forgetting is identified as a major issue in TTA
  - Quick check question: What causes catastrophic forgetting in neural networks, and what are the main strategies to prevent it?

## Architecture Onboarding

- **Component map**: TTA methods (TENT, SAR, EATA, AdaContrast, RMT, MEMO, LAME) that adapt a pretrained model using different objectives → hyperparameter selection strategies (S-ACC, C-ACC, ENT, CON, SND, 100-RND, ORACLE) that evaluate adaptation quality without true labels
- **Critical path**: For each TTA method and dataset combination, iterate through hyperparameter configurations, run adaptation, compute surrogate metrics, select best configuration based on metric, evaluate final accuracy
- **Design tradeoffs**: Using batch size 10 instead of 1 for computational feasibility vs. single-image adaptation performance; computational cost of MEMO vs. its robustness to forgetting; access to source data for some methods vs. privacy concerns
- **Failure signatures**: Surrogate metrics plateau while accuracy continues to improve (or vice versa); performance degradation over adaptation sequence; high variance in results across different random seeds
- **First 3 experiments**:
  1. Implement basic TENT method with entropy-based hyperparameter selection and verify it improves over source model on CIFAR100-C
  2. Add consistency-based selection (CON) and compare its performance against entropy-based selection across multiple methods
  3. Implement MEMO method and measure the trade-off between its robustness to forgetting and its computational cost compared to other methods

## Open Questions the Paper Calls Out

- **Open Question 1**: How do different batch sizes impact the effectiveness of unsupervised hyperparameter selection strategies in TTA?
- **Open Question 2**: Can we develop more robust unsupervised model selection metrics that consistently identify optimal hyperparameters across varying adaptation scenarios?
- **Open Question 3**: How does the temporal correlation of class distribution in test data affect the convergence and generalization of TTA methods?
- **Open Question 4**: What is the theoretical relationship between the margin between viable and suboptimal solutions and the effectiveness of unsupervised hyperparameter selection?

## Limitations

- Significant variability in TTA method rankings across different unsupervised hyperparameter selection strategies
- High computational cost of MEMO method raises practical applicability concerns
- Batch size 10 evaluation may not capture single-image adaptation performance
- Weak correlation between surrogate metrics and true accuracy limits reliability of unsupervised model selection

## Confidence

- **High confidence**: Forgetting is a major issue in TTA and only reset-based methods are robust to hyperparameter selection
- **Medium confidence**: No single TTA method consistently outperforms others across different selection strategies
- **Medium confidence**: The margin between viable and suboptimal solutions is small for unsupervised hyperparameter selection
- **Low confidence**: Cross-validation accuracy using limited annotated samples is the most reliable unsupervised selection strategy

## Next Checks

1. Replicate the forgetting analysis with different source architectures and datasets to verify if reset-based adaptation remains the only robust approach
2. Test single-image adaptation performance to determine if batch-based evaluation underestimates method capabilities
3. Investigate whether additional surrogate metrics or ensemble approaches could improve the correlation between model selection and true accuracy