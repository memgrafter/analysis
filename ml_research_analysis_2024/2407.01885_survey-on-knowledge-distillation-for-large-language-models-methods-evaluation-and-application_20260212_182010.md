---
ver: rpa2
title: 'Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation,
  and Application'
arxiv_id: '2407.01885'
source_url: https://arxiv.org/abs/2407.01885
tags:
- distillation
- language
- knowledge
- arxiv
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive survey of knowledge distillation
  methods for large language models (LLMs), addressing the challenge of compressing
  LLMs while maintaining their performance. The authors categorize knowledge distillation
  methods into white-box and black-box approaches, with white-box methods including
  logits-based and hint-based techniques, and black-box methods encompassing In-Context
  Learning, Chain-of-Thought, and Instruction Following.
---

# Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application

## Quick Facts
- arXiv ID: 2407.01885
- Source URL: https://arxiv.org/abs/2407.01885
- Authors: Chuanpeng Yang; Wang Lu; Yao Zhu; Yidong Wang; Qian Chen; Chenlong Gao; Bingjie Yan; Yiqiang Chen
- Reference count: 40
- One-line primary result: Comprehensive survey categorizing knowledge distillation methods for LLMs into white-box and black-box approaches, evaluating effectiveness on robustness benchmarks and discussing applications across multiple domains.

## Executive Summary
This paper presents a thorough survey of knowledge distillation techniques tailored specifically for large language models (LLMs), addressing the challenge of compressing LLMs while maintaining their performance. The authors systematically categorize knowledge distillation methods into white-box approaches (logits-based and hint-based) and black-box approaches (In-Context Learning, Chain-of-Thought, and Instruction Following). The survey evaluates these methods across multiple dimensions including robustness, efficiency, and application-specific performance, providing researchers and practitioners with a comprehensive framework for understanding and implementing knowledge distillation for LLMs.

## Method Summary
The survey synthesizes knowledge distillation methods for LLMs by categorizing them into two main approaches: white-box and black-box. White-box methods include logits-based techniques that align output probability distributions using metrics like Kullback-Leibler divergence, and hint-based techniques that transfer knowledge through intermediate feature representations. Black-box methods encompass API-based approaches including In-Context Learning, Chain-of-Thought, and Instruction Following, which transfer knowledge through teacher model outputs without accessing internal data. The evaluation considers various teacher LLMs (GPT-2, OPT, LLaMA) and student architectures across diverse datasets and benchmarks including GLUE, MMLU, and robustness tests.

## Key Results
- Knowledge distillation methods for LLMs are systematically categorized into white-box (logits-based, hint-based) and black-box (ICL, CoT, Instruction Following) approaches
- White-box methods show varying effectiveness across different LLM architectures, with some methods performing better on specific model sizes and architectures
- Robustness evaluation reveals that knowledge distillation can improve student models' resilience to adversarial attacks and out-of-distribution data
- Applications span multiple domains including healthcare, education, and law, demonstrating the practical value of compressed LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Logits-based knowledge distillation aligns the output probability distributions of student and teacher models.
- Mechanism: The student model is trained to minimize the Kullback-Leibler divergence between its predicted logits and the softened logits of the teacher model. This forces the student to mimic the teacher's decision boundaries.
- Core assumption: The teacher model's logits contain meaningful information about the correct class probabilities that can be transferred to the student.
- Evidence anchors:
  - [abstract] "This paper presents a thorough survey from three aspects: method, evaluation, and application, exploring knowledge distillation techniques tailored specifically for LLMs."
  - [section] "The Kullback-Leibler divergence (KLD) [43] loss can also be replaced with other functions, such as Reverse Kullback–Leibler (RKL) [ 20, 53, 65, 96] distillation, Jenson–Shannon (JS) [ 129] distillation, etc."
  - [corpus] Weak evidence: No direct mention of logits-based KD in corpus neighbors.
- Break condition: If the teacher model's logits are poorly calibrated or if the student model lacks the capacity to represent the teacher's output distribution.

### Mechanism 2
- Claim: Hint-based knowledge distillation transfers knowledge through intermediate feature representations.
- Mechanism: The student model's intermediate layer outputs are aligned with the teacher model's intermediate features using a metric function (e.g., mean squared error). This enables the student to learn not just the final outputs but also the internal representations.
- Core assumption: The intermediate features of the teacher model contain rich information that is beneficial for the student model's learning process.
- Evidence anchors:
  - [section] "This approach involves matching the outputs of the intermediate layers between student and teacher models."
  - [section] "This technique requires students to understand both the results and the processes leading to those results."
  - [corpus] Weak evidence: No direct mention of hint-based KD in corpus neighbors.
- Break condition: If the student model's architecture cannot be aligned with the teacher model's intermediate layers, or if the teacher's intermediate features are too complex for the student to learn.

### Mechanism 3
- Claim: Black-box knowledge distillation transfers knowledge through the teacher model's predictions without access to internal data.
- Mechanism: The student model is trained using the outputs generated by the teacher model, such as logits or explanations, without requiring access to the teacher's internal parameters or features. This is useful when the teacher model is a closed-source API.
- Core assumption: The teacher model's predictions contain sufficient information to guide the student model's learning, even without access to internal representations.
- Evidence anchors:
  - [section] "Black-box KD involves an API-based approach where only the outputs from the teacher model are accessible."
  - [section] "This category typically includes three methods: In-Context Learning, Chain-of-Thought, and Instruction Following."
  - [corpus] Weak evidence: No direct mention of black-box KD in corpus neighbors.
- Break condition: If the teacher model's predictions are not informative enough to guide the student model, or if the student model requires more detailed information than what is provided by the teacher's outputs.

## Foundational Learning

- Concept: Transformer Architecture
  - Why needed here: LLMs are based on the Transformer architecture, and understanding its components (attention, feed-forward networks) is crucial for grasping how knowledge distillation methods work.
  - Quick check question: What are the main components of a Transformer layer, and how do they contribute to the model's ability to process sequential data?

- Concept: Kullback-Leibler Divergence
  - Why needed here: KL divergence is a key metric used in logits-based knowledge distillation to measure the difference between the student and teacher model's output distributions.
  - Quick check question: How is KL divergence calculated, and why is it a suitable measure for comparing probability distributions in knowledge distillation?

- Concept: Chain-of-Thought Reasoning
  - Why needed here: Chain-of-Thought is a black-box knowledge distillation method that involves generating intermediate reasoning steps to improve the student model's ability to solve complex tasks.
  - Quick check question: How does Chain-of-Thought differ from In-Context Learning, and what are the benefits of incorporating intermediate reasoning steps in the distillation process?

## Architecture Onboarding

- Component map: Teacher Model -> Distillation Algorithm -> Student Model -> Evaluation Benchmark
- Critical path:
  1. Select a teacher model and a student model.
  2. Choose a distillation algorithm based on the availability of the teacher model's internal data.
  3. Prepare the evaluation benchmark and datasets.
  4. Train the student model using the chosen distillation algorithm.
  5. Evaluate the student model's performance on the benchmark.

- Design tradeoffs:
  - White-box vs. Black-box: White-box methods offer more control and potentially better performance but require access to the teacher model's internal data. Black-box methods are more flexible but may be less effective.
  - Logits-based vs. Hint-based: Logits-based methods are simpler but may not capture as much information as hint-based methods, which align intermediate features.
  - Model Size: Larger student models may achieve better performance but require more computational resources.

- Failure signatures:
  - Student model underfits: The student model's performance is significantly worse than the teacher model's.
  - Student model overfits: The student model performs well on the training data but poorly on the evaluation benchmark.
  - Distillation algorithm fails: The chosen distillation algorithm does not improve the student model's performance.

- First 3 experiments:
  1. Implement a simple logits-based knowledge distillation method using a pre-trained BERT model as the teacher and a smaller BERT model as the student. Evaluate the student model's performance on the GLUE benchmark.
  2. Implement a hint-based knowledge distillation method using a pre-trained GPT-2 model as the teacher and a smaller GPT-2 model as the student. Align the intermediate features of the teacher and student models and evaluate the student model's performance on a text generation task.
  3. Implement a black-box knowledge distillation method using a pre-trained LLaMA model as the teacher and a smaller LLaMA model as the student. Use Chain-of-Thought to generate intermediate reasoning steps and evaluate the student model's performance on a complex reasoning task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of white-box distillation methods vary across different LLM architectures and sizes?
- Basis in paper: [explicit] The paper evaluates robustness of white-box KD methods across GPT-2, OPT, and LLaMA models of varying sizes.
- Why unresolved: The paper shows variability in effectiveness but does not provide a comprehensive model explaining why certain architectures respond better to specific KD methods.
- What evidence would resolve it: A systematic study mapping specific KD method components to architectural features that influence effectiveness.

### Open Question 2
- Question: What is the optimal trade-off between model compression ratio and preservation of emergent capabilities in LLMs?
- Basis in paper: [inferred] The paper discusses compression while maintaining performance but doesn't explicitly address emergent capabilities.
- Why unresolved: The paper focuses on task-specific performance metrics rather than the preservation of emergent abilities like multi-step reasoning.
- What evidence would resolve it: Comparative studies measuring both task performance and emergent capability retention across different compression ratios.

### Open Question 3
- Question: How can interpretability be integrated into the knowledge distillation process to improve reliability and predictability?
- Basis in paper: [explicit] Section 5.3 discusses the need for interpretability in LLM knowledge distillation.
- Why unresolved: While the paper identifies interpretability as important, it doesn't provide concrete methods for integrating it into the distillation process.
- What evidence would resolve it: Development and validation of interpretability metrics that correlate with distillation effectiveness and model reliability.

## Limitations

- The survey's comprehensiveness is limited by the rapidly evolving nature of knowledge distillation for LLMs, with new methods emerging continuously
- The evaluation section relies on published benchmark results rather than direct experimental validation by the authors
- The application section covers diverse domains but lacks detailed quantitative comparisons across different distillation methods in real-world scenarios

## Confidence

- **High Confidence**: The categorization of knowledge distillation methods into white-box and black-box approaches is well-established in the literature and aligns with standard practices in the field
- **Medium Confidence**: The effectiveness claims for different distillation methods on robustness benchmarks are based on reported results from various studies, though direct comparisons may be affected by differences in experimental setups
- **Low Confidence**: The survey's predictions about future directions in knowledge distillation for LLMs involve significant speculation about technological developments and their adoption patterns

## Next Checks

1. **Replication Study**: Implement and compare at least two different distillation methods (one white-box and one black-box) on a standard benchmark like GLUE to verify the relative performance claims made in the survey

2. **Robustness Evaluation**: Conduct systematic testing of student models produced by different distillation methods on adversarial examples and out-of-distribution data to validate the robustness claims presented in the survey

3. **Application Case Study**: Apply knowledge distillation techniques to a specific real-world domain (e.g., healthcare) and measure the trade-offs between model compression, performance maintenance, and domain-specific requirements