---
ver: rpa2
title: Linear Projections of Teacher Embeddings for Few-Class Distillation
arxiv_id: '2409.20449'
source_url: https://arxiv.org/abs/2409.20449
tags:
- teacher
- distillation
- student
- lelp
- subclass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LELP, a novel method for improving knowledge
  distillation in few-class classification tasks. LELP works by identifying informative
  linear subspaces in a teacher model's embedding space, projecting them into pseudo-subclasses,
  and training a student model to replicate these subclasses using a unified cross-entropy
  loss.
---

# Linear Projections of Teacher Embeddings for Few-Class Distillation

## Quick Facts
- arXiv ID: 2409.20449
- Source URL: https://arxiv.org/abs/2409.20449
- Reference count: 40
- Primary result: Novel LELP method improves knowledge distillation for few-class problems, achieving up to 1.85% accuracy gains over state-of-the-art baselines

## Executive Summary
This paper introduces LELP (Learning Embedding Linear Projections), a method for improving knowledge distillation in few-class classification tasks. LELP works by identifying informative linear subspaces in a teacher model's embedding space, projecting them into pseudo-subclasses, and training a student model to replicate these subclasses using a unified cross-entropy loss. The method is modality-independent and does not require retraining the teacher model. Experiments on large-scale NLP benchmarks demonstrate that LELP consistently outperforms existing state-of-the-art distillation algorithms for binary and few-class problems.

## Method Summary
LELP extracts pseudo-subclasses from teacher embeddings via PCA decomposition, then trains students on these subclasses using unified cross-entropy loss. The three-step process: (1) apply PCA to teacher embeddings to identify informative linear subspaces, (2) generate pseudo-subclasses by projecting embeddings onto these subspaces, and (3) train student models using these subclasses as expanded class targets. The approach is modality-independent and works with existing teacher models without modification.

## Key Results
- LELP outperforms state-of-the-art distillation algorithms on binary and few-class problems
- Achieves up to 1.85% accuracy improvement over best baselines on tested datasets
- Demonstrates consistent performance across both vision and language domains
- Effective for both binary (2-class) and few-class (4-10 class) classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear projections of teacher embeddings capture fine-grained subclass structure that is not visible in final logits.
- Mechanism: Teacher's final-layer representations retain internal structure along specific linear subspaces. PCA identifies these subspaces, and projecting onto them reveals pseudo-subclasses.
- Core assumption: The fine-grained structure within each class cluster is linearly separable in the teacher's embedding space.
- Evidence anchors:
  - [abstract] "identifying informative linear subspaces in the teacher’s embedding space, and splitting them into pseudo-subclasses."
  - [section] "inspired by recent findings about the structure of final-layer representations... aspects of the phenomenon known as Neural Collapse"
  - [corpus] Weak - no direct mention of linear projections or pseudo-subclasses.
- Break condition: If the teacher embeddings are not linearly separable, or if the PCA directions do not capture meaningful subclass structure.

### Mechanism 2
- Claim: Subclass information improves student learning by increasing the effective number of training targets.
- Mechanism: Expanding C classes into S×C subclasses provides more granular supervision signals, especially beneficial when C is small.
- Core assumption: The subclass structure discovered in the teacher embeddings is semantically meaningful and transferable to the student.
- Evidence anchors:
  - [abstract] "the information about the teacher model’s generalization patterns scales directly with the number of classes."
  - [section] "inventing pseudo-subclasses via unsupervised clustering of the teacher model’s embeddings can improve distillation effectiveness"
  - [corpus] Weak - no mention of subclass expansion or its benefits.
- Break condition: If the subclasses are not semantically meaningful or if the student cannot learn to distinguish them.

### Mechanism 3
- Claim: Using a single unified cross-entropy loss over subclasses avoids the hyperparameter tuning issues of multi-term distillation objectives.
- Mechanism: Unlike methods combining logit loss, embedding loss, and other terms with balancing coefficients, LELP uses only one loss term.
- Core assumption: A single cross-entropy loss over subclasses is sufficient to transfer the relevant knowledge from teacher to student.
- Evidence anchors:
  - [abstract] "training a student model to replicate these subclasses using a unified cross-entropy loss."
  - [section] "our algorithm avoids careful balancing of training objectives such as with Embedding Distillation"
  - [corpus] Weak - no mention of unified loss or hyperparameter tuning.
- Break condition: If the single loss term is insufficient to capture all relevant knowledge from the teacher.

## Foundational Learning

- Concept: Principal Component Analysis (PCA)
  - Why needed here: PCA is used to identify the most informative linear subspaces in the teacher's embedding space for each class.
  - Quick check question: How does PCA identify the most important directions in high-dimensional data?

- Concept: Knowledge Distillation
  - Why needed here: LELP is a method for knowledge distillation, transferring knowledge from a teacher model to a student model.
  - Quick check question: What is the difference between logit-based and feature-based knowledge distillation?

- Concept: Neural Collapse
  - Why needed here: Neural Collapse explains the structure of final-layer representations, which motivates the use of linear projections to extract subclass information.
  - Quick check question: What is Neural Collapse and how does it relate to the structure of final-layer representations?

## Architecture Onboarding

- Component map: Teacher model (embedding extractor, final layer) -> PCA module (identifies linear subspaces) -> Subclass splitter (generates subclass probabilities) -> Student model (final layer modified to output S×C classes)
- Critical path: Teacher embedding extraction → PCA → Subclass splitting → Student training with unified cross-entropy loss
- Design tradeoffs: Using linear projections is simple and efficient but may miss non-linear subclass structure. Subclass expansion increases training complexity but provides more granular supervision.
- Failure signatures: Student performance not improving over vanilla KD, PCA directions not capturing meaningful structure, subclass splitting not improving student accuracy.
- First 3 experiments:
  1. Implement PCA on teacher embeddings and visualize the top principal components for a few classes.
  2. Implement subclass splitting and verify that the subclass probabilities are meaningful (e.g., check if subclasses within a class are more similar than subclasses across classes).
  3. Train a student model using the unified cross-entropy loss over subclasses and compare its performance to vanilla KD on a small dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LELP performance scale with the number of classes beyond the "few-class" regime tested in the paper?
- Basis in paper: [explicit] The paper notes that "as the number of classes increases, we anticipate LELP's performance to converge with vanilla knowledge distillation."
- Why unresolved: The authors deliberately limited experiments to few-class scenarios and did not test LELP on datasets with many classes like ImageNet-1k.
- What evidence would resolve it: Systematic experiments comparing LELP to vanilla KD across datasets with increasing class counts (e.g., CIFAR-10, CIFAR-100, ImageNet-1k) would reveal the scaling behavior.

### Open Question 2
- Question: Would more sophisticated unsupervised clustering methods beyond linear projections yield better subclass separation in LELP?
- Basis in paper: [explicit] "there is no reason to assume that the subclasses should be linearly separable in the teacher embedding, and it is likely that more sophisticated unsupervised clustering methods could extract richer information."
- Why unresolved: The paper only tested linear PCA projections and compared them to other simple clustering methods, not more advanced techniques.
- What evidence would resolve it: Testing LELP with non-linear clustering methods (e.g., spectral clustering, deep clustering) and comparing performance against the linear projection approach.

### Open Question 3
- Question: How does LELP perform in cross-modal knowledge distillation scenarios (e.g., vision to language)?
- Basis in paper: [inferred] The paper emphasizes LELP's "modality-independent" nature and ability to "bridge diverse model architectures," suggesting potential for cross-modal applications.
- Why unresolved: All experiments used teacher-student pairs within the same modality (vision-vision or language-language).
- What evidence would resolve it: Experiments distilling from vision models to language models (or vice versa) on tasks that make sense across modalities would demonstrate cross-modal effectiveness.

## Limitations
- The semantic meaningfulness of discovered subclasses is not validated through qualitative analysis
- Computational cost of applying PCA to large embedding matrices may limit practical applicability
- Performance scaling with increasing number of classes beyond few-class regime is untested

## Confidence
- **High confidence**: The empirical results showing LELP outperforming baselines on benchmark datasets, the mathematical formulation of the unified cross-entropy loss, and the general principle that subclass expansion provides more granular supervision
- **Medium confidence**: The claim that linear projections are sufficient to capture meaningful subclass structure, and that a single unified loss is adequate for knowledge transfer
- **Low confidence**: The semantic meaningfulness of the discovered subclasses and the scalability of the approach to very large models/datasets without additional implementation details

## Next Checks
1. **Qualitative subclass analysis**: Extract and manually examine a sample of pseudo-subclasses from a trained teacher on the Amazon Reviews dataset, categorizing them to verify they correspond to semantically distinct sentiment subtypes (e.g., "positive-subclass-1" = enthusiastic reviews, "positive-subclass-2" = cautiously positive reviews)
2. **Ablation on projection dimensionality**: Systematically vary the number of PCA components used for subclass splitting (e.g., using 1, 2, 3, and 5 components) and measure the impact on student performance to determine if the observed improvements are robust to this hyperparameter
3. **Non-linear projection comparison**: Implement a simple non-linear projection method (e.g., using a small MLP on teacher embeddings before subclass splitting) and compare its performance to the linear PCA approach to test whether the linear assumption is justified