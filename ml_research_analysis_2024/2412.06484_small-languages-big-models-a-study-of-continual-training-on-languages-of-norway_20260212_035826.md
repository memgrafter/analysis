---
ver: rpa2
title: 'Small Languages, Big Models: A Study of Continual Training on Languages of
  Norway'
arxiv_id: '2412.06484'
source_url: https://arxiv.org/abs/2412.06484
tags:
- answer
- prompt
- language
- prediction
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of training large language\
  \ models for lower-resource languages like Norwegian and Northern S\xE1mi by proposing\
  \ a novel three-stage continual training approach. The method involves adapting\
  \ a pre-trained English-centric model through tokenizer change, embedding update,\
  \ and full training on a mixed corpus that includes the target languages, related\
  \ Scandinavian languages, and English."
---

# Small Languages, Big Models: A Study of Continual Training on Languages of Norway

## Quick Facts
- arXiv ID: 2412.06484
- Source URL: https://arxiv.org/abs/2412.06484
- Reference count: 40
- Results in a state-of-the-art Norwegian language model with 30% faster inference and strong Northern Sámi capabilities

## Executive Summary
This paper presents a three-stage continual training approach for adapting large language models to lower-resource languages, specifically Norwegian and Northern Sámi. Starting from an English-centric 11.4B parameter model, the researchers developed a method that changes the tokenizer, realigns embeddings, and then performs full training on a mixed corpus. The resulting NorMistral-11B model achieves state-of-the-art performance on Norwegian language tasks while demonstrating strong capabilities in Northern Sámi, with more than 30% faster inference speed compared to the original model on Norwegian inputs.

## Method Summary
The approach involves three stages of continual training: first changing the tokenizer to one optimized for the target language distribution, then updating the embedding weights to align with the new tokenizer, and finally performing full training on a mixed corpus of 250B tokens. The training uses a hybrid masked-causal language modeling objective (90% causal, 10% masked) and includes upsampling strategies to address data scarcity in low-resource languages. The model is built on the Mistral-Nemo-12B architecture with specific modifications to the tokenizer vocabulary size and training procedures.

## Key Results
- Achieved state-of-the-art performance on Norwegian language tasks (Bokmål and Nynorsk)
- Demonstrated strong capabilities in Northern Sámi despite limited training data (40M words)
- Reduced average tokens per word from 1.79 to 1.22 for Bokmål, resulting in >30% faster inference
- Successfully adapted a 11.4B parameter model through three-stage continual training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Changing the tokenizer to one optimized for the target language distribution substantially improves inference efficiency.
- Mechanism: The new tokenizer uses a vocabulary tailored to the combined Norwegian and Sámi text, resulting in shorter sequences (e.g., 30% reduction from 1.79 tokens/word for Bokmål to 1.22 tokens/word) and fewer parameters (vocabulary size 51,200 vs. 131,072), which directly speeds up both training and inference.
- Core assumption: The subword vocabulary learned from the target corpus better captures frequent character n-grams in Norwegian and Sámi, reducing the number of tokens per word.
- Evidence anchors:
  - [abstract] states the new model is "more than 30% faster than the original Mistral model on Norwegian inputs."
  - [section] provides tokenizer statistics showing reduced sequence length and vocabulary size.
  - [corpus] confirms the corpus composition includes Norwegian, Sámi, and related languages, enabling an optimized tokenizer.
- Break condition: If the target corpus distribution is too sparse or unbalanced, the learned tokenizer may overfit to rare subwords, increasing average tokens per word.

### Mechanism 2
- Claim: Three-stage continual pretraining enables efficient knowledge transfer from an English-centric model to low-resource languages.
- Mechanism: Stage 1 adapts the tokenizer for the target language distribution, Stage 2 realigns embedding weights to the new tokens to avoid catastrophic forgetting, and Stage 3 trains the full model, leveraging the pre-existing English knowledge while adapting to Norwegian and Sámi.
- Core assumption: The base model's representations are sufficiently general that they can be transferred to morphologically different languages with proper tokenizer and embedding alignment.
- Evidence anchors:
  - [abstract] describes the three-stage approach and its success in improving performance.
  - [section] explains each stage and cites de Vries and Nissim (2021) for embedding alignment.
  - [corpus] details the corpus composition and upsampling strategy that makes the adaptation feasible.
- Break condition: If the base model's knowledge is too language-specific, the embedding realignment step may fail to prevent catastrophic forgetting, leading to poor adaptation.

### Mechanism 3
- Claim: Hybrid masked-causal training allows the model to serve flexibly as both a generative and an encoder model without significant performance loss.
- Mechanism: By combining 90% causal language modeling with 10% masked next-token prediction, the model learns bidirectional context while maintaining its generative capability, enabling downstream finetuning as either a causal or prefix language model.
- Core assumption: The model can learn from both objectives simultaneously without one dominating the other, preserving its generative nature.
- Evidence anchors:
  - [abstract] mentions the model can "act as a causal generative model as well as a fully-bidirectional embedding model."
  - [section] describes the hybrid objective and its ratio, referencing Charpentier and Samuel (2024) for improved finetuning performance.
  - [corpus] provides the corpus size and diversity that supports learning from both objectives.
- Break condition: If the ratio of causal to masked training is not well-tuned, the model may drift from its original generative objective, harming its performance in standard causal tasks.

## Foundational Learning

- Concept: Subword tokenization and byte-pair encoding (BPE).
  - Why needed here: Efficiently represents words in low-resource languages by splitting them into frequent character sequences, reducing vocabulary size and sequence length.
  - Quick check question: Why does a smaller vocabulary size lead to faster inference?
- Concept: Continual pretraining and catastrophic forgetting.
  - Why needed here: Adapting a pre-trained model to a new language distribution without losing the knowledge gained from the original corpus.
  - Quick check question: What happens to the model's performance if embedding weights are not realigned after changing the tokenizer?
- Concept: Masked language modeling vs. causal language modeling.
  - Why needed here: Understanding the trade-offs between bidirectional context (useful for fine-tuning) and unidirectional generation (useful for text completion).
  - Quick check question: How does the masked next-token prediction objective differ from standard masked language modeling?

## Architecture Onboarding

- Component map: Tokenizer → Embedding Layer → Transformer Blocks (RMSNorm, SwiGLU, Rotary Positional Embeddings, Grouped-Query Attention) → Output Embedding
- Critical path: Tokenizer change → Embedding update → Full training → Evaluation
- Design tradeoffs: Larger vocabulary improves tokenization but increases memory and inference time; hybrid training objectives improve flexibility but may complicate optimization
- Failure signatures: Poor tokenization leads to long sequences and slow inference; misaligned embeddings cause catastrophic forgetting; unbalanced training objectives degrade generative quality
- First 3 experiments:
  1. Train a new tokenizer on the target corpus and measure average tokens per word vs. the original tokenizer
  2. Perform embedding alignment with frozen parameters for 1,000 steps and monitor loss stability
  3. Evaluate the model with both causal and masked objectives on a small validation set to check for objective drift

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the three-stage continual pretraining method perform when applied to languages with different typological characteristics than Norwegian and Northern Sámi?
- Basis in paper: [explicit] The authors state "we believe that our approach could be beneficial for developing large language models for other smaller languages" and tested their method specifically on Norwegian and Northern Sámi
- Why unresolved: The study only evaluated the method on these two specific languages and related Scandinavian languages. There is no evidence about how well this approach would work for languages with different linguistic features (e.g., agglutinative, tonal, or click languages).
- What evidence would resolve it: Systematic experiments applying the three-stage method to diverse language families with varying typological characteristics, comparing performance against baseline approaches.

### Open Question 2
- Question: What is the long-term impact of the upsampling strategy on model performance and potential biases?
- Basis in paper: [explicit] The authors used upsampling to address data scarcity, repeating data up to 16 times based on Muennighoff et al. (2023), but note they did not observe negative effects
- Why unresolved: The paper only examined short-term effects of upsampling. The long-term impact on model behavior, potential memorization, and introduction of biases from repeated exposure to the same data is unknown.
- What evidence would resolve it: Extended training experiments with varying repetition rates, followed by comprehensive bias and fairness analyses, and studies of model behavior over time.

### Open Question 3
- Question: What is the optimal balance between causal and masked language modeling objectives for different downstream tasks?
- Basis in paper: [explicit] The authors used 90% causal and 10% masked objectives but observed no overall performance increase, contrary to Charpentier and Samuel (2024), and suggest further investigation is needed
- Why unresolved: The study used a fixed ratio and found mixed results across tasks. The optimal ratio may vary depending on the specific task type and language characteristics.
- What evidence would resolve it: Systematic experiments varying the causal-to-masked ratio across different task categories, measuring performance trade-offs and identifying optimal ratios for each task type.

### Open Question 4
- Question: How does the three-stage pretraining method compare to other efficient adaptation methods like adapter tuning or vocabulary extension in terms of computational efficiency and final performance?
- Basis in paper: [explicit] The authors excluded certain methods because "they necessarily lead to inefficient inference" but acknowledge this limits comparison
- Why unresolved: The paper chose their method based on inference efficiency but did not directly compare against other methods that also aim to balance efficiency with performance.
- What evidence would resolve it: Head-to-head comparisons of inference speed, memory usage, and task performance across different adaptation methods on the same language pairs and tasks.

## Limitations

- Limited evaluation on Northern Sámi despite claims of "strong capabilities" in this truly low-resource language
- The 30% inference speedup claim lacks direct empirical validation across different hardware configurations
- Three-stage approach requires significant computational resources (250B tokens over 60,000 steps) that may not be feasible for all practitioners
- Corpus composition details beyond target languages are vague, making it difficult to assess data quality and distribution

## Confidence

**High Confidence**: The three-stage continual training methodology is technically sound and well-supported by established literature on catastrophic forgetting and tokenizer optimization.

**Medium Confidence**: The claim of achieving state-of-the-art performance on Norwegian tasks is supported by the methodology but depends on the specific benchmarks used and their comparability to other models.

**Low Confidence**: The assertion of "strong capabilities" in Northern Sámi is based on limited evidence, as the paper provides minimal evaluation data for this truly low-resource language.

## Next Checks

1. **Corpus Composition Audit**: Obtain and analyze the full training corpus composition, including exact proportions of each language, quality filtering criteria, and data sources to verify the claimed 250B token distribution and upsampling strategy effectiveness.

2. **Northern Sámi Performance Validation**: Conduct comprehensive evaluation of the model on Northern Sámi benchmarks, including both generative and classification tasks, to verify the claimed "strong capabilities" in this low-resource language.

3. **Inference Speed Benchmarking**: Measure actual inference latency and throughput across different hardware configurations (GPU vs CPU, various batch sizes) to validate the claimed 30% speedup compared to the original model on Norwegian inputs.