---
ver: rpa2
title: 'Forget NLI, Use a Dictionary: Zero-Shot Topic Classification for Low-Resource
  Languages with Application to Luxembourgish'
arxiv_id: '2404.03912'
source_url: https://arxiv.org/abs/2404.03912
tags:
- language
- dataset
- languages
- datasets
- luxembourgish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using dictionaries instead of NLI datasets for
  zero-shot topic classification in low-resource languages. They construct two new
  datasets based on a Luxembourgish dictionary and fine-tune models on these, showing
  better performance than using NLI datasets, especially with limited training data.
---

# Forget NLI, Use a Dictionary: Zero-Shot Topic Classification for Low-Resource Languages with Application to Luxembourgish

## Quick Facts
- arXiv ID: 2404.03912
- Source URL: https://arxiv.org/abs/2404.03912
- Reference count: 0
- Models trained on dictionary-based data achieved up to 65% accuracy vs 25-50% for NLI-based models

## Executive Summary
This paper proposes using dictionaries instead of NLI datasets for zero-shot topic classification in low-resource languages. The authors construct two new datasets based on a Luxembourgish dictionary and fine-tune models on these, showing better performance than using NLI datasets, especially with limited training data. On two evaluation datasets, models trained on the dictionary-based data achieved accuracy up to 65% compared to 25-50% for NLI-based models. The approach is promising for low-resource languages where dictionaries are available but NLI data is scarce.

## Method Summary
The authors construct two dictionary-based datasets (LETZ-SYN and LETZ-WoT) from a Luxembourgish dictionary containing approximately 10,000 synonyms and 48,000 example sentences. They fine-tune pre-trained language models (mBERT and LuxemBERT) on these datasets using an entailment approach, then evaluate zero-shot topic classification performance on two Luxembourgish datasets (SIB-200 and LuxNews). The method is tested in both high-resource (11,822 training samples) and low-resource (568 training samples) settings, comparing against NLI-based approaches.

## Key Results
- Dictionary-based models achieved up to 65% accuracy vs 25-50% for NLI-based models on evaluation datasets
- In low-resource settings, mBERT trained on dictionary data significantly outperformed NLI-trained models
- LuxemBERT showed competitive performance in high-resource settings but struggled in low-resource scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dictionary-based datasets align better with zero-shot topic classification task semantics than NLI datasets.
- Mechanism: NLI datasets require reasoning about logical relations (entailment, contradiction, neutral) while ZSC requires relevance scoring. Dictionary entries provide direct synonym and translation links to topics, making the relevance judgment more straightforward.
- Core assumption: The model can leverage synonym/translation relationships as direct relevance signals without needing complex logical reasoning.
- Evidence anchors:
  - [abstract] "we propose an alternative solution that leverages dictionaries as a source of data for ZSC"
  - [section 2] "there is a mismatch between the NLI and ZSC tasks"
  - [corpus] Weak - no direct corpus evidence for this mechanism

### Mechanism 2
- Claim: Dictionary-based approach provides data that is more task-relevant and available in low-resource languages.
- Mechanism: Dictionaries are fundamental educational resources often prioritized in low-resource language development, making them more likely to exist than specialized NLI datasets. They provide direct topic-word mappings that can be converted into training examples.
- Core assumption: Dictionaries are more readily available and prioritized in low-resource language contexts than NLI datasets.
- Evidence anchors:
  - [abstract] "it leverages resources that are more readily available in many low-resource languages"
  - [section 7] "dictionaries of-ten receive priority due to their fundamental role in educational and cultural preservation efforts"
  - [corpus] No direct corpus evidence - this is inferred from general knowledge about language resource prioritization

### Mechanism 3
- Claim: Dictionary-based datasets require fewer training samples to achieve good performance compared to NLI datasets.
- Mechanism: The dictionary-based approach provides more direct, relevant signal for topic classification (synonyms/translations are directly about topics) compared to NLI's indirect logical reasoning requirement, allowing models to learn with less data.
- Core assumption: Direct relevance signals from dictionary entries are more efficient for learning topic classification than indirect logical reasoning signals.
- Evidence anchors:
  - [abstract] "models trained on our datasets and NLI datasets" with "accuracy up to 65% compared to 25-50% for NLI-based models"
  - [section 5.2] "we perform experiments in 'high-resource' (11,822 train... samples) and 'low-resource' (568 train... samples) settings"
  - [corpus] No direct corpus evidence for sample efficiency comparison

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: Understanding the limitations of NLI datasets for ZSC task is crucial to justify the dictionary approach
  - Quick check question: What are the three possible relationships in NLI (entailment, contradiction, neutral) and why might these be mismatched with ZSC's relevance scoring?

- Concept: Zero-shot classification
  - Why needed here: The core task being addressed - classifying text into categories without labeled examples
  - Quick check question: How does the entailment approach work for zero-shot classification, and what are its limitations?

- Concept: Low-resource languages
  - Why needed here: Understanding the specific challenges and resource constraints that motivate this approach
  - Quick check question: Why are NLI datasets particularly difficult to create for low-resource languages, and what alternative resources might be more available?

## Architecture Onboarding

- Component map: Dictionary extraction -> Dataset construction (synonym-based and translation-based) -> Model fine-tuning (mBERT, LuxemBERT) -> Evaluation (entailment approach)
- Critical path: Dictionary data extraction → Dataset construction → Model fine-tuning → Evaluation on topic classification tasks
- Design tradeoffs:
  - Dictionary-based vs NLI-based: Better task alignment vs established methodology
  - Synonym-based vs translation-based: Language-specific relevance vs cross-lingual applicability
  - Limited dataset size vs comprehensive coverage
- Failure signatures:
  - Poor performance on both dictionary-based and NLI-based approaches: Suggests fundamental model limitations
  - Dictionary-based approach outperforms but still low accuracy: Suggests dictionary data quality or relevance issues
  - Large variance in results: Suggests instability in training or evaluation
- First 3 experiments:
  1. Replicate the basic comparison: Fine-tune mBERT on LETZ-SYN vs NLI-lb in low-resource setting and compare accuracy on SIB-200
  2. Test translation-based dataset: Fine-tune mBERT on LETZ-WoT vs NLI-lb and compare performance
  3. High-resource setting: Fine-tune on full LETZ-SYN (11,822 samples) vs NLI-fr and compare results on both evaluation datasets

## Open Questions the Paper Calls Out
- How well does the dictionary-based approach generalize to other low-resource languages beyond Luxembourgish?
- What is the minimum size and quality requirements for a dictionary to be effective for this approach?
- How does the approach handle polysemous words and context-dependent meanings in dictionaries?

## Limitations
- The approach only tested on Luxembourgish, limiting generalizability to other low-resource languages
- Limited error analysis makes it difficult to understand when and why the dictionary-based approach might fail
- Dictionary-based datasets are constructed from a single source, potentially limiting diversity

## Confidence
- **High Confidence**: The empirical results showing dictionary-based approaches outperforming NLI datasets on the two Luxembourgish evaluation datasets
- **Medium Confidence**: The mechanism explaining why dictionary-based approaches work better (task alignment and availability in low-resource contexts)
- **Low Confidence**: The generalizability of the approach to other low-resource languages beyond Luxembourgish

## Next Checks
1. Apply the dictionary-based approach to another low-resource language (e.g., Basque, Georgian, or Maltese) and compare performance against NLI-based approaches
2. Conduct detailed error analysis on the dictionary-based approach to identify specific failure modes and limitations
3. Experiment with dictionaries of varying quality, coverage, and structure to determine how dictionary characteristics affect performance