---
ver: rpa2
title: Multimodal Misinformation Detection by Learning from Synthetic Data with Multimodal
  LLMs
arxiv_id: '2409.19656'
source_url: https://arxiv.org/abs/2409.19656
tags:
- data
- instances
- synthetic
- multimodal
- real-world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting multimodal misinformation
  (MMD) by learning from synthetic data, given the scarcity of real-world fact-checking
  datasets. The authors propose a data selection approach that leverages semantic
  and distributional similarity metrics to identify relevant synthetic instances for
  fine-tuning multimodal large language models (MLLMs).
---

# Multimodal Misinformation Detection by Learning from Synthetic Data with Multimodal LLMs

## Quick Facts
- **arXiv ID**: 2409.19656
- **Source URL**: https://arxiv.org/abs/2409.19656
- **Reference count**: 40
- **Primary result**: Data selection method improves MLLM performance on real-world fact-checking datasets, with 13B model surpassing GPT-4V

## Executive Summary
This paper addresses the critical challenge of multimodal misinformation detection (MMD) in the face of scarce real-world fact-checking data. The authors propose a novel approach that leverages synthetic data generation combined with intelligent data selection to fine-tune multimodal large language models (MLLMs). By using semantic and distributional similarity metrics to identify the most relevant synthetic instances, their method effectively bridges the gap between synthetic and real-world data distributions. The results demonstrate significant performance improvements, with even relatively small MLLMs achieving state-of-the-art results that surpass GPT-4V on benchmark datasets.

## Method Summary
The proposed approach employs a data selection strategy that uses a mixed-modal encoder (based on CLIP) to extract features from both synthetic and real-world data. Two similarity metrics are computed: semantic similarity (SemSim) based on cosine distance of extracted features, and distributional similarity (DisSim) based on matching data distributions. These metrics are used to select the most relevant synthetic instances for fine-tuning MLLMs on real-world fact-checking tasks. The method is model-agnostic, allowing it to be applied to various MLLM architectures, and focuses on optimizing the selection process rather than generating new synthetic data.

## Key Results
- Data selection approach significantly improves MLLM performance on real-world fact-checking datasets
- A 13B parameter MLLM trained with the proposed method outperforms GPT-4V
- Both semantic and distributional similarity metrics contribute to performance improvements
- The method effectively bridges the gap between synthetic and real-world data distributions

## Why This Works (Mechanism)
The approach works by intelligently selecting synthetic data that closely matches the distribution and semantic characteristics of real-world misinformation. By computing similarity metrics between synthetic and real data using a CLIP-based encoder, the method identifies instances that provide the most relevant training signals. This targeted selection process ensures that MLLMs learn from synthetic data that best represents real-world scenarios, overcoming the limitations of using uncurated synthetic data which may contain irrelevant or misleading examples.

## Foundational Learning
- **Multimodal Large Language Models (MLLMs)**: AI systems that process and generate both text and image content. Needed to understand the core technology being fine-tuned. Quick check: Can process and reason about both modalities simultaneously.
- **Synthetic Data Generation**: Creating artificial training data through templates or other methods. Needed to understand how the initial training data is produced. Quick check: Generated data follows realistic patterns but lacks real-world complexity.
- **CLIP-based Feature Extraction**: Using Contrastive Language-Image Pre-training models to extract multimodal features. Needed to understand how similarity metrics are computed. Quick check: Produces fixed-length embeddings that capture semantic content across modalities.
- **Data Selection via Similarity Metrics**: Identifying relevant training instances based on semantic and distributional similarity. Needed to understand the core innovation. Quick check: Effectively filters synthetic data to match real-world distribution characteristics.
- **Fact-checking Dataset Characteristics**: Understanding the structure and challenges of real-world misinformation datasets. Needed to appreciate the performance evaluation context. Quick check: Contains limited samples but represents authentic misinformation patterns.

## Architecture Onboarding

**Component Map**: Real-world datasets -> CLIP encoder -> Synthetic data generation -> Similarity computation (SemSim + DisSim) -> Selected synthetic instances -> MLLM fine-tuning -> Evaluation

**Critical Path**: CLIP encoder → Similarity computation → Data selection → MLLM fine-tuning

**Design Tradeoffs**: 
- Template-based synthetic data generation is efficient but may miss complex real-world patterns
- Model-agnostic selection allows flexibility but may not optimize for specific MLLM architectures
- CLIP-based features are readily available but introduce potential bias without fine-tuning

**Failure Signatures**:
- Poor performance when real-world misinformation significantly deviates from synthetic data distribution
- Overfitting to synthetic data patterns that don't generalize to real examples
- Computational bottlenecks during similarity computation for large synthetic datasets

**3 First Experiments**:
1. Evaluate baseline MLLM performance on fact-checking datasets without any synthetic data
2. Test data selection with only semantic similarity (SemSim) to isolate its contribution
3. Test data selection with only distributional similarity (DisSim) to isolate its contribution

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the optimal number of selected synthetic instances that yields the best performance for multimodal misinformation detection?
- Basis in paper: [inferred] The paper discusses that increasing the number of selected synthetic instances does not necessarily improve performance and suggests there might be an optimal number.
- Why unresolved: The paper does not provide a definitive answer or methodology to determine the optimal number of instances, indicating this as an area for future research.
- What evidence would resolve it: Conducting experiments with varying numbers of synthetic instances and analyzing the performance to identify a point of diminishing returns or optimal performance.

### Open Question 2
- Question: How can a unified data selection method be developed to handle different data distributions effectively for multimodal misinformation detection?
- Basis in paper: [inferred] The paper notes that different similarity-based methods (SemSim and DisSim) perform better on different datasets, suggesting the need for a unified approach.
- Why unresolved: The paper highlights the effectiveness of both methods but does not propose a unified solution, indicating the complexity of handling diverse data distributions.
- What evidence would resolve it: Developing and testing a hybrid or adaptive data selection method that combines or dynamically chooses between different similarity metrics based on the dataset characteristics.

### Open Question 3
- Question: How can better multimodal features be obtained to improve misinformation detection performance beyond the current use of CLIP models?
- Basis in paper: [explicit] The paper mentions that relying on the CLIP model without fine-tuning introduces potential bias and suggests the need for further exploration on obtaining better multimodal features.
- Why unresolved: The paper acknowledges the limitations of the current approach and the potential for improvement but does not provide a solution or alternative method for feature extraction.
- What evidence would resolve it: Experimenting with alternative feature extraction methods or fine-tuning the CLIP model to reduce bias and enhance feature quality, followed by evaluating the impact on detection performance.

## Limitations
- Template-based synthetic data generation may not capture the full diversity and complexity of real-world misinformation patterns
- Results are validated on limited real-world datasets, constraining generalizability claims
- The model-agnostic approach may not fully optimize for specific MLLM architectures

## Confidence
- **Method soundness**: Medium
- **Statistical significance**: Medium
- **Generalizability**: Low-Medium

## Next Checks
1. Test the approach on a broader range of real-world misinformation datasets, particularly those from different domains and cultural contexts
2. Evaluate performance degradation when exposed to adversarial examples designed to exploit gaps between synthetic and real data distributions
3. Conduct ablation studies to quantify the individual contributions of semantic versus distributional similarity metrics to overall performance improvements