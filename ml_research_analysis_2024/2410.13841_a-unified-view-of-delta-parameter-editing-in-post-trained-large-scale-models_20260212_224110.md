---
ver: rpa2
title: A Unified View of Delta Parameter Editing in Post-Trained Large-Scale Models
arxiv_id: '2410.13841'
source_url: https://arxiv.org/abs/2410.13841
tags:
- performance
- delta
- parameters
- drop
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified view of delta parameter editing in
  post-trained large-scale models using Riemann sum approximation. It categorizes
  existing methods into competitive, decreased, and improved performance based on
  their impact on model loss.
---

# A Unified View of Delta Parameter Editing in Post-Trained Large-Scale Models

## Quick Facts
- arXiv ID: 2410.13841
- Source URL: https://arxiv.org/abs/2410.13841
- Authors: Qiaoyu Tang; Le Yu; Bowen Yu; Hongyu Lin; Keming Lu; Yaojie Lu; Xianpei Han; Le Sun
- Reference count: 40
- One-line primary result: Riemann sum approximation categorizes delta parameter editing methods into competitive, decreased, and improved performance based on their impact on model loss.

## Executive Summary
This paper presents a unified theoretical framework for analyzing delta parameter editing in post-trained large-scale models using Riemann sum approximation. The authors demonstrate that existing methods can be categorized based on how they affect the Riemann sum approximation term, which predicts the impact on model loss. Methods like DARE achieve competitive performance by zeroing the approximation term, while BitDelta, Twin-Merging, and TIES-Merging incur decreased performance due to positive approximation terms. The framework also explains how EXPO improves performance by producing negative loss changes. Extensive experiments on ViT, LLaMA 3, Qwen 2, and Mistral validate these theoretical findings.

## Method Summary
The paper proposes using Riemann sum approximation to analyze delta parameter editing operations in post-trained models. The approach computes the loss difference between edited and original models by approximating the integral of the gradient of the loss function over the interval [0,1], using delta parameters as the function being integrated. This framework is applied to various existing methods including DARE (random drop and rescale), BitDelta (low-bit quantization), Twin-Merging and TIES-Merging (low-rank approximation), and EXPO (extrapolation). The approximation term determines whether a method achieves competitive, decreased, or improved performance. The authors validate their theoretical analysis through experiments on multiple model architectures and datasets.

## Key Results
- DARE achieves competitive performance by ensuring the Riemann sum approximation term equals zero through random drop and rescale operations
- BitDelta, Twin-Merging, and TIES-Merging incur decreased performance due to positive approximation terms
- EXPO improves performance by producing negative loss changes through extrapolation
- The framework accurately predicts method performance across ViT, LLaMA 3, Qwen 2, and Mistral models
- Approximation term magnitude correlates with the extent of performance degradation or improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Riemann sum approximation can predict the impact of delta parameter editing on model performance by measuring the loss difference between edited and original models.
- Mechanism: The loss change from delta parameter editing is approximated using a Riemann sum over subdivisions of the interval [0,1], where the gradient of the loss function is computed at intermediate points and combined with the delta parameters.
- Core assumption: The loss function is smooth enough in the neighborhood of WPOST that a Riemann sum provides a good approximation, and the approximation term accurately reflects the impact on model performance.
- Evidence anchors:
  - [abstract] "We formulate the editing operations of delta parameters based on Riemann sum approximation of the loss difference."
  - [section 3.2] "We represent the changes caused by existing editing methods by ∆fW and aim to investigate their effects on performance via analyzing the loss difference."
  - [corpus] Weak - corpus papers focus on delta parameter methods but don't specifically discuss Riemann sum approximation as a predictive tool.
- Break condition: The approximation fails when the loss function is highly non-linear in the region of interest, or when the gradient changes rapidly, making the Riemann sum a poor estimator of the true loss difference.

### Mechanism 2
- Claim: DARE achieves competitive performance by ensuring the Riemann sum approximation term equals zero through random drop and rescale operations.
- Mechanism: By randomly dropping delta parameters with probability p and rescaling the remaining ones by 1/(1-p), DARE creates a situation where the expected value of the approximation term is zero, leading to equal loss between the edited and original models.
- Core assumption: The randomness of the drop operation ensures that the expected value of the product of delta parameters and gradients cancels out across the large number of parameters in the model.
- Evidence anchors:
  - [section 4.1] "Based on Equation (5), we derive ∆LDARE ≈ 0" showing that the expected value of the approximation term is zero.
  - [section 4.1] "We can conclude that after editing delta parameters with DARE, the loss L(WDARE) is approximately equal to L(WPOST)" validating the mechanism.
  - [corpus] Weak - corpus papers discuss DARE but don't analyze it through the lens of Riemann sum approximation.
- Break condition: The mechanism breaks when the drop operation is not truly random (e.g., biased by magnitude), or when the number of parameters is too small for the Law of Large Numbers to apply effectively.

### Mechanism 3
- Claim: EXPO improves model performance by extrapolating delta parameters in a direction that reduces loss on alignment data, as indicated by a negative Riemann sum approximation term.
- Mechanism: By scaling delta parameters by a factor α > 1, EXPO moves the model parameters further in the direction that reduces loss on alignment data, as the approximation term becomes negative when the gradient and delta parameters align.
- Core assumption: The DPO/RLHF training process is suboptimal, leaving room for further improvement by moving in the direction of the delta parameters, and the optimal scaling factor α depends on the specific alignment data.
- Evidence anchors:
  - [section 6.1] "We derive ∆LEXPO ≈ α · (1/C) · Σ(∆Wij · ∇Lc_ij)" showing how the approximation term scales with α.
  - [section 6.1] "the loss of the edited model on alignment dataset is lower than that of the original post-training model, resulting in enhanced performance" validating the mechanism.
  - [section 6.2] "on most datasets, interpolation outperformed extrapolation" showing that the optimal direction depends on the data.
- Break condition: The mechanism breaks when the model is already well-aligned (no further improvement possible), or when the gradient and delta parameters are not aligned, making further extrapolation detrimental.

## Foundational Learning

- Concept: Riemann sum approximation for estimating function integrals
  - Why needed here: Used to approximate the loss difference caused by delta parameter editing, providing a way to predict the impact on model performance.
  - Quick check question: What is the main advantage of using Riemann sum approximation over other numerical integration methods in this context?

- Concept: Law of Large Numbers
  - Why needed here: Applied to justify replacing summations over individual parameters with their expected values in the derivation of DARE's competitive performance.
  - Quick check question: Why is the Law of Large Numbers particularly applicable to the analysis of delta parameter editing in large-scale models?

- Concept: Random drop and rescale operations
  - Why needed here: Central to DARE's mechanism for achieving competitive performance by ensuring the Riemann sum approximation term equals zero.
  - Quick check question: How does the random drop and rescale operation in DARE create a situation where the expected value of the approximation term is zero?

## Architecture Onboarding

- Component map:
  - Delta parameters (∆W) -> Riemann sum approximation -> Editing operations -> Loss function (L) -> Performance metrics

- Critical path:
  1. Compute delta parameters from pre-trained and post-trained models.
  2. Apply an editing operation to the delta parameters.
  3. Calculate the Riemann sum approximation term to predict the impact on model performance.
  4. Validate the prediction through experiments on representative datasets.

- Design tradeoffs:
  - Accuracy vs. efficiency: More subdivisions in the Riemann sum (larger C) provide better approximation but increase computation cost.
  - Randomness vs. bias: DARE's random drop ensures zero approximation term on average, but may not be optimal for specific datasets.
  - Interpolation vs. extrapolation: EXPO shows that whether to interpolate or extrapolate depends on the alignment data, requiring careful tuning.

- Failure signatures:
  - High approximation term: Indicates the editing operation will likely degrade model performance.
  - Negative approximation term: Suggests the edited model may outperform the original, but the extent depends on the magnitude.
  - Large variance in approximation term across different subdivisions: Implies the loss function is highly non-linear in the region of interest, making the Riemann sum a poor predictor.

- First 3 experiments:
  1. Validate the Riemann sum approximation by comparing its prediction with actual performance changes on a small model and dataset.
  2. Test DARE with different drop rates (p) and rescale factors (k) to find the optimal configuration for competitive performance.
  3. Experiment with interpolation and extrapolation in EXPO to determine the best scaling factor (α) for improving model alignment on a specific dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper limit on the number of bits that can be used in BitDelta before performance degrades?
- Basis in paper: [inferred] The paper shows that increasing the number of bits in BitDelta improves performance, with 4 bits surpassing the original post-trained model, but doesn't explore the full range of possible bits.
- Why unresolved: The paper only tests up to a certain number of bits and doesn't provide a theoretical analysis of the maximum effective bit count.
- What evidence would resolve it: Experiments testing BitDelta with progressively higher numbers of bits until performance begins to degrade, combined with a theoretical analysis of the trade-off between quantization granularity and model capacity.

### Open Question 2
- Question: How does the choice of holistic statistic in BitDelta affect the model's performance on different types of tasks?
- Basis in paper: [explicit] The paper shows that using the average magnitude of delta parameters yields optimal performance, but also demonstrates that other statistics (normal and uniform distributions) can achieve comparable results.
- Why unresolved: The paper only tests a few holistic statistics and doesn't explore the relationship between statistic choice and task type.
- What evidence would resolve it: Experiments testing BitDelta with various holistic statistics across a diverse set of tasks, analyzing the correlation between statistic choice and task characteristics.

### Open Question 3
- Question: Can the Riemann sum approximation framework be extended to other parameter editing operations beyond those discussed in the paper?
- Basis in paper: [explicit] The paper applies the Riemann sum approximation to DARE, BitDelta, Twin-Merging, TIES-Merging, and EXPO, but doesn't explore other potential operations.
- Why unresolved: The paper focuses on existing methods and doesn't investigate the applicability of the framework to novel editing techniques.
- What evidence would resolve it: Applying the Riemann sum approximation to a range of new parameter editing operations, demonstrating its effectiveness in predicting their impact on model performance.

## Limitations

- The accuracy of the Riemann sum approximation depends on the smoothness of the loss function in the neighborhood of WPOST
- The assumption of parameter independence when computing expected values for DARE's competitive performance is an approximation that may not hold exactly in practice
- The framework primarily addresses delta parameter editing for post-training scenarios and may not generalize directly to other fine-tuning contexts

## Confidence

- High confidence in the mathematical formulation of the Riemann sum approximation and its application to delta parameter editing
- Medium confidence in the empirical validation across the tested models and datasets, as the paper provides results but limited details on hyperparameter sensitivity
- Medium confidence in the mechanism explanations, particularly for why certain methods incur decreased performance, as the analysis relies on theoretical approximations

## Next Checks

1. **Ablation on subdivision count C**: Systematically vary the number of subdivisions in the Riemann sum approximation to determine how approximation accuracy affects performance predictions, particularly for methods categorized as decreased performance.

2. **Cross-dataset validation**: Test whether methods classified as competitive, decreased, or improved performance based on approximation terms maintain these classifications across different alignment datasets and model architectures.

3. **Perturbation sensitivity analysis**: Apply small random perturbations to delta parameters and measure whether the actual loss changes align with the Riemann sum approximation predictions, providing empirical validation of the approximation's reliability.