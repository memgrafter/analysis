---
ver: rpa2
title: 'CNNSum: Exploring Long-Context Summarization with Large Language Models in
  Chinese Novels'
arxiv_id: '2412.02819'
source_url: https://arxiv.org/abs/2412.02819
tags:
- arxiv
- context
- llms
- preprint
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CNNSum, a multi-scale Chinese long-context
  novel summarization benchmark designed to address the scarcity of high-quality long-context
  summarization datasets. CNNSum includes four subsets with lengths ranging from 16k
  to 128k tokens, totaling 695 samples, with human-driven annotations.
---

# CNNSum: Exploring Long-Context Summarization with Large Language Models in Chinese Novels

## Quick Facts
- arXiv ID: 2412.02819
- Source URL: https://arxiv.org/abs/2412.02819
- Reference count: 40
- Authors: Lingxiao Wei; He Yan; Xiangju Lu; Junmin Zhu; Jun Wang; Wei Zhang
- Primary result: Introduces CNNSum benchmark and demonstrates that short-context fine-tuning significantly improves long-context summarization performance

## Executive Summary
CNNSum addresses the scarcity of high-quality long-context summarization datasets by introducing a multi-scale Chinese novel summarization benchmark. The benchmark includes four subsets with context lengths ranging from 16k to 128k tokens, totaling 695 human-annotated samples from Jin Yong's martial arts novels. The authors evaluate numerous commercial and open-source LLMs on CNNSum, finding that advanced models often generate subjective commentary instead of summaries, while small models with stable longer context lengths prove more cost-effective. The study demonstrates that fine-tuning LLMs using short-context summary data significantly improves long-context summarization performance, providing more reliable evaluation results than existing benchmarks.

## Method Summary
The CNNSum benchmark construction involves collecting a Chinese novel corpus (~100 books), implementing a multi-scale sampling strategy based on chapter structure, and generating human-driven annotations assisted by LLMs. The process uses Yi tokenizer for tokenization and creates four subsets (L: 16k, XL: 32k, 2XL: 64k, 3XL: 128k) through sliding window approaches. Annotations are created by generating chapter synopses with commercial LLMs and having human annotators refine them into final summaries. The evaluation involves testing various commercial and open-source LLMs with different prompt templates, followed by fine-tuning experiments using concatenated short-context data to improve long-context summarization performance.

## Key Results
- Advanced LLMs often generate subjective commentary instead of objective summaries, leading to vague outputs
- Small LLMs with stable longer context lengths are more cost-effective for long-context summarization tasks
- Fine-tuning using short-context summary data significantly improves long-context summarization performance, with Yi-6B showing 2-3x improvement on L and XL subsets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using short-context summary data for fine-tuning significantly improves long-context summarization performance by activating extrapolation potential.
- Mechanism: Models pre-trained with RoPE base scaling retain long-context potential that can be unlocked through fine-tuning on longer concatenated data.
- Core assumption: RoPE base scaling methods endow models with extrapolation capability that remains dormant until activated by appropriate fine-tuning data.
- Evidence anchors:
  - [abstract]: "using short-context data can significantly improve long-context summarization performance"
  - [section]: "The performance of Yi-6B on L and XL significantly improved by 2 to 3 times"
  - [corpus]: Weak evidence - corpus doesn't directly support this mechanism
- Break condition: If concatenated short data interferes with model learning or if extrapolation potential is exhausted

### Mechanism 2
- Claim: Different prompt templates (IB vs IE) cause significant performance gaps in long-context summarization.
- Mechanism: The relative position of context and instruction affects the model's ability to follow instructions after processing long contexts.
- Core assumption: Models struggle to maintain instruction awareness when processing long contexts, making instruction placement critical.
- Evidence anchors:
  - [abstract]: "Prompt templates may cause a large performance gap but can be mitigated through fine-tuning"
  - [section]: "We defined prompt types: Instruction at the Beginning (Prompt-IB) and Instruction at the End (Prompt-IE)"
  - [corpus]: Weak evidence - corpus doesn't directly support this mechanism
- Break condition: If fine-tuning eliminates the performance gap between prompt types

### Mechanism 3
- Claim: Base models are better suited for fine-tuning and extrapolation than instruction-tuned versions.
- Mechanism: Fine-tuning preserves base capabilities while enhancing specific tasks, whereas instruction-tuning may damage these capabilities.
- Core assumption: Instruction-tuning optimizes for specific response patterns that may not generalize well to novel tasks like long-context summarization.
- Evidence anchors:
  - [abstract]: "Fine-tuned Chat or Instruction versions may harm the Base model and further fine-tuning cannot bridge performance gap"
  - [section]: "Within each model series, the best results for each subset were almost always achieved by base models"
  - [corpus]: Weak evidence - corpus doesn't directly support this mechanism
- Break condition: If instruction-tuned models consistently outperform base models on long-context tasks

## Foundational Learning

- Concept: RoPE (Rotary Position Embedding) and its scaling methods
  - Why needed here: Understanding how positional encoding affects context length and extrapolation is crucial for interpreting the paper's findings about model performance
  - Quick check question: How does adjusting RoPE base frequency enable longer context processing?

- Concept: Context length and tokenization efficiency
  - Why needed here: The paper discusses Chinese encoding efficiency and its impact on model performance, making it essential to understand how context length is measured
  - Quick check question: Why does lower Chinese encoding efficiency lead to more tokens per character and potentially worse performance?

- Concept: ROUGE metrics for summarization evaluation
  - Why needed here: The paper uses ROUGE-L to evaluate summary quality, so understanding what this metric measures is important for interpreting results
  - Quick check question: What does ROUGE-L measure in summarization tasks, and what are its limitations?

## Architecture Onboarding

- Component map: Data collection (Chinese novels) → Preprocessing (tokenization and multi-scale sampling) → Annotation (human-driven with LLM assistance) → Model evaluation (commercial and open-source LLMs) → Analysis → Fine-tuning experiments (using concatenated short-context data) → Further analysis
- Critical path: Data collection → Preprocessing and sampling → Annotation → Model evaluation → Analysis → Fine-tuning experiments → Further analysis
- Design tradeoffs: The choice between using short-context data for fine-tuning versus directly training on long-context data involves tradeoffs between cost, data availability, and model performance
- Failure signatures: Models generating subjective commentary instead of summaries, meaningless repetition in outputs, or failure to follow instructions indicate issues with long-context understanding
- First 3 experiments:
  1. Evaluate baseline models on CNNSum with different prompt templates (IB vs IE)
  2. Fine-tune Yi-6B using concatenated short-context data with average length 16k
  3. Compare fine-tuned model performance on CNNSum versus CLongEval-LStSum to assess evaluation reliability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal way to combine short-context summarization data with long-context data to maximize performance gains in long-context summarization tasks?
- Basis in paper: [explicit] The paper discusses the use of short-context summary data to improve long-context summarization performance through fine-tuning, but does not provide a detailed analysis of the optimal combination strategy.
- Why unresolved: The paper mentions the strategy of concatenating short-context summaries into longer sequences for training, but does not explore variations in this approach, such as different concatenation methods, the number of short summaries to combine, or the impact of varying the ratio of short to long data.
- What evidence would resolve it: A systematic study comparing different concatenation strategies, the impact of varying the number of short summaries, and the optimal ratio of short to long data on the performance of long-context summarization tasks would provide insights into the optimal combination method.

### Open Question 2
- Question: How do different RoPE scaling methods (e.g., NTK, PI, YaRN) affect the extrapolation performance of LLMs on long-context summarization tasks, and which method is most effective?
- Basis in paper: [explicit] The paper explores the extrapolation potential of different RoPE scaling methods (e.g., NTK, PI, YaRN) on long-context summarization tasks, but does not provide a definitive answer on which method is most effective.
- Why unresolved: The paper shows that different RoPE scaling methods exhibit varying extrapolation performance, but the reasons for these differences are not fully explained. The paper also does not explore the impact of these methods on the overall quality and coherence of the generated summaries.
- What evidence would resolve it: A comprehensive comparison of different RoPE scaling methods on long-context summarization tasks, including their impact on extrapolation performance, summary quality, and coherence, would provide insights into the most effective method.

### Open Question 3
- Question: What are the limitations of using concatenated short-context summaries as training data for long-context summarization, and how can these limitations be addressed?
- Basis in paper: [inferred] The paper mentions the use of concatenated short-context summaries as training data for long-context summarization, but does not discuss the potential limitations of this approach.
- Why unresolved: The paper does not explore the potential drawbacks of using concatenated short-context summaries, such as interference between the individual summaries, the impact on the model's ability to learn long-range dependencies, or the effect on the overall coherence of the generated summaries.
- What evidence would resolve it: A study investigating the limitations of using concatenated short-context summaries as training data, including the impact on the model's ability to learn long-range dependencies and the overall coherence of the generated summaries, would provide insights into how to address these limitations.

## Limitations

- The benchmark focuses exclusively on Jin Yong's martial arts novels, creating potential domain-specific bias that may not generalize to other novel types
- The human annotation process is resource-intensive and may not scale well for expanding the benchmark to more diverse content or larger sample sizes
- The evaluation relies heavily on ROUGE-L metrics, which have limitations in capturing semantic similarity and may not fully reflect long-context understanding quality

## Confidence

- High Confidence: Advanced LLMs generating subjective commentary instead of summaries
- Medium Confidence: Base models outperforming instruction-tuned versions for long-context summarization
- Medium Confidence: Small LLMs with stable longer context lengths being more cost-effective
- Low Confidence: Extrapolation mechanism through RoPE base scaling activation

## Next Checks

1. **Cross-genre validation**: Test CNNSum-trained models on summaries of non-martial arts novels to verify whether observed performance patterns and fine-tuning benefits generalize beyond the Jin Yong corpus domain.

2. **Annotation reliability assessment**: Conduct inter-annotator agreement studies using the same annotation guidelines to quantify subjectivity and consistency in identifying "subjective commentary" versus objective summaries, and validate guidelines with additional annotators.

3. **ROUGE-L limitation analysis**: Perform comprehensive human evaluation on a subset of model outputs to assess correlation between ROUGE-L scores and actual summary quality, identifying specific cases where automatic metrics may misrepresent performance.