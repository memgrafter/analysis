---
ver: rpa2
title: 'Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation
  Analysis for Large Language Models'
arxiv_id: '2403.18159'
source_url: https://arxiv.org/abs/2403.18159
tags:
- freeze
- int4
- training
- quantization
- proj
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying large language
  models (LLMs) on resource-constrained edge devices by proposing a lightweight quantization-aware
  fine-tuning technique using knowledge distillation (KD-QAT) to improve the performance
  of 4-bit weight quantized LLMs. The core method idea involves analyzing the gradient
  propagation during training to identify vulnerabilities in the multi-head self-attention
  modules of LLaMAv2-Chat, specifically the o- and v-projection layers, which are
  more susceptible to quantization errors.
---

# Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models

## Quick Facts
- arXiv ID: 2403.18159
- Source URL: https://arxiv.org/abs/2403.18159
- Reference count: 40
- Primary result: ov-freeze achieves near floating point precision performance on Commonsense Reasoning benchmarks, with less than 0.7% loss of accuracy

## Executive Summary
This paper addresses the challenge of deploying large language models on resource-constrained edge devices by proposing a lightweight quantization-aware fine-tuning technique using knowledge distillation. The core innovation is the ov-freeze method, which stabilizes training by freezing the most sensitive projection layers in multi-head self-attention modules during 4-bit quantization. The technique achieves near floating point precision performance on commonsense reasoning benchmarks while significantly improving over vanilla quantization-aware training approaches.

## Method Summary
The method combines knowledge distillation with quantization-aware training (KD-QAT) for 4-bit quantized LLaMAv2-Chat models. Through gradient propagation analysis, the authors identify o- and v-projection layers in multi-head self-attention modules as most vulnerable to quantization errors. The ov-freeze technique freezes these layers to their post-training quantization values during KD-QAT, allowing the model to adapt to quantization errors without destabilizing the most sensitive components. The training uses a weighted sum of cross-entropy and KL divergence losses, with INT4 uniform, static, channelwise weight quantization.

## Key Results
- ov-freeze achieves less than 0.7% accuracy loss on Commonsense Reasoning benchmarks compared to full-precision models
- The technique significantly outperforms vanilla QAT and other freezing schemes on multiple evaluation datasets
- The method maintains near floating point precision performance while enabling efficient 4-bit deployment on edge devices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The gradient magnitudes in the o- and v-projection layers are significantly higher than in the q- and k-projection layers during KD-QAT.
- Mechanism: High gradient magnitudes in low-bit quantization lead to unstable parameter updates and larger quantization errors in the most sensitive layers, which can destabilize the entire training trajectory.
- Core assumption: Gradient magnitude correlates with layer sensitivity to quantization noise in multi-head self-attention modules.
- Evidence anchors:
  - [abstract] "Based on this analysis, the authors propose ov-freeze, a technique that freezes the o- and v-projection weights to their post-training quantization values, allowing the model to adapt to quantization errors without modifying the most sensitive parts of the network."
  - [section] "Specifically, o- and v-projection layers are more susceptible to quantization errors than other weight layers in the multi-head self-attention module."
  - [corpus] Weak - no direct corpus evidence found.
- Break condition: If gradient magnitudes in o- and v-layers are not significantly higher than q- and k-layers, or if other layers become more sensitive under different quantization schemes.

### Mechanism 2
- Claim: Freezing the o- and v-projection layers stabilizes the forward and backward pass behavior of the quantized model.
- Mechanism: By freezing the most sensitive layers, the model avoids large parameter perturbations during training, making the forward and backward pass characteristics similar to those seen during floating point training.
- Core assumption: Stabilizing the most sensitive layers is sufficient to improve overall model stability during quantization-aware training.
- Evidence anchors:
  - [abstract] "ov-freeze results in near floating point precision performance, i.e., less than 0.7% loss of accuracy on Commonsense Reasoning benchmarks."
  - [section] "ov-freeze makes the forward pass behavior of the INT4 quantized network resembles that of the floating point model."
  - [corpus] Weak - no direct corpus evidence found.
- Break condition: If freezing o- and v-layers does not improve stability, or if other layers become unstable when o- and v-layers are frozen.

### Mechanism 3
- Claim: The proposed ov-freeze technique significantly improves the performance of 4-bit weight quantized LLMs on commonsense reasoning benchmarks.
- Mechanism: By stabilizing the training process and avoiding large quantization errors in sensitive layers, ov-freeze enables the quantized model to maintain near floating point precision performance.
- Core assumption: Stabilizing the training process is the key to achieving high accuracy in low-bit quantized LLMs.
- Evidence anchors:
  - [abstract] "ov-freeze achieves near floating point precision performance on Commonsense Reasoning benchmarks, with less than 0.7% loss of accuracy."
  - [section] "ov-freeze results in near floating point precision performance, i.e., less than 0.7% loss of accuracy on Commonsense Reasoning benchmarks."
  - [corpus] Weak - no direct corpus evidence found.
- Break condition: If the performance improvement is not significant or if other techniques can achieve similar results without freezing layers.

## Foundational Learning

- Concept: Knowledge Distillation (KD)
  - Why needed here: KD is used to transfer knowledge from a full-precision teacher model to a quantized student model, helping the student model maintain performance after quantization.
  - Quick check question: What is the primary purpose of using knowledge distillation in the context of quantized LLMs?

- Concept: Quantization-Aware Training (QAT)
  - Why needed here: QAT is a technique used to train quantized models by simulating quantization during training, allowing the model to adapt to quantization errors.
  - Quick check question: How does quantization-aware training differ from post-training quantization in terms of model adaptation to quantization errors?

- Concept: Multi-Head Self-Attention (MHSA)
  - Why needed here: MHSA is a key component of transformer-based LLMs, and understanding its sensitivity to quantization errors is crucial for developing effective quantization techniques.
  - Quick check question: What are the main components of a multi-head self-attention module, and how do they contribute to the overall attention mechanism?

## Architecture Onboarding

- Component map:
  - LLaMAv2-Chat model: 7B parameter LLM
  - Multi-Head Self-Attention (MHSA) modules: 32 layers, each containing q-, k-, v-, and o-projection layers
  - Knowledge Distillation (KD) setup: Full-precision teacher model and quantized student model
  - Quantization scheme: INT4 uniform, static, channelwise weight and tensorwise activation quantization
  - Datasets: 660B-token language dataset for finetuning, Wikitext and Commonsense benchmarks for evaluation

- Critical path:
  - Forward pass: Input → Tokenization → MHSA layers → Output generation
  - Backward pass: Loss computation → Gradient propagation → Parameter updates
  - Key components: q-, k-, v-, and o-projection layers in MHSA modules

- Design tradeoffs:
  - Quantization bit-width: Lower bit-width reduces memory and computation but increases quantization noise
  - Freezing layers: Freezing sensitive layers improves stability but may limit model adaptation
  - Knowledge distillation: Using public datasets for distillation is more accessible but may be less effective than using teacher-generated data

- Failure signatures:
  - High perplexity on evaluation datasets
  - Unstable training loss or accuracy
  - Significant performance drop compared to floating-point model

- First 3 experiments:
  1. Compare gradient magnitudes in q-, k-, v-, and o-projection layers during KD-QAT to identify sensitive layers.
  2. Implement and evaluate the ov-freeze technique on a small-scale model to verify its effectiveness in stabilizing training.
  3. Assess the performance of ov-freeze on the full LLaMAv2-Chat model using commonsense reasoning benchmarks.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The analysis of gradient propagation sensitivity is based on a single model architecture (LLaMAv2-Chat-7B) and dataset configuration, limiting generalizability to other LLM variants or quantization schemes.
- The claim that o- and v-projection layers are universally the most sensitive components remains unverified across different model sizes, training objectives, or quantization bit-widths.
- The paper does not provide specific information on the inference speed or memory usage of models using ov-freeze when deployed on edge devices.

## Confidence
- **High confidence**: The empirical results showing improved performance with ov-freeze on the tested benchmarks
- **Medium confidence**: The gradient analysis identifying o- and v-layers as sensitive components
- **Medium confidence**: The claim that freezing these layers stabilizes training and maintains near floating-point precision

## Next Checks
1. **Cross-model validation**: Apply the gradient analysis and ov-freeze technique to other transformer architectures (GPT variants, Mistral) to verify if o- and v-layers consistently show highest sensitivity across different model families.

2. **Quantization scheme generalization**: Test the ov-freeze approach with different quantization configurations (non-uniform quantization, dynamic quantization, mixed-precision schemes) to determine if the identified sensitive layers remain consistent.

3. **Ablation on gradient correlation**: Design controlled experiments that artificially manipulate gradient magnitudes in different layers during training to establish whether high gradients directly cause quantization sensitivity, or if they merely correlate with other underlying factors.