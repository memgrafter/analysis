---
ver: rpa2
title: How Entangled is Factuality and Deception in German?
arxiv_id: '2409.20165'
source_url: https://arxiv.org/abs/2409.20165
tags:
- deception
- deceptive
- cues
- text
- fact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates deception detection in German, disentangling
  factual accuracy from deceptive intent. Using the belief-based deception framework,
  texts are labeled as deceptive when there is a mismatch between what people say
  and what they truly believe.
---

# How Entangled is Factuality and Deception in German?

## Quick Facts
- arXiv ID: 2409.20165
- Source URL: https://arxiv.org/abs/2409.20165
- Authors: Aswathy Velutharambath; Amelie WÃ¼hrl; Roman Klinger
- Reference count: 23
- Primary result: Deception detection models in German perform no better than random guessing; linguistic cues show no significant correlation with deception labels.

## Executive Summary
This study investigates deception detection and factuality in German using the belief-based deception framework, where deception is defined as arguing against one's own beliefs. Surprisingly, the analysis finds no correlation with established cues of deception, and automatic models struggle with the task, performing no better than random guessing. For fact checking, the study finds that non-factual and deceptive content poses particular challenges, with models showing reduced performance on these instances. The results highlight the complexity of disentangling factuality from deception in computational models.

## Method Summary
The study uses the DEFABEL corpus of belief-based deception in German, containing 1031 argumentative texts labeled for deceptive intent. Linguistic features are extracted and analyzed for correlation with deception labels. Multiple models are evaluated: feature-based models (logistic regression, SVM), transformer-based models (German BERT), and large language models (Mistral 8x7B). Fact checking is framed as a Natural Language Inference task, using a RoBERTa-based model and Mistral7B-Instruct to predict entailment labels. Performance is evaluated using standard metrics like precision, recall, F1-score, and accuracy.

## Key Results
- No statistically significant correlation found between linguistic cues and deception labels in German texts.
- Automatic deception detection models perform no better than random guessing, achieving F1 scores close to majority class prediction.
- Fact checking models show reduced performance on non-factual and deceptive content, indicating these instances are more difficult to verify.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deception detection fails because linguistic cues assumed to be universal do not generalize to German language texts.
- Mechanism: When models rely on linguistic features (e.g., reduced self-references, exclusive word usage) that were identified in English corpora, these cues fail to manifest consistently in German data, leading to no statistically significant correlation between the cues and deception labels.
- Core assumption: Linguistic patterns of deception are language-independent and can be transferred across languages without modification.
- Evidence anchors:
  - [abstract] "Surprisingly, our analysis finds no correlation with established cues of deception."
  - [section 3] "none of the 128 cues show any statistically significant correlation with the deception label"
  - [corpus] Weak evidence: DEFABEL corpus is the only German deception dataset; no direct cross-linguistic validation of cues provided.
- Break condition: If German texts exhibit unique cultural or linguistic structures that alter how deception is expressed, the universal cue assumption fails.

### Mechanism 2
- Claim: Automatic models perform no better than random guessing because they cannot accurately capture deception cues from language in the belief-based framework.
- Mechanism: The belief-based deception framework defines deception as arguing against one's own beliefs, a property that may not be encoded in surface linguistic features; models trained on such features cannot distinguish deceptive from non-deceptive arguments reliably.
- Core assumption: Deception cues are embedded in observable linguistic features that models can extract and classify.
- Evidence anchors:
  - [abstract] "our experiments show that both traditional and state-of-the-art models struggle with the task, performing no better than random guessing."
  - [section 4] "GBERT model achieves an F1 score of 0.79 on the deception label. This result is almost the same as majority class prediction, as the model labeled all instances as deceptive in 9 out of 10 folds."
  - [corpus] Weak evidence: No feature importance analysis to confirm models are not capturing non-deceptive patterns.
- Break condition: If deception cues are encoded in deeper semantic or pragmatic aspects rather than surface features, models relying on linguistic features will fail.

### Mechanism 3
- Claim: Fact verification performance degrades on non-factual and deceptive content because models conflate factual accuracy with deceptive intent.
- Mechanism: Fact checking models trained to predict entailment may inadvertently use non-propositional cues (e.g., sentiment, concreteness) that differ between factual/non-factual or deceptive/non-deceptive instances, leading to reduced performance on these subsets.
- Core assumption: Fact checking models focus solely on logical entailment between claim and evidence, ignoring properties of the claim and evidence themselves.
- Evidence anchors:
  - [abstract] "For fact checking, we find that Natural Language Inference-based verification performs worse on non-factual and deceptive content, while prompting Large Language Models for the same task is less sensitive to these properties."
  - [section 5] "A RoBERTa-based model shows lower performance for instances with non-factual claims and deceptive evidence documents, indicating that these instances are more difficult to verify."
  - [corpus] Weak evidence: No ablation study to isolate the effect of factual accuracy vs. deceptive intent on model predictions.
- Break condition: If models are explicitly trained to ignore claim and evidence properties, or if the dataset is balanced for these properties, the performance degradation may not occur.

## Foundational Learning

- Concept: Belief-based deception framework
  - Why needed here: The study uses this framework to disentangle factual accuracy from deceptive intent, defining deception as arguing against one's own beliefs. Understanding this framework is essential to interpret the results and design appropriate experiments.
  - Quick check question: In the belief-based deception framework, is the statement "The Earth is flat" deceptive if someone believes it and argues in its favor?

- Concept: Point-biserial correlation
  - Why needed here: The study uses point-biserial correlation to analyze the relationship between linguistic cues (continuous) and deception labels (discrete). Knowing how this correlation works helps understand why no significant correlations were found.
  - Quick check question: What does a high positive point-biserial correlation between a linguistic cue and deception label indicate?

- Concept: Natural Language Inference (NLI)
  - Why needed here: The study frames fact checking as an NLI task, where the model predicts whether the evidence entails, contradicts, or is neutral to the claim. Understanding NLI is crucial to interpret the fact verification results.
  - Quick check question: In the context of fact checking, what does it mean if the evidence entails the claim?

## Architecture Onboarding

- Component map:
  Data ingestion: DEFABEL corpus -> Feature extraction: Linguistic cues -> Model training: Logistic regression, SVM, BERT, Mistral -> Evaluation: Deception detection (accuracy, F1), Fact verification (entailment prediction) -> Analysis: Correlation analysis, performance comparison

- Critical path:
  1. Load DEFABEL corpus
  2. Extract linguistic features for correlation analysis
  3. Train deception detection models
  4. Evaluate deception detection models
  5. Frame fact checking as NLI task
  6. Train fact checking models
  7. Evaluate fact checking models on different subsets
  8. Analyze performance differences and error patterns

- Design tradeoffs:
  - Using linguistic features vs. contextual embeddings: Linguistic features are interpretable but may miss complex patterns; embeddings capture context but are less interpretable.
  - Fine-tuning vs. prompting: Fine-tuning adapts models to the task but requires more data and compute; prompting is efficient but may not fully capture task nuances.
  - Balanced vs. imbalanced data: Balanced data ensures fair evaluation but may not reflect real-world distributions; imbalanced data reflects reality but may skew performance metrics.

- Failure signatures:
  - No significant correlation between linguistic cues and deception labels
  - Deception detection models performing no better than random guessing
  - Fact checking models showing degraded performance on non-factual and deceptive content
  - High error rates on instances with non-factual claims and deceptive evidence

- First 3 experiments:
  1. Replicate correlation analysis with a subset of linguistic cues to verify the absence of significant correlations.
  2. Train a deception detection model using only the top 5 linguistic cues by point-biserial correlation to see if performance improves.
  3. Evaluate a fact checking model on a balanced dataset (equal factual/non-factual and deceptive/non-deceptive instances) to isolate the effect of class imbalance.

## Open Questions the Paper Calls Out
- Do German texts exhibit unique linguistic patterns in deceptive contexts compared to other languages?
- How does the belief-based deception framework perform in English compared to German?
- What additional properties of (non-)deceptive evidence or (non-)factual claims impact the performance of fact checking models?

## Limitations
- Exclusive reliance on the DEFABEL corpus, which may not capture the full complexity of deception in German discourse.
- Absence of feature importance analysis for transformer models, limiting understanding of model failures.
- No exploration of whether deception manifests differently in spoken vs. written German or across different domains.

## Confidence
- **High Confidence**: The finding that no significant correlation exists between established linguistic cues and deception labels in German texts.
- **Medium Confidence**: The conclusion that automatic deception detection models perform no better than random guessing.
- **Medium Confidence**: The observation that fact checking models show reduced performance on non-factual and deceptive content.

## Next Checks
1. Conduct cross-linguistic validation to test whether linguistic cues identified in English deception detection studies show significant correlations with deception labels in German texts.
2. Perform detailed error analysis of the best-performing deception detection model to identify whether failures stem from inability to capture deception cues or from confounding factors in the dataset.
3. Create a balanced subset of the DEFABEL corpus and re-evaluate fact checking model performance to isolate the effect of class imbalance on the observed performance degradation.