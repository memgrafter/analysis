---
ver: rpa2
title: Are Transformers in Pre-trained LM A Good ASR Encoder? An Empirical Study
arxiv_id: '2409.17750'
source_url: https://arxiv.org/abs/2409.17750
tags:
- pre-trained
- qwen
- encoder
- these
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of transformer layers from pre-trained
  language models (PLMs) as ASR encoders. The authors evaluate whether these text-trained
  transformers can effectively process speech input when used as the encoder in an
  ASR system.
---

# Are Transformers in Pre-trained LM A Good ASR Encoder? An Empirical Study

## Quick Facts
- arXiv ID: 2409.17750
- Source URL: https://arxiv.org/abs/2409.17750
- Authors: Keyu An; Shiliang Zhang; Zhijie Yan
- Reference count: 10
- Key outcome: Pre-trained language model transformers outperform random initialization and convolutional layers as ASR encoders, especially when fine-tuned

## Executive Summary
This paper investigates whether transformers from pre-trained language models (PLMs) can serve as effective encoders for automatic speech recognition (ASR). The authors hypothesize that despite being trained solely on text, these transformers can extract meaningful features from speech input sequences. Through experiments on AISHELL-1, LibriSpeech, and a large in-house Mandarin dataset, they demonstrate that PLM transformers consistently outperform randomly initialized transformers and convolutional layers alone. The results show that PLM transformers not only serve as advantageous starting points for ASR encoder initialization but also excel at tasks requiring semantic understanding.

## Method Summary
The authors evaluate Qwen model transformers as ASR encoders in two configurations: (1) with a convolutional input layer, and (2) stacked on top of a pre-trained ASR encoder. They compare frozen versus fine-tuned PLM transformers against randomly initialized transformers and convolutional baselines using CTC loss. Experiments are conducted on AISHELL-1, LibriSpeech, and a large in-house Mandarin dataset using filterbank features computed over 25ms windows with 10ms frame shift.

## Key Results
- On AISHELL-1, frozen Qwen transformers achieved 21.52% CER vs 33.67% for random transformers (12.15% absolute improvement)
- Fine-tuned Qwen transformers achieved 10.19% CER on AISHELL-1, outperforming frozen versions
- On the large in-house dataset, adding Qwen transformers to a pre-trained ASR encoder reduced CER by 7% on general speech and 47% on ancient Chinese poetry

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers from PLMs can extract useful acoustic features even without speech-specific pre-training.
- Mechanism: The transformer architecture inherently models relationships between input elements, enabling it to capture patterns in speech feature sequences analogous to text tokens.
- Core assumption: The structural patterns in speech sequences are sufficiently similar to text sequences for transformers to generalize.
- Evidence anchors:
  - [abstract] "our underlying hypothesis posits that, despite being initially trained on text-based corpora, these transformers possess a remarkable capacity to extract effective features from the input sequence"
  - [section 2] "Acknowledging the proficiency of transformer [7] layers within PLMs in modeling relationships among textual elements, we posit that this capability can generalize to acoustic signals"
  - [corpus] Weak evidence - only general LLM-as-encoder papers found, no specific speech-focused transformer mechanism papers
- Break condition: If speech features have fundamentally different sequential dependencies than text tokens, the transformer's text-trained attention patterns may not generalize effectively.

### Mechanism 2
- Claim: Pre-trained transformer weights provide better initialization than random weights for ASR encoders.
- Mechanism: The contextual understanding learned from text provides a strong prior that can be adapted to speech tasks, reducing the need to learn representations from scratch.
- Core assumption: Semantic knowledge from text pre-training transfers to speech processing tasks.
- Evidence anchors:
  - [section 3.2] "Models using pre-trained Qwen significantly outperform those initialized randomly, regardless of whether the Qwen parameters are frozen or not"
  - [abstract] "Particularly, they serve as an advantageous starting point for initializing ASR encoders"
  - [corpus] No direct evidence found - corpus lacks papers specifically on transformer weight transfer from text to speech
- Break condition: If the ASR task domain is too different from the text corpus domain, the pre-trained weights may introduce harmful biases rather than helpful priors.

### Mechanism 3
- Claim: Fine-tuning PLM transformers on ASR tasks improves performance over frozen weights.
- Mechanism: Task-specific adaptation allows the pre-trained transformer to adjust its representations to better match speech patterns and task requirements.
- Core assumption: The pre-trained weights provide a good starting point that can be refined for the specific ASR task.
- Evidence anchors:
  - [section 3.2] "Qwen models that undergo optimization with the ASR loss perform better compared to their frozen counterparts"
  - [section 3.2] "Models using pre-trained Qwen significantly outperform those initialized randomly, regardless of whether the Qwen parameters are frozen or not"
  - [corpus] No direct evidence found - corpus lacks papers specifically on fine-tuning vs freezing comparisons for ASR
- Break condition: If the task domain is too different from the pre-training domain, fine-tuning may lead to catastrophic forgetting of useful pre-trained representations.

## Foundational Learning

- Concept: CTC (Connectionist Temporal Classification)
  - Why needed here: The paper uses CTC loss for sequence-to-sequence mapping without explicit alignment
  - Quick check question: What is the main advantage of CTC over traditional forced alignment approaches in ASR?

- Concept: Attention mechanisms in transformers
  - Why needed here: The paper relies on transformer self-attention to model relationships in speech features
  - Quick check question: How does self-attention differ from cross-attention in the transformer architecture?

- Concept: Fine-tuning vs. feature extraction (frozen layers)
  - Why needed here: The paper compares frozen vs. fine-tuned PLM transformers
  - Quick check question: What are the computational and performance tradeoffs between freezing pre-trained layers versus fine-tuning them?

## Architecture Onboarding

- Component map: Filterbanks → Convolutional/Encoder → Qwen Transformers → Linear → CTC Loss
- Critical path: Input → Conv/Encoder → Qwen Transformers → Linear → CTC Loss
- Design tradeoffs:
  - Larger Qwen models don't always improve performance (1.8B slightly worse than 0.5B on AISHELL-1)
  - Higher dropout (0.5) can help prevent overfitting with larger models
  - Joint fine-tuning of all components vs. freezing PLM layers
- Failure signatures:
  - Performance worse than convolutional baseline → transformer architecture not generalizing
  - Minimal improvement from pre-trained vs random initialization → domain mismatch
  - Overfitting on small datasets → need stronger regularization or smaller model
- First 3 experiments:
  1. Baseline: Convolutional input layer only (61.95/66.21 CER on AISHELL-1)
  2. Random initialization: Convolutional + randomly initialized Qwen (33.67/37.24 CER)
  3. Frozen pre-trained: Convolutional + frozen Qwen (21.52/23.71 CER)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties of transformers trained on text enable them to effectively model speech data?
- Basis in paper: [explicit] The authors state that while empirical benefits are observed, "the underlying theoretical rationale behind why transformers, trained solely on text, can contribute positively to acoustic modeling remains elusive."
- Why unresolved: The paper demonstrates effectiveness through experiments but does not provide theoretical explanation for cross-modal generalization.
- What evidence would resolve it: Detailed analysis of attention patterns, probing tasks, or theoretical framework explaining how text-trained transformers capture acoustic features.

### Open Question 2
- Question: How can the integration of PLM transformers into ASR systems be optimized for efficiency without sacrificing performance gains?
- Basis in paper: [explicit] The authors note that "the addition of pre-trained LM transformers onto a pre-trained ASR encoder...exhibits only marginal gains on standard test sets" and call for "optimizing the integration strategy."
- Why unresolved: Current approaches show diminishing returns with increasing model size, suggesting inefficiencies in how transformers are integrated.
- What evidence would resolve it: Novel integration architectures, adaptive layer selection, or sparse transformer approaches that maintain performance while reducing computational cost.

### Open Question 3
- Question: What is the optimal dropout rate for pre-trained transformers when used as ASR encoders, and how does it vary with dataset characteristics?
- Basis in paper: [explicit] The authors observe that "escalating the size of the pre-trained LM does not necessarily lead to enhanced ASR performance" but "performance improvements when a higher dropout rate (set at 0.5) is applied."
- Why unresolved: The paper only tests one dropout rate on one dataset, leaving the relationship between dropout, model size, and dataset complexity unexplored.
- What evidence would resolve it: Systematic ablation studies varying dropout rates across different dataset sizes and model scales to establish optimal configurations.

## Limitations

- Limited validation on diverse languages and acoustic conditions beyond Mandarin and English
- No investigation of computational overhead compared to traditional ASR encoders
- Theoretical understanding of cross-modal generalization remains speculative without mechanistic insights

## Confidence

**High Confidence Claims:**
- PLM transformers outperform randomly initialized transformers as ASR encoders
- Fine-tuning PLM transformers improves performance over frozen weights
- PLM transformers are particularly effective for tasks requiring semantic understanding

**Medium Confidence Claims:**
- PLM transformers serve as advantageous starting points for initializing ASR encoders
- The effectiveness generalizes across different dataset sizes and domains

**Low Confidence Claims:**
- The specific mechanisms by which text-trained transformers extract acoustic features
- The claim that this approach will scale to other languages and acoustic conditions

## Next Checks

1. **Architectural ablation study**: Systematically vary the number of Qwen transformer layers and test whether performance improvements are due to depth or the pre-trained weights themselves.

2. **Cross-lingual generalization test**: Evaluate the approach on a typologically diverse set of languages (e.g., tonal vs. non-tonal, different writing systems) to assess whether the text-pretraining advantage transfers across language families.

3. **Computational efficiency analysis**: Measure inference time and memory requirements for PLM-based encoders versus traditional ASR encoders on the same hardware, quantifying the practical tradeoffs between performance gains and computational costs.