---
ver: rpa2
title: 'Self-Reflection in LLM Agents: Effects on Problem-Solving Performance'
arxiv_id: '2405.06682'
source_url: https://arxiv.org/abs/2405.06682
tags:
- self-reflection
- agents
- arxiv
- agent
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated how self-reflection in large language model
  (LLM) agents affects problem-solving performance. Nine popular LLMs were tested
  on 1,000 multiple-choice questions across ten domains.
---

# Self-Reflection in LLM Agents: Effects on Problem-Solving Performance

## Quick Facts
- arXiv ID: 2405.06682
- Source URL: https://arxiv.org/abs/2405.06682
- Reference count: 40
- All eight types of self-reflection significantly improved LLM accuracy (p < 0.001) across nine models

## Executive Summary
This study investigated how self-reflection in large language model (LLM) agents affects problem-solving performance. Nine popular LLMs were tested on 1,000 multiple-choice questions across ten domains. For each incorrectly answered question, eight types of self-reflecting agents were prompted to analyze their errors and generate guidance, then re-attempt the same questions using this reflection. All self-reflection types significantly improved accuracy (p < 0.001) across all models. The most effective self-reflections included detailed instructions, explanations, and solutions, while even a simple retry without feedback yielded improvements. Performance gains were consistent across models but varied by exam difficulty, with the largest improvements seen on the most challenging tests.

## Method Summary
The experiment tested nine LLMs on 1,000 multiple-choice questions from ten standardized benchmarks. Each incorrectly answered question was processed by eight self-reflecting agents (Retry, Keywords, Advice, Explanation, Instructions, Solution, Composite, Unredacted) that analyzed their mistakes and generated guidance. The agents then re-attempted the questions using their self-generated reflections. A baseline agent answered all questions without self-reflection for comparison. All code and data are available on GitHub for reproduction.

## Key Results
- All eight self-reflection types significantly improved accuracy (p < 0.001) across all nine LLMs
- The most effective self-reflections included detailed instructions, explanations, and solutions
- Performance gains were consistent across models but varied by exam difficulty, with largest improvements on most challenging tests
- Even simple retry (without feedback) yielded improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-reflection provides error signals that help LLMs identify and correct reasoning mistakes in their chain-of-thought
- Mechanism: When LLMs receive external feedback (correct answer) during reflection phase, they analyze their incorrect reasoning, generate explanations and instructions for solving similar problems, then use this guidance to improve their next attempt
- Core assumption: LLMs can meaningfully analyze their own reasoning errors when provided with the correct answer
- Evidence anchors:
  - [abstract] "For each incorrectly answered question, we instructed eight types of self-reflecting LLM agents to reflect on their mistakes and provide themselves with guidance to improve problem-solving"
  - [section] "Self-reflection in LLM agents is a metacognitive strategy also known as introspection"
  - [corpus] Weak - corpus shows related work on LLM agents and problem-solving but doesn't specifically address the reflection mechanism described here
- Break condition: If LLMs cannot meaningfully analyze their reasoning errors even with correct answers provided

### Mechanism 2
- Claim: More detailed self-reflection types (instructions, explanations, solutions) provide better guidance than simple retries
- Mechanism: Different self-reflection types vary in the amount of information they provide - instructions give step-by-step guidance, explanations clarify reasoning errors, solutions show complete problem-solving paths, while simple retries only indicate an error occurred
- Core assumption: The quantity and quality of information in self-reflection directly correlates with improvement in problem-solving
- Evidence anchors:
  - [abstract] "The most effective self-reflections included detailed instructions, explanations, and solutions"
  - [section] "Self-reflections that contain more information (e.g., Instructions, Explanation, and Solution) outperform types of self-reflection with limited information (e.g., Retry, Keywords, and Advice)"
  - [corpus] Weak - corpus shows various LLM prompting strategies but doesn't specifically compare different levels of reflection detail
- Break condition: If the relationship between reflection detail and performance improvement is not linear or follows different patterns

### Mechanism 3
- Claim: Self-reflection benefits all tested LLMs regardless of their base capabilities
- Mechanism: The reflection process helps weaker LLMs improve proportionally similar to stronger LLMs, suggesting the mechanism works across different model architectures and parameter counts
- Core assumption: The reflection improvement mechanism is independent of the underlying LLM architecture
- Evidence anchors:
  - [abstract] "Performance gains were consistent across models but varied by exam difficulty"
  - [section] "In terms of performance by model, every LLM that we tested demonstrated similar increases in accuracy across all types of self-reflection"
  - [corpus] Weak - corpus mentions various LLMs but doesn't provide evidence about reflection improvements across different models
- Break condition: If reflection improvements vary significantly based on model architecture or parameter count

## Foundational Learning

- Concept: Chain of Thought (CoT) prompting
  - Why needed here: Self-reflection builds upon CoT by allowing LLMs to analyze and correct their reasoning chains
  - Quick check question: What is the primary purpose of CoT prompting in LLMs?

- Concept: Metacognition in AI systems
  - Why needed here: Self-reflection is a metacognitive process where LLMs think about their own thinking processes
  - Quick check question: How does self-reflection differ from standard CoT prompting?

- Concept: Statistical significance testing (McNemar test)
  - Why needed here: Used to determine if accuracy improvements from self-reflection are statistically significant
  - Quick check question: When would you use a McNemar test instead of a standard t-test?

## Architecture Onboarding

- Component map:
  Baseline agent -> Question answering -> Incorrect answer queue -> Self-reflection generator (8 variants) -> Redaction system -> Re-answer engine -> Evaluation pipeline

- Critical path:
  1. Baseline answering of all questions
  2. Queueing incorrect answers for reflection
  3. Generating self-reflections
  4. Redacting sensitive information
  5. Re-answering with reflection guidance
  6. Calculating accuracy improvements

- Design tradeoffs:
  - Batch vs. iterative processing (batch used for cost/time efficiency)
  - Redaction completeness vs. information retention
  - Reflection detail vs. generation cost
  - External feedback availability vs. self-sufficiency

- Failure signatures:
  - No improvement across all models suggests reflection mechanism is broken
  - Performance regression indicates harmful reflection guidance
  - Inconsistent improvements across similar question types suggests guidance quality issues
  - High variance in results suggests instability in reflection generation

- First 3 experiments:
  1. Run baseline accuracy calculation for all 9 LLMs on 1000 questions
  2. Implement and test single self-reflection type (e.g., Retry) on subset of questions
  3. Compare improvement rates across different self-reflection types on same question subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the order of self-reflection types in the Composite agent affect its performance compared to when they are presented separately?
- Basis in paper: [explicit] The paper mentions that the Composite agent includes all six types of self-reflections combined
- Why unresolved: The paper does not specify whether the order of reflection types matters or if combining them produces synergistic effects beyond simple addition
- What evidence would resolve it: Testing different orderings of reflection types in the Composite agent and comparing performance to individual reflection types

### Open Question 2
- Question: How does self-reflection performance vary when agents are given more complex, multi-step problems rather than single-step multiple choice questions?
- Basis in paper: [inferred] The paper notes that "the LLM agent we created for this experiment only solved a single-step problem" and suggests future research on multi-step problems
- Why unresolved: The current experiment only tested single-step problems, which doesn't capture the full potential of self-reflecting agents on complex tasks
- What evidence would resolve it: Repeating the experiment with multi-step problems that require sequential reasoning and tool use

### Open Question 3
- Question: What is the minimum amount of correct answer information needed in self-reflections to achieve significant performance improvements?
- Basis in paper: [explicit] The paper shows that even the Retry agent (which only knows it was wrong) improves performance, and that redacting answers was effective at preventing direct answer leakage
- Why unresolved: While the paper shows that redacting answers works, it doesn't test intermediate levels of answer information disclosure
- What evidence would resolve it: Systematically varying the amount of answer information in self-reflections while measuring performance gains

## Limitations
- Experiments used batch processing rather than true iterative reflection, potentially limiting real-world applicability
- Redaction system may have removed useful contextual information that could have enhanced reflection quality
- Experiments conducted on multiple-choice questions with external feedback, not reflecting real-world scenarios without answer keys

## Confidence

*High Confidence*: The finding that all self-reflection types significantly improved accuracy (p < 0.001) across all tested models is well-supported by the experimental design and statistical analysis.

*Medium Confidence*: The ranking of self-reflection effectiveness shows clear patterns but may be sensitive to specific question domains and redaction procedures.

*Low Confidence*: The claim about self-reflection working as a general metacognitive strategy for LLMs requires further validation beyond the multiple-choice question format used in this study.

## Next Checks

1. **Iterative Validation**: Implement true iterative self-reflection (multiple reflection cycles per question) rather than batch processing to assess whether performance improvements compound over time and whether this changes the relative effectiveness of different reflection types.

2. **Real-World Transfer**: Test self-reflection mechanisms on open-ended problems without answer keys, using human evaluation or automated metrics to assess whether the same improvement patterns hold when external feedback isn't available.

3. **Cross-Domain Robustness**: Validate the findings across additional domains not represented in the current dataset (e.g., creative writing, code generation, strategic planning) to determine if the self-reflection benefits are truly domain-agnostic or specific to the tested MCQA format.