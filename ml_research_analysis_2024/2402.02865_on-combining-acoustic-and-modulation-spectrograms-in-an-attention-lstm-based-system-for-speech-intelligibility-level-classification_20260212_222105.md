---
ver: rpa2
title: On combining acoustic and modulation spectrograms in an attention LSTM-based
  system for speech intelligibility level classification
arxiv_id: '2402.02865'
source_url: https://arxiv.org/abs/2402.02865
tags:
- modulation
- speech
- intelligibility
- lstm
- spectrograms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the task of automatic prediction of speech
  intelligibility levels in dysarthric speech using an attention-based LSTM framework.
  The authors propose the use of per-frame modulation spectrograms as input features
  to capture long-term temporal dynamics, complementing traditional log-mel spectrograms
  that capture short-term artifacts.
---

# On combining acoustic and modulation spectrograms in an attention LSTM-based system for speech intelligibility level classification

## Quick Facts
- arXiv ID: 2402.02865
- Source URL: https://arxiv.org/abs/2402.02865
- Reference count: 40
- Primary result: Attention-based LSTM with modulation spectrograms outperforms SVM baselines and single-feature systems for dysarthric speech intelligibility classification

## Executive Summary
This paper addresses automatic prediction of speech intelligibility levels in dysarthric speech using an attention-based LSTM framework. The authors propose using per-frame modulation spectrograms as input features to capture long-term temporal dynamics, complementing traditional log-mel spectrograms that capture short-term artifacts. Two fusion strategies—late fusion at decision level and WP fusion at utterance level—are explored to combine these feature types. Evaluated on the UA-Speech database, the system with WP fusion and Attention-Pooling achieves the best results, significantly outperforming single-feature systems and SVM-based baselines. The study demonstrates that attentional LSTM networks effectively model both feature types, with WP fusion providing superior accuracy by leveraging complementary information from short- and long-term speech dynamics.

## Method Summary
The proposed method uses LSTM-based architectures with attention pooling to classify dysarthric speech into three intelligibility levels. Per-frame log-mel spectrograms (32 filters, 20ms windows, 10ms frame shift) and modulation spectrograms (23 critical bands, 8 modulation filters, 256ms windows, 64ms frame shift) serve as input features. The system processes variable-length sequences using masking layers, followed by dense layers for feature extraction, LSTM layers with nL units, and weighted pooling (basic, mean, or attention-based). Two fusion strategies combine the feature types: late fusion (decision level) and WP fusion (utterance level). The model is trained using Adam optimizer with 0.0002 learning rate, 32 batch size, and 50 epochs maximum, evaluated through 5-fold cross-validation with subject-wise splits on the UA-Speech database.

## Key Results
- WP fusion with Attention-Pooling achieves the highest accuracy at 72.3% on UA-Speech database
- Both fusion strategies outperform single-feature systems, demonstrating complementarity between log-mel and modulation spectrograms
- Attention-Pooling consistently outperforms Basic LSTM and Mean-Pooling across all configurations
- The proposed system significantly outperforms SVM-based baselines that use compact feature representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention-weighted pooling in LSTM networks learns to emphasize temporal frames that are most relevant to predicting intelligibility level, improving classification over uniform pooling.
- Mechanism: The attention layer computes a weight vector over LSTM output frames using a learned attention parameter. Frames with higher relevance to intelligibility get larger weights, allowing the system to focus on critical acoustic events while suppressing irrelevant frames (e.g., pauses).
- Core assumption: Not all temporal frames contribute equally to intelligibility classification; some frames contain more discriminative cues than others.
- Evidence anchors:
  - [abstract] "attention mechanism... tries to determine the structure of the temporal sequences by learning the relevance of each frame to the task under consideration"
  - [section] "Attention-Pooling... tries to determine the structure of the temporal sequences by learning the relevance of each frame to the task under consideration"
  - [corpus] Weak correlation - no corpus entries directly mention attention pooling mechanisms
- Break condition: If the attention weights become uniform or the system fails to learn meaningful differentiation between frames, indicating the attention mechanism isn't capturing relevant patterns.

### Mechanism 2
- Claim: Combining short-term log-mel spectrograms with long-term modulation spectrograms provides complementary information about dysarthric speech intelligibility.
- Mechanism: Log-mel spectrograms capture short-term artifacts (phoneme articulation, spectral distortions), while modulation spectrograms capture long-term temporal dynamics (rhythm disturbances, disfluencies). Fusing both allows the model to leverage both types of cues.
- Core assumption: Dysarthric speech impairments manifest across multiple time scales, requiring features that capture both short-term and long-term characteristics.
- Evidence anchors:
  - [abstract] "both combination strategies, late and WP fusion, outperform the single-feature systems, suggesting that per-frame log-mel and modulation spectrograms carry complementary information"
  - [section] "dysarthric speech characteristics are related to short and long-term phenomena... a combination of features extracted at different time scales is required"
  - [corpus] No direct corpus evidence - the related papers don't specifically address modulation-spectrogram fusion
- Break condition: If fusion doesn't improve accuracy beyond individual features, suggesting the features aren't truly complementary or the fusion method is inadequate.

### Mechanism 3
- Claim: LSTM networks can directly process per-frame spectrograms without requiring compact feature representations, preserving temporal information.
- Mechanism: Unlike traditional machine learning that requires summary features (e.g., average MFCC), LSTMs can process the full temporal sequence of per-frame spectrograms, maintaining all temporal information through their recurrent structure.
- Core assumption: Temporal information in spectrograms is important for intelligibility classification and isn't adequately captured by summary statistics.
- Evidence anchors:
  - [abstract] "per-frame modulation spectrograms as input features, instead of compact representations derived from them that discard important temporal information"
  - [section] "we propose the use of per-frame modulation spectrograms as input features... has not been previously reported for this task"
  - [corpus] No corpus evidence - related papers don't discuss per-frame spectrogram processing
- Break condition: If the system performs worse than summary-based approaches, indicating that the temporal information isn't actually useful or the LSTM architecture isn't effectively utilizing it.

## Foundational Learning

- Concept: LSTM architecture and operation
  - Why needed here: Understanding how LSTMs process temporal sequences is crucial for implementing and debugging the model
  - Quick check question: What is the difference between an LSTM cell's hidden state and cell state, and how do they relate to temporal modeling?

- Concept: Attention mechanisms in neural networks
  - Why needed here: The attention pooling component is central to the model's performance, requiring understanding of how attention weights are computed and applied
  - Quick check question: How does the softmax transformation in attention pooling ensure that attention weights sum to one across all frames?

- Concept: Spectrogram computation and interpretation
  - Why needed here: The system uses both log-mel and modulation spectrograms as inputs, requiring understanding of how these features are computed and what speech characteristics they capture
  - Quick check question: What is the fundamental difference between the information captured by a log-mel spectrogram versus a modulation spectrogram?

## Architecture Onboarding

- Component map: Input → Masking → Dense (nD1) → LSTM (nL) → Weighted Pooling → Dense (nD2) → Dropout → Dense (nC) → Softmax
- Critical path: Input → Masking → Dense (nD1) → LSTM (nL) → Weighted Pooling → Dense (nD2) → Dropout → Dense (nC) → Softmax
- Design tradeoffs:
  - Log-mel vs. modulation spectrograms: Log-mels capture short-term artifacts but may miss rhythmic disturbances; modulation spectrograms capture long-term dynamics but have lower time resolution
  - Fusion strategies: Late fusion is simpler but may lose some interaction information; WP fusion combines features earlier but increases complexity
  - Pooling methods: Basic LSTM is fastest but ignores most temporal information; Mean-Pooling uses all frames equally; Attention-Pooling learns frame importance but requires more training
- Failure signatures:
  - Overfitting: Training accuracy much higher than validation/test accuracy, especially with Attention-Pooling
  - Vanishing gradients: Poor learning with long sequences, particularly for modulation spectrograms with longer windows
  - Poor attention learning: Attention weights become uniform or don't change meaningfully across frames
- First 3 experiments:
  1. Implement and train the basic LSTM system with log-mel spectrograms only, using no pooling to establish baseline performance
  2. Add Mean-Pooling to the same system and compare accuracy improvements
  3. Implement Attention-Pooling and verify that attention weights vary meaningfully across frames and correlate with intelligibility-relevant events

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the attention mechanism in the LSTM model prioritize temporal frames for speech intelligibility classification, and can its decision-making process be further interpreted or visualized?
- Basis in paper: [explicit] The paper discusses the attention mechanism in the LSTM framework and its effectiveness in emphasizing the most relevant frames for the task of speech intelligibility prediction. It also provides an example of attention weights for both log-mel and modulation spectrograms, showing how the mechanism assigns higher weights to certain segments, including pauses or speech artifacts.
- Why unresolved: While the paper demonstrates the attention mechanism's effectiveness, it does not delve into the interpretability of the attention weights or provide a detailed analysis of how the mechanism prioritizes frames based on their relevance to intelligibility.
- What evidence would resolve it: Further analysis of attention weights across a broader range of utterances, including visualizations of the weights over time, could provide insights into the decision-making process of the attention mechanism. Additionally, comparing the attention weights to human annotations of intelligibility-relevant segments could validate the model's focus.

### Open Question 2
- Question: Can the proposed fusion strategies (late fusion and WP fusion) be extended to incorporate additional feature types beyond log-mel and modulation spectrograms, and how would this impact the system's performance?
- Basis in paper: [inferred] The paper explores two fusion strategies for combining log-mel and modulation spectrograms, showing that both approaches improve performance over single-feature systems. However, it does not investigate the potential benefits of incorporating additional feature types or the scalability of the fusion strategies.
- Why unresolved: The paper's focus is on the combination of two specific feature types, and it does not explore the potential of integrating other relevant features, such as pitch or formant information, into the fusion framework.
- What evidence would resolve it: Experiments incorporating additional feature types into the fusion strategies, followed by an analysis of the impact on classification accuracy and computational complexity, could determine the benefits and limitations of extending the fusion approaches.

### Open Question 3
- Question: How does the performance of the proposed LSTM-based system vary across different dysarthric speech severities, and can the model be adapted to provide more granular intelligibility assessments?
- Basis in paper: [inferred] The paper evaluates the system on the UA-Speech database, which contains dysarthric speech with varying degrees of severity. However, it does not provide a detailed analysis of the system's performance across different severity levels or explore the possibility of adapting the model for more nuanced intelligibility assessments.
- Why unresolved: The paper's evaluation focuses on overall classification accuracy across three intelligibility levels (low, medium, high) but does not investigate how the system performs on specific severity subgroups or the potential for refining the intelligibility scale.
- What evidence would resolve it: A detailed analysis of the system's performance on different severity subgroups, along with experiments to adapt the model for a more granular intelligibility scale, could provide insights into the system's robustness and potential for refinement.

## Limitations
- The UA-Speech database contains only 15 speakers, limiting generalizability to broader dysarthric populations
- Three intelligibility levels (low/medium/high) may oversimplify the continuous nature of intelligibility impairment
- The study doesn't explore alternative fusion strategies or attention mechanisms that might yield further improvements

## Confidence

**High Confidence:** The core finding that LSTM networks with attention pooling outperform SVM baselines and basic LSTM approaches is well-supported by the experimental results. The ablation studies clearly demonstrate the benefit of each architectural component.

**Medium Confidence:** The claim that modulation spectrograms provide complementary information to log-mel spectrograms is supported by the fusion results, but the exact nature of this complementarity and whether it generalizes to other speech disorders remains to be fully established.

**Low Confidence:** The assertion that per-frame spectrograms preserve more temporal information than summary features is theoretically sound but lacks direct empirical comparison to summary-based approaches within this study.

## Next Checks
1. **Cross-database validation:** Evaluate the trained model on a different dysarthric speech corpus (e.g., TORGO or MND database) to assess generalizability beyond the UA-Speech database.

2. **Alternative fusion strategies:** Implement and compare additional fusion methods such as attention-based feature weighting or transformer-based fusion to determine if WP fusion is optimal or if better strategies exist.

3. **Temporal resolution analysis:** Systematically vary the frame shift and window size parameters for both log-mel and modulation spectrograms to identify optimal temporal resolutions for intelligibility classification and understand the trade-offs between short-term and long-term feature extraction.