---
ver: rpa2
title: 'ShowUI: One Vision-Language-Action Model for GUI Visual Agent'
arxiv_id: '2411.17465'
source_url: https://arxiv.org/abs/2411.17465
tags:
- visual
- action
- data
- arxiv
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces ShowUI, a vision-language-action model for\
  \ GUI automation that addresses challenges in modeling high-resolution screenshots,\
  \ managing interleaved vision-language-action data, and curating high-quality training\
  \ data. The model employs UI-guided visual token selection, which reduces redundant\
  \ visual tokens by 33% and speeds up training by 1.4\xD7 through a UI-connected\
  \ graph in RGB space that identifies and prunes redundancy while preserving positional\
  \ information."
---

# ShowUI: One Vision-Language-Action Model for GUI Visual Agent

## Quick Facts
- arXiv ID: 2411.17465
- Source URL: https://arxiv.org/abs/2411.17465
- Authors: Kevin Qinghong Lin; Linjie Li; Difei Gao; Zhengyuan Yang; Shiwei Wu; Zechen Bai; Weixian Lei; Lijuan Wang; Mike Zheng Shou
- Reference count: 40
- One-line primary result: Achieves 75.1% zero-shot grounding accuracy on Screenspot benchmark while reducing visual tokens by 33% and speeding training by 1.4×

## Executive Summary
ShowUI is a vision-language-action model designed for GUI automation that addresses key challenges in processing high-resolution screenshots, managing interleaved vision-language-action data, and curating high-quality training data. The model introduces UI-guided visual token selection that reduces redundant visual tokens by 33% while preserving positional information, enabling faster training without sacrificing performance. It also employs interleaved vision-language-action streaming to effectively handle multi-step navigation history and visual grounding tasks across diverse GUI environments including web, mobile, and desktop interfaces.

## Method Summary
ShowUI enhances the Qwen2-VL-2B backbone with UI-guided visual token selection and interleaved vision-language-action streaming. The token selection mechanism constructs a UI-connected graph in RGB space to identify and prune redundant patches while maintaining positional relationships. The model is trained on a carefully curated dataset of 256K samples using balanced sampling across device types and diverse query formulations. LoRA adapters enable efficient fine-tuning while preserving the core VLM capabilities. The architecture processes high-resolution UI screenshots through patchification, token selection, and interleaved action history before generating structured JSON actions for GUI interaction.

## Key Results
- Achieves 75.1% zero-shot grounding accuracy on Screenspot benchmark
- Reduces redundant visual tokens by 33% while speeding training by 1.4×
- Demonstrates competitive performance across web (Mind2Web), mobile (AITW), and online (MiniWob) navigation tasks
- Shows strong generalization in cross-device and cross-task settings with balanced dataset curation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UI-guided visual token selection reduces redundant tokens while preserving positional relationships essential for grounding.
- Mechanism: Constructs a UI-connected graph in RGB space where patches with identical RGB values are grouped into connected components. During training, a subset of tokens within each component is randomly selected, maintaining positional embeddings and reducing computational load.
- Core assumption: UI screenshots contain redundant regions (e.g., whitespace, uniform backgrounds) that can be identified via RGB similarity without losing semantic or functional information.
- Evidence anchors: Abstract and section descriptions confirm the 33% reduction and 1.4× speedup claims. Weak corpus support for UI-connected graph approach.

### Mechanism 2
- Claim: Interleaved vision-language-action streaming effectively manages multi-step navigation history and improves grounding efficiency.
- Mechanism: Combines action history with visual context in a sequential stream. For navigation, past screenshots and actions are interleaved so the model can reason over complete trajectories. For grounding, multi-turn queries are paired with single screenshots to optimize data usage.
- Core assumption: GUI navigation and grounding benefit from temporal context, and interleaving modalities prevents information loss that would occur if modalities were processed independently.
- Evidence anchors: Abstract and section descriptions validate the streaming approach. Weak corpus support for interleaved VLA streaming in GUI context.

### Mechanism 3
- Claim: Curated, balanced instruction-following dataset improves grounding performance with fewer samples.
- Mechanism: Selects high-quality visual elements (buttons, checkboxes) over static text; augments desktop data with diverse query types (appearance, spatial, intention) via GPT-4o; balances data distribution across device types using resampling.
- Core assumption: Visual elements are more informative for grounding than static text; diverse query formulations improve model generalization; balanced data exposure prevents overfitting to dominant device types.
- Evidence anchors: Abstract and section descriptions support the curation strategy. Moderate corpus support for similar data curation strategies in multimodal training.

## Foundational Learning

- Concept: Vision-language model tokenization and self-attention
  - Why needed here: ShowUI extends a VLM, so understanding how visual patches are tokenized and processed through self-attention is essential for modifying the architecture (e.g., token selection, layer insertion).
  - Quick check question: How does the model convert a 2K screenshot into tokens, and where in the forward pass can token pruning occur without breaking positional encoding?

- Concept: Graph-based image segmentation (connected components)
  - Why needed here: The UI-connected graph relies on identifying connected components in RGB space. Engineers must understand how Union-Find or BFS/DFS can construct these components efficiently.
  - Quick check question: Given a patch grid, how do you group adjacent patches with similar RGB values into connected components, and how does patch size affect granularity?

- Concept: Action space design and JSON structuring
  - Why needed here: ShowUI handles device-specific actions via structured JSON. Engineers must design consistent action schemas that can generalize across web, mobile, and desktop environments.
  - Quick check question: How do you represent a "SCROLL" action that has different parameters on web (2 directions) vs. mobile (4 directions) in a unified JSON format?

## Architecture Onboarding

- Component map: Screenshot → patchify (28x28 grid) → UI-connected graph construction → token selection/masking → Core model (Qwen2-VL-2B + LoRA) → Streaming manager → Data sampler → Output JSON action
- Critical path: 1) Preprocess screenshot → build UI graph → select tokens 2) Pass selected tokens + actions + query through VLM 3) Predict next action (type, value, position) 4) Update environment screenshot and repeat
- Design tradeoffs: Token selection ratio vs. accuracy (higher pruning speeds training but risks losing fine-grained UI elements); Interleaving depth vs. memory (deeper history improves navigation but increases sequence length); Data balance vs. domain specificity (balanced sampling aids generalization but may dilute device-specific expertise)
- Failure signatures: High token pruning → missed small icons or text; Insufficient visual history → navigation errors on mobile; Imbalanced data → poor grounding on underrepresented devices
- First 3 experiments: 1) Ablation: Compare baseline Qwen2-VL-2B vs. ShowUI with UI token selection on Screenspot grounding (measure accuracy and training speed) 2) Ablation: Test interleaved vs. non-interleaved streaming on AITW navigation (measure step success rate) 3) Ablation: Vary token selection ratio (0.25, 0.5, 0.75) and measure tradeoff between speedup and grounding accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can online reinforcement learning be effectively integrated into ShowUI to handle novel error cases and improve zero-shot performance in dynamic GUI environments?
- Basis in paper: The authors note that ShowUI's zero-shot performance on MiniWob is significantly lower than its fine-tuned performance, suggesting that offline instruction-tuning alone is insufficient. They mention that future work should focus on developing a learning strategy tailored for online environments to handle novel error cases.
- Why unresolved: The paper does not provide any experimental results or detailed methodology for integrating reinforcement learning into ShowUI. The authors only suggest it as a future direction without exploring its potential impact.
- What evidence would resolve it: Experimental results comparing ShowUI with and without reinforcement learning in online environments, along with a detailed analysis of how reinforcement learning improves handling of novel error cases and zero-shot performance.

### Open Question 2
- Question: What is the optimal balance between visual and action history in interleaved streaming for different GUI tasks and devices?
- Basis in paper: The authors discuss the importance of interleaved vision-language-action streaming and how it improves navigation performance, particularly on mobile devices. However, they also mention that the significance of visual context varies across tasks, such as being less critical for web navigation on Mind2Web compared to mobile navigation on AITW.
- Why unresolved: The paper does not provide a systematic study on the optimal balance between visual and action history for different GUI tasks and devices. The authors only present anecdotal evidence and general observations without a comprehensive analysis.
- What evidence would resolve it: A detailed ablation study varying the amount of visual and action history in interleaved streaming across different GUI tasks and devices, along with a quantitative analysis of the impact on performance.

### Open Question 3
- Question: How can ShowUI's grounding performance be further improved for icon recognition across different platforms?
- Basis in paper: The authors note that ShowUI's icon grounding performance is lower than its text grounding performance, and that mobile icon grounding scores are significantly higher than desktop and web. They suggest that the lack of visual UI grounding data beyond mobile devices is a limiting factor.
- Why unresolved: The paper does not provide any specific strategies or experiments for improving icon grounding performance across different platforms. The authors only identify the problem and suggest the need for more diverse visual grounding data.
- What evidence would resolve it: Experimental results showing the impact of collecting and incorporating additional visual grounding data for desktop and web platforms, along with a detailed analysis of the improvements in icon grounding performance across different platforms.

## Limitations

- Limited empirical validation of UI-connected graph design against alternative token pruning methods
- Data curation methodology gaps regarding quality control and synthetic query rejection rates
- Unclear separation between navigation and grounding task optimization in the same model

## Confidence

**High confidence:** The UI-guided visual token selection mechanism works as described - the connected component approach is technically sound and the reported speed gains are plausible given the 33% token reduction.

**Medium confidence:** The grounding performance claims (75.1% zero-shot accuracy on Screenspot) are credible but would benefit from comparison against more recent baselines.

**Low confidence:** The assertion that a "small-scale, high-quality dataset" outperforms larger datasets lacks rigorous comparison against models trained on more extensive data.

## Next Checks

1. **Token selection ablation with alternative methods:** Compare UI-connected graph pruning against vision transformer-specific token merging (e.g., V-Merge, PoolFormer) and attention-based token sparsification on the same grounding benchmark, measuring both accuracy and computational efficiency.

2. **Dataset size scaling experiment:** Train ShowUI on progressively larger datasets (64K → 128K → 256K → 512K samples) while keeping the curation methodology constant, then evaluate grounding accuracy to determine if the claimed quality-over-quantity advantage holds.

3. **Cross-task weight transferability test:** Train separate models for grounding and navigation tasks, then measure performance when transferring weights between tasks (grounding → navigation and vice versa) to determine if interleaved streaming truly enables unified capability or if task-specific optimization is required.