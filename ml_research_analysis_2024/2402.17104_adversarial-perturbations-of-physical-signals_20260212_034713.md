---
ver: rpa2
title: Adversarial Perturbations of Physical Signals
arxiv_id: '2402.17104'
source_url: https://arxiv.org/abs/2402.17104
tags:
- signals
- signal
- perturbations
- adversarial
- physical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of generating physically realizable
  adversarial perturbations for computer-vision-based signal classifiers under physical
  access constraints. The authors model the physical propagation of acoustic signals
  using partial differential equations (PDEs) and formulate an optimization problem
  to compute interfering signals that cause misclassification of spectrograms while
  keeping perturbations imperceptible.
---

# Adversarial Perturbations of Physical Signals

## Quick Facts
- arXiv ID: 2402.17104
- Source URL: https://arxiv.org/abs/2402.17104
- Authors: Robert L. Bassett; Austin Van Dellen; Anthony P. Austin
- Reference count: 9
- Primary result: Physical adversarial perturbations can consistently fool spectrogram classifiers with signals 10-20 times weaker than original

## Executive Summary
This paper addresses the challenge of generating physically realizable adversarial perturbations for computer-vision-based signal classifiers under physical access constraints. The authors model acoustic signal propagation using partial differential equations and formulate an optimization problem to compute interfering signals that cause misclassification while keeping perturbations imperceptible. By exploiting the structure of the discretized PDE system, they achieve significant computational speedups while maintaining effectiveness across multiple neural network architectures.

## Method Summary
The approach models acoustic propagation using the linear wave equation, where a submarine emits signal g(t) and an interferer emits perturbing signal f(t). The optimization problem finds f(t) that causes misclassification of the received signal's spectrogram by a neural network classifier. The key innovation is an efficient solution method that pre-computes the influence of the entire PDE system on the detector's output offline, avoiding repeated expensive PDE solves during optimization. The method is validated using spectrograms from sine waves classified by VGG-19, Inception V3, and GoogLeNet neural networks.

## Key Results
- Adversarial perturbations consistently fool classifiers with high confidence across all three neural network architectures
- Perturbations can be 10-20 times weaker than the original signal while maintaining effectiveness
- Computational method achieves 60-140x speedups compared to naive approaches
- Success rate varies by model: VGG-19 (85%), Inception V3 (95%), GoogLeNet (85%) on validation data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial perturbations can be physically realized by exploiting the wave equation's linearity and superposition
- Mechanism: The interfering signal f(t) and source signal g(t) are modeled as point sources in the linear wave equation. Due to linearity, their contributions to the received signal s(t) can be computed separately and summed. This allows the optimization to focus solely on f(t) while keeping g(t) fixed, simplifying the problem.
- Core assumption: The wave equation accurately models physical propagation and the medium behaves linearly without significant nonlinear effects
- Break condition: If the medium exhibits significant nonlinearity or the wave equation is a poor approximation, the linear superposition assumption fails

### Mechanism 2
- Claim: The PDE-constrained optimization problem can be solved efficiently by exploiting the discretized system's structure
- Mechanism: The detector's output depends only on the received signal at a single point (xd). This allows pre-computing the influence of the entire PDE system on this point offline, avoiding repeated expensive PDE solves during optimization. The structure of the discretized system (block upper triangular, block Toeplitz) further enables efficient computation of this influence.
- Core assumption: The detector's location is fixed, allowing pre-computation of the system's influence on the received signal
- Break condition: If the detector's location changes dynamically or the PDE system's structure is altered, the pre-computation approach becomes invalid

### Mechanism 3
- Claim: The adversarial perturbations are effective because they exploit the vulnerability of deep neural networks to small input changes
- Mechanism: Deep neural networks are sensitive to small perturbations in their inputs. By crafting a perturbing signal that causes small changes to the spectrogram, the optimization can induce large changes in the network's classification output, leading to misclassification.
- Core assumption: The neural network classifier is susceptible to adversarial examples, and the spectrogram representation preserves the features that the network relies on for classification
- Break condition: If the neural network is trained with adversarial examples or uses a different representation that is less sensitive to small input changes

## Foundational Learning

- Concept: Partial Differential Equations (PDEs) and their numerical solution using finite element methods
  - Why needed here: The physical propagation of acoustic signals is modeled using the wave equation, a PDE. Numerical methods are required to solve this equation and compute the received signal at the detector.
  - Quick check question: What are the key steps in solving a PDE numerically using the finite element method?

- Concept: Deep Neural Networks and their vulnerability to adversarial examples
  - Why needed here: The detector uses a pre-trained neural network to classify spectrograms. Understanding the network's architecture and its susceptibility to adversarial perturbations is crucial for designing effective attacks.
  - Quick check question: What are some common techniques for generating adversarial examples for deep neural networks?

- Concept: Optimization methods for constrained problems
  - Why needed here: The problem of finding an effective perturbing signal is formulated as a PDE-constrained optimization problem. Knowledge of optimization techniques, such as gradient descent and the adjoint method, is essential for solving this problem efficiently.
  - Quick check question: What is the adjoint method, and how is it used in PDE-constrained optimization?

## Architecture Onboarding

- Component map: Source -> Medium -> Detector -> Neural Network -> Optimization Engine -> Interferer
- Critical path: 1) Source emits signal g(t), 2) Interferer emits perturbing signal f(t), 3) Signals propagate through medium according to wave equation, 4) Detector receives combined signal s(t), 5) Neural network classifies spectrogram of s(t), 6) Optimization engine adjusts f(t) to induce misclassification
- Design tradeoffs: Computational efficiency vs. physical realism, perturbation strength vs. perceptibility, generalization vs. specificity
- Failure signatures: Optimization fails to converge or produces ineffective perturbations, perturbing signal has large amplitude or introduces detectable artifacts, perturbations are not robust to environmental noise or changes in physical setup
- First 3 experiments: 1) Verify PDE solver by comparing computed received signal with analytical solution for simple case, 2) Test neural network's vulnerability to adversarial examples by generating perturbations in spectrogram domain, 3) Validate efficiency of pre-computation approach by comparing computation time with and without pre-computation for small-scale problem

## Open Questions the Paper Calls Out
- How does the effectiveness of adversarial perturbations change when the submarine and interferer are mobile rather than fixed in position?
- Can universal adversarial perturbations be constructed that work across different noise levels and signal types?
- How would using more complex wave equations (e.g., incorporating nonlinearity or higher dimensions) affect the feasibility and effectiveness of adversarial perturbations?

## Limitations
- The approach relies heavily on linear wave equation assumptions that may not hold in real-world scenarios with complex acoustic environments
- Computational efficiency claims depend on the specific discretized system structure, which may not generalize to other PDE models or detector configurations
- Experiments focus on spectrogram classification of sine waves, limiting generalizability to more complex real-world signals

## Confidence
- High Confidence: The mathematical framework for PDE-constrained optimization and the structural exploitation for computational efficiency
- Medium Confidence: The effectiveness of adversarial perturbations against neural network classifiers, given the well-established vulnerability of deep networks to adversarial examples
- Medium Confidence: The practical feasibility of implementing these attacks in real-world scenarios, given the controlled experimental conditions

## Next Checks
1. Test the perturbations' robustness against environmental noise and different acoustic propagation conditions by introducing varying levels of background noise and non-ideal transmission media
2. Validate the approach with more complex, non-sinusoidal signals (speech, music, or multi-frequency sources) to assess real-world applicability
3. Evaluate the transferability of perturbations across different neural network architectures beyond the three tested models to understand generalization limitations