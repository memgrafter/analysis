---
ver: rpa2
title: 'FedPT: Federated Proxy-Tuning of Large Language Models on Resource-Constrained
  Edge Devices'
arxiv_id: '2410.00362'
source_url: https://arxiv.org/abs/2410.00362
tags:
- generation
- fedavg
- large
- fedpt
- title
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedPT introduces a federated proxy-tuning framework that enables
  efficient, privacy-preserving fine-tuning of large language models on resource-constrained
  devices without requiring direct access to model parameters. The approach involves
  collaborative tuning of smaller LMs by edge devices, followed by proxy-tuning and
  knowledge distillation on the server to construct a high-performance large proxy-tuned
  model.
---

# FedPT: Federated Proxy-Tuning of Large Language Models on Resource-Constrained Edge Devices

## Quick Facts
- **arXiv ID**: 2410.00362
- **Source URL**: https://arxiv.org/abs/2410.00362
- **Authors**: Zhidong Gao; Yu Zhang; Zhenxiao Zhang; Yanmin Gong; Yuanxiong Guo
- **Reference count**: 40
- **Primary result**: FedPT achieves 36-44% lower VRAM usage and 36-40% reduced communication costs while maintaining competitive performance to directly federated fine-tuning of large models

## Executive Summary
FedPT introduces a federated proxy-tuning framework that enables efficient, privacy-preserving fine-tuning of large language models on resource-constrained devices without requiring direct access to model parameters. The approach involves collaborative tuning of smaller LMs by edge devices, followed by proxy-tuning and knowledge distillation on the server to construct a high-performance large proxy-tuned model. Experimental results show that FedPT significantly reduces memory, computation, and communication overhead while maintaining competitive performance compared to directly federated fine-tuning of large models. Across tasks like instruction following, FedPT consistently matched or exceeded baseline performance, demonstrating its scalability and effectiveness in FL environments with non-IID data distributions.

## Method Summary
FedPT is a federated learning framework that enables large language model fine-tuning on resource-constrained edge devices. The method works in three phases: (1) devices collaboratively tune smaller language models using their local private data, (2) the server constructs a large proxy-tuned model by applying a logit offset derived from the difference between small fine-tuned and pre-trained models to the large pre-trained model, and (3) knowledge distillation transfers knowledge from the large proxy-tuned model back to the small aggregated model. This iterative process continues until convergence. The framework uses LoRA adapters for efficient local fine-tuning and handles non-IID data distributions across devices.

## Key Results
- FedPT achieves 36-44% lower VRAM usage compared to directly federated fine-tuning of large models
- Communication costs reduced by 36-40% due to smaller model parameter updates
- Performance on instruction following tasks matches or exceeds directly federated fine-tuning baselines
- Maintains effectiveness under pathological non-IID data distributions with α = 0.8 parameter setting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FedPT achieves performance comparable to directly federated fine-tuning of large LMs by leveraging proxy-tuning to guide the large model's predictions using the difference between small fine-tuned and pre-trained models.
- Mechanism: The large proxy-tuned model is constructed by adding a logit offset, defined as the difference between logits from the small fine-tuned LM and the small pre-trained LM, to every token of the large pre-trained model. This effectively shifts the original predictions of the larger pre-trained LM in the direction of tuning.
- Core assumption: The difference in logits between the small fine-tuned and pre-trained models captures the essential knowledge gained from fine-tuning, and this difference can be used to guide the larger model's predictions without requiring access to the larger model's parameters.
- Evidence anchors:
  - [abstract]: "devices in FedPT first collaboratively tune a smaller LM, and then the server combines the knowledge learned by the tuned small LM with the knowledge learned by the larger pre-trained LM to construct a large proxy-tuned LM that can reach the performance of directly tuned large LMs."
  - [section]: "Specifically, as demonstrated in Figure 1, devices first collaboratively tune a smaller LM based on their private data. Then, with the small fine-tuned LM, the cloud server constructs a large proxy-tuned LM by leveraging the difference between the predictions of the small pre-trained and fine-tuned LMs (Liu et al. 2024a; Mitchell et al. 2023) to shift the original predictions of the larger pre-trained LM in the direction of tuning."
  - [corpus]: Weak evidence; the related papers focus on zero-order optimization and model pruning, not proxy-tuning.
- Break condition: If the small fine-tuned model fails to capture meaningful knowledge, the proxy-tuning approach will not effectively guide the large model's predictions.

### Mechanism 2
- Claim: Knowledge distillation from the large proxy-tuned model to the small aggregated model ensures continuous improvement of the small model's performance, leading to better overall performance.
- Mechanism: After constructing the large proxy-tuned model, the server uses knowledge distillation to transfer the knowledge from the large proxy-tuned model to the small aggregated model. This process is repeated iteratively to improve the small model's performance.
- Core assumption: The large proxy-tuned model contains valuable knowledge that can be effectively distilled into the small model, and this distillation process leads to improved performance on the downstream task.
- Evidence anchors:
  - [abstract]: "After that, the server leverages knowledge distillation to transfer the knowledge from the large proxy-tuned LM to the small aggregated LM to obtain an updated small LM for further training."
  - [section]: "Therefore, we further leverage the knowledge distillation (Sanh et al. 2019; Muhamed et al. 2021; Song et al. 2020) to transfer the general knowledge from the teacher model (i.e., large proxy-tuned LM ˜θl) to the student model (i.e., small LM θs) in each training round."
  - [corpus]: Weak evidence; the related papers focus on model pruning and gradient quantization, not knowledge distillation.
- Break condition: If the knowledge distillation process fails to effectively transfer knowledge from the large proxy-tuned model to the small model, the iterative improvement process will be hindered.

### Mechanism 3
- Claim: FedPT significantly reduces computation, communication, and memory overhead compared to directly federated fine-tuning of large LMs by only requiring the fine-tuning of smaller models on edge devices.
- Mechanism: Each device only needs to tune a smaller LM, which has significantly lower memory, computation, and communication requirements compared to fine-tuning a large LM. The server then aggregates the small models and constructs the large proxy-tuned model.
- Core assumption: Fine-tuning smaller models is significantly more efficient than fine-tuning large models in terms of memory, computation, and communication requirements.
- Evidence anchors:
  - [abstract]: "Experimental results show that FedPT significantly reduces memory, computation, and communication overhead—achieving 36-44% lower VRAM usage and 36-40% reduced communication costs—while maintaining competitive performance compared to directly federated fine-tuning of large models."
  - [section]: "As shown in Table 1, compared to FedAvg with LLaMA-13B, both FedPT and FedAvg+PT achieve a 44% reduction in VRAM usage and a 36% reduction in communications costs."
  - [corpus]: Weak evidence; the related papers focus on zero-order optimization and model pruning, not the specific efficiency gains of FedPT.
- Break condition: If the efficiency gains from fine-tuning smaller models are not sufficient to outweigh the overhead of the proxy-tuning and knowledge distillation processes, the overall efficiency benefits of FedPT may be diminished.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: FedPT is a federated learning framework, and understanding the basics of FL is crucial for understanding how FedPT works.
  - Quick check question: What are the key challenges in federated learning, and how does FedPT address these challenges?
- Concept: Knowledge Distillation
  - Why needed here: FedPT uses knowledge distillation to transfer knowledge from the large proxy-tuned model to the small aggregated model. Understanding the principles of knowledge distillation is essential for understanding how FedPT improves the small model's performance.
  - Quick check question: How does knowledge distillation work, and what are the key factors that influence its effectiveness?
- Concept: Proxy-Tuning
  - Why needed here: FedPT uses proxy-tuning to guide the large model's predictions using the difference between small fine-tuned and pre-trained models. Understanding the principles of proxy-tuning is crucial for understanding how FedPT achieves performance comparable to directly federated fine-tuning of large LMs.
  - Quick check question: How does proxy-tuning work, and what are the key factors that influence its effectiveness?

## Architecture Onboarding

- Component map:
  - Edge devices → Tune smaller LMs using local data
  - Server → Aggregate small models, construct large proxy-tuned model, perform knowledge distillation
  - Small pre-trained LM → Shared among all devices and server
  - Large pre-trained LM → Only used on server
- Critical path: Edge devices → Server aggregation → Large proxy-tuned model construction → Knowledge distillation → Updated small model → Next round
- Design tradeoffs:
  - Using smaller models on edge devices reduces resource requirements but may limit the model's capacity
  - Proxy-tuning allows for performance comparable to directly fine-tuning large models without requiring access to their parameters, but it may introduce additional complexity
  - Knowledge distillation ensures continuous improvement of the small model's performance but may require additional computational resources
- Failure signatures:
  - Poor performance on downstream tasks: The small fine-tuned models may not be capturing meaningful knowledge, or the proxy-tuning and knowledge distillation processes may not be effective
  - High resource usage: The proxy-tuning and knowledge distillation processes may be introducing significant overhead, negating the efficiency gains from using smaller models on edge devices
  - Communication bottlenecks: The aggregation of small models or the transfer of knowledge from the large proxy-tuned model to the small model may be introducing communication bottlenecks
- First 3 experiments:
  1. Implement the FedPT framework and evaluate its performance on a simple task with a small dataset
  2. Compare the performance of FedPT to directly federated fine-tuning of large models on a more complex task with a larger dataset
  3. Evaluate the resource usage of FedPT compared to directly federated fine-tuning of large models on a variety of tasks and datasets

## Open Questions the Paper Calls Out

- What is the optimal rank (r) for LoRA adapters in FedPT when scaling to trillion-parameter models?
  - Basis in paper: The paper uses ranks of 4 for GPT-2 models and 8 for LLaMA models, but notes these values may need adjustment for larger models.
  - Why unresolved: The paper does not explore how the optimal LoRA rank scales with model size or how this affects convergence speed and final performance.
  - What evidence would resolve it: Systematic experiments varying the LoRA rank across different model scales while measuring convergence time, memory usage, and final task performance.

- How does FedPT perform under extreme non-IID data distributions where devices have completely disjoint label sets?
  - Basis in paper: The paper evaluates pathological non-IID and Dirichlet distributions, but these still maintain some overlap in label space across devices.
  - Why unresolved: The paper does not test scenarios where devices have entirely non-overlapping data distributions, which could reveal fundamental limitations of the knowledge distillation approach.
  - What evidence would resolve it: Experiments with devices having completely disjoint task categories and measuring how FedPT's proxy-tuning mechanism handles such extreme heterogeneity.

- What is the theoretical relationship between the proxy-tuning weight α and the KL divergence penalty in the original fine-tuning objective?
  - Basis in paper: The paper shows empirical results for different α values but does not establish a theoretical connection between α and the β parameter in the KL-penalized RL objective.
  - Why unresolved: The paper observes that α affects model behavior but does not explain the mathematical relationship between this empirical tuning parameter and the theoretical fine-tuning framework.
  - What evidence would resolve it: Mathematical derivation showing how α relates to β in the KL-penalized objective, potentially through analysis of the partition function approximations used in proxy-tuning.

## Limitations

- The efficiency gains (36-44% VRAM reduction, 36-40% communication savings) are based on simulated experiments with specific hardware configurations not fully detailed
- The effectiveness of proxy-tuning depends on the assumption that small model fine-tuning captures transferable knowledge, but limited ablation studies explore failure scenarios
- Knowledge distillation effectiveness is only evaluated through downstream task performance without direct analysis of what knowledge is being transferred

## Confidence

- **High confidence**: The overall FedPT framework architecture and the general principle that smaller models require fewer resources for fine-tuning
- **Medium confidence**: The specific efficiency gains (36-44% VRAM reduction, 36-40% communication savings) and the proxy-tuning mechanism's ability to maintain performance parity with direct large model fine-tuning
- **Low confidence**: The robustness of FedPT under extreme non-IID data distributions and the long-term stability of the iterative knowledge distillation process

## Next Checks

1. **Resource Usage Validation**: Implement FedPT on actual resource-constrained edge devices (not just simulation) to verify the claimed VRAM and communication cost reductions under realistic operating conditions.

2. **Proxy-Tuning Robustness Test**: Systematically evaluate FedPT's performance when the small fine-tuned models fail to capture meaningful knowledge by introducing controlled noise or limiting training data to verify the break conditions identified in the mechanism analysis.

3. **Non-IID Distribution Stress Test**: Evaluate FedPT across a spectrum of data heterogeneity levels beyond the moderate non-IID setup used in the paper to identify the framework's breaking points and characterize its robustness to extreme data skew scenarios.