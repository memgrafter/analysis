---
ver: rpa2
title: Question answering systems for health professionals at the point of care --
  a systematic review
arxiv_id: '2402.01700'
source_url: https://arxiv.org/abs/2402.01700
tags:
- question
- answering
- biomedical
- systems
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review examined 79 biomedical question answering
  (QA) systems to assess their clinical utility. The review found that most systems
  were built by computer scientists with limited clinical input and focused on accuracy
  rather than real-world usefulness.
---

# Question answering systems for health professionals at the point of care -- a systematic review

## Quick Facts
- arXiv ID: 2402.01700
- Source URL: https://arxiv.org/abs/2402.01700
- Reference count: 40
- Most biomedical QA systems focus on accuracy rather than real-world clinical usefulness

## Executive Summary
This systematic review examined 79 biomedical question answering systems to assess their clinical utility. The review found that while machine learning methods have improved system accuracy over time, most systems fail to meet real clinical needs. Key issues include unrealistic question datasets, lack of clinician input during development, and failure to communicate answer reliability or uncertainty. Only a small fraction of systems fully met clinical utility criteria, and even fewer reported user evaluations. The review emphasizes the need for more realistic clinical datasets and evaluation methods that consider reliability and transparency beyond just accuracy metrics.

## Method Summary
The review systematically searched PubMed, IEEE Xplore, ACM Digital Library, and ACL Anthology for biomedical QA systems using the query: ("question answering" OR "question-answering") AND (clinic* OR medic* OR biomedic* OR health*). After screening titles and abstracts, full-text review was conducted on potentially relevant papers, extracting data on methodology, datasets, evaluation methods, and findings. The review followed PRISMA/SWiM guidelines with narrative synthesis and risk of bias assessment using an adapted PROBAST framework. Systems were evaluated against predefined clinical utility criteria including reliable sources, contextual relevance, and uncertainty communication.

## Key Results
- Most biomedical QA systems were developed by computer scientists with limited clinical input, focusing on technical accuracy rather than clinical utility
- Only 8 of 79 studies fully met criteria for clinical utility, and just 7 reported user evaluations
- Systems commonly used unrealistic question datasets and failed to communicate answer confidence or source reliability
- Machine learning methods improved accuracy over time, but systems still poorly reflected real clinical information needs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Most biomedical QA systems perform well on accuracy benchmarks but poorly in real clinical settings because they are trained on unrealistic question datasets.
- Mechanism: The systems learn to answer questions that are simpler and more narrowly focused than the complex, contextual questions clinicians actually ask during patient care. This leads to high accuracy scores on benchmark datasets while failing to generalize to real-world needs.
- Core assumption: The quality of training data directly determines the real-world utility of the system.
- Evidence anchors:
  - [abstract] "Most systems were built by computer scientists with limited clinical input and focused on accuracy rather than real-world usefulness."
  - [section] "Many studies used unrealistically simple questions or covered too few information needs for a general biomedical QA system."
  - [corpus] Corpus shows emerging work on "realistic clinical questions" suggesting prior datasets were limited.
- Break condition: If future QA systems use high-quality, clinician-generated datasets that reflect real clinical complexity, this mechanism would no longer explain the gap.

### Mechanism 2
- Claim: Systems fail to communicate answer reliability and confidence, which undermines clinical trust and adoption.
- Mechanism: Without transparency about source reliability or answer certainty, clinicians cannot assess whether to act on the information. This lack of trust prevents adoption even when the underlying accuracy is adequate.
- Core assumption: Clinicians require explicit reliability indicators to trust and act on QA system outputs.
- Evidence anchors:
  - [abstract] "No system communicated confidence levels in the answers or sources."
  - [section] "Key research priorities include developing more realistic healthcare QA datasets and considering the reliability of answer sources, rather than merely focusing on accuracy."
  - [corpus] Corpus neighbors mention "reliable text-to-SQL modeling" and "realistic clinical questions," supporting the theme of reliability gaps.
- Break condition: If systems begin incorporating explicit reliability metrics and confidence scores that clinicians find trustworthy, this mechanism would no longer hold.

### Mechanism 3
- Claim: The lack of clinician involvement in system design leads to misalignment between system outputs and actual clinical needs.
- Mechanism: Computer scientists design systems optimized for technical metrics without understanding clinical workflows, information needs, or decision-making contexts. This results in systems that technically work but are clinically unusable.
- Core assumption: Clinician input is essential for aligning QA systems with real clinical workflows and needs.
- Evidence anchors:
  - [abstract] "Most systems were built by computer scientists with limited clinical input."
  - [section] "Most of the included studies were method papers describing systems that were built by computer scientists with limited input from clinicians."
  - [corpus] Corpus shows new focus on "multi-agent systems" and "patient-level clinical QA," suggesting prior systems lacked clinical context.
- Break condition: If future systems are co-designed with clinicians from the start, this mechanism would no longer explain the adoption problem.

## Foundational Learning

- Concept: Understanding the difference between technical accuracy metrics and clinical utility
  - Why needed here: Engineers need to recognize that high accuracy scores don't guarantee real-world effectiveness
  - Quick check question: If a system achieves 95% accuracy on BioASQ but clinicians don't use it, what's the likely cause?

- Concept: Question-answering system evaluation methods (ROUGE, BLEU, MAP, etc.)
  - Why needed here: To understand how current evaluation methods may not capture clinical relevance
  - Quick check question: What evaluation metric would best capture whether a QA system provides clinically actionable answers?

- Concept: Clinical information needs and workflow
  - Why needed here: To understand what makes questions "realistic" vs. unrealistic in clinical contexts
  - Quick check question: What's the difference between a BioASQ question and a question a clinician asks during patient rounds?

## Architecture Onboarding

- Component map: Question analysis -> Document retrieval -> Answer extraction -> Answer ranking -> User interface
- Critical path: Document retrieval and answer extraction are most critical for accuracy; user interface is most critical for adoption
- Design tradeoffs: Accuracy vs. explainability, speed vs. comprehensiveness, general vs. specialized domains
- Failure signatures: High accuracy on benchmarks but low clinical adoption; inability to communicate confidence; irrelevant answers to real clinical questions
- First 3 experiments:
  1. Replace benchmark dataset questions with clinician-generated questions and measure accuracy drop
  2. Add explicit reliability scoring to existing system and measure clinician trust scores
  3. Build simple user interface for existing accurate system and measure adoption rates compared to no interface

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific dataset characteristics would make biomedical QA systems more reflective of real clinical information needs?
- Basis in paper: [explicit] The review found that most studies used datasets of factoids/multiple choice questions which do not resemble real-life clinical queries, and called for "high quality datasets derived from real clinical queries"
- Why unresolved: While the paper identifies this need, it does not specify what features such datasets should have (e.g., complexity level, types of questions, clinical context, etc.)
- What evidence would resolve it: Development and evaluation of QA systems using newly created clinical datasets with detailed documentation of their characteristics and comparison of performance to systems trained on existing datasets

### Open Question 2
- Question: How can biomedical QA systems effectively communicate answer confidence levels and source reliability to users?
- Basis in paper: [explicit] The review found that no system communicated confidence levels in answers or sources, and emphasized the need to "consider the reliability of answer sources, rather than merely focusing on accuracy"
- Why unresolved: While the paper identifies this as a key limitation, it does not propose specific methods for how systems should communicate uncertainty or evaluate source reliability
- What evidence would resolve it: Implementation and user testing of different approaches to displaying confidence levels and source reliability, measuring their impact on clinical decision-making

### Open Question 3
- Question: What evaluation metrics beyond accuracy would better assess the clinical utility of biomedical QA systems?
- Basis in paper: [explicit] The review notes that most studies focused on accuracy metrics and only 7 studies evaluated usability or clinical relevance, calling for research to "move beyond maximising accuracy of a model alone"
- Why unresolved: The paper identifies this gap but does not specify which alternative metrics would be most valuable for assessing real-world clinical utility
- What evidence would resolve it: Development and validation of new evaluation frameworks incorporating metrics for answer reliability, clinical actionability, and user trust, with comparison to traditional accuracy metrics

## Limitations
- Small sample size of clinically useful systems (only 8 of 79 studies fully met utility criteria) limits generalizability
- Limited user evaluation studies (only 7 studies reported user testing) means most findings are based on technical rather than clinical validation
- Potential publication bias toward technically-focused studies rather than clinically-evaluated systems

## Confidence
- **High confidence**: The finding that most systems were built by computer scientists with limited clinical input is well-supported by the systematic review methodology and multiple data sources
- **Medium confidence**: The assertion that machine learning methods improved accuracy over time is supported by temporal analysis of included studies, though the clinical impact remains unclear
- **Medium confidence**: The identification of unrealistic question datasets as a key limitation is plausible given the evidence, but specific quantitative data on question realism is limited

## Next Checks
1. Replicate the systematic review with updated search terms including modern large language models and evaluate whether newer systems show improved clinical utility
2. Conduct user studies with clinicians testing existing high-accuracy systems to quantify the gap between technical performance and clinical usability
3. Develop and validate a standardized framework for assessing clinical question realism that can be applied across different biomedical QA datasets