---
ver: rpa2
title: 'Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review'
arxiv_id: '2406.12655'
source_url: https://arxiv.org/abs/2406.12655
tags:
- code
- generation
- language
- generated
- pass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper provides a critical review of benchmarks and metrics
  used to evaluate code generation by large language models (LLMs). It categorizes
  programming tasks, surveys popular benchmarks, and analyzes their construction,
  structure, and limitations.
---

# Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review

## Quick Facts
- arXiv ID: 2406.12655
- Source URL: https://arxiv.org/abs/2406.12655
- Authors: Debalina Ghosh Paul; Hong Zhu; Ian Bayley
- Reference count: 40
- Primary result: Critical review of code generation benchmarks and metrics, identifying limitations in test-based correctness, similarity metrics, and lack of real-world scenario evaluation

## Executive Summary
This paper provides a comprehensive critical review of benchmarks and metrics used to evaluate code generation by large language models. The authors systematically categorize programming tasks, survey popular benchmarks, and analyze their construction, structure, and limitations. The review covers multiple quality attributes including functional correctness, syntactic similarity metrics, and usability considerations. A key finding is that while test-based correctness metrics are widely adopted, they may not adequately capture the real-world usability of generated code. The paper also highlights that existing syntactic similarity metrics like BLEU, ROUGE, and CodeBLEU may not correlate well with human judgment of code quality.

## Method Summary
The paper employs a systematic literature review methodology to analyze existing code generation benchmarks and evaluation metrics. The authors categorize programming tasks, survey popular benchmarks across multiple dimensions including their construction and structure, and critically examine the limitations of current evaluation approaches. The review methodology involves comprehensive analysis of published benchmarks, their associated metrics, and the contexts in which they are applied. The authors also identify gaps in current evaluation practices and propose future research directions.

## Key Results
- Test-based correctness metrics are widely used but may not fully capture real-world usability of generated code
- Existing syntactic similarity metrics (BLEU, ROUGE, CodeBLEU) may not correlate well with human judgment of code quality
- Current benchmarks lack scenario-based evaluation approaches and show limited diversity in sources, failing to represent real-world programming contexts adequately

## Why This Works (Mechanism)
The review's effectiveness stems from its systematic approach to categorizing and analyzing code generation evaluation practices. By examining multiple quality attributes and surveying a wide range of benchmarks, the authors can identify patterns and limitations across the field. The mechanism works because it combines quantitative analysis of existing metrics with qualitative assessment of their real-world applicability, revealing gaps that might not be apparent when considering metrics in isolation.

## Foundational Learning
1. **Test-based correctness metrics**: Automated testing frameworks that verify functional correctness of generated code. Why needed: Provides objective measurement of whether generated code performs intended functions. Quick check: Compare pass rates across different code generation models on same test suite.

2. **Syntactic similarity metrics**: Measures like BLEU, ROUGE, and CodeBLEU that compare generated code to reference implementations based on structural similarity. Why needed: Quantifies how closely generated code matches expected solutions. Quick check: Calculate correlation between similarity scores and human evaluation ratings.

3. **Scenario-based evaluation**: Assessment approaches that evaluate code generation in realistic programming contexts rather than isolated tasks. Why needed: Better reflects practical use cases and usability concerns. Quick check: Design evaluation scenarios that mirror common programming workflows.

4. **Benchmark diversity analysis**: Systematic examination of sources, domains, and contexts represented in evaluation benchmarks. Why needed: Ensures evaluation covers relevant programming scenarios and avoids bias toward specific domains. Quick check: Categorize benchmarks by source type, programming domain, and complexity level.

## Architecture Onboarding
Component map: Programming Tasks -> Benchmarks -> Metrics -> Quality Attributes -> Evaluation Framework
Critical path: Task definition → Benchmark selection → Metric application → Quality assessment → Usability evaluation
Design tradeoffs: Test-based metrics offer automation but may miss usability; similarity metrics provide quantitative comparison but may not align with human judgment; comprehensive evaluation requires balancing multiple quality attributes
Failure signatures: Over-reliance on test metrics may produce functionally correct but unusable code; similarity metrics may reward superficial matches over practical solutions; lack of scenario-based evaluation may miss context-dependent issues
First experiments:
1. Conduct correlation study between BLEU/CodeBLEU scores and human code quality ratings
2. Compare functional correctness rates with usability assessments for same code samples
3. Develop prototype scenario-based evaluation framework for common programming tasks

## Open Questions the Paper Calls Out
The paper identifies several open questions in code generation evaluation:
- How to develop usability-focused metrics that better capture real-world code quality beyond functional correctness
- Methods for automating metadata assignment to code generation benchmarks to improve categorization and analysis
- Approaches for constructing more versatile benchmarks that better align with real-world programming scenarios
- How to balance the trade-offs between automated metric computation and human evaluation in large-scale assessment

## Limitations
- Test-based correctness metrics may not adequately capture real-world usability of generated code
- Existing syntactic similarity metrics may not correlate well with human judgment of code quality
- Current benchmarks show limited diversity in sources and may not adequately represent real-world programming scenarios

## Confidence
- High confidence in the categorization of programming tasks and benchmark survey methodology
- Medium confidence in the analysis of metric limitations, given the rapid evolution of evaluation techniques
- Medium confidence in the identified gaps, as some limitations may be addressed by emerging research not yet widely adopted

## Next Checks
1. Conduct correlation studies between existing similarity metrics and human evaluation scores across multiple code generation benchmarks
2. Develop and validate scenario-based evaluation frameworks that better capture real-world programming contexts
3. Analyze the diversity of code generation benchmarks by systematically categorizing their sources and use cases to identify representation gaps