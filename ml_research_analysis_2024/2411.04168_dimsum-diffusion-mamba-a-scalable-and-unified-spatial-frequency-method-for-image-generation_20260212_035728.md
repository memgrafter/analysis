---
ver: rpa2
title: 'DiMSUM: Diffusion Mamba -- A Scalable and Unified Spatial-Frequency Method
  for Image Generation'
arxiv_id: '2411.04168'
source_url: https://arxiv.org/abs/2411.04168
tags:
- mamba
- image
- scanning
- wavelet
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiMSUM, a novel diffusion architecture that
  integrates spatial and frequency information processing within a Mamba-based framework.
  The method addresses the challenge of optimal scanning order in Mamba models for
  image generation by incorporating wavelet transforms to capture both local structure
  and long-range frequency dependencies.
---

# DiMSUM: Diffusion Mamba -- A Scalable and Unified Spatial-Frequency Method for Image Generation

## Quick Facts
- arXiv ID: 2411.04168
- Source URL: https://arxiv.org/abs/2411.04168
- Reference count: 40
- Achieves state-of-the-art FID scores of 2.11 on ImageNet and 4.62 on CelebA-HQ 256

## Executive Summary
DiMSUM introduces a novel diffusion architecture that integrates spatial and frequency information processing within a Mamba-based framework. The method addresses the challenge of optimal scanning order in Mamba models for image generation by incorporating wavelet transforms to capture both local structure and long-range frequency dependencies. By processing wavelet-transformed features alongside spatial features using a cross-attention fusion layer, and including globally shared transformer blocks to enhance global context modeling, DiMSUM achieves state-of-the-art performance on standard image generation benchmarks while maintaining computational efficiency comparable to existing architectures.

## Method Summary
DiMSUM processes images through a hybrid architecture combining spatial and frequency information. The model applies a two-level Haar wavelet transform to decompose images into subbands, then processes both spatial features and wavelet subbands through Mamba layers with window scanning. A cross-attention fusion layer combines these complementary representations, followed by globally shared transformer blocks that capture global context. The architecture also introduces conditional Mamba initialization, where the first hidden state is initialized with condition embeddings rather than zero, enabling effective conditioning injection for class-conditional generation.

## Key Results
- Achieves state-of-the-art FID scores of 2.11 on ImageNet and 4.62 on CelebA-HQ 256
- Demonstrates faster training convergence compared to baseline Mamba diffusion models
- Maintains computational efficiency with comparable GFLOPs and parameters to existing architectures

## Why This Works (Mechanism)

### Mechanism 1: Wavelet-Mamba Integration
Wavelet transformation within Mamba enhances local structure awareness and captures long-range frequency relationships by decomposing images into wavelet subbands. The image is decomposed into low and high-frequency subbands using two-level Haar wavelet transform. Each subband undergoes window scanning, allowing the model to process both local pixel dependencies and global frequency patterns. The wavelet-based outputs are then fused with spatial Mamba outputs through a cross-attention fusion layer. This works because wavelet subbands preserve both local structure (through spatial scanning) and long-range frequency relationships (through frequency decomposition), and these can be effectively combined via cross-attention.

### Mechanism 2: Globally Shared Transformers
The globally shared transformer blocks address Mamba's limitations in capturing global context and mitigate order dependence. After every four DiM blocks, a globally shared transformer layer is inserted. This layer acts as a token-mixing mechanism that captures global relationships without relying on manually defined scanning orders, complementing Mamba's strengths in long-range modeling. This works because transformer layers can effectively capture global context and their shared weights reduce parameter overhead while maintaining performance.

### Mechanism 3: Conditional Mamba Initialization
Conditional Mamba enables better conditioning injection compared to standard Mamba architectures. Instead of initializing the first hidden state with zero, conditional Mamba initializes it with the condition embedding (e.g., class embedding for class-conditional generation). This acts as prior injection into the Mamba flow. This works because initializing the first hidden state with condition information provides a stronger conditioning signal than other injection methods.

## Foundational Learning

- **State Space Models (SSMs) and Mamba architecture**: Understanding how Mamba processes sequences and its limitations with 2D image data is crucial for grasping why wavelet integration and global transformers are needed. Quick check: How does Mamba's selective state space mechanism differ from traditional attention-based models in terms of complexity and long-range modeling?

- **Wavelet transform and frequency decomposition**: The core innovation relies on decomposing images into frequency subbands, so understanding how wavelet transforms work and what information each subband captures is essential. Quick check: What information is preserved in the LL (low-low) subband versus the HH (high-high) subband after a two-level Haar wavelet decomposition?

- **Cross-attention mechanisms and feature fusion**: The proposed cross-attention fusion layer is key to combining spatial and frequency information effectively, so understanding how query-key-value swapping works in cross-attention is important. Quick check: How does swapping queries between spatial and wavelet features in cross-attention enable better information fusion compared to simple concatenation?

## Architecture Onboarding

- **Component map**: Input → Encoder → DiMSUM Blocks (DiM blocks with Spatial-Frequency Mamba + Cross-Attention Fusion + Globally Shared Transformer) → Decoder → Output
- **Critical path**: Input latent → Spatial Mamba → Wavelet Mamba → Cross-Attention Fusion → Globally Shared Transformer → Output
- **Design tradeoffs**: Spatial-frequency Mamba adds computational overhead but improves quality; globally shared transformers reduce parameters but may limit capacity; conditional Mamba adds flexibility but requires careful embedding design
- **Failure signatures**: Poor FID scores may indicate ineffective wavelet integration; slow convergence may suggest insufficient global context modeling; mode collapse may indicate conditioning issues
- **First 3 experiments**:
  1. Baseline Mamba diffusion model without any modifications
  2. Mamba with conditional initialization only (test conditioning effectiveness)
  3. Mamba with wavelet integration but simple concatenation (test wavelet contribution before cross-attention fusion)

## Open Questions the Paper Calls Out
- How would DiMSUM perform on text-to-image generation tasks where spatial coherence and global context are critical?
- What is the optimal number of wavelet decomposition levels for different image resolutions, and how does it affect performance?
- How does the cross-attention fusion layer's performance compare to alternative fusion methods like concatenation with learned alignment or adaptive weighting?

## Limitations
- The effectiveness of wavelet-Mamba integration relies heavily on empirical observation rather than established theory linking frequency decomposition to improved spatial modeling
- The globally shared transformer shows promising results but doesn't adequately address potential capacity limitations from parameter sharing across layers
- The conditional Mamba initialization lacks ablation studies comparing it to alternative conditioning injection methods like cross-attention or FiLM layers

## Confidence
- **High confidence**: Core architecture implementation, experimental setup and dataset details, quantitative results on standard benchmarks
- **Medium confidence**: The effectiveness of wavelet decomposition for spatial-frequency modeling in Mamba architectures, the specific contribution of each architectural component to overall performance
- **Low confidence**: Theoretical justification for design choices, comparison with alternative approaches for spatial-frequency integration, analysis of failure modes and limitations

## Next Checks
1. **Component Ablation Study**: Systematically remove wavelet integration, globally shared transformers, and conditional Mamba to quantify individual contributions to performance gains, particularly examining whether wavelet decomposition provides benefits beyond simple spatial processing.

2. **Frequency vs Spatial Resolution Analysis**: Evaluate DiMSUM performance across different input resolutions and frequency decomposition levels to determine if the method scales effectively and identify resolution thresholds where wavelet benefits diminish.

3. **Alternative Conditioning Methods**: Compare conditional Mamba initialization against other conditioning injection techniques (cross-attention, FiLM, adapter layers) to determine if the initialization approach provides unique advantages or if similar gains could be achieved with simpler methods.