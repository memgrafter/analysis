---
ver: rpa2
title: 'DeeperImpact: Optimizing Sparse Learned Index Structures'
arxiv_id: '2405.17093'
source_url: https://arxiv.org/abs/2405.17093
tags:
- retrieval
- effectiveness
- queries
- deepimpact
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents DeeperImpact, an optimized version of the DeepImpact
  sparse retrieval model. The authors address the gap between sparse learned index
  structures and dense retrievers by enhancing document expansion using a fine-tuned
  Llama 2 model and improving training strategies with hard negatives, distillation,
  and CoCondenser initialization.
---

# DeeperImpact: Optimizing Sparse Learned Index Structures

## Quick Facts
- arXiv ID: 2405.17093
- Source URL: https://arxiv.org/abs/2405.17093
- Reference count: 40
- Key outcome: DeeperImpact achieves NDCG@10 of 43.49 on MS MARCO Dev Queries, improving upon DeepImpact's 38.65 through enhanced document expansion, CoCondenser initialization, and advanced training strategies.

## Executive Summary
DeeperImpact addresses the effectiveness gap between sparse learned index structures and dense retrievers by enhancing document expansion quality, improving model initialization, and refining training strategies. The authors replace DocT5Query with a fine-tuned Llama 2 model for query prediction, initialize the BERT-based model with CoCondenser pre-training, and incorporate hard negatives and distillation during training. The resulting model achieves state-of-the-art effectiveness on MS MARCO benchmarks while maintaining high efficiency with a mean response time of 39.62 ms. Ablation studies confirm that Llama 2 expansions contribute the most to effectiveness improvements, with subsequent refinements further enhancing retrieval performance.

## Method Summary
DeeperImpact optimizes sparse learned index structures by enhancing document expansion using fine-tuned Llama 2 for query prediction, improving model initialization through CoCondenser pre-training, and refining training strategies with hard negatives and distillation. The method involves expanding documents with 80 queries per document generated by Llama 2, training a BERT-based model initialized with CoCondenser, and indexing the expanded documents with quantized impact scores. During training, hard negatives from dense retrieval models and distillation from a cross-encoder teacher are incorporated to improve the model's ability to distinguish between relevant and non-relevant documents.

## Key Results
- DeeperImpact achieves NDCG@10 of 43.49 on MS MARCO Dev Queries, significantly outperforming DeepImpact's 38.65
- The model maintains high efficiency with a mean response time of 39.62 ms while improving recall@1000 from 0.8397 to 0.8601
- Ablation studies show Llama 2 expansions contribute the most to effectiveness improvements, with subsequent refinements further enhancing MRR@10

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using Llama 2 for document expansion improves recall compared to DocT5Query by generating more relevant and diverse query terms.
- Mechanism: Llama 2, a more advanced LLM, is fine-tuned for query prediction using LoRA. This fine-tuning enables it to generate contextually accurate and diverse expansion queries that better address the vocabulary mismatch problem.
- Core assumption: Llama 2's superior language understanding leads to higher quality expansion queries that capture semantic relevance beyond exact term matches.
- Evidence anchors:
  - [abstract] "substituting T5 with a fine-tuned Llama 2 model for query prediction results in a considerable improvement"
  - [section 2.1.2] "By increasing the quality and relevance of these queries for document expansion, we aim to reduce the vocabulary mismatch problem further"
- Break condition: If the generated queries contain too many hallucinations or irrelevant terms, the recall gains could be negated by noise in the index.

### Mechanism 2
- Claim: Initializing with CoCondenser improves retrieval effectiveness by providing better contextualized embeddings.
- Mechanism: CoCondenser uses contrastive learning to generate high-quality document representations that are less sensitive to noise and don't require large batch sizes. This pre-training provides a stronger foundation for the sparse retrieval model.
- Core assumption: Contrastive learning produces more robust and semantically meaningful embeddings than standard masked language modeling.
- Evidence anchors:
  - [section 2.2.1] "CoCondenser model has demonstrated considerable improvements when used for dense retrieval... It has shown a significant ability to reduce the sensitivity to noise present in the training data"
- Break condition: If the CoCondenser checkpoint is not well-aligned with the downstream sparse retrieval task, the benefits may not materialize.

### Mechanism 3
- Claim: Using hard negatives and distillation during training improves the model's ability to distinguish between relevant and non-relevant documents.
- Mechanism: Hard negatives provide semantically similar but non-relevant documents that challenge the model to learn finer distinctions. Distillation transfers knowledge from a cross-encoder teacher to the student model, capturing complex relevance patterns.
- Core assumption: The cross-encoder teacher model can identify nuanced relevance patterns that the student model can learn to approximate.
- Evidence anchors:
  - [section 2.2.2] "Training with hard negatives compels the model to make finer distinctions between relevant and non-relevant documents, thereby improving its generalization capabilities"
- Break condition: If the hard negatives are too difficult or the teacher model is too noisy, the student model may overfit or learn incorrect patterns.

## Foundational Learning

- Concept: Document expansion techniques (DocT5Query, Doc2Query--)
  - Why needed here: Document expansion addresses the vocabulary mismatch problem by adding relevant terms that may not appear in the original document but are semantically related to the query.
  - Quick check question: How does document expansion help solve the vocabulary mismatch problem in information retrieval?

- Concept: Contrastive learning and pre-training methods (CoCondenser)
  - Why needed here: Contrastive learning produces high-quality document representations that serve as a strong foundation for the sparse retrieval model, improving its ability to capture semantic relevance.
  - Quick check question: What is the key difference between contrastive learning and masked language modeling in pre-training language models?

- Concept: Knowledge distillation and hard negative sampling
  - Why needed here: These techniques improve the model's ability to distinguish between relevant and non-relevant documents by providing challenging training examples and transferring knowledge from more powerful models.
  - Quick check question: How does using hard negatives during training differ from using standard BM25 negatives?

## Architecture Onboarding

- Component map: Document expansion (Llama 2) -> Model backbone (BERT with CoCondenser initialization) -> Training components (hard negatives, distillation) -> Index structure (inverted index with quantized impact scores) -> Query processing (MaxScore algorithm)

- Critical path: 1. Expand documents using Llama 2 query generation 2. Train DeepImpact model with CoCondenser initialization, hard negatives, and distillation 3. Index expanded documents with quantized impact scores 4. Process queries using efficient MaxScore algorithm

- Design tradeoffs:
  - Expansion quality vs. index size: More expansion queries improve recall but increase index size and query processing time
  - Model complexity vs. efficiency: CoCondenser initialization and distillation improve effectiveness but add training complexity
  - Hard negatives vs. standard negatives: Hard negatives improve discrimination but require additional data processing

- Failure signatures:
  - Poor effectiveness: Could indicate issues with expansion quality, model initialization, or training data
  - Slow query processing: May be caused by excessive document expansion or inefficient indexing
  - High memory usage: Could result from large index size due to extensive expansion or model complexity

- First 3 experiments:
  1. Compare MRR@10 with and without Llama 2 expansions on MS MARCO Dev Queries to validate expansion quality
  2. Test different numbers of expansion queries (5, 10, 20, 40, 80) to find the optimal balance between effectiveness and efficiency
  3. Compare retrieval effectiveness with and without hard negatives and distillation to measure their individual contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Llama 2 expansion approach consistently improve retrieval effectiveness across diverse datasets beyond MS MARCO and BEIR?
- Basis in paper: [explicit] The authors note that Llama 2 expansions show better recall metrics compared to DocT5Query and aim to enhance recall across datasets.
- Why unresolved: The paper provides limited evidence beyond BEIR datasets, which might not represent all possible domains.
- What evidence would resolve it: Testing on a wider range of datasets, including specialized domains, to confirm consistent effectiveness.

### Open Question 2
- Question: What is the optimal balance between document expansion and computational efficiency in sparse learned index structures?
- Basis in paper: [inferred] The authors highlight the trade-off between effectiveness and efficiency, noting that document expansion inflates index size and increases MRT.
- Why unresolved: The paper does not explore different levels of expansion to determine the optimal point where effectiveness gains plateau.
- What evidence would resolve it: Conducting experiments with varying degrees of document expansion to find the sweet spot between effectiveness and efficiency.

### Open Question 3
- Question: How does the choice of distillation loss function (KL divergence vs. Margin-MSE) impact the performance of sparse retrieval models?
- Basis in paper: [explicit] The authors compare KL divergence loss with Margin-MSE loss, finding KL divergence to yield better performance, but suggest further exploration.
- Why unresolved: The paper does not explore other potential loss functions or their impact on model performance.
- What evidence would resolve it: Testing additional distillation loss functions and analyzing their effects on retrieval effectiveness and efficiency.

## Limitations

- The paper's claims about effectiveness improvements rely heavily on the quality of the Llama 2 fine-tuning process, which is not fully specified in terms of hyperparameters and training details
- While the model shows competitive performance on BEIR datasets, the evaluation is limited to only three datasets, making it difficult to assess generalizability across diverse retrieval scenarios
- The interaction between the different optimization components (expansion, initialization, hard negatives, distillation) remains unclear despite the ablation study

## Confidence

- **High confidence**: The effectiveness improvements on MS MARCO Dev Queries (NDCG@10 increase from 38.65 to 43.49) are well-supported by the ablation study and comparative results against established baselines. The efficiency claims (MRT of 39.62ms) are also well-documented with specific hardware specifications.
- **Medium confidence**: The claims about CoCondenser initialization providing better contextualized embeddings are supported by general literature but lack direct ablation evidence showing its specific contribution when combined with other optimizations.
- **Low confidence**: The generalizability of DeeperImpact to the broader BEIR benchmark is low confidence given the limited evaluation to only MS MARCO, TREC 2019/2020, and three BEIR datasets (Quora, DBpedia, SCIDOCS).

## Next Checks

1. **Ablation study with varying expansion quantities**: Conduct a more granular ablation study testing 5, 10, 20, 40, and 80 expansion queries per document to precisely quantify the trade-off between effectiveness gains and efficiency costs.

2. **Component interaction analysis**: Design experiments that isolate the contributions of each optimization component (expansion quality, CoCondenser initialization, hard negatives, distillation) by systematically enabling/disabling them in different combinations.

3. **Extended BEIR evaluation**: Evaluate DeeperImpact on the full BEIR benchmark across diverse retrieval scenarios (argument retrieval, question answering, fact checking) to validate the generalizability claims and identify potential failure modes in specialized domains.