---
ver: rpa2
title: Curriculum Demonstration Selection for In-Context Learning
arxiv_id: '2411.18126'
source_url: https://arxiv.org/abs/2411.18126
tags:
- arxiv
- demonstrations
- learning
- code
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Curriculum Demonstration Selection (CDS),
  a novel demonstration selection method for in-context learning (ICL) that leverages
  curriculum learning principles. Unlike existing methods that rely solely on similarity,
  CDS partitions the training set by difficulty levels and selects demonstrations
  from easy to difficult, ensuring a diverse range of complexities.
---

# Curriculum Demonstration Selection for In-Context Learning

## Quick Facts
- **arXiv ID**: 2411.18126
- **Source URL**: https://arxiv.org/abs/2411.18126
- **Reference count**: 40
- **Primary result**: CDS outperforms random and similarity-only baselines by up to 6% accuracy, especially on harder tasks across MATH, ARC-Challenge, and Mercury benchmarks.

## Executive Summary
This paper introduces Curriculum Demonstration Selection (CDS), a novel demonstration selection method for in-context learning that leverages curriculum learning principles. Unlike existing methods that rely solely on similarity, CDS partitions the training set by difficulty levels and selects demonstrations from easy to difficult, ensuring a diverse range of complexities. This approach enables large language models to learn from varied task difficulties within the training set. Experiments across three benchmarks‚ÄîMATH (mathematical reasoning), ARC-Challenge (commonsense reasoning), and Mercury (code generation)‚Äîdemonstrate that CDS consistently outperforms baseline methods, including random selection and similarity-based approaches like KATE. Notably, CDS shows significant improvements in solving harder problems, with accuracy gains of up to 6% on challenging tasks. The method proves effective across nine LLMs, including Llama-2, Llama-3, Mistral, and Qwen, highlighting its robustness and scalability in enhancing LLM performance in diverse problem-solving tasks.

## Method Summary
CDS improves in-context learning by selecting demonstrations that span multiple difficulty levels rather than relying solely on similarity. The method partitions the training set into k difficulty levels based on predefined complexity metadata (e.g., grade levels, acceptance rates), then retrieves one demonstration from each level for each test instance. Demonstrations are shuffled before being used as in-context examples. The approach combines curriculum learning principles with similarity-based retrieval, using CLS embeddings for relevance. Experiments evaluate CDS across three benchmarks (MATH, ARC-c, Mercury) using nine LLMs, comparing against random selection and KATE baselines.

## Key Results
- CDS consistently outperforms random selection and KATE similarity-based baselines across all three benchmarks
- Largest accuracy gains (up to 6%) occur on harder problems within each benchmark
- Effectiveness demonstrated across nine different LLMs including Llama-2, Llama-3, Mistral, and Qwen
- Fixed k=5 demonstrations balances coverage and prompt length constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting demonstrations across multiple difficulty levels provides broader task context than similarity-only methods.
- Mechanism: By partitioning the training set into difficulty levels and selecting one demonstration from each level, the model encounters examples spanning the full complexity spectrum rather than being anchored to only similar or only easy examples.
- Core assumption: The difficulty metadata (grade levels, acceptance rates) accurately reflects true task complexity and is consistent across tasks.
- Evidence anchors:
  - [abstract] "Instead of merely using similarity, CDS additionally partitions samples by their complexity measurements. Following curriculum learning, CDS then selects demonstrations from easy to difficult."
  - [section] "The dataset is divided into ùëò distinct difficulty partitions, where ùëò is determined by the distribution of complexity scores in the dataset."
  - [corpus] Weak evidence - no direct comparison of difficulty metadata accuracy in corpus.

### Mechanism 2
- Claim: Exposure to varied difficulty levels improves generalization on harder problems.
- Mechanism: When demonstrations span easy to hard, the model builds foundational skills from easier examples before encountering harder ones, enabling better transfer to complex test instances.
- Core assumption: Models can effectively leverage the difficulty progression even when demonstrations are randomly shuffled, as the exposure itself is sufficient.
- Evidence anchors:
  - [abstract] "CDS proves especially effective in enhancing LLM performance in solving challenging problems."
  - [section] "Both the MATH and ARC-c datasets show a clear positive trend, with harder problems benefiting the most from CDS."
  - [corpus] Moderate evidence - several related works explore similar curriculum ordering but do not directly measure difficulty span effects.

### Mechanism 3
- Claim: Similarity-based retrieval within difficulty partitions refines demonstration relevance without sacrificing diversity.
- Mechanism: After partitioning by difficulty, selecting the most similar example within each partition ensures relevance to the test instance while maintaining coverage of different complexity levels.
- Core assumption: CLS embeddings from a pre-trained Transformers model capture meaningful semantic similarity for the target tasks.
- Evidence anchors:
  - [section] "For similarity-based retrieval, we use CLS embeddings from a pre-trained Transformers model to represent the sentences, following KATE [25]."
  - [section] "This result suggests that similarity-based retrieval enhances the effectiveness of CDS, providing better demonstration selection."
  - [corpus] Moderate evidence - KATE and similar works validate embedding-based similarity but not within a curriculum framework.

## Foundational Learning

- **Concept**: Curriculum Learning
  - Why needed here: The paper builds directly on curriculum learning theory to justify selecting demonstrations from easy to hard difficulty levels.
  - Quick check question: What is the primary hypothesis behind curriculum learning in machine learning training?

- **Concept**: In-Context Learning (ICL)
  - Why needed here: CDS is an ICL demonstration selection method; understanding ICL is critical to grasp why demonstration choice matters.
  - Quick check question: How does ICL differ from traditional fine-tuning in terms of model adaptation?

- **Concept**: Task Complexity Metadata
  - Why needed here: The method depends on predefined difficulty labels (grade levels, acceptance rates); knowing how such metadata is defined and validated is key.
  - Quick check question: What are common sources of task complexity metadata in educational or coding datasets?

## Architecture Onboarding

- **Component map**: Dataset ‚Üí Difficulty Measurer ‚Üí Partitioner ‚Üí Demonstration Retriever ‚Üí Shuffler ‚Üí Prompt Builder ‚Üí LLM
- **Critical path**:
  1. Partition training set into difficulty levels using metadata
  2. For each test instance, retrieve one demonstration per difficulty partition
  3. Shuffle retrieved demonstrations
  4. Construct prompt with Chain-of-Thought format
  5. Run inference with greedy decoding
- **Design tradeoffs**:
  - Fixed k=5 demonstrations balances coverage and prompt length constraints
  - Random shuffling vs ordered presentation trades potential ordering benefits for robustness
  - Similarity retrieval adds computational cost but improves relevance
- **Failure signatures**:
  - Poor performance on easy tasks may indicate difficulty metadata is noisy
  - No improvement over baselines may suggest demonstrations are too uniform in difficulty
  - Performance drops when increasing k may indicate prompt length saturation
- **First 3 experiments**:
  1. Run CDS with random retrieval only on MATH dataset to confirm difficulty-based partitioning helps
  2. Add similarity retrieval within partitions and compare to random retrieval baseline
  3. Test demonstration order sensitivity by comparing shuffled vs easy-to-hard ordering on ARC-c dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CDS performance scale with the number of demonstrations per task, and is there an optimal number that balances performance gains against computational cost?
- Basis in paper: [explicit] The paper mentions that all experiments used k=5 demonstrations, balancing computational costs and performance consistency, but does not explore different values of k.
- Why unresolved: The paper explicitly states this as a limitation, noting that the fixed five-shot setting may not fully capture CDS's potential across different shot settings.
- What evidence would resolve it: Experiments varying k (e.g., 1, 3, 5, 10, 15) across different task complexities and model sizes, measuring accuracy and computational overhead.

### Open Question 2
- Question: Can CDS effectiveness be maintained or improved when difficulty metadata is unavailable or unreliable, and what alternative methods can estimate task complexity?
- Basis in paper: [explicit] The paper acknowledges that CDS relies on predefined metadata (like grade levels) and notes this as a limitation when such metadata is unavailable or inaccurate.
- Why unresolved: The paper does not provide solutions or experiments for cases where difficulty annotations are missing or imperfect.
- What evidence would resolve it: Experiments comparing CDS with alternative complexity estimation methods (e.g., model-based estimates, task-specific heuristics) on datasets without metadata.

### Open Question 3
- Question: How generalizable is CDS to other domains beyond mathematical reasoning, commonsense reasoning, and code generation, and what are its strengths and limitations in diverse contexts?
- Basis in paper: [explicit] The paper explicitly states that generalizability to other task types remains an open question and calls for future work to expand CDS evaluation to broader benchmarks.
- Why unresolved: The current evaluation is limited to three specific benchmarks, and the paper does not test CDS on other domains like vision, multi-modal tasks, or specialized scientific domains.
- What evidence would resolve it: Extensive experiments applying CDS to diverse benchmarks (e.g., visual question answering, scientific reasoning, medical diagnosis) and analyzing performance trends across domains.

### Open Question 4
- Question: Does the order of demonstrations within each difficulty level affect CDS performance, and if so, what ordering strategies optimize learning outcomes?
- Basis in paper: [inferred] While the paper shows that overall demonstration order (easy to hard vs. random) does not significantly impact performance, it does not explore ordering within difficulty levels or the effect of demonstration diversity within each level.
- Why unresolved: The experiments only test global shuffling after selecting one demonstration per difficulty level, not the internal ordering or diversity of demonstrations within each difficulty partition.
- What evidence would resolve it: Experiments testing different strategies for ordering demonstrations within each difficulty level (e.g., by similarity, by complexity sub-levels, by diversity metrics) and measuring their impact on performance.

## Limitations

- CDS relies on pre-defined difficulty metadata, which may be noisy or unavailable for many datasets
- The method's effectiveness beyond problem-solving domains (MATH, ARC-c, Mercury) remains untested
- Fixed k=5 demonstrations may not be optimal across different task complexities and model sizes

## Confidence

- **High Confidence**: CDS improves accuracy over random and similarity-only baselines on the tested benchmarks
- **Medium Confidence**: The mechanism of spanning difficulty levels is the primary driver of improvement; the curriculum order itself may be less critical
- **Low Confidence**: The robustness of CDS to noisy or mis-specified difficulty metadata; the method's effectiveness outside of problem-solving domains

## Next Checks

1. **Difficulty Metadata Validation**: Conduct a pilot study to assess the alignment between predefined difficulty labels and model-perceived task complexity (e.g., using perplexity or human evaluation)
2. **Order Sensitivity Test**: Compare CDS performance when demonstrations are presented in easy-to-hard order versus randomly shuffled to isolate the effect of difficulty progression from mere diversity
3. **Generalization Test**: Apply CDS to a non-problem-solving ICL task (e.g., sentiment analysis or summarization) to evaluate its effectiveness beyond mathematical, commonsense, and code generation domains