---
ver: rpa2
title: Streaming Bilingual End-to-End ASR model using Attention over Multiple Softmax
arxiv_id: '2401.11645'
source_url: https://arxiv.org/abs/2401.11645
tags:
- bilingual
- attention
- english
- language
- hindi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel bilingual end-to-end (E2E) modeling
  approach for streaming speech recognition. The key idea is to use a shared encoder
  and prediction network with language-specific joint networks combined via a self-attention
  mechanism.
---

# Streaming Bilingual End-to-End ASR model using Attention over Multiple Softmax

## Quick Facts
- arXiv ID: 2401.11645
- Source URL: https://arxiv.org/abs/2401.11645
- Reference count: 0
- Primary result: Bilingual E2E ASR model with attention over multiple softmaxes achieves WER reductions of 13.3% (Hindi), 8.23% (English), and 1.3% (code-mixed) compared to conventional bilingual baseline

## Executive Summary
This paper introduces a novel bilingual end-to-end (E2E) modeling approach for streaming speech recognition that leverages attention over multiple softmax outputs. The proposed architecture uses a shared encoder and prediction network with language-specific joint networks, connected via a self-attention mechanism to produce a unified posterior probability distribution. This design enables single beam search decoding and dynamic language switching, addressing limitations of conventional bilingual E2E models that require separate decoders per language. Experimental results on Hindi, English, and code-mixed test sets demonstrate relative word error rate improvements of 13.3%, 8.23%, and 1.3% respectively compared to a conventional bilingual baseline.

## Method Summary
The proposed approach employs a shared encoder that processes acoustic features and a shared prediction network that generates language-agnostic representations. These are combined with language-specific joint networks, each producing softmax outputs corresponding to their respective vocabularies. A self-attention mechanism operates over these multiple softmax distributions to generate a single unified posterior probability. This architecture enables streaming decoding with a single beam search while allowing the model to dynamically switch between languages during inference. The method is evaluated using RNN-T models on Hindi, English, and code-mixed datasets, with results showing significant WER improvements over conventional bilingual E2E baselines.

## Key Results
- Achieved 13.3% relative WER reduction on Hindi test sets compared to conventional bilingual baseline
- Achieved 8.23% relative WER reduction on English test sets
- Achieved 1.3% relative WER reduction on code-mixed test sets
- Analysis of attention weights demonstrates the model's ability to capture language-specific information and improve overall performance

## Why This Works (Mechanism)
The architecture works by leveraging shared representations from a common encoder and prediction network while maintaining language-specific decision boundaries through joint networks. The self-attention mechanism learns to weight these language-specific outputs appropriately based on the input context, enabling the model to dynamically adjust its language focus during decoding. This approach addresses the fundamental challenge in bilingual E2E models where separate decoders struggle with language switching and require complex ensemble strategies. By producing a single unified posterior, the model simplifies the decoding process while maintaining the flexibility to handle code-switching scenarios effectively.

## Foundational Learning

**E2E ASR Modeling**: Why needed - End-to-end models directly map speech to text without intermediate components; Quick check - Model should produce character or word-level predictions directly from acoustic features

**Streaming Speech Recognition**: Why needed - Real-time applications require immediate hypothesis generation; Quick check - Model should generate partial hypotheses while processing ongoing speech

**Joint Networks in RNN-T**: Why needed - Combine acoustic and linguistic information for prediction; Quick check - Network should fuse encoder outputs with prediction network states

**Language-specific Modeling**: Why needed - Different languages have distinct phonotactic and lexical patterns; Quick check - Model should maintain separate vocabularies and language-specific processing paths

**Attention Mechanisms**: Why needed - Learn dynamic weighting of multiple information sources; Quick check - Attention weights should vary based on input context and language requirements

## Architecture Onboarding

**Component Map**: Audio features -> Shared Encoder -> Shared Prediction Network -> Language-specific Joint Networks -> Multiple Softmaxes -> Self-Attention -> Unified Posterior

**Critical Path**: The streaming path follows: Encoder processes acoustic frames sequentially, prediction network generates prefix states, joint networks combine these with language-specific context, multiple softmaxes produce per-language distributions, and self-attention creates final unified output

**Design Tradeoffs**: Shared encoder reduces parameters and enables transfer learning but may limit language-specific specialization; multiple softmaxes maintain language separation while unified attention simplifies decoding; streaming requires careful buffer management to balance latency and accuracy

**Failure Signatures**: If attention weights become uniform across languages, model loses language discrimination; if encoder representations don't capture language-agnostic features, attention mechanism cannot effectively fuse information; if streaming constraints are violated, model may produce non-monotonic predictions

**First Experiments**: 1) Test attention weight distributions on monolingual vs code-mixed inputs to verify language discrimination, 2) Measure latency impact of different streaming buffer sizes, 3) Evaluate performance with frozen vs trainable encoder parameters

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to Hindi-English language pair plus code-mixed data, restricting generalizability
- Substantial variation in performance improvements across languages (13.3% for Hindi, 8.23% for English, only 1.3% for code-mixed) suggests approach may not uniformly benefit all language combinations
- Lacks comparison against alternative bilingual E2E architectures and detailed ablation studies to isolate self-attention contribution

## Confidence
- High confidence: Technical feasibility of architecture combining shared encoder with language-specific joint networks connected via self-attention
- Medium confidence: Claimed WER improvements, as results limited to specific language pairs without broader validation
- Medium confidence: Assertion that single posterior probability generation enables more effective beam search decoding compared to conventional bilingual models

## Next Checks
1. Evaluate the model on additional language pairs beyond Hindi-English to assess cross-linguistic generalizability
2. Conduct ablation studies removing the self-attention mechanism to quantify its specific contribution to performance gains
3. Test the model's behavior with more than two languages simultaneously to validate scalability claims