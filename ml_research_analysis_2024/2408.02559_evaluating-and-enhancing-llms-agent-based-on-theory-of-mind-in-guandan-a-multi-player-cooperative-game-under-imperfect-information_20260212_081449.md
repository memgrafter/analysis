---
ver: rpa2
title: 'Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A
  Multi-Player Cooperative Game under Imperfect Information'
arxiv_id: '2408.02559'
source_url: https://arxiv.org/abs/2408.02559
tags:
- game
- card
- cards
- arxiv
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of large language models (LLMs)
  in a complex multi-player card game requiring collaboration under imperfect information.
  The authors propose a Theory of Mind (ToM) planning technique that enables LLM agents
  to adapt their strategies against various adversaries using only game rules, current
  state, and historical context as input.
---

# Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information

## Quick Facts
- **arXiv ID**: 2408.02559
- **Source URL**: https://arxiv.org/abs/2408.02559
- **Reference count**: 40
- **Primary result**: GPT-4 with ToM planning and action recommender achieves performance comparable to state-of-the-art RL models in Guandan

## Executive Summary
This study evaluates large language models (LLMs) in Guandan, a complex multi-player card game requiring collaboration under imperfect information. The authors introduce a Theory of Mind (ToM) planning technique that enables LLM agents to adapt strategies against various adversaries using only game rules, current state, and historical context. An external tool is incorporated to handle the dynamic and extensive action spaces. Experimental results demonstrate that while a performance gap exists between current LLMs and state-of-the-art reinforcement learning models, LLMs exhibit ToM capabilities in this game setting. The ToM planning approach consistently improves LLM performance against opposing agents, indicating their ability to understand ally and adversary actions and establish collaboration.

## Method Summary
The authors propose a Theory of Mind planning technique for LLM agents in Guandan, a complex multi-player card game. The approach uses only game rules, current state, and historical context as input, enabling agents to adapt strategies against various adversaries. An external action recommender tool is incorporated to address the challenge of dynamic and extensive action spaces. The ToM planning technique is evaluated against different types of simulated opponents, measuring the LLM's ability to understand and collaborate with allies while countering adversaries. The study compares LLM performance with state-of-the-art reinforcement learning models to establish baseline capabilities and measure improvement through the proposed approach.

## Key Results
- GPT-4 with ToM planning and action recommender tool achieves performance comparable to state-of-the-art RL models in Guandan
- ToM planning consistently improves LLM performance against opposing agents
- LLMs demonstrate Theory of Mind capabilities by understanding ally and adversary actions and establishing collaboration
- Performance gap exists between current LLMs and state-of-the-art RL models, though partially addressed by ToM planning

## Why This Works (Mechanism)
The proposed ToM planning technique works by enabling LLM agents to model and predict the behaviors of both allies and adversaries in the game. By incorporating game rules, current state, and historical context, the LLMs can develop strategic reasoning that goes beyond simple rule-following. The external action recommender tool helps manage the complexity of the game's extensive action space, allowing the LLM to focus on strategic decision-making rather than computational enumeration. The ToM capability allows agents to anticipate opponents' moves and coordinate effectively with allies, creating a more sophisticated gameplay approach that mimics human strategic thinking in imperfect information settings.

## Foundational Learning

**Imperfect Information Games**: Games where players have incomplete knowledge about opponents' states or actions - needed to understand the fundamental challenge in Guandan; quick check: identify what information is hidden in the game setup.

**Theory of Mind**: The ability to attribute mental states, beliefs, and intentions to others - needed to model opponent behavior and predict actions; quick check: verify if agent can explain why an opponent made a specific move.

**Multi-Agent Reinforcement Learning**: Training multiple agents that interact within the same environment - needed as baseline comparison; quick check: confirm RL agent performance metrics.

**Action Space Complexity**: The number and variety of possible moves in a game state - needed to understand why external tools are necessary; quick check: count unique legal moves in a sample game state.

**Cooperative Gameplay**: Strategic collaboration between agents toward shared objectives - needed to evaluate alliance dynamics; quick check: measure win rate when allies coordinate versus when they don't.

## Architecture Onboarding

**Component Map**: Game Environment -> LLM Agent -> ToM Planner -> Action Recommender -> Game State Update -> Opponent Models

**Critical Path**: Input State → ToM Reasoning → Action Generation → Execution → Outcome → State Update → Next Decision Cycle

**Design Tradeoffs**: Computational efficiency vs. strategic depth (external tool reduces reasoning burden but adds latency), simulation accuracy vs. training data diversity (simulated opponents may not capture all human behaviors), model complexity vs. interpretability (complex ToM models harder to debug but more capable).

**Failure Signatures**: Over-reliance on action recommender leading to predictable patterns, inability to adapt to novel opponent strategies, breakdown in alliance coordination under high-pressure situations, performance degradation with increased number of players.

**First Experiments**: 1) Benchmark LLM performance without ToM planning against RL baseline, 2) Test ToM planning with different opponent types to measure adaptability, 3) Evaluate alliance effectiveness by measuring collaborative win rates versus solo performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental setup relies on simulated opponents rather than human players, raising questions about real-world applicability
- Performance gap between current LLMs and state-of-the-art RL models remains significant in absolute terms
- Generalizability of findings to other imperfect information games or real-world collaborative scenarios is uncertain

## Confidence

**High confidence**: ToM planning technique consistently improves LLM performance against simulated opponents; GPT-4 with ToM planning and action recommender achieves comparable performance to RL models in Guandan environment.

**Medium confidence**: Claim that LLMs demonstrate genuine Theory of Mind capabilities, based on in-game performance rather than direct behavioral validation.

**Low confidence**: Generalizability of findings to other imperfect information games or real-world collaborative scenarios beyond controlled Guandan environment.

## Next Checks

1. Test the ToM planning approach against human players rather than simulated agents to verify if observed collaboration capabilities transfer to human opponents.

2. Conduct ablation studies removing the external action recommender tool to isolate the specific contribution of ToM planning versus computational assistance.

3. Validate the approach across multiple imperfect information games with varying complexity to assess generalizability beyond the Guandan domain.