---
ver: rpa2
title: 'Learning to Ask: When LLM Agents Meet Unclear Instruction'
arxiv_id: '2409.00557'
source_url: https://arxiv.org/abs/2409.00557
tags:
- llms
- user
- instructions
- arxiv
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of executing large language model
  (LLM) tool usage under unclear or ambiguous user instructions. It identifies common
  instruction issues through analysis of real-world user queries, categorizes them,
  and introduces the NoisyToolBench benchmark to evaluate LLM performance under such
  conditions.
---

# Learning to Ask: When LLM Agents Meet Unclear Instruction

## Quick Facts
- **arXiv ID**: 2409.00557
- **Source URL**: https://arxiv.org/abs/2409.00557
- **Reference count**: 4
- **Primary Result**: Ask-when-Needed (AwN) framework improves LLM tool-use accuracy under ambiguous instructions by prompting LLMs to seek clarification

## Executive Summary
This paper addresses the challenge of executing large language model (LLM) tool usage under unclear or ambiguous user instructions. Through analysis of real-world user queries, the authors identify common instruction issues and categorize them to create the NoisyToolBench benchmark. They propose the Ask-when-Needed (AwN) framework, which enables LLMs to actively seek clarification from users when facing ambiguities. An automated evaluation system, ToolEvaluator, is designed to proxy human interaction and assess LLM performance from both accuracy and efficiency perspectives. Experimental results demonstrate that AwN significantly improves LLM tool-use accuracy under unclear instructions, outperforming existing methods, though at the cost of some efficiency due to additional questions.

## Method Summary
The paper introduces the Ask-when-Needed (AwN) framework to address LLM tool usage under ambiguous instructions. The approach works by prompting LLMs to actively seek clarification from users when encountering unclear instructions, rather than proceeding with potentially incorrect assumptions. The framework is evaluated using the ToolEvaluator system, an automated proxy for human interaction that assesses both accuracy and efficiency of LLM responses. The NoisyToolBench benchmark provides a standardized evaluation environment with categorized instruction ambiguities. Experiments show that AwN significantly improves tool-use accuracy compared to baseline approaches, though it introduces some efficiency overhead due to additional clarification questions.

## Key Results
- AwN framework significantly improves LLM tool-use accuracy under unclear instructions
- Automated ToolEvaluator system effectively proxies human evaluation of LLM performance
- Experimental results show clear accuracy gains, with acceptable trade-offs in efficiency
- The framework outperforms existing methods in handling ambiguous tool-use scenarios

## Why This Works (Mechanism)
The mechanism works by leveraging the LLM's ability to recognize ambiguity and formulate clarifying questions. When an instruction is unclear, the LLM identifies specific missing information or ambiguous terms and generates targeted questions to resolve these issues. This approach prevents incorrect assumptions that would lead to tool misuse, while maintaining task flow through interactive clarification. The ToolEvaluator system validates that this questioning strategy leads to more accurate tool selection and parameter specification.

## Foundational Learning
1. **Ambiguity Detection**: Ability to recognize unclear or missing information in instructions
   - Why needed: Without detection, LLMs cannot identify when clarification is required
   - Quick check: Can the system flag instructions with missing parameters or unclear objectives?

2. **Question Generation**: Formulating clear, specific questions to resolve ambiguities
   - Why needed: Poor questions lead to circular conversations or insufficient clarification
   - Quick check: Are generated questions actionable and likely to produce useful responses?

3. **Tool Knowledge**: Understanding tool capabilities, parameters, and expected inputs
   - Why needed: Clarification questions must be grounded in tool requirements
   - Quick check: Can the system map ambiguous terms to specific tool parameters?

## Architecture Onboarding

**Component Map**: User Query -> Ambiguity Detector -> Question Generator -> Tool Evaluator -> Tool Execution -> User Response

**Critical Path**: The sequence from receiving unclear instructions through to clarification and tool execution forms the critical path. Performance bottlenecks occur primarily in the ambiguity detection and question generation stages.

**Design Tradeoffs**: The system trades immediate execution speed for accuracy by introducing clarification steps. This creates a fundamental tension between efficiency (fewer interactions) and correctness (more accurate tool usage). The automated evaluation framework prioritizes accuracy metrics but also captures efficiency costs.

**Failure Signatures**: Common failure modes include over-questioning (asking about obvious information), under-questioning (missing critical ambiguities), and generating questions that don't lead to actionable clarifications. The ToolEvaluator system is designed to catch these through both accuracy and efficiency metrics.

**First 3 Experiments**:
1. Evaluate baseline LLM performance on unambiguous vs. ambiguous instructions in NoisyToolBench
2. Test AwN framework's accuracy improvement across different types of instruction ambiguities
3. Measure efficiency trade-offs by comparing task completion time with and without clarification steps

## Open Questions the Paper Calls Out
None

## Limitations
- Automated evaluation framework may not fully capture human judgment nuances
- Trade-off between accuracy gains and efficiency losses through additional questioning needs more nuanced exploration
- Benchmark dataset may not fully represent the complexity of real-world ambiguous instructions

## Confidence

**High Confidence**: The core methodology of prompting LLMs to ask clarifying questions is technically sound and demonstrates measurable improvements in tool-use accuracy.

**Medium Confidence**: The automated evaluation system provides a practical solution for large-scale testing, though its validity as a human proxy needs further validation.

**Medium Confidence**: The benchmark dataset represents a valuable contribution, but its coverage of real-world ambiguity scenarios may be limited.

## Next Checks

1. Conduct user studies to validate that the automated ToolEvaluator results align with human judgment of LLM performance under ambiguous instructions.

2. Test the AwN framework across diverse application domains (beyond the current tool-use scenarios) to assess generalizability.

3. Investigate the impact of different questioning strategies within AwN on both task completion rates and user satisfaction, particularly in multi-turn interactions.