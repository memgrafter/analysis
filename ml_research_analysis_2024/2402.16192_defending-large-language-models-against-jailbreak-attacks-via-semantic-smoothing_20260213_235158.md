---
ver: rpa2
title: Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing
arxiv_id: '2402.16192'
source_url: https://arxiv.org/abs/2402.16192
tags:
- instruction
- response
- defense
- arxiv
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEMANTICSMOOTH, a smoothing-based defense
  against jailbreak attacks on large language models. The core idea is to use semantic-preserving
  transformations like paraphrasing to perturb inputs, then aggregate model responses.
---

# Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing

## Quick Facts
- arXiv ID: 2402.16192
- Source URL: https://arxiv.org/abs/2402.16192
- Reference count: 40
- Primary result: Introduces SEMANTICSMOOTH, achieving state-of-the-art robustness against GCG, PAIR, and AutoDAN jailbreak attacks while maintaining strong nominal performance on instruction-following benchmarks.

## Executive Summary
This paper presents SEMANTICSMOOTH, a novel defense against jailbreak attacks on large language models that leverages semantic-preserving transformations and randomized smoothing. The approach perturbs input prompts through paraphrasing, summarizing, translating, and other semantic transformations, then aggregates model responses to improve robustness. Experiments demonstrate that SEMANTICSMOOTH achieves superior defense performance against multiple attack types while maintaining high accuracy on benign inputs, outperforming existing defenses like LIME, STRIP, and SHAP.

## Method Summary
SEMANTICSMOOTH applies semantic-preserving transformations to input prompts to create multiple perturbed copies, which are then processed through the LLM and aggregated via majority vote. The defense employs an input-dependent policy network that learns to adaptively select transformations based on the input type, optimizing the trade-off between robustness against attacks and performance on benign inputs. The approach uses seven transformation types including spell check, verb tense modification, synonym replacement, translation, summarization, paraphrasing, and format changes.

## Key Results
- Achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN jailbreak attacks
- Maintains strong nominal performance on InstructionFollowing and AlpacaEval benchmarks
- Provides interpretability for nonsensical GCG attack suffixes by transforming them into meaningful prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic transformations preserve meaning while disrupting adversarial suffixes.
- Mechanism: By paraphrasing, summarizing, or translating input prompts, the nonsensical GCG suffixes are transformed into coherent sentences that align with the original harmful request, causing the LLM to recognize the ethical violation.
- Core assumption: The semantic transformation process does not inadvertently reinforce the harmful intent but instead clarifies it to trigger the model's safety alignment.
- Evidence anchors:
  - [abstract] "SEMANTICSMOOTH achieves state-of-the-art robustness against GCG... while maintaining strong nominal performance"
  - [section] "the proposed semantic transformations can decipher the nonsensical suffix into a meaningful and coherent natural sentence"
  - [corpus] "Advancing the Robustness of Large Language Models through Self-Denoised Smoothing" (related defense approach)

### Mechanism 2
- Claim: An input-dependent policy network improves the balance between robustness and nominal performance.
- Mechanism: The policy network learns to select appropriate transformations based on the input type (adversarial vs. benign), favoring substantial changes for harmful inputs and minimal changes for benign inputs.
- Core assumption: Different transformations have varying effectiveness against different attack types and input categories.
- Evidence anchors:
  - [abstract] "SEMANTICSMOOTH uses an input-dependent policy network that adaptively selects the transformations applied to each input"
  - [section] "different transformations are optimal for different inputs"
  - [corpus] "RobustKV: Defending Large Language Models against Jailbreak Attacks via KV Eviction" (related adaptive defense)

### Mechanism 3
- Claim: Aggregation of multiple semantically transformed copies reduces the success rate of jailbreak attacks.
- Mechanism: By creating multiple perturbed versions of an input and taking a majority vote on the responses, the defense ensures that at least one transformed version will be recognized as harmful by the LLM.
- Core assumption: The LLM's safety mechanisms are robust enough to identify harmful content even when presented in transformed forms.
- Evidence anchors:
  - [abstract] "SEMANTICSMOOTH...aggregates the predictions of multiple semantically transformed copies of a given input prompt"
  - [section] "The next step is to aggregate the outputs of the perturbed inputs, which is typically done via majority vote"
  - [corpus] "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing" (related aggregation defense)

## Foundational Learning

- Concept: Randomized smoothing for robustness
  - Why needed here: Provides theoretical foundation for aggregating predictions over perturbed inputs to improve model robustness
  - Quick check question: How does randomized smoothing differ from traditional adversarial training in terms of computational efficiency?

- Concept: Semantic-preserving transformations
  - Why needed here: Ensures that input meaning is maintained while disrupting adversarial patterns, balancing robustness and performance
  - Quick check question: What distinguishes semantic-preserving transformations from character-level perturbations in terms of impact on benign inputs?

- Concept: Policy gradient methods for adaptive selection
  - Why needed here: Enables learning an optimal transformation selection strategy based on input characteristics
  - Quick check question: How does the reward function balance the trade-off between rejecting adversarial queries and correctly answering benign ones?

## Architecture Onboarding

- Component map:
  Input prompt → Transformation selector (policy network) → Multiple transformed copies → LLM inference → JUDGE evaluation → Majority vote → Final response

- Critical path:
  1. Receive input prompt
  2. Policy network selects transformations
  3. Apply transformations to create N copies
  4. Pass each copy through LLM
  5. Evaluate responses with JUDGE function
  6. Aggregate results via majority vote
  7. Return final response

- Design tradeoffs:
  - Transformation diversity vs. computational cost
  - Robustness vs. nominal performance (conservative transformations may hurt benign performance)
  - Policy network complexity vs. adaptation speed

- Failure signatures:
  - High false positive rate on benign inputs (overly aggressive transformations)
  - Low robustness against adaptive attacks (policy network fails to select effective transformations)
  - Increased latency due to multiple LLM inferences

- First 3 experiments:
  1. Test each individual transformation type against GCG attack to establish baseline effectiveness
  2. Evaluate uniform random transformation ensemble vs. no defense
  3. Compare learned policy network performance against fixed transformation policies on mixed adversarial/benign inputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of SEMANTICSMOOTH vary across different languages beyond French?
- Basis in paper: [inferred] The paper mentions using French for TRANSLATE transformations because it is well-resourced for the LLMs considered, but does not explore other languages or their impact on defense effectiveness.
- Why unresolved: The paper only tests with French as the target language for translation-based perturbations, leaving open the question of how other languages might perform.
- What evidence would resolve it: Empirical results comparing SEMANTICSMOOTH performance when using different target languages for TRANSLATE transformations across multiple attacks and LLMs.

### Open Question 2
- Question: What is the optimal number of semantic transformations to apply per input for balancing defense strength and computational cost?
- Basis in paper: [explicit] The paper uses 10 perturbed copies in their experiments but does not explore how varying this number affects the trade-off between robustness and nominal performance.
- Why unresolved: The paper fixes the number of transformations at 10 without investigating whether this is optimal or how the defense performance scales with different numbers of copies.
- What evidence would resolve it: Systematic experiments varying the number of perturbed copies (e.g., 5, 10, 20, 50) and measuring corresponding changes in ASR and nominal performance across multiple attack types.

### Open Question 3
- Question: How does SEMANTICSMOOTH perform against jailbreak attacks that specifically target the semantic transformations themselves?
- Basis in paper: [inferred] The paper demonstrates robustness against existing attacks but does not consider whether attackers could adapt their strategies to exploit the semantic transformation process.
- Why unresolved: All evaluated attacks target the base LLM without considering the additional defense layer of semantic transformations, leaving open whether adaptive attacks could circumvent this approach.
- What evidence would resolve it: Evaluation of SEMANTICSMOOTH against attacks specifically designed to either predict which transformations will be applied or to craft prompts that remain effective after common semantic transformations.

## Limitations
- Computational overhead from multiple LLM inferences and policy network computation not quantified
- Policy network generalization to unseen attack types and model architectures uncertain
- Implementation quality of semantic transformations critical but not fully specified

## Confidence
- High Confidence: The core concept of using semantic-preserving transformations to disrupt adversarial suffixes is theoretically sound and shows empirical effectiveness against tested attack types.
- Medium Confidence: The adaptive policy network approach for transformation selection is promising but requires further validation on diverse attack types and model architectures.
- Low Confidence: The claim about interpretability for nonsensical GCG attack suffixes, while interesting, needs more rigorous evaluation beyond the qualitative examples provided.

## Next Checks
1. **Compute Cost Analysis**: Measure the end-to-end latency and computational cost of SEMANTICSMOOTH compared to baseline defenses, including the overhead of multiple LLM inferences and policy network computation.

2. **Cross-Model Transferability**: Evaluate SEMANTICSMOOTH's effectiveness on LLMs not seen during policy network training, particularly smaller or larger models than those tested (LLaMA-2-7b and Vicuna-13b).

3. **Transformation Quality Assessment**: Implement a systematic evaluation of transformation quality using metrics like semantic similarity preservation (e.g., BERTScore) and conduct ablation studies removing individual transformation types to identify their relative contributions.