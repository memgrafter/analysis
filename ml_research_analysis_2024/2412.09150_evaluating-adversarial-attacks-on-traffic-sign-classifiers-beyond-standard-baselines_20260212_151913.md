---
ver: rpa2
title: Evaluating Adversarial Attacks on Traffic Sign Classifiers beyond Standard
  Baselines
arxiv_id: '2412.09150'
source_url: https://arxiv.org/abs/2412.09150
tags:
- sign
- attack
- attacks
- traffic
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares adversarial attacks on traffic sign classifiers
  across multiple model architectures and datasets. It decouples three established
  baselines (CNNsmall, CNNlarge, CNN-STN) from their training datasets and evaluates
  them against five generic CNNs.
---

# Evaluating Adversarial Attacks on Traffic Sign Classifiers beyond Standard Baselines

## Quick Facts
- arXiv ID: 2412.09150
- Source URL: https://arxiv.org/abs/2412.09150
- Reference count: 40
- Key outcome: This paper compares adversarial attacks on traffic sign classifiers across multiple model architectures and datasets, showing that standard baselines are significantly more susceptible to attacks than generic models, particularly on the GTSRB dataset.

## Executive Summary
This study systematically evaluates the robustness of traffic sign classifiers against adversarial attacks by decoupling established baselines from their training datasets and comparing them with generic CNN models. The research demonstrates that standard benchmarks used in previous attack studies may overestimate attack success rates because these models are more vulnerable than general-purpose architectures. The paper conducts extensive experiments across multiple attack types and datasets, revealing significant differences in model susceptibility that have implications for how adversarial attack research should be evaluated and reported.

## Method Summary
The paper conducts a comprehensive evaluation of adversarial attacks on traffic sign classifiers by testing five established baselines (CNNsmall, CNNlarge, CNN-STN) against five generic CNN models across two datasets (GTSRB and LISA). Two attack settings are evaluated: inconspicuous attacks (small perturbations targeting misclassification) and visible attacks (larger perturbations targeting incorrect but plausible classes). The study uses Projected Gradient Descent (PGD) attacks with both l1 and l2 norms, measuring attack success rates across different combinations of models, attacks, and datasets. The decoupled evaluation approach isolates the effects of model architecture from dataset-specific training effects.

## Key Results
- Standard baselines are significantly more susceptible to adversarial attacks than generic CNN models, particularly on GTSRB dataset
- CNN-STN models show almost invisible perturbations in digital domain but poor real-world transferability
- GTSRB-based models are less susceptible to attacks than LISA-based models
- Visible attacks generally have higher success rates than inconspicuous attacks across all model types
- The choice of l1 vs l2 norm affects attack effectiveness and physical-world transferability

## Why This Works (Mechanism)
The paper's decoupled evaluation approach works by systematically isolating the vulnerability of model architectures from their training data effects. By testing established baselines against generic models on the same datasets, the study reveals that the high attack success rates reported in previous literature are partly due to the inherent vulnerability of the standard benchmark models rather than the attack methods themselves. The comparison between inconspicuous and visible attacks demonstrates different mechanisms of model failure, with inconspicuous attacks exploiting subtle decision boundary shifts while visible attacks overwhelm the model's ability to recognize the original class.

## Foundational Learning

**Projected Gradient Descent (PGD) attacks**: Iterative optimization method that generates adversarial examples by maximizing the loss function within a constrained perturbation budget. Needed to understand the primary attack methodology used in the evaluation.

**Model decoupling concept**: The practice of separating model architecture from training data effects to isolate specific sources of vulnerability. Critical for understanding why established baselines may not be representative of general model robustness.

**l1 vs l2 norm regularization**: Different perturbation constraints that affect attack generation and physical-world transferability. Important for understanding why some attacks that work digitally fail in real-world scenarios.

**Transferability of adversarial examples**: The ability of adversarial perturbations to remain effective when moving from digital to physical domains. Essential for assessing the practical relevance of attack results.

**Traffic sign classification datasets**: Understanding GTSRB and LISA datasets, including their collection methods and class distributions, is necessary to interpret why different models perform differently on each dataset.

## Architecture Onboarding

**Component map**: Attack generator -> Perturbation application -> Model prediction -> Success rate calculation -> Dataset comparison -> Architecture comparison

**Critical path**: PGD attack generation -> Perturbation application to input images -> Model classification -> Attack success evaluation -> Cross-architecture comparison

**Design tradeoffs**: The study prioritizes comprehensive model comparison over exploring the full space of attack variants, choosing to focus on two well-established attack settings rather than many different attack types.

**Failure signatures**: High attack success rates on standard baselines but low rates on generic models indicate that previous literature may have overestimated attack effectiveness due to model vulnerability rather than attack strength.

**First experiments**:
1. Test a single established baseline (CNN-STN) against a generic model (ResNet18) on GTSRB dataset with both l1 and l2 attacks
2. Compare inconspicuous vs visible attack success rates on the same model pair
3. Evaluate transferability by testing digital attack success against physical-world robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different regularization terms (l1 vs l2 norm) in the attack generation phase affect the transferability of adversarial perturbations from digital to physical domains?
- Basis in paper: [explicit] The authors note that l1 norm targets makes the noise more compact and less scattered over the sign, while l2 restricts the insensitivity and amount of noise, and observe that CNN-STN models with almost invisible perturbations had poor real-world results despite good digital performance.
- Why unresolved: The paper shows this is an open question through their observation that perturbations almost invisible in digital domain may not transfer well to physical domain, and they don't provide a systematic analysis of why this happens.
- What evidence would resolve it: Systematic experiments comparing transferability rates between l1 and l2 regularization across multiple models and attack types, with quantitative metrics for physical-world effectiveness.

### Open Question 2
- Question: What is the relationship between model architecture complexity (parameter count) and vulnerability to adversarial attacks in traffic sign classification tasks?
- Basis in paper: [explicit] The authors observe that CNNlarge (largest model with 16.57M parameters) performed worse than ResNet18 (11.19M parameters) on GTSRB dataset, and that established baselines are more susceptible to attacks than generic models despite varying parameter counts.
- Why unresolved: While the paper presents these observations, it doesn't provide a systematic analysis of how parameter count relates to attack vulnerability across different model architectures.
- What evidence would resolve it: Controlled experiments varying model complexity while keeping architecture type constant, measuring attack success rates across a spectrum of model sizes.

### Open Question 3
- Question: How do inconspicuous and visible adversarial attacks differ in their effectiveness across different traffic sign datasets (LISA vs GTSRB)?
- Basis in paper: [explicit] The authors compare both attack types and note that GTSRB-based models tend to be less susceptible to attacks than LISA-based models, and that visible attacks generally have higher success rates than inconspicuous ones.
- Why unresolved: The paper presents these observations but doesn't provide a detailed analysis of why dataset characteristics affect attack effectiveness differently for conspicuous vs inconspicuous attacks.
- What evidence would resolve it: Detailed comparative analysis of attack success rates across datasets for both attack types, including investigation of dataset-specific features that influence attack effectiveness.

## Limitations
- The study focuses primarily on GTSRB dataset, which may not represent full diversity of real-world traffic sign scenarios
- Limited sample size of generic models (5) compared to extensive literature on adversarial attacks
- Physical-world experiments are not comprehensively conducted, leaving questions about real-world transferability
- Does not address environmental variations like lighting, weather, or perspective changes

## Confidence
- Standard baselines more susceptible than generic models: Medium - Based on comparison of 5 generic models against 3 established baselines, but broader ecosystem unexplored
- Decoupled evaluation methodology validity: High - Methodologically sound approach with clear isolation of model architecture effects
- Physical-world transferability findings: Low - Limited real-world testing, digital results may not fully translate to physical scenarios

## Next Checks
1. **Expand model diversity**: Test the attack robustness of a broader range of generic CNN architectures, including recent state-of-the-art models specifically designed for traffic sign classification, to validate the generalizability of the findings.

2. **Cross-dataset evaluation**: Assess the attack effectiveness across multiple traffic sign datasets (e.g., LISA, BelgiumTS) to determine if the observed vulnerabilities in standard baselines are consistent across different data distributions and collection methodologies.

3. **Real-world scenario testing**: Conduct physical-world experiments to evaluate the practical effectiveness of adversarial attacks on both standard baselines and generic models, accounting for factors such as perspective changes, occlusions, and environmental variations that may not be captured in controlled digital evaluations.