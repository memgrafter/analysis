---
ver: rpa2
title: 'Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization'
arxiv_id: '2404.11531'
source_url: https://arxiv.org/abs/2404.11531
tags:
- llms
- fusion
- perplexity
- packllm
- seed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of effectively combining knowledge
  from multiple Large Language Models (LLMs) at test-time to improve performance on
  various tasks. The proposed Pack of LLMs (PackLLM) method performs model fusion
  by solving an optimization problem to determine each LLM's importance weight, minimizing
  perplexity over the input prompt.
---

# Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization

## Quick Facts
- arXiv ID: 2404.11531
- Source URL: https://arxiv.org/abs/2404.11531
- Reference count: 40
- PackLLM improves test-time LLM fusion accuracy by 1.89% over baselines and outperforms learning-based fusion by 3.92–11.94% accuracy points

## Executive Summary
This paper introduces PackLLM, a method for test-time fusion of multiple Large Language Models by optimizing perplexity. The approach determines importance weights for each LLM by solving an optimization problem that minimizes perplexity over the input prompt. PackLLM includes two variants: PackLLMsim (a simple perplexity-based approach) and PackLLMopt (which approximately solves the perplexity minimization problem via a greedy algorithm). Experiments with over 100 LLMs across diverse tasks demonstrate that perplexity is a reliable measure for LLM fusion, and PackLLM consistently outperforms both simple baselines and learning-based fusion approaches.

## Method Summary
PackLLM performs model fusion by assigning weights to each LLM based on their ability to minimize perplexity on the input prompt. PackLLMsim uses normalized perplexity scores as weights, while PackLLMopt implements a greedy algorithm that sequentially combines models in order of increasing perplexity, solving for optimal weights at each step. The method handles different tokenizers through Minimum Edit Distance alignment, mapping tokens between vocabularies before combining logits. This approach requires no retraining and leverages the existing diversity of LLMs to improve performance across various tasks including question answering, commonsense reasoning, and biomedical applications.

## Key Results
- Perplexity-based fusion is reliable, with PackLLM outperforming test-time fusion baselines by 1.89% accuracy points
- PackLLMopt's greedy algorithm achieves results comparable to exhaustive search while running 225x faster (2-4 seconds vs 902 seconds for 3-5 LLMs)
- PackLLM can leverage new LLMs to improve performance over learning-based fusion approaches by 3.92–11.94% accuracy points
- The method successfully combines over 100 diverse LLMs including OPT models, Mistral, LLaMa2, and specialized biomedical models

## Why This Works (Mechanism)

### Mechanism 1
Lower perplexity signals higher expertise for a given prompt, so assigning higher weights to low-perplexity models improves fusion quality. Perplexity measures how well a model predicts the input; the optimization problem in Eq. 3 explicitly minimizes perplexity across the ensemble. Core assumption: Perplexity correlates with "expertise" on the test prompt and can be computed without training. Break condition: If perplexity poorly correlates with task performance (e.g., adversarial prompts), weight assignment may degrade fusion.

### Mechanism 2
Greedy sequential ensemble reduces the combinatorial search space from K-dimensional to a series of 2-model problems, keeping runtime linear in K. PackLLMopt sorts models by perplexity, then merges them pairwise, solving for λ(k) that minimizes perplexity at each step. Core assumption: The order from lowest to highest perplexity ensures that early models contribute most and later ones can be pruned. Break condition: If the greedy path misses a better global combination (non-submodular gains), performance may be suboptimal.

### Mechanism 3
Tokenizer alignment via Minimum Edit Distance (MinED) allows fusion of models with disjoint vocabularies without retraining. Map reference tokens to target tokens with MinED, forward through each model, then map logits back to reference vocabulary before softmax. Core assumption: MinED provides a stable, one-to-one mapping sufficient for logit fusion. Break condition: If token mappings are ambiguous or noisy, logit misalignment could hurt fusion quality.

## Foundational Learning

- Concept: Perplexity as a language modeling metric
  - Why needed here: Core to PackLLM's weight derivation; without it, the fusion objective collapses.
  - Quick check question: If a model has PPL=5 and another PPL=10 on a prompt, which gets higher PackLLM weight?

- Concept: Greedy optimization and early stopping
  - Why needed here: Enables tractable test-time fusion without exhaustive grid search.
  - Quick check question: What condition causes PackLLMopt to stop merging additional models?

- Concept: Logit-level ensemble vs probability-level ensemble
  - Why needed here: PackLLM aggregates raw logits before softmax, preserving gradients and model calibration.
  - Quick check question: Why might adding probabilities (after softmax) yield different results than adding logits?

## Architecture Onboarding

- Component map:
  Input prompt → Tokenizer selection (reference = top-1 model's tokenizer) → MinED mapping to each seed model's tokenizer → Forward pass through each seed model → Collect logits per model → PackLLMopt greedy fusion loop → Output token sampling via combined logits

- Critical path:
  Forward pass + greedy optimization (dominated by model inference + grid search over λ∈[0,1])

- Design tradeoffs:
  Runtime vs optimality: greedy vs exhaustive grid search
  Simplicity vs precision: PackLLMsim (heuristic) vs PackLLMopt (optimization)
  Tokenizer complexity: MinED mapping adds preprocessing overhead

- Failure signatures:
  Degraded accuracy when seed LLMs have very different tokenizers (mapping errors)
  Slow inference if greedy step includes many models or fine-grained λ grid
  Suboptimal fusion if perplexity poorly reflects downstream task performance

- First 3 experiments:
  1. Verify perplexity ranking matches known model strengths on a held-out dev set.
  2. Compare PackLLMsim vs PackLLMopt runtime and accuracy on 2–4 seed models.
  3. Test MinED mapping robustness by fusing models with disjoint vocabularies and measuring token alignment accuracy.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text.

## Limitations
- Perplexity may not reliably indicate expertise for adversarial or highly specialized prompts where low perplexity doesn't guarantee high task performance
- Greedy optimization trades optimality for speed, potentially missing globally optimal weight combinations when non-submodular interactions exist
- Token alignment via Minimum Edit Distance lacks empirical validation for token alignment accuracy or robustness to tokenization ambiguities

## Confidence
- High confidence: Perplexity-based test-time fusion improves accuracy over simple baselines (1.89% gain, Table 3)
- Medium confidence: Superiority over learning-based fusion methods (3.92–11.94% gain, but lacks statistical significance testing)
- Medium confidence: Greedy algorithm efficiency claims (2–4 seconds vs 902 seconds, but scalability analysis stops at 5 LLMs)
- Low confidence: Tokenizer alignment mechanism robustness (MinED mapping described but not empirically validated)

## Next Checks
1. Measure correlation between individual model perplexities and actual task performance across different prompt types to verify the core assumption that lower perplexity indicates better expertise.

2. Implement and compare PackLLMopt against exact (exhaustive) fusion for small model sets (2-4 LLMs) to quantify the accuracy loss from greedy approximation.

3. Design experiments that systematically vary tokenizer vocabulary overlap and measure both MinED mapping accuracy and resulting fusion performance to establish when and why token alignment breaks down.