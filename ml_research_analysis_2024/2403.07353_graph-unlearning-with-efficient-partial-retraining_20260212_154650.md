---
ver: rpa2
title: Graph Unlearning with Efficient Partial Retraining
arxiv_id: '2403.07353'
source_url: https://arxiv.org/abs/2403.07353
tags:
- graph
- unlearning
- data
- partition
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GraphRevoker, a novel graph unlearning framework
  that better maintains the model utility of unlearnable GNNs. Specifically, it introduces
  graph property-aware sharding to preserve the graph property and graph contrastive
  sub-model aggregation to effectively aggregate the sub-GNN models for prediction.
---

# Graph Unlearning with Efficient Partial Retraining

## Quick Facts
- arXiv ID: 2403.07353
- Source URL: https://arxiv.org/abs/2403.07353
- Authors: Jiahao Zhang; Lin Wang; Shijie Wang; Wenqi Fan
- Reference count: 40
- Primary result: Proposes GraphRevoker, a graph unlearning framework that achieves state-of-the-art model performance while enabling efficient unlearning through property-aware sharding and contrastive aggregation

## Executive Summary
This paper addresses the challenge of graph unlearning - removing the influence of undesirable data points from trained Graph Neural Networks (GNNs) while maintaining model utility and achieving efficient unlearning. The authors propose GraphRevoker, a novel framework that combines graph property-aware sharding with graph contrastive sub-model aggregation. Through extensive experiments on multiple datasets, GraphRevoker demonstrates superior performance compared to existing methods like SISA and GraphEraser, achieving both higher model utility and faster unlearning efficiency.

## Method Summary
GraphRevoker is a three-stage framework for graph unlearning. First, it uses a neural network to partition the graph into subgraphs while optimizing for unlearning time, structural preservation, and semantic preservation. Second, isolated sub-GNN models are trained on these disjoint subgraphs. Third, a contrastive learning-based aggregator combines the sub-model predictions using attentive fusion enhanced by local-global and local-local contrastive objectives. The framework is evaluated on four real-world graph datasets using five different GNN architectures, with performance measured by F1-score and unlearning efficiency measured by retraining time.

## Key Results
- GraphRevoker achieves state-of-the-art model performance among efficient unlearning frameworks
- Outperforms previous methods like SISA and GraphEraser across multiple datasets and GNN architectures
- Exhibits faster unlearning efficiency compared to existing approaches
- Successfully preserves both structural and semantic information during graph partitioning
- The contrastive learning-based aggregation module effectively leverages disjoint sub-models for prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph property-aware sharding preserves model utility by maintaining structural and semantic properties during graph partitioning
- Mechanism: The framework partitions the graph using a differentiable neural network that optimizes three objectives simultaneously: unlearning time (balancing nodes and edges), structural preservation (minimizing edge cuts), and semantic preservation (maximizing label entropy)
- Core assumption: Graph structure and label distribution can be effectively preserved through differentiable optimization of these three objectives
- Evidence anchors:
  - [abstract] "we preserve the graph property with graph property-aware sharding and effectively aggregate the sub-GNN models for prediction with graph contrastive sub-model aggregation"
  - [section 4.1] "we first formulate the unlearning goals in Section 2 into three reachable optimization objectives, and then solve them with an effective neural framework to give desirable graph partitions"
  - [section 4.1] "We propose the unlearning time objective as the expectation of retraining time... Lstructure = Nc↵(G1,G2,· · ·,GS)... Lsem = 1/S * sum(Entropy[d(Vi)])"
- Break condition: If the differentiable optimization cannot effectively balance the three competing objectives, the partition quality degrades, leading to poor sub-model performance

### Mechanism 2
- Claim: Graph contrastive sub-model aggregation effectively leverages disjoint sub-models by learning better feature representations
- Mechanism: The framework learns an aggregator that performs attentive fusion of sub-model embeddings, enhanced by local-global contrastive learning (pulling similar views together while pushing apart different nodes) and local-local reconstruction (restoring dropped edge information)
- Core assumption: Contrastive learning can effectively learn to combine weak sub-models into a strong global model
- Evidence anchors:
  - [abstract] "effectively aggregate the sub-GNN models for prediction with graph contrastive sub-model aggregation"
  - [section 4.2] "we develop a contrastive learning framework to learn an effective aggregator to ensemble the weak sub-GNN models"
  - [section 4.2] "Lcontrast = 1/|U| * sum(LInfoNCE(...))... Lrecon = 1/|U| * sum(max{phi(e_u, e_v+) - phi(e_u, e_v-) + 1, 0})"
- Break condition: If the contrastive learning objectives don't align with the prediction task, the aggregator may learn representations that don't improve classification accuracy

### Mechanism 3
- Claim: The combination of property-aware sharding and contrastive aggregation maintains utility while enabling efficient unlearning
- Mechanism: The framework first creates balanced, informative partitions that preserve graph properties, then uses contrastive learning to effectively combine sub-models trained on these partitions
- Core assumption: The two-stage approach (partition then aggregate) is superior to either stage alone for maintaining utility while enabling efficient unlearning
- Evidence anchors:
  - [abstract] "Our contributions can be summarized as follows: 1) We propose the graph property-aware sharding module to preserve the sub-GNN models' prediction performance by keeping the structural and semantic properties in the training graph. 2) To effectively leverage the disjoint sub-models for prediction, we propose the graph contrastive sub-model aggregation module"
  - [section 4] "To achieve this goal, we systematically review the limitations of prior works in graph unlearning... and then introduce a novel graph unlearning framework, namely GraphRevoker, equipped with graph property-aware sharding and graph contrastive sub-model aggregation"
  - [table 2] Experimental results showing GraphRevoker outperforms both SISA and GraphEraser across multiple datasets and GNN architectures
- Break condition: If either the partitioning or aggregation stage fails to meet their respective objectives, the overall framework cannot maintain utility while enabling efficient unlearning

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their inductive node classification
  - Why needed here: The framework builds on GNNs and needs to maintain their classification performance while enabling unlearning
  - Quick check question: What are the key architectural differences between GAT, GCN, SAGE, APPNP, and JKNet that might affect unlearning performance?

- Concept: Graph partitioning and its impact on model performance
  - Why needed here: The framework relies on graph partitioning to enable efficient unlearning, so understanding how partitioning affects model utility is crucial
  - Quick check question: How do random partitioning, clustering-based partitioning, and the proposed property-aware sharding differ in terms of structural and semantic preservation?

- Concept: Contrastive learning and its application to graph data
  - Why needed here: The framework uses contrastive learning to aggregate sub-models, so understanding how contrastive learning works on graphs is essential
  - Quick check question: What are the key differences between local-local reconstruction and local-global contrastive objectives in the context of graph unlearning?

## Architecture Onboarding

- Component map: Input Graph Data -> Partition Module -> Sub-model Training -> Aggregation Module -> Final Predictions/Unlearning
- Critical path: Graph data → Partition module → Sub-model training → Aggregation module → Final predictions/unlearning
- Design tradeoffs:
  - Partition module: More expressive neural networks may better preserve properties but increase computational cost
  - Aggregation module: More complex contrastive learning objectives may improve performance but increase training time
  - Number of shards: More shards enable faster unlearning but may degrade sub-model performance
- Failure signatures:
  - Poor partition quality: High edge cuts, imbalanced label distributions, or high retraining time variance
  - Weak sub-models: Low individual performance before aggregation
  - Ineffective aggregation: Little or no improvement from aggregation, or degradation in performance
- First 3 experiments:
  1. Compare partition quality metrics (edge cuts, label balance) between random, clustering, and property-aware partitioning on Cora dataset
  2. Evaluate sub-model performance before and after aggregation using the contrastive learning framework on a small dataset
  3. Test unlearning efficiency by measuring retraining time for different numbers of shards on a medium-sized dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the GraphRevoker framework maintain its effectiveness when applied to dynamic graphs where the structure and node attributes change over time?
- Basis in paper: [inferred] The paper evaluates GraphRevoker on static graphs and does not discuss its applicability to dynamic graphs.
- Why unresolved: The paper does not provide any experimental results or theoretical analysis on the performance of GraphRevoker in dynamic graph settings.
- What evidence would resolve it: Experiments on dynamic graph datasets, showing the framework's performance and efficiency in handling evolving graph structures and node attributes.

### Open Question 2
- Question: How does the GraphRevoker framework perform in terms of model utility and unlearning efficiency when dealing with graphs that have highly imbalanced label distributions?
- Basis in paper: [inferred] The paper does not discuss the impact of label imbalance on the framework's performance, focusing instead on balanced datasets.
- Why unresolved: The effectiveness of the semantic preservation objective and the overall framework in scenarios with skewed label distributions is not explored.
- What evidence would resolve it: Experimental results on graphs with varying degrees of label imbalance, comparing the performance of GraphRevoker with other unlearning methods.

### Open Question 3
- Question: What is the theoretical guarantee for the complete removal of the undesirable data's influence in the GraphRevoker framework, especially considering potential information leakage during the graph partitioning phase?
- Basis in paper: [explicit] The paper acknowledges the potential for information leakage during the graph partitioning phase and the need for more thorough theoretical analysis to ensure the robustness of data impact removal.
- Why unresolved: The paper does not provide a detailed theoretical analysis or formal proof of the framework's ability to completely remove the influence of undesirable data.
- What evidence would resolve it: A formal theoretical analysis or proof demonstrating the conditions under which the GraphRevoker framework guarantees the complete removal of undesirable data's influence, along with empirical validation of these conditions.

## Limitations
- The framework's performance heavily depends on the quality of graph partitioning, which may not generalize well to graphs with highly irregular structures or extreme label imbalance
- The computational overhead of the partitioning and aggregation modules could be significant for very large graphs
- The contrastive learning objectives for aggregation, while theoretically sound, may require careful hyperparameter tuning for different datasets

## Confidence
- **High confidence**: The overall framework design and its three-stage approach (partition → train → aggregate) is well-motivated and addresses known limitations in graph unlearning literature
- **Medium confidence**: The specific implementation details of the neural partition network and contrastive aggregation objectives are described sufficiently for reproduction, though some architectural hyperparameters are unspecified
- **Medium confidence**: The experimental results showing superior performance over SISA and GraphEraser are compelling, but the evaluation focuses on a limited set of datasets and GNN architectures

## Next Checks
1. **Partition quality analysis**: Systematically evaluate how different graph structures (scale-free, small-world, random) affect the partitioning quality and downstream sub-model performance
2. **Ablation study on aggregation**: Test the impact of removing each contrastive learning objective (local-global vs local-local) on final model utility to quantify their individual contributions
3. **Scalability benchmark**: Measure the framework's performance and computational requirements on graphs with 100K+ nodes to assess real-world applicability beyond the tested datasets