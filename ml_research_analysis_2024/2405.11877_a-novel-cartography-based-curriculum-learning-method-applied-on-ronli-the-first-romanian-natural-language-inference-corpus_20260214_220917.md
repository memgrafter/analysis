---
ver: rpa2
title: 'A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The
  First Romanian Natural Language Inference Corpus'
arxiv_id: '2405.11877'
source_url: https://arxiv.org/abs/2405.11877
tags:
- uni00000013
- language
- ro-bert
- natural
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces RoNLI, the first Romanian natural language
  inference corpus, comprising 58K training and 6K validation/test sentence pairs.
  The training pairs are obtained via distant supervision using linking phrases from
  Romanian Wikipedia, while validation/test pairs are manually annotated.
---

# A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus

## Quick Facts
- **arXiv ID:** 2405.11877
- **Source URL:** https://arxiv.org/abs/2405.11877
- **Reference count:** 32
- **Primary result:** First Romanian NLI corpus with 58K training pairs, achieved 0.75 micro F1 and 0.59 macro F1 using curriculum learning

## Executive Summary
This paper introduces RoNLI, the first Romanian natural language inference corpus, consisting of 58K training and 6K validation/test sentence pairs. The training data is automatically labeled using linking phrases from Romanian Wikipedia, while the validation/test sets are manually annotated. The authors evaluate multiple models, from word embeddings to transformers, and establish competitive baselines. The best-performing model, Ro-BERT, is further improved using a novel curriculum learning strategy based on data cartography and stratified sampling, achieving statistically significant improvements over the baseline.

## Method Summary
The method involves collecting Romanian sentence pairs from Wikipedia, automatically labeling training data using linking phrases for distant supervision, and manually annotating validation/test sets. Multiple models are trained, including CBOW+SVM, multilingual BERT, and RoBERTa. A novel curriculum learning approach is applied to the best model, RoBERTa, using data cartography to categorize samples as easy-to-learn, ambiguous, or hard-to-learn, and stratified sampling to ensure class diversity in each batch. The approach aims to mitigate class imbalance and prevent model bias toward majority classes.

## Key Results
- RoNLI corpus contains 58K training and 6K validation/test pairs
- Ro-BERT with curriculum learning achieves 0.75 micro F1 and 0.59 macro F1 on test set
- Results are statistically significant and outperform best baseline
- Romanian NLI is challenging due to linguistic complexity and lack of resources

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Curriculum learning based on data cartography and stratified sampling improves NLI model performance.
- **Mechanism:** The model first learns from easy-to-learn (E2L) examples, then progressively incorporates ambiguous (A) and hard-to-learn (H2L) samples, ensuring class diversity in each batch.
- **Core assumption:** Stratified sampling mitigates class imbalance and prevents model bias toward majority classes.
- **Evidence anchors:**
  - [abstract] "We improve on the best model by employing a new curriculum learning strategy based on data cartography and stratified sampling (Ro-BERT + Cart-Stra-CL++), achieving a micro F1 of 0.75 and macro F1 of 0.59 on the test set."
  - [section] "Our novel curriculum learning approach is more suitable for imbalanced datasets, which are more prone to be affected by introducing further class biases."
- **Break condition:** If the stratification process fails to maintain balanced class distribution across difficulty levels, model bias may persist.

### Mechanism 2
- **Claim:** Data cartography identifies training samples as E2L, A, or H2L based on confidence and variability during training.
- **Mechanism:** Samples with high confidence and low variability are classified as E2L, while those with low confidence and low variability are classified as H2L, indicating potential labeling noise.
- **Core assumption:** Variability and confidence metrics accurately reflect the difficulty and correctness of training samples.
- **Evidence anchors:**
  - [section] "The method is built upon two insightful metrics recorded during the training process, specifically the level of confidence when accurately categorizing a data point into the correct class, and the variability (fluctuation) of the confidence during training."
  - [section] "In contrast, hard-to-learn examples, occupying the bottom-left quadrant, demonstrate low variability but differentiate themselves from the previous category in terms of confidence, which stays low in this case."
- **Break condition:** If the confidence and variability metrics do not correlate with actual sample difficulty, the classification into E2L, A, and H2L groups may be inaccurate.

### Mechanism 3
- **Claim:** Romanian NLI is challenging due to linguistic complexity and lack of resources.
- **Mechanism:** The unique syntactic structures and clitic systems in Romanian, combined with the scarcity of NLI datasets, make it difficult for models to achieve high performance.
- **Core assumption:** Linguistic features such as complex sentence structures and clitic systems impact model performance.
- **Evidence anchors:**
  - [section] "Romanian is a unique Eastern Romance language... Therefore, the results obtained by multilingual BERT in the zero-shot and fine-tuned settings... suggest that there are linguistic differences between our dataset for NLI in Romanian and other NLI-supported languages."
  - [section] "Romanian tends to have a complex sentence structure, which often confuses NLI models."
- **Break condition:** If the linguistic complexity is not adequately captured by the model's architecture, performance may plateau despite increased data or training.

## Foundational Learning

- **Concept: Natural Language Inference (NLI)**
  - Why needed here: NLI is the core task being addressed; understanding its definition and challenges is essential for interpreting results.
  - Quick check question: What are the three main relationship types in NLI, and how do they differ?

- **Concept: Curriculum Learning**
  - Why needed here: The paper employs a novel curriculum learning strategy; understanding its principles is crucial for grasping the proposed method.
  - Quick check question: How does curriculum learning mimic human learning, and why is it beneficial for model training?

- **Concept: Data Cartography**
  - Why needed here: Data cartography is used to categorize training samples; understanding its methodology is key to interpreting the results.
  - Quick check question: What are the two main metrics used in data cartography, and how do they contribute to sample categorization?

## Architecture Onboarding

- **Component map:**
  - Data collection and preprocessing
  - Model training (Ro-BERT, SVM, Softmax, etc.)
  - Data cartography for sample categorization
  - Curriculum learning with stratified sampling
  - Evaluation and statistical testing

- **Critical path:**
  1. Collect and preprocess Romanian Wikipedia sentences.
  2. Automatically label training data using linking phrases.
  3. Manually annotate validation and test sets.
  4. Train baseline models (Ro-BERT, SVM, etc.).
  5. Apply data cartography to identify E2L, A, and H2L samples.
  6. Implement curriculum learning with stratified sampling.
  7. Evaluate model performance and conduct statistical testing.

- **Design tradeoffs:**
  - Automatic labeling vs. manual annotation: Automatic labeling is faster but may introduce noise; manual annotation is more accurate but time-consuming.
  - Curriculum learning vs. standard training: Curriculum learning may improve performance but requires additional complexity in training pipeline.
  - Stratified sampling vs. random sampling: Stratified sampling ensures class diversity but may be computationally more expensive.

- **Failure signatures:**
  - Low inter-rater agreement in manual annotations.
  - Poor performance on underrepresented classes.
  - High variability in model performance across runs.
  - Failure of curriculum learning to improve performance.

- **First 3 experiments:**
  1. Train Ro-BERT on the full training set without curriculum learning to establish a baseline.
  2. Apply data cartography to the training set and analyze the distribution of E2L, A, and H2L samples.
  3. Implement curriculum learning with stratified sampling and compare performance to the baseline.

## Open Questions the Paper Calls Out
None

## Limitations
- Distant supervision approach may introduce spurious correlations in training data
- Small test set (2,000 pairs) limits statistical power of results
- Results only evaluated on Romanian NLI, unclear if approach generalizes to other languages

## Confidence
- **High Confidence:** RoNLI is the first Romanian NLI corpus and Ro-BERT with curriculum learning outperforms baseline models
- **Medium Confidence:** Romanian NLI is particularly challenging due to linguistic complexity
- **Low Confidence:** Specific mechanism by which data cartography identifies E2L, A, and H2L samples

## Next Checks
1. Conduct ablation studies to determine individual contributions of data cartography and stratified sampling
2. Test curriculum learning approach on a subset of manually labeled training data
3. Evaluate model performance on out-of-domain Romanian text to assess generalization