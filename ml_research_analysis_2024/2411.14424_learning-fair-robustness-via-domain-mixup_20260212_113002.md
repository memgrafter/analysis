---
ver: rpa2
title: Learning Fair Robustness via Domain Mixup
arxiv_id: '2411.14424'
source_url: https://arxiv.org/abs/2411.14424
tags:
- adversarial
- risk
- mixup
- training
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of class-wise fairness in adversarial
  training, where adversarial training often results in significant disparities in
  robustness across different classes. The authors propose using same-domain mixup,
  which involves mixing inputs from the same class and performing adversarial training
  on the mixed-up inputs, to provably reduce class-wise robustness disparity.
---

# Learning Fair Robustness via Domain Mixup

## Quick Facts
- arXiv ID: 2411.14424
- Source URL: https://arxiv.org/abs/2411.14424
- Reference count: 36
- Primary result: Mixup-based adversarial training reduces class-wise robustness disparity, improving worst-class adversarial risk from 12.70% to 3.80% on CIFAR-10 with ε=0.3

## Executive Summary
This paper addresses class-wise fairness in adversarial training, where standard adversarial training often creates significant disparities in robustness across different classes. The authors propose same-domain mixup - mixing inputs from the same class and performing adversarial training on these mixed samples - to provably reduce these disparities. They provide theoretical analysis for linear classifiers with Gaussian data showing that mixup reduces variance in the decision boundary, leading to more balanced class-wise risks. The method is validated on both synthetic and real-world datasets (CIFAR-10), demonstrating substantial improvements in class-wise fairness while maintaining overall accuracy.

## Method Summary
The method combines same-domain mixup with adversarial training. For same-domain mixup, samples from the same class are mixed using λxi + (1-λ)xj where λ ∈ [0,1], maintaining the class label. This mixed data is then used in adversarial training, typically with FGSM attacks. The theoretical analysis shows that mixup reduces the variance of the resulting distribution (g(λ) = λ² + (1-λ)² < 1), creating smoother decision boundaries within each class. The method is evaluated on CIFAR-10 using ResNet20 architecture with standard adversarial training hyperparameters.

## Key Results
- Theoretical proof: Mixup with adversarial training reduces the gap between class-wise natural and adversarial risks for linear classifiers with Gaussian data
- CIFAR-10 results: Worst-class test adversarial risk improves from 12.70% to 3.80% when ε = 0.3
- Overall accuracy: Maintained or slightly improved compared to standard adversarial training
- Class-wise disparities: Significant reduction in ∆adv(f) measures across all tested conditions

## Why This Works (Mechanism)

### Mechanism 1: Variance Reduction
Mixup reduces class-wise robustness disparity by creating a lower-variance distribution that balances decision boundaries. When mixing inputs from the same class, the resulting variance decreases because the variance of the mixup sample is proportional to g(λ) = λ² + (1-λ)², which is strictly less than 1 for any 0 < λ < 1. This lower variance creates more balanced decision boundaries across classes, reducing the gap between class-wise risks. Core assumption: The original class-conditioned data follows Gaussian distributions with the same variance across classes.

### Mechanism 2: Preserved Class Identity
Same-domain mixup preserves class identity while improving robustness through interpolated decision regions. By mixing samples from the same class (rather than different classes), the resulting mixup samples maintain their original class label while being positioned in regions of feature space that are less susceptible to adversarial perturbations. This creates smoother decision boundaries within each class. Core assumption: Linear classifiers can benefit from smoother decision boundaries within each class.

### Mechanism 3: Uniform Robustness Learning
Mixup-based adversarial training reduces the worst-case class risk more effectively than standard adversarial training. By training on mixup samples that have reduced variance, the model learns to be robust to perturbations in regions of feature space that are more representative of the entire class distribution, not just extreme points. This leads to more uniform robustness across all classes. Core assumption: Standard adversarial training focuses too heavily on the boundaries between classes rather than within-class robustness.

## Foundational Learning

- **Adversarial training fundamentals**: Understanding how adversarial training works and its limitations is crucial for grasping why mixup can address fairness issues. Quick check: What is the primary goal of adversarial training, and how does it differ from standard training?

- **Mixup regularization**: The paper builds on mixup as a data augmentation technique, so understanding its basic mechanics is essential. Quick check: How does mixup differ from traditional data augmentation methods like rotation or flipping?

- **Class-wise risk analysis**: The paper focuses on disparities in class-wise risks, so understanding how to measure and analyze these is fundamental. Quick check: How would you calculate the class-wise natural risk for a binary classifier?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Same-domain mixup layer -> Adversarial example generator (FGSM) -> Model (ResNet20/linear) -> Training loop -> Evaluation
- **Critical path**: 1. Load and normalize CIFAR-10 data, 2. Implement same-domain mixup function, 3. Create adversarial examples using FGSM, 4. Train model with mixup + adversarial training, 5. Evaluate class-wise risks on test set
- **Design tradeoffs**: Mixup ratio (λ) vs. robustness gain (higher λ preserves more information but reduces mixup benefits), Perturbation budget (ε) vs. training stability (larger ε may cause instability), Model complexity vs. fairness (simpler models show clearer theoretical guarantees)
- **Failure signatures**: If class-wise disparities increase, mixup ratio may be too extreme; if overall accuracy drops significantly, perturbation budget may be too large; if training becomes unstable, FGSM step size may need adjustment
- **First 3 experiments**: 1. Implement same-domain mixup on CIFAR-10 without adversarial training to observe natural risk reduction, 2. Apply standard adversarial training on CIFAR-10 to establish baseline class-wise disparities, 3. Combine same-domain mixup with adversarial training and compare worst-class risk improvement

## Open Questions the Paper Calls Out

### Open Question 1
Does the mixup mechanism work equally well for all perturbation radii (epsilon) in adversarial training, or is there an optimal range of epsilon values where it is most effective? The paper provides results for specific epsilon values but does not explore the relationship between epsilon and the effectiveness of mixup.

### Open Question 2
Can the mixup mechanism be extended to other types of adversarial attacks beyond FGSM, such as PGD or Carlini-Wagner attacks? The paper uses FGSM as the adversarial attack technique, but does not explore the effectiveness of mixup against other types of attacks.

### Open Question 3
How does the mixup mechanism perform with other types of data distributions beyond Gaussian, such as real-world datasets with non-Gaussian characteristics? The theoretical analysis is based on linear classifiers and Gaussian data, but the experimental results are on CIFAR-10, which is not Gaussian.

## Limitations

- The theoretical analysis relies heavily on Gaussian assumptions about class-conditioned data distributions, which may not hold for real-world datasets
- The specific mixup ratio λ is not explicitly stated in experiments, leaving ambiguity about optimal hyperparameter settings
- The paper focuses on same-domain mixup while not exploring cross-domain alternatives that might offer different tradeoffs

## Confidence

- **High Confidence**: The empirical results on CIFAR-10 showing reduced class-wise disparities (worst class improvement from 12.70% to 3.80%) are well-supported by the experimental setup and align with the theoretical framework
- **Medium Confidence**: The theoretical analysis for linear classifiers under Gaussian assumptions provides reasonable but idealized guarantees that may not fully translate to complex, real-world data distributions
- **Low Confidence**: The specific mechanisms by which mixup improves robustness in deep neural networks are not fully explained, and the paper relies on empirical validation rather than theoretical justification for this case

## Next Checks

1. **Sensitivity Analysis**: Systematically vary the mixup ratio λ ∈ {0.1, 0.3, 0.5, 0.7, 0.9} to quantify its impact on class-wise disparities and identify optimal values for different datasets

2. **Distributional Robustness**: Test the method on datasets with non-Gaussian class distributions (e.g., CIFAR-100 or synthetic multimodal distributions) to evaluate the robustness of the theoretical guarantees beyond idealized assumptions

3. **Cross-domain Mixup**: Implement and compare same-domain mixup with cross-domain mixup (mixing samples from different classes) to understand whether class identity preservation is essential for the observed fairness improvements