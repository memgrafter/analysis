---
ver: rpa2
title: 'Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts
  in Retrieval-Augmented Language Models'
arxiv_id: '2402.14409'
source_url: https://arxiv.org/abs/2402.14409
tags:
- uni00000003
- uni00000013
- knowledge
- evidence
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates knowledge conflicts in retrieval-augmented\
  \ language models (RALMs), where conflicting internal memory and external evidence\
  \ lead to incorrect answers. The authors introduce an evaluation framework and analyze\
  \ RALM behavior across four QA datasets, finding that more capable models exhibit\
  \ the Dunning-Kruger effect\u2014over-trusting incorrect internal memory despite\
  \ contradictory evidence."
---

# Tug-of-War Between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models

## Quick Facts
- arXiv ID: 2402.14409
- Source URL: https://arxiv.org/abs/2402.14409
- Reference count: 0
- Primary result: CD2 method improves Recall by 2.35% and 2.41% on NQ and TriviaQA datasets

## Executive Summary
This paper investigates knowledge conflicts in retrieval-augmented language models (RALMs), where conflicting internal memory and external evidence lead to incorrect answers. The authors introduce an evaluation framework and analyze RALM behavior across four QA datasets, finding that more capable models exhibit the Dunning-Kruger effect—over-trusting incorrect internal memory despite contradictory evidence. They also observe availability bias toward common knowledge and confirmation bias when external evidence aligns with internal memory. To address these issues, the authors propose Conflict-Disentangle Contrastive Decoding (CD2), which calibrates model confidence by contrasting expert and amateur logits. Experiments show CD2 improves Recall by 2.35% and 2.41% on NQ and TriviaQA datasets, effectively resolving knowledge conflicts.

## Method Summary
The authors develop a comprehensive framework for studying knowledge conflicts in RALMs. They first induce internal memory beliefs through closed-book QA training, then generate conflicting evidence using counterfactuals created with ChatGPT. The Conflict-Disentangle Contrastive Decoding (CD2) method is introduced to resolve these conflicts by calibrating confidence through contrastive decoding between expert and amateur logits. The framework is evaluated across four QA datasets (NQ, TriviaQA, PopQA, MuSiQue) using multiple RALM architectures including FLAN-T5, LLaMA2, Baichuan2, and ChatGPT.

## Key Results
- More capable RALMs exhibit Dunning-Kruger effect, persistently favoring faulty internal memory even when correct evidence is provided
- RALMs show availability bias toward common knowledge and confirmation bias when evidence aligns with internal memory
- CD2 method improves Recall by 2.35% and 2.41% on NQ and TriviaQA datasets respectively
- CD2 effectively resolves knowledge conflicts by calibrating model confidence through contrastive decoding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented language models (RALMs) exhibit Dunning-Kruger effect—over-trusting incorrect internal memory despite contradictory external evidence.
- Mechanism: As RALMs increase in size and capability, their confidence in internal memory grows disproportionately. This causes them to persistently favor their faulty internal memory even when correct external evidence is provided.
- Core assumption: Model confidence correlates with size/capability, not correctness of internal knowledge.
- Evidence anchors:
  - [abstract] "We find that stronger RALMs emerge with the Dunning-Kruger effect, persistently favoring their faulty internal memory even when correct evidence is provided."
  - [section 4] "As model sizes and capabilities expand, the model gains greater confidence in its internal memory and has a certain ability to perceive conflicts; but if the model is not calibrated well to resolve conflicts, it will still answer incorrectly."
- Break condition: If model confidence calibration is properly implemented or if the model learns to accurately assess the reliability of its internal memory versus external evidence.

### Mechanism 2
- Claim: RALMs exhibit availability bias towards common knowledge and follow majority rule when faced with conflicting evidence.
- Mechanism: RALMs prefer knowledge that is easily accessible in memory (common knowledge) over less common information. When presented with multiple pieces of conflicting evidence, they lean towards evidence that appears more frequently.
- Core assumption: Model generation is influenced by token frequency and ease of recall.
- Evidence anchors:
  - [abstract] "RALMs exhibit an availability bias towards common knowledge, preferring knowledge that is easily accessible in memory."
  - [section 4] "RALMs follow the principle of majority rule, leaning towards placing trust in evidence that appears more frequently."
- Break condition: If model generation is not influenced by token frequency or if external evidence frequency does not impact model's choice.

### Mechanism 3
- Claim: RALMs exhibit confirmation bias, being more inclined to choose evidence that is consistent with their internal memory.
- Mechanism: When external sources contain evidence supporting the model's internal memory, RALMs show a stronger inclination to select that evidence, regardless of whether it is correct or incorrect.
- Core assumption: Model generation is influenced by consistency with internal beliefs.
- Evidence anchors:
  - [abstract] "RALMs exhibit confirmation bias, and are more willing to choose evidence that is consistent with their internal memory."
  - [section 5] "RALMs exhibit confirmation bias, displaying a stronger inclination to select evidence that aligns with their own internal memory."
- Break condition: If model generation is not influenced by consistency with internal beliefs or if external evidence consistency does not impact model's choice.

## Foundational Learning

- Concept: Knowledge Conflicts
  - Why needed here: Understanding how conflicting information between internal memory and external sources affects RALM performance is crucial for developing effective conflict resolution methods.
  - Quick check question: What are the two main types of knowledge conflicts discussed in the paper?

- Concept: Dunning-Kruger Effect
  - Why needed here: Recognizing this psychological phenomenon in RALMs helps explain why more capable models might still produce incorrect answers due to overconfidence in their internal memory.
  - Quick check question: How does the Dunning-Kruger effect manifest in RALMs according to the paper?

- Concept: Contrastive Decoding
  - Why needed here: This technique is used in the proposed CD2 method to amplify differences between expert and amateur logits, improving model confidence calibration.
  - Quick check question: What is the purpose of contrastive decoding in the CD2 method?

## Architecture Onboarding

- Component map: Retrieval → Evidence Integration → Confidence Calibration (CD2) → Answer Generation
- Critical path: Evidence retrieval → Internal memory integration → Confidence calibration → Final answer generation
- Design tradeoffs: Using larger models increases internal memory confidence but may lead to over-reliance on incorrect information. Providing more external evidence can help but may also overwhelm the model. The CD2 method requires additional computation during inference.
- Failure signatures: Decreased performance when conflicting evidence is present, persistent incorrect answers despite correct external evidence, overconfidence in internal memory leading to wrong answers.
- First 3 experiments:
  1. Test RALM performance on NQ-Conf dataset with and without CD2 to measure improvement in Recall.
  2. Analyze model confidence scores under knowledge conflicts to verify Dunning-Kruger effect manifestation.
  3. Evaluate model preference for common vs. rare knowledge to confirm availability bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do knowledge conflicts in RALMs differ from those in traditional language models, and what specific challenges arise due to the integration of external evidence?
- Basis in paper: [explicit] The paper explicitly states that RALMs will inevitably encounter knowledge conflicts when integrating their internal memory with external sources, leading to a "tug-of-war between knowledge."
- Why unresolved: While the paper identifies the existence of knowledge conflicts in RALMs, it does not provide a comprehensive comparison with traditional language models or a detailed analysis of the unique challenges posed by external evidence integration.
- What evidence would resolve it: A comparative study analyzing knowledge conflicts in RALMs and traditional language models, focusing on the impact of external evidence integration on conflict resolution and model performance.

### Open Question 2
- Question: How does the Dunning-Kruger effect manifest in RALMs, and what are the underlying mechanisms that lead capable models to overestimate their knowledge and persistently rely on incorrect internal memory?
- Basis in paper: [explicit] The paper identifies the Dunning-Kruger effect in capable RALMs, where they may lack confidence in their correct internal memory but stubbornly persist in their incorrect internal memory, even when provided with correct external evidence.
- Why unresolved: The paper does not provide a detailed explanation of the mechanisms behind the Dunning-Kruger effect in RALMs or explore potential strategies to mitigate this phenomenon.
- What evidence would resolve it: A comprehensive analysis of the model's internal representations and decision-making processes, along with experiments testing interventions to improve confidence calibration and reduce reliance on incorrect internal memory.

### Open Question 3
- Question: How does the proposed Conflict-Disentangle Contrastive Decoding (CD2) method perform in resolving knowledge conflicts in RALMs, and what are its limitations and potential areas for improvement?
- Basis in paper: [explicit] The paper introduces CD2 as a novel method to resolve knowledge conflicts by calibrating the model's confidence. Experimental results demonstrate its effectiveness in improving Recall on NQ and TriviaQA datasets.
- Why unresolved: The paper does not provide a thorough evaluation of CD2's performance across different RALM architectures, knowledge conflict types, or datasets. Additionally, the potential limitations and areas for improvement of the method are not discussed.
- What evidence would resolve it: Extensive experiments evaluating CD2's performance on diverse RALM architectures, knowledge conflict scenarios, and datasets, along with a detailed analysis of its limitations and suggestions for future improvements.

## Limitations

- The analysis focuses primarily on English Wikipedia-based datasets, which may limit generalizability to other knowledge domains or languages.
- The exact mechanism by which contrastive decoding resolves knowledge conflicts remains somewhat underspecified.
- The conflict generation process using counterfactuals introduces potential artifacts that could influence the observed effects.

## Confidence

- **High Confidence**: The empirical results showing CD2 improves Recall by 2.35% and 2.41% on NQ and TriviaQA datasets are well-supported by the experimental data presented.
- **Medium Confidence**: The observations of Dunning-Kruger effect, availability bias, and confirmation bias in RALMs are supported by behavioral evidence but would benefit from more direct measurement of the underlying mechanisms.
- **Low Confidence**: The exact causal mechanisms by which model size influences overconfidence in internal memory versus external evidence require further validation beyond the correlational observations presented.

## Next Checks

1. **Ablation Study on Conflict Generation**: Test whether the observed Dunning-Kruger effect persists when using different methods for generating conflicting evidence (e.g., manual annotation vs. automated counterfactuals) to ensure the effect isn't an artifact of the generation process.

2. **Cross-Domain Generalization Test**: Evaluate CD2's effectiveness on non-Wikipedia knowledge sources (e.g., scientific literature, news articles) to verify the method's robustness across different knowledge domains.

3. **Confidence Calibration Analysis**: Conduct a detailed analysis of model confidence scores before and after CD2 application, specifically measuring whether the method actually improves calibration (e.g., using expected calibration error metrics) rather than just shifting confidence distributions.