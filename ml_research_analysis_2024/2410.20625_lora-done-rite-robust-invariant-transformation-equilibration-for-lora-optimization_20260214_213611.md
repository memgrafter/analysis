---
ver: rpa2
title: 'LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization'
arxiv_id: '2410.20625'
source_url: https://arxiv.org/abs/2410.20625
tags:
- lora
- learning
- invariance
- transformation
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LoRA-RITE, a novel optimizer for LoRA fine-tuning
  that addresses the problem of transformation invariance. Most existing optimizers,
  when applied to LoRA, do not satisfy transformation invariance, leading to inefficient
  learning and suboptimal solutions.
---

# LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization

## Quick Facts
- arXiv ID: 2410.20625
- Source URL: https://arxiv.org/abs/2410.20625
- Authors: Jui-Nan Yen; Si Si; Zhao Meng; Felix Yu; Sai Surya Duvvuri; Inderjit S. Dhillon; Cho-Jui Hsieh; Sanjiv Kumar
- Reference count: 16
- Key outcome: Introduces LoRA-RITE optimizer that achieves transformation invariance for LoRA fine-tuning, yielding 3.5-4.6% accuracy gains over Adam on LLM benchmarks

## Executive Summary
This paper addresses a fundamental problem in LoRA optimization: most existing optimizers are not transformation invariant, leading to inefficient learning and suboptimal solutions. The authors introduce LoRA-RITE (Robust Invariant Transformation Equilibration), a novel optimizer that achieves transformation invariance through unmagnified gradients and matrix preconditioning on the low-rank side. By ensuring equivalent LoRA factorizations produce identical updates, LoRA-RITE improves feature learning efficiency and overall performance. Experiments demonstrate consistent improvements across Gemma 2B, 7B, and mT5-XXL models, with LoRA-RITE achieving 4.6% accuracy gain on Super-Natural Instructions and 3.5% across other LLM benchmarks.

## Method Summary
LoRA-RITE addresses the transformation invariance problem in LoRA optimization by using unmagnified gradients and one-sided matrix preconditioning. The method computes gradients with respect to the column space of LoRA factors rather than the factors themselves, ensuring equivalent factorizations produce identical updates. It maintains first and second moments with basis-adjusted accumulation, tracking escaped mass to compensate for information loss during basis changes. The optimizer applies matrix preconditioning only to the rank dimension (size r) rather than both dimensions, maintaining computational efficiency with O(mr + nr) space complexity. The approach incorporates adaptive learning rates while preserving transformation invariance, making it suitable for practical fine-tuning scenarios.

## Key Results
- LoRA-RITE achieves 4.6% accuracy gain over Adam when fine-tuning Gemma-2B on Super-Natural Instructions
- Consistent 3.5% accuracy improvement across four other LLM benchmarks (HellaSwag, ArcChallenge, GSM8K, OpenBookQA)
- Maintains similar time and space complexity to Adam when LoRA rank is small compared to original matrix dimensions
- Performance improvements consistent across different ranks (4 and 16) and model sizes (Gemma 2B, 7B, mT5-XXL)

## Why This Works (Mechanism)

### Mechanism 1
LoRA-RITE achieves transformation invariance by using unmagnified gradients and matrix preconditioning on the low-rank side. The optimizer computes gradients with respect to the column space of LoRA factors rather than the factors themselves, ensuring equivalent LoRA factorizations produce identical updates. The method applies adaptive matrix preconditioning only to the shorter dimension (rank r) rather than both dimensions, maintaining computational efficiency. The core assumption is that the column space representation is sufficient for transformation-invariant optimization, and matrix preconditioning on the rank dimension provides adequate adaptation without requiring full matrix operations.

### Mechanism 2
The optimizer incorporates first and second moments while maintaining transformation invariance by adjusting them for changes in basis at each step using projection matrices. The second moment is updated with a transformation that accounts for the varying basis, and escaped mass is tracked to compensate for information loss during basis changes. The core assumption is that accumulated moments can be transformed appropriately across basis changes without losing critical optimization information, and the escaped mass compensation is sufficient to maintain optimization quality.

### Mechanism 3
One-sided matrix preconditioning provides computational efficiency while maintaining effectiveness. Instead of preconditioning both dimensions of the gradient matrix (which would require O(m² + n²) space), LoRA-RITE preconditions only the rank dimension (size r), reducing space complexity to O(mr + nr) and time complexity to O(mr² + nr² + r³). The core assumption is that preconditioning only the rank dimension is sufficient to achieve transformation invariance and maintain optimization quality, and the rank r is small enough relative to m and n to make this approach computationally efficient.

## Foundational Learning

- **Concept:** Transformation invariance in optimization
  - **Why needed here:** LoRA factors can represent the same update in multiple ways through different factorizations, and optimizers should produce the same update regardless of the specific factorization to ensure consistent and efficient learning.
  - **Quick check question:** If A₁B₁ᵀ = A₂B₂ᵀ, should an optimizer produce the same update to Z regardless of whether it uses (A₁, B₁) or (A₂, B₂)?

- **Concept:** Matrix preconditioning vs diagonal preconditioning
  - **Why needed here:** Diagonal preconditioners (like Adam) cannot achieve transformation invariance for LoRA because they cannot account for the rotational/scaling degrees of freedom in LoRA factorizations, while matrix preconditioners can.
  - **Quick check question:** Why does Adam fail to achieve transformation invariance for LoRA, and what mathematical property of diagonal matrices causes this failure?

- **Concept:** Efficient feature learning in LoRA
  - **Why needed here:** Traditional optimizers for LoRA often cause one LoRA factor to dominate updates while the other remains nearly fixed, leading to inefficient learning. Transformation-invariant optimizers can achieve balanced updates and more efficient feature learning.
  - **Quick check question:** What does it mean for LoRA training to achieve "efficient feature learning," and how does transformation invariance help achieve this?

## Architecture Onboarding

- **Component map:** Input LoRA factors A (m×r) and B (n×r) -> QR decomposition of B -> Unmagnified gradients ¯∇A = ∇A R⁻¹B, ¯∇B = ∇B R⁻¹A -> Preconditioner ¯VAt with basis-adjusted accumulation -> Update δA = -¯MAt R⁻⊤B

- **Critical path:** 1) Forward pass to compute loss, 2) Backward pass to compute gradients ∇A and ∇B, 3) QR decomposition of B to get basis UB, 4) Compute unmagnified gradients ¯∇A and ¯∇B, 5) Update accumulated moments ¯VAt and ¯MAt with basis adjustments, 6) Compute update δA and δB, 7) Apply updates to LoRA factors

- **Design tradeoffs:** One-sided vs two-sided preconditioning (one-sided is computationally efficient but may be less powerful), basis adjustment frequency (more frequent adjustments maintain better invariance but increase computational overhead), rank selection (higher rank provides more representational power but increases computational cost and memory usage)

- **Failure signatures:** One LoRA factor stops updating while the other continues to change (indicating loss of transformation invariance), training becomes unstable or diverges (potentially due to improper basis adjustments or moment accumulation), performance improvements are marginal compared to baseline optimizers (suggesting the rank is too small or the preconditioning is insufficient)

- **First 3 experiments:** 1) Implement LoRA-RITE with rank 4 on Gemma-2B fine-tuning task and compare training curves with Adam, 2) Test transformation invariance by initializing two equivalent LoRA factorizations and verifying they produce identical updates, 3) Measure computational overhead by timing training steps and comparing memory usage with Adam across different model sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of LoRA rank (r) affect the trade-off between computational efficiency and performance for LoRA-RITE compared to other optimizers?
- Basis in paper: [explicit] The paper mentions an ablation study on different ranks (4 and 16) for Gemma 2B and mT5-XXL, showing consistent performance across ranks, but does not provide a comprehensive analysis of the trade-off across a wider range of ranks.
- Why unresolved: The paper does not provide a detailed analysis of how varying the LoRA rank affects the computational efficiency and performance trade-off for LoRA-RITE compared to other optimizers across a broader range of ranks.
- What evidence would resolve it: A systematic study varying the LoRA rank across a wider range and comparing the computational efficiency and performance of LoRA-RITE with other optimizers would provide insights into the optimal rank selection for different use cases.

### Open Question 2
- Question: Can LoRA-RITE be effectively extended to other parameter-efficient fine-tuning (PEFT) methods beyond LoRA, such as prefix tuning or adapter-based methods?
- Basis in paper: [inferred] The paper focuses on LoRA optimization and does not explore the applicability of the transformation invariance principle to other PEFT methods.
- Why unresolved: The paper does not investigate whether the transformation invariance principle and the proposed LoRA-RITE algorithm can be adapted to other PEFT methods that have different parameterizations and optimization challenges.
- What evidence would resolve it: Experiments applying LoRA-RITE or its underlying principles to other PEFT methods and comparing the results with existing optimizers for those methods would determine the generalizability of the approach.

### Open Question 3
- Question: How does LoRA-RITE perform on larger language models (e.g., models with hundreds of billions of parameters) compared to smaller models like Gemma 2B and 7B?
- Basis in paper: [inferred] The paper evaluates LoRA-RITE on Gemma 2B, 7B, and mT5-XXL, but does not test its performance on significantly larger models.
- Why unresolved: The paper does not provide evidence of LoRA-RITE's effectiveness on much larger language models, which are increasingly common and pose different optimization challenges.
- What evidence would resolve it: Benchmarking LoRA-RITE on larger language models and comparing its performance and computational efficiency with other optimizers would demonstrate its scalability and potential advantages for training state-of-the-art models.

## Limitations
- The theoretical foundation for why transformation invariance specifically improves LoRA performance remains underdeveloped
- Implementation details are not fully specified, making reproduction challenging without significant engineering effort
- Comparison with other advanced optimizers (like Shampoo) could be more comprehensive

## Confidence
- **High confidence:** The mathematical framework for unmagnified gradients and one-sided preconditioning is internally consistent and addresses a well-defined problem in LoRA optimization
- **Medium confidence:** The empirical improvements are substantial and consistent across multiple datasets and model sizes, though the sample size of experiments is limited
- **Low confidence:** The theoretical justification for why transformation invariance specifically improves LoRA performance beyond what can be explained by standard optimization dynamics

## Next Checks
1. Implement a minimal version of LoRA-RITE and verify transformation invariance by comparing updates from equivalent LoRA factorizations on a simple regression task
2. Conduct ablation studies to isolate the contribution of unmagnified gradients versus matrix preconditioning to overall performance improvements
3. Test the method on a wider range of model sizes and tasks, particularly focusing on scenarios where LoRA rank is not negligible compared to original matrix dimensions