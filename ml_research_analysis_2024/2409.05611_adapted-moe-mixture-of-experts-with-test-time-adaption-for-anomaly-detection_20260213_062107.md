---
ver: rpa2
title: 'Adapted-MoE: Mixture of Experts with Test-Time Adaption for Anomaly Detection'
arxiv_id: '2409.05611'
source_url: https://arxiv.org/abs/2409.05611
tags:
- anomaly
- samples
- feature
- detection
- normal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses unsupervised anomaly detection in industrial
  quality control, where real-world data shows variation within categories and distribution
  shifts between training and test sets. Existing methods using single decision boundaries
  struggle with these variations.
---

# Adapted-MoE: Mixture of Experts with Test-Time Adaption for Anomaly Detection

## Quick Facts
- arXiv ID: 2409.05611
- Source URL: https://arxiv.org/abs/2409.05611
- Reference count: 13
- Key outcome: Outperforms state-of-the-art methods with 2.18%-7.20% and 1.57%-16.30% improvements in I-AUROC and P-AUROC respectively

## Executive Summary
This paper addresses unsupervised anomaly detection in industrial quality control where real-world data shows variation within categories and distribution shifts between training and test sets. Existing methods using single decision boundaries struggle with these variations. The proposed Adapted-MoE uses a Mixture of Experts with a routing network to divide samples into subclasses and train separate expert models for each. A test-time adaptation normalizes features to handle distribution shifts. Experiments on the Texture AD benchmark show significant improvements, outperforming state-of-the-art methods.

## Method Summary
The method uses a pre-trained WideResNet50 as a frozen feature extractor, followed by a routing network with center loss to assign samples to expert models. Each expert is a multi-layer perceptron trained on its assigned subclass. During inference, test samples are routed to the most appropriate expert and undergo test-time adaptation through mean and variance normalization to align with the expert's learned distribution. The total loss combines routing network loss and expert model losses.

## Key Results
- Achieves 2.18%-7.20% improvements in I-AUROC over state-of-the-art methods
- Achieves 1.57%-16.30% improvements in P-AUROC over state-of-the-art methods
- Outperforms baseline SimpleNet and other methods (EfficientAD, PyramidFlow, DRAEM, Mean-Shift, MSFlow) on all three categories (cloth, metal, wafer)

## Why This Works (Mechanism)

### Mechanism 1
The routing network effectively partitions normal samples into multiple subclasses based on feature distribution similarity, allowing each expert to learn a more homogeneous decision boundary. The routing network uses a center loss formulation where feature embeddings are clustered around subclass centers, with cosine similarity scoring routing samples to the closest expert. This divide-and-conquer approach reduces the complexity of the anomaly decision boundary problem.

### Mechanism 2
Test-time adaptation normalizes test sample features to match the learned distribution of the assigned expert, eliminating distribution shift bias between training and test data. The adaptation calculates the mean and standard deviation of the routed test embedding, then scales and shifts it to align with the expert's learned center and variance before passing it to the expert model.

### Mechanism 3
The combination of MoE and test-time adaptation outperforms either approach alone because they address orthogonal challenges in anomaly detection. MoE handles intra-category variation by learning multiple decision boundaries, while test-time adaptation handles inter-set distribution shift. Together they address both sources of uncertainty.

## Foundational Learning

- Concept: Feature embedding and representation learning
  - Why needed here: The entire method relies on extracting meaningful feature embeddings from images using pre-trained networks, then manipulating these embeddings for routing and anomaly detection
  - Quick check question: How does the projection layer transform the extracted feature maps into the anomaly detection feature space?

- Concept: Center loss and clustering objectives
  - Why needed here: The routing network uses center loss to group samples into subclasses, which requires understanding how this loss function works differently from standard classification losses
  - Quick check question: What is the role of the center update in the routing network's center loss formulation?

- Concept: Domain adaptation and distribution shift
  - Why needed here: Test-time adaptation specifically addresses distribution shift between training and test data, requiring understanding of how feature distributions can vary across datasets
  - Quick check question: Why might the same category of samples have different distributions in training versus test sets?

## Architecture Onboarding

- Component map:
  Feature extractor (frozen pre-trained CNN) → Routing network (center loss + classifier) → Test-time adaptation (normalization) → Expert models (MLP + anomaly detection) → Score aggregation

- Critical path:
  During training: Feature extraction → Routing network loss computation → Expert model loss computation → Total loss backprop
  During inference: Feature extraction → Routing network → Test-time adaptation → Expert model → Score aggregation

- Design tradeoffs:
  - Number of experts vs. computational cost: More experts can capture more subclasses but increase inference time
  - Routing network complexity vs. generalization: More complex routing may overfit to training subclasses
  - Test-time adaptation strength vs. stability: Stronger adaptation may correct more bias but risk introducing artifacts

- Failure signatures:
  - Poor routing: Samples from the same subclass routed to different experts, visible as high variance in expert outputs
  - Ineffective adaptation: Expert outputs show clear distribution shift patterns between training and test samples
  - Expert overfitting: Individual experts perform well on their training subclasses but poorly on others

- First 3 experiments:
  1. Validate routing network on a simple dataset: Train with 2-3 experts and visualize routing decisions to ensure subclasses are being properly separated
  2. Test adaptation effectiveness: Compare expert outputs with and without adaptation on a validation set with known distribution shift
  3. Ablation study on expert count: Measure performance with 1, 2, 4, 8 experts to find the optimal number for your dataset

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Adapted-MoE scale with the number of subclasses and experts when applied to datasets with significantly higher intra-category variation than the Texture AD benchmark? The paper demonstrates effectiveness on Texture AD with limited subclasses but notes that improvement is constrained by over-division in low-diversity scenarios and overfitting with complex expert models.

### Open Question 2
What is the theoretical limit of test-time adaptation in handling distribution shifts between training and test sets, and how can we quantify when this approach becomes ineffective? The paper proposes test-time adaptation to eliminate bias between unseen samples and learned distributions, but acknowledges that even with MoE, test samples may still have distribution bias that needs calibration.

### Open Question 3
How does the choice of feature extractor (frozen backbone) affect the routing network's ability to distinguish subclasses, and could learning-based feature adaptation improve performance? The paper uses frozen pre-trained CNNs on ImageNet for feature extraction, noting this is common practice, but doesn't explore alternatives or analyze the impact on routing quality.

## Limitations

- Weak empirical validation of core mechanisms with limited ablation studies and diagnostic visualizations
- Limited external validation as corpus analysis shows no direct supporting evidence from related papers
- Theoretical mechanism explanations with minimal empirical support for routing network effectiveness and test-time adaptation

## Confidence

- Medium: Overall performance improvement claims (2.18%-7.20% I-AUROC gains) are supported by experimental results on the Texture AD benchmark
- Low: Mechanism explanations for how routing and test-time adaptation work are largely theoretical with minimal empirical validation
- Medium: Architecture design choices appear reasonable given the problem domain, though specific hyperparameters are not fully justified

## Next Checks

1. **Routing effectiveness validation**: Create a synthetic dataset with known subclass structure and visualize routing decisions to verify the network correctly separates subclasses
2. **Adaptation sensitivity analysis**: Systematically vary the strength of test-time adaptation and measure its impact on performance across different levels of distribution shift
3. **Expert count optimization**: Conduct systematic experiments with varying numbers of experts (1, 2, 4, 8) on multiple datasets to determine optimal configuration and identify diminishing returns