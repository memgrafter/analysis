---
ver: rpa2
title: 'Lacuna Language Learning: Leveraging RNNs for Ranked Text Completion in Digitized
  Coptic Manuscripts'
arxiv_id: '2407.12247'
source_url: https://arxiv.org/abs/2407.12247
tags: []
core_contribution: This paper applies neural language modeling to the reconstruction
  of lacunae (missing text gaps) in ancient Coptic manuscripts. The authors train
  a bidirectional RNN with character-level masked language modeling on nearly 1.22
  million Coptic tokens.
---

# Lacuna Language Learning: Leveraging RNNs for Ranked Text Completion in Digitized Coptic Manuscripts

## Quick Facts
- arXiv ID: 2407.12247
- Source URL: https://arxiv.org/abs/2407.12247
- Reference count: 9
- Primary result: 72% accuracy for single-character reconstruction, 37% for lacunae of 6+ characters

## Executive Summary
This paper applies neural language modeling to reconstruct missing text (lacunae) in ancient Coptic manuscripts. The authors train a bidirectional RNN with character-level masked language modeling on nearly 1.22 million Coptic tokens. While the model achieves strong performance for single-character reconstruction (72% accuracy), accuracy drops significantly for longer gaps (37% for 6+ characters). The approach is positioned as a ranking tool rather than a definitive reconstruction system, providing scholars with quantitative evidence to complement traditional qualitative methods.

## Method Summary
The authors train a bidirectional LSTM with 4 layers (300 hidden units each) on Coptic text using character-level masked language modeling. The model uses SentencePiece for a 134-character vocabulary including Coptic alphabet, punctuation, and control symbols. Two masking strategies are tested: random masking (15% of characters) and smart masking that mimics lacuna distribution. The best-performing configuration uses random-dynamic masking. The model is evaluated on three test sets: randomly masked data, smart masked data, and manually reconstructed lacunae from actual manuscripts.

## Key Results
- 72% accuracy for single-character reconstruction on reconstructed lacuna test set
- 37% accuracy for lacunae of 6+ characters on the same test set
- Random-dynamic masking strategy outperforms other masking approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bidirectional RNN with character-level masked language modeling can infer missing characters in Coptic manuscripts by leveraging both left and right context.
- Mechanism: The bidirectional LSTM processes the sequence in both directions, allowing each hidden state to incorporate information from before and after the lacuna. During training, 15% of characters are randomly masked and the model learns to predict them based on surrounding context.
- Core assumption: The missing character can be inferred from local context within a sentence.
- Evidence anchors:
  - [abstract]: "we present a bidirectional RNN model for character prediction of Coptic characters in manuscript lacunae"
  - [section 4]: "we implement a character based bidirectional RNN model, trained with a character-level masked language modeling task"
- Break condition: When lacunae are longer than 6 characters or when context is insufficient due to complex syntax or rare vocabulary.

### Mechanism 2
- Claim: Random masking during training creates a robust model that generalizes to unpredictable lacuna positions in real manuscripts.
- Mechanism: By randomly masking 15% of characters during training, the model learns to handle gaps at any position rather than memorizing specific patterns. The dynamic masking strategy re-masks at each epoch, forcing the model to adapt to new gap locations.
- Core assumption: Random gaps in training data simulate the unpredictable nature of manuscript damage.
- Evidence anchors:
  - [section 4]: "For the first masking strategy, which we refer to as 'random masking', we used the BERT masking strategy of randomly masking 15% of the characters"
  - [section 5.2]: "Out of the four different masking strategies we explored, we found that the model utilizing the random-dynamic masking strategy had the highest performance"
- Break condition: When training data distribution doesn't match real lacuna distribution (e.g., too many short gaps in training when real manuscripts have longer gaps).

### Mechanism 3
- Claim: Character-level embeddings capture morphological patterns in Coptic that help predict missing characters.
- Mechanism: The 134-character vocabulary includes Coptic alphabet, punctuation, and special symbols. The embedding layer maps each character to a 200-dimensional space where morphologically related characters are closer together, helping the model learn patterns like prefixes, suffixes, and common letter combinations.
- Core assumption: Morphological patterns in Coptic are consistent enough to be learned from character co-occurrence statistics.
- Evidence anchors:
  - [section 4]: "We start with a character-level vocabulary and embedding layer, generated with SentencePiece. The vocabulary is 134 characters, including some control symbols, the mask token, the lower-cased Coptic alphabet, and some punctuation"
- Break condition: When Coptic morphology is irregular or when the training data is too small to capture all morphological patterns.

## Foundational Learning

- Concept: Bidirectional processing
  - Why needed here: Lacunae can appear anywhere in a sentence, and both preceding and following context may be necessary to predict missing characters accurately.
  - Quick check question: If you only had left context for a lacuna, what information would you lose that could help predict the missing characters?

- Concept: Masked language modeling
  - Why needed here: The model needs to learn to predict missing information, which is exactly what masked language modeling trains for by hiding random tokens and requiring predictions.
  - Quick check question: How does masking 15% of characters during training differ from the actual use case where we need to predict entire lacunae?

- Concept: Character-level vs. word-level modeling
  - Why needed here: Coptic has complex morphology and irregular orthography, making character-level modeling more appropriate than word-level for handling rare words and morphological variations.
  - Quick check question: Why might character-level modeling be more robust than word-level for a language with many morphological variations and rare words?

## Architecture Onboarding

- Component map:
  Input -> SentencePiece encoding -> Embedding layer (200-dim) -> 4-layer bidirectional LSTM (300 hidden units) -> Projection layer (150-dim) -> Output layer -> Character prediction probabilities

- Critical path:
  1. Preprocess text with SentencePiece to create character vocabulary
  2. Create mask vectors (random or smart strategy)
  3. Forward pass through bidirectional LSTM layers
  4. Apply projection layer
  5. Compute predictions only for masked positions
  6. Calculate loss and backpropagate

- Design tradeoffs:
  - RNN vs. Transformer: RNNs better handle smaller datasets and don't require complex attention mechanisms; Transformers would need more data and computational resources
  - Character vs. subword level: Character level handles morphological complexity but increases sequence length; subword level reduces sequence length but may miss fine-grained patterns
  - Random vs. smart masking: Random masking generalizes better but doesn't match real lacuna distribution; smart masking matches real data but may overfit to specific patterns

- Failure signatures:
  - Model predicts same character regardless of context: Embedding layer or LSTM initialization issues
  - Performance drops sharply with longer lacunae: Context window too small or model can't capture long-distance dependencies
  - Model overfits to training data: Insufficient regularization or training data too small

- First 3 experiments:
  1. Train with only left-to-right LSTM (not bidirectional) on random masking data to verify bidirectional context helps
  2. Compare random masking vs. smart masking on dev set to confirm random masking generalizes better
  3. Test character-level vs. word-level models on short lacunae to verify character-level approach is superior for Coptic morphology

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific transformer architectures (beyond RNNs) would be most effective for Coptic lacuna reconstruction, and how do they compare to the current RNN approach?
- Basis in paper: [explicit] The authors state that while they focused on RNN-based architectures, they believe exploring transformer architectures would be worthwhile in future work.
- Why unresolved: The paper only mentions preliminary prototyping with transformers like ELECTRA but does not present detailed results or comparisons.
- What evidence would resolve it: Direct experimental comparisons between RNN and transformer models (BERT, ELECTRA, etc.) on the same Coptic reconstruction task, measuring accuracy across different lacuna lengths.

### Open Question 2
- Question: How would incorporating linguistic annotations (morphosyntactic information, dictionary lookups) affect model performance compared to the current character-level approach?
- Basis in paper: [explicit] The authors identify this as a limitation and suggest future work could benefit from incorporating lexicographic information and linguistic annotations from Coptic treebank data.
- Why unresolved: The current model only uses character-level information without leveraging available linguistic resources.
- What evidence would resolve it: Experiments training models with additional linguistic features (POS tags, morphological features, dictionary constraints) and measuring improvements in reconstruction accuracy, particularly for longer lacunae.

### Open Question 3
- Question: What impact would document-level context (rather than sentence-level context) have on the model's ability to reconstruct lacunae?
- Basis in paper: [explicit] The authors note their model is trained on a sentence-wise basis without incorporating document-level information such as surrounding sentences or page layout details.
- Why unresolved: The current approach treats each sentence in isolation, potentially missing important contextual cues available at the document level.
- What evidence would resolve it: Comparative experiments using models trained with document-level context (neighboring sentences, page structure) versus sentence-level models, measuring performance differences especially for ambiguous reconstructions.

## Limitations
- Model accuracy drops significantly for longer lacunae (37% for 6+ characters vs 72% for single characters)
- Limited to sentence-level context without document-level information
- Does not incorporate available linguistic annotations or lexicographic information

## Confidence

High confidence in the bidirectional RNN architecture effectiveness for short lacunae (1-3 characters)
Medium confidence in the character-level modeling approach for Coptic morphological patterns
Medium confidence in the random masking strategy generalization benefits
Low confidence in the model's ability to handle lacunae longer than 6 characters

## Next Checks

1. **Cross-linguistic validation**: Test the same architecture on another morphologically rich language with available manuscript data (e.g., Ancient Greek or Latin) to determine if the approach generalizes beyond Coptic.

2. **Error analysis for longer lacunae**: Systematically analyze the 63% error rate for 6+ character lacunae to identify whether failures stem from insufficient context, rare vocabulary, or morphological complexity, then test targeted architectural modifications.

3. **Human-in-the-loop evaluation**: Conduct a study where scholars use the model's ranked outputs to assist in actual manuscript reconstruction, measuring whether the rankings meaningfully accelerate the scholarly process compared to traditional methods.