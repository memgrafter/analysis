---
ver: rpa2
title: 'Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language
  Models'
arxiv_id: '2404.02936'
source_url: https://arxiv.org/abs/2404.02936
tags:
- min-k
- training
- data
- input
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of detecting whether a text sample
  was used in the pre-training data of a large language model (LLM), a task important
  for copyright compliance and data privacy. The core method, Min-K%++, is motivated
  by the insight that training samples tend to be local maxima or near local maxima
  along each input dimension under maximum likelihood training.
---

# Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models

## Quick Facts
- arXiv ID: 2404.02936
- Source URL: https://arxiv.org/abs/2404.02936
- Reference count: 24
- Key outcome: Achieves 6.2%-10.5% AUROC improvement over previous best method on WikiMIA and MIMIR benchmarks

## Executive Summary
This work addresses the critical problem of detecting whether text samples were used in pre-training data of large language models, important for copyright compliance and data privacy. The core method, Min-K%++, leverages the insight that training samples form local maxima in the learned distribution under maximum likelihood training. By comparing token probabilities to expected vocabulary statistics using z-score normalization, Min-K%++ achieves state-of-the-art performance without requiring reference models.

## Method Summary
Min-K%++ detects pre-training data by scoring text sequences based on whether they form modes or have relatively high probability under the LLM's conditional categorical distribution. The method computes token-wise scores using the difference between log probability of target token and expected log probability over vocabulary, normalized by standard deviation. These scores are aggregated by averaging the minimum k% token scores to produce a final sequence score. The approach requires only grey-box access to pre-trained LLMs (logits, token probabilities, and loss) and no additional training.

## Key Results
- Achieves 6.2%-10.5% AUROC improvement over previous best method on WikiMIA and MIMIR benchmarks
- Outperforms reference-based methods without requiring an extra reference model
- Shows consistent performance across 10 different models including Pythia, GPT-NeoX,