---
ver: rpa2
title: In-Context Learning with Topological Information for Knowledge Graph Completion
arxiv_id: '2412.08742'
source_url: https://arxiv.org/abs/2412.08742
tags:
- node
- individual
- missing
- graph
- organization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach for knowledge graph completion
  (KGC) using in-context learning with large language models (LLMs). The method constructs
  an ontology from the graph and leverages graph topology, including paths between
  nodes, to provide structured hints to LLMs for predicting missing links.
---

# In-Context Learning with Topological Information for Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2412.08742
- Source URL: https://arxiv.org/abs/2412.08742
- Authors: Udari Madhushani Sehwag; Kassiani Papasotiriou; Jared Vann; Sumitra Ganesh
- Reference count: 27
- Primary result: Novel approach for knowledge graph completion using in-context learning with LLMs, achieving Hit@1 scores up to 0.178 in transductive settings

## Executive Summary
This paper presents a novel approach for knowledge graph completion (KGC) using in-context learning with large language models (LLMs). The method constructs an ontology from the graph and leverages graph topology, including paths between nodes, to provide structured hints to LLMs for predicting missing links. By combining ontological knowledge and candidate solutions from the graph structure, the approach improves KGC performance in both transductive and inductive settings. Experimental results on ILPC-small and ILPC-large datasets show significant improvements over baseline methods, with Hit@1 scores reaching up to 0.174-0.178 in transductive settings and 0.138-0.152 in inductive settings. The method demonstrates the potential of integrating graph topology with LLM capabilities for effective knowledge graph completion.

## Method Summary
The proposed method uses GPT-4 for knowledge graph completion by providing ontology-derived hints and candidate solutions within the context window. First, an ontology is constructed from the knowledge graph using the LLM's domain understanding. For each test triplet with a missing node, the method infers the node's category using the ontology, generates candidate solutions from the training graph (in transductive setting), and applies chain-of-thought reasoning with relevant hints. The LLM then predicts the missing node from the constrained set of candidates. The approach handles GPT-4's context window limitations through batch-wise processing of candidate nodes.

## Key Results
- GPT-4 + ontology hints + candidate solutions achieves Hit@1 scores of 0.174 and 0.177 on ILPC-small and ILPC-large datasets respectively in transductive setting
- The method outperforms baselines (IndNodePiece, IndNodePieceGNN) by 0.092-0.104 points in Hit@1
- In inductive setting, the approach achieves Hit@1 scores of 0.138-0.152, demonstrating effectiveness with disjoint training and test nodes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM's inherent knowledge about entity types and relationships can be leveraged to infer missing graph nodes.
- Mechanism: By providing the LLM with ontology-derived hints about the expected type of missing node, it uses its pre-trained understanding of entity categories to make more accurate predictions.
- Core assumption: The LLM has sufficient pre-trained knowledge about the types of entities and relationships present in the knowledge graph domain.
- Evidence anchors:
  - [abstract] "By integrating ontological knowledge and graph structure into the context of LLMs, our approach achieves strong performance in the transductive setting"
  - [section 4.1] "We introduce a two-step process: first, we construct an ontology from the knowledge graph using the LLM's domain understanding, capturing the types of nodes and relationships in the graph"
  - [corpus] Weak - corpus papers focus on subgraph-based methods rather than pure LLM reasoning
- Break condition: If the LLM lacks sufficient pre-trained knowledge about the domain or if the ontology categories are too ambiguous.

### Mechanism 2
- Claim: Providing candidate solutions from the graph structure improves LLM prediction accuracy by constraining the search space.
- Mechanism: The method identifies nodes from the training graph that match the expected category of the missing node, then uses the LLM to select the most appropriate candidate from this constrained set.
- Core assumption: The correct answer is likely to be among nodes of the same category that already exist in the training graph.
- Evidence anchors:
  - [section 4.3] "we create candidate solutions vcandidatei âˆˆ V train that are of category cvj and prompt the LLM to use the list of candidates as an hint"
  - [section 5, Finding 2] "GPT-4 + candidate solutions achieves Hit@1 scores of 0.172 and 0.177 on the ILPC-small and ILPC-large datasets, respectively"
  - [corpus] Missing - corpus papers focus on contrastive learning approaches rather than candidate generation
- Break condition: If the training graph lacks sufficient diversity in node categories or if the correct answer doesn't exist in the training data.

### Mechanism 3
- Claim: Chain-of-thought reasoning guides the LLM through intermediate steps to reach better predictions.
- Mechanism: The LLM is prompted with a series of reasoning steps that help it connect the available information (ontology paths, graph structure) to the missing node prediction.
- Core assumption: LLMs can effectively use intermediate reasoning steps to improve prediction accuracy when provided with structured guidance.
- Evidence anchors:
  - [section 4.4] "we employ chain-of-thought (CoT) reasoning to guide the LLM in making informed predictions about missing nodes in the knowledge graph"
  - [section 5, Finding 1] "GPT-4, even without any additional context or information, performs significantly better than the baselines"
  - [corpus] Weak - corpus papers focus on training methods rather than inference-time reasoning strategies
- Break condition: If the LLM cannot effectively process the chain-of-thought prompts or if the intermediate steps are too complex.

## Foundational Learning

- Concept: Knowledge Graph Structure and Triplets
  - Why needed here: Understanding the basic structure of KGs (nodes, relations, triplets) is essential for implementing the completion algorithm
  - Quick check question: Given a triplet (Paris, capitalOf, France), what are the head node, relation, and tail node?

- Concept: Transductive vs Inductive Learning
  - Why needed here: The method has different implementations for transductive (test nodes in training set) and inductive (test nodes disjoint from training set) settings
  - Quick check question: In inductive setting, can the model see the test nodes during training? (Answer: No)

- Concept: Ontology and Category Inference
  - Why needed here: The method constructs ontologies to infer node categories, which are used as hints for the LLM
  - Quick check question: If a relation is "bornIn" and we have examples like (John Lennon, bornIn, Liverpool), what category might the LLM infer for Liverpool?

## Architecture Onboarding

- Component map:
  Ontology Generator -> Candidate Generator -> Chain-of-Thought Engine -> LLM Interface -> Evaluation Module

- Critical path:
  1. Build ontology from training graph
  2. For each test triplet with missing node:
     - Infer node category using ontology
     - Generate candidate solutions (transductive) or use category inference (inductive)
     - Apply chain-of-thought reasoning with relevant hints
     - Get prediction from LLM
  3. Evaluate using Hit@k metrics

- Design tradeoffs:
  - Context window vs. candidate node coverage: Batch processing needed due to GPT-4's 32k token limit
  - Ontology granularity vs. LLM understanding: More specific categories may improve accuracy but risk LLM confusion
  - Prompt complexity vs. response reliability: More detailed prompts may help reasoning but could lead to inconsistent outputs

- Failure signatures:
  - Low Hit@1 but high Hit@10: LLM struggles to identify top candidate but often includes correct answer in top 10
  - Consistently wrong category predictions: Ontology inference is failing
  - Long response times: Chain-of-thought prompts may be too complex

- First 3 experiments:
  1. Test vanilla GPT-4 predictions on ILPC-small without any context to establish baseline
  2. Implement ontology hint generation and test on a small subset of the data
  3. Add candidate solution generation for transductive setting and compare performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method scale with the size of the candidate node set provided to the LLM?
- Basis in paper: [explicit] The paper mentions that increasing the number of candidate nodes given in the context improves performance, as shown in Table 4.
- Why unresolved: While the paper demonstrates that performance improves with more candidate nodes, it does not explore the upper limit of this improvement or the computational cost associated with larger candidate sets.
- What evidence would resolve it: Conducting experiments with varying sizes of candidate node sets (e.g., 100, 500, 1000) and measuring both performance metrics and computational time would provide insights into the scalability and efficiency of the method.

### Open Question 2
- Question: How does the proposed method perform on knowledge graphs with different densities and sparsity levels?
- Basis in paper: [inferred] The paper mentions that the effectiveness of leveraging ontology paths relies on the presence of a rich set of relationships and connections within the graph, indicating that performance may vary with graph density.
- Why unresolved: The paper does not provide empirical results on knowledge graphs with varying densities, nor does it discuss the impact of sparsity on the method's performance.
- What evidence would resolve it: Evaluating the method on a diverse set of knowledge graphs with different densities and sparsity levels, and comparing the performance metrics, would reveal how well the method generalizes across different graph structures.

### Open Question 3
- Question: Can the proposed method be adapted to handle dynamic knowledge graphs where new entities are frequently added?
- Basis in paper: [explicit] The paper mentions that the ontology constructed operates under a closed world assumption, which may limit adaptability in dynamic environments.
- Why unresolved: The paper does not explore potential modifications or extensions to the method that would allow it to handle dynamic knowledge graphs with evolving entities.
- What evidence would resolve it: Developing and testing an extension of the method that can incorporate new entities into the ontology and evaluate its performance on dynamic knowledge graphs would address this limitation.

### Open Question 4
- Question: How does the proposed method compare to other state-of-the-art KGC methods that use graph neural networks or other embedding techniques?
- Basis in paper: [inferred] The paper compares the proposed method to baselines like IndNodePiece and IndNodePieceGNN, but it does not provide a comprehensive comparison with other advanced KGC methods that use different techniques.
- Why unresolved: The paper does not include a broader range of state-of-the-art methods in the comparison, limiting the understanding of the proposed method's relative performance.
- What evidence would resolve it: Conducting experiments that compare the proposed method to a wider array of state-of-the-art KGC methods, including those using graph neural networks, embedding techniques, and other advanced approaches, would provide a more comprehensive evaluation of its effectiveness.

## Limitations

- The method's performance depends on GPT-4's context window constraints, requiring batch-wise processing that may lose information when candidate sets are large
- The ontology generation relies on LLM's ability to correctly infer node categories, which may not generalize well to domains with ambiguous or multi-category relations
- The approach assumes correct answers are likely to be found among existing training nodes in transductive setting, which may not hold for sparse or specialized graphs

## Confidence

**High Confidence (80-100%)**
- Claim: GPT-4 outperforms baselines in both transductive and inductive settings
- Evidence: Multiple experimental results with consistent Hit@k improvements across ILPC-small and ILPC-large datasets

**Medium Confidence (60-80%)**
- Claim: Ontology hints and candidate solutions significantly improve LLM performance
- Evidence: Ablation studies show performance gains, but improvements are incremental

**Low Confidence (40-60%)**
- Claim: Chain-of-thought reasoning is essential for the method's success
- Evidence: No ablation study specifically isolating CoT effects; improvement may come primarily from candidates and ontology

## Next Checks

1. **Ablation Study on Chain-of-Thought**: Run experiments comparing performance with and without CoT prompts while keeping ontology hints and candidate solutions constant to isolate the contribution of reasoning steps.

2. **Cross-Domain Generalization Test**: Evaluate the method on a KG from a completely different domain (e.g., biomedical) to assess whether the ontology inference and candidate generation strategies transfer effectively.

3. **Context Window Stress Test**: Systematically vary the number of candidate nodes presented to the LLM (e.g., 100, 500, 1000, 2000) to quantify the performance degradation as the context window approaches its limits and identify optimal batching strategies.