---
ver: rpa2
title: 'Tarsier: Recipes for Training and Evaluating Large Video Description Models'
arxiv_id: '2407.00634'
source_url: https://arxiv.org/abs/2407.00634
tags:
- video
- arxiv
- description
- wang
- tarsier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Tarsier, a family of large-scale video-language
  models designed for generating fine-grained video descriptions. The model architecture
  employs a frozen CLIP-ViT encoder to process frames separately, with a large language
  model (LLM) to model temporal relationships and generate descriptions.
---

# Tarsier: Recipes for Training and Evaluating Large Video Description Models

## Quick Facts
- arXiv ID: 2407.00634
- Source URL: https://arxiv.org/abs/2407.00634
- Reference count: 40
- Key outcome: Tarsier-34B achieves state-of-the-art performance on nine public benchmarks and significantly outperforms existing models on DREAM-1K

## Executive Summary
This paper introduces Tarsier, a family of large-scale video-language models designed for generating fine-grained video descriptions. The models employ a frozen CLIP-ViT encoder to process frames separately, with a large language model (LLM) to model temporal relationships and generate descriptions. Tarsier is trained in two stages: multi-task pre-training on diverse video tasks using high-quality data, followed by instruction tuning on human-annotated, multi-grained video descriptions. The authors introduce DREAM-1K, a challenging benchmark dataset featuring 1,000 videos from diverse sources with detailed annotations, and AutoDQ, an automatic evaluation method that assesses description quality through event extraction and entailment. Tarsier achieves state-of-the-art results on nine public benchmarks and significantly outperforms existing open-source models on DREAM-1K.

## Method Summary
Tarsier employs a two-stage training procedure. First, multi-task video-to-text pre-training enables the model to understand videos from different perspectives through tasks like captioning, VQA, and action recognition. Second, instruction tuning focuses on generating multi-grained video descriptions using human-annotated data. The architecture uses CLIP-ViT to encode frames separately, then maps visual features to LLM token space via MLP projection. The LLM processes these visual tokens and generates descriptions, modeling temporal relationships without specialized video encoding.

## Key Results
- Tarsier-34B achieves state-of-the-art results on nine public benchmarks including multi-choice VQA, open-ended VQA, and zero-shot video captioning
- On DREAM-1K, Tarsier-34B significantly outperforms all existing open-source models (+51.4% advantage in human evaluation)
- Tarsier-34B surpasses GPT-4V (+12.3%) and closely approaches Gemini 1.5 Pro (-6.7%) on DREAM-1K
- Tarsier2-7B, built on SigLIP and Qwen2, further improves performance (+4.8% advantage over GPT-4o)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tarsier's two-stage training procedure (multi-task pre-training followed by instruction tuning on fine-grained descriptions) significantly improves video description quality.
- Mechanism: Multi-task pre-training on diverse video tasks builds generalist video understanding capabilities, while instruction tuning on human-annotated, multi-grained descriptions fine-tunes the model for detailed, accurate video description generation.
- Core assumption: Large-scale, diverse pre-training data combined with high-quality fine-tuning data enables the model to learn both broad video understanding and specific description generation skills.
- Evidence anchors:
  - [abstract]: "with a meticulously designed two-stage training procedure, the Tarsier models exhibit substantially stronger video description capabilities"
  - [section 3.2]: "multi-task video-to-text pre-training, which enables the model to understand videos from different perspectives. The second stage is instruction tuning focused on generating multi-grained video descriptions."
  - [corpus]: Weak - No specific corpus evidence directly supporting this mechanism.

### Mechanism 2
- Claim: Using a frozen CLIP-ViT encoder to encode frames separately, combined with an LLM to model temporal relationships, achieves strong video description performance.
- Mechanism: The CLIP-ViT encoder provides robust visual feature extraction for individual frames, while the LLM leverages its strong reasoning capabilities to understand the temporal relationships between frames and generate coherent descriptions.
- Core assumption: The LLM's reasoning abilities are sufficient to model temporal relationships without requiring specialized video encoding architectures.
- Evidence anchors:
  - [section 3.1]: "it encodes each video frame separately. Initially, each frame is encoded using CLIP-ViT and then mapped to the downstream LLM's token embedding space via a multi-layer perceptron (MLP). The resulting visual tokens are concatenated... The LLM is then trained."
  - [abstract]: "Tarsier employs CLIP-ViT to encode frames separately and then uses an LLM to model temporal relationships."
  - [corpus]: Weak - The corpus neighbors discuss related work but don't directly provide evidence for this specific architectural choice.

### Mechanism 3
- Claim: Tarsier's simple architecture, despite its simplicity, achieves state-of-the-art performance on video description benchmarks.
- Mechanism: The combination of a powerful LLM, high-quality pre-training and fine-tuning data, and a simple yet effective architecture allows Tarsier to outperform more complex models.
- Core assumption: A powerful LLM, when properly trained, can achieve strong performance even with a simple architecture.
- Evidence anchors:
  - [abstract]: "Despite its simple architecture, we demonstrate that with a meticulously designed two-stage training procedure, the Tarsier models exhibit substantially stronger video description capabilities"
  - [section 5.1.2]: "Human side-by-side comparison is a gold standard for evaluating video descriptions. In this test... Tarsier-34B model significantly outperforms PLLaVA, demonstrating strong capabilities in describing dynamic events with less hallucination."
  - [corpus]: Weak - The corpus neighbors are related papers but don't directly provide evidence for this claim about Tarsier's simple architecture achieving state-of-the-art performance.

## Foundational Learning

- Concept: Video understanding
  - Why needed here: Tarsier is a video-language model designed for generating fine-grained video descriptions, so a strong understanding of video content is crucial.
  - Quick check question: Can you explain the difference between encoding video frames separately vs. jointly, and the trade-offs of each approach?

- Concept: Large language models (LLMs)
  - Why needed here: Tarsier uses an LLM to model temporal relationships between video frames and generate descriptions, so understanding LLM capabilities and limitations is essential.
  - Quick check question: What are the key differences between fine-tuning an LLM vs. training it from scratch, and when would you choose one approach over the other?

- Concept: Multimodal learning
  - Why needed here: Tarsier combines visual and textual information to understand and describe videos, so understanding how to effectively integrate different modalities is important.
  - Quick check question: Can you describe the challenges of training a model that can understand and generate both visual and textual information, and how Tarsier addresses these challenges?

## Architecture Onboarding

- Component map: CLIP-ViT encoder -> MLP projection layer -> LLM
- Critical path:
  1. Video frames are fed into CLIP-ViT encoder
  2. Encoded visual features are projected to LLM token space via MLP
  3. LLM processes visual tokens and generates description
- Design tradeoffs:
  - Simplicity vs. complexity: Tarsier uses a simple architecture but achieves strong performance through high-quality training data and a powerful LLM
  - Separate frame encoding vs. joint encoding: Tarsier encodes frames separately, which is simpler but may miss some spatial-temporal relationships
  - Frozen visual encoder vs. fine-tuned: Tarsier uses a frozen CLIP-ViT, which is faster to train but may not be optimal for all video domains
- Failure signatures:
  - If the LLM is not sufficiently powerful, the model may struggle to generate coherent descriptions or understand complex temporal relationships
  - If the CLIP-ViT encoder is not robust for the video domain, the visual features may be poor quality, leading to degraded performance
  - If the training data is not diverse or high-quality enough, the model may not generalize well to new videos
- First 3 experiments:
  1. Test the model on a held-out validation set to ensure it is not overfitting to the training data
  2. Evaluate the model's performance on videos with different characteristics (e.g., length, complexity, domain) to identify any weaknesses
  3. Compare the model's performance to a baseline that uses a simpler architecture (e.g., just an LLM without visual features) to quantify the benefit of the visual encoder

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Tarsier's performance scale with even larger model sizes (e.g., 70B parameters) compared to its current 34B variant?
- Basis in paper: [inferred] The paper demonstrates consistent performance improvements as the LLM size increases from 7B to 34B parameters, suggesting potential for further gains with larger models.
- Why unresolved: The paper only tests two model sizes (7B and 34B) and does not explore the upper limits of scaling. The relationship between model size and performance on video description tasks is not fully characterized.
- What evidence would resolve it: Training and evaluating Tarsier models with significantly larger LLM sizes (e.g., 70B parameters) on the DREAM-1K benchmark and comparing their performance to the 34B variant would provide concrete evidence of scaling trends.

### Open Question 2
- Question: What is the impact of using different visual encoders (e.g., SigLIP, ConvNeXt, or other state-of-the-art architectures) on Tarsier's performance?
- Basis in paper: [explicit] The paper introduces Tarsier2-7B, which uses SigLIP as the vision encoder and shows improved performance over the original Tarsier-34B model with CLIP-ViT. This suggests that the choice of visual encoder significantly affects performance.
- Why unresolved: While the paper demonstrates the benefits of using SigLIP, it does not comprehensively explore other visual encoder options or provide a systematic comparison of different architectures.
- What evidence would resolve it: Training and evaluating Tarsier models with various state-of-the-art visual encoders (e.g., SigLIP, ConvNeXt, DINOv2) on the DREAM-1K benchmark and comparing their performance would provide insights into the impact of visual encoder choice.

### Open Question 3
- Question: How does Tarsier's performance vary across different video domains (e.g., surveillance, sports, educational content) beyond the five categories tested in DREAM-1K?
- Basis in paper: [inferred] The paper introduces DREAM-1K, a benchmark with five diverse video categories, but does not explore Tarsier's performance on other video domains that may have unique characteristics or challenges.
- Why unresolved: The paper focuses on a specific set of video categories and does not investigate the model's generalization capabilities across a broader range of video domains.
- What evidence would resolve it: Evaluating Tarsier's performance on video description tasks in various domains (e.g., surveillance, sports, educational content, news) and comparing the results to its performance on DREAM-1K would provide insights into its domain-specific strengths and weaknesses.

## Limitations

- Training data quality and diversity are critical but specific details about in-house datasets are not provided, making it difficult to assess their true impact on performance
- The model's ability to handle complex temporal relationships is assumed but not thoroughly tested against specialized video encoding architectures
- Evaluation methodology relies heavily on automatic metrics and limited human evaluation, which may not fully capture real-world performance

## Confidence

- **High Confidence**: Tarsier achieves state-of-the-art results on multiple public benchmarks and outperforms existing open-source models on DREAM-1K with clear quantitative evidence
- **Medium Confidence**: The effectiveness of Tarsier's two-stage training procedure and simple architecture, though limited information about training data and lack of comparison with more complex architectures affects confidence
- **Low Confidence**: The model's ability to handle complex temporal relationships and its performance on diverse real-world videos due to insufficient testing and reliance on automatic evaluation

## Next Checks

1. **Ablation Study on Training Data**: Conduct an ablation study to quantify the impact of data quality and diversity on Tarsier's performance by training on subsets with varying levels of quality and diversity

2. **Temporal Modeling Analysis**: Evaluate Tarsier's ability to handle complex temporal dynamics by testing on videos with intricate temporal relationships and comparing results with specialized video encoding architectures

3. **Real-World Deployment Evaluation**: Deploy Tarsier on diverse real-world videos and conduct comprehensive human evaluation to assess performance in practical scenarios, collecting user feedback to identify strengths and weaknesses