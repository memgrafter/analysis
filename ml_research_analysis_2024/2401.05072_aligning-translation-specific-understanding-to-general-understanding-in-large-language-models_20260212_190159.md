---
ver: rpa2
title: Aligning Translation-Specific Understanding to General Understanding in Large
  Language Models
arxiv_id: '2401.05072'
source_url: https://arxiv.org/abs/2401.05072
tags:
- translation
- duat
- words
- difficult
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uncovers a mismatch between how LLMs understand text
  in general and how they translate it, leading to frequent mistranslations. The authors
  propose DUAT, a method that detects difficult words, generates target-language interpretations
  to align understanding, and filters low-quality interpretations.
---

# Aligning Translation-Specific Understanding to General Understanding in Large Language Models

## Quick Facts
- arXiv ID: 2401.05072
- Source URL: https://arxiv.org/abs/2401.05072
- Reference count: 24
- Improves COMET scores by up to 3.85 on translation benchmark

## Executive Summary
This paper addresses a fundamental mismatch between how large language models understand text in general scenarios versus how they translate that understanding into target languages. The authors identify that LLMs often mistranslate complex concepts they accurately comprehend in general contexts (like question answering) due to this understanding misalignment. They propose DUAT, a method that detects difficult words, generates target-language interpretations to align understanding, and filters low-quality interpretations. Using token-level quality estimation for difficulty detection and sentence-level quality estimation for interpretation control, DUAT significantly improves translation accuracy on a challenging benchmark.

## Method Summary
DUAT consists of three sequential steps: (1) difficult word detection using draft translation and token-level quality estimation to identify source words misaligned with their translations, (2) cross-lingual interpretation where the LLM generates target-language explanations for detected difficult words to leverage general understanding, and (3) interpretation quality control using sentence-level quality estimation to iteratively remove unhelpful interpretations. The method employs in-context learning with demonstrations synthesized from parallel data. DUAT was evaluated on the Challenge-WMT benchmark across six language pairs (Chinese, Estonian, Icelandic to/from English) using GPT-3.5-turbo and GPT-4.

## Key Results
- DUAT improves COMET scores by up to 3.85 on the Challenge-WMT benchmark
- Reduces literal translations by 25-51% while maintaining semantic accuracy
- Significantly reduces generalization failures in translation without introducing new errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning translation-specific understanding with general understanding reduces mistranslation of complex concepts
- Mechanism: DUAT detects difficult words in source text, generates target-language interpretations to reveal general understanding, and filters low-quality interpretations to guide translation
- Core assumption: LLMs possess accurate general understanding of complex concepts but fail to translate them due to understanding misalignment
- Evidence anchors:
  - [abstract]: "This understanding misalignment leads to LLMs mistakenly or literally translating some complicated concepts that they accurately comprehend in general scenarios (e.g., QA)."
  - [section]: "We refer to these failures as language models' generalization failures on translation."
- Break condition: If LLMs lack accurate general understanding of the complex concepts, interpretation generation will fail

### Mechanism 2
- Claim: Token-level QE effectively identifies difficult-to-translate words
- Mechanism: DUAT uses token-level QE to measure misalignment between source spans and draft translation, selecting words above difficulty threshold
- Core assumption: Token-level QE accurately reflects translation difficulty
- Evidence anchors:
  - [section]: "DUAT-E annotates each candidate word with its degree of misalignment with respect to the draft translation, which reflects the translation-specific difficulty."
  - [section]: "DUAT-E gains a further improvement of +0.3 COMET, showing the effectiveness of the external token-level QE tool in this task."
- Break condition: If token-level QE fails to correlate with human judgment of translation difficulty

### Mechanism 3
- Claim: Interpretation quality control via ablation improves translation quality
- Mechanism: DUAT iteratively removes interpretations and measures QE score changes, keeping only helpful interpretations
- Core assumption: Removing harmful interpretations improves translation quality measurably
- Evidence anchors:
  - [section]: "If the better translation performance is achieved by ablation, which is measured by the QE2 tool... the interpretation Ai is removed from A"
  - [section]: "DUAT significantly reduces the bias towards literal translation, indicating that the process of interpreting the difficult words first and then translating aligns better with sense-for-sense translation."
- Break condition: If QE score changes don't reliably indicate interpretation helpfulness

## Foundational Learning

- Concept: Cross-lingual interpretation generation
  - Why needed here: Transforms general understanding into target language space to guide translation
  - Quick check question: Can the LLM generate accurate interpretations in the target language for complex source concepts?

- Concept: Token-level quality estimation
  - Why needed here: Identifies difficult-to-translate words by measuring misalignment between source and translation
  - Quick check question: Does the token-level QE tool assign higher scores to source words that are mistranslated in the draft?

- Concept: Interpretation quality control via ablation
  - Why needed here: Filters out harmful interpretations that could bias translation away from original meaning
  - Quick check question: Does removing a particular interpretation consistently improve the QE score of the resulting translation?

## Architecture Onboarding

- Component map: Source sentence → Draft translation → Difficult word detection → Cross-lingual interpretation → IQC → Final translation
- Critical path: Source sentence → Draft translation → Difficult word detection → Cross-lingual interpretation → IQC → Final translation
- Design tradeoffs: More interpretations provide better guidance but increase latency; token-level QE adds accuracy but requires external tool
- Failure signatures: Poor performance indicates either ineffective difficult word detection or low-quality interpretations passing IQC
- First 3 experiments:
  1. Test difficult word detection with known mistranslated words to verify accuracy
  2. Validate interpretation quality control by measuring QE score changes when removing interpretations
  3. Benchmark translation quality with and without DUAT on simple vs. complex sentences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using external knowledge sources (e.g., dictionaries, encyclopedias) in conjunction with DUAT to resolve generalization failures on translation?
- Basis in paper: [inferred] The paper mentions that DUAT relies on the general understanding of LLMs, which might not be sufficient for concepts requiring external knowledge like neologisms.
- Why unresolved: The paper does not explore the integration of external knowledge sources with DUAT to address the limitations of relying solely on intrinsic LLM understanding.
- What evidence would resolve it: Experiments comparing DUAT's performance with and without external knowledge sources, particularly for translations involving neologisms or domain-specific terminology.

### Open Question 2
- Question: How does the performance of DUAT vary across different LLM architectures and sizes, and what are the optimal configurations for different language pairs?
- Basis in paper: [explicit] The paper mentions that DUAT was evaluated on GPT-3.5-turbo and GPT-4, but the impact of different LLM architectures and sizes is not explored.
- Why unresolved: The paper does not provide a comprehensive analysis of how DUAT's performance scales with different LLM configurations, leaving the optimal setup unclear.
- What evidence would resolve it: Comparative experiments across a range of LLM architectures and sizes, including smaller models, to determine the impact on DUAT's effectiveness and efficiency.

### Open Question 3
- Question: Can the difficult word detection component of DUAT be further improved by incorporating contextual information beyond the source sentence and preliminary translation?
- Basis in paper: [explicit] The paper mentions that DUAT detects difficult words based on the source sentence and preliminary translation, but it does not explore the use of additional contextual information.
- Why unresolved: The paper does not investigate whether incorporating broader context, such as surrounding sentences or document-level information, could enhance the accuracy of difficult word detection.
- What evidence would resolve it: Experiments comparing DUAT's performance with and without the incorporation of additional contextual information, measuring the impact on difficult word detection accuracy and overall translation quality.

## Limitations

- Reliance on external token-level and sentence-level QE tools without characterizing their accuracy or reliability
- Absence of ablation studies to quantify individual component contributions to performance gains
- Limited evaluation to six language pairs without analysis of method scalability or language-specific challenges

## Confidence

- Mechanism 1: High confidence - Strong empirical evidence across multiple domains supports the understanding misalignment claim
- Mechanism 2: Medium confidence - Theoretical soundness but lacks direct validation against human judgments of translation difficulty
- Mechanism 3: Medium confidence - Ablation approach is theoretically sound but QE score reliability needs verification

## Next Checks

1. **Component ablation analysis**: Run DUAT with each component (difficult word detection, interpretation generation, interpretation quality control) disabled individually to quantify each component's contribution to the final performance gains.

2. **QE tool validation**: Compare token-level QE scores against human annotations of translation difficulty on a held-out set to establish correlation and determine if the tool reliably identifies problematic source words.

3. **Generalization testing**: Evaluate DUAT on sentences containing words that are difficult to translate but were NOT identified by the token-level QE tool to assess whether the method misses important cases of translation difficulty.