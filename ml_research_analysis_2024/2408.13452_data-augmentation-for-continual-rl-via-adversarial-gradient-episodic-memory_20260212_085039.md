---
ver: rpa2
title: Data Augmentation for Continual RL via Adversarial Gradient Episodic Memory
arxiv_id: '2408.13452'
source_url: https://arxiv.org/abs/2408.13452
tags:
- data
- augmentation
- continual
- adv-gem
- packnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of data inefficiency in continual
  reinforcement learning (RL) by introducing data augmentation techniques to improve
  performance across sequential tasks. The authors propose a plug-in framework that
  enhances continual RL methods by generating diverse training samples without modifying
  the underlying RL algorithm.
---

# Data Augmentation for Continual RL via Adversarial Gradient Episodic Memory

## Quick Facts
- arXiv ID: 2408.13452
- Source URL: https://arxiv.org/abs/2408.13452
- Authors: Sihao Wu; Xingyu Zhao; Xiaowei Huang
- Reference count: 40
- Primary result: Data augmentation methods, especially Adv-GEM, significantly improve average performance and reduce forgetting in continual RL

## Executive Summary
This paper addresses data inefficiency in continual reinforcement learning by introducing data augmentation techniques as a plug-in framework. The authors propose several augmentation methods including random amplitude scaling, state-switch, mixup, and adversarial augmentation, with the novel Adversarial Augmentation with Gradient Episodic Memory (Adv-GEM) achieving the best results. Experiments on robot control tasks demonstrate that these augmentation methods significantly improve average performance, reduce catastrophic forgetting, and enhance forward transfer across sequential tasks.

## Method Summary
The paper presents a plug-in data augmentation framework for continual RL that can be combined with existing methods like EWC and PackNet. The approach uses Soft Actor-Critic (SAC) as the base algorithm and applies augmentation to state inputs before policy and Q-value updates. Key augmentation methods include uniform/gaussian noise, random amplitude scaling, dimension dropout, state-switch, mixup, adversarial augmentation (Adv-AUG), and the novel Adv-GEM which incorporates gradients from episodic memory into adversarial sample generation. The framework stores representative samples in episodic memory buffers and uses them to guide augmentation, particularly for Adv-GEM which projects perturbations onto the null space of previous task gradients.

## Key Results
- Adv-GEM achieves the highest performance gains among all augmentation methods tested
- Data augmentation significantly improves average performance across sequential tasks
- Augmentation methods reduce catastrophic forgetting compared to baseline continual RL methods
- Forward transfer to future tasks is enhanced through better generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data augmentation improves continual RL by expanding the state space diversity within each task.
- Mechanism: Augmentation transforms raw states into perturbed versions, effectively increasing the number of training samples without additional environment interactions.
- Core assumption: State space is consistent across tasks, so augmented samples from one task remain valid for others.
- Evidence anchors:
  - [abstract] "data augmentations, such as random amplitude scaling, state-switch, mixup, adversarial augmentation, and Adv-GEM, can improve existing continual RL algorithms"
  - [section 3.2] "data augmentation can allow the agent to generate more diverse data samples during each task"
  - [corpus] Weak evidence - no neighboring papers directly address augmentation in continual RL.
- Break condition: Augmentation introduces physically impossible states or breaks task semantics (e.g., state-switch swapping incompatible objects).

### Mechanism 2
- Claim: Adversarial augmentation generates worst-case scenarios that improve model robustness and generalization.
- Mechanism: Gradient-based perturbation maximizes policy/critic loss within a constrained neighborhood of real states.
- Core assumption: Perturbations small enough to remain within valid state space still produce meaningful adversarial examples.
- Evidence anchors:
  - [section 3.2] "generate adversarial data augmentations to enhance generalization capabilities"
  - [section 3.2] "minimizing the maximum policy lossLπθ (˜si) over ˜si"
  - [corpus] No direct corpus evidence for adversarial augmentation in continual RL.
- Break condition: Perturbation radius too large causing out-of-distribution states or too small to be effective.

### Mechanism 3
- Claim: Adv-GEM integrates episodic memory gradients to ensure augmented samples challenge previous tasks while optimizing current ones.
- Mechanism: Projects adversarial perturbations onto the null space of previous task gradients when angle > 90°, otherwise preserves gradient direction.
- Core assumption: Episodic memory contains representative samples from past tasks whose gradients guide relevant augmentation.
- Evidence anchors:
  - [section 3.2] "indirectly embeds the gradient from previous tasks into the generation of adversarial examples"
  - [section 3.2] "we compare the angle between the policy gradient vectors of previous tasks gM, and the gradient of current taskgD"
  - [corpus] No neighboring papers discuss Adv-GEM specifically.
- Break condition: Episodic memory too small or unrepresentative, causing poor gradient guidance.

## Foundational Learning

- Concept: Reinforcement Learning with Soft Actor-Critic (SAC)
  - Why needed here: Base algorithm for continual RL; data augmentation modifies SAC updates.
  - Quick check question: How does SAC's entropy regularization interact with augmented state inputs?

- Concept: Catastrophic Forgetting in Continual Learning
  - Why needed here: Primary problem being addressed; motivates both memory and augmentation approaches.
  - Quick check question: What distinguishes regularization-based (EWC) from memory-based (A-GEM) forgetting mitigation?

- Concept: Episodic Memory in Continual Learning
  - Why needed here: Stores past task samples used by Adv-GEM for gradient constraints.
  - Quick check question: How does episodic memory size affect forgetting and computational cost?

## Architecture Onboarding

- Component map:
  Environment → State/Action spaces → RL Agent (SAC) → Replay Buffer → Data Augmentation Module → Policy/Critic Networks
  Episodic Memory Buffer (for Adv-GEM) → Gradient Computation Module → Augmentation Transformation

- Critical path:
  1. Collect transition (s,a,r,s') → Store in replay buffer
  2. Sample mini-batch → Apply augmentation → Update networks
  3. At task boundary → Store samples in episodic memory (Adv-GEM)

- Design tradeoffs:
  - Augmentation strength vs. physical validity of states
  - Episodic memory size vs. memory/computation constraints
  - Adversarial perturbation radius vs. robustness vs. stability

- Failure signatures:
  - Performance collapse → Augmentation creating invalid states
  - Forgetting persists → Memory too small or augmentation ineffective
  - Training instability → Adversarial perturbations too aggressive

- First 3 experiments:
  1. Run baseline EWC on MW4 without augmentation; establish forgetting baseline.
  2. Add simple RAS augmentation to EWC; measure average performance change.
  3. Implement Adv-GEM; compare against EWC+Adv-AUG on forward transfer metric.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited empirical robustness across diverse task distributions
- Sensitivity of Adv-GEM to episodic memory size and gradient quality
- Lack of ablation studies on augmentation strength, perturbation radius, and memory buffer capacity

## Confidence

Major uncertainties center on the empirical robustness of augmentation across diverse task distributions and the sensitivity of Adv-GEM to episodic memory size and gradient quality. The paper lacks ablation studies on augmentation strength, perturbation radius, and memory buffer capacity.

- Confidence in adversarial perturbations improving robustness: **Medium** - theory sound but tuning critical
- Confidence in Adv-GEM's gradient projection reducing forgetting: **Medium** - novel but no simpler baseline comparison
- Confidence in augmentation consistently improving average performance: **High** - supported by experimental results

## Next Checks

1. **Hyperparameter Sensitivity**: Test Adv-GEM performance across varying episodic memory sizes (100, 500, 1000 samples) and adversarial perturbation radii to identify breaking points.

2. **Augmentation Transferability**: Apply the same augmentation methods to different continual RL algorithms (e.g., PPO, TD3) on MW4 to verify generality beyond SAC.

3. **State Space Validity**: Measure the percentage of augmented samples that fall outside valid state distributions and correlate with performance degradation to validate the physical validity constraint.