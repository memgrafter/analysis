---
ver: rpa2
title: 'UnCLe: Benchmarking Unsupervised Continual Learning for Depth Completion'
arxiv_id: '2410.18074'
source_url: https://arxiv.org/abs/2410.18074
tags:
- depth
- learning
- continual
- completion
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UnCLe, the first benchmark for unsupervised
  continual learning in depth completion, a multimodal 3D reconstruction task. The
  authors address the problem of catastrophic forgetting when depth completion models
  adapt to non-stationary data distributions over time.
---

# UnCLe: Benchmarking Unsupervised Continual Learning for Depth Completion

## Quick Facts
- arXiv ID: 2410.18074
- Source URL: https://arxiv.org/abs/2410.18074
- Reference count: 40
- Primary result: Existing continual learning methods still exhibit significant forgetting in unsupervised depth completion, demonstrating this remains an open problem

## Executive Summary
This paper introduces UnCLe, the first benchmark for unsupervised continual learning in depth completion, a multimodal 3D reconstruction task. The authors address catastrophic forgetting when depth completion models adapt to non-stationary data distributions over time. They adapt five continual learning methods (EWC, LwF, ANCL, Experience Replay, CMP) to unsupervised depth completion and evaluate them across indoor and outdoor datasets arranged in challenging sequences. Through over 1,000 experiments, they find that existing continual learning methods still exhibit significant forgetting and performance degradation, demonstrating that unsupervised continual learning for depth completion remains an open problem.

## Method Summary
UnCLe benchmarks five continual learning methods adapted to unsupervised depth completion: Elastic Weight Consolidation (EWC), Learning without Forgetting (LwF), Approximate Natural Continual Learning (ANCL), Experience Replay, and Conditional Meta-Plasticity (CMP). The benchmark evaluates these methods on six datasets (NYUv2, VOID, ScanNet, KITTI, Waymo, Virtual KITTI) arranged in five sequences, using three evaluation metrics: Average Forgetting, Average Performance, and Stability-Plasticity Trade-Off (SPTO). The core task involves training depth completion models sequentially on datasets with different sensor characteristics and scene distributions, measuring how well methods prevent catastrophic forgetting while maintaining plasticity for new learning.

## Key Results
- Existing continual learning methods exhibit significant forgetting in unsupervised depth completion tasks
- No single method dominates across all metrics and settings for multimodal depth completion
- Experience Replay and EWC show the most promise but still fail to prevent substantial performance degradation
- The benchmark establishes that unsupervised continual learning for depth completion remains an open problem

## Why This Works (Mechanism)

### Mechanism 1: Catastrophic forgetting from parameter drift
When training sequentially on different datasets with distinct sensor characteristics, gradient updates on new non-stationary distributions overwrite parameters important for previous datasets. The loss landscape for different datasets has substantially different local minima that share few common optimal parameter values.

### Mechanism 2: Experience replay regularization
Experience Replay mitigates forgetting by maintaining a buffer of representative samples from previously seen datasets and including them in training batches alongside new data. This regularizes the model by regularly exposing it to past distributions, preventing complete parameter drift away from previously learned patterns.

### Mechanism 3: Regularization-based parameter constraints
Regularization methods (EWC, LwF) prevent forgetting by adding penalty terms to the loss function that discourage changes to parameters deemed important for previous tasks (EWC) or that encourage consistency with previous model outputs (LwF), creating a stability-plasticity tradeoff.

## Foundational Learning

- **Catastrophic forgetting in neural networks**: Why needed - core problem the paper addresses; Quick check - Why does a neural network trained sequentially on datasets A then B typically perform worse on A than a model trained only on A?

- **Multimodal learning (RGB + depth fusion)**: Why needed - depth completion requires understanding how to fuse information from two different sensor modalities; Quick check - What are the key challenges in fusing RGB images with sparse depth maps for dense depth completion?

- **Unsupervised learning objectives**: Why needed - the paper focuses on unsupervised depth completion methods that don't require ground truth depth labels; Quick check - How can photometric reprojection error be used as a training signal when ground truth depth is unavailable?

## Architecture Onboarding

- **Component map**: Depth completion model (VOICED, FusionNet, KBNet) -> Continual learning wrapper (EWC, LwF, ANCL, Experience Replay, CMP) -> Evaluation pipeline -> Dataset manager

- **Critical path**: 1) Pretrain depth completion model on initial dataset (NYUv2 or KITTI) 2) Sequentially train on target datasets with continual learning method applied 3) Evaluate on all datasets after each training step 4) Compute metrics (Average Forgetting, Average Performance, SPTO)

- **Design tradeoffs**: Buffer size vs. forgetting (larger buffers reduce forgetting but increase memory/computation); Regularization strength vs. plasticity (stronger regularization preserves old knowledge but limits new learning); Dataset ordering (sequences starting with easier datasets may show different behavior)

- **Failure signatures**: Performance degradation on early datasets during later training steps; High variance in results across different runs due to stochasticity; Poor performance on dataset transitions; Metrics that conflict (low forgetting but also low overall performance)

- **First 3 experiments**: 1) Run baseline finetuning on NYUv2 → ScanNet → VOID sequence to establish forgetting baseline 2) Apply Experience Replay with buffer size 64 to same sequence to compare forgetting reduction 3) Test EWC with λ=1 on NYUv2 → VOID → ScanNet to evaluate regularization effectiveness

## Open Questions the Paper Calls Out

1. How do continual learning methods for depth completion perform when scaled to significantly larger datasets or longer task sequences beyond what was tested in UnCLe? The benchmark only tests on 5 dataset sequences with specific dataset combinations, and the performance degradation observed might be exacerbated or mitigated with longer task sequences or larger-scale datasets that weren't evaluated.

2. What are the fundamental architectural or algorithmic modifications needed to effectively address catastrophic forgetting in multimodal depth completion tasks specifically, as opposed to unimodal depth estimation? The paper adapts existing continual learning methods designed for classification or single-modality regression to depth completion without exploring whether the multimodal nature requires fundamentally different approaches to knowledge preservation and plasticity.

3. How does the choice of buffer size and sampling strategy in experience replay methods affect the trade-off between computational efficiency and performance retention in continual depth completion? The paper presents a specific buffer size as optimal but doesn't investigate how performance scales with different buffer sizes, nor does it compare the computational cost-benefit trade-offs across different sampling strategies for the replay buffer.

## Limitations

- Only five specific continual learning methods were evaluated, leaving uncertainty about generalizability to other approaches
- Results may be sensitive to the choice of specific depth completion architectures (VOICED, FusionNet, KBNet)
- Dataset sequences and ordering could significantly impact method effectiveness, as catastrophic forgetting patterns may depend heavily on specific transitions between indoor and outdoor domains

## Confidence

**High Confidence**: The core observation that catastrophic forgetting occurs in unsupervised depth completion and that existing continual learning methods struggle to fully mitigate this issue. The benchmark framework and metrics are well-defined and empirically validated.

**Medium Confidence**: The relative effectiveness rankings of different continual learning methods for this specific task. Small performance differences between methods may be sensitive to implementation details and hyper-parameters not fully specified.

**Low Confidence**: Claims about the fundamental limitations of current continual learning approaches for unsupervised depth completion. The paper demonstrates significant forgetting but does not definitively establish whether this represents an inherent limitation or could be overcome with different architectural choices.

## Next Checks

1. **Architecture Sensitivity Analysis**: Repeat key experiments using alternative depth completion backbones (e.g., TransDepth, PackNet) to assess whether method rankings are architecture-dependent.

2. **Hyper-parameter Robustness**: Conduct systematic sweeps of critical hyper-parameters (regularization strength λ for EWC/LwF, buffer size for Experience Replay/CMP, learning rates) to determine the stability of method performance across the hyper-parameter space.

3. **Alternative Dataset Sequences**: Test all methods on different dataset orderings and combinations (e.g., indoor→outdoor→indoor, or sequences mixing multiple sensor types) to evaluate whether catastrophic forgetting patterns are consistent across varied domain transitions.