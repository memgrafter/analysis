---
ver: rpa2
title: Where Did Your Model Learn That? Label-free Influence for Self-supervised Learning
arxiv_id: '2412.17170'
source_url: https://arxiv.org/abs/2412.17170
tags:
- influence
- learning
- examples
- data
- influence-ssl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Influence-SSL introduces a label-free method to quantify the influence
  of training examples on self-supervised learning models. By leveraging representation
  stability against data augmentations, it adapts classical influence functions to
  the SSL setting without requiring labels.
---

# Where Did Your Model Learn That? Label-free Influence for Self-supervised Learning

## Quick Facts
- arXiv ID: 2412.17170
- Source URL: https://arxiv.org/abs/2412.17170
- Authors: Nidhin Harilal; Amit Kiran Rege; Reza Akbarian Bafghi; Maziar Raissi; Claire Monteleoni
- Reference count: 40
- Key outcome: Influence-SSL introduces a label-free method to quantify the influence of training examples on self-supervised learning models using representation stability against data augmentations.

## Executive Summary
Influence-SSL introduces a novel approach to quantify the influence of individual training examples on self-supervised learning (SSL) models without requiring labels. The method leverages the stability of learned representations against data augmentations, using cosine distance between augmented views to measure influence. Through experiments across multiple SSL frameworks (SimCLR, BYOL, Barlow Twins), the approach demonstrates that high-influence examples often have uniform backgrounds and act as semantic bridges that degrade downstream performance when removed. The method also reveals biases in representation learning and detects duplicate images, providing insights into the training dynamics of SSL models.

## Method Summary
Influence-SSL adapts classical influence functions to the SSL setting by measuring how individual training examples affect the stability of learned representations under data augmentations. The method computes influence scores using cosine distance between augmented views of the same image, with Gaussian noise perturbations (µ=0.05, σ=0.2) as the primary augmentation strategy. It employs LoGra for efficient inverse Hessian estimation and calculates influence through gradient/Hessian calculations on pre-trained SSL models. The approach is validated on CIFAR-10, CIFAR-100, and FairFace datasets using three SSL frameworks with ResNet18 backbone, examining downstream classification performance, duplicate detection, and fairness analysis.

## Key Results
- High-influence examples in SSL often have uniform backgrounds and act as semantic bridges, artificially connecting instances from different classes
- Influence-SSL consistently identifies duplicate or near-duplicate images across different SSL frameworks
- Removing high-influence points with uniform backgrounds improves downstream classification performance, revealing biases in representation learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Influence-SSL quantifies how individual training examples affect the stability of learned representations under data augmentations.
- Mechanism: Uses cosine distance between augmented views of the same image to measure representation stability. High influence indicates examples where augmentations produce unexpectedly large changes in representation space.
- Core assumption: Representation stability against augmentations is a meaningful proxy for influence in SSL, capturing the invariance-distinctiveness trade-off inherent in SSL objectives.
- Evidence anchors:
  - [abstract] "Our method harnesses the stability of learned representations against data augmentations to identify training examples that help explain model predictions."
  - [section] "We formalize this concept by introducing a refined definition of the influence score I for an unlabelled image xi, utilizing a pre-trained SSL model fθ: I(f, i) = −∇θL(fθ(xi), fθ(ˆxi))⊤H −1θ ∇θL(fθ(xi), fθ(ˆxi))"
  - [corpus] Weak - no direct mentions of representation stability in neighbors
- Break condition: If augmentation strategies change fundamentally (e.g., deterministic vs stochastic), the stability proxy may no longer capture influence accurately.

### Mechanism 2
- Claim: High-influence examples in SSL often act as semantic bridges, artificially connecting instances from different classes based on background similarities rather than semantic content.
- Mechanism: Examples with uniform backgrounds create ambiguous decision boundaries in representation space by emphasizing background features over object-level semantic differences. Removing these improves downstream performance.
- Core assumption: SSL models inadvertently learn to use background characteristics as similarity signals, creating spurious connections between semantically different instances.
- Evidence anchors:
  - [section] "We observed that images with the highest influence scores predominantly feature uniform backgrounds (either white or black), deviating significantly from the natural image distribution."
  - [section] "Our analysis reveals a different reality: the high concentration of uniform background images among influential examples, despite representing different object classes, suggests that the SSL model is inadvertently using background characteristics as a strong similarity signal."
  - [corpus] Weak - no direct mentions of background-based semantic bridges in neighbors
- Break condition: If augmentation strategies explicitly remove background features or if SSL objectives explicitly penalize background-based similarity.

### Mechanism 3
- Claim: Influence-SSL provides label-free duplicate detection by identifying training examples with low influence scores that are redundant in the representation space.
- Mechanism: Duplicate or near-duplicate images maintain low influence due to their redundant nature and low contribution to learning diverse representations. The method naturally identifies these without requiring labels.
- Core assumption: Redundant examples contribute minimally to learning invariant representations and can be identified through their representation stability patterns.
- Evidence anchors:
  - [section] "Building upon representation-level insights from Figure 3, we discover an intriguing capability of our influence estimation method. Across different self-supervised learning frameworks, the influence scores consistently identify duplicate or near-duplicate images within classes"
  - [section] "When examining the 5 lowest influential images for the automobile class on CIFAR-10 as shown in Figure 4, we observed notable differences between the methods themselves. For BYOL, all 5 of the lowest influential images were duplicates of the same car"
  - [corpus] Weak - no direct mentions of duplicate detection in neighbors
- Break condition: If the dataset contains many semantically similar but not identical examples, the method may struggle to distinguish true duplicates from similar instances.

## Foundational Learning

- Concept: Representation stability under augmentations
  - Why needed here: Forms the core mechanism for measuring influence in SSL without labels
  - Quick check question: Why does measuring cosine distance between augmented views capture influence better than using downstream task performance?

- Concept: Self-influence vs cross-influence
  - Why needed here: The paper focuses on how examples influence their own representations rather than others, which is a specific but useful measure
  - Quick check question: What would change in the methodology if we wanted to measure how example A influences the representation of example B?

- Concept: Invariance-distinctiveness trade-off in SSL
  - Why needed here: Explains why certain examples become highly influential - they challenge the model's ability to balance these competing objectives
  - Quick check question: How does the presence of high-influence examples with uniform backgrounds reveal a failure in this trade-off?

## Architecture Onboarding

- Component map: Pre-trained SSL model (SimCLR, BYOL, Barlow Twins) -> Data augmentation pipeline (Gaussian noise perturbations) -> Influence computation module (gradient/Hessian calculations) -> Downstream evaluation (supervised classification on CIFAR datasets)

- Critical path:
  1. Load pre-trained SSL model
  2. Apply Gaussian noise perturbations to training images
  3. Compute gradients and Hessian for cosine distance loss
  4. Calculate influence scores using inverse Hessian-vector products
  5. Analyze results for duplicate detection, outlier identification, and fairness analysis

- Design tradeoffs:
  - Computational cost vs accuracy: Using low-rank approximation for inverse Hessian enables scaling but may reduce precision
  - Perturbation choice: Gaussian noise is simple but other augmentations might capture different aspects of representation stability
  - Linear vs non-linear analysis: Theoretical insights from linear networks provide intuition but may not fully explain deep network behavior

- Failure signatures:
  - Inconsistent influence scores across different runs suggest instability in the estimation method
  - High correlation between influence scores and simple image statistics (like brightness) indicates the method may be capturing superficial rather than semantic features
  - No improvement in downstream performance after removing high-influence points suggests the semantic bridge hypothesis may not hold

- First 3 experiments:
  1. Reproduce the stability analysis across different random seeds to verify correlation of influence scores
  2. Test duplicate detection on a controlled dataset with known duplicates to validate the method's effectiveness
  3. Compare influence scores using different perturbation strategies (Gaussian noise vs random cropping) to assess robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between Influence-SSL and downstream task performance in non-linear architectures?
- Basis in paper: [explicit] The paper shows empirically that removing high-influence points improves downstream performance, but acknowledges that extending theoretical analysis to deep networks remains an open challenge.
- Why unresolved: The current theoretical analysis is limited to linear networks where closed-form solutions exist. Non-linear architectures introduce complex interactions that prevent direct application of these results.
- What evidence would resolve it: A theoretical framework characterizing how Influence-SSL approximates quantities related to downstream performance in deep networks, validated through experiments on multiple architectures and tasks.

### Open Question 2
- Question: How do different types of data augmentations (beyond Gaussian noise) affect the stability and interpretation of Influence-SSL scores?
- Basis in paper: [explicit] The paper includes an ablation study showing that Gaussian noise yields the highest Pearson correlation (0.974), while augmentations involving visual transformations result in lower correlations (0.719-0.744).
- Why unresolved: While the paper identifies that Gaussian noise is most stable, it doesn't fully explain why certain augmentations degrade correlation or what this means for the interpretation of influence scores.
- What evidence would resolve it: A comprehensive study mapping different augmentation types to their effects on influence score stability, including theoretical justification for why certain augmentations preserve ranking better than others.

### Open Question 3
- Question: What is the mechanism by which high-influence points with uniform backgrounds act as "semantic bridges" that degrade downstream performance?
- Basis in paper: [explicit] The paper observes that high-influence examples often have uniform backgrounds and that removing them improves performance, suggesting they create artificial connections between semantically different classes.
- Why unresolved: The paper provides empirical evidence but lacks a rigorous explanation of the geometric mechanism by which these background features create misleading representational similarities.
- What evidence would resolve it: Detailed analysis of the representation space showing how uniform background features create spurious correlations between classes, potentially through visualization techniques or geometric analysis of learned manifolds.

### Open Question 4
- Question: How does Influence-SSL behave in continual learning scenarios where the data distribution changes over time?
- Basis in paper: [inferred] The paper focuses on static datasets and doesn't address scenarios where the training distribution evolves, which is a limitation for real-world applications.
- Why unresolved: The current theoretical framework assumes a fixed data distribution and doesn't account for distributional shifts that occur in continual learning settings.
- What evidence would resolve it: Experiments showing how Influence-SSL scores evolve with changing data distributions, and whether the method can identify influential examples that become harmful as the task evolves.

## Limitations
- The method relies on Gaussian noise perturbations, which may not capture all aspects of representation stability that affect influence
- The semantic bridge hypothesis, while supported by background analysis, lacks direct experimental validation beyond correlation with downstream performance
- The correlation analysis across seeds shows consistency but doesn't prove robustness to different augmentation strategies

## Confidence
- Mechanism 1 (Representation stability): Medium - theoretically grounded but empirical validation is limited to correlation analysis
- Mechanism 2 (Semantic bridges): Low-Medium - supported by qualitative observations but lacks controlled experiments
- Mechanism 3 (Duplicate detection): Medium - demonstrates effectiveness on specific cases but not systematically evaluated across datasets

## Next Checks
1. Test Influence-SSL with alternative perturbation strategies (random cropping, color jitter) to assess sensitivity to augmentation choice
2. Conduct controlled experiments on datasets with known semantic relationships to validate the semantic bridge hypothesis
3. Implement systematic duplicate detection evaluation on datasets with ground-truth duplicate annotations to measure precision and recall