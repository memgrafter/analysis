---
ver: rpa2
title: Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical
  Study Over Open-ended Question Answering
arxiv_id: '2410.08085'
source_url: https://arxiv.org/abs/2410.08085
tags:
- llms
- knowledge
- query
- language
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OKGQA, a new benchmark for evaluating large
  language models (LLMs) augmented with knowledge graphs (KGs) in open-ended question
  answering. OKGQA features diverse real-world query types and includes metrics to
  measure both hallucination rates and reasoning improvements.
---

# Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study Over Open-ended Question Answering

## Quick Facts
- arXiv ID: 2410.08085
- Source URL: https://arxiv.org/abs/2410.08085
- Authors: Yuan Sui; Yufei He; Zifeng Ding; Bryan Hooi
- Reference count: 23
- Key outcome: KG integration reduces factual errors in LLM open-ended QA, with subgraph retrieval showing particular robustness even under KG errors

## Executive Summary
This paper introduces OKGQA, a benchmark for evaluating KG-augmented LLMs in open-ended question answering. The study demonstrates that knowledge graphs can effectively reduce hallucinations in LLM outputs, with subgraph-based retrieval methods showing particular promise. The evaluation framework includes metrics for hallucination detection (FActScore, SAFE) and reasoning quality (G-Eval), providing a comprehensive assessment of trustworthiness. Experiments show that even when KGs contain errors, KG-augmented models outperform chain-of-thought baselines.

## Method Summary
The study employs retrieval-augmented generation with knowledge graphs, using DBpedia as the knowledge source. Three retrieval methods are compared: triplet, path, and subgraph retrieval, with subgraph methods using Personalized PageRank pruning. The prize-cost trade-off strategy optimizes subgraph selection. Evaluation uses FActScore and SAFE for hallucination detection and G-Eval for assessing reasoning improvements across four dimensions: context relevance, comprehensiveness, correctness, and empowerment.

## Key Results
- KG integration significantly reduces factual errors across query types
- Subgraph retrieval outperforms triplet and path methods, especially for simpler queries
- KG-augmented models maintain performance advantages even when KGs contain errors
- Improvements are most pronounced for complex query types requiring multi-hop reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** KG integration improves factual accuracy by grounding LLM outputs in verifiable knowledge.
- **Mechanism:** The KG provides structured, explicit facts that can be retrieved and used to guide generation, reducing reliance on internal knowledge that may be outdated or incorrect.
- **Core assumption:** The KG contains accurate, relevant information for the query.
- **Evidence anchors:**
  - [abstract] "KGs provide structured, explicit, and often domain-specific factual information, with each fact traceable to its original source."
  - [section] "KGs offer structured, explicit, and up-to-date factual knowledge, including domain-specific knowledge, providing a faithful knowledge source for reasoning."
  - [corpus] Weak evidence; no direct citation supporting this mechanism.
- **Break condition:** KG contains incorrect or irrelevant information.

### Mechanism 2
- **Claim:** Subgraph retrieval outperforms other methods by providing comprehensive context.
- **Mechanism:** Subgraphs capture richer relationships and context compared to isolated triplets or paths, enabling better reasoning.
- **Core assumption:** Richer context leads to better reasoning and reduced hallucination.
- **Evidence anchors:**
  - [abstract] "subgraph-based methods often achieve the best performance for simpler query types."
  - [section] "Subgraph retrieval generally achieves best performance across different query types."
  - [corpus] No direct evidence; inferred from results.
- **Break condition:** Subgraph becomes too large or noisy.

### Mechanism 3
- **Claim:** KG-augmented models remain robust even when KGs contain errors.
- **Mechanism:** The model can still leverage correct information within the KG, mitigating the impact of errors.
- **Core assumption:** Correct information in KG outweighs incorrect information.
- **Evidence anchors:**
  - [abstract] "Even when KGs contain errors, KG-augmented models still outperform baselines using chain-of-thought reasoning alone."
  - [section] "incorporating KGs effectively reduces hallucinations in LLMs even when the KG is partially contaminated."
  - [corpus] No direct evidence; inferred from results.
- **Break condition:** Errors in KG overwhelm correct information.

## Foundational Learning

- **Concept:** Knowledge Graphs (KGs)
  - Why needed here: KGs provide structured, explicit factual knowledge that can be used to ground LLM outputs and reduce hallucination.
  - Quick check question: What are the key components of a KG (entities, relations, triplets)?

- **Concept:** Graph Retrieval Methods
  - Why needed here: Different retrieval methods (triplets, paths, subgraphs) offer varying levels of context and comprehensiveness, impacting reasoning and hallucination reduction.
  - Quick check question: How do triplet, path, and subgraph retrieval differ in terms of the information they provide?

- **Concept:** Hallucination Metrics (FActScore, SAFE)
  - Why needed here: These metrics decompose long-form text into atomic facts and validate them against external knowledge sources, enabling quantification of hallucination.
  - Quick check question: How do FActScore and SAFE differ in their approach to evaluating factual precision?

## Architecture Onboarding

- **Component map:** Query → Graph-guided retrieval (triplets/paths/subgraphs) → Graph-guided generation → Output
- **Critical path:** Retrieval → Generation → Evaluation
- **Design tradeoffs:**
  - Triplet retrieval: Fast, but limited context
  - Path retrieval: More context, but potentially noisy
  - Subgraph retrieval: Most comprehensive, but computationally expensive
- **Failure signatures:**
  - High hallucination rates despite KG integration: KG may contain errors or be irrelevant
  - Poor performance with subgraph retrieval: Subgraph may be too large or noisy
- **First 3 experiments:**
  1. Compare triplet, path, and subgraph retrieval on a small dataset.
  2. Evaluate the impact of KG errors on model performance.
  3. Test different KG perturbation methods and their effects on hallucination rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different subgraph retrieval methods (triplets, paths, subgraphs) perform under varying levels of KG contamination, and at what perturbation threshold does KG-derived information become detrimental?
- Basis in paper: [explicit] The paper explicitly compares subgraph retrieval against triplet and path retrieval under perturbed KGs in OKGQA-P, finding that subgraph methods are more robust to noise and maintain better performance even at higher perturbation levels.
- Why unresolved: While the paper shows that subgraph retrieval is generally more robust, it doesn't fully explore the relationship between perturbation level and method performance across different KG structures or query types. The exact threshold where KG-derived information becomes harmful across different scenarios remains unclear.
- What evidence would resolve it: Systematic experiments varying perturbation types (relation swapping, edge rewiring, deletion) and levels (0% to 100%) across diverse query types, measuring performance degradation points for each retrieval method.

### Open Question 2
- Question: How does the integration of multilingual subgraphs affect hallucination detection and reasoning accuracy in KG-augmented LLMs, and what are the trade-offs between context richness and noise introduction?
- Basis in paper: [explicit] The paper introduces a multilingual extension of OKGQA using Greek, Polish, Portuguese, Spanish, and English subgraphs, finding that multilingual context generally improves performance metrics.
- Why unresolved: The study only tested multilingual subgraphs on a subset of 300 queries and didn't systematically analyze the trade-offs between additional context and potential noise from language mismatches or duplicate entities across languages.
- What evidence would resolve it: Controlled experiments comparing monolingual vs multilingual subgraphs across the full OKGQA dataset, analyzing performance variance across different language combinations and measuring the impact of language-specific noise on hallucination detection accuracy.

### Open Question 3
- Question: How well do KG-augmented LLM methods generalize to domain-specific knowledge graphs beyond DBpedia, and what architectural modifications are needed for specialized domains?
- Basis in paper: [inferred] The paper acknowledges limitations in §6, noting that OKGQA primarily uses DBpedia and may not generalize to domain-specific knowledge graphs requiring specialized knowledge construction.
- Why unresolved: The experiments are limited to DBpedia, and the paper doesn't explore how KG-augmentation techniques would perform on specialized domains like biomedical, legal, or technical knowledge graphs with different structural properties and entity distributions.
- What evidence would resolve it: Experiments applying OKGQA and KG-augmentation methods to multiple domain-specific knowledge graphs (e.g., biomedical ontologies, legal databases) measuring performance differences and identifying necessary architectural adaptations for different domain characteristics.

### Open Question 4
- Question: What is the optimal balance between graph retrieval depth and computational efficiency for different query complexity levels in KG-augmented LLMs?
- Basis in paper: [explicit] The paper sets K=2 for 2-hop neighborhood retrieval and discusses that increasing beyond 2 hops leads to exponential growth in edges and nodes, increasing noise and computational complexity.
- Why unresolved: While the paper uses 2-hop subgraphs, it doesn't systematically explore how different retrieval depths affect performance across simple vs complex queries, or provide guidelines for optimizing the depth-complexity trade-off.
- What evidence would resolve it: Systematic experiments varying K (hop depth) from 1 to 4 across different query types in OKGQA, measuring performance gains vs computational costs to identify optimal depth settings for different query complexity categories.

### Open Question 5
- Question: How do different LLM backbone choices affect the reliability of LLM-as-evaluator frameworks for hallucination detection in KG-augmented systems?
- Basis in paper: [explicit] The paper validates its evaluation approach using three different LLMs (gpt-4o-mini, llama-3.1-8b-instruct, gemma-2-9b-it) and reports high inter-model agreement with Cohen's Kappa coefficients ranging from 0.70 to 0.84.
- Why unresolved: While the paper shows consistency across three models, it doesn't explore whether larger or more specialized LLMs would provide more reliable hallucination detection, or whether certain model architectures are better suited for this evaluation task.
- What evidence would resolve it: Comparative evaluation of hallucination detection reliability using a wider range of LLM backbones (including larger models like GPT-4, specialized models, and open-source alternatives) measuring inter-model agreement and correlation with human judgments across different hallucination types and query complexities.

## Limitations
- Limited evaluation to a single KG (DBpedia) restricts domain generalization claims
- Automated metrics may not capture all aspects of "trustworthiness"
- Performance improvements are measured primarily on English queries

## Confidence

**High Confidence:**
- KG integration reduces factual errors compared to baseline models
- Subgraph retrieval generally outperforms other retrieval methods
- KG-augmented models maintain performance advantages even with KG errors

**Medium Confidence:**
- Improvements are most pronounced for complex query types
- Prize-cost trade-off retrieval strategy effectively balances precision and comprehensiveness
- G-Eval metrics provide reliable assessment of reasoning improvements

## Next Checks

1. **Cross-domain validation**: Evaluate OKGQA performance across multiple KGs (e.g., Wikidata, domain-specific KGs) to assess generalizability beyond DBpedia.

2. **Human evaluation validation**: Conduct comprehensive human studies to validate automated metric findings, particularly for nuanced aspects of trustworthiness not captured by FActScore and SAFE.

3. **Error propagation analysis**: Systematically analyze how different types and distributions of KG errors affect model performance to better understand robustness limits.