---
ver: rpa2
title: 'Investigating the Robustness of Modelling Decisions for Few-Shot Cross-Topic
  Stance Detection: A Preregistered Study'
arxiv_id: '2404.03987'
source_url: https://arxiv.org/abs/2404.03987
tags:
- stance
- datasets
- task
- topics
- same
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how task definition (Pro/Con vs Same Side
  Stance), architecture (bi-encoding vs cross-encoding), and pre-training with Natural
  Language Inference (NLI) affect few-shot cross-topic stance detection. Using 7 stance
  detection datasets with 100 training examples each, the experiments test 4 hypotheses
  on stance robustness across topics.
---

# Investigating the Robustness of Modelling Decisions for Few-Shot Cross-Topic Stance Detection: A Preregistered Study

## Quick Facts
- arXiv ID: 2404.03987
- Source URL: https://arxiv.org/abs/2404.03987
- Reference count: 0
- Primary result: Cross-encoding architecture generally outperforms bi-encoding in stance detection when using Same Side Stance task definition, but results vary significantly by dataset and modeling choices.

## Executive Summary
This preregistered study investigates how task definition (Pro/Con vs Same Side Stance), architecture (bi-encoding vs cross-encoding), and Natural Language Inference (NLI) pre-training affect few-shot cross-topic stance detection performance. Using 7 stance detection datasets with only 100 training examples each, the experiments test four hypotheses about stance robustness across topics. The findings reveal that different modeling choices interact in complex ways across datasets, with no single approach consistently outperforming others in cross-topic settings.

## Method Summary
The study uses 7 stance detection datasets, each reduced to 100 training examples, and evaluates four modeling configurations: Pro/Con vs Same Side Stance task definitions, bi-encoding vs cross-encoding architectures, and with or without NLI pre-training. Bi-encoding uses SETFIT with RoBERTa-large, while cross-encoding uses RoBERTa-large with classification head. Models are trained for 20 epochs with batch size 2 and learning rate 16×10⁻⁶, then evaluated on cross-topic and in-topic test sets using macro F1 score.

## Key Results
- Cross-encoding generally outperforms bi-encoding for Same Side Stance, though some datasets show opposite results
- Adding NLI pre-training substantially improves performance for some datasets but yields inconsistent results across all datasets
- Same Side Stance definition provides better cross-topic robustness than Pro/Con, but this advantage varies by dataset and architecture
- No clear relationship exists between number of training topics and cross-topic performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-encoding architecture generally outperforms bi-encoding in stance detection when using the Same Side Stance task definition.
- Mechanism: Cross-encoding models jointly process both input texts with a shared transformer encoder, allowing for richer interaction between texts through self-attention. This is particularly beneficial for Same Side Stance where detecting similarity/dissimilarity between texts is crucial.
- Core assumption: The Same Side Stance task benefits more from joint text processing than Pro/Con stance classification.
- Evidence anchors:
  - [abstract]: "Cross-encoding generally outperforms bi-encoding, but some datasets show the opposite effect."
  - [section]: "Cross-encoding in general out-performs bi-encoding for Same Side Stance, when averaging performance across all other modelling choices."
  - [corpus]: Weak evidence - the corpus shows related work on cross-topic stance detection but doesn't directly address this specific architectural comparison.
- Break condition: If the task requires encoding text pairs independently (e.g., when topic information is crucial), bi-encoding might perform better.

### Mechanism 2
- Claim: Adding Natural Language Inference (NLI) knowledge through pre-fine-tuning improves stance detection performance, especially for Same Side Stance tasks.
- Mechanism: NLI training provides models with knowledge about textual relationships (entailment, contradiction, neutral), which transfers to stance detection by helping the model understand how arguments relate to each other and to topics.
- Core assumption: Stance detection and NLI share similar underlying reasoning about textual relationships.
- Evidence anchors:
  - [abstract]: "Adding NLI training to our models gives considerable improvement, but these results are not consistent across all datasets."
  - [section]: "Adding NLI as auxiliary knowledge improved the classification performance of our models substantially, which becomes visible when looking at the effect on other modelling choices."
  - [corpus]: Weak evidence - the corpus shows related work on stance detection but doesn't specifically address NLI pre-fine-tuning.
- Break condition: If the dataset has very different characteristics from NLI data (e.g., informal social media text vs. formal NLI examples), the transfer benefit may be limited.

### Mechanism 3
- Claim: Same Side Stance task definition provides better cross-topic robustness than Pro/Con definition, but this advantage varies by dataset and architecture.
- Mechanism: Same Side Stance focuses on similarity between texts rather than topic-specific stance labels, making it less dependent on topic-specific knowledge and more generalizable across topics.
- Core assumption: Cross-topic robustness is primarily determined by how much the task definition relies on topic-specific knowledge.
- Evidence anchors:
  - [abstract]: "The effect of the Same Side Stance definition on performance differs per dataset and is influenced by other modelling choices."
  - [section]: "SSSC does seem to consistently improve results when combined with NLI training, and this is mostly visible in datasets with two Pro/Con stance labels."
  - [corpus]: Weak evidence - the corpus shows related work on stance detection but doesn't directly compare these specific task definitions.
- Break condition: If the dataset contains many topics with similar argumentation patterns, the advantage of Same Side Stance may be reduced.

## Foundational Learning

- Concept: Task definition differences (Pro/Con vs Same Side Stance)
  - Why needed here: Understanding these definitions is crucial for interpreting results and choosing appropriate architectures.
  - Quick check question: What is the key difference between Pro/Con stance detection and Same Side Stance classification?

- Concept: Cross-encoding vs bi-encoding architectures
  - Why needed here: The architectural choice significantly impacts performance and is a core experimental variable.
  - Quick check question: How does cross-encoding differ from bi-encoding in terms of how it processes text pairs?

- Concept: Few-shot learning principles
  - Why needed here: The study uses only 100 training examples per dataset, making few-shot learning techniques essential.
  - Quick check question: What is the main challenge of few-shot learning and how does SETFIT address it?

## Architecture Onboarding

- Component map: Dataset preprocessing (re-coding to Same Side Stance) -> Model training (SETFIT bi-encoding or RoBERTa cross-encoding) -> NLI pre-fine-tuning (optional) -> Evaluation (cross-topic vs in-topic testing)

- Critical path: The critical path for experiments is: dataset selection → re-coding to Same Side Stance → model training with different configurations → evaluation on test sets. The most time-consuming step is model training, especially for cross-encoding.

- Design tradeoffs: Cross-encoding provides better performance but requires more computational resources and doesn't scale as well to large datasets. Bi-encoding is more efficient but may miss important interactions between texts. NLI pre-fine-tuning improves performance but adds training time and complexity.

- Failure signatures: If a model performs poorly on cross-topic data but well on in-topic data, this suggests topic overfitting. If bi-encoding outperforms cross-encoding, this might indicate that text interactions are less important than efficient encoding. If NLI pre-fine-tuning doesn't help, the dataset characteristics might differ significantly from NLI data.

- First 3 experiments:
  1. Compare Pro/Con vs Same Side Stance definitions on a simple dataset to understand task definition impact.
  2. Test cross-encoding vs bi-encoding on Same Side Stance to verify the general performance trend.
  3. Evaluate NLI pre-fine-tuning impact on a dataset showing good baseline performance to measure improvement magnitude.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the robustness of cross-topic stance detection vary when using datasets with a broader range of topics or different cultural contexts?
- Basis in paper: [inferred] The paper mentions that current datasets are limited to socio-political debates in the English-speaking United States, suggesting that cross-topic robustness might differ with more diverse topics or cultural contexts.
- Why unresolved: The study did not explore datasets beyond the English-speaking United States, leaving the impact of broader topic diversity or cultural context on cross-topic robustness untested.
- What evidence would resolve it: Conducting experiments with datasets that include a wider variety of topics and cultural contexts, then comparing cross-topic performance to the current study.

### Open Question 2
- Question: To what extent does the number of training topics influence the performance of stance detection models in few-shot settings, beyond the datasets studied?
- Basis in paper: [explicit] The paper found no clear relationship between the number of training topics and performance, but noted that datasets with more topics generally performed better with NLI models.
- Why unresolved: The study's dataset selection might not fully capture the range of possible topic numbers, and the influence of topic diversity on performance remains unclear.
- What evidence would resolve it: Systematically varying the number of training topics across a broader range of datasets and measuring the impact on model performance.

### Open Question 3
- Question: How do different pre-trained models, beyond RoBERTa, affect the robustness of cross-topic stance detection in few-shot settings?
- Basis in paper: [inferred] The paper focused on RoBERTa models, but did not explore other pre-trained models, leaving the potential impact of different models on cross-topic robustness untested.
- Why unresolved: The study did not compare RoBERTa with other pre-trained models, so the relative effectiveness of different models for cross-topic stance detection is unknown.
- What evidence would resolve it: Repeating the experiments with a variety of pre-trained models (e.g., BERT, GPT, T5) and comparing their cross-topic performance.

## Limitations

- The inconsistent effects of NLI fine-tuning across datasets suggest that the underlying mechanisms are not fully understood.
- The interaction between task definition and architecture is complex and dataset-dependent.
- The study's focus on few-shot learning (100 examples) may not generalize to settings with more training data.

## Confidence

- High Confidence: Cross-encoding architecture generally outperforms bi-encoding, as this finding is consistent across multiple datasets and experimental conditions.
- Medium Confidence: Same Side Stance task definition provides better cross-topic robustness than Pro/Con definition, but this advantage varies significantly by dataset and is influenced by other modeling choices.
- Low Confidence: The relationship between number of training topics and cross-topic generalization performance, as the study found no clear relationship but this may be due to the specific datasets used or the few-shot setting.

## Next Checks

1. Conduct ablation studies to isolate the contribution of NLI fine-tuning on datasets where it fails to improve performance, testing whether different NLI training datasets or fine-tuning strategies might yield better results.

2. Test the Same Side Stance and Pro/Con task definitions on additional datasets with varying characteristics (topic diversity, text domain, argumentation complexity) to better understand when each definition is advantageous.

3. Evaluate whether increasing the number of training examples per dataset (e.g., to 500 or 1000) changes the relative performance of the different modeling choices, particularly for cross-topic generalization.