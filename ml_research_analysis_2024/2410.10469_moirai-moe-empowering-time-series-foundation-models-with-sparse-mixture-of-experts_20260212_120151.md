---
ver: rpa2
title: 'Moirai-MoE: Empowering Time Series Foundation Models with Sparse Mixture of
  Experts'
arxiv_id: '2410.10469'
source_url: https://arxiv.org/abs/2410.10469
tags:
- time
- series
- moirai
- expert
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Moirai-MoE, a sparse mixture-of-experts (MoE)
  approach for time series foundation models that overcomes the limitations of frequency-based
  model specialization. Unlike previous methods that rely on separate projection layers
  for different frequencies, Moirai-MoE uses a single projection layer with MoE Transformers
  to achieve token-level specialization in a data-driven manner.
---

# Moirai-MoE: Empowering Time Series Foundation Models with Sparse Mixture of Experts

## Quick Facts
- arXiv ID: 2410.10469
- Source URL: https://arxiv.org/abs/2410.10469
- Authors: Xu Liu; Juncheng Liu; Gerald Woo; Taha Aksu; Yuxuan Liang; Roger Zimmermann; Chenghao Liu; Silvio Savarese; Caiming Xiong; Doyen Sahoo
- Reference count: 40
- Primary result: 17% performance improvement over Moirai with up to 65× fewer activated parameters

## Executive Summary
This paper introduces Moirai-MoE, a sparse mixture-of-experts (MoE) approach for time series foundation models that overcomes the limitations of frequency-based model specialization. Unlike previous methods that rely on separate projection layers for different frequencies, Moirai-MoE uses a single projection layer with MoE Transformers to achieve token-level specialization in a data-driven manner. The model employs a novel gating function using cluster centroids from pretrained models to improve expert assignment.

## Method Summary
Moirai-MoE replaces frequency-specific projection layers with a single projection layer and sparse MoE Transformers. The model uses 32 total experts with 2 activated per token, employing a novel gating function based on cluster centroids from pretrained token embeddings. The training uses a decoder-only autoregressive approach with patch size 16, masking ratio 0.3, and is trained on the LOTSA dataset for 50k-250k steps using AdamW optimizer with specific hyperparameters.

## Key Results
- Achieves up to 17% performance improvements over Moirai baseline
- Uses up to 65× fewer activated parameters compared to other foundation models
- Shows superior performance on 39 datasets including 29 from Monash benchmark and 10 additional datasets for zero-shot forecasting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Frequency-based model specialization is insufficient because frequency doesn't reliably indicate underlying patterns.
- Mechanism: Different time series with the same frequency can exhibit varied patterns, while those with different frequencies may display similar patterns. This mismatch between frequency and pattern undermines the efficacy of model specialization.
- Core assumption: Time series patterns are more informative than frequency metadata for determining appropriate model specialization.
- Evidence anchors:
  - [abstract] "Frequency is not a reliable indicator of the underlying patterns in time series. For example, time series with different frequencies can display similar patterns, while those with the same frequency may exhibit varied patterns."
  - [section] "the diversity of patterns within the same frequency group, the similarity of patterns across different frequencies"
  - [corpus] Weak - no corpus evidence directly supporting this specific mechanism
- Break condition: If time series patterns and frequency become highly correlated in a given dataset, frequency-based specialization could regain effectiveness.

### Mechanism 2
- Claim: MoE enables token-level specialization that adapts to non-stationary time series patterns.
- Mechanism: The gating function routes tokens to specialized experts based on their characteristics, allowing the model to handle varied distributions within short context windows of single time series.
- Core assumption: Sparse expert routing can capture fine-grained variations in time series patterns that frequency-based methods miss.
- Evidence anchors:
  - [abstract] "delegating the modeling of diverse time series patterns to the sparse mixture of experts (MoE) within Transformers"
  - [section] "token-level model specialization in a data-driven manner"
  - [corpus] Moderate - corpus shows other MoE approaches exist but doesn't confirm this specific token-level effectiveness
- Break condition: If the gating function becomes too uniform (all tokens routed to same experts), token-level specialization loses effectiveness.

### Mechanism 3
- Claim: Cluster-based gating function improves expert allocation by leveraging pretrained knowledge.
- Mechanism: Using cluster centroids from pretrained model embeddings as gating targets produces more effective expert specialization than random initialization.
- Core assumption: Pretrained embeddings better reflect the true distribution of time series patterns than randomly initialized gating.
- Evidence anchors:
  - [section] "introduces a new gating mechanism that leverages cluster centroids derived from the token representations of a pretrained model to guide expert allocations"
  - [section] "clusters of pretrained token embeddings more closely reflect the real distribution of the data, leading to more effective expert specialization"
  - [corpus] Weak - corpus shows similar MoE approaches but doesn't confirm clustering superiority
- Break condition: If pretrained model embeddings are poorly aligned with target task distribution, cluster-based gating could underperform random initialization.

## Foundational Learning

- Concept: Time series patch embedding
  - Why needed here: Converts raw time series into token sequences that transformers can process efficiently
  - Quick check question: How does patch size affect both model performance and inference speed?

- Concept: Mixture of Experts (MoE) routing
  - Why needed here: Enables sparse activation of specialized networks for different time series patterns
  - Quick check question: What determines which tokens are routed to which experts?

- Concept: Non-stationary time series
  - Why needed here: Explains why frequency-based approaches fail and why token-level specialization is needed
  - Quick check question: How does non-stationarity manifest in real-world time series data?

## Architecture Onboarding

- Component map: Single projection layer → Patch embedding → Transformer layers with MoE blocks → Output projection → Mixture distribution parameters
- Critical path: Token embedding → Gating function → Expert routing → Expert computation → Output aggregation
- Design tradeoffs:
  - Single projection vs multiple frequency-specific projections: Simpler architecture but requires MoE to handle diversity
  - Cluster-based gating vs learned gating: Better initial specialization but depends on quality of pretrained embeddings
  - Decoder-only training vs masked encoder: More efficient training but requires careful masking ratio selection
- Failure signatures:
  - All tokens routed to same experts → Gating function collapsed
  - No performance improvement over baseline → MoE not learning useful specialization
  - Degraded performance → Too much specialization or poor gating function
- First 3 experiments:
  1. Compare MoE with different numbers of experts (4, 8, 16, 32) on a small dataset
  2. Test cluster-based gating vs learned gating on a validation set
  3. Evaluate patch size impact on both accuracy and inference speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of experts and expert capacity for different time series forecasting tasks and data distributions?
- Basis in paper: [inferred] The paper mentions that performance improves as the number of experts increases, and explores expert configurations up to 32 experts, but does not systematically investigate the relationship between task characteristics and optimal expert settings.
- Why unresolved: The paper only evaluates performance at specific expert counts (32) without exploring the full design space or analyzing how task complexity, data distribution, and model size affect the optimal number of experts.
- What evidence would resolve it: Systematic experiments varying the number of experts (e.g., 4, 8, 16, 32, 64) across different types of time series tasks (e.g., univariate vs multivariate, seasonal vs non-seasonal) while measuring both performance and computational efficiency.

### Open Question 2
- Question: How does the token clustering gating mechanism perform when applied to different pretrained models or when trained from scratch?
- Basis in paper: [explicit] The paper introduces a novel gating function using cluster centroids from a pretrained MOIRAI model and demonstrates its superiority over other gating functions, but does not test alternative pretrained models or the no-pretraining scenario.
- Why unresolved: The paper only evaluates the clustering approach using MOIRAI as the pretrained model, leaving open questions about the generality of this approach and whether it requires large-scale pretraining to be effective.
- What evidence would resolve it: Comparative experiments using different pretrained models (e.g., Chronos, TimesFM) or training the gating mechanism from scratch, measuring both performance and the quality of expert specialization.

### Open Question 3
- Question: What is the relationship between expert specialization patterns and the types of time series patterns being modeled (e.g., trend, seasonality, noise)?
- Basis in paper: [inferred] The paper shows that expert allocation reflects time series periodicity patterns and that shallow layers show more routing preferences than deep layers, but does not systematically analyze which experts specialize in which types of patterns.
- Why unresolved: While the paper provides visualizations showing expert allocation patterns, it does not conduct a detailed analysis linking specific experts to specific time series characteristics or patterns.
- What evidence would resolve it: Detailed analysis of expert activation patterns across different types of time series (e.g., purely seasonal, trending, noisy, mixed patterns) combined with ablation studies where specific experts are removed to determine their contribution to modeling different patterns.

## Limitations

- Dataset Dependency: Effectiveness relies heavily on LOTSA dataset for pretraining with limited composition details
- Scalability Constraints: Token-level routing decisions may introduce computational overhead at inference time
- Generalization Boundaries: Performance improvements haven't been thoroughly tested across diverse domain types

## Confidence

- High Confidence: Frequency-based specialization limitations are well-established; MoE architecture is proven
- Medium Confidence: 17% performance improvement over Moirai depends on exact implementation details; 65× parameter reduction is mathematically sound but practical impact unclear
- Low Confidence: Superiority of cluster-based gating over learned gating lacks rigorous validation; token-level specialization necessity for non-stationary series lacks empirical boundary testing

## Next Checks

1. **Gating Function Sensitivity Analysis**: Conduct ablation studies comparing cluster-based gating against random initialization and learned gating functions across multiple pretraining datasets, measuring both performance differences and expert utilization patterns.

2. **Inference Time Overhead Measurement**: Implement timing benchmarks comparing Moirai-MoE against frequency-based specialization models, measuring actual inference latency and memory usage across different sequence lengths and batch sizes.

3. **Failure Mode Characterization**: Systematically test Moirai-MoE on datasets where frequency patterns strongly correlate with underlying patterns, comparing performance against frequency-based models to identify boundary conditions where each approach excels or fails.