---
ver: rpa2
title: 'StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video Understanding'
arxiv_id: '2411.03628'
source_url: https://arxiv.org/abs/2411.03628
tags:
- video
- understanding
- streaming
- tasks
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces StreamingBench, the first comprehensive benchmark
  designed to evaluate the streaming video understanding capabilities of multimodal
  large language models (MLLMs). Unlike existing video benchmarks that focus on offline
  video comprehension, StreamingBench simulates real-time streaming scenarios with
  900 videos and 4,500 human-curated QA pairs across 18 tasks.
---

# StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video Understanding

## Quick Facts
- arXiv ID: 2411.03628
- Source URL: https://arxiv.org/abs/2411.03628
- Reference count: 40
- 13 state-of-the-art MLLMs tested on streaming video understanding achieve only 67.07% accuracy, significantly below human performance of 91.66%

## Executive Summary
This paper introduces StreamingBench, the first comprehensive benchmark designed to evaluate streaming video understanding capabilities of multimodal large language models (MLLMs). Unlike existing video benchmarks that focus on offline comprehension, StreamingBench simulates real-time streaming scenarios with 900 videos and 4,500 human-curated QA pairs across 18 tasks. The benchmark evaluates three core aspects: real-time visual understanding, omni-source understanding, and contextual understanding. Experiments with 13 state-of-the-art MLLMs, including GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet, reveal that even the best-performing models achieve only 67.07% accuracy, significantly below human-level performance of 91.66%. The results highlight substantial gaps in MLLMs' ability to process streaming inputs, particularly in handling redundant information, proactive outputs, and sequential question-answering contexts.

## Method Summary
StreamingBench evaluates MLLMs on streaming video understanding by presenting questions at specific timestamps during video playback, requiring models to answer based only on available context up to that point. The benchmark includes 900 videos across 8 categories with 4,500 QA pairs spanning 18 tasks organized into three categories: real-time visual understanding (3 tasks), omni-source understanding (7 tasks), and contextual understanding (8 tasks). Models process video segments from start to question timestamp, with different frame extraction rates based on video length and model constraints. Evaluation uses multiple-choice questions for most tasks and timestamp-based accuracy for proactive output tasks. The benchmark provides standardized prompts and conversion methodology to transform streaming tasks into offline evaluation format.

## Key Results
- Best-performing model (GPT-4o) achieves only 67.07% accuracy, significantly below human performance of 91.66%
- MLLMs struggle particularly with concurrent-type temporal clues and proactive output tasks
- All models perform worse on videos longer than 60 seconds compared to shorter videos
- Omni-source understanding tasks show the largest performance gap, indicating difficulties in multimodal integration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: StreamingBench simulates real-time streaming scenarios by presenting questions at specific timestamps during video playback rather than at the end.
- Mechanism: The benchmark breaks videos into segments ending at each question's timestamp, forcing models to process only the video content available up to that point without access to future frames.
- Core assumption: Real-time understanding requires temporal constraints where models cannot access complete video context before answering questions.
- Evidence anchors:
  - [abstract] "Each video features five questions presented at different time points to simulate a continuous streaming scenario"
  - [section] "We convert each streaming task into an offline one for evaluation... Each video is clipped into the segment from the beginning to the timestamp when the question is asked"

### Mechanism 2
- Claim: The benchmark evaluates three distinct aspects of streaming understanding: real-time visual processing, omni-source integration, and contextual awareness.
- Mechanism: Each task category targets specific capabilities - visual-only processing for real-time understanding, synchronized audio-visual processing for omni-source tasks, and historical context handling for contextual tasks.
- Core assumption: Streaming video comprehension requires different cognitive capabilities than offline video analysis, including temporal reasoning and context management.
- Evidence anchors:
  - [abstract] "StreamingBench assesses three core aspects of streaming video understanding: (1) real-time visual understanding, (2) omni-source understanding, and (3) contextual understanding"
  - [section] "We identify three key distinctions between a streaming video understanding benchmark and traditional offline video benchmarks"

### Mechanism 3
- Claim: Human-level performance serves as the reference standard for evaluating model capabilities in streaming scenarios.
- Mechanism: The benchmark includes human evaluation on sampled tasks, providing a performance ceiling that current MLLMs cannot reach, demonstrating the gap between AI and human streaming comprehension.
- Core assumption: Human performance represents the optimal capability for streaming video understanding tasks, providing a meaningful benchmark for AI progress.
- Evidence anchors:
  - [abstract] "even the best-performing models achieve only 67.07% accuracy, significantly below human-level performance of 91.66%"
  - [section] "For comparison, we sample 10% of the tasks from each of the 18 tasks for human evaluation"

## Foundational Learning

- Concept: Temporal reasoning in video understanding
  - Why needed here: Streaming video tasks require understanding events as they unfold over time, distinguishing between what has happened, what is happening, and what might happen next
  - Quick check question: Can you explain the difference between prior-type, concurrent-type, and subsequent-type temporal clues in video understanding?

- Concept: Multimodal integration
  - Why needed here: Omni-source understanding tasks require combining visual and audio information that may be temporally aligned or provide complementary information
  - Quick check question: How would you design a system to synchronize audio cues with corresponding visual events in streaming video?

- Concept: Context management in sequential interactions
  - Why needed here: Contextual understanding tasks involve maintaining and utilizing information from previous question-answer pairs to inform current responses
  - Quick check question: What strategies could you use to maintain episodic memory across a sequence of related questions in a streaming video context?

## Architecture Onboarding

- Component map:
  Video preprocessing pipeline -> Question generation system -> Evaluation framework -> Model interface layer -> Quality control module

- Critical path:
  1. Video loading and frame extraction
  2. Timestamp assignment for questions
  3. Video segmentation up to each question timestamp
  4. Model inference with appropriate prompts
  5. Answer evaluation against ground truth
  6. Performance aggregation across tasks

- Design tradeoffs:
  - Frame extraction rate vs. computational cost (higher fps provides more detail but increases processing time)
  - Multiple-choice vs. open-ended questions (deterministic evaluation vs. richer responses)
  - Video length limits vs. realistic streaming scenarios (processing constraints vs. real-world applicability)
  - Human annotation vs. automatic generation (quality vs. scalability)

- Failure signatures:
  - Models performing well on offline benchmarks but poorly on streaming tasks indicates lack of temporal reasoning capability
  - Consistent failure on omni-source tasks suggests poor multimodal integration
  - Poor performance on contextual tasks indicates inability to maintain or utilize historical information

- First 3 experiments:
  1. Test model performance on short vs. long videos to identify length-related limitations
  2. Evaluate model behavior with and without redundant information to assess context filtering ability
  3. Compare performance on tasks with resolved vs. unresolved references to measure sequential understanding capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications would enable MLLMs to better handle concurrent-type temporal clues in streaming video understanding?
- Basis in paper: [inferred] The paper shows that model performance is significantly worse on concurrent-type tasks compared to prior-type tasks, and attributes this to underrepresentation of concurrent tasks in current training datasets.
- Why unresolved: The paper identifies the performance gap but does not explore specific technical solutions for improving concurrent temporal understanding.
- What evidence would resolve it: Empirical studies comparing different architectural designs (e.g., attention mechanisms, temporal encoders) or training strategies that explicitly incorporate concurrent temporal reasoning tasks.

### Open Question 2
- Question: How can MLLMs be optimized to maintain performance consistency across varying video lengths, particularly for long-form streaming content?
- Basis in paper: [explicit] The paper demonstrates that all models perform worse on videos longer than 60 seconds compared to shorter videos, with Qwen2-VL being an exception in certain tasks.
- Why unresolved: The paper identifies the length-related performance gap but does not investigate the underlying causes or propose solutions for long-form video processing.
- What evidence would resolve it: Comparative analysis of different frame sampling strategies, memory mechanisms, or hierarchical processing approaches for long video inputs.

### Open Question 3
- Question: What are the key limitations preventing MLLMs from effectively handling proactive output tasks in streaming scenarios?
- Basis in paper: [explicit] The paper finds that MLLMs struggle with proactive output tasks, showing that even with relaxed evaluation thresholds, performance remains poor, and transforming these tasks into passive formats significantly improves results.
- Why unresolved: The paper identifies the problem but does not deeply investigate whether the issue stems from instruction-following capabilities, temporal reasoning, or other factors.
- What evidence would resolve it: Detailed analysis of model behavior during proactive output tasks, including error pattern analysis and ablation studies of different prompt formulations or internal state management strategies.

## Limitations
- Benchmark relies on GPT-4o for caption generation and QA pair creation, potentially introducing model-specific biases in task design and difficulty distribution
- Proprietary models evaluated with different frame extraction rates and context limits, making direct comparison with open-source models challenging
- Proactive output evaluation methodology with 2-second tolerance window may artificially inflate performance on tasks requiring precise temporal reasoning

## Confidence
- High confidence: The fundamental observation that current MLLMs significantly underperform humans on streaming video understanding tasks (67.07% vs 91.66% accuracy)
- Medium confidence: The characterization of specific failure modes in redundant information handling and proactive outputs, given the methodological constraints in evaluation
- Low confidence: Claims about the relative strengths of specific models (e.g., GPT-4o's superior performance) due to varying technical constraints and evaluation conditions

## Next Checks
1. Replicate the streaming benchmark evaluation using a consistent frame extraction strategy across all models to isolate true capability differences from technical constraints
2. Conduct ablation studies on the proactive output tolerance window to determine the impact of the 2-second parameter on reported performance
3. Compare model performance on automatically generated QA pairs versus human-curated pairs to assess potential bias in the benchmark construction methodology