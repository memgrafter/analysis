---
ver: rpa2
title: 'CiTrus: Squeezing Extra Performance out of Low-data Bio-signal Transfer Learning'
arxiv_id: '2412.11695'
source_url: https://arxiv.org/abs/2412.11695
tags:
- citrus
- pre-training
- dataset
- data
- nlpatchtst
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving prediction performance
  on low-data bio-signal downstream tasks using transfer learning. The authors propose
  CiTrus, a convolution-transformer hybrid model with masked auto-encoding for bio-signal
  transfer learning, introducing a frequency-based masked auto-encoding task and a
  more comprehensive evaluation framework.
---

# CiTrus: Squeezing Extra Performance out of Low-data Bio-signal Transfer Learning

## Quick Facts
- arXiv ID: 2412.11695
- Source URL: https://arxiv.org/abs/2412.11695
- Authors: Eloy Geenjaar; Lie Lu
- Reference count: 13
- Primary result: CiTrus achieves up to 60% performance improvement on low-data bio-signal downstream tasks compared to baseline models.

## Executive Summary
This paper addresses the challenge of improving prediction performance on low-data bio-signal downstream tasks using transfer learning. The authors propose CiTrus, a convolution-transformer hybrid model with masked auto-encoding for bio-signal transfer learning, introducing a frequency-based masked auto-encoding task and a more comprehensive evaluation framework. They also introduce a method of aligning downstream datasets with different temporal lengths and sampling rates to the pre-training dataset. The primary results show that CiTrus significantly outperforms previous models on some low-data downstream tasks, with improvements up to 60% in some cases. The frequency-based pre-training performs best on average for the lowest and highest data regimes.

## Method Summary
CiTrus combines a 3-layer residual CNN encoder with a PatchTST transformer, using masked auto-encoding for pre-training on a large EEG dataset (SleepEDF). The model learns temporal relationships through patch masking and reconstruction, with the CNN providing frequency-domain filtering capabilities. A novel frequency-based pre-training approach generates spectrograms from bio-signals for reconstruction, while a resampling strategy aligns downstream datasets with different sampling rates to the pre-training dataset. The model is fine-tuned on various bio-signal datasets (EMG, ECG, PPG, HAR, FDB, Epilepsy, Gesture, SleepEDF) using linear classifiers or regressors.

## Key Results
- CiTrus significantly outperforms previous models on some low-data downstream tasks, with improvements up to 60% in some cases
- Frequency-based pre-training performs best on average for the lowest and highest data regimes
- The resampling approach effectively aligns downstream datasets with different sampling rates to the pre-training dataset

## Why This Works (Mechanism)

### Mechanism 1
Convolution layers improve transfer learning for bio-signals because they act as frequency-domain filters and share weights efficiently across time steps. The model learns localized time-frequency features through shared convolutional filters, reducing parameter count and enforcing inductive bias toward spectral consistency. Core assumption: Low-level spectral patterns are transferable across different bio-signal modalities (EEG, EMG, ECG, PPG, HAR). Break condition: If downstream datasets have vastly different spectral characteristics, the shared frequency filters may not transfer effectively.

### Mechanism 2
Masked auto-encoding on bio-signals forces the model to learn temporal relationships and patch dependencies, which generalize to new tasks. By masking random patches of encoded features and reconstructing them, the transformer learns to infer missing data from surrounding context, building robust temporal representations. Core assumption: Temporal dependencies learned during reconstruction are relevant for the target prediction tasks in downstream datasets. Break condition: If downstream datasets require completely different temporal patterns than those learned during pre-training, the reconstructed context may not generalize.

### Mechanism 3
Resampling downstream datasets to match the pre-training frequency avoids mismatches in learned temporal and spectral representations. By aligning sampling rates, the model attends to the same frequency ranges and temporal scales in both pre-training and fine-tuning, preserving learned feature relevance. Core assumption: Learned representations are frequency- and time-scale dependent, so matching sampling rates improves fine-tuning. Break condition: If downstream data has very short windows that cannot be resampled meaningfully, interpolation may distort features.

## Foundational Learning

- Concept: Masked auto-encoding (MAE) for time-series
  - Why needed here: Enables unsupervised pre-training on large unlabeled bio-signal datasets, which are expensive to label.
  - Quick check question: What is the purpose of masking patches in MAE, and how does the model use non-masked patches to reconstruct them?

- Concept: Convolutional neural networks as frequency filters
  - Why needed here: Convolutions provide parameter-efficient, localized feature extraction that captures spectral patterns common across bio-signals.
  - Quick check question: How do shared convolutional filters enforce an inductive bias toward frequency-domain learning?

- Concept: Multimodal pre-training
  - Why needed here: Combining complementary bio-signal modalities (e.g., EEG + EOG) during pre-training can improve downstream generalization by exposing the model to richer feature relationships.
  - Quick check question: How does masking patches independently per modality encourage cross-modal information flow?

## Architecture Onboarding

- Component map: Input → CNN → Patches → Masking → Transformer → Pre-train head (pre-training) / Linear head (fine-tuning)
- Critical path: Input → 3-layer residual CNN with increasing channel width → Non-overlapping windows (patch size 20) passed through CNN → Random patch masking (50% ratio, block size 5 for CiTrus) → PatchTST transformer (4 layers, 64 latent dims, 128 feed-forward, 8 heads) → Inverted CNN for signal reconstruction or spectrogram prediction (pre-training) / Linear classifier (or regressor) on encoded or context features (fine-tuning)
- Design tradeoffs:
  - Convolution vs. linear patch embedding: Convolutions add frequency bias but increase computation; linear embeddings are simpler but less expressive for spectral data.
  - Masked vs. full reconstruction: Masking forces context learning but may slow convergence; full reconstruction is faster but may not learn temporal inference as well.
  - Single vs. multimodal pre-training: Multimodal adds cross-modal learning but increases complexity and data requirements.
- Failure signatures:
  - Low performance on downstream tasks: Likely due to frequency mismatch between pre-training and fine-tuning sampling rates.
  - Overfitting on small datasets: Reduce dropout or simplify model; use early stopping.
  - Poor convergence during pre-training: Check masking ratio, patch size, and learning rate.
- First 3 experiments:
  1. Verify that CNN encoder produces meaningful frequency features by visualizing learned filters and comparing to raw signal spectrograms.
  2. Test different masking ratios (30%, 50%, 70%) on a small downstream dataset to see impact on fine-tuning performance.
  3. Compare fine-tuning with and without resampling to match pre-training frequency on a dataset with different sampling rate.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CiTrus (fp) compare to other models on datasets with sampling frequencies significantly different from the pre-training dataset? The paper states that CiTrus (fp) performs the best on average across datasets for the lowest and highest data regimes, and that the frequency-based pre-training approach improves performance on datasets with different sampling frequencies, but does not provide a direct comparison of CiTrus (fp) performance across datasets with varying sampling frequencies.

### Open Question 2
What is the impact of using a different window size for pre-training on the SleepEDF dataset (e.g., 30 seconds vs. 2 seconds) on the performance of CiTrus models on downstream tasks? The paper mentions that using 30s windows for pre-training and interpolating the size of the dataset to 3000 timesteps improves performance for some datasets, but does not provide a comprehensive comparison.

### Open Question 3
How does the performance of CiTrus models with multimodal pre-training compare to models with unimodal pre-training on datasets with limited data availability? The paper states that multimodal pre-training often improves downstream fine-tuning performance, especially for transformer-based models, but does not provide a detailed comparison across data availability regimes.

## Limitations

- Evaluation relies heavily on specific bio-signal modalities without exploring broader applications
- Frequency-based pre-training advantage demonstrated primarily on SleepEDF dataset may not generalize to all bio-signal domains
- Resampling approach assumes linear interpolation is sufficient for all temporal alignments, which may not hold for highly non-linear signal patterns

## Confidence

- **High Confidence**: The general framework of combining convolution-transformer architectures with masked auto-encoding for bio-signal transfer learning is well-supported by the empirical results across multiple datasets.
- **Medium Confidence**: The specific advantage of frequency-based pre-training over time-based approaches is supported but requires more extensive validation across diverse bio-signal modalities and sampling rates.
- **Medium Confidence**: The resampling strategy for aligning downstream datasets with different sampling rates shows promise but lacks comprehensive analysis of potential interpolation artifacts or alternative alignment methods.

## Next Checks

1. **Cross-domain validation**: Test CiTrus on bio-signal datasets from completely different domains (e.g., medical imaging signals, audio bio-signals) to assess generalizability beyond the current evaluation set.
2. **Frequency analysis validation**: Perform spectral analysis comparing learned filters from the CNN encoder across different pre-training configurations to verify that frequency-based pre-training consistently captures relevant spectral patterns.
3. **Resampling robustness check**: Evaluate the impact of different resampling methods (linear interpolation vs. more sophisticated approaches like spline interpolation or signal reconstruction) on downstream task performance, particularly for datasets with significant temporal mismatches.