---
ver: rpa2
title: Adversarial Attacks on Hidden Tasks in Multi-Task Learning
arxiv_id: '2405.15244'
source_url: https://arxiv.org/abs/2405.15244
tags:
- task
- attack
- target
- tasks
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies adversarial attacks on hidden tasks in multi-task
  learning, where an attacker aims to degrade performance of a target task without
  access to that task's information. The authors propose a novel method that exploits
  catastrophic forgetting in multi-task models by fine-tuning the model on non-target
  tasks, then generating adversarial examples that mimic the forgotten features of
  the target task.
---

# Adversarial Attacks on Hidden Tasks in Multi-Task Learning

## Quick Facts
- arXiv ID: 2405.15244
- Source URL: https://arxiv.org/abs/2405.15244
- Reference count: 40
- Key outcome: Novel adversarial attack method exploits catastrophic forgetting to degrade hidden task performance by at least 20% while preserving non-target task accuracy

## Executive Summary
This paper introduces a novel adversarial attack method targeting hidden tasks in multi-task learning systems. The attacker aims to degrade performance of a specific target task without access to that task's information or training data. The method exploits catastrophic forgetting by fine-tuning the shared backbone network on non-target tasks, then generating adversarial examples that mimic the forgotten feature representations of the hidden task. Experiments on CelebA and DeepFashion datasets demonstrate significant performance degradation on hidden tasks while maintaining stealthiness on non-target tasks.

## Method Summary
The method involves fine-tuning the shared backbone network on non-target tasks to induce catastrophic forgetting of the hidden task's features. Two attack variants are proposed: CF attack, which generates adversarial examples mimicking post-forgetting feature vectors, and CF delta attack, which additionally amplifies the direction of feature vector changes. A regularization term ensures stealthiness by constraining perturbations to preserve non-target task performance. The approach achieves selective degradation of the hidden task while maintaining performance on other tasks.

## Key Results
- CF attack achieves at least 20% performance degradation on target tasks compared to existing no-box attacks
- CF delta attack further improves attack effectiveness by amplifying feature vector differences
- Stealthiness is maintained with non-target task accuracy degradation below 0.1 threshold
- Results validated across multiple datasets (CelebA, DeepFashion) and task configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The CF attack exploits catastrophic forgetting by fine-tuning the backbone on non-target tasks, causing the backbone to lose features critical for the hidden target task.
- Mechanism: When the backbone is fine-tuned only on non-target tasks, it forgets the representations useful for the hidden task. Adversarial examples are then generated to mimic the post-forgetting feature vectors, causing the hidden task to fail while preserving non-target task performance.
- Core assumption: The backbone's features for the hidden task degrade significantly after catastrophic forgetting, while non-target task features remain relatively intact.
- Evidence anchors:
  - [abstract]: "We propose a novel adversarial attack method that leverages knowledge from non-target tasks and the shared backbone network... to force the model to forget knowledge related to the target task."
  - [section 4.1]: "Catastrophic forgetting refers to the phenomenon where the knowledge of a task is lost after training a model with a different task... we propose the Catastrophic Forgetting (CF) attack..."
  - [corpus]: Weak anchor. Related papers focus on general multi-task adversarial attacks, not specifically on catastrophic forgetting exploitation.
- Break condition: If the hidden task's features are robust to catastrophic forgetting (e.g., if the task is highly dissimilar to non-target tasks), the attack loses effectiveness.

### Mechanism 2
- Claim: The CF delta attack extends CF by not only mimicking post-forgetting features but also amplifying the direction of change, strengthening the attack on weakly-forgetting tasks.
- Mechanism: The difference vector Î” between pre- and post-forgetting feature maps is computed. Adversarial examples are generated to reproduce both the post-forgetting features and push them further along Î”, enhancing degradation of the hidden task.
- Core assumption: Even tasks resistant to catastrophic forgetting undergo directional shifts in feature space during fine-tuning that can be exploited.
- Evidence anchors:
  - [section 4.2]: "we propose an attack called CF delta attack, which focuses not only on replicating the feature vectors after forgetting but also on the change in the direction of feature vectors before and after forgetting."
  - [section 6.1]: "The CF delta attack significantly changes the feature vectors compared to the CF attack... we expect to maintain stealthiness by adding a penalty term..."
  - [corpus]: No direct evidence; this is a novel contribution not well-covered in related work.
- Break condition: If the direction of feature change (Î”) is negligible or orthogonal to the hidden task's decision boundary, amplification has little effect.

### Mechanism 3
- Claim: Stealthiness is achieved by penalizing adversarial perturbations that degrade non-target task performance, ensuring selective degradation.
- Mechanism: A regularization term â„’ğ‘‡ \ğ‘¡tgt (H, ğµ, ğ’™, ğ’š) is added to the loss function during adversarial example generation, constraining the perturbations to preserve non-target task accuracy.
- Core assumption: Adversarial perturbations that strongly degrade non-target tasks can be identified and penalized without compromising the hidden task attack.
- Evidence anchors:
  - [section 4.2]: "To further ensure stealthiness, we optimize ğœ¹ by adding a penalty term to maintain the performance of the non-target tasks."
  - [section 5.2]: "Our proposal achieves not only good attack performance compared with the existing attack but also good stealthiness."
  - [corpus]: Weak anchor. Related work does not explicitly address stealthiness via regularization in hidden-task attacks.
- Break condition: If the non-target tasks are highly correlated with the hidden task, penalizing their degradation may inadvertently weaken the attack on the hidden task.

## Foundational Learning

- Concept: Multi-task learning with shared backbone and task-specific heads.
  - Why needed here: The attack relies on understanding how tasks share representations in the backbone and how fine-tuning affects them differently.
  - Quick check question: What happens to task-specific feature representations in the backbone when only some tasks are fine-tuned?

- Concept: Catastrophic forgetting in neural networks.
  - Why needed here: The core mechanism exploits this phenomenon to degrade hidden task performance without explicit access to it.
  - Quick check question: After fine-tuning on tasks A and B, how does performance typically change on task C if C was not included in fine-tuning?

- Concept: Adversarial example generation via gradient-based optimization.
  - Why needed here: The attack uses optimization to find perturbations that match post-forgetting feature vectors while preserving non-target task accuracy.
  - Quick check question: How does adding a regularization term to the adversarial loss affect the perturbation search process?

## Architecture Onboarding

- Component map:
  - Backbone network ğµ: Shared feature extractor (e.g., ResNet-18)
  - Task headers ğ»ğ‘–: One linear (or nonlinear) layer per task
  - Fine-tuning module: Retrains ğµ on non-target tasks only
  - Adversarial generator: Optimizes perturbations to mimic post-forgetting features

- Critical path:
  1. Load pre-trained multi-task model
  2. Fine-tune backbone on non-target tasks â†’ ğµâ€²
  3. For each input, compute Î” = ğµâ€² (ğ’™) âˆ’ ğµ(ğ’™)
  4. Optimize perturbation ğœ¹ to minimize âˆ¥ğµ(ğ’™ + ğœ¹) âˆ’ (ğµâ€² (ğ’™) + Î²Î”)âˆ¥â‚‚ + Î³â„’ğ‘‡ \ğ‘¡tgt
  5. Apply perturbation to generate adversarial example

- Design tradeoffs:
  - Î² controls attack strength vs. stealthiness on non-target tasks
  - Î³ controls regularization strength vs. hidden task degradation
  - Choice of fine-tuning epochs affects degree of catastrophic forgetting

- Failure signatures:
  - Non-target task accuracy drops significantly â†’ Î² too high or Î³ too low
  - Hidden task accuracy remains high â†’ insufficient forgetting or Î² too low
  - Generated perturbations too large â†’ Îµ constraint too tight or optimization failing

- First 3 experiments:
  1. Verify catastrophic forgetting: Fine-tune on non-target tasks, measure hidden task degradation
  2. Baseline attack: Generate perturbations mimicking only ğµâ€² (ğ’™), evaluate hidden vs. non-target task performance
  3. Hyperparameter sweep: Vary Î² and Î³, plot hidden task degradation vs. non-target task preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the CF delta attack be improved to maintain stealthiness while further degrading the accuracy of the target task, particularly for tasks less prone to catastrophic forgetting?
- Basis in paper: [explicit] The paper discusses that the CF delta attack focuses on both replicating feature vectors after catastrophic forgetting and the change in direction of feature vectors before and after forgetting. It mentions that this may lead to a greater impact on tasks other than the target task, thereby compromising stealthiness.
- Why unresolved: The paper only briefly mentions the need for a penalty term to preserve the performance of non-target tasks but does not explore this in depth or provide specific solutions for improving the balance between attack effectiveness and stealthiness.
- What evidence would resolve it: Experimental results showing the CF delta attack with different penalty term configurations, demonstrating improved attack performance on the target task while maintaining or improving stealthiness on non-target tasks.

### Open Question 2
- Question: Can defenses designed for non-hidden tasks be effective against attacks specifically designed for hidden target tasks, as discussed in this study?
- Basis in paper: [explicit] The paper mentions that defenses against non-hidden tasks exist but does not explore whether these defenses can be effective against attacks designed for hidden target tasks.
- Why unresolved: The paper focuses on developing attacks against hidden tasks and does not investigate potential defenses or their effectiveness against these new types of attacks.
- What evidence would resolve it: Experimental results showing the effectiveness of existing defenses against non-hidden tasks when applied to the proposed CF and CF delta attacks on hidden tasks.

### Open Question 3
- Question: How do the proposed CF and CF delta attacks perform against more complex multi-task learning architectures, such as those with deeper networks or attention mechanisms?
- Basis in paper: [inferred] The paper evaluates the attacks on relatively simple multi-task architectures with ResNet-18 backbones and linear or non-linear task headers. It does not explore the attacks' effectiveness against more complex architectures.
- Why unresolved: The paper's experimental results are limited to a specific type of multi-task architecture, and it is unclear how the attacks would perform against more complex or diverse architectures commonly used in practice.
- What evidence would resolve it: Experimental results showing the performance of the CF and CF delta attacks against various multi-task architectures, including deeper networks, attention mechanisms, or other advanced techniques.

## Limitations
- The paper does not specify exact preprocessing pipeline for CelebA and DeepFashion datasets, affecting reproducibility
- Hyperparameter selection process for CF delta attack is underspecified, particularly Î² and Î³ tuning
- Attack assumes attacker has access to 10% of data for fine-tuning, which may not be realistic in all scenarios

## Confidence
- **High confidence**: Core claim that catastrophic forgetting can be exploited to attack hidden tasks, well-supported by experimental results
- **Medium confidence**: CF delta attack's effectiveness, results show improvements but mechanism less directly validated
- **Low confidence**: Stealthiness claims, only single metric provided without exploring other potential side effects

## Next Checks
1. Reproduce catastrophic forgetting: Fine-tune the backbone on non-target tasks and measure hidden task performance degradation across multiple runs to establish baseline effectiveness
2. Hyperparameter sensitivity analysis: Systematically vary Î² and Î³ in the CF delta attack to quantify their impact on both attack strength and stealthiness, including different non-target task accuracy degradation thresholds
3. Cross-dataset validation: Apply the attack to additional multi-task learning datasets (e.g., NYU Depth V2) to test generalizability beyond CelebA and DeepFashion