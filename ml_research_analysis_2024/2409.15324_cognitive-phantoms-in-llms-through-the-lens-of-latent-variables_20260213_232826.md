---
ver: rpa2
title: Cognitive phantoms in LLMs through the lens of latent variables
arxiv_id: '2409.15324'
source_url: https://arxiv.org/abs/2409.15324
tags:
- latent
- llms
- human
- responses
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether personality questionnaires validated
  for humans can validly measure similar constructs in LLMs. The authors administered
  two validated personality questionnaires (HEXACO-60 and DSHS) to human participants
  and three GPT models (GPT-3.5-T, GPT-4, and GPT-4-T), collecting 401 responses for
  each.
---

# Cognitive phantoms in LLMs through the lens of latent variables

## Quick Facts
- arXiv ID: 2409.15324
- Source URL: https://arxiv.org/abs/2409.15324
- Authors: Sanne Peereboom; Inga Schwabe; Bennett Kleinberg
- Reference count: 32
- Key outcome: Existing psychometric questionnaires do not validly measure personality constructs in LLMs, suggesting such traits may not exist in LLMs

## Executive Summary
This study investigates whether validated personality questionnaires can measure similar constructs in LLMs as they do in humans. The authors administered the HEXACO-60 and DSHS questionnaires to human participants and three GPT models (GPT-3.5-T, GPT-4, and GPT-4-T), collecting 401 responses for each. Using confirmatory and exploratory factor analyses, they compared latent structures between humans and LLMs. The analyses revealed that LLM responses did not exhibit meaningful latent structures, with factor analyses often failing to converge or producing arbitrary factors. Composite score analysis showed LLMs scoring higher on socially desirable traits and lower on undesirable traits compared to humans, but these scores were not underpinned by valid latent constructs.

## Method Summary
The study collected responses from 365 human participants recruited via Prolific and three GPT models (399 GPT-3.5-T, 387 GPT-4, 401 GPT-4-T responses). Two validated personality questionnaires were administered: HEXACO-60 (60 items) and Dark Side of Humanity Scale (DSHS, 42 items). LLM responses were generated using OpenAI API with temperature sampling ranging from 0.1 to 1.0. The authors performed confirmatory and exploratory factor analyses on both questionnaires to evaluate latent structures, comparing results between human and LLM samples. They also examined composite scores across groups to assess whether LLMs exhibited meaningful personality trait patterns.

## Key Results
- Factor analyses on LLM responses failed to replicate theoretical latent structures from human data, often not converging or producing arbitrary factors
- LLM composite scores showed higher socially desirable and lower undesirable trait ratings compared to humans, but without valid latent constructs
- GPT-3.5-T scores on the DSHS dimensions showed moderate positive correlations with the Honesty-Humility dimension, opposite to expected negative correlations
- Temperature variation (0.1-1.0) did not restore meaningful latent structure in LLM responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Psychometric questionnaires designed for humans do not validly measure personality constructs in LLMs.
- Mechanism: Factor analysis on LLM responses fails to replicate the theoretical latent structure that exists in human data, indicating absence of equivalent latent traits.
- Core assumption: An LLM can be treated analogously to a population distribution for factor analysis purposes.
- Evidence anchors:
  - [abstract] "questionnaires designed for humans do not validly measure similar constructs in LLMs"
  - [section] "factor analyses often failing to converge or producing arbitrary factors"
  - [corpus] Found 25 related papers; none cited this specific mechanism, suggesting it is novel.
- Break condition: If LLMs were instead analogous to individual samples, the factor analysis would not be valid even if latent traits existed.

### Mechanism 2
- Claim: Composite scores from questionnaires can be misleading when applied to LLM responses.
- Mechanism: LLM responses generate consistent numerical outputs across questionnaire items, but these outputs do not stem from meaningful latent dimensions, leading to spurious correlations and invalid construct interpretations.
- Core assumption: The correlation between dark personality traits and Honesty-Humility should be negative in valid measurements.
- Evidence anchors:
  - [abstract] "Composite score analysis showed LLMs scoring higher on socially desirable traits... but these scores were not underpinned by valid latent constructs"
  - [section] "GPT-3.5-T scores on the DSHS dimensions show moderate positive correlations with the Humility-Honesty dimension... the opposite direction of what one would expect"
  - [corpus] Limited corpus support for this specific claim; the novelty suggests the mechanism is not yet widely recognized.
- Break condition: If latent structures were valid in LLMs, composite scores would correlate meaningfully with theoretical expectations.

### Mechanism 3
- Claim: Temperature sampling in LLM prompts does not restore meaningful latent structure.
- Mechanism: Varying temperature during prompt generation introduces variability in responses, but EFA still yields arbitrary factor patterns, ruling out deterministic output artifacts as the sole cause.
- Core assumption: Higher temperature values should increase response variability without fundamentally altering latent factor structures if they exist.
- Evidence anchors:
  - [section] "we collected 401 additional responses... for each temperature value from 0.1 to 1.0... no evidence of a truly sensible factor structure"
  - [corpus] No direct corpus evidence; this is an experimental finding not widely reported.
- Break condition: If temperature variation had revealed stable latent factors, it would indicate a measurement artifact rather than absence of traits.

## Foundational Learning

- Concept: Confirmatory Factor Analysis (CFA)
  - Why needed here: To test whether the theoretical latent structure from human data replicates in LLM samples.
  - Quick check question: What does it mean if CFA fails to converge or produces factor correlations >1 in LLM data?

- Concept: Exploratory Factor Analysis (EFA)
  - Why needed here: To identify potential alternative latent structures when CFA fails, and to compare patterns between humans and LLMs.
  - Quick check question: How do you decide the number of factors to extract in EFA?

- Concept: Measurement Validity
  - Why needed here: To determine whether questionnaires measure the intended latent constructs in LLMs, not just produce numerical outputs.
  - Quick check question: Why is it problematic to interpret composite scores without establishing validity?

## Architecture Onboarding

- Component map: Data collection (human + LLM prompts) → Preprocessing (reverse coding) → Assumption checks (linearity, normality, factorability) → CFA/EFA → Interpretation → Composite score analysis
- Critical path: Prompt generation → Response collection → Assumption validation → Factor analysis → Validity assessment
- Design tradeoffs: Using temperature sampling increases response variability but may obscure any existing latent structure; restricting temperature risks deterministic artifacts
- Failure signatures: CFA non-convergence, factor correlations >1, arbitrary EFA loadings, lack of inter-item covariance, inconsistent inter-factor correlations
- First 3 experiments:
  1. Collect LLM responses with fixed temperature (0) and run CFA to check for deterministic artifacts
  2. Run EFA on human data only to confirm theoretical structure before comparing with LLM data
  3. Test different questionnaires (e.g., Big Five) to see if arbitrary factor patterns generalize across constructs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What underlying cognitive mechanisms, if any, generate responses in LLMs that superficially resemble human personality traits?
- Basis in paper: [explicit] The paper concludes that existing questionnaires do not validly measure similar constructs in LLMs and that these constructs may not exist in LLMs at all, suggesting the need to understand the actual cognitive mechanisms at play.
- Why unresolved: The study found arbitrary factor structures in LLM responses, indicating that responses are not driven by coherent latent traits. The paper explicitly states that the existence of latent phenomena and their causal effects on measurement outcomes cannot be proven due to their unobservable nature.
- What evidence would resolve it: Evidence could come from studies using alternative measurement approaches such as item response theory (IRT) models, or from research that investigates the relationship between training data patterns and LLM responses to personality questionnaires.

### Open Question 2
- Question: How does temperature setting in LLMs affect the latent structure of their responses to psychometric questionnaires?
- Basis in paper: [explicit] The paper notes that temperature controls how deterministic responses are and that it has been shown to affect average scores on latent constructs, though potential effects on underlying factor structure are unknown.
- Why unresolved: The study found arbitrary factor structures regardless of temperature settings tested, but did not systematically investigate the relationship between temperature and factor structure. The authors acknowledge this as an area needing further investigation.
- What evidence would resolve it: A systematic study varying temperature across a wide range and examining how this affects both composite scores and latent factor structures would provide clarity on this relationship.

### Open Question 3
- Question: What is the appropriate theoretical framework for conceptualizing LLMs in psychometric research - as analogous to populations, individuals, or something entirely different?
- Basis in paper: [explicit] The paper discusses this measurement unit problem, noting that LLMs could be seen as analogous to populations or individuals, and that this distinction has far-reaching implications for validation studies.
- Why unresolved: The study assumes LLMs are analogous to populations for their analysis, but acknowledges this assumption may not hold and that the nature of the analogy between LLMs and humans remains a complex question requiring further investigation.
- What evidence would resolve it: Comparative studies examining between- versus within-response variances for various questionnaires and constructs, or theoretical work developing new frameworks for understanding LLM cognition, would help resolve this question.

## Limitations
- The assumption that LLMs can be treated as population distributions for factor analysis may not hold, potentially invalidating the methodology regardless of whether latent traits exist
- The study focuses on specific questionnaires (HEXACO-60 and DSHS), leaving open the possibility that other validated instruments might yield different results
- The temperature sampling experiments, while informative, do not systematically investigate the relationship between temperature and factor structure

## Confidence

| Claim | Confidence |
|-------|------------|
| LLM responses lack meaningful latent structures | High |
| Psychometric questionnaires do not validly measure personality constructs in LLMs | Medium |
| LLMs lack personality traits | Medium |

## Next Checks
1. **Alternative Questionnaire Testing**: Administer different validated personality instruments (e.g., Big Five Inventory, MBTI) to determine if arbitrary factor patterns generalize across multiple construct domains or are specific to HEXACO and DSHS.

2. **Human-LLM Hybrid Analysis**: Conduct joint factor analysis incorporating both human and LLM responses to test whether the latent structure emerges when groups are analyzed together, potentially revealing measurement artifacts versus genuine construct differences.

3. **Latent Trait Simulation**: Generate synthetic LLM responses based on known latent structures to establish whether factor analysis can detect valid traits when they are deliberately embedded, validating the methodology's sensitivity to detect latent constructs.