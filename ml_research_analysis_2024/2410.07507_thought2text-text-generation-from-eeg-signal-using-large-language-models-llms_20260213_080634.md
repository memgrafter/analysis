---
ver: rpa2
title: 'Thought2Text: Text Generation from EEG Signal using Large Language Models
  (LLMs)'
arxiv_id: '2410.07507'
source_url: https://arxiv.org/abs/2410.07507
tags:
- text
- embeddings
- language
- signals
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Thought2Text, a novel approach that leverages
  large language models (LLMs) to decode brain activity, specifically EEG signals,
  into comprehensible text. The method involves three stages: (1) training an EEG
  encoder to extract visual features from multi-channel EEG signals, (2) fine-tuning
  LLMs using image and text data to enable multimodal description generation, and
  (3) further fine-tuning the LLMs on EEG embeddings to generate text directly from
  EEG signals during inference.'
---

# Thought2Text: Text Generation from EEG Signal using Large Language Models (LLMs)

## Quick Facts
- arXiv ID: 2410.07507
- Source URL: https://arxiv.org/abs/2410.07507
- Reference count: 23
- Authors: Abhijit Mishra; Shreya Shukla; Jose Torres; Jacek Gwizdka; Shounak Roychowdhury
- Primary result: Novel three-stage approach using LLMs to decode EEG signals into text, outperforming chance-level baselines on standard NLG metrics

## Executive Summary
This paper introduces Thought2Text, a novel approach that leverages large language models (LLMs) to decode brain activity, specifically EEG signals, into comprehensible text. The method involves three stages: (1) training an EEG encoder to extract visual features from multi-channel EEG signals, (2) fine-tuning LLMs using image and text data to enable multimodal description generation, and (3) further fine-tuning the LLMs on EEG embeddings to generate text directly from EEG signals during inference. The approach is validated using a public EEG dataset of six subjects viewing visual stimuli, with image descriptions generated by GPT-4 and validated by human annotators. The results show that Thought2Text outperforms chance-level baselines across standard NLG metrics (e.g., BLEU, ROUGE, BERTScore) and GPT-4-based fluency and adequacy assessments.

## Method Summary
Thought2Text employs a three-stage training process to decode EEG signals into text. First, an EEG encoder (ChannelNet) is trained to extract visual features from multi-channel EEG signals while predicting object labels. Second, LLMs are fine-tuned using image embeddings and corresponding text descriptions. Third, the projector module is further tuned on EEG embeddings while keeping the LLM frozen, enabling direct text generation from EEG during inference. The approach uses standard NLG metrics (BLEU, METEOR, ROUGE, BERTScore) and GPT-4-based fluency and adequacy assessments for evaluation.

## Key Results
- Outperforms chance-level baselines across BLEU, ROUGE, BERTScore, and GPT-4-based assessments
- Stage-wise fine-tuning (image first, then EEG) improves generation quality over direct EEG tuning
- Object label inclusion in prompts stabilizes generation by grounding LLM output

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning EEG embeddings with CLIP image embeddings through MSE loss enables multimodal LLMs to process neural signals
- Mechanism: The EEG encoder learns to project noisy multichannel EEG signals into a high-dimensional space that mirrors the semantic structure of CLIP's image embeddings, allowing pretrained LLMs to interpret brain activity via the shared embedding space
- Core assumption: EEG signals contain sufficient spatial and temporal patterns to reconstruct image-level semantics when processed through a deep CNN encoder
- Evidence anchors: [abstract] "training an EEG encoder for visual feature extraction" and "fine-tuning LLMs on EEG embeddings to generate text directly from EEG during inference"; [section 4.1] "a mean squared error (MSE) between the EEG embeddings (Heeg) and pooled image embeddings (Hclip) from a pretrained CLIP model"; [corpus] Weak. Related works focus on EEG-to-image or EEG-to-text via direct decoding; none explicitly describe this two-stage CLIP-based alignment approach
- Break condition: If EEG signals are too noisy or non-stationary across subjects, the MSE alignment will fail to generalize, causing the projector to map embeddings into unrelated token spaces

### Mechanism 2
- Claim: Fine-tuning LLMs in two stages (image first, then EEG) improves generation quality over direct EEG tuning
- Mechanism: Stage 2 primes the LLM and projector on clean image-text pairs, establishing multimodal fluency. Stage 3 then adapts the projector to EEG embeddings while freezing the LLM, reducing catastrophic forgetting and preserving language generation capabilities
- Core assumption: Multimodal projector weights learned from image embeddings transfer effectively to EEG embeddings with minimal retraining
- Evidence anchors: [abstract] "fine-tuning LLMs on image and text data, enabling multimodal description generation" followed by "further fine-tuning on EEG embeddings"; [section 4.3] "During this stage, the projector parameters Wmm and bmm are further tuned. We would like to highlight that throughout this and the previous stage, only the projector is trained, while the LLM and EEG encoders remain frozen"; [corpus] No direct evidence; this staged approach is novel relative to concurrent EEG2TEXT and related work
- Break condition: If projector capacity is insufficient to bridge the gap between image and EEG embedding distributions, Stage 3 tuning will produce poor alignment and degraded text quality

### Mechanism 3
- Claim: Including object labels in the prompt stabilizes EEG-to-text generation by grounding the LLM in the most salient image feature
- Mechanism: Predicted object labels act as a semantic anchor, constraining the LLM's output space and reducing hallucinations when EEG embeddings are ambiguous or noisy
- Core assumption: Object labels derived from EEG are accurate enough to meaningfully guide text generation, even when EEG-only decoding would fail
- Evidence anchors: [abstract] "These responses are compared with gold standard image descriptions to compute the training loss"; [section 4.1] "The model is trained by minimizing two losses: (A) a categorical cross-entropy loss (CE) between the predicted and ground-truth object labels..."; [corpus] Weak. Most related papers skip explicit grounding; this paper's ablation shows object labels improve adequacy scores
- Break condition: If object classification accuracy drops below ~50%, the grounding signal becomes unreliable and may mislead the LLM

## Foundational Learning

- Concept: Multimodal embedding alignment (vision-language models)
  - Why needed here: Enables EEG signals, which are inherently non-text, to be processed by text-based LLMs via a shared vector space
  - Quick check question: What is the dimensionality of CLIP's image embeddings and how does it match the EEG encoder output?

- Concept: Cross-entropy loss for sequence generation
  - Why needed here: Standard training objective for autoregressive LLMs to predict next token given multimodal input
  - Quick check question: How does the right-shift of ground-truth tokens during training implement teacher forcing?

- Concept: Teacher forcing in sequence modeling
  - Why needed here: Stabilizes training by providing correct previous tokens as input, preventing error accumulation in early epochs
  - Quick check question: Why is the projector frozen during LLM fine-tuning but not vice versa?

## Architecture Onboarding

- Component map: EEG ChannelNet encoder → 512-dim embeddings → Projector (FFN) → token embedding space → LLM (chat template) → text generation; MLP classifier → object label prediction

- Critical path: EEG → ChannelNet → projector → LLM prompt → generated text
  - Bottleneck: Projector must learn robust mapping from noisy EEG to clean image embedding space

- Design tradeoffs:
  - Freezing LLM vs. fine-tuning: Freezing avoids instability from noisy EEG gradients but limits adaptation to EEG-specific language patterns
  - Object label inclusion: Improves grounding but requires reliable classification; omission increases hallucination risk

- Failure signatures:
  - Low BLEU/ROUGE but high fluency → Projector misalignment but LLM still coherent
  - Low adequacy scores → Object label errors or EEG embeddings failing to capture salient features
  - High variance across subjects → EEG encoder overfitting to individual noise patterns

- First 3 experiments:
  1. Train EEG encoder with MSE+CE loss; verify alignment by computing cosine similarity between Heeg and Hclip on validation set
  2. Stage 2: Fine-tune LLM with image embeddings only; measure perplexity drop on held-out image captions
  3. Stage 3: Fine-tune projector on EEG embeddings; evaluate text generation quality vs. baselines (ONLY_OBJ, ONLY_EEG, NO_STAGE2)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would Thought2Text perform on EEG datasets with more diverse and naturalistic stimuli, such as videos or longer viewing sessions?
- Basis in paper: [inferred] The paper uses a dataset with static images and short 0.5-second EEG segments, noting the controlled experimental conditions. It mentions potential applications like AR/VR enhancement but does not test with dynamic stimuli
- Why unresolved: The current dataset and methodology are limited to short, static visual stimuli, which may not generalize to more complex or naturalistic brain activity patterns
- What evidence would resolve it: Testing Thought2Text on EEG datasets with videos or longer, dynamic stimuli, and comparing performance metrics to the current results

### Open Question 2
- Question: What is the impact of using personalized EEG encoders versus cross-subject training for Thought2Text, and how does this affect model generalizability?
- Basis in paper: [explicit] The paper discusses the need for privacy-preserving, personalized solutions due to the sensitive nature of EEG data and mentions subject-wise analysis but does not explore cross-subject training or model generalization
- Why unresolved: The study focuses on subject-specific training but does not evaluate how well the model generalizes across subjects or whether cross-subject training could improve performance
- What evidence would resolve it: Comparative experiments using cross-subject training versus subject-specific training, measuring performance and generalizability across multiple subjects

### Open Question 3
- Question: How does the choice of frequency range (e.g., 55-95 Hz vs. 5-95 Hz) affect the performance of Thought2Text, and are there optimal frequency bands for different types of visual stimuli?
- Basis in paper: [explicit] The paper selects the 55-95 Hz frequency range based on prior research (Palazzo et al., 2020) but does not explore the impact of other frequency ranges on performance
- Why unresolved: The study assumes the 55-95 Hz range is optimal without testing alternative frequency bands or their effects on different types of stimuli
- What evidence would resolve it: Experiments testing Thought2Text with multiple frequency ranges (e.g., 5-95 Hz, 14-70 Hz) and analyzing performance differences for various visual stimuli categories

## Limitations
- Limited dataset with only six subjects and 50 stimuli, raising questions about generalization
- Reliance on GPT-4 for caption generation may introduce distribution mismatch with real EEG-derived descriptions
- 55-95Hz frequency range selection may not capture full spectrum of relevant neural signatures for complex imagery

## Confidence

**High confidence**: The staged fine-tuning approach and multimodal projector design are technically sound and well-supported by the architecture description. The use of standard NLG metrics and GPT-4-based human assessments provides reasonable evaluation frameworks.

**Medium confidence**: The claim that EEG embeddings align with CLIP image embeddings is supported by the MSE loss formulation, but actual alignment quality (e.g., cosine similarity scores) is not reported. The effectiveness of object label grounding is demonstrated through ablation studies, but the robustness of classification across subjects is unclear.

**Low confidence**: Generalization claims to broader contexts are weakly supported given the limited dataset size and controlled stimulus conditions. The assertion that this approach enables "portable, low-cost" technology overlooks the practical challenges of EEG data collection outside laboratory settings.

## Next Checks

1. **Cross-subject generalization test**: Evaluate Thought2Text performance when training on 5 subjects and testing on the held-out 6th subject, measuring BLEU/ROUGE degradation to quantify subject-specific bias

2. **Embedding alignment quantification**: Compute and report cosine similarity distributions between EEG encoder outputs and CLIP embeddings on a validation set, comparing pre- and post-alignment to verify the MSE loss objective is achieving semantic alignment

3. **Frequency band ablation study**: Systematically evaluate Thought2Text performance across the three frequency ranges (14-70Hz, 5-95Hz, 55-95Hz) to determine whether the selected band provides optimal information for text generation or if other ranges could yield better results