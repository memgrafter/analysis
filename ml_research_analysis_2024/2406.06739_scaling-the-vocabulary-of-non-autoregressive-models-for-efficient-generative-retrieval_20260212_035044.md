---
ver: rpa2
title: Scaling the Vocabulary of Non-autoregressive Models for Efficient Generative
  Retrieval
arxiv_id: '2406.06739'
source_url: https://arxiv.org/abs/2406.06739
tags:
- pixar
- vocabulary
- retrieval
- tokens
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes expanding the target vocabulary of non-autoregressive
  (NAR) generative retrieval models to include multi-word entities and phrases (up
  to 5 million tokens), addressing the limitations of standard NAR models in capturing
  dependencies between target tokens. The proposed PIXAR model leverages a novel inference
  optimization strategy that efficiently selects relevant tokens from the expanded
  vocabulary, maintaining low inference latency.
---

# Scaling the Vocabulary of Non-autoregressive Models for Efficient Generative Retrieval

## Quick Facts
- **arXiv ID**: 2406.06739
- **Source URL**: https://arxiv.org/abs/2406.06739
- **Reference count**: 40
- **Primary result**: Proposes PIXAR, expanding NAR generative retrieval vocabulary to 5M tokens (multi-word phrases), achieving 31.0% relative MRR@10 improvement on MS MARCO and 23.2% Hits@5 gain on Natural Questions, with 5.08% ad click increase in online A/B tests.

## Executive Summary
This paper addresses the limitations of non-autoregressive (NAR) generative retrieval models by expanding their target vocabulary to include multi-word entities and phrases, up to 5 million tokens. The proposed PIXAR model leverages a novel inference optimization strategy—using shortlist embeddings and cluster-based token selection—to efficiently handle the large vocabulary while maintaining low inference latency. Experiments on MS MARCO and Natural Questions datasets demonstrate significant performance gains over standard NAR models, and online A/B tests on a commercial search engine show improved ad clicks and revenue.

## Method Summary
PIXAR expands the target vocabulary of NAR generative retrieval models to include multi-word phrases (up to 5 million tokens) using a phrase-based tokenizer. It employs a shortlist embedding mechanism to efficiently select relevant tokens from the large vocabulary during inference, using cluster vectors and per-position re-ranking. The model is trained with a combination of cross-entropy, shortlist loss, and self-normalization loss, and uses constrained beam search for document identifier generation. PIXAR is evaluated on MS MARCO and Natural Questions datasets, with comparisons to baselines like CLOVERv2.

## Key Results
- 31.0% relative improvement in MRR@10 on MS MARCO compared to CLOVERv2 with similar latency.
- 23.2% increase in Hits@5 on Natural Questions over CLOVERv2.
- 5.08% increase in ad clicks and 4.02% boost in revenue in online A/B tests on a commercial search engine.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanding the target vocabulary to include multi-word phrases reduces the number of independent predictions required in NAR models, thereby improving retrieval performance.
- Mechanism: By representing common phrases as single tokens, PIXAR reduces the output sequence length compared to subword or word-level tokenizers. This simplification means each token prediction carries more semantic content, mitigating the loss of inter-token dependency modeling inherent in NAR architectures.
- Core assumption: Multi-word phrases in the target vocabulary co-occur frequently enough in the document identifiers to justify their inclusion and enable effective learning of their embeddings.
- Evidence anchors:
  - [abstract]: "We propose PIXAR, a novel approach that expands the target vocabulary of NAR models to include multi-word entities and common phrases (up to 5 million tokens), thereby reducing token dependencies."
  - [section]: "While generating phrases at output instead of solely words leads to shorter output sequences and helps latency, as the vocabulary size grows, predicting the most likely tokens at each of these output positions becomes computationally far more demanding leading to much higher overall latency."
  - [corpus]: Weak. The related papers focus on NAR improvements in different domains (speech, reasoning, TSP) but do not directly validate phrase-based vocabulary scaling in retrieval contexts.
- Break condition: If the document identifier corpus lacks sufficient phrase co-occurrence frequency, the expanded vocabulary will contain many low-frequency or noisy tokens, degrading model performance and increasing computational waste.

### Mechanism 2
- Claim: The shortlist embedding x0(q) combined with learnable cluster vectors {c1,...,cm} efficiently narrows down the relevant tokens from a large vocabulary during inference.
- Mechanism: For a given query, the model computes the inner product between the shortlist embedding and each cluster vector, selects the top k clusters, and forms a shortlist W0(q) as their union. This shortlist is then re-ranked per output position using the corresponding token embeddings, drastically reducing the number of softmax operations needed compared to scoring the full vocabulary.
- Core assumption: The shortlist embedding captures query semantics well enough that the top k cluster vectors reliably cover the tokens needed for correct document identifier generation.
- Evidence anchors:
  - [abstract]: "PIXAR employs inference optimization strategies to maintain low inference latency despite the significantly larger vocabulary."
  - [section]: "By choosing hyper-parameters appropriately, we can ensure that rk ≪ |V |."
  - [corpus]: Weak. No corpus evidence directly confirms that this shortlist-based approach consistently maintains accuracy across different retrieval tasks.
- Break condition: If the shortlist embedding poorly represents query semantics, or if k is too small, the shortlist may miss critical tokens, leading to retrieval errors despite low latency.

### Mechanism 3
- Claim: The combination of a self-normalization loss term and shortlist re-ranking ensures computational efficiency while maintaining retrieval quality in large-vocabulary NAR models.
- Mechanism: The self-normalization loss ℓ3(¯θ) allows the model to compute softmax scores using only the numerator exp(xt(qi)T wv), avoiding the expensive denominator sum over the full vocabulary. This, combined with shortlist re-ranking, ensures that even with 5 million tokens, inference remains fast and effective.
- Core assumption: The self-normalized scores approximate the true softmax distribution well enough for the shortlist re-ranking to produce accurate top-k token selections.
- Evidence anchors:
  - [abstract]: "While a positive answer to the above question will provide an approach to get high quality retrieval from NAR-based GR, it also comes at a cost to the inference latency."
  - [section]: "For large target vocabularies (e.g., our expanded vocabulary with phrase tokens), these estimates are much faster to compute since the sum in the denominator over the entire target vocabulary is avoided."
  - [corpus]: Weak. No corpus evidence directly confirms the quality of self-normalized softmax approximations in retrieval tasks.
- Break condition: If the self-normalized scores deviate significantly from true softmax probabilities, the shortlist re-ranking will mis-rank tokens, harming retrieval accuracy despite computational gains.

## Foundational Learning

- **Concept**: Non-autoregressive (NAR) language models and their limitations in capturing token dependencies.
  - Why needed here: PIXAR builds on NAR models but must overcome their key weakness—lack of inter-token dependency modeling—by expanding the vocabulary with phrases.
  - Quick check question: Why do NAR models typically underperform autoregressive models in sequence generation tasks?

- **Concept**: Vocabulary construction methods, especially subword tokenization and phrase-based tokenization.
  - Why needed here: PIXAR’s performance hinges on constructing an efficient phrase-based tokenizer that captures multi-word entities and respects word boundaries.
  - Quick check question: How does a phrase-based tokenizer differ from BPE or Unigram in handling multi-word entities?

- **Concept**: Inference optimization in large-vocabulary settings, including softmax approximation and shortlist-based re-ranking.
  - Why needed here: PIXAR must scale to 5 million tokens while keeping latency low; this requires efficient shortlist construction and self-normalized softmax estimation.
  - Quick check question: What is the computational bottleneck when scaling vocabulary size in NAR models, and how does shortlist re-ranking address it?

## Architecture Onboarding

- **Component map**: Transformer encoder (DeBERTa-v3-base) → Shortlist embedding x0(q) → Cluster vector selection → Shortlist W0(q) → Token embeddings {x1,...,xs(q)} → Per-position re-ranking → Constrained beam search → Document identifier generation.
- **Critical path**: Query → Encoder → Shortlist embedding → Cluster selection → Shortlist construction → Per-position token re-ranking → Beam search → Output docid.
- **Design tradeoffs**:
  - Larger vocabulary → Better phrase coverage and fewer tokens per docid, but higher computational cost if not optimized.
  - Smaller shortlist (k, r, m) → Faster inference, but higher risk of missing relevant tokens.
  - More cluster vectors (m) → Better shortlist coverage, but increased memory and compute for shortlist selection.
- **Failure signatures**:
  - Retrieval accuracy drops but latency is low → shortlist too small or cluster vectors poorly aligned.
  - High latency despite large vocabulary → shortlist construction or re-ranking not optimized.
  - Model fails to learn phrase embeddings → insufficient phrase co-occurrence frequency in training data.
- **First 3 experiments**:
  1. Train PIXAR with a moderate vocabulary (e.g., 500K tokens) and baseline CLOVERv2 (128K) on MS MARCO; compare MRR@10 and inference latency to verify the retrieval gain and latency tradeoff.
  2. Vary k (number of clusters selected per query) and measure the impact on retrieval accuracy and latency; identify the sweet spot for shortlist size.
  3. Compare shortlist-based re-ranking vs. full softmax on a subset of queries; quantify the accuracy loss/gain and latency savings to validate the efficiency of the shortlist approach.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does PIXAR's performance scale with even larger vocabularies beyond 5 million tokens, and what is the theoretical limit of vocabulary size before diminishing returns or computational infeasibility occur?
  - Basis in paper: [explicit] The paper demonstrates performance improvements with increasing vocabulary sizes up to 5 million tokens, but does not explore beyond this limit.
  - Why unresolved: The paper focuses on the effectiveness of scaling up to 5 million tokens and does not investigate the impact of further vocabulary expansion.
  - What evidence would resolve it: Experiments comparing PIXAR's performance and computational efficiency with vocabularies of 10 million, 50 million, or even larger sizes, along with an analysis of the trade-offs between retrieval quality and computational cost.

- **Open Question 2**: How does PIXAR's performance compare to other state-of-the-art generative retrieval models, such as those using more complex document identifiers (e.g., combinations of titles, n-grams, and pseudo-queries) or multi-stage training procedures?
  - Basis in paper: [explicit] The paper mentions that PIXAR achieves competitive performance compared to models like MINDER and LTRGR but does not provide a direct comparison with all state-of-the-art models.
  - Why unresolved: The paper focuses on comparing PIXAR to specific baselines and does not exhaustively evaluate its performance against all existing generative retrieval models.
  - What evidence would resolve it: Comprehensive benchmarking of PIXAR against a wide range of state-of-the-art generative retrieval models on multiple datasets, including those using complex document identifiers and multi-stage training procedures.

- **Open Question 3**: How does PIXAR's performance generalize to other domains and tasks beyond text retrieval, such as image retrieval or cross-modal retrieval?
  - Basis in paper: [explicit] The paper evaluates PIXAR on text retrieval tasks using MS MARCO and Natural Questions datasets, but does not explore its applicability to other domains or tasks.
  - Why unresolved: The paper focuses on demonstrating PIXAR's effectiveness in text retrieval and does not investigate its potential for other applications.
  - What evidence would resolve it: Experiments evaluating PIXAR's performance on image retrieval tasks using datasets like MS COCO or Flickr30k, and cross-modal retrieval tasks using datasets like MS COCO or Flickr30k Entities.

## Limitations

- The shortlist-based inference optimization's effectiveness at 5M tokens is weakly supported by direct experimental evidence and corpus validation.
- The paper does not provide exact shortlist embedding implementation details or hyperparameter values, hindering faithful reproduction.
- The phrase-based vocabulary construction assumes sufficient phrase co-occurrence frequency in the document identifier corpus, which is not universally verified.

## Confidence

**High confidence**: The core motivation—that NAR models struggle with token dependencies and that phrase-based vocabularies can help—is well-grounded in prior literature and clearly articulated.

**Medium confidence**: The experimental results (MRR@10, Hits@5, latency, online A/B metrics) are specific and show strong gains, but the absence of ablation studies on shortlist size and vocabulary size, as well as the lack of direct evidence for shortlist and self-normalization quality, reduce confidence in the claimed mechanisms.

**Low confidence**: Claims about the exact impact of shortlist embedding and self-normalized softmax on both accuracy and efficiency are not well-supported by direct experimental evidence, especially for diverse retrieval tasks and very large vocabularies.

## Next Checks

1. **Ablation on shortlist parameters (k, m, r)**: Systematically vary the shortlist size and cluster count to quantify the tradeoff between retrieval accuracy and inference latency. This would validate whether the proposed shortlist mechanism truly balances efficiency and performance.

2. **Quality of self-normalized softmax approximation**: Compare retrieval accuracy and latency between self-normalized softmax and full softmax on a subset of queries. This would directly test whether the computational shortcut preserves retrieval quality.

3. **Phrase coverage and frequency analysis**: Analyze the frequency and co-occurrence statistics of the constructed phrase vocabulary on the target document identifier corpus. This would confirm whether the vocabulary expansion is justified and effective for the retrieval task.