---
ver: rpa2
title: Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation
  Engineering
arxiv_id: '2410.15999'
source_url: https://arxiv.org/abs/2410.15999
tags:
- knowledge
- activations
- spare
- llms
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses context-memory knowledge conflicts in large
  language models (LLMs), where parametric knowledge conflicts with contextual information,
  leading to reliance on incorrect information. The authors propose SPARE, a training-free
  representation engineering method that uses pre-trained sparse auto-encoders (SAEs)
  to control knowledge selection behaviors.
---

# Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering

## Quick Facts
- arXiv ID: 2410.15999
- Source URL: https://arxiv.org/abs/2410.15999
- Reference count: 40
- Primary result: +10% improvement over state-of-the-art representation engineering methods in controlling knowledge selection during conflicts

## Executive Summary
This paper addresses context-memory knowledge conflicts in LLMs, where parametric knowledge conflicts with contextual information, leading to incorrect information reliance. The authors propose SPARE, a training-free representation engineering method that uses pre-trained sparse auto-encoders (SAEs) to control knowledge selection behaviors. By identifying functional SAE features related to knowledge selection and applying them to edit internal LLM activations at inference time, SPARE effectively steers models to use either contextual or parametric knowledge when conflicts arise. Experimental results on open-domain question-answering tasks show significant improvements over state-of-the-art methods, achieving +10% improvement over representation engineering methods and +15% improvement over contrastive decoding methods.

## Method Summary
SPARE uses pre-trained sparse auto-encoders (SAEs) to identify functional features that control knowledge selection behaviors in LLMs. The method first detects knowledge conflict signals in the residual stream of mid-layers using linear probing, then identifies SAE activations correlated with choosing contextual vs parametric knowledge. At inference time, SPARE edits the hidden states by calculating z- (features to remove) and z+ (features to add) based on the input activation and functional features, allowing precise control over knowledge selection without affecting other model behaviors.

## Key Results
- SPARE achieves +10% improvement over state-of-the-art representation engineering methods in controlling knowledge selection
- The method shows +15% improvement over contrastive decoding methods for knowledge conflict resolution
- Effectiveness is concentrated in mid-layers where knowledge conflict signals are strongest
- SPARE successfully steers models to use either contextual or parametric knowledge when conflicts arise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge conflict signals can be detected in the residual stream of mid-layers in LLMs.
- Mechanism: The model's internal activations encode whether contextual knowledge conflicts with parametric knowledge, which can be identified through linear probing methods.
- Core assumption: The residual stream contains distinguishable patterns when knowledge conflicts occur, and these patterns emerge at specific layers.
- Evidence anchors:
  - [abstract] "Analysing the internal activations of LLMs, we find that they can internally register the signals of knowledge conflict at mid-layers."
  - [section 3] "We observe that the probing accuracy increases from the first layer to the middle layers, and this trend is the same across different types of activations."
  - [corpus] Weak - no corpus evidence provided for this specific mechanism.
- Break condition: If knowledge conflicts do not create distinguishable activation patterns in the residual stream, or if these patterns are not layer-specific.

### Mechanism 2
- Claim: Sparse Auto-encoders (SAEs) can identify functional features that control knowledge selection behaviors.
- Mechanism: SAEs decompose polysemantic activations into monosemantic features, allowing identification of specific activations that correlate with choosing contextual vs parametric knowledge.
- Core assumption: A small set of SAE activations can capture the functional information driving knowledge selection behavior, and these can be manipulated to control the model's behavior.
- Evidence anchors:
  - [abstract] "SPARE identifies the functional features that control the knowledge selection behaviours and applies them to edit the internal activations of LLMs at inference time."
  - [section 4.2] "A single SAE activation can capture one monosemantic feature... we hypothesise a combination of a small set of SAE activations can be responsible for a functional feature."
  - [corpus] Moderate - related papers on SAE-based representation steering exist but don't specifically address knowledge conflict resolution.
- Break condition: If SAEs cannot decompose activations into interpretable features, or if the identified features don't control knowledge selection as expected.

### Mechanism 3
- Claim: Editing SAE activations at inference time can steer knowledge selection without affecting other model behaviors.
- Mechanism: By calculating z- (features to remove) and z+ (features to add) based on the input activation and functional features, the method precisely edits the hidden states to encourage desired knowledge usage.
- Core assumption: The SAE-based editing approach allows precise control over specific behaviors without introducing unintended side effects or information loss.
- Evidence anchors:
  - [abstract] "SPARE identifies the functional features that control the knowledge selection behaviours and applies them to edit the internal activations of LLMs at inference time."
  - [section 4.3] "we determine the values we need to remove from zi to avoid the undesired behaviour... we determine the value we need to add to zi to encourage the desired behaviour."
  - [corpus] Moderate - some evidence from related SAE steering work, but specific validation for knowledge conflict steering is in the paper.
- Break condition: If editing SAE activations introduces significant information loss or affects unintended features, reducing accuracy or causing other behavioral changes.

## Foundational Learning

- Concept: Knowledge conflicts in LLMs
  - Why needed here: Understanding what knowledge conflicts are and why they matter is essential for grasping the problem this paper addresses.
  - Quick check question: What are the three types of knowledge conflicts identified in LLM literature, and which type does this paper focus on?

- Concept: Sparse Auto-encoders (SAEs) for feature decomposition
  - Why needed here: SAEs are the core technical tool used to identify and manipulate the features controlling knowledge selection.
  - Quick check question: How do SAEs differ from traditional auto-encoders in their approach to representing activations, and why is this important for interpretability?

- Concept: Representation engineering vs mechanistic interpretability
  - Why needed here: The paper's approach falls under representation engineering, which is distinct from lower-level mechanistic interpretability approaches.
  - Quick check question: What is the key difference between representation engineering and mechanistic interpretability in the context of understanding and controlling LLM behavior?

## Architecture Onboarding

- Component map: Knowledge conflict detection → SAE feature identification → Inference-time activation editing → Evaluation
- Critical path: Detection → Feature identification → Activation editing → Evaluation. Each step depends on the previous one being successful.
- Design tradeoffs: Using pre-trained SAEs allows training-free operation but requires availability of suitable SAEs for the target model; editing at mid-layers balances effectiveness with minimal interference.
- Failure signatures: Poor knowledge conflict detection accuracy, inability to identify functional SAE features, minimal behavioral change after editing, or significant accuracy degradation.
- First 3 experiments:
  1. Validate knowledge conflict detection accuracy across layers using linear probing on a small subset of NQSwap.
  2. Test SAE feature identification by manually inspecting top-k activations for interpretability and correlation with knowledge selection.
  3. Evaluate simple activation editing (z- only, then z+ only) on a small validation set to confirm directional control before implementing full SPARE.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the selected SAE features relate to the specific knowledge being selected (parametric vs contextual) beyond the observed correlations?
- Basis in paper: [explicit] The paper identifies SAE features correlated with knowledge selection behaviors but doesn't analyze their semantic content or what specific knowledge they represent.
- Why unresolved: The analysis stops at identifying functional features without investigating what linguistic or factual content these features encode.
- What evidence would resolve it: Detailed semantic analysis of the top SAE features, correlating them with specific types of factual knowledge or linguistic patterns in the context.

### Open Question 2
- Question: Would SPARE generalize to knowledge conflicts involving more complex reasoning tasks beyond simple fact recall?
- Basis in paper: [inferred] The paper tests only open-domain QA with simple knowledge conflicts, not multi-hop reasoning or complex inference scenarios.
- Why unresolved: The experiments focus on straightforward factual conflicts where the correct answer is explicitly stated in the context or known to the model.
- What evidence would resolve it: Testing SPARE on datasets requiring multi-step reasoning, commonsense inference, or synthesis of multiple evidence sources.

### Open Question 3
- Question: What is the computational overhead of using SPARE at inference time compared to the baseline methods?
- Basis in paper: [explicit] The paper mentions SPARE is "training-free" but doesn't provide timing comparisons with baseline methods.
- Why unresolved: The paper focuses on accuracy improvements but doesn't quantify latency or computational cost differences.
- What evidence would resolve it: Runtime measurements comparing SPARE against DoLa, CAD, and representation engineering baselines across different model sizes.

## Limitations
- The method's dependence on pre-trained SAEs raises questions about accessibility and compatibility across different models
- Limited discussion of edge cases where knowledge conflicts might be more nuanced or involve multiple conflicting knowledge sources simultaneously
- Uncertainty about generalizability beyond the specific ODQA tasks and models tested

## Confidence
- High confidence: The knowledge conflict detection mechanism using linear probing at mid-layers is well-supported by empirical evidence showing increased probing accuracy at specific layers.
- Medium confidence: The SAE-based feature identification approach is theoretically sound and shows promise, but the paper provides limited evidence that the identified features are truly "functional" rather than correlational.
- Medium confidence: The inference-time activation editing demonstrates effectiveness in steering behaviors, but the precision and potential side effects of the editing process warrant further investigation.

## Next Checks
1. Test SPARE's effectiveness on knowledge conflict scenarios in code generation tasks, where the conflicts involve outdated API usage vs current documentation, to assess generalizability beyond QA tasks.
2. Conduct ablation studies to determine the minimum number of SAE features needed for effective steering and evaluate whether the method introduces information loss or affects unintended behaviors.
3. Evaluate SPARE's performance on larger models (e.g., Llama3-70B, GPT-4) to understand scalability and whether the mid-layer concentration of effectiveness remains consistent across model sizes.