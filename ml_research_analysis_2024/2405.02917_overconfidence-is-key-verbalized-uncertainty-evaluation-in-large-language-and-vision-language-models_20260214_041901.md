---
ver: rpa2
title: 'Overconfidence is Key: Verbalized Uncertainty Evaluation in Large Language
  and Vision-Language Models'
arxiv_id: '2405.02917'
source_url: https://arxiv.org/abs/2405.02917
tags:
- answer
- confidence
- your
- calibration
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the uncertainty estimation capabilities of
  large language models (GPT-4, GPT-3.5, LLaMA2, and PaLM 2) and vision-language models
  (GPT-4V and Gemini Pro Vision) by prompting them to verbalize their confidence levels.
  The study introduces the Japanese Uncertain Scenes dataset for challenging image
  recognition tasks and the Net Calibration Error (NCE) metric to measure miscalibration
  direction.
---

# Overconfidence is Key: Verbalized Uncertainty Evaluation in Large Language and Vision-Language Models

## Quick Facts
- arXiv ID: 2405.02917
- Source URL: https://arxiv.org/abs/2405.02917
- Authors: Tobias Groot; Matias Valdenegro-Toro
- Reference count: 40
- Key outcome: Both LLMs and VLMs exhibit high calibration errors and are predominantly overconfident, indicating poor uncertainty estimation.

## Executive Summary
This paper evaluates the uncertainty estimation capabilities of large language models (GPT-4, GPT-3.5, LLaMA2, and PaLM 2) and vision-language models (GPT-4V and Gemini Pro Vision) by prompting them to verbalize their confidence levels. The study introduces the Japanese Uncertain Scenes dataset for challenging image recognition tasks and the Net Calibration Error (NCE) metric to measure miscalibration direction. Results show that both LLMs and VLMs exhibit high calibration errors and are predominantly overconfident, indicating poor uncertainty estimation. Additionally, VLMs perform poorly when producing mean/standard deviation and 95% confidence intervals for regression tasks. GPT-4 demonstrates the best calibration among LLMs, but all models struggle with accurate uncertainty estimation.

## Method Summary
The study evaluates LLMs and VLMs by prompting them to verbalize their confidence alongside answers. For NLP tasks, datasets include SST sentiment analysis, GSM8K math problems, and CoNLL 2003 NER, each with 100 random samples. For VLM tasks, the newly created Japanese Uncertain Scenes (JUS) dataset with 39 images is used. Models are prompted with task-specific instructions requiring confidence verbalization. Evaluation metrics include Expected Calibration Error (ECE), Maximum Calibration Error (MCE), Net Calibration Error (NCE), accuracy, and confidence. Calibration plots and confidence density histograms are generated for analysis.

## Key Results
- All evaluated models (GPT-4, GPT-3.5, LLaMA2, PaLM 2, GPT-4V, Gemini Pro Vision) exhibit high calibration errors and are predominantly overconfident
- GPT-4 demonstrates the best calibration among LLMs but still shows significant miscalibration
- VLMs perform poorly when producing mean/standard deviation and 95% confidence intervals for regression tasks
- The NCE metric effectively distinguishes between underconfidence and overconfidence in models

## Why This Works (Mechanism)

### Mechanism 1
The Net Calibration Error (NCE) can distinguish between underconfidence and overconfidence in LLMs and VLMs. NCE is calculated as the weighted average of the difference between accuracy and confidence, without taking the absolute value. A positive NCE indicates underconfidence, while a negative NCE indicates overconfidence. The core assumption is that the distribution of confidence scores across different bins is representative of the model's overall calibration behavior.

### Mechanism 2
Verbalized uncertainty prompts can elicit more calibrated confidence estimates from LLMs and VLMs. By explicitly asking models to express their confidence alongside their answers, the models are encouraged to consider their own uncertainty and provide more calibrated responses. The core assumption is that the models have some internal representation of their own uncertainty that can be accessed through appropriate prompting.

### Mechanism 3
Challenging datasets with difficult queries and object counting tasks can expose limitations in VLM uncertainty estimation. By presenting VLMs with images that are difficult to interpret or require complex reasoning (e.g., counting objects), the models' uncertainty estimation capabilities are tested more rigorously. The core assumption is that the difficulty of the queries and tasks is appropriately calibrated to challenge the VLMs without being impossible.

## Foundational Learning

- Concept: Calibration error and its relationship to model uncertainty estimation
  - Why needed here: Understanding calibration error is crucial for evaluating the effectiveness of the proposed NCE metric and the overall uncertainty estimation capabilities of LLMs and VLMs
  - Quick check question: What is the difference between Expected Calibration Error (ECE) and Net Calibration Error (NCE)?

- Concept: Verbalized uncertainty and its role in model interpretability
  - Why needed here: The study focuses on evaluating models' ability to express their uncertainty verbally, which is an important aspect of interpretability and trust in AI systems
  - Quick check question: How does verbalized uncertainty differ from traditional uncertainty estimation methods?

- Concept: Vision-language models and their unique challenges in uncertainty estimation
  - Why needed here: The study includes VLMs, which face additional challenges in uncertainty estimation due to the multimodal nature of their inputs and the complexity of visual reasoning tasks
  - Quick check question: What are some specific challenges that VLMs face in estimating their uncertainty compared to LLMs?

## Architecture Onboarding

- Component map: Data gathering (prompts, datasets) -> Evaluation metrics (ECE, MCE, NCE, accuracy, confidence) -> Analysis tools (calibration plots, confidence density histograms, Pearson correlation tests)
- Critical path: 1. Prepare prompts and datasets 2. Gather model responses using the prompts 3. Calculate evaluation metrics 4. Generate calibration plots and confidence density histograms 5. Perform statistical analysis 6. Interpret results and draw conclusions
- Design tradeoffs: Using fixed number of samples (100) from each dataset vs. using entire dataset for more robust evaluation; choosing between different evaluation metrics based on strengths and limitations; deciding on appropriate bin size for calibration plots and confidence density histograms
- Failure signatures: Models consistently outputting very high confidence scores regardless of accuracy; lack of variation in confidence scores across different tasks or models; inconsistent or unexpected results when using different evaluation metrics
- First 3 experiments: 1. Evaluate calibration of single LLM (e.g., GPT-4) on single task (e.g., sentiment analysis) using ECE and NCE 2. Compare calibration of multiple LLMs (e.g., GPT-4, GPT-3.5, LLaMA2) on same task (e.g., math word problems) using ECE and NCE 3. Evaluate calibration of single VLM (e.g., GPT-4V) on JUS dataset using ECE, MCE, and NCE

## Open Questions the Paper Calls Out

- Question: Can Chain-of-Thought (CoT) prompting improve the uncertainty estimation quality in LLMs and VLMs?
  - Basis in paper: The paper suggests investigating if CoT-prompting could improve uncertainty estimation quality, as it has been shown to significantly increase accuracy in LLMs on certain tasks
  - Why unresolved: This is a proposed direction for future research, but no experiments have been conducted to test the effectiveness of CoT-prompting on uncertainty estimation
  - What evidence would resolve it: Experimental results comparing the calibration of LLMs and VLMs with and without CoT-prompting on various tasks, using metrics like ECE, MCE, and NCE

- Question: How can uncertainty estimation capabilities be directly improved in open-source models like LLaMA-2-70b?
  - Basis in paper: The paper mentions that LLaMA-2-70b is an open-source model, presenting an opportunity for future research to investigate how direct modifications to the model could improve its uncertainty estimation capabilities
  - Why unresolved: This is a proposed direction for future research, but no specific methods or experiments have been proposed or conducted to modify the model for better uncertainty estimation
  - What evidence would resolve it: Successful implementation and evaluation of modifications to LLaMA-2-70b or other open-source models that lead to improved uncertainty estimation, as measured by calibration metrics

- Question: How does the size of the dataset used for evaluation impact the reliability of conclusions about uncertainty estimation in LLMs and VLMs?
  - Basis in paper: The paper uses a limited number of samples (100 per task) and a small dataset (39 images) for the image recognition task, which may not provide a comprehensive assessment of uncertainty estimation capabilities
  - Why unresolved: The paper acknowledges the limitation of using a small dataset but does not explore how this impacts the reliability of the conclusions or whether larger datasets would yield different results
  - What evidence would resolve it: Comparative studies using datasets of varying sizes to evaluate uncertainty estimation in LLMs and VLMs, assessing the stability and reliability of the conclusions across different dataset sizes

## Limitations
- The evaluation relies on verbalized uncertainty prompts, which may not capture the full range of model uncertainty representations
- The JUS dataset, while designed to be challenging, consists of only 39 images, limiting statistical power
- Confidence interval estimates from VLMs for regression tasks were often nonsensical, suggesting potential issues with the prompting strategy

## Confidence
- Calibration metric validation (High): The NCE metric is well-defined and its relationship to ECE is clearly established
- Model comparison results (Medium): Consistent patterns observed across multiple models and tasks, but limited by prompt variations
- VLM-specific findings (Low): Results based on small dataset and may be sensitive to specific image selections

## Next Checks
1. Test additional prompting strategies for eliciting confidence intervals from VLMs, including different parameterizations of mean/std deviation and 95% CI formats
2. Expand evaluation to include more diverse image recognition datasets with larger sample sizes to validate the JUS findings
3. Implement and compare alternative calibration methods (Platt scaling, temperature scaling) to assess whether post-hoc calibration can improve verbalized uncertainty estimates