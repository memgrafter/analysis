---
ver: rpa2
title: 'Prompting Fairness: Integrating Causality to Debias Large Language Models'
arxiv_id: '2403.08743'
source_url: https://arxiv.org/abs/2403.08743
tags:
- reasoning
- causal
- llms
- bias
- debiasing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a causal framework for mitigating social bias
  in large language models (LLMs). The framework is based on modeling the data generation
  process of training corpora and the LLM reasoning process, identifying how prompts
  influence output through selection mechanisms.
---

# Prompting Fairness: Integrating Causality to Debias Large Language Models

## Quick Facts
- arXiv ID: 2403.08743
- Source URL: https://arxiv.org/abs/2403.08743
- Authors: Jingling Li; Zeyu Tang; Xiaoyu Liu; Peter Spirtes; Kun Zhang; Liu Leqi; Yang Liu
- Reference count: 40
- Key outcome: This work introduces a causal framework for mitigating social bias in large language models (LLMs). The framework is based on modeling the data generation process of training corpora and the LLM reasoning process, identifying how prompts influence output through selection mechanisms. Two principles are proposed: discouraging biased reasoning (counteracting existing selection bias) and encouraging bias-free reasoning (nudging towards demographic-agnostic facts and away from demographic-aware text). This unifies existing debiasing prompting methods and suggests new strategies. Experiments on WinoBias and Discrim-Eval datasets show that combining both principles significantly outperforms baseline prompting techniques in reducing gender and demographic biases across multiple LLMs, even with only black-box access. The framework provides a principled approach to LLM debiasing.

## Executive Summary
This paper presents a causality-guided framework for mitigating social bias in large language models through prompt engineering. The authors model both the training corpus generation process and the LLM reasoning process as causal graphs, identifying how prompts can influence model outputs by reshaping selection mechanisms. The framework introduces two key principles: counteracting existing selection bias by enforcing conditional independence between demographic and entity representations, and encouraging bias-free reasoning by prioritizing demographic-agnostic factual knowledge. Experiments demonstrate that combining these approaches significantly outperforms existing prompting-based debiasing methods across multiple LLMs and bias evaluation datasets.

## Method Summary
The approach constructs a causal model of how demographic information flows through the LLM reasoning process, identifying a "prompt properly considered" (PPC) variable that gates the relationship between demographic representations and outputs. Three debiasing strategies are implemented through prompt design: (1) nudging the model toward demographic-agnostic facts before answering the original question, (2) explicitly stating that demographics are equally likely to hold any role to counteract historical selection bias, and (3) combining both approaches. The method requires only black-box access to LLMs and operates entirely through prompt engineering without model fine-tuning. Experiments test these strategies on WinoBias (gender bias in coreference resolution) and Discrim-Eval (demographic bias in binary decisions) using multiple commercial LLMs.

## Key Results
- Combining both debiasing principles ("Reduce + Fact") significantly outperforms individual strategies and baseline prompting methods on WinoBias and Discrim-Eval datasets
- The framework successfully reduces gender and demographic biases across multiple LLMs (GPT-3, GPT-3.5, GPT-4, Claude 2, Mistral-7B) while maintaining task performance
- Counterfactual reasoning prompts that explicitly state demographic equality are particularly effective at reducing stereotypical associations
- Fact-based reasoning prompts that separate demographic-neutral factual judgments from the main task help reduce reliance on biased social cues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selection mechanisms reshape dependence patterns in both the training corpus and the LLM's internal reasoning, enabling bias modulation through prompt design.
- Mechanism: The prompt influences a "prompt properly considered" (PPC) variable that gates the flow of information from demographic representations to the LLM output. By constraining or conditioning PPC, the model is steered away from using biased pathways.
- Core assumption: The LLM's internal reasoning process mirrors the dependence patterns captured from the training corpus, and selection mechanisms can be modulated by external prompts.
- Evidence anchors:
  - [abstract] "Our framework introduces a novel perspective to identify how social information can affect an LLM's decision through different causal pathways."
  - [section 2.2] "Selection mechanisms can reshape dependence patterns among involved variables, and as a consequence, potentially change downstream outputs as well."
  - [corpus] Weak - corpus shows related work on bias mitigation but no direct evidence of selection mechanisms being explicitly modeled in LLMs.
- Break condition: If the LLM does not internalize the dependence patterns from training data, or if prompts cannot effectively modulate PPC, the debiasing strategy fails.

### Mechanism 2
- Claim: Encouraging bias-free reasoning by nudging the model toward demographic-agnostic facts reduces reliance on biased social cues.
- Mechanism: The prompt first asks the model to compare factual scenarios without demographic information, then uses the model's factual judgment to inform the original biased task. This prioritizes factual knowledge over social stereotypes.
- Core assumption: LLMs have internalized factual knowledge that is less biased than socially conditioned stereotypes, and they can be guided to use this knowledge through structured prompts.
- Evidence anchors:
  - [abstract] "This framework not only unifies existing prompting-based debiasing techniques, but also opens up new directions for reducing bias by encouraging the model to prioritize fact-based reasoning over reliance on biased social cues."
  - [section 3.2] "Strategy I (Nudge Towards Demographic-Agnostic Fact)... nudges LLMs towards utilizing demographic-agnostic fact when generating the output."
  - [corpus] Weak - corpus lists related work but lacks direct experimental evidence that fact-based nudging reduces bias more than other methods.
- Break condition: If the factual knowledge itself is biased, or if the model fails to integrate the factual answer into the original task, the approach loses effectiveness.

### Mechanism 3
- Claim: Counteracting existing selection bias by enforcing conditional independence between demographic representations and entity representations reduces stereotypical associations.
- Mechanism: The prompt explicitly states that both genders (or demographics) are equally likely to hold any role, thereby breaking the historical association learned from biased training data.
- Core assumption: Historical selection bias in the training corpus has created spurious associations between demographics and entities that can be broken by explicit prompt-based constraints.
- Evidence anchors:
  - [abstract] "We introduce a causality-guided debiasing framework... aiming to reduce the objectionable dependence between LLMs' decisions and the social information in the input."
  - [section 3.2] "Strategy II (Counteract Existing Selection Bias)... The intuition behind this strategy is to directly counteract the effect of existing historical discriminations."
  - [corpus] Moderate - corpus lists related work on debiasing but does not provide evidence that conditional independence constraints in prompts reduce bias.
- Break condition: If the LLM ignores the prompt's explicit constraint, or if the historical bias is too deeply embedded to be overridden by a single prompt, the debiasing fails.

## Foundational Learning

- Concept: Causal modeling and directed acyclic graphs (DAGs)
  - Why needed here: The framework relies on modeling the data generation process and the LLM reasoning process as causal graphs to identify how biases propagate and how prompts can intervene.
  - Quick check question: Can you draw a simple DAG showing how demographic information flows to an LLM output, including a selection node that represents bias?

- Concept: Selection mechanisms and their role in shaping data dependence
  - Why needed here: Understanding how selection mechanisms create spurious associations in the training corpus is key to designing prompts that counteract these biases.
  - Quick check question: What is the difference between a selection mechanism based solely on the cause versus one based solely on the effect?

- Concept: Conditional independence and its use in debiasing
  - Why needed here: The debiasing strategies rely on enforcing conditional independence between demographic representations and other variables to break biased associations.
  - Quick check question: How would you express the condition "demographic representation is independent of entity representation given PPC and selection S" in formal notation?

## Architecture Onboarding

- Component map: Training corpus generator (simulated by causal model) -> LLM inference engine (black-box, accessed via prompts) -> Prompt designer (implements Strategies I, II, III) -> Evaluation harness (WinoBias, Discrim-Eval datasets) -> Causal model builder (DAG construction and condition checking)

- Critical path:
  1. Construct causal model of training corpus bias
  2. Map prompt design to selection mechanism constraints
  3. Generate debiasing prompt using Strategies I, II, III
  4. Run prompt against LLM and capture output
  5. Evaluate bias reduction using accuracy gap or relative gap metrics

- Design tradeoffs:
  - Granularity vs. tractability: Finer-grained causal models may better capture bias but are harder to construct and validate.
  - Prompt complexity vs. LLM compliance: More complex prompts may be ignored or misunderstood by the model.
  - Generalizability vs. specificity: Strategies tailored to specific datasets may not generalize to others.

- Failure signatures:
  - No reduction in bias metrics despite prompt changes
  - LLM refuses to answer or outputs off-topic responses
  - Accuracy drops significantly, indicating the prompt disrupted task performance

- First 3 experiments:
  1. Test Strategy I alone on WinoBias Type I to measure fact-based reasoning improvement.
  2. Test Strategy II alone on Discrim-Eval gender category to measure stereotype reduction.
  3. Combine Strategies I and II on GPT-4 WinoBias Type II to measure joint effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do selection mechanisms reshape dependence patterns in the LLM reasoning process, and how can we precisely quantify their impact on bias?
- Basis in paper: The paper discusses selection mechanisms in both the data generation process and the LLM reasoning process, but does not provide a detailed quantification of their impact on bias.
- Why unresolved: The paper focuses on the theoretical framework and experimental results, but does not delve into the specific mechanisms by which selection mechanisms influence bias in the LLM reasoning process.
- What evidence would resolve it: A detailed analysis of the LLM reasoning process, including the identification and quantification of specific selection mechanisms and their impact on bias in the output.

### Open Question 2
- Question: How does the effectiveness of the proposed debiasing framework vary across different LLM architectures and training data?
- Basis in paper: The paper presents experimental results on a limited set of LLMs and does not explore the generalizability of the framework across different architectures and training data.
- Why unresolved: The paper does not provide a comprehensive analysis of the framework's effectiveness across diverse LLM architectures and training data.
- What evidence would resolve it: Extensive experiments on a wide range of LLM architectures and training data, comparing the framework's performance to existing debiasing methods.

### Open Question 3
- Question: How can the proposed framework be extended to address other forms of bias beyond social bias, such as cognitive or linguistic bias?
- Basis in paper: The paper focuses on social bias and does not discuss the potential application of the framework to other forms of bias.
- Why unresolved: The paper does not explore the applicability of the framework to other types of bias that may be present in LLM outputs.
- What evidence would resolve it: Research on the extension of the framework to address other forms of bias, including the identification of relevant causal factors and the development of appropriate debiasing strategies.

## Limitations

- The causal framework's effectiveness depends on LLMs actually modeling the dependence patterns from training data, which lacks direct empirical validation
- The framework only addresses gender and simple demographic biases, with unknown generalizability to intersectional or more complex bias categories
- The approach requires careful prompt engineering that may not transfer well across different tasks or domains

## Confidence

- High confidence: The experimental results showing bias reduction on WinoBias and Discrim-Eval datasets are well-documented and reproducible. The accuracy gap and relative gap metrics provide clear evidence of effectiveness.
- Medium confidence: The theoretical framework connecting causal modeling to prompt design is coherent and well-motivated, but the mapping between abstract causal concepts and practical prompt engineering contains some hand-waving. The claim that combining both principles outperforms individual strategies is supported but could benefit from more rigorous ablation studies.
- Low confidence: The assertion that LLMs internalize and can be manipulated through selection mechanism constraints based on training corpus dependence patterns is the weakest link. The paper does not provide evidence that LLMs actually model these causal relationships in the way described.

## Next Checks

1. **Pathway validation experiment**: Use mechanistic interpretability techniques (e.g., activation patching, probing classifiers) to verify whether the proposed PPC variable and selection mechanism pathways actually exist in LLM internal representations when processing debiasing prompts.

2. **Cross-bias generalization test**: Apply the framework to different types of social biases (e.g., racial, age-related, or intersectional biases) not covered in the original experiments to assess whether the causal principles generalize beyond gender and simple demographic categories.

3. **Model architecture ablation**: Test the framework across diverse model architectures including open-weight models where internal structure is accessible, and compare results with proprietary models to determine whether the debiasing effectiveness depends on specific architectural features or training approaches.