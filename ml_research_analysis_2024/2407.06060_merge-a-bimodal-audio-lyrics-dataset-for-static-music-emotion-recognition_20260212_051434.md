---
ver: rpa2
title: MERGE -- A Bimodal Audio-Lyrics Dataset for Static Music Emotion Recognition
arxiv_id: '2407.06060'
source_url: https://arxiv.org/abs/2407.06060
tags:
- emotion
- audio
- lyrics
- dataset
- music
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents MERGE, a novel bimodal audio-lyrics dataset\
  \ for Music Emotion Recognition (MER). The dataset includes 2,216 validated bimodal\
  \ samples, along with separate audio-only and lyrics-only subsets, annotated using\
  \ Russell\u2019s emotion quadrants and continuous arousal-valence values."
---

# MERGE -- A Bimodal Audio-Lyrics Dataset for Static Music Emotion Recognition

## Quick Facts
- arXiv ID: 2407.06060
- Source URL: https://arxiv.org/abs/2407.06060
- Reference count: 40
- Key outcome: MERGE is the largest quality-controlled bimodal audio-lyrics dataset for music emotion recognition, with 2,216 validated samples and 81.74% F1-score baseline performance.

## Executive Summary
This paper presents MERGE, a novel bimodal audio-lyrics dataset for Music Emotion Recognition (MER). The dataset includes 2,216 validated bimodal samples, along with separate audio-only and lyrics-only subsets, annotated using Russell's emotion quadrants and continuous arousal-valence values. A semi-automatic creation protocol based on AllMusic emotion tags and manual validation was used to ensure high quality. Experiments using classical ML and deep learning methods establish a strong baseline, with the best bimodal model achieving 81.74% F1-score. The dataset is publicly available and includes metadata and standardized train-validation-test splits to support future benchmarking. MERGE is the largest quality-controlled bimodal MER dataset to date, demonstrating the effectiveness of combining audio and lyrics for emotion recognition.

## Method Summary
MERGE was created using a semi-automatic protocol that maps AllMusic emotion tags to Russell's circumplex model via the Warriner affective lexicon, followed by human validation of quadrant alignment. The dataset includes audio clips, lyrics, genre metadata, and emotion annotations in both categorical (quadrant) and continuous (arousal-valence) formats. Two dataset versions are provided: complete (all songs) and balanced (equal distribution across quadrants and genres). Baseline experiments compare classical machine learning (SVM with handcrafted features) against deep learning approaches (wav2vec2 audio embeddings + RoBERTa word embeddings) with early fusion, evaluated using F1-score for classification and RMSE/R² for regression.

## Key Results
- MERGE contains 2,216 validated bimodal samples, 3,554 audio-only samples, and 2,568 lyrics-only samples
- Best bimodal model achieves 81.74% F1-score, outperforming both audio-only (72.76%) and lyrics-only (70.30%) approaches
- Balanced dataset variations enable controlled experimentation on genre and quadrant representation effects
- Regression analysis shows strong correlation between predicted and actual arousal-valence values (R² scores > 0.7 for some quadrants)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The semi-automatic protocol reduces annotation workload while maintaining quality through expert-based tag mapping and human validation.
- **Mechanism:** The protocol leverages expert-curated AllMusic emotion tags, maps them to Russell's circumplex model using Warriner's affective lexicon, and applies human validation only to verify quadrant alignment rather than full manual annotation.
- **Core assumption:** Expert tags from AllMusic are sufficiently reliable as a starting point, and the Warriner lexicon provides accurate valence-arousal mappings for most emotion terms.
- **Evidence anchors:**
  - [abstract] "A semi-automatic creation protocol based on AllMusic emotion tags and manual validation was used to ensure high quality."
  - [section] "To partly overcome the described limitations, in [7], we proposed a semi-automatic data collection and annotation strategy based on AllMusic annotations. The basic idea was to map the provided multi-label annotations of each song to a single emotion quadrant (according to Russell's model)."
  - [corpus] Weak evidence; no direct citations of similar semi-automatic approaches in related works.
- **Break condition:** If AllMusic tags are inconsistent or the Warriner lexicon mapping fails for significant terms, manual validation burden increases and quality degrades.

### Mechanism 2
- **Claim:** The balanced and complete dataset variations allow controlled experimentation on genre and quadrant representation effects.
- **Mechanism:** Two dataset versions are created: "complete" (all songs) and "balanced" (equal per quadrant and genre by removing overrepresented samples), enabling studies of both real-world distribution and balanced evaluation.
- **Core assumption:** Quadrant and genre balancing improves model generalization without losing meaningful emotional patterns.
- **Evidence anchors:**
  - [section] "Each modality includes two variations: i) complete, i.e., all songs without any balancing; ii) ii) balanced, focusing on even distribution across both quadrant and genre."
  - [section] "The bimodal dataset is created from validated audio and lyrics datasets, including songs that correspond to the same audio and lyrics quadrants."
  - [corpus] No direct evidence in related works; this is a novel contribution of MERGE.
- **Break condition:** If balancing removes too many samples, dataset size may become insufficient for robust training, especially for deep learning models.

### Mechanism 3
- **Claim:** Early fusion of audio embeddings and word embeddings captures complementary emotional information more effectively than single-modality approaches.
- **Mechanism:** Audio embeddings (wav2vec2) capture acoustic/emotional timbre, while word embeddings (RoBERTa) capture lyrical semantic content; concatenation allows the model to learn joint emotional representations.
- **Core assumption:** Emotional content in music is partially separable between audio (arousal) and lyrics (valence), and these modalities provide non-redundant signals.
- **Evidence anchors:**
  - [section] "The result, illustrated in Fig. 1, is a two-dimensional plane, referred to as the arousal-valence (A V) plane. There, the X-axis represents valence and the Y-axis represents arousal, resulting in four quadrants..."
  - [section] "In summary, the discrete class of dimensional models entails the same advantages and drawbacks discussed above in the categorical emotion paradigm."
  - [corpus] Weak evidence; while related works exist, none directly validate the specific early fusion architecture proposed here.
- **Break condition:** If audio and lyrics convey highly redundant or conflicting emotions, fusion may add noise rather than complementary signal, reducing classification performance.

## Foundational Learning

- **Concept:** Russell's circumplex model of emotion (valence-arousal plane with four quadrants)
  - Why needed here: The dataset and all experiments are built around this emotion representation, mapping both categorical quadrants and continuous A-V values.
  - Quick check question: What emotional states fall into Q2 (negative valence, positive arousal) according to Russell's model?
- **Concept:** Semi-supervised data annotation and validation protocols
  - Why needed here: The dataset creation relies on mapping expert tags and validating only quadrant alignment, not full manual annotation, to reduce cost while maintaining quality.
  - Quick check question: In the MERGE protocol, what is the minimum number of annotator confirmations required to validate a song?
- **Concept:** Feature engineering vs. deep learning embeddings for emotion recognition
  - Why needed here: The baseline experiments compare classical handcrafted features with deep learning embeddings (wav2vec2 for audio, RoBERTa for lyrics) to establish performance baselines.
  - Quick check question: Which audio feature extraction method achieved the highest F1-score in the MERGE experiments, handcrafted or embedding-based?

## Architecture Onboarding

- **Component map:** AllMusic API (audio clips + emotion tags) -> lyrics platforms (lyrics.com, ChartLyrics, etc.) -> preprocessing (audio standardization, lyrics cleaning, genre balancing) -> tag mapping (AllMusic tags -> Warriner lexicon -> A-V values -> Russell quadrants) -> validation (human annotators verify quadrant alignment) -> dataset creation (complete vs. balanced versions) -> train-validation-test splits (70-15-15 and 40-30-30) -> baseline experiments (classical ML vs. DL vs. bimodal fusion)
- **Critical path:** Data collection -> preprocessing -> tag mapping -> validation -> dataset creation -> train-validation-test splits -> baseline experiments
- **Design tradeoffs:**
  - Manual validation vs. full automation: MERGE opts for semi-automatic to balance quality and scalability
  - Balanced vs. complete datasets: Balanced ensures even evaluation but may reduce real-world representativeness
  - Early fusion vs. late fusion: Early fusion used here for simplicity, but late fusion might capture modality-specific nuances better
- **Failure signatures:**
  - Low inter-annotator agreement during validation -> indicates ambiguous samples or poor tag mapping
  - High standard deviation in cross-validation -> suggests instability or insufficient data
  - Confusion between Q3 and Q4 in results -> indicates difficulty distinguishing low-arousal valence states
- **First 3 experiments:**
  1. Train SVM with handcrafted audio features on MERGE AC (complete) using 70-15-15 split; evaluate F1-score per quadrant
  2. Train bimodal DL model (wav2vec2 + RoBERTa + early fusion) on MERGE BC (complete) with same split; compare to unimodal baselines
  3. Evaluate regression performance (R², RMSE) for arousal and valence predictions on MERGE BC; analyze quadrant mapping accuracy from regression outputs

## Open Questions the Paper Calls Out

- **Question:** Does fine-tuning the audio embeddings (wav2vec2) improve emotion recognition performance compared to using the pre-trained model as-is?
- **Basis in paper:** [explicit] The paper notes that the wav2vec2 embeddings were used without fine-tuning and states "the employed audio embeddings have room for improvement compared to state-of-the-art acoustic features, such as through fine-tuning."
- **Why unresolved:** The experiments used the pre-trained wav2vec2 model without adaptation to the music emotion domain, leaving the potential benefit of fine-tuning unexplored.
- **What evidence would resolve it:** Comparing emotion recognition F1-scores using fine-tuned vs. non-fine-tuned audio embeddings on the MERGE dataset.

- **Question:** How much does manual validation of arousal-valence (AV) values improve regression performance compared to automatically mapped values?
- **Basis in paper:** [inferred] The paper uses automatically mapped AV values from emotion tags and notes "we believe the obtained scores are acceptable" but acknowledges this is a limitation since "the reference AV values were obtained automatically."
- **Why unresolved:** No manual validation of AV values was performed, so the potential improvement from human-annotated continuous values remains unknown.
- **What evidence would resolve it:** Conducting a user study to manually annotate AV values for a subset of MERGE songs and comparing regression RMSE/R² scores with the current automatic approach.

- **Question:** Does combining handcrafted features with deep learning embeddings (hybrid approach) outperform using either method alone?
- **Basis in paper:** [explicit] The paper states "a preliminary study shows the promise of hybrid approaches" and that "the combination of handcrafted features with deep neural networks outperformed traditional feature engineering and machine learning methods."
- **Why unresolved:** While mentioned, the hybrid approach was not fully explored in the main experiments, leaving its effectiveness unconfirmed on MERGE.
- **What evidence would resolve it:** Training models that concatenate handcrafted audio/lyrics features with wav2vec2/RoBERTa embeddings and comparing F1-scores to pure feature engineering and pure deep learning models.

## Limitations

- The semi-automatic annotation protocol depends heavily on the reliability of AllMusic tags and the Warriner lexicon mapping, neither of which are independently validated in this work.
- The balanced dataset versions, while useful for controlled experiments, may not reflect real-world emotion distributions and reduce dataset size for training.
- The baseline experiments use referenced preprocessing methods ([7], [13]) without full implementation details, making exact reproduction challenging.

## Confidence

- Dataset creation protocol: Medium - protocol is described but relies on external sources without full validation
- Baseline results: Medium - performance metrics are reported but dependent on unspecified preprocessing details
- Bimodal fusion effectiveness: Low-Medium - demonstrated performance gains but insufficient analysis of modality complementarity

## Next Checks

1. Replicate the semi-automatic annotation protocol with a small subset to verify quadrant mapping accuracy and inter-annotator agreement
2. Conduct ablation studies comparing early fusion vs. late fusion and unimodal vs. bimodal performance to isolate modality contributions
3. Evaluate model performance on out-of-distribution genres and emotion quadrants not well-represented in the balanced dataset