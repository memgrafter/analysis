---
ver: rpa2
title: Can VLMs Play Action Role-Playing Games? Take Black Myth Wukong as a Study
  Case
arxiv_id: '2409.12889'
source_url: https://arxiv.org/abs/2409.12889
tags:
- action
- game
- tasks
- task
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores the capabilities of Vision Language Models
  (VLMs) in action role-playing games (ARPGs), using Black Myth: Wukong as a case
  study. Traditional methods rely on game APIs, which are not widely available and
  don''t reflect human gameplay.'
---

# Can VLMs Play Action Role-Playing Games? Take Black Myth Wukong as a Study Case

## Quick Facts
- **arXiv ID:** 2409.12889
- **Source URL:** https://arxiv.org/abs/2409.12889
- **Reference count:** 40
- **One-line primary result:** VARP agent achieves 90% success on easy/medium combat tasks using only visual inputs

## Executive Summary
This paper explores the capabilities of Vision Language Models (VLMs) in action role-playing games, using Black Myth: Wukong as a case study. The authors propose the VARP agent framework that uses only visual inputs and generates complex actions without relying on game APIs. The framework combines VLMs with a human-guided trajectory system and achieves success in 90% of easy and medium-level combat scenarios, though performance drops to 40% on very hard navigation tasks.

## Method Summary
The VARP agent framework uses visual game screenshots as input and generates mouse/keyboard actions through a two-part system: an action planning system that composes atomic commands into task-specific sequences, and a human-guided trajectory system that retrieves and replays relevant human gameplay segments. The framework incorporates state-of-the-art VLMs (GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro) and uses Decomposable Task-Specific Auxiliary modules to improve decision-making precision. The authors define 12 tasks in Black Myth: Wukong, with 9 focused on combat, and release a human operation dataset with recorded gameplay videos and operation logs.

## Key Results
- VARP agent achieves 90% success rate on easy and medium combat scenarios
- Performance drops to 40% on the very hard navigation task
- Framework successfully generates complex action sequences using only visual inputs
- Human-guided trajectory system improves performance on hard tasks by learning from human gameplay data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** VLMs can generalize across diverse action sequences by decomposing complex tasks into atomic commands
- **Mechanism:** The framework maps visual input to a library of atomic actions (light attack, dodge, heavy attack, recover health) and uses a planning system to compose these into task-specific sequences
- **Core assumption:** Atomic commands can be meaningfully combined in predictable ways to handle varied in-game situations
- **Evidence anchors:**
  - [abstract] The framework "generates complex actions" from "only visual inputs"
  - [section] The action planning system "generates action combos" and stores them in an updatable action library
  - [corpus] Weak evidence; related papers focus on game-playing agents but don't specifically address action decomposition via VLMs
- **Break condition:** If enemy behaviors or game mechanics introduce action types not covered by the atomic command set, the decomposition strategy fails

### Mechanism 2
- **Claim:** Human-guided trajectory system enables rapid transfer of complex navigation and combat strategies
- **Mechanism:** By retrieving and replaying human gameplay segments that match current visual states, the agent bypasses the need for costly trial-and-error learning in complex 3D navigation and boss fights
- **Core assumption:** Human gameplay contains generalizable patterns that can be matched to new in-game situations via visual similarity
- **Evidence anchors:**
  - [section] The human-guided trajectory system "learns from human data via retrieval" and achieves 40% success in the "very hard" navigation task
  - [corpus] Weak evidence; related work on human-like agents does not specifically discuss retrieval-based trajectory learning in ARPGs
- **Break condition:** If the visual similarity metric fails to match relevant human segments, or if the human data lacks coverage of the required scenario, the system cannot generate effective actions

### Mechanism 3
- **Claim:** Decomposable Task-Specific Auxiliary (DTSA) modules mitigate VLM attention dilution and reduce hallucinations in multi-step reasoning
- **Mechanism:** By splitting the decision-making process into specialized submodules (enemy analysis, combat mode, health monitoring, spell skill timing), each VLM instance focuses on a narrow token set, improving precision and reducing errors compared to monolithic reasoning
- **Core assumption:** VLMs perform better on focused, low-token inputs than on long, complex instructions with interleaved reasoning steps
- **Evidence anchors:**
  - [section] DTSA "decomposes large tasks into smaller subtasks" and "significantly improving the accuracy of the decision-making module"
  - [corpus] Weak evidence; related work on agent decision-making does not explicitly discuss attention dilution mitigation via task decomposition
- **Break condition:** If task boundaries are unclear or if cross-module coordination is required for correct action sequencing, the DTSA approach may introduce synchronization errors

## Foundational Learning

- **Concept:** Vision-Language Model (VLM) multimodal reasoning
  - **Why needed here:** The agent must interpret visual game states and map them to textual reasoning and action outputs without access to game APIs
  - **Quick check question:** Can the VLM reliably extract enemy health bars, weapon states, and environmental cues from raw screenshots under varying lighting and occlusion?

- **Concept:** Reinforcement Learning (RL) limitations in sparse reward environments
  - **Why needed here:** The paper contrasts its VLM-based approach with RL, highlighting RL's poor generalization and need for extensive training in ARPGs with many tasks
  - **Quick check question:** Why does RL struggle more than VLMs in tasks with infrequent or delayed rewards, such as long boss fights?

- **Concept:** Retrieval-augmented generation (RAG) for action learning
  - **Why needed here:** The human-guided trajectory system uses a retrieval-based approach to match current game states with past human gameplay, enabling transfer of complex strategies
  - **Quick check question:** How does the embedding similarity metric handle variations in viewpoint, enemy positioning, or partial occlusion when matching human gameplay segments?

## Architecture Onboarding

- **Component map:** Screenshot → Grounding DINO detection → VLMs group inference → Action selection/composition → Execute Python action → Update libraries
- **Critical path:** Screenshot → Grounding DINO detection → VLMs group inference → Action selection/composition → Execute Python action → Update libraries
- **Design tradeoffs:**
  - Using VLMs instead of RL trades training time for inference cost and potential hallucination risk
  - Human-guided trajectory system improves performance on hard tasks but requires curated human data and limits generalization to unseen scenarios
  - Decomposing tasks into DTSA modules improves precision but adds coordination complexity
- **Failure signatures:**
  - VLM hallucinations producing invalid or unsafe actions
  - Retrieval system returning mismatched human segments due to visual similarity errors
  - Action library growing too large, causing retrieval inefficiency or outdated action entries
- **First 3 experiments:**
  1. Run the agent on Task 1 (Guidance) and verify success rate matches reported 100%
  2. Disable SOAG and measure performance drop on Task 10 (Defeat Bullguard) to confirm its contribution
  3. Remove DTSA and test Task 2 (Combat 1) to observe increase in action errors or timeouts

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several limitations are acknowledged including the framework being tested only within Black Myth: Wukong and not yet extended to other scenarios, the limitation of only inputting keyframes at second-level intervals which can miss critical information, and the potential for redundancy in human gameplay data during annotation.

## Limitations
- The reported 90% success rate applies only to "easy and medium" combat scenarios, with performance dropping to 40% on the "very hard" navigation task
- The evaluation framework uses only 12 tasks, 9 of which are combat-focused, potentially limiting generalizability to broader ARPG gameplay
- The paper does not address computational costs or inference latency, which could be significant given the reliance on multiple VLMs and real-time action generation

## Confidence
- **High Confidence:** The core claim that VLMs can interpret visual game states and generate appropriate actions is well-supported by the demonstrated success on easy and medium tasks
- **Medium Confidence:** The claim that VLMs can handle "complex" ARPG gameplay is partially supported but limited by the narrow task set and performance drop on harder scenarios
- **Low Confidence:** The paper's assertion that this approach generalizes to arbitrary ARPGs beyond Black Myth: Wukong lacks empirical support

## Next Checks
1. **Cross-Game Generalization Test:** Deploy the VARP framework on a different ARPG title (e.g., Elden Ring or Dark Souls) to assess whether the approach generalizes beyond Black Myth: Wukong's specific mechanics and visual style
2. **VLM Ablation Study:** Systematically test each VLM (GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro) individually across all task difficulty levels to quantify performance differences and identify potential bias in the "VLMs group" approach
3. **Computational Cost Analysis:** Measure and report inference latency and computational resources required for real-time gameplay across different hardware configurations, as this represents a critical practical limitation not addressed in the current evaluation