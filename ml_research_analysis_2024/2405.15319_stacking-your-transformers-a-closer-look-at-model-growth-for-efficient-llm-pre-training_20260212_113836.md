---
ver: rpa2
title: 'Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM
  Pre-Training'
arxiv_id: '2405.15319'
source_url: https://arxiv.org/abs/2405.15319
tags:
- gstack
- scratch
- flops
- billions
- okens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates model growth methods for efficient
  LLM pre-training. It categorizes existing approaches into four atomic growth operators
  and tests them in a standardized setting, finding that depthwise stacking (Gstack)
  significantly accelerates training while reducing loss.
---

# Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training

## Quick Facts
- arXiv ID: 2405.15319
- Source URL: https://arxiv.org/abs/2405.15319
- Authors: Wenyu Du; Tongxu Luo; Zihan Qiu; Zeyu Huang; Yikang Shen; Reynold Cheng; Yike Guo; Jie Fu
- Reference count: 40
- One-line primary result: Depthwise stacking (Gstack) significantly accelerates LLM pre-training, achieving up to 54.6% speedup and improved loss reduction.

## Executive Summary
This paper systematically evaluates model growth methods for efficient LLM pre-training by categorizing existing approaches into four atomic growth operators. The authors find that depthwise stacking (Gstack) significantly accelerates training while reducing loss, scaling well to larger models (up to 7B parameters) and extensive training (750B tokens). The method achieves up to 54.6% speedup and provides empirical guidelines for optimal growth timing and growth factor. Surprisingly, the paper demonstrates that function preservation is not essential for effective growth, challenging conventional assumptions about model growth techniques.

## Method Summary
The paper systematically evaluates model growth techniques by categorizing them into four atomic operators: Gdirect (direct stacking), Glearn (learned initialization), Gzero (zero initialization), and Grandom (random initialization). The primary focus is on Gstack, a depthwise stacking operator that reuses knowledge from smaller models by stacking trained layers from a base model to create a deeper target model. Experiments are conducted using the Slimpajama-627B dataset with models ranging from 410M to 7B parameters, evaluating training loss and average accuracy across eight NLP benchmarks. The authors provide empirical guidelines for determining optimal growth timing and growth factor, demonstrating that Gstack outperforms other growth operators and training from scratch.

## Key Results
- Gstack achieves up to 54.6% speedup in pre-training compared to baseline training from scratch
- Function preservation is not essential for effective growth, with 20% noise sometimes improving performance
- Gstack scales effectively to larger models (up to 7B parameters) and extensive training (up to 750B tokens)
- Optimal growth timing and growth factor can be determined empirically for different model sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Depthwise stacking (Gstack) accelerates LLM pre-training by reusing knowledge from smaller models.
- **Mechanism**: The method stacks trained layers from a smaller base model to create a deeper target model, initializing new layers with learned parameters rather than random initialization. This allows the larger model to start from a better point in parameter space, reducing the number of training steps needed to reach the same loss.
- **Core assumption**: The knowledge captured in the smaller model's layers is transferable to the larger architecture without degradation.
- **Evidence anchors**:
  - [abstract]: "a depth-wise stacking operator, called Gstack, exhibits remarkable acceleration in training, leading to decreased loss"
  - [section]: "Gstack achieves a 54.5% speedup in pre-training, improvement of 2.1 in NLP benchmarks average accuracy"
  - [corpus]: No direct corpus evidence, but related papers on catastrophic forgetting suggest knowledge retention is possible.
- **Break condition**: If the smaller model's learned representations are too domain-specific or if architectural differences between base and target models are too large.

### Mechanism 2
- **Claim**: Function preservation is not essential for effective model growth.
- **Mechanism**: The paper demonstrates that breaking strict function preservation by adding noise to the initialization can sometimes improve performance, suggesting that rapid adaptation is more important than maintaining exact functional equivalence.
- **Core assumption**: The optimization process can correct suboptimal initializations more effectively than preserving exact functions.
- **Evidence anchors**:
  - [section]: "adding 20% noise outperforms original Gdirect by 0.27 on the Wikitext PPL and 0.41 on the average accuracy score"
  - [abstract]: "function preservation is not essential for effective growth"
  - [corpus]: No direct corpus evidence, but recent work on "on the inductive bias of stacking towards improving reasoning" suggests stacking can improve reasoning capabilities.
- **Break condition**: If the noise level becomes too high (>20% in their experiments) and overwhelms the optimization process.

### Mechanism 3
- **Claim**: Gstack scales effectively to larger models and more training tokens.
- **Mechanism**: The method maintains acceleration benefits as model size increases (tested up to 7B parameters) and remains effective even with extensive training (tested up to 750B tokens).
- **Core assumption**: The scaling law for Gstack follows a predictable pattern that allows estimation of computational savings.
- **Evidence anchors**:
  - [abstract]: "our study shows that Gstack is scalable and consistently performs well, with experiments up to 7B LLMs after growth and pre-training LLMs with 750B tokens"
  - [section]: "we consistently observe significant improvements Gstack offers in both scenarios" and "Gstack will continue to exhibit acceleration effects even after 8T tokens"
  - [corpus]: No direct corpus evidence, but scaling laws in related work suggest predictable performance improvements with size.
- **Break condition**: If the scaling relationship breaks down at extreme sizes or if training dynamics change fundamentally with very large models.

## Foundational Learning

- **Concept: Model Growth Operators**
  - Why needed here: The paper categorizes existing growth methods into four atomic operators (Gdirect, Glearn, Gzero, Grandom) to systematically evaluate their effectiveness.
  - Quick check question: What is the key difference between Gdirect and Glearn operators in terms of how they generate new parameters?

- **Concept: Function Preservation**
  - Why needed here: Understanding whether maintaining the same function as the smaller model is necessary for effective growth is central to the paper's findings.
  - Quick check question: How does the paper demonstrate that function preservation is not essential for model growth effectiveness?

- **Concept: Scaling Laws**
  - Why needed here: The paper estimates a "stacking law" to quantify the computational savings of Gstack compared to conventional training.
  - Quick check question: According to the paper's findings, how does the computational requirement of Gstack compare to traditional scaling laws for reaching the same loss?

## Architecture Onboarding

- **Component map**: Base model (smaller, pre-trained) -> Growth operator (stacking mechanism) -> Target model (larger, continued training)
- **Critical path**: 1) Train base model with d tokens, 2) Apply Gstack operator to create target model, 3) Continue training target model with D tokens. The most critical decision points are choosing growth timing d and growth factor g.
- **Design tradeoffs**: Gstack prioritizes simplicity and universality over sophisticated strategies like multi-step growth or dynamic learning rate adjustments. This makes it broadly applicable but potentially leaves performance gains on the table compared to more complex approaches.
- **Failure signatures**: Loss spikes immediately after stacking, underperformance compared to baseline training from scratch, or diminishing returns as model size increases. The paper notes loss spikes occur right after stacking but typically recover.
- **First 3 experiments**:
  1. Implement Gstack with growth factor g=4 and growth timing d=10B tokens on a 400M base model, compare training loss and evaluation metrics against baseline.
  2. Test different growth timings (d=1B, 5B, 10B, 20B, 50B) to identify optimal growth timing for a given computational budget.
  3. Apply Gstack to progressively larger models (1.1B, 3B, 7B) to verify scalability and measure acceleration benefits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Gstack operator's performance scale when applied to models with non-standard layer counts (e.g., 24 layers vs. 32 layers)?
- Basis in paper: [explicit] The paper mentions that models with 3B and 7B parameters have identical depths, suggesting non-standard layer counts.
- Why unresolved: The paper primarily tests with growth factors of 4 and discusses optimal growth timing and factor, but does not explore performance variations with different layer counts.
- What evidence would resolve it: Experiments comparing Gstack performance across models with varying layer counts (e.g., 24, 32, 40 layers) to determine if layer count affects the operator's efficiency and effectiveness.

### Open Question 2
- Question: What are the theoretical underpinnings that explain why Gstack outperforms other growth operators and training from scratch?
- Basis in paper: [inferred] The paper notes that Gstack violates function preservation, yet outperforms other methods, and acknowledges a lack of theoretical insights into its success.
- Why unresolved: While empirical results are provided, the paper does not delve into the theoretical reasons behind Gstack's superior performance.
- What evidence would resolve it: Theoretical analysis or mathematical proofs explaining the mechanisms by which Gstack enhances training efficiency and loss reduction compared to other methods.

### Open Question 3
- Question: How does the Gstack operator perform when integrated with other advanced training techniques, such as dynamic learning rate adjustments or multi-step growth strategies?
- Basis in paper: [explicit] The paper focuses on simple operator choices and does not extensively investigate multi-step growth or dynamic modifications to the training process.
- Why unresolved: The study prioritizes simplicity and does not explore the potential synergies or trade-offs when combining Gstack with other sophisticated training strategies.
- What evidence would resolve it: Experiments evaluating Gstack in conjunction with dynamic learning rate schedules or multi-step growth, comparing performance metrics to baseline and single-step Gstack implementations.

## Limitations

- **Dataset Scope**: Experiments primarily use English NLP benchmarks, limiting generalizability to other domains like code or scientific literature.
- **Hyperparameter Sensitivity**: The paper does not provide a theoretical framework for predicting optimal growth timing and factor for arbitrary configurations.
- **Generalization to Other Architectures**: All experiments use transformer-based architectures; effectiveness for other model families remains unexplored.

## Confidence

**High Confidence**: The core finding that depthwise stacking (Gstack) accelerates training while reducing loss is well-supported by extensive experiments across multiple model sizes and training durations. The systematic categorization of growth operators provides a clear framework for understanding model growth techniques.

**Medium Confidence**: The claim that function preservation is not essential for effective growth is supported by ablation studies, but the optimal noise level (20%) may be architecture-specific. The scaling law estimates provide useful guidance but are based on empirical observations rather than theoretical derivation.

**Low Confidence**: The paper's assertion that Gstack will continue to exhibit acceleration effects "even after 8T tokens" is speculative extrapolation beyond the tested range. Similarly, the universal applicability of the identified optimal growth timing across all possible configurations remains unproven.

## Next Checks

1. **Cross-domain validation**: Apply Gstack to models trained on code (e.g., The Stack dataset) or scientific literature (e.g., ArXiv papers) and evaluate performance on domain-specific benchmarks. This would test whether the acceleration benefits generalize beyond general NLP tasks.

2. **Multi-step growth experiments**: Implement a multi-step growth strategy where models are grown incrementally (e.g., 400M → 1.1B → 3B → 7B) rather than using single large growth steps. Compare the total computational savings and final performance against the single-step approach used in the paper.

3. **Function preservation boundary testing**: Systematically vary the noise level in Gdirect initialization beyond the tested 20% range (e.g., 0%, 10%, 20%, 30%, 40%) to identify the threshold where function preservation becomes essential for stable training. This would provide clearer guidance on when strict function preservation matters.