---
ver: rpa2
title: 'Caption-Driven Explorations: Aligning Image and Text Embeddings through Human-Inspired
  Foveated Vision'
arxiv_id: '2408.09948'
source_url: https://arxiv.org/abs/2408.09948
tags:
- image
- attention
- visual
- nevaclip
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CapMIT1003, a dataset combining image captions
  with click-contingent human explorations, and presents NevaClip, a zero-shot method
  for predicting scanpaths in captioning tasks. The dataset captures human attention
  during image captioning by recording up to 10 click-based fixations per image, simulating
  foveated vision, followed by caption generation.
---

# Caption-Driven Explorations: Aligning Image and Text Embeddings through Human-Inspired Foveated Vision

## Quick Facts
- **arXiv ID:** 2408.09948
- **Source URL:** https://arxiv.org/abs/2408.09948
- **Reference count:** 21
- **Key outcome:** Introduces CapMIT1003 dataset and NevaClip method, achieving state-of-the-art scanpath plausibility for captioning and free-viewing tasks

## Executive Summary
This work introduces CapMIT1003, a dataset combining image captions with click-contingent human explorations, and presents NevaClip, a zero-shot method for predicting scanpaths in captioning tasks. The dataset captures human attention during image captioning by recording up to 10 click-based fixations per image, simulating foveated vision, followed by caption generation. NevaClip leverages CLIP models and NeVA algorithms to optimize scanpaths that align foveated visual stimuli with captions. Experiments show that NevaClip achieves state-of-the-art scanpath plausibility scores for both captioning and free-viewing tasks, outperforming baselines like random and center fixations, as well as competitor models like G-Eymol and Neva.

## Method Summary
The method involves two main components: the CapMIT1003 dataset and the NevaClip algorithm. CapMIT1003 collects click-contingent fixations from users captioning images, with up to 10 fixations per image, followed by caption generation. NevaClip uses CLIP embeddings (Resnet50 default) to extract visual and caption features, then applies the NeVA algorithm to optimize scanpaths by minimizing a loss function that balances alignment between foveated image patches and caption embeddings. The method is zero-shot, requiring no task-specific training, and adapts to different CLIP backbones. Scanpaths are evaluated using plausibility metrics comparing predicted sequences to human-derived patterns.

## Key Results
- NevaClip outperforms baselines (random, center fixations) and competitor models (G-Eymol, Neva) on scanpath plausibility scores
- Scanpaths generated using correct captions significantly outperform those using unrelated captions, validating task-specific alignment
- Substantial overlap observed between free-viewing and captioning scanpaths, especially in early exploration phases driven by bottom-up factors

## Why This Works (Mechanism)
The method works by leveraging CLIP's ability to align image and text embeddings, then using NeVA's optimization to generate scanpaths that maximize this alignment under foveated vision constraints. By minimizing the distance between foveated image patches and caption embeddings across the fixation sequence, NevaClip produces plausible task-driven attention patterns without task-specific training.

## Foundational Learning
- **CLIP embeddings (why needed):** Bridge visual and textual modalities for alignment
  - *Quick check:* Verify Resnet50 CLIP embeddings correlate with human caption-relevant features
- **Foveated vision simulation (why needed):** Models human visual attention constraints
  - *Quick check:* Compare scanpath plausibility with and without foveation
- **NeVA algorithm (why needed):** Optimizes fixation sequences under visual constraints
  - *Quick check:* Validate NeVA's ability to generate plausible scanpaths on simple test cases

## Architecture Onboarding

**Component Map:** CLIP Backbone -> NeVA Optimizer -> Scanpath Output

**Critical Path:** Image input → CLIP Resnet50 → Feature extraction → NeVA optimization → Fixation sequence → Caption alignment loss → Scanpath refinement

**Design Tradeoffs:** Zero-shot approach avoids task-specific training but may miss dataset-specific patterns; click-contingent data is easier to collect than eye-tracking but less precise

**Failure Signatures:** Poor scanpath plausibility when captions are unrelated to images; failure to converge on complex images with multiple salient regions

**First Experiments:** 1) Compare scanpath plausibility with random caption inputs 2) Test different CLIP backbones (ViT vs Resnet) 3) Validate against small subset of eye-tracking data if available

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** How do scanpaths differ between click-contingent and actual gaze data in captioning tasks?
- **Basis in paper:** [explicit] The authors note that click-contingent data collection is a limitation and suggest future work explore differences with actual gaze data.
- **Why unresolved:** The current dataset and method rely on click-contingent data rather than continuous gaze tracking, which may miss subtle fixation patterns and micro-saccades that occur during natural viewing.
- **What evidence would resolve it:** A comparative study using the same participants and images, collecting both click-contingent and eye-tracking data, then analyzing differences in scanpath patterns, fixation durations, and caption quality.

### Open Question 2
- **Question:** To what extent do early fixations in scanpaths depend on bottom-up saliency versus top-down task demands in captioning?
- **Basis in paper:** [inferred] The authors observe substantial overlap between free-viewing and captioning tasks, particularly in early exploration phases driven by bottom-up factors.
- **Why unresolved:** While the current experiments show overlap, they don't quantify the relative contribution of bottom-up versus top-down factors at different fixation stages or how this varies across image types and caption complexity.
- **What evidence would resolve it:** Controlled experiments manipulating image complexity and caption difficulty, measuring fixation patterns with saliency models and task-specific predictors, and using regression or causal analysis to quantify each factor's contribution.

### Open Question 3
- **Question:** How does the choice of CLIP backbone architecture affect the quality of task-driven scanpath predictions?
- **Basis in paper:** [explicit] The authors use Resnet50 as the default backbone and mention adapting the Neva algorithm to run on CLIP backbones, but don't compare different architectures.
- **Why unresolved:** The performance differences between CLIP variants (Resnet50, ViT-B/32, ViT-L/14) on task-driven attention modeling remain unexplored, despite CLIP's known sensitivity to architectural choices.
- **What evidence would resolve it:** Systematic evaluation of NevaClip using multiple CLIP backbones on the CapMIT1003 dataset, comparing scanpath plausibility scores and analyzing whether certain architectures better capture task-relevant visual features.

## Limitations
- Click-contingent data may not fully capture natural gaze dynamics and micro-saccades
- Limited comparison against task-specific baselines beyond basic random/center fixes
- Reliance on CLIP models introduces potential domain-specific biases

## Confidence
**Major Claims and Confidence:**
- **Dataset validity and utility (Medium):** CapMIT1003 provides novel task-specific attention data, but click-contingent data may not fully capture natural viewing behavior
- **NevaClip effectiveness (Medium):** Outperforms basic baselines but needs comparison with more sophisticated task-specific models
- **Task overlap hypothesis (Medium):** Supported for early phases but not comprehensively validated across entire scanpaths

## Next Checks
1. Compare NevaClip performance against a strong task-specific baseline trained on gaze data from captioning tasks to better establish relative effectiveness
2. Conduct ablation studies removing the caption input to quantify its specific contribution versus visual features alone
3. Validate findings on an independent dataset with actual eye-tracking data (rather than click-contingent data) to assess generalizability across data collection methods