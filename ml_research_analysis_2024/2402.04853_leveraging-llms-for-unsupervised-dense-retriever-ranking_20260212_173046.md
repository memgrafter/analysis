---
ver: rpa2
title: Leveraging LLMs for Unsupervised Dense Retriever Ranking
arxiv_id: '2402.04853'
source_url: https://arxiv.org/abs/2402.04853
tags:
- dense
- target
- query
- queries
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of selecting the most effective
  dense retriever for a target corpus when no relevance labels or queries from that
  corpus are available. The authors propose LARMOR, a novel method that leverages
  large language models (LLMs) to generate pseudo-relevant queries, labels, and reference
  lists from the target corpus itself.
---

# Leveraging LLMs for Unsupervised Dense Retriever Ranking

## Quick Facts
- **arXiv ID:** 2402.04853
- **Source URL:** https://arxiv.org/abs/2402.04853
- **Reference count:** 40
- **One-line primary result:** First unsupervised, query-free method to select dense retrievers for target corpora, achieving 55.24% average nDCG@10 on 13 BEIR collections.

## Executive Summary
This paper introduces LARMOR, a novel approach for selecting the most effective dense retriever for a target corpus when no relevance labels or queries from that corpus are available. LARMOR leverages large language models (LLMs) to generate synthetic queries, pseudo-relevance judgments, and reference lists from the target corpus itself. These synthetic signals are then used to rank dense retrievers based on their estimated performance. Evaluated on 13 BEIR collections and a pool of 47 state-of-the-art dense retrievers, LARMOR achieves an average nDCG@10 of 55.24%, outperforming existing baselines by 10.95% and closely matching the theoretical upper bound of 57.32%.

## Method Summary
LARMOR uses LLMs to generate synthetic queries and relevance signals from a target corpus, then ranks candidate dense retrievers based on their performance on these synthetic signals. The method generates pseudo-relevant queries for documents in the target corpus, retrieves candidate document lists from all dense retrievers, fuses these lists using Reciprocal Rank Fusion (RRF), and generates pseudo-relevance judgments and reference lists. Finally, it ranks dense retrievers using these synthetic signals and fuses the rankings to produce the final output.

## Key Results
- LARMOR achieves an average nDCG@10 of 55.24% across 13 BEIR collections.
- Outperforms existing baselines by 10.95% in nDCG@10.
- Closely matches the theoretical upper bound of 57.32% nDCG@10.
- First unsupervised, query-free approach for dense retriever selection.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating synthetic queries from the target corpus compensates for the lack of real user queries in unsupervised dense retriever selection.
- Mechanism: The LLM is prompted with domain-specific templates to produce queries that mimic real user intent for each document. These pseudo-queries serve as test inputs to rank the candidate dense retrievers.
- Core assumption: The LLM can generate representative queries that reflect the semantic space of the target corpus without seeing real user queries.
- Evidence anchors:
  - [abstract] "Specifically, we address the challenge posed by the absence of queries by using LLMs to generate synthetic queries for a (subset of the) target corpus."
  - [section 4.1] "Once the subset ùê∑‚Ä≤ T is built, each document from ùê∑‚Ä≤ T is passed to LARMOR‚Äôs LLM, accompanied by a domain-specific query generation promptùëÉùëûùëî, to generate a set of pseudo-relevant queries eùëÑ T specific to the target collection"
- Break condition: If the generated queries do not cover the true semantic variance of the target corpus, the rankings will be unreliable.

### Mechanism 2
- Claim: Fusing rankings from multiple synthetic queries and documents provides a more stable evaluation signal than single-query judgments.
- Mechanism: For each generated query, rankings from all candidate retrievers are fused (using Reciprocal Rank Fusion) to produce a consensus ranking. The fused ranking is then used to generate pseudo-relevance judgments or reference lists for evaluation.
- Core assumption: Rank fusion reduces noise and bias from individual retriever outputs, yielding a more representative evaluation baseline.
- Evidence anchors:
  - [section 4.2] "A rank fusion algorithm is employed to merge all the rankings for Àúùëûùëëùëñ,ùëó, resulting in a single fused document ranking from which we select only the top-ùëö documents"
  - [section 5.2] "For the fusion algorithm used in LARMOR... we opt for Reciprocal Rank Fusion (RRF)"
- Break condition: If the fusion fails to represent the true retrieval quality‚Äîe.g., due to uniform low performance across retrievers‚Äîthe evaluation becomes meaningless.

### Mechanism 3
- Claim: Combining pseudo-relevance judgments and pseudo-reference lists via ranking fusion yields the most accurate dense retriever ranking.
- Mechanism: Two independent rankings of retrievers are produced‚Äîone based on synthetic judgments, the other on synthetic reference lists. These rankings are merged (again via RRF) to produce the final selection output.
- Core assumption: Different types of synthetic signals capture complementary aspects of retrieval quality, and their combination improves robustness.
- Evidence anchors:
  - [section 4.3] "To rank DRs for a target collection with the pseudo-relevance judgements... evaluate each of these rankings using the target evaluation measure E... We then average the evaluation values... Subsequently we rank DRs in descending order of eE."
  - [section 6.2] "Our whole pipeline LARMOR achieved the overall best performance by fusing QFJ and QFR"
- Break condition: If either pseudo-judgments or pseudo-reference lists are systematically biased, the fusion will inherit and compound that bias.

## Foundational Learning

- Concept: Query Performance Prediction (QPP)
  - Why needed here: QPP methods traditionally predict retrieval quality for real queries; LARMOR adapts these ideas to synthetic query sets, enabling unsupervised evaluation.
  - Quick check question: What distinguishes QPP from direct evaluation in the context of dense retriever selection?

- Concept: Rank Fusion (Reciprocal Rank Fusion, RRF)
  - Why needed here: RRF is used to merge document rankings from multiple retrievers for synthetic queries, and later to merge DR rankings from different signal types.
  - Quick check question: How does RRF differ from score-based fusion, and why is it preferred when score distributions vary across models?

- Concept: Prompt Engineering for LLMs
  - Why needed here: Domain-specific prompts are critical for generating high-quality synthetic queries and judgments; poor prompts lead to unrepresentative signals.
  - Quick check question: What role do the query type and document type specifications play in the prompt templates?

## Architecture Onboarding

- Component map:
  - Query Generation -> Rank Fusion -> Pseudo-Judgments / Pseudo-Reference Lists -> DR Ranking -> Final Fusion

- Critical path:
  1. Sample documents from target corpus.
  2. Generate synthetic queries via LLM.
  3. For each query, retrieve candidate document lists from all DRs.
  4. Fuse these lists to create a representative set.
  5. Generate synthetic judgments or re-rank to create reference lists.
  6. Evaluate DRs using judgments or RBO against reference lists.
  7. Merge the two rankings to produce final DR ranking.

- Design tradeoffs:
  - Using synthetic queries avoids needing real user data but risks mismatch with actual query semantics.
  - Generating multiple queries per document increases coverage but raises computational cost.
  - Fusing multiple signal types improves robustness but may dilute strong individual signals.

- Failure signatures:
  - Kendall Tau drops sharply when synthetic queries fail to match corpus semantics.
  - Œîùëí spikes if pseudo-judgments are too lenient or too strict compared to human judgments.
  - Overfitting to synthetic signals can cause poor real-world performance; this shows up as low transferability to held-out queries.

- First 3 experiments:
  1. Run LARMOR on a small subset of BEIR with FlanT5-large to confirm pipeline execution and measure baseline Kendall Tau.
  2. Swap FlanT5-large for FlanT5-XXL and measure the change in Kendall Tau and Œîùëí to isolate LLM size impact.
  3. Disable pseudo-reference list generation and use only pseudo-judgments; compare final ranking quality to full LARMOR.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM architectures (e.g., transformer variants, attention mechanisms) impact the effectiveness of LARMOR compared to the FLAN-T5 backbone used in this study?
- Basis in paper: [inferred] The paper mentions using FLAN-T5 consistently and suggests exploring advanced open-source LLMs like Mistral and Llama3 in future work, implying potential performance differences with other architectures.
- Why unresolved: The paper only uses FLAN-T5 for all experiments, leaving the comparative performance of other LLM architectures unexplored.
- What evidence would resolve it: A controlled experiment comparing LARMOR's performance using different LLM architectures (e.g., GPT-3.5, GPT-4, Mistral, Llama3) while keeping other components constant.

### Open Question 2
- Question: What is the optimal number of generated queries per document and how does this impact LARMOR's performance across different target corpora?
- Basis in paper: [explicit] Section 6.4 investigates the effect of the number of generated queries per document, finding that more queries tend to be beneficial for Kendall Tau performance, especially for FLAN-T5-XXL, but the impact varies for Œîùëí scores.
- Why unresolved: The paper only tests a limited range of query numbers (1, 3, 5, 10) and finds marginal improvements beyond 10 for some models, suggesting the optimal number might be corpus-specific.
- What evidence would resolve it: A systematic study testing a wider range of query numbers per document across all target corpora and reporting both Kendall Tau and Œîùëí scores to identify the optimal number for each corpus.

### Open Question 3
- Question: How does the quality of LLM-generated pseudo-relevant queries and judgments compare to human-annotated labels in terms of DR selection effectiveness?
- Basis in paper: [inferred] The paper relies on LLM-generated pseudo-relevant signals, but does not compare their quality to human-annotated labels. The discussion mentions that collecting human relevance judgments is costly and time-consuming, implying a need to understand the trade-off.
- Why unresolved: The paper does not conduct a direct comparison between LLM-generated signals and human-annotated labels, leaving the relative quality of the generated signals uncertain.
- What evidence would resolve it: A study where LARMOR is run using both LLM-generated signals and human-annotated labels (where available) on the same target corpora, comparing the DR selection performance using metrics like Kendall Tau and Œîùëí.

## Limitations
- The approach relies on LLMs' ability to generate queries and relevance signals that are representative of the target corpus's true retrieval needs, which may not always be the case.
- The evaluation is based on 13 BEIR collections, which may not generalize to all domains, especially those with highly specialized or domain-specific query patterns.
- The fusion of multiple synthetic signals, though intended to increase robustness, may compound biases if the underlying signals are systematically flawed.

## Confidence
- **High confidence**: The overall pipeline structure and the use of LLMs for query generation and synthetic signal creation are well-defined and supported by the ablation study.
- **Medium confidence**: The claim that LARMOR closely matches the theoretical upper bound of 57.32% nDCG@10 is supported by results, but the upper bound itself is not clearly defined in the paper.
- **Low confidence**: The generalizability of LARMOR to domains with highly specialized or domain-specific query patterns is not fully validated.

## Next Checks
1. **Cross-domain validation**: Test LARMOR on a broader set of domains, including those with highly specialized or domain-specific query patterns, to assess generalizability.
2. **Signal quality assessment**: Compare the synthetic queries and relevance signals generated by LARMOR to real user queries and relevance judgments (if available) to evaluate alignment.
3. **Impact of LLM size**: Conduct experiments varying the LLM size (e.g., FlanT5-large vs. FlanT5-XXL) to quantify the impact on performance and resource efficiency.