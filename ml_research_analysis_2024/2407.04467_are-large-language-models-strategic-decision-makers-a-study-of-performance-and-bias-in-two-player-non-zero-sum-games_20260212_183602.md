---
ver: rpa2
title: Are Large Language Models Strategic Decision Makers? A Study of Performance
  and Bias in Two-Player Non-Zero-Sum Games
arxiv_id: '2407.04467'
source_url: https://arxiv.org/abs/2407.04467
tags:
- label
- both
- player
- points
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigates systematic biases in large language models
  (LLMs) when making strategic decisions in two-player non-zero-sum games, specifically
  Stag Hunt and Prisoner''s Dilemma. It identifies three types of biases: positional
  bias (preference for the order of action labels), payoff bias (preference for actions
  leading to maximum possible gains), and behavioural bias (preference based on prompted
  player behaviors).'
---

# Are Large Language Models Strategic Decision Makers? A Study of Performance and Bias in Two-Player Non-Zero-Sum Games

## Quick Facts
- arXiv ID: 2407.04467
- Source URL: https://arxiv.org/abs/2407.04467
- Reference count: 40
- Large language models exhibit systematic biases in strategic decision-making, leading to performance drops in non-zero-sum games.

## Executive Summary
This study investigates systematic biases in large language models (LLMs) when making strategic decisions in two-player non-zero-sum games, specifically Stag Hunt and Prisoner's Dilemma. The research identifies three types of biases: positional bias (preference for the order of action labels), payoff bias (preference for actions leading to maximum possible gains), and behavioral bias (preference based on prompted player behaviors). Experiments with GPT-3.5, GPT-4-Turbo, GPT-4o, and Llama-3-8B reveal that all models exhibit at least one bias, causing performance drops when game configurations are misaligned with these biases. The study finds that chain-of-thought prompting reduces biases in most models but increases them in GPT-4-Turbo, indicating it is not a universal solution. These findings highlight the importance of considering systematic biases when evaluating LLMs in game-theoretic tasks.

## Method Summary
The study designed 16 experimental setups for both Stag Hunt and Prisoner's Dilemma games, varying the order of actions, payoff values, and player behaviors in the prompts. Four LLMs (GPT-3.5, GPT-4-Turbo, GPT-4o, and Llama-3-8B) were tested using both Answer-Only and Chain-of-Thought prompting methods. Each setup was run 100 times to measure the frequency of action selections and calculate performance drops when models were misaligned with game configurations. The experiments measured how often models chose optimal strategies and identified three types of biases affecting their decisions.

## Key Results
- All four tested models (GPT-3.5, GPT-4-Turbo, GPT-4o, and Llama-3-8B) exhibit at least one systematic bias in strategic decision-making.
- Performance drops average 32%, 25%, 34%, and 29% respectively in Stag Hunt, and 28%, 16%, 34%, and 24% respectively in Prisoner's Dilemma when misaligned with model biases.
- GPT-4o, despite being a top-performing model on standard benchmarks, suffers the most substantial performance drop (34% in Stag Hunt, 34% in Prisoner's Dilemma).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs do not fully rely on logical reasoning when making strategic decisions, instead being influenced by systematic biases.
- Mechanism: The models are affected by positional, payoff, and behavioral biases that guide their decision-making process, causing performance drops when game configurations are misaligned with these biases.
- Core assumption: LLMs have inherent biases that influence their decision-making, regardless of the underlying logic of the task.
- Evidence anchors:
  - [abstract] "This indicates that LLMs do not fully rely on logical reasoning when making these strategic decisions."
  - [section 4] "It can be seen that all models, except GPT-4-Turbo, are affected less by the biases when using CoT prompting."
  - [corpus] Weak evidence; corpus provides general AI decision-making studies but not specific LLM bias mechanisms in games.
- Break condition: If models consistently ignore bias patterns and rely purely on payoff maximization logic, this mechanism would fail.

### Mechanism 2
- Claim: Chain-of-thought prompting reduces biases in most models but increases them in GPT-4-Turbo.
- Mechanism: CoT prompting enhances reasoning capabilities in some models, reducing the impact of biases, but in GPT-4-Turbo it amplifies existing biases, leading to greater performance drops.
- Core assumption: CoT prompting's effect on bias is model-dependent, not universal.
- Evidence anchors:
  - [abstract] "Interestingly, we found that a commonly used method of improving the reasoning capabilities of LLMs, chain-of-thought (CoT) prompting, reduces the biases in GPT-3.5, GPT-4o, and Llama-3-8B but increases the effect of the bias in GPT-4-Turbo."
  - [section 4] "It's worth noting that employing CoT prompting lessens the performance drop due to bias misalignment by 31.1%, 6.93%, and 45.39% in GPT-3.5, GPT-4o, and Llama-3-8B respectively, and increases the effect of the bias in GPT-4-Turbo by 247.22%."
  - [corpus] No direct evidence; corpus focuses on AI bias broadly but not CoT-specific effects.
- Break condition: If CoT prompting consistently reduces biases across all models, this mechanism would be invalid.

### Mechanism 3
- Claim: Fine-tuning LLMs reduces the average significance of biases but may introduce new biases.
- Mechanism: Instruction fine-tuning can mitigate some biases but also alters the model's decision-making process, potentially introducing new biases like the behavioral bias in Llama-3-8B-Instruct.
- Core assumption: Fine-tuning changes the model's inherent biases but does not eliminate them entirely.
- Evidence anchors:
  - [section 5] "Our results show that fine-tuning LLMs reduces the average significance of the biases, however, they also show that fine-tuning increases the behavioural bias in Llama-3-8b (where it was null to begin with), indicating fine-tuning may not be a sufficient remedy for this issue."
  - [section 5] "However, the overall effect of the biases is weakened when the LLM is fine-tuned."
  - [corpus] Weak evidence; corpus mentions AI bias but not specific fine-tuning effects on LLM game-theoretic biases.
- Break condition: If fine-tuning consistently eliminates all biases without introducing new ones, this mechanism would fail.

## Foundational Learning

- Concept: Game Theory
  - Why needed here: Understanding game theory is crucial for analyzing strategic decision-making in two-player non-zero-sum games like Stag Hunt and Prisoner's Dilemma.
  - Quick check question: What is a Nash Equilibrium, and how does it apply to the games studied in this paper?

- Concept: Large Language Models (LLMs)
  - Why needed here: LLMs are the subject of the study, and understanding their capabilities and limitations is essential for interpreting the results.
  - Quick check question: What are the key differences between GPT-3.5, GPT-4-Turbo, GPT-4o, and Llama-3-8B, and how might these differences affect their performance in strategic decision-making tasks?

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: CoT prompting is a method used to improve reasoning capabilities in LLMs, and its effects on bias are a key focus of the study.
  - Quick check question: How does CoT prompting differ from Answer-Only prompting, and why might it have different effects on bias in different models?

## Architecture Onboarding

- Component map: LLMs (GPT-3.5, GPT-4-Turbo, GPT-4o, Llama-3-8B) -> Game-theoretic tasks (Stag Hunt, Prisoner's Dilemma) -> Prompting methods (Answer-Only, CoT) -> Bias analysis (positional, payoff, behavioral)
- Critical path: 1) Design prompts for game-theoretic tasks, 2) Run experiments with different LLMs and prompting methods, 3) Analyze results for biases and performance drops, 4) Interpret findings and draw conclusions
- Design tradeoffs: Balancing the complexity of game-theoretic tasks with the interpretability of results; choosing appropriate prompting methods to reveal biases without overwhelming the models
- Failure signatures: If models consistently perform well regardless of game configuration, it may indicate a lack of bias. If all models show identical bias patterns, it may suggest a universal flaw in the prompting approach.
- First 3 experiments:
  1. Test each LLM with Answer-Only prompting on both games to establish baseline performance and identify initial biases.
  2. Repeat the experiments with CoT prompting to compare the effects on bias across models.
  3. Analyze the alignment of model performance with game configurations to quantify the impact of biases on decision-making.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the positional bias observed in LLMs stem from token-level processing (like prefix bias) or higher-level structural processing (like syntactic/semantic ordering)?
- Basis in paper: Inferred from the observation that positional bias affects models differently (GPT-3.5 prefers first position, GPT-4-Turbo prefers second position), suggesting it's not simply a token-level artifact
- Why unresolved: The paper identifies positional bias but doesn't investigate its underlying mechanism or whether it operates at the token, phrase, or structural level
- What evidence would resolve it: Experiments testing positional bias with semantically equivalent but structurally different prompts (e.g., bullet vs code vs natural language formats) to isolate whether the bias is token-based or structure-based

### Open Question 2
- Question: Can fine-tuning strategies be developed to specifically mitigate systematic biases in LLMs without introducing new biases or degrading performance on other tasks?
- Basis in paper: Explicit finding that fine-tuning reduced overall bias magnitude in Llama-3-8b but introduced a new behavioral bias, suggesting fine-tuning is not a simple solution
- Why unresolved: The paper shows fine-tuning has mixed effects but doesn't explore whether targeted fine-tuning approaches could selectively reduce biases while preserving reasoning capabilities
- What evidence would resolve it: Comparative studies of different fine-tuning approaches (curriculum learning, bias-aware loss functions, targeted data augmentation) measuring both bias reduction and task performance

### Open Question 3
- Question: How do systematic biases in LLMs compare to cognitive biases in human decision-making in game-theoretic scenarios, and what does this reveal about the nature of LLM reasoning?
- Basis in paper: Explicit comparison showing GPT-4o has much stronger positional bias than humans in similar Stag Hunt tasks, suggesting fundamental differences in decision-making processes
- Why unresolved: The paper provides initial comparison but doesn't explore whether LLM biases map to known human cognitive biases or represent fundamentally different reasoning limitations
- What evidence would resolve it: Systematic mapping of LLM biases to established human cognitive biases (anchoring, availability heuristic, etc.) and investigation of whether LLM biases can be reduced through approaches inspired by human cognitive debiasing techniques

## Limitations

- The study is limited to only two specific game types (Stag Hunt and Prisoner's Dilemma), which may not generalize to other strategic scenarios.
- Temperature was held constant at 1.0 for most experiments, potentially masking more nuanced bias patterns that might emerge at different temperatures.
- The study does not explore the relationship between bias strength and model size beyond comparing four specific models.

## Confidence

- **High confidence**: The identification of positional, payoff, and behavioral biases in LLMs is well-supported by consistent experimental results across multiple models and games.
- **Medium confidence**: The claim that GPT-4o suffers the most substantial performance drop despite being a top-performing model on standard benchmarks.
- **Medium confidence**: The assertion that chain-of-thought prompting reduces biases in most models but increases them in GPT-4-Turbo.

## Next Checks

1. **Temperature Sensitivity Analysis**: Re-run the experiments across a wider temperature range (0.0 to 1.5 in 0.1 increments) for each model to determine if the observed biases persist or change in magnitude at different temperature settings.

2. **Generalization to Additional Games**: Test the same experimental framework with additional non-zero-sum games such as Battle of the Sexes and Chicken to determine whether the identified biases are specific to Stag Hunt and Prisoner's Dilemma or represent a more general pattern in LLM strategic decision-making.

3. **Fine-tuning Impact Study**: Conduct a controlled experiment where models are fine-tuned on a diverse set of strategic game outcomes to assess whether this training regime can systematically reduce positional, payoff, and behavioral biases, and whether new biases emerge from the fine-tuning process itself.