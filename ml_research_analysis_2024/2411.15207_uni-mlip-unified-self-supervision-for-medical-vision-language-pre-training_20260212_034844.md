---
ver: rpa2
title: 'Uni-Mlip: Unified Self-supervision for Medical Vision Language Pre-training'
arxiv_id: '2411.15207'
source_url: https://arxiv.org/abs/2411.15207
tags:
- medical
- image
- uni-mlip
- text
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of limited multimodal medical
  data by introducing Uni-Mlip, a unified self-supervision framework designed to enhance
  medical vision-language pre-training. Uni-Mlip integrates cross-modal, uni-modal,
  and fused-modal self-supervision techniques at both data-level and feature-level,
  with tailored adaptations for the unique characteristics of medical images.
---

# Uni-Mlip: Unified Self-supervision for Medical Vision Language Pre-training

## Quick Facts
- arXiv ID: 2411.15207
- Source URL: https://arxiv.org/abs/2411.15207
- Authors: Ameera Bawazir; Kebin Wu; Wenbin Li
- Reference count: 40
- Key outcome: State-of-the-art performance across medical image-text retrieval, classification, and VQA tasks using unified self-supervision framework

## Executive Summary
Uni-Mlip addresses the challenge of limited multimodal medical data by introducing a unified self-supervision framework for medical vision-language pre-training. The method integrates cross-modal, uni-modal, and fused-modal self-supervision techniques at both data-level and feature-level, with tailored adaptations for medical images. Uni-Mlip combines contrastive learning, feature perturbation, and masked language modeling to effectively align image and text modalities. Experimental results demonstrate superior performance over state-of-the-art methods across three key downstream tasks: image-text retrieval (ROCO dataset: 25.9% Recall@1 for I2T, 24.5% for T2I), image classification (MIMIC, CXP, NIH datasets: AUC scores up to 82.9%), and visual question answering (VQA-RAD and SLAKE datasets: overall accuracy up to 81.90%).

## Method Summary
Uni-Mlip is a unified self-supervision framework designed to enhance medical vision-language pre-training by integrating cross-modal, uni-modal, and fused-modal self-supervision techniques at both data-level and feature-level. The framework adapts to the unique characteristics of medical images by incorporating contrastive learning, feature perturbation, and masked language modeling. Specifically, Uni-Mlip employs image-text contrastive learning (ITC) to align image and text embeddings, uni-modal image-to-image contrastive learning (LI2I) to enhance image representations, and multimodal fusion with masked language modeling (LMLM) to improve text understanding. Feature perturbation is introduced at the representation level to boost cross-modal alignment. The method uses frozen batch normalization during SimCLR integration to preserve absolute intensity information critical for medical images.

## Key Results
- Image-text retrieval: Achieves 25.9% Recall@1 and 60.7% Recall@5 for image-to-text retrieval on ROCO dataset
- Image classification: Achieves AUC scores up to 82.9% across MIMIC, CXP, and NIH datasets
- Visual question answering: Achieves 81.90% overall accuracy on VQA-RAD and SLAKE datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Freezing batch normalization statistics during SimCLR training improves medical image representation learning.
- Mechanism: Medical images rely on absolute intensity values rather than relative intensity relationships. Batch normalization layers normalize based on batch statistics, which can be significantly altered by strong augmentations used in SimCLR. By freezing the mean and variance in batch normalization layers while keeping weights and biases trainable, the model preserves the absolute intensity information critical for medical diagnosis.
- Core assumption: The mean and variance statistics learned from weakly augmented data are more representative of medical image characteristics than those from strongly augmented data.
- Evidence anchors:
  - [section] "When the vision encoder, such as a ResNet, includes batch normalization layers, strong image augmentations will significantly alter the mean and variance statistics in batch normalization layers, harming the intensity information critical for medical images."
  - [section] "To address this issue, we propose to train without SimCLR for some epochs first, and then we freeze the mean and variance in the batch normalization layers while keeping the weight and bias as learnable parameters when incorporating the SimCLR loss."

### Mechanism 2
- Claim: Combining cross-modal, uni-modal, and fused-modal self-supervision at both data and feature levels creates more robust medical vision-language representations.
- Mechanism: The framework integrates multiple self-supervision signals: cross-modal contrastive learning aligns image and text embeddings (data-level), feature perturbation adds robustness to the alignment (feature-level), uni-modal contrastive learning enhances image representations independently (data-level), and masked language modeling with multimodal fusion improves text understanding (data-level). This multi-faceted approach captures different aspects of the medical domain that single-objective methods miss.
- Core assumption: Each self-supervision component addresses a different aspect of the representation learning challenge, and their combination is synergistic rather than redundant.
- Evidence anchors:
  - [abstract] "Uni-Mlip seamlessly integrates cross-modality, uni-modality, and fused-modality self-supervision techniques at the data-level and the feature-level."
  - [section] "Our experiments across datasets of varying scales demonstrate that Uni-Mlip significantly surpasses current state-of-the-art methods in three key downstream tasks: image-text retrieval, image classification, and visual question answering (VQA)."
  - [section] "Incorporating cross-modal self-supervision at the feature level yields notable performance gains of 1.0 and 1.6 in the I2T retrieval task, and 1.6 and 1.7 in the T2I retrieval task for LI2TC and LITC respectively. Additionally, integrating the LI2I objective significantly boosts performance by 3.5 and 2.3 in the I2T and T2I tasks, respectively."

### Mechanism 3
- Claim: Feature-level self-supervision through perturbation creates more robust cross-modal alignment than data-level augmentation alone.
- Mechanism: By applying dropout/DropBlock to features after the encoder but before the projection layer, the model learns to maintain semantic alignment between image and text representations even when local feature patterns are disrupted. This forces the model to rely on more global and robust features for cross-modal matching.
- Core assumption: Perturbing features at the representation level creates more challenging alignment tasks than perturbing inputs at the data level, leading to stronger learned representations.
- Evidence anchors:
  - [section] "To further boost the cross-modality alignment, we also introduce a feature-level self-supervision by performing a feature perturbation."
  - [section] "The goal of the ITC loss is to maximize the similarity between the embedding of the B image-text pairs i.e.,(Ii,Ti) while minimizing the similarity with the rest of the B2 − B non-pair samples i.e.,(Ij,Ti) in the batch, where i ̸= j."
  - [section] "Incorporating cross-modal self-supervision at the feature level yields notable performance gains of 1.0 and 1.6 in the I2T retrieval task, and 1.6 and 1.7 in the T2I retrieval task for LI2TC and LITC respectively."

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: Uni-Mlip uses contrastive learning objectives (ITC, LI2I) to align representations between modalities and within modalities. Understanding the InfoNCE formulation is crucial for grasping how the model distinguishes positive pairs from negative pairs.
  - Quick check question: In the InfoNCE loss, what is the mathematical relationship between the temperature parameter τ and the model's sensitivity to hard negative samples?

- Concept: Batch normalization mechanics
  - Why needed here: The key innovation of freezing batch normalization statistics during SimCLR training requires understanding how batch normalization works, including the difference between training and inference modes, and how batch statistics are computed and used.
  - Quick check question: What specific components of batch normalization are frozen during the SimCLR training phase in Uni-Mlip, and which components remain trainable?

- Concept: Masked language modeling (MLM)
  - Why needed here: The fused-modal self-supervision uses MLM as part of the training objective. Understanding how MLM works, including the masking strategy and the prediction objective, is essential for implementing and debugging this component.
  - Quick check question: In Uni-Mlip's MLM implementation, what percentage of tokens are masked, and what special token is used to replace masked tokens?

## Architecture Onboarding

- Component map:
  - Vision encoder (ResNet50 variant) -> Text encoder (PubMedBERT) -> Projection heads -> Fusion module (4-layer transformer) -> Feature perturbation modules -> Loss computation modules

- Critical path:
  1. Input images and text are encoded separately
  2. Features are perturbed for feature-level self-supervision
  3. Features are projected to shared embedding space
  4. Cross-modal and uni-modal contrastive losses are computed
  5. Fused features are passed through transformer for MLM
  6. Total loss is aggregated and backpropagated

- Design tradeoffs:
  - Using frozen batch normalization sacrifices some flexibility in feature normalization for preserving absolute intensity information
  - The multi-objective training approach increases complexity but provides complementary learning signals
  - The choice of ResNet50 and PubMedBERT balances model capacity with computational efficiency

- Failure signatures:
  - If batch normalization is not properly frozen, performance will degrade significantly on medical tasks (observed in ablation study)
  - If feature perturbation rates are too high, the model may fail to learn meaningful alignments
  - If the MLM masking rate is too aggressive, the fusion module may not receive sufficient context

- First 3 experiments:
  1. Implement the basic CLIP architecture with ITC loss only and verify it can learn cross-modal alignment on ROCO dataset
  2. Add the frozen batch normalization modification and verify it improves performance over the baseline CLIP model on medical image classification
  3. Add feature perturbation self-supervision and measure the incremental improvement in retrieval tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does freezing batch normalization layers during the integration of image-only self-supervised learning affect performance in other medical imaging modalities beyond chest X-rays?
- Basis in paper: [explicit] The paper discusses the importance of freezing batch normalization layers when incorporating image-only contrastive learning (SimCLR) into CLIP-based models for medical images, specifically noting that this approach preserves the absolute intensity values critical for medical imaging.
- Why unresolved: The study primarily focuses on chest X-rays, and while the authors mention that medical images rely on absolute intensity values, they do not explore whether this freezing technique is equally effective for other modalities like MRI, CT scans, or histopathology images.
- What evidence would resolve it: Comparative experiments showing the performance of Uni-Mlip with and without frozen batch normalization across multiple medical imaging modalities, including non-chest X-ray data, would clarify the generalizability of this technique.

### Open Question 2
- Question: Can Uni-Mlip's unified self-supervision framework be effectively adapted for non-medical domains with limited multimodal data, such as satellite imagery or agriculture analysis?
- Basis in paper: [explicit] The authors suggest that Uni-Mlip holds potential for domains with limited multimodal data, mentioning satellite and agriculture imagery analysis as possible applications.
- Why unresolved: While the paper demonstrates superior performance in medical tasks, it does not provide empirical evidence or experiments to support the framework's effectiveness in non-medical domains.
- What evidence would resolve it: Empirical results from applying Uni-Mlip to satellite imagery or agriculture analysis tasks, comparing its performance to existing models, would validate its adaptability to non-medical domains.

### Open Question 3
- Question: What is the impact of different feature perturbation techniques (e.g., Dropout vs. DropBlock) on the cross-modal alignment performance in medical vision-language pre-training?
- Basis in paper: [explicit] The paper mentions the use of DropBlock for image feature perturbation and Dropout for text feature perturbation but does not explore alternative perturbation methods or their comparative effectiveness.
- Why unresolved: The study uses specific perturbation techniques without investigating whether other methods could yield better or comparable results in aligning medical image and text modalities.
- What evidence would resolve it: Comparative experiments testing various feature perturbation techniques (e.g., Gaussian noise, adversarial perturbations) on the cross-modal alignment task would determine the optimal perturbation strategy for medical VLP.

## Limitations

- Limited generalizability to other medical imaging modalities beyond chest X-rays
- Requires substantial computational resources (8 GPUs, batch size 768)
- Specific augmentation parameters for SimCLR component are not fully specified

## Confidence

- Cross-modal alignment improvements: High confidence
- Uni-modal contrastive learning benefits: Medium confidence
- Overall framework superiority: Medium confidence

## Next Checks

1. Ablation of frozen batch normalization across modalities: Test the frozen BN strategy on CT scans and pathology images to verify if the performance benefits extend beyond chest X-rays, using datasets like DeepLesion or TCIA collections.

2. Scaling behavior analysis: Conduct experiments with varying pre-training dataset sizes (1K, 10K, 100K, 1M pairs) to quantify the framework's effectiveness at different data scales and identify potential saturation points.

3. Zero-shot transfer evaluation: Assess Uni-Mlip's zero-shot performance on external medical datasets without fine-tuning to measure the quality of learned representations and compare against supervised baselines.