---
ver: rpa2
title: 'HGOE: Hybrid External and Internal Graph Outlier Exposure for Graph Out-of-Distribution
  Detection'
arxiv_id: '2407.21742'
source_url: https://arxiv.org/abs/2407.21742
tags:
- graph
- outliers
- data
- detection
- outlier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of out-of-distribution (OOD) detection
  for graph data. Unlike images, graph data exhibits greater diversity but lower robustness
  to perturbations, complicating the integration of outliers.
---

# HGOE: Hybrid External and Internal Graph Outlier Exposure for Graph Out-of-Distribution Detection

## Quick Facts
- arXiv ID: 2407.21742
- Source URL: https://arxiv.org/abs/2407.21742
- Reference count: 40
- Primary result: Hybrid approach combining external and internal graph outliers improves OOD detection with up to 5.29% AUC gain

## Executive Summary
This paper addresses the challenge of out-of-distribution (OOD) detection for graph data by proposing HGOE, a framework that combines external graph outliers with internally synthesized outliers. Unlike images, graph data is more diverse but less robust to perturbations, making traditional outlier integration difficult. HGOE uses external outliers from public graph databases and synthesizes internal outliers between in-distribution (ID) subgroups using a graphon-based ID-mixup strategy. The framework introduces a boundary-aware OE loss that adaptively weights outliers based on their proximity to the ID boundary, maximizing the use of high-quality OOD samples while minimizing the impact of low-quality ones.

## Method Summary
HGOE is a hybrid framework for graph OOD detection that combines external outliers from public graph databases with internally synthesized outliers. The method first clusters ID graphs into subgroups using GraphCL feature extraction, then estimates graphons for each subgroup to enable ID-mixup synthesis between subgroups. The boundary-aware OE loss assigns higher weights to outliers near the ID boundary based on their scores relative to a threshold œÑ (set to the minimum ID score). This hybrid approach addresses the challenges of acquiring high-quality graph outliers (C1) and effectively utilizing them (C2) while improving detection of subtle distribution shifts.

## Key Results
- HGOE achieves up to 5.29% AUC improvement over state-of-the-art baselines on 8 real-world graph datasets
- Combining external and internal outliers outperforms using either source alone
- The boundary-aware OE loss effectively prioritizes outliers near the ID-OOD boundary, improving detection of subtle distribution shifts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthesizing internal outliers between ID subgroups helps detect subtle OOD samples that lie near the boundaries of in-distribution classes.
- Mechanism: The ID-mixup strategy estimates graphons for each ID subgroup and interpolates between them to generate new graph structures that represent the OOD region between subgroups. These synthetic graphs preserve subgroup topologies while occupying the space where true OOD samples would appear.
- Core assumption: Graphons accurately represent the structural distribution of ID subgroups, and linear interpolation between graphons produces realistic OOD samples.
- Evidence anchors:
  - [abstract] "synthesizing internal outliers within ID subgroups using a graphon-based ID-mixup strategy"
  - [section] "we design an internal outlier generation strategy... we consider synthesizing some internal outliers between subgroups"
- Break condition: If the graphon estimation fails to capture true subgroup structure, or if the interpolation produces samples that fall within the in-distribution region rather than in the OOD boundary space.

### Mechanism 2
- Claim: Boundary-aware OE loss assigns higher weights to outliers near the ID boundary, improving detection of subtle distribution shifts.
- Mechanism: The loss function uses an adaptive threshold (œÑ) set to the minimum ID score, and weights outliers by (l - s)^Œ≥ when their scores exceed œÑ. This prioritizes samples closer to the decision boundary where OOD samples are hardest to distinguish.
- Core assumption: The minimum ID score provides a reasonable estimate of the ID-OOD boundary, and weighting by proximity to this boundary improves overall detection performance.
- Evidence anchors:
  - [abstract] "develop a boundary-aware OE loss that adaptively assigns weights to outliers"
  - [section] "we design the following boundary-aware OE Loss ‚Ñìùëèùëé which is adaptively aware of whether the sample is in ID or OOD space"
- Break condition: If the minimum ID score is an unreliable boundary estimate due to outliers in the ID set, or if the weighting overemphasizes difficult but rare cases at the expense of overall performance.

### Mechanism 3
- Claim: Combining external outliers with internally synthesized outliers provides more diverse and representative OOD training data than either source alone.
- Mechanism: External outliers provide realistic examples from other domains, while internal outliers fill the gap between subgroups where OOD samples are likely to appear. The combination addresses both the diversity challenge (external) and the robustness-to-perturbation challenge (internal).
- Core assumption: External outliers from similar feature domains are relevant to the ID distribution, and internal outliers effectively capture the OOD space between subgroups.
- Evidence anchors:
  - [abstract] "incorporates both external and internal outliers to enhance the diversity"
  - [section] "we identify two pivotal challenges: (C1) How to acquire high-quality graph outlier data? (C2) How to design a LGOE to effectively utilize these OE data?"
- Break condition: If external outliers are too dissimilar to be useful, or if internal synthesis produces samples that are actually in-distribution rather than OOD.

## Foundational Learning

- Concept: Graphon theory and estimation
  - Why needed here: Graphons provide the mathematical foundation for synthesizing realistic graph structures between subgroups
  - Quick check question: What is the relationship between a graphon and the probability distribution of edges in a graph?

- Concept: Outlier Exposure (OE) methodology
  - Why needed here: OE is the core technique being adapted from image/text domains to graph data
  - Quick check question: How does OE differ from traditional anomaly detection approaches in terms of training objectives?

- Concept: Graph contrastive learning
  - Why needed here: Used for feature extraction to cluster ID graphs into subgroups before synthesis
  - Quick check question: What is the main difference between supervised and self-supervised contrastive learning on graphs?

## Architecture Onboarding

- Component map: Data preprocessing ‚Üí GraphCL feature extraction ‚Üí k-means clustering ‚Üí Graphon estimation ‚Üí ID-mixup synthesis ‚Üí External feature alignment ‚Üí Boundary-aware loss training ‚Üí OOD scoring
- Critical path: The synthesis pipeline (clustering ‚Üí graphon estimation ‚Üí mixup ‚Üí feature alignment) must complete before training can begin with the combined outlier dataset
- Design tradeoffs: More internal outliers improve diversity but increase computation; tighter Œª range produces more realistic outliers but fewer of them
- Failure signatures: Poor clustering leads to unrealistic internal outliers; boundary-aware loss with wrong œÑ causes overfitting to ID data
- First 3 experiments:
  1. Verify graphon estimation accuracy by comparing synthesized graphs to real graphs from the same subgroup
  2. Test boundary-aware loss with fixed œÑ to isolate the effect of the weighting mechanism
  3. Compare performance using only external outliers vs only internal outliers to validate the hybrid approach

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas warrant further investigation based on the methodology and experimental setup.

## Limitations

- The method's effectiveness depends heavily on successful graphon estimation, which can be computationally expensive and sensitive to parameter choices.
- The selection criteria for "appropriate" external outlier datasets is not rigorously defined, potentially limiting generalizability across different domain pairs.
- The boundary-aware loss mechanism assumes the minimum ID score provides a reliable boundary estimate, which may fail when ID data contains natural outliers or is multimodal.

## Confidence

- **High Confidence**: The hybrid approach of combining external and internal outliers is methodologically sound and addresses a real gap in graph OOD detection. The 5.29% AUC improvement claim is supported by experimental results.
- **Medium Confidence**: The ID-mixup strategy using graphons is innovative but depends on accurate graphon estimation. The boundary-aware loss shows promise but may be sensitive to hyperparameter choices like Œ≥ and œÑ.
- **Low Confidence**: The selection of external outlier datasets and the exact criteria for their relevance to specific ID distributions is not clearly specified, making replication challenging.

## Next Checks

1. **Graphon Estimation Validation**: Test synthesized internal outliers by computing their distances to original subgroup graphs to verify they occupy the intended OOD boundary space rather than falling within ID regions.
2. **Boundary-Aware Loss Sensitivity**: Run ablation studies varying Œ≥ and œÑ parameters to determine optimal settings and test robustness to boundary estimation errors.
3. **External Outlier Selection Criteria**: Systematically evaluate performance using different external outlier datasets to establish guidelines for selecting appropriate external OOD examples across domain pairs.