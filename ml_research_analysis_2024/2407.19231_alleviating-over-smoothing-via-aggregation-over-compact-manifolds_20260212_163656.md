---
ver: rpa2
title: Alleviating Over-Smoothing via Aggregation over Compact Manifolds
arxiv_id: '2407.19231'
source_url: https://arxiv.org/abs/2407.19231
tags:
- aggregation
- over-smoothing
- contracted
- graph
- compact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the over-smoothing problem in Graph Neural
  Networks (GNNs), where node features become indistinguishable after multiple layers,
  leading to performance deterioration. The authors identify that existing aggregation
  methods in GNNs are "contracted aggregations," causing node features to converge
  to a single point after many layers.
---

# Alleviating Over-Smoothing via Aggregation over Compact Manifolds

## Quick Facts
- arXiv ID: 2407.19231
- Source URL: https://arxiv.org/abs/2407.19231
- Reference count: 40
- One-line primary result: Aggregation over compact manifolds effectively alleviates over-smoothing in GNNs, outperforming state-of-the-art methods especially in scenarios with missing features.

## Executive Summary
This paper addresses the over-smoothing problem in Graph Neural Networks (GNNs), where node features become indistinguishable after multiple layers, leading to performance deterioration. The authors identify that existing aggregation methods in GNNs are "contracted aggregations," causing node features to converge to a single point after many layers. To address this, they propose a new method called "Aggregation over Compact Manifolds" (ACM). ACM replaces the standard Euclidean space aggregation with aggregation over compact manifolds, which prevents the features from converging. The authors provide theoretical analysis and extensive empirical evaluation showing that ACM effectively alleviates over-smoothing and outperforms state-of-the-art methods in node classification tasks. The improvement is particularly significant in scenarios with missing features, where more layers are required for good performance.

## Method Summary
The authors propose ACM, which aggregates node features over compact Riemannian manifolds instead of Euclidean space to prevent over-smoothing. The method transforms features to a compact manifold (like a hypersphere), performs aggregation, then maps back to Euclidean space for standard transformations. The key insight is that aggregations over compact manifolds are non-contracted, preventing the convergence of features that causes over-smoothing. The implementation involves push-forward and push-back operations between the manifold and hyperplane, maintaining the manifold structure throughout the layer.

## Key Results
- ACM outperforms state-of-the-art methods (PairNorm, BatchNorm, DropEdge, DGN, DeCorr, DAGNN, GPRGNN, GCNII, APPNP) on both homophilic and heterophilic datasets
- Improvement is particularly significant in scenarios with missing features, where ACM with more layers achieves higher accuracy
- ACM effectively prevents over-smoothing even with 30+ layers, while standard normalization methods fail

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard aggregation in GNNs is "contracted aggregation," causing node features to converge to the same point after many layers, leading to over-smoothing.
- Mechanism: The aggregation functions used in SGC, GCN, and GAT (and mathematically equivalent variants) satisfy two properties: (1) if all neighbor features are identical, the result is identical; (2) the distance from any point to the aggregated result is less than or equal to the maximum distance from that point to any neighbor, with equality only in the identical case. This causes features to progressively shrink toward each other.
- Core assumption: The aggregation functions in common GNNs are defined by matrix multiplications of the form L·H where L has specific properties (e.g., L = (1-λ)I + λ·normalized adjacency).
- Evidence anchors:
  - [abstract] "After the study, we found that the information aggregations in existing work are all contracted aggregations, with the intrinsic property that features will inevitably converge to the same single point after many layers."
  - [section 4.1] Proposition 1 shows SGC, GCN, and GAT use contracted or equivalently contracted aggregations.
  - [corpus] No direct corpus evidence found for contracted aggregation concept; the mechanism is derived from the paper's theoretical analysis.
- Break condition: If the aggregation function violates either of the two contraction properties, or if the graph is disconnected (requires component-wise analysis), the convergence may not occur as described.

### Mechanism 2
- Claim: Aggregating over compact manifolds prevents the contraction property and thus prevents over-smoothing.
- Mechanism: Compact manifolds (like spheres, tori) have the property that any aggregation function over them is non-contracted when the graph has a node with more than two neighbors. This means features do not converge to a single point even after many layers.
- Core assumption: The embedding space is a compact Riemannian manifold, and the aggregation function satisfies basic continuity properties.
- Evidence anchors:
  - [section 4.3] Theorem 2 states that aggregations over compact manifolds are non-contracted.
  - [section 5] ACM uses aggregation over specific compact manifolds (MU) defined by xUx^T=1 for positive definite U.
  - [corpus] No direct corpus evidence found for aggregation over compact manifolds preventing over-smoothing; this is the paper's novel theoretical contribution.
- Break condition: If the manifold is not compact (e.g., Euclidean space) or if the graph structure prevents the non-contracted property from applying, over-smoothing may still occur.

### Mechanism 3
- Claim: ACM transforms the aggregation result back to the compact manifold using push-forward and push-back operations, preserving the non-contracted property while allowing standard transformations.
- Mechanism: After aggregation on the compact manifold MU, features are mapped to a hyperplane (push-forward), undergo standard transformation (e.g., linear layer with activation), then mapped back to MU (push-back). This maintains the manifold structure throughout the layer.
- Core assumption: The push-forward and push-back operations are differentiable and preserve the manifold structure.
- Evidence anchors:
  - [section 5] Describes the three-step transformation: MU → hyperplane → R^n → MU using PF and PB functions.
  - [section 5] Specifically defines PF and PB for the MU manifold.
  - [corpus] No direct corpus evidence found for this specific push-forward/push-back mechanism; derived from the paper's methodology section.
- Break condition: If the transformation steps introduce contraction (e.g., through the activation function or linear transformation), or if the PF/PB operations are not correctly implemented, the benefits may be lost.

## Foundational Learning

- Concept: Contracted aggregation and its mathematical definition
  - Why needed here: Understanding why standard GNNs over-smooth requires grasping the contraction property of their aggregation functions.
  - Quick check question: Can you state the two properties that define a contracted aggregation function?

- Concept: Compact Riemannian manifolds and their properties
  - Why needed here: The core innovation relies on replacing Euclidean space with compact manifolds to prevent contraction.
  - Quick check question: What is the key topological property of compact manifolds that makes aggregations over them non-contracted?

- Concept: Push-forward and push-back operations between manifolds and Euclidean spaces
  - Why needed here: ACM requires mapping between the compact manifold and Euclidean space for standard transformations.
  - Quick check question: How does the push-forward operation map points from MU to a hyperplane?

## Architecture Onboarding

- Component map:
  - Input features → Aggregation over MU (compact manifold) → Push-forward to hyperplane → Linear transformation + activation → Push-back to MU → Output
  - For GCN/GAT: Additional attention or normalized adjacency matrix operations before aggregation
  - For SGC: Simpler aggregation without attention or transformation between layers

- Critical path:
  1. Aggregation step: Compute L·H then project to MU using PU(x) = x/√(xUx^T)
  2. Transformation step: Apply PF, then linear transformation W·PF(H), then activation σ, then PB to return to MU
  3. Final classification: Apply softmax to last layer output

- Design tradeoffs:
  - MU manifold choice: Identity matrix U gives hypersphere (simpler), trainable diagonal U gives ellipsoid (more flexible but more parameters)
  - Layer depth: ACM enables deeper models but increases computational cost of PF/PB operations (O(n^2))
  - Manifold dimension: Must match feature dimension; higher dimensions increase expressiveness but computational cost

- Failure signatures:
  - Training instability or NaN outputs: Likely issues with PF/PB operations or matrix U becoming singular
  - No improvement over baseline: May indicate insufficient depth, poor hyperparameter tuning, or that the dataset doesn't benefit from deeper models
  - Overfitting with ACM*: Too many trainable parameters in diagonal U matrix

- First 3 experiments:
  1. Replace GCN aggregation with ACM aggregation on Cora dataset with 2-3 layers; compare with vanilla GCN
  2. Test ACM with different U matrices (identity vs trainable) on a small dataset to observe impact of manifold choice
  3. Gradually increase depth (5, 15, 30 layers) on Cora with ACM to observe over-smoothing mitigation compared to standard normalization methods

## Open Questions the Paper Calls Out
- How does ACM's aggregation over compact manifolds generalize to more complex manifold topologies beyond spheres and tori?
- What is the theoretical relationship between ACM's parameter matrix U and the optimal manifold structure for different graph datasets?
- How does ACM's performance scale with graph size and density compared to other over-smoothing solutions?

## Limitations
- The method introduces computational overhead through push-forward and push-back operations, making it less scalable for large graphs
- The choice of compact manifold (MU) and its parameterization adds complexity and potential hyperparameters to tune
- The paper doesn't extensively explore the impact of different manifold choices beyond the hypersphere and ellipsoid variants

## Confidence
- **High confidence**: The theoretical framework for contracted vs non-contracted aggregations, the proof that standard GNNs use contracted aggregations, and the basic mechanism of ACM
- **Medium confidence**: The empirical results showing ACM's effectiveness across different datasets, as implementation details for PF/PB operations could affect outcomes
- **Low confidence**: The generalizability of ACM to extremely large-scale graphs and its performance in inductive learning scenarios not covered in the paper

## Next Checks
1. **Ablation study**: Test ACM with different compact manifolds (e.g., projective spaces, tori) to verify the non-contracted property holds across various topologies
2. **Scalability analysis**: Measure training time and memory usage of ACM vs standard GNNs on progressively larger graphs to quantify the computational overhead
3. **Robustness testing**: Evaluate ACM's performance when the graph structure is noisy or contains adversarial perturbations to assess its stability beyond the standard benchmark datasets