---
ver: rpa2
title: 'From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models'
arxiv_id: '2403.03893'
source_url: https://arxiv.org/abs/2403.03893
tags:
- toxicity
- languages
- language
- arxiv
- mitigation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends toxicity mitigation research to multilingual
  settings, evaluating mitigation techniques across 9 languages spanning 5 scripts.
  The study compares retrieval-augmented Goodtriever and finetuning-based DExperts
  methods in both static and continual learning scenarios.
---

# From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models

## Quick Facts
- arXiv ID: 2403.03893
- Source URL: https://arxiv.org/abs/2403.03893
- Reference count: 38
- Key outcome: This paper extends toxicity mitigation research to multilingual settings, evaluating mitigation techniques across 9 languages spanning 5 scripts.

## Executive Summary
This paper extends toxicity mitigation research to multilingual settings, evaluating mitigation techniques across 9 languages spanning 5 scripts. The study compares retrieval-augmented Goodtriever and finetuning-based DExperts methods in both static and continual learning scenarios. Key findings include: (1) Goodtriever outperforms DExperts overall, with 38% vs 28% relative toxicity reduction using in-language data for high-resource languages; (2) Translated data proves more effective than in-language data for toxicity mitigation (44% vs 38% reduction with Goodtriever); (3) DExperts shows stronger cross-lingual transfer effects and sensitivity to training language order compared to Goodtriever; (4) Mitigation effectiveness increases with datastore size up to 30K samples but plateaus beyond that.

## Method Summary
The paper evaluates two toxicity mitigation approaches - Goodtriever (retrieval-augmented generation) and DExperts (finetuning-based) - across 9 languages spanning 5 scripts. Both methods are tested using CivilComments and HolisticBias datasets translated to 8 languages, plus Jigsaw multilingual data. The evaluation uses Expected Maximum Toxicity (EMT) reduction as the primary metric, with models trained on both in-language and translated data. Experiments are conducted in both static and continual learning settings to understand language interdependencies.

## Key Results
- Goodtriever outperforms DExperts overall, with 38% vs 28% relative toxicity reduction using in-language data for high-resource languages
- Translated data proves more effective than in-language data for toxicity mitigation (44% vs 38% reduction with Goodtriever)
- DExperts shows stronger cross-lingual transfer effects and sensitivity to training language order compared to Goodtriever
- Mitigation effectiveness increases with datastore size up to 30K samples but plateaus beyond that

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translated data outperforms in-language data for toxicity mitigation
- Mechanism: Translated data is more in-distribution to PerspectiveAPI's training data, which was predominantly translated from English
- Core assumption: Toxicity perception is consistent enough across translation that mitigation techniques can leverage this pattern
- Evidence anchors:
  - [abstract]: "Translated data proves more effective than in-language data for toxicity mitigation (44% vs 38% reduction with Goodtriever)"
  - [section 4]: "This is interesting for two reasons: (1) the in-language dataset contains more training tokens for all languages as shown in Table 1; (2) we have shown how toxicity information can be eroded in an extreme translation scenario (Figure 3), so we expected translation to lead to lower mitigation, which was not the case."
  - [corpus]: Weak - corpus shows related work on cross-lingual transfer but doesn't directly support the in-distribution hypothesis
- Break condition: If toxicity perception changes significantly during translation or if PerspectiveAPI's training data composition differs substantially from the stated assumption

### Mechanism 2
- Claim: Goodtriever (RAG-based) outperforms DExperts (finetuning-based) for mid-resource languages
- Mechanism: Retrieval-based approaches maintain consistent performance across languages without requiring permanent weight updates, making them more adaptable to languages with limited training data
- Core assumption: Cross-lingual transfer benefits from retrieval-based methods are comparable to or better than finetuning when data is limited
- Evidence anchors:
  - [abstract]: "(1) Goodtriever outperforms DExperts overall, with 38% vs 28% relative toxicity reduction using in-language data for high-resource languages"
  - [section 3]: "Goodtrieverâ€™s mitigation performance stands out, with a notable overall EMT reduction of 38% (from 0.37 to 0.23 in absolute terms), while DExperts reduces the EMT by 28% (from 0.37 to 0.27)"
  - [corpus]: Weak - corpus mentions cross-lingual transfer but doesn't provide direct comparison evidence
- Break condition: If finetuning approaches can achieve better cross-lingual transfer through other mechanisms, or if retrieval-based methods encounter significant scaling limitations

### Mechanism 3
- Claim: DExperts shows stronger cross-lingual transfer effects and sensitivity to training language order
- Mechanism: Finetuning-based approaches can exchange information gains in the representation space by updating parameters, enabling stronger cross-lingual transfer but also making them more sensitive to training order
- Core assumption: The order of language presentation during training affects how representations are learned and transferred across languages
- Evidence anchors:
  - [abstract]: "(3) DExperts shows stronger cross-lingual transfer effects and sensitivity to training language order compared to Goodtriever"
  - [section 5.1.1]: "Figure 6 demonstrates that the language order significantly impacts the cross-lingual mitigation effect for both DExperts and Goodtriever. Particularly, under order (2), Goodtriever shows strong cross-lingual effects when three languages are added: French, English, and Spanish with CLME scores of 0.19, 0.18, and 0.20, respectively, which does not happen for order (1)"
  - [corpus]: Weak - corpus doesn't provide direct evidence for training order sensitivity
- Break condition: If cross-lingual transfer effects are primarily driven by factors other than training order, or if the sensitivity to order doesn't translate to meaningful performance differences

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: The paper compares techniques that rely on cross-lingual transfer (DExperts) versus those that don't (Goodtriever)
  - Quick check question: What are the key differences between how finetuning and retrieval-based approaches handle cross-lingual transfer?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: Goodtriever is a RAG-based approach that uses external datastores for toxicity mitigation
  - Quick check question: How does the use of external datastores in Goodtriever differ from the parameter updates in DExperts?

- Concept: Continual learning in multilingual settings
  - Why needed here: The paper evaluates techniques in both static and continual learning scenarios to understand language interdependencies
  - Quick check question: What are the potential benefits and challenges of evaluating toxicity mitigation in a continual learning setting versus a static setting?

## Architecture Onboarding

- Component map: Base model (mGPT) -> Mitigation technique (Goodtriever/DExperts) -> External resources (datastores/translated data) -> Evaluation (PerspectiveAPI)
- Critical path: Data preparation (translation/curation) -> Model training/inference -> Generation -> Toxicity evaluation -> Analysis
- Design tradeoffs: Retrieval-based approaches offer adaptability without weight updates but may lack the cross-lingual transfer benefits of finetuning; finetuning provides better cross-lingual transfer but requires more data and is sensitive to training order
- Failure signatures: Poor translation quality leading to reduced toxicity mitigation; inadequate datastore size causing performance plateaus; training order sensitivity causing inconsistent cross-lingual effects
- First 3 experiments:
  1. Compare Goodtriever and DExperts on a small set of high-resource languages using in-language data
  2. Evaluate the impact of translation quality on toxicity mitigation by comparing different translation models
  3. Test the scalability of Goodtriever by varying the base model size and datastore size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of translated data for toxicity mitigation vary across different language families and scripts?
- Basis in paper: [explicit] The paper found that translated data outperformed in-language data for high-resource languages (38% vs 33% reduction), but did not specifically analyze differences across language families.
- Why unresolved: The paper covers 9 languages spanning 5 scripts but does not provide a detailed breakdown of mitigation effectiveness by language family or script type.
- What evidence would resolve it: A comprehensive analysis comparing toxicity mitigation performance across different language families (e.g., Indo-European, Afro-Asiatic) and scripts (e.g., Latin, Cyrillic, Arabic) using the same translation and evaluation protocols.

### Open Question 2
- Question: What is the optimal datastore size for achieving the best balance between toxicity mitigation effectiveness and computational efficiency across different languages?
- Basis in paper: [explicit] The paper found that mitigation effectiveness increases with datastore size up to 30K samples but plateaus beyond that, without specifying optimal sizes for different languages.
- Why unresolved: The study only tested up to 30K samples and did not explore the relationship between datastore size and language-specific factors like toxicity prevalence or linguistic complexity.
- What evidence would resolve it: A systematic evaluation of Goodtriever performance across a wider range of datastore sizes (e.g., 10K to 100K) for each language, measuring both toxicity reduction and computational costs to identify optimal trade-offs.

### Open Question 3
- Question: How do cultural and contextual nuances in different languages affect the perception and evaluation of toxic content, and how can these be accounted for in multilingual toxicity mitigation?
- Basis in paper: [inferred] The paper acknowledges challenges in evaluating toxicity comparably across diverse languages and mentions the use of a template-based dataset for standardization, suggesting that cultural factors may impact toxicity assessment.
- Why unresolved: The study relies on PerspectiveAPI for toxicity evaluation, which may not fully capture cultural and contextual differences in how toxicity is perceived across languages. The template-based dataset used for evaluation may also introduce Western-centric biases.
- What evidence would resolve it: A comparative study of toxicity perception across cultures, involving human evaluators from different linguistic backgrounds to assess the same content and identify cultural variations in toxicity judgments. Additionally, developing language-specific toxicity evaluation frameworks that account for cultural nuances would be valuable.

## Limitations

- The evaluation relies heavily on PerspectiveAPI as the toxicity detector, which varies in effectiveness across languages, creating potential evaluation bias
- The mechanism behind translated data's superior performance remains unclear despite the authors' hypothesis about PerspectiveAPI's training data composition
- Cross-lingual transfer findings, particularly training order sensitivity for DExperts, lack interpretability and clear explanation of the underlying patterns

## Confidence

**High Confidence Claims:**
- Goodtriever outperforms DExperts for toxicity reduction in high-resource languages using in-language data (EMT reduction: 38% vs 28%)
- Mitigation effectiveness plateaus beyond 30K samples in the datastore
- Cross-lingual transfer effects exist and can be leveraged for toxicity mitigation

**Medium Confidence Claims:**
- Translated data is more effective than in-language data for toxicity mitigation (44% vs 38% reduction)
- DExperts shows stronger cross-lingual transfer effects compared to Goodtriever
- DExperts is sensitive to training language order

**Low Confidence Claims:**
- The exact mechanism behind translated data's superior performance
- The interpretability of cross-lingual transfer patterns and training order effects
- Generalizability of findings to languages outside the 9-language scope studied

## Next Checks

1. **Cross-validation with alternative toxicity detectors**: Re-run the experiments using multiple toxicity detection systems (e.g., different APIs, human evaluation) to verify that the Goodtriever vs DExperts performance gap persists independent of PerspectiveAPI's potential biases.

2. **Ablation study on translation quality**: Systematically vary translation quality (using different translation models or controlled perturbations) to establish the causal relationship between translation fidelity and toxicity mitigation effectiveness.

3. **Probing analysis of cross-lingual transfer**: Conduct representation similarity analysis between languages to understand what drives the observed cross-lingual transfer patterns.