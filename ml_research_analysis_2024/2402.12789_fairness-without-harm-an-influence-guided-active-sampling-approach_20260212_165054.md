---
ver: rpa2
title: 'Fairness Without Harm: An Influence-Guided Active Sampling Approach'
arxiv_id: '2402.12789'
source_url: https://arxiv.org/abs/2402.12789
tags:
- fairness
- accuracy
- data
- test
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a tractable active data sampling algorithm,
  Fair Influential Sampling (FIS), to train fair models without requiring training
  sensitive attributes. The algorithm scores each new example by its influence on
  fairness and accuracy evaluated on a small validation set with sensitive attributes,
  and selects examples for training.
---

# Fairness Without Harm: An Influence-Guided Active Sampling Approach

## Quick Facts
- arXiv ID: 2402.12789
- Source URL: https://arxiv.org/abs/2402.12789
- Reference count: 40
- Primary result: Proposes Fair Influential Sampling (FIS) algorithm that improves fairness without harming accuracy by actively selecting data based on influence scores

## Executive Summary
This paper introduces a tractable active data sampling algorithm called Fair Influential Sampling (FIS) that improves fairness in machine learning models without requiring sensitive attribute annotations during training. The algorithm scores each new example by its influence on fairness and accuracy evaluated on a small validation set with sensitive attributes, then selects examples for training. The method is validated on real-world datasets, demonstrating its effectiveness in improving fairness without harming accuracy. Theoretical analysis shows that acquiring more data can improve fairness without harm by reducing distribution shift and group gap.

## Method Summary
The paper proposes Fair Influential Sampling (FIS), an active learning algorithm that improves fairness without requiring sensitive attribute annotations during training. The method works by first training an initial model on a small labeled dataset without sensitive attributes. It then evaluates each example in an unlabeled pool by its influence on fairness and accuracy using gradients computed from a small validation set that does contain sensitive attributes. Examples are selected based on combined influence scores, labeled, and added to the training set. This process repeats until the labeling budget is exhausted. The algorithm is theoretically grounded, showing that additional data can shift the fairness-accuracy Pareto frontier by reducing distribution shift and group gap.

## Key Results
- FIS improves fairness on CelebA, Adult, and COMPAS datasets without harming accuracy
- The algorithm outperforms baselines including Random sampling, BALD, ISAL, and JTT-20
- Theoretical analysis proves that acquiring more data can improve fairness without harm by reducing distribution shift and group gap
- FIS is flexible and compatible with other labeling methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Active data sampling can improve fairness without harming accuracy by shifting the Pareto frontier.
- Mechanism: Acquiring more informative data allows the model to reach a better trade-off point where both fairness and accuracy are improved simultaneously, compared to models trained on scarce data.
- Core assumption: The additional data reduces distribution shift and group gap, which are primary sources of unfairness and the fairness-accuracy tradeoff.
- Evidence anchors:
  - [abstract]: "acquiring more data is a natural and promising approach to achieve this goal by reaching a better Pareto frontier of the fairness-accuracy tradeoff."
  - [section 1]: "acquiring more data is capable of shifting the Pareto frontier toward lower disparity and lower error rates."
  - [corpus]: Weak evidence; related works discuss Pareto frontiers but not this specific mechanism of using active sampling to shift it.

### Mechanism 2
- Claim: The influence-guided sampling algorithm can score examples by their impact on fairness and accuracy without requiring training sensitive attributes.
- Mechanism: The algorithm calculates the influence of each example on fairness and accuracy using gradients derived from a small validation set with sensitive attributes, then selects examples that improve both metrics.
- Core assumption: The gradients from the validation set with sensitive attributes can accurately approximate the influence of unlabeled examples on fairness and accuracy.
- Evidence anchors:
  - [section 4.1]: "We evaluate the importance (influences) of each new example by comparing its gradient to that derived from the entire validation set."
  - [section 4]: "the algorithm evaluates each example's influence on fairness and accuracy using the validation dataset for ranking and then selects a certain number of examples to supplement the training set for training."
  - [corpus]: Weak evidence; related works discuss influence functions but not this specific application to fairness without training sensitive attributes.

### Mechanism 3
- Claim: Theoretical analysis shows that acquiring more data can improve fairness without harm by reducing distribution shift and group gap.
- Mechanism: The paper provides upper bounds on generalization error and risk disparity, showing that controlling the negative impact of distribution shift on generalization error allows for improving fairness without harming accuracy.
- Core assumption: The distribution shift and group gap are the main factors affecting both accuracy and fairness, and reducing them can improve both simultaneously.
- Evidence anchors:
  - [section 5]: "Theorem 5.1 underscores how the generalization error is impacted by distribution shifts. Theorem 5.2 implies that the risk disparity is essentially influenced by the distribution shift and the inherent group gap term."
  - [section 5]: "if one can control the negative impacts of potential distribution shifts through generalization error while implementing fairness-enhancing strategies, it is possible to achieve the goal of improving fairness without causing harm."
  - [corpus]: Weak evidence; related works discuss distribution shift but not this specific theoretical analysis of its impact on fairness-accuracy tradeoff.

## Foundational Learning

- Concept: Pareto optimality and fairness-accuracy tradeoff
  - Why needed here: Understanding the theoretical foundation of why fairness often comes at the cost of accuracy, and how acquiring more data can shift this tradeoff.
  - Quick check question: Can you explain what a Pareto frontier is in the context of fairness and accuracy, and why it's relevant to this paper?

- Concept: Active learning and influence functions
  - Why needed here: Understanding the basic principles of active learning and how influence functions can be used to score examples based on their impact on model performance.
  - Quick check question: What is the difference between traditional active learning and the influence-guided active sampling approach proposed in this paper?

- Concept: Group fairness definitions (e.g., demographic parity, equalized odds)
  - Why needed here: Understanding the different notions of group fairness and how they relate to the risk disparity measure used in this paper.
  - Quick check question: Can you explain the difference between demographic parity and equalized odds, and why the paper focuses on risk disparity as an intermediate measure?

## Architecture Onboarding

- Component map:
  Training set (without sensitive attributes) -> Initial classifier -> Unlabeled set -> Validation set (with sensitive attributes) -> FIS algorithm -> Selected examples -> Labeled examples -> Augmented training set -> Final classifier -> Test set

- Critical path:
  1. Train initial classifier on the training set
  2. For each round of sampling:
     a. Guess proxy labels for unlabeled examples
     b. Compute influence scores on fairness and accuracy using the validation set
     c. Select examples with the most negative influence scores
     d. Inquire true labels for selected examples
     e. Train classifier on the augmented training set
  3. Evaluate final classifier on the test set

- Design tradeoffs:
  - Using a small validation set with sensitive attributes allows for avoiding the need for training sensitive attributes, but relies on the validation set being representative.
  - Guessing proxy labels for unlabeled examples speeds up the sampling process but may introduce some noise.
  - The algorithm can be combined with other labeling methods, but the proposed strategy of using lowest-influence labels is specifically designed to minimize the impact on accuracy.

- Failure signatures:
  - If the validation set is not representative, the influence scores may not accurately reflect the true impact of examples on fairness and accuracy.
  - If the proxy labels are too inaccurate, the selected examples may not actually improve fairness and accuracy.
  - If the additional data introduces significant distribution shift, it may harm both fairness and accuracy.

- First 3 experiments:
  1. Run the algorithm on a small dataset with known sensitive attributes to verify that it can improve fairness without harming accuracy.
  2. Compare the performance of the algorithm with different labeling strategies (e.g., using model predictions vs. lowest-influence labels) to assess the impact of proxy label accuracy.
  3. Test the algorithm with different sizes of validation sets to determine the minimum size needed for accurate influence scoring.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the FIS algorithm degrade as the validation set size approaches zero, and what is the minimum validation set size required to maintain effectiveness?
- Basis in paper: [inferred] The paper explores the impact of validation set size on FIS performance, testing sizes down to 1/5 of the default. It concludes that FIS retains accuracy and fairness even with smaller validation sets, but does not specify a minimum threshold.
- Why unresolved: The study only tests down to 1/5 of the default size (around 400 images for CelebA) and does not explore smaller sizes or establish a minimum viable validation set size.
- What evidence would resolve it: Experimental results showing FIS performance with validation set sizes smaller than 1/5 of the default, including edge cases approaching zero, to determine the minimum effective size.

### Open Question 2
- Question: What is the impact of noise in the validation set on the accuracy and fairness of the FIS algorithm, and how can the algorithm be made more robust to noisy validation data?
- Basis in paper: [inferred] The paper acknowledges that a noisy validation set could be a limitation for FIS, suggesting the use of loss correction methods or noise-tolerant fairness loss functions as potential solutions, but does not provide experimental validation of these approaches.
- Why unresolved: The paper mentions the potential issue of noisy validation sets and possible mitigation strategies, but does not conduct experiments to test the effectiveness of these strategies or quantify the impact of noise on FIS performance.
- What evidence would resolve it: Experimental results comparing FIS performance on clean vs. noisy validation sets, and tests of the proposed noise mitigation strategies (loss correction methods, noise-tolerant fairness loss functions) to assess their effectiveness in improving robustness.

### Open Question 3
- Question: How does the performance of the FIS algorithm compare to other active learning methods that do not require sensitive attribute information, and what are the trade-offs in terms of computational cost and accuracy?
- Basis in paper: [explicit] The paper compares FIS to several baselines including BALD, ISAL, and JTT-20, demonstrating FIS's superior performance in terms of fairness without sacrificing accuracy. However, it does not compare to other active learning methods that do not require sensitive attributes.
- Why unresolved: The paper focuses on comparing FIS to specific baselines but does not explore the broader landscape of active learning methods that do not require sensitive attribute information, nor does it provide a comprehensive analysis of computational costs.
- What evidence would resolve it: Comparative experiments between FIS and a wider range of active learning methods that do not require sensitive attributes, including a detailed analysis of computational costs and accuracy trade-offs.

## Limitations
- The empirical validation is constrained to three datasets (CelebA, Adult, COMPAS), limiting generalizability.
- The influence approximation mechanism relies heavily on a small validation set with sensitive attributes, which may not generalize well to all scenarios.
- Theoretical analysis assumes specific conditions about distribution shifts and group gaps that may not hold in practice.

## Confidence
- Mechanism 1 (Pareto frontier shift): Medium - The theoretical framework is sound, but empirical evidence is limited to specific datasets
- Mechanism 2 (Influence-guided sampling): Medium - The approach is novel but the proxy label generation and influence calculation details are underspecified
- Mechanism 3 (Theoretical analysis): Low-Medium - The theoretical bounds are provided but their practical applicability depends on assumptions about distribution shift

## Next Checks
1. **Robustness to Validation Set Size**: Systematically vary the size of the validation set Qv and measure the stability of FIS performance across different dataset proportions to determine the minimum viable validation set size.

2. **Cross-Dataset Generalization**: Test FIS on additional fairness-relevant datasets (e.g., credit scoring, hiring) with different data characteristics to assess whether the Pareto frontier shifting mechanism generalizes beyond the initial three datasets.

3. **Proxy Label Quality Impact**: Compare FIS performance when using different proxy labeling strategies (model predictions, random labels, lowest-influence labels) to quantify the sensitivity to proxy label quality and identify optimal labeling approaches.