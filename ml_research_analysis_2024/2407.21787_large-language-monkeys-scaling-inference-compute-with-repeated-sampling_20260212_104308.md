---
ver: rpa2
title: 'Large Language Monkeys: Scaling Inference Compute with Repeated Sampling'
arxiv_id: '2407.21787'
source_url: https://arxiv.org/abs/2407.21787
tags:
- samples
- coverage
- sampling
- number
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces "Large Language Monkeys," a technique that
  scales inference compute through repeated sampling to improve large language model
  (LLM) performance on reasoning tasks. The core idea is to generate many candidate
  solutions for a problem and use automatic verifiers (like unit tests or proof checkers)
  to identify correct answers.
---

# Large Language Monkeys: Scaling Inference Compute with Repeated Sampling

## Quick Facts
- arXiv ID: 2407.21787
- Source URL: https://arxiv.org/abs/2407.21787
- Authors: Bradley Brown; Jordan Juravsky; Ryan Ehrlich; Ronald Clark; Quoc V. Le; Christopher Ré; Azalia Mirhoseini
- Reference count: 40
- Primary result: Coverage scales nearly log-linearly with sample count, enabling 56% SWE-bench Lite solution rate with 250 samples versus 43% state-of-the-art

## Executive Summary
This paper introduces "Large Language Monkeys," a technique that scales inference compute through repeated sampling to improve large language model performance on reasoning tasks. The core idea is to generate many candidate solutions for a problem and use automatic verifiers (like unit tests or proof checkers) to identify correct answers. The primary result is that coverage - the fraction of problems solved by any generated sample - scales nearly log-linearly with the number of samples over four orders of magnitude, following an exponentiated power law. This enables dramatic performance gains: on SWE-bench Lite (real-world GitHub issues), DeepSeek-Coder-V2-Instruct solves 56% of issues with 250 samples, exceeding the single-sample state-of-the-art of 43%.

## Method Summary
The method generates N independent samples per problem using temperature-based sampling, then uses automatic verifiers (unit tests, proof checkers) or manual methods (majority voting, reward models) to identify correct solutions. Code and data are available at https://github.com/ScalingIntelligence/large_language_monkeys and https://huggingface.co/datasets/ScalingIntelligence/monkey_business. The approach evaluates coverage (fraction of problems solved by any generated sample) and success rates across different sample budgets on tasks including GSM8K, MATH, MiniF2F-MATH, CodeContests, and SWE-bench Lite.

## Key Results
- Coverage scales nearly log-linearly with sample count following an exponentiated power law across four orders of magnitude
- DeepSeek-Coder-V2-Instruct solves 56% of SWE-bench Lite issues with 250 samples, exceeding 43% single-sample state-of-the-art
- Sampling five times from cheaper DeepSeek model solves more SWE-bench Lite issues than single samples from GPT-4o or Claude 3.5 Sonnet while costing over 3x less

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coverage scales predictably with the number of samples due to an exponentiated power law relationship
- Mechanism: The probability of generating a correct solution for a problem increases with more samples, following c ≈ exp(akb) where c is coverage and k is sample count
- Core assumption: The distribution of correct solutions across problems is sufficiently smooth that averaging over many problems reveals a consistent scaling pattern
- Evidence anchors:
  - [abstract]: "Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws"
  - [section]: "we develop an explicit model for the relationship between coverage and the number of samples...c ≈ exp(akb)"
- Break condition: Coverage would plateau if the model's ability to generate novel correct solutions reaches fundamental limits, or if the sampling distribution becomes too peaked around incorrect solutions

### Mechanism 2
- Claim: Repeated sampling amplifies weaker models by generating correct solutions they rarely produce with single samples
- Mechanism: Models that assign non-zero probability to every sequence will eventually generate correct solutions for problems they previously failed, given enough samples
- Core assumption: The correct solutions have non-zero probability under the model's distribution, even if extremely small
- Evidence anchors:
  - [abstract]: "In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance"
  - [section]: "On CodeContests programming problems, coverage increases from 0.02% to 7.1% when sampling 10,000 times per problem"
- Break condition: If models assign zero probability to correct solutions for certain problems, or if the correct solution space becomes vanishingly small relative to incorrect possibilities

### Mechanism 3
- Claim: Automatic verification tools are essential for converting coverage gains into actual performance improvements
- Mechanism: In domains with verification tools (unit tests, proof checkers), every correct sample found increases success rate; without verification, correct samples must be identified through other methods
- Core assumption: The verification process has low false positive/negative rates for correctly generated solutions
- Evidence anchors:
  - [abstract]: "In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples"
  - [section]: "In some settings, existing tools like proof checkers and unit tests can automatically verify every sample"
- Break condition: Verification tools become ineffective if they have high error rates, or if the signal distinguishing correct from incorrect samples becomes too weak relative to noise

## Foundational Learning

- Concept: Power laws and log-linear relationships
  - Why needed here: Understanding the exponentiated power law model c ≈ exp(akb) requires familiarity with how power laws behave on log scales
  - Quick check question: If log(coverage) follows a power law with respect to log(samples), what functional form does coverage follow with respect to samples?

- Concept: Probability distributions and sampling
  - Why needed here: The effectiveness of repeated sampling depends on understanding how sampling from a probability distribution can eventually find rare events
  - Quick check question: If a model assigns probability p to a correct solution, what is the probability of finding at least one correct solution in k independent samples?

- Concept: Verification and evaluation metrics
  - Why needed here: Different tasks require different verification approaches (automatic vs. manual), affecting how coverage translates to performance
  - Quick check question: What's the difference between measuring coverage (pass@k) versus success rate when verification tools are imperfect?

## Architecture Onboarding

- Component map: LLM → Generate samples → Verify candidates → Select answer → Measure performance
- Critical path: Sample generator produces candidates, verification system identifies correct ones, selection mechanism picks final answer, evaluation framework measures coverage and success rate
- Design tradeoffs:
  - Model strength vs. sample count: Weaker models need more samples; stronger models need fewer
  - Verification automation: Automatic verification enables direct coverage-to-performance conversion; manual verification requires additional selection mechanisms
  - Sample diversity: Higher temperature increases diversity but may reduce quality of individual samples
- Failure signatures:
  - Coverage increases but success rate plateaus → Verification system failure
  - Coverage plateaus despite more samples → Model has fundamental limitations
  - Performance degrades with more samples → Verification system has high false positive rate
- First 3 experiments:
  1. Baseline test: Single sample performance on a task with automatic verification
  2. Scaling test: Measure coverage growth with 10, 100, 1000, 10000 samples on same task
  3. Verification test: Compare performance with automatic verification vs. reward model selection on same sample set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve verification methods for unstructured tasks like creative writing where subjective comparisons are needed?
- Basis in paper: The paper notes that scaling inference compute through repeated sampling faces limitations in domains without automatic verifiers, and existing methods like majority voting and reward models plateau beyond several hundred samples. The authors suggest that developing model-based verifiers or designing converters to make unstructured tasks verifiable could be potential solutions.

- Why unresolved: Current verification methods for unstructured tasks rely on subjective human judgment or imperfect automated approaches. The paper demonstrates that even with near-perfect coverage (e.g., 98.44% for MATH problems), standard verification methods like majority voting fail to fully capture this improvement, suggesting a fundamental gap in our ability to identify correct samples in these domains.

- What evidence would resolve it: A breakthrough verification method that can consistently identify correct samples from large collections (e.g., maintaining high precision as sample count increases from 100 to 10,000) across multiple unstructured tasks would demonstrate a solution. Alternatively, successful formalization of an unstructured task into a verifiable format with automated checking would provide evidence for one of the proposed approaches.

### Open Question 2
- Question: What are the optimal tradeoffs between single-turn and multi-turn interactions when applying repeated sampling to coding problems?
- Basis in paper: The authors mention that despite automatic verification tools being available for CodeContests and MiniF2F, they use only single-turn setups where models generate solutions without iteration. They express interest in exploring the tradeoffs of multi-turn interactions where models could receive execution feedback and iterate on their solutions.

- Why unresolved: The paper acknowledges this as an unexplored direction but doesn't provide empirical results. The tradeoff involves increased computational cost per attempt versus potentially higher success rates per problem. The optimal balance likely depends on task complexity, model capability, and cost constraints.

- What evidence would resolve it: A systematic comparison showing success rates, coverage, and cost-effectiveness of single-turn versus multi-turn repeated sampling across multiple coding tasks and model sizes would provide clear guidance. The comparison should include metrics like FLOPs per solved problem and total inference cost.

### Open Question 3
- Question: How do different diversity mechanisms beyond temperature-based sampling affect coverage scaling in repeated sampling?
- Basis in paper: The authors state they explore only a simple version of repeated sampling using temperature-based sampling as the sole mechanism for creating diversity among samples. They suggest that combining token-level sampling with higher-level approaches (like conditioning on different metadata tags as AlphaCode does) may further increase diversity.

- Why unresolved: The paper demonstrates significant coverage improvements with basic temperature sampling but doesn't explore whether more sophisticated diversity mechanisms could yield even better scaling. The relationship between diversity mechanisms and the exponentiated power law coverage scaling observed in the paper remains unexplored.

- What evidence would resolve it: Empirical results comparing coverage scaling across different diversity mechanisms (temperature sampling, metadata conditioning, solution diversity techniques, etc.) would show whether more sophisticated approaches provide multiplicative or additive benefits. A theoretical analysis connecting diversity mechanisms to the power law parameters (a and b in the coverage equation) would provide deeper insight into the scaling behavior.

## Limitations

- Verification bottleneck: Automatic verification tools are essential for realizing benefits; without them, common verification methods plateau beyond several hundred samples
- Scaling law applicability: The exponentiated power law is derived from averaging across problems and may not hold consistently for individual problems or diverse problem types
- Cost-effectiveness assumptions: Claims about cost-effectiveness versus stronger models need more detailed breakdown accounting for verification overhead and different hardware configurations

## Confidence

**High Confidence**: The empirical observation that coverage scales predictably with sample count (log-linear relationship observed across multiple domains and sample budgets)

**Medium Confidence**: The conclusion that automatic verification is essential for converting coverage gains into performance improvements

**Low Confidence**: The broader applicability of repeated sampling to domains without automatic verification

## Next Checks

1. **Verification method ablation study**: Systematically compare performance of repeated sampling with automatic verification versus reward model selection across multiple sample budgets (10, 100, 1000, 10000) on a task without perfect verification tools

2. **Cross-domain scaling validation**: Test the exponentiated power law model on diverse problem types beyond coding and math (e.g., commonsense reasoning, creative writing) to assess whether scaling behavior generalizes

3. **Cost model refinement**: Conduct detailed cost analysis comparing total compute (sampling + verification) for repeated sampling approaches versus single samples from stronger models, including memory usage and context window effects