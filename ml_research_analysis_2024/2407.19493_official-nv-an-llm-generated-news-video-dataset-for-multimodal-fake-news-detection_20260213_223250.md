---
ver: rpa2
title: 'Official-NV: An LLM-Generated News Video Dataset for Multimodal Fake News
  Detection'
arxiv_id: '2407.19493'
source_url: https://arxiv.org/abs/2407.19493
tags:
- news
- video
- videos
- dataset
- fake
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Official-NV, a dataset for multimodal fake
  news detection that consists of officially published English news videos. Unlike
  existing datasets that rely on user-generated content with significant noise and
  duplicates, Official-NV comprises 10,000 videos (5,000 authentic and 5,000 fabricated)
  collected from official news sources.
---

# Official-NV: An LLM-Generated News Video Dataset for Multimodal Fake News Detection

## Quick Facts
- **arXiv ID**: 2407.19493
- **Source URL**: https://arxiv.org/abs/2407.19493
- **Reference count**: 21
- **Primary result**: OFNVD achieves 71.60% accuracy on Official-NV dataset with superior performance compared to baselines

## Executive Summary
This paper introduces Official-NV, a novel dataset for multimodal fake news detection that addresses limitations in existing user-generated video datasets by using officially published news videos. The dataset contains 10,000 videos (5,000 authentic and 5,000 fabricated) collected from official news sources and expanded through LLM-based text augmentation and manual verification. The authors propose OFNVD, a baseline model using GLU attention to capture key information from multimodal features and cross-modal Transformer for feature enhancement and aggregation. Experimental results demonstrate that OFNVD achieves 71.60% accuracy on the dataset, with superior performance compared to existing baselines, and shows effectiveness across different augmentation methods and imbalanced data scenarios.

## Method Summary
The method involves collecting official news videos from sources like ChinaNews, CCTV, and Xinhua, filtering videos shorter than 15 seconds, and removing duplicates using cosine similarity (>0.8). LLM-based data augmentation with ChatGPT4o generates modified titles (True Titles and Fake Titles) and replaces video frames (Fake Frames) while maintaining original titles, followed by manual verification. The OFNVD model uses BERT for title feature extraction, Faster R-CNN and CLIP for video frame features, GLU attention module for feature refinement, and cross-modal Transformer for feature aggregation, trained with AdamW optimizer and cross-entropy loss.

## Key Results
- OFNVD achieves 71.60% accuracy on Official-NV dataset
- Model outperforms existing baselines in fake news detection
- Dataset shows effectiveness across different augmentation methods and can maintain performance with imbalanced data
- Lower noise and higher video quality compared to user-generated video datasets

## Why This Works (Mechanism)

### Mechanism 1
Official news sources provide higher editorial standards and lower likelihood of duplicate or meme content. The authors apply cosine similarity filtering (>0.8) to remove near-duplicate videos, ensuring unique entries. This approach achieves lower noise than existing user-generated video datasets because it is sourced from official news outlets with higher editorial standards and filtered for duplicates.

### Mechanism 2
LLM-based text augmentation preserves semantic similarity in "True Titles" while introducing opposite meaning in "Fake Titles," maintaining dataset balance. Prompts instruct ChatGPT to change text meaning by altering position, quantity, action, or object. Manual verification ensures semantic consistency for TT and opposition for FT, allowing reliable generation of semantically consistent or opposite titles based on explicit instructions.

### Mechanism 3
GLU Attention improves fake news detection by filtering irrelevant features and capturing cross-modal inconsistencies. GLU refines attention outputs by gating feature importance, helping the model focus on mismatched elements (e.g., title mentions snow but frames show grass). Cross-modal inconsistency is a strong signal for fake news, and GLU can effectively highlight these mismatches by capturing key information from multimodal features.

## Foundational Learning

- **Concept**: Multimodal feature extraction (BERT for text, Faster R-CNN + CLIP for video)
  - Why needed here: The dataset contains both titles and video frames; extracting meaningful features from each modality is essential before fusion
  - Quick check question: What are the two primary visual feature extractors used in OFNVD, and what does each capture?

- **Concept**: Cross-modal attention and fusion
  - Why needed here: Fake news often involves semantic mismatches between modalities; fusion allows the model to detect such inconsistencies
  - Quick check question: How does the cross-modal Transformer in OFNVD enhance the interaction between title and frame features?

- **Concept**: Data augmentation strategies for multimodal tasks
  - Why needed here: The dataset is built from a small seed of official videos; augmentation expands it while preserving or altering semantics appropriately
  - Quick check question: What are the two types of text augmentation applied, and how do they differ in intent?

## Architecture Onboarding

- **Component map**: Faster R-CNN (object detection) → CLIP (patch features) → concat → GLU Attention → Cross-modal Transformer → Classifier
- **Critical path**: Feature extraction → GLU Attention refinement → Cross-modal fusion → Prediction
- **Design tradeoffs**: Using both Faster R-CNN and CLIP increases feature richness but also computational cost; GLU adds selectivity but may drop useful signals
- **Failure signatures**: If GLU gates too aggressively, cross-modal signals may be lost; if cosine filtering is too lenient, duplicates may remain
- **First 3 experiments**:
  1. Run OFNVD with only title modality to confirm performance drop
  2. Run OFNVD with only frame modality to measure visual-only capability
  3. Vary the cosine similarity threshold for duplicate removal and observe dataset quality and model accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of OFNVD on Official-NV compare to its performance on other existing multimodal fake news detection datasets like FakeSV or COVID-VTS? The paper focuses on benchmarking OFNVD on Official-NV but does not provide cross-dataset performance comparisons to validate if the improvements are dataset-specific or generalizable.

### Open Question 2
How sensitive is the OFNVD model to the quality of LLM-generated text augmentations, and what happens if imperfect or low-quality augmentations are used? The paper states that LLM-generated titles are manually verified and screened, but does not explore what happens when this manual filtering is removed or when imperfect augmentations are introduced.

### Open Question 3
Can OFNVD generalize to other languages beyond English, and what modifications would be needed to adapt it to non-English fake news video datasets? Official-NV is an English-language dataset, and the paper does not discuss multilingual capabilities or cross-lingual transfer of the model.

## Limitations

- Dataset construction relies heavily on LLM-generated text with manual verification, but exact quality control criteria are not specified
- Cosine similarity threshold (>0.8) for duplicate removal is somewhat arbitrary and may not optimally balance between removing duplicates and preserving unique content
- Model's performance on longer videos or videos with more complex temporal dynamics is unknown, as focus is on short video clips

## Confidence

- **High confidence**: Dataset construction methodology and basic model architecture are well-documented
- **Medium confidence**: Effectiveness of GLU attention for cross-modal inconsistency detection, as paper lacks ablation studies isolating its impact
- **Low confidence**: Generalization to real-world fake news scenarios beyond controlled dataset, particularly for user-generated content

## Next Checks

1. Conduct an ablation study removing the GLU attention module to quantify its specific contribution to detection accuracy
2. Test the model on an external, publicly available multimodal fake news dataset to assess generalization
3. Perform sensitivity analysis on the cosine similarity threshold to determine optimal duplicate removal settings