---
ver: rpa2
title: Large Language Models as Evaluators for Scientific Synthesis
arxiv_id: '2407.02977'
source_url: https://arxiv.org/abs/2407.02977
tags:
- answer
- gpt-4
- question
- llms
- mistral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the use of large language models (LLMs) for
  assessing the quality of scientific syntheses, comparing their ratings to human
  judgments across three dimensions: comprehensiveness, trustworthiness, and utility.
  Using GPT-4 Turbo and Mistral-7B to evaluate 100 scientific syntheses, the research
  found that while LLMs provided credible and logically consistent ratings with detailed
  rationales, their performance did not align with human evaluators.'
---

# Large Language Models as Evaluators for Scientific Synthesis

## Quick Facts
- arXiv ID: 2407.02977
- Source URL: https://arxiv.org/abs/2407.02977
- Reference count: 15
- Key outcome: LLM evaluations show weak correlation with human judgments for scientific synthesis quality assessment

## Executive Summary
This study evaluates the use of large language models (LLMs) for assessing the quality of scientific syntheses, comparing their ratings to human judgments across three dimensions: comprehensiveness, trustworthiness, and utility. Using GPT-4 Turbo and Mistral-7B to evaluate 100 scientific syntheses, the research found that while LLMs provided credible and logically consistent ratings with detailed rationales, their performance did not align with human evaluators. Spearman's correlation analysis revealed a strong correlation between the two LLMs but only weak correlations between LLM and human ratings. The findings highlight both the potential and current limitations of LLMs in scientific synthesis evaluation, particularly regarding their sensitivity to citations and the need for further validation and refinement of the methodology.

## Method Summary
The study evaluated 100 scientific syntheses generated by GPT-4 from abstracts of five related papers, using GPT-4 Turbo and Mistral-7B to provide ratings on comprehensiveness, trustworthiness, and utility. Each synthesis was evaluated using a structured prompt requesting numeric ratings (0-10) and rationales for each dimension. The LLMs' ratings were then compared to human judgments using Spearman's rank correlation coefficient to assess alignment between model and human evaluations.

## Key Results
- GPT-4 Turbo and Mistral-7B showed strong internal correlation (r = 0.786) but weak correlation with human ratings (r = 0.089-0.339)
- LLMs generated credible and logically consistent ratings with detailed rationales across all three evaluation dimensions
- GPT-4 demonstrated greater sensitivity to citations compared to Mistral, particularly in trustworthiness evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can provide credible and logically consistent ratings and rationales for scientific synthesis evaluation
- Mechanism: LLMs generate structured JSON responses containing numeric ratings and textual rationales based on their understanding of evaluation criteria
- Core assumption: LLMs have sufficient domain knowledge to evaluate scientific content quality
- Evidence anchors:
  - [abstract] "Preliminary results show that LLMs can offer logical explanations that somewhat match the quality ratings"
  - [section] "Both models generally produce credible and logically consistent ratings and rationales"
  - [corpus] Weak evidence - no direct corpus support found for this mechanism
- Break condition: When LLM lacks domain-specific knowledge or encounters ambiguous evaluation criteria

### Mechanism 2
- Claim: LLMs demonstrate consistency in their evaluation performance when compared to each other
- Mechanism: Statistical correlation analysis reveals strong positive relationships between different LLM models' ratings
- Core assumption: Different LLM architectures can converge on similar quality assessments
- Evidence anchors:
  - [abstract] "Spearman's correlation analysis revealed a strong correlation between the two LLMs"
  - [section] "GPT-4 Turbo and Mistral are utilized to obtain quality ratings for 100 syntheses from the CORE-GPT dataset"
  - [corpus] Moderate evidence - corpus shows related work on LLM evaluation consistency
- Break condition: When evaluation criteria are subjective or context-dependent

### Mechanism 3
- Claim: LLM evaluation sensitivity varies across different quality dimensions
- Mechanism: LLMs show differential sensitivity to comprehensiveness, trustworthiness, and utility criteria
- Core assumption: Different evaluation dimensions require distinct analytical capabilities
- Evidence anchors:
  - [section] "GPT-4 also displays greater sensitivity to the presence or absence of citations compared to Mistral"
  - [section] "The LLMs seemed to show the greatest discrepancy between rating and rationale, and the greatest inconsistencies, in their evaluations of trust"
  - [corpus] Weak evidence - limited corpus support for dimension-specific sensitivity
- Break condition: When evaluation criteria overlap or are poorly defined

## Foundational Learning

- Concept: Spearman's rank correlation coefficient
  - Why needed here: To quantify the relationship between LLM ratings and human judgments
  - Quick check question: What does a Spearman correlation of 0.786 between two LLM models indicate about their evaluation consistency?

- Concept: Scientific synthesis evaluation criteria
  - Why needed here: To understand the three dimensions (comprehensiveness, trustworthiness, utility) being assessed
  - Quick check question: How might the presence or absence of citations affect trustworthiness ratings differently for GPT-4 versus Mistral?

- Concept: JSON response parsing
  - Why needed here: To extract structured ratings and rationales from LLM outputs
  - Quick check question: What should be done if an LLM returns ratings in words ("excellent") instead of numbers ("10")?

## Architecture Onboarding

- Component map: Input layer (research questions, abstracts, human ratings) -> LLM layer (GPT-4 Turbo and Mistral-7B evaluation engines) -> Output layer (JSON-formatted ratings and rationales) -> Analysis layer (Spearman correlation calculations)

- Critical path:
  1. Prepare input data (100 research questions with abstracts)
  2. Generate LLM evaluations with structured prompts
  3. Parse and normalize LLM outputs
  4. Calculate correlation metrics
  5. Conduct qualitative analysis of rationales

- Design tradeoffs:
  - Single vs. multiple LLM runs: Cost vs. reliability
  - Numeric vs. descriptive ratings: Precision vs. interpretability
  - Abstract-only vs. full-text evaluation: Scope vs. depth

- Failure signatures:
  - Inconsistent JSON formatting across LLM runs
  - Missing rationales or incomplete ratings
  - Extreme correlation values (near 0 or 1) indicating data issues

- First 3 experiments:
  1. Test prompt variations with a small subset of questions
  2. Compare single vs. averaged LLM ratings for reliability
  3. Validate correlation calculations with synthetic datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM evaluations compare across different scientific domains with varying levels of specialization?
- Basis in paper: Inferred from the paper's mention that "judging how comprehensive a synthesis is requires some knowledge of the scope of potential information which might be appropriate to include" and that "highly specialized domain knowledge still presents a challenge to general use LLMs"
- Why unresolved: The study used a diverse dataset spanning 20 domains but did not analyze domain-specific performance differences in detail. The paper notes the difficulty of assessing syntheses across such diverse domains but doesn't provide comparative analysis between them.
- What evidence would resolve it: A systematic analysis comparing LLM performance across different scientific domains, measuring correlation with human ratings within each domain and identifying which types of domains (e.g., highly technical vs. general) pose greater challenges for LLM evaluators.

### Open Question 2
- Question: What specific aspects of LLM prompts most influence their evaluation consistency and alignment with human judgments?
- Basis in paper: Explicit - The paper references prior work showing "only minor variations in results depending on task instructions and hyperparameters" but also notes "a high degree of variation in performance of different LLMs and the quality characteristics being assessed"
- Why unresolved: While the paper used a specific prompt format and found weak correlations with human judgments, it only tested one prompt configuration and didn't systematically vary prompt elements to identify which aspects most affect performance.
- What evidence would resolve it: A controlled study testing different prompt formulations (e.g., varying instruction specificity, evaluation criteria emphasis, or output format requirements) and measuring how each variation affects LLM-human correlation across multiple quality dimensions.

### Open Question 3
- Question: How can we determine whether LLM-generated syntheses genuinely synthesize provided abstracts versus relying on general knowledge?
- Basis in paper: Explicit - The paper specifically notes "the extent to which the responses are evaluated as syntheses and not simply as answers, without reliance on general knowledge, remains unclear, particularly in the case of Mistral"
- Why unresolved: The study identified concerning cases where LLMs rated highly trustworthy syntheses that contained no citations or only one citation, suggesting they may not be properly evaluating synthesis quality versus general knowledge generation.
- What evidence would resolve it: A systematic evaluation methodology that includes test cases specifically designed to detect when LLMs generate content from general knowledge rather than synthesizing provided abstracts, potentially using controlled experiments with abstracts containing contradictory information.

## Limitations

- Weak correlation between LLM and human ratings suggests fundamental differences in evaluation criteria or methodology
- Reliance on abstract-only syntheses rather than full-text documents may limit ecological validity
- GPT-4's greater sensitivity to citations compared to Mistral indicates potential systematic bias across models

## Confidence

- Weak LLM-human correlation (r = 0.089-0.339): Medium confidence - statistically significant but practically limited
- Strong LLM-LLM correlation (r = 0.786): High confidence - consistent internal performance
- Abstract-only evaluation methodology: Low confidence - potential validity concerns

## Next Checks

1. **Full-text validation**: Repeat the evaluation protocol using complete research papers rather than abstracts to assess whether the weak LLM-human correlation persists with more comprehensive source material.

2. **Cross-domain testing**: Apply the same evaluation methodology to scientific syntheses from different disciplines to determine whether observed patterns hold across various research domains.

3. **Human-LLM agreement calibration**: Conduct a detailed error analysis comparing specific rating differences to identify systematic biases or misunderstanding of evaluation criteria that could be addressed through prompt engineering or model fine-tuning.