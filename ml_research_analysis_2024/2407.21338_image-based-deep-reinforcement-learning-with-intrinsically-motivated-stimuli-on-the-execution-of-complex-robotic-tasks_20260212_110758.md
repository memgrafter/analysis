---
ver: rpa2
title: 'Image-Based Deep Reinforcement Learning with Intrinsically Motivated Stimuli:
  On the Execution of Complex Robotic Tasks'
arxiv_id: '2407.21338'
source_url: https://arxiv.org/abs/2407.21338
tags:
- learning
- intrinsic
- novelty
- surprise
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of training reinforcement learning
  agents for complex robotics tasks in sparse reward environments using only image
  observations. The authors propose NaSA-TD3, an image-based extension of TD3 that
  incorporates intrinsic rewards based on novelty and surprise.
---

# Image-Based Deep Reinforcement Learning with Intrinsically Motivated Stimuli: On the Execution of Complex Robotic Tasks

## Quick Facts
- arXiv ID: 2407.21338
- Source URL: https://arxiv.org/abs/2407.21338
- Reference count: 40
- Authors successfully train RL agents using only image observations for complex robotics tasks with sparse rewards

## Executive Summary
This paper addresses the challenge of training reinforcement learning agents for complex robotics tasks using only image observations in sparse reward environments. The authors propose NaSA-TD3, an image-based extension of TD3 that incorporates intrinsic rewards based on novelty and surprise. The method was evaluated on six continuous control tasks from the DeepMind Control Suite and a real-world dexterous manipulation task, demonstrating superior performance compared to existing image-based RL methods.

## Method Summary
The authors extend TD3 with an autoencoder that reconstructs input images, using the reconstruction error (measured via SSIM) as a novelty signal. A predictive ensemble models the next latent state, with prediction error serving as a surprise signal. Both intrinsic rewards are combined with the extrinsic reward to form the total reward signal. The same encoder is used for both policy learning and novelty detection, creating a synergistic training process. The method was evaluated on both simulated tasks from DeepMind Control Suite and a real-world 4-DoF robotic gripper manipulation task.

## Key Results
- NaSA-TD3 outperforms SAC-AE and Pixel-TD3 on all six simulated tasks from DeepMind Control Suite
- On the Walker task, NaSA-TD3 achieved 960 reward compared to SAC-AE's 140 and Pixel-TD3's 320
- On the real-world dexterous manipulation task, NaSA-TD3 successfully solved both reinitialization scenarios while other methods failed
- NaSA-TD3 reached near-maximum reward values on simulated tasks, demonstrating effective exploration in sparse reward environments

## Why This Works (Mechanism)

### Mechanism 1
Intrinsic novelty and surprise rewards improve exploration in sparse reward environments by encouraging the agent to visit under-explored state-action pairs. The autoencoder reconstructs observed images, and the reconstruction error (SSIM-based) serves as the novelty signal. A predictive ensemble forecasts the next latent state, and prediction error (MSE) serves as the surprise signal. Both are added to the extrinsic reward to form the total reward signal.

### Mechanism 2
Using the same encoder for both policy learning and novelty detection creates synergy between representation learning and policy optimization. The encoder is updated by gradients from both the critic (policy learning) and the autoencoder reconstruction loss (novelty detection). This joint training encourages the latent representation to be both predictive of future states and useful for control.

### Mechanism 3
Predictive ensemble models improve surprise signal stability and reduce sensitivity to noise in real-world applications. Multiple predictive models each predict the next latent state; their mean prediction is used to compute the surprise reward. Ensemble averaging smooths out prediction errors from individual models.

## Foundational Learning

- Concept: Autoencoder reconstruction error as novelty signal
  - Why needed here: Provides a scalable way to measure novelty in image-based RL without maintaining a memory of all visited states
  - Quick check question: If an image is reconstructed perfectly, does that mean it is novel or familiar?

- Concept: Predictive model error as surprise signal
  - Why needed here: Encourages exploration of states where the agent's internal model is inaccurate, promoting learning of dynamics
  - Quick check question: If the predictive model perfectly forecasts the next state, should the surprise reward be high or low?

- Concept: Joint training of representation and policy
  - Why needed here: Ensures the learned latent representation is optimized for both reconstruction (novelty detection) and control (policy learning)
  - Quick check question: What happens if the encoder is only updated by the autoencoder loss and not by policy gradients?

## Architecture Onboarding

- Component map:
  Image → Encoder → Latent z → Actor/Critics → Action → Environment → Next Image → Encoder → Latent z+1 → Predictive ensemble → Surprise reward + Autoencoder → Novelty reward → Total reward

- Critical path: Image → Encoder → Latent z → Actor/Critics → Action → Environment → Next Image → Encoder → Latent z+1 → Predictive ensemble → Surprise reward + Autoencoder → Novelty reward → Total reward

- Design tradeoffs:
  - Using the same encoder for policy and novelty detection saves parameters but couples representation learning to exploration strategy
  - Ensemble predictions add computation but improve stability in real-world settings
  - Fixed α=β=1 weights simplify tuning but may not be optimal for all tasks

- Failure signatures:
  - High reconstruction error but low policy performance: Encoder may be overfitting to reconstruction at the expense of control-relevant features
  - Surprise reward dominates total reward: Predictive model may be poorly trained, causing excessive exploration of irrelevant states
  - Memory errors during training: Large replay buffer size needed for image-based methods may exceed available RAM

- First 3 experiments:
  1. Train AE-TD3 (without intrinsic rewards) on Cartpole to establish baseline performance
  2. Add novelty reward only to AE-TD3 and compare performance on Reacher task
  3. Add both novelty and surprise rewards to AE-TD3 and evaluate on Walker task

## Open Questions the Paper Calls Out

- Open Question 1: How do different autoencoder architectures affect the performance of NaSA-TD3 in complex robotics tasks?
  - Basis in paper: [explicit] The authors mention "a more comprehensive analysis of other autoencoder architectures and their effect on policy learning" as future work
  - Why unresolved: The current implementation uses a standard convolutional autoencoder, but the impact of alternative architectures (e.g., variational autoencoders, transformers) on learning efficiency and policy performance remains unexplored
  - What evidence would resolve it: Comparative experiments using various autoencoder architectures (VAE, transformer-based, etc.) with identical tasks and evaluation metrics to measure differences in sample efficiency, convergence speed, and final performance

- Open Question 2: What is the optimal balance between novelty and surprise intrinsic rewards for different types of robotics tasks?
  - Basis in paper: [explicit] The authors note that novelty and surprise signals "do not significantly impact the final results when the task is not highly complex, or the extrinsic reward is not sparse"
  - Why unresolved: The paper uses fixed weights (α = 1, β = 1) for novelty and surprise rewards, but the optimal balance likely varies by task complexity and environment characteristics
  - What evidence would resolve it: Systematic experiments varying the novelty/surprise reward weights across different task types (sparse vs. dense rewards, simple vs. complex) to identify optimal weight configurations for each scenario

- Open Question 3: Can other intrinsic motivation signals (boredom, frustration, pleasure) further improve NaSA-TD3's performance?
  - Basis in paper: [explicit] The authors state "Future work will seek to include other intrinsic signals such as boredom, frustration, and pleasure"
  - Why unresolved: While novelty and surprise are implemented, the paper acknowledges potential benefits from additional intrinsic motivation signals but does not explore them
  - What evidence would resolve it: Implementation and evaluation of additional intrinsic signals alongside novelty and surprise, comparing performance against the current NaSA-TD3 baseline across multiple robotics tasks

## Limitations

- The paper lacks quantitative comparisons of the proposed method against other intrinsic reward formulations
- No ablation studies on the relative contributions of novelty and surprise rewards are provided
- The ensemble approach for predictive models is mentioned but not thoroughly evaluated against single-model baselines

## Confidence

- High confidence: NaSA-TD3 outperforms baseline methods (SAC-AE and Pixel-TD3) on both simulated and real-world tasks
- Medium confidence: The combination of novelty and surprise rewards improves exploration in sparse reward environments
- Low confidence: The claim that using the same encoder for both policy learning and novelty detection creates synergy between representation learning and policy optimization lacks direct empirical validation

## Next Checks

1. Conduct an ablation study comparing NaSA-TD3 with variants that use only novelty rewards, only surprise rewards, or no intrinsic rewards to quantify their individual contributions
2. Compare the ensemble predictive model approach against a single predictive model baseline to assess the impact on stability and performance
3. Experiment with adaptive weighting schemes for intrinsic rewards (α and β) to determine if task-specific tuning improves performance across different environments