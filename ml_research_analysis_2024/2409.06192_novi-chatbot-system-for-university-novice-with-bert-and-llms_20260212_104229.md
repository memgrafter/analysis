---
ver: rpa2
title: 'NOVI : Chatbot System for University Novice with BERT and LLMs'
arxiv_id: '2409.06192'
source_url: https://arxiv.org/abs/2409.06192
tags:
- university
- chatbot
- information
- data
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed NOVI, a chatbot system for university freshmen
  using GPT-4o and LangChain to help students adapt to university life. The system
  leverages community data from SKKU's Everytime platform, filtering academic-related
  posts through topic modeling and manual labeling.
---

# NOVI : Chatbot System for University Novice with BERT and LLMs

## Quick Facts
- arXiv ID: 2409.06192
- Source URL: https://arxiv.org/abs/2409.06192
- Authors: Yoonji Nam; TaeWoong Seo; Gyeongcheol Shin; Sangji Lee; JaeEun Im
- Reference count: 19
- Key outcome: Chatbot system for university freshmen using GPT-4o and LangChain with BLEU (0.0551), Perplexity (15.8756), ROUGE (1:0.0360, 2:0.0134, L:0.0340), and METEOR (0.0243) scores

## Executive Summary
This study presents NOVI, a chatbot system designed to help university freshmen adapt to university life by providing practical information based on community data from SKKU's Everytime platform. The system leverages GPT-4o and LangChain's RAG framework to retrieve and generate responses from filtered academic-related posts. While evaluation metrics indicate room for improvement in response quality and predictive accuracy, NOVI represents a promising AI-driven approach to addressing the information needs of Generation Z students who prefer text-based communication.

## Method Summary
The study developed NOVI using GPT-4o and LangChain to create a RetrievalQA system that processes user queries through a vector store containing community posts from SKKU's Everytime platform. The methodology involved collecting data from three boards (Ask Broly, Freshmen Board, and Insa Campus Board), filtering academic-related posts through topic modeling, and manually labeling posts for useful information. The system uses KLUE-BERT-base and GPT-ADA v2 embeddings to search the FAISS vector store, with Flask providing the web interface for user interaction.

## Key Results
- BLEU score of 0.0551 indicates limited machine translation quality
- Perplexity score of 15.8756 suggests moderate model prediction performance
- ROUGE scores (1:0.0360, 2:0.0134, L:0.0340) and METEOR score of 0.0243 indicate room for improvement in response quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Community-sourced data from Everytime provides more practical, student-relevant university information than official university resources.
- Mechanism: By filtering and labeling posts from the Everytime community platform using topic modeling and manual classification, NOVI can train on real student questions and answers that reflect actual needs and challenges faced by freshmen.
- Core assumption: The informal, peer-to-peer nature of Everytime posts contains more actionable, context-specific information than formal university documentation.
- Evidence anchors:
  - [abstract] "This system utilizes post and comment data from SKKU 'Everytime', a university community site."
  - [section] "In a situation where the academic information provided by the school is limited, university students are increasingly seeking information from online communities such as 'Everytime' instead of the official academic information provided by their universities."
- Break condition: If the community data becomes outdated or no longer reflects current student concerns, the system's relevance would degrade.

### Mechanism 2
- Claim: Combining GPT-4o with LangChain's RAG framework enables accurate retrieval of relevant community information for freshman queries.
- Mechanism: User queries are embedded and used to search the vector store containing community posts. The QA chain then generates responses based on the retrieved information, providing context-specific answers.
- Core assumption: The combination of GPT-4o's language understanding and LangChain's retrieval capabilities can effectively match freshman queries to relevant community content.
- Evidence anchors:
  - [abstract] "Developed using LangChain, NOVI's performance has been evaluated..."
  - [section] "The system built using LangChain operates as follows: 1. The user accesses the URL created with Flask. 2. The user enters a question. 3. The entered question is tokenized and embedded. 4. The embedded question is used to search for relevant data in the Vector Store. 5. The retrieved data is passed to the QA chain to generate the final response."
- Break condition: If the vector store becomes too large or the embeddings poorly capture semantic relationships, retrieval accuracy would suffer.

### Mechanism 3
- Claim: Topic modeling and manual labeling improve the quality of training data by filtering out irrelevant content.
- Mechanism: The system first uses topic modeling to extract academic-related posts from the broader Everytime dataset, then applies manual labeling to identify posts with useful information, creating a high-quality training dataset.
- Core assumption: Manual labeling of academic relevance significantly improves the quality of the training dataset compared to purely automated filtering.
- Evidence anchors:
  - [section] "b. Topic Modeling: The discussion boards included questions and answers unrelated to school life...We applied topic modeling techniques to filter out questions not related to academics."
  - [section] "c. Manual Labeling: Researchers manually read the questions and labeled them in binary terms to determine whether the answers provided useful information."
- Break condition: If manual labeling is incomplete or inconsistent, the quality of the filtered dataset would be compromised.

## Foundational Learning

- Concept: Text preprocessing and tokenization
  - Why needed here: The system needs to convert Korean text from Everytime into a format that can be processed by BERT and GPT models
  - Quick check question: What preprocessing steps were taken to prepare the community data for embedding and classification?

- Concept: Vector embeddings and similarity search
  - Why needed here: To retrieve relevant community posts based on user queries, the system must convert text to numerical representations and find similar items
  - Quick check question: How does the system use embeddings to match user queries with relevant community posts?

- Concept: Evaluation metrics for NLP systems
  - Why needed here: To assess the quality of the chatbot's responses, multiple metrics (BLEU, ROUGE, Perplexity, METEOR) are used
  - Quick check question: What do the BLEU and ROUGE scores indicate about the quality of the chatbot's responses?

## Architecture Onboarding

- Component map:
  - Frontend (Flask web interface) → Query processing → LangChain RetrievalQA → Vector Store (FAISS) → GPT-4o model → Response generation → Frontend
  - Data pipeline: Web crawler (Selenium) → CSV Loader → Topic modeling → Manual labeling → Embedding (KLUE-BERT-base/GPT-ADA v2) → Vector Store

- Critical path:
  1. User query submission
  2. Query embedding and vector store search
  3. Retrieval of relevant community posts
  4. Response generation using GPT-4o
  5. Response delivery to user

- Design tradeoffs:
  - Community data provides practical information but requires extensive filtering
  - Manual labeling ensures quality but is resource-intensive
  - Using GPT-4o provides strong language understanding but requires careful prompt engineering

- Failure signatures:
  - High perplexity scores indicate poor model prediction
  - Low BLEU/ROUGE scores suggest responses don't match reference quality
  - Slow response times may indicate vector store inefficiencies

- First 3 experiments:
  1. Test the complete pipeline with a small set of hand-crafted queries and verify end-to-end functionality
  2. Evaluate the topic modeling accuracy by manually checking a sample of filtered vs. unfiltered posts
  3. Benchmark the retrieval system by measuring response time and relevance for various query types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the data filtering methodology be improved to better distinguish between comments and replies when collecting data from community platforms like Everytime?
- Basis in paper: [explicit] The paper identifies a major challenge in data collection: "the inability to distinguish between comments and replies, which prevented us from including additional questions in the question portion of the dataset."
- Why unresolved: The paper mentions this as a limitation but does not propose specific solutions or methods to address this issue in future work.
- What evidence would resolve it: A detailed methodology or algorithm that successfully distinguishes between comments and replies, validated through improved data collection and filtering results.

### Open Question 2
- Question: What are the most effective evaluation metrics for assessing the performance of chatbots, beyond traditional metrics like BLEU and ROUGE?
- Basis in paper: [explicit] The paper states, "we found that these metrics are not optimized for chatbot evaluation" and suggests that "Testing with human evaluators would have been ideal."
- Why unresolved: The paper acknowledges the limitations of current metrics but does not explore or propose alternative evaluation methods that might be more suitable for chatbots.
- What evidence would resolve it: Development and validation of new evaluation metrics specifically designed for chatbots, showing improved correlation with human judgment and practical performance.

### Open Question 3
- Question: How can the performance of the chatbot be improved to achieve higher BLEU, Perplexity, ROUGE, and METEOR scores?
- Basis in paper: [explicit] The paper reports relatively low scores: BLEU (0.0551), Perplexity (15.8756), ROUGE (1:0.0360, 2:0.0134, L:0.0340), and METEOR (0.0243), indicating room for improvement.
- Why unresolved: The paper identifies the need for improvement but does not provide specific strategies or experiments to enhance the model's performance in these metrics.
- What evidence would resolve it: Implementation of targeted improvements (e.g., fine-tuning techniques, data augmentation, model architecture changes) that result in measurable increases in these evaluation metrics.

## Limitations

- Evaluation methodology relies solely on automated metrics without human evaluation, which may not accurately reflect real-world chatbot performance
- Data collection is limited to a single university's community platform (SKKU Everytime), potentially limiting generalizability to other institutions or regions
- Performance scores indicate room for improvement, with relatively low BLEU (0.0551) and ROUGE scores suggesting responses may not consistently meet quality standards

## Confidence

- **High confidence**: The basic architecture and implementation approach using GPT-4o with LangChain for RAG-based retrieval is technically sound and well-established
- **Medium confidence**: The data collection and filtering methodology is appropriate but may have inconsistencies in manual labeling that could affect system quality
- **Low confidence**: The evaluation results and their interpretation are questionable without human evaluation to validate the automated metric scores

## Next Checks

1. Conduct human evaluation study with actual university freshmen to assess response quality, relevance, and helpfulness, comparing these results against the automated metric scores
2. Test the system with queries from multiple universities or different educational contexts to evaluate generalizability beyond SKKU's Everytime platform
3. Perform ablation studies to determine the individual contributions of topic modeling, manual labeling, and the RAG framework to overall system performance