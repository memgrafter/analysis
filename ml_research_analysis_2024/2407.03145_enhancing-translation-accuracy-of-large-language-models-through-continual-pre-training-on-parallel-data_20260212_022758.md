---
ver: rpa2
title: Enhancing Translation Accuracy of Large Language Models through Continual Pre-Training
  on Parallel Data
arxiv_id: '2407.03145'
source_url: https://arxiv.org/abs/2407.03145
tags:
- data
- translation
- pre-training
- continual
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-phase training approach where pre-trained
  large language models are continually pre-trained on parallel data and then supervised
  fine-tuned with a small amount of high-quality parallel data. We conducted continual
  pre-training with a 3.8B-parameter model and parallel data across eight different
  formats.
---

# Enhancing Translation Accuracy of Large Language Models through Continual Pre-Training on Parallel Data

## Quick Facts
- arXiv ID: 2407.03145
- Source URL: https://arxiv.org/abs/2407.03145
- Reference count: 30
- A two-phase training approach (continual pre-training + supervised fine-tuning) improves translation accuracy for Japanese-English translation using a 3.8B parameter LLM.

## Executive Summary
This paper proposes a two-phase training approach where pre-trained large language models are continually pre-trained on parallel data and then supervised fine-tuned with a small amount of high-quality parallel data. The authors conducted continual pre-training with a 3.8B-parameter model and parallel data across eight different formats, evaluating on thirteen test sets for Japanese-to-English and English-to-Japanese translation. The results demonstrate that when utilizing parallel data in continual pre-training, it is essential to alternate between source and target sentences, and that translation accuracy improves only for translation directions where the order of source and target sentences aligns between continual pre-training data and inference.

## Method Summary
The approach uses a 3.8B parameter decoder-only LLM (rinna/bilingual-gpt-neox-4b) and applies a two-phase training strategy. First, continual pre-training is performed on JParaCrawl v3.0 parallel data (20.8M sampled sentences) using eight different data formats including interleaved, tagged, and prefix variations. Then, supervised fine-tuning is conducted on high-quality parallel data (15k samples from WMT20, Flores-200, and KFTT). The model is evaluated using BLEU and COMET metrics across 13 test sets spanning scientific papers, movie subtitles, TED talks, and Reddit domains.

## Key Results
- Interleaving source and target sentences during continual pre-training significantly improves translation accuracy compared to consecutive sentence presentation.
- Translation accuracy improves only for directions where the source-target order in training matches inference direction.
- Adding explicit tags or prefixes to source sentences yields higher accuracy than simple concatenation.
- LLM-based translation models demonstrate greater robustness to spoken language domains compared to traditional encoder-decoder models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual pre-training with interleaved source and target sentences enables the model to learn translation-specific patterns that are absent in standard causal language modeling.
- Mechanism: By alternating between source and target sentences during training, the model observes the natural alignment between them in a continuous sequence, allowing it to learn the conditional dependencies required for translation.
- Core assumption: The model can generalize from this interleaved structure to perform translation during inference without requiring explicit encoder-decoder architecture.
- Evidence anchors:
  - [abstract]: "when utilizing parallel data in continual pre-training, it is essential to alternate between source and target sentences."
  - [section]: "continual pre-training involves training on data where the source and target sentences appear alternately."
  - [corpus]: Weak evidence; no direct mention of interleaving mechanism in neighboring papers.
- Break condition: If the alternating pattern is broken or source-target order is inconsistent, the model loses translation directionality and accuracy degrades.

### Mechanism 2
- Claim: Adding explicit tags or prefixes indicating translation direction improves accuracy compared to simple concatenation of source and target sentences.
- Mechanism: Tags and prefixes provide explicit conditioning signals to the model about the desired output language, reducing ambiguity in the generation process.
- Core assumption: The model can effectively use these explicit markers to condition generation without interfering with the learned translation patterns.
- Evidence anchors:
  - [abstract]: "the highest accuracy is achieved when the data for continual pre-training consists of interleaved source and target sentences and when tags are added to the source sentences."
  - [section]: "augmenting source sentences with tags or using prefixes yields higher accuracy than simple concatenation."
  - [corpus]: Weak evidence; neighboring papers don't discuss tagging mechanisms in detail.
- Break condition: If tags are inconsistent, missing, or incorrectly placed, the model may generate in the wrong language direction.

### Mechanism 3
- Claim: The LLM-based translation model demonstrates greater robustness to spoken language domains compared to traditional encoder-decoder models.
- Mechanism: LLMs trained on diverse web data, including conversational and informal text, develop better generalization to spoken language patterns than models trained primarily on formal text.
- Core assumption: The pre-training corpus contains sufficient spoken language examples for the model to learn these patterns.
- Evidence anchors:
  - [abstract]: "the LLM-based translation model is more robust in translating spoken language and achieves higher accuracy with less training data compared to supervised encoder-decoder models."
  - [section]: "LLM-based translation model significantly outperforms the Transformer on the WMT19, 20 Robustness Task for the Reddit domain, and on the TED (tst2015), IWSLT21 En-Ja Dev, and JESC for the TED Talk and movie subtitles domains."
  - [corpus]: No direct evidence in neighboring papers about spoken language robustness.
- Break condition: If training data lacks spoken language examples, the model's robustness advantage disappears.

## Foundational Learning

- Concept: Causal language modeling
  - Why needed here: The model predicts the next token based on previous tokens, which is fundamental to how continual pre-training and supervised fine-tuning work.
  - Quick check question: What distinguishes causal language modeling from masked language modeling in transformer architectures?

- Concept: Fine-tuning vs. full fine-tuning
  - Why needed here: The paper compares direct supervised fine-tuning (SFT) with LoRA tuning and full fine-tuning, showing different parameter efficiency and performance characteristics.
  - Quick check question: How does LoRA reduce the number of trainable parameters while maintaining performance in translation tasks?

- Concept: Translation direction alignment
  - Why needed here: The paper demonstrates that translation accuracy improves only when the source-target order in training matches the inference direction.
  - Quick check question: Why would alternating between English-Japanese and Japanese-English data in Mix format still improve accuracy for both directions?

## Architecture Onboarding

- Component map: Pre-trained 3.8B parameter decoder-only transformer (rinna-4b) → Continual pre-training module → Supervised fine-tuning module → Inference module with prompts
- Critical path: Continual pre-training (with parallel data) → Supervised fine-tuning (with high-quality parallel data) → Inference with appropriate prompts
- Design tradeoffs: Using a decoder-only architecture trades the explicit separation of encoder/decoder for parameter efficiency, but requires careful handling of translation direction through data formatting
- Failure signatures: Catastrophic forgetting when switching translation directions, degraded performance with monolingual pre-training, loss of directionality with inconsistent source-target ordering
- First 3 experiments:
  1. Compare Mono (consecutive sentences) vs. Mix (alternating sentences) to verify the importance of interleaving
  2. Test Tagged vs. Prefix vs. JSON formats to identify the most effective explicit direction indicators
  3. Evaluate performance on spoken language domains (TED, JESC) vs. formal domains to confirm robustness claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the two-phase training approach generalize to other language pairs beyond Japanese-English?
- Basis in paper: [explicit] The authors state "all experiments utilizing rinna-4b are conducted using the open-source huggingface transformers library" and only evaluate on Japanese-English pairs, but note "whether these formats can be applied to other translation directions and models remains a matter of our future work."
- Why unresolved: The paper only tests the approach on one specific language pair (Japanese-English) using a single pre-trained model (rinna-4b). The authors explicitly acknowledge this limitation.
- What evidence would resolve it: Experiments showing similar improvements in translation accuracy using the same two-phase approach (continual pre-training on parallel data + supervised fine-tuning) for other language pairs like English-French, English-Chinese, or low-resource language pairs.

### Open Question 2
- Question: What is the optimal ratio of continual pre-training data to supervised fine-tuning data for maximizing translation accuracy?
- Basis in paper: [inferred] The authors demonstrate that both phases are necessary ("achieving high accuracy is most feasible when both continual pre-training and supervised fine-tuning are conducted") and test different data quantities in Section 6.3, but don't systematically vary the ratio between the two phases.
- Why unresolved: While the paper shows that both phases improve accuracy, it doesn't explore how different proportions of data between the two phases affect final performance. The 15k samples used for supervised fine-tuning appears arbitrary.
- What evidence would resolve it: A systematic ablation study varying the amount of data used for continual pre-training versus supervised fine-tuning (e.g., 5k/30k, 15k/15k, 30k/5k) to identify the optimal data ratio for different model sizes.

### Open Question 3
- Question: How does the two-phase approach compare to alternative fine-tuning strategies like QLoRA or full fine-tuning on the same computational budget?
- Basis in paper: [explicit] The authors mention "LoRA enables training with fewer computational resources" and reference studies showing LoRA effectiveness, but don't compare their approach against LoRA-based methods directly.
- Why unresolved: The paper uses LoRA only for the Direct-SFT baseline and full fine-tuning for their main approach, without comparing against other efficient fine-tuning methods on equal computational footing.
- What evidence would resolve it: A head-to-head comparison between the proposed two-phase approach and QLoRA-based methods (using similar computational resources and data quantities) to determine which strategy yields better translation accuracy per training step.

## Limitations
- The translation accuracy improvements are highly dependent on maintaining consistent source-target ordering between training and inference.
- The reliance on specific data formatting (interleaving, tagging) introduces a brittle dependency that may limit practical applicability.
- The comparison with traditional encoder-decoder models may not fully account for architectural differences in handling long-range dependencies and domain robustness.

## Confidence
- High Confidence: The importance of interleaving source and target sentences during continual pre-training (Mechanism 1) is well-supported by systematic ablation studies across multiple formats and test sets.
- Medium Confidence: The superiority of LLM-based models for spoken language domains (Mechanism 3) is demonstrated but requires further validation across additional language pairs and more diverse spoken language datasets.
- Low Confidence: The robustness claims compared to encoder-decoder models may be overstated, as the comparison doesn't fully control for differences in training data, model size, and fine-tuning procedures.

## Next Checks
1. **Directionality Consistency Test**: Systematically vary the source-target ordering during inference while keeping training data consistent to quantify the exact sensitivity of translation accuracy to ordering mismatches.

2. **Spoken Language Generalization**: Test the model's performance on additional spoken language datasets (podcasts, conversational transcripts) and compare with encoder-decoder models trained on similarly diverse data to isolate the LLM advantage.

3. **Format Robustness Evaluation**: Introduce controlled noise into the training data formats (randomly broken interleaving, missing tags, incorrect prefixes) to measure the model's tolerance to formatting inconsistencies and identify failure thresholds.