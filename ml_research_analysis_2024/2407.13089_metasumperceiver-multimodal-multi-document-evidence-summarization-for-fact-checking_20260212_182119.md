---
ver: rpa2
title: 'MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for Fact-Checking'
arxiv_id: '2407.13089'
source_url: https://arxiv.org/abs/2407.13089
tags:
- evidence
- claim
- summary
- claims
- fact-checking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MetaSumPerceiver, a multimodal multi-document
  summarization model designed for fact-checking applications. The approach combines
  a dynamic perceiver architecture with reinforcement learning to generate claim-specific
  summaries from documents and images that assist human fact-checkers.
---

# MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for Fact-Checking

## Quick Facts
- arXiv ID: 2407.13089
- Source URL: https://arxiv.org/abs/2407.13089
- Reference count: 27
- 4.6% F-score improvement on claim verification over state-of-the-art

## Executive Summary
This paper introduces MetaSumPerceiver (MSP), a multimodal multi-document summarization model designed for fact-checking applications. The approach combines a dynamic perceiver architecture with reinforcement learning to generate claim-specific summaries from documents and images that assist human fact-checkers. The model employs a reward mechanism based on entailment classification to ensure generated summaries contain relevant evidence for verifying claims. Experiments on the MOCHEG dataset and a new Multi-News-Fact-Checking dataset show the method outperforms state-of-the-art by 4.6% F-score on claim verification. The system also demonstrates strong performance on explanation generation tasks, producing summaries that contain the necessary evidence for determining claim truthfulness across multiple modalities.

## Method Summary
MSP uses a dynamic perceiver-based architecture that handles multimodal inputs of arbitrary lengths by iteratively compressing long sequences into fixed-size representations while preserving essential information. The model combines cross-attention between claims and images with BART for text embedding and CLIP for image embedding. During training, MSP employs reinforcement learning with PPO using an entailment classifier as a reward model to guide summary generation. A KL divergence penalty maintains summary quality while encouraging exploration. The system also incorporates an LM critic (Mistral) to ensure summaries are concise and clear while retaining essential evidence for fact-checking.

## Key Results
- Outperforms state-of-the-art by 4.6% F-score on claim verification
- Demonstrates strong performance on new Multi-News-Fact-Checking dataset
- Generates high-quality summaries for explanation generation tasks
- Effectively handles multimodal inputs combining text documents and images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dynamic perceiver architecture allows MSP to handle multimodal inputs of arbitrary lengths effectively.
- Mechanism: The perceiver architecture uses iterative attention to compress long sequences into fixed-size representations while preserving essential information. By combining this with cross-attention between claims and images, MSP can process varying numbers of documents and images without sequence length limitations.
- Core assumption: Iterative attention can effectively capture relevant information across long sequences while maintaining computational efficiency.
- Evidence anchors: [abstract] "We introduce a dynamic perceiver-based model that can handle inputs from multiple modalities of arbitrary lengths." [section] "Due to the sequence length limitation, we utilize a combination of Perceiver with BART (Lewis et al., 2020) and CLIP (Wang et al., 2022b) to extract embeddings."
- Break condition: If the iterative attention fails to capture relevant evidence or if the cross-attention between modalities produces noisy representations that confuse the summarizer.

### Mechanism 2
- Claim: Reinforcement learning with entailment-based rewards guides the model to generate summaries containing evidence relevant for fact-checking.
- Mechanism: The model is trained using a reward function that measures the probability of the entailment model correctly predicting the claim's truthfulness label given the generated summary. This reward is combined with KL divergence to maintain summary quality while encouraging exploration.
- Core assumption: An entailment model can effectively serve as a surrogate fact-checker to provide meaningful feedback during training.
- Evidence anchors: [abstract] "To train our model, we leverage a novel reinforcement learning-based entailment objective to generate summaries that provide evidence distinguishing between different truthfulness labels." [section] "We utilized a comprehensive dataset consisted with MultiNLI (Williams et al., 2018), Fever-NLI (Thorne et al., 2018), and Adversarial-NLI (ANLI) (Nie et al., 2020), encompassing a total of 763,193 premise-claim pairs."
- Break condition: If the entailment model's predictions are not aligned with human fact-checking judgments, or if the KL penalty becomes too restrictive and prevents the model from generating novel, useful summaries.

### Mechanism 3
- Claim: The LM critic reward ensures generated summaries are concise and clear while retaining essential evidence for fact-checking.
- Mechanism: Mistral is used with a quality assessment prompt to evaluate summary clarity and conciseness. This reward is combined with the entailment reward to balance evidence content with readability.
- Core assumption: An LM can effectively assess summary quality in terms of clarity and conciseness in a way that aligns with human preferences for fact-checking summaries.
- Evidence anchors: [section] "We utilize Mistral (Jiang et al., 2023) along with a detailed quality testing prompt provided in the appendix to assess this aspect." [section] "The LM critic ensures that the summary is more concise and clear while retaining the essential meaning in the summary."
- Break condition: If the LM critic's quality assessments don't correlate with human judgments of summary usefulness for fact-checking, or if the conciseness reward causes loss of important evidence.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: MSP uses a similar approach but replaces human feedback with an entailment model to guide summary generation for fact-checking.
  - Quick check question: How does the reward function in MSP differ from standard RLHF approaches that use human feedback?

- Concept: Multimodal Representation Learning
  - Why needed here: MSP needs to effectively combine text and image representations to create summaries that capture evidence across modalities.
  - Quick check question: What role does cross-attention play in combining claim embeddings with image embeddings?

- Concept: Abstractive Summarization
  - Why needed here: Unlike extractive methods, MSP generates novel summaries that synthesize evidence from multiple documents rather than simply selecting sentences.
  - Quick check question: How does the pre-training with cross-entropy loss prepare the model for the reinforcement learning phase?

## Architecture Onboarding

- Component map: Documents/Images → Perceiver → Cross-attention → Summarizer → Entailment/LM critics → PPO update → Improved summary
- Critical path: Documents/Images → Perceiver → Cross-attention → Summarizer → Entailment/LM critics → PPO update → Improved summary
- Design tradeoffs: 
  - Using entailment model as critic vs human feedback (faster but potentially less aligned)
  - Including image evidence vs text-only (more comprehensive but computationally heavier)
  - KL penalty coefficient (exploration vs stability)
- Failure signatures:
  - Low entailment accuracy indicates missing key evidence
  - High KL divergence suggests summaries diverging from original style
  - Low LM critic scores indicate unclear or verbose summaries
- First 3 experiments:
  1. Test claim verification accuracy with and without image evidence on MOCHEG
  2. Compare entailment accuracy using different surrogate models (DeBERTa V3 vs Llama 2)
  3. Evaluate summary quality with and without LM critic reward on the Multi-News-Fact-Checking dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of generated claims affect the overall performance of the fact-checking system?
- Basis in paper: [inferred] The paper mentions that the generated claims from Llama 2 had an average quality score of 3.61, and that neutral claims might mix consistent and conflicting details, suggesting potential quality issues
- Why unresolved: The paper does not conduct experiments to isolate the impact of claim quality on downstream fact-checking performance
- What evidence would resolve it: Controlled experiments comparing system performance using claims of varying quality scores, or ablation studies removing low-quality claims

### Open Question 2
- Question: What is the impact of visual information on summary quality and fact-checking performance?
- Basis in paper: [explicit] The paper states "our method demonstrates strong performance on our new Multi-News-Fact-Checking dataset" and includes visual embeddings, but does not provide direct comparisons with and without visual information
- Why unresolved: The paper mentions using CLIP for visual features but doesn't provide ablation studies showing the specific contribution of visual information to performance
- What evidence would resolve it: Direct performance comparisons of the system with and without visual information across multiple metrics (F-score, ROUGE, etc.)

### Open Question 3
- Question: How well does the system generalize to languages other than English?
- Basis in paper: [explicit] The paper states "Our model, trained on English text and topics from the Multi-News benchmarks, may not perform well in other languages without retraining"
- Why unresolved: The paper only evaluates on English datasets and explicitly acknowledges this limitation without exploring multilingual capabilities
- What evidence would resolve it: Experiments testing the system on non-English fact-checking datasets or multilingual fact-checking benchmarks

### Open Question 4
- Question: What is the effect of the KL divergence coefficient (η) on the quality of generated summaries?
- Basis in paper: [explicit] The paper mentions setting η to 0.2 but states it "functions as an entropy boost, enhancing exploration throughout the policy domain" without exploring how different values affect performance
- Why unresolved: The paper uses a fixed value for η without conducting sensitivity analysis or ablation studies with different coefficient values
- What evidence would resolve it: Experiments varying η across a range of values and measuring impact on summary quality metrics and fact-checking performance

### Open Question 5
- Question: How does the system perform on fact-checking tasks outside of news articles?
- Basis in paper: [inferred] The system is trained on Multi-News dataset containing news, policy, weather, and sports topics, but the paper doesn't test on other domains like scientific claims or social media content
- Why unresolved: The paper only evaluates on datasets derived from news sources and doesn't explore performance on other types of claims or domains
- What evidence would resolve it: Testing the system on fact-checking datasets from different domains (scientific literature, social media, political speeches, etc.) and comparing performance across domains

## Limitations
- Reliance on surrogate entailment model rather than human feedback may introduce alignment issues
- Computational complexity of dynamic perceiver architecture may limit real-world deployment scalability
- Evaluation primarily uses automated metrics rather than human judgment of summary usefulness

## Confidence
- **High Confidence**: The architectural design combining perceiver and reinforcement learning is well-justified and technically sound
- **Medium Confidence**: The experimental results showing 4.6% F-score improvement are promising but depend on the quality of the entailment classifier
- **Medium Confidence**: The approach for handling multimodal inputs is theoretically sound, though practical effectiveness needs further validation

## Next Checks
1. Conduct human evaluation studies comparing MSP-generated summaries against human-written evidence summaries for claim verification accuracy
2. Test model performance on claims with mixed or uncertain truthfulness labels (NEI) to assess robustness
3. Measure computational efficiency and memory requirements during inference to evaluate real-world deployment feasibility