---
ver: rpa2
title: Dynamic Evidence Decoupling for Trusted Multi-view Learning
arxiv_id: '2410.03796'
source_url: https://arxiv.org/abs/2410.03796
tags:
- evidence
- multi-view
- learning
- consistent
- ccml
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of uncertainty estimation in multi-view
  learning, specifically addressing the issue of semantic vagueness in real-world
  data. Existing methods often neglect this problem, leading to inaccurate uncertainty
  estimates.
---

# Dynamic Evidence Decoupling for Trusted Multi-view Learning

## Quick Facts
- arXiv ID: 2410.03796
- Source URL: https://arxiv.org/abs/2410.03796
- Reference count: 40
- Key outcome: CCML achieves 91.54% accuracy on Colored-MNIST vs 87.15% for second-best method

## Executive Summary
This paper addresses the challenge of uncertainty estimation in multi-view learning by proposing a method that dynamically decouples consistent and complementary evidence from multiple views. The Consistent and Complementary-aware trusted Multi-view Learning (CCML) method improves upon existing approaches by recognizing that real-world multi-view data often contains semantic vagueness where different views may provide conflicting but not entirely incorrect information about categories. By separately processing shared information (consistent evidence) and view-specific uncertainty (complementary evidence), CCML achieves superior accuracy and more reliable uncertainty estimates compared to state-of-the-art baselines.

## Method Summary
The CCML method uses evidential deep neural networks to learn view-specific evidence, then dynamically decouples consistent and complementary evidence. Consistent evidence is derived from shared information across all views (using minimum evidence across views for each class), while complementary evidence is obtained by averaging the differing portions. The method enforces strict alignment between consistent evidence and ground-truth labels using a separation degree objective and KL divergence term, while allowing potential vagueness in complementary evidence. Training uses a combination of cross-entropy loss, KL divergence loss, and a joint loss function with hyperparameters Î´ and Î³, implemented with Adam optimizer and L2 regularization.

## Key Results
- CCML achieves 91.54% accuracy on Colored-MNIST dataset, outperforming second-best method (87.15%)
- Superior uncertainty estimation capability, effectively distinguishing in-distribution from out-of-distribution data
- Consistent performance improvements across multiple real-world datasets (HandWritten1, Scene15, CUB, LandUse, PIE)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamically decoupling consistent and complementary evidence improves accuracy when semantic vagueness is present.
- Mechanism: The method computes consistent evidence by taking the minimum of evidence across views for each class, and complementary evidence by averaging the differing portions. This separation allows consistent evidence to focus on shared information and complementary evidence to capture view-specific uncertainty without forcing one view to resolve all ambiguities.
- Core assumption: Semantic vagueness exists when different views provide conflicting but not entirely incorrect information about categories.
- Evidence anchors:
  - [abstract] "The consistent evidence is derived from the shared portions across all views, while the complementary evidence is obtained by averaging the differing portions across all views."
  - [section 3.2.2] "The consistent evidence ð’†ð‘ð‘œð‘› is obtained by aggregating the consistent portions from all views, while the complementary evidence ð’†ð‘ð‘šð‘ is calculated as the average of the differing portions across all views."
- Break condition: If all views are equally vague for all categories, the minimum-based consistent evidence may become uninformative.

### Mechanism 2
- Claim: Allowing potential vagueness in complementary evidence prevents overconfident predictions.
- Mechanism: The complementary loss only requires the probability of the true category to be reflected, without enforcing separation or reducing evidence for false categories. This allows the model to express uncertainty when views disagree.
- Core assumption: Some views may be ambiguous or unreliable for certain categories, and forcing them to resolve this ambiguity can hurt performance.
- Evidence anchors:
  - [abstract] "For the opinion constructed from the complementary evidence, we allow it for potential vagueness in the evidence."
  - [section 3.3.3] "Complementary evidence usually represents complementary or even conflicting information between different views. Its reliability is generally lower than that of the consistent evidence."
- Break condition: If complementary evidence is systematically wrong, allowing vagueness could propagate errors.

### Mechanism 3
- Claim: Enforcing strict alignment between consistent evidence and ground-truth labels improves separability.
- Mechanism: The consistent loss includes a separation degree objective that increases the disparity in belief masses between classes, and a KL divergence term that reduces evidence for incorrect categories.
- Core assumption: When views agree, they provide reliable information that should be leveraged to create clear class boundaries.
- Evidence anchors:
  - [section 3.3.2] "We enforce strict alignment between the opinion constructed from the consistent evidence and the ground-truth category. This is achieved by adjusting the probabilities of the true and false categories, as well as enhancing the separation between them."
  - [section 3.3.2] "The Separability Principle emphasizes the importance of creating a significant distinction between the evidence supporting different categories during the classification process."
- Break condition: If consistent evidence is based on noise or systematic bias, enforcing alignment could amplify errors.

## Foundational Learning

- Concept: Evidential Deep Learning (EDL) with subjective logic
  - Why needed here: EDL allows modeling both belief masses and uncertainty, which is essential for trusted multi-view learning.
  - Quick check question: How does EDL differ from softmax-based classification in terms of uncertainty representation?

- Concept: Multi-view fusion strategies (feature vs. decision level)
  - Why needed here: The paper uses decision-level fusion to avoid propagating low-quality view features.
  - Quick check question: What are the advantages and disadvantages of feature-level vs. decision-level multi-view fusion?

- Concept: Semantic vagueness in multi-view data
  - Why needed here: Understanding this phenomenon is crucial for appreciating why existing methods fail and why CCML is needed.
  - Quick check question: Can you provide an example of semantic vagueness from a real-world multi-view dataset?

## Architecture Onboarding

- Component map: View-specific evidential DNNs -> Dynamic decoupling layer -> Consistent loss module -> Complementary loss module -> Final aggregation -> Prediction

- Critical path: View-specific DNNs â†’ Dynamic decoupling â†’ Separate loss computation â†’ Final aggregation â†’ Prediction

- Design tradeoffs:
  - Using minimum for consistent evidence: Robust to view noise but may lose information
  - Averaging complementary evidence: Simple but may not capture complex view interactions
  - Strict alignment for consistent evidence: Improves separability but may overfit to consistent views

- Failure signatures:
  - Low accuracy on datasets with little semantic vagueness
  - High uncertainty estimates even for confident predictions
  - Performance degradation when consistent evidence is based on systematic bias

- First 3 experiments:
  1. Compare CCML with TMC on a synthetic dataset with controlled semantic vagueness.
  2. Ablation study removing dynamic decoupling to quantify its impact.
  3. Test CCML on a real-world dataset with known view reliability issues (e.g., Colored-MNIST).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dynamic evidence decoupling strategy in CCML compare to static methods in terms of handling varying degrees of semantic vagueness across different instances?
- Basis in paper: [explicit] The paper mentions that previous work [37] uses a relatively static fusion strategy, while CCML employs a dynamic fusion strategy that adjusts the fusion weight based on the consistency of different samples.
- Why unresolved: The paper does not provide a direct comparison between CCML's dynamic strategy and static methods on datasets with varying degrees of semantic vagueness.
- What evidence would resolve it: Conducting experiments on datasets with varying degrees of semantic vagueness and comparing the performance of CCML with static methods would provide insights into the effectiveness of the dynamic decoupling strategy.

### Open Question 2
- Question: What is the impact of the hyperparameter Î² on the model's performance in handling semantic vagueness, and how can an optimal value be determined for different datasets?
- Basis in paper: [explicit] The paper mentions that the hyperparameter Î² is used to increase the separation degree of consistent opinions and that its value can significantly impact the model's performance, as demonstrated on the Colored-MNIST dataset.
- Why unresolved: The paper does not provide a systematic analysis of the impact of Î² on the model's performance across different datasets or guidelines for determining an optimal value.
- What evidence would resolve it: Conducting a comprehensive analysis of the impact of Î² on the model's performance across various datasets and developing guidelines for selecting an optimal value would provide insights into its role in handling semantic vagueness.

### Open Question 3
- Question: How does CCML's uncertainty estimation capability compare to other methods when dealing with out-of-distribution data in multi-view learning scenarios?
- Basis in paper: [explicit] The paper mentions that CCML's uncertainty estimation is evaluated by constructing out-of-distribution instances and observing the increase in uncertainty as noise intensity increases.
- Why unresolved: The paper does not provide a direct comparison of CCML's uncertainty estimation capability with other methods in multi-view learning scenarios involving out-of-distribution data.
- What evidence would resolve it: Conducting experiments comparing CCML's uncertainty estimation capability with other methods on datasets containing out-of-distribution data in multi-view learning scenarios would provide insights into its effectiveness in handling such cases.

## Limitations

- The method assumes semantic vagueness manifests as systematic disagreement between views, but real-world data may exhibit more complex patterns of ambiguity.
- The minimum-based approach for consistent evidence may lose information when views have different but valid perspectives on a category.
- Performance may degrade when consistent evidence is based on noise or systematic bias, as enforcing alignment could amplify these errors.

## Confidence

- **High confidence**: The mechanism of separating consistent and complementary evidence is sound and well-justified by the literature on subjective logic and evidential deep learning.
- **Medium confidence**: The specific implementation choices (minimum for consistent evidence, averaging for complementary) are reasonable but may not be optimal for all types of semantic vagueness.
- **Medium confidence**: The experimental results demonstrate clear improvements over baselines, but the evaluation on synthetic datasets (Colored-MNIST) may not fully capture real-world complexity.

## Next Checks

1. Test CCML on a multi-view dataset where one view is known to be systematically biased or noisy to verify that the complementary evidence mechanism effectively isolates and handles this unreliability.

2. Perform an ablation study comparing CCML with and without the dynamic decoupling layer on a dataset with controlled amounts of semantic vagueness to quantify the specific contribution of this mechanism.

3. Evaluate CCML's uncertainty estimates on out-of-distribution samples from a different domain than the training data to verify that the method can distinguish between in-distribution ambiguity and true distributional shift.