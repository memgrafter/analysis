---
ver: rpa2
title: Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach for Temporal
  Knowledge Graph Forecasting
arxiv_id: '2408.13273'
source_url: https://arxiv.org/abs/2408.13273
tags:
- sla-tkgf
- forecasting
- framework
- historical
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: sLA-tKGF addresses limitations of large language models in temporal
  knowledge graph (tKG) forecasting by using retrieval-augmented generation with custom-trained
  small-scale models. It constructs knowledge-augmented prompts from historical tKG
  data, web search, and PLLM-generated descriptions to ground predictions in historical
  context.
---

# Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach for Temporal Knowledge Graph Forecasting

## Quick Facts
- **arXiv ID:** 2408.13273
- **Source URL:** https://arxiv.org/abs/2408.13273
- **Reference count:** 40
- **Key outcome:** sLA-tKGF achieves state-of-the-art performance on tKG forecasting with reduced hallucinations through knowledge-augmented prompts and custom-trained small-scale models.

## Executive Summary
sLA-tKGF addresses the challenge of temporal knowledge graph (tKG) forecasting by combining retrieval-augmented generation with custom-trained small-scale language models. The framework constructs knowledge-augmented prompts from historical tKG data, web search results, and PLLM-generated descriptions to ground predictions in historical context. This approach reduces hallucinations, mitigates distributional shift, and achieves state-of-the-art performance on benchmark datasets while avoiding the pitfalls of pre-trained models.

## Method Summary
The sLA-tKGF framework employs a retrieval-augmented generation approach with custom-trained small-scale transformer models trained from scratch. It constructs knowledge-augmented prompts by combining historical tKG facts, web search results, and PLLM-generated entity relationship descriptions. The framework uses a temporal context window to extract relevant historical facts, filters them using semantic similarity, and trains small-scale models from scratch with specific hyperparameters (batch size=48, epochs=30, embedding dimension=128) to predict missing entities in future tKG snapshots.

## Key Results
- Achieves state-of-the-art performance on benchmark datasets (ICEWS14, ICEWS18, ICEWS05-15, WIKI, YAGO, ACLED-CD22) with MRR and Hits@K metrics
- Reduces hallucinations through knowledge-augmented prompts grounded in verifiable historical context
- Demonstrates robustness to distributional shifts while avoiding future data leakage through the tabula rasa approach

## Why This Works (Mechanism)

### Mechanism 1
Knowledge-augmented prompts reduce hallucinations by grounding predictions in verifiable historical context. The framework constructs prompts that combine historical tKG facts, web search results, and PLLM-generated entity relationship descriptions. This multi-source grounding provides traceable, context-rich input that constrains the model's generation to factually consistent outputs.

### Mechanism 2
Training from scratch (Tabula Rasa) avoids bias and data leakage inherent in pre-trained models. By custom-training a small-scale language model from scratch, the framework ensures predictions are based solely on the provided knowledge-augmented prompts rather than pre-existing knowledge that may include future information.

### Mechanism 3
Temporal information in historical facts is critical for accurate forecasting. The framework leverages chronological ordering of historical events to capture evolving entity relationships and trends over time, enabling the model to reason about temporal dynamics.

## Foundational Learning

- **Temporal Knowledge Graphs (tKGs)**
  - Why needed here: Understanding tKGs is fundamental to grasping how the framework models evolving entity relationships over time.
  - Quick check question: How does a tKG differ from a static KG, and what additional information does it capture?

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: RAG is the core technique that enables the framework to ground language model predictions in external knowledge sources.
  - Quick check question: What are the key components of a RAG system, and how do they work together to improve generation quality?

- **Zero-shot learning**
  - Why needed here: The framework uses zero-shot prompting to make predictions without fine-tuning on the specific forecasting task.
  - Quick check question: What is the difference between zero-shot, one-shot, and few-shot learning, and when is each approach most appropriate?

## Architecture Onboarding

- **Component map:** Small-scale language model (trained from scratch) → RAG pipeline (retrieves historical tKG facts, web search results, PLLM descriptions) → Knowledge-augmented prompt construction → Zero-shot prediction generation
- **Critical path:** Web scraping → Historical fact retrieval → PLLM description generation → Prompt construction → Small-scale model inference → Prediction output
- **Design tradeoffs:** Smaller model size for reduced computational cost vs. larger models for potentially better performance; from-scratch training for bias avoidance vs. pre-training for faster convergence
- **Failure signatures:** High hallucination rates (grounding insufficient), poor performance on unseen entities (inductive bias too strong), slow inference times (computational bottleneck)
- **First 3 experiments:**
  1. Test knowledge-augmented prompts vs. plain prompts on a small dataset to measure hallucination reduction
  2. Compare performance of scratch-trained vs. pre-trained small-scale models on tKG forecasting
  3. Evaluate the impact of different temporal context window sizes on prediction accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of sLA-tKGF scale with increasingly large temporal knowledge graphs (tKGs) in terms of both accuracy and computational efficiency? The paper demonstrates state-of-the-art performance on benchmark datasets but does not explicitly evaluate scalability to massive real-world tKGs or analyze computational efficiency trade-offs.

### Open Question 2
What is the optimal balance between retrieved historical facts and web search results for constructing knowledge-augmented prompts across different tKG forecasting scenarios? The paper shows that knowledge-augmented prompts improve performance but does not systematically explore the optimal weighting or combination of historical facts versus web search results.

### Open Question 3
How robust is sLA-tKGF to temporal distribution shifts in tKG data, particularly when forecasting events that occur in drastically different temporal contexts than historical data? While the paper mentions handling distributional shifts, it does not test the framework's performance when forecasting events that occur in temporal contexts significantly different from the training data.

## Limitations

- The small-scale model's performance heavily depends on the quality of retrieved knowledge, making it vulnerable to web search noise and outdated information
- The "Tabula Rasa" approach may sacrifice the rich semantic understanding that pre-trained models offer, potentially limiting generalization
- Temporal context window selection (m=25) appears arbitrary and may not generalize across different temporal granularities or domain characteristics

## Confidence

- **High Confidence**: Knowledge-augmented prompts reduce hallucinations (supported by multiple ablation studies and clear causal mechanism)
- **Medium Confidence**: Training from scratch avoids bias and data leakage (plausible but requires more rigorous validation against pre-trained alternatives)
- **Low Confidence**: Small-scale models achieve competitive performance (benchmark comparisons show strong results, but comparisons may not be entirely fair due to different training approaches)

## Next Checks

1. **Ablation Study on Knowledge Sources**: Systematically disable each knowledge source (historical facts, web search, PLLM descriptions) to quantify their individual contributions to performance and hallucination reduction.

2. **Temporal Context Sensitivity Analysis**: Vary the context window size (m) across different datasets and temporal granularities to determine optimal settings and identify failure modes when context is too narrow or too broad.

3. **Cross-Domain Generalization Test**: Evaluate the framework on temporal knowledge graphs from domains not represented in the training data (e.g., financial transactions, healthcare records) to assess true zero-shot generalization capabilities.