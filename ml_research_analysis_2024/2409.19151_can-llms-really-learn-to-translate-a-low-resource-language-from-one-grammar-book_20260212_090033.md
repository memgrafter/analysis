---
ver: rpa2
title: Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar
  Book?
arxiv_id: '2409.19151'
source_url: https://arxiv.org/abs/2409.19151
tags:
- book
- translation
- grammar
- language
- parallel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work evaluates the claim that large language models can learn\
  \ to translate extremely low-resource languages from a single grammar book, using\
  \ Kalamang as a case study. Through controlled experiments, the authors show that\
  \ most translation improvements come from the grammar book\u2019s parallel examples\
  \ rather than its grammatical explanations, and that similar results can be achieved\
  \ by fine-tuning a small translation model on the same parallel data."
---

# Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?

## Quick Facts
- **arXiv ID**: 2409.19151
- **Source URL**: https://arxiv.org/abs/2409.19151
- **Reference count**: 40
- **One-line primary result**: Most translation improvements from grammar books come from parallel examples rather than grammatical explanations.

## Executive Summary
This work evaluates whether large language models can learn to translate extremely low-resource languages from a single grammar book, using Kalamang as a case study. Through controlled experiments, the authors show that most translation improvements come from the grammar book's parallel examples rather than its grammatical explanations, and that similar results can be achieved by fine-tuning a small translation model on the same parallel data. They also introduce a typological feature prompt, which outperforms grammatical explanations on linguistic tasks like grammaticality judgment and gloss prediction but not on translation. Overall, the results suggest that for low-resource machine translation, parallel data is more effective and efficient than linguistic descriptions, and that task-appropriate data should be prioritized.

## Method Summary
The study evaluates whether LLMs can learn to translate low-resource languages from a single grammar book by disentangling the effects of parallel examples versus grammatical explanations. The authors use Kalamang, Nepali, and Guarani grammar books, extracting parallel examples, grammatical explanations, and typological features. They conduct in-context learning experiments with Gemini-1.5-Flash and Llama-3.1-8B using various prompt configurations, and also fine-tune NLLB and Llama models on the parallel data. Translation quality is measured by CHRF++, with additional linguistic tasks including grammaticality judgment and IGT prediction. Controlled ablation experiments compare performance across different prompt types and model architectures.

## Key Results
- Translation performance is primarily driven by parallel examples rather than grammatical explanations in grammar books
- Typological feature prompts outperform grammatical explanations on linguistic tasks but not translation
- Fine-tuning small translation models on parallel data achieves similar results to in-context learning with LLMs
- For low-resource MT, parallel data is more effective and efficient than linguistic descriptions

## Why This Works (Mechanism)
The effectiveness of grammar books for translation depends on the quality and quantity of parallel examples they contain. Grammatical explanations provide limited benefit because they describe abstract rules that may not directly map to translation patterns. LLMs can leverage parallel examples through pattern matching and statistical generalization, but struggle with purely descriptive grammatical content. The typological feature prompt works better for linguistic tasks because it provides explicit structural information that aligns with how models process typological patterns.

## Foundational Learning
- **In-context learning**: LLMs can learn from examples without parameter updates; needed because it's the primary method for evaluating grammar book effectiveness
- **Parallel data extraction**: Converting grammar book examples into usable translation pairs; needed to isolate the contribution of parallel examples
- **Typological features**: Linguistic characteristics that describe language structure; needed for the typological prompt intervention
- **CHRF++ evaluation**: Character-level metric for translation quality; needed for robust evaluation of low-resource translation
- **Controlled ablation**: Method for isolating variable effects; needed to separate contributions of different grammar book components
- **Fine-tuning vs in-context**: Different approaches to adapting models; needed to compare efficiency and effectiveness

## Architecture Onboarding

**Component Map**: Grammar Book → Preprocessing → Prompt Construction → Model → Evaluation → Analysis

**Critical Path**: The experiments flow from grammar book preprocessing through various prompt constructions to model evaluation, with controlled comparisons between parallel examples, grammatical explanations, and typological prompts.

**Design Tradeoffs**: 
- Long-context LLMs vs fine-tuning: In-context learning preserves original parameters but has context limits; fine-tuning requires data but can achieve similar performance
- Parallel examples vs grammatical explanations: Parallel data is more effective but may be limited in grammar books; grammatical explanations provide broader coverage but less direct translation guidance

**Failure Signatures**: 
- Poor translation performance indicates insufficient parallel examples or vocabulary coverage
- Context length errors suggest grammar book subsets are too large for model context windows
- No significant differences between prompt types suggests the interventions aren't effectively isolating their intended contributions

**First Experiments**:
1. Test basic in-context translation with BOOK para prompt to establish baseline performance
2. Compare BOOK para vs BOOK non-para to verify parallel examples drive improvements
3. Evaluate typological prompt on grammaticality judgment task to confirm it works for linguistic tasks

## Open Questions the Paper Calls Out
- How do results change with test sets containing more complex and varied sentence structures in Kalamang?
- Would different LLM architectures or fine-tuning approaches yield better results for low-resource translation?
- Can the typological feature prompt be further optimized to improve translation performance?

## Limitations
- Kalamang case study may not generalize to other extremely low-resource languages with different grammatical structures
- Evaluation relies solely on CHRF++, which may not capture all aspects of translation adequacy and fluency
- Preprocessing steps for converting LaTeX-formatted grammar books into usable subsets are not fully detailed

## Confidence
- Translation performance is primarily driven by parallel examples rather than grammatical explanations: **High**
- Typological feature prompts outperform grammatical explanations on linguistic tasks but not translation: **Medium**
- Fine-tuning small translation models on parallel data achieves similar results to in-context learning: **High**
- For low-resource MT, parallel data is more effective and efficient than linguistic descriptions: **Medium**

## Next Checks
1. Replicate the preprocessing pipeline on the LaTeX-formatted grammar books to ensure consistent extraction of parallel examples versus grammatical explanations
2. Conduct experiments with additional low-resource languages to test generalizability beyond Kalamang
3. Evaluate translation quality using complementary metrics (e.g., BLEU, COMET) alongside CHRF++ to assess robustness of findings