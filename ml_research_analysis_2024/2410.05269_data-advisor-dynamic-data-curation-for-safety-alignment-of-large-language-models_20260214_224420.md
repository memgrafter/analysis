---
ver: rpa2
title: 'Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language
  Models'
arxiv_id: '2410.05269'
source_url: https://arxiv.org/abs/2410.05269
tags:
- data
- safety
- advisor
- arxiv
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DATA ADVISOR, a method to dynamically guide
  LLM-based data generation using predefined principles to improve data quality and
  coverage for safety alignment. It iteratively summarizes generated data, identifies
  underrepresented safety issues, and advises the next data generation step accordingly.
---

# Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models

## Quick Facts
- **arXiv ID:** 2410.05269
- **Source URL:** https://arxiv.org/abs/2410.05269
- **Reference count:** 14
- **Key outcome:** Dynamic LLM-guided data curation achieves +10.1 safety score on CatQA and +4.6 on BeaverTails while maintaining utility across Mistral, Llama2, and Falcon models

## Executive Summary
This paper introduces DATA ADVISOR, a dynamic data curation framework that iteratively guides LLM-based safety data generation using predefined principles. The method summarizes generated data to identify underrepresented safety issues and directs subsequent generation steps to fill these gaps. Experiments demonstrate that models fine-tuned with DATA ADVISOR-generated data significantly outperform Self-Instruct baselines on safety benchmarks while maintaining or improving utility across multiple base models.

## Method Summary
DATA ADVISOR operates through an iterative process where LLM-generated safety data is continuously summarized and analyzed against predefined safety principles. The system identifies gaps in safety issue coverage and generates targeted prompts to address these underrepresented areas. This dynamic guidance ensures comprehensive safety issue coverage throughout the data generation process, resulting in higher-quality safety alignment datasets compared to static approaches like Self-Instruct.

## Key Results
- Models trained on DATA ADVISOR-generated data achieve +10.1 safety score improvement on CatQA benchmark
- Significant +4.6 safety score improvement on BeaverTails benchmark
- Maintains or improves utility while enhancing safety across Mistral, Llama2, and Falcon base models
- Generated datasets demonstrate diverse coverage of fine-grained safety issues

## Why This Works (Mechanism)
The effectiveness stems from the iterative feedback loop that continuously identifies and addresses coverage gaps in safety issues. By summarizing generated data and comparing against predefined principles, the system can target specific underrepresented safety scenarios rather than relying on random or uniform sampling. This targeted approach ensures comprehensive safety issue coverage while avoiding redundancy, leading to more efficient and effective safety alignment compared to static data generation methods.

## Foundational Learning
- **Iterative data curation**: Why needed - static datasets often miss rare safety scenarios; Quick check - measure coverage diversity across iterations
- **Safety principle alignment**: Why needed - ensures generated data addresses all relevant harmful behaviors; Quick check - verify principle completeness through expert review
- **LLM-guided generation**: Why needed - leverages model capabilities for targeted safety issue creation; Quick check - compare coverage quality against human-curated datasets
- **Dynamic prompt adaptation**: Why needed - adjusts generation strategy based on real-time coverage analysis; Quick check - track prompt effectiveness across generation cycles
- **Multi-model evaluation**: Why needed - validates method robustness across different base model architectures; Quick check - test on additional model families beyond Mistral, Llama2, and Falcon

## Architecture Onboarding

**Component Map:**
DATA ADVISOR -> Data Generation Engine -> Coverage Analyzer -> Principle Comparator -> Prompt Optimizer

**Critical Path:**
Generation → Summarization → Gap Analysis → Targeted Prompt Generation → Validation → Next Generation

**Design Tradeoffs:**
- **Coverage vs. efficiency**: Comprehensive safety coverage requires more iterations but ensures better alignment
- **Principle specificity vs. generality**: Detailed principles enable precise targeting but may miss unforeseen safety issues
- **Model dependency vs. independence**: Method works across models but optimal principles may vary by architecture

**Failure Signatures:**
- Coverage saturation where additional iterations yield diminishing returns
- Principle misalignment leading to generation of irrelevant safety scenarios
- Model-specific blind spots where certain safety issues remain underrepresented

**First 3 Experiments to Run:**
1. Baseline comparison between DATA ADVISOR and random safety data generation
2. Ablation study removing individual safety principles to measure contribution impact
3. Cross-architecture validation testing method effectiveness on GPT and Claude family models

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes predefined safety principles capture all relevant harmful behaviors
- Safety evaluations limited to specific datasets (CatQA, BeaverTails) without broader generalization testing
- Utility benchmarks not specified in detail, making trade-off assessment difficult
- Potential saturation or diminishing returns over extended iteration cycles not fully analyzed

## Confidence
- **High confidence**: Iterative summarization and targeted generation methodology is technically sound with consistent improvements across multiple base models
- **Medium confidence**: Generated datasets cover "diverse fine-grained safety issues" based on process design but needs granular coverage analysis
- **Medium confidence**: Method remains effective "throughout generation process" supported by results but lacks detailed saturation analysis

## Next Checks
1. Conduct ablation studies removing individual safety principles to quantify their relative contribution to safety improvement
2. Test the method on additional base models from different training regimes (GPT, Claude families) to assess cross-architecture robustness
3. Perform long-term stability analysis to measure safety performance degradation or improvement over multiple fine-tuning iterations using DATA ADVISOR-generated data