---
ver: rpa2
title: Semi-Supervised Learning for Bilingual Lexicon Induction
arxiv_id: '2402.07028'
source_url: https://arxiv.org/abs/2402.07028
tags:
- learning
- languages
- translation
- word
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents RUBI, a semi-supervised learning framework
  for bilingual lexicon induction that leverages prior knowledge of multiple languages
  to improve translation accuracy without requiring parallel data for the target language
  pair. The core innovation lies in treating lexicon induction as a ranking problem,
  using Learning to Rank techniques to learn complex similarity criteria from a pivot
  language dictionary, then applying this learned criterion to infer translations
  between other language pairs.
---

# Semi-Supervised Learning for Bilingual Lexicon Induction

## Quick Facts
- arXiv ID: 2402.07028
- Source URL: https://arxiv.org/abs/2402.07028
- Reference count: 11
- Primary result: RUBI framework improves bilingual lexicon induction accuracy by up to 25% over unsupervised methods, reaching 95.3% for English-Spanish

## Executive Summary
This paper introduces RUBI, a semi-supervised learning framework for bilingual lexicon induction that leverages prior knowledge of multiple languages to improve translation accuracy without requiring parallel data. The core innovation treats lexicon induction as a ranking problem, using Learning to Rank techniques to learn complex similarity criteria from a pivot language dictionary, then applying this learned criterion to infer translations between other language pairs. Experiments on over 20 languages show consistent improvements over state-of-the-art unsupervised methods, with particularly strong performance for language pairs with initially poor alignments.

## Method Summary
RUBI operates in two phases: first, it aligns embedding spaces using Wasserstein Procrustes without parallel data, then trains a Learning to Rank model on a pivot language dictionary to learn complex similarity criteria. The method extracts training queries from the pivot dictionary, uses 11 features including CSLS-based similarity measures, and applies the learned ranking function to induce bilingual lexicons between other language pairs. The framework uses FastText embeddings (300 dimensions, 200k words) and standard bilingual dictionaries (5k words for training/evaluation).

## Key Results
- RUBI achieves 95.3% translation accuracy for English-Spanish, compared to 84.1% for best previous approach
- Gains of over 25% for Russian-English translation where initial alignments were poor
- Quality of learned ranking criterion correlates strongly with initial alignment quality between pivot and target languages
- Consistent improvements across 20+ languages with gains ranging from 5-25% over unsupervised baselines

## Why This Works (Mechanism)

### Mechanism 1
Learning a ranking criterion from a pivot language and applying it to another pair improves translation accuracy without parallel data. The learned ranking function captures complex geometric relationships between embeddings beyond simple cosine similarity, allowing better discrimination between true translations and false positives. This works because word embedding spaces have similar structures across languages, enabling effective transfer of learned patterns.

### Mechanism 2
Incorporating CSLS during both alignment and inference reduces hubness impact in high-dimensional spaces. CSLS adjusts similarity scores by subtracting mean similarity to neighbors, penalizing vectors similar to many others and boosting those uniquely similar to the query. This addresses the hubness phenomenon that significantly degrades nearest neighbor retrieval accuracy in word embedding spaces.

### Mechanism 3
The quality of learned ranking criterion correlates with initial alignment quality between pivot language and English. Better initial alignments provide cleaner training data for the ranking algorithm, leading to more accurate learned criteria. This relationship can be quantified and predicts downstream learning performance.

## Foundational Learning

- Learning to Rank
  - Why needed here: Traditional nearest neighbor or CSLS criteria are simple heuristics; learning to rank allows capturing complex, non-linear relationships between embeddings that better distinguish true translations
  - Quick check question: What is the difference between pointwise, pairwise, and listwise loss functions in Learning to Rank?

- Wasserstein Procrustes Alignment
  - Why needed here: This method aligns embedding spaces without parallel data by finding optimal linear transformation, providing foundation for dictionary induction
  - Quick check question: How does the Wasserstein Procrustes approach differ from simple orthogonal Procrustes in handling the alignment problem?

- Cross-Domain Similarity Local Scaling (CSLS)
  - Why needed here: CSLS addresses hubness problem in high-dimensional spaces, crucial for accurate nearest neighbor retrieval in bilingual lexicon induction
  - Quick check question: Why does subtracting mean similarity to neighbors improve translation retrieval accuracy?

## Architecture Onboarding

- Component map: Monolingual embeddings (FastText) -> Wasserstein Procrustes alignment -> Learning to Rank training -> Dictionary induction inference -> Evaluation pipeline

- Critical path:
  1. Load monolingual embeddings for source, target, and pivot languages
  2. Align source-pivot and target-pivot using Wasserstein Procrustes
  3. Extract training queries from pivot dictionary
  4. Train Learning to Rank model on pivot data
  5. Align source-target and apply learned ranking for inference
  6. Evaluate using standard BLI benchmarks

- Design tradeoffs:
  - Using pivot language adds complexity but enables semi-supervised learning
  - Learning to Rank adds computational cost but improves accuracy significantly
  - Choice of features (CSLS vs. raw embeddings) impacts both performance and training stability

- Failure signatures:
  - Poor alignment quality between languages leads to bad ranking criteria
  - Overfitting in Learning to Rank due to insufficient training data or too many features
  - Numerical instability in Wasserstein Procrustes optimization

- First 3 experiments:
  1. Verify Wasserstein Procrustes alignment improves over random initialization on simple language pair
  2. Test different Learning to Rank loss functions (approxNDCG, pairwise logistic, listwise) on pivot pair
  3. Evaluate how choice of pivot language (Spanish vs. French) affects downstream translation accuracy for target language

## Open Questions the Paper Calls Out

### Open Question 1
Does incorporating exogenous linguistic information (e.g., etymological trees, syntactic similarity) into alignment process provide additional improvements beyond current CSLS-based features? The paper mentions exogenous distances were considered but not implemented due to poor results with intra-distance methods, suggesting this remains an exciting avenue for future exploration.

### Open Question 2
What is the theoretical relationship between quality of initial alignment and effectiveness of learning-to-rank approach? Can this relationship be mathematically formalized? The paper observes strong negative correlation empirically but does not provide theoretical explanation for why this relationship exists or its mathematical foundation.

### Open Question 3
How does RUBI approach perform when pivot language is not English but another language with different alignment characteristics? All experiments use English as pivot, but the framework is potentially applicable to any pivot, so effectiveness with other pivot languages remains unexplored.

## Limitations
- RUBI's effectiveness highly dependent on availability of high-quality pivot language dictionary, which may not exist for all language pairs
- Approach assumes linear relationships between embedding spaces after Wasserstein Procrustes alignment, which may not hold for languages with very different typological structures
- Learning to Rank adds computational complexity without addressing potential fundamental limitations in monolingual embeddings themselves

## Confidence
- High confidence: Improvement over unsupervised methods is well-supported by experimental results (95.3% vs 84.1% accuracy for English-Spanish)
- Medium confidence: Claim that alignment quality correlates with learned ranking criterion quality is supported but requires further validation across diverse language families
- Medium confidence: Generalization of learned criterion to unseen language pairs is demonstrated but theoretical foundation for cross-linguistic effectiveness is not fully established

## Next Checks
1. Cross-family validation: Test RUBI's performance when pivot language and target language belong to different language families to assess cross-linguistic generalization limits
2. Dictionary size sensitivity: Systematically vary size of pivot dictionary to determine minimum training data required for effective Learning to Rank
3. Hubness impact analysis: Quantify contribution of CSLS vs. ranking model by ablating each component and measuring marginal benefit of combining both approaches