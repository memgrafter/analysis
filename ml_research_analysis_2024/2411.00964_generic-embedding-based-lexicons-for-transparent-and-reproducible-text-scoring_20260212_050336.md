---
ver: rpa2
title: Generic Embedding-Based Lexicons for Transparent and Reproducible Text Scoring
arxiv_id: '2411.00964'
source_url: https://arxiv.org/abs/2411.00964
tags:
- moral
- lexicons
- glove
- fasttext
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes embedding-based lexicons as a transparent, high-performance
  alternative to complex text analysis models and traditional manually crafted lexicons.
  These lexicons are created by populating word lists with semantically similar terms
  from pretrained FastText and GloVe embeddings, requiring minimal researcher input.
---

# Generic Embedding-Based Lexicons for Transparent and Reproducible Text Scoring

## Quick Facts
- arXiv ID: 2411.00964
- Source URL: https://arxiv.org/abs/2411.00964
- Authors: Catherine Moez
- Reference count: 40
- Primary result: Embedding-based lexicons achieve competitive performance with full transparency

## Executive Summary
This paper proposes embedding-based lexicons as a transparent, high-performance alternative to complex text analysis models and traditional manually crafted lexicons. The method creates lexicons by populating word lists with semantically similar terms from pretrained FastText and GloVe embeddings, requiring minimal researcher input. The approach achieves competitive performance on sentiment and moral foundations classification tasks, often matching or exceeding state-of-the-art models while offering full transparency and ease of reuse across domains.

## Method Summary
The method populates lexicons by identifying words with highest cosine similarity to seed words representing conceptual poles in pretrained embedding spaces. Words are weighted by net similarity (positive minus negative pole), creating polarity scores ranging from -1 to +1. The approach requires only seed words and pretrained embeddings, making it accessible to researchers without extensive text analysis expertise.

## Key Results
- F1 scores of 0.26-0.37 for moral frames, competitive with MoralBERT and LLM-based models
- Accuracy above 0.59 for sentiment tasks, exceeding many traditional lexicons
- Strong correlations with human-coded moral foundations data (r = 0.63-0.75)
- Comparable performance across diverse corpora including Amazon reviews, Reddit posts, and political speeches

## Why This Works (Mechanism)

### Mechanism 1
Embedding-based lexicons achieve competitive performance by leveraging semantic similarity in pretrained vector spaces. Seed words representing conceptual poles are expanded into full lexicons by selecting words with highest cosine similarity to each pole, weighted by net similarity (positive minus negative pole). Core assumption: Words that are semantically close in pretrained embedding space share conceptual associations relevant to the target domain. Evidence anchors include the paper's demonstration of competitive performance across multiple tasks, though the direct proof that semantic similarity equals conceptual relevance remains weak. Break condition: If pretrained embeddings encode domain-specific biases or associations that differ from general usage, lexicon performance degrades.

### Mechanism 2
Generic embedding lexicons combine transparency with performance, avoiding opacity of complex models. Word-level scoring using transparent lexicon contents allows researchers to inspect and modify feature contributions directly. Core assumption: Researchers value ability to audit and modify text scoring features over marginal performance gains from opaque models. Evidence anchors include the paper's performance results and the historical citation count of transparent tools like LIWC. Break condition: If researchers prioritize maximum performance regardless of transparency, complex models may be preferred.

### Mechanism 3
Low-resource creation of lexicons enables measurement of novel conceptual dimensions. Minimal seed words (5-100 per pole) combined with pretrained embeddings allow rapid lexicon creation without extensive annotation. Core assumption: Semantic similarity in pretrained embeddings captures relevant conceptual associations with minimal seed words. Evidence anchors include the paper's exploration of varying seed word counts and performance comparisons. Break condition: If seed words poorly represent conceptual poles or embeddings don't capture relevant associations, lexicon quality suffers.

## Foundational Learning

- **Concept: Cosine similarity in high-dimensional vector spaces**
  - Why needed here: Used to measure semantic similarity between words for lexicon expansion
  - Quick check question: How does cosine similarity differ from Euclidean distance in vector space?

- **Concept: Semantic embeddings and distributional hypothesis**
  - Why needed here: Underpins the assumption that words appearing in similar contexts have related meanings
  - Quick check question: What evidence supports the claim that co-occurrence patterns capture conceptual meaning?

- **Concept: Bag-of-words text representation**
  - Why needed here: Lexicons score text based on unigram word presence without considering word order
  - Quick check question: When might ignoring word order be problematic for text analysis?

## Architecture Onboarding

- **Component map**: Seed word input → Pretrained embedding lookup → Cosine similarity computation → Lexicon population → Text scoring (polarity or valence)
- **Critical path**: Seed word selection and embedding source selection are most critical for lexicon quality
- **Design tradeoffs**: Simpler (unigram) vs. more complex (multi-word, sense-specific) representations; transparency vs. potential performance gains
- **Failure signatures**: Poor performance when embeddings encode domain-specific biases; suboptimal when seed words poorly represent conceptual poles
- **First 3 experiments**:
  1. Create sentiment lexicon using 10 seed words per pole from established dictionaries, test on labeled sentiment datasets
  2. Create moral foundations lexicon with 5 carefully chosen seed words per pole, compare performance to MFD
  3. Test sensitivity to seed word count by creating multiple lexicons with 5, 10, 25, 50, and 100 seeds per pole and comparing performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How many seed words are needed to create a stable and accurate embedding-based lexicon for a given concept?
- **Basis in paper**: The paper explores varying numbers of seed words (5, 10, 25, 50, 75, 100) at each pole and finds that larger numbers produce more reliable dictionaries with lower performance variance.
- **Why unresolved**: While the paper shows that more seeds generally improve reliability, it doesn't determine the optimal number for balancing accuracy, stability, and computational efficiency.
- **What evidence would resolve it**: A systematic comparison of lexicon performance, stability, and resource requirements across a wide range of seed word counts and types, using multiple concepts and validation datasets.

### Open Question 2
- **Question**: How do biases in the embedding training corpus affect the conceptual associations in the resulting lexicons?
- **Basis in paper**: The paper notes that differences in training data can lead to variations in the lexicon content and associations, and that biases in large text corpora can be encapsulated in embeddings.
- **Why unresolved**: The paper acknowledges the potential for bias but doesn't thoroughly investigate how these biases manifest in the lexicons or how to mitigate them.
- **