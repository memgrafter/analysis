---
ver: rpa2
title: Be My Donor. Transfer the NLP Datasets Between the Languages Using LLM
arxiv_id: '2410.14074'
source_url: https://arxiv.org/abs/2410.14074
tags:
- text
- annotation
- task
- russian
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the use of large language models (LLMs)\
  \ to transfer NLP datasets and annotations across languages. The authors focus on\
  \ transferring the English DEFT corpus to Russian, which contains term-definition\
  \ pair annotations\u2014a rare annotation type for Russian."
---

# Be My Donor. Transfer the NLP Datasets Between the Languages Using LLM

## Quick Facts
- arXiv ID: 2410.14074
- Source URL: https://arxiv.org/abs/2410.14074
- Reference count: 32
- Primary result: Successfully transferred term-definition pair annotations from English to Russian using LLMs, establishing baseline models for definition detection and NER tasks

## Executive Summary
This paper investigates the use of large language models (LLMs) to transfer NLP datasets and annotations across languages. The authors focus on transferring the English DEFT corpus to Russian, which contains term-definition pair annotationsâ€”a rare annotation type for Russian. They develop a pipeline using ChatGPT3.5-turbo and Llama-3.1-8b for translation and annotation transfer. For NER annotation, they find that asking the LLM to directly extract substrings in Russian corresponding to English spans yields better results than using indices. They evaluate translation quality using BLEU scores and embedding-based metrics, achieving strong performance. Finally, they train BERT-based models on the transferred dataset, establishing baselines for definition detection and named entity recognition tasks. The results show that while definition detection performs well, named entity recognition requires further refinement.

## Method Summary
The authors develop a pipeline for transferring NLP datasets between languages using LLMs. The process involves translating English text to Russian while simultaneously transferring annotations. They experiment with two approaches: using translation indices to map spans between languages, and directly prompting the LLM to extract corresponding substrings in Russian. The pipeline employs both commercial (ChatGPT 3.5-turbo) and open-source (Llama-3.1-8b) models. For evaluation, they use BLEU scores for translation quality and embedding-based metrics to assess semantic preservation. They also train BERT-based models on the transferred datasets to establish performance baselines for downstream tasks.

## Key Results
- Direct substring extraction method outperforms index-based span mapping for NER annotation transfer
- Achieved strong translation quality with BLEU scores and embedding metrics for the English-to-Russian transfer
- BERT models trained on transferred data show good performance on definition detection but need refinement for named entity recognition

## Why This Works (Mechanism)
The approach leverages LLMs' ability to understand context and semantics across languages, allowing for more accurate annotation transfer than simple word-for-word translation. By prompting the LLM to directly extract corresponding substrings in the target language, the method accounts for linguistic differences in sentence structure and word order that would break index-based mapping approaches. The use of both commercial and open-source models provides flexibility in balancing performance with accessibility and cost considerations.

## Foundational Learning

**Cross-lingual annotation transfer**: The process of mapping annotations from one language to another while preserving semantic meaning and relationships. Why needed: Enables leveraging existing annotated datasets across languages without manual re-annotation. Quick check: Compare annotation quality metrics before and after transfer.

**Translation quality metrics**: Methods like BLEU scores and embedding-based evaluations to assess translation fidelity. Why needed: Ensures that semantic meaning is preserved during language transfer. Quick check: Calculate multiple metrics and compare against human judgments.

**LLM prompt engineering**: Designing specific prompts to elicit desired behaviors from LLMs, such as direct substring extraction. Why needed: Optimizes LLM performance for specific tasks beyond general language understanding. Quick check: A/B test different prompt formulations on a validation set.

## Architecture Onboarding

**Component Map**: English text -> LLM (translation + annotation transfer) -> Russian text with annotations -> BERT model training -> Evaluation

**Critical Path**: The annotation transfer process via LLM is the critical path, as translation quality directly impacts downstream model performance. The pipeline must ensure semantic preservation during transfer.

**Design Tradeoffs**: Commercial vs. open-source LLMs (performance vs. cost and reproducibility), direct extraction vs. index mapping (simplicity vs. precision control), single vs. multiple evaluation metrics (efficiency vs. comprehensiveness).

**Failure Signatures**: Poor translation quality leading to annotation misalignment, loss of semantic nuance during transfer, inconsistent annotation boundaries due to linguistic differences, overfitting to the specific language pair characteristics.

**First 3 Experiments**:
1. Compare BLEU scores and embedding metrics between commercial and open-source LLM translations
2. A/B test direct substring extraction vs. index mapping approaches on a validation set
3. Evaluate annotation consistency across multiple LLM inference runs with identical inputs

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on a single domain (term-definition pairs) and language pair (English to Russian) limits generalizability
- Reliance on commercial LLM APIs raises concerns about reproducibility and cost scalability
- Evaluation focuses on BLEU scores and embedding metrics without human validation of annotation quality

## Confidence

**High Confidence**: Translation quality assessment using BLEU scores and embedding metrics, direct substring extraction method for NER outperforming index-based approaches

**Medium Confidence**: Baseline BERT model performance on transferred datasets, comparative analysis between different LLM approaches

**Low Confidence**: Generalizability of methods to other annotation types and language pairs, long-term effectiveness of transferred annotations in downstream tasks

## Next Checks

1. Conduct human evaluation studies comparing LLM-transferred annotations against expert-annotated gold standards across multiple annotators to validate quality metrics.

2. Test the annotation transfer pipeline on additional language pairs and different annotation types (e.g., sentiment analysis, relation extraction) to assess generalizability.

3. Implement a cost-benefit analysis comparing commercial LLM APIs versus open-source alternatives for large-scale annotation transfer projects, including reproducibility assessments.