---
ver: rpa2
title: Towards Exact Gradient-based Training on Analog In-memory Computing
arxiv_id: '2406.12774'
source_url: https://arxiv.org/abs/2406.12774
tags:
- analog
- training
- theorem
- assumption
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a theoretical foundation for gradient-based
  training on analog in-memory computing (AIMC) accelerators. It characterizes the
  convergence behavior of stochastic gradient descent (SGD) on analog devices, identifying
  an asymptotic error caused by asymmetric updates inherent to analog hardware.
---

# Towards Exact Gradient-based Training on Analog In-memory Computing

## Quick Facts
- arXiv ID: 2406.12774
- Source URL: https://arxiv.org/abs/2406.12774
- Reference count: 40
- Primary result: Establishes theoretical foundation for gradient-based training on analog in-memory computing accelerators, identifying asymptotic error in Analog SGD and proposing Tiki-Taka algorithm for exact convergence.

## Executive Summary
This paper provides a rigorous theoretical foundation for understanding gradient-based training on analog in-memory computing (AIMC) accelerators. The authors characterize the convergence behavior of stochastic gradient descent (SGD) on analog devices, identifying an asymptotic error caused by asymmetric updates inherent to analog hardware. They prove that Analog SGD converges inexactly to a critical point, with error depending on device non-ideality and noise. To address this limitation, the paper analyzes a heuristic algorithm called Tiki-Taka, demonstrating its ability to exactly converge to a critical point by reducing the effect of asymmetric bias and noise. Theoretical analysis is supported by simulations on both synthetic and real datasets.

## Method Summary
The paper proposes a discrete-time dynamic model for Analog SGD and analyzes its convergence properties on analog in-memory computing accelerators. The method involves characterizing the asymptotic error caused by asymmetric updates in analog devices and developing the Tiki-Taka algorithm as a solution. Tiki-Taka uses an auxiliary array with a smaller learning rate to estimate the true gradient, effectively eliminating the asymptotic error. The analysis is implemented using the AIHWK IT simulator with asymmetric linear device models and validated through experiments on least-squares synthetic datasets and image classification tasks (MNIST, CIFAR10).

## Key Results
- Analog SGD suffers from asymptotic error due to asymmetric updates in analog devices, preventing exact convergence to critical points
- Tiki-Taka algorithm eliminates asymptotic error by using an auxiliary array to reduce noise impact and asymmetric bias
- Theoretical bounds show that convergence rate of Analog SGD is slower than digital SGD due to saturation effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Analog SGD suffers from asymptotic error due to asymmetric update in analog devices.
- Mechanism: The asymmetric update causes the weight to drift toward the symmetric point, preventing convergence to a critical point. This drift is amplified by the fast reset property when the weight moves toward the symmetric point and suppressed by saturation when moving away.
- Core assumption: The response factors q+(w) and q-(w) are not equal, leading to asymmetric updates.
- Evidence anchors:
  - [abstract]: "characterizes the convergence behavior of stochastic gradient descent (SGD) on analog devices, identifying an asymptotic error caused by asymmetric updates inherent to analog hardware."
  - [section 2.2]: "We then provide a lower bound of the asymptotic error to show that there is a fundamental performance limit of SGD-based analog training rather than an artifact of our analysis."
- Break condition: If the response factors become symmetric (q+(w) = q-(w)), the asymptotic error disappears.

### Mechanism 2
- Claim: Tiki-Taka eliminates the asymptotic error by reducing the effect of asymmetric bias and noise.
- Mechanism: Tiki-Taka introduces an auxiliary array Pk to estimate the true gradient. By using a smaller learning rate β for Pk compared to the main array Wk, the noise impact is reduced, and the weight stays near the critical point.
- Core assumption: The noise in analog devices has a non-zero expectation, which allows the auxiliary array to track the true gradient.
- Evidence anchors:
  - [abstract]: "To address this, we study a heuristic analog algorithm called Tiki-Taka that has recently exhibited superior empirical performance compared to SGD and rigorously show its ability to exactly converge to a critical point and hence eliminates the asymptotic error."
  - [section 4]: "To eliminate the error, the idea under Tiki-Taka is to reduce the noise impact."
- Break condition: If the noise expectation becomes zero or the learning rate β is not sufficiently small, the asymptotic error persists.

### Mechanism 3
- Claim: The convergence rate of Analog SGD is slower than digital SGD due to the saturation degree Wmax/τ.
- Mechanism: The saturation degree Wmax/τ affects both the convergence rate and the asymptotic error. A smaller Wmax/τ leads to a faster convergence rate but a larger asymptotic error, and vice versa.
- Core assumption: The weight Wk is bounded in the ℓ∞ norm by Wmax, which is smaller than the device parameter τ.
- Evidence anchors:
  - [section 3]: "The saturation degree Wmax/τ affects both convergence rate and asymptotic error."
  - [section 3]: "The asymmetric update has a negative impact on both rate and error."
- Break condition: If the weight Wk is not bounded or the saturation degree is not properly controlled, the convergence rate and asymptotic error are affected.

## Foundational Learning

- Concept: Stochastic gradient descent (SGD) and its convergence properties in digital training.
  - Why needed here: To understand the difference between digital and analog training and the limitations of applying SGD directly to analog devices.
  - Quick check question: What is the main difference between digital and analog training in terms of weight updates?

- Concept: Asymmetric update and its impact on analog training.
  - Why needed here: To understand the core issue with analog training and how it leads to asymptotic error.
  - Quick check question: How does the asymmetric update in analog devices affect the convergence of SGD?

- Concept: Noise in analog devices and its impact on training.
  - Why needed here: To understand how noise in analog devices contributes to the asymptotic error and how Tiki-Taka mitigates this issue.
  - Quick check question: How does the noise in analog devices affect the convergence of SGD and how does Tiki-Taka address this issue?

## Architecture Onboarding

- Component map: Analog in-memory computing (AIMC) accelerators with resistive crossbar arrays -> Digital SGD algorithm for comparison -> Tiki-Taka algorithm for mitigating asymptotic error -> Noise models for analog devices -> Saturation and bounded weight assumptions

- Critical path: Model training on analog devices using SGD -> Characterization of convergence behavior and identification of asymptotic error -> Development of Tiki-Taka algorithm to eliminate asymptotic error -> Theoretical analysis of convergence rates and asymptotic errors -> Experimental validation on synthetic and real datasets

- Design tradeoffs:
  - Balancing the saturation degree Wmax/τ to optimize convergence rate and asymptotic error
  - Choosing the learning rate α and auxiliary array learning rate β for Tiki-Taka
  - Managing noise in analog devices to improve training accuracy
  - Tradeoff between energy efficiency and training accuracy in analog devices

- Failure signatures:
  - Large asymptotic error in Analog SGD indicating the presence of asymmetric updates
  - Slow convergence rate in Analog SGD due to saturation effects
  - Instability in Tiki-Taka if the noise expectation is zero or the learning rate β is not sufficiently small
  - Divergence in training if the weight Wk exceeds the bounded saturation assumption

- First 3 experiments:
  1. Compare the convergence behavior of digital SGD and Analog SGD on a synthetic dataset to observe the asymptotic error in Analog SGD.
  2. Implement Tiki-Taka on the same synthetic dataset and compare its convergence behavior with Analog SGD to demonstrate the elimination of asymptotic error.
  3. Train a convolutional neural network on a real dataset using Analog SGD and Tiki-Taka, and compare their training accuracy and convergence rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the convergence guarantees of Tiki-Taka extend to devices with non-linear response factors beyond the asymmetric linear device model?
- Basis in paper: [explicit] The paper notes that current analysis is device-specific to asymmetric linear devices and suggests extending convergence analysis to more general analog devices as future work.
- Why unresolved: The paper's convergence proofs rely heavily on the linear structure of the asymmetric linear device model, which may not hold for devices with non-linear response factors.
- What evidence would resolve it: Rigorous convergence proofs for Tiki-Taka on devices with non-linear response factors, or experimental validation showing Tiki-Taka's effectiveness across a range of analog device types.

### Open Question 2
- Question: What is the impact of device noise and asymmetry on the training of larger, more complex models beyond those tested in the paper?
- Basis in paper: [inferred] The paper tests Tiki-Taka on relatively small models (3-layer FCN, CNN, and ResNets) and notes the need for further investigation into device-specific algorithms for future work.
- Why unresolved: The paper does not explore the performance of Tiki-Taka on larger, state-of-the-art models, which may have different noise and asymmetry tolerances.
- What evidence would resolve it: Training results and convergence analysis of Tiki-Taka on large-scale models (e.g., transformers, vision transformers) with varying levels of device noise and asymmetry.

### Open Question 3
- Question: How does the choice of learning rates for Wk and Pk in Tiki-Taka affect its convergence and final accuracy, especially in the presence of high device noise?
- Basis in paper: [explicit] The paper sets β = 8αL in the convergence proof but does not explore the sensitivity of Tiki-Taka's performance to different learning rate choices.
- Why unresolved: The optimal learning rate ratio between Wk and Pk is not investigated, and the impact of high device noise on this choice is not explored.
- What evidence would resolve it: An ablation study varying the learning rate ratio and noise levels, showing the impact on Tiki-Taka's convergence speed and final accuracy.

## Limitations

- The theoretical bounds on asymptotic error and convergence rates may be conservative and not fully reflective of practical performance across diverse hardware implementations
- The assumption of bounded weight Wmax < τ may not hold for all analog devices or training scenarios, potentially affecting the convergence guarantees
- The noise model used in simulations may not capture all real-world noise sources present in actual analog hardware, particularly device-to-device variations and thermal noise

## Confidence

- High confidence: The characterization of asymptotic error in Analog SGD due to asymmetric updates is well-supported by theoretical analysis and simulation results
- Medium confidence: The theoretical analysis of Tiki-Taka's convergence properties is sound, but practical effectiveness may depend on specific hardware implementations
- Medium confidence: The simulation results demonstrating superior performance of Tiki-Taka are convincing for tested scenarios, but generalizability to more complex models remains to be validated

## Next Checks

1. Implement Tiki-Taka on actual analog hardware prototypes to verify that theoretical convergence guarantees hold under realistic noise conditions and device non-idealities not captured in simulations
2. Test the algorithm's robustness across different types of analog devices with varying asymmetric update characteristics to determine if the theoretical framework generalizes beyond the specific ALD model used
3. Evaluate Tiki-Taka's performance on larger-scale models and more complex tasks (e.g., language models or vision transformers) to assess scalability and practical utility in real-world applications