---
ver: rpa2
title: Multi-Task Learning for Features Extraction in Financial Annual Reports
arxiv_id: '2404.05281'
source_url: https://arxiv.org/abs/2404.05281
tags:
- task
- tasks
- reports
- learning
- multi-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores multi-task learning (MTL) for extracting features
  from financial annual reports, focusing on tasks like financial sentiment, objectivity,
  forward-looking sentence prediction, and ESG-content detection. The authors propose
  a novel MTL method, ExGF-MTL, which explicitly uses auxiliary task predictions as
  features for the target task.
---

# Multi-Task Learning for Features Extraction in Financial Annual Reports

## Quick Facts
- arXiv ID: 2404.05281
- Source URL: https://arxiv.org/abs/2404.05281
- Reference count: 0
- Primary result: ExGF-MTL outperforms other MTL methods for ESG classification in financial annual reports

## Executive Summary
This paper investigates multi-task learning (MTL) for extracting features from financial annual reports, focusing on five classification tasks: financial sentiment, objectivity, forward-looking sentence prediction, ESG-content detection, and relevance. The authors propose a novel ExGF-MTL method that explicitly uses auxiliary task predictions as features for the target ESG task. Their experiments on FTSE350 annual reports show that ExGF-MTL achieves superior performance, particularly when excluding low-agreement tasks (relevance and objectivity) with poor inter-annotator agreement. The extracted textual features also correlate with real-world ESG scores from Reuters.

## Method Summary
The study uses RoBERTa as the base encoder, fine-tuned on a masked language model task using FTSE350 annual reports (2017-2019). Five classification tasks are defined on an annotated dataset of 2651 sentences. Several MTL architectures are compared: Joint (summing task losses), Weighted (learned task weights), Sequential (pre-training on auxiliary tasks), ExGF-MTL (explicitly using auxiliary task predictions as features), and TARS (task-aware representations). The ExGF-MTL method concatenates auxiliary task logits with the [CLS] representation before passing to the ESG classifier. Models are trained for 5 epochs and evaluated using macro-F1 scores averaged over 5 random seeds.

## Key Results
- ExGF-MTL outperforms other MTL methods for ESG classification
- Excluding low-agreement tasks (relevance and objectivity) improves performance
- Textual features extracted via ExGF-MTL correlate with real-world ESG scores from Reuters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit auxiliary task predictions as features (ExGF-MTL) improve ESG classification by providing structured intermediate representations.
- Mechanism: The model learns auxiliary task classifiers (sentiment, objectivity, forward-looking) whose output logits are concatenated and fed as additional features into the final ESG classifier. This forces the encoder to learn task-specific representations useful for the target.
- Core assumption: Auxiliary tasks are related enough to ESG that their predictions contain complementary information, and the model can effectively use concatenated logits as features.
- Evidence anchors:
  - [abstract] "our best-performing method highlights the positive effect of explicitly adding auxiliary task predictions as features for the final target task"
  - [section] "ExGF - MTL", "Explicitly Giving the output of the classification heads for 'auxiliary tasks' as additional Features for the prediction of the final target task"
  - [corpus] Weak: No direct citations found; method appears novel in this context.
- Break condition: If auxiliary tasks are too noisy (low inter-annotator agreement) or unrelated, their predictions add noise rather than signal, harming performance.

### Mechanism 2
- Claim: Task weighting improves multi-task training by prioritizing more informative tasks.
- Mechanism: A learned weight layer normalizes and scales individual task losses before summing, allowing the model to focus on harder or more relevant tasks during training.
- Core assumption: The relative difficulty and importance of tasks varies; optimal weights can be learned from data rather than using uniform weights.
- Evidence anchors:
  - [section] "we also investigate an approach where weights for each task... are derived automatically"
  - [section] "Higher weights are given to the financial sentiment and the forward-looking tasks, while lower weights are given to the other three"
  - [corpus] Weak: No citations; weighting approach is straightforward but not deeply evaluated.
- Break condition: If learned weights converge to degenerate values (e.g., near zero for most tasks) or if the weight layer destabilizes training.

### Mechanism 3
- Claim: Excluding low-agreement tasks (relevance, objectivity) improves multi-task learning performance.
- Mechanism: Removing tasks with poor inter-annotator agreement reduces label noise, allowing the model to focus on cleaner, more consistent signals.
- Core assumption: Label noise from ambiguous tasks hurts generalization more than the benefit of multi-task regularization.
- Evidence anchors:
  - [section] "the best systems exclude the Objectivity task... and often the Relevance task... These tasks have low inter-rater agreement"
  - [section] "Removing the Objectivity and Relevance tasks leads to a small improvement in several cases"
  - [corpus] Weak: No direct citations; this is an empirical observation from the paper's own data.
- Break condition: If the excluded tasks contain useful complementary information not captured by remaining tasks, performance may degrade.

## Foundational Learning

- Concept: Inter-annotator agreement and its impact on model training
  - Why needed here: Low agreement tasks introduce label noise, which can mislead models and reduce generalization.
  - Quick check question: If two annotators agree on 50% of labels, is that high or low agreement? (Answer: Low)

- Concept: Multi-task learning loss balancing and task similarity
  - Why needed here: Related tasks improve performance via shared representations; unrelated tasks can harm it. Proper loss weighting is crucial.
  - Quick check question: If task A and task B are unrelated, should they be trained jointly? (Answer: Usually not recommended)

- Concept: Masked language model pretraining and fine-tuning
  - Why needed here: The base RoBERTa model is adapted to the financial domain and then fine-tuned on classification tasks.
  - Quick check question: What token representation is used for sequence classification in BERT/RoBERTa? (Answer: [CLS] token)

## Architecture Onboarding

- Component map: RoBERTa encoder -> Auxiliary task heads -> ExGF layer -> ESG head
- Critical path:
  1. Encode sentence → [CLS] vector
  2. Pass through auxiliary heads → auxiliary logits
  3. Concatenate auxiliary logits + [CLS] → ExGF layer → ESG features
  4. Sum ESG features + original ESG logits → ESG loss
  5. Backpropagate combined loss
- Design tradeoffs:
  - Joint vs sequential MTL: Joint allows simultaneous learning of task interactions; sequential isolates target task learning but may miss cross-task benefits.
  - ExGF vs plain MTL: ExGF adds parameters and complexity but explicitly leverages auxiliary predictions; plain MTL relies on shared representations only.
  - Task inclusion: More tasks increase regularization but also noise risk.
- Failure signatures:
  - Performance worse than mono-task baseline: Possible task interference or noisy labels.
  - ExGF model diverges or gradients explode: Auxiliary logits may be poorly scaled.
  - One task dominates loss: Weighting layer may be stuck or tasks are highly imbalanced.
- First 3 experiments:
  1. Implement baseline mono-task ESG classifier (no MTL) to establish performance floor.
  2. Implement joint MTL with all tasks, compare to baseline to measure MTL benefit.
  3. Implement ExGF-MTL with all tasks, compare to joint MTL to measure explicit feature benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the ExGF-MTL method generalize to other domains beyond financial annual reports?
- Basis in paper: [explicit] The authors mention that "our experimental framework and MTL methods are generic and can be applied to any other topic of interest" and plan future work on causal inference between textual features, ESG scores, and financial performance indicators.
- Why unresolved: The study only evaluates ExGF-MTL on financial annual reports, so its effectiveness in other domains remains untested.
- What evidence would resolve it: Applying ExGF-MTL to different domains (e.g., medical reports, legal documents) and comparing its performance against other MTL methods and single-task baselines.

### Open Question 2
- Question: What is the optimal number of tasks for multi-task learning in this context?
- Basis in paper: [explicit] The authors note that "a higher number of tasks would allow the system to compensate for low-performance tasks" but do not experiment with more than five tasks.
- Why unresolved: The study only investigates five tasks, so the impact of increasing or decreasing the number of tasks on performance is unknown.
- What evidence would resolve it: Conducting experiments with varying numbers of tasks (e.g., 3, 7, 10) and analyzing the trade-offs between task diversity and model performance.

### Open Question 3
- Question: How do the extracted textual features influence actual ESG scores and financial performance?
- Basis in paper: [explicit] The authors find correlations between textual features and ESG scores but state they plan to extend their method to "perform causal discovery and causal inference between textual features, ESG scores and various financial performance indicators."
- Why unresolved: The study only establishes correlations, not causation, between textual features and ESG scores.
- What evidence would resolve it: Using causal inference techniques (e.g., propensity score matching, instrumental variables) to determine if changes in textual features lead to changes in ESG scores or financial performance.

## Limitations

- The study is limited to a single domain (FTSE350 annual reports) and specific time period (2017-2019), raising questions about external validity.
- Two of the five tasks (relevance and objectivity) have particularly low inter-annotator agreement, suggesting potential label noise.
- The ablation studies are incomplete - the paper does not systematically test which auxiliary tasks contribute most to performance gains.

## Confidence

- High Confidence: The claim that ExGF-MTL outperforms standard MTL architectures on ESG classification within this specific dataset.
- Medium Confidence: The assertion that excluding low-agreement tasks (relevance and objectivity) improves overall performance.
- Low Confidence: The claim that textual features extracted via ExGF-MTL correlate with real-world ESG scores from Reuters.

## Next Checks

1. **Inter-annotator Agreement Impact Analysis**: Systematically retrain all MTL models with varying subsets of tasks, including only high-agreement tasks versus including low-agreement tasks, to quantify the precise impact of label noise on each architecture's performance.

2. **Cross-Domain Generalization Test**: Apply the best-performing ExGF-MTL model to financial documents from different domains (e.g., quarterly earnings reports, sustainability reports, or non-UK financial documents) to assess whether the learned representations transfer effectively.

3. **Ablation Study of Auxiliary Tasks**: Conduct a comprehensive ablation study that systematically removes each auxiliary task from the ExGF-MTL architecture to determine which specific task predictions contribute most to ESG classification performance, and whether the full set of five tasks is necessary.