---
ver: rpa2
title: Clustered Retrieved Augmented Generation (CRAG)
arxiv_id: '2406.00029'
source_url: https://arxiv.org/abs/2406.00029
tags:
- reviews
- crag
- tokens
- number
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CRAG, a method to reduce token usage in large
  language model (LLM) applications that rely on external knowledge. The key problem
  addressed is the high cost and latency of providing all relevant knowledge to an
  LLM via traditional retrieval-augmented generation (RAG), especially when dealing
  with large volumes of data like customer reviews.
---

# Clustered Retrieved Augmented Generation (CRAG)

## Quick Facts
- arXiv ID: 2406.00029
- Source URL: https://arxiv.org/abs/2406.00029
- Reference count: 3
- This paper introduces CRAG, a method to reduce token usage in large language model (LLM) applications that rely on external knowledge

## Executive Summary
CRAG addresses the high cost and latency of providing all relevant knowledge to LLMs via traditional retrieval-augmented generation (RAG) when dealing with large volumes of data like customer reviews. The method clusters similar knowledge items, summarizes each cluster, and aggregates these summaries to create a condensed knowledge representation. Experiments using a dataset of mobile phone reviews demonstrate that CRAG can reduce token usage by at least 46%, achieving over 90% reduction in some cases, compared to RAG. This significant reduction translates to substantial cost savings and faster response times without compromising the quality of the LLM's output.

## Method Summary
CRAG is a token-efficient alternative to traditional RAG for LLM applications that rely on external knowledge. The method works by clustering similar reviews using K-means clustering with embeddings, summarizing each cluster using a pre-trained language model (Mistral 7B) with few-shot prompting, and aggregating these summaries into a single knowledge representation. This aggregated summary is then provided to the LLM as context for answering questions. The approach was evaluated on a dataset of Amazon mobile phone reviews, comparing token usage and output quality against traditional RAG using the same LLM models and evaluation metrics.

## Key Results
- CRAG reduces token usage by at least 46% compared to traditional RAG
- In some cases, CRAG achieves over 90% reduction in token usage
- The method maintains output quality while significantly reducing costs and latency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Clustering reduces redundant information before summarization
- **Mechanism**: Reviews are grouped by semantic similarity using embeddings, so that repetitive or closely related content is collapsed into a single cluster
- **Core assumption**: Similar reviews within a cluster share overlapping semantic content that can be meaningfully summarized without loss of key information
- **Evidence anchors**:
  - [abstract] "CRAG splits the reviews into k clusters, where k must be informed as an input parameter. Then, for every cluster obtained, CRAG summarizes the cluster content to generate a brief review that has the main information that represents the entire cluster."
  - [section] "We can see that the number of tokens with CRAG does not increase considerably when the number of reviews increases. On the other hand, using RAG, the number of tokens is almost 9x higher when there are 75 reviews in comparison to 4 reviews."
- **Break condition**: If clustering fails to group semantically similar reviews (e.g., due to poor embedding quality or inappropriate k value), redundant information will remain and the token reduction benefit will be lost

### Mechanism 2
- **Claim**: Summarization compresses each cluster into a compact representation
- **Mechanism**: A pre-trained language model (Mistral 7B) generates a concise summary for each cluster, capturing the essential sentiment and themes without quoting individual reviews
- **Core assumption**: The summarizer can accurately distill the key points of a cluster while maintaining semantic fidelity to the original reviews
- **Evidence anchors**:
  - [abstract] "For every cluster obtained, CRAG summarizes the cluster content to generate a brief review that has the main information that represents the entire cluster."
  - [section] "By using zero-shot prompting, we could not yield satisfactory results. In some cases, more than one example should be provided to the model (called few-shot prompting), depending on the complexity of the task. In our case, using only one example was sufficient."
- **Break condition**: If the summarizer produces overly generic or inaccurate summaries, the downstream LLM will receive distorted or insufficient context, degrading answer quality

### Mechanism 3
- **Claim**: Aggregation merges cluster summaries into a single knowledge representation
- **Mechanism**: The k summaries (one per cluster) are concatenated into one continuous text block, which is then provided to the LLM as context for answering questions
- **Core assumption**: Concatenating summaries preserves the collective information of all reviews while fitting within the LLM's context window
- **Evidence anchors**:
  - [abstract] "Finally, all summaries are merged into a single text to create the knowledge generated by CRAG."
  - [section] "Finally, for every product, we merge its summaries into a single text to create the knowledge generated by CRAG."
- **Break condition**: If the merged summary exceeds the LLM's context window, truncation will occur, potentially losing critical information and hurting performance

## Foundational Learning

- **Concept**: Vector embeddings and semantic similarity
  - **Why needed here**: Reviews are converted into numerical vectors so that clustering can group semantically similar items together
  - **Quick check question**: What embedding model was used in CRAG to represent reviews in vector space?
    - *Answer: paraphrase-multilingual-mpnet-base-v2*

- **Concept**: K-means clustering and the elbow method
  - **Why needed here**: Clustering is used to partition reviews into groups based on similarity; the elbow method helps choose an appropriate number of clusters (k)
  - **Quick check question**: What value of k was chosen for clustering in the experiments?
    - *Answer: k = 4*

- **Concept**: Few-shot prompting for summarization
  - **Why needed here**: The summarization step uses few-shot prompting to guide the LLM to produce concise, accurate summaries from clustered reviews
  - **Quick check question**: Why did zero-shot prompting fail in CRAG's summarization step?
    - *Answer: It did not yield satisfactory results; few-shot prompting was required*

## Architecture Onboarding

- **Component map**: Data source (Amazon reviews) -> Embedding model (paraphrase-multilingual-mpnet-base-v2) -> Clustering algorithm (K-means with k=4) -> Summarizer (Mistral 7B with few-shot prompt) -> Aggregator (concatenate k summaries) -> QA LLM (GPT-4, Llama2-70B, or Mixtral8x7B)

- **Critical path**:
  1. Generate embeddings for all reviews
  2. Cluster reviews into k groups
  3. Summarize each cluster using Mistral 7B
  4. Concatenate summaries into one knowledge block
  5. Provide block to QA LLM with user question
  6. Return LLM-generated answer

- **Design tradeoffs**:
  - Clustering granularity (k) vs. summary coherence: higher k may yield more specific summaries but risk fragmentation
  - Embedding quality vs. computational cost: better embeddings improve clustering but increase preprocessing time
  - Few-shot prompt complexity vs. summarizer performance: more examples may improve accuracy but add prompt tokens

- **Failure signatures**:
  - Token count not reduced → clustering or summarization failed
  - Poor QA answer quality → summaries lost critical information or were inaccurate
  - Context window exceeded → merged summaries too long for LLM input

- **First 3 experiments**:
  1. Run CRAG on a small dataset (e.g., 10 products, 4 reviews each) and verify token reduction vs. RAG
  2. Vary k (e.g., k=3,4,5) and measure impact on summary quality and token count
  3. Compare few-shot vs. zero-shot summarization outputs to confirm performance difference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CRAG's performance scale with the number of clusters (k) when dealing with very large datasets?
- Basis in paper: [explicit] The paper uses k=4 based on the elbow method but mentions that larger values of k (5 and 6) could be considered
- Why unresolved: The paper only experiments with k=4 and does not explore the impact of different k values on performance metrics such as token reduction and output quality
- What evidence would resolve it: Experiments comparing CRAG's performance (token reduction, response quality) across a range of k values (e.g., 2 to 10) on datasets of varying sizes

### Open Question 2
- Question: How does the choice of clustering algorithm affect CRAG's performance compared to K-means?
- Basis in paper: [explicit] The paper uses K-means but mentions that alternative clustering algorithms might be explored in future work
- Why unresolved: The paper only uses K-means and does not compare its performance with other clustering algorithms like DBSCAN or hierarchical clustering
- What evidence would resolve it: Experiments comparing CRAG's performance (token reduction, response quality) using different clustering algorithms on the same datasets

### Open Question 3
- Question: Can fine-tuning the summarization model (Mistral 7B) improve CRAG's performance compared to one-shot prompting?
- Basis in paper: [explicit] The paper uses one-shot prompting for summarization but suggests that a fine-tuning approach might help improve the model's performance in this task
- Why unresolved: The paper only uses one-shot prompting and does not explore the potential benefits of fine-tuning the summarization model
- What evidence would resolve it: Experiments comparing CRAG's performance (token reduction, response quality) using one-shot prompting versus fine-tuned summarization models

## Limitations
- The choice of k=4 clusters is not justified through systematic analysis, and different datasets may require different k values
- The evaluation focuses primarily on token reduction and similarity metrics rather than direct measures of answer quality or user satisfaction
- Experiments use only mobile phone reviews from Amazon, limiting generalizability to other domains or review types

## Confidence

**High Confidence**: The token reduction mechanism is sound and well-demonstrated. The clustering → summarization → aggregation pipeline logically follows from the problem statement, and the experimental results showing 46-90% token reduction are clearly presented with appropriate metrics.

**Medium Confidence**: The claim that CRAG maintains output quality while reducing tokens rests on indirect evidence. The semantic similarity (CosSim) between CRAG and RAG outputs suggests comparable information content, but this doesn't guarantee that CRAG answers are equally useful or accurate for end users.

**Low Confidence**: The generalizability of CRAG across different domains, review types, or languages remains unproven. The paper doesn't explore how sensitive the method is to parameter choices (k value, embedding model, summarization prompt) or how it performs when reviews are short, sparse, or contain contradictory information.

## Next Checks

1. **Quality Validation Study**: Conduct a human evaluation study where annotators rate answer quality for both CRAG and RAG outputs on the same questions. Measure whether CRAG's token reduction comes at the cost of answer usefulness, accuracy, or completeness.

2. **Parameter Sensitivity Analysis**: Systematically vary the number of clusters (k from 2 to 10) and measure the impact on both token reduction and answer quality. Identify the optimal k range for different dataset sizes and review characteristics.

3. **Cross-Domain Testing**: Apply CRAG to at least two different review domains (e.g., restaurant reviews, product reviews from a different category) and measure whether the same token reduction benefits hold. Test with reviews in different languages to assess the multilingual embedding model's effectiveness.