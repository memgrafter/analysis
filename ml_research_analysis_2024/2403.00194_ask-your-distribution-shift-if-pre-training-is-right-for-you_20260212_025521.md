---
ver: rpa2
title: Ask Your Distribution Shift if Pre-Training is Right for You
arxiv_id: '2403.00194'
source_url: https://arxiv.org/abs/2403.00194
tags:
- dataset
- robustness
- pre-training
- reference
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates when pre-training can and cannot improve
  robustness to distribution shifts. Through theoretical analysis and empirical evaluation,
  the authors show that pre-training helps with extrapolation (e.g., generalizing
  to new domains) but does not address dataset biases (e.g., reliance on spurious
  features).
---

# Ask Your Distribution Shift if Pre-Training is Right for You

## Quick Facts
- arXiv ID: 2403.00194
- Source URL: https://arxiv.org/abs/2403.00194
- Reference count: 40
- This work investigates when pre-training can and cannot improve robustness to distribution shifts.

## Executive Summary
This paper investigates when pre-training can improve robustness to distribution shifts by analyzing two distinct failure modes: poor extrapolation (out-of-support shifts) and dataset biases (in-support shifts). Through theoretical analysis and empirical evaluation, the authors demonstrate that pre-training helps with extrapolation but does not address dataset biases. They show that combining pre-training with bias-mitigation interventions yields complementary robustness benefits, and that fine-tuning on a small, de-biased dataset can achieve high robustness when pre-training is leveraged for extrapolation. These findings provide practical guidance for practitioners on when to use pre-training for robustness and how to complement it with other interventions.

## Method Summary
The authors conduct experiments on both synthetic and natural distribution shifts. They construct synthetic in-support and out-of-support shifts by modifying ImageNet, then train baseline models from scratch and fine-tune pre-trained models on reference datasets. They measure effective robustness (ER) by comparing model performance on shifted distributions against baseline models. For natural shifts, they use ImageNet-V2, ImageNet Sketch, ImageNet-R, and WILDS-FMoW, dividing these into in-support and out-of-support splits using a classifier. The experiments validate that pre-training provides substantial robustness gains on out-of-support shifts but minimal gains on in-support shifts, and that combining pre-training with bias-mitigation interventions yields complementary benefits.

## Key Results
- Pre-training provides substantial robustness gains on out-of-support shifts but minimal gains on in-support shifts
- Pre-training and bias-mitigation interventions (e.g., Deep Feature Reweighting) have complementary robustness benefits
- Fine-tuning on a small, de-biased dataset can achieve higher robustness than fine-tuning on a large, biased dataset when pre-training is leveraged for extrapolation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-training improves robustness specifically for out-of-support shifts (extrapolation failures) but not for in-support shifts (bias failures).
- **Mechanism:** When reference data lies in a low-dimensional subspace Wref, the initialization determines how the model extends outside this subspace. Pre-training provides a better initialization for extrapolation beyond Wref, improving robustness to out-of-support shifts where inputs lie outside this subspace.
- **Core assumption:** The reference distribution lacks certain variations (features) that appear in the shifted distribution.
- **Evidence anchors:**
  - [abstract] "pre-training can help mitigate poor extrapolation but not dataset biases"
  - [section] "our findings suggest that pre-training can help with extrapolation, but does not address other failures, for example, those stemming from dataset biases"
  - [corpus] "Pre-training provides substantial robustness gains on out-of-support shifts but minimal gains on in-support shifts" - Strong alignment with corpus finding
- **Break condition:** If the reference and shifted distributions share the same support (in-support shift), pre-training cannot address failures caused by dataset biases.

### Mechanism 2
- **Claim:** Combining pre-training with bias-mitigation interventions yields complementary robustness benefits.
- **Mechanism:** Pre-training addresses extrapolation failures while bias-mitigation interventions address failures stemming from dataset biases. These interventions target different failure modes, so combining them improves robustness to both types of shifts.
- **Core assumption:** Pre-training and bias-mitigation interventions address orthogonal failure modes.
- **Evidence anchors:**
  - [abstract] "pre-training and interventions designed to prevent exploiting biases have complementary robustness benefits"
  - [section] "we observe that the corrected examples of pre-training and DFR have little overlap, suggesting that their benefits are indeed complementary"
  - [corpus] "combining pre-training with DFR not only yields high effective robustness but in fact leads to models with both sets of benefits" - Strong alignment with corpus finding
- **Break condition:** If pre-training itself introduces harmful biases that persist during fine-tuning, the complementary benefit may be reduced.

### Mechanism 3
- **Claim:** Fine-tuning on a small, de-biased dataset can achieve high robustness when pre-training is leveraged for extrapolation.
- **Mechanism:** Pre-training handles extrapolation to new domains, reducing the need for a large, diverse fine-tuning dataset. A small, carefully de-biased dataset is sufficient for fine-tuning, making the de-biasing process more feasible and cost-effective.
- **Core assumption:** Extrapolation benefits from pre-training reduce the diversity requirements for the fine-tuning dataset.
- **Evidence anchors:**
  - [abstract] "fine-tuning on a (very) small, non-diverse but de-biased dataset can result in significantly more robust models than fine-tuning on a large and diverse but biased dataset"
  - [section] "fine-tuning on a carefully de-biased hair color classification dataset with only 64 examples yields greater robustness than fine-tuning on the entire CelebA dataset"
  - [corpus] "combining pre-training with a carefully curated (and, in particular, de-biased) dataset instead of the original reference dataset" - Strong alignment with corpus finding
- **Break condition:** If the pre-training model itself has harmful biases that are not addressed during fine-tuning, the small de-biased dataset may not be sufficient.

## Foundational Learning

- **Concept:** Distribution shift types (in-support vs. out-of-support)
  - **Why needed here:** Understanding these shift types is crucial for determining when pre-training can help with robustness. In-support shifts do not contain "new" inputs, while out-of-support shifts do, requiring extrapolation.
  - **Quick check question:** If a model is trained on daytime photos and deployed on nighttime photos, is this an in-support or out-of-support shift?

- **Concept:** Effective robustness (ER)
  - **Why needed here:** ER quantifies the robustness improvement of a model above the baseline of models trained from scratch. It's the key metric for measuring the benefits of pre-training on different distribution shifts.
  - **Quick check question:** If a pre-trained model has an accuracy of 80% on the shifted distribution and the baseline model's predicted accuracy (from the linear fit) is 75%, what is the effective robustness?

- **Concept:** Bias-mitigation interventions (e.g., Deep Feature Reweighting)
  - **Why needed here:** These interventions are complementary to pre-training for handling dataset biases. Understanding their mechanism is important for combining them effectively with pre-training.
  - **Quick check question:** How does Deep Feature Reweighting (DFR) aim to improve model robustness?

## Architecture Onboarding

- **Component map:** Pre-training on large dataset -> Fine-tuning on task-specific dataset (potentially curated and de-biased) -> Bias-mitigation interventions (optional)
- **Critical path:** Pre-train on large dataset → Fine-tune on task-specific dataset (potentially curated and de-biased) → Apply bias-mitigation interventions (if needed)
- **Design tradeoffs:** A key tradeoff is between the size and diversity of the fine-tuning dataset. A large, diverse dataset may contain biases that hurt robustness, while a small, de-biased dataset may lack the diversity needed for good performance. Pre-training can help mitigate this tradeoff by providing extrapolation benefits.
- **Failure signatures:** If a model shows high accuracy on the reference distribution but poor robustness to a distribution shift, it may be due to extrapolation failures (out-of-support shift) or dataset biases (in-support shift). Analyzing the type of shift can help determine the appropriate intervention (pre-training or bias-mitigation).
- **First 3 experiments:**
  1. Evaluate effective robustness on synthetic in-support and out-of-support shifts
  2. Combine pre-training with a bias-mitigation intervention (e.g., DFR) and measure effective robustness
  3. Fine-tune a pre-trained model on a small, de-biased dataset and compare to fine-tuning on a large, biased dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the composition of the pre-training dataset affect the robustness of the resulting model to different types of distribution shifts?
- Basis in paper: Explicit - The paper mentions that Ramanujan et al. [RNO+23] explored how the composition of the pre-training dataset affects robustness on the WILDS-iWildCam distribution shift.
- Why unresolved: While the paper references this work, it does not delve into the specifics of how different pre-training dataset compositions impact robustness to various distribution shifts.
- What evidence would resolve it: A comprehensive study analyzing the effects of different pre-training dataset compositions on model robustness to various distribution shifts, including in-support and out-of-support shifts.

### Open Question 2
- Question: Can pre-training ever hurt a model's ability to extrapolate, and under what circumstances?
- Basis in paper: Explicit - The paper mentions that Salman et al. [SJI+22] showed that biases of pre-trained models can persist during fine-tuning, leading to worse extrapolation in certain cases.
- Why unresolved: The paper acknowledges this possibility but does not explore it further or provide specific examples of when pre-training might hinder extrapolation.
- What evidence would resolve it: Empirical studies identifying specific scenarios where pre-training leads to worse extrapolation compared to training from scratch, along with analysis of the underlying mechanisms.

### Open Question 3
- Question: How effective are alternative fine-tuning strategies, such as linear probing followed by full fine-tuning (LP-FT) or zero-shot initialization, in mitigating dataset biases compared to full fine-tuning?
- Basis in paper: Explicit - The paper mentions that Kumar et al. [KRJ+22] proposed LP-FT to prevent full fine-tuning from "distorting" pre-trained features, and that zero-shot initialization may have a similar effect.
- Why unresolved: The paper only briefly mentions these strategies and their potential benefits, but does not provide a detailed comparison of their effectiveness in addressing dataset biases.
- What evidence would resolve it: A systematic evaluation of various fine-tuning strategies, including LP-FT and zero-shot initialization, on their ability to mitigate dataset biases and improve robustness to in-support shifts.

## Limitations

- Theoretical analysis assumes linear regression models, which may not fully capture deep neural network behavior
- Empirical validation relies on specific architectures and datasets that may not generalize to all domains
- The relationship between pre-training scale and robustness benefits across different shift types needs more systematic study

## Confidence

- **High confidence:** The finding that pre-training helps with out-of-support shifts but not in-support shifts is well-supported by both theory and experiments across multiple datasets and shift types.
- **Medium confidence:** The claim about complementary benefits of pre-training and bias-mitigation interventions is supported but could be strengthened with more systematic ablations of different bias mitigation techniques.
- **Medium confidence:** The guidance on using small, de-biased datasets when leveraging pre-training is demonstrated on specific tasks but requires validation across broader domains.

## Next Checks

1. Test the pre-training benefits across different model scales (from small to very large models) to determine if the relationship between model capacity and extrapolation robustness is consistent
2. Evaluate the complementary benefits of pre-training with multiple bias mitigation techniques beyond DFR to verify the general applicability of this finding
3. Conduct ablation studies varying the degree of bias in the fine-tuning dataset while keeping pre-training constant to quantify the tradeoff between dataset size and bias level