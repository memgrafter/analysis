---
ver: rpa2
title: Multi-Granularity Tibetan Textual Adversarial Attack Method Based on Masked
  Language Model
arxiv_id: '2412.02343'
source_url: https://arxiv.org/abs/2412.02343
tags:
- adversarial
- attack
- language
- tibetan
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multi-granularity Tibetan textual adversarial
  attack method called TSTricker. The method utilizes masked language models to generate
  candidate substitution syllables or words, adopts a scoring mechanism to determine
  substitution order, and conducts attacks on several fine-tuned victim models.
---

# Multi-Granularity Tibetan Textual Adversarial Attack Method Based on Masked Language Model

## Quick Facts
- arXiv ID: 2412.02343
- Source URL: https://arxiv.org/abs/2412.02343
- Reference count: 30
- This paper proposes a multi-granularity Tibetan textual adversarial attack method called TSTricker. Experiments show that TSTricker reduces the accuracy of classification models by more than 28.70% and changes the predictions of more than 90.60% of samples.

## Executive Summary
This paper introduces TSTricker, a multi-granularity adversarial attack method for Tibetan text classification. The method leverages masked language models (Tibetan-BERT and TiBERT) to generate semantically plausible syllable and word substitutions. By combining probability-weighted word saliency scoring with substitution ordering, TSTricker achieves high attack success rates while maintaining semantic preservation, particularly at the syllable level. Experiments demonstrate significant improvements over baseline methods across two Tibetan datasets and multiple victim models.

## Method Summary
TSTricker generates adversarial examples by first using Tibetan-BERT and TiBERT to predict top-k candidate substitutions for each syllable and word in the input text. It then calculates word saliency by measuring the probability drop when setting each element to unknown, weighted by the probability drop from the best substitution. Substitutions are ordered by this score and applied sequentially until the model prediction changes. The method offers both syllable-level (smaller perturbations, better semantic preservation) and word-level (larger perturbations, more efficient) attacks.

## Key Results
- TSTricker reduces classification accuracy by more than 28.70% on fine-tuned victim models
- Changes predictions of more than 90.60% of samples across tested models
- Achieves significantly higher attack success rates compared to baseline methods (TSCheater, TextFooler, PWWS)
- Syllable-level attacks maintain better semantic preservation with lower Levenshtein Distance

## Why This Works (Mechanism)

### Mechanism 1
- Masked language models can generate plausible syllable/word substitutions that preserve semantic meaning while causing misclassification.
- Core assumption: Masked language models trained on Tibetan data can predict semantically reasonable alternatives that humans would still interpret as equivalent.
- Evidence anchors: [abstract], [section 3.1]
- Break condition: If the masked language model fails to generate semantically plausible alternatives, or if the substitution order scoring mechanism doesn't identify effective attack points.

### Mechanism 2
- Word saliency scoring combined with probability-weighted saliency identifies the most effective substitution order for maximal attack impact.
- Core assumption: Syllables/words with high saliency are more critical to model decision-making, so attacking them first yields higher success rates.
- Evidence anchors: [section 3.2]
- Break condition: If saliency doesn't correlate with attack effectiveness, or if the probability-weighted scoring doesn't improve attack success over random ordering.

### Mechanism 3
- Multi-granularity attacks (syllable-level vs word-level) provide different tradeoffs between perturbation size and semantic preservation.
- Core assumption: Smaller granularity changes preserve semantics better but may require more substitutions, while larger granularity changes are more efficient but risk semantic drift.
- Evidence anchors: [abstract], [section 3], [section 4.3]
- Break condition: If neither granularity level provides acceptable tradeoff between attack effectiveness and semantic preservation.

## Foundational Learning

- **Tibetan script structure (letters → syllables → words with Tsheg separators)**
  - Why needed here: Understanding this structure is crucial for correctly segmenting text and applying the right granularity of attack
  - Quick check question: How many letters typically compose a Tibetan syllable, and what separates words in Tibetan script?

- **Masked language model prediction mechanics**
  - Why needed here: The attack relies on predicting plausible substitutions using masked language models
  - Quick check question: What is the difference between Tibetan-BERT and TiBERT tokenizers, and how does this affect substitution generation?

- **Adversarial attack evaluation metrics (ADV, ASR, LD)**
  - Why needed here: These metrics determine whether the attack is effective and how much it perturbs the original text
  - Quick check question: What does a Levenshtein Distance of 5 mean for a 20-syllable Tibetan sentence in terms of edit operations?

## Architecture Onboarding

- **Component map**: Masked language model → Candidate generation → Saliency calculation → Substitution ordering → Attack execution → Evaluation
- **Critical path**: Generate candidates → Calculate saliency → Order substitutions → Apply until misclassification
- **Design tradeoffs**: Syllable-level vs word-level attacks, number of candidates (k), scoring function complexity vs speed
- **Failure signatures**: High LD with low ASR indicates semantic changes not causing misclassification; Low ADV with high ASR suggests ineffective perturbations
- **First 3 experiments**:
  1. Test candidate generation with k=10 vs k=50 to find optimal balance between quality and computational cost
  2. Compare syllable-level vs word-level attacks on a small validation set to verify semantic preservation claims
  3. Evaluate saliency-based ordering against random ordering to validate the effectiveness of the scoring mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do Tibetan adversarial attack methods compare to English or Chinese ones in terms of semantic preservation?
- **Basis in paper**: [inferred] The paper mentions that Tibetan speakers found TSTricker-s generated texts maintained semantics while word-level attacks may change them, but does not provide direct comparisons to English/Chinese attacks.
- **Why unresolved**: No empirical comparisons with English/Chinese adversarial attacks are provided.
- **What evidence would resolve it**: Comparative experiments measuring semantic preservation metrics across languages for similar attack methods.

### Open Question 2
- **Question**: What is the transferability of Tibetan adversarial examples across different Tibetan language models?
- **Basis in paper**: [explicit] The paper tests attacks on BERT-based and XLMRoBERTa-based models but does not investigate cross-model transferability.
- **Why unresolved**: Only white-box attacks are conducted; black-box transferability is not studied.
- **What evidence would resolve it**: Experiments measuring attack success rates when adversarial examples generated for one model are tested on other Tibetan language models.

### Open Question 3
- **Question**: How do hard-label black-box attacks perform on Tibetan text classification models?
- **Basis in paper**: [inferred] The paper uses white-box attacks with access to model probabilities; no mention of hard-label black-box scenarios.
- **Why unresolved**: All experiments assume white-box access to model probabilities.
- **What evidence would resolve it**: Implementation and testing of black-box attack methods (e.g., gradient estimation, query-based attacks) on the same Tibetan datasets.

### Open Question 4
- **Question**: What is the impact of adversarial training on Tibetan language model robustness?
- **Basis in paper**: [explicit] The paper mentions adversarial training as a future direction but does not implement it.
- **Why unresolved**: No experiments with adversarial training or evaluation of robustness improvements are conducted.
- **What evidence would resolve it**: Training victim models on adversarial examples and measuring changes in attack success rates.

## Limitations

- Attack effectiveness evaluated only on two specific datasets and two fine-tuned models, limiting generalizability
- Semantic preservation claims based on Levenshtein Distance rather than human evaluation
- Missing implementation details for critical components like candidate generation parameters and scoring mechanism

## Confidence

- **Attack Effectiveness Claims**: High confidence - well-supported by experimental results
- **Multi-granularity Advantage Claims**: Medium confidence - supported by results but lacks theoretical justification
- **Comparison to Baselines**: High confidence - well-supported by quantitative comparisons
- **Semantic Preservation Claims**: Medium confidence - based on LD measurements but lacks human validation
- **Scoring Mechanism Effectiveness**: Medium confidence - supported by results but no ablation studies

## Next Checks

1. Conduct ablation studies comparing the probability-weighted word saliency scoring against random ordering and simpler saliency metrics to validate the effectiveness of the scoring mechanism.

2. Recruit native Tibetan speakers to evaluate the semantic similarity between original and attacked texts, particularly for the syllable-level vs word-level attacks, to validate the LD-based semantic preservation claims.

3. Evaluate TSTricker on additional Tibetan NLP tasks (e.g., named entity recognition, machine translation) and different model architectures to assess generalizability beyond the two tested models and classification tasks.