---
ver: rpa2
title: 'Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality Parallel
  Data Outperforms Traditional Web-Crawled Data'
arxiv_id: '2408.06537'
source_url: https://arxiv.org/abs/2408.06537
tags:
- dataset
- data
- sentence-level
- finetuning
- blob-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the NewsPaLM dataset, a large-scale, LLM-generated
  parallel corpus for English-German and German-English translation. The dataset is
  created using MBR decoding and QE reranking on Newscrawl data, with cluster-based
  selection for diversity.
---

# Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality Parallel Data Outperforms Traditional Web-Crawled Data

## Quick Facts
- arXiv ID: 2408.06537
- Source URL: https://arxiv.org/abs/2408.06537
- Reference count: 40
- Primary result: LLM-generated MBR-decoded parallel data outperforms traditional web-crawled datasets for NMT

## Executive Summary
This work presents NewsPaLM, a large-scale parallel corpus for English-German translation generated using PaLM-2 Bison. The dataset is created through MBR decoding and quality estimation reranking of Newscrawl data, with cluster-based selection for diversity. Extensive experiments demonstrate that models trained from scratch on NewsPaLM significantly outperform those trained on the much larger WMT'23 dataset. The paper also shows that self-distillation via MBR finetuning improves performance over few-shot prompting, and that pretraining is more sensitive to dataset quality than finetuning.

## Method Summary
The authors generate parallel data by prompting PaLM-2 Bison with source sentences from Newscrawl, then apply epsilon sampling to generate multiple candidate translations. These candidates undergo MBR decoding (selecting translations that maximize expected quality under utility metrics like BLEURT) or QE reranking (faster O(n) alternative). Cluster-based selection ensures diversity by avoiding similar source sentences. The resulting datasets (sentence-level MBR, blob-level QE, and greedy) are used to pretrain and finetune Transformer encoder-decoder models (602M parameters), with performance evaluated on WMT test sets using BLEURT, MetricX, COMET20, and COMET22 metrics.

## Key Results
- Models trained from scratch on NewsPaLM (3.7M examples) outperform those trained on WMT'23 (35.3M examples) across all metrics
- Pretraining is more sensitive to dataset quality than finetuning - performance drops substantially when reducing dataset size
- Self-distillation via MBR finetuning improves performance over few-shot prompting
- Cluster-based selection marginally improves diversity and downstream performance

## Why This Works (Mechanism)

### Mechanism 1
MBR-decoded synthetic data outperforms larger web-crawled datasets because it directly optimizes for translation quality metrics rather than posterior probability. MBR decoding selects translations that maximize expected quality under a utility function (e.g., BLEURT) rather than choosing the highest probability output. This shifts the objective from likelihood to quality. The core assumption is that the utility metric correlates well with downstream task performance.

### Mechanism 2
Pretraining is more sensitive to dataset quality than finetuning because it establishes foundational representations. During pretraining, the model learns general translation capabilities from scratch. High-quality data provides better signal for these foundational representations, while finetuning adapts an already-learned model. The core assumption is that quality signals during pretraining have a multiplicative effect on downstream performance.

### Mechanism 3
Self-distillation via MBR finetuning improves LLM performance by refining translations without catastrophic forgetting. The LLM generates translations, then finetunes on its own MBR-decoded outputs, improving quality while maintaining general capabilities. The core assumption is that the LLM can learn from its own synthetic data without degrading other capabilities.

## Foundational Learning

- Concept: MBR decoding optimization
  - Why needed here: Understanding why MBR decoding produces higher quality translations than likelihood-based methods is crucial for grasping the dataset quality advantage.
  - Quick check question: What is the key difference between MBR decoding and MAP decoding in terms of their optimization objectives?

- Concept: Quality estimation metrics
  - Why needed here: The paper relies heavily on BLEURT and MetricX for both generating data and evaluating it. Understanding these metrics is essential.
  - Quick check question: How do reference-based metrics like BLEURT differ from reference-free metrics like MetricX in their evaluation approach?

- Concept: Knowledge distillation
  - Why needed here: The paper uses self-distillation as a key technique. Understanding how distillation transfers knowledge from teacher to student models is important.
  - Quick check question: What is the fundamental principle behind knowledge distillation in machine learning?

## Architecture Onboarding

- Component map: Source data collection from Newscrawl → Clustering → Candidate generation using epsilon sampling → MBR/QE scoring with utility metrics → Parallel dataset creation → Model training
- Critical path: Source data → Clustering → Candidate generation → MBR/QE scoring → Parallel dataset creation → Model training
- Design tradeoffs: MBR decoding provides higher quality but is computationally expensive (O(n²)), while QE reranking is faster (O(n)) but may be less optimal. Cluster-based selection improves diversity but adds preprocessing overhead.
- Failure signatures: Poor translation quality indicates issues with MBR utility metric selection, insufficient candidate generation, or noisy source data. Overfitting to evaluation metrics suggests utility function misalignment.
- First 3 experiments:
  1. Generate a small MBR dataset (100 examples) and compare BLEURT scores against greedy decoding to verify quality improvement.
  2. Train a baseline model on 1% of WMT'23 data vs 1% of MBR data to establish quality advantage.
  3. Test MBR finetuning on a pre-trained model using a held-out validation set to verify self-distillation effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
What is the exact performance ceiling for pretraining on synthetic data like NewsPaLM, and how does it scale with dataset size? The paper shows pretraining performance drops substantially when reducing dataset size to 25%, while finetuning is less sensitive. This suggests pretraining has more headroom for improvement with larger synthetic datasets. However, the NewsPaLM dataset is already orders of magnitude smaller than typical NMT training corpora, and the authors hypothesize further improvements are possible. Training experiments with incrementally larger synthetic datasets (e.g., 2x, 5x, 10x current size) while keeping model architecture constant, measuring pretraining performance gains, would resolve this.

### Open Question 2
How does the quality of MBR-decoded data compare to QE-reranked data across different domains and sequence lengths? The authors find MBR finetuning outperforms QE finetuning on the sentence-level dataset, but the opposite holds during pretraining. They also observe MBR data is more "free-style" and harder to learn. The paper only compares MBR vs QE on specific datasets (sentence-level MBR vs blob-level QE) and doesn't isolate domain or sequence length effects. Controlled experiments generating both MBR and QE versions of the same source data across different domains and sequence lengths, then measuring downstream performance, would resolve this.

### Open Question 3
Does iterative dataset generation (self-distillation of the LLM on its own synthetic data) lead to compounding quality improvements? The authors mention this as a future direction but did not implement it, noting the high computational cost. While self-distillation is shown to improve performance, the potential for iterative improvements remains untested. Experiments generating new MBR/QE datasets from an LLM finetuned on the original NewsPaLM data, measuring performance gains at each iteration, would resolve this.

### Open Question 4
What is the optimal curriculum for training models on mixed greedy, MBR, and QE data? The authors find pretraining on greedy data then finetuning on MBR/QE data outperforms the reverse order, suggesting a curriculum effect. The experiments only test two simple curricula (greedy-then-MBR/QE vs MBR/QE-then-greedy). The optimal ordering and mixing ratios remain unknown. Systematic experiments varying the order, proportions, and timing of exposure to different decoding methods during training, measuring final model performance, would resolve this.

## Limitations
- The approach's effectiveness for other language pairs and non-news domains remains untested
- MBR decoding's quadratic computational complexity may limit practical applicability
- Quality estimation metrics used for generation and evaluation may lead to metric gaming or overfitting

## Confidence
- **High Confidence**: NewsPaLM outperforms WMT'23 training data in direct comparisons
- **Medium Confidence**: Pretraining is more sensitive to dataset quality than finetuning
- **Medium Confidence**: Effectiveness of self-distillation via MBR finetuning

## Next Checks
1. **Domain Generalization Test**: Evaluate NewsPaLM-trained models on non-news domains (e.g., literature, technical documentation, social media) to assess the approach's robustness beyond the Newscrawl source material.
2. **Human Evaluation Correlation**: Conduct human evaluations to validate whether metric improvements (BLEURT, MetricX, COMET) actually correlate with human judgments of translation quality.
3. **Computational Cost Analysis**: Quantify the actual computational overhead of MBR decoding versus traditional data collection methods, including wall-clock time, GPU hours, and energy consumption.