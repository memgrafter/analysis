---
ver: rpa2
title: 'ReLU''s Revival: On the Entropic Overload in Normalization-Free Large Language
  Models'
arxiv_id: '2410.09637'
source_url: https://arxiv.org/abs/2410.09637
tags:
- layer
- entropy
- gelu
- relu
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates activation functions in normalization-free
  decoder-only language models. While GELU is standard in modern transformers, the
  authors show that ReLU significantly outperforms GELU in LayerNorm-free models,
  achieving 8.2% perplexity improvement on GPT-2.
---

# ReLU's Revival: On the Entropic Overload in Normalization-Free Large Language Models

## Quick Facts
- arXiv ID: 2410.09637
- Source URL: https://arxiv.org/abs/2410.09637
- Authors: Nandan Kumar Jha; Brandon Reagen
- Reference count: 40
- ReLU outperforms GELU by 8.2% perplexity in LayerNorm-free models

## Executive Summary
This work investigates activation functions in normalization-free decoder-only language models, revealing that ReLU significantly outperforms the standard GELU in LayerNorm-free settings. The authors demonstrate that LayerNorm-free models with GELU suffer from "entropic overload" in early layers, where attention heads reach near-maximum entropy values and underutilize their representational capacity. Through entropy analysis and learnable slope experiments, they show that ReLU's sparse activation patterns naturally reduce entropy and promote attention head specialization in the absence of normalization layers.

## Method Summary
The authors trained GPT-2 and Pythia models from scratch with various nonlinearity configurations (SM+LN+G, SM+LN+R, SM+G, SM+R) using the CodeParrot dataset (8 GB, 16.7 million Python files, 2.1B tokens) tokenized with a 50K vocabulary. Models were trained on a single RTX 3090 GPU with consistent settings from prior work. The study compared GELU and ReLU performance in both standard and normalization-free settings, measuring perplexity improvement and analyzing attention head entropy distributions across layers.

## Key Results
- ReLU achieves 8.2% perplexity improvement over GELU in LayerNorm-free models
- 58% of heads in LayerNorm-free GELU models have entropy values between 3max/4 and max, compared to only 23% in LayerNorm-free ReLU models
- Learnable negative slopes in leaky ReLU naturally converge toward zero during training, indicating inherent preference for ReLU-like behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReLU activation reduces entropy in early transformer layers compared to GELU, leading to better utilization of attention head representational capacity.
- Mechanism: ReLU's hard thresholding and sparse activation patterns create more focused attention distributions with lower entropy, while GELU's smooth transitions cause entropy overload where many heads reach near-maximum entropy values.
- Core assumption: Lower entropy in attention distributions indicates better specialization and information retention in the absence of LayerNorm.
- Evidence anchors: [abstract] "early layers experience entropic overload, leading to the under-utilization of the representational capacity of attention heads"; [section] "58% of heads in the LN-free GELU model have entropy values between 3max/4 and max, compared to only 23% in the LN-free ReLU model"

### Mechanism 2
- Claim: Learnable negative slopes in leaky ReLU naturally converge toward zero in LayerNorm-free models, indicating inherent preference for ReLU-like behavior.
- Mechanism: During training, the optimization process discovers that zero negative slope provides optimal gradient flow and information retention when LayerNorm is absent, causing all learnable slopes to converge to near-zero values.
- Core assumption: The convergence of learnable slopes to zero is not a training artifact but reflects a fundamental architectural preference.
- Evidence anchors: [section] "Interestingly, in the layerwise setting, the early layers initially learn a positive slope while the deeper layers learn a negative slope. However, as training progresses, all layers converge to a near-zero slope."; [section] "In the global setting, the slope first shifts to positive before converging to near zero."

### Mechanism 3
- Claim: LayerNorm-free models with GELU suffer from geometric incompatibility where smoother activations cannot maintain distinct attention head behaviors without normalization.
- Mechanism: Without LayerNorm to normalize feature distributions, GELU's smooth transitions fail to create the geometric properties (specialization in input space and intra-class selectivity) that ReLU naturally provides, leading to feature collapse and entropic overload.
- Core assumption: The geometric properties of activation functions become more critical in the absence of LayerNorm.
- Evidence anchors: [section] "These observations align with geometrical properties of ReLUs: it preserve more information about the structure of the raw input, encouraging neurons to specialize in different regions of the input space"; [section] "GELU's smoother nonlinearity causes issues in maintaining distinct attention head behaviors"

## Foundational Learning

- Concept: Shannon entropy as a metric for attention distribution
  - Why needed here: Entropy quantifies the uniformity of attention weights, allowing comparison of how different activations affect attention specialization across layers
  - Quick check question: If an attention head has uniform weights across all positions, what would its entropy value be relative to a head focused on a single position?

- Concept: LayerNorm's role in feature normalization and stability
  - Why needed here: Understanding what LayerNorm provides (feature scaling, outlier suppression) explains why its absence creates specific requirements for activation functions
  - Quick check question: How does LayerNorm's feature-wise normalization affect the gradient flow through activation functions?

- Concept: Transformer attention mechanism and self-attention computation
  - Why needed here: The entropy analysis specifically targets attention distributions, requiring understanding of how queries, keys, and values interact to produce attention weights
  - Quick check question: What mathematical operation transforms raw dot products into attention weights, and how does this affect entropy?

## Architecture Onboarding

- Component map: Input → LayerNorm1 → MHA → Residual → LayerNorm2 → FFN → Output
- Critical path: Attention computation → Softmax → Entropy calculation → FFN activation → Residual connection
- Design tradeoffs: ReLU vs GELU (Performance vs smoothness and differentiability), Normalization-free (Reduced computational overhead vs increased sensitivity to activation function choice), Learnable slopes (Flexibility vs convergence to zero slope preference)
- Failure signatures: High entropy in early layers (>3max/4) indicates entropic overload, Poor perplexity scores (>2.5) suggest suboptimal activation choice, Divergent learnable slopes across layers indicate training instability
- First 3 experiments:
  1. Compare entropy distributions between ReLU and GELU in LayerNorm-free model across all layers
  2. Train LayerNorm-free model with learnable negative slopes, monitoring slope evolution over training steps
  3. Measure perplexity improvement when switching from GELU to ReLU in existing LayerNorm-free implementation

## Open Questions the Paper Calls Out
- Question: Does ReLU's advantage over GELU in LayerNorm-free models persist when scaling to larger transformer models with billions of parameters?
  - Basis in paper: The paper notes that current findings are validated on models with fewer than 1B parameters and suggests this as future work in the limitations section.
  - Why unresolved: The authors have not conducted experiments on larger-scale models, and the current study's scope is limited to smaller architectures.
  - What evidence would resolve it: Conducting experiments on transformer models with 1B+ parameters and comparing perplexity and entropy metrics between ReLU and GELU in LayerNorm-free settings.

- Question: How does the choice of activation function (ReLU vs GELU) in LayerNorm-free models affect downstream task performance and fine-tuning capabilities?
  - Basis in paper: The paper acknowledges this limitation, stating it focused on pre-training performance with perplexity as the primary metric and did not evaluate transfer learning or few-shot learning.
  - Why unresolved: The current study only examined perplexity and entropy during pre-training, without assessing performance on specific downstream tasks or fine-tuning scenarios.
  - What evidence would resolve it: Evaluating LayerNorm-free models with different activation functions on standard benchmarks like GLUE, SuperGLUE, or code-specific tasks, and measuring performance after fine-tuning.

- Question: Would a hybrid activation strategy (using ReLU in early layers and GELU in later layers) provide optimal performance in LayerNorm-free transformer architectures?
  - Basis in paper: The authors suggest exploring hybrid activation strategies as future work, noting the possibility of balancing early-layer benefits of ReLU with later-layer benefits of GELU.
  - Why unresolved: The current study only tested uniform activation functions across all layers, not exploring the potential benefits of layer-specific activation choices.
  - What evidence would resolve it: Training LayerNorm-free models with different activation functions in different layers and comparing their perplexity, entropy distributions, and overall performance against uniform activation models.

## Limitations
- Study focuses primarily on decoder-only architectures with specific model sizes (GPT-2 and Pythia configurations)
- Entropy analysis is limited to attention distributions without examining feature space geometry directly
- Learnable slope experiments use relatively small models, and scaling effects remain unexplored

## Confidence
- High Confidence: ReLU's 8.2% perplexity improvement over GELU in LayerNorm-free models is well-supported by experimental results across multiple model scales and architectures
- Medium Confidence: The entropic overload mechanism is theoretically sound but requires additional validation to confirm whether entropy reduction directly causes the performance gains
- Medium Confidence: The convergence of learnable slopes to zero is consistently observed but needs broader testing across different initialization schemes and optimization hyperparameters

## Next Checks
1. Cross-architecture validation: Test ReLU vs GELU in encoder-decoder models and vision transformers to determine if entropic overload is specific to decoder-only architectures
2. Alternative normalization methods: Introduce dynamic scaling or other lightweight normalization techniques to test whether they can mitigate entropic overload while preserving GELU's benefits
3. Geometry verification: Directly measure feature space specialization and intra-class selectivity to empirically validate the geometric incompatibility hypothesis between GELU and LayerNorm-free architectures