---
ver: rpa2
title: Achieving Fairness in Predictive Process Analytics via Adversarial Learning
  (Extended Version)
arxiv_id: '2410.02618'
source_url: https://arxiv.org/abs/2410.02618
tags:
- process
- variables
- framework
- protected
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces an adversarial debiasing framework for predictive
  process analytics that mitigates bias from protected variables (e.g., gender, nationality)
  while maintaining high prediction accuracy. The approach uses two neural networks:
  one for prediction and an adversarial network that constrains the prediction model
  from relying on protected variables.'
---

# Achieving Fairness in Predictive Process Analytics via Adversarial Learning (Extended Version)

## Quick Facts
- arXiv ID: 2410.02618
- Source URL: https://arxiv.org/abs/2410.02618
- Reference count: 21
- Primary result: Adversarial debiasing framework reduces protected variable influence by up to 99% while maintaining or improving prediction accuracy across four case studies

## Executive Summary
This paper introduces an adversarial debiasing framework for predictive process analytics that mitigates bias from protected variables (e.g., gender, nationality) while maintaining high prediction accuracy. The approach uses two neural networks: one for prediction and an adversarial network that constrains the prediction model from relying on protected variables. Tested on four case studies (incident management, hiring, and hospital processes), the framework significantly reduces the influence of protected variables (e.g., reducing Shapley values by up to 99%) while achieving better accuracy and fairness metrics compared to existing approaches. The method particularly excels at reducing bias correlations between protected and correlated variables, demonstrating its effectiveness in creating fairer predictive process analytics systems.

## Method Summary
The framework employs two fully connected neural networks trained jointly: a prediction network (Φ) that forecasts process outcomes and an adversarial network (ΦZ) that predicts protected variables from the prediction network's output. During training, the prediction network is penalized for allowing the adversarial network to accurately predict protected variables, forcing it to minimize reliance on protected variable information. The event log traces are encoded into feature vectors using a trace-to-instance encoding function that captures activity frequencies and latest attribute values. The loss function combines prediction accuracy and adversarial debiasing with negative sign, optimized through gradient descent with weight decay. Hyper-parameters are tuned via grid search, and evaluation includes measuring Shapley values for protected variables, prediction accuracy, and fairness metrics like Equalized Odds.

## Key Results
- The framework reduces Shapley values for protected variables by up to 99% across all case studies
- Prediction accuracy is maintained or improved compared to baseline models, with accuracy reduction limited to 4% in worst cases
- The approach successfully reduces bias not only in protected variables but also in variables correlated with protected variables
- Equalized Odds criterion is satisfied for all classification problems, demonstrating strong fairness performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adversarial debiasing framework reduces the influence of protected variables on predictions by training two neural networks in competition.
- Mechanism: A prediction model (FCNN) is trained to forecast outcomes while an adversarial network is trained to predict protected variable values from the prediction model's output. The prediction model is penalized for allowing the adversarial network to succeed, forcing it to minimize reliance on protected variables.
- Core assumption: If the prediction model does not rely on protected variables, the adversarial network cannot accurately predict those variables from the model's output.
- Evidence anchors:
  - [abstract] "the proposed framework is based on the idea to train the model to predict the process’ outcome values while constraining a jointly-trained adversary from accurately predicting the protected variables, reducing bias in its learned representations."
  - [section 4] "if the neural network that implements Φ - in our case a FCNN - does not build the prediction on the protected variables, then the adversarial network that implements ΦZ - in our case another FCNN - is unable to predict the protected-variables values from the output of the network implementing Φ."
- Break condition: If protected variables have strong predictive power that cannot be captured by other variables, forcing the model to ignore them will significantly reduce prediction accuracy.

### Mechanism 2
- Claim: The framework reduces bias even for variables correlated with protected variables, not just the protected variables themselves.
- Mechanism: When the prediction model learns to minimize the adversarial network's ability to predict protected variables, it must also minimize reliance on any correlated variables that could indirectly reveal protected variable information.
- Core assumption: Variables correlated with protected variables contain information that can help the adversarial network infer protected variable values.
- Evidence anchors:
  - [abstract] "The experimental results also highlight that the influence is also reduced for those process’ variables that are strongly correlated with the protected variables."
  - [section 5.4] "the Shapley value for variable organization country is also significantly reduced, likely because it is correlated with the protected variable."
- Break condition: If correlation between protected and other variables is weak or non-linear in ways the model cannot capture, the framework may not reduce bias in correlated variables.

### Mechanism 3
- Claim: The framework maintains prediction accuracy while reducing bias, outperforming simpler fairness approaches.
- Mechanism: By using a joint loss function that balances prediction accuracy with adversarial constraint, the framework can find solutions that preserve useful predictive information while removing bias, rather than simply removing variables or using post-processing.
- Core assumption: The adversarial training process can find parameter configurations that satisfy both accuracy and fairness constraints simultaneously.
- Evidence anchors:
  - [abstract] "our framework allows for a more enhanced level of fairness, while retaining a better prediction quality."
  - [section 5.4] "Our framework consistently obtains higher accuracy for all case studies, if compared with Qafari et al. [16], and also the accuracy reduction is significantly more limited."
- Break condition: If the optimization landscape has no feasible solutions satisfying both constraints, the model may fail to converge or achieve neither goal well.

## Foundational Learning

- Concept: Event log structure and trace encoding
  - Why needed here: The framework operates on event logs, converting traces into feature vectors for neural network input. Understanding this encoding is essential for implementing and modifying the system.
  - Quick check question: How does the encoding function ρL convert a trace into a feature vector, and what information does each dimension represent?

- Concept: Shapley values for feature importance
  - Why needed here: The evaluation methodology uses Shapley values to quantify the influence of protected variables on predictions, both before and after debiasing. Understanding this metric is crucial for interpreting results.
  - Quick check question: How is a Shapley value calculated for a feature, and what does a reduction in Shapley value indicate about that feature's influence?

- Concept: Adversarial training in neural networks
  - Why needed here: The core mechanism relies on training two neural networks in competition. Understanding how adversarial training works is essential for implementing the framework and troubleshooting issues.
  - Quick check question: In the context of this framework, what is the objective function for each network, and how do they interact during training?

## Architecture Onboarding

- Component map:
  - Event log traces -> ρL encoding -> Prediction model (FCNN Φ) -> Output predictions
  - Event log traces -> ρL encoding -> Adversarial model (FCNN ΦZ) -> Protected variable predictions (training only)

- Critical path: Trace -> ρL encoding -> Prediction model -> Output prediction
  The adversarial model operates on the prediction model's output layer during training but is not part of inference

- Design tradeoffs:
  - FCNN vs LSTM: FCNNs train faster with similar accuracy but may miss temporal patterns that LSTMs capture
  - Number of adversarial layers: More layers may improve adversarial performance but increase training complexity
  - Loss function weighting: Balancing accuracy vs fairness requires tuning hyperparameters

- Failure signatures:
  - Poor convergence: Training loss oscillates or plateaus early
  - Accuracy collapse: Protected variable Shapley values drop but overall accuracy significantly degrades
  - Bias persistence: Protected variable Shapley values remain high despite training
  - Overfitting: Model performs well on training data but poorly on test data

- First 3 experiments:
  1. Train prediction model alone on dataset, measure baseline accuracy and protected variable Shapley values
  2. Add adversarial component with equal weighting, observe changes in accuracy and Shapley values
  3. Vary adversarial loss weighting to find optimal balance between accuracy preservation and bias reduction

## Open Questions the Paper Calls Out

- Can the adversarial debiasing framework be effectively extended from predictive to prescriptive analytics for providing fair recommendations?
  - Basis in paper: [explicit] The authors explicitly identify this as future work, noting that adding fairness to recommendations could ensure fair assignment of activities to resources and prevent overload.
  - Why unresolved: The framework is currently only validated for predictive analytics, and prescriptive analytics involves additional complexity in recommendation generation.
  - What evidence would resolve it: Implementation and evaluation of the framework for prescriptive analytics on case studies where recommendations are generated, with fairness metrics comparing resource allocation patterns.

- How does the performance of the adversarial debiasing framework change when using alternative trace encoding methods beyond the one described in Section 3.1?
  - Basis in paper: [explicit] The authors acknowledge using a specific encoding method for event-log traces but note that alternatives are possible [2] and could lead to further improvements.
  - Why unresolved: The paper only evaluates one encoding method (frequency-based encoding) while acknowledging other encoding techniques exist.
  - What evidence would resolve it: Systematic comparison of the framework's performance across multiple trace encoding methods (e.g., embedding-based, one-hot encoding, aggregation-based) using the same case studies and evaluation metrics.

## Limitations
- The framework's effectiveness is demonstrated on four specific case studies with binary protected variables and relatively controlled datasets.
- The approach may face challenges when scaling to datasets with multiple protected variables, continuous protected attributes, or highly complex correlations between variables.
- The evaluation focuses primarily on Shapley value reduction and prediction accuracy, with limited exploration of long-term stability or performance on dynamic, evolving processes.

## Confidence
- **High**: The core adversarial debiasing mechanism (Mechanism 1) is well-established in machine learning literature and the implementation details are sufficiently specified for reproduction
- **Medium**: Claims about bias reduction in correlated variables (Mechanism 2) and maintained accuracy (Mechanism 3) are supported by the experimental results but could benefit from additional stress testing across diverse datasets
- **Low**: The generalizability of results to real-world production systems with streaming data and changing distributions remains unproven

## Next Checks
1. Test the framework on datasets with multiple protected variables simultaneously to evaluate if it can handle intersectional fairness concerns
2. Conduct stress testing by introducing synthetic noise and distributional shifts to assess robustness of debiasing under real-world conditions
3. Implement an ablation study removing the adversarial component to quantify the exact contribution of adversarial training versus standard neural network regularization