---
ver: rpa2
title: Benchmarking Benchmark Leakage in Large Language Models
arxiv_id: '2404.18824'
source_url: https://arxiv.org/abs/2404.18824
tags:
- training
- benchmark
- data
- test
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses benchmark dataset leakage in large language
  models (LLMs) by proposing a detection pipeline using perplexity and N-gram accuracy
  metrics. The authors analyze 31 LLMs on mathematical reasoning benchmarks (GSM8K
  and MATH) and reveal substantial instances of training and test set misuse, leading
  to potentially unfair comparisons.
---

# Benchmarking Benchmark Leakage in Large Language Models

## Quick Facts
- arXiv ID: 2404.18824
- Source URL: https://arxiv.org/abs/2404.18824
- Authors: Ruijie Xu; Zengzhi Wang; Run-Ze Fan; Pengfei Liu
- Reference count: 40
- Key outcome: Detection pipeline reveals substantial benchmark leakage across 31 LLMs, suggesting many models may have unfair performance advantages

## Executive Summary
This paper addresses the critical issue of benchmark dataset leakage in large language models (LLMs) by proposing a detection pipeline using perplexity and N-gram accuracy metrics. The authors analyze 31 LLMs on mathematical reasoning benchmarks (GSM8K and MATH) and reveal substantial instances of training and test set misuse, leading to potentially unfair comparisons. Their approach involves synthesizing paraphrased reference benchmarks and comparing atomic metrics between original and synthesized data. The results show that many models, including well-known ones, may have inadvertently leveraged benchmark data to boost performance. The paper introduces a "Benchmark Transparency Card" to encourage clear documentation of benchmark utilization, promoting transparency and healthy development of LLMs.

## Method Summary
The authors propose a detection pipeline to identify benchmark dataset leakage in LLMs by comparing model performance on original and paraphrased benchmarks. They synthesize paraphrased versions of GSM8K and MATH using ChatGPT with temperature=0.7 and top_p=0.9. For each model, they calculate perplexity on solution parts and N-gram accuracy (n=5,10) for both original and synthesized benchmarks. They then normalize the metric differences as percentage decreases and compute δtrain-test = δtrain - δtest to identify potential training set leakage. Models with high δtrain-test scores (≥1.0) are flagged as potentially having seen training data, while those with low δtrain-test but high absolute δ scores may have seen test data.

## Key Results
- 31 LLMs analyzed, with many showing evidence of benchmark exposure through high δtrain-test scores
- The pipeline successfully identifies known cases of leakage (e.g., Aquila2's documented GSM8K exposure)
- N-gram accuracy provides instance-level detection capability, with exact 5-gram predictions suggesting verbatim memorization
- "Benchmark Transparency Card" proposed as a community standard for documenting benchmark utilization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity and N-gram accuracy metrics detect data leakage by comparing model performance on original vs. paraphrased benchmarks.
- Mechanism: Lower perplexity or higher N-gram accuracy on original benchmarks compared to paraphrased versions indicates familiarity due to prior exposure during training.
- Core assumption: Paraphrased benchmarks maintain the same semantic content and difficulty as original benchmarks while changing surface-level text.
- Evidence anchors:
  - [abstract] "We introduce a detection pipeline utilizing Perplexity and N-gram accuracy, two simple and scalable metrics that gauge a model's prediction precision on benchmark, to identify potential data leakages."
  - [section 3.1] "Both Perplexity and N-gram Accuracy metrics provide unique insights into a language model's performance, primarily assessing precision in next-token prediction."
  - [corpus] Weak - corpus does not directly address the effectiveness of perplexity/N-gram metrics for leakage detection.
- Break condition: If the model has seen both training and test sets, the difference metric becomes unreliable as both sets would show similar familiarity.

### Mechanism 2
- Claim: N-gram accuracy enables instance-level leakage detection by measuring exact prediction matches.
- Mechanism: When a model correctly predicts all 5-grams in a sample, it strongly suggests the model has seen that exact instance during training.
- Core assumption: N-gram predictions are exact matches to the ground truth, indicating verbatim memorization.
- Evidence anchors:
  - [section 3.1] "N-gram Accuracy is a discrete metric focusing on the model's ability to replicate exact subsequences (n-grams) from training data."
  - [section 5.3] "high accuracy for each n-gram of an example's prediction suggests a high probability that the sample was encountered during the training process."
  - [corpus] Weak - corpus does not provide direct evidence about instance-level detection capabilities.
- Break condition: If benchmark data undergoes reformatting, paraphrasing, or augmentation, exact matches become less reliable.

### Mechanism 3
- Claim: Percentage decrease normalization (δ) enables fair comparison across models of different sizes and capabilities.
- Mechanism: By normalizing the metric difference as a percentage of the original score, the approach accounts for baseline performance differences between models.
- Core assumption: Percentage decrease provides a standardized measure that accounts for varying absolute metric values across models.
- Evidence anchors:
  - [section 3.3] "we normalize this metric by dividing it with the original metric score, Mori, to standardize it. This results in the percentage decrease in the atomic metric δ."
  - [section 4] "As the calculation of n-gram accuracy spans the entire text of both question and solution, it yields in higher ∆ and δ scores for models trained through pre-training compared to those trained via supervised fine-tuning."
  - [corpus] Weak - corpus does not directly validate the normalization approach.
- Break condition: When absolute metric differences are small, percentage calculations can produce unstable or misleading results.

## Foundational Learning

- Concept: Language modeling fundamentals (perplexity, token prediction)
  - Why needed here: Understanding how perplexity measures model uncertainty and prediction confidence is crucial for interpreting leakage detection results.
  - Quick check question: What does a lower perplexity score indicate about a model's predictions on a given text?

- Concept: N-gram and sequence prediction
  - Why needed here: The N-gram accuracy metric relies on understanding how models predict subsequences and how exact matches indicate memorization.
  - Quick check question: How does predicting exact 5-grams differ from predicting individual tokens in terms of what it reveals about model behavior?

- Concept: Data synthesis and paraphrasing techniques
  - Why needed here: The approach depends on creating synthetic benchmarks that maintain semantic content while changing surface form.
  - Quick check question: What are the risks of using synthetic data for leakage detection, and how does the approach attempt to mitigate them?

## Architecture Onboarding

- Component map:
  Data synthesis module (ChatGPT-based paraphrasing) -> Metric calculation pipeline (perplexity and N-gram accuracy) -> Comparison engine (percentage decrease normalization) -> Visualization tools (leaderboard display) -> Case study interface (instance-level analysis)

- Critical path:
  1. Synthesize paraphrased benchmarks
  2. Calculate metrics on original and synthetic data
  3. Normalize and compare results
  4. Generate visualizations and reports

- Design tradeoffs:
  - Using ChatGPT for synthesis trades off control for scalability and diversity
  - N-gram accuracy provides instance-level detail but may miss augmented data
  - Percentage normalization enables cross-model comparison but can be unstable with small absolute differences

- Failure signatures:
  - Zero δtrain-test scores could indicate either no leakage or concurrent leakage of both training and test sets
  - High δ scores with poor N-gram accuracy suggest data augmentation or reformatting
  - Inconsistent results across different atomic metrics may indicate model-specific behaviors

- First 3 experiments:
  1. Test the pipeline on a known contaminated model (e.g., Aquila2 with documented GSM8K exposure) to verify detection capability
  2. Apply the approach to a model known to be clean to establish baseline behavior
  3. Test with paraphrased benchmarks that have different levels of semantic preservation to understand sensitivity thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed pipeline perform in detecting data leakage when the benchmark data has undergone significant augmentation or reformatting?
- Basis in paper: [inferred] The paper mentions that the pipeline might not detect cases where models are trained on benchmarks that have been augmented or reformatted.
- Why unresolved: The paper acknowledges this limitation but does not provide extensive experiments or results on the pipeline's performance in such scenarios.
- What evidence would resolve it: Additional experiments showing the pipeline's effectiveness in detecting data leakage in cases where benchmark data has been significantly augmented or reformatted.

### Open Question 2
- Question: How does the detection pipeline handle simultaneous leakage in both training and testing datasets?
- Basis in paper: [explicit] The paper states that the pipeline might not effectively detect cases where both training and testing sets are leaked simultaneously.
- Why unresolved: The paper acknowledges this limitation but does not provide a detailed explanation or potential solutions for handling this scenario.
- What evidence would resolve it: A thorough analysis of the pipeline's performance in cases of simultaneous leakage, along with potential modifications or improvements to address this issue.

### Open Question 3
- Question: How does the proposed "Benchmark Transparency Card" impact the development and evaluation of large language models in practice?
- Basis in paper: [explicit] The paper proposes the "Benchmark Transparency Card" as a recommendation for model documentation, but does not provide empirical evidence of its impact.
- Why unresolved: The paper introduces the concept and its potential benefits but does not provide case studies or real-world examples of its implementation and effects.
- What evidence would resolve it: Case studies or examples of models that have implemented the "Benchmark Transparency Card" and the resulting impact on model development, evaluation, and community perception.

## Limitations
- The approach cannot distinguish between models that have seen both training and test sets versus those that have seen neither, as both produce zero δtrain-test scores
- N-gram accuracy only detects exact or near-exact format matches, potentially missing models trained on augmented or reformatted versions of benchmark data
- The effectiveness of ChatGPT-based paraphrasing in creating semantically equivalent but textually distinct benchmarks has not been rigorously validated

## Confidence

**High Confidence**: The core methodology of using perplexity and N-gram accuracy to detect benchmark leakage is well-established in the literature. The normalization approach (δtrain-test) provides a reasonable metric for cross-model comparison, and the general observation that many LLMs show evidence of benchmark exposure is supported by the consistent patterns across multiple models.

**Medium Confidence**: The specific threshold of δtrain-test ≥ 1.0 for flagging potential leakage is somewhat arbitrary and may not account for model size differences or other confounding factors. The effectiveness of ChatGPT-based paraphrasing in creating semantically equivalent but textually distinct benchmarks has not been rigorously validated.

**Low Confidence**: The claim that N-gram accuracy provides instance-level leakage detection is based on theoretical reasoning rather than empirical validation. The paper does not demonstrate that models with high N-gram accuracy scores have actually memorized specific instances, only that they show patterns consistent with memorization.

## Next Checks
1. **Controlled Contamination Test**: Apply the detection pipeline to a model with known, documented exposure to GSM8K (such as Aquila2) and verify that it produces high δtrain-test scores, while simultaneously testing on a verified clean model to establish baseline behavior.

2. **Paraphrase Fidelity Analysis**: Systematically vary the paraphrasing parameters (temperature, top_p, prompt specificity) and measure how changes affect both the semantic preservation of benchmarks and the leakage detection sensitivity. This would quantify the robustness of the approach to synthesis variations.

3. **Augmentation Resilience Test**: Evaluate the pipeline's performance on models trained on augmented versions of benchmark data (e.g., with added noise, different formatting, or combined with other datasets) to determine whether the approach can detect leakage when exact matches are unavailable.