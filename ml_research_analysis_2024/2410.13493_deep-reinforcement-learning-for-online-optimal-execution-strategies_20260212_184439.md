---
ver: rpa2
title: Deep Reinforcement Learning for Online Optimal Execution Strategies
arxiv_id: '2410.13493'
source_url: https://arxiv.org/abs/2410.13493
tags:
- optimal
- agent
- strategy
- algorithm
- execution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of learning optimal execution
  strategies in dynamic financial markets with transient price impact. The authors
  propose a novel actor-critic algorithm based on Deep Deterministic Policy Gradient
  (DDPG) to approximate the optimal execution strategy without relying on strict assumptions
  about market price impact models.
---

# Deep Reinforcement Learning for Online Optimal Execution Strategies

## Quick Facts
- arXiv ID: 2410.13493
- Source URL: https://arxiv.org/abs/2410.13493
- Authors: Alessandro Micheli; Mélodie Monod
- Reference count: 5
- Primary result: DDPG algorithm with auxiliary Q-function successfully learns optimal execution strategies in dynamic markets with transient price impact.

## Executive Summary
This paper proposes a novel actor-critic algorithm based on Deep Deterministic Policy Gradient (DDPG) to solve optimal execution problems in dynamic financial markets. The approach approximates optimal execution strategies without relying on strict assumptions about market price impact models, using an auxiliary Q-function that improves convergence stability and speed. Through numerical experiments with various decay kernels, the algorithm demonstrates successful convergence to optimal strategies and adaptability to evolving market conditions.

## Method Summary
The authors formulate optimal execution as a Markov Decision Process where an agent liquidates an initial portfolio over a fixed time horizon while minimizing market impact. They employ a DDPG algorithm with continuous action spaces to enable flexible execution strategies. The key innovation is an auxiliary Q-function that simplifies the input space from four to three deterministic inputs, improving convergence stability. The agent learns through experience replay and target networks, with online learning capability enabling adaptation to changing market parameters during execution.

## Key Results
- The DDPG algorithm successfully converges to optimal execution strategies for various decay kernels (exponential, power-law, linear resilience).
- The auxiliary Q-function significantly improves convergence stability and speed compared to standard Q-function approaches.
- The method demonstrates adaptability to evolving market conditions through online learning within and across episodes.
- The continuous action space enables more flexible and complex execution strategies than discrete-action methods.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The auxiliary Q-function simplifies the input space and stabilizes convergence by reducing the state representation from four to three deterministic inputs.
- Mechanism: By projecting the state to (t, Xξt, ξ0:(t-1)) and removing the price component, the critic learns over a smaller, deterministic input space, reducing function approximation complexity.
- Core assumption: The auxiliary Q-function satisfies a Bellman equation and can replace the standard Q-function in DPG.
- Evidence anchors: Abstract states auxiliary Q-function improves convergence stability and speed; Proposition 3.1 confirms the auxiliary Bellman equation.
- Break condition: If market price impact includes permanent components, the auxiliary Q-function may no longer satisfy the simplified Bellman equation.

### Mechanism 2
- Claim: The DDPG algorithm's continuous action space enables more flexible and complex execution strategies than discrete-action methods.
- Mechanism: Parameterizing the policy as ξθ(π(s)) allows output of any action within continuous interval [-Xξt, 0], enabling fine-grained control over trade sizes.
- Core assumption: Continuous action space benefits outweigh optimization difficulties.
- Evidence anchors: Abstract mentions DDPG's ability to handle continuous action spaces for decisions beyond binary actions.
- Break condition: If optimal strategy is piecewise constant or discrete, continuous action space may introduce unnecessary complexity.

### Mechanism 3
- Claim: The DDPG algorithm's online learning capability enables real-time adaptation to evolving market conditions without manual recalibration.
- Mechanism: Continuous updating of actor and critic networks using experience replay and target networks allows tracking of decay kernel parameter changes.
- Core assumption: Market parameters change slowly enough that replay buffer and pre-trained weights provide good starting points.
- Evidence anchors: Abstract states method reduces need for frequent human intervention by enabling online learning.
- Break condition: If market parameters change abruptly, online learning may not adapt quickly enough.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: Optimal execution problem formalized as MDP to leverage RL algorithms that maximize expected cumulative reward.
  - Quick check question: In MDP formulation, what is the state representation and how does it capture transient price impact?

- Concept: Bellman Equations
  - Why needed here: Bellman equation (3.3) foundation for learning optimal Q-function guiding agent to maximize execution profits.
  - Quick check question: How does auxiliary Bellman equation (3.6) differ from standard Bellman equation (3.3), and why is this beneficial?

- Concept: Policy Gradient Methods
  - Why needed here: DPG algorithm uses policy gradient methods to update actor network parameters in direction maximizing expected execution profits.
  - Quick check question: What role does critic network play in estimating Q-function, and how does it enable actor network to improve policy?

## Architecture Onboarding

- Component map:
  Actor network (ξθ) -> Critic network (Qϕ) -> Target networks (θtarget, ϕtarget) -> Experience replay buffer (D) -> OU noise process (ε)

- Critical path:
  1. Observe state st and execute action at
  2. Store transition (st, at, rt, st+1) in memory buffer
  3. Sample batch B from experience replay buffer
  4. Compute target Q-values using target networks
  5. Update critic network by minimizing MSBE loss
  6. Update actor network by maximizing expected Q-value
  7. Update target networks using Polyak averaging

- Design tradeoffs:
  - Continuous vs. discrete action space: Continuous space allows more flexible strategies but requires function approximation for optimization
  - Auxiliary Q-function vs. standard Q-function: Auxiliary Q-function simplifies input space and stabilizes convergence but may not capture all relevant information
  - Online vs. offline learning: Online learning enables adaptation to changing market conditions but may be slower to converge initially

- Failure signatures:
  - Critic network collapses to constant Q-value: Indicates poor exploration or learning rate issues
  - Actor network outputs actions outside valid range: Indicates incorrect action scaling or activation function issues
  - Performance plateaus or degrades over time: Indicates insufficient exploration, poor replay buffer management, or non-stationary environment

- First 3 experiments:
  1. Verify convergence to optimal strategy on simple exponential kernel with known closed-form solution
  2. Test impact of auxiliary Q-function on convergence speed and stability by comparing with standard Q-function
  3. Evaluate online learning performance in dynamic environment with slowly varying decay kernel parameters

## Open Questions the Paper Calls Out

- Question: How does DDPG algorithm perform in environments with more complex, non-convex decay kernels?
  - Basis in paper: [inferred] Paper tests convergence only on strictly convex kernels and assumes this guarantees absence of transaction-triggered price manipulation
  - Why unresolved: Paper explicitly assumes strictly convex kernels and does not test or discuss performance with non-convex kernels
  - What evidence would resolve it: Testing DDPG algorithm on various non-convex decay kernel functions and comparing convergence rates and final performance to strictly convex case

- Question: What is the impact of incorporating exogenous signals (e.g., market volatility, order flow) into state representation?
  - Basis in paper: [inferred] Paper mentions this as potential direction for future work but does not explore it experimentally
  - Why unresolved: State representation in experiments only includes time, inventory, and past actions, without external market indicators
  - What evidence would resolve it: Implementing and testing DDPG algorithm with augmented state spaces including various exogenous signals and measuring performance improvements

- Question: How does DDPG algorithm scale to optimal execution problems involving multiple assets or portfolio of stocks?
  - Basis in paper: [explicit] Paper explicitly states this as future work in conclusion section
  - Why unresolved: Current formulation and experiments focus on single-asset liquidation problems
  - What evidence would resolve it: Extending DDPG algorithm to handle multi-asset environments and testing performance on portfolio liquidation problems with correlated assets

## Limitations

- The auxiliary Q-function's performance heavily depends on the assumption of purely transient price impact; permanent impact components could undermine convergence benefits.
- The continuous action space may introduce optimization challenges if the true optimal strategy is inherently discrete in nature.
- The online learning mechanism's effectiveness in rapidly changing market conditions remains uncertain, as abrupt parameter shifts could overwhelm the adaptation process.

## Confidence

- **High confidence**: DDPG algorithm's basic architecture and training procedure are sound and well-established; convergence to optimal strategy on simple exponential kernels is strongly supported.
- **Medium confidence**: Auxiliary Q-function's benefits for convergence stability and speed are demonstrated, but mechanism's robustness to complex market dynamics is unclear.
- **Low confidence**: Algorithm's performance in highly dynamic environments with rapid parameter changes is not thoroughly tested; impact of continuous action space on learning efficiency is not fully explored.

## Next Checks

1. Test algorithm's performance when permanent price impact is introduced alongside transient impact, assessing auxiliary Q-function's robustness.
2. Evaluate impact of discretizing action space on convergence speed and final performance, comparing with continuous action space approach.
3. Simulate abrupt changes in decay kernel parameters to stress-test online learning mechanism's adaptation speed and stability.