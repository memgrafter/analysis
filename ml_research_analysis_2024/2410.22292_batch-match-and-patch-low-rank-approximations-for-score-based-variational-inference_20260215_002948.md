---
ver: rpa2
title: 'Batch, match, and patch: low-rank approximations for score-based variational
  inference'
arxiv_id: '2410.22292'
source_url: https://arxiv.org/abs/2410.22292
tags:
- variational
- gaussian
- covariance
- matrix
- pbam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of scaling score-based variational
  inference to high-dimensional problems by introducing a novel algorithm that combines
  batch-and-match (BaM) updates with a "patch" step that projects covariance matrices
  into a structured low-rank plus diagonal form. This approach significantly reduces
  memory and computational costs while maintaining convergence speed.
---

# Batch, match, and patch: low-rank approximations for score-based variational inference

## Quick Facts
- arXiv ID: 2410.22292
- Source URL: https://arxiv.org/abs/2410.22292
- Reference count: 40
- Key outcome: Combines batch-and-match updates with low-rank plus diagonal covariance projection to achieve scalable score-based variational inference with faster convergence and better accuracy than ADVI on high-dimensional problems.

## Executive Summary
This paper addresses the challenge of scaling score-based variational inference to high-dimensional problems by introducing a novel algorithm that combines batch-and-match (BaM) updates with a "patch" step that projects covariance matrices into a structured low-rank plus diagonal form. This approach significantly reduces memory and computational costs while maintaining convergence speed. Experiments demonstrate that the proposed method outperforms existing approaches like ADVI on synthetic Gaussian targets and real-world inference tasks, including Poisson regression and log-Gaussian Cox processes, achieving faster convergence and better accuracy in high-dimensional settings.

## Method Summary
The proposed method, pBaM (patch batch-and-match), extends the BaM algorithm by adding a projection step that converts dense covariance updates into low-rank plus diagonal (LR+D) form. The algorithm alternates between three steps: batch sampling to compute statistics, match step to update mean and covariance via closed-form equations, and patch step to project the updated covariance into LR+D structure using an EM algorithm. This maintains O(D) memory and computation complexity by leveraging the Woodbury identity and the low-rank structure of batch statistics.

## Key Results
- pBaM achieves faster convergence than ADVI on synthetic Gaussian targets with D=512 and K=32
- Maintains O(D) memory and computation by projecting covariance updates into low-rank plus diagonal form
- Outperforms ADVI-LR and ADVI-D baselines on Poisson regression (D=100) and Log-Gaussian Cox processes (D=811) inference tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: pBaM avoids the quadratic scaling of full covariance storage by projecting each covariance update into a low-rank plus diagonal (LR+D) form.
- Mechanism: After each BaM covariance update, an EM-based "patch" step projects the dense matrix into the LR+D form, preserving the rank-B+1 structure implied by the batch statistics.
- Core assumption: The optimal LR+D approximation to the BaM update can be computed via EM without significantly degrading convergence.
- Evidence anchors: [abstract] "we add an extra step to each iteration of BaM—a patch—that projects each newly updated covariance matrix into a more efficiently parameterized family of diagonal plus low rank matrices."
- Break condition: If the KL divergence between the true BaM update and its LR+D projection becomes too large, convergence may stall or slow dramatically.

### Mechanism 2
- Claim: The patch step can be implemented with O(D) memory and computation because all required matrices (Qt, Rt) are already low rank from the batch statistics.
- Mechanism: Qt and Rt from the BaM batch step are rank-(B+1) and rank-(K+B+1) respectively; Woodbury identities allow implicit covariance evaluations without forming D×D matrices.
- Core assumption: B ≪ D and K ≪ D so that low-rank structure dominates storage and compute.
- Evidence anchors: [section] "When B ≪ D, the matrix Ut = QtQ⊤t is low rank, as it is a sum of (B + 1) rank-1 matrices... Hence the updated covariance matrix Σt+1/2 can be implicitly stored via the matrices Ψ, Qt, and Rt, which are of size D, D × (B + 1) and D × (K + 1), respectively; i.e., the memory cost is linear in D."
- Break condition: If B or K grows comparable to D, the O(D) gains vanish and memory/compute revert to quadratic.

### Mechanism 3
- Claim: pBaM converges faster than ADVI because it uses specialized score-matching updates rather than generic stochastic gradient descent on the ELBO.
- Mechanism: BaM matches the score functions of target and approximation directly, which for Gaussians has a closed-form update that avoids noisy gradient estimates and learning-rate tuning.
- Core assumption: Score-matching is more efficient than ELBO gradient descent for Gaussian approximations.
- Evidence anchors: [abstract] "Unlike classical algorithms for BBVI, which use gradient descent to minimize the reverse Kullback-Leibler divergence, BaM uses more specialized updates to match the scores of the target density and its Gaussian approximation."
- Break condition: If the target score is expensive to compute or noisy, the advantage over ELBO-based methods may disappear.

## Foundational Learning

- Concept: Woodbury matrix identity
  - Why needed here: Allows efficient inversion and determinant computation for LR+D covariances without forming full D×D matrices.
  - Quick check question: Given Σ = Ψ + ΛΛ⊤, how do you compute Σ⁻¹v for v ∈ ℝᴰ in O(DK²) time?

- Concept: EM algorithm for factor analysis
  - Why needed here: Provides the projection step that finds the best LR+D approximation to the BaM covariance update.
  - Quick check question: In the E-step, what is the conditional covariance of the latent factor given the observed vector?

- Concept: Score-matching divergence
  - Why needed here: The objective BaM optimizes; understanding it explains why the closed-form updates exist.
  - Quick check question: Write the score-matching objective for two Gaussian densities in terms of their means and covariances.

## Architecture Onboarding

- Component map: Batch -> Match -> Patch -> (loop)
- Critical path: Batch → Match → Patch → (loop), with Patch dependent on Match output.
- Design tradeoffs:
  - Higher rank K → better approximation but more EM iterations and memory
  - Larger batch B → better covariance estimate but higher O(B³) cost in Match step
  - Larger λt → faster mean updates but potential instability in covariance updates
- Failure signatures:
  - Divergence in ADVI but not pBaM → suggests BaM's closed form is more stable
  - Patch KL plateauing → LR+D projection losing too much information
  - EM step taking many iterations → patch tolerance too strict or poor initialization
- First 3 experiments:
  1. Run pBaM vs ADVI-LR on synthetic low-rank Gaussian target with D=512, K=32; compare KL vs gradient evals.
  2. Sweep B ∈ {8,16,32,64} and λt ∈ {0.1,1,10} on the same target; measure convergence speed and EM steps.
  3. Replace EM patch with rank-K truncated SVD of Σt+1/2; compare KL and runtime to assess patch optimality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different learning rate schedules (e.g., constant, linear, cosine) on the convergence of pBaM compared to ADVI?
- Basis in paper: [explicit] The paper compares different learning rate schedules for ADVI and notes that pBaM is less sensitive to learning rate choices.
- Why unresolved: While the paper shows that pBaM is robust to different learning rates, a detailed comparison of learning rate schedules for pBaM is not provided.
- What evidence would resolve it: Experiments comparing pBaM's performance with different learning rate schedules on various datasets.

### Open Question 2
- Question: How does the performance of pBaM scale with increasing rank of the variational approximation compared to the rank of the target distribution?
- Basis in paper: [explicit] The paper shows that pBaM can fit targets with varying ranks, but the relationship between the rank of the variational approximation and the target is not fully explored.
- Why unresolved: The paper demonstrates that pBaM can handle different ranks, but the optimal rank for the variational approximation relative to the target is not determined.
- What evidence would resolve it: Experiments varying the rank of the variational approximation for different target ranks and measuring the resulting KL divergence.

### Open Question 3
- Question: What is the effect of the momentum factor (η) and tolerance (ε) on the convergence speed and stability of the EM updates in the patch step?
- Basis in paper: [explicit] The paper mentions that the algorithm is robust to a range of η and ε values but does not provide a detailed analysis of their impact.
- Why unresolved: The paper states that pBaM is robust to different values of η and ε but does not explore how these parameters affect convergence speed and stability.
- What evidence would resolve it: Experiments varying η and ε and measuring their impact on the number of EM steps required for convergence and the stability of the algorithm.

### Open Question 4
- Question: How does pBaM perform on Bayesian neural networks compared to other structured variational families?
- Basis in paper: [inferred] The paper suggests that pBaM could be applied to Bayesian neural networks but does not provide experimental results.
- Why unresolved: The paper discusses the potential application of pBaM to Bayesian neural networks but does not include experimental validation.
- What evidence would resolve it: Experiments applying pBaM to Bayesian neural networks and comparing its performance to other structured variational families.

### Open Question 5
- Question: Can the patch step be adapted to other structured covariance matrices, such as sparse matrices, to further improve scalability?
- Basis in paper: [inferred] The paper mentions the possibility of using other structured covariances but does not explore this direction.
- Why unresolved: The paper introduces the patch step for low-rank plus diagonal covariances but does not investigate its applicability to other structured covariances.
- What evidence would resolve it: Experiments applying the patch step to other structured covariance matrices and evaluating the impact on scalability and performance.

## Limitations
- Limited empirical validation on truly massive-scale problems (D ≥ 10,000) where O(D) advantages would be most pronounced
- Comparison with ADVI baselines may not be fully fair due to undisclosed hyperparameter tuning strategies
- Patch step's EM convergence guarantees are empirically observed but not theoretically proven across all problem types

## Confidence
**High confidence** in the core algorithmic innovation - the combination of BaM updates with the LR+D projection patch is technically novel and the mathematical derivations appear sound.

**Medium confidence** in the scalability claims - the O(D) complexity analysis is convincing, but empirical validation is limited to moderate dimensions.

**Medium confidence** in the empirical superiority claims - pBaM shows consistent improvements over baselines, but the experimental setup and hyperparameter choices for baselines lack full transparency.

## Next Checks
1. **Scaling test**: Implement pBaM on synthetic problems with D = 10,000 to 100,000 and verify that memory usage and runtime scale linearly as predicted by the O(D) analysis.

2. **Patch robustness**: Compare the EM-based patch with alternative LR+D projection methods (e.g., truncated SVD) on the same problems to quantify the optimality gap and determine if the patch step is the performance bottleneck.

3. **ADVI fairness test**: Re-run the benchmark experiments with ADVI using the same learning rate grid and early stopping criteria as pBaM to ensure the comparison is not biased by different hyperparameter tuning strategies.