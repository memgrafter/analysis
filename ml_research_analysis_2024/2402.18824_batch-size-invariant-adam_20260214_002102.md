---
ver: rpa2
title: Batch size invariant Adam
arxiv_id: '2402.18824'
source_url: https://arxiv.org/abs/2402.18824
tags:
- adam
- batch
- size
- invariant
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a batch size invariant version of the Adam
  optimizer for large-scale, distributed settings. The key idea is to modify Adam's
  updates so that it first squares the micro-batch gradients, then averages across
  micro-batches, rather than the standard approach of averaging micro-batch gradients
  and then squaring.
---

# Batch size invariant Adam

## Quick Facts
- arXiv ID: 2402.18824
- Source URL: https://arxiv.org/abs/2402.18824
- Authors: Xi Wang; Laurence Aitchison
- Reference count: 23
- Primary result: Batch size invariant Adam eliminates batch size dependence in Adam's effective learning rate through a modified update rule that squares micro-batch gradients before averaging.

## Executive Summary
This paper proposes a batch size invariant version of the Adam optimizer designed for large-scale distributed training settings. The key innovation involves modifying Adam's computation of the v term by squaring micro-batch gradients before averaging them, rather than averaging first and then squaring. This change eliminates the batch size dependence that arises from gradient variance in standard Adam, without requiring strong assumptions about gradient statistics. The authors prove batch size invariance under the mild condition of sufficiently small updates and demonstrate through experiments on ResNet and ViT architectures that their method achieves consistent optimization results across different batch sizes.

## Method Summary
The method modifies standard Adam by changing the order of operations in computing the second moment estimate. Instead of computing v as the exponential moving average of squared average gradients, it computes v as the exponential moving average of the average of squared micro-batch gradients. This eliminates the batch size-dependent variance term that appears in standard Adam's v computation. The method maintains the same first moment estimate m as standard Adam, computed as the exponential moving average of average gradients. The weight update rule remains similar to standard Adam but with the modified v term, using a linear learning rate scaling with batch size rather than square-root scaling.

## Key Results
- Batch size invariant Adam shows almost exactly equivalent optimization results under different batch sizes (25-800) on ResNet-18 and ViT architectures
- Standard Adam with square-root learning rate scaling shows more pronounced differences in optimization trajectories across batch sizes
- The method achieves batch size invariance without requiring the strong assumption that gradient variance dominates squared gradient mean
- Theoretical proof demonstrates invariance under sufficiently small learning rates without requiring stochastic differential equation analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Batch size invariant Adam eliminates batch size dependence by averaging squared micro-batch gradients instead of squaring averaged micro-batch gradients.
- Mechanism: Standard Adam computes v by averaging micro-batch gradients and then squaring, which introduces batch size dependence through gradient variance. Batch size invariant Adam squares each micro-batch gradient first, then averages, which estimates the expected raw second moment without batch size dependence.
- Core assumption: The effective learning rate depends on v, and v can be computed in a batch size invariant way by reversing the order of averaging and squaring.
- Evidence anchors:
  - [abstract] "For the v term, standard Adam first computes the average over micro-batch gradients, then squares, while in the batch size invariant Adam proposed here, we first square the micro-batch gradients, then average."
  - [section] "In contrast, for standard Adam (Alg. 2), ˆvt is an exponential moving average of the raw second moment of the minibatch gradients E[(1/κ ∑κ k=1gt−k+κ(wt−κ))2] = E[g]2 + 1/κ Var[g]."

### Mechanism 2
- Claim: The proposed method achieves batch size invariance under weaker assumptions than previous approaches.
- Mechanism: Previous work required the gradient variance to dominate the square of the expected gradient. This approach only requires sufficiently small updates, which is a weaker assumption.
- Core assumption: Small updates are sufficient for batch size invariance, regardless of the relative magnitude of gradient variance and squared gradient mean.
- Evidence anchors:
  - [abstract] "Previous work (e.g. Malladi et al. 2022) used an alternative approach that involved a square-root scaling of the learning rate, but this approach requires strong assumptions to work; in particular that the gradient variance dominates the square of the expected gradient."
  - [section] "Previous work suggested correcting for these effects by tweaking the learning rate, η (Granziol et al., 2022; Malladi et al., 2022; Hilton et al., 2022). In particular, they proposed a square-root scaling (i.e. η∝√B)."

### Mechanism 3
- Claim: The approach may be more efficient in practice in large-scale distributed settings.
- Mechanism: Standard Adam requires accumulating four quantities per parameter (g′, m, v, w), while the proposed method only requires three (m, v, w) by immediately incorporating micro-batch gradients into m and v.
- Core assumption: Memory savings at the server that aggregates gradient updates can be achieved by eliminating the need for a gradient accumulator.
- Evidence anchors:
  - [section] "While Zhang et al. (2023) did not consider the multi-node setting, the key insight holds true in multi-node settings for the server that aggregates gradient updates. In particular, this server again does not need to retain a gradient accumulator, but can just accumulate directly into mt and vt."

## Foundational Learning

- Concept: Exponential moving average (EMA)
  - Why needed here: Adam uses EMA to estimate the first and second moments of gradients, which is crucial for the batch size invariant formulation.
  - Quick check question: What is the formula for updating an exponential moving average of a quantity x with parameter β?

- Concept: Gradient variance and its relationship to batch size
  - Why needed here: Understanding how gradient variance scales with batch size is essential for understanding why standard Adam has batch size dependence and how the proposed method eliminates it.
  - Quick check question: If the variance of a single sample gradient is σ², what is the variance of the average gradient over a batch of size B?

- Concept: Stochastic differential equations (SDEs) in optimization
  - Why needed here: Previous work used SDEs to analyze batch size invariance, so understanding their approach provides context for why this work takes a different path.
  - Quick check question: In the context of optimization, what does the noise term in a stochastic differential equation typically represent?

## Architecture Onboarding

- Component map:
  Worker nodes -> Server -> Optimizer

- Critical path:
  1. Worker nodes compute gradients for micro-batches
  2. Worker nodes compute squared gradients for micro-batches
  3. Worker nodes send gradients and squared gradients to server
  4. Server aggregates gradients into m and squared gradients into v
  5. Server updates weights using batch size invariant Adam rule

- Design tradeoffs:
  - Memory vs. computation: Computing squared gradients increases computation but reduces memory requirements
  - Communication overhead: Sending squared gradients increases communication but may reduce overall communication if it enables larger batch sizes
  - Convergence vs. invariance: Very small learning rates may be needed for perfect invariance, potentially slowing convergence

- Failure signatures:
  - Batch size dependence persists: Check if squared gradients are being computed correctly at worker nodes
  - Training becomes unstable: Verify that learning rate scaling is correct for the chosen batch size
  - Memory usage doesn't decrease: Confirm that gradient accumulation is being eliminated at the server

- First 3 experiments:
  1. Verify batch size invariance: Train the same model with different batch sizes using the proposed method and compare convergence curves
  2. Compare memory usage: Measure memory consumption of the proposed method vs. standard Adam in a distributed setting
  3. Test learning rate scaling: Train with various learning rates to find the threshold where invariance breaks down

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions do batch size invariant Adam and standard Adam with square-root scaling show equivalent performance, and how does this depend on the ratio of gradient variance to gradient mean?
- Basis in paper: [explicit] The paper states that standard Adam with square-root scaling requires the gradient variance to dominate the expectation, which may not hold near initialization or for very large batch sizes.
- Why unresolved: The paper does not provide a quantitative threshold or analytical framework for when this condition is met.
- What evidence would resolve it: Empirical studies comparing the two methods across different network architectures, datasets, and optimization stages, coupled with analysis of gradient statistics.

### Open Question 2
- Question: How does the batch size invariant Adam's use of squared micro-batch gradients affect memory consumption and computational efficiency in large-scale distributed settings compared to standard Adam?
- Basis in paper: [explicit] The paper mentions that batch size invariant Adam may reduce memory consumption by avoiding explicit accumulation of average gradients, potentially saving 25% memory at the server.
- Why unresolved: The paper does not provide empirical measurements of memory usage or computational overhead for the proposed method.
- What evidence would resolve it: Experiments measuring memory usage and computational time for both methods under various batch sizes and model architectures.

### Open Question 3
- Question: How does the batch size invariant Adam's performance change when applied to optimization problems beyond image classification, such as natural language processing or reinforcement learning tasks?
- Basis in paper: [explicit] The paper only tests the method on ResNet and ViT architectures for image classification tasks.
- Why unresolved: The paper does not explore the method's effectiveness on other domains or tasks.
- What evidence would resolve it: Applying the method to a diverse set of tasks and comparing its performance to standard Adam and other optimizers.

## Limitations
- Theoretical proof only holds for small learning rates, which may not be optimal for training
- Limited experimental validation on a narrow range of architectures and datasets
- No comparison against other batch size invariant methods in distributed settings
- Potential computational overhead from computing squared micro-batch gradients on workers

## Confidence

**Confidence Labels:**
- **High**: The mathematical formulation and proof of batch size invariance under small learning rates
- **Medium**: Experimental results showing invariance across batch sizes for the tested architectures
- **Medium**: Claim about potential memory efficiency gains in distributed settings

## Next Checks
1. Test the proposed method on larger, more complex architectures (e.g., ResNet-50, BERT) to verify generalizability
2. Conduct experiments with varying learning rates to identify the threshold where invariance breaks down
3. Measure actual memory usage and communication overhead in a multi-node distributed setup to validate efficiency claims