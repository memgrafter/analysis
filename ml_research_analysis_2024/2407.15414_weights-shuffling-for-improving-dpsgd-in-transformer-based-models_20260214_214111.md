---
ver: rpa2
title: Weights Shuffling for Improving DPSGD in Transformer-based Models
arxiv_id: '2407.15414'
source_url: https://arxiv.org/abs/2407.15414
tags:
- privacy
- dpsgd
- shuffling
- shuffled
- gaussian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel shuffling mechanism for Differentially-Private
  Stochastic Gradient Descent (DPSGD) to improve the utility of large models at the
  same privacy guarantee. The key idea is to leverage the permutation invariance property
  of MLP and Transformer encoder blocks, randomly shuffling model weights without
  affecting accuracy.
---

# Weights Shuffling for Improving DPSGD in Transformer-based Models

## Quick Facts
- arXiv ID: 2407.15414
- Source URL: https://arxiv.org/abs/2407.15414
- Reference count: 40
- One-line primary result: Shuffling model weights in permutation-invariant layers improves DPSGD privacy-utility tradeoff without accuracy loss

## Executive Summary
This paper introduces a novel shuffling mechanism for Differentially-Private Stochastic Gradient Descent (DPSGD) that leverages the permutation invariance property of MLP and Transformer encoder blocks. By randomly shuffling model weights during training, the mechanism introduces additional randomness that enhances privacy while maintaining accuracy. The authors develop an analytical framework using sum of lognormal distribution approximations to derive privacy guarantees for the shuffled mechanism. Experimental results demonstrate significant improvements in accuracy over state-of-the-art baselines across various models and tasks, particularly at tight privacy budgets.

## Method Summary
The core innovation involves randomly shuffling weights in MLP and Transformer encoder blocks during DPSGD training. The shuffling operation exploits permutation invariance, where the forward and backward computations remain unchanged despite weight permutations. This additional randomness enhances privacy by obscuring training trajectories. The privacy accounting uses approximations of sum of lognormal distributions to derive conditions for (ε, δ)-DP guarantees. The mechanism integrates seamlessly into existing DPSGD frameworks with minimal overhead.

## Key Results
- Improves accuracy over state-of-the-art baselines (Ghost clipping and MixOpt) on CIFAR-100, GLUE tasks, and text generation datasets
- Achieves better privacy-utility tradeoff, especially at tight privacy budgets (ε < 10)
- Maintains accuracy while providing enhanced privacy guarantees through additional randomness
- Incurs minimal computational overhead and is readily deployable in current DPSGD frameworks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random weight shuffling in MLP and Transformer layers improves privacy without harming accuracy due to permutation invariance
- Mechanism: Random permutation of weights in MLP and Transformer encoder blocks, with inverse permutations applied in subsequent layers, maintains unchanged computation while introducing additional randomness
- Core assumption: Permutation invariance holds for both forward and backward propagation in MLP and Transformer encoder blocks
- Evidence anchors: [abstract] reveals shuffling brings additional randomness while maintaining accuracy through permutation invariance; [section] Theorem 3 and 4 prove permutation invariance for MLP and Transformer blocks

### Mechanism 2
- Claim: Shuffling creates mixture distributions that are harder to distinguish than single distributions, improving privacy
- Mechanism: Shuffling transforms single Gaussian distributions into mixtures of Gaussians, reducing the distance between distributions and making them harder to distinguish
- Core assumption: Mixture of Gaussians created by shuffling provides better privacy than original Gaussian mechanism
- Evidence anchors: [abstract] mentions taking advantage of high dimensionality; [section] discusses how shuffling increases outcome ambiguity, enhancing privacy strength

### Mechanism 3
- Claim: Shuffled Gaussian mechanism requires less noise for the same privacy guarantee compared to unshuffled Gaussian
- Mechanism: Noise variance in shuffled Gaussian is approximately O(1/log d), meaning higher dimensional models require less noise for the same privacy level
- Core assumption: Approximation of sum of log-normal distributions is accurate enough for practical privacy accounting
- Evidence anchors: [abstract] mentions exploiting approximation on sum of lognormal distributions; [section] states noise variance σ2 is approximately O(1/log d)

## Foundational Learning

- Concept: Differential Privacy and (ε, δ)-DP definition
  - Why needed here: The entire paper builds on differential privacy framework to quantify privacy guarantees
  - Quick check question: What is the difference between ε-DP and (ε, δ)-DP?

- Concept: Sum of log-normal distributions and their approximation
  - Why needed here: Privacy accounting for shuffled Gaussian relies on approximating sum of log-normal distributions
  - Quick check question: Why can't we compute the exact distribution of sum of log-normal distributions?

- Concept: Permutation invariance in neural networks
  - Why needed here: Shuffling mechanism relies on certain layers being permutation invariant
  - Quick check question: Which neural network layers are permutation invariant and why?

## Architecture Onboarding

- Component map:
  DPSGD framework (gradient clipping -> noise addition -> weight update) with shuffling mechanism inserted after noise addition but before weight update

- Critical path:
  1. Sample batch and compute per-sample gradients
  2. Clip gradients to bound sensitivity
  3. Add Gaussian noise to clipped gradients
  4. Shuffle weights in permutation-invariant layers
  5. Update weights
  6. Track privacy budget using advanced composition

- Design tradeoffs:
  - Shuffling vs. no shuffling: improved privacy vs. computational overhead
  - Approximation accuracy vs. privacy guarantee tightness
  - Permutation space size vs. privacy improvement magnitude

- Failure signatures:
  - Accuracy degradation: indicates permutation invariance doesn't hold for some layers
  - Privacy guarantee violation: indicates approximation error is too large
  - High computational overhead: indicates inefficient shuffling implementation

- First 3 experiments:
  1. Verify permutation invariance by training with and without shuffling and comparing results
  2. Measure approximation error of sum of log-normal distribution using different methods
  3. Compare privacy-utility tradeoff of shuffled vs. unshuffled DPSGD on a simple model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact analytical form of the privacy guarantee for shuffled Gaussian mechanisms with arbitrary weight permutation spaces?
- Basis in paper: [explicit] Authors derive approximate condition for (ε, δ)-DP in Theorem 6 but acknowledge it's based on approximations
- Why unresolved: Exact privacy guarantee involves analyzing distribution of outputs under all possible permutations, which is computationally intractable for large models
- What evidence would resolve it: Rigorous proof of exact privacy guarantee without approximations, or demonstration that approximation error is negligible

### Open Question 2
- Question: How does the shuffled DPSGD mechanism perform under different privacy accountant methods beyond advanced composition?
- Basis in paper: [inferred] Authors mention moments accountant, RDP accountant, PLDs, and PRVs accountant are not suitable due to incompatibility with mixture of Gaussian distributions
- Why unresolved: Authors choose advanced composition without exploring if other methods could provide tighter privacy guarantees
- What evidence would resolve it: Experimental comparison of shuffled DPSGD performance under different privacy accountant methods

### Open Question 3
- Question: What is the impact of the shuffling mechanism on convergence rate and generalization performance beyond privacy guarantees?
- Basis in paper: [inferred] Authors focus on privacy benefits and accuracy at same privacy level but don't explore optimization process or generalization
- Why unresolved: While shuffling improves privacy without hurting accuracy, its effect on training dynamics and test performance is not investigated
- What evidence would resolve it: Empirical study comparing convergence rate, final accuracy, and generalization performance of models with and without shuffling

## Limitations
- Privacy accounting relies heavily on sum of lognormal distribution approximations, introducing potential unquantified error
- Experimental evaluation focuses primarily on classification and language tasks, with limited testing on other domains
- Scalability claims for extremely large models haven't been thoroughly validated

## Confidence

- **High Confidence**: Basic shuffling mechanism works as described for standard MLP and Transformer encoder blocks with fixed normalization parameters
- **Medium Confidence**: Privacy improvement claims are supported by theoretical analysis, though approximation errors could be larger than anticipated
- **Low Confidence**: Scalability claims for extremely large models haven't been thoroughly validated, and computational overhead at scale is uncertain

## Next Checks

1. **Approximation Error Validation**: Systematically measure actual privacy loss vs. theoretical guarantee across different dimensionality settings to quantify approximation error in sum of lognormal distribution calculations

2. **Cross-Architecture Testing**: Test shuffling mechanism on architectures with learnable normalization parameters and other non-permutation-invariant components to identify boundaries of applicability

3. **Large-Scale Performance**: Evaluate mechanism on models with over 1 billion parameters to verify claimed computational efficiency and privacy benefits at scale, measuring both wall-clock time and memory overhead