---
ver: rpa2
title: Hierarchical Windowed Graph Attention Network and a Large Scale Dataset for
  Isolated Indian Sign Language Recognition
arxiv_id: '2407.14224'
source_url: https://arxiv.org/abs/2407.14224
tags:
- sign
- dataset
- language
- recognition
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a large-scale isolated Indian Sign Language
  (ISL) dataset containing 40,033 videos of 2,002 daily used words, recorded by 20
  native ISL signers. The authors also propose a novel Hierarchical Windowed Graph
  Attention Network (HWGAT) for sign language recognition using skeleton graph structure.
---

# Hierarchical Windowed Graph Attention Network and a Large Scale Dataset for Isolated Indian Sign Language Recognition

## Quick Facts
- arXiv ID: 2407.14224
- Source URL: https://arxiv.org/abs/2407.14224
- Reference count: 40
- This paper introduces a large-scale ISL dataset and a novel HWGAT model showing improvements of 1.10-6.84 percentage points over state-of-the-art skeleton-based models.

## Executive Summary
This paper presents a comprehensive approach to Indian Sign Language (ISL) recognition, introducing both a large-scale dataset and a novel deep learning model. The FDMSE-ISL dataset contains 40,033 videos of 2,002 daily used words recorded by 20 native ISL signers. The proposed Hierarchical Windowed Graph Attention Network (HWGAT) leverages skeleton graph structure with spatial windowing and hierarchical temporal processing to achieve state-of-the-art performance across multiple sign language datasets.

## Method Summary
The approach uses MediaPipe holistic pose estimation to extract 27 keypoints per frame from sign language videos. These keypoints form skeleton graphs that are processed through a novel HWGAT architecture. The model divides the skeleton into spatial windows based on body parts, applies graph attention within temporal blocks, and uses hierarchical temporal merging with shifting to capture both local and global motion patterns. The model is trained with AdamW optimizer, label smoothed cross entropy loss, and attention dropout regularization.

## Key Results
- FDMSE-ISL dataset: 40,033 videos of 2,002 daily used ISL words by 20 native signers
- HWGAT achieves 1.10, 0.46, 0.78, and 6.84 percentage point improvements on INCLUDE, LSA64, AUTSL, and WLASL datasets respectively
- Ablation studies show temporal block length of 2 and 5 spatial windows provide optimal performance
- Attention dropout mechanism improves generalization by reducing over-reliance on similar adjacent nodes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spatial windowing improves recognition by reducing irrelevant motion influence from non-signing body parts
- Mechanism: The model divides the skeleton graph into spatial windows based on body parts, restricting attention to relevant parts during classification
- Core assumption: Attention to irrelevant body parts introduces noise that harms recognition accuracy
- Evidence anchors:
  - [abstract] "The HWGAT tries to capture distinctive motions by giving attention to different body parts induced by the human skeleton graph structure."
  - [section 4.1] "To mitigate these issues we divide the whole graph into multiple spatial windows by stacking the part keypoints combinations. This spatial division restricts the flow of information from one window to another."
- Break condition: If all signs require full-body coordination, windowing could remove essential context

### Mechanism 2
- Claim: Temporal block merging with shifting enables both local and global motion context capture
- Mechanism: Frames are grouped into temporal blocks, attention is applied within blocks, then blocks are merged hierarchically with temporal shifting for cross-block information flow
- Core assumption: Local motion patterns differ from global motion patterns in sign language
- Evidence anchors:
  - [section 4.2.2] "The attention mechanism in the first Part Attention Layer captures the local spatio-temporal features. To obtain global context, we use a hierarchical temporal merging technique."
  - [section 4.2.1] "To address this issue, we have used a shifting mechanism inspired by [27] that shifts the temporal window through the input sequence resulting in the propagation of information between consecutive temporal blocks."
- Break condition: If signs are primarily static or have very long durations, temporal block size becomes critical

### Mechanism 3
- Claim: Attention dropout regularizes the model by reducing over-reliance on highly similar adjacent nodes
- Mechanism: During training, attention weights above a random threshold are set to zero, preventing dominance of similar nodes
- Core assumption: High attention weights between similar nodes cause loss of important information from other nodes
- Evidence anchors:
  - [section 4.2.1] "The high attention values among similar nodes can sometimes reduce the attention weights of other nodes, leading to a loss of important information. To overcome this issue, we propose an attention dropout mechanism inspired by [53]."
  - [section 4.2.1] "First a random variable γ is uniformly sampled from(0, 1). Then the attention matrix GAttn is masked with value 0 where GAttn is greater than γ."
- Break condition: If signs have very distinct body part movements, this regularization might hurt performance

## Foundational Learning

- Concept: Skeleton graph representation of human pose
  - Why needed here: The model operates on skeleton graphs rather than raw video, requiring understanding of how human body parts form graph structures
  - Quick check question: What are the key differences between a skeleton graph and a regular image for deep learning?

- Concept: Graph attention mechanisms
  - Why needed here: The model uses graph attention instead of standard GCNs, requiring understanding of attention-based information propagation
  - Quick check question: How does graph attention differ from standard graph convolution in terms of weight calculation?

- Concept: Temporal context in sequential data
  - Why needed here: Sign language recognition requires understanding motion over time, requiring knowledge of how to model temporal dependencies
  - Quick check question: What are the trade-offs between modeling temporal context at different granularities (frame-level vs block-level)?

## Architecture Onboarding

- Component map: Pose estimation -> 27 keypoint extraction -> coordinate normalization -> spatial windowing -> temporal blocks -> graph attention -> attention dropout -> hierarchical merging -> classification

- Critical path: Pose estimation → spatial windowing → temporal block processing → graph attention → hierarchical merging → classification

- Design tradeoffs:
  - Spatial windows vs full graph: Better accuracy vs potential loss of cross-body context
  - Temporal block size: Captures local patterns vs computational efficiency
  - Attention dropout rate: Regularization vs potential information loss

- Failure signatures:
  - Poor pose estimation → broken skeleton graph → model failure
  - Inappropriate spatial windowing → missing critical motion context
  - Wrong temporal block size → failure to capture motion patterns
  - High attention dropout → underfitting

- First 3 experiments:
  1. Test with different numbers of spatial windows (1, 2, 4) on a small subset to verify windowing benefit
  2. Test temporal block lengths (2, 4, 8) to find optimal temporal granularity
  3. Test with and without attention dropout to verify regularization effect

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Dataset validation concerns: Limited validation on diversity and representativeness with only 20 signers
- Generalizability questions: No cross-signer validation studies reported
- Architectural uncertainty: Lack of detailed ablation studies on attention dropout mechanism

## Confidence
- Dataset contribution: Medium - Large scale but limited validation on diversity and representativeness
- Model architecture claims: Medium - Novel approach with reported improvements but lacking detailed ablation studies
- Performance improvements: Medium - State-of-the-art results but no direct architectural ablation studies

## Next Checks
1. Conduct signer-independent validation: Train on 15 signers and test on 5 unseen signers to assess model generalization across different signers
2. Perform ablation study on attention dropout: Compare HWGAT with standard dropout versus proposed attention dropout mechanism
3. Test spatial window configuration: Experiment with different numbers of spatial windows (1, 2, 4) and document the impact on recognition accuracy to verify the windowing mechanism's effectiveness