---
ver: rpa2
title: 'Juru: Legal Brazilian Large Language Model from Reputable Sources'
arxiv_id: '2403.18140'
source_url: https://arxiv.org/abs/2403.18140
tags:
- legal
- knowledge
- juru
- domain
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Juru, a Brazilian legal language model developed
  by continuing pretraining Mistral-7B on 1.9 billion tokens from reputable Brazilian
  legal sources. The authors aimed to demonstrate that domain specialization with
  high-quality data can improve LLM performance while reducing computational costs.
---

# Juru: Legal Brazilian Large Language Model from Reputable Sources

## Quick Facts
- arXiv ID: 2403.18140
- Source URL: https://arxiv.org/abs/2403.18140
- Authors: Roseval Malaquias Junior; Ramon Pires; Roseli Romero; Rodrigo Nogueira
- Reference count: 40
- Primary result: Juru achieves 4.7% accuracy improvement on Brazilian legal benchmarks while showing 2.4-3.6% degradation on general knowledge tasks

## Executive Summary
This paper presents Juru, a Brazilian legal language model developed by continuing pretraining Mistral-7B on 1.9 billion tokens from reputable Brazilian legal sources. The authors demonstrate that domain specialization with high-quality data can improve LLM performance in legal domains while reducing computational costs. Juru shows improved performance on legal benchmarks but exhibits performance degradation on general knowledge tasks, highlighting the trade-off between domain specialization and general knowledge retention. The findings suggest that continued pretraining can be an effective approach for domain adaptation when high-quality specialized data is available.

## Method Summary
Juru is developed by continuing pretraining Mistral-7B on 1.9 billion tokens from Brazilian legal sources including academic papers, federal laws, and Supreme Court decisions. The model uses the Mistral-7B tokenizer and is trained using t5x/seqio frameworks with AdaFactor optimizer, dynamic weight decay, and auxiliary loss. Training involves 3,800 steps with learning rate warmup of 250 steps to 0.001. The model is evaluated on legal benchmarks (OAB, ENAM exams) and general knowledge benchmarks (MMLU, ENEM) using few-shot learning with 3 examples. The methodology focuses on achieving domain specialization while minimizing catastrophic forgetting of general knowledge.

## Key Results
- Juru achieves 4.7% accuracy improvement on Brazilian legal knowledge benchmarks compared to base Mistral-7B
- Performance degradation observed on general knowledge tasks: 3.6% drop on English benchmarks and 2.4% drop on Portuguese benchmarks
- Greater forgetting observed on English general knowledge compared to Portuguese, suggesting domain similarity affects forgetting rates
- Continued pretraining with 1.9B specialized tokens achieves domain gains without excessive computational costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continued pretraining on domain-specific high-quality data can improve model performance in that domain while requiring fewer total tokens than full pretraining.
- Mechanism: The model leverages its already-learned general language capabilities and applies them to specialized content, achieving faster domain adaptation through targeted exposure rather than broad foundational learning.
- Core assumption: General pretraining provides sufficient foundational knowledge that can be efficiently specialized with focused domain data.
- Evidence anchors:
  - [abstract] "Juru demonstrates the benefits of domain specialization by achieving improved performance on legal benchmarks, even with a reduced amount of pretraining data"
  - [section 3.1] "Pretraining dataset, using the Mistral-7B tokenizer. We gathered 1.9 billion tokens"
  - [corpus] Weak - only general LLM terms, no specific domain evidence
- Break condition: If the base model lacks sufficient general knowledge or if the domain requires completely different reasoning patterns than those learned during general pretraining.

### Mechanism 2
- Claim: Domain specialization through continued pretraining causes performance degradation on general knowledge tasks, with the extent of forgetting correlated to domain similarity.
- Mechanism: The model reallocates capacity from general knowledge to domain-specific patterns, with greater forgetting occurring when the specialized domain is less similar to previously learned domains.
- Core assumption: Neural network capacity is finite and specialization requires some degree of general knowledge displacement.
- Evidence anchors:
  - [abstract] "this domain specialization through continued pretraining comes at the cost of increased forgetting in unrelated domains"
  - [section 5.2] "performance on general knowledge test suites decreased relative to the base model... a more pronounced decrease was observed on the English test suite compared to the Portuguese one"
  - [corpus] Weak - general LLM specialization patterns, no specific forgetting evidence
- Break condition: If the model has excess capacity or if the specialized domain is highly similar to the general knowledge domain.

### Mechanism 3
- Claim: High-quality curated data from reputable sources enables effective domain specialization with fewer training steps than using large-scale general web data.
- Mechanism: Quality data reduces noise and redundancy, allowing the model to learn domain patterns more efficiently per token processed.
- Core assumption: Curated data provides denser information content than randomly scraped web data.
- Evidence anchors:
  - [abstract] "using 1.9 billion unique tokens from reputable Brazilian legal sources"
  - [section 3.1] "We conducted web scraping of academic papers in Portuguese within the Brazilian legal domain, prioritizing data with educational value"
  - [corpus] Moderate - mentions reputable sources but lacks specific quality metrics
- Break condition: If the quality filtering process removes too much relevant data or if the curation process introduces bias.

## Foundational Learning

- Concept: Transfer learning and catastrophic forgetting
  - Why needed here: Understanding how knowledge transfers between domains and why specialization causes forgetting is crucial for designing effective domain adaptation strategies
  - Quick check question: If a model achieves 95% accuracy on task A and 60% on task B before specialization, and after specialization achieves 80% on task A and 90% on task B, what phenomenon is occurring?

- Concept: Scaling laws and compute efficiency
  - Why needed here: The paper leverages scaling principles to justify why continued pretraining with less data can be effective
  - Quick check question: If full pretraining requires 10^24 FLOPs and continued pretraining requires 10^20 FLOPs for similar domain performance, what efficiency ratio is achieved?

- Concept: Domain similarity and knowledge retention
  - Why needed here: The paper shows different forgetting rates for Portuguese vs English general knowledge, highlighting the importance of domain relationships
  - Quick check question: If a model specialized in Brazilian law shows 5% forgetting on Portuguese general knowledge but 10% forgetting on English general knowledge, what does this suggest about the relationship between specialization domain and forgetting?

## Architecture Onboarding

- Component map:
  - Mistral-7B model -> t5x/seqio training framework -> AdaFactor optimizer -> Legal dataset (1.9B tokens) -> Benchmark evaluation suites

- Critical path:
  1. Data collection and curation (academic papers, federal laws, court decisions)
  2. Base model loading and tokenizer setup
  3. Training loop with AdaFactor optimization
  4. Checkpoint evaluation on legal and general benchmarks
  5. Performance analysis and model selection

- Design tradeoffs:
  - Data quality vs quantity: 1.9B high-quality tokens vs potentially more but noisier data
  - Training steps vs performance: 3,800 steps achieving good specialization without excessive forgetting
  - Language specificity: Portuguese legal specialization vs broader multilingual capabilities

- Failure signatures:
  - Poor legal performance: Indicates data quality issues or insufficient training
  - Excessive general knowledge forgetting: Suggests over-specialization or insufficient capacity
  - Training instability: May indicate learning rate issues or data pipeline problems

- First 3 experiments:
  1. Train on 10% of legal data for 380 steps to establish baseline specialization patterns
  2. Compare Portuguese vs English general knowledge forgetting at 50%, 100%, and 150% of training steps
  3. Test different data quality thresholds by filtering academic papers at 80%, 90%, and 95% quality scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance improvement of Juru on legal benchmarks generalize to other specialized domains beyond Brazilian law?
- Basis in paper: [inferred] The paper focuses specifically on Brazilian legal domain specialization and notes that results may not generalize to other subdomains or data types beyond reputable sources.
- Why unresolved: The study only evaluated one domain (Brazilian legal), limiting conclusions about cross-domain generalization.
- What evidence would resolve it: Experiments with domain specialization on other fields (e.g., medicine, engineering) using similar methodologies would reveal whether the observed patterns hold across different domains.

### Open Question 2
- Question: What is the optimal balance between domain specialization and general knowledge retention in language models?
- Basis in paper: [explicit] The paper demonstrates a trade-off between improved legal performance and decreased general knowledge across both Portuguese and English benchmarks.
- Why unresolved: The paper shows the trade-off exists but doesn't identify the optimal specialization point where domain gains outweigh general knowledge losses.
- What evidence would resolve it: Systematic experiments varying specialization data size and evaluating performance across multiple domain and general benchmarks would identify the sweet spot.

### Open Question 3
- Question: Can domain similarity metrics predict the extent of catastrophic forgetting during continued pretraining?
- Basis in paper: [explicit] The authors observed greater forgetting on English benchmarks compared to Portuguese, suggesting that less similar previously learned knowledge experiences higher forgetting rates.
- Why unresolved: While correlation between domain similarity and forgetting is suggested, the paper doesn't quantify this relationship or test it across diverse domain pairs.
- What evidence would resolve it: Experiments measuring domain similarity using embedding distance or other metrics, then correlating these with forgetting rates across multiple domain pairs, would validate this hypothesis.

## Limitations
- Lack of detailed validation of data quality filtering process, creating uncertainty about whether performance improvements are due to genuine domain specialization or reduced noise
- Focus on single domain (Brazilian legal) and single base model (Mistral-7B) limits generalizability to other domains or model architectures
- Specific criteria for determining data quality and effectiveness of filtering approach remain unspecified

## Confidence
- High Confidence: The observation that domain specialization improves performance on target domain tasks while causing degradation on general knowledge tasks
- Medium Confidence: The claim that high-quality curated data enables more efficient domain specialization
- Medium Confidence: The quantitative performance improvements (4.7% accuracy gain on legal tasks) are specific and reproducible

## Next Checks
1. **Data Quality Validation**: Conduct an ablation study comparing Juru's performance when trained on filtered data versus unfiltered data from the same sources, to isolate the effect of data quality from domain specialization.

2. **Domain Similarity Analysis**: Systematically test forgetting patterns across multiple domains with varying similarity to the specialization domain (e.g., Spanish legal, Brazilian business law, general Portuguese knowledge) to validate the hypothesis about forgetting being correlated with domain similarity.

3. **Capacity Efficiency Measurement**: Compare Juru's performance against a model trained from scratch on the same 1.9 billion legal tokens to quantify whether continued pretraining actually achieves compute efficiency, as claimed.