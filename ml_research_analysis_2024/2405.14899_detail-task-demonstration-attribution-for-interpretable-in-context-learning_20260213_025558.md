---
ver: rpa2
title: 'DETAIL: Task DEmonsTration Attribution for Interpretable In-context Learning'
arxiv_id: '2405.14899'
source_url: https://arxiv.org/abs/2405.14899
tags:
- demonstrations
- accuracy
- detail
- demonstration
- e-02
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of interpreting in-context learning
  (ICL) demonstrations in transformer-based language models. The core method, DETAIL,
  adapts influence functions to attribute demonstration importance by treating transformers
  as implementing internal optimizers during ICL.
---

# DETAIL: Task DEmonsTration Attribution for Interpretable In-context Learning

## Quick Facts
- arXiv ID: 2405.14899
- Source URL: https://arxiv.org/abs/2405.14899
- Authors: Zijian Zhou; Xiaoqiang Lin; Xinyi Xu; Alok Prakash; Daniela Rus; Bryan Kian Hsiang Low
- Reference count: 40
- One-line primary result: DETAIL achieves up to 17.9% accuracy improvement by attributing demonstration importance in in-context learning

## Executive Summary
This paper addresses the challenge of interpreting in-context learning (ICL) demonstrations in transformer-based language models by proposing DETAIL, a method that attributes demonstration importance through influence functions. DETAIL treats transformers as implementing internal optimizers during ICL and computes influence scores efficiently using kernelized ridge regression on internal representations. The approach handles ICL-specific challenges including ordering sensitivity, token-level attribution, and parameter-free learning. Experiments demonstrate DETAIL's effectiveness across multiple tasks and its ability to improve model performance through demonstration curation.

## Method Summary
DETAIL adapts influence functions to attribute demonstration importance in ICL by treating transformers as internal optimizers. The method computes influence scores using kernelized ridge regression on transformer hidden states, with optional random projection for computational efficiency. It extracts embeddings from the middle layer of the transformer, computes influence scores through a closed-form solution involving matrix inversion, and provides token-level attribution capabilities. The approach is designed to handle ICL's unique characteristics including ordering sensitivity and the absence of parameter updates during demonstration processing.

## Key Results
- DETAIL achieves up to 17.9% accuracy improvement through effective demonstration curation
- The method runs efficiently in 4-10 seconds per attribution using random projection
- DETAIL's influence scores are transferable across different model architectures, including black-box models like GPT-3.5
- The approach successfully handles ICL-specific challenges including ordering sensitivity and token-level attribution

## Why This Works (Mechanism)
DETAIL works by leveraging the mathematical framework of influence functions, which traditionally measure how much each training example influences model parameters. In ICL, where no parameter updates occur, DETAIL instead measures how much each demonstration influences the model's internal representations during inference. By treating the transformer as an implicit optimizer that "optimizes" its internal state to match demonstration patterns, DETAIL can compute attribution scores that identify helpful versus harmful demonstrations. The kernelized ridge regression formulation provides a computationally tractable way to approximate these influence scores while maintaining theoretical guarantees about their accuracy.

## Foundational Learning
- **Influence Functions**: A classical statistical technique for measuring how much each training example influences model parameters or predictions. Needed to quantify demonstration importance in ICL; check by verifying the matrix inversion step in the influence computation.
- **Kernelized Ridge Regression**: A machine learning method that combines kernel methods with regularization to solve regression problems. Needed to efficiently approximate influence scores; check by validating the closed-form solution against numerical optimization.
- **Transformer Internal Representations**: The hidden states and embeddings produced at different layers of a transformer model. Needed as the basis for computing demonstration influence; check by examining layer-wise performance differences.
- **Random Projection**: A dimensionality reduction technique that preserves distances between points while reducing computational complexity. Needed to make influence computation tractable for large models; check by varying projection dimensions and measuring accuracy trade-offs.
- **In-Context Learning (ICL)**: The ability of language models to learn from demonstration examples provided in the prompt without parameter updates. Needed as the target application domain; check by verifying that the method handles demonstration ordering and formatting correctly.

## Architecture Onboarding

**Component Map**: Transformer model -> Middle layer extraction -> Embedding projection -> Kernelized ridge regression -> Influence score computation

**Critical Path**: The most critical path is the extraction of meaningful embeddings from the middle transformer layer, as this directly impacts the quality of influence score computation. Poor layer selection or embedding extraction can cascade through the entire attribution pipeline.

**Design Tradeoffs**: The primary tradeoff is between computational efficiency and attribution accuracy. Using random projection (d' â‰ˆ 1000) provides 4-10 second computation time but may lose some fine-grained attribution information. Full-dimensional computation would be more accurate but computationally prohibitive for large models.

**Failure Signatures**: Common failure modes include poor lambda selection leading to over/under-regularization, incorrect layer selection resulting in meaningless embeddings, and dimension mismatch in the projection step. These typically manifest as random or counterintuitive attribution scores.

**3 First Experiments**:
1. Verify layer selection by computing attribution scores using different transformer layers (1-12) and measuring consistency
2. Test random projection sensitivity by varying d' from 100 to 5000 and measuring attribution accuracy
3. Validate transferability by computing scores on a white-box model and testing prediction on a black-box model

## Open Questions the Paper Calls Out
- How does DETAIL's performance scale when applied to demonstration sets larger than 20-30 examples, and what is the upper bound on demonstration size where the method remains effective?
- What is the relationship between projection dimension d' and attribution accuracy for different types of downstream tasks, and is there an optimal d' that generalizes across task types?
- How does DETAIL's attribution quality compare to human judgment when evaluating which demonstrations are truly helpful for specific queries?

## Limitations
- The method's effectiveness depends heavily on proper transformer layer selection, which may vary across different model architectures and training objectives
- Computational efficiency claims rely on random projection, which may sacrifice attribution accuracy for speed
- Transferability results are limited to specific model pairs and may not extend to more divergent architectures or larger capability gaps

## Confidence
- **High Confidence**: The core mathematical formulation using kernelized ridge regression is well-specified and reproducible
- **Medium Confidence**: Computational efficiency claims and transferability results are supported but may vary with different hardware and architectures
- **Low Confidence**: Generalizability across transformer variants and optimal layer selection process lacks comprehensive validation

## Next Checks
1. Systematically test DETAIL's performance across different transformer layers for multiple model architectures to establish robustness
2. Evaluate transferability when moving between fundamentally different transformer designs (encoder-only, decoder-only, encoder-decoder)
3. Conduct controlled experiments varying random projection dimensions to quantify the relationship between computational efficiency and attribution accuracy