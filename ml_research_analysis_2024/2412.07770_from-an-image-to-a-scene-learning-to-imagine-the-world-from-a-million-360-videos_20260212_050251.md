---
ver: rpa2
title: 'From an Image to a Scene: Learning to Imagine the World from a Million 360
  Videos'
arxiv_id: '2412.07770'
source_url: https://arxiv.org/abs/2412.07770
tags:
- view
- video
- videos
- scenes
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of learning 3D scene understanding\
  \ from real-world videos, which has been limited by the lack of large-scale multi-view\
  \ datasets. To overcome this, the authors propose using 360\xB0 videos from YouTube\
  \ as a scalable source for diverse, corresponding views of real-world scenes."
---

# From an Image to a Scene: Learning to Imagine the World from a Million 360 Videos

## Quick Facts
- arXiv ID: 2412.07770
- Source URL: https://arxiv.org/abs/2412.07770
- Authors: Matthew Wallingford; Anand Bhattad; Aditya Kusupati; Vivek Ramanujan; Matt Deitke; Sham Kakade; Aniruddha Kembhavi; Roozbeh Mottaghi; Wei-Chiu Ma; Ali Farhadi
- Reference count: 40
- Key outcome: Learned to generate novel views of real-world scenes from a single input image using the largest real-world multi-view dataset to date.

## Executive Summary
This paper addresses the challenge of learning 3D scene understanding from real-world videos, which has been limited by the lack of large-scale multi-view datasets. To overcome this, the authors propose using 360° videos from YouTube as a scalable source for diverse, corresponding views of real-world scenes. They introduce a method to efficiently extract multi-view data from 360° videos by leveraging 360° videos' ability to provide overlapping views from different frames, combined with structure-from-motion techniques to estimate camera poses and refine correspondences. This process enables the creation of the largest real-world multi-view dataset to date, 360-1M, containing over 1 million 360° videos and 80 million unique frames with estimated camera poses. The authors train a diffusion-based model, ODIN, on this dataset to generate novel views of real-world scenes from a single input image, with free camera movement throughout the scene. ODIN demonstrates improved performance on standard novel view synthesis benchmarks (DTU and MipNeRF360) and 3D reconstruction tasks (Google Scanned Objects and 360-1M) compared to existing methods, without fine-tuning on the target data. The model can also reconstruct the 3D geometry of scenes from generated views.

## Method Summary
The authors propose a method to extract multi-view data from 360° videos by leveraging the equirectangular projection of 360° frames, which allows rotation of virtual viewpoints within each frame to align overlapping scene regions. They use Dust3R to estimate relative camera poses from aligned pairs and refine correspondences through gradient descent on pitch and yaw. This process enables the creation of the 360-1M dataset, containing over 1 million 360° videos and 80 million unique frames with estimated camera poses. They train a diffusion-based model, ODIN, on this dataset to generate novel views of real-world scenes from a single input image, with free camera movement throughout the scene. ODIN uses a latent diffusion U-Net conditioned on camera pose and includes a motion masking mechanism to handle dynamic objects.

## Key Results
- ODIN trained on 360-1M achieves improved performance on standard novel view synthesis benchmarks (DTU and MipNeRF360) compared to existing methods, without fine-tuning on the target data.
- The model can reconstruct the 3D geometry of scenes from generated views, as demonstrated on Google Scanned Objects and held-out 360-1M scenes.
- The motion masking mechanism enables training on dynamic scenes by filtering out moving objects from the loss.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: 360° videos provide diverse, overlapping views that enable scalable multi-view correspondence discovery.
- **Mechanism**: The equirectangular projection of 360° frames allows rotation of virtual viewpoints within each frame to align overlapping scene regions, bypassing the fixed-viewpoint limitation of standard videos.
- **Core assumption**: Sufficient overlap exists between frames within a small time window, and Dust3R can reliably estimate relative poses from these aligned pairs.
- **Evidence anchors**:
  - [abstract] "We argue that large scale 360° videos can address these limitations to provide: scalable corresponding frames from diverse views."
  - [section 3.1] "The 360° nature allows the views of frames to be rotated such that they contain overlapping content."
  - [corpus] Weak—no direct neighbor papers focus on 360° video correspondence extraction; FMR 0.599 for nearest neighbor on inverse rendering.
- **Break condition**: Overlap between frames becomes insufficient due to rapid camera motion or sparse sampling rate, making alignment and pose estimation unreliable.

### Mechanism 2
- **Claim**: Motion masking enables training on dynamic scenes by filtering out moving objects from the loss.
- **Mechanism**: An auxiliary mask channel is added to the U-Net, producing a soft per-pixel weight map that downweights contributions from likely dynamic regions during diffusion training.
- **Core assumption**: The model can distinguish static from dynamic scene parts based on consistency across frames in a correspondence pair.
- **Evidence anchors**:
  - [section 5.2] "This soft mask allows the model to filter out portions of the scene which may be difficult to predict due to object movement."
  - [section 5.2] "we introduce an auxiliary loss term that incentivizes the mask to be non-zero."
  - [corpus] No direct evidence—corpus lacks motion segmentation papers; nearest neighbor on UAV-based scenes (FMR 0.460).
- **Break condition**: Dynamic objects occupy most of the scene or move in ways that mimic static structure, causing the mask to suppress too much content and degrade geometry learning.

### Mechanism 3
- **Claim**: Training on the largest real-world multi-view dataset yields improved generalization to novel view synthesis benchmarks without fine-tuning.
- **Mechanism**: Large-scale diverse training data provides rich priors for camera pose and geometry, allowing the model to infer plausible views from a single input image even in unseen scenes.
- **Core assumption**: Diversity and scale of 360-1M outweigh domain gaps between YouTube scenes and benchmark datasets (DTU, MipNeRF360).
- **Evidence anchors**:
  - [abstract] "Empowered by the largest real-world, multi-view dataset to date, ODIN is able to freely generate novel views of real-world scenes."
  - [section 6.2] "We observe improved performance on DTU and Mip-NeRF 360 on the standard NVS metrics (Tables 1 and 2)."
  - [corpus] Weak—no direct neighbor on large-scale multi-view training; nearest neighbor on steerable scene generation (FMR 0.000).
- **Break condition**: Training data bias toward certain scene types (e.g., outdoor travel) causes poor extrapolation to structured indoor or tabletop benchmarks.

## Foundational Learning

- **Concept**: Equirectangular projection and spherical coordinates for 360° images
  - **Why needed here**: Enables mapping of 360° frames to controllable viewpoints for correspondence search.
  - **Quick check question**: Given a yaw angle φ and pitch θ, how do you map a pixel from equirectangular to a 3D direction vector on the unit sphere?

- **Concept**: Structure from motion (SfM) and camera pose estimation
  - **Why needed here**: Dust3R estimates relative camera poses from overlapping image pairs to build the multi-view dataset.
  - **Quick check question**: What are the minimal number of point correspondences required to solve for a relative pose between two calibrated cameras?

- **Concept**: Diffusion probabilistic models and latent diffusion
  - **Why needed here**: ODIN uses a latent diffusion U-Net conditioned on camera pose to synthesize novel views.
  - **Quick check question**: In a diffusion model, what is the role of the noise schedule, and how does conditioning on pose modify the denoiser?

## Architecture Onboarding

- **Component map**: 360° video downloader -> frame extractor (1 FPS) -> correspondence finder (Dust3R + alignment) -> scale resolver -> 360-1M dataset -> ODIN model (encoder E -> latent diffusion U-Net fθ (with motion mask channel) -> decoder D) -> evaluation / 3D reconstruction
- **Critical path**: Data pipeline -> multi-view dataset creation -> ODIN training -> evaluation -> 3D reconstruction
- **Design tradeoffs**:
  - Sampling rate vs. compute: 1 FPS balances coverage and O(L²) search cost.
  - Motion mask strength λ vs. geometry fidelity: too high masks out useful detail.
  - Long-range correspondence propagation vs. graph complexity: increases recall but adds merge/split logic.
- **Failure signatures**:
  - Sparse correspondences -> poor geometry priors -> noisy novel views.
  - Overly aggressive motion masking -> holes or blur in dynamic regions.
  - Insufficient scale calibration -> metric misalignment in reconstructions.
- **First 3 experiments**:
  1. Run correspondence search on a 10-second clip at 0.5 FPS vs. 1 FPS; compare number and quality of pairs.
  2. Train ODIN on a subset of 1000 videos with λ = 0 vs. λ = 1; evaluate LPIPS on held-out pairs.
  3. Generate a trajectory of 5 views from a held-out scene; reconstruct geometry and measure Chamfer distance vs. ground truth.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How would the performance of ODIN scale if trained on a larger dataset than 360-1M?
- Basis in paper: [inferred] The authors note that ODIN is the first model to synthesize real-world 3D scenes and reconstruct their geometry from a single image, enabled by the scale and diversity of 360-1M. They also suggest further potential in using 360-1M and 360° video for novel view synthesis and other domains.
- Why unresolved: The authors do not experiment with training ODIN on datasets larger than 360-1M, so the scalability of the model is unknown.
- What evidence would resolve it: Training ODIN on a dataset larger than 360-1M and comparing its performance to the current version on standard benchmarks.

### Open Question 2
- Question: Can ODIN be extended to model dynamic scenes and objects, not just static ones?
- Basis in paper: [explicit] The authors acknowledge that while the motion mask allows filtering out dynamic elements, ideally they would like to learn to model dynamic elements as well. They mention that 4D NeRF models can move the camera in both time and space, but generalized 4D models are largely unexplored.
- Why unresolved: The current version of ODIN is designed for static scenes, and the authors do not experiment with extending it to handle dynamic elements.
- What evidence would resolve it: Extending ODIN to incorporate 4D modeling and testing its performance on datasets with dynamic scenes and objects.

### Open Question 3
- Question: How would ODIN perform on other 3D reconstruction benchmarks beyond Google Scanned Objects and the held-out set of 360-1M?
- Basis in paper: [inferred] The authors evaluate ODIN's 3D reconstruction capabilities on Google Scanned Objects and a held-out set of 360-1M, but do not mention testing it on other benchmarks.
- Why unresolved: The authors do not provide results for other 3D reconstruction benchmarks, so ODIN's generalization to different datasets is unknown.
- What evidence would resolve it: Testing ODIN on other 3D reconstruction benchmarks, such as ShapeNet or DTU, and comparing its performance to existing methods.

## Limitations
- The reliance on automatic correspondence extraction from YouTube videos introduces domain shift risks when evaluating on curated benchmarks like DTU.
- The motion masking mechanism's effectiveness is demonstrated only qualitatively, and the auxiliary loss weighting (λ) is treated as a hyperparameter without sensitivity analysis.
- The claim that ODIN generalizes to 3D reconstruction without fine-tuning is based on a single qualitative example, with quantitative validation on held-out scenes or other reconstruction benchmarks absent.

## Confidence
- **High confidence**: The core mechanism of using 360° videos for scalable multi-view data extraction is technically sound and well-supported by the literature on equirectangular projections and SfM. The performance gains on DTU and MipNeRF360 are measurable and reproducible.
- **Medium confidence**: The diffusion model architecture (ODIN) and its training procedure follow established latent diffusion practices. However, the specific design choices (e.g., motion mask implementation, scale calibration post-processing) lack sufficient detail for exact replication.
- **Low confidence**: The claim that ODIN generalizes to 3D reconstruction without fine-tuning is based on a single qualitative example (Fig. 5). Quantitative validation on held-out 360-1M scenes or other reconstruction benchmarks is absent, making this claim speculative.

## Next Checks
1. **Correspondence Quality Audit**: Randomly sample 100 correspondence pairs from the 360-1M dataset and manually verify pose accuracy and overlap quality. Measure the percentage of pairs with geometrically consistent views.
2. **Motion Masking Ablation**: Train ODIN variants with λ = 0, λ = 0.5, and λ = 1 on a fixed subset of 1000 videos. Compare LPIPS scores on held-out pairs with varying degrees of dynamic content to quantify the trade-off between static geometry preservation and dynamic object suppression.
3. **Generalization Stress Test**: Evaluate ODIN on a held-out subset of 360-1M scenes (not used in training) for both NVS (LPIPS, PSNR) and 3D reconstruction (Chamfer distance, IoU). Compare against fine-tuned baselines to isolate the benefit of large-scale pretraining.