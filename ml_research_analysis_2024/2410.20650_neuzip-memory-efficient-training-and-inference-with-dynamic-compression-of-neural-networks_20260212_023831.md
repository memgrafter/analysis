---
ver: rpa2
title: 'NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of
  Neural Networks'
arxiv_id: '2410.20650'
source_url: https://arxiv.org/abs/2410.20650
tags:
- neuzip
- memory
- training
- https
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NeuZip, a compression method for neural networks
  that enables memory-efficient training and inference without sacrificing performance.
  The method exploits the low-entropy nature of exponent bits in floating-point representations
  of neural network parameters, compressing them losslessly using asymmetric numeral
  systems (ANS) during training.
---

# NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks

## Quick Facts
- arXiv ID: 2410.20650
- Source URL: https://arxiv.org/abs/2410.20650
- Reference count: 40
- Primary result: Reduces memory usage by >50% for training and inference while maintaining near-lossless performance

## Executive Summary
NeuZip is a novel compression method for neural networks that enables memory-efficient training and inference by exploiting the low-entropy nature of exponent bits in floating-point representations. The method uses asymmetric numeral systems (ANS) for lossless compression during training and lossy mantissa truncation during inference, achieving more than 50% memory reduction while maintaining near-lossless performance. Notably, NeuZip can train a Llama-3 8B model with less than 16GB memory (down from 31GB) without any performance degradation, and provides better memory-performance trade-offs compared to quantization baselines across multiple model architectures and tasks.

## Method Summary
NeuZip operates by first observing that the exponent bits in floating-point representations of neural network parameters tend to have low entropy. During training, it compresses these exponent bits losslessly using asymmetric numeral systems (ANS), while maintaining full precision for mantissa bits to preserve gradient computation accuracy. For inference, NeuZip adds an additional lossy compression step by truncating mantissa bits to further reduce memory usage. The method dynamically adapts compression parameters based on the observed entropy of exponent distributions and uses careful bookkeeping to enable seamless integration with standard training pipelines without requiring architectural modifications.

## Key Results
- Reduces memory usage by more than half during training and inference
- Trains Llama-3 8B model with less than 16GB memory (vs 31GB baseline) without performance degradation
- Achieves better memory-performance trade-offs than quantization baselines across multiple architectures including LLMs and vision models

## Why This Works (Mechanism)
NeuZip exploits the observation that neural network parameters, particularly in their exponent representations, often exhibit low entropy distributions. During training, lossless compression of these low-entropy exponent bits using ANS significantly reduces memory footprint without affecting numerical precision needed for gradient computations. During inference, additional lossy compression through mantissa truncation provides further memory savings at the cost of minimal performance degradation, which is acceptable for most practical applications. The method's effectiveness stems from the inherent structure in neural network weights rather than requiring architectural changes or sacrificing numerical precision during the critical training phase.

## Foundational Learning

**Asymmetric Numeral Systems (ANS)**: Entropy coding method that combines arithmetic coding efficiency with Huffman coding speed, essential for lossless compression of exponent bits. Quick check: Verify that ANS can achieve compression ratios close to theoretical entropy limits for typical exponent distributions.

**Floating-point representation structure**: Understanding exponent and mantissa bit organization in IEEE 754 format, critical for identifying compressible components. Quick check: Confirm that exponent bits indeed show lower entropy than mantissa bits across various neural network parameters.

**Memory bandwidth vs computation trade-offs**: Recognizing that memory access often dominates training costs, making compression techniques valuable even with some computational overhead. Quick check: Measure the ratio of memory-bound to compute-bound operations in typical training workloads.

## Architecture Onboarding

**Component map**: Input tensors -> Exponent extraction -> ANS compression -> Memory storage -> Decompression -> Full precision reconstruction for training / Truncated reconstruction for inference

**Critical path**: The most critical component is the dynamic entropy estimation of exponent bits, as this determines the effectiveness of compression and requires real-time monitoring during training to adapt compression parameters.

**Design tradeoffs**: The method balances between compression ratio and computational overhead, choosing lossless compression for training (to maintain gradient accuracy) while allowing lossy compression for inference (where some precision loss is acceptable). This creates a dual-mode system that maximizes utility across both phases.

**Failure signatures**: Compression effectiveness drops when exponent bits have high entropy (e.g., in networks with unusual weight distributions), or when mantissa truncation exceeds acceptable thresholds causing significant performance degradation. Training instability may occur if decompression timing affects gradient computation order.

**First experiments**:
1. Measure entropy distribution of exponent vs mantissa bits across different layer types and model scales
2. Benchmark compression/decompression throughput against memory bandwidth savings
3. Evaluate gradient computation accuracy with compressed vs uncompressed parameters

## Open Questions the Paper Calls Out

None identified in the provided materials.

## Limitations

- Effectiveness depends on low entropy in exponent bits, which may not generalize across all network architectures
- Lossy mantissa truncation during inference introduces performance trade-offs that vary with model scale and task complexity
- The method's benefits are primarily measured on LLMs and vision models, with uncertain performance on other architectures

## Confidence

High: Memory reduction claims and Llama-3 8B training demonstration are directly measurable and reproducible
Medium: Inference performance trade-offs due to lossy mantissa truncation have architecture-dependent effects
Low: Claims about universal applicability across diverse neural network types are not empirically validated

## Next Checks

1. Test NeuZip on recurrent architectures (LSTMs, GRUs) and vision transformers to verify cross-architecture effectiveness
2. Evaluate performance degradation under different precision requirements (e.g., medical imaging, scientific computing) to establish practical boundaries
3. Measure training time overhead and compare against hardware-aware baselines to determine practical efficiency gains beyond memory savings