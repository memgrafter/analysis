---
ver: rpa2
title: Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench
arxiv_id: '2410.22108'
source_url: https://arxiv.org/abs/2410.22108
tags:
- unlearning
- image
- forget
- ext0
- retain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses privacy risks in multimodal large language
  models (MLLMs), where models can memorize and disclose individuals' confidential
  data. To mitigate this, the authors introduce MLLMU-Bench, a benchmark designed
  to evaluate multimodal machine unlearning, consisting of 500 fictitious and 153
  celebrity profiles with over 14 customized question-answer pairs per profile.
---

# Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench

## Quick Facts
- arXiv ID: 2410.22108
- Source URL: https://arxiv.org/abs/2410.22108
- Reference count: 40
- Key outcome: Introduces MLLMU-Bench to evaluate multimodal machine unlearning, revealing that unimodal methods outperform multimodal ones in generation/cloze tasks while multimodal methods excel in classification with multimodal inputs, highlighting fundamental trade-offs between forgetting effectiveness and model utility.

## Executive Summary
This paper addresses critical privacy risks in multimodal large language models (MLLMs) that can memorize and potentially disclose individuals' confidential data. The authors introduce MLLMU-Bench, a comprehensive benchmark designed to evaluate multimodal machine unlearning across both synthetic and real celebrity profiles. Through systematic experiments with two base MLLMs and five unlearning methods, the study reveals surprising findings about the relative performance of unimodal versus multimodal approaches and identifies fundamental trade-offs between unlearning effectiveness and model utility.

## Method Summary
The methodology involves generating 500 fictitious profiles and 153 celebrity profiles, each with over 14 customized question-answer pairs. Base MLLMs (LLaVA-1.5-7B and Idefics2-8B) are first fine-tuned on the complete dataset, then subjected to various unlearning algorithms including gradient ascent, gradient difference, KL minimization, and prompting-based methods. The benchmark evaluates unlearning efficacy through separate datasets (Forget Set, Test Set, Retain Set, Real Celebrity Set) across multimodal (image+text) and unimodal (text-only) settings using classification, generation, and cloze tasks with specific metrics like ROUGE-L.

## Key Results
- Unimodal unlearning methods consistently outperform multimodal approaches in generation and cloze tasks while multimodal methods excel in classification with multimodal inputs
- A fundamental trade-off exists between unlearning effectiveness and model utility across retained samples, neighboring concepts, and general reasoning ability
- The benchmark reveals that current multimodal unlearning approaches struggle to match the performance of their unimodal counterparts despite using multimodal inputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MLLMU-Bench enables precise evaluation of multimodal machine unlearning through structured profiles with over 14 question-answer pairs per profile
- **Mechanism**: The benchmark's design isolates unlearning efficacy, generalizability, and model utility through separate datasets (Forget Set, Test Set, Retain Set, Real Celebrity Set), allowing controlled assessment of how well unlearning methods remove specific knowledge while preserving unrelated performance
- **Core assumption**: Structured, profile-based questions can effectively probe both multimodal (image+text) and unimodal (text-only) knowledge representations in MLLMs
- **Evidence anchors**:
  - [abstract]: "MLLMU-Bench consists of 500 fictitious profiles and 153 profiles for public celebrities, each profile feature over 14 customized question-answer pairs, evaluated from both multimodal (image+text) and unimodal (text) perspectives."
  - [section]: "MLLMU-Bench provides a comprehensive evaluation of unlearning in both multimodal and unimodal settings, highlighting the focus of each setup and the interplay between modalities in affecting unlearning performance."
  - [corpus]: Corpus shows related works focus on unlearning misinformation or visual concepts, but MLLMU-Bench is the first to provide comprehensive multimodal privacy-focused evaluation
- **Break condition**: If the question-answer pairs fail to adequately probe the multimodal representations, or if the profiles are too generic to represent real privacy scenarios, the benchmark's evaluation power diminishes

### Mechanism 2
- **Claim**: Unimodal unlearning methods outperform multimodal ones in generation and cloze tasks, while multimodal methods excel in classification with multimodal inputs
- **Mechanism**: Unimodal methods focus solely on text, which is the dominant modality in decision-making for MLLMs. This focused approach leads to better preservation of instruction-following and contextual understanding in generation/cloze tasks, while multimodal methods benefit from joint image-text processing for classification
- **Core assumption**: The textual modality plays a central role in MLLM decision-making, making unimodal unlearning more effective for tasks requiring instruction alignment
- **Evidence anchors**:
  - [abstract]: "Surprisingly, our experiments show that unimodal unlearning algorithms excel in generation and cloze tasks, while multimodal unlearning approaches perform better in classification tasks with multimodal inputs."
  - [section]: "Unlike the classification results, the unimodal GA approach always shows better unlearning effectiveness than multimodal GA on both multimodal and unimodal setups, as indicated by the larger Rouge-L difference compared to the multimodal GA."
  - [corpus]: Related work focuses on neuron pruning or adversarial unlearning, but none systematically compare unimodal vs multimodal unlearning performance across task types
- **Break condition**: If the dominance of text modality in MLLMs changes (e.g., with more balanced multimodal architectures), or if generation/cloze tasks require stronger multimodal understanding, this mechanism would break

### Mechanism 3
- **Claim**: There exists a fundamental trade-off between unlearning effectiveness and model utility across retain accuracy, neighboring concepts, and general reasoning ability
- **Mechanism**: Unlearning methods that strongly remove target knowledge (e.g., gradient ascent) inevitably affect retained knowledge and general capabilities due to the entangled nature of learned representations in MLLMs
- **Core assumption**: Knowledge in MLLMs is not neatly compartmentalized, so removing specific information requires modifying shared representations that also encode retained knowledge
- **Evidence anchors**:
  - [abstract]: "Additionally, we find a trade-off between unlearning effectiveness and model utility across various factors, including performance on retained samples, neighboring concepts, and model general ability."
  - [section]: "We observe a similar trend on other perspectives of model utility such as neighboring concepts (i.e. Figure 4b), model reasoning ability (i.e. Figure 4c), and model helpfulness ability (i.e. Figure 4d)."
  - [corpus]: Corpus shows limited work on unlearning utility trade-offs in MLLMs specifically, though related work exists for LLMs
- **Break condition**: If future architectures enable more modular knowledge representation where specific information can be removed without affecting shared representations, this trade-off could be mitigated

## Foundational Learning

- **Concept**: Multimodal knowledge entanglement in MLLMs
  - **Why needed here**: Understanding how image and text information become intertwined in MLLM representations is crucial for designing effective unlearning methods that can selectively remove knowledge without collateral damage
  - **Quick check question**: How do MLLMs represent and process joint image-text information, and why does this make selective forgetting challenging?

- **Concept**: Gradient-based unlearning methods and their limitations
  - **Why needed here**: The benchmark evaluates several gradient-based approaches (GA, Gradient Difference, KL Minimization), requiring understanding of how these methods work and why they may cause catastrophic forgetting
  - **Quick check question**: What are the key differences between gradient ascent, gradient difference, and KL minimization approaches in the context of machine unlearning?

- **Concept**: Multimodal evaluation metrics for generation and classification
  - **Why needed here**: The benchmark uses specific metrics like ROUGE-L for generation and accuracy for classification across both multimodal and unimodal settings, requiring understanding of how these metrics capture unlearning effectiveness
  - **Quick check question**: How do multimodal and unimodal evaluation setups differ in their ability to detect incomplete unlearning, and why are different metrics used for generation vs classification tasks?

## Architecture Onboarding

- **Component map**: Profile generation system (GPT-4o-based synthetic profiles) -> Question-answer pair generator (multimodal and unimodal questions) -> Image transformation pipeline (Arc2Face for Test Set) -> Evaluation framework (classification, generation, cloze metrics) -> Unlearning method implementations (GA, Gradient Difference, KL Minimization, NPO, prompting) -> Base MLLM models (LLaVA-1.5-7B, Idefics2-8B)

- **Critical path**: Profile generation → Question generation → Model fine-tuning → Unlearning application → Multimodal/unimodal evaluation → Trade-off analysis

- **Design tradeoffs**:
  - Synthetic vs real profiles: Synthetic ensures privacy compliance but may lack real-world complexity
  - Fixed question sets vs dynamic generation: Fixed enables consistent evaluation but may not capture all knowledge aspects
  - Multimodal vs unimodal evaluation: Multimodal better reflects real usage but requires more resources

- **Failure signatures**:
  - Poor generalization on Test Set indicates incomplete unlearning
  - Significant drop in Retain Set performance indicates over-aggressive unlearning
  - Inconsistent performance across modalities suggests modality-specific knowledge entanglement

- **First 3 experiments**:
  1. **Baseline establishment**: Fine-tune LLaVA-1.5-7B on full profile dataset, evaluate on all four sets to establish vanilla performance
  2. **Unimodal unlearning comparison**: Apply gradient ascent with text-only inputs vs image+text inputs, compare performance across task types
  3. **Trade-off analysis**: Apply different unlearning methods with varying forget percentages (5%, 10%, 15%), measure impact on forget set removal vs retain set preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multimodal unlearning approaches achieve both effective forgetting and maintained model utility across all modalities?
- Basis in paper: [inferred] from discussion in section 5.1 and 5.2 on performance gaps between unimodal and multimodal approaches, and the trade-off analysis in Figure 4
- Why unresolved: The paper demonstrates performance gaps but does not conclusively determine whether a multimodal approach can balance forgetting effectiveness and utility preservation
- What evidence would resolve it: Experiments comparing advanced multimodal unlearning techniques that explicitly account for modality interactions, measuring both forgetting effectiveness and utility across all modalities

### Open Question 2
- Question: What are the underlying causes of the performance gap between unimodal and multimodal unlearning approaches?
- Basis in paper: [explicit] from section 5.1 discussion on why multimodal approaches underperform in certain tasks despite using multimodal inputs
- Why unresolved: The paper identifies the performance gap but does not investigate the root causes of why unimodal methods outperform multimodal ones in generation and cloze tasks
- What evidence would resolve it: Detailed analysis of how modality entanglement affects the unlearning process, potentially through ablation studies or modality-specific attention analysis

### Open Question 3
- Question: How can unlearning methods be made robust against attacks that attempt to recover forgotten information?
- Basis in paper: [inferred] from the limitation section mentioning potential future work on evaluating robustness against attack techniques like training data extraction or jailbreaking
- Why unresolved: The benchmark focuses on standard evaluation but does not include adversarial robustness testing
- What evidence would resolve it: Experiments applying attack methods to unlearned models and developing defensive techniques to prevent information recovery

### Open Question 4
- Question: Can selective unlearning be achieved where specific attributes of a profile are forgotten while preserving others?
- Basis in paper: [explicit] from limitations section mentioning future work on selectively unlearning specific key attributes rather than entire profiles
- Why unresolved: The current benchmark assumes complete forgetting of all profile information when unlearning is triggered
- What evidence would resolve it: Development and evaluation of attribute-level unlearning methods that can selectively remove specific information while maintaining other profile details

## Limitations

- The benchmark relies heavily on synthetic profile generation, which may not capture the full complexity and variability of real-world privacy scenarios
- The study uses only two base MLLM architectures (LLaVA-1.5-7B and Idefics2-8B), limiting generalizability to other MLLM variants
- The evaluation focuses on short-term unlearning effectiveness without examining potential knowledge recovery or forgetting decay over time

## Confidence

**High Confidence**: The benchmark provides a comprehensive evaluation framework for multimodal machine unlearning, as evidenced by the systematic evaluation across multiple datasets (Forget Set, Test Set, Retain Set, Real Celebrity Set) and task types.

**Medium Confidence**: Unimodal unlearning methods outperform multimodal ones in generation and cloze tasks while multimodal methods excel in classification tasks. This conclusion is based on limited experiments with specific models and unlearning algorithms.

**Medium Confidence**: The fundamental trade-off between unlearning effectiveness and model utility is observed, though the extent and nature of this trade-off may vary with different model architectures and unlearning approaches.

## Next Checks

1. **Generalizability test**: Apply the MLLMU-Bench evaluation to additional MLLM architectures beyond LLaVA-1.5-7B and Idefics2-8B to verify the robustness of findings across different model families

2. **Longitudinal analysis**: Implement a time-based study to track unlearning effectiveness and model utility over extended periods (e.g., weeks or months) to assess knowledge recovery and forgetting stability

3. **Real-world scenario validation**: Replace synthetic profiles with anonymized real-world privacy cases (with proper consent) to validate whether the benchmark's findings translate to actual privacy protection needs