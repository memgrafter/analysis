---
ver: rpa2
title: ICPR 2024 Competition on Domain Adaptation and GEneralization for Character
  Classification (DAGECC)
arxiv_id: '2412.17984'
source_url: https://arxiv.org/abs/2412.17984
tags:
- domain
- data
- dataset
- competition
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper describes the DAGECC competition held at ICPR 2024,
  focusing on domain adaptation and generalization for character classification. The
  competition introduced Safran-MNIST, a new real-world dataset suite containing industrial
  serial number images, comprising two datasets: Safran-MNIST-D (10 classes, digits
  0-9) and Safran-MNIST-DLS (32 classes, including digits, letters, and symbols).'
---

# ICPR 2024 Competition on Domain Adaptation and GEneralization for Character Classification (DAGECC)

## Quick Facts
- arXiv ID: 2412.17984
- Source URL: https://arxiv.org/abs/2412.17984
- Reference count: 33
- Six teams participated, with Team Deng winning both tasks

## Executive Summary
The DAGECC competition at ICPR 2024 focused on domain adaptation and generalization for character classification, introducing the Safran-MNIST dataset suite containing industrial serial number images. Two tasks were proposed: Task 1 (Domain Generalization) without target domain data during training, and Task 2 (Unsupervised Domain Adaptation) with unlabeled target domain data. Six teams participated, with Team Deng winning both tasks using synthetic data generation and pretrained deep learning architectures. The competition aimed to foster advancement in domain adaptation and generalization techniques by providing a high-quality, lightweight dataset suitable for fast prototyping and validation of novel ideas in character recognition applications.

## Method Summary
The competition introduced Safran-MNIST, a real-world dataset suite with industrial serial number images, comprising Safran-MNIST-D (10 digit classes) and Safran-MNIST-DLS (32 classes including digits, letters, and symbols). Two tasks were proposed: Task 1 (Domain Generalization) where participants developed models to generalize to unseen target domains without using target data during training, and Task 2 (Unsupervised Domain Adaptation) where participants used unlabeled target domain data during training. Teams employed various approaches including synthetic data generation, pretrained deep learning architectures (ResNet50, GoogLeNet), and ensemble methods to address domain adaptation and generalization challenges.

## Key Results
- Team Deng won both Task 1 and Task 2 using synthetic data generation and pretrained ResNet50
- Synthetic data generation was a key strategy, with teams creating custom datasets using image processing and generative AI
- Ensemble methods combining multiple network architectures (AlexNet and custom CNN) were used to improve robustness and accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data generation bridges domain gaps when target domain data is scarce.
- Mechanism: Teams generated custom synthetic datasets using image processing and generative AI to simulate target domain characteristics (e.g., weathered digits on metallic surfaces), then trained models on this augmented data to improve generalization.
- Core assumption: Synthetic data sufficiently captures the distribution and variability of real target domain data to enable effective transfer learning.
- Evidence anchors:
  - [abstract] The competition aimed to foster advancements in domain adaptation and generalization techniques by providing a high-quality, lightweight dataset suitable for fast prototyping and validation.
  - [section] Team Deng generated custom datasets using random colors, noise, embossing effects, and geometric transformations to mimic real-world imperfections.
  - [corpus] Weak or missing corpus evidence for this specific mechanism.
- Break condition: If synthetic data fails to capture critical domain-specific features (e.g., texture variations, lighting conditions), model performance on real target data will degrade significantly.

### Mechanism 2
- Claim: Pretrained deep learning architectures provide strong feature extraction capabilities that can be fine-tuned for domain-specific tasks.
- Mechanism: Teams leveraged pretrained models (ResNet50, GoogLeNet, AlexNet) initialized with ImageNet weights, then fine-tuned them on domain-relevant datasets to adapt to character classification tasks.
- Core assumption: Features learned from large-scale natural image datasets transfer effectively to character recognition in industrial contexts.
- Evidence anchors:
  - [section] The winning solution relied on the ResNet50 architecture, initialized with pre-trained weights on the ImageNet dataset.
  - [section] The team used a GoogLeNet pretrained on ImageNet and fine-tuned on various public datasets.
  - [corpus] Weak or missing corpus evidence for this specific mechanism.
- Break condition: If the target domain characteristics differ substantially from ImageNet (e.g., metallic surfaces, engraved characters), fine-tuning may not be sufficient to achieve good performance.

### Mechanism 3
- Claim: Ensemble methods combining multiple network architectures improve robustness and accuracy.
- Mechanism: JasonMendoza2008 used a weighted mean of predictions from five distinct networks (AlexNet and custom CNN) to achieve better classification performance.
- Core assumption: Different network architectures capture complementary features that, when combined, reduce individual model biases and errors.
- Evidence anchors:
  - [section] Prediction was achieved by a weighted mean of the predictions of five distinct networks.
  - [corpus] Weak or missing corpus evidence for this specific mechanism.
- Break condition: If ensemble members are too similar in their failure modes or if the weighting strategy is poorly calibrated, the ensemble may not provide significant performance gains.

## Foundational Learning

- Concept: Domain Adaptation vs. Domain Generalization
  - Why needed here: Understanding the distinction is crucial for selecting appropriate strategies and evaluating model performance on the two competition tasks.
  - Quick check question: What is the key difference between domain adaptation and domain generalization in terms of target domain data availability during training?
- Concept: Synthetic Data Generation
  - Why needed here: Creating realistic synthetic data is a core strategy used by top-performing teams to address data scarcity and domain shift issues.
  - Quick check question: What image processing techniques were used by Team Deng to generate synthetic data that mimics real-world imperfections?
- Concept: Transfer Learning with Pretrained Models
  - Why needed here: Fine-tuning pretrained deep learning architectures is a fundamental approach for leveraging existing knowledge and adapting to new tasks efficiently.
  - Quick check question: Which pretrained architectures were used by the top teams, and how were they initialized?

## Architecture Onboarding

- Component map: Data preprocessing pipeline -> Feature extraction backbone -> Classification head -> Synthetic data generation module -> Ensemble integration
- Critical path: Data preprocessing → Feature extraction → Classification → Ensemble aggregation
- Design tradeoffs:
  - Using larger pretrained models vs. smaller custom architectures for computational efficiency
  - Generating synthetic data vs. collecting more real target domain data
  - Ensemble methods vs. single model optimization for simplicity and speed
- Failure signatures:
  - Poor performance on specific character classes may indicate imbalanced training data or inadequate synthetic data generation for those classes
  - Degradation when tested on unseen target domains suggests insufficient domain generalization
  - High computational cost during inference may indicate overly complex architectures or inefficient ensemble integration
- First 3 experiments:
  1. Train a simple CNN baseline on the provided Safran-MNIST training data and evaluate on the validation set to establish a performance benchmark.
  2. Implement and test synthetic data generation techniques (e.g., geometric transformations, noise addition) and evaluate their impact on model generalization.
  3. Fine-tune a pretrained ResNet50 on the augmented dataset and compare performance against the baseline model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for leveraging unlabeled target domain data in domain adaptation tasks, particularly for the Safran-MNIST-DLS dataset?
- Basis in paper: [inferred] The paper mentions that the Machine Learning Group LTU team focused on exploiting the unlabeled corpus through pseudo-label generation and retraining, but this approach did not achieve the highest performance.
- Why unresolved: The paper suggests that while data generation methods were effective, better exploitation of unlabeled data could improve performance, but does not provide a definitive answer on the optimal strategy.
- What evidence would resolve it: Comparative studies evaluating different methods for utilizing unlabeled data (e.g., pseudo-labeling, consistency regularization, entropy minimization) on the Safran-MNIST-DLS dataset would provide insights into the most effective approach.

### Open Question 2
- Question: How does the performance of domain adaptation and generalization techniques vary across different character recognition datasets, and what factors contribute to this variation?
- Basis in paper: [explicit] The paper introduces the Safran-MNIST dataset suite and describes the competition tasks, but does not compare the performance across different datasets.
- Why unresolved: The paper focuses on the Safran-MNIST dataset and does not provide a comprehensive analysis of how these techniques perform on other character recognition datasets.
- What evidence would resolve it: Experiments comparing the performance of domain adaptation and generalization techniques on multiple character recognition datasets, including Safran-MNIST, MNIST, and others, would reveal factors contributing to performance variation.

### Open Question 3
- Question: What are the limitations of using synthetic data generation techniques for domain adaptation and generalization in character recognition, and how can these limitations be addressed?
- Basis in paper: [inferred] The paper highlights the success of teams using synthetic data generation but does not discuss the limitations of these methods.
- Why unresolved: While synthetic data generation was effective for the competition, the paper does not explore potential drawbacks or challenges associated with these techniques.
- What evidence would resolve it: Analysis of the quality, diversity, and realism of synthetic data compared to real-world data, along with experiments evaluating the impact of synthetic data limitations on model performance, would provide insights into addressing these challenges.

## Limitations
- Limited information on preprocessing pipelines and hyperparameter choices of winning solutions
- No reported statistical significance testing for performance differences between teams
- Specific synthetic data generation techniques used by Team Deng are not fully detailed

## Confidence
- High confidence: The competition structure, dataset characteristics, and evaluation metrics are clearly specified
- Medium confidence: The general approaches used by winning teams (pretrained models, synthetic data generation, ensemble methods) are described, but implementation details remain unclear
- Low confidence: Claims about specific mechanisms' effectiveness lack corpus-level validation beyond the competition results

## Next Checks
1. Replicate the winning approach using publicly available implementations of ResNet50 and synthetic data generation techniques to verify claimed performance improvements
2. Conduct ablation studies removing synthetic data generation to quantify its contribution to domain generalization performance
3. Test model performance across multiple random seeds and dataset splits to establish statistical significance of the reported results