---
ver: rpa2
title: Adversarially Trained Weighted Actor-Critic for Safe Offline Reinforcement
  Learning
arxiv_id: '2401.00629'
source_url: https://arxiv.org/abs/2401.00629
tags:
- policy
- offline
- safe
- learning
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes WSAC, a novel algorithm for Safe Offline Reinforcement
  Learning (Safe-RL) under functional approximation. The key innovation is a two-player
  Stackelberg game that optimizes a refined objective function.
---

# Adversarially Trained Weighted Actor-Critic for Safe Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2401.00629
- Source URL: https://arxiv.org/abs/2401.00629
- Authors: Honghao Wei; Xiyue Peng; Arnob Ghosh; Xin Liu
- Reference count: 40
- One-line primary result: WSAC achieves safe policy improvement with optimal 1/√N convergence rate

## Executive Summary
This paper introduces WSAC, a novel algorithm for Safe Offline Reinforcement Learning under functional approximation. The key innovation is a two-player Stackelberg game formulation that optimizes a refined objective function, allowing the actor to improve policy performance while maintaining safety constraints. The algorithm uses adversarially trained critics with importance-weighted Bellman errors to focus learning on scenarios where the actor's performance is inferior to the reference policy.

WSAC provides several theoretical guarantees including safe policy improvement over any reference policy, optimal statistical convergence rate of 1/√N, and practical robustness across hyperparameter settings. The authors also present empirical results demonstrating that WSAC outperforms state-of-the-art safe offline RL algorithms across multiple continuous control environments.

## Method Summary
WSAC is a safe offline RL algorithm that frames the actor-critic interaction as a two-player Stackelberg game. The actor optimizes an aggression-limited objective that balances reward maximization against cost constraint violations, while the critics minimize importance-weighted Bellman errors. The algorithm uses a policy optimization oracle that achieves no-regret against any competitor policy. WSAC operates under weaker single-policy concentrability assumptions compared to previous methods, making it more practical for real-world applications where data coverage is limited.

## Key Results
- WSAC produces policies that outperform any reference policy while maintaining the same level of safety
- The algorithm achieves optimal statistical convergence rate of 1/√N under single-policy ℓ2 concentrability
- WSAC guarantees safe policy improvement across a broad range of hyperparameters controlling pessimism
- Empirically outperforms all baseline safe offline RL algorithms across multiple continuous control tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** WSAC guarantees robust safe policy improvement over any reference policy including the behavior policy.
- **Mechanism:** Uses an aggression-limited objective function fr(s0, π) − λ · [fc(s0, π)]+ that penalizes policies violating safety constraints, combined with adversarially trained critics that minimize importance-weighted Bellman errors.
- **Core assumption:** The importance weight function class W contains the ratio wπref = dπref/µ for the reference policy, and the policy optimization oracle is no-regret.
- **Evidence anchors:**
  - [abstract]: "WSAC can produce a policy that outperforms any reference policy while maintaining the same level of safety"
  - [section]: Theorem 4.1 shows Jr(ˆπ∗) ≥ Jr(µ) and {Jc(ˆπ∗)}+ ≤ {Jc(µ)}+ + 1/λ for any β ≥ 0, λ > 0
  - [corpus]: Weak - no direct corpus evidence for this specific mechanism
- **Break condition:** If wπref ∉ W or the no-regret oracle fails to achieve o(1) regret, the safety guarantee may break.

### Mechanism 2
- **Claim:** WSAC achieves optimal statistical convergence rate of 1/√N under single-policy ℓ2 concentrability.
- **Mechanism:** Uses weighted Bellman error regularization with importance weights to control estimation error, avoiding the need for all-policy concentrability assumptions.
- **Core assumption:** The weight function class W has bounded ℓ2 norm (Assumption 3.6) and single-policy realizability holds (Assumption 3.7).
- **Evidence anchors:**
  - [abstract]: "WSAC achieves the optimal statistical convergence rate of 1/√N to the reference policy"
  - [section]: Theorem 5.2 shows suboptimality bounds with O(ϵstat) where ϵstat = VmaxC*ℓ2√(log|F||Π||W|/δ)/N
  - [corpus]: Weak - no direct corpus evidence for this specific mechanism
- **Break condition:** If C*ℓ2 grows with N or the function approximation error ϵ1 is large, the rate may degrade.

### Mechanism 3
- **Claim:** WSAC provides practical robustness across a broad range of hyperparameters controlling pessimism.
- **Mechanism:** The aggression-limited objective and weighted Bellman regularization work together to ensure safe policy improvement even with suboptimal hyperparameter choices.
- **Core assumption:** The hyperparameter β can be chosen as o(√N) and λ is sufficiently large.
- **Evidence anchors:**
  - [abstract]: "WSAC guarantees a safe policy improvement across a broad range of hyperparameters that control the degree of pessimism"
  - [section]: Theorem 5.6 shows that for β = o(√N), the algorithm achieves vanishing suboptimality with high probability
  - [corpus]: Weak - no direct corpus evidence for this specific mechanism
- **Break condition:** If β is chosen too large relative to N, the regularization may dominate and prevent learning.

## Foundational Learning

- **Concept: Stackelberg game formulation for constrained optimization**
  - Why needed here: WSAC frames the actor-critic interaction as a two-player Stackelberg game where the actor optimizes against adversarially trained critics
  - Quick check question: What is the key difference between a Stackelberg game and a standard min-max game in this context?

- **Concept: Importance-weighted Bellman error**
  - Why needed here: The critics minimize importance-weighted Bellman errors to handle insufficient data coverage while maintaining statistical efficiency
  - Quick check question: How does the weighted Bellman error differ from standard squared Bellman error in terms of bias and variance?

- **Concept: No-regret optimization oracle**
  - Why needed here: The actor uses a no-regret oracle to ensure robust policy improvement against any competitor policy
  - Quick check question: What properties must a policy optimization algorithm have to qualify as a no-regret oracle in this setting?

## Architecture Onboarding

- **Component map:**
  - Actor network: Outputs policy π(a|s)
  - Reward critic network: Estimates Qr(s,a) with weighted Bellman error regularization
  - Cost critic network: Estimates Qc(s,a) with weighted Bellman error regularization
  - Importance weight function: Represents marginalized importance weights w(s,a)
  - Policy optimization oracle: Updates policy using gradient-based methods

- **Critical path:** Dataset → Weighted Bellman error computation → Critic updates → Actor loss computation → Policy update → Evaluation

- **Design tradeoffs:**
  - Single-policy vs all-policy concentrability: WSAC trades weaker assumptions for more complex critic training
  - Pessimism level: λ controls safety vs performance tradeoff
  - Regularization strength: β balances bias vs variance in critic estimation

- **Failure signatures:**
  - Poor performance: Check if λ is too large or β is too small
  - Safety violations: Verify that wπref ∈ W and that the importance weights are correctly computed
  - Instability: Ensure the critic updates are stable and the learning rates are appropriate

- **First 3 experiments:**
  1. Run WSAC on a simple tabular environment with known behavior policy to verify safety guarantees
  2. Test sensitivity to λ by running with different values on a continuous control benchmark
  3. Compare WSAC against a primal-dual baseline on an environment where all-policy concentrability fails

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does WSAC's performance scale with the size and diversity of the offline dataset?
- Basis in paper: [inferred] The paper discusses the importance of data coverage and single-policy concentrability, but does not provide empirical analysis on dataset size effects.
- Why unresolved: The theoretical analysis focuses on statistical convergence rates, but practical dataset requirements are not explored.
- What evidence would resolve it: Systematic experiments varying dataset size and diversity, measuring performance degradation and convergence speed.

### Open Question 2
- Question: Can WSAC's safety guarantees be extended to environments with dynamic or stochastic safety constraints?
- Basis in paper: [inferred] The current formulation assumes static cost constraints, but many real-world applications involve changing safety requirements.
- Why unresolved: The theoretical framework is built around fixed cost functions, and the paper doesn't address dynamic scenarios.
- What evidence would resolve it: Empirical results on environments with time-varying or probabilistic safety constraints, along with theoretical analysis of convergence under such conditions.

### Open Question 3
- Question: How does WSAC compare to online safe RL methods in terms of sample efficiency and final performance?
- Basis in paper: [inferred] The paper focuses on offline settings but mentions the potential for combining with online exploration.
- Why unresolved: The paper does not include direct comparisons with online safe RL algorithms or explore the trade-offs between offline and online approaches.
- What evidence would resolve it: Comparative experiments between WSAC and online safe RL methods, measuring sample efficiency and performance across various tasks.

## Limitations
- The safety guarantees critically depend on the assumption that wπref ∈ W, which may not hold in practice with complex behavior policies
- The empirical evaluation is limited to specific continuous control environments from Liu et al. (2023a)
- The paper does not investigate how WSAC performs with different offline datasets or in environments with different characteristics

## Confidence
- **High confidence**: The theoretical framework and algorithm formulation are sound and well-developed
- **Medium confidence**: The empirical results show consistent improvement over baselines in the tested environments
- **Low confidence**: The practical robustness across hyperparameters is theoretically guaranteed but not extensively validated empirically

## Next Checks
1. **Theoretical validation**: Verify the assumption wπref ∈ W on a simple tabular environment where the behavior policy is known, to test whether the safety guarantee holds in practice.
2. **Empirical robustness**: Conduct an ablation study varying λ and β across a wider range to empirically validate the claimed hyperparameter robustness.
3. **Generalization test**: Evaluate WSAC on a different offline dataset with a different behavior policy to test whether the performance gains are dataset-specific or more general.