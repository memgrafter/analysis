---
ver: rpa2
title: 'The Role of Generative Systems in Historical Photography Management: A Case
  Study on Catalan Archives'
arxiv_id: '2409.03911'
source_url: https://arxiv.org/abs/2409.03911
tags:
- historical
- image
- images
- data
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of generative systems to improve image
  captioning for historical photographs from Catalan archives. It investigates whether
  synthetic images and captions can help adapt models to historical and linguistic
  domains, and whether language modeling benefits from linguistic proximity.
---

# The Role of Generative Systems in Historical Photography Management: A Case Study on Catalan Archives

## Quick Facts
- arXiv ID: 2409.03911
- Source URL: https://arxiv.org/abs/2409.03911
- Reference count: 40
- Primary result: Synthetic captions significantly improve historical image captioning more than synthetic images, with language modeling performance sensitive to linguistic proximity

## Executive Summary
This study investigates how generative systems can improve image captioning for historical photographs from Catalan archives, addressing challenges of domain shift in both visual (historical vs. modern) and linguistic (Catalan vs. English-dominant) contexts. The researchers explore whether synthetic data generation can help adapt models to historical and linguistic domains, and whether language modeling benefits from linguistic proximity. Using Stable Diffusion XL for image generation and automatic translation for captions, they create training datasets in Catalan, Spanish, Italian, German, and Dutch. The findings reveal that synthetic captions are more effective than synthetic images for historical domain adaptation, and that language modeling performance is sensitive to linguistic proximity, with closely related languages providing better pretraining.

## Method Summary
The study uses CATR (Caption Transformer) architecture with ResNet + Vision Transformer encoder and Transformer decoder. The methodology involves generating synthetic datasets: 591K images using Stable Diffusion XL with COCO captions plus historical year prompts (1930-1999), and translated captions using OpenNMT and M2M100 models. The approach follows a pretraining → fine-tuning → evaluation pipeline, where models are first trained on synthetic data (images only, captions only, or both) then fine-tuned on the XAC historical dataset. Performance is evaluated using CIDEr and BLEU4 metrics on masked named entities in Catalan captions.

## Key Results
- Synthetic captions significantly improve historical domain adaptation more than synthetic images
- Language modeling performance shows sensitivity to linguistic proximity
- Current image generation models struggle to capture historical context effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic captions improve model performance more than synthetic images for historical photography domain adaptation.
- Mechanism: Textual descriptions capture semantic relationships and context that are critical for captioning tasks, while synthetic images introduce noise and fail to accurately represent historical visual attributes.
- Core assumption: The quality of language modeling has a greater impact on captioning performance than visual feature quality in the historical domain.
- Evidence anchors:
  - [abstract]: "synthetic captions significantly improve performance when fine-tuning on historical data, more so than synthetic images"
  - [section]: "the use of syntheticcaptionsprovestobeadominantstrategyinknowledgetransferduringthefine-tuning step, unlike the case with synthetic images"
  - [corpus]: Found related work on image captioning and domain adaptation, suggesting this is a recognized challenge in the field
- Break condition: If synthetic image generation improves significantly in capturing historical visual attributes, or if the language modeling approach becomes less critical for the task.

### Mechanism 2
- Claim: Language modeling fine-tuning performance is sensitive to linguistic proximity.
- Mechanism: Models pretrained on closely related languages transfer knowledge more effectively to the target language, improving performance in low-resource scenarios.
- Core assumption: Languages with similar structures and vocabularies share transferable linguistic features that benefit pretraining.
- Evidence anchors:
  - [abstract]: "Language modeling shows sensitivity to linguistic proximity, with better results when pretraining on closely related languages"
  - [section]: "the English-German cluster proves more convenient for Ducth" and "Spanish is the best pre-training language for Catalan"
  - [corpus]: Related work on multilingual models and cross-lingual transfer learning supports this mechanism
- Break condition: If the model architecture changes to reduce dependence on language-specific features, or if the linguistic proximity effect diminishes with larger datasets.

### Mechanism 3
- Claim: Historical context sensitivity is crucial for effective captioning of historical photographs.
- Mechanism: Current image generation models cannot accurately capture fine-grained historical cues, leading to less effective captioning when synthetic images are used.
- Core assumption: Historical photographs contain contextual attributes that are difficult to represent in synthetic images, affecting the quality of generated captions.
- Evidence anchors:
  - [abstract]: "synthetic image generation introduces noise and fails to capture historical context effectively"
  - [section]: "Current image generation models are not capable of introducing fine-grained historical cues"
  - [corpus]: Related work on historical image analysis and domain adaptation highlights the importance of historical context
- Break condition: If image generation models improve significantly in capturing historical context, or if the importance of historical context diminishes for the task.

## Foundational Learning

- Concept: Image Captioning
  - Why needed here: Understanding the task of generating textual descriptions for images is fundamental to the study
  - Quick check question: What are the main components of an image captioning model, and how do they work together?

- Concept: Domain Adaptation
  - Why needed here: The study focuses on adapting models to historical and linguistic domains, requiring knowledge of domain adaptation techniques
  - Quick check question: What are the main challenges in domain adaptation, and how can they be addressed?

- Concept: Language Modeling
  - Why needed here: The study investigates the role of language modeling in captioning performance, particularly in relation to linguistic proximity
  - Quick check question: How does pretraining on related languages affect the performance of language models in low-resource scenarios?

## Architecture Onboarding

- Component map: CATR (Caption Transformer) -> ResNet + Vision Transformer Encoder -> Transformer Decoder
- Critical path: Pretraining → Fine-tuning → Evaluation
  - Pretraining: Use synthetic data (images and/or captions) to adapt the model to the historical domain
  - Fine-tuning: Further adapt the model to the specific linguistic domain (Catalan) using real data
  - Evaluation: Assess the performance of the model using metrics such as CIDEr and BLEU4
- Design tradeoffs:
  - Synthetic vs. Real Data: Using synthetic data can help address data scarcity but may introduce noise and fail to capture historical context
  - Language Proximity: Pretraining on closely related languages can improve performance but may not be feasible for all language pairs
  - Model Complexity: More complex models may capture more nuanced features but require more computational resources
- Failure signatures:
  - Low performance on historical data: Indicates issues with domain adaptation or historical context sensitivity
  - Poor performance on Catalan data: Suggests problems with linguistic proximity or language modeling
  - Inconsistent results across experiments: May point to issues with data quality, model architecture, or evaluation metrics
- First 3 experiments:
  1. Compare the performance of models pretrained on synthetic images vs. synthetic captions
  2. Evaluate the impact of pretraining on different languages (e.g., Spanish, Italian, German) for Catalan captioning
  3. Assess the sensitivity of the model to historical context by varying the temporal range of the synthetic data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do synthetic images and captions compare to real images with translated captions in improving captioning models for historical photographs?
- Basis in paper: [explicit] The paper concludes that synthetic captions significantly improve performance when fine-tuning on historical data, more so than synthetic images, and suggests that using natural images with translated captions is more advantageous.
- Why unresolved: The paper suggests synthetic images introduce noise and fail to capture historical context effectively, but does not provide a detailed comparative analysis of the two approaches across multiple historical periods or languages.
- What evidence would resolve it: A comprehensive study comparing the performance of synthetic images versus real images with translated captions across diverse historical periods and languages, using multiple metrics like CIDEr and BLEu4 scores.

### Open Question 2
- Question: To what extent does linguistic proximity influence the performance of language modeling in captioning systems for minoritized languages?
- Basis in paper: [explicit] The study finds that language modeling shows sensitivity to linguistic proximity, with better results when pretraining on closely related languages, as demonstrated by the performance differences between Spanish and German when adapting to Catalan.
- Why unresolved: While the paper highlights the importance of linguistic proximity, it does not explore the full spectrum of linguistic relationships or provide a detailed analysis of how different degrees of linguistic similarity affect performance.
- What evidence would resolve it: An extensive analysis of language modeling performance across a wide range of language pairs with varying degrees of linguistic similarity, using a consistent dataset and evaluation metrics.

### Open Question 3
- Question: How does the effectiveness of image generation models vary across different historical periods and cultural contexts?
- Basis in paper: [inferred] The paper suggests that image generation models struggle to capture historical context effectively and introduces noise, particularly when dealing with historical photographs, but does not provide a detailed analysis across different time periods.
- Why unresolved: The paper does not explore how the effectiveness of image generation models varies across different historical periods or cultural contexts, which could provide insights into the limitations of these models.
- What evidence would resolve it: A study comparing the performance of image generation models across various historical periods and cultural contexts, using both quantitative metrics and qualitative assessments to evaluate their effectiveness.

## Limitations

- Synthetic image generation using Stable Diffusion XL may not accurately capture historical visual attributes, limiting the validity of comparisons
- Translation quality for generating synthetic captions in target languages is not independently verified
- Evaluation metrics (CIDEr and BLEU4) may not fully capture the quality of historical image descriptions, particularly regarding named entities and historical context
- Study focuses on a specific historical range (1930-1999) and language pair (Catalan), limiting generalizability

## Confidence

**High Confidence:**
- Synthetic captions improve performance more than synthetic images for historical domain adaptation
- Language modeling shows sensitivity to linguistic proximity
- The CATR model architecture is appropriate for image captioning tasks

**Medium Confidence:**
- Historical context sensitivity is crucial for effective captioning
- Specific language proximity rankings (e.g., Spanish being optimal for Catalan)
- The relative performance differences between pretraining strategies

**Low Confidence:**
- The exact mechanisms by which synthetic captions outperform synthetic images
- The long-term stability of findings across different historical periods
- The impact of translation quality on language modeling results

## Next Checks

1. **Cross-dataset validation**: Test the same pretraining strategies on a different historical archive (e.g., American or British historical photographs) to assess generalizability of language proximity findings.

2. **Human evaluation study**: Conduct qualitative assessment of generated captions to verify whether CIDEr/BLEU4 scores align with human judgment of historical context preservation and named entity accuracy.

3. **Synthetic image quality analysis**: Systematically evaluate the historical authenticity of synthetic images using both automated CLIP-based similarity metrics and expert historical photography assessment to quantify the impact of visual noise on captioning performance.