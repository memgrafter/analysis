---
ver: rpa2
title: 'Dialectal Toxicity Detection: Evaluating LLM-as-a-Judge Consistency Across
  Language Varieties'
arxiv_id: '2411.10954'
source_url: https://arxiv.org/abs/2411.10954
tags:
- dialectal
- language
- arabic
- standard
- toxicity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates how dialectal variations
  affect toxicity detection by large language models (LLMs), addressing a critical
  gap in understanding LLM performance across diverse language varieties. The authors
  created a comprehensive multi-dialect dataset covering 10 language clusters and
  60 varieties through synthetic transformations and human-assisted translations,
  including real-world utterances from a Bengali dialect speaker.
---

# Dialectal Toxicity Detection: Evaluating LLM-as-a-Judge Consistency Across Language Varieties

## Quick Facts
- **arXiv ID**: 2411.10954
- **Source URL**: https://arxiv.org/abs/2411.10954
- **Reference count**: 8
- **Primary result**: LLMs show 96.22% dialectal consistency but only 49.15% LLM-human agreement when evaluating toxicity across 60 language varieties

## Executive Summary
This paper investigates how dialectal variations affect toxicity detection by large language models (LLMs), addressing a critical gap in understanding LLM performance across diverse language varieties. The authors created a comprehensive multi-dialect dataset covering 10 language clusters and 60 varieties through synthetic transformations and human-assisted translations, including real-world utterances from a Bengali dialect speaker. They evaluated three LLMs (AYA-23-12B, Phi-3 Mini-3.8B, and Mistral-NEMO-8B) on multilingual and dialectal consistency, as well as LLM-human agreement. The results show that while LLMs demonstrate sensitivity to dialectal nuances, the weakest area is LLM-human agreement (averaging 49.15%), followed by dialectal consistency (96.22%). The strongest performance is in multilingual consistency (99.39%).

## Method Summary
The study created a multi-dialect dataset by starting with the ToxiGen corpus, translating statements across 10 language clusters using NLLB-200, applying dialect synthesis tools (Multi-VALUE for English, Murre for Finnish/Swedish) to generate dialectal variations, and adding real-world Bengali utterances with ASR transcription. Three instruction-tuned LLMs were evaluated using structured prompts with a toxicity evaluation rubric including severity scoring (1-5), target identification (T0-T3), and dialectal sensitivity impact (D0-D3). Consistency metrics were computed for LLM-Human agreement, multilingual consistency across standard varieties, and dialectal consistency between standard and dialect forms.

## Key Results
- LLM-human agreement averages only 49.15%, the weakest performance metric
- Dialectal consistency reaches 96.22% across all language varieties
- Multilingual consistency is strongest at 99.39%
- Standard language varieties show minimal dialectal sensitivity impact (averaging 35.25% acknowledgment) compared to non-standard dialects
- NeMo's multilingual instruction-following ability degrades significantly for certain low-resource languages, particularly Bengali (46.5 percentage point drop)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM-as-a-judge framework is sensitive to dialectal nuances but shows weakest alignment with human judgments.
- Mechanism: LLMs use instruction-tuned rubrics to evaluate toxicity across dialects, but their perception of dialectal sensitivity (D0-D3 levels) creates divergence from human annotators.
- Core assumption: The rubric's dialectal sensitivity categories (D0-D3) effectively capture how LLMs perceive linguistic variations in toxicity judgments.
- Evidence anchors:
  - [abstract] shows LLM-human agreement averaging 49.15%, while dialectal consistency is 96.22%
  - [section] states "the weakest area is LLM-human agreement, followed by dialectal consistency"
  - [corpus] shows related work on "How Reliable is Multilingual LLM-as-a-Judge?" indicating this is an active research question
- Break condition: If the rubric fails to account for cultural and linguistic context in dialectal toxicity perception, LLM judgments will consistently diverge from human assessments.

### Mechanism 2
- Claim: Standard language varieties show minimal dialectal sensitivity impact compared to non-standard dialects in LLM judgments.
- Mechanism: LLMs recognize that standard varieties contain minimal dialectal cues and respond with D0 (no impact), while non-standard dialects trigger higher acknowledgment of dialectal impact.
- Core assumption: The distinction between standard and non-standard varieties is clear enough for LLMs to consistently differentiate.
- Evidence anchors:
  - [section] reports "percentage of LLMs acknowledging an impact was significantly lower when the text was presented in standard varieties"
  - [section] shows "Standard English, Finnish, and Standard Bengali" averaging around 60% acknowledgment
  - [corpus] includes "DialectalArabicMMLU: Benchmarking Dialectal Capabilities" suggesting dialectal differentiation is measurable
- Break condition: If language clusters lack clearly defined standard varieties, LLMs will struggle to consistently identify minimal dialectal sensitivity.

### Mechanism 3
- Claim: NeMo's multilingual instruction-following ability degrades significantly for certain low-resource languages.
- Mechanism: NeMo's performance drops when handling languages with complex morphological structures and less training data, despite its multilingual instruction tuning.
- Core assumption: NeMo's instruction-following capability is sensitive to resource level and morphological complexity of target languages.
- Evidence anchors:
  - [section] states "NeMo also performs well and in some cases surpasses Aya but its performance drops significantly in Bengali (a drop of 46.5 percentage points)"
  - [section] shows NeMo struggles particularly with "Finnish and Latvian, both high-resource language groups"
  - [corpus] includes "Translate, then Detect: Leveraging Machine Translation for Cross-Lingual Toxicity Classification" suggesting translation-based approaches have limitations
- Break condition: If instruction-following capability is primarily dependent on parameter count rather than language-specific tuning, then NeMo's performance should be consistent across all languages.

## Foundational Learning

- Concept: Dialectal variation in toxicity perception
  - Why needed here: The paper evaluates how LLMs handle toxicity across different language varieties, requiring understanding of how linguistic variations affect perceived toxicity
  - Quick check question: What distinguishes a standard language variety from its dialectal counterparts in terms of toxicity perception?

- Concept: LLM-as-a-judge framework
  - Why needed here: The evaluation relies on LLMs acting as judges using specific rubrics to assess toxicity, which is central to the consistency metrics
  - Quick check question: How does the D0-D3 dialectal sensitivity rubric work to capture LLM perception of dialectal variations?

- Concept: Consistency metrics for multilingual evaluation
  - Why needed here: The paper introduces specific metrics (LLM-Human, Multilingual, Dialectal consistency) to quantify LLM performance across language varieties
  - Quick check question: What's the difference between multilingual consistency and dialectal consistency in this evaluation framework?

## Architecture Onboarding

- Component map: ToxiGen dataset -> machine translation -> dialect synthesis -> ASR transcription -> LLM toxicity evaluation (prompt with rubric) -> Consistency calculation (LLM-Human, Multilingual, Dialectal)
- Critical path: Synthetic dataset creation -> LLM evaluation with rubric -> Consistency metric computation -> Analysis of dialectal sensitivity
- Design tradeoffs: Using synthetic transformations vs. authentic dialectal data; instruction-tuned vs. base LLMs; comprehensive rubric vs. simpler evaluation criteria
- Failure signatures: Low LLM-human agreement (below 50%); inconsistent performance across dialects within same language cluster; high variance in multilingual consistency scores
- First 3 experiments:
  1. Test LLM-human agreement baseline using only standard language varieties
  2. Compare synthetic vs. authentic dialectal data performance on single language cluster
  3. Evaluate impact of rubric complexity on LLM consistency scores across all language varieties

## Open Questions the Paper Calls Out

The paper acknowledges several limitations but doesn't explicitly call out open questions. The primary limitation is that the dataset is mostly synthetic and machine-translated, with a low percentage of real-world dialectal examples. The authors note this affects the generalizability of their findings to authentic dialectal toxicity detection scenarios.

## Limitations
- The dataset relies heavily on synthetic transformations rather than authentic dialectal utterances, with only one real Bengali speaker's data included
- LLM-human agreement is consistently low (49.15%), suggesting fundamental challenges in aligning model judgments with human toxicity perception across dialects
- Performance degradation in low-resource languages (Sotho-Tswana, Kurdish) indicates potential bias in the evaluation framework

## Confidence
- **High Confidence**: The consistency metrics framework and the observation that standard varieties show lower dialectal sensitivity acknowledgment compared to non-standard dialects
- **Medium Confidence**: The specific performance rankings of the three LLMs across different language clusters, particularly NeMo's performance degradation
- **Low Confidence**: The generalizability of findings to languages beyond the 10 clusters studied and real-world toxicity detection systems

## Next Checks
1. Collect authentic dialectal utterances across multiple languages and speakers to validate whether synthetic transformations capture the same dialectal toxicity nuances as natural language use
2. Measure consistency among human annotators when evaluating toxicity across dialectal variations, particularly for non-standard varieties, to establish a baseline for "agreement"
3. Test whether the D0-D3 dialectal sensitivity rubric developed for one language cluster applies consistently to other language families to validate its universality