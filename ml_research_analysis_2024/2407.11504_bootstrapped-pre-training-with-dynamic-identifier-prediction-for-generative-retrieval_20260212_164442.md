---
ver: rpa2
title: Bootstrapped Pre-training with Dynamic Identifier Prediction for Generative
  Retrieval
arxiv_id: '2407.11504'
source_url: https://arxiv.org/abs/2407.11504
tags:
- retrieval
- pre-training
- docids
- documents
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces BootRet, a bootstrapped pre-training method
  for generative retrieval that dynamically updates document identifiers (docids)
  during training. BootRet addresses the limitation of static docids by iteratively
  refining both the model parameters and the docids through three key phases: initial
  docid generation using product quantization, pre-training with corpus indexing and
  relevance prediction tasks, and enhanced bootstrapping for docid updates.'
---

# Bootstrapped Pre-training with Dynamic Identifier Prediction for Generative Retrieval

## Quick Facts
- arXiv ID: 2407.11504
- Source URL: https://arxiv.org/abs/2407.11504
- Reference count: 32
- Primary result: BootRet achieves 11.8% improvement in Hits@1 on MS MARCO over existing pre-training generative retrieval baselines

## Executive Summary
This paper introduces BootRet, a bootstrapped pre-training method for generative retrieval that dynamically updates document identifiers (docids) during training. The method addresses the limitation of static docids in pre-trained generative retrieval models by iteratively refining both model parameters and docids through three key phases: initial docid generation using product quantization, pre-training with corpus indexing and relevance prediction tasks, and enhanced bootstrapping for docid updates. Experimental results on MS MARCO and Natural Questions datasets show that BootRet significantly outperforms existing pre-training generative retrieval baselines, achieving 11.8% improvement in Hits@1 on MS MARCO and demonstrating strong zero-shot performance.

## Method Summary
BootRet employs a three-phase training approach: (1) Initial docid generation using product quantization codes derived from document embeddings, (2) Pre-training with corpus indexing and relevance prediction tasks that incorporate noisy documents and pseudo-queries generated by large language models, and (3) Enhanced bootstrapping where docids are iteratively updated based on the model's learned representations. The method uses contrastive losses to improve discriminative learning and enables the model to capture both document-document and query-document relationships. During fine-tuning, the model is trained with standard maximum likelihood estimation using document-docid pairs and labeled query-docid pairs.

## Key Results
- BootRet achieves 11.8% improvement in Hits@1 on MS MARCO Document Ranking dataset compared to existing pre-training baselines
- The method demonstrates strong zero-shot performance across different retrieval tasks
- BootRet shows consistent improvements in both MS MARCO and Natural Questions datasets
- Iterative bootstrapping with dynamic docid updates proves more effective than static pre-training approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BootRet improves retrieval by iteratively refining both model parameters and document identifiers (docids) during pre-training.
- Mechanism: The bootstrapping process involves pre-training the model with static docids, then updating the docids based on the model's learned representations. This creates a feedback loop where the model's understanding of document-document relationships improves as the identifiers become more semantically meaningful.
- Core assumption: The assumption is that docids that better reflect the model's learned document representations will lead to improved retrieval performance.
- Evidence anchors:
  - [abstract] "BootRet involves three key training phases: (i) initial identifier generation, (ii) pre-training via corpus indexing and relevance prediction tasks, and (iii) bootstrapping for identifier updates."
  - [section] "BootRet includes three key steps: (i) Initial docid generation. We leverage the encoder of the initial model to encode documents and then obtain the product quantization code (Ge et al., 2013; Zhan et al., 2021) as the initial docids. (ii) Pre-training... (iii) Enhanced bootstrapping. The encoder of the model pre-trained with the above two tasks is further used to encode documents, updating document representations, and then updating the PQ code, i.e., docids."
- Break condition: The process could break if the model overfits to the pre-training data, causing the updated docids to become too specialized and lose generalizability.

### Mechanism 2
- Claim: BootRet improves retrieval by incorporating noisy documents and pseudo-queries during pre-training.
- Mechanism: Noisy documents are created by applying transformations to original documents, while pseudo-queries are generated for each document. These are used in contrastive learning tasks to help the model distinguish between similar documents and learn relevance information.
- Core assumption: The assumption is that exposing the model to noisy variations of documents and pseudo-queries will improve its ability to generalize and discriminate between relevant and irrelevant documents.
- Evidence anchors:
  - [abstract] "To facilitate the pre-training phase, we further introduce noisy documents and pseudo-queries, generated by large language models, to resemble semantic connections in both indexing and retrieval tasks."
  - [section] "We introduce the construction of noisy documents and pre-training objectives in detail... We propose leveraging a LLM to effectively achieve this... We employ a LLM to construct pseudo-queries..."
- Break condition: This mechanism could break if the noisy documents and pseudo-queries are too dissimilar from the original documents, causing the model to learn incorrect associations.

### Mechanism 3
- Claim: BootRet improves retrieval by using contrastive losses during pre-training.
- Mechanism: Contrastive losses are used to encourage the model to generate the correct docid for a given document or query with higher probability than generating docids for other documents. This helps the model learn to discriminate between similar documents.
- Core assumption: The assumption is that contrastive learning will help the model learn more discriminative representations of documents and docids.
- Evidence anchors:
  - [abstract] "Experimental results demonstrate that BootRet significantly outperforms existing pre-training generative retrieval baselines and performs well even in zero-shot settings."
  - [section] "Inspired by contrastive learning (Khosla et al., 2020), this loss LC1(D, ID; θt−1) is formalized as: -NX i=1 log exp(P (idi | di)/τ )PN j=1 exp(P (idj | di)/τ )"
- Break condition: This mechanism could break if the temperature hyperparameter τ is not set appropriately, leading to either too weak or too strong contrastive signals.

## Foundational Learning

- Concept: Product Quantization (PQ)
  - Why needed here: PQ is used to generate the initial docids, which are then iteratively updated during pre-training.
  - Quick check question: What is the purpose of using PQ to generate docids in BootRet?

- Concept: Contrastive Learning
  - Why needed here: Contrastive losses are used to encourage the model to generate the correct docid for a given document or query with higher probability than generating docids for other documents.
  - Quick check question: How do contrastive losses help the model learn to discriminate between similar documents?

- Concept: Bootstrapping
  - Why needed here: Bootstrapping is the iterative process of updating the model parameters and docids based on each other's current state.
  - Quick check question: What is the purpose of the bootstrapping process in BootRet?

## Architecture Onboarding

- Component map: Encoder-decoder architecture with T5-base transformer encoder and identifier decoder, using product quantization codes as docids
- Critical path: The bootstrapping process, which involves pre-training the model with static docids, then updating the docids based on the model's learned representations
- Design tradeoffs: The main tradeoff is between the number of iterations in the bootstrapping process and the computational cost. More iterations can lead to better performance but also increase the training time.
- Failure signatures: Potential failures include overfitting to the pre-training data, noisy documents and pseudo-queries being too dissimilar from the original documents, and contrastive losses not being effective due to improper hyperparameter settings.
- First 3 experiments:
  1. Evaluate the performance of BootRet with different numbers of bootstrapping iterations on a downstream retrieval task.
  2. Compare the performance of BootRet with and without noisy documents and pseudo-queries during pre-training.
  3. Analyze the impact of different contrastive loss formulations on the performance of BootRet.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the bootstrapping approach perform when applied to dynamically changing corpora where new documents are continuously added?
- Basis in paper: [inferred] The paper mentions that handling incremental documents is a limitation and suggests future work could explore adaptively adjusting cluster centers in the docid generation process based on the similarity between new and old documents.
- Why unresolved: The current BootRet framework assumes a static corpus and does not address the challenges of updating docids and model parameters when new documents are introduced over time.
- What evidence would resolve it: Experimental results comparing BootRet's performance on a corpus with continuously added documents versus a static corpus, showing how the model adapts and maintains retrieval accuracy.

### Open Question 2
- Question: What is the optimal number of iterations for the bootstrapping process, and how does it vary across different types of corpora?
- Basis in paper: [explicit] The paper discusses the impact of the number of iterations, finding that performance generally improves up to a certain point (around 7 iterations for MS 300K and 6 for NQ) before declining due to overfitting.
- Why unresolved: The paper does not provide a clear method for determining the optimal number of iterations for different datasets or explain why the optimal number varies.
- What evidence would resolve it: A systematic study across various datasets with different characteristics (size, domain, document length) to identify patterns in the optimal number of iterations and factors influencing it.

### Open Question 3
- Question: How does BootRet's performance compare to state-of-the-art dense retrieval methods on extremely large-scale datasets (e.g., web-scale corpora)?
- Basis in paper: [inferred] The paper focuses on relatively smaller datasets (MS MARCO and Natural Questions) and mentions that scalability is a significant challenge in current GR.
- Why unresolved: The experiments do not test BootRet on datasets approaching web-scale size, leaving questions about its scalability and efficiency in handling millions or billions of documents.
- What evidence would resolve it: Performance comparisons between BootRet and top dense retrieval methods (like ANCE or RepBERT) on large-scale datasets, including metrics on memory usage, inference speed, and retrieval accuracy.

### Open Question 4
- Question: How sensitive is BootRet's performance to the choice of prompts for generating noisy documents and pseudo-queries?
- Basis in paper: [explicit] The paper describes using specific prompts for generating noisy documents and pseudo-queries, and mentions that the synonym replacement prompt performs more modestly compared to others.
- Why unresolved: The paper does not provide a comprehensive analysis of how different prompt designs affect the model's performance or whether there's an optimal set of prompts for different types of corpora.
- What evidence would resolve it: An ablation study testing various prompt designs (including more complex or domain-specific prompts) and their impact on retrieval performance across multiple datasets, identifying which prompt characteristics are most beneficial.

### Open Question 5
- Question: Can BootRet's dynamic identifier approach be effectively combined with learnable docids (optimized jointly with the retrieval task) rather than using product quantization codes?
- Basis in paper: [inferred] The paper contrasts its approach with methods using learnable docids but does not explore combining dynamic identifiers with joint optimization.
- Why unresolved: The paper focuses on using PQ codes as static identifiers that are dynamically updated, but does not investigate whether combining this with learnable docids during the retrieval phase could yield better results.
- What evidence would resolve it: An experimental comparison between BootRet using PQ codes versus BootRet using learnable docids, measuring differences in retrieval performance, training efficiency, and model flexibility.

## Limitations

- The iterative bootstrapping process has unclear convergence properties and may lead to overfitting beyond a certain number of iterations
- The effectiveness of noisy documents and pseudo-queries depends heavily on the quality of the LLM used to generate them, with limited exploration of different generation strategies
- The paper focuses on English datasets, leaving questions about generalizability to other languages and domains

## Confidence

- High confidence in the core mechanism of using product quantization for initial docid generation and contrastive learning for training
- Medium confidence in the effectiveness of the bootstrapping process and its contribution to performance gains
- Medium confidence in the contribution of noisy documents and pseudo-queries to the model's success
- Low confidence in the scalability of BootRet to very large document collections approaching web-scale

## Next Checks

1. **Convergence Analysis**: Run BootRet with varying numbers of bootstrapping iterations (1-15) on a held-out validation set to empirically determine the optimal stopping point and assess overfitting risks.

2. **Noisy Data Sensitivity**: Systematically vary the quality and diversity of noisy documents and pseudo-queries by using different LLMs or generation parameters to quantify their impact on final performance.

3. **Zero-Shot Generalization**: Evaluate BootRet on zero-shot retrieval tasks in different languages (e.g., using the BEIR benchmark) to assess its ability to generalize beyond the pre-training domain.