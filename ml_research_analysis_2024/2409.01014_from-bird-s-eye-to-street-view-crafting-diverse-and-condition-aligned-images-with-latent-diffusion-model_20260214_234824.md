---
ver: rpa2
title: 'From Bird''s-Eye to Street View: Crafting Diverse and Condition-Aligned Images
  with Latent Diffusion Model'
arxiv_id: '2409.01014'
source_url: https://arxiv.org/abs/2409.01014
tags:
- image
- diffusion
- view
- generation
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a framework for generating street-view images
  from bird''s-eye-view (BEV) maps by fine-tuning a large pretrained diffusion model.
  The approach involves two stages: neural view transformation to project BEV maps
  into camera views, and street image generation using a fine-tuned latent diffusion
  model.'
---

# From Bird's-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model

## Quick Facts
- arXiv ID: 2409.01014
- Source URL: https://arxiv.org/abs/2409.01014
- Authors: Xiaojie Xu; Tianshuo Xu; Fulong Ma; Yingcong Chen
- Reference count: 40
- Primary result: Proposes a two-stage pipeline using neural view transformation and fine-tuned diffusion models to generate multi-view street images from BEV maps, outperforming existing methods in FID and mIOU metrics

## Executive Summary
This paper introduces a framework for generating street-view images from bird's-eye-view (BEV) semantic maps by leveraging a large pretrained diffusion model. The approach consists of two stages: first projecting BEV maps into camera views using neural view transformation with height priors and shape refinement, then generating street images using a fine-tuned latent diffusion model conditioned on these refined semantic maps. The method incorporates view adaptation and conditional generation to ensure viewpoint consistency and style alignment across multiple camera perspectives. Experiments demonstrate that the proposed approach achieves competitive or superior performance compared to existing methods in terms of visual quality and condition consistency.

## Method Summary
The proposed method employs a two-stage pipeline to transform BEV semantic maps into multi-view street images. In the first stage, neural view transformation projects BEV maps into camera views using camera parameters and a height prior (1.5-2 meters for vehicles), followed by shape refinement using a convolutional network to improve semantic accuracy. In the second stage, a pretrained Stable Diffusion model is fine-tuned using LoRA layers on driving scene imagery, with ControlNet layers for semantic conditioning and viewpoint-specific prompts to ensure consistent orientations across camera views. The system generates 400×224 RGB images from 200×200 BEV maps with two channels (vehicles and roads), evaluated using FID and mIOU metrics on the nuScenes dataset.

## Key Results
- Outperforms or matches existing methods in visual quality and condition consistency
- Achieves competitive FID scores indicating strong image quality
- Demonstrates improved mIOU scores showing better alignment with input conditions
- Successfully generates diverse street-view images across multiple camera perspectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage pipeline separates view transformation from image generation, enabling specialized refinement of semantic maps before passing them to the diffusion model.
- Mechanism: Neural view transformation projects BEV maps to camera views using camera parameters and a height prior, then refines the semantic shapes with a CNN. This refined semantic map serves as a high-fidelity condition for the diffusion model.
- Core assumption: Semantic shape correspondence between BEV and camera views can be learned and refined to guide image generation.
- Evidence anchors:
  - [abstract] "Our approach comprises two main components: the Neural View Transformation and the Street Image Generation."
  - [section] "Our pipeline operates in two stages... a shape-refinement post-processing step is imperative."
  - [corpus] Weak - no direct evidence in neighbors; paper is the first to propose this specific two-stage view transformation.
- Break condition: If the height prior or shape refinement fails to capture the true geometry of objects, the semantic map will be inaccurate and the final image quality will degrade.

### Mechanism 2
- Claim: Fine-tuning a large pretrained diffusion model with LoRA on driving scenes allows the model to adapt both style and viewpoint without full retraining.
- Mechanism: LoRA layers are added to the diffusion model, enabling efficient fine-tuning on a small dataset of driving images. View-specific prompts and a dual-loss function ensure viewpoint alignment.
- Core assumption: The base diffusion model has learned generalizable visual features that can be adapted to the driving domain with limited data.
- Evidence anchors:
  - [abstract] "leveraging a large, pretrained latent diffusion model... fine-tuned using driving scene imagery."
  - [section] "We leverage the Low Rank Adaptation (LoRA) technique to achieve rapid training and enhanced flexibility."
  - [corpus] Weak - no direct evidence in neighbors; paper is the first to apply LoRA fine-tuning to BEV-to-street view generation.
- Break condition: If the driving dataset is too small or unrepresentative, the fine-tuned model may overfit or fail to generalize to new scenarios.

### Mechanism 3
- Claim: Explicit viewpoint encoding in prompts allows the diffusion model to generate consistent, view-aligned images across multiple camera perspectives.
- Mechanism: Viewpoint-specific prompts (e.g., "cam0") are used during fine-tuning to associate each camera's unique perspective with its corresponding image generation task. This ensures that objects and roads appear correctly oriented for each view.
- Core assumption: The CLIP text encoder can distinguish and associate viewpoint prompts with their corresponding visual features.
- Evidence anchors:
  - [abstract] "to ensure accurate viewpoints across various camera perspectives, we fine-tune the network."
  - [section] "To specify viewpoints, we use alphanumeric designations... to prevent any overlap with existing concepts within the pretrained CLIP text encoder."
  - [corpus] Weak - no direct evidence in neighbors; paper is the first to apply viewpoint-specific prompting to BEV generation.
- Break condition: If viewpoint prompts are not distinct enough or the model fails to learn the association, generated images may have incorrect object orientations or inconsistent road layouts.

## Foundational Learning

- Concept: Camera geometry and coordinate transformations (perspective projection, homography)
  - Why needed here: The system must project BEV semantic maps into camera views using intrinsics, extrinsics, and height estimates.
  - Quick check question: Given a 3D point in world coordinates, a camera's intrinsic matrix K, and its extrinsic rotation R and translation t, what is the projected 2D pixel coordinate in the image?

- Concept: Diffusion models and latent space conditioning
  - Why needed here: The system uses a latent diffusion model (Stable Diffusion) conditioned on semantic maps and viewpoint prompts to generate realistic street-view images.
  - Quick check question: In a diffusion model, what is the role of the noise schedule and how does it affect the quality of the generated image?

- Concept: Semantic segmentation and shape refinement
  - Why needed here: The system must generate accurate semantic maps in camera view, which requires refining the initial projection with a CNN.
  - Quick check question: What is the purpose of the shape refinement network in the BEV-to-street view generation pipeline, and how does it improve the final image quality?

## Architecture Onboarding

- Component map: BEV map → height estimation → perspective projection → shape refinement → semantic conditioning → diffusion denoising → street-view image

- Critical path: BEV map → height estimation → perspective projection → shape refinement → semantic conditioning → diffusion denoising → street-view image

- Design tradeoffs:
  - Using a height prior vs. learning height from data: Height prior is simpler but may introduce inaccuracies for tall objects like trucks.
  - LoRA fine-tuning vs. full fine-tuning: LoRA is more efficient but may limit the extent of adaptation to the driving domain.
  - Viewpoint-specific prompts vs. continuous viewpoint encoding: Discrete prompts are easier to implement but less flexible than continuous encoding.

- Failure signatures:
  - Poor semantic map quality: Objects appear distorted or misplaced in the final image.
  - Inconsistent viewpoints: Objects and roads have incorrect orientations across different camera views.
  - Style mismatch: Generated images look unrealistic or don't match the driving scene style.

- First 3 experiments:
  1. Generate images with and without shape refinement to assess its impact on semantic accuracy.
  2. Compare images generated with and without viewpoint-specific prompts to evaluate viewpoint consistency.
  3. Ablation study: Remove LoRA fine-tuning and generate images to see the effect on style and viewpoint alignment.

## Open Questions the Paper Calls Out
- How can large-scale pretrained diffusion models be effectively adapted to generate multi-view panoramic images with extended aspect ratios, as opposed to standard aspect ratios?
- What is the impact of using different height distributions for vehicles in the initial projection phase on the quality and accuracy of the generated street-view images?
- How does the fine-tuning process with viewpoint-specific loss affect the generalization ability of the model across different driving scenarios and camera viewpoints?

## Limitations
- Height prior of 1.5-2 meters for vehicles is a significant simplification that may not generalize to trucks, buses, or other vehicle types
- LoRA fine-tuning strategy may not capture full complexity of driving domain given relatively small dataset (1,000 scenes)
- Evaluation metrics (FID and mIOU) measure different aspects of quality but don't directly assess semantic accuracy of generated objects or consistency of multi-view outputs

## Confidence
- **High Confidence**: The two-stage pipeline architecture is technically sound and the basic implementation using existing tools (Stable Diffusion, LoRA) is feasible. The camera projection mathematics are well-established.
- **Medium Confidence**: The effectiveness of the shape refinement network is plausible but depends on training quality. The viewpoint-specific prompting strategy is reasonable but may face practical challenges with CLIP's text encoder.
- **Low Confidence**: The generalizability of the approach to diverse driving conditions and the actual quality of generated images beyond the reported metrics remain uncertain without independent reproduction.

## Next Checks
1. **Geometry Accuracy Test**: Generate a set of street-view images with known BEV inputs (e.g., simple geometric patterns) and measure the actual 3D-to-2D projection accuracy of vehicles and roads to validate the height prior assumption.

2. **View Consistency Test**: Generate multi-view images from the same BEV map and quantitatively measure object consistency across views (e.g., vehicle positions, road boundaries) to verify the viewpoint encoding effectiveness.

3. **Cross-Weather Generalization Test**: Fine-tune the model on a subset of weather conditions (e.g., clear weather only) and evaluate performance on held-out weather conditions (e.g., rain, snow) to assess robustness and overfitting.