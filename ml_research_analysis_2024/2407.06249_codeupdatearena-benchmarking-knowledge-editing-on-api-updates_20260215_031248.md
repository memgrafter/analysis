---
ver: rpa2
title: 'CodeUpdateArena: Benchmarking Knowledge Editing on API Updates'
arxiv_id: '2407.06249'
source_url: https://arxiv.org/abs/2407.06249
tags:
- update
- function
- test
- code
- unit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CodeUpdateArena, a benchmark designed to\
  \ evaluate knowledge editing in large language models for code APIs. The task involves\
  \ updating an LLM\u2019s knowledge about API function changes (e.g., adding a boolean\
  \ flag to numpy.argsort) and assessing whether it can solve program synthesis problems\
  \ using the updated functionality without explicit documentation at inference time."
---

# CodeUpdateArena: Benchmarking Knowledge Editing on API Updates

## Quick Facts
- arXiv ID: 2407.06249
- Source URL: https://arxiv.org/abs/2407.06249
- Reference count: 40
- Primary result: Current knowledge editing techniques fail to effectively update code LLMs on API changes, requiring models to use updated functionality without explicit documentation

## Executive Summary
This paper introduces CodeUpdateArena, a benchmark designed to evaluate knowledge editing in large language models when APIs change. The task requires updating an LLM's knowledge about API function changes (such as adding a boolean flag to numpy.argsort) and assessing whether it can solve program synthesis problems using the updated functionality without explicit documentation at inference time. The dataset is synthetically generated using GPT-4, covering 54 functions from seven Python packages with 670 program synthesis examples. Experiments with open-source models (CodeLlama, DeepSeek-Coder) show that prepending documentation or fine-tuning on updates does not effectively incorporate the changes, leaving substantial room for improvement in knowledge editing techniques for code LLMs.

## Method Summary
The benchmark uses GPT-4 to generate synthetic API updates that are atomic and executable, then creates program synthesis examples requiring the updated functionality. The evaluation involves fine-tuning code LLMs using three approaches: fine-tuning on update documentation (FT(U)), fine-tuning on program synthesis examples (FT(PS)), and prepending documentation at inference. Models are evaluated using UPass@k (pass@k only counting solutions that meaningfully use the updated function) and SPass@k (specificity measured by change in HumanEval performance). The study tests three code LLMs (CodeLlama, DeepSeek-Coder, DeepSeek-Coder-v1.5) with LoRA fine-tuning.

## Key Results
- Prepending documentation and fine-tuning approaches fail to effectively internalize API updates in code LLMs
- Models cannot reliably solve program synthesis problems using updated functions without explicit documentation at inference
- Performance on HumanEval decreases substantially after fine-tuning, indicating loss of general capabilities
- Knowledge editing for code APIs is more challenging than text-based knowledge editing due to semantic reasoning requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can generate synthetic API updates that are atomic and executable, enabling benchmark creation without waiting for real-world API changes.
- Mechanism: GPT-4 is prompted with a function path and docstring, then generates an update description, new signature, docstring, and rationale that satisfy constraints (atomicity, non-trivial implementation difference, sensible change).
- Core assumption: GPT-4 can reason about API design principles and generate updates that preserve functionality while introducing meaningful changes.
- Evidence anchors:
  - [abstract] "Our dataset is constructed by first prompting GPT-4 to generate atomic and executable function updates."
  - [section] "We generate the update providing the model only the function path and the function's docstring, obtained from the importlib library."
  - [corpus] "Average neighbor FMR=0.363, average citations=0.0" - weak corpus evidence for this specific generation approach.

### Mechanism 2
- Claim: Code language models fail to internalize API updates through fine-tuning on either update documentation or program synthesis examples alone.
- Mechanism: Fine-tuning on update information (FT(U)) or program synthesis examples (FT(PS)) does not effectively transfer the update knowledge to model parameters, as evidenced by low UPass scores and improved performance only when updates are prepended at inference.
- Core assumption: Fine-tuning methods effective for text knowledge editing do not transfer to code domains where semantic understanding of function behavior is required.
- Evidence anchors:
  - [abstract] "Our experiments show that prepending documentation of the update to open-source code LLMs... does not allow them to incorporate changes for problem solving"
  - [section] "Performance on HumanEval (SPass) decreases substantially on CodeLlama and DeepSeekCoder-v1.5 with these fine-tuning setups."
  - [corpus] "Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations" - related work but weak direct evidence for this specific mechanism.

### Mechanism 3
- Claim: Knowledge editing for code requires semantic reasoning about function behavior, not just syntactic changes, making it more challenging than text-based knowledge editing.
- Mechanism: Code LLMs must understand how new arguments or modified semantics affect function output and usage patterns, rather than simply reproducing updated syntax as in text knowledge editing.
- Core assumption: The complexity of reasoning about code semantics creates a higher barrier for knowledge editing compared to factual knowledge updates.
- Evidence anchors:
  - [abstract] "Compared to knowledge editing for facts encoded in text, success here is more challenging: a code LLM must correctly reason about the semantics of the modified function rather than just reproduce its syntax."
  - [section] "Our experimental evaluation focuses on how existing small-scale LLMs... perform at this update setting when combined with existing knowledge updating techniques."
  - [corpus] "AnyEdit: Edit Any Knowledge Encoded in Language Models" - suggests general difficulty but weak specific evidence for code vs text comparison.

## Foundational Learning

- Concept: Knowledge editing for language models
  - Why needed here: The paper builds on existing knowledge editing techniques but applies them to the code domain, requiring understanding of how these methods work and their limitations.
  - Quick check question: What is the key difference between knowledge editing for facts (e.g., prime minister) and knowledge editing for code APIs?

- Concept: Program synthesis evaluation
  - Why needed here: The benchmark uses program synthesis examples to test whether models can apply updated API knowledge, requiring understanding of evaluation metrics like pass@k.
  - Quick check question: How does UPass@k differ from standard pass@k, and why is this distinction important for evaluating knowledge editing?

- Concept: Synthetic data generation with LLMs
  - Why needed here: The benchmark relies on GPT-4 to generate synthetic API updates and program synthesis examples, requiring understanding of prompt engineering and quality control.
  - Quick check question: What are the key constraints imposed on GPT-4 when generating API updates, and why are these constraints important?

## Architecture Onboarding

- Component map: GPT-4 (synthetic data generation) -> Code LLMs (CodeLlama, DeepSeek-Coder) -> LoRA fine-tuning -> Evaluation framework (UPass@k, SPass@k) -> Unit test infrastructure
- Critical path: Generate API updates → Generate program synthesis examples → Fine-tune code LLMs → Evaluate with UPass@k metric → Analyze results
- Design tradeoffs: Synthetic updates provide scalability and control but lack real-world complexity; fine-tuning on documentation vs examples represents different knowledge transfer strategies with varying effectiveness.
- Failure signatures: Low UPass scores indicate knowledge editing failure; decreased SPass scores indicate loss of general capabilities; inability to use updated functions despite prepended context suggests incomplete knowledge internalization.
- First 3 experiments:
  1. Generate a small set of API updates using GPT-4 and manually verify they meet atomicity and non-triviality constraints
  2. Fine-tune a code LLM on update documentation only and evaluate on corresponding program synthesis examples
  3. Fine-tune the same LLM on program synthesis examples only and compare performance to documentation-only fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does knowledge editing for code APIs compare to knowledge editing for textual facts in terms of effectiveness and specificity preservation?
- Basis in paper: [explicit] The paper states that "Compared to knowledge editing for facts encoded in text, success here is more challenging: a code LLM must correctly reason about the semantics of the modified function rather than just reproduce its syntax."
- Why unresolved: The paper shows that current knowledge editing techniques perform poorly on code API updates, but doesn't directly compare these results to similar techniques on textual facts to quantify the relative difficulty.
- What evidence would resolve it: A controlled experiment applying the same knowledge editing techniques (prepended documentation, fine-tuning) to both code API updates and textual facts, measuring UPass@1 and SPass scores for each domain.

### Open Question 2
- Question: What is the optimal balance between updating on function documentation versus usage examples for effective knowledge incorporation?
- Basis in paper: [explicit] The paper tests three methods: prepending documentation, fine-tuning on documentation (FT(U)), and fine-tuning on usage examples (FT(PS)), finding mixed results across different models.
- Why unresolved: While the paper shows that neither method alone is fully effective, it doesn't explore intermediate approaches like combining documentation with a small number of examples, or curriculum learning approaches that start with documentation and gradually incorporate examples.
- What evidence would resolve it: Systematic experiments varying the ratio of documentation to examples in training, testing different scheduling strategies for presenting this information, and measuring both UPass and SPass metrics.

### Open Question 3
- Question: Are there specific types of API updates (e.g., adding arguments vs. modifying semantics) that are inherently more difficult for models to learn?
- Basis in paper: [inferred] The paper covers 17 update types but doesn't analyze performance differences across these types, though Figure 5 shows different frequencies of update types in the dataset.
- Why unresolved: The paper presents aggregate results across all update types but doesn't break down performance by update category to identify which transformations pose the greatest challenges for knowledge editing.
- What evidence would resolve it: Detailed analysis of UPass@1 scores for each update type, examining whether certain semantic changes (like modifying function behavior) are harder to learn than syntactic changes (like adding arguments).

## Limitations

- Synthetic API updates generated by GPT-4 may not capture the full complexity of real-world API changes
- Fine-tuning approaches lead to decreased performance on HumanEval, suggesting loss of general capabilities
- The benchmark covers only 54 functions from seven Python packages, limiting scope of evaluation

## Confidence

- **High Confidence**: The experimental finding that prepending documentation and fine-tuning approaches fail to effectively internalize API updates is well-supported by multiple experiments across different models and datasets.
- **Medium Confidence**: The claim that code knowledge editing is fundamentally more challenging than text knowledge editing due to semantic reasoning requirements is plausible but requires more direct comparative evidence.
- **Low Confidence**: The assumption that GPT-4-generated updates perfectly simulate real API evolution is difficult to verify without extensive testing against actual API changes.

## Next Checks

1. **Real-World Validation**: Apply the best-performing fine-tuning approach from CodeUpdateArena to a model that has been trained on pre-existing code, then test its ability to adapt to actual API changes in libraries like NumPy or Pandas that have evolved in recent versions.

2. **Cross-Domain Transferability**: Evaluate whether knowledge editing techniques successful in CodeUpdateArena transfer to other programming languages and API ecosystems (e.g., JavaScript/TypeScript with Node.js modules) to assess the generality of the findings.

3. **Long-Tail Update Analysis**: Systematically analyze the subset of API updates in CodeUpdateArena where models perform particularly poorly to identify whether these represent inherently more complex semantic changes or limitations in the synthetic data generation process.