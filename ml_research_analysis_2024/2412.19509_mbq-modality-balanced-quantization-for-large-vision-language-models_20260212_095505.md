---
ver: rpa2
title: 'MBQ: Modality-Balanced Quantization for Large Vision-Language Models'
arxiv_id: '2412.19509'
source_url: https://arxiv.org/abs/2412.19509
tags:
- quantization
- eb110
- tokens
- language
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a significant performance issue in quantizing
  large vision-language models (VLMs): the sensitivity of vision and language tokens
  to quantization differs greatly, but existing methods treat them equally, leading
  to accuracy loss. The authors propose Modality-Balanced Quantization (MBQ), which
  uses gradients of the loss function to weight the reconstruction loss differently
  for vision and language tokens during calibration.'
---

# MBQ: Modality-Balanced Quantization for Large Vision-Language Models

## Quick Facts
- arXiv ID: 2412.19509
- Source URL: https://arxiv.org/abs/2412.19509
- Authors: Shiyao Li; Yingchun Hu; Xuefei Ning; Xihui Liu; Ke Hong; Xiaotao Jia; Xiuhong Li; Yaqi Yan; Pei Ran; Guohao Dai; Shengen Yan; Huazhong Yang; Yu Wang
- Reference count: 40
- Primary result: Up to 4.4% accuracy improvement under W3A16 and 11.6% under W4A8 quantization for VLMs

## Executive Summary
This paper addresses the challenge of post-training quantization for large vision-language models (VLMs) by identifying a critical sensitivity imbalance between vision and language tokens during quantization. The authors discover that language tokens are significantly more sensitive to quantization noise than vision tokens, but existing methods treat them equally, leading to accuracy degradation. To address this, they propose Modality-Balanced Quantization (MBQ), which incorporates gradient-based sensitivity indicators into the calibration process to achieve better quantization parameters and improved task accuracy.

## Method Summary
MBQ modifies the post-training quantization process by using gradients of the loss function with respect to token features as sensitivity indicators. During calibration, MBQ incorporates these modality-specific gradients into the reconstruction loss, giving higher weight to language tokens and lower weight to vision tokens when searching for quantization parameters. This modality-balanced approach minimizes the overall impact on model accuracy. Additionally, the authors implement a fused W3 GPU kernel that combines dequantization with GEMV operations, achieving significant speedup by reducing memory access overhead.

## Key Results
- MBQ improves task accuracy by up to 4.4% under W3A16 and 11.6% under W4A8 quantization compared to state-of-the-art baselines
- Extensive experiments on 7B-70B VLMs demonstrate consistent performance improvements across different model scales
- The fused W3 GPU kernel achieves 1.4× speedup on LLaVA-onevision-7B on RTX 4090
- MBQ shows improvements across multiple VLM architectures including LLaVA-onevision, InternVL2, and Qwen2-VL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different modalities in VLMs exhibit different sensitivity to quantization noise, with vision tokens being significantly less sensitive than language tokens.
- Mechanism: The method leverages gradients of the loss function with respect to token features as sensitivity indicators. During calibration, MBQ incorporates these modality-specific gradients into the reconstruction loss, giving higher weight to language tokens and lower weight to vision tokens when searching for quantization parameters.
- Core assumption: The gradient of the loss function with respect to token features serves as a reliable proxy for sensitivity to quantization perturbations.
- Evidence anchors:
  - [abstract] "we discover that there is a significant difference in sensitivity between language and vision tokens in large VLMs"
  - [section] "using an image-caption pair from the COCO dataset as the input, we visualize the loss gradient w.r.t. the output feature of the 13th layer in LLaVA-onevision-7B. We can see that the average absolute gradient value of the language token features is over 10× larger than that of vision token features"
  - [corpus] Weak evidence - related works focus on general quantization techniques but don't address modality-specific sensitivity differences
- Break condition: If the gradient-based sensitivity measure doesn't correlate with actual quantization performance impact, or if the sensitivity difference varies unpredictably across layers or models.

### Mechanism 2
- Claim: Balancing reconstruction loss across modalities during calibration leads to better quantization parameters than treating all tokens equally.
- Mechanism: MBQ modifies the channel-wise equalization optimization objective by weighting the reconstruction error for vision and language tokens differently based on their sensitivities. This creates a modality-balanced calibration process that minimizes the overall impact on model accuracy.
- Core assumption: Minimizing a weighted reconstruction loss that accounts for modality sensitivity will result in quantization parameters that better preserve task accuracy.
- Evidence anchors:
  - [abstract] "MBQ incorporates the different sensitivities across modalities during the calibration process to minimize the reconstruction loss for better quantization parameters"
  - [section] "Specifically, MBQ uses the gradients of the loss function w.r.t. vision and language token features as the sensitivity indicators. These sensitivity indicators are then incorporated into the reconstruction loss"
  - [corpus] Moderate evidence - related works use channel-wise equalization but don't account for modality differences
- Break condition: If the optimal weighting strategy varies too much across different models or tasks, making it difficult to generalize.

### Mechanism 3
- Claim: The fused W3 GPU kernel combining dequantization with GEMV operations achieves significant speedup by reducing memory access overhead.
- Mechanism: The W3 kernel packs three 3-bit weights into three bytes and fuses the dequantization process with the GEMV computation, reducing memory access latency and improving throughput, especially for large matrix operations.
- Core assumption: Fusing dequantization with GEMV computation reduces memory access overhead sufficiently to achieve meaningful speedup.
- Evidence anchors:
  - [abstract] "we implement a W3 GPU kernel that fuses the dequantization and GEMV operators, achieving a 1.4× speedup on LLaVA-onevision-7B on the RTX 4090"
  - [section] "Specifically, we pack eight 3-bit weights into three bytes, and the fused W3 kernel first loads the W3 weights instead of FP16 weights to reduce memory access overhead"
  - [corpus] Weak evidence - related works focus on quantization but not on specific kernel fusion optimizations
- Break condition: If memory bandwidth is not the bottleneck or if the overhead of fusing operations outweighs the benefits.

## Foundational Learning

- Concept: Post-Training Quantization (PTQ)
  - Why needed here: MBQ builds upon existing PTQ techniques but modifies them to account for modality differences. Understanding PTQ is fundamental to grasping how MBQ improves upon existing methods.
  - Quick check question: What is the key difference between post-training quantization and quantization-aware training?

- Concept: Channel-wise Equalization (CWE)
  - Why needed here: MBQ uses CWE as its underlying mechanism but modifies it with modality-balancing factors. Understanding CWE is essential to understand how MBQ achieves its improvements.
  - Quick check question: How does channel-wise equalization help mitigate quantization errors in transformer models?

- Concept: Gradient-based sensitivity analysis
  - Why needed here: MBQ uses gradients of the loss function as sensitivity indicators for different modalities. Understanding how gradients relate to sensitivity is crucial for understanding the mechanism.
  - Quick check question: Why can gradients of the loss function with respect to token features serve as indicators of sensitivity to perturbations?

## Architecture Onboarding

- Component map: Image → ViT encoder → Vision tokens + Language tokens → VLM (prefill + decode) → Output
- Critical path: Image → ViT encoder → Vision tokens + Language tokens → VLM (prefill + decode) → Output
  The prefill stage processes all input tokens simultaneously, while the decode stage generates output tokens autoregressively.
- Design tradeoffs:
  - Modality-balancing factor selection: Fixed heuristic vs. learned vs. gradient-based
  - Precision levels: W3A16 vs W4A8 vs W4A16 vs W8A8 - tradeoff between accuracy and speed
  - Kernel fusion: Fused W3 kernel vs separate operations - tradeoff between implementation complexity and performance
- Failure signatures:
  - Accuracy degradation on vision-language tasks but not on language-only tasks indicates modality-balancing issues
  - No speedup from W3 kernel suggests memory access is not the bottleneck
  - Randomness or repetition in outputs suggests severe quantization errors
- First 3 experiments:
  1. Run MBQ with a fixed modality-balancing factor (0.1 for vision) and compare to baseline AWQ on a small VLM
  2. Evaluate gradient distributions across layers to validate sensitivity differences between modalities
  3. Profile memory access patterns to confirm that the fused W3 kernel reduces memory bandwidth usage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MBQ perform on VLMs with different architectures beyond transformer-based models?
- Basis in paper: Inferred from the focus on transformer-based VLMs in the experiments
- Why unresolved: The paper only evaluates MBQ on transformer-based VLMs (LLaV A-onevision, InternVL2, Qwen2-VL), leaving its effectiveness on other VLM architectures unexplored
- What evidence would resolve it: Testing MBQ on VLMs with alternative architectures (e.g., Mamba-based VLMs, hybrid CNN-transformer models) and comparing performance to existing quantization methods

### Open Question 2
- Question: What is the impact of MBQ on VLMs when using different calibration datasets beyond COCO caption?
- Basis in paper: Inferred from the limited exploration of calibration datasets in the ablation study
- Why unresolved: The paper only uses COCO caption dataset for calibration and mentions that different datasets may yield different results, but does not systematically explore this
- What evidence would resolve it: Evaluating MBQ with various vision-language datasets (e.g., Flickr30k, Conceptual Captions) and analyzing how dataset choice affects quantization performance

### Open Question 3
- Question: How does MBQ scale with even larger VLMs beyond 70B parameters?
- Basis in paper: Inferred from the focus on 7B-70B parameter models
- Why unresolved: The paper evaluates MBQ on VLMs up to 72B parameters, but does not explore its effectiveness on frontier VLMs with 100B+ parameters
- What evidence would resolve it: Applying MBQ to VLMs with 100B+ parameters and analyzing whether the modality-balancing approach maintains its effectiveness at larger scales

## Limitations
- The effectiveness of gradient-based sensitivity indicators may vary across different model architectures beyond the tested 7B-70B VLMs
- The fixed modality-balancing factor (0.1 for vision) is a heuristic choice that may not be optimal across all scenarios
- Evaluation focuses primarily on image captioning and visual question answering tasks, leaving open questions about performance on other VLM applications

## Confidence
- High Confidence: The core observation that vision and language tokens exhibit different sensitivities to quantization is well-supported by empirical gradient analysis showing 10× differences in gradient magnitudes
- Medium Confidence: The assumption that gradient-based sensitivity indicators reliably predict quantization impact is reasonable but not exhaustively validated
- Low Confidence: The generalizability of the 0.1 modality-balancing factor across different VLM architectures and tasks is uncertain

## Next Checks
1. **Cross-Architecture Sensitivity Analysis**: Evaluate gradient-based sensitivity indicators across different VLM architectures (e.g., Flamingo, BLIP-2, LLaVA-v1.6) and vision encoders (e.g., CLIP, DINOv2) to determine if the 10× sensitivity difference is consistent or architecture-dependent.

2. **Adaptive Balancing Factor Optimization**: Implement and evaluate an adaptive mechanism for selecting the modality-balancing factor during calibration, potentially using validation loss on a held-out subset or automated hyperparameter search.

3. **Kernel Performance Portability Testing**: Port the fused W3 kernel implementation to different GPU architectures (AMD, Apple Silicon) and inference frameworks (OpenVINO, ONNX Runtime) to assess the portability and generalizability of the speedup claims.