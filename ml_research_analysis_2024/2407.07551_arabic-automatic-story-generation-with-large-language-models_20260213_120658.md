---
ver: rpa2
title: Arabic Automatic Story Generation with Large Language Models
arxiv_id: '2407.07551'
source_url: https://arxiv.org/abs/2407.07551
tags:
- story
- stories
- arabic
- prompt
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first LLM-based approach for automatic
  Arabic story generation, targeting Modern Standard Arabic (MSA) and two dialects
  (Egyptian and Moroccan). The authors develop a novel dataset by translating English
  GPT-4-generated stories and generating new Arabic stories with custom prompts using
  GPT-4-Turbo.
---

# Arabic Automatic Story Generation with Large Language Models

## Quick Facts
- arXiv ID: 2407.07551
- Source URL: https://arxiv.org/abs/2407.07551
- Reference count: 6
- Key outcome: Introduces first LLM-based approach for Arabic story generation, achieving competitive performance with smaller models through high-quality instruction-tuned data

## Executive Summary
This paper presents the first large language model (LLM)-based approach for automatic Arabic story generation, targeting Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian and Moroccan). The authors develop a novel dataset by translating English GPT-4-generated stories and generating new Arabic stories with custom prompts using GPT-4-Turbo. They fine-tune AraLLaMa-2-base, a 7B Arabic LLM, using two strategies: direct fine-tuning on GPT-4-generated data and a two-step approach involving translated data followed by GPT-4-generated data. Extensive automatic and human evaluations show that their models outperform strong baselines like GPT-3.5, Command-R, and AceGPT-7B-Chat, despite being an order of magnitude smaller.

## Method Summary
The authors fine-tune AraLLaMa-2-base (7B parameters) using PEFT library with QLoRA, training for 20 epochs with AdamW optimizer (lr=4e-5, batch size=1). They use two datasets: translated English stories from TinyStories (545K samples filtered to 92% similarity) and GPT-4-Turbo-generated stories with custom prompts. Two fine-tuning strategies are explored: direct fine-tuning on GPT-4-generated data (Model A) and a two-step approach with translated data followed by GPT-4-generated data (Model B). Evaluation includes GPT-4 as judge on five criteria (Fluency, Coherence, Instruction Following, Consistency, Variety) and human evaluation by native Arabic speakers.

## Key Results
- Models outperform larger baselines (GPT-3.5, Command-R, AceGPT-7B-Chat) despite being an order of magnitude smaller
- Two-step fine-tuning (Model B) outperforms direct fine-tuning (Model A) across most metrics and dialects
- Dialectal prompt engineering improves dialectal story generation fluency and variety adherence
- Models demonstrate strong performance on both MSA and dialectal story generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompting in dialectal varieties improves dialectal story generation fluency and variety adherence
- Mechanism: LLMs perform better when input prompt matches target dialect, leading to more natural and consistent dialectal output
- Core assumption: LLM has sufficient dialectal data exposure during pretraining
- Evidence anchors:
  - [abstract] "crafted prompts that allow us to generate data well-suited to the Arabic context in both MSA and two Arabic dialects"
  - [section] "GPT-4 is able to generate coherent stories from dialectal prompts"

### Mechanism 2
- Claim: Fine-tuning on translated data followed by GPT-4-generated data improves model performance across dialects
- Mechanism: Two-step fine-tuning provides broader data distribution, helping model learn both MSA and dialectal features more effectively
- Core assumption: Translated data provides complementary signal to synthetic data
- Evidence anchors:
  - [section] "Model B which was exposed to additional training steps on translated data, performs better than Model A"
  - [abstract] "beginning with an analogous synthetic dataset translated from English"

### Mechanism 3
- Claim: Fine-tuning 7B Arabic LLM on high-quality, targeted story data yields performance competitive with larger models
- Mechanism: High-quality instruction-tuned data compensates for smaller model size by providing focused learning signals
- Core assumption: Model architecture and data quality sufficient to capture task-specific nuances despite smaller scale
- Evidence anchors:
  - [abstract] "models outperform strong baselines... even though our models are an order of magnitude smaller"
  - [section] "Both our model A and model B are very competitive with larger models"

## Foundational Learning

- Concept: Arabic morphological and dialectal variation
  - Why needed here: Different Arabic varieties require different linguistic features; models must handle these differences for coherent generation
  - Quick check question: Can you name the three main Arabic varieties used in this work and one key difference between them?

- Concept: Prompt engineering and template design
  - Why needed here: Prompts guide LLM's output; structured templates ensure consistency and coverage of desired features
  - Quick check question: What are the two mandatory features in every prompt according to the paper?

- Concept: Fine-tuning strategies (SFT vs. two-step)
  - Why needed here: Different fine-tuning pipelines affect model adaptation to task and data; understanding trade-offs guides design choices
  - Quick check question: What is the key difference between Model A and Model B in terms of fine-tuning procedure?

## Architecture Onboarding

- Component map: AraLLaMa-2-base (7B Arabic LLM) -> QLoRA adapter layer -> GPT-4 Turbo data generation -> Translation pipeline -> Evaluation pipeline
- Critical path: 1. Generate/collect training data (translated + GPT-4) -> 2. Apply similarity filtering -> 3. Fine-tune AraLLaMa-2-base with SFT (or two-step) -> 4. Evaluate with GPT-4 judge and human raters -> 5. Deploy or iterate
- Design tradeoffs:
  - Model size vs. data quality: Smaller models compensated by high-quality instruction-tuned data
  - Dialect coverage vs. pretraining data: Limited dialectal data may reduce dialectal generation quality
  - Compute budget vs. fine-tuning steps: Two-step fine-tuning improves results but increases training time
- Failure signatures:
  - Poor dialect adherence: Likely due to insufficient dialectal pretraining data or prompt mismatch
  - Low fluency or coherence: Often due to low-quality training data or inadequate fine-tuning steps
  - Overfitting to training prompts: Model may generate repetitive or formulaic stories
- First 3 experiments:
  1. Run SFT on translated data only; evaluate fluency and variety adherence
  2. Run SFT on GPT-4-generated data only; compare against step 1
  3. Run two-step fine-tuning (translated â†’ GPT-4); compare against both prior runs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does quality of Arabic dialect generation vary when using larger models (e.g., 13B or 30B parameters) compared to 7B models tested?
- Basis in paper: [inferred] Computational constraints prevented comparison against larger Arabic LLMs like Jais-30B and AceGPT-13B
- Why unresolved: Computational limitations prevented testing larger models
- What evidence would resolve it: Empirical results from fine-tuning and evaluating larger Arabic LLMs on same dialect generation tasks

### Open Question 2
- Question: What is the impact of increasing dataset size beyond 3,000 samples on performance of fine-tuned models?
- Basis in paper: [explicit] Study used only 3,000 high-quality samples and plans to generate more data with GPT-4o in future
- Why unresolved: Study limited to 3,000 samples, relationship between dataset size and model performance remains unclear
- What evidence would resolve it: Fine-tuning models on progressively larger datasets and comparing performance on same evaluation metrics

### Open Question 3
- Question: How does model performance differ when generating stories for Arabic dialects other than Egyptian and Moroccan?
- Basis in paper: [inferred] Study focused on MSA, Egyptian, and Moroccan dialects, implying performance on other dialects is unexplored
- Why unresolved: Paper did not include other Arabic dialects in study, leaving their performance unknown
- What evidence would resolve it: Fine-tuning and evaluating models on datasets containing stories in additional Arabic dialects

## Limitations

- Dataset Generalization: Reliance on filtered subset of translated English stories and GPT-4-generated stories raises questions about selection bias and representativeness
- Dialect Representation: Coverage limited to only two dialects (Egyptian and Moroccan) out of many Arabic varieties, with unclear effectiveness for other dialects
- Evaluation Methodology: GPT-4 judge may have inherent preferences for MSA over dialects, and human evaluation sample size and demographics are not provided

## Confidence

- High Confidence: Core claim that fine-tuning 7B Arabic LLM on high-quality, targeted story data can yield performance competitive with much larger models
- Medium Confidence: Effectiveness of two-step fine-tuning strategy demonstrated, but exact contribution of each step not fully quantified
- Low Confidence: Claim that dialectal prompt engineering significantly improves dialectal story generation fluency based on preliminary observations without rigorous ablation studies

## Next Checks

1. Analyze pretraining data distribution of AraLLaMa-2-base to quantify representation of Egyptian and Moroccan dialects, and correlate with dialectal generation quality across different prompt types

2. Conduct controlled ablation study comparing two-step fine-tuning approach against single-step fine-tuning on either translated or GPT-4-generated data alone

3. Expand human evaluation to include larger and more diverse group of native Arabic speakers from different dialect backgrounds, explicitly testing for dialect adherence and naturalness