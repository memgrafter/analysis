---
ver: rpa2
title: A Novel Audio Representation for Music Genre Identification in MIR
arxiv_id: '2404.01058'
source_url: https://arxiv.org/abs/2404.01058
tags:
- audio
- music
- spectrograms
- genre
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the effectiveness of Jukebox's deep vector
  quantization (VQ) audio representation for music genre identification compared to
  traditional Mel spectrograms. Using a dataset of 20k tracks and transformer-based
  models, the research finds that Mel spectrograms outperform Jukebox's audio representation
  in genre classification.
---

# A Novel Audio Representation for Music Genre Identification in MIR

## Quick Facts
- arXiv ID: 2404.01058
- Source URL: https://arxiv.org/abs/2404.01058
- Reference count: 31
- Primary result: Mel spectrograms outperform Jukebox's deep VQ audio representation for music genre identification

## Executive Summary
This study investigates the effectiveness of Jukebox's deep vector quantization (VQ) audio representation for music genre identification compared to traditional Mel spectrograms. Using a dataset of 20k tracks and transformer-based models, the research finds that Mel spectrograms outperform Jukebox's audio representation in genre classification. Specifically, the SpectroFormer model using Mel spectrograms achieves superior performance, with F1 scores significantly above baseline levels. In contrast, TokenFormer and CodebookFormer models using deep VQ tokens and codebooks perform only marginally better than random guessing. The results suggest that Jukebox's audio representation does not sufficiently capture the nuances relevant to human auditory perception, particularly when pretrained on a modest dataset. The study concludes that Mel spectrograms remain the preferred audio representation for music genre classification tasks, at least under the tested conditions.

## Method Summary
The study compares three transformer-based models for music genre classification: SpectroFormer using Mel spectrograms, TokenFormer using deep VQ tokens, and CodebookFormer using VQ codebooks. All models are 4-layer transformers pretrained using self-supervised techniques and fine-tuned on the FMA medium-sized dataset (20k tracks, 16 genres). The dataset is split into 80% training, 10% validation, and 10% testing. Models are evaluated using macro-averaged F1 scores, with random guessing achieving a baseline of 0.11. Mel spectrograms are generated using Librosa with specific parameters (Hann window, 512 frame size, 128 hop size, 86 Mel bands), while deep VQ representations are obtained through Jukebox's VQ-VAE encoder.

## Key Results
- SpectroFormer with Mel spectrograms achieves significantly higher F1 scores than baseline (0.11)
- TokenFormer and CodebookFormer with deep VQ tokens/codebooks perform only marginally better than random guessing
- Deep VQ representations fail to capture genre-relevant acoustic cues as effectively as Mel spectrograms
- Small pretraining dataset (20k tracks) may limit deep VQ model performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Jukebox's deep VQ audio representation compresses raw audio into discrete tokens but fails to retain perceptual features critical for human-like genre recognition.
- **Mechanism**: Deep VQ uses vector quantization to map continuous audio signals into a finite codebook of 2048 vectors. The transformer models then process these tokens as discrete sequences. However, this compression prioritizes generative coherence over perceptual fidelity, losing genre-relevant acoustic cues.
- **Core assumption**: Genre classification requires features that match human auditory perception, which Mel spectrograms are designed to capture via logarithmic frequency scaling and amplitude compression.
- **Evidence anchors**:
  - [abstract]: "Jukebox's audio representation does not sufficiently take into account the peculiarities of human hearing perception."
  - [section]: "Jukebox's audio representation does not sufficiently capture the nuances relevant to human auditory perception, particularly when pretrained on a modest dataset of 20k tracks."
  - [corpus]: Weak evidence—no direct comparison of perceptual features between deep VQ and Mel spectrograms in related papers.
- **Break condition**: If the dataset size were scaled up to hundreds of thousands of tracks, deep VQ representations might better approximate perceptual features, or if the VQ codebook were designed with perceptual weighting.

### Mechanism 2
- **Claim**: Transformer models perform better with Mel spectrograms because the pretraining objectives align more naturally with spectrogram structure.
- **Mechanism**: Mel spectrograms have a 2D structure (time x frequency) that matches the masked language modeling objective used in pretraining SpectroFormer. TokenFormer and CodebookFormer, however, operate on 1D token sequences that lose the spatial-temporal locality inherent in spectrograms.
- **Core assumption**: The locality of spectral features (e.g., harmonic patterns over short time windows) is essential for genre classification, and Mel spectrograms preserve this better than flattened token sequences.
- **Evidence anchors**:
  - [section]: "SpectroFormer model using Mel spectrograms achieves superior performance, with F1 scores significantly above baseline levels."
  - [section]: "TokenFormer and CodebookFormer models using deep VQ tokens and codebooks perform only marginally better than random guessing."
  - [corpus]: Assumption: Related papers do not explicitly discuss pretraining alignment, but transfer learning literature supports the importance of structural alignment between pretraining and downstream tasks.
- **Break condition**: If the deep VQ tokens were reshaped into 2D patches or if a different pretraining objective (e.g., token order prediction) were used, performance could improve.

### Mechanism 3
- **Claim**: The small pretraining dataset (20k tracks) is insufficient for deep VQ models to learn genre-relevant abstractions.
- **Mechanism**: Deep VQ representations are highly compressed and abstract; they require large-scale pretraining to capture meaningful musical structure. With only 20k tracks, the VQ encoder underfits, producing tokens that lack genre-specific discriminative information.
- **Core assumption**: Deep generative models like Jukebox require orders of magnitude more data than discriminative models to learn useful representations for classification tasks.
- **Evidence anchors**:
  - [abstract]: "at least when the transformers are pretrained using a very modest dataset of 20k tracks, Jukebox's audio representation is not superior to Mel spectrograms."
  - [section]: "The amount of training data can have a big impact on model performance. Therefore, SOTA suggests that Jukebox's encoder, which was trained on a significantly larger dataset, is capable of efficiently capturing musical elements pertinent to MIR tasks."
  - [corpus]: Assumption: No direct evidence in corpus about dataset size effects, but transfer learning literature supports scaling laws for representation learning.
- **Break condition**: If pretraining data were increased to match Jukebox's original scale (~1.2M tracks), deep VQ representations could surpass Mel spectrograms.

## Foundational Learning

- **Concept**: Vector Quantization (VQ) and codebook-based audio compression
  - **Why needed here**: Deep VQ encodes raw audio into discrete tokens via a learned codebook; understanding this compression is key to why the representation loses perceptual nuance.
  - **Quick check question**: What is the vocabulary size of the VQ codebook used in Jukebox, and how does this limit the expressiveness of the audio representation?

- **Concept**: Mel spectrogram generation and human auditory perception alignment
  - **Why needed here**: Mel spectrograms are designed to mimic human hearing via Mel-scale frequency warping and logarithmic amplitude scaling, making them more suitable for genre classification.
  - **Quick check question**: How does the Mel scale differ from linear frequency scaling, and why does this matter for music genre identification?

- **Concept**: Transformer pretraining objectives and their alignment with input structure
  - **Why needed here**: The study uses masked language modeling for SpectroFormer (2D input) and token prediction for TokenFormer/CodebookFormer (1D input); the mismatch affects learning efficiency.
  - **Quick check question**: Why might a 2D spectrogram input be more naturally suited to masked patch prediction than a 1D token sequence?

## Architecture Onboarding

- **Component map**: Raw audio (44.1 kHz) -> Preprocessing -> Mel spectrogram (SpectroFormer) OR VQ tokens/codebooks (TokenFormer/CodebookFormer) -> 4-layer transformer with BERT-style pretraining -> Fine-tuning for genre classification -> Evaluation (F1 score)

- **Critical path**: Raw audio → preprocessing → transformer model → pretraining → fine-tuning → evaluation

- **Design tradeoffs**:
  - Deep VQ: High compression, generative coherence, but loss of perceptual features
  - Mel spectrograms: Lower compression, better perceptual alignment, but larger input size
  - Pretraining dataset size: Small (20k) limits deep VQ performance; large (1.2M) may enable it

- **Failure signatures**:
  - TokenFormer/CodebookFormer: F1 scores near chance level (0.11), suggesting the tokens lack discriminative information
  - SpectroFormer: Possible overfitting if training loss drops much faster than validation loss
  - Pretraining loss: Large gap between train and validation loss indicates underfitting or data inefficiency

- **First 3 experiments**:
  1. Train TokenFormer/CodebookFormer with a larger pretraining dataset (e.g., 100k+ tracks) and compare F1 scores.
  2. Modify TokenFormer pretraining objective to predict token order or local context instead of masked tokens.
  3. Train SpectroFormer with a smaller frame size (matching VQ-VAE) to test fairness of comparison.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does pretraining deep VQ-based models on larger datasets improve their genre classification performance to match or exceed Mel spectrograms?
- **Basis in paper**: [explicit] The paper notes that the TokenFormer and CodebookFormer were pretrained on a dataset 70 times smaller than Jukebox, suggesting dataset size impacts performance.
- **Why unresolved**: The study only tested pretraining on a small dataset (20k tracks) and found deep VQ-based models underperforming compared to Mel spectrograms. The impact of larger pretraining datasets remains untested.
- **What evidence would resolve it**: Experiments comparing genre classification performance of deep VQ-based models pretrained on datasets of varying sizes, including datasets comparable to Jukebox's training data, would determine if larger pretraining data improves performance.

### Open Question 2
- **Question**: Can incorporating phase information into deep VQ-based audio representations improve music genre classification accuracy?
- **Basis in paper**: [inferred] The paper mentions that phase information is typically disregarded in MIR tasks but is inherently considered by waveform-based models like Jukebox, hinting at potential benefits.
- **Why unresolved**: The study did not explore the impact of phase information on genre classification, focusing instead on comparing token-based and codebook-based deep VQ representations to Mel spectrograms.
- **What evidence would resolve it**: Experiments comparing genre classification performance of deep VQ-based models with and without explicit phase information encoding would reveal if phase information improves accuracy.

### Open Question 3
- **Question**: Are there specific musical genres or characteristics that deep VQ-based representations are better suited to capture compared to Mel spectrograms?
- **Basis in paper**: [explicit] The paper found that deep VQ-based models performed marginally better than random guessing in genre classification, suggesting limitations in capturing genre-specific nuances.
- **Why unresolved**: The study used a macro-averaged F1 score, treating all genres equally, and did not analyze performance variations across different genres.
- **What evidence would resolve it**: A detailed analysis of genre-wise F1 scores for deep VQ-based models and Mel spectrograms would reveal if certain genres or musical characteristics are better represented by deep VQ.

## Limitations
- Conclusions based on a single dataset (FMA medium) and fixed transformer architectures, limiting generalizability
- Performance gap may be influenced by uncontrolled factors like pretraining dataset size and model architecture specifics
- Claims about deep VQ representations lacking genre-relevant information are based solely on F1 scores without feature importance analysis
- Assumption that larger pretraining datasets would improve deep VQ performance is speculative and not empirically validated

## Confidence
- **High Confidence**: The finding that SpectroFormer with Mel spectrograms outperforms TokenFormer and CodebookFormer with deep VQ representations on the FMA medium dataset.
- **Medium Confidence**: The assertion that Mel spectrograms are more suitable for genre classification due to their alignment with human auditory perception, based on the study's results and general knowledge of auditory modeling.
- **Low Confidence**: The claim that deep VQ representations will underperform Mel spectrograms in all MIR tasks unless trained on much larger datasets, as this is extrapolated from a single experiment.

## Next Checks
1. **Dataset Scaling Experiment**: Replicate the study using progressively larger subsets of the FMA dataset (e.g., 50k, 100k tracks) to empirically test whether deep VQ performance improves with more pretraining data.
2. **Feature Ablation Analysis**: Perform ablation studies on deep VQ tokens and Mel spectrograms to identify which frequency bands or token types contribute most to genre classification, validating the claim about perceptual feature retention.
3. **Cross-Dataset Evaluation**: Test the same models on a different MIR dataset (e.g., GTZAN or MagnaTagATune) to assess whether the observed performance gap is consistent across datasets and tasks.