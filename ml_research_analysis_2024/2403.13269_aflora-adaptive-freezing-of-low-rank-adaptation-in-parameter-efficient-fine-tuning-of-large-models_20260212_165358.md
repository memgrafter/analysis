---
ver: rpa2
title: 'AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning
  of Large Models'
arxiv_id: '2403.13269'
source_url: https://arxiv.org/abs/2403.13269
tags:
- lora
- freezing
- trainable
- aflora
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AFLoRA, an adaptive freezing approach for low-rank
  adaptation in parameter-efficient fine-tuning of large language models. AFLoRA starts
  with low-rank trainable projection matrices and feature transformation vectors,
  then incrementally freezes the projection matrices based on a novel freezing score
  to reduce computation and alleviate overfitting.
---

# AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models

## Quick Facts
- arXiv ID: 2403.13269
- Source URL: https://arxiv.org/abs/2403.13269
- Reference count: 7
- Key outcome: Achieves 0.85% average improvement on GLUE benchmark while requiring up to 9.5x fewer trainable parameters compared to LoRA

## Executive Summary
AFLoRA introduces an adaptive freezing mechanism for LoRA-based parameter-efficient fine-tuning of large language models. The method starts with low-rank trainable projection matrices and feature transformation vectors, then incrementally freezes projection matrices based on a novel freezing score. This approach reduces computation, alleviates overfitting, and achieves state-of-the-art performance with significant efficiency gains on the GLUE benchmark.

## Method Summary
AFLoRA builds on LoRA by introducing adaptive freezing of low-rank projection matrices during fine-tuning. The method maintains both projection matrices (loraA, loraB) and feature transformation vectors (sd, sb) as trainable initially at low rank. A freezing score based on gradient magnitude and uncertainty is computed for each projection matrix, and matrices are progressively frozen using a cubic schedule once their scores fall below a threshold. This approach contrasts with ELoRA, which freezes projection matrices from the start, and LoRA, which keeps all parameters trainable throughout.

## Key Results
- Achieves 0.85% average improvement on GLUE benchmark compared to standard LoRA
- Requires up to 9.5x fewer trainable parameters while maintaining or improving performance
- Yields up to 1.86x improvement in runtime compared to similar PEFT methods
- Demonstrates effective freezing across all BERT layers with task-specific patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive freezing of LoRA projection matrices reduces overfitting while maintaining performance
- Mechanism: The method starts with low-rank trainable projection matrices and feature transformation vectors, then incrementally freezes projection matrices based on a novel freezing score that acts as a proxy for trainability requirement. This allows the model to learn important adaptations initially, then freeze less critical parameters to reduce computation and prevent overfitting.
- Core assumption: The freezing score accurately reflects which projection matrices are no longer providing significant value to the model's performance
- Evidence anchors:
  - [abstract]: "Based on a novel freezing score, we then incrementally freeze these projection matrices during fine-tuning to reduce the computation and alleviate over-fitting"
  - [section 4]: "This score quantifies the degree to which weights vary throughout the training process. Consequently, when the expected changes to the weights become negligible, we may consider them to be frozen, thereby saving computational resources and energy"
- Break condition: If the freezing score fails to accurately identify which parameters can be safely frozen, performance will degrade or the computational savings will be insufficient

### Mechanism 2
- Claim: Starting with low-rank trainable paths and gradually freezing provides better performance than freezing from the start
- Mechanism: AFLoRA begins with both projection matrices and feature transformation vectors trainable at low rank, trains for initial epochs, then progressively freezes projection matrices. This contrasts with ELoRA which freezes projection matrices from the start, requiring higher rank for frozen matrices.
- Core assumption: The initial training period allows projection matrices to learn important adaptations that would be missed if frozen from the start
- Evidence anchors:
  - [section 3]: "Based on this insight, we present AFLoRA, which starts with a low-rank trainable path that includes projection matrices and feature transformation vectors and trains the path for some epochs. We then gradually freeze the projection matrices"
  - [section 6]: Table 4 shows that keeping projection matrices trainable yields better average performance compared to keeping them frozen throughout
- Break condition: If the initial training period is too short or too long, the method may either miss important adaptations or waste computation on unnecessary training

### Mechanism 3
- Claim: Freezing only projection matrices while keeping feature transformation vectors trainable provides optimal balance of efficiency and performance
- Mechanism: AFLoRA freezes projection matrices but keeps the two feature transformation vectors (sd and sb) trainable throughout fine-tuning. This reduces trainable parameters while maintaining the ability to adapt feature transformations.
- Core assumption: Feature transformation vectors are more critical for downstream task adaptation than the projection matrices themselves
- Evidence anchors:
  - [section 4]: "We keep both the projection matrices (loraA and loraB) and vectors trainable at the beginning and keep the rank very low"
  - [section 6]: Table 4 shows that keeping both trainable yields better performance than freezing projection matrices
- Break condition: If feature transformation vectors are not sufficient to adapt to downstream tasks without projection matrices, performance will suffer

## Foundational Learning

- Concept: Low-rank adaptation (LoRA) and parameter-efficient fine-tuning (PEFT)
  - Why needed here: AFLoRA builds directly on LoRA by adding adaptive freezing to the low-rank adaptation matrices. Understanding how LoRA works is essential to grasp AFLoRA's innovations.
  - Quick check question: How does LoRA modify the pre-trained weights during fine-tuning, and what are the roles of the down-projection and up-projection matrices?

- Concept: Freezing/unfreezing parameters during training
  - Why needed here: AFLoRA's core innovation is the adaptive freezing mechanism. Understanding why and how parameters are frozen during training is crucial.
  - Quick check question: What is the purpose of freezing parameters during training, and what are the risks of freezing too early or too late?

- Concept: Gradient-based sensitivity measures for pruning and freezing
  - Why needed here: AFLoRA uses a freezing score based on gradient information to determine which parameters to freeze. This is similar to sensitivity-based pruning methods.
  - Quick check question: How do sensitivity measures like magnitude and gradient help identify which parameters can be pruned or frozen?

## Architecture Onboarding

- Component map: AFLoRA consists of a LoRA module with four components: down-projection linear layer (loraA), up-projection linear layer (loraB), and two feature transform vectors (sd and sb). The full operation for a layer is: Y = W0X + ΛbBlΛdAlX, where Al and Bl are trainable LoRA tensors, Λd and Λb are feature transform vectors, and W0 is the frozen pre-trained weight.
- Critical path: The critical path is the forward pass through the LoRA module, where the input X is processed through the down-projection, feature transformation, and up-projection stages. The freezing score computation and parameter freezing are additional operations that occur during training but don't affect the forward pass.
- Design tradeoffs: The main tradeoff is between performance and efficiency. Starting with more parameters trainable (low-rank matrices and feature vectors) allows better adaptation but increases computation. Gradually freezing parameters reduces computation but risks losing important adaptations if frozen too early.
- Failure signatures: Performance degradation when freezing occurs too early, insufficient computational savings if freezing is too conservative, and instability if the freezing score is not well-calibrated.
- First 3 experiments:
  1. Implement the basic LoRA module with trainable down-projection, up-projection, and feature transform vectors, verify forward pass and parameter count
  2. Implement the freezing score computation based on gradient magnitude and uncertainty, verify it produces sensible values
  3. Implement the adaptive freezing schedule with cubic progression, verify parameters are being frozen according to the schedule

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal freezing schedule (ti and tf) for different model architectures and datasets?
- Basis in paper: [inferred] The paper mentions specific ti and tf values used for experiments, but does not explore the impact of varying these parameters.
- Why unresolved: The authors acknowledge that these values were chosen based on their experiments, but a more comprehensive study across different architectures and datasets could reveal whether these values are universally optimal or if they should be tuned for specific cases.
- What evidence would resolve it: Experiments varying ti and tf values across different model architectures and datasets, and analyzing the impact on performance and efficiency.

### Open Question 2
- Question: How does the choice of freezing score metric impact the performance of AFLoRA across different datasets and tasks?
- Basis in paper: [explicit] The paper presents an ablation study comparing three different freezing score metrics, but does not provide a comprehensive analysis of their performance across various datasets and tasks.
- Why unresolved: While the paper shows that the adopted freezing score metric generally performs well, it does not explore its performance across different datasets and tasks, leaving open the possibility that other metrics might be more suitable for certain scenarios.
- What evidence would resolve it: Experiments using AFLoRA with different freezing score metrics across a wide range of datasets and tasks, and comparing the results to determine the most effective metric for each case.

### Open Question 3
- Question: What is the impact of freezing the projection matrices in the attention layers on the overall performance of AFLoRA?
- Basis in paper: [explicit] The paper mentions that the projection matrices in the attention layers are kept frozen to random values, but does not provide a detailed analysis of the impact of this decision.
- Why unresolved: The authors state that keeping the PMs frozen in the attention layers yields better performance, but they do not explore the reasons behind this or the potential impact of keeping them trainable.
- What evidence would resolve it: Experiments comparing the performance of AFLoRA with and without trainable projection matrices in the attention layers, and analyzing the differences to understand the role of these layers in the fine-tuning process.

## Limitations
- Implementation details for freezing score computation are underspecified, making exact reproduction challenging
- Comparison limited to specific rank settings (r=8 for LoRA vs r=4 for AFLoRA) without exploring full hyperparameter space
- Critical hyperparameters (β1, β2, ti, tf) are not reported, preventing exact replication

## Confidence
- **High confidence** in the core mechanism: Adaptive freezing of LoRA projection matrices based on a freezing score derived from gradient information
- **Medium confidence** in the superiority claims: Improvements shown are consistent but comparisons are limited to specific rank settings
- **Low confidence** in the freezing score implementation: The paper mentions using smoothed gradients and uncertainty tensors but does not provide sufficient detail to replicate the exact computation

## Next Checks
1. **Freezing score sensitivity analysis**: Implement AFLoRA with varying β1 and β2 values to determine how sensitive the freezing schedule is to these hyperparameters and identify optimal ranges for different model sizes and tasks.
2. **Layer-wise freezing pattern validation**: Track which layers get frozen at different stages of training across multiple runs to verify the consistency of the freezing pattern and identify any task-specific or layer-specific variations.
3. **Rank-accuracy tradeoff verification**: Conduct experiments comparing AFLoRA at different rank settings (r=2, 4, 8, 16) to quantify the exact relationship between rank, parameter efficiency, and downstream task performance across the GLUE benchmark.