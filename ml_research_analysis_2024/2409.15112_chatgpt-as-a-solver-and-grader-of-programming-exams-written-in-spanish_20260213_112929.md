---
ver: rpa2
title: ChatGPT as a Solver and Grader of Programming Exams written in Spanish
arxiv_id: '2409.15112'
source_url: https://arxiv.org/abs/2409.15112
tags:
- chatgpt
- question
- programming
- also
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChatGPT's performance as a solver of programming tasks written
  in Spanish is limited to simple coding exercises, achieving a 65% score overall.
  The model struggles with complex problems involving algorithmic reasoning, data
  structures, and computational complexity.
---

# ChatGPT as a Solver and Grader of Programming Exams written in Spanish

## Quick Facts
- arXiv ID: 2409.15112
- Source URL: https://arxiv.org/abs/2409.15112
- Authors: Pablo Saborido-Fernández; Marcos Fernández-Pichel; David E. Losada
- Reference count: 4
- Key outcome: ChatGPT achieves 65% score solving Spanish programming exams but overestimates student solutions by 38% when grading

## Executive Summary
This study evaluates ChatGPT's performance as both a solver and grader of real programming exams from a first-year Computer Science course taught in Spanish. The model achieved a 65% score overall when solving the exam questions, performing well on simple coding tasks but struggling significantly with complex problems involving algorithmic reasoning, data structures, and computational complexity. When acting as a grader for student solutions, ChatGPT demonstrated a substantial bias, assigning scores 38% higher on average than human instructors, particularly struggling to evaluate abstract data type specifications and recursive functions accurately.

## Method Summary
The study used a real programming exam from a Spanish university's first-year Computer Science course containing 7 questions. Two prompt variants (Simple and Complex) were tested using OpenAI's gpt-3.5-turbo API. For the solver evaluation, ChatGPT's responses were graded by a human instructor using official exam criteria. For the grader evaluation, ChatGPT assigned scores to 5 sample student exams with varying quality levels, and these scores were compared against instructor grades. The exam questions covered topics including C programming, abstract data types, complexity analysis, and recursive functions.

## Key Results
- ChatGPT achieved 65% score overall as a solver, performing well on simple coding tasks but poorly on complex reasoning problems
- As a grader, ChatGPT overestimated solution quality by 38% compared to human instructors
- The model struggled significantly with abstract data type specifications and recursive function evaluation
- Complex prompts showed no performance improvement over simple prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT performs well on simple coding tasks but struggles with complex reasoning problems.
- Mechanism: The model's training data includes abundant simple code examples but limited complex problem-solving scenarios, leading to good pattern matching for basic tasks but poor performance on novel or abstract reasoning.
- Core assumption: Performance correlates with the frequency and quality of similar examples in training data.
- Evidence anchors:
  - [abstract] "ChatGPT's performance as a solver of programming tasks written in Spanish is limited to simple coding exercises, achieving a 65% score overall. The model struggles with complex problems involving algorithmic reasoning, data structures, and computational complexity."
  - [section] "Three of them were mainly coding tasks (#3, #5, #6) and two of them (#2, #4) required some sort of reasoning but they refer to well-known computing examples (Fibonacci or list search)."
- Break condition: When tasks require novel algorithmic strategies not well-represented in training data or involve abstract data type specifications with formal notation.

### Mechanism 2
- Claim: ChatGPT overestimates the quality of student solutions when acting as a grader.
- Mechanism: The model's generative nature leads to leniency in evaluation, treating partial credit generously and missing subtle errors that human instructors catch.
- Core assumption: Grading requires discriminative rather than generative capabilities, which LLMs are less optimized for.
- Evidence anchors:
  - [abstract] "As a grader, ChatGPT significantly overestimates the quality of student solutions, assigning scores 38% higher on average than human instructors."
  - [section] "Even low quality solutions, such as the exam that officially got a 38% overall score, were assigned very good scores."
- Break condition: When evaluating solutions requiring deep understanding of algorithmic complexity or formal specification correctness.

### Mechanism 3
- Claim: Prompt engineering has minimal impact on ChatGPT's performance for these programming tasks.
- Mechanism: The model's reasoning capabilities are limited by its fundamental understanding rather than instruction format, making complex prompts no more effective than simple ones.
- Core assumption: Prompt sophistication cannot compensate for lack of domain knowledge in training data.
- Evidence anchors:
  - [section] "The first noticeable result is that, for both variants, the model achieved a score above the required threshold to pass the final exam. A second interesting result is that the use of a complex prompt did not help the model."
  - [section] "One might argue that matching human performance is profoundly meaningful. However, we see here two main sources of concern."
- Break condition: When prompts include specific domain knowledge injection or when task complexity exceeds model's reasoning threshold regardless of prompt format.

## Foundational Learning

- Concept: Abstract Data Type (ADT) specification and formal notation
  - Why needed here: Question 1 specifically tests syntactic and semantic formal specification of ADTs, which ChatGPT struggled with significantly.
  - Quick check question: What is the difference between a syntactic specification and a semantic specification of an ADT, and how would you represent each in formal notation?

- Concept: Computational complexity analysis and Big-O notation
  - Why needed here: Multiple questions (#2, #4, #7) require reasoning about worst/best/average case complexity, which ChatGPT handled inconsistently.
  - Quick check question: How would you determine the time complexity of a recursive Fibonacci implementation, and what factors would you consider when comparing different algorithmic strategies?

- Concept: Recursive function implementation and analysis
  - Why needed here: Question 5 involved implementing and evaluating recursive functions, where ChatGPT showed contradictory performance (good at solving but poor at grading).
  - Quick check question: What are the key considerations when analyzing the correctness and efficiency of a recursive function, and how would you trace its execution to verify proper behavior?

## Architecture Onboarding

- Component map: Exam question → Prompt generation → ChatGPT API call → Response collection → Human evaluation (solver) or score assignment (grader) → Performance analysis
- Critical path: Exam question → Prompt generation → ChatGPT API call → Response collection → Human evaluation (solver) or score assignment (grader) → Performance analysis
- Design tradeoffs: Simple prompts reduce complexity but may miss optimization opportunities; complex prompts add overhead without clear benefit based on results. Spanish language adds complexity due to potentially less training data.
- Failure signatures: Consistently poor performance on ADT specification questions, significant grade inflation when grading, inability to handle image-based questions, and occasional language switching (C to Python).
- First 3 experiments:
  1. Test prompt variation systematically across different question types to identify if specific prompt structures work better for certain problem categories.
  2. Implement a confidence scoring mechanism to identify when ChatGPT is uncertain about its responses, particularly for grading tasks.
  3. Create a feedback loop where ChatGPT's solutions are used to generate training examples for improving future performance on similar problems.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the low performance of ChatGPT on abstract data type (ADT) specifications stem from insufficient training data on formal specifications or from the model's inability to understand the formal notation?
- Basis in paper: [explicit] The paper states that ChatGPT struggled with question 1 on ADT specification and suggests this might be related to low availability of ADT examples with proper notations in the training data.
- Why unresolved: The paper only conjectures about the cause without testing whether providing more examples or better prompts would improve performance.
- What evidence would resolve it: A controlled experiment comparing ChatGPT's performance on ADT specifications with and without additional training examples or with enhanced prompts containing more formal specification examples.

### Open Question 2
- Question: Would more sophisticated prompt engineering techniques, such as few-shot learning with multiple examples or chain-of-thought prompting, significantly improve ChatGPT's performance on complex programming problems?
- Basis in paper: [inferred] The paper mentions that more sophisticated prompting strategies are left for future work and that the current study used only two basic prompt types.
- Why unresolved: The paper only tested simple and complex prompts but did not explore more advanced prompting techniques that could potentially enhance performance.
- What evidence would resolve it: Testing ChatGPT with various prompt engineering techniques (few-shot learning, chain-of-thought, self-consistency, etc.) on the same exam questions and comparing results.

### Open Question 3
- Question: Does the Spanish language of the exam questions significantly impact ChatGPT's performance compared to English-language programming problems?
- Basis in paper: [explicit] The paper specifically chose a Spanish exam to evaluate how LLMs perform in languages other than English, acknowledging that performance is often lower than English.
- Why unresolved: The study only tested Spanish-language problems and did not compare results with equivalent English-language problems to isolate the language effect.
- What evidence would resolve it: Conducting the same evaluation with equivalent English-language programming problems and comparing performance metrics between the two languages.

## Limitations

- Single exam from one Spanish university limits generalizability across different programming courses and difficulty levels
- Spanish language constraint may introduce additional performance limitations not directly comparable to English-language evaluations
- Single instructor grading introduces potential subjectivity and bias in evaluation

## Confidence

**High Confidence**: ChatGPT's overall performance on Spanish programming tasks (65% accuracy) and its tendency to overestimate solution quality (38% grade inflation) are well-supported by the data.

**Medium Confidence**: The specific mechanisms explaining why ChatGPT struggles with ADT specifications, recursive functions, and complexity analysis are plausible but not definitively proven.

**Low Confidence**: The claim that prompt engineering has minimal impact on performance requires further investigation with more extensive prompt variations.

## Next Checks

1. **Multi-instructor validation**: Have 3-5 additional instructors grade the same ChatGPT solutions independently to establish inter-rater reliability and identify systematic grading biases.

2. **Cross-linguistic replication**: Repeat the experiment with equivalent programming exams in English and other major languages to isolate whether performance differences are language-specific or reflect fundamental model limitations.

3. **Prompt engineering ablation**: Systematically vary individual components of the complex prompt while keeping other factors constant to identify which prompt elements significantly impact performance on different question types.