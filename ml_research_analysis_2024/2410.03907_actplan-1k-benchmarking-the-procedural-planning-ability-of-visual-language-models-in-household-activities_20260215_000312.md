---
ver: rpa2
title: 'ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language
  Models in Household Activities'
arxiv_id: '2410.03907'
source_url: https://arxiv.org/abs/2410.03907
tags:
- activity
- walk
- activities
- counterfactual
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ActPlan-1K, a benchmark for evaluating the
  procedural planning abilities of vision-language models (VLMs) on household activities.
  The benchmark includes 153 activities and 1,187 instances, each with a natural language
  task description and multiple environment images.
---

# ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities

## Quick Facts
- arXiv ID: 2410.03907
- Source URL: https://arxiv.org/abs/2410.03907
- Authors: Ying Su; Zhan Ling; Haochen Shi; Jiayang Cheng; Yauwai Yim; Yangqiu Song
- Reference count: 40
- Primary result: VLMs struggle with generating human-level procedural plans for both normal and counterfactual household activities

## Executive Summary
This paper introduces ActPlan-1K, a benchmark designed to evaluate the procedural planning capabilities of vision-language models (VLMs) on household activities. The benchmark comprises 153 activities and 1,187 instances, each defined by a natural language task description and multiple environment images from a simulated household environment. The goal is to generate action sequences over objects in the provided scenes. The study evaluates typical VLMs like Claude, Gemini-Pro, and GPT-4V on correctness and commonsense satisfaction, revealing that current VLMs struggle with generating human-level procedural plans for both normal and counterfactual activities. The paper also introduces automatic evaluation metrics using BLEURT to facilitate future research.

## Method Summary
The ActPlan-1K benchmark is constructed using the iGibson2 household activity simulator, where each instance consists of a task description and environment images. Human annotators write gold procedural plans as action sequences over objects. VLMs are then prompted with the task description and images to generate their own plans. These generated plans are evaluated using human annotation for correctness and commonsense satisfaction, and automatic metrics like LCS and BLEURT-based accuracy. The study compares the performance of different VLMs on normal and counterfactual activities, analyzing error types and the impact of visual input.

## Key Results
- VLMs achieve significantly lower scores on counterfactual activities compared to normal activities, indicating difficulty with reasoning about alternative task situations.
- Plan length inversely correlates with VLM planning accuracy, with errors in early steps compounding in longer plans.
- Removing visual input significantly degrades VLM performance, highlighting the importance of multi-modal integration.
- BLEURT-based automatic evaluation shows high correlation with human judgment, suggesting its potential for scalable evaluation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLMs fail at procedural planning due to inability to reliably ground multi-modal inputs into executable action sequences.
- Core assumption: Visual modality is essential for accurate planning.
- Evidence anchors:
  - "current VLMs are still struggling at generating human-level procedural plans"
  - "VLMs achieves much higher scores on normal activities than counterfactual activities"
  - "without images, the correctness or commonsense quality drops while with longer steps"
- Break condition: Fine-tuning on procedural planning with spatial reasoning objectives or using a symbolic planner to post-process VLM output.

### Mechanism 2
- Claim: Counterfactual reasoning exacerbates VLM failure modes in procedural planning.
- Core assumption: VLMs lack training on handling exceptions and adapting plans based on altered object states.
- Evidence anchors:
  - "counterfactual planning that evaluates the model's reasoning ability over alternative task situations are also under exploited"
  - "VLMs achieves much higher scores on normal activities than counterfactual activities"
  - "Mistake of object property/function... are mostly from situated circumstances"
- Break condition: Augmenting VLM with knowledge base of object properties and exception handling, or training with counterfactual examples.

### Mechanism 3
- Claim: Plan length is inversely correlated with VLM planning accuracy due to increased complexity and error propagation.
- Core assumption: VLM's attention mechanism and context window are insufficient for long sequences.
- Evidence anchors:
  - "Correctness is degrading with sequence length increases"
  - "without images, the correctness or commonsense quality drops while with longer steps"
- Break condition: Using a hierarchical planning module to break down long plans into sub-goals, or expanding the context window.

## Foundational Learning

- Concept: Multi-modal input integration
  - Why needed here: VLMs must combine visual information with textual descriptions to understand the environment and task context.
  - Quick check question: What happens to VLM performance when visual input is removed? (Answer: Correctness and commonsense scores drop significantly)

- Concept: Counterfactual reasoning
  - Why needed here: Evaluating VLMs on their ability to adapt plans when faced with unexpected situations or altered object states.
  - Quick check question: Why are counterfactual activities harder for VLMs than normal activities? (Answer: They require reasoning about object properties and event causality not present in the original task)

- Concept: Procedural planning evaluation
  - Why needed here: Assessing the quality of generated action sequences against human-annotated gold plans.
  - Quick check question: What is the difference between correctness and commonsense satisfaction? (Answer: Correctness requires achieving the final goal; commonsense satisfaction requires each step to be plausible)

## Architecture Onboarding

- Component map: Input (Natural language task description + Environment images) -> Processor (VLM like Claude, Gemini-Pro, GPT-4V) -> Output (Procedural plan) -> Evaluators (Human annotators, Automatic metrics)

- Critical path:
  1. Load BDDL activity definition in iGibson2 simulator
  2. Generate natural language description via ChatGPT
  3. Collect environment images from simulator
  4. Human annotators write gold procedural plans
  5. VLMs generate plans from multi-modal inputs
  6. Evaluate plans using human and automatic metrics

- Design tradeoffs:
  - Low-resolution simulator images vs. real-world image complexity
  - Limited object states and scene sampling vs. rich counterfactual scenarios
  - Human annotation cost vs. automatic evaluation scalability

- Failure signatures:
  - Missing actions between consecutive steps
  - Incorrect handling of object properties or functions
  - Misunderstanding of event cause/effect relationships
  - Generation of actions in incorrect format
  - Hallucination of tools not present in input
  - Incorrect understanding of image contents

- First 3 experiments:
  1. Run VLMs on a small subset of ActPlan-1K with and without images to quantify the impact of visual input.
  2. Analyze error types on a sample of generated plans to identify the most common failure modes.
  3. Fine-tune BLEURT on a synthetic dataset of plan pairs to establish a baseline for automatic evaluation accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do VLMs perform on ActPlan-1K when evaluated in a real-world physical environment rather than a simulated one?
- Basis in paper: The paper uses iGibson2 simulator and symbolic plans, not addressing real-world execution.
- Why unresolved: Evaluations are based on simulated environment and symbolic plans, not real-world physical execution.
- What evidence would resolve it: A study evaluating VLMs by implementing their plans in a real-world physical environment and measuring success rate and commonsense adherence.

### Open Question 2
- Question: What is the impact of visual resolution and quality on VLM performance in ActPlan-1K?
- Basis in paper: The paper acknowledges low-resolution simulator images and limited object states as constraints.
- Why unresolved: While acknowledging limitations of low-resolution images, the paper does not investigate how different visual resolutions affect performance.
- What evidence would resolve it: An experiment varying visual resolution and quality in ActPlan-1K and measuring changes in VLM performance.

### Open Question 3
- Question: How does the performance of VLMs on ActPlan-1K compare to human performance on the same tasks?
- Basis in paper: The paper shows VLMs are not at human-level performance but does not quantify the gap.
- Why unresolved: The paper demonstrates VLMs are not at human-level but does not provide a direct comparison.
- What evidence would resolve it: A study comparing VLM and human performance on ActPlan-1K tasks using the same evaluation metrics.

## Limitations
- The study relies on simulated images from iGibson2, which may not fully capture real-world visual complexity.
- Human annotation for correctness and commonsense satisfaction could introduce subjective variability.
- Automatic evaluation metrics, while promising, may not fully capture all aspects of plan quality.

## Confidence

- **High confidence**: VLMs perform significantly worse on counterfactual activities compared to normal activities.
- **Medium confidence**: Plan length inversely correlates with accuracy, but error propagation mechanisms need more analysis.
- **Medium confidence**: BLEURT-based automatic evaluation is effective, but its reliability across diverse scenarios needs further validation.

## Next Checks

1. Validate the BLEURT-based evaluation metric on a held-out test set of plans to assess its generalizability.
2. Conduct experiments with fine-tuned VLMs specifically trained on procedural planning with spatial reasoning objectives to test the break conditions of Mechanism 1.
3. Implement a hierarchical planning module to break down long plans into sub-goals and evaluate its impact on accuracy for long sequence plans.