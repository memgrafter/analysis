---
ver: rpa2
title: A Review and Efficient Implementation of Scene Graph Generation Metrics
arxiv_id: '2404.09616'
source_url: https://arxiv.org/abs/2404.09616
tags:
- graph
- scene
- triplets
- recall
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive review and precise definitions
  of commonly used metrics in scene graph generation, addressing the lack of formal
  metric definitions in the literature. The authors introduce a standalone Python
  package called SGBench that efficiently implements all defined metrics and a benchmarking
  web service for comparing scene graph generation methods.
---

# A Review and Efficient Implementation of Scene Graph Generation Metrics

## Quick Facts
- arXiv ID: 2404.09616
- Source URL: https://arxiv.org/abs/2404.09616
- Reference count: 29
- Key outcome: Introduces SGBench, a Python package providing formal definitions and efficient implementations of scene graph generation metrics, addressing the lack of standardized evaluation in the field.

## Executive Summary
This paper addresses a critical gap in scene graph generation (SGG) research by providing formal, unambiguous definitions for commonly used evaluation metrics. The authors introduce SGBench, a lightweight Python package that implements all defined metrics efficiently, along with a benchmarking web service for comparing SGG methods. Through experiments on the PSG dataset, they demonstrate that HiLo achieves state-of-the-art performance across all metrics, particularly excelling on the main mR@50 and mN gR@50 metrics. The work provides a solid foundation for consistent evaluation and fair comparison of scene graph generation models.

## Method Summary
The paper introduces formal definitions for scene graph generation metrics including Recall@k, Mean Recall@k, Pair Recall@k, and No Graph Constraint Recall@k. A key architectural decision is decoupling triplet conversion from metric calculation, allowing models to control their own triplet selection strategy. The SGBench package implements these metrics efficiently in Python, supporting both SGG and panoptic scene graph generation (PSGG) tasks. Model outputs are standardized in JSON format with segmentation masks in TIFF format. The implementation includes instance matching using IoU thresholds and provides a benchmarking web service for comparing methods. The approach emphasizes flexibility, security, and interoperability while maintaining computational efficiency.

## Key Results
- HiLo outperforms existing methods across all metrics on the PSG dataset, achieving state-of-the-art scores on mR@50 and mN gR@50
- SGBench implementation is lightweight and easy to use, with only four core dependencies (NumPy, Pillow, tifffile, imagecodecs)
- Relative k values (×1, ×10) provide more comparable evaluation across different datasets compared to absolute k values
- The decoupled architecture allows model developers to control triplet selection while maintaining consistent metric evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Formal metric definitions eliminate ambiguity and improve reproducibility in scene graph evaluation.
- Mechanism: By providing strict mathematical definitions and pseudo-code for each metric, the paper removes the implicit assumptions that previously existed in the literature. This allows different research groups to evaluate models consistently and compare results fairly.
- Core assumption: The existing implementations of scene graph metrics were inconsistent due to informal definitions.
- Evidence anchors:
  - [abstract] "precise and thorough definitions for the metrics used to evaluate scene graph generation models are lacking"
  - [section] "In contrast to existing definitions, we define them more formally and provide pseudo code for a better understanding."
- Break condition: If the formal definitions introduce new edge cases or constraints that weren't present in original informal usage, this could create compatibility issues with existing research.

### Mechanism 2
- Claim: Decoupling triplet conversion from metric calculation increases flexibility and reduces coupling.
- Mechanism: Instead of having metrics directly operate on raw confidence scores, the implementation first converts scores to ordered triplets, then applies metrics. This separation allows models to control their own triplet selection strategy while metrics remain consistent.
- Core assumption: The original approach of having metrics directly consume scores was too restrictive for model developers.
- Evidence anchors:
  - [section] "Therefore, we decide to strictly decouple the triplet conversion step from the definition of the metrics."
  - [section] "How these triplets are selected is up to the model."
- Break condition: If the conversion step introduces significant computational overhead or if different models produce incompatible triplet formats, the benefits of decoupling could be outweighed by practical issues.

### Mechanism 3
- Claim: Relative k values (like R@×10) provide more comparable evaluation across different datasets.
- Mechanism: By making k relative to the number of ground truth annotations rather than absolute, the evaluation becomes less dependent on dataset characteristics like image complexity or annotation density.
- Core assumption: Absolute k values create unfair comparisons between datasets with different annotation densities.
- Evidence anchors:
  - [section] "For R@×10, it is allowed to select 10 triplets per ground truth annotation. For R@×1, the number of output triplets has to be the same as the number ground truth triplets."
  - [section] "However, the Recall@k scores with an absolute k depend on the number of ground truth annotations in the dataset."
- Break condition: If the relative k approach masks important differences in model performance at different scale thresholds, or if it makes it harder to track absolute performance improvements over time.

## Foundational Learning

- Concept: Intersection over Union (IoU) for instance matching
  - Why needed here: Instance matching is a core component of scene graph metrics, where predicted instances must be aligned with ground truth instances using IoU thresholds.
  - Quick check question: What IoU threshold is typically used for instance matching in scene graph evaluation, and what happens to instances that fall below this threshold?

- Concept: Graph representation of visual relationships
  - Why needed here: Understanding how scene graphs encode visual relationships as nodes (instances) and edges (predicates) is fundamental to grasping the metrics.
  - Quick check question: In a scene graph, what do the nodes represent and what do the edges represent?

- Concept: Precision vs Recall in information retrieval
  - Why needed here: Scene graph metrics like Recall@k are based on information retrieval principles, measuring how well a model can retrieve relevant triplets from its predictions.
  - Quick check question: How does Recall@k differ from precision in the context of scene graph evaluation?

## Architecture Onboarding

- Component map:
  SGBench package -> JSON triplet file format -> TIFF segmentation masks -> Instance matching -> Metric calculation -> Benchmarking web service

- Critical path:
  1. Model outputs triplets and segmentation masks
  2. Conversion script transforms outputs to JSON/TIFF format
  3. SGBench package loads ground truth and predictions
  4. Instance matching aligns predicted instances with ground truth
  5. Metrics calculate scores based on matched triplets
  6. Results displayed on benchmarking service leaderboard

- Design tradeoffs:
  - JSON vs Pickle: JSON is more secure and interoperable but potentially less efficient for very large datasets
  - TIFF vs PNG for masks: TIFF supports overlapping masks needed for panoptic graphs but may have larger file sizes
  - Relative vs absolute k: Relative k provides better cross-dataset comparison but may obscure absolute performance differences

- Failure signatures:
  - Low Instance Recall (InstR): Indicates problems with object detection accuracy
  - Large gap between R@k and R@∞: Suggests the model struggles with relationship prediction given correct instances
  - Poor performance on mR@k vs R@k: Indicates difficulty with rare predicates

- First 3 experiments:
  1. Run SGBench on a small test dataset with ground truth to verify metric calculations match manual calculations
  2. Compare SGBench output with existing implementation (OpenPSG) on the same dataset to validate correctness
  3. Test the instance matching functionality with different IoU thresholds to understand its impact on metric scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different scene graph generation models perform on datasets with varying levels of predicate imbalance, and how does this affect their Mean Recall@k scores?
- Basis in paper: [inferred] The paper discusses the predicate imbalance in scene graph datasets and the importance of Mean Recall@k in addressing this issue. It compares the performance of various models on the PSG dataset, which likely has a certain level of predicate imbalance.
- Why unresolved: The paper does not provide a detailed analysis of how different models perform on datasets with varying levels of predicate imbalance. It only evaluates models on the PSG dataset, which may not represent the full spectrum of predicate imbalance in scene graph datasets.
- What evidence would resolve it: A comprehensive evaluation of scene graph generation models on multiple datasets with varying levels of predicate imbalance, along with a detailed analysis of their Mean Recall@k scores on each dataset.

### Open Question 2
- Question: How does the choice of k in Recall@k metrics affect the evaluation of scene graph generation models, and what is the optimal value of k for different applications?
- Basis in paper: [explicit] The paper discusses the choice of k in Recall@k metrics and provides a comparison between absolute and relative values of k. It also mentions common values of k used in the literature (20, 50, 100).
- Why unresolved: The paper does not provide a clear guideline on the optimal value of k for different applications. It only compares absolute and relative values of k and shows that both choices are equally suited for evaluation.
- What evidence would resolve it: A thorough analysis of the impact of different k values on the evaluation of scene graph generation models across various applications, along with recommendations for the optimal value of k for each application.

### Open Question 3
- Question: How do the proposed metrics (Mean Recall@k, Pair Recall@k, No Graph Constraint Recall@k, etc.) correlate with each other, and what insights can be gained from their relationship?
- Basis in paper: [inferred] The paper introduces several metrics for evaluating scene graph generation models and provides a comparison of their performance on the PSG dataset. It also mentions that methods that perform well on one metric may not necessarily perform well on another metric.
- Why unresolved: The paper does not provide a detailed analysis of the correlation between the proposed metrics. It only mentions that methods may perform differently on different metrics without exploring the underlying reasons for this behavior.
- What evidence would resolve it: A comprehensive analysis of the correlation between the proposed metrics, including their strengths and weaknesses, and how they relate to each other in terms of evaluating scene graph generation models.

## Limitations

- The impact of the decoupled architecture on practical model development and comparison across different model architectures remains to be fully validated
- The choice of relative k values, while addressing dataset-specific issues, may mask important performance differences at absolute scale thresholds
- The long-term effectiveness of these metrics in driving meaningful improvements in scene graph generation research has not been established

## Confidence

- **High Confidence**: The core metric definitions (Recall@k, mR@k, PR@k, ngR@k) are mathematically rigorous and well-justified. The necessity for formal definitions in scene graph evaluation is clearly demonstrated.
- **Medium Confidence**: The architectural decisions around decoupling and relative k values are reasonable but may have unintended consequences in practice. The benchmarking service implementation appears sound but requires real-world testing.
- **Low Confidence**: The impact of these metrics on model development practices and whether they capture all relevant aspects of scene graph generation quality remains to be fully validated.

## Next Checks

1. **Cross-implementation consistency test**: Evaluate the same model outputs using both SGBench and OpenPSG implementations to verify metric consistency and identify any discrepancies in handling edge cases.

2. **Relative k value sensitivity analysis**: Compare model rankings using absolute k values versus relative k values across multiple datasets to understand how the choice affects method comparisons and identify potential biases.

3. **Real-world impact study**: Analyze whether the use of SGBench metrics leads to different model design decisions compared to previous informal evaluation approaches, measuring the actual impact on scene graph generation research progress.