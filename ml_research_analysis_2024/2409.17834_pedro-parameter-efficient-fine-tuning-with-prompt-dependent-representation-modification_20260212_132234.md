---
ver: rpa2
title: 'PEDRO: Parameter-Efficient Fine-tuning with Prompt DEpenDent Representation
  MOdification'
arxiv_id: '2409.17834'
source_url: https://arxiv.org/abs/2409.17834
tags:
- arxiv
- pedro
- language
- peft
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of efficiently adapting large language
  models (LLMs) to multiple downstream tasks in a single-backbone multi-tenant deployment
  setting, where multiple users share the same LLM instance but require task-specific
  adaptations. The authors propose PEDRO, a parameter-efficient fine-tuning (PEFT)
  method that generates prompt-dependent adjustment vectors for each Transformer layer
  of the LLM.
---

# PEDRO: Parameter-Efficient Fine-tuning with Prompt DEpenDent Representation MOdification

## Quick Facts
- arXiv ID: 2409.17834
- Source URL: https://arxiv.org/abs/2409.17834
- Authors: Tianfang Xie; Tianjing Li; Wei Zhu; Wei Han; Yi Zhao
- Reference count: 40
- Key outcome: Outperforms LoRA, AdaLoRA, and (IA)3 across 7 benchmark tasks using LLaMA-2 7B, with median accuracy scores from 86.9% to 94.7%

## Executive Summary
PEDRO introduces a novel parameter-efficient fine-tuning method that generates prompt-dependent adjustment vectors for each Transformer layer in a multi-tenant LLM deployment scenario. The approach uses a lightweight vector generator module that takes input prompt hidden states to produce task-specific modulation vectors, enabling efficient adaptation without modifying backbone parameters. Experiments demonstrate superior performance compared to strong PEFT baselines while using fewer tunable parameters and achieving significantly lower inference latency through KV-cache integration.

## Method Summary
PEDRO integrates lightweight vector generator modules into each Transformer layer of a pre-trained LLM. These generators take the hidden states of the input prompt as input and produce three adjustment vectors (lq, lv, lu) that modify the Query, Value, and Up module outputs through element-wise multiplication. The method employs learnable rational activation functions in the vector generators and works seamlessly with KV-cache mechanisms during autoregressive generation. Training uses bi-level optimization for activation parameters combined with standard PEFT training, allowing task-specific adaptation while maintaining parameter efficiency.

## Key Results
- Outperformed LoRA, AdaLoRA, and (IA)3 baselines across sentiment classification, NLI, QA, and instruction tuning tasks
- Achieved median accuracy scores ranging from 86.9% to 94.7% on seven benchmark tasks using LLaMA-2 7B
- Demonstrated 1.8x-3.8x lower inference latency compared to LoRA in multi-tenant settings through KV-cache optimization
- Used fewer tunable parameters than baseline methods while maintaining or exceeding their performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-aware modulation enables task-specific adaptation within a shared model backbone.
- Mechanism: The vector generator (VG) produces adjustment vectors conditioned on input prompt hidden states, allowing task-specific transformation of Query/Value representations and feed-forward module outputs without modifying backbone parameters.
- Core assumption: The hidden states of the input prompt contain sufficient task-relevant information to generate effective adjustment vectors.
- Evidence anchors:
  - [abstract]: "The proposed method involves integrating a lightweight vector generator into each Transformer layer, which generates vectors contingent upon the input prompts."
  - [section 3.3]: "The vector generator VG () use h as input to generate three learned vectors, lq, lv ∈ Rdmodel and lu ∈ Rdf f n, with a vector generator: lq, l v, l u = VG(h)."
  - [corpus]: Weak - no direct citations, but related works like PARA and IAPT suggest prompt-awareness improves PEFT performance.
- Break condition: If prompt hidden states lack discriminative features for different tasks, generated vectors will be ineffective, leading to poor performance.

### Mechanism 2
- Claim: Learnable activation functions in vector generators improve adaptation quality.
- Mechanism: The activation function gvg in the vector generator is replaced with trainable rational functions that approximate and adapt to task-specific patterns during bi-level optimization.
- Core assumption: Different downstream tasks require different activation functions for optimal performance.
- Evidence anchors:
  - [section 3.3]: "We utilize rational functions to render the activation functions trainable... These rational activation functions are integrated in image classification models [29], sequence modeling [5]..."
  - [section 4.5]: "The comparison among PEDRO-5, PEDRO-6, and PEDRO-7 shows that proper activation functions are required for our PEDRO framework, and different Transformer layers may require different gvg."
- Break condition: If rational activation functions overfit to training data or fail to converge during bi-level optimization, performance will degrade.

### Mechanism 3
- Claim: Integration with KV-cache mechanism enables efficient inference in multi-tenant settings.
- Mechanism: Generated adjustment vectors (lq, lv, lu) are computed once when input prompt is processed and reused across all token generation steps, avoiding repeated vector generator computation.
- Core assumption: Input prompt hidden states are processed once at the beginning of generation, making single computation of adjustment vectors feasible.
- Evidence anchors:
  - [section 3.3]: "Note that the decoder-based causal language models (CLM) usually employ the KV cache mechanism8 during generation... Our vector generators work seamlessly with the KV cache mechanism since the generated vectors... are generated when the input instruction (or prompt) is passed through the LLM for the first time."
  - [section 4.5]: "PEDRO is much faster than LoRA... The vectors will be reused in the subsequent generation steps with KV-cache, and the vector generators will not be called repetitively."
- Break condition: If KV-cache is not available or if adjustment vectors need to be recomputed for each token, latency benefits disappear.

## Foundational Learning

- Concept: Transformer architecture fundamentals (MHA and FFN modules)
  - Why needed here: PEDRO modifies internal representations in these modules through element-wise multiplication with adjustment vectors
  - Quick check question: What are the mathematical operations performed by the Query, Key, Value, Gate, and Up modules in a standard Transformer layer?

- Concept: Parameter-efficient fine-tuning (PEFT) methods
  - Why needed here: PEDRO is compared against LoRA, (IA)3, and other PEFT baselines, requiring understanding of how they modify model behavior
  - Quick check question: How do LoRA and (IA)3 differ in their approach to modifying Transformer weights or representations?

- Concept: KV-cache mechanism in autoregressive generation
  - Why needed here: PEDRO's efficiency advantage relies on KV-cache for reusing adjustment vectors across generation steps
  - Quick check question: What data is stored in the KV-cache and how does it improve generation efficiency?

## Architecture Onboarding

- Component map:
  Input prompt → Hidden states → Vector Generator → Adjustment vectors (lq, lv, lu) → Modified Q,V,U → Transformer output

- Critical path:
  1. Forward pass: Prompt → hidden states → VG → adjustment vectors → modified Q,V,U → Transformer output
  2. Training: Compute loss → backpropagate through modified Q,V,U only → update VG parameters
  3. Inference: Compute adjustment vectors once → reuse with KV-cache → generate tokens

- Design tradeoffs:
  - Bottleneck dimension r vs performance: Smaller r reduces parameters but may limit expressiveness
  - Learnable vs fixed activation: Learnable activations adapt to tasks but add training complexity
  - Adjustment scope: Modifying Q,V,U only vs also modifying K,G requires more parameters but may not improve performance

- Failure signatures:
  - Performance collapse: Adjustment vectors becoming near-zero or saturated
  - Training instability: Rational activation function parameters diverging during bi-level optimization
  - Memory issues: Vector generator parameters not being properly shared across layers

- First 3 experiments:
  1. Sanity check: Apply PEDRO to single task (e.g., SST-2) and verify performance exceeds (IA)3 baseline
  2. KV-cache integration: Measure inference latency with and without KV-cache to confirm efficiency advantage
  3. Ablation study: Compare performance with fixed GeLU vs learned rational activation function on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PEDRO's performance scale with larger language models beyond LLaMA-2 7B and 13B?
- Basis in paper: [inferred] The paper only tests PEDRO on LLaMA-2 7B, 13B and Gemma 2B models, leaving open the question of performance on larger models like LLaMA-2 70B or GPT-3.5
- Why unresolved: The paper does not explore the scaling behavior of PEDRO with model size, which is crucial for understanding its effectiveness in industrial applications with larger LLMs
- What evidence would resolve it: Experiments comparing PEDRO performance across a range of model sizes (e.g., 7B, 13B, 34B, 70B parameters) on the same tasks would provide insights into scaling properties

### Open Question 2
- Question: How does PEDRO's prompt-dependent adjustment mechanism compare to explicit in-context learning demonstrations in terms of effectiveness?
- Basis in paper: [explicit] The paper mentions that in-context learning can be improved by adaptively constructing augmented prompts with demonstrations, and hypothesizes that prompt-conditioned PEFT parameters could improve expressiveness
- Why unresolved: While the paper introduces a prompt-aware mechanism, it doesn't directly compare this approach to explicit in-context learning with demonstrations
- What evidence would resolve it: A controlled experiment comparing PEDRO's prompt-dependent adjustments to traditional in-context learning with various numbers and qualities of demonstrations on the same tasks

### Open Question 3
- Question: What is the optimal strategy for determining the activation functions for vector generators across different transformer layers?
- Basis in paper: [explicit] The paper notes that different tasks require different activation functions and that different layers may benefit from different activation functions, but uses a learned rational function approach
- Why unresolved: While the paper proposes learnable activation functions, it doesn't explore whether this is the optimal approach or how to determine the best activation function strategy
- What evidence would resolve it: Comparative experiments testing different strategies (fixed vs. learned activation functions, per-layer vs. shared activation functions) across multiple tasks and model architectures

## Limitations
- KV-cache dependency limits efficiency gains for non-autoregressive tasks or architectures without this mechanism
- Bi-level optimization for activation function parameters adds training complexity that may not generalize well to extremely large models
- Empirical evaluation is limited to LLaMA-2 7B and select tasks, leaving questions about scalability to larger models and diverse domains

## Confidence
- High confidence: The core mechanism of prompt-dependent vector generation is technically sound and the empirical comparisons with established PEFT baselines are methodologically rigorous
- Medium confidence: The claims about learnable activation functions providing meaningful performance improvements are supported by ablation studies but may be task-dependent
- Low confidence: The scalability claims beyond LLaMA-2 7B are not empirically validated and the paper doesn't address potential challenges in multi-task learning scenarios

## Next Checks
1. **Scalability test**: Evaluate PEDRO on larger model families (LLaMA-2 13B, 70B) to verify parameter efficiency and performance benefits scale proportionally with model size

2. **Task diversity expansion**: Test on code generation, mathematical reasoning, and long-context tasks to assess whether prompt-dependent adaptation generalizes beyond classification and instruction-following tasks

3. **Multi-tenant conflict analysis**: Design experiments where multiple users simultaneously fine-tune the same model for conflicting objectives to identify potential interference patterns and evaluate whether PEDRO's prompt-awareness mitigates such conflicts