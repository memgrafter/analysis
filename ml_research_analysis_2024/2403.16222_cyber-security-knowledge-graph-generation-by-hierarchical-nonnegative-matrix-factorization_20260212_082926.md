---
ver: rpa2
title: Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative Matrix
  Factorization
arxiv_id: '2403.16222'
source_url: https://arxiv.org/abs/2403.16222
tags:
- topic
- knowledge
- topics
- matrix
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors developed a hierarchical, semantic non-negative matrix
  factorization (HSNMFk-SPLIT) method to extract latent topic structures from large
  scientific corpora, specifically using over two million arXiv papers. By jointly
  factorizing TF-IDF, word-context (co-occurrence), and category matrices, the method
  automatically determines the number of latent topics while producing coherent, interpretable
  topic hierarchies.
---

# Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative Matrix Factorization

## Quick Facts
- arXiv ID: 2403.16222
- Source URL: https://arxiv.org/abs/2403.16222
- Reference count: 40
- The authors developed a hierarchical, semantic non-negative matrix factorization (HSNMFk-SPLIT) method to extract latent topic structures from large scientific corpora, specifically using over two million arXiv papers.

## Executive Summary
This paper presents a hierarchical semantic non-negative matrix factorization approach (HSNMFk-SPLIT) for constructing domain-specific knowledge graphs from large unstructured text corpora. The method automatically determines the number of latent topics while producing coherent, interpretable topic hierarchies by jointly factorizing TF-IDF, word-context, and category matrices. Applied to over two million arXiv papers, the approach successfully narrows from broad scientific literature to specific cyber-security and adversarial ML sub-topics, creating a knowledge graph with 3,758 nodes and 9,428 edges. The system demonstrates effective extraction of domain-specific knowledge, supporting cyber-security research discovery and trend analysis through its Neo4j-based graph database.

## Method Summary
The method employs hierarchical semantic non-negative matrix factorization (HSNMFk-SPLIT) that jointly factorizes three matrices: TF-IDF for document-term representation, word-context for semantic relationships, and category for document labels. The SPLIT technique enables distributed factorization of large TF-IDF matrices by splitting them into chunks, processing in parallel, and combining results. The method automatically determines the optimal number of latent topics and applies hierarchical decomposition recursively to extract fine-grained sub-topics. Finally, the extracted topics, keywords, and named entities are structured into a Neo4j knowledge graph with documents, topics, keywords, and entities as nodes connected by appropriate relationships.

## Key Results
- Successfully extracted 3,758 nodes and 9,428 edges in the knowledge graph from 2.1 million arXiv papers
- Demonstrated hierarchical decomposition from broad deep learning topics to specific cyber-security and adversarial ML sub-topics
- Achieved automatic determination of optimal topic numbers while maintaining semantic coherence across the corpus
- Created interpretable topic hierarchies revealing domain-specific ontologies in cyber-security literature

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint factorization of TF-IDF, word-context, and category matrices enables automatic determination of the optimal number of latent topics while maintaining semantic coherence.
- Mechanism: The HSNMFk-SPLIT method uses NMFk to perform joint factorization across three data modalities simultaneously. The word-context matrix captures semantic relationships between words, while the category matrix incorporates document-level labels. By optimizing the combined objective function, the method identifies the number of topics that best explain all three data sources without over- or under-fitting.
- Core assumption: The semantic structure of the text (captured in word-context relationships) and categorical information are complementary signals that together provide a more complete representation of the latent topic structure than either signal alone.
- Evidence anchors:
  - [abstract] "By jointly factorizing TF-IDF, word-context (co-occurrence), and category matrices, the method automatically determines the number of latent topics while producing coherent, interpretable topic hierarchies."
  - [section IV.A] "HSNMFk-SPLIT is a method that automatically estimates the number of latent topics and extracts coherent topics by exploiting the semantic representation encoded in the word-context matrix, categorical information encoded in the word-category matrix, together with the text-document matrix."
- Break condition: The method would fail if the word-context matrix becomes too sparse (e.g., very large vocabulary with short documents), making semantic relationships unreliable, or if category assignments are noisy or irrelevant to the domain.

### Mechanism 2
- Claim: Hierarchical decomposition enables extraction of fine-grained sub-topics and domain-specific ontologies from large text corpora.
- Mechanism: The method applies NMFk recursively at each node of a topic tree. After the initial factorization produces broad topics, documents within each topic are factorized again to discover sub-topics. This hierarchical approach allows the system to progressively narrow from general scientific literature to highly specific domains like cyber-security and adversarial ML.
- Core assumption: Topic hierarchies naturally exist in scientific literature, where broader fields contain nested sub-disciplines, and these hierarchies can be discovered through recursive factorization of document subsets.
- Evidence anchors:
  - [abstract] "Applied hierarchically three times, it narrowed a deep learning topic to cyber-security and adversarial ML sub-topics, revealing domain-specific ontologies."
  - [section IV.A] "For each node in the tree, once W and H are obtained, W models the topics, and H models the document's topic assignment. Here we compute the maximum indices for each column of H (also known as H-clustering) [35] to estimate the best topic representation of each document given as topic(Xj) = argmax Hj. The documents in each topic (super-topic) can then be factorized again following the above steps to estimate the underlying sub-topics along the tree depth."
- Break condition: The hierarchy would fail if the initial topic separation is poor (e.g., too many or too few topics), causing documents to be incorrectly grouped and preventing meaningful sub-topic extraction.

### Mechanism 3
- Claim: Distributed factorization with SPLIT method handles large-scale corpora efficiently while preserving topic coherence.
- Mechanism: The large TF-IDF matrix is split into smaller chunks that can be factorized in parallel. The resulting patterns from each chunk are then combined using the SPLIT technique, which concatenates and re-factorizes the individual topic matrices. This approach reduces computational overhead while maintaining the ability to discover coherent topics across the entire corpus.
- Core assumption: Topic patterns are locally consistent across document subsets, meaning that factorizing smaller document groups and then combining the results will approximate the results of factorizing the full corpus.
- Evidence anchors:
  - [section IV.A] "HSNMFk-SPLIT addresses the computation overhead that arises from factorizing a large TF-IDF matrix X by separating N documents into an arbitrary number of distinct matrices, factorizing each smaller TF-IDF matrix separately in a distributed manner [1], and finally combining the patterns from each matrix with the SPLIT method."
  - [section IV.A] "• Factorizing large matrices via SPLIT: First, split X ∈ RF×N+ into m chunks so that the ith chunk given by Xi ∈ RF×N/m+ can be factorized with NMFk as Xi ≈ WiHi where Wi ∈ RF×ki+ and Hi ∈ Rki×N/m+. Here m chunks can be factorized in parallel."
- Break condition: The method would fail if topics are not locally consistent (e.g., very heterogeneous corpus where different document subsets contain completely different topic distributions), causing the combined patterns to be incoherent.

## Foundational Learning

- Concept: Non-negative Matrix Factorization (NMF)
  - Why needed here: NMF is the core mathematical technique that decomposes the document-term matrix into interpretable topics and their distributions across documents. The non-negativity constraint ensures that topics are additive combinations of words, making them more interpretable than methods that allow negative values.
  - Quick check question: Why does the non-negativity constraint in NMF lead to more interpretable topics compared to other matrix factorization methods?

- Concept: Topic Modeling and Latent Semantic Analysis
  - Why needed here: Understanding how latent topics represent hidden patterns in text data is crucial for interpreting the results. The method extracts topics that are not explicitly labeled but emerge from patterns in word co-occurrence and document structure.
  - Quick check question: How does the joint factorization of multiple matrices (TF-IDF, word-context, category) improve topic coherence compared to using only the document-term matrix?

- Concept: Knowledge Graph Construction and Neo4j
  - Why needed here: The extracted topics, keywords, and named entities must be structured into a graph database for querying and analysis. Understanding how nodes and relationships are created from the NMF results is essential for the complete pipeline.
  - Quick check question: What types of relationships would you create between documents, topics, and named entities in the knowledge graph to enable effective cyber-security research discovery?

## Architecture Onboarding

- Component map:
  Data Preprocessing -> Feature Extraction -> Distributed Factorization -> Joint NMFk -> Hierarchical Decomposition -> Knowledge Graph Construction -> NER Extraction

- Critical path:
  1. Preprocess arXiv corpus (text cleaning and feature extraction)
  2. Construct TF-IDF, word-context, and category matrices
  3. Apply distributed SPLIT factorization on TF-IDF matrix
  4. Perform joint NMFk factorization with semantic and categorical constraints
  5. Extract hierarchical topics through recursive decomposition
  6. Build Neo4j knowledge graph with extracted entities and relationships

- Design tradeoffs:
  - Memory vs. Accuracy: Splitting the TF-IDF matrix reduces memory requirements but may slightly reduce topic coherence if topics are not locally consistent
  - Parallelism vs. Coordination: Distributed factorization improves speed but requires careful combination of results
  - Granularity vs. Interpretability: More hierarchical levels provide finer topics but may reduce interpretability
  - Semantic vs. Statistical: Incorporating word-context improves semantic coherence but adds computational complexity

- Failure signatures:
  - Poor topic separation: Too many or too few topics identified, causing documents to be incorrectly grouped
  - Low topic coherence: Extracted topics contain unrelated words or fail to represent meaningful concepts
  - Semantic inconsistency: Word-context relationships don't align with document topics
  - Graph sparsity: Knowledge graph contains few connections between entities, limiting query effectiveness
  - Computational bottlenecks: Distributed factorization fails to scale or produces inconsistent results across chunks

- First 3 experiments:
  1. Test NMFk on a small subset of the arXiv corpus (e.g., 10,000 documents) to verify automatic topic number determination and basic topic coherence
  2. Validate the SPLIT method by comparing results from distributed factorization vs. full-matrix factorization on a medium-sized dataset
  3. Build a small knowledge graph from a single topic decomposition and test basic Cypher queries to ensure entity relationships are correctly established

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the hierarchical semantic NMF approach compare to probabilistic hierarchical topic models like hLDA or HDP in terms of topic coherence and interpretability when applied to large scientific corpora?
- Basis in paper: [explicit] The paper contrasts HSNMFk-SPLIT with hierarchical LDA-based methods (hLDA using nCRP, HDP) and notes that these probabilistic approaches require extensive training data, while HSNMFk-SPLIT is instance-based and only requires the data being modeled.
- Why unresolved: The paper mentions these approaches exist but does not provide direct empirical comparisons between HSNMFk-SPLIT and probabilistic hierarchical models on the same datasets.
- What evidence would resolve it: Head-to-head experiments comparing topic coherence metrics (e.g., C_v, U_mass) and human interpretability assessments between HSNMFk-SPLIT and hLDA/HDP on identical large-scale scientific corpora.

### Open Question 2
- Question: What is the optimal balance between semantic regularization (word-context matrix) and category regularization (category matrix) for achieving domain-specific topic extraction in cyber-security literature?
- Basis in paper: [explicit] The method includes parameters α and β controlling the weight of semantic structure and category information in the joint factorization, but the paper does not explore how varying these parameters affects the quality of extracted topics or the specificity of domain knowledge.
- Why unresolved: The paper demonstrates the method works but does not systematically investigate the sensitivity of results to the regularization parameters or provide guidance on parameter selection for different domains.
- What evidence would resolve it: Systematic ablation studies varying α and β across multiple values, measuring topic coherence, domain specificity, and computational efficiency to identify optimal parameter ranges for cyber-security applications.

### Open Question 3
- Question: How scalable is the HSNMFk-SPLIT approach when applied to datasets significantly larger than two million documents, and what are the computational bottlenecks?
- Basis in paper: [explicit] The paper mentions HSNMFk-SPLIT addresses computation overhead by chunking documents and performing distributed factorization, and demonstrates results on 2.1 million documents, but does not explore performance beyond this scale.
- Why unresolved: The paper provides preliminary evidence of scalability but does not investigate the method's performance limits, parallel efficiency, or identify which components become bottlenecks with orders-of-magnitude larger datasets.
- What evidence would resolve it: Empirical scaling studies on datasets ranging from millions to tens or hundreds of millions of documents, measuring runtime, memory usage, and topic quality metrics, along with profiling to identify computational bottlenecks at different scales.

## Limitations
- Dependence on quality and completeness of category labels in input data
- Assumes natural topic hierarchies exist and can be discovered through recursive decomposition
- Computational complexity of joint factorization across multiple matrices remains significant for very large corpora

## Confidence
- High: The core mechanism of hierarchical NMF and knowledge graph construction using Neo4j is well-established and reproducible.
- Medium: The automatic determination of topic numbers through joint factorization across three matrices, while theoretically sound, may require careful parameter tuning for different datasets.
- Low: The scalability claims regarding distributed SPLIT factorization would need validation on datasets significantly larger than the arXiv corpus used in this study.

## Next Checks
1. Test the method on a dataset with known topic hierarchies to verify that the hierarchical decomposition accurately captures the ground-truth structure.
2. Evaluate the sensitivity of the automatic topic number determination to different values of regularization parameters α and β across multiple datasets.
3. Benchmark the distributed SPLIT factorization against traditional single-machine NMF on datasets 10× larger than the arXiv corpus to validate scalability claims.