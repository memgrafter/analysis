---
ver: rpa2
title: Does Knowledge Localization Hold True? Surprising Differences Between Entity
  and Relation Perspectives in Language Models
arxiv_id: '2409.00617'
source_url: https://arxiv.org/abs/2409.00617
tags:
- knowledge
- entity
- editing
- language
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how factual knowledge is stored in large
  language models (LLMs) by comparing entity and relation perspectives. Through knowledge
  editing experiments, the authors reveal that entity and relational knowledge cannot
  be directly transferred or mapped to each other, contrary to logical expectations.
---

# Does Knowledge Localization Hold True? Surprising Differences Between Entity and Relation Perspectives in Language Models

## Quick Facts
- arXiv ID: 2409.00617
- Source URL: https://arxiv.org/abs/2409.00617
- Reference count: 22
- Entity and relational knowledge cannot be directly transferred or mapped to each other

## Executive Summary
This paper investigates how factual knowledge is stored in large language models (LLMs) by comparing entity and relation perspectives. Through knowledge editing experiments, the authors reveal that entity and relational knowledge cannot be directly transferred or mapped to each other, contrary to logical expectations. Using causal analysis, they find that relational knowledge is significantly encoded not only in MLP weights (as previously thought) but also in attention modules. Their experiments with GPT-2 XL and GPT-J models demonstrate that editing relational knowledge yields high reliability and generality metrics for relations but poor performance for entities, while editing entity knowledge shows mixed results for relations. This finding challenges existing model editing approaches and highlights the complex, multifaceted nature of knowledge storage in LLMs, suggesting that manipulating specific types of knowledge requires understanding their distinct storage mechanisms.

## Method Summary
The study employs causal tracing analysis to identify knowledge storage locations in transformer models, then applies various model editing techniques (ROME, MEMIT, FT, KN, MEND) to modify factual knowledge from both entity and relation perspectives. The researchers evaluate edited models using Reliability (editing accuracy) and Generality (generalization across rephrased prompts) metrics. The experimental pipeline involves clean runs, corrupted runs, and corrupted-with-restoration runs to isolate the causal effects of MLP and attention modules on knowledge representation.

## Key Results
- Relational knowledge is encoded in both MLP weights and attention modules, contrary to previous assumptions
- Editing relational knowledge achieves high Reliability and Generality for relations but poor performance for entities
- Entity and relational knowledge are stored separately and cannot be directly mapped to each other

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relational knowledge in LLMs is stored not only in MLP layers but also significantly in attention modules.
- Mechanism: Causal tracing analysis reveals that modifying hidden states in attention modules has a measurable indirect effect on predicting relational outcomes, indicating that attention weights encode relational information.
- Core assumption: Causal mediation analysis can isolate and measure the contribution of individual model components (MLP vs attention) to knowledge representation.
- Evidence anchors:
  - [abstract] "Our experiments demonstrate that relational knowledge is also significantly encoded in attention modules."
  - [section] "we observed a pronounced AIE in the middle attention layers of the last corrupted token."
  - [corpus] Weak support; no direct citations of attention-based relational encoding in related papers.
- Break condition: If the causal tracing methodology cannot isolate component effects due to interference between MLP and attention computations.

### Mechanism 2
- Claim: Entity and relational knowledge are stored separately within model parameters and cannot be directly mapped to each other.
- Mechanism: Editing relational knowledge affects relational predictions reliably but fails to transfer the same effect to entity predictions, and vice versa, showing a lack of one-to-one mapping between entity and relation storage.
- Core assumption: Knowledge triplets (s, r, o) are logically equivalent units, so editing one component should reflect on the others.
- Evidence anchors:
  - [abstract] "entity and relational knowledge cannot be directly transferred or mapped to each other."
  - [section] "we are surprised the evaluation score for entity lags far from that for relation."
  - [corpus] Weak; no corpus papers explicitly discussing non-transferability between entity and relation editing.
- Break condition: If model editing methods inadvertently alter both entity and relation knowledge simultaneously due to overlapping parameter usage.

### Mechanism 3
- Claim: Knowledge expression localization differs between entity and relation perspectives, invalidating the assumption that knowledge is stored in fixed "knowledge neurons."
- Mechanism: When probing from a relation perspective, the causal locations of knowledge expression shift toward higher MLP and middle-to-upper attention layers, unlike the lower MLP layers identified in entity-based localization.
- Core assumption: Causal tracing from different knowledge perspectives (entity vs relation) will identify the same storage locations if knowledge is stored as unified units.
- Evidence anchors:
  - [abstract] "knowledge expression localized through relations is closely associated with higher MLP layers and mid-to-upper attention layers."
  - [section] "the causal locations of knowledge expression in the model change significantly."
  - [corpus] No direct corpus support; neighboring papers focus on extraction or alignment rather than localization perspective differences.
- Break condition: If future causal tracing methods can reconcile entity and relation localization into a unified framework.

## Foundational Learning

- Concept: Causal mediation analysis
  - Why needed here: To identify which model components (MLP, attention) causally contribute to relational knowledge storage.
  - Quick check question: Can you explain how the indirect effect (IE) is calculated and what it measures in causal tracing?
- Concept: Knowledge triplet representation (s, r, o)
  - Why needed here: The study investigates differences between entities and relations within the same knowledge triplet.
  - Quick check question: Why does the study assume that modifying either the entity or relation in a triplet should yield equivalent outcomes?
- Concept: Model editing evaluation metrics (Reliability, Generality)
  - Why needed here: To quantify the success and robustness of editing operations on knowledge.
  - Quick check question: How do Reliability and Generality metrics differ in measuring the effectiveness of model editing?

## Architecture Onboarding

- Component map:
  - GPT-2 XL (1.5B) and GPT-J (6B) transformer architectures
  - MLP layers: Intermediate feed-forward networks
  - Attention layers: Multi-head self-attention modules
  - Causal tracing pipeline: Clean run → Corrupted run → Corrupted-with-restoration run
- Critical path:
  1. Load pre-trained model
  2. Perform causal tracing to locate knowledge-relevant parameters
  3. Apply model editing methods (e.g., ROME, MEMIT)
  4. Evaluate edited model on Reliability and Generality metrics
- Design tradeoffs:
  - Causal tracing vs direct parameter inspection: Causal tracing isolates effects but is computationally expensive
  - Entity vs relation editing: Entity editing yields higher entity Reliability but lower relation Generality; relation editing shows the opposite
- Failure signatures:
  - Low Reliability: Editing failed to change model predictions as intended
  - Low Generality: Edited knowledge does not generalize across rephrased prompts
  - Mismatched effects: Editing entity knowledge does not affect relation knowledge (and vice versa)
- First 3 experiments:
  1. Run causal tracing on a small set of factual triplets to confirm AIE patterns in MLP and attention layers
  2. Apply ROME to edit entity knowledge in GPT-2 XL and measure changes in both entity and relation predictions
  3. Apply MEMIT to edit relation knowledge in GPT-J and compare Reliability/Generality outcomes for entities vs relations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the encoding of relational knowledge in attention modules compare to its encoding in MLP weights in terms of causal effects and editing reliability?
- Basis in paper: [explicit] The paper explicitly states that relational knowledge is encoded in both MLP weights and attention modules, and it provides causal analysis results showing the impact of severing MLP and attention computations.
- Why unresolved: While the paper identifies that relational knowledge is encoded in both MLP and attention modules, it does not provide a detailed comparison of the causal effects or editing reliability between these two types of encoding. The paper suggests that the storage location of knowledge is complex and multifaceted, but it does not delve into the relative importance or effectiveness of MLP versus attention modules for storing relational knowledge.
- What evidence would resolve it: Further experiments comparing the causal effects and editing reliability when editing relational knowledge stored in MLP weights versus attention modules would provide a clearer understanding of their relative importance and effectiveness.

### Open Question 2
- Question: Can the observed differences between entity and relation perspectives in knowledge storage be generalized across different model architectures and datasets?
- Basis in paper: [inferred] The paper's findings are based on experiments with GPT-2 XL and GPT-J models, and it is not clear whether these results can be generalized to other transformer-based models or datasets.
- Why unresolved: The paper focuses on specific models and datasets, and it does not explore whether the differences between entity and relation perspectives in knowledge storage are consistent across different model architectures and datasets. This limitation raises questions about the generalizability of the findings.
- What evidence would resolve it: Conducting similar experiments with a wider range of transformer-based models and datasets would help determine whether the observed differences between entity and relation perspectives are consistent across different architectures and data sources.

### Open Question 3
- Question: What are the underlying mechanisms that lead to the separation of entity and relational knowledge storage in LLMs?
- Basis in paper: [explicit] The paper reveals that entity and relational knowledge are stored separately within LLMs, but it does not provide a detailed explanation of the underlying mechanisms that lead to this separation.
- Why unresolved: While the paper identifies the separation of entity and relational knowledge storage, it does not explore the underlying mechanisms or reasons for this separation. Understanding these mechanisms would provide deeper insights into the nature of knowledge representation in LLMs.
- What evidence would resolve it: Investigating the training processes and architectural features of LLMs that contribute to the separation of entity and relational knowledge storage would help uncover the underlying mechanisms responsible for this phenomenon.

## Limitations
- The study's findings are based on GPT-2 XL and GPT-J models, limiting generalization to other transformer architectures
- Causal tracing methodology may have limitations in isolating component effects due to interference between MLP and attention computations
- The paper does not explore the underlying mechanisms that cause the separation of entity and relational knowledge storage

## Confidence

**High confidence:** The finding that relational knowledge is encoded in both MLP and attention modules is well-supported by causal tracing results

**Medium confidence:** The claim that entity and relation knowledge cannot be directly mapped requires further validation across different model architectures and editing techniques

**Medium confidence:** The observation about different localization patterns between entity and relation perspectives is supported by experimental data but needs replication

## Next Checks

1. Replicate the causal tracing analysis on a larger model (e.g., GPT-3) to verify whether the MLP+attention encoding pattern holds at scale

2. Test whether simultaneous editing of both entity and relation knowledge in the same triplet produces additive, interfering, or no effects on model outputs

3. Apply the same experimental protocol to encoder-only transformers (e.g., BERT) to determine if these findings generalize across transformer architectures