---
ver: rpa2
title: Learning Hidden Subgoals under Temporal Ordering Constraints in Reinforcement
  Learning
arxiv_id: '2411.01425'
source_url: https://arxiv.org/abs/2411.01425
tags:
- subgoals
- task
- learning
- subgoal
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning hidden subgoals under
  temporal ordering constraints in reinforcement learning, where subgoals are unknown
  and must be achieved in specific time orders. The authors propose LSTOC (Learning
  Subgoals under Temporal Ordering Constraints), a novel framework that iteratively
  discovers subgoals one-by-one using a contrastive learning approach combined with
  first-occupancy representation and temporal geometric sampling.
---

# Learning Hidden Subgoals under Temporal Ordering Constraints in Reinforcement Learning

## Quick Facts
- arXiv ID: 2411.01425
- Source URL: https://arxiv.org/abs/2411.01425
- Authors: Duo Xu; Faramarz Fekri
- Reference count: 40
- Key outcome: Novel framework (LSTOC) learns hidden subgoals and their temporal ordering simultaneously, outperforming baselines by up to 60% in convergence speed across nine tasks in three environments.

## Executive Summary
This paper addresses the challenge of learning hidden subgoals under temporal ordering constraints in reinforcement learning, where subgoals are unknown and must be achieved in specific time orders. The authors propose LSTOC, a novel framework that iteratively discovers subgoals one-by-one using contrastive learning combined with first-occupancy representation and temporal geometric sampling. The method builds a subgoal tree to represent discovered subgoals and their temporal relationships, which guides trajectory collection and accelerates task solving. Experiments demonstrate significant improvements in both subgoal learning accuracy and task-solving efficiency compared to baseline methods.

## Method Summary
LSTOC uses an iterative approach to discover hidden subgoals by building a subgoal tree. It employs contrastive learning with first-occupancy representation to focus on first visits to states and temporal geometric sampling to encode temporal ordering. The method expands the subgoal tree depth-first, discovering the next subgoal relative to the current working node. Once the tree is complete, an integer linear programming problem determines the mapping from discovered key states to subgoal semantic symbols. The framework includes an exploration policy with reward shaping to collect trajectories conditioned on working nodes, enabling sample-efficient subgoal discovery.

## Key Results
- LSTOC significantly outperforms baseline methods in subgoal learning accuracy and task-solving efficiency
- Up to 60% faster convergence compared to baselines across nine tasks in three environments
- Demonstrates good generalization to unseen tasks involving the same subgoals
- Effectively learns both the subgoals and their temporal ordering simultaneously

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Contrastive learning with first-occupancy representation and temporal geometric sampling can discover hidden subgoals and learn their temporal ordering simultaneously.
- **Mechanism:** The method removes repetitive states in trajectories to focus only on first visits to states, then uses temporal geometric sampling to select positive and negative samples for contrastive learning. This ensures that states closer to the initial state have higher sampling probability, allowing the importance function to reflect temporal ordering.
- **Core assumption:** First visits to states are more informative for detecting subgoals than repeated visits, and temporal distance can be encoded through sampling probability.
- **Evidence anchors:**
  - [abstract]: "We propose a new contrastive learning objective which can effectively learn hidden subgoals (key states) and their temporal orderings at the same time, based on first-occupancy representation and temporal geometric sampling."
  - [section]: "Since the task has multiple temporally extended hidden subgoals, applying conventional contrastive learning to train fω based on FRs of selected trajectories would produce multiple subgoals, some of which are not next subgoal to achieve."
- **Break condition:** If states that are not subgoals happen to be visited first frequently, or if temporal distance cannot be meaningfully encoded through sampling probability, the method may fail to identify correct subgoals or their ordering.

### Mechanism 2
- **Claim:** Building a subgoal tree iteratively allows the agent to discover subgoals one-by-one following temporal ordering constraints.
- **Mechanism:** The agent starts with a root node and expands the tree depth-first, discovering the next subgoal relative to the current working node. The tree structure encodes temporal dependencies, guiding trajectory collection and accelerating task solving.
- **Core assumption:** Subgoals must be discovered in temporal order, and the tree structure can represent these temporal dependencies accurately.
- **Evidence anchors:**
  - [abstract]: "We propose a sample-efficient learning strategy to discover subgoals one-by-one following their temporal order constraints by building a subgoal tree to represent discovered subgoals and their temporal ordering relationships."
  - [section]: "Based on discovered key states, Tφ is expanded iteratively in a depth-first manner. Initially,Tφ only has the root node v0. For a node vl, we define the path of vl (denoted as ξl) as the sequence of key states along the path from v0 to vl in Tφ."
- **Break condition:** If the environment has cycles in subgoal dependencies or if the agent cannot reliably determine the next subgoal to achieve, the tree expansion may fail or become inconsistent.

### Mechanism 3
- **Claim:** The labeling component can map discovered key states to subgoal symbols using an ILP problem, providing semantic meaning to learned subgoals.
- **Mechanism:** Given the FSM specification of temporal dependencies and the constructed subgoal tree, an ILP problem is formulated to determine the mapping from discovered key states to subgoal symbols, ensuring every path in the tree satisfies the given specification.
- **Core assumption:** The FSM specification is available and accurately represents the temporal dependencies of subgoals, and the discovered subgoal tree can be mapped to this specification.
- **Evidence anchors:**
  - [abstract]: "If the specification representing the temporal dependencies of subgoal semantic symbols is given, we formulate an integer linear programming (ILP) problem to determine the mapping from the discovered key states of subgoals to subgoal semantic symbols."
  - [section]: "Based on Mφ, ˆSK and Tφ, the mapping from discovered key states to subgoal symbols is determined by solving an ILP problem, yielding the semantic meaning of every hidden subgoal."
- **Break condition:** If the FSM specification is incorrect or incomplete, or if the discovered subgoal tree does not match the specification structure, the ILP problem may have no solution or produce incorrect mappings.

## Foundational Learning

- **Concept: Contrastive Learning**
  - Why needed here: To detect important states (subgoals) by comparing positive and negative trajectories, allowing the agent to identify key states without explicit supervision.
  - Quick check question: How does contrastive learning distinguish between important and unimportant states in a trajectory?

- **Concept: First-occupancy Representation**
  - Why needed here: To focus on the first visit to each state in a trajectory, removing repetitive visits that may distract from identifying true subgoals.
  - Quick check question: Why might repetitive state visits interfere with subgoal detection in tasks with temporal ordering constraints?

- **Concept: Temporal Logic and FSM**
  - Why needed here: To formally specify the temporal dependencies between subgoals, providing a framework for representing and learning the correct order of subgoal achievement.
  - Quick check question: How does a finite state machine represent the temporal ordering constraints between subgoals?

## Architecture Onboarding

- **Component map:** Subgoal Learning (Contrastive Learning -> Tree Expansion -> Exploration Policy) -> Labeling (ILP Problem)

- **Critical path:**
  1. Initialize with root node and empty buffers
  2. Collect trajectories using exploration policy
  3. Select working node and process trajectories with FR
  4. Apply contrastive learning to discover next subgoal
  5. Expand tree and update working node
  6. Repeat until all positive trajectories are explained
  7. Solve ILP problem for labeling

- **Design tradeoffs:**
  - Iterative vs. batch subgoal discovery: Iterative approach is more sample-efficient but may be slower for tasks with many subgoals
  - Tree expansion strategy: Depth-first ensures temporal ordering but may miss alternative paths initially
  - Contrastive learning formulation: Using FR and temporal geometric sampling improves accuracy but increases computational complexity

- **Failure signatures:**
  - Tree expansion gets stuck at a working node (insufficient trajectories or incorrect subgoal discovery)
  - ILP problem has no solution (specification mismatch or incorrect tree structure)
  - Importance function fω fails to distinguish subgoals from non-important states
  - Exploration policy fails to collect trajectories conditioned on working nodes

- **First 3 experiments:**
  1. Test subgoal discovery on a simple sequential task with known subgoals to verify contrastive learning accuracy
  2. Evaluate tree expansion on a task with alternative subgoal paths to test discovery of branching structures
  3. Assess labeling component on a task with repetitive subgoals to verify correct mapping to FSM specification

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions in the text provided.

## Limitations
- The effectiveness of first-occupancy representation and temporal geometric sampling in accurately encoding temporal distance requires further empirical validation
- The iterative tree expansion approach may struggle with tasks containing cycles or alternative subgoal paths
- The ILP-based labeling component's reliance on accurate FSM specifications could limit its applicability to environments with complex or ambiguous temporal dependencies

## Confidence
- **Medium**: The contrastive learning mechanism's ability to discover subgoals and their temporal ordering simultaneously
- **Medium**: The iterative subgoal tree construction and expansion strategy
- **Low**: The ILP formulation for mapping discovered key states to subgoal symbols

## Next Checks
1. Test LSTOC on environments with known subgoal cycles to assess whether the tree expansion strategy can handle non-linear temporal dependencies.
2. Evaluate the robustness of the first-occupancy representation and temporal geometric sampling to noise and repetitive state visits in trajectories with varying lengths and complexities.
3. Investigate the sensitivity of the ILP-based labeling component to errors in the FSM specification by introducing controlled mismatches between the discovered subgoal tree and the given temporal dependencies.