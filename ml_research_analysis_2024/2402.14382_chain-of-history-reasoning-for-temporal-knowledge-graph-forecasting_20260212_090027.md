---
ver: rpa2
title: Chain-of-History Reasoning for Temporal Knowledge Graph Forecasting
arxiv_id: '2402.14382'
source_url: https://arxiv.org/abs/2402.14382
tags:
- llms
- reasoning
- histories
- temporal
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes Chain-of-History (CoH) reasoning to address
  three shortcomings of existing LLM-based TKG prediction models: limited use of high-order
  historical information, poor performance under heavy information loads, and insufficient
  temporal reasoning capability. CoH explores high-order history chains step-by-step,
  preventing information overload while leveraging comprehensive historical data.'
---

# Chain-of-History Reasoning for Temporal Knowledge Graph Forecasting

## Quick Facts
- arXiv ID: 2402.14382
- Source URL: https://arxiv.org/abs/2402.14382
- Reference count: 39
- Primary result: CoH improves LLM-only performance and significantly boosts graph-based TKG models' accuracy, with MRR improvements of 4.34-5.26% when integrated with graph-based models

## Executive Summary
This paper addresses key limitations in existing LLM-based TKG prediction models by proposing Chain-of-History (CoH) reasoning. CoH tackles three main challenges: limited use of high-order historical information, poor performance under heavy information loads, and insufficient temporal reasoning capability. The method explores high-order history chains step-by-step to prevent information overload while leveraging comprehensive historical data, and serves as a plug-and-play module to enhance graph-based TKG models by compensating for their semantic modeling limitations.

## Method Summary
Chain-of-History reasoning processes temporal knowledge graph forecasting in two main steps. First, it uses an LLM to process first-order historical facts and identify the most important ones. Then, in the second step, it constructs second-order history chains based on the Step 1 outputs and generates predictions. The LLM's answer rankings are converted to scores using an exponential decay function, which are then fused with graph-based model predictions through weighted combination. The method is evaluated on three TKG datasets (ICEWS14, ICEWS18, ICEWS05-15) using three backbone models (RE-NET, RE-GCN, TiRGN).

## Key Results
- CoH significantly improves LLM-only performance on TKG forecasting tasks
- When integrated with graph-based models, CoH improves MRR by 4.34-5.26% compared to standalone graph-based models
- CoH consistently outperforms baseline approaches across all three datasets and three backbone models
- The method demonstrates effective utilization of high-order historical information while preventing LLM information overload

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Chain-of-History reasoning addresses LLM information overload by processing high-order historical information step-by-step rather than providing all histories at once.
- **Mechanism**: Instead of presenting LLMs with all historical facts simultaneously, CoH provides first-order histories in Step 1, then uses the LLM's inferred important histories to construct second-order history chains for Step 2, continuing iteratively. This prevents overwhelming the LLM while still leveraging comprehensive historical information.
- **Core assumption**: LLMs can effectively reason about which historical facts are most relevant when given focused subsets of information, rather than needing the complete history at once.
- **Evidence anchors**:
  - [abstract]: "CoH provides LLMs with high-order histories step-by-step" and "CoH provides LLMs with high-order histories step-by-step, achieving effective utilization of high-order historical information"
  - [section 3.2]: "Instead of providing LLMs with all histories at once, CoH provides LLMs with high-order histories step-by-step"
  - [corpus]: Weak - no direct comparison to baseline showing information overload metrics
- **Break condition**: If LLMs cannot reliably identify which historical facts are most relevant to a query when given focused subsets, the step-by-step approach would fail to capture necessary information.

### Mechanism 2
- **Claim**: CoH compensates for graph-based models' semantic modeling limitations by using LLM semantic reasoning capabilities.
- **Mechanism**: Graph-based models excel at structural information but lack semantic understanding. CoH uses LLM semantic reasoning to identify meaningful historical patterns and then fuses these results with graph-based predictions, creating a hybrid approach that leverages both structural and semantic information.
- **Core assumption**: The semantic reasoning capability of LLMs can identify relevant historical patterns that graph-based models miss due to their focus on structural features.
- **Evidence anchors**:
  - [abstract]: "We design CoH as a plug-and-play module to enhance the performance of graph-based models for TKG prediction" and "the semantic understanding advantage of LLM to compensate for the shortcoming of graph-based models"
  - [section 3.3]: "considering the importance of both structural and semantic information within TKGs, we propose to fuse the predicted results of LLMs and graph-based models"
  - [corpus]: Weak - no direct evidence showing semantic vs structural performance breakdown
- **Break condition**: If the semantic patterns identified by LLMs do not correlate with accurate predictions, the fusion approach would not improve performance.

### Mechanism 3
- **Claim**: The index-based scoring mechanism converts LLM answer rankings into meaningful prediction scores that can be fused with graph-based scores.
- **Mechanism**: CoH instructs LLMs to output answers with numerical indices representing their confidence. These indices are converted to scores using an exponential decay function, allowing integration with graph-based model scores through weighted fusion.
- **Core assumption**: The numerical indices assigned by LLMs during generation reflect meaningful confidence levels that can be quantitatively converted to prediction scores.
- **Evidence anchors**:
  - [section 3.3]: "Each answer is assigned a numerical index...with a lower index indicating a higher probability of the answer being correct" and "We convert the index of each answer ei âˆˆ A q LLM into its corresponding score with an exponential decay function"
  - [section 4.4]: "Each answer is assigned a numerical index (1,2,3,...), with a lower index indicating a higher probability of the answer being correct"
  - [corpus]: Weak - no ablation