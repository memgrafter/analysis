---
ver: rpa2
title: Leading Whitespaces of Language Models' Subword Vocabulary Pose a Confound
  for Calculating Word Probabilities
arxiv_id: '2406.10851'
source_url: https://arxiv.org/abs/2406.10851
tags:
- word
- probabilities
- surprisal
- decoding
- reading
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The leading whitespaces of language models' subword vocabulary\
  \ result in word probabilities that can sum to more than one, violating Kolmogorov's\
  \ axiom that P(\u03A9) = 1. This misallocation of word-by-word surprisal incorrectly\
  \ carries over the unacceptability of the current word to the next word and makes\
  \ LM predictions incompatible with psycholinguistic experiments where human subjects\
  \ directly observe upcoming word boundaries."
---

# Leading Whitespaces of Language Models' Subword Vocabulary Pose a Confound for Calculating Word Probabilities

## Quick Facts
- **arXiv ID**: 2406.10851
- **Source URL**: https://arxiv.org/abs/2406.10851
- **Reference count**: 9
- **Primary result**: Leading whitespaces in subword tokens cause word probabilities to sum to more than one, violating Kolmogorov's axiom P(Ω) = 1, and misallocate surprisal across word boundaries.

## Executive Summary
This paper identifies a fundamental confound in how language models with subword tokenization calculate word probabilities. Tokens in the subword vocabulary of most language models have leading whitespaces, which means word probabilities can sum to more than one, violating Kolmogorov's probability axioms. This misallocation of word-by-word surprisal incorrectly carries the unacceptability of the current word to the next word and makes LM predictions incompatible with psycholinguistic experiments where human subjects directly observe upcoming word boundaries. The authors propose a simple decoding technique (whitespace-trailing decoding) that reaccounts the probability of the trailing whitespace into that of the current word, resolving this confound.

## Method Summary
The paper proposes whitespace-trailing (WT) decoding, which modifies probability calculations by reaccounting the probability of the trailing whitespace into the current word using Equation 5. This is applied to GPT-2 and Pythia language models. Linear mixed-effects regression (LMER) models are used to estimate linking functions and quantify effects. The methodology is tested on garden-path effects in transitive/intransitive sentences (using Provo Corpus) and fit of LM surprisal to naturalistic reading times (using Natural Stories and Dundee corpora).

## Key Results
- Leading whitespaces cause word probabilities to sum to more than one, violating Kolmogorov's axiom P(Ω) = 1
- Misallocation of word-by-word surprisal incorrectly carries unacceptability of current word to the next word
- WT decoding resolves the confound and reveals lower estimates of garden-path effects in transitive/intransitive sentences
- WT decoding shows poorer fits to naturalistic reading times when applied

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Leading whitespaces in subword tokens cause word probabilities to sum to more than one, violating Kolmogorov's axiom P(Ω) = 1.
- Mechanism: Subword tokens with leading whitespaces factor word probabilities incorrectly. For example, P(matron | I was a) is decomposed into P(mat | I was a) · P(ron | I was a mat), which allows P(matron | I was a) ≤ P(mat | I was a) even when "matron" is more probable than "mat" in context.
- Core assumption: The tokenizer builds whitespace characters directly into token beginnings, so stop probabilities of words are never explicitly calculated.
- Evidence anchors:
  - [abstract] "tokens in the subword vocabulary of most language models have leading whitespaces and therefore do not naturally define stop probabilities of words."
  - [section 2.1] Theorem 1 proves that leading whitespaces can result in word probabilities summing to more than one.
  - [corpus] Weak - related papers focus on tokenization granularity and bias but do not directly confirm the sum > 1 violation.
- Break condition: If tokenizers remove leading whitespaces or if word probability aggregation explicitly includes stop probability, the violation disappears.

### Mechanism 2
- Claim: Misallocation of word-by-word surprisal carries unacceptability of the current word to the next word.
- Mechanism: When a subword like " mat" is followed by "in", the surprisal of "in" absorbs the unacceptability of " mat" because P(matron | I was a) ≤ P(mat | I was a). This incorrectly attributes low acceptability to the next token.
- Core assumption: Language models assign conditional probabilities at the subword level, not at word boundaries.
- Evidence anchors:
  - [section 2.1] Example shows "part of the 'unacceptability' of mat is incorrectly carried over to P(in | I was a mat)".
  - [abstract] "This property results in a misallocation of word-by-word surprisal, where the unacceptability of the end of the current word is incorrectly carried over to the next word."
  - [corpus] No direct evidence - corpus neighbors discuss tokenization bias but not surprisal misallocation.
- Break condition: If surprisal is computed at true word boundaries (e.g., with trailing whitespace decoding), the misallocation stops.

### Mechanism 3
- Claim: Leading whitespaces make LM predictions incompatible with psycholinguistic experiments where humans directly observe word boundaries.
- Mechanism: In self-paced reading, humans see a token like "mat" and know the next input will start a new word. LMs, however, assign probabilities over both VB (beginning-of-word tokens) and VI (inside-word tokens), so they predict possible continuations like "ron" after "mat".
- Core assumption: Humans use parafoveal processing to detect upcoming whitespaces and plan eye movements accordingly.
- Evidence anchors:
  - [section 2.2] "native speakers of languages with whitespace orthographies have been shown to be sensitive to the location of upcoming whitespaces through parafoveal processing."
  - [abstract] "Additionally, this implicit prediction of word boundaries incorrectly models psycholinguistic experiments where human subjects directly observe upcoming word boundaries."
  - [corpus] No direct evidence - corpus neighbors focus on tokenization evaluation but not psycholinguistic alignment.
- Break condition: If LMs explicitly predict trailing whitespace as part of the word, their predictions align with human boundary detection.

## Foundational Learning

- Concept: Subword tokenization (e.g., Byte-Pair Encoding)
  - Why needed here: The paper's confound arises because BPE and similar schemes build whitespace into tokens, creating the leading whitespace problem.
  - Quick check question: What happens to the probability mass of a word's trailing whitespace under standard BPE tokenization?

- Concept: Kolmogorov's probability axioms
  - Why needed here: The paper proves that word probabilities can sum to more than one, violating P(Ω) = 1.
  - Quick check question: If P(word1) + P(word1+word2) > 1, which axiom is violated?

- Concept: Surprisal and its role in psycholinguistic modeling
  - Why needed here: The paper evaluates how subword tokenization affects surprisal-based estimates of garden-path effects and reading time fits.
  - Quick check question: How does misallocated surprisal at a word boundary affect predictions of reading difficulty?

## Architecture Onboarding

- Component map: Tokenizer -> Language Model -> Decoding Layer (optional) -> Evaluation
- Critical path:
  1. Tokenize input text using subword tokenizer
  2. Feed token sequence to LM to obtain conditional probabilities
  3. Aggregate subword probabilities into word probabilities (standard or with WT decoding)
  4. Compute surprisal and use in regression models to predict reading times
- Design tradeoffs:
  - Standard decoding: Simple, minimal overhead, but violates probability axioms and misaligns with human boundary detection
  - WT decoding: Corrects the confound but adds a marginalization step over VB tokens
- Failure signatures:
  - Word probability sums > 1
  - Surprisal values that do not align with human reading difficulty, especially at word boundaries
  - Poor fit of LM surprisal to naturalistic reading time data
- First 3 experiments:
  1. Replicate Theorem 1 with a small LM to confirm P(Ω) > 1 under standard decoding
  2. Apply WT decoding to the same LM and verify P(Ω) = 1
  3. Compare surprisal-based garden-path effect estimates with and without WT decoding on a minimal pair dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed whitespace-trailing decoding affect surprisal-based estimates of garden-path effects in other syntactic constructions beyond transitive/intransitive sentences?
- Basis in paper: explicit
- Why unresolved: The paper only tests garden-path effects in transitive/intransitive sentences. Other constructions like relative clause attachment ambiguities or subject/object relative clauses could show different patterns.
- What evidence would resolve it: Apply WT decoding to analyze garden-path effects in various syntactic constructions using the same methodology as Experiment 1, comparing results with and without WT decoding.

### Open Question 2
- Question: Does the confound posed by leading whitespaces affect language models trained on languages without whitespace orthography?
- Basis in paper: inferred
- Why unresolved: The paper focuses on languages with whitespace orthography and does not address how tokenization schemes in languages like Chinese or Japanese might affect word probability calculations.
- What evidence would resolve it: Test word probability calculations and surprisal estimates on language models trained on languages without whitespace orthography, comparing results with and without modifications similar to WT decoding.

### Open Question 3
- Question: How does the performance of WT decoding change when applied to different subword tokenization schemes beyond byte-pair encoding?
- Basis in paper: explicit
- Why unresolved: The paper mentions that other tokenization schemes exist and that WT decoding might not resolve all issues, but does not test other schemes like SentencePiece or WordPiece.
- What evidence would resolve it: Apply WT decoding to language models trained with different tokenization schemes and compare word probability distributions, surprisal estimates, and fits to human reading times across schemes.

## Limitations

- The confound relies on the assumption that tokenizers universally insert leading whitespaces, which may not hold for all tokenization architectures
- The empirical validation is limited to English corpora and specific LM architectures (GPT-2 and Pythia)
- The proposed WT decoding assumes clean separation of trailing whitespace, which may not work for all tokenization schemes

## Confidence

- **High Confidence**: The mathematical proof that leading whitespaces can cause word probabilities to sum to more than one (Theorem 1) is rigorous and the mechanism of surprisal misallocation is well-explained theoretically.
- **Medium Confidence**: The empirical demonstrations on garden-path effects and reading time fits are compelling but limited to specific datasets and models. The magnitude of the confound's impact may vary with different corpora or tokenization schemes.
- **Low Confidence**: The generalizability of the confound and proposed solution across all subword tokenization methods and language models remains uncertain without broader empirical testing.

## Next Checks

1. **Cross-tokenizer validation**: Test the confound and WT decoding solution across multiple tokenizer architectures (BPE, SentencePiece, WordPiece, Unigram) and verify that the violation of P(Ω) = 1 is consistent across all implementations.
2. **Alternative language evaluation**: Apply the WT decoding to LMs trained on languages with different orthographic properties (e.g., Chinese without spaces, Arabic with different whitespace conventions) to assess the confound's generalizability beyond English.
3. **Human reading time replication**: Conduct a controlled self-paced reading experiment comparing human surprisal estimates with LM predictions under both standard and WT decoding to directly validate the psycholinguistic incompatibility claim.