---
ver: rpa2
title: 'WPO: Enhancing RLHF with Weighted Preference Optimization'
arxiv_id: '2406.11827'
source_url: https://arxiv.org/abs/2406.11827
tags:
- preference
- arxiv
- optimization
- policy
- off-policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Weighted Preference Optimization (WPO) to
  address distributional gaps in off-policy reinforcement learning from human feedback
  (RLHF). WPO simulates on-policy learning by reweighting preference pairs according
  to their probability under the current policy, using a weighting alignment mechanism
  to ensure equal treatment of on-policy outputs.
---

# WPO: Enhancing RLHF with Weighted Preference Optimization

## Quick Facts
- arXiv ID: 2406.11827
- Source URL: https://arxiv.org/abs/2406.11827
- Authors: Wenxuan Zhou; Ravi Agrawal; Shujian Zhang; Sathish Reddy Indurthi; Sanqiang Zhao; Kaiqiang Song; Silei Xu; Chenguang Zhu
- Reference count: 13
- Primary result: WPO outperforms DPO by up to 5.6% on Alpaca Eval 2 and achieves 76.7% length-controlled win rate against GPT-4-turbo

## Executive Summary
This paper introduces Weighted Preference Optimization (WPO) to address distributional gaps in off-policy reinforcement learning from human feedback (RLHF). WPO simulates on-policy learning by reweighting preference pairs according to their probability under the current policy, using a weighting alignment mechanism to ensure equal treatment of on-policy outputs. Experiments show WPO outperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2 and achieves a state-of-the-art length-controlled winning rate of 76.7% against GPT-4-turbo using Gemma-2-9b-it in a hybrid RL setting. WPO also consistently improves other preference optimization loss functions.

## Method Summary
WPO adapts off-policy preference data to resemble on-policy data by reweighting preference pairs based on their probability under the current policy model. The method computes weights proportional to πθ(yw|x) · πθ(yl|x) for each preference pair, then applies alignment mechanisms to ensure uniform weights for on-policy outputs. Two alignment methods are proposed: greedy alignment and sampled alignment, with the latter showing superior performance. The weighted loss is integrated into standard preference optimization frameworks, and the method is tested across multiple base models and datasets.

## Key Results
- WPO outperforms DPO by up to 5.6% on Alpaca Eval 2
- Achieves 76.7% length-controlled win rate against GPT-4-turbo using Gemma-2-9b-it
- Consistently improves other preference optimization loss functions beyond DPO
- Shows 3.6% improvement on MT-bench compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WPO mitigates the distributional gap by reweighting preference pairs based on their probability under the current policy, simulating on-policy learning.
- Mechanism: Preference pairs are assigned weights proportional to πθ(yw|x) · πθ(yl|x), with alignment mechanisms ensuring uniform weights for on-policy outputs.
- Core assumption: The preference model can be effectively approximated by reweighting data based on policy likelihood.
- Evidence anchors:
  - [abstract]: "Our Weighted Preference Optimization (WPO) method adapts off-policy data to resemble on-policy data more closely by reweighting preference pairs according to their probability under the current policy."
  - [section]: "The conceptual process above is equivalent to optimizing the following weighted preference optimization (WPO) objective, where different pairs in the original preference dataset are reweighed..."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.522, average citations=0.0. Top related titles: SeRA: Self-Reviewing and Alignment of Large Language Models using Implicit Reward Margins, SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning, Simplify RLHF as Reward-Weighted SFT: A Variational Method.
- Break condition: If the policy model's confidence varies significantly across inputs, leading to highly non-uniform weights even for on-policy data.

### Mechanism 2
- Claim: Weight alignment ensures that outputs generated by the current policy model receive equal treatment during optimization.
- Mechanism: Two alignment methods—greedy and sampled—adjust token-level probabilities to normalize weights, preventing low-confidence on-policy outputs from being underweighted.
- Core assumption: Token-level probability normalization effectively corrects for confidence bias without introducing new biases.
- Evidence anchors:
  - [section]: "To address this and ensure equal weighting of these outputs, we propose to align the weights in WPO... we operate at the token level and adjust the probability of output tokens according to the token distribution in the policy model."
  - [corpus]: Same corpus evidence as Mechanism 1.
- Break condition: If alignment overcorrects, introducing uniform weights that ignore meaningful differences in output quality.

### Mechanism 3
- Claim: On-policy dispreferred data (yl) is more critical for optimization than on-policy preferred data (yw).
- Mechanism: The WPO gradient emphasizes reducing the probability of yl, while increasing yw's probability is less impactful.
- Core assumption: The optimization process benefits more from correcting dispreferred outputs than reinforcing preferred ones.
- Evidence anchors:
  - [section]: "Our analysis reveals that... on-policy, dispreferred data is more important for preference optimization than on-policy preferred data."
  - [corpus]: Same corpus evidence as Mechanism 1.
- Break condition: If the quality of preferred data becomes the limiting factor in model performance.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: WPO builds on RLHF's framework of using human preference data to align LLMs.
  - Quick check question: What are the key differences between on-policy and off-policy RLHF?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: WPO extends DPO by introducing weighting mechanisms to address distributional gaps.
  - Quick check question: How does DPO formulate the preference optimization objective without a reward model?

- Concept: Distributional shift in off-policy learning
  - Why needed here: WPO directly addresses the distributional gap between data collection policy and target policy.
  - Quick check question: Why does the distributional gap in off-policy preference optimization lead to suboptimal training?

## Architecture Onboarding

- Component map: Policy model (πθ) -> Preference dataset -> Weight alignment module -> Loss computation -> Policy update
- Critical path: Sample preference pairs → Compute weights → Align weights → Compute WPO loss → Update policy parameters
- Design tradeoffs: Weight normalization improves stability but may reduce sensitivity to quality differences
- Failure signatures: Unstable training (over-optimization), poor performance on on-policy data, excessive variance in weights
- First 3 experiments:
  1. Train WPO with sampled alignment vs. no alignment to verify improvement
  2. Compare WPO performance on Alpaca Eval 2 and MT-bench vs. DPO
  3. Test WPO integration with other preference optimization loss functions (e.g., IPO, SimPO)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics of the SFT model (like training data diversity or architecture) most strongly influence the relative performance gap between on-policy and off-policy preference optimization?
- Basis in paper: [explicit] The paper notes that "the effectiveness of on-policy versus off-policy preference optimization is model-dependent" and suggests that the quality of the SFT model influences this relationship, but doesn't specify which characteristics matter most.
- Why unresolved: The paper establishes correlation but doesn't isolate specific SFT model characteristics that drive the performance differences across different base models.
- What evidence would resolve it: Controlled experiments varying specific SFT model characteristics (dataset size, domain coverage, pretraining objectives) while keeping other factors constant, then measuring the on-policy/off-policy performance gap.

### Open Question 2
- Question: How does the proposed weight alignment mechanism affect the optimization trajectory and final model behavior compared to unaligned weighting, beyond just improving final performance metrics?
- Basis in paper: [explicit] The paper compares different alignment methods and shows sampled alignment works best, but doesn't analyze how alignment affects the learning dynamics or resulting model behaviors.
- Why unresolved: The paper focuses on final performance outcomes rather than analyzing the intermediate optimization process or behavioral differences between aligned and unaligned approaches.
- What evidence would resolve it: Visualization of training curves showing loss landscapes, analysis of model outputs at different training stages, or behavioral tests showing differences in how models handle edge cases or novel prompts.

### Open Question 3
- Question: Does the observed phenomenon that on-policy dispreferred data is more important than on-policy preferred data hold across different types of tasks beyond instruction following, such as reasoning, creative writing, or code generation?
- Basis in paper: [explicit] The paper finds that "making yl on-policy explains most of the improvements of WPO" in instruction following tasks, but doesn't test this across task domains.
- Why unresolved: The analysis is limited to instruction following benchmarks, and the importance of dispreferred data might vary by task type.
- What evidence would resolve it: Systematic experiments applying WPO across diverse task domains (mathematical reasoning, creative generation, code completion) while separately analyzing the contribution of preferred vs dispreferred on-policy data.

## Limitations
- Relies heavily on a single Ultrafeedback dataset and three base models, limiting generalizability
- Weight alignment mechanism lacks detailed implementation specifics, reducing reproducibility
- Does not analyze how alignment affects optimization trajectory and intermediate model behaviors

## Confidence
- Distributional gap mitigation claim: Medium-High (theoretical grounding and empirical results)
- On-policy dispreferred data importance claim: Medium (limited ablation studies)
- Weight alignment mechanism reproducibility: Medium (implementation details unclear)

## Next Checks
1. **Generalization Test**: Apply WPO to a different preference dataset (e.g., UltraFeedback v2 or HH-RLHF) to verify the distributional gap mitigation generalizes beyond the original dataset.
2. **Ablation Study**: Compare WPO performance with and without weight alignment across multiple base models to isolate the alignment mechanism's contribution.
3. **Failure Mode Analysis**: Intentionally introduce distributional gaps (e.g., using a mismatched preference model) and test whether WPO's reweighting effectively mitigates performance degradation.