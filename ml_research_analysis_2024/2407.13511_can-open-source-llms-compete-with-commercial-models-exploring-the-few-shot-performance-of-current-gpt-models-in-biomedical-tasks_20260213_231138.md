---
ver: rpa2
title: Can Open-Source LLMs Compete with Commercial Models? Exploring the Few-Shot
  Performance of Current GPT Models in Biomedical Tasks
arxiv_id: '2407.13511'
source_url: https://arxiv.org/abs/2407.13511
tags:
- batch
- shot
- mixtral
- task
- also
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance of open-source large language
  models (LLMs) in biomedical question-answering tasks using retrieval-augmented generation
  (RAG) setups. The authors compare Mixtral 8x7B with commercial models like GPT-3.5-turbo
  and Claude 3 Opus using zero-shot and few-shot learning approaches.
---

# Can Open-Source LLMs Compete with Commercial Models? Exploring the Few-Shot Performance of Current GPT Models in Biomedical Tasks

## Quick Facts
- **arXiv ID**: 2407.13511
- **Source URL**: https://arxiv.org/abs/2407.13511
- **Reference count**: 40
- **Primary result**: Mixtral 8x7B performs competitively with commercial models in few-shot settings but struggles in zero-shot scenarios for biomedical RAG tasks.

## Executive Summary
This paper evaluates open-source large language models (LLMs) in biomedical question-answering tasks using retrieval-augmented generation (RAG) setups. The authors compare Mixtral 8x7B with commercial models like GPT-3.5-turbo and Claude 3 Opus using zero-shot and few-shot learning approaches. They find that Mixtral 8x7B is competitive with commercial models in the 10-shot setting but struggles in zero-shot scenarios. Fine-tuning with QLoRa and adding Wikipedia context did not lead to consistent performance improvements. The study demonstrates that the performance gap between commercial and open-source models in RAG setups can be closed by collecting few-shot examples for domain-specific use cases, making open-source models viable alternatives for enterprise and clinical applications where data privacy is crucial.

## Method Summary
The study uses BioASQ Phase A and B data to evaluate LLMs in a RAG pipeline for biomedical question answering. The method involves indexing PubMed articles in Elasticsearch, generating queries from questions using 1-shot or 10-shot prompts, retrieving top 50 documents, extracting snippets (zero-shot), reranking snippets, and answering questions. The study compares Mixtral 8x7B, GPT-3.5-turbo, and Claude 3 Opus across zero-shot and few-shot settings, with additional experiments on QLoRa fine-tuning and Wikipedia context augmentation.

## Key Results
- Mixtral 8x7B performs competitively with commercial models in 10-shot settings for biomedical RAG tasks
- Zero-shot performance of Mixtral 8x7B is significantly worse than commercial models
- QLoRa fine-tuning and Wikipedia context did not provide consistent performance improvements
- Open-source models can be self-hosted, making them viable for privacy-sensitive enterprise and clinical applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixtral 8x7B performs competitively with commercial models in few-shot settings but not in zero-shot settings.
- Mechanism: Few-shot examples provide sufficient in-context learning signals to compensate for domain-specific knowledge gaps, whereas zero-shot relies entirely on pre-training coverage.
- Core assumption: The model can generalize from few examples to the biomedical domain without catastrophic forgetting or task misalignment.
- Evidence anchors:
  - [abstract]: "Mixtral 8x7b was competitive in the 10-shot setting, both with and without fine-tuning, but failed to produce usable results in the zero-shot setting."
  - [section 4.2]: Tables show 10-shot Mixtral performing comparably to GPT-3.5-turbo and Claude 3 Opus in Phase A tasks.
  - [corpus]: No direct corpus evidence; assumption based on internal BioASQ results.
- Break condition: If few-shot examples are poorly chosen or insufficient in number, the performance gap reappears.

### Mechanism 2
- Claim: Fine-tuning with QLoRa and adding Wikipedia context does not lead to consistent performance gains.
- Mechanism: The domain-specific knowledge in biomedical text and the retrieval-augmented setup already provide sufficient grounding, so additional fine-tuning or Wikipedia context offers diminishing returns.
- Core assumption: The RAG pipeline supplies enough relevant context to the model that further adaptation via fine-tuning is unnecessary.
- Evidence anchors:
  - [abstract]: "QLoRa fine-tuning and Wikipedia context did not lead to measurable performance gains."
  - [section 4.3]: Wikipedia context improved results in some batches but worsened in others; fine-tuning showed inconsistent improvements.
  - [corpus]: No strong corpus evidence; results are internal to the BioASQ challenge.
- Break condition: If the RAG retrieval quality degrades or the domain shifts significantly, fine-tuning or context augmentation may become beneficial.

### Mechanism 3
- Claim: Open-source models can be self-hosted, making them viable for enterprise/clinical use where data privacy is crucial.
- Mechanism: Self-hosting eliminates the need to share sensitive data with third-party API providers, satisfying regulatory and confidentiality requirements.
- Core assumption: The open-source model's performance is sufficient for the task when properly deployed and does not require ongoing API calls.
- Evidence anchors:
  - [abstract]: "Open-Source LLMs can also be self-hosted, which makes them interesting for enterprise and clinical use cases where sensitive data should not be processed by third parties."
  - [section 3.1]: Mentions using fireworks.ai for hosting Mixtral 8x7B and discusses cost/speed trade-offs.
  - [corpus]: No direct corpus evidence; assumption based on licensing and deployment choices.
- Break condition: If model performance degrades in self-hosted environments due to hardware limitations or if compliance requirements change.

## Foundational Learning

- Concept: In-context learning (zero-shot and few-shot)
  - Why needed here: The study compares model performance under different in-context learning regimes, which is central to the results.
  - Quick check question: Can you explain the difference between zero-shot and few-shot learning and why few-shot might help in domain-specific tasks?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: The BioASQ challenge uses a RAG setup, and the paper explores how this interacts with model performance.
  - Quick check question: How does RAG help mitigate the knowledge limitations of pre-trained LLMs?

- Concept: Model fine-tuning (adapter-based, e.g., QLoRa)
  - Why needed here: The paper tests QLoRa fine-tuning to see if it improves performance, but finds limited gains.
  - Quick check question: What are the trade-offs between full fine-tuning and adapter-based fine-tuning in terms of cost and performance?

## Architecture Onboarding

- Component map:
  Elasticsearch index of PubMed baseline (title + abstract fields) -> Query generation (1-shot or 10-shot prompts) -> Document retrieval (top 50 per query) -> Snippet extraction (zero-shot from retrieved docs) -> Snippet reranking (top 10 selection) -> Question answering (with or without gold snippets) -> Optional: Wikipedia context augmentation

- Critical path:
  1. Generate query from question (few-shot)
  2. Retrieve documents from Elasticsearch
  3. Extract snippets (zero-shot)
  4. Rerank snippets
  5. Answer question (with snippets as context)

- Design tradeoffs:
  - 1-shot vs 10-shot: More examples improve performance but increase token usage and cost.
  - Fine-tuning vs few-shot: Fine-tuning may overfit; few-shot is more flexible but requires good examples.
  - Wikipedia context: May help with entity disambiguation but can introduce noise if irrelevant.

- Failure signatures:
  - Poor query generation → low recall in document retrieval
  - Snippet extraction failures → empty or irrelevant context for QA
  - Inconsistent Wikipedia context → degraded performance in some batches

- First 3 experiments:
  1. Compare 1-shot vs 10-shot query generation on a held-out BioASQ batch to measure performance gain.
  2. Test snippet extraction with and without Wikipedia context on a sample of questions to assess consistency.
  3. Evaluate fine-tuned vs non-fine-tuned Mixtral on a small set of factoid questions to check if tuning helps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of few-shot examples for open-source LLMs to achieve competitive performance with commercial models in biomedical RAG tasks?
- Basis in paper: [explicit] The paper compares 1-shot and 10-shot learning approaches and notes that 10-shot examples fit in most models' context lengths, but does not systematically explore the optimal number of examples
- Why unresolved: The study only tested 1-shot and 10-shot settings, leaving the question of whether fewer or more examples might yield better results
- What evidence would resolve it: Systematic experiments varying the number of few-shot examples (e.g., 2, 5, 15, 20) while measuring performance metrics across different task types

### Open Question 2
- Question: Does the performance gap between commercial and open-source models in zero-shot settings persist across different domain-specific tasks beyond biomedical applications?
- Basis in paper: [explicit] The paper states that Mixtral 8x7B "failed to produce usable results in the zero-shot setting" while commercial models performed better
- Why unresolved: The study only examined one domain-specific application (biomedical RAG), limiting generalizability to other specialized domains
- What evidence would resolve it: Comparative zero-shot performance testing of commercial vs. open-source models across multiple domain-specific tasks (legal, financial, technical documentation, etc.)

### Open Question 3
- Question: What characteristics of few-shot examples most effectively improve open-source LLM performance in domain-specific RAG tasks?
- Basis in paper: [inferred] The paper uses few-shot examples but does not analyze which features of these examples (complexity, diversity, relevance to the target task) contribute most to performance gains
- Why unresolved: The study uses pre-selected few-shot examples without analyzing their characteristics or testing variations in example selection strategies
- What evidence would resolve it: Experiments systematically varying example characteristics (question difficulty, similarity to target questions, answer formats) while measuring performance impact

### Open Question 4
- Question: How does additional Wikipedia context affect open-source LLM performance when the knowledge is truly novel rather than likely present in pretraining data?
- Basis in paper: [explicit] The study adds Wikipedia context but acknowledges "Wikipedia might not be a good proxy knowledge base for doing domain-specific RAG for these models because they are probably already highly trained on Wikipedia data"
- Why unresolved: The study uses Wikipedia as a knowledge source despite acknowledging models may already know this content, limiting conclusions about knowledge augmentation
- What evidence would resolve it: Experiments using domain-specific knowledge bases with content unlikely to be in pretraining data (proprietary research databases, specialized technical documentation)

## Limitations
- Limited to BioASQ dataset, lacking broader domain validation
- Few-shot examples used for experiments are not provided, hindering exact reproduction
- Inconsistent effects of Wikipedia context augmentation suggest implementation sensitivity

## Confidence

- **High confidence**: The comparative performance results between Mixtral 8x7B and commercial models in few-shot settings, as these are directly measured and consistent across tasks.
- **Medium confidence**: The claim that QLoRa fine-tuning and Wikipedia context do not provide consistent gains, as results show variability across batches that suggests implementation sensitivity.
- **Low confidence**: The broader generalizability of these findings to other biomedical datasets or question types beyond the BioASQ challenge.

## Next Checks

1. **Ablation study**: Run the RAG pipeline with isolated components disabled (e.g., Wikipedia context, fine-tuning) on a held-out BioASQ batch to measure individual contribution to performance.

2. **Example quality analysis**: Systematically vary the quality and diversity of few-shot examples used for query generation and measure performance degradation to establish robustness boundaries.

3. **Cross-dataset validation**: Test the same Mixtral 8x7B and commercial model configurations on a different biomedical QA dataset (e.g., MedQA) to assess generalizability beyond BioASQ.