---
ver: rpa2
title: Towards Continual Learning Desiderata via HSIC-Bottleneck Orthogonalization
  and Equiangular Embedding
arxiv_id: '2401.09067'
source_url: https://arxiv.org/abs/2401.09067
tags:
- learning
- tasks
- hsic
- conference
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses catastrophic forgetting in continual learning,
  where neural networks struggle to retain knowledge from previous tasks when learning
  new ones. The proposed method, CLDNet, tackles this by introducing two key components:
  HSIC-Bottleneck Orthogonalization (HBO) and EquiAngular Embedding (EAE).'
---

# Towards Continual Learning Desiderata via HSIC-Bottleneck Orthogonalization and Equiangular Embedding

## Quick Facts
- **arXiv ID**: 2401.09067
- **Source URL**: https://arxiv.org/abs/2401.09067
- **Reference count**: 20
- **Primary result**: Achieves competitive accuracy in class-incremental learning without access to previous task data or exemplar buffer, with minimal model expansion

## Executive Summary
This paper addresses catastrophic forgetting in continual learning by proposing CLDNet, which combines HSIC-Bottleneck Orthogonalization (HBO) and EquiAngular Embedding (EAE). The method tackles the fundamental challenge where neural networks forget previously learned tasks when adapting to new ones. CLDNet achieves multiple desiderata simultaneously: it operates without access to previous task data, requires minimal model expansion, and maintains a good balance between stability and plasticity. On CIFAR-100, it outperforms state-of-the-art rehearsal-based baselines by 7.54% while using only 1.02× the base model parameters and zero exemplar buffer.

## Method Summary
CLDNet tackles continual learning by introducing two key components that work synergistically. HBO minimizes feature bias through HSIC-based orthogonalization, encouraging learning of all possible features rather than just discriminative ones for the current task. This is achieved by balancing independence from current input with dependence on current output using Hilbert-Schmidt independence criterion. EAE replaces traditional trainable classifiers with predefined equiangular basis vectors on a unit hypersphere, allowing decision boundary adaptation without additional parameters. The combination enables non-overwritten parameter updates through HBO while maintaining effective decision boundaries through EAE, achieving the rare feat of satisfying multiple continual learning desiderata simultaneously.

## Key Results
- Achieves 7.54% higher accuracy than state-of-the-art rehearsal-based methods on CIFAR-100
- Maintains zero exemplar buffer requirement while achieving competitive performance
- Expands model size by only 1.02× compared to base model
- Successfully balances stability-plasticity tradeoff without accessing previous task data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HBO minimizes feature bias by balancing independence from current input with dependence on current output
- Mechanism: HSIC-Bottleneck Orthogonalization uses HSIC to measure nonlinear dependencies between hidden representations and both input (X) and output (Y). By minimizing HSIC(Zl, X) while maximizing HSIC(Zl, Y), HBO encourages learning all possible features rather than just discriminative ones.
- Core assumption: Feature bias is a primary driver of catastrophic forgetting, and balancing these HSIC terms creates hidden representations that are less task-specific and more generalizable
- Evidence anchors:
  - [abstract]: "HSIC-Bottleneck Orthogonalization (HBO) implements non-overwritten parameter updates mediated by Hilbert-Schmidt independence criterion in an orthogonal space"
  - [section]: "HBO encourages learning all possible features from a sequence of tasks, implying that some of the features that may not be sufficiently discriminative for the current task are also holistically considered"
  - [corpus]: Weak evidence - no direct corpus citations about HSIC-bottleneck for continual learning
- Break condition: If the HSIC estimates become unstable or if the balance between the two terms is incorrect (β not properly tuned)

### Mechanism 2
- Claim: EAE replaces trainable classifiers with predefined basis vectors to enhance decision boundary adaptation
- Mechanism: EquiAngular Embedding uses predefined equiangular basis vectors on a unit hypersphere. During inference, inputs are matched to their closest class-specific basis vector, avoiding the need for additional trainable parameters and preventing decision boundary distortion.
- Core assumption: Using predefined, non-trainable basis vectors provides sufficient discriminative power while avoiding parameter overwriting issues
- Evidence anchors:
  - [abstract]: "EquiAngular Embedding (EAE) enhances decision boundary adaptation between old and new tasks with predefined basis vectors"
  - [section]: "Unlike the trainable fully-connected layer with softmax, the EBVs is parameter-free since its learning objective is to minimize the spherical distance of learned representations with predefined basis vectors"
  - [corpus]: Weak evidence - no direct corpus citations about EAE for continual learning
- Break condition: If the predefined basis vectors cannot adequately represent the feature space for new tasks or if the angular separation becomes insufficient

### Mechanism 3
- Claim: The synergy between HBO and EAE achieves multiple desiderata simultaneously
- Mechanism: HBO handles parameter updates through orthogonalization and feature learning, while EAE handles decision making through predefined basis vectors. This separation allows for non-overwritten parameter updates while maintaining decision boundary adaptation.
- Core assumption: Separating parameter update concerns (HBO) from decision boundary concerns (EAE) allows each to be optimized independently without interfering with the other
- Evidence anchors:
  - [abstract]: "The synergy between two key components: HSIC-Bottleneck Orthogonalization (HBO) implements non-overwritten parameter updates... and EquiAngular Embedding (EAE) enhances decision boundary adaptation"
  - [section]: "Benefiting from the synergy between HBO and EAE, our CLDNet reaches multiple CL desiderata"
  - [corpus]: Weak evidence - no direct corpus citations about this specific synergy
- Break condition: If the interaction between HBO and EAE creates unintended dependencies or if one component's optimization negatively impacts the other

## Foundational Learning

- Concept: Hilbert-Schmidt Independence Criterion (HSIC)
  - Why needed here: HSIC provides a non-parametric way to measure statistical dependence between variables, which is crucial for the HBO component's objective function
  - Quick check question: What is the empirical estimation formula for HSIC given n i.i.d. samples?

- Concept: Orthogonal projection and gradient updates
  - Why needed here: Orthogonal projection is used to ensure parameter updates don't overwrite knowledge from previous tasks, implemented through the P^HSIC matrix
  - Quick check question: How does the orthogonal projector P^HSIC(l)(k+1) update recursively in batch learning?

- Concept: Equiangular basis vectors and spherical distance
  - Why needed here: EAE relies on predefined basis vectors with specific angular properties and uses spherical distance for classification
  - Quick check question: How are the equiangular basis vectors initialized and what constraint must they satisfy?

## Architecture Onboarding

- Component map: Backbone network f_θ (with L hidden layers) → HBO processing → EAE classifier σ → Output
- Critical path: Input → Forward pass through f_θ → Calculate HSIC terms for each layer → Apply orthogonal projection updates → Match to nearest basis vector via EAE
- Design tradeoffs: Using predefined basis vectors avoids parameter growth but may limit flexibility; balancing HSIC terms requires careful tuning of β
- Failure signatures: If catastrophic forgetting occurs despite HBO, likely the HSIC balance is incorrect; if classification accuracy drops, the EAE basis vectors may not be well-suited to the data
- First 3 experiments:
  1. Verify HBO works independently: Implement HBO with a simple cross-entropy classifier (not EAE) and check if parameter overwriting is reduced
  2. Verify EAE works independently: Implement EAE with a standard backbone (no HBO) and check if classification accuracy is maintained
  3. End-to-end test: Implement full CLDNet on a simple dataset (like split MNIST) and verify both desiderata (no forgetting, no parameter growth) are achieved

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important questions remain unaddressed regarding the practical implementation and broader applicability of CLDNet.

## Limitations

- The paper lacks detailed ablation studies on critical hyperparameters (β in HBO, basis vector initialization in EAE)
- No comparisons against rehearsal-based methods using minimal buffer sizes to contextualize the exemplar-free advantage
- Limited empirical validation to standard benchmarks without addressing more complex real-world scenarios

## Confidence

- **High confidence**: The theoretical formulation of HBO using HSIC and the mathematical derivation of orthogonal projection updates
- **Medium confidence**: The empirical results showing competitive accuracy on standard benchmarks
- **Low confidence**: The claim about synergy between HBO and EAE, as the paper lacks detailed ablation studies isolating each component's contribution

## Next Checks

1. Conduct ablation studies varying β in HBO to determine optimal balance between input independence and output dependence
2. Perform sensitivity analysis on EAE performance with different numbers of basis vectors and initialization strategies
3. Compare against rehearsal-based methods using minimal buffer sizes (1-5 exemplars per class) to better contextualize the exemplar-free advantage