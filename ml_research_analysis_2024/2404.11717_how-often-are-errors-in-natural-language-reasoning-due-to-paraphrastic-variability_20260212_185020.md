---
ver: rpa2
title: How often are errors in natural language reasoning due to paraphrastic variability?
arxiv_id: '2404.11717'
source_url: https://arxiv.org/abs/2404.11717
tags:
- examples
- paraphrases
- consistency
- paraphrastic
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a method for measuring how sensitive natural\
  \ language reasoning models are to different phrasings of the same problem, which\
  \ they call paraphrastic consistency. The authors propose a metric, PC, that quantifies\
  \ the probability that a model\u2019s predictions remain consistent across multiple\
  \ paraphrases of the same reasoning problem."
---

# How often are errors in natural language reasoning due to paraphrastic variability?

## Quick Facts
- arXiv ID: 2404.11717
- Source URL: https://arxiv.org/abs/2404.11717
- Reference count: 21
- Key outcome: Models exhibit significant sensitivity to paraphrasing, with PC values ranging from 0.8 to 0.9, showing pretraining improves consistency more than finetuning

## Executive Summary
This paper introduces a method for measuring how sensitive natural language reasoning models are to different phrasings of the same problem, which they call paraphrastic consistency. The authors propose a metric, PC, that quantifies the probability that a model's predictions remain consistent across multiple paraphrases of the same reasoning problem. They construct PARA NLU, a dataset of 7,782 human-verified paraphrases across two NLI tasks, and use it to evaluate several model classes. Results show that models exhibit significant sensitivity to paraphrasing, with PC values ranging from 0.8 to 0.9, and that pretraining improves consistency more than finetuning. Even high-performing models leave room for improvement in robustness to paraphrastic variability.

## Method Summary
The authors construct PARA NLU by collecting human-verified paraphrases of reasoning problems from α-NLI and δ-NLI datasets. They use Amazon Mechanical Turk to generate paraphrases, then validate them for label preservation. Models are trained on original NLI data and evaluated on PARA NLU paraphrases. The PC metric is computed using the formula PC = E[θ²] + E[(1-θ)²], where θ represents model confidence. They also compute ePC, a corrected version that accounts for stratified sampling based on model confidence. The analysis decomposes variance in model correctness into components attributable to paraphrasing versus inherent reasoning difficulty using the law of total variance.

## Key Results
- Models show paraphrastic consistency (PC) values ranging from 0.8 to 0.9, indicating significant sensitivity to phrasing
- Pretraining dramatically improves paraphrastic consistency compared to finetuning alone
- Models achieve higher consistency on automatically generated paraphrases than human-written ones
- The proportion of variance attributable to paraphrasing (PVAP) increases with pretraining but plateaus after 100M tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Paraphrastic consistency captures the proportion of model performance variance attributable to phrasing rather than reasoning ability
- Mechanism: By collecting multiple paraphrases of the same reasoning problem and measuring consistency across them, variance within paraphrase buckets (Var(R_i)) reflects sensitivity to phrasing, while variance across buckets (Var(θ_i)) reflects differences in reasoning difficulty
- Core assumption: Paraphrases preserve the underlying reasoning problem while varying surface form, so variance within buckets is due to paraphrasing
- Evidence anchors:
  - [abstract]: "We mathematically connect this metric to the proportion of a model's variance in correctness attributable to paraphrasing."
  - [section 2.2]: "Using the law of total variance... we decompose the total variance of the correctness... into two terms: the average variance of correctness within a bucket, E[Var(R_i)], and the variance in mean correctness across buckets, Var(E[R_i])."
  - [corpus]: Weak - no direct citations found in corpus neighbors
- Break condition: If paraphrases don't preserve reasoning problems, variance within buckets would reflect reasoning changes rather than phrasing sensitivity

### Mechanism 2
- Claim: Models trained on large pretraining corpora develop better paraphrastic consistency than those trained only on task-specific data
- Mechanism: Pretraining on diverse text exposes models to multiple ways of expressing the same meaning, building robustness to paraphrasing that finetuning alone doesn't provide
- Core assumption: Large-scale pretraining corpora contain diverse paraphrases and surface forms that teach models robustness to phrasing variations
- Evidence anchors:
  - [abstract]: "consistency dramatically increases with pretraining but not fine-tuning"
  - [section 8.1]: "As expected, pretraining on increasing amounts of data yields both monotonically increasing accuracy and paraphrastic consistency."
  - [corpus]: Weak - corpus neighbors don't directly address pretraining effects on paraphrastic consistency
- Break condition: If pretraining data lacks diversity or if models overfit to specific phrasings during pretraining

### Mechanism 3
- Claim: Models show higher paraphrastic consistency on automatically generated paraphrases than human-written ones
- Mechanism: Automatic paraphrase generation tends to produce simpler, more predictable transformations that align with patterns models learned during pretraining, while human paraphrases introduce more diverse and complex variations
- Core assumption: Automatic paraphrase generation models produce paraphrases that follow patterns more similar to pretraining data distribution than human-generated paraphrases
- Evidence anchors:
  - [section 6.3]: "On all datasets with the exception of δ-SOCIAL, we observe that models have a higher ePC value on automatically generated paraphrased examples than on human-elicited paraphrases."
  - [section 6]: "Human-written paraphrases in PARA NLU span all of the transformations delineated by Bhagat and Hovy (2013), sometimes involving more complex reasoning that falls between linguistic and world knowledge."
  - [corpus]: Weak - corpus neighbors don't address differences between human and automatic paraphrases
- Break condition: If automatic paraphrase generation models are improved to produce more diverse transformations, or if models are explicitly trained to handle human-like paraphrases

## Foundational Learning

- Concept: Law of Total Variance
  - Why needed here: To decompose model performance variance into components attributable to paraphrasing versus inherent reasoning difficulty
  - Quick check question: If a model has PC=0.9, what proportion of its variance in correctness is attributable to paraphrasing?

- Concept: Label-preserving paraphrasing
  - Why needed here: To create paraphrases that test model robustness to phrasing while maintaining the same reasoning problem
  - Quick check question: What's the difference between semantic equivalence and label-preserving paraphrasing in the context of NLI tasks?

- Concept: Stratified sampling based on model confidence
  - Why needed here: To ensure PARA NLU contains diverse examples ranging from easy to difficult, preventing bias in paraphrastic consistency measurement
  - Quick check question: Why is it important to sample examples from both high-confidence and low-confidence regions when building a consistency evaluation set?

## Architecture Onboarding

- Component map: ParaNLU dataset construction → Paraphrase validation → Model evaluation pipeline → PC metric computation
- Critical path: Collect original examples → Generate/collect paraphrases → Validate paraphrases → Evaluate models → Compute PC metrics
- Design tradeoffs: Human validation ensures quality but is expensive vs. automatic generation which is scalable but may not capture true paraphrase diversity
- Failure signatures: Low PC despite high accuracy suggests models rely on shallow patterns rather than robust reasoning; high PC but low accuracy suggests models understand phrasing but lack reasoning ability
- First 3 experiments:
  1. Measure PC on a simple bag-of-words model vs. RoBERTa to establish baseline consistency differences
  2. Test PC on automatically generated vs. human-written paraphrases of the same examples
  3. Measure PC changes as a model is finetuned on increasing amounts of task-specific data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does paraphrastic consistency improve with scale, or does it plateau for very large models?
- Basis in paper: [inferred] The paper shows that PC increases with pretraining tokens but grows rapidly only in early pretraining (1M-100M tokens) and then hugs a single PVAP curve past 100M tokens, suggesting a possible plateau.
- Why unresolved: The paper only tested up to 30B tokens and did not explore models significantly larger than RoBERTa-base.
- What evidence would resolve it: Testing paraphrastic consistency on much larger models (e.g., GPT-3, GPT-4) and measuring whether PC continues to increase or plateaus.

### Open Question 2
- Question: How does the inclusion of paraphrased examples during fine-tuning affect both accuracy and paraphrastic consistency?
- Basis in paper: [explicit] The paper suggests that over-reliance on evaluation using synthetically generated data may be misleading and that including paraphrased examples during fine-tuning may help models learn both the task and paraphrastic consistency.
- Why unresolved: The paper did not experiment with fine-tuning on paraphrased examples, only evaluated models trained on original data.
- What evidence would resolve it: Fine-tuning models on datasets augmented with paraphrased examples and measuring the impact on both accuracy and PC.

### Open Question 3
- Question: Are there specific linguistic or semantic properties of paraphrases that consistently lead to model errors?
- Basis in paper: [explicit] The paper notes that human-written paraphrases are more lexically and syntactically diverse than automatically generated ones, and models are less consistent on human paraphrases.
- Why unresolved: The paper did not perform a detailed error analysis to identify specific properties (e.g., syntactic complexity, lexical diversity, semantic similarity) that correlate with model errors.
- What evidence would resolve it: Analyzing model errors on human vs. automatic paraphrases to identify specific linguistic or semantic properties that contribute to inconsistency.

## Limitations

- Dataset construction process is underspecified, particularly AFLITE parameters and sampling strategy, which could introduce systematic biases
- The fundamental assumption that paraphrases preserve reasoning problems is not empirically validated
- Claims about pretraining superiority over finetuning lack rigorous causal evidence and remain correlational

## Confidence

- **High**: The mathematical formulation of the PC metric using law of total variance is sound and well-supported by the theoretical framework
- **Medium**: The empirical observation that models show higher consistency on automatic vs. human paraphrases is supported by results, though the underlying mechanism remains speculative
- **Low**: Claims about pretraining superiority over finetuning for paraphrastic consistency lack rigorous causal evidence and remain correlational

## Next Checks

1. **Cross-dataset consistency validation**: Evaluate the same models on an independently constructed paraphrase dataset for NLI tasks. If PC values are consistent across datasets, this strengthens confidence that the metric captures genuine model properties rather than dataset artifacts.

2. **Controlled paraphrase perturbation study**: Systematically vary paraphrase transformations (synonym replacement, word order changes, sentence restructuring) and measure PC sensitivity to each type. This would validate whether PC actually captures phrasing sensitivity or just general robustness to any perturbation.

3. **Pretraining ablation with controlled data**: Train identical models with controlled pretraining data that varies only in paraphrase diversity (e.g., Wikipedia vs. diverse paraphrase corpora). If paraphrastic consistency correlates specifically with paraphrase diversity rather than general pretraining scale, this would strengthen Mechanism 2.