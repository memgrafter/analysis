---
ver: rpa2
title: 'Don''t Do RAG: When Cache-Augmented Generation is All You Need for Knowledge
  Tasks'
arxiv_id: '2412.15605'
source_url: https://arxiv.org/abs/2412.15605
tags:
- retrieval
- knowledge
- context
- generation
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the default reliance on retrieval-augmented
  generation (RAG) by introducing cache-augmented generation (CAG), a retrieval-free
  approach that preloads all relevant documents into a long-context LLM's extended
  context and precomputes its KV cache. By eliminating real-time retrieval, CAG avoids
  retrieval latency and errors while maintaining high-quality responses.
---

# Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks

## Quick Facts
- arXiv ID: 2412.15605
- Source URL: https://arxiv.org/abs/2412.15605
- Reference count: 14
- CAG consistently outperforms or matches traditional RAG systems, particularly for constrained knowledge bases, with faster response times and reduced complexity

## Executive Summary
This paper challenges the default reliance on retrieval-augmented generation (RAG) by introducing cache-augmented generation (CAG), a retrieval-free approach that preloads all relevant documents into a long-context LLM's extended context and precomputes its KV cache. By eliminating real-time retrieval, CAG avoids retrieval latency and errors while maintaining high-quality responses. Experiments on SQuAD and HotPotQA datasets show that CAG consistently outperforms or matches traditional RAG systems, particularly for constrained knowledge bases, with faster response times and reduced complexity. The findings demonstrate that for certain applications, CAG offers a streamlined and efficient alternative to RAG.

## Method Summary
The paper proposes CAG as an alternative to RAG for knowledge-intensive tasks. Instead of retrieving relevant documents during inference, CAG preloads all documents into the LLM's extended context and precomputes the KV cache offline. This eliminates real-time retrieval, reducing latency and errors while maintaining answer quality. The approach was evaluated on SQuAD and HotPotQA datasets using Llama 3.1 8B with a 128K context window, comparing CAG against RAG baselines using BM25 and OpenAI Indexes retrieval with varying numbers of retrieved passages.

## Key Results
- CAG consistently achieved the highest BERTScore across most test conditions, outperforming both sparse and dense RAG methods
- CAG dramatically reduced generation time compared to in-context learning, with average response times of 2.44 seconds versus 15.66 seconds for standard approaches
- CAG eliminated retrieval errors entirely while maintaining comparable or superior context relevance to RAG systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preloading all relevant documents into the LLM's extended context eliminates retrieval latency and errors.
- Mechanism: By precomputing the KV cache for all documents upfront, the model can generate responses without waiting for real-time retrieval or risking incorrect document selection.
- Core assumption: The knowledge base is small enough to fit within the LLM's context window.
- Evidence anchors:
  - [abstract] "bypasses real-time retrieval" and "eliminates retrieval latency and errors"
  - [section] "Our method involves preloading all relevant resources... into the LLM's extended context and caching its runtime parameters"
  - [corpus] Weak - the corpus doesn't provide direct evidence for this specific mechanism
- Break condition: When the knowledge base exceeds the LLM's context window or when documents change frequently requiring constant recomputation

### Mechanism 2
- Claim: Using precomputed KV cache reduces generation time compared to in-context learning with dynamic KV computation.
- Mechanism: The KV cache is computed once offline and stored, avoiding the need to process the entire context during each inference call.
- Core assumption: The cost of computing and storing the KV cache is offset by faster inference times.
- Evidence anchors:
  - [section] "eliminates retrieval latency and minimizes retrieval errors while maintaining context relevance"
  - [section] "Table 3 also compares our CAG approach with standard in-context learning... The results demonstrate that CAG dramatically reduces generation time"
  - [corpus] Missing - no corpus evidence for this specific performance comparison
- Break condition: When the KV cache becomes too large to efficiently load into memory or when frequent updates make caching impractical

### Mechanism 3
- Claim: CAG provides a unified context that improves answer quality compared to RAG's fragmented retrieved passages.
- Mechanism: The model processes all relevant information holistically rather than as separate retrieved chunks, enabling better cross-document reasoning.
- Core assumption: Long-context LLMs can effectively process and reason over the entire preloaded context.
- Evidence anchors:
  - [section] "CAG consistently achieved the highest BERTScore in most cases, outperforming both sparse and dense RAG methods"
  - [section] "By preloading the entire knowledge collection into the LLM provides a holistic and coherent understanding"
  - [corpus] Weak - the corpus mentions "long-context LLMs" but doesn't specifically validate this holistic reasoning advantage
- Break condition: When the context becomes too long for the model to effectively attend to all relevant information, or when only a small subset of documents is relevant to a query

## Foundational Learning

- Concept: KV Cache computation and storage
  - Why needed here: CAG's efficiency relies on precomputing and storing KV caches rather than computing them during inference
  - Quick check question: What data structures are stored in a KV cache and how are they used during generation?

- Concept: Context window limitations
  - Why needed here: CAG only works when the knowledge base fits within the LLM's context window
  - Quick check question: How do you calculate the total token count of your knowledge base and compare it to the model's context window?

- Concept: BERTScore metric
  - Why needed here: The paper uses BERTScore to evaluate answer quality, so understanding this metric is crucial for interpreting results
  - Quick check question: What does BERTScore measure and why is it appropriate for evaluating QA systems?

## Architecture Onboarding

- Component map:
  Knowledge Base -> KV Cache Generator -> Cache Manager -> Inference Engine -> Storage Layer

- Critical path:
  1. Knowledge base preprocessing and KV cache computation (offline)
  2. Cache storage to persistent storage
  3. Cache loading during inference startup
  4. Query processing using preloaded cache
  5. Cache reset after inference completes

- Design tradeoffs:
  - Memory vs. Speed: Larger caches improve response quality but consume more memory
  - Freshness vs. Efficiency: Frequent cache updates improve accuracy but reduce efficiency gains
  - Granularity vs. Complexity: Caching at document level vs. chunk level affects both performance and implementation complexity

- Failure signatures:
  - Out-of-memory errors during cache loading
  - Degraded answer quality when context exceeds effective attention limits
  - Increased inference times due to cache swapping or loading delays
  - Stale responses when cache updates are not properly synchronized

- First 3 experiments:
  1. Measure inference latency and answer quality on SQuAD with varying context sizes to identify the optimal balance
  2. Compare BERTScore performance against RAG baselines with different retrieval strategies
  3. Test cache loading times and memory usage with progressively larger knowledge bases to establish practical limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal context window size for CAG across different knowledge domains and document types?
- Basis in paper: [explicit] The paper mentions that Llama 3.1's 32K-64K context window is "sufficient for storing knowledge sources such as internal company documentation, FAQs, customer support logs, and domain-specific databases" but notes this approach "becomes impractical for significantly larger datasets"
- Why unresolved: The paper doesn't empirically test varying context window sizes or provide guidance on when the approach becomes suboptimal. It only mentions the current model capabilities without exploring scalability limits.
- What evidence would resolve it: Systematic experiments varying document sizes, types, and context windows to determine performance degradation points and identify optimal configurations for different use cases.

### Open Question 2
- Question: How does CAG performance compare to hybrid approaches that combine preloading with selective retrieval for edge cases?
- Basis in paper: [inferred] The conclusion section explicitly suggests "potential for hybrid approaches that combine preloading with selective retrieval" and mentions "a system could preload a foundation context and use retrieval only to augment edge cases or highly specific queries"
- Why unresolved: The paper only evaluates pure CAG versus pure RAG systems without exploring hybrid approaches that could potentially offer the best of both worlds.
- What evidence would resolve it: Comparative experiments testing hybrid systems against both pure CAG and pure RAG across various query types and knowledge domains.

### Open Question 3
- Question: What is the relationship between knowledge base complexity and the narrowing performance gap between CAG and RAG as dataset size increases?
- Basis in paper: [explicit] The results section states "as the data size increases, the performance gap between CAG and RAG narrows slightly" and suggests this "aligns with prior findings that long-context LLMs may experience degradation when handling very long contexts"
- Why unresolved: The paper doesn't systematically investigate why the performance gap narrows or what specific aspects of knowledge base complexity (e.g., document diversity, query difficulty, semantic complexity) drive this effect.
- What evidence would resolve it: Controlled experiments varying knowledge base characteristics while holding dataset size constant to isolate the factors contributing to performance degradation in long-context scenarios.

## Limitations

- Context window scalability: CAG only works when the entire knowledge base fits within the LLM's context window, limiting its applicability to large-scale knowledge repositories
- KV cache management overhead: The paper doesn't fully address the computational and memory costs of generating and maintaining precomputed KV caches, especially for frequently updated knowledge bases
- Limited comparative analysis: The evaluation focuses on basic RAG variants without comparing against more sophisticated retrieval-free approaches or advanced RAG techniques with re-ranking and hybrid retrieval

## Confidence

- High Confidence: The core claim that CAG eliminates retrieval latency and errors by preloading documents into context is well-supported by the experimental design and results
- Medium Confidence: The generalizability of CAG to larger knowledge bases and its practical efficiency advantages in real-world deployments remain uncertain
- Low Confidence: The assertion that CAG provides superior cross-document reasoning compared to RAG's fragmented passages lacks direct evidence beyond indirect BERTScore performance

## Next Checks

1. **Scalability Test**: Evaluate CAG performance on progressively larger knowledge bases (100, 1,000, 10,000 documents) to empirically determine the context window limitation and identify the point where RAG becomes more efficient than CAG

2. **KV Cache Overhead Analysis**: Measure the complete lifecycle costs of CAG including offline KV cache computation time, storage requirements, cache loading times, and memory usage during inference, comparing these against RAG's retrieval overhead for various knowledge base sizes

3. **Document Organization Impact**: Conduct controlled experiments varying document ordering, segmentation strategies, and organization within the context window to quantify how these factors affect answer quality and to identify optimal document arrangement patterns for CAG