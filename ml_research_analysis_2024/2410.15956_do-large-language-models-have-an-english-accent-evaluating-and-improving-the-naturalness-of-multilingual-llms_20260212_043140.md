---
ver: rpa2
title: Do Large Language Models Have an English Accent? Evaluating and Improving the
  Naturalness of Multilingual LLMs
arxiv_id: '2410.15956'
source_url: https://arxiv.org/abs/2410.15956
tags:
- naturalness
- language
- english
- chinese
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces new metrics to evaluate the lexical and syntactic
  naturalness of multilingual LLM outputs at the corpus level. These metrics measure
  the divergence between LLM-generated and human-written text distributions, avoiding
  reliance on external embedding models.
---

# Do Large Language Models Have an English Accent? Evaluating and Improving the Naturalness of Multilingual LLMs

## Quick Facts
- arXiv ID: 2410.15956
- Source URL: https://arxiv.org/abs/2410.15956
- Reference count: 28
- Key outcome: All evaluated models exhibit English-centric patterns in non-English outputs, with Chinese showing the largest naturalness gap

## Executive Summary
This paper addresses the issue of English-centric bias in multilingual LLM outputs by introducing new metrics to evaluate lexical and syntactic naturalness at the corpus level. The authors develop a benchmark using topically aligned Wikipedia data in English, French, and Chinese to evaluate state-of-the-art models including Llama, Qwen, and Mistral. Results show that all models exhibit English-centric patterns in non-English outputs, with Chinese showing the largest naturalness gap. To address this, the authors propose a preference tuning approach using DPO with synthetically manipulated preference data, demonstrating consistent improvements in naturalness across models and tasks without compromising general capabilities.

## Method Summary
The authors create a benchmark using topically aligned Wikipedia data in English, French, and Chinese (3,722 entries). They evaluate models including Llama-3.1-8B, Qwen2-7B, Qwen1.5-7B, Mistral-v0.3-7B, Mistral-Nemo-12B, and Llama-3-8B. Lexical naturalness is measured using Jensen-Shannon Divergence between vocabulary distributions, while syntactic naturalness uses Maximum Mean Discrepancy with Weisfeiler-Lehman graph kernel on dependency trees. To improve naturalness, they apply Direct Preference Optimization with synthetically manipulated preference data through paraphrasing and back-translation via English. The tuned models are evaluated on both naturalness metrics and general capability benchmarks.

## Key Results
- All evaluated models exhibit English-centric patterns in non-English outputs
- Chinese outputs show the largest naturalness gap compared to human-written text
- Preference tuning with synthetic data consistently improves naturalness across models and tasks
- General capabilities are maintained or slightly improved after naturalness alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Jensen-Shannon Divergence between vocabulary distributions effectively measures lexical naturalness by quantifying distributional differences between human and model-generated texts.
- Mechanism: By comparing the frequency distributions of words in human-written text versus LLM-generated text, the JSD captures how closely the model's vocabulary usage aligns with natural language patterns.
- Core assumption: The vocabulary distribution of naturally written text by native speakers serves as a valid reference distribution for measuring naturalness.
- Evidence anchors: [abstract] "We develop new metrics to evaluate the lexical and syntactic naturalness of LLM outputs in a multilingual setting" - [section 3.1] "We propose to measure the lexical naturalness of an LLM by comparing the vocabulary distribution of its generated text with that of human-written text"
- Break condition: If the human reference data itself contains non-native patterns or if the vocabulary distributions are too sparse to reliably compute divergence.

### Mechanism 2
- Claim: Weisfeiler-Lehman graph kernel combined with Maximum Mean Discrepancy effectively measures syntactic naturalness by comparing structural patterns in dependency trees.
- Mechanism: The WL kernel captures hierarchical structural similarities between dependency trees by iteratively refining node labels based on neighboring nodes, while MMD compares the distributions of these structural patterns between human and model-generated sentences.
- Core assumption: Dependency tree structures parsed using Universal Dependencies grammar provide a language-agnostic representation of syntactic patterns that can be meaningfully compared across languages.
- Evidence anchors: [abstract] "Our proposed metric is based on representing each sentence as a dependency tree, where nodes correspond to words and edges specify the dependency relations between them" - [section 3.2] "We propose to use the Weisfeiler-Lehman (WL) graph kernel... The resulting value provides a measure of syntactic divergence between the human-generated and model-generated sentences"
- Break condition: If the dependency parsing introduces language-specific biases or if the WL kernel fails to capture meaningful syntactic differences at the corpus level.

### Mechanism 3
- Claim: Preference tuning with synthetically manipulated preference data effectively improves naturalness without compromising general capabilities.
- Mechanism: By contrasting human-written responses with synthetically manipulated unnatural responses using DPO, the model learns to prefer more natural linguistic patterns while maintaining its ability to solve general tasks.
- Core assumption: Synthetic manipulation of text through paraphrasing and back-translation creates sufficiently unnatural examples that the model can learn to distinguish from natural text.
- Evidence anchors: [abstract] "We propose a preference tuning approach using DPO with synthetically manipulated preference data... Experiments show consistent improvements in naturalness across models and tasks without compromising general capabilities" - [section 5] "To generate the unnatural, i.e., rejected, response we apply synthetic manipulations through paraphrasing and back-translation via English"
- Break condition: If the synthetic manipulation doesn't create sufficiently unnatural examples or if the preference tuning overfits to the specific dataset used.

## Foundational Learning

- Concept: Jensen-Shannon Divergence
  - Why needed here: Provides a symmetric, bounded measure of difference between two probability distributions, which is essential for comparing vocabulary distributions between human and model-generated texts.
  - Quick check question: What property of JSD makes it particularly suitable for comparing language model outputs to human text when the vocabularies might not perfectly overlap?

- Concept: Dependency Parsing and Universal Dependencies
  - Why needed here: Enables consistent syntactic analysis across multiple languages, allowing for meaningful comparison of grammatical structures between human and model-generated text.
  - Quick check question: How does the Universal Dependencies framework ensure that syntactic comparisons are meaningful across typologically different languages like English, French, and Chinese?

- Concept: Graph Kernels (specifically Weisfeiler-Lehman)
  - Why needed here: Provides a way to measure structural similarity between dependency trees by comparing their hierarchical substructures, which captures syntactic patterns that simple string-based metrics miss.
  - Quick check question: Why is the iterative refinement of node labels in the WL kernel important for capturing syntactic structure at multiple levels of granularity?

## Architecture Onboarding

- Component map: Wikipedia data extraction -> preprocessing (tokenization, language detection) -> metric computation -> LLM API calls for generation -> prompt formatting -> temperature sampling -> dataset construction (back-translation/paraphrasing) -> DPO training with LoRA -> evaluation

- Critical path: Wikipedia data preprocessing -> metric computation -> model benchmarking -> preference tuning -> evaluation

- Design tradeoffs:
  - Using corpus-level metrics vs. instance-level metrics: Corpus metrics capture statistical patterns but lose fine-grained information about individual generations
  - Word-level vs. subword-level vocabulary analysis: Word-level is more interpretable but may miss morphological patterns
  - Synthetic manipulation vs. human-annotated preferences: Synthetic is scalable but may not capture all aspects of naturalness

- Failure signatures:
  - JSD values remain high despite preference tuning -> synthetic manipulation isn't creating sufficiently unnatural examples
  - Syntactic divergence varies widely across different UD treebanks -> parsing inconsistencies across languages
  - BLEU/ROUGE scores don't correlate with naturalness metrics -> need for corpus-level evaluation methods

- First 3 experiments:
  1. Compute JSD and syntactic divergence on a small sample of human-written Wikipedia text vs. model-generated text to verify the metrics are working as expected
  2. Generate text with different temperatures (0.3, 0.6, 0.9) for one model-language pair to observe the impact on naturalness metrics
  3. Run a small preference tuning experiment (1 epoch) on one model-task pair to verify the training pipeline works before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed naturalness metrics perform when applied to polysynthetic or agglutinative languages that require morpheme-level rather than word-level analysis?
- Basis in paper: Explicit - The authors acknowledge in the Limitations section that their current lexical naturalness metric operates at the word level, which may not be suitable for polysynthetic and agglutinative languages that feature richer morphological structures.
- Why unresolved: The paper only evaluates the metrics on English, French, and Chinese, which are not polysynthetic or agglutinative languages. The authors note that adapting the metrics would require morphological parsing, which is challenging for these language types.
- What evidence would resolve it: Empirical testing of the metrics on languages like Turkish, Inuktitut, or Swahili, along with comparison to human judgments of naturalness in these languages, would demonstrate whether the metrics need modification for different morphological structures.

### Open Question 2
- Question: Does the improvement in naturalness through preference tuning generalize across different domains beyond essay generation and question answering?
- Basis in paper: Inferred - The authors state that their alignment approach has only been tested on essay generation and general domain question-answering tasks, and they express concern about applying it to more knowledge-intensive tasks due to potential unintended knowledge editing and increased hallucination risks.
- Why unresolved: The experiments were limited to two specific task types, and the authors explicitly call for future research to extend the method to other domains.
- What evidence would resolve it: Systematic testing of the preference tuning approach on domains like code generation, medical diagnosis, or scientific reasoning, with evaluation of both naturalness improvements and retention of task-specific accuracy, would determine the method's generalizability.

### Open Question 3
- Question: What is the optimal balance between naturalness and general capability performance when aligning models, and how does this trade-off vary across languages?
- Basis in paper: Explicit - The authors evaluate their aligned models on the CMMLU benchmark to ensure general capabilities are not compromised, noting that naturalness does not always positively correlate with language understanding performance (e.g., Qwen2 achieves higher CMMLU scores but lower naturalness than Llama-3.1).
- Why unresolved: While the authors show that naturalness alignment can be achieved without sacrificing general capabilities, they do not explore whether pushing for even higher naturalness might eventually degrade other capabilities, or whether this trade-off differs across languages.
- What evidence would resolve it: Systematic ablation studies varying the strength of naturalness alignment across different languages, coupled with comprehensive evaluation of both naturalness metrics and task-specific benchmarks, would reveal the optimal balance and whether it varies by language.

## Limitations

- The synthetic preference data generation process assumes that back-translation and paraphrasing through English creates sufficiently unnatural examples, but this assumption remains unverified
- The evaluation methodology relies heavily on corpus-level divergence metrics that have not been validated against human judgments of naturalness
- The approach has only been tested on essay generation and question-answering tasks, limiting generalizability to other domains

## Confidence

**High Confidence**: The observation that multilingual LLMs exhibit English-centric patterns in non-English outputs is well-supported by the empirical results. The methodology for computing lexical and syntactic divergence is clearly specified and reproducible.

**Medium Confidence**: The claim that preference tuning improves naturalness without compromising general capabilities is supported by the experimental results, but the evaluation of "general capabilities" relies on relatively narrow task sets (Chinese essay generation and open-domain QA). The long-term effects of this tuning approach on other capabilities remain unclear.

**Low Confidence**: The assertion that the proposed metrics provide a "holistic" evaluation of multilingual LLM naturalness is not fully substantiated. While the metrics capture distributional differences, they may miss other aspects of naturalness such as pragmatic appropriateness or cultural context that are particularly important in cross-lingual settings.

## Next Checks

1. **Human Evaluation Validation**: Conduct a human study where native speakers of French and Chinese rate the naturalness of model-generated text before and after preference tuning. Compare these ratings with the proposed JSD and syntactic divergence metrics to validate whether the metrics correlate with human perceptions of naturalness.

2. **Cross-Model Generalization Test**: Apply the preference tuning approach to a model with known strong multilingual capabilities (such as BLOOM or GPT-4) to determine whether the improvements are specific to the models tested or represent a more general solution to the English-centric output problem.

3. **Longitudinal Capability Assessment**: Evaluate the tuned models on a broader range of tasks beyond essay generation and QA, including creative writing, technical documentation, and dialogue, to verify that the preference tuning doesn't introduce subtle degradations in capabilities that aren't captured by the current evaluation framework.