---
ver: rpa2
title: Instruction-tuned Large Language Models for Machine Translation in the Medical
  Domain
arxiv_id: '2408.16440'
source_url: https://arxiv.org/abs/2408.16440
tags:
- translation
- language
- llms
- source
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Instruction-tuning with specialized medical terminology improves
  machine translation quality in large language models, particularly in terminology
  accuracy and automatic evaluation metrics (BLEU, chrF, COMET), outperforming baseline
  models. Using parameter-efficient fine-tuning and quantization (QLoRA) on models
  like FLAN-T5, Llama-3, and Tower with a medical domain dataset, the instruction-tuned
  models significantly reduce translation errors and improve terminology consistency.
---

# Instruction-tuned Large Language Models for Machine Translation in the Medical Domain

## Quick Facts
- arXiv ID: 2408.16440
- Source URL: https://arxiv.org/abs/2408.16440
- Authors: Miguel Rios
- Reference count: 16
- Primary result: Instruction-tuning with specialized medical terminology significantly improves translation quality in large language models, particularly in terminology accuracy and automatic evaluation metrics.

## Executive Summary
This study explores instruction-tuning large language models (LLMs) for machine translation in the medical domain, focusing on improving terminology accuracy and translation quality. Using parameter-efficient fine-tuning (PEFT) and quantization (QLoRA), the research fine-tunes models like FLAN-T5, Llama-3, and Tower on a specialized medical dataset. The instruction-tuned models demonstrate superior performance in terminology accuracy and automatic evaluation metrics (BLEU, chrF, COMET) compared to baseline models. The study also introduces automatic error annotation and quality estimation techniques to assess translation quality without reference translations.

## Method Summary
The research employs instruction-tuning on large language models for medical domain translation using the EMEA corpus and IATE medical terminology database. The process involves fine-tuning baseline models (FLAN-T5, Llama-3, Tower) with QLoRA on a dataset of 60K segments, incorporating medical terminology glossaries. The models are evaluated using automatic metrics (BLEU, chrF, COMET), terminology accuracy, and quality estimation scores (COMETKiwi). The study also explores automatic error annotation using XCOMET to assess translation quality without reference translations.

## Key Results
- Instruction-tuned models outperform baseline models in terminology accuracy and automatic evaluation metrics (BLEU, chrF, COMET).
- Using QLoRA and PEFT significantly reduces computational resources while maintaining translation quality.
- Automatic error annotation and quality estimation effectively assess translation quality without reference translations.
- Llama-3 models exhibit over-generation issues, requiring post-processing to extract translation outputs.

## Why This Works (Mechanism)
None

## Foundational Learning
- **QLoRA (Quantized Low-Rank Adaptation)**: A parameter-efficient fine-tuning method that reduces memory usage by quantizing model weights and using low-rank adapters. Why needed: Enables fine-tuning large models on limited hardware resources. Quick check: Verify reduced memory usage during fine-tuning compared to full fine-tuning.
- **Instruction-tuning**: A method where models are fine-tuned on task-specific instructions to improve performance on downstream tasks. Why needed: Improves model's ability to follow instructions and perform specialized tasks like medical translation. Quick check: Compare performance on instruction-following tasks before and after instruction-tuning.
- **Automatic Error Annotation (XCOMET)**: A system that automatically annotates translation errors using machine learning models. Why needed: Provides a scalable way to assess translation quality without manual annotation. Quick check: Validate XCOMET annotations against human-annotated references.
- **Quality Estimation (COMETKiwi)**: A method to estimate translation quality without reference translations. Why needed: Enables quality assessment in low-resource settings where reference translations are unavailable. Quick check: Compare COMETKiwi scores with human quality assessments.
- **Medical Terminology Databases (IATE)**: Specialized databases containing medical terms and their translations. Why needed: Provides domain-specific terminology for instruction-tuning to improve translation accuracy. Quick check: Measure terminology accuracy improvement after incorporating IATE terms.
- **Parameter-Efficient Fine-Tuning (PEFT)**: A family of techniques that fine-tune only a small subset of model parameters. Why needed: Reduces computational cost while maintaining or improving model performance. Quick check: Compare performance and resource usage between PEFT and full fine-tuning.

## Architecture Onboarding

### Component Map
Instruction-tuning pipeline: Data Preparation -> Fine-tuning (QLoRA) -> Evaluation (Automatic Metrics, Terminology Accuracy, Quality Estimation)

### Critical Path
1. Prepare instruction-tuning dataset with medical terminology
2. Fine-tune baseline models using QLoRA for one epoch
3. Evaluate using automatic metrics, terminology accuracy, and quality estimation

### Design Tradeoffs
- **QLoRA vs Full Fine-tuning**: QLoRA reduces memory usage but may limit model capacity compared to full fine-tuning.
- **Automatic vs Manual Error Annotation**: Automatic annotation is scalable but may be less accurate than manual annotation.
- **Terminology Database Coverage**: Using comprehensive databases like IATE improves accuracy but may miss domain-specific terms not included.

### Failure Signatures
- Over-generation in Llama-3 models producing excessive tokens
- Inconsistent confidence levels in automatic error annotation (XCOMET)
- Limited coverage of medical terminology in IATE database

### First Experiments
1. Verify data preparation by checking terminology glossary integration and dataset balance.
2. Test fine-tuning on a small subset to ensure QLoRA configuration works correctly.
3. Evaluate a single model using automatic metrics to confirm evaluation pipeline functionality.

## Open Questions the Paper Calls Out
- How does the performance of instruction-tuned LLMs compare when using different terminology sources beyond IATE, such as medical ontologies like MeSH or UMLS?
- What is the effect of different prompt engineering strategies on reducing over-generation in Llama-3 models during translation tasks?
- How does the terminology accuracy of instruction-tuned models vary with the size of the fine-tuning dataset in the medical domain?
- What is the impact of using multilingual terminology databases on the performance of instruction-tuned models for low-resource language pairs?

## Limitations
- Exact prompt templates for instruction-tuning are unspecified, critical for reproduction.
- Automatic error annotation shows inconsistent confidence levels, particularly for less frequent error categories.
- The study does not explore the impact of varying fine-tuning dataset sizes on terminology accuracy.

## Confidence
- Method reproducibility: Medium (due to unspecified prompt templates and preprocessing steps)
- Automatic error annotation reliability: Medium (inconsistent confidence levels reported)
- Terminology database coverage: Medium (limited to IATE, no exploration of other sources)

## Next Checks
1. Verify data preparation by checking terminology glossary integration and dataset balance.
2. Test fine-tuning on a small subset to ensure QLoRA configuration works correctly.
3. Evaluate a single model using automatic metrics to confirm evaluation pipeline functionality.