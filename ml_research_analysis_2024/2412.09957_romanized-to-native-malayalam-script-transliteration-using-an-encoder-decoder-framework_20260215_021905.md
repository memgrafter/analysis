---
ver: rpa2
title: Romanized to Native Malayalam Script Transliteration Using an Encoder-Decoder
  Framework
arxiv_id: '2412.09957'
source_url: https://arxiv.org/abs/2412.09957
tags:
- transliteration
- test
- script
- malayalam
- typing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents a reverse transliteration model to convert romanized
  Malayalam text to native script using an attention-based Bi-LSTM encoder-decoder
  framework. The model was trained on a combined dataset of 4.3 million transliteration
  pairs from Dakshina and Aksharantar datasets.
---

# Romanized to Native Malayalam Script Transliteration Using an Encoder-Decoder Framework

## Quick Facts
- arXiv ID: 2412.09957
- Source URL: https://arxiv.org/abs/2412.09957
- Reference count: 2
- Primary result: Attention-based Bi-LSTM encoder-decoder achieves 7.4% CER on standard typing but 22.7% CER on adhoc patterns

## Executive Summary
This study presents a reverse transliteration model for converting romanized Malayalam text to native script using an attention-based Bi-LSTM encoder-decoder framework. The model was trained on a combined dataset of 4.3 million transliteration pairs from Dakshina and Aksharantar datasets. Evaluation on IndoNLP-2025-Shared-Task datasets revealed strong performance on standard typing patterns (7.4% CER) but significantly higher error rates on adhoc patterns with missing vowels (22.7% CER). The model demonstrates that character-level bidirectional processing with attention can effectively handle standard romanization but struggles with non-standard input styles, highlighting the need for more diverse training data.

## Method Summary
The approach uses an attention-based Bi-LSTM encoder-decoder framework to convert romanized Malayalam to native script. The encoder processes input sequences up to 57 characters through bidirectional LSTM layers to capture context from both directions, while the attention mechanism enables the decoder to focus on relevant input parts during transliteration generation. The model was trained on a combined dataset of 4.3 million transliteration pairs from Dakshina (244K pairs) and Aksharantar (4.1M pairs) datasets. Character-level tokenization was employed with separate tokenizers for Latin characters and Malayalam characters. The architecture includes embedding layers, dense layers, and time-distributed dense output layers with 76 possible output characters.

## Key Results
- 7.4% character error rate on standard typing patterns (Test Set-1)
- 22.7% character error rate on adhoc patterns with missing vowels (Test Set-2)
- High performance on standard patterns but significant degradation on non-standard input styles
- Character Error Rate (CER) identified as more reliable metric than WER and BLEU for transliteration evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attention-enabled Bi-LSTM encoder-decoder framework captures both character-level context and long-range dependencies in romanized Malayalam input, enabling accurate native script conversion.
- Mechanism: The encoder processes the input sequence with bidirectional LSTM layers, capturing context from both directions, while the attention mechanism allows the decoder to focus on relevant parts of the encoder output during transliteration generation.
- Core assumption: Character-level context and bidirectional information flow are sufficient to disambiguate romanized Malayalam text into correct native script characters.
- Evidence anchors:
  - [abstract] The model "leverages an attention-based bidirectional Long Short Term Memory (Bi-LSTM) encoder-decoder framework"
  - [section] "The architecture begins with the encoder input layer, which accepts input sequences of up to 57 characters... A bidirectional LSTM layer processes the embedded input sequences to capture information from both past and future characters"
- Break condition: When romanized input contains ambiguous patterns that cannot be resolved through character-level context alone, such as adhoc typing patterns with missing vowels.

### Mechanism 2
- Claim: The combined dataset of 4.3 million transliteration pairs from Dakshina and Aksharantar provides sufficient training data to learn standard typing patterns effectively.
- Mechanism: Large-scale training data enables the model to learn statistical mappings between romanized and native script representations, particularly for common transliteration patterns.
- Core assumption: The training data distribution adequately represents the target domain's romanization patterns.
- Evidence anchors:
  - [abstract] "To train the model, we have used curated and combined collection of 4.3 million transliteration pairs derived from publicly available Indic language transliteration datasets"
  - [section] "The Dakshina dataset comprises of 244 thousand single word transliteration pairs, while the Aksharantar dataset adds a significantly larger volume of 4.100 million pairs"
- Break condition: When test data contains patterns significantly different from training data, such as adhoc typing patterns with vowel omissions.

### Mechanism 3
- Claim: Character Error Rate (CER) provides a more reliable evaluation metric for transliteration quality than WER and BLEU, which are overly sensitive to minor character variations.
- Mechanism: CER measures individual character-level accuracy, while WER and BLEU penalize entire word errors even for minor character mistakes, potentially misrepresenting actual transliteration quality.
- Core assumption: Character-level accuracy is the most appropriate measure for evaluating transliteration systems.
- Evidence anchors:
  - [abstract] "we report CER, WER, and BLEU scores separately for each test set"
  - [section] "This does not indicate model inadequacy, but rather reflects the inherent limitations of WER and BLEU scores in evaluating sentence transliterations. As they penalize even minor character variations as complete word errors misrepresenting the transliteration quality"
- Break condition: When evaluating systems where word-level semantic preservation is more important than character-level accuracy.

## Foundational Learning

- Concept: Encoder-decoder architecture with attention
  - Why needed here: Enables the model to handle variable-length input and output sequences while focusing on relevant parts of the input during generation
  - Quick check question: What is the role of the attention mechanism in the decoder component?

- Concept: Bidirectional processing
  - Why needed here: Captures context from both left and right directions to disambiguate romanized characters that may map to different native script characters
  - Quick check question: How does bidirectional processing help resolve ambiguous romanized inputs?

- Concept: Character-level tokenization
  - Why needed here: Allows fine-grained control over the transliteration process and enables learning of character-level mappings between romanized and native scripts
  - Quick check question: Why is character-level tokenization preferred over word-level tokenization for transliteration tasks?

## Architecture Onboarding

- Component map: Input layer (57 chars) -> Embedding (64D) -> Bi-LSTM Encoder (256D) -> Dense (128D) -> Context vector -> Repeat vector -> LSTM Decoder with attention -> Concatenated decoder output and attention -> Time-distributed dense (76 chars) -> Output layer

- Critical path: Input → Embedding → Bi-LSTM Encoder → Attention → LSTM Decoder → Output

- Design tradeoffs:
  - Character-level vs word-level processing: Character-level provides finer control but may miss word-level patterns
  - LSTM vs Transformer: LSTM handles variable-length sequences well but may be slower than transformers
  - Attention mechanism: Improves focus on relevant input but adds computational complexity

- Failure signatures:
  - High CER on adhoc patterns: Model struggles with non-standard romanization
  - Inconsistent output for similar inputs: Model may not capture all contextual dependencies
  - Slow inference: May indicate attention mechanism overhead or inefficient implementation

- First 3 experiments:
  1. Train on Dakshina dataset alone to establish baseline performance
  2. Compare Bi-LSTM with standard LSTM to measure bidirectional benefit
  3. Test attention mechanism vs no-attention to quantify its contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific character pairs does the model most frequently misclassify when romanized forms are phonetically similar, and what patterns emerge in these errors?
- Basis in paper: [explicit] The paper mentions the model struggles to distinguish phonetically similar Malayalam characters represented using the same romanized form, specifically citing "character pairs such as"
- Why unresolved: The authors identify this as a problem but do not specify which exact character pairs are most problematic or provide quantitative data on error patterns.
- What evidence would resolve it: Detailed error analysis showing frequency of specific character pair misclassifications, with examples of common confusions and their relative occurrence rates.

### Open Question 2
- Question: How does the inclusion of adhoc typing patterns in training data affect model performance on both adhoc and standard typing patterns?
- Basis in paper: [explicit] The authors note that training data primarily covers standard typing patterns and lacks adhoc variations, which significantly constrains the model's ability to handle diverse input styles.
- Why unresolved: The study only evaluates the model trained on standard patterns and does not experiment with training on datasets containing adhoc patterns to measure the impact.
- What evidence would resolve it: Comparative evaluation showing CER, WER, and BLEU scores for models trained with varying proportions of adhoc patterns versus standard patterns on both Test Set-1 and Test Set-2.

### Open Question 3
- Question: Would incorporating a language model improve sentence-level transliteration performance beyond the current encoder-decoder architecture?
- Basis in paper: [explicit] The authors identify as a limitation that the model "lacks a language model that could capture word dependencies and improve overall sentence-level transliteration."
- Why unresolved: The study uses a pure encoder-decoder architecture without exploring whether contextual information from a language model would enhance performance.
- What evidence would resolve it: Performance comparison between the current model and variants incorporating language models, measuring improvements in WER and BLEU scores on sentence-level evaluation.

## Limitations
- Data Domain Representation: Training data lacks adhoc typing patterns, causing significant performance degradation on non-standard inputs with missing vowels
- Architecture Generalization: The model struggles with phonetically similar characters that share the same romanized representation
- Metric Interpretation: Claims about CER superiority over WER and BLEU require further validation across multiple transliteration tasks

## Confidence

- High Confidence: Bidirectional LSTM processing capturing context from both directions is well-established and the reported 7.4% CER on standard patterns is likely accurate
- Medium Confidence: Attention mechanism contribution to transliteration accuracy requires empirical validation through ablation studies
- Low Confidence: Claims about WER and BLEU being inherently unsuitable for transliteration evaluation lack comprehensive comparative analysis

## Next Checks
1. **Ablation Study on Attention Mechanism**: Train and evaluate identical models with and without the attention component on both test sets to quantify the exact contribution of attention to handling adhoc typing patterns.

2. **Training Data Diversity Analysis**: Systematically evaluate model performance on subsets of the training data to identify which patterns (standard vs. adhoc) are most predictive of test performance, and determine if the 4.3 million pairs contain sufficient diversity.

3. **Metric Correlation Study**: Conduct a comprehensive comparison of CER, WER, and BLEU across multiple transliteration tasks and languages to validate whether CER consistently provides more reliable quality assessment than the other metrics.