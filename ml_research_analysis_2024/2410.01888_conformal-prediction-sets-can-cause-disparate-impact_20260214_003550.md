---
ver: rpa2
title: Conformal Prediction Sets Can Cause Disparate Impact
arxiv_id: '2410.01888'
source_url: https://arxiv.org/abs/2410.01888
tags:
- coverage
- sets
- prediction
- groups
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the fairness of conformal prediction (CP)
  sets when used as decision aids for humans. While CP sets are intended to provide
  statistically rigorous uncertainty quantification, the authors show that they can
  lead to disparate impact in human decision-making.
---

# Conformal Prediction Sets Can Cause Disparate Impact

## Quick Facts
- arXiv ID: 2410.01888
- Source URL: https://arxiv.org/abs/2410.01888
- Reference count: 21
- Primary result: Conformal prediction sets can cause disparate impact in human decision-making, with equalized coverage actually worsening fairness outcomes

## Executive Summary
This paper investigates the fairness implications of using conformal prediction (CP) sets as decision aids for humans. Through randomized controlled trials with human participants across three classification tasks (image, text, and audio), the authors demonstrate that prediction sets can benefit different demographic groups unequally, resulting in disparate impact. Surprisingly, they find that prediction sets designed to ensure equalized coverage across groups actually exacerbate disparate impact compared to marginal CP. Instead, the authors propose equalizing set sizes across groups as a more effective fairness standard, which correlates strongly with reduced disparate impact. Their findings challenge the prevailing assumption in the CP literature that equalized coverage is sufficient for fairness.

## Method Summary
The study employs a randomized controlled trial design where human participants are presented with classification tasks and asked to make decisions with the aid of prediction sets generated by different CP methods. Participants (50 per treatment-task combination) are recruited via Prolific and complete simplified versions of three classification tasks: FACET (age classification from images), BiosBias (gender classification from biographies), and RA VDESS (gender classification from audio). Four treatments are tested: control (no prediction set), marginal CP (standard conformal prediction), conditional CP (equalized coverage), and avg-k (equalized set size). Hyperparameters for CP methods are tuned to achieve target coverage of 90% or minimal average set size. Statistical analysis uses Generalized Estimating Equations (GEE) to account for clustered responses, with disparate impact measured as accuracy improvement differences across protected groups.

## Key Results
- Prediction sets caused disparate impact in all three experimental tasks, benefiting some groups more than others
- Equalized coverage (conditional CP) significantly increased disparate impact compared to marginal CP, with odds ratios ranging from 2.29 to 3.94
- Equalizing set sizes (avg-k method) correlated strongly with reduced disparate impact and was associated with better fairness outcomes
- Set size differences between groups correlated strongly with disparate impact, while coverage differences showed no apparent correlation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prediction sets influence human accuracy more through set size than coverage
- Mechanism: Humans rely on the number of options in a prediction set to judge certainty; smaller sets with fewer options lead to better accuracy than larger sets, regardless of coverage levels
- Core assumption: Set size is a stronger signal for human decision-making than coverage guarantee
- Evidence anchors:
  - [abstract] states "prediction sets are larger when the model is more uncertain" and the authors find "set size has greater influence on human accuracy than coverage"
  - [section 6.2] shows "strong correlation between set size differences and ∆t" and "no apparent correlation between coverage difference and ∆t"
  - [corpus] shows related work on conformal prediction improving human decision making, supporting the general mechanism
- Break condition: If human participants rely more on coverage information than set size in future studies, or if set size differences no longer correlate with disparate impact

### Mechanism 2
- Claim: Equalizing coverage increases disparate impact by creating unequal set sizes
- Mechanism: When Mondrian CP equalizes coverage across groups, it must increase set sizes for harder groups (with lower accuracy) and decrease them for easier groups, creating larger set size disparities that translate to unequal accuracy improvements
- Core assumption: There is a tradeoff between equalizing coverage and equalizing set size, and set size differences drive human accuracy differences
- Evidence anchors:
  - [abstract] states "providing sets that satisfy Equalized Coverage actually increases disparate impact compared to marginal coverage"
  - [section 4.1] explains "To achieve group-wise conditional coverage...average set size must increase for h, and correspondingly decrease for e"
  - [section 6.2] shows "the conditional treatment which increases set size differences...also increases disparate impact"
- Break condition: If equalizing coverage can be achieved without increasing set size disparities, or if set size differences no longer correlate with disparate impact

### Mechanism 3
- Claim: Model bias propagates to humans through prediction sets, causing disparate impact
- Mechanism: When a model has different accuracy across groups, marginal CP produces different set sizes for each group, which leads to unequal accuracy improvements when humans use these sets as decision aids
- Core assumption: Model accuracy differences across groups translate to set size differences that affect human decision-making
- Evidence anchors:
  - [abstract] notes "prediction sets do not benefit all groups equally, while sets with Equalized Coverage lead to the most unfair outcomes"
  - [section 4.1] states "if a model has biased performance across groups, marginal CP will tend to produce unequal set sizes which will in turn provide uneven utility to human decision makers"
  - [section 6.1] shows "all three treatments caused disparate impact" and "both groups benefited from each treatment, but for the avg-k and marginal treatments the increase actually was commensurate across groups"
- Break condition: If model accuracy differences across groups no longer translate to set size differences, or if human decision-making becomes insensitive to set size differences

## Foundational Learning

- Concept: Conformal prediction and prediction sets
  - Why needed here: The entire study investigates fairness of conformal prediction sets as human decision aids
  - Quick check question: What is the main property that conformal prediction sets guarantee, and how do they differ from traditional point predictions?

- Concept: Disparate impact measurement
  - Why needed here: The study measures whether prediction sets cause unequal accuracy improvements across groups
  - Quick check question: How is disparate impact calculated in this study, and what does a value of zero represent?

- Concept: Generalized Estimating Equations (GEE)
  - Why needed here: The statistical analysis accounts for clustered responses from participants who provide multiple answers
  - Quick check question: Why is GEE preferred over simple averaging for analyzing this experimental data?

## Architecture Onboarding

- Component map: Experiment design -> Data collection (human participants) -> Statistical analysis (GEE) -> Results interpretation -> Recommendations
- Critical path: Model creation -> Set prediction generation -> Human experiment execution -> Disparate impact calculation -> Fairness conclusions
- Design tradeoffs: Using real human participants provides ecological validity but limits sample size and statistical power compared to synthetic data approaches
- Failure signatures: High variance in human responses, low statistical significance for individual groups, inconsistent results across different tasks
- First 3 experiments:
  1. Replicate the FACET experiment with a different age grouping scheme to test robustness
  2. Test whether equalizing singleton frequency instead of set size reduces disparate impact
  3. Apply the methodology to a regression task instead of classification to check generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the results differ if the experiments were conducted with participants who have domain expertise (e.g., doctors for medical image classification or linguists for text classification)?
- Basis in paper: [inferred] The paper uses general participants recruited through Prolific without domain expertise, and the tasks are simplified versions of real-world applications.
- Why unresolved: The study focuses on general human decision-making rather than specialized expertise, leaving the question of domain-specific performance unexplored.
- What evidence would resolve it: Conducting similar experiments with domain experts and comparing their accuracy improvements and disparate impact across treatments.

### Open Question 2
- Question: Would the findings about disparate impact generalize to regression tasks where conformal prediction sets output intervals instead of discrete class sets?
- Basis in paper: [explicit] The authors explicitly note their experiments only used classification tasks and state that other instantiations of conformal prediction may be susceptible to the same unfairness mechanism but are out-of-scope.
- Why unresolved: The study's scope is limited to classification, and the mechanisms of fairness in regression-based CP sets remain untested.
- What evidence would resolve it: Running experiments with regression tasks where CP outputs prediction intervals, measuring accuracy improvements and disparate impact across groups.

### Open Question 3
- Question: How would the fairness outcomes change if the model itself was debiased before generating prediction sets?
- Basis in paper: [explicit] The authors note that their models had inherent biases (∆Top-1 values in Table 1) which translated to unequal set sizes and disparate impact.
- Why unresolved: The study uses biased models as a given and examines the downstream effects of prediction sets, but does not explore whether debiasing the model first would mitigate these issues.
- What evidence would resolve it: Training debiased versions of the models used in the experiments and comparing the resulting accuracy improvements and disparate impact across treatments.

## Limitations
- Limited sample size of 50 participants per treatment-task combination constrains statistical power and generalizability
- Experiments only test one type of model bias (differential accuracy across groups), leaving other forms of bias unexplored
- Focus exclusively on classification tasks leaves open questions about applicability to regression or structured prediction

## Confidence
- CP sets cause disparate impact in human decision-making (High confidence): Multiple experimental replications across different modalities show consistent patterns of unequal accuracy improvements
- Equalizing coverage exacerbates disparate impact (High confidence): Statistical analysis shows odds ratios significantly above 1 for conditional coverage treatment across all tasks
- Set size equalization is a better fairness criterion than coverage equalization (Medium confidence): Strong correlations observed, but the causal mechanism linking set size to human accuracy remains to be directly validated

## Next Checks
1. Conduct experiments with manipulated set sizes (holding coverage constant) to directly test whether set size, rather than coverage, drives disparate impact
2. Test the methodology on regression tasks where output uncertainty quantification differs fundamentally from classification
3. Investigate whether equalizing singleton frequency provides benefits beyond set size equalization by conducting a dedicated experiment comparing these two approaches head-to-head