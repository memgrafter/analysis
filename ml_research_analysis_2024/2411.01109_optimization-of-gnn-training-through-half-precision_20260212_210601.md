---
ver: rpa2
title: Optimization of GNN Training Through Half-precision
arxiv_id: '2411.01109'
source_url: https://arxiv.org/abs/2411.01109
tags:
- half-precision
- spmm
- data
- performance
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing graph neural network
  (GNN) training using half-precision floating point to improve performance and reduce
  memory usage while maintaining accuracy. Current GNN systems underperform with half-precision
  due to value overflow issues, under-utilized hardware resources, and poor training
  performance.
---

# Optimization of GNN Training Through Half-precision

## Quick Facts
- arXiv ID: 2411.01109
- Source URL: https://arxiv.org/abs/2411.01109
- Authors: Arnab Kanti Tarafder; Yidong Gong; Pradeep Kumar
- Reference count: 40
- Achieves 2.30x speedup in training time and 2.67x memory savings over DGL (float-based) for GAT, GCN, and GIN models while maintaining similar accuracy

## Executive Summary
This paper addresses the challenge of optimizing graph neural network (GNN) training using half-precision floating point to improve performance and reduce memory usage while maintaining accuracy. Current GNN systems underperform with half-precision due to value overflow issues, under-utilized hardware resources, and poor training performance. The authors introduce HalfGNN, a half-precision based GNN system that proposes novel techniques including new vector operations for half-precision data types (half4 and half8) to improve data load and reduction performance, and discretized SpMM to overcome value overflow and provide workload balancing. These techniques improve hardware utilization, reduce memory usage, and remove atomic writes.

## Method Summary
The authors developed HalfGNN by implementing half2 vector data types with edge-feature mirroring and feature padding, then added discretized SpMM with batch-wise reduction and scaling to prevent overflow. They enhanced SDDMM performance with half4 and half8 vector types that reduce inter-thread communication and improve data-load performance. To address atomic write overhead, they implemented a staging buffer approach with follow-up kernels. The system was evaluated across five standard graph datasets (Cora, PubMed, Citeseer, Ogb-product, Reddit) and eleven additional datasets with generated features/labels, comparing against DGL float and half-precision baselines.

## Key Results
- Achieves on average 2.30x speedup in training time over DGL float-based implementation
- Reduces memory usage by 2.67x compared to DGL float-based implementation
- Maintains similar accuracy to float-based training while preventing NaN errors through discretized reduction scaling
- Half8 vector type achieves 1.67× speedup over half2-based SDDMM by reducing inter-thread communication

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Half-precision sparse kernels underperform float-based versions due to memory coalescing issues and underutilization of GPU resources.
- Mechanism: When using half-precision, each warp issues only 64 bytes of data-load, compared to 128 bytes for float. This prevents full memory coalescing. Additionally, arithmetic throughput in half-precision is sub-optimal without vector data types.
- Core assumption: The performance bottleneck stems from hardware-level data-load and compute inefficiencies when naively substituting half for float.
- Evidence anchors:
  - [abstract] Current GNN systems massively underperform while showing abnormal accuracy when using half-precision due to under-utilization of hardware resources.
  - [section 3.1.1] DGL half-precision SpMM is significantly slower than float SpMM, and DGL half-precision SDDMM does not improve runtime over float.
  - [corpus] The neighbor query shows high FMR scores for papers discussing low-precision training, indicating relevance but no direct citations for half-precision GNN optimization.
- Break condition: If the GPU hardware changes to support native half-precision memory coalescing or if the graph sparsity pattern becomes more regular.

### Mechanism 2
- Claim: Value overflow in SpMM with half-precision causes NaN training errors, which existing degree-norm scaling fails to prevent if applied post-reduction.
- Mechanism: Large neighborhood sizes in graphs cause dot product results to exceed half-precision's maximum representable value, producing INF. Subsequent operations like softmax on INF produce NaN. Degree-norm scaling must be applied during reduction, not after, to prevent overflow.
- Core assumption: GNN models contain built-in mechanisms (like degree-norm) that can prevent overflow if applied at the right time in the computation.
- Evidence anchors:
  - [abstract] Half-precision suffers from value overflow issues due to lowered precision, leading to NaN after a couple of iterations.
  - [section 3.1.3] GCN's degree-norm scaling divides output vertex features by vertex degree, but if applied after reduction, overflow has already occurred.
  - [section 5.2.2] Discretized reduction scaling applies degree-norm during reduction on batches of neighbors, protecting against overflow.
- Break condition: If the graph contains vertices with extremely large neighborhoods that exceed half-precision range even after discretization.

### Mechanism 3
- Claim: Half8 vector data type improves SDDMM performance by reducing inter-thread communication and increasing instruction-level parallelism.
- Mechanism: Half8 packs eight half-precision values, allowing each warp thread to load 8 features in one instruction. This reduces the number of rounds of inter-thread communication needed for reduction from 5 (half-only) to 2 (half8), while also increasing data-load instruction-level parallelism.
- Core assumption: The performance bottleneck in half-precision SDDMM is primarily due to excessive inter-thread communication for reduction.
- Evidence anchors:
  - [section 5.1.3] Half8 allows each warp thread to load 8 half features and reduces inter-thread communication rounds from 5 to 2.
  - [section 6.3.1] Half8 achieves 1.67× speedup over half2-based SDDMM by reducing inter-thread communication and improving data-load performance.
  - [corpus] No direct corpus evidence for half8 in GNN systems, indicating this is a novel contribution.
- Break condition: If the feature length is not a multiple of 8, requiring feature padding that negates the performance benefit.

## Foundational Learning

- Concept: Vector data types (half2, half4, half8)
  - Why needed here: Half-precision alone causes memory coalescing and compute throughput issues. Vector data types pack multiple half values to achieve full 128-byte memory coalescing and improve arithmetic throughput.
  - Quick check question: What is the size in bytes of a half4 vector type and how many half-precision values does it contain?

- Concept: Discretized reduction and scaling
  - Why needed here: Standard reduction in SpMM with half-precision causes overflow when neighborhood sizes are large. Discretized reduction applies scaling during reduction in batches, preventing overflow while maintaining workload balance.
  - Quick check question: In discretized reduction, at what point is degree-norm scaling applied compared to traditional post-reduction scaling?

- Concept: Atomic write overhead in half-precision
  - Why needed here: Half-precision atomic writes are more expensive than float atomic writes. Conflicting writes in SpMM can be resolved using staging buffers and follow-up kernels instead of atomics.
  - Quick check question: Why are half-precision atomic writes more costly than float atomic writes in GPU implementations?

## Architecture Onboarding

- Component map:
  - Half2 baseline kernel: Uses half2 vector type for data-load and computation, requires edge-feature mirroring and feature padding
  - Half4/Half8 kernels: Additional vector types for SDDMM, improve data-load and reduce inter-thread communication
  - Discretized SpMM: SpMM with batch-wise reduction and scaling to prevent overflow
  - Non-atomic write system: Staging buffer + follow-up kernel to handle conflicting writes
  - Shadow APIs: Custom implementations to ensure half-precision operations are used when safe

- Critical path: Data load → Computation (dot product) → Reduction → Data store
  - The reduction phase is the most critical for overflow prevention and performance optimization

- Design tradeoffs:
  - Half2 vs native half: Half2 provides better performance but requires careful data layout and mirroring
  - Discretized vs full reduction: Discretized prevents overflow but may introduce slight numerical differences
  - Staging buffer: Eliminates atomic write overhead but requires additional kernel launch and memory

- Failure signatures:
  - NaN accuracy after few epochs: Indicates overflow in SpMM reduction
  - No performance improvement over float: Indicates half-precision kernels not properly optimized (missing vector types, atomic writes still present)
  - Memory alignment issues: Indicates feature lengths not properly padded for vector types

- First 3 experiments:
  1. Measure memory bandwidth utilization and compute utilization of DGL half-precision SpMM vs HalfGNN SpMM on a small dataset to verify the 80.92% vs 20.22% improvement claim
  2. Run GCN training with discretized reduction disabled to confirm overflow causes NaN accuracy degradation
  3. Compare half8 vs half2 SDDMM runtime on a medium-sized dataset to verify the 1.67× average speedup claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the discretized reduction scaling technique be effectively extended to other GNN models beyond GCN and GIN, such as GraphSAGE or Graph Attention Networks (GAT), without compromising accuracy?
- Basis in paper: [explicit] The paper mentions that the discretized reduction scaling is applied to GCN and GIN models, and it is suggested that the approach could be applied to other GNN models. However, it is not explicitly tested or confirmed for other models.
- Why unresolved: The paper does not provide experimental results or theoretical analysis for other GNN models. The effectiveness of the technique for these models remains unverified.
- What evidence would resolve it: Experimental results demonstrating the accuracy and performance of the discretized reduction scaling technique on other GNN models, such as GraphSAGE or GAT, would provide evidence of its generalizability.

### Open Question 2
- Question: How does the choice of batch size in discretized reduction scaling affect the trade-off between computational efficiency and numerical stability in SpMM operations?
- Basis in paper: [explicit] The paper discusses the concept of discretized reduction scaling and mentions that the batch size can be chosen to allocate an equal number of computation cores to each discretization unit, leading to workload balancing. However, it does not provide detailed analysis on how different batch sizes impact performance and stability.
- Why unresolved: The paper does not explore the impact of varying batch sizes on the trade-off between computational efficiency and numerical stability. This leaves uncertainty about the optimal batch size for different scenarios.
- What evidence would resolve it: A comprehensive study analyzing the performance and stability of SpMM operations with different batch sizes in discretized reduction scaling would provide insights into the optimal choice for various applications.

### Open Question 3
- Question: What are the potential limitations or challenges of applying the half2 data type and vectorization techniques to other hardware architectures, such as CPUs or specialized accelerators, beyond GPUs?
- Basis in paper: [inferred] The paper focuses on GPU-specific optimizations using half2 data type and vectorization techniques. While these techniques are effective for GPUs, their applicability to other hardware architectures is not discussed.
- Why unresolved: The paper does not address the potential challenges or limitations of adapting these techniques to different hardware architectures. This leaves uncertainty about their broader applicability.
- What evidence would resolve it: Research and experimentation demonstrating the effectiveness and challenges of implementing half2 data type and vectorization techniques on CPUs or other accelerators would provide insights into their broader applicability and limitations.

## Limitations

- The 2.30× speedup claim depends heavily on specific GPU architectures with optimal half-precision support, which may not translate to all hardware platforms
- The staging buffer approach for non-atomic writes requires significant memory overhead that isn't fully characterized
- Generalizability of half-precision gains across different graph types and sizes remains uncertain, particularly for graphs with extreme degree distributions

## Confidence

- High confidence in the mechanism showing half-precision SpMM underperforms due to memory coalescing and compute throughput issues - supported by direct measurements and well-established GPU architecture principles
- Medium confidence in discretized reduction preventing overflow while maintaining accuracy - while the mechanism is sound, the specific batch size optimization appears to be empirically derived rather than theoretically proven
- Medium confidence in the 2.30× average speedup claim - based on evaluation results but dependent on specific hardware and workload characteristics that may not generalize

## Next Checks

1. Test HalfGNN on graphs with extreme degree distributions (both very low and very high) to verify overflow protection effectiveness and identify break conditions
2. Profile GPU resource utilization (SM, memory bandwidth) on additional GPU architectures to validate portability of the 2.30× speedup claim
3. Conduct ablation studies removing individual optimization components (vector types, discretized reduction, non-atomic writes) to quantify their individual contributions to overall performance gains