---
ver: rpa2
title: Redefining Proactivity for Information Seeking Dialogue
arxiv_id: '2410.15297'
source_url: https://arxiv.org/abs/2410.15297
tags:
- user
- proactive
- response
- information
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a novel definition of proactivity for Information-Seeking\
  \ Dialogue (ISD) that focuses on enhancing each response with new, related information\
  \ to sustain user engagement. The authors construct a dataset of 2,000 single-turn\
  \ dialogues and propose automatic metrics\u2014semantic similarity and user-simulation\
  \ based\u2014that correlate well with human judgments."
---

# Redefining Proactivity for Information Seeking Dialogue

## Quick Facts
- arXiv ID: 2410.15297
- Source URL: https://arxiv.org/abs/2410.15297
- Reference count: 40
- Primary result: Introduces a novel definition of proactivity for Information-Seeking Dialogue that focuses on enhancing responses with new, related information to sustain user engagement

## Executive Summary
This paper addresses the challenge of generating proactive responses in Information-Seeking Dialogue (ISD) systems. Unlike traditional dialogue systems that respond reactively to user queries, the authors propose a new definition of proactivity that requires each response to include not just an answer, but also additional related information or follow-up questions to sustain user engagement. They construct a dataset of 2,000 single-turn dialogues and introduce automatic metrics to evaluate response proactiveness. The paper presents two innovative Chain-of-Thought prompting strategies (3-step and 3-in-1) that outperform standard prompting by up to 90% in zero-shot settings, along with instruction tuning that further improves proactive response generation.

## Method Summary
The authors construct a proactive dialogue dataset by modifying short answers from the Natural Questions Question Answer (NQQA) dataset to include conversational elements and proactive components. They implement two Chain-of-Thought prompting strategies: the 3-step CoT which decomposes the task into answering the query, generating related information, and formulating a proactive element using three separate prompts; and the 3-in-1 CoT which combines these into a single prompt for improved efficiency. For evaluation, they introduce automatic metrics including semantic similarity-based scores (combining BERTScore between query and response with mean pairwise BERTScore of the response) and user-simulation based scores. They also demonstrate the efficacy of instruction tuning via QLoRA on their constructed corpus to improve proactive response generation.

## Key Results
- 3-step and 3-in-1 Chain-of-Thought prompting strategies outperform standard prompts by up to 90% in zero-shot settings
- Automatic metrics (semantic similarity and user-simulation based) achieve high correlation with human annotation
- Instruction tuning on the constructed corpus significantly improves proactive response generation
- Proactive responses sustain user interaction, increasing conversational turns and informativeness in multi-turn scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The 3-step Chain-of-Thought (CoT) prompting systematically decomposes proactive response generation into three simpler subtasks, improving zero-shot performance.
- Mechanism: The approach breaks down the task into (1) answering the query, (2) generating related information, and (3) formulating a proactive element. Each subtask is addressed by an independent prompt, with outputs chained together.
- Core assumption: Decomposing complex tasks into simpler, sequential steps reduces cognitive load on the LLM and ensures all required components are present in the final response.
- Evidence anchors:
  - [abstract]: "We introduce two innovative Chain-of-Thought (CoT) prompts, the 3-step CoT and the 3-in-1 CoT prompts, which consistently outperform standard prompts by up to 90% in the zero-shot setting."
  - [section]: "Our approach involves systematically decomposing the proactive response generation task into three distinct subtasks, each addressed by an independent prompt."
- Break condition: If the LLM fails to generate useful intermediate outputs in any step, the final response quality degrades significantly.

### Mechanism 2
- Claim: The semantic similarity-based metric effectively evaluates proactive response quality by measuring relevance and informativeness.
- Mechanism: The metric combines BERTScore between query and response with mean pairwise BERTScore of the response itself, weighted by hyperparameter α. This captures both query relevance and internal consistency.
- Core assumption: High-quality proactive responses will have high semantic similarity to the query while maintaining internal coherence.
- Evidence anchors:
  - [abstract]: "We introduce several automatic metrics to evaluate response 'proactiveness' which achieved high correlation with human annotation."
  - [section]: "The respective semantic scores for the FQ and AI are computed as follows: FQ: α ∗ BS(Q, R) + (1 − α) ¯BS(R)"
- Break condition: If the response contains highly relevant but contextually inappropriate information, semantic similarity may still score it highly.

### Mechanism 3
- Claim: Instruction tuning on the constructed corpus improves proactive response generation by learning the specific structure and criteria defined in the paper.
- Mechanism: The LLM is fine-tuned on 1000 proactive responses with explicit instructions, learning to generate responses that include both answers and proactive elements while adhering to defined criteria.
- Core assumption: Supervised learning on high-quality examples can teach the model to consistently generate responses meeting the defined proactive criteria.
- Evidence anchors:
  - [abstract]: "Utilizing our corpus, we demonstrate the efficacy of instruction-tuning in the context of proactive response generation."
  - [section]: "We instruction tuned an LLM via QLoRA to generate proactive responses. Leveraging our proposed corpus, we conducted instruction tuning on two distinct tasks corresponding to the generation of proactive responses with either a FQ or AI."
- Break condition: If the training data contains inconsistencies or the model overfits to specific patterns, generalization to new queries may suffer.

## Foundational Learning

- Concept: Chain-of-Thought prompting
  - Why needed here: The task requires generating complex responses with multiple components (answer + proactive element), which benefits from step-by-step reasoning.
  - Quick check question: How does breaking down proactive response generation into three steps improve LLM performance compared to direct prompting?

- Concept: Semantic similarity metrics (BERTScore)
  - Why needed here: Need automated way to evaluate whether generated responses are relevant to queries and internally coherent.
  - Quick check question: What are the two components of the semantic similarity score, and how do they capture different aspects of response quality?

- Concept: Instruction tuning for dialogue tasks
  - Why needed here: Standard LLMs are reactive; fine-tuning teaches them to generate proactive responses following specific criteria.
  - Quick check question: Why is instruction tuning particularly effective for teaching LLMs to generate proactive responses?

## Architecture Onboarding

- Component map: NQQA dataset → AMT annotation → validation → corpus → Prompt templates → LLM inference → response post-processing → Automatic metrics (semantic similarity, user simulation) → human correlation validation → User simulation + agent interaction → conversation analysis

- Critical path: Corpus construction → prompt development → evaluation metric design → generation experimentation → multi-turn validation

- Design tradeoffs:
  - 3-step vs 3-in-1 CoT: Latency vs. performance (3-step requires 3 inferences, 3-in-1 requires 1)
  - Semantic vs. user simulation metrics: Coverage of criteria vs. practical interpretability
  - Corpus size vs. annotation quality: 2000 samples balanced annotation cost with coverage

- Failure signatures:
  - Missing answer component: Response only contains proactive element
  - Generic proactive elements: Follow-up questions too broad, additional information too vague
  - Conversational unnaturalness: AI introduced abruptly rather than conversationally
  - Repetition in multi-turn: Same proactive element repeated across turns

- First 3 experiments:
  1. Compare 3-step CoT vs direct prompting on 100 test queries, measuring presence of proactive elements
  2. Evaluate semantic similarity metric correlation with human judgments on 200 annotated responses
  3. Test instruction tuning on 500 samples, comparing pre/post fine-tuning quality on held-out test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between semantic similarity and user simulation scores for demonstration selection in few-shot prompting?
- Basis in paper: [explicit] The paper shows that selecting demonstrations based on the sum of both scores results in balanced improvements across all criteria.
- Why unresolved: The paper does not explore the relative weighting between semantic similarity and user simulation scores, or whether different weights might be optimal for different types of proactive elements.
- What evidence would resolve it: Systematic experiments varying the relative weights between semantic similarity and user simulation scores in demonstration selection, and measuring the impact on final response quality.

### Open Question 2
- Question: How does the effectiveness of proactive responses vary across different types of user queries (e.g., factual vs. opinion-based, simple vs. complex)?
- Basis in paper: [inferred] The paper uses the Natural Questions dataset which contains factual queries, but does not analyze how proactive response generation performs on different query types.
- Why unresolved: The paper does not segment results by query type or analyze whether certain types of queries benefit more from proactive elements than others.
- What evidence would resolve it: Analysis of proactive response effectiveness stratified by query characteristics such as complexity, topic domain, and information density.

### Open Question 3
- Question: What is the optimal frequency and timing for introducing proactive elements in multi-turn conversations?
- Basis in paper: [explicit] The paper notes that not every turn in real-world ISD would warrant a proactive response and suggests this as future work.
- Why unresolved: The paper applies proactive prompts at every conversational turn in their multi-turn experiments but acknowledges this may not be optimal.
- What evidence would resolve it: Experiments varying when and how often proactive elements are introduced in multi-turn conversations, measuring user engagement and conversation quality.

## Limitations

- The constructed dataset of 2,000 single-turn dialogues may not capture the full diversity of information-seeking scenarios, particularly for complex or ambiguous queries.
- The semantic similarity metrics, though showing good correlation with human judgments, may miss nuanced aspects of conversational quality like tone and contextual appropriateness.
- The user simulation approach relies on pretrained language models that may not fully represent actual user behavior patterns.

## Confidence

**High confidence (7/10)**: The effectiveness of Chain-of-Thought prompting strategies in improving proactive response generation. The 90% performance improvement in zero-shot settings is well-supported by the experimental results, and the decomposition mechanism is clearly demonstrated.

**Medium confidence (5/10)**: The automatic evaluation metrics' correlation with human judgments. While the paper reports high correlation, the exact correlation coefficients and statistical significance tests are not provided, making it difficult to assess the robustness of these metrics.

**Medium confidence (6/10)**: The practical impact on multi-turn conversation sustainability. The paper shows increased conversational turns and informativeness, but the user simulation approach may not fully capture real user engagement patterns.

## Next Checks

1. **Dataset Diversity Validation**: Conduct a comprehensive analysis of the constructed dataset to ensure it covers a wide range of information-seeking scenarios, including complex queries, ambiguous questions, and domain-specific knowledge areas.

2. **Cross-LLM Generalization**: Test the proposed 3-step and 3-in-1 Chain-of-Thought prompting strategies across multiple LLM architectures (e.g., GPT, Claude, PaLM) to verify that the performance improvements are not model-specific.

3. **Real-User Interaction Study**: Replace the user simulation approach with actual human user studies to validate whether the observed increases in conversational turns and informativeness translate to improved user satisfaction and engagement in real-world scenarios.