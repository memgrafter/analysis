---
ver: rpa2
title: Do These LLM Benchmarks Agree? Fixing Benchmark Evaluation with BenchBench
arxiv_id: '2407.13696'
source_url: https://arxiv.org/abs/2407.13696
tags:
- benchmarks
- benchmark
- agreement
- reference
- benchbench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies significant methodological flaws in current\
  \ benchmark agreement testing (BAT) and proposes standardized best practices to\
  \ improve reliability. The authors analyze over 50 prominent benchmarks and demonstrate\
  \ that common BAT choices\u2014such as reference benchmark selection, model subsets,\
  \ and correlation metrics\u2014can dramatically alter agreement conclusions."
---

# Do These LLM Benchmarks Agree? Fixing Benchmark Evaluation with BenchBench

## Quick Facts
- **arXiv ID:** 2407.13696
- **Source URL:** https://arxiv.org/abs/2407.13696
- **Reference count:** 40
- **Primary result:** Proposes standardized best practices for benchmark agreement testing, demonstrating that common methodological choices can dramatically alter agreement conclusions and introducing BenchBench for reproducible evaluation.

## Executive Summary
This paper identifies significant methodological flaws in current benchmark agreement testing (BAT) and proposes standardized best practices to improve reliability. The authors analyze over 50 prominent benchmarks and demonstrate that common BAT choices—such as reference benchmark selection, model subsets, and correlation metrics—can dramatically alter agreement conclusions. They find that correlation scores vary widely depending on these choices, with standard deviations reaching 0.25 for small model subsets. To address these issues, they recommend using an aggregate reference benchmark, employing data-driven correlation thresholds, sampling more models randomly, and reporting multiple granularity levels.

The paper implements these guidelines in BenchBench, a Python package, and introduces the BenchBench-Leaderboard, a meta-benchmark for evaluating benchmark agreement. Using their methodology reduces BAT variance by 67%, providing more robust and reproducible benchmark evaluation. The authors make their code and data publicly available, enabling the community to apply these standardized practices to their own benchmark evaluations.

## Method Summary
The authors systematically analyzed over 50 benchmarks across 19 model types using 61 correlation metrics. They tested various BAT configurations including different reference benchmarks, model subsets, correlation metrics, and granularity levels. To establish standardized best practices, they employed random sampling of models to reduce variance, used an aggregate reference benchmark (the average score across all benchmarks), applied data-driven correlation thresholds (0.73-0.79 based on percentiles), and recommended reporting results at multiple granularity levels (individual benchmarks, benchmark groups, and all benchmarks combined).

They implemented these practices in the BenchBench Python package and created the BenchBench-Leaderboard as a meta-benchmark for evaluating benchmark agreement. The methodology was validated by demonstrating a 67% reduction in BAT variance compared to traditional approaches. The authors also conducted extensive sensitivity analyses to identify which methodological choices most impact agreement conclusions.

## Key Results
- Common BAT methodological choices (reference benchmark, model subset, correlation metric) can cause correlation score variations with standard deviations up to 0.25
- Proposed standardized BAT methodology reduces variance by 67% compared to traditional approaches
- Data-driven correlation thresholds (0.73-0.79) provide more robust agreement assessment than arbitrary fixed values
- BenchBench-Leaderboard successfully identifies agreement patterns across 50+ benchmarks and 19 model types

## Why This Works (Mechanism)
The proposed methodology works by systematically addressing sources of variance in benchmark agreement testing. By using an aggregate reference benchmark instead of selecting individual benchmarks, the approach eliminates bias from arbitrary reference choice. Random sampling of models ensures representative coverage rather than cherry-picking subsets that may artificially inflate agreement scores. Data-driven correlation thresholds adapt to the specific characteristics of the benchmark set rather than applying universal values. Multiple granularity reporting captures agreement patterns at different levels, from individual benchmarks to overall trends.

## Foundational Learning

**Correlation metrics**: Statistical measures (Pearson, Spearman, Kendall) that quantify relationships between benchmark scores. Why needed: Different metrics capture different types of relationships and are sensitive to different data distributions. Quick check: Understand that Pearson measures linear relationships while Spearman measures monotonic relationships.

**Benchmark granularity**: The level at which benchmark results are analyzed (individual, group, or aggregate). Why needed: Agreement patterns may vary across different granularity levels, requiring comprehensive reporting. Quick check: Can identify examples of benchmarks grouped by capability type (reasoning, knowledge, language).

**Meta-benchmark**: A benchmark designed to evaluate the quality and agreement of other benchmarks. Why needed: Provides a systematic way to assess benchmark reliability and consistency. Quick check: Understand that BenchBench-Leaderboard serves this purpose for LLM benchmarks.

**Random sampling in evaluation**: Statistical technique for selecting representative subsets from larger populations. Why needed: Reduces variance in agreement testing by ensuring unbiased model selection. Quick check: Can explain how sample size affects variance in correlation estimates.

**Variance reduction techniques**: Statistical methods to minimize variability in measurements. Why needed: Critical for obtaining reproducible and reliable agreement scores. Quick check: Can calculate standard deviation and understand its impact on result reliability.

## Architecture Onboarding

**Component map**: BenchBench package -> BAT methodology -> Correlation analysis -> Benchmark evaluation -> BenchBench-Leaderboard
Critical path: Benchmark data collection -> Random model sampling -> Multiple correlation calculations -> Aggregate reference benchmark computation -> Agreement threshold application -> Granularity-level reporting

**Design tradeoffs**: The methodology trades computational complexity (calculating multiple correlations across various configurations) for increased reliability and reproducibility. Random sampling adds computational overhead but reduces variance. Multiple granularity reporting provides comprehensive insights but requires more storage and processing.

**Failure signatures**: High variance in correlation scores across different model subsets suggests inadequate sampling. Agreement scores below threshold across all granularity levels indicates fundamental benchmark disagreement. Large discrepancies between granularity levels may indicate inconsistent agreement patterns.

**First experiments**:
1. Run BAT on a small benchmark subset using different reference benchmarks to observe variance
2. Compare correlation scores using random sampling vs. fixed model subsets
3. Calculate agreement thresholds using different percentile methods to validate data-driven approach

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions, focusing instead on providing concrete solutions to identified methodological problems in benchmark agreement testing.

## Limitations
- Analysis primarily focuses on English-language benchmarks, limiting generalizability to multilingual or domain-specific contexts
- Correlation thresholds (0.73-0.79) are data-driven but may not capture all meaningful agreement patterns, particularly for qualitatively different capabilities
- Reliance on pairwise correlations may miss higher-order relationships between benchmarks

## Confidence
- **High confidence**: Methodology reduces BAT variance by 67% and provides more robust evaluation
- **Medium confidence**: Correlation thresholds generalize well but may need adjustment for specialized benchmark types
- **Low confidence**: Effectiveness of BenchBench-Leaderboard across diverse LLM architectures beyond tested models

## Next Checks
1. Test the proposed BAT methodology on non-English benchmarks and specialized domains (medical, legal, technical) to assess generalizability
2. Evaluate whether the correlation thresholds remain valid when applied to emerging benchmark types like multi-modal or interactive assessments
3. Replicate the variance reduction findings using alternative statistical approaches (e.g., factor analysis, hierarchical clustering) to confirm robustness of the methodological improvements