---
ver: rpa2
title: 'LLM Critics Help Catch Bugs in Mathematics: Towards a Better Mathematical
  Verifier with Natural Language Feedback'
arxiv_id: '2406.14024'
source_url: https://arxiv.org/abs/2406.14024
tags:
- language
- feedback
- natural
- training
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of binary classification labels
  in training mathematical verifiers, which are insufficient for accurately assessing
  solutions. The authors propose MATH-Minos, a verifier enhanced with step-wise natural
  language feedback as rationale labels, providing detailed explanations for correctness.
---

# LLM Critics Help Catch Bugs in Mathematics: Towards a Better Mathematical Verifier with Natural Language Feedback

## Quick Facts
- arXiv ID: 2406.14024
- Source URL: https://arxiv.org/abs/2406.14024
- Reference count: 11
- This paper addresses limitations of binary classification labels in training mathematical verifiers by introducing natural language feedback, showing accuracy gains of up to 1.8% on GSM8K and 1.5% on MATH.

## Executive Summary
This paper addresses the limitations of binary classification labels in training mathematical verifiers, which are insufficient for accurately assessing solutions. The authors propose MATH-Minos, a verifier enhanced with step-wise natural language feedback as rationale labels, providing detailed explanations for correctness. They introduce a label-aware natural language feedback curation method and a two-stage training paradigm: first, supervised fine-tuning on natural language feedback, followed by standard binary classification training. Experiments show that MATH-Minos significantly improves performance in both verification and reinforcement learning, with accuracy gains of up to 1.8% on GSM8K and 1.5% on MATH compared to traditional verifiers. The approach demonstrates that natural language feedback can complement binary labels, leading to more effective training and better evaluation capabilities.

## Method Summary
The paper proposes MATH-Minos, a mathematical verifier enhanced with step-wise natural language feedback to address limitations of binary classification labels. The method uses a two-stage training paradigm: first, supervised fine-tuning on natural language feedback to improve evaluation capabilities, then standard binary classification training for efficient inference. A label-aware natural language feedback curation method is introduced, which incorporates step-level binary classification labels to simplify the feedback generation task and improve quality. The approach is evaluated on GSM8K and MATH datasets, showing significant improvements in verification and reinforcement learning tasks compared to traditional verifiers.

## Key Results
- MATH-Minos achieves accuracy improvements of up to 1.8% on GSM8K and 1.5% on MATH compared to traditional verifiers
- The two-stage training paradigm significantly enhances performance, with stage 1 (SFT on natural language feedback) being crucial for effectiveness
- Label-aware natural language feedback curation improves the quality of automatically generated feedback data, leading to better model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Natural language feedback improves training efficiency by providing richer supervision signals compared to binary labels.
- Mechanism: Natural language feedback contains explanations for correctness/incorrectness at the step level, allowing the model to understand the underlying reasons for errors and thus learn more effectively from fewer samples.
- Core assumption: The additional semantic information in natural language feedback is processed by the model to improve its evaluation capability beyond what binary labels can achieve.
- Evidence anchors:
  - [abstract]: "Our experiments reveal that a small set of natural language feedback can significantly boost the performance of the verifier in both verification and reinforcement learning."
  - [section 5.2]: "MATH-Minos consistently outperforms vanilla ORM in meta-evaluation... MATH-Minos exhibit a faster convergence rate, surpassing the baseline at only approximately 120 steps."
  - [corpus]: Weak evidence - related work "LLM Critics Help Catch LLM Bugs" suggests general critic models can improve evaluation, but not specifically about natural language feedback for mathematics.
- Break condition: If the natural language feedback quality is poor (hallucinations, contradictions) or the model cannot effectively process the additional information, the benefit would diminish or reverse.

### Mechanism 2
- Claim: The two-stage training paradigm synergistically combines evaluation generation and discrimination capabilities.
- Mechanism: Stage 1 (supervised fine-tuning on natural language feedback) enhances the model's evaluation ability by teaching it to generate detailed explanations. Stage 2 (binary classification training) converts this enhanced capability into efficient inference by training the model to output a single score.
- Core assumption: The skills learned in stage 1 (understanding correctness and explaining reasons) transfer to stage 2, making the binary classification more accurate than training from scratch.
- Evidence anchors:
  - [section 3.3]: "Benefiting the proposed two-stage training, we can enhance the verifier's evaluation ability with natural language feedbacks and efficiently apply the verifier to PRM or ORM with one single forward pass."
  - [section 5.4]: "Removing stage 1 essentially reverts our method to a vanilla ORM... the performance noticeably declines compared to MATH-Minos."
- Break condition: If stage 1 doesn't significantly improve evaluation capability, or if the binary classification in stage 2 overwrites the nuanced understanding gained in stage 1, the two-stage approach would provide minimal benefit.

### Mechanism 3
- Claim: Label-aware natural language feedback curation improves the quality of automatically generated feedback data.
- Mechanism: By providing step-level binary labels as context in the prompt, the complexity of the generation task is reduced from "determine correctness AND explain" to "explain given correctness," which helps the model generate more accurate explanations.
- Core assumption: Simplifying the task for the feedback generator (GPT-4) by providing labels reduces hallucinations and contradictions in the generated explanations.
- Evidence anchors:
  - [section 5.3]: "The experimental results indicate that directly prompting GPT does not significantly enhance the performance of the verifier. This is possibly due to the poor quality of the data shown in Table 1."
  - [section 3.2]: "To facilitate GPT-4 in generating higher-quality data, we propose a label-aware prompting method, which simplifies the evaluation task by introducing the binary classification label within the prompt."
- Break condition: If the provided labels are incorrect or if the model still generates poor quality feedback despite the labels, the curation method would not improve data quality.

## Foundational Learning

- Concept: Binary classification vs. multi-class classification
  - Why needed here: Understanding why binary labels (correct/incorrect) are insufficient for complex reasoning tasks requires recognizing the limitations of binary classification in capturing nuanced information.
  - Quick check question: What information is lost when reducing a multi-dimensional evaluation (e.g., "this step is wrong because of a calculation error in the previous step") to a binary label?

- Concept: Supervised fine-tuning vs. standard training
  - Why needed here: The paper uses supervised fine-tuning on natural language feedback in stage 1, which is different from standard classification training and requires understanding how SFT works with sequence-to-sequence tasks.
  - Quick check question: How does supervised fine-tuning on a sequence generation task (like natural language feedback) differ from fine-tuning for a classification task?

- Concept: Reinforcement learning with reward models
  - Why needed here: The paper evaluates verifiers as reward models for PPO in reinforcement learning, so understanding how reward models guide policy improvement is essential.
  - Quick check question: In PPO, how does the reward model influence the policy's behavior, and why would a better reward model lead to improved final performance?

## Architecture Onboarding

- Component map: Generator (MetaMATH-Mistral 7B) -> Multiple solutions -> Verifier (MATH-Minos Mistral-7B) -> Best solution selection
- Critical path: Question → Generator → Multiple solutions → Verifier → Best solution selection
- Design tradeoffs:
  - Using smaller models (7B) for computational efficiency vs. potentially better performance with larger models
  - Automatically generated feedback vs. human annotation (cost vs. quality)
  - Two-stage training complexity vs. potential performance gains
- Failure signatures:
  - Poor performance on verification: likely issues with feedback quality or insufficient training data
  - Slow convergence: may indicate the natural language feedback isn't effectively transferring knowledge to the binary classifier
  - Hallucinations in feedback: suggests the label-aware curation isn't constraining the generator sufficiently
- First 3 experiments:
  1. Train a baseline ORM on binary labels only and measure performance on GSM8K verification task
  2. Generate natural language feedback using direct GPT-4 prompting (without label-aware curation) and measure its impact on verification performance
  3. Implement the full MATH-Minos with label-aware curation and two-stage training, comparing against both baselines on verification and RL tasks

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but leaves the exploration of the impact of model scaling on evaluation ability to future work, noting that due to computational resource limitations, experiments were conducted solely on a model with 7 billion parameters.

## Limitations
- The primary limitation is the reliance on automatically generated natural language feedback rather than human-annotated data, which may introduce noise or inconsistencies in the training signal.
- The experiments focus on 7B parameter models for computational efficiency, leaving open questions about whether the benefits of natural language feedback scale to larger models.
- The paper doesn't thoroughly explore failure cases where natural language feedback might mislead the model, such as when explanations contain subtle errors or when the model learns to mimic explanation patterns without genuine understanding.

## Confidence

- **High Confidence**: The general finding that MATH-Minos outperforms baseline verifiers on both GSM8K and MATH datasets is well-supported by the experimental results presented. The meta-evaluation framework provides robust evidence for improved verification capabilities.
- **Medium Confidence**: The mechanism by which natural language feedback improves performance is plausible but not fully isolated. While ablation studies show the two-stage training is important, the specific contribution of natural language feedback versus other factors (model architecture, training procedures) remains partially unclear.
- **Medium Confidence**: The claim that label-aware curation significantly improves feedback quality is supported by comparison with direct prompting, but the evaluation doesn't directly measure feedback quality independently of downstream model performance.

## Next Checks

1. **Human Evaluation Study**: Conduct a human evaluation of the automatically generated natural language feedback to assess its accuracy and quality independently of model performance, establishing a baseline for how much the label-aware curation method actually improves feedback quality.

2. **Scaling Experiment**: Test the MATH-Minos approach with different model sizes (e.g., 3B, 13B, 33B parameters) to determine whether the benefits of natural language feedback scale with model capacity or if there's an optimal model size for this approach.

3. **Failure Case Analysis**: Systematically analyze cases where MATH-Minos makes errors despite having access to natural language feedback, comparing these to baseline model failures to identify whether the feedback sometimes introduces confusion or whether certain types of mathematical errors remain challenging even with enhanced supervision.