---
ver: rpa2
title: 'Tango 2: Aligning Diffusion-based Text-to-Audio Generations through Direct
  Preference Optimization'
arxiv_id: '2404.09956'
source_url: https://arxiv.org/abs/2404.09956
tags:
- audio
- tango
- preference
- prompt
- clap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Tango 2, a diffusion-based text-to-audio
  model that aligns generated audio with human preferences through direct preference
  optimization (DPO). The authors create a preference dataset, Audio-alpaca, by generating
  winner and loser audio samples using Tango, then fine-tuning Tango with DPO-diffusion
  loss on this dataset.
---

# Tango 2: Aligning Diffusion-based Text-to-Audio Generations through Direct Preference Optimization

## Quick Facts
- arXiv ID: 2404.09956
- Source URL: https://arxiv.org/abs/2404.09956
- Reference count: 40
- Key outcome: Tango 2 significantly outperforms Tango and AudioLDM2 on both objective metrics (CLAP score of 0.57 vs 0.43-0.54) and subjective evaluations (OVL 3.99, REL 4.07 vs 3.56-3.81), particularly excelling at handling prompts with multiple concepts or temporal events.

## Executive Summary
Tango 2 introduces a novel approach to aligning text-to-audio generation with human preferences through direct preference optimization (DPO) applied to a diffusion model. The authors create a synthetic preference dataset, Audio-alpaca, by generating audio from perturbed prompts and using CLAP scores to identify preferred vs undesirable samples. By fine-tuning the Tango model with a DPO-diffusion loss on this dataset, Tango 2 achieves significant improvements in both objective metrics (CLAP score 0.57) and subjective evaluations (OVL 3.99, REL 4.07) compared to baseline models. The approach is particularly effective at handling complex prompts with multiple concepts and temporal events.

## Method Summary
Tango 2 fine-tunes the pre-trained Tango model using a diffusion-DPO loss on a synthetic preference dataset called Audio-alpaca. The dataset is created by generating audio from perturbed prompts (using GPT-4) and filtering pairs based on CLAP scores to identify preferred (winner) and undesirable (loser) samples. The model is trained for 4 epochs with AdamW (learning rate 9.6e-7, batch size 32), minimizing a loss that differentially weights noise estimation errors for preferred vs undesirable samples. The approach combines latent diffusion architecture with preference optimization to improve semantic alignment between prompts and generated audio.

## Key Results
- Tango 2 achieves CLAP score of 0.57, outperforming Tango (0.54) and AudioLDM2 (0.43) on AudioCaps test set
- Subjective evaluation shows Tango 2 scores 3.99 (OVL) and 4.07 (REL) vs 3.56-3.81 for baselines
- Outperforms Tango across all temporal metrics except duration, showing better handling of event ordering
- Ablation study confirms that removing human preference modeling or pairwise ranking reduces performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The DPO-diffusion loss directly improves semantic alignment between prompts and generated audio by penalizing noise estimation errors differently for preferred vs undesirable samples.
- Mechanism: During fine-tuning, Tango 2 learns to minimize the L2 noise-estimation loss more for preferred samples and less for undesirable ones, forcing the model to denoise in ways that better reflect prompt semantics.
- Core assumption: The CLAP score used to select preferred/undesirable pairs accurately reflects human preference for prompt-audio alignment.
- Evidence anchors: [abstract]: "The loser outputs, in theory, have some concepts from the prompt missing or in an incorrect order"; [section 4.2]: "DPO objective also allows the model to learn from undesirable (loser) outputs, which is key in the absence of a high-quality reward model."
- Break condition: If CLAP scores poorly correlate with human judgments of audio quality or prompt relevance, the preference dataset becomes noisy and DPO training fails to improve alignment.

### Mechanism 2
- Claim: Temporal prompt perturbations create training data that specifically teaches the model to preserve event order in generated audio.
- Mechanism: By generating audio from prompts where event sequences are altered, the model learns to map prompt temporal structure to audio temporal structure through contrastive learning.
- Core assumption: The model's latent space can represent temporal relationships between audio events in a way that correlates with prompt structure.
- Evidence anchors: [section 4.1.1]: "Strategy 3: Inferences from Temporally Perturbed Prompts" describes generating undesirable samples by changing event order; [section 5.4]: "Tango 2 shows consistent superiority over Tango across all other temporal measurements" except duration.
- Break condition: If the latent diffusion model cannot capture temporal dependencies between audio events, temporal perturbations won't improve temporal controllability.

### Mechanism 3
- Claim: The combination of LLM-based prompt perturbations and CLAP-based adversarial filtering creates a high-quality synthetic preference dataset without human annotation.
- Mechanism: GPT-4 generates semantically close but flawed prompt variants; Tango generates audio from both original and perturbed prompts; CLAP scores filter for pairs where original-prompt audio is clearly better, creating preference pairs that teach the model what good audio sounds like.
- Core assumption: GPT-4 can generate meaningful semantic perturbations that create genuinely flawed audio when passed to Tango.
- Evidence anchors: [section 4.1.1]: Describes using GPT-4 to create perturbed prompts and CLAP to rank generated audios; [section 4.1.2]: "We want to ensure that the winning audio sample ùë•ùë§ is strongly aligned with the text prompt ùúè."
- Break condition: If GPT-4 generates perturbations that are too far from the original prompt or too similar, the resulting preference pairs won't provide useful training signal.

## Foundational Learning

- Concept: Diffusion models and reverse diffusion process
  - Why needed here: Tango 2 builds directly on Tango's latent diffusion architecture; understanding how noise estimation works is critical for implementing DPO-diffusion loss.
  - Quick check question: How does the reverse diffusion process reconstruct audio from noise using text guidance?

- Concept: Preference optimization and Bradley-Terry model
  - Why needed here: DPO is the core alignment technique; understanding how it converts preference pairs into a loss function is essential for implementing the training procedure.
  - Quick check question: How does the Bradley-Terry model estimate the probability that one sample is preferred over another?

- Concept: Audio-text representation learning (CLAP)
  - Why needed here: CLAP scores are used both to create the preference dataset and evaluate model performance; understanding its strengths and limitations is crucial.
  - Quick check question: What are the limitations of using cosine similarity between CLAP embeddings as a proxy for human preference?

## Architecture Onboarding

- Component map: Text encoder (FLAN-T5) ‚Üí Cross-attention in U-Net ‚Üí Noise estimation ‚Üí Latent diffusion ‚Üí VAE decoder ‚Üí Vocoder ‚Üí Audio output
- Critical path: Text prompt ‚Üí Text encoder ‚Üí Cross-attention ‚Üí Noise estimation ‚Üí Latent audio prior ‚Üí VAE decoding ‚Üí Audio generation
- Design tradeoffs: Using FLAN-T5 instead of CLAP for text encoding trades some audio-text alignment capability for better handling of complex prompts; DPO fine-tuning trades computational efficiency for improved alignment
- Failure signatures: Poor CLAP scores indicate text-audio misalignment; poor temporal metrics indicate event ordering issues; degraded FAD/KL suggest loss of general audio quality
- First 3 experiments:
  1. Generate audio from a simple prompt and measure CLAP score to verify baseline alignment
  2. Generate audio from a temporally complex prompt and check if event order is preserved
  3. Run a small DPO fine-tuning run and compare CLAP scores before/after to verify alignment improvement

## Open Questions the Paper Calls Out

- Question: How does the quality of Tango 2's generated audio compare when using different adversarial filtering thresholds for selecting undesirable audio samples?
- Basis in paper: [explicit] The paper mentions using adversarial filtering by generating multiple audios and selecting those with CLAP scores below a certain threshold, but does not explore the impact of different threshold values.
- Why unresolved: The paper uses a fixed threshold but does not investigate how varying this threshold affects the final model performance.
- What evidence would resolve it: Systematic experiments varying the CLAP score threshold used in adversarial filtering, measuring resulting Tango 2 performance across different thresholds.

- Question: Would Tango 2's performance improve further if trained on a larger or more diverse Audio-alpaca dataset?
- Basis in paper: [explicit] The paper acknowledges creating Audio-alpaca from limited data (AudioCaps) and notes Tango 2's performance despite not using additional out-of-distribution data.
- Why unresolved: The current study is constrained by the size and scope of the AudioCaps dataset used to create Audio-alpaca.
- What evidence would resolve it: Experiments training Tango 2 on progressively larger or more diverse Audio-alpaca datasets, comparing performance metrics.

- Question: How would Tango 2's performance compare to models trained with human-annotated preference data rather than synthetically generated preferences?
- Basis in paper: [inferred] The paper notes that BATON uses human-annotated preferences but doesn't provide comparison, and Tango 2 uses synthetically generated preferences through GPT-4 and CLAP scores.
- Why unresolved: The paper does not compare Tango 2's performance against models trained on human-annotated preference data, which could potentially be more reliable.
- What evidence would resolve it: Direct comparison between Tango 2 and models trained on human-annotated preference data using identical evaluation metrics.

## Limitations
- Reliance on CLAP scores for preference dataset creation without validation of correlation with human preference
- Limited ablation study (only 1 epoch) on DPO components that may not reflect full 4-epoch training impact
- Small human evaluation sample size (25 samples) for subjective metrics

## Confidence
- **High confidence**: The technical implementation of DPO-diffusion loss and the overall training methodology are well-specified and reproducible.
- **Medium confidence**: The performance improvements over baselines are demonstrated through multiple objective and subjective metrics, though the human evaluation sample size (25 samples) is relatively small.
- **Low confidence**: The assumption that CLAP scores accurately proxy human preference for audio quality, and that GPT-4 perturbations create meaningful semantic flaws in generated audio.

## Next Checks
1. **CLAP Correlation Validation**: Conduct a small-scale human study (50+ samples) to measure the correlation between CLAP scores and human judgments of prompt-audio alignment to validate the preference dataset creation pipeline.
2. **Ablation Study Extension**: Run full 4-epoch ablations for all proposed modifications (human preference modeling, pairwise ranking) to confirm their individual contributions to performance gains.
3. **Temporal Alignment Analysis**: Generate audio from prompts with varying temporal complexity and conduct a detailed analysis of whether event ordering in prompts actually translates to temporal structure in the generated audio outputs.