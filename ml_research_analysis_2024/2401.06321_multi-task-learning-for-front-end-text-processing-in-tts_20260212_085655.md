---
ver: rpa2
title: Multi-Task Learning for Front-End Text Processing in TTS
arxiv_id: '2401.06321'
source_url: https://arxiv.org/abs/2401.06321
tags:
- tasks
- dataset
- tagging
- homograph
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a multi-task learning model for three common
  text-to-speech front-end tasks: text normalization, part-of-speech tagging, and
  homograph disambiguation. The proposed model uses a tree-like structure with a shared
  trunk for general feature extraction and separate task-specific heads.'
---

# Multi-Task Learning for Front-End Text Processing in TTS

## Quick Facts
- arXiv ID: 2401.06321
- Source URL: https://arxiv.org/abs/2401.06321
- Authors: Wonjune Kang; Yun Wang; Shun Zhang; Arthur Hinsvark; Qing He
- Reference count: 0
- Key outcome: Multi-task learning model achieves strong performance on text normalization, POS tagging, and homograph disambiguation through shared linguistic representations

## Executive Summary
This paper introduces a multi-task learning approach for three key text-to-speech front-end tasks: text normalization, part-of-speech tagging, and homograph disambiguation. The proposed model uses a tree-like architecture with a shared trunk for general feature extraction and separate task-specific heads. Through ablation studies, the authors demonstrate that multi-task learning provides clear benefits, with the full model achieving the strongest overall performance compared to individual task models. The paper also highlights the importance of incorporating pre-trained language model embeddings and a new balanced homograph disambiguation dataset in improving model performance.

## Method Summary
The model processes input text through two information streams - one operating on a token sequence and another utilizing ALBERT embeddings - which are combined using cross-attention in a shared trunk. This trunk learns general linguistic features that are then passed to three task-specific heads for text normalization, POS tagging, and homograph disambiguation. The model is trained on balanced datasets for each task, with a new homograph disambiguation dataset containing diverse sentences for 162 American English homographs. Training proceeds through 90k iterations (30k per task), cycling through tasks and using cross-entropy loss with AdamW optimizer.

## Key Results
- Multi-task learning achieves stronger overall performance than individual task models
- Incorporating ALBERT embeddings significantly improves performance on all three tasks
- The new balanced homograph disambiguation dataset enables more accurate measurement and improvement of homograph disambiguation performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning enables positive transfer between tasks by sharing high-level linguistic representations in a common trunk.
- Mechanism: The shared trunk learns general feature representations that capture complementary knowledge beneficial to all tasks, as these tasks are linguistically related (e.g., POS tags help recognize non-standard words in text normalization).
- Core assumption: The tasks share sufficient linguistic structure that can be effectively represented in a common feature space.
- Evidence anchors:
  - [abstract] states that "one might expect them to be able to take advantage of shared representations containing common high-level information"
  - [section] confirms this intuition through ablation studies, showing that the full model trained on all three tasks "achieves the strongest overall performance compared to models trained on individual or sub-combinations of tasks"

### Mechanism 2
- Claim: Incorporating ALBERT embeddings improves performance by providing additional lexical and contextual knowledge.
- Mechanism: The model processes input through two streams (token sequence and ALBERT embeddings) combined using cross-attention, allowing leverage of ALBERT's built-in linguistic knowledge.
- Core assumption: ALBERT's embeddings contain useful linguistic information that benefits the three tasks.
- Evidence anchors:
  - [abstract] states that the model "further incorporate[s] a pre-trained language model to utilize its built-in lexical and contextual knowledge"
  - [section] shows that removing ALBERT significantly decreases performance on text normalization and POS tagging

### Mechanism 3
- Claim: The new balanced homograph disambiguation dataset enables more accurate measurement and improvement of performance.
- Mechanism: The dataset contains equal sentences for each pronunciation of each homograph, allowing the model to learn true disambiguation rather than memorizing dominant pronunciations.
- Core assumption: Balanced data is necessary for accurate homograph disambiguation.
- Evidence anchors:
  - [abstract] introduces "a new HD dataset containing a balanced number of sentences in diverse contexts for a variety of homographs and their pronunciations"
  - [section] shows that incorporating the new dataset "significantly improves HD performance over only using a commonly used, but imbalanced, pre-existing dataset"

## Foundational Learning

- Concept: Multi-task learning
  - Why needed here: Enables leveraging shared linguistic representations between text normalization, POS tagging, and homograph disambiguation
  - Quick check question: What are the potential benefits and drawbacks of using a shared trunk versus separate models for each task?

- Concept: Cross-attention mechanism
  - Why needed here: Combines token sequence and ALBERT embeddings to leverage both sources of information
  - Quick check question: How does cross-attention differ from self-attention, and why is it more appropriate for combining the two information streams?

- Concept: Homograph disambiguation
  - Why needed here: Determines which pronunciation of a homograph to use based on surrounding context
  - Quick check question: What are common challenges in homograph disambiguation, and how can a model effectively learn to disambiguate between different pronunciations?

## Architecture Onboarding

- Component map:
  Input text → Text normalization tokenizer → Character-level CNN → Bi-LSTM → Transformer → Cross-attention with ALBERT embeddings → Shared trunk embeddings → Task-specific heads

- Critical path: Input text → Text normalization tokenizer → Shared trunk → Task-specific heads

- Design tradeoffs:
  - Shared trunk vs. separate models: Shared trunk allows positive transfer but may introduce interference
  - ALBERT embeddings vs. no pre-trained embeddings: Provides additional linguistic knowledge but increases complexity
  - Balanced vs. imbalanced homograph dataset: Balanced enables accurate measurement but requires more effort to create

- Failure signatures:
  - Poor performance on all tasks: Shared trunk may not capture common linguistic structure or has too much interference
  - Good performance on some tasks, poor on others: Task-specific heads may not effectively leverage shared trunk representations
  - Overfitting to training data: Model may be memorizing specific examples rather than learning general patterns

- First 3 experiments:
  1. Train the model on each task individually to establish baseline performance
  2. Train the model on pairs of tasks to assess multi-task learning impact and transfer effects
  3. Incorporate the new homograph disambiguation dataset and evaluate its impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the effectiveness of incorporating ALBERT's final layer embeddings be optimized for homograph disambiguation performance?
- Basis in paper: [explicit] The paper notes that removing the residual connection from ALBERT's final layer to the HD head decreases HD performance on the Wikipedia dataset but improves it slightly on the Llama 2 dataset, indicating inconclusive benefits.
- Why unresolved: The results show varying impacts on different datasets, suggesting that the current method of incorporation might not be optimal or that its effectiveness depends on dataset characteristics.
- What evidence would resolve it: Systematic experiments varying the method of incorporation and evaluating on diverse datasets could identify optimal strategies.

### Open Question 2
- Question: What is the impact of task-wise loss weighting or balancing on the performance of the multi-task learning model?
- Basis in paper: [explicit] The paper mentions considering strategies like task-wise loss weighting but ultimately using a simpler approach without balancing, noting it was sufficient for stable convergence.
- Why unresolved: The decision to avoid complex loss weighting strategies leaves open the question of whether more sophisticated approaches could further enhance performance.
- What evidence would resolve it: Comparative studies training the model with various loss weighting strategies and evaluating their impact on each task's performance would provide insights.

### Open Question 3
- Question: How does the proposed multi-task learning model perform on other languages or dialects beyond American English?
- Basis in paper: [inferred] The model is specifically designed and evaluated for American English tasks, with no mention of testing on other languages or dialects.
- Why unresolved: The paper does not explore the model's generalizability to other linguistic contexts, which is crucial for understanding its broader applicability and limitations.
- What evidence would resolve it: Extending the model to other languages or dialects and conducting comparative evaluations would demonstrate its adaptability.

## Limitations
- Internal text normalization dataset is relatively small (37k samples) and its specific format remains unclear
- Homograph disambiguation dataset, while balanced, was generated using Llama 2 and may not fully capture real-world pronunciation variation
- Model's dependence on ALBERT embeddings introduces additional complexity and potential brittleness

## Confidence

**High Confidence:** The ablation studies clearly demonstrate that multi-task learning provides performance benefits across all three tasks, with the full model achieving the strongest overall results. The architectural components are well-specified and reproducible.

**Medium Confidence:** The specific contribution of the new homograph disambiguation dataset is supported by experiments, but the exact impact depends on how representative the Llama 2-generated sentences are of natural language usage. The model's reliance on ALBERT embeddings shows strong performance, but the optimal embedding strategy for TTS applications remains an open question.

**Low Confidence:** The paper's claims about why multi-task learning works are based on reasonable linguistic intuitions rather than rigorous theoretical analysis. The interaction between the two information streams in the shared trunk and their specific contributions to each task are not fully characterized.

## Next Checks

1. **Task Interference Analysis:** Conduct systematic experiments to quantify positive vs. negative transfer between tasks by training all possible task combinations and measuring performance changes relative to single-task baselines.

2. **Embedding Ablation Study:** Replace ALBERT embeddings with alternative pre-trained models (e.g., BERT, RoBERTa) and with frozen vs. fine-tuned embeddings to isolate the contribution of pre-trained knowledge vs. the cross-attention mechanism.

3. **Generalization Test:** Evaluate the model on out-of-domain TTS datasets (e.g., different accents, domains, or noise conditions) to assess robustness and identify potential overfitting to the training data distribution.