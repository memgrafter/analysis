---
ver: rpa2
title: Counting Network for Learning from Majority Label
arxiv_id: '2403.13370'
source_url: https://arxiv.org/abs/2403.13370
tags:
- class
- majority
- instances
- label
- counting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Learning from Majority Label (LML), a novel
  multi-class Multiple-Instance Learning problem where bag labels are determined by
  majority class instances. Existing MIL methods fail due to inconsistency between
  confidence-based aggregation and instance counting.
---

# Counting Network for Learning from Majority Label

## Quick Facts
- arXiv ID: 2403.13370
- Source URL: https://arxiv.org/abs/2403.13370
- Reference count: 0
- This paper proposes Learning from Majority Label (LML), a novel multi-class Multiple-Instance Learning problem where bag labels are determined by majority class instances.

## Executive Summary
This paper introduces Learning from Majority Label (LML), a novel multi-class Multiple-Instance Learning (MIL) problem where bag labels are determined by the majority class of instances within each bag. Existing MIL methods struggle with this task due to inconsistency between confidence-based aggregation and actual instance counting. The authors propose a counting network with temperature-controlled softmax that outputs binary-like confidences, enabling differentiable counting operations while maintaining label consistency. The method achieves up to 74.1% accuracy on four datasets (CIFAR10, SVHN, PATHMNIST, OCTMNIST), outperforming eight comparative MIL approaches.

## Method Summary
The proposed counting network addresses LML by using temperature-controlled softmax to constrain confidences to binary-like values, enabling differentiable counting operations. The network architecture consists of a ResNet18 backbone for feature extraction, followed by a temperature-controlled softmax for instance-level classification. The counting operation sums these binary-like confidences across instances, and a bag-level classifier uses argmax with temperature for differentiable majority class selection. The model is trained with Adam optimizer (lr=3e-4, epochs=1500, batch size=64, T=0.1) using cross-entropy loss between estimated and ground truth majority labels.

## Key Results
- Achieves up to 74.1% accuracy on four datasets (CIFAR10, SVHN, PATHMNIST, OCTMNIST)
- Outperforms eight comparative MIL approaches across all tested datasets
- Ablation studies confirm effectiveness of both counting and argmax operations
- Counting operation particularly improves performance in challenging scenarios with small majority proportions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The temperature-controlled softmax ensures binary-like confidences, enabling differentiable counting operations that are consistent with instance counting.
- Mechanism: By using a low temperature value (T = 0.1) in the softmax function, the network outputs approximate one-hot vectors where only the correct class has confidence close to 1 and others close to 0. This allows the sum of confidences to equal the actual number of instances in each class.
- Core assumption: The softmax with temperature can be trained to produce binary-like outputs while remaining differentiable for backpropagation.
- Evidence anchors:
  - [abstract] "By introducing a softmax with temperature, we can almost constrain confidences to binary values, in which the confidence of a class is close to 1 and the others close to 0 by the constraint."
  - [section 3.2] "Using a small temperature ensures that the estimated value closely approximates either 0 or 1, where if an instance belongs to the class, it takes 1; otherwise, 0."

### Mechanism 2
- Claim: The argmax operation with temperature prevents overestimation of the majority class by ensuring the loss only penalizes when the estimated majority class differs from ground truth.
- Mechanism: The argmax operation selects the class with maximum count, but when combined with temperature-controlled softmax, it creates a differentiable approximation that doesn't continue decreasing loss after correct majority class prediction.
- Core assumption: Using temperature-controlled softmax for argmax operation provides a smooth approximation that stops penalizing once the correct class is identified.
- Evidence anchors:
  - [section 3.2] "To avoid this overestimation, we also use softmax with temperature s(Ni, T) to take a majority class with a differentiable process. This process outputs the one-hot vector ˆYi = s(Ni, T), where each element of ˆYi approximately takes 0 or 1, and if 1, the majority class of the bag."

### Mechanism 3
- Claim: The counting network avoids bad local minima by constraining the solution space to only one valid solution per bag where the majority class has the most instances.
- Mechanism: By forcing confidence values to be binary-like, the network eliminates ambiguous solutions where different combinations of instance confidences could produce the same bag-level sum but inconsistent with actual instance counts.
- Core assumption: Binary-like confidences eliminate the ambiguity in solution space that exists with standard confidence aggregation methods.
- Evidence anchors:
  - [abstract] "Let us consider a case when a bag contains two instances, and the majority label is class 1. In this case, the above MIL method can take various solutions... This ambiguity and inconsistency lead training to fall to a bad local minimum."
  - [section 3.2] "If the confidence only takes binary values, the solution is only one case: (1,0,0) + (1,0,0) = (2,0,0)."

## Foundational Learning

- Concept: Multiple Instance Learning (MIL)
  - Why needed here: The paper builds on MIL framework where bags contain instances with only bag-level labels available for training.
  - Quick check question: In standard MIL, what is the relationship between bag labels and instance labels?

- Concept: Softmax with Temperature
  - Why needed here: Temperature parameter controls the sharpness of the softmax distribution, enabling binary-like outputs for differentiable counting.
  - Quick check question: How does decreasing the temperature parameter affect the output distribution of a softmax function?

- Concept: Differentiable Counting Operations
  - Why needed here: Standard counting is non-differentiable, but the temperature-controlled softmax allows counting-like operations to remain differentiable for backpropagation.
  - Quick check question: Why is it important for the counting operation to be differentiable in neural network training?

## Architecture Onboarding

- Component map: Input -> Feature Extractor -> Temperature-Controlled Softmax -> Counting Operation -> Bag-level Classifier -> Loss Computation
- Critical path: Feature extraction → Temperature-controlled softmax → Counting operation → Bag-level classification → Loss computation
- Design tradeoffs:
  - Temperature value: Low values give better binary approximation but risk gradient vanishing; high values maintain gradients but lose binary property
  - Network architecture: ResNet18 chosen for balance between capacity and efficiency
  - Bag size: Larger bags provide more counting information but increase computational cost
- Failure signatures:
  - Inconsistent majority class predictions between counting and confidence aggregation methods
  - Network overfitting to majority class (predicted labels don't match ground truth)
  - Training instability with temperature values that are too extreme
- First 3 experiments:
  1. Test temperature sensitivity: Train with T = 0.01, 0.1, 1.0 and compare binary approximation quality and accuracy
  2. Compare with standard softmax: Replace temperature-controlled softmax with standard softmax and measure consistency rate between counting and aggregation methods
  3. Ablation study on argmax: Remove temperature-controlled softmax from bag-level classification and observe overestimation behavior in various majority proportion scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the counting network scale with the number of classes (C) and bag sizes, particularly in extreme cases where C is very large or bags contain thousands of instances?
- Basis in paper: [explicit] The paper notes that the majority proportion (ratio of majority class instances to total instances) influences difficulty, but does not explore scenarios with very large C or extremely large bag sizes. The experiments used moderate-sized datasets with C ranging from 10 to 4 classes.
- Why unresolved: The paper only tested on datasets with limited class numbers and bag sizes. The theoretical analysis does not address computational complexity or accuracy degradation with scale.
- What evidence would resolve it: Systematic experiments varying C from 10 to 1000 classes and bag sizes from 10 to 10,000 instances, measuring accuracy, training time, and memory requirements across these ranges.

### Open Question 2
- Question: What is the optimal temperature T for the softmax with temperature function, and how sensitive is the model's performance to temperature variations across different datasets and majority proportion scenarios?
- Basis in paper: [explicit] The paper sets temperature T = 0.1 as a fixed hyperparameter without exploring its sensitivity or optimization. The effectiveness of softmax with temperature is assumed but not thoroughly analyzed.
- Why unresolved: Only one temperature value was tested, and no ablation study examines temperature sensitivity or proposes a method for temperature selection.
- What evidence would resolve it: Experiments varying temperature across a wide range (e.g., 0.01 to 1.0) on all datasets and scenarios, showing accuracy curves and potentially proposing a data-driven method for temperature selection.

### Open Question 3
- Question: How does the counting network perform on real-world medical imaging data where class distributions are heavily imbalanced and the majority class may only represent a small fraction of instances?
- Basis in paper: [explicit] The paper used MedMNIST datasets which are relatively balanced, and focused on synthetic majority proportion scenarios. The "Small" scenario (1/C to 0.4) still assumes relatively balanced conditions compared to real medical data.
- Why unresolved: The experiments did not use actual clinical datasets with severe class imbalance, and the theoretical framework does not address how to handle cases where the majority class represents less than 10% of instances.
- What evidence would resolve it: Testing on real histopathological or radiological datasets with severe class imbalance (e.g., cancer detection where malignant regions are <5% of tissue), comparing counting network performance against MIL methods specifically designed for imbalanced data.

## Limitations

- The experimental validation is limited to only four datasets, with CIFAR10 and SVHN being relatively standard benchmarks
- The temperature parameter T=0.1 is treated as fixed across all experiments without sensitivity analysis
- The ablation studies focus primarily on the counting operation and argmax effectiveness, but don't explore other architectural choices like alternative backbone networks or loss functions

## Confidence

- **High Confidence**: The core mechanism of using temperature-controlled softmax for differentiable counting operations is well-justified theoretically and supported by the ablation studies.
- **Medium Confidence**: The superiority over existing MIL methods is demonstrated empirically, but the narrow dataset scope limits generalizability claims.
- **Low Confidence**: The claim that the method "overcomes the inconsistency between confidence aggregation and instance counting" is primarily supported by qualitative analysis rather than quantitative comparison of solution spaces.

## Next Checks

1. Conduct a comprehensive ablation study on the temperature parameter T across a range of values (0.01, 0.05, 0.1, 0.5) to establish its sensitivity and optimal range for different dataset characteristics.
2. Test the counting network on additional diverse datasets beyond the four used, particularly including datasets with different class distributions and instance characteristics to validate generalizability.
3. Implement a quantitative comparison of solution space consistency by measuring the frequency of ambiguous solutions under standard MIL methods versus the counting network approach, using metrics beyond just accuracy.