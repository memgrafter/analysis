---
ver: rpa2
title: Counterfactuals As a Means for Evaluating Faithfulness of Attribution Methods
  in Autoregressive Language Models
arxiv_id: '2408.11252'
source_url: https://arxiv.org/abs/2408.11252
tags:
- methods
- linguistics
- computational
- attribution
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of evaluating the faithfulness
  of attribution methods for autoregressive language models, where standard techniques
  can create out-of-distribution (OOD) inputs that confound evaluation. To solve this,
  the authors propose using counterfactual generation to create fluent, in-distribution
  modifications of inputs by replacing tokens identified as important by attribution
  methods.
---

# Counterfactuals As a Means for Evaluating Faithfulness of Attribution Methods in Autoregressive Language Models

## Quick Facts
- arXiv ID: 2408.11252
- Source URL: https://arxiv.org/abs/2408.11252
- Reference count: 40
- Key outcome: Counterfactual generation approach evaluates attribution method faithfulness by creating fluent, in-distribution input modifications that flip model predictions

## Executive Summary
This paper addresses the challenge of evaluating attribution methods for autoregressive language models, where standard evaluation techniques can create out-of-distribution (OOD) inputs that confound results. The authors propose using counterfactual generation to create fluent, in-distribution modifications of inputs by replacing tokens identified as important by attribution methods. Their evaluation protocol uses an editor model to generate counterfactuals that flip the model's prediction while maintaining input fluency. Experiments comparing attribution methods on sentiment and topic classification tasks show that gradient norm methods generally perform best on fine-tuned models, while erasure works better on off-the-shelf models. The counterfactual approach yields consistent rankings across different editors and avoids OOD issues that plague other methods.

## Method Summary
The authors propose evaluating attribution method faithfulness through counterfactual generation. First, an attribution method identifies important tokens in an input. Then, an editor model generates counterfactuals by replacing these important tokens to flip the model's prediction while maintaining fluency. The evaluation metric is the number of tokens that need to be changed - fewer changes indicate better attribution methods. This approach creates in-distribution counterfactuals, avoiding the OOD issues that plague perturbation-based evaluations. The method was tested on sentiment and topic classification tasks using various attribution methods including gradient norm, gradient × input, erasure, KernelSHAP, and integrated gradients.

## Key Results
- Gradient norm methods (gradient norm and gradient × input) generally outperform other attribution methods on fine-tuned models
- Erasure method performs better on off-the-shelf models compared to fine-tuned models
- Counterfactual approach yields consistent rankings across different editor models
- The method successfully avoids out-of-distribution issues that affect perturbation-based evaluations, particularly for off-the-shelf models

## Why This Works (Mechanism)
The counterfactual generation approach works because it creates in-distribution modifications that maintain the natural fluency of language while systematically testing which tokens are truly important for model predictions. By using an editor model to generate counterfactuals, the method ensures that replacements are contextually appropriate and don't introduce artifacts that could confound evaluation. The approach measures faithfulness by the efficiency of prediction flipping - better attribution methods should identify the minimal set of tokens needed to change the prediction, demonstrating they capture the model's true decision-making process.

## Foundational Learning
- **Attribution methods**: Techniques for identifying which input tokens most influence a model's prediction - needed to understand how different methods rank token importance; quick check: compare how different methods score the same input tokens
- **Counterfactual generation**: Creating modified inputs that change model predictions while maintaining input properties - needed to create controlled test cases for evaluation; quick check: verify generated counterfactuals flip predictions as intended
- **Out-of-distribution evaluation**: Assessing models on data that differs from training distribution - needed to understand why standard perturbation methods fail; quick check: compare model behavior on original vs perturbed inputs
- **Editor models**: Models that modify inputs while preserving fluency - needed to generate natural counterfactuals; quick check: evaluate fluency of generated counterfactuals with human judges
- **Autoregressive language models**: Models that generate text sequentially - needed context for why attribution evaluation is challenging; quick check: verify model generates tokens left-to-right

## Architecture Onboarding

**Component Map**
Editor Model -> Counterfactual Generator -> Attribution Method Evaluator

**Critical Path**
1. Input text is processed by attribution method to identify important tokens
2. Editor model receives attribution scores and generates counterfactual by replacing important tokens
3. Counterfactual is evaluated by checking if prediction flips and counting token changes

**Design Tradeoffs**
- Using editor model ensures fluency but introduces dependency on another model's quality
- Evaluating based on minimal token changes assumes attribution methods should identify all necessary changes, which may not always be optimal
- Focus on classification tasks limits applicability to generation tasks

**Failure Signatures**
- Counterfactuals fail to flip predictions despite replacing important tokens (suggests attribution method missed critical features)
- Counterfactuals are unnatural or incoherent (suggests editor model failure)
- Evaluation shows inconsistent rankings across different editors (suggests OOD issues or editor model artifacts)

**First 3 Experiments**
1. Compare token importance rankings from different attribution methods on the same inputs
2. Generate counterfactuals using top-k important tokens and verify prediction flips
3. Test whether counterfactuals maintain fluency and in-distribution properties

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can we evaluate the faithfulness of attribution methods for non-classification tasks like machine translation or summarization?
- Basis in paper: [explicit] "we evaluated our protocol for classification datasets; evaluating it for other tasks like translation is left for future work."
- Why unresolved: The current evaluation protocol relies on generating counterfactuals that flip the model's prediction to a specific label, which is straightforward for classification but unclear how to adapt for generation tasks where there's no single correct output.
- What evidence would resolve it: Demonstrating the protocol on non-classification tasks, showing how to generate counterfactuals that meaningfully change the output in ways that can be evaluated for faithfulness.

### Open Question 2
- Question: How can we ensure counterfactual generators don't inadvertently exploit artifacts or shortcuts in the predictor model rather than capturing genuine decision-making processes?
- Basis in paper: [explicit] "the counterfactual generator may unintentionally know the artifacts and shortcuts used by the predictor to flip the label, and this could limit the intended application of our approach."
- Why unresolved: While the authors acknowledge this limitation, they don't propose methods to detect or prevent generators from exploiting model-specific artifacts rather than understanding true feature importance.
- What evidence would resolve it: Developing methods to test whether counterfactual generators are exploiting artifacts versus genuinely understanding feature importance, possibly through ablation studies or testing on models with known vulnerabilities.

### Open Question 3
- Question: How can we scale the counterfactual generation approach to work with extremely long sequences while maintaining computational efficiency?
- Basis in paper: [explicit] "Generating counterfactuals, especially for long sequences, is computationally expensive."
- Why unresolved: The authors identify computational cost as a limitation but don't propose solutions for scaling the approach to handle longer inputs efficiently.
- What evidence would resolve it: Demonstrating the approach on longer sequences with reduced computational cost through techniques like sub-sampling, hierarchical generation, or more efficient editing strategies.

## Limitations
- The counterfactual generation approach relies heavily on the quality of the editor model, which could introduce its own biases or distributional shifts not fully captured by the evaluation
- The evaluation protocol assumes that flipping predictions while maintaining fluency is the primary criterion for successful counterfactual generation, but this may not fully capture all aspects of model behavior or robustness
- The comparison of attribution methods is limited to a specific set of techniques and model configurations (fine-tuned vs. off-the-shelf), potentially missing other relevant methods or model types

## Confidence
- Counterfactual evaluation approach is effective: Medium confidence
- Gradient norm methods perform best on fine-tuned models: Medium confidence
- Counterfactual approach avoids OOD issues: High confidence for fine-tuned models, Medium confidence for off-the-shelf models

## Next Checks
1. Test the counterfactual generation and evaluation approach across a broader range of NLP tasks (e.g., question answering, natural language inference) to assess generalizability.
2. Evaluate whether the editor model itself exhibits OOD behavior when generating counterfactuals, and if so, develop methods to mitigate this potential confounding factor.
3. Compare the counterfactual evaluation results with human evaluations of attribution method faithfulness to validate the automated protocol's alignment with human judgment.