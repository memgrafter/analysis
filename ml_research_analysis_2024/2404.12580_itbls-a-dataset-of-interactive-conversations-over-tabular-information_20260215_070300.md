---
ver: rpa2
title: 'iTBLS: A Dataset of Interactive Conversations Over Tabular Information'
arxiv_id: '2404.12580'
source_url: https://arxiv.org/abs/2404.12580
tags:
- tabular
- table
- itbls
- data
- tables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'iTBLS is a dataset of interactive conversations over tables from
  scientific papers. It covers three tasks: interpretation (understanding tables),
  modification (manipulating table content), and generation (expanding tables with
  new information).'
---

# iTBLS: A Dataset of Interactive Conversations Over Tabular Information

## Quick Facts
- arXiv ID: 2404.12580
- Source URL: https://arxiv.org/abs/2404.12580
- Reference count: 35
- Primary result: Multi-step approach improves performance on interpretation, modification, and generation tasks by up to 15%, 18%, and 38% respectively compared to standard PEFT

## Executive Summary
iTBLS is a novel dataset of interactive conversations over tables from scientific papers, designed to evaluate language models' capabilities in interpreting, modifying, and generating tabular information. The dataset contains 4000 tables with 3-turn conversational interactions covering three distinct tasks. The authors present baseline approaches using zero-shot prompting, parameter-efficient fine-tuning (QLoRA), and a multi-step approach that breaks down complex tasks into simpler sub-tasks. The multi-step approach shows significant improvements over standard fine-tuning, demonstrating the effectiveness of task decomposition for table-based reasoning.

## Method Summary
The authors collected 4000 tables from arXiv papers and created 3-turn conversational interactions for three tasks: interpretation (understanding table content), modification (manipulating table content), and generation (expanding tables with new information). Tables are serialized using a comma-separated format with newline characters for row separation. Baseline approaches include zero-shot prompting using Llama-2 models, parameter-efficient fine-tuning using QLoRA with rank 64 and learning rate 2e-4, and a multi-step approach that first identifies the task type and then executes specific operations. The multi-step approach uses program synthesis for modification tasks and a two-stage question-answering approach for generation tasks.

## Key Results
- Multi-step approach improves interpretation accuracy by up to 15% compared to standard PEFT
- Multi-step approach improves modification accuracy by up to 18% compared to standard PEFT
- Multi-step approach improves generation F1 score by up to 38% compared to standard PEFT

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The multi-step approach improves accuracy by breaking down complex tasks into simpler sub-tasks that can be handled more effectively by LLMs.
- **Mechanism:** For interpretation and modification, the approach first identifies the specific cells referred to by the user request using an LLM, then uses program synthesis to execute the action on the table. For generation, it uses a two-stage question-answering approach where the first stage determines whether a row or column needs to be generated, and the second stage answers synthesized questions to generate the content.
- **Core assumption:** Breaking down complex table operations into simpler sub-tasks improves LLM performance by reducing cognitive load and enabling more focused reasoning.
- **Evidence anchors:**
  - [abstract] "The developed approach results in an improvement on all tasks on a sequence-to-sequence modeling baseline on iTBLS. In addition, the question-answering-based reformulation is applied to datasets from prior work for the text-to-table task where textual paragraphs are summarized into tables. The novel approach results in up to 13% improvement in Exact-Match accuracy and up to 16% improvement in BERTScores compared to the prior state-of-the-art."
  - [section] "This multi-step method results in an improvement over standard parameter-efficient fine-tuning by up to 15% on interpretation, 18% on modification, and 38% on generation."
- **Break condition:** The mechanism breaks when the sub-tasks themselves become too complex for the LLM to handle reliably, or when the overhead of coordinating multiple steps outweighs the benefits.

### Mechanism 2
- **Claim:** Parameter-efficient fine-tuning (PEFT) using QLoRA significantly improves performance while requiring fewer computational resources than full fine-tuning.
- **Mechanism:** QLoRA fine-tunes LLMs by optimizing a smaller set of parameters while keeping most model weights frozen, allowing for efficient adaptation to the specific tasks in iTBLS.
- **Core assumption:** Fine-tuning on task-specific data improves model performance more than zero-shot prompting, and PEFT provides a good trade-off between performance and computational efficiency.
- **Evidence anchors:**
  - [abstract] "The developed approach results in an improvement on all tasks on a sequence-to-sequence modeling baseline on iTBLS."
  - [section] "Our best-performing method is a multi-step approach combined with parameter-efficient fine-tuning."
- **Break condition:** The mechanism breaks when the fine-tuned model overfits to the training data or when the task distribution in iTBLS differs significantly from the pretraining distribution of the LLM.

### Mechanism 3
- **Claim:** Using a structured encoding format for table data (comma-separated format with newline characters) enables LLMs to better understand and process tabular information.
- **Mechanism:** Tables are serialized using a comma-separated format where commas separate cells in the same row and newline characters separate rows, providing a clear 2D structure that LLMs can parse.
- **Core assumption:** LLMs can effectively learn to interpret structured text representations of tables when provided with appropriate prompts and fine-tuning.
- **Evidence anchors:**
  - [section] "To augment LLMs with 2D tabular information, we serialize tables in iTBLS using a comma-separated format (Singha et al., 2023)."
  - [section] "Commas separate cells in the same row belonging to different columns and newline characters '\n' separate cells across rows."
- **Break condition:** The mechanism breaks when tables become too complex for the simple serialization format to capture their structure effectively, or when the LLM fails to learn the mapping between the serialized format and the underlying table semantics.

## Foundational Learning

- **Concept: Tabular data representation and manipulation**
  - Why needed here: Understanding how tables are structured and how to perform operations like cell swapping, row/column addition is fundamental to working with iTBLS.
  - Quick check question: How would you represent a 3x3 table with values 1-9 in the comma-separated format used by iTBLS?

- **Concept: Natural language understanding and generation**
  - Why needed here: The tasks in iTBLS involve interpreting natural language requests and generating appropriate responses, requiring strong NLP capabilities.
  - Quick check question: Given a table and the request "What is the value in the second row, third column?", what would be the expected output?

- **Concept: Question answering and reasoning**
  - Why needed here: The generation task in iTBLS is reformulated as a question-answering problem, requiring the ability to answer questions about table content.
  - Quick check question: If asked to generate a row based on the existing column headers "Name", "Age", "City", what kind of questions would you need to answer to complete this task?

## Architecture Onboarding

- **Component map:** Serialized table input -> Interaction identification -> Task-specific processing -> Processed table or extracted information
- **Critical path:**
  1. Serialize table input
  2. Identify interaction type (interpretation, modification, generation)
  3. Process request using appropriate method
  4. Generate output
  5. Deserialize output if necessary
- **Design tradeoffs:**
  - Single-step vs multi-step approaches: Multi-step provides better accuracy but adds complexity
  - Zero-shot vs fine-tuning: Fine-tuning provides better performance but requires more resources
  - Model size vs performance: Larger models generally perform better but are more expensive to run
- **Failure signatures:**
  - Incorrect interaction identification
  - Malformed table outputs
  - Inaccurate cell references
  - Poor generalization to unseen table structures
- **First 3 experiments:**
  1. Test interaction identification on a small sample of iTBLS data using zero-shot prompting
  2. Evaluate zero-shot performance on interpretation task with a simple table and request
  3. Compare single-step vs multi-step approaches on a modification task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the iTBLS dataset compare in size and diversity to existing tabular datasets like WIKI TABLE QUESTIONS, TABERT, or FeTaQA?
- Basis in paper: [explicit] The paper mentions that iTBLS is sourced from arXiv and consists of 4000 tables, but does not provide a direct comparison to other datasets.
- Why unresolved: The paper does not provide a detailed comparison of iTBLS with other existing tabular datasets in terms of size, diversity, or task coverage.
- What evidence would resolve it: A comparative analysis of iTBLS with other tabular datasets, including statistics on the number of tables, types of tasks covered, and domain diversity.

### Open Question 2
- Question: What are the specific performance differences between the zero-shot, parameter-efficient fine-tuning (PEFT), and multi-step approaches on each of the three tasks (interpretation, modification, and generation) in iTBLS?
- Basis in paper: [explicit] The paper presents baseline approaches using zero-shot prompting, PEFT, and a multi-step approach, and mentions improvements of up to 15% on interpretation, 18% on modification, and 38% on generation compared to standard PEFT.
- Why unresolved: The paper does not provide detailed performance metrics for each approach on each task, making it difficult to assess the relative strengths and weaknesses of each method.
- What evidence would resolve it: A comprehensive table or figure showing the exact performance metrics (e.g., accuracy, F1 score) for each approach on each task, allowing for a direct comparison.

### Open Question 3
- Question: How does the multi-step approach with syntax generation specifically improve the performance on the modification task compared to standard PEFT?
- Basis in paper: [explicit] The paper mentions that the multi-step approach involves identifying cells referred to by the user using an LLM in a zero-shot configuration followed by program synthesis to execute the action, and that this approach results in an 18.8% absolute increase in accuracy on the test split for modification.
- Why unresolved: The paper does not provide a detailed explanation of how the multi-step approach with syntax generation works in practice, or why it is particularly effective for the modification task.
- What evidence would resolve it: A detailed description of the syntax generation process, including examples of how the LLM identifies cells and how the program synthesis is executed, along with ablation studies showing the impact of each component on performance.

## Limitations

- The specific prompt templates used for zero-shot and multi-step approaches are not provided, making exact replication difficult
- The dataset may have biases toward certain types of scientific tables, potentially limiting generalizability to other domains
- The evaluation focuses on Exact Match and F1 metrics but doesn't report on the diversity of generated table content or robustness to out-of-distribution table structures

## Confidence

- **High Confidence**: The claim that the multi-step approach improves performance over standard PEFT is well-supported by the reported metrics (up to 15% on interpretation, 18% on modification, 38% on generation). The mechanism of breaking down complex tasks into simpler sub-tasks is also well-established in the literature.
- **Medium Confidence**: The effectiveness of QLoRA for parameter-efficient fine-tuning is reasonably supported, though the specific hyperparameters and their impact on performance could benefit from more detailed analysis.
- **Low Confidence**: The assumption that the comma-separated serialization format optimally captures table structure for LLM processing lacks empirical validation against alternative formats.

## Next Checks

1. Conduct ablation studies on the prompt templates used in both zero-shot and multi-step approaches to quantify the contribution of specific prompt engineering choices to the reported improvements.

2. Evaluate the approaches on tables from domains outside scientific papers (e.g., financial reports, sports statistics) to assess robustness and identify potential domain-specific biases in the dataset.

3. Compare the comma-separated serialization format against alternative representations (such as JSON or grid-based formats) to empirically determine which format enables the best LLM performance on table understanding and manipulation tasks.