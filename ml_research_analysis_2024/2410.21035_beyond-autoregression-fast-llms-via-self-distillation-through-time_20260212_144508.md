---
ver: rpa2
title: 'Beyond Autoregression: Fast LLMs via Self-Distillation Through Time'
arxiv_id: '2410.21035'
source_url: https://arxiv.org/abs/2410.21035
tags:
- steps
- rounds
- sdtt
- sampling
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the slow inference speed of autoregressive
  language models by introducing a novel self-distillation method for discrete diffusion
  models. The core idea is Self-Distillation Through Time (SDTT), which fine-tunes
  diffusion models to generate multiple tokens in parallel, reducing the number of
  required sampling steps by a factor of 32-64.
---

# Beyond Autoregression: Fast LLMs via Self-Distillation Through Time

## Quick Facts
- arXiv ID: 2410.21035
- Source URL: https://arxiv.org/abs/2410.21035
- Authors: Justin Deschenaux; Caglar Gulcehre
- Reference count: 40
- One-line primary result: SDTT achieves lower perplexity than GPT-2 with nucleus sampling while being up to 8x faster than autoregressive models using KV caching

## Executive Summary
This paper introduces Self-Distillation Through Time (SDTT), a novel method that dramatically accelerates inference in autoregressive language models by training discrete diffusion models to generate multiple tokens in parallel. The approach fine-tunes a pre-trained diffusion model to match teacher model predictions across multiple sampling steps, reducing the number of required sampling steps by 32-64x. Experimental results demonstrate that SDTT achieves lower perplexity than GPT-2 with nucleus sampling while being up to 8x faster than autoregressive models using KV caching, with minimal impact on downstream task performance.

## Method Summary
SDTT works by training a student discrete diffusion model to replicate the multi-step denoising process of a teacher model in a single forward pass. The method uses Kullback-Leibler divergence as the distillation loss to preserve the probabilistic structure needed for high-quality text generation. Through iterated distillation with progressively increasing step sizes (2→4→8→16→32→64 steps), the student learns to generate up to 32 tokens in parallel. The approach is evaluated across models ranging from 140M to 860M parameters, showing consistent improvements in inference speed while maintaining or improving text quality metrics.

## Key Results
- SDTT reduces inference steps by 32-64x while maintaining quality
- Achieves lower perplexity than GPT-2 with nucleus sampling
- Up to 8x faster inference than autoregressive models using KV caching
- Scales effectively to models with up to 860M parameters

## Why This Works (Mechanism)

### Mechanism 1
Self-Distillation Through Time allows a discrete diffusion model to generate multiple tokens in parallel by fine-tuning it to mimic teacher model predictions across multiple sampling steps. The student diffusion model is trained to match the log probabilities of a teacher model that denoises tokens over many steps. When generating, the student can produce up to 32 tokens in one forward pass by learning to replicate the multi-step denoising process in a single step. Core assumption: The teacher model's multi-step predictions can be compressed into a single-step student model without significant loss of quality. Evidence: SDTT achieves lower perplexity than GPT-2 with nucleus sampling while being up to 8x faster than autoregressive models using KV caching.

### Mechanism 2
Using Kullback-Leibler divergence (KLD) as the distillation loss preserves downstream performance better than other divergence measures. KLD measures the difference between the student's predicted distribution and the teacher's target distribution. By minimizing KLD, the student is encouraged to match the teacher's predictions while maintaining the probabilistic structure needed for tasks like LAMBADA. Core assumption: KLD is a suitable divergence measure for preserving the quality of multi-token predictions in language modeling. Evidence: Students distilled with KLD clearly outperform students trained using MSE and TVD on LAMBADA, and reach higher accuracies than other students in all but one task.

### Mechanism 3
Iterated SDTT (applying SDTT multiple times with increasing step sizes) is more stable and effective than trying to distill many steps at once. By progressively increasing the number of steps the student can generate (e.g., from 2 to 4 to 8), the student learns to compress more information without being overwhelmed by the complexity of multi-step predictions. Core assumption: Progressive distillation allows the student to learn complex mappings incrementally, reducing error accumulation. Evidence: The LAMBADA accuracy remains unchanged with shorter rounds, and resetting the EMA and optimizer state shows stable training behavior.

## Foundational Learning

- **Discrete diffusion language modeling**: Understanding how discrete diffusion models work is crucial for grasping how SDTT modifies them to generate multiple tokens in parallel. Quick check: What is the key difference between discrete and continuous diffusion models in the context of language modeling?

- **Knowledge distillation**: SDTT is a form of knowledge distillation, so understanding how student models learn from teacher models is essential. Quick check: What are the common divergence measures used in knowledge distillation, and how do they differ?

- **Kullback-Leibler divergence**: KLD is the primary divergence measure used in SDTT, so understanding its properties and how it compares to other measures is important. Quick check: What is the difference between forward and reverse KLD, and why is reverse KLD used in SDTT?

## Architecture Onboarding

- **Component map**: Pre-trained MDLM (teacher) -> Distillation targets (log probabilities) -> Student MDLM (fine-tuned) -> Generated text

- **Critical path**: 1. Pre-train a discrete diffusion language model (teacher) 2. Generate distillation targets by running teacher for multiple steps 3. Fine-tune student model to match teacher's predictions using divergence measure 4. Evaluate student model's performance on various tasks

- **Design tradeoffs**: Number of distillation rounds (more rounds increase training time but may improve performance), divergence measure choice (different measures suit different tasks), step size (larger steps reduce inference time but may degrade quality)

- **Failure signatures**: Performance degradation on downstream tasks (student not effectively learning from teacher), increased perplexity (poor quality text generation), unstable training (issues with divergence measure or distillation targets)

- **First 3 experiments**: 1. Implement SDTT with small discrete diffusion model on simple text generation task 2. Compare performance using different divergence measures (KLD, TVD, MSE) on LAMBADA 3. Scale SDTT to larger discrete diffusion model and compare performance and inference speed to autoregressive baseline

## Open Questions the Paper Calls Out

### Open Question 1
Can SDTT be extended to continuous diffusion models or other generative architectures beyond discrete diffusion? The paper mentions that SDTT does not rely on deterministic mappings like DDIM, which are used in continuous diffusion distillation methods. This remains unresolved as the paper focuses exclusively on discrete diffusion models for language modeling.

### Open Question 2
What is the theoretical explanation for why iterated SDTT with increasing step sizes works better than attempting to distill multiple steps at once? The paper observes that distilling more than 2 teacher steps at once is difficult and results in weaker performance, while iterated SDTT works better, but provides no theoretical explanation for this phenomenon.

### Open Question 3
How does the choice of divergence measure (KLD, MSE, TVD) affect the diversity of generated samples in the long term, beyond the self-BLEU scores reported? The paper only measures immediate diversity effects through self-BLEU scores, not long-term diversity implications of different divergence measures.

## Limitations

- Performance ceiling unclear for very large models, with diminishing returns observed at 3B parameters
- All experiments conducted on autoregressive teacher models, limiting claims about broader applicability
- Focus on language modeling tasks only, with no demonstration of effectiveness for other sequential data types

## Confidence

- **High Confidence**: Core claim of 32-64x reduction in inference steps while maintaining quality is well-supported
- **Medium Confidence**: Scalability claims to 3B parameters supported but show diminishing returns
- **Low Confidence**: Claims about preserving downstream performance should be qualified as relative to autoregressive baselines

## Next Checks

1. Apply SDTT using non-autoregressive teacher models to determine if effectiveness depends on teacher architecture
2. Conduct targeted experiments measuring performance on tasks requiring long-range coherence to quantify distributional shifts
3. Implement SDTT for discrete diffusion models in code generation or mathematical reasoning domains to test generalizability beyond natural language