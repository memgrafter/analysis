---
ver: rpa2
title: The Impact of Syntactic and Semantic Proximity on Machine Translation with
  Back-Translation
arxiv_id: '2403.18031'
source_url: https://arxiv.org/abs/2403.18031
tags:
- translation
- languages
- back-translation
- language
- bleu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We use artificial languages to systematically investigate the effectiveness
  of back-translation (BT) for unsupervised neural machine translation (UNMT). While
  BT is empirically successful, we show it is theoretically insufficient in general,
  as the back-translation objective can be met without accurate translation.
---

# The Impact of Syntactic and Semantic Proximity on Machine Translation with Back-Translation

## Quick Facts
- arXiv ID: 2403.18031
- Source URL: https://arxiv.org/abs/2403.18031
- Reference count: 18
- Primary result: Back-translation's success relies critically on shared semantic dependencies across languages, beyond lexical or syntactic similarities.

## Executive Summary
This paper systematically investigates back-translation effectiveness in unsupervised neural machine translation using artificial languages with controlled properties. While back-translation is empirically successful, the authors demonstrate it is theoretically insufficient in general, as the back-translation objective can be met without accurate translation. Through controlled experiments, they show that shared syntactic structures, vocabulary, and word frequency distributions fail to explain back-translation's effectiveness. The key finding is that back-translation succeeds when languages share deep semantic dependencies across categories, with more fine-grained semantic distinctions yielding better results.

## Method Summary
The paper uses artificial languages generated from Context-Free Grammars (CFGs) with 64 possible grammars and 1,374 fictitious words. Two sets of 100,000 sentence structures each are created for training, with 10,000 parallel sentence pairs for validation and testing. A transformer-based NMT model is trained using on-the-fly back-translation (OTF-BT) with denoising auto-encoding (DAE) as an auxiliary objective. The model is trained for 40 epochs with batch size 16 and Adam optimizer (10^-4 learning rate). BLEU score is used for translation accuracy, with additional analysis of part-of-speech accuracy and word-level entropy for vocabulary mapping.

## Key Results
- Similar syntactic structures between languages do not guarantee back-translation success
- Shared vocabulary and word frequency distributions also fail to explain back-translation's effectiveness
- Even with these features, back-translation fails to produce accurate translations due to vocabulary alignment issues
- Adding supervised signals (parallel sentences or bilingual dictionaries) dramatically improves performance
- Introducing semantic information via lexical fields improves alignment, with more fine-grained semantic distinctions yielding better results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Back-translation works when languages share deep semantic dependencies across categories.
- Mechanism: The model learns to align lexical fields (semantic categories) between languages, ensuring that words from the same semantic class in L1 map to the same class in L2.
- Core assumption: Semantic dependencies are parallel across languages, creating co-occurrence patterns that guide alignment beyond syntax.
- Evidence anchors: Adding semantic information via lexical fields improves alignment, with more fine-grained semantic distinctions yielding better results.

### Mechanism 2
- Claim: Back-translation can exploit syntactic similarity to bootstrap translation, but only as a secondary signal.
- Mechanism: Shared grammatical structures reduce the search space for alignment, making it easier to map syntactic roles between languages.
- Core assumption: Similar syntactic rules mean that the decoder can more easily reconstruct sentences when given a translation that preserves structure.
- Evidence anchors: Back-translation becomes less performant as grammars differ more (Table 4).

### Mechanism 3
- Claim: Shared vocabulary provides weak but non-zero alignment signal.
- Mechanism: Common words between languages act as reference points that help bootstrap lexical mapping, but are not sufficient alone.
- Core assumption: Identical or near-identical words across languages can anchor the rest of the lexicon.
- Evidence anchors: Parallel word frequency distributions and partially shared vocabulary are not sufficient to explain back-translation success.

## Foundational Learning

- Concept: Artificial languages with controlled lexical/syntactic variation
  - Why needed here: Allows isolating effects of specific properties without confounding real-world noise.
  - Quick check question: Can you generate two artificial languages that differ only in word order but share vocabulary?

- Concept: Context-Free Grammars (CFGs) for language generation
  - Why needed here: Provides a formal, switchable syntax that can be systematically varied to test grammar impact.
  - Quick check question: What happens to BLEU when you flip switch S1 in the CFG?

- Concept: Lexical fields and semantic clustering
  - Why needed here: Models coarse semantic structure to test whether semantic dependencies aid alignment.
  - Quick check question: If words from the same semantic field never co-occur, how does the model learn to map them across languages?

## Architecture Onboarding

- Component map: Encoder → Shared hidden space → Decoder, with separate L1/L2 modules; denoising auto-encoding + back-translation objectives.
- Critical path: DAE pre-training → BT iteration → supervised fine-tuning (if available) → evaluation.
- Design tradeoffs: Fully unsupervised vs. semi-supervised (small parallel set or dictionary); coarse vs. fine semantic fields.
- Failure signatures: High round-trip BLEU but low translation BLEU; correct POS sequences but wrong word choices; identity mapping learned instead of translation.
- First 3 experiments:
  1. Within-language translation (sanity check: BLEU > 97).
  2. Cross-grammar translation with shared lexicon (test syntactic impact).
  3. Cross-lexicon translation with identical grammar (test vocabulary mapping failure).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific semantic properties (beyond lexical fields) enable back-translation to succeed in unsupervised neural machine translation?
- Basis in paper: The paper concludes that rich semantic dependencies parallel across languages are crucial for back-translation's success, but acknowledges that their simple implementation of lexical fields is insufficient.
- Why unresolved: The paper's experiments with lexical fields only implement a crude approximation of semantics (word co-occurrence constraints) and show limited success.
- What evidence would resolve it: Systematic experiments varying different semantic properties in artificial languages to measure their impact on back-translation performance.

### Open Question 2
- Question: Why do some grammatical switches have dramatically larger effects on back-translation performance than others?
- Basis in paper: The paper finds that switch 1 (S→NP VP vs VP NP) causes the largest performance drop, followed by switch 6 (NP→Adj NP vs NP Adj), while others like switch 5 have minimal impact.
- Why unresolved: The paper suggests this may relate to whether changes occur near the root vs leaves of the parse tree, but this doesn't fully explain the observed differences.
- What evidence would resolve it: Detailed analysis of how different switch types affect the learned embedding spaces and translation functions.

### Open Question 3
- Question: How much supervised signal is theoretically required to make back-translation effective, and what form is most efficient?
- Basis in paper: The paper shows that even small amounts of supervision dramatically improve back-translation performance, but doesn't explore the minimal effective supervision threshold.
- Why unresolved: The experiments only test two extremes - no supervision and substantial supervision - without exploring the continuous spectrum between them.
- What evidence would resolve it: Systematic experiments varying supervision amount and type to map the relationship between supervision and performance.

## Limitations
- Reliance on artificial languages may not fully capture the complexity of natural language translation
- Controlled approach may exaggerate effects compared to natural languages
- Semantic field implementation is a crude approximation that may not reflect true semantic dependencies

## Confidence
- **High confidence**: Back-translation alone is theoretically insufficient and can produce high round-trip BLEU without accurate translation
- **Medium confidence**: Shared vocabulary and syntactic similarity are insufficient to explain back-translation success
- **Low confidence**: Semantic dependencies are the critical factor for back-translation success

## Next Checks
1. Test core finding that BT can achieve high round-trip BLEU without translation accuracy on a small set of related natural languages (e.g., Spanish-Portuguese)
2. Conduct controlled experiments that vary the granularity of semantic fields while holding syntactic and lexical properties constant
3. Apply attention visualization and embedding analysis to examine whether the model actually learns to align semantic categories as hypothesized