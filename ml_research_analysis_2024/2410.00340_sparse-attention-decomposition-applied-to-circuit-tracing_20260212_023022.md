---
ver: rpa2
title: Sparse Attention Decomposition Applied to Circuit Tracing
arxiv_id: '2410.00340'
source_url: https://arxiv.org/abs/2410.00340
tags:
- attention
- head
- token
- figure
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for identifying communication pathways
  between attention heads in language models, addressing the challenge of tracing
  circuits through complex neural networks. The core idea leverages the observation
  that attention scores can be sparsely decomposed in terms of singular vectors of
  attention head matrices, allowing for efficient separation of signal from noise
  in model activations.
---

# Sparse Attention Decomposition Applied to Circuit Tracing

## Quick Facts
- arXiv ID: 2410.00340
- Source URL: https://arxiv.org/abs/2410.00340
- Authors: Gabriel Franco; Mark Crovella
- Reference count: 40
- Introduces method for identifying communication pathways between attention heads in language models

## Executive Summary
This paper presents a novel approach for tracing communication pathways between attention heads in transformer language models through sparse attention decomposition. The method leverages singular vector decomposition of attention head matrices to efficiently separate signal from noise in model activations. Applied to GPT-2 small on the Indirect Object Identification task, the technique identifies functionally significant causal communication paths between attention heads using only a single forward pass.

## Method Summary
The core methodology involves decomposing attention scores using singular vectors of attention head matrices, which allows for sparse representation of attention patterns. By identifying low-dimensional components of inputs that most contribute to attention head outputs, the approach traces causal communication paths between heads. The technique requires only one forward pass through the model, making it computationally efficient while still revealing detailed circuit structures including redundant paths and early-layer feature construction for IO tokens.

## Key Results
- Successfully traces functionally significant causal communication paths between attention heads in GPT-2 small
- Identifies specific features used for inter-head communication with single forward pass efficiency
- Reveals detailed circuit structures including redundant paths and early-layer feature construction for IO tokens
- Demonstrates causal effects through edge intervention experiments on model performance

## Why This Works (Mechanism)
The method works by exploiting the observation that attention scores can be sparsely decomposed using singular vectors of attention head matrices. This mathematical decomposition allows separation of signal from noise in model activations, revealing the underlying communication structure between attention heads. The approach identifies which low-dimensional input components contribute most significantly to attention head outputs, effectively tracing the information flow through the network.

## Foundational Learning
1. **Singular Value Decomposition (SVD)** - why needed: Core mathematical tool for decomposing attention matrices; quick check: Verify understanding of SVD properties and applications in dimensionality reduction
2. **Attention Head Mechanisms** - why needed: Understanding how attention heads process and communicate information; quick check: Confirm knowledge of scaled dot-product attention computation
3. **Causal Tracing in Neural Networks** - why needed: Framework for identifying causal relationships between model components; quick check: Review concepts of causal attribution and intervention experiments

## Architecture Onboarding
- **Component Map**: Input tokens → Attention heads → Singular vector decomposition → Low-dimensional component extraction → Communication path identification → Edge intervention validation
- **Critical Path**: Forward pass through model → Attention matrix computation → SVD decomposition → Component contribution analysis → Pathway tracing
- **Design Tradeoffs**: Computational efficiency (single pass) vs. completeness of pathway identification
- **Failure Signatures**: Missing long-range dependencies, false positives in redundant path identification
- **First Experiments**:
  1. Apply method to synthetic attention patterns with known ground truth
  2. Compare results on different random seeds of same model
  3. Test on attention-only transformer without MLPs

## Open Questions the Paper Calls Out
None

## Limitations
- Causal claims about communication paths rely on intervention experiments that may not capture full complexity of information flow
- Identified "redundant paths" and "early-layer feature construction" remain observational without comprehensive ablation studies
- Single forward pass efficiency may trade off against completeness in capturing all relevant communication pathways

## Confidence
- **High confidence**: Mathematical framework for sparse attention decomposition using singular vectors is well-established and technically correct
- **Medium confidence**: Causal claims about communication paths are supported but could benefit from more rigorous analysis including counterfactuals
- **Low confidence**: Generalizability to larger models and different tasks remains untested

## Next Checks
1. Perform comprehensive ablation studies on identified communication edges to distinguish necessary versus sufficient pathways for task performance
2. Test the method on larger models (GPT-2 medium/large) and different tasks to assess scalability and generalizability
3. Conduct quantitative comparison between single-pass results and more exhaustive search methods to characterize the completeness-efficiency trade-off