---
ver: rpa2
title: Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal Learning
  for Glaucoma Forecasting from Irregular Time Series Images
arxiv_id: '2402.13475'
source_url: https://arxiv.org/abs/2402.13475
tags:
- glaucoma
- dataset
- image
- which
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multi-scale spatio-temporal transformer network
  (MST-former) for glaucoma forecasting from irregularly sampled fundus image sequences.
  The method introduces space-time positional encoding, time-aware temporal attention,
  and multi-scale encoder-decoder architecture to capture both spatial and temporal
  features.
---

# Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal Learning for Glaucoma Forecasting from Irregular Time Series Images

## Quick Facts
- arXiv ID: 2402.13475
- Source URL: https://arxiv.org/abs/2402.13475
- Reference count: 40
- Primary result: Achieves state-of-the-art glaucoma forecasting with AUC of 98.6% on SIGF dataset

## Executive Summary
This paper introduces a multi-scale spatio-temporal transformer network (MST-former) for glaucoma forecasting from irregularly sampled fundus image sequences. The method addresses the challenges of irregular time intervals, class imbalance, and the need to capture both spatial and temporal disease progression patterns. By combining multi-scale spatial feature extraction with time-aware temporal attention and a temperature-controlled balanced loss function, the approach achieves significant performance improvements over existing methods. The model demonstrates strong generalization capabilities, showing competitive results on both glaucoma forecasting and Alzheimer's disease prediction tasks.

## Method Summary
MST-former processes sequential fundus images through a multi-scale encoder-decoder transformer architecture. The method introduces space-time positional encoding to capture both spatial patch positions and temporal observation order, while time-aware temporal attention weights attention scores based on time intervals between observations. A temperature-controlled Balanced Softmax Cross-entropy loss addresses the severe class imbalance in glaucoma datasets. The network operates on patch-embedded tokens processed at multiple spatial resolutions, with scale transition modules enabling information flow between scales. The final classification head predicts glaucoma probability at the next time point.

## Key Results
- Achieves state-of-the-art performance on SIGF dataset: AUC 98.6%, Accuracy 97.1%, Sensitivity 94.1%, Specificity 97.2%
- Outperforms existing methods including ResT, ViT, and CNN-based approaches by significant margins
- Demonstrates strong generalization with 90.3% accuracy on ADNI MRI dataset for Alzheimer's disease prediction
- Temperature-controlled Balanced Softmax Cross-entropy with τ=2.0 provides optimal class imbalance handling

## Why This Works (Mechanism)

### Mechanism 1
Multi-scale spatio-temporal transformer architecture enables effective modeling of both spatial and temporal disease progression cues in irregularly sampled longitudinal fundus images. The network processes sequential images through a multi-scale encoder-decoder structure, where spatial attention operates within individual images at multiple resolutions while temporal attention captures disease progression across time points. Time-aware temporal attention weights attention scores based on time intervals between observations. Core assumption: Disease progression patterns can be captured by modeling spatial features at multiple scales simultaneously with temporal dynamics, and irregular sampling intervals can be effectively handled by scaling temporal attention. Evidence anchors: [abstract] "we employ a multi-scale structure to extract features at various resolutions, which can largely exploit rich spatial information encoded in each image. Besides, we design a time distance matrix to scale time attention in a non-linear manner, which could effectively deal with the irregularly sampled data." Break condition: If disease progression patterns are not primarily captured by spatial-temporal relationships, or if time intervals cannot be meaningfully incorporated into attention scaling.

### Mechanism 2
Temperature-controlled Balanced Softmax Cross-entropy loss effectively addresses class imbalance in glaucoma forecasting datasets. The loss function adjusts class weights based on sample distribution while temperature scaling (τ > 1) controls the extent of this adjustment, preventing extreme weight values while maintaining class balance focus. Core assumption: Class imbalance in glaucoma datasets can be mitigated through weighted loss functions that account for class distribution differences, and temperature scaling provides appropriate control over weight magnitude. Evidence anchors: [abstract] "we introduce a temperature-controlled Balanced Softmax Cross-entropy loss to address the class imbalance issue" Break condition: If temperature scaling causes optimization instability or if class imbalance effects cannot be effectively mitigated through loss function modification.

### Mechanism 3
Space-time positional encoding captures both intra-image spatial relationships and inter-image temporal relationships in sequential data. Positional encoding combines patch indices within images and time intervals between images using sinusoidal functions, allowing the transformer to distinguish both spatial patch positions and temporal observation order. Core assumption: Positional information is critical for transformer performance on sequential image data, and combining spatial and temporal positional encoding in a unified scheme improves representation learning. Evidence anchors: [section] "we propose the space-time positional encoding (STP) which is compatible with the three-dimensional token input" Break condition: If positional encoding does not significantly improve model performance or if the combined encoding scheme creates interference between spatial and temporal signals.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The entire method is built on transformer principles, using multi-head self-attention to capture spatial and temporal relationships in sequential images
  - Quick check question: What is the key difference between standard self-attention and multi-head spatial-temporal attention?

- Concept: Class imbalance and imbalanced learning techniques
  - Why needed here: The glaucoma forecasting dataset has severe class imbalance (19:1 negative to positive ratio), requiring specialized loss functions
  - Quick check question: How does temperature-controlled balanced softmax cross-entropy differ from standard cross-entropy in handling imbalanced data?

- Concept: Multi-scale feature extraction
  - Why needed here: Different disease progression patterns may be captured at different spatial resolutions, requiring features at multiple scales
  - Quick check question: Why might processing images at multiple scales improve disease progression modeling compared to single-scale processing?

## Architecture Onboarding

- Component map: Input -> Patch Embedding + Space-Time Positional Encoding -> Multi-Scale Encoder-Decoder Blocks -> Classification Head
- Critical path: Sequential image input -> patch embedding -> space-time positional encoding -> multi-head spatial-temporal attention -> scale transitions -> masked temporal attention -> classification head -> output prediction
- Design tradeoffs: Multi-scale architecture increases parameter count and computation but improves spatial feature capture; temperature-controlled loss requires hyperparameter tuning but handles imbalance better than standard methods; space-time positional encoding adds complexity but enables better sequence modeling
- Failure signatures: Poor performance on irregular sampling suggests time-aware attention issues; class imbalance problems indicate temperature scaling problems; spatial feature limitations suggest multi-scale architecture issues
- First 3 experiments:
  1. Compare single-scale vs multi-scale versions on SIGF dataset to validate spatial feature improvement
  2. Test different temperature values (τ = 1.0, 1.5, 2.0, 2.5) to find optimal class imbalance handling
  3. Evaluate models with and without space-time positional encoding to confirm its contribution to performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed method's performance scale when forecasting glaucoma probability at different future time points (e.g., 6 months vs 2 years)? The paper focuses on forecasting the next visit but doesn't explore multi-horizon forecasting or how prediction accuracy changes with temporal distance. Experiments comparing model performance across different prediction horizons (1, 3, 6, 12 months) would reveal how well the method maintains accuracy over time.

### Open Question 2
What is the minimum sequence length required for the MST-former to achieve optimal performance, and how does performance degrade with shorter sequences? The SIGF dataset has sequences ranging from 6 to 28 images, but the paper doesn't systematically analyze how sequence length affects performance or determine the optimal minimum length. Testing the model with sequences of varying lengths (e.g., 3, 5, 7, 10 images) and comparing performance metrics would establish the minimum effective sequence length.

### Open Question 3
How sensitive is the temperature-controlled Balanced Softmax Cross-entropy loss to the choice of τ, and is there an adaptive method to determine optimal τ per dataset? The paper shows that τ = 2.00 works well for their datasets but uses grid search, suggesting the need for a more systematic approach to hyperparameter selection. Developing and validating an adaptive method for selecting τ (e.g., based on class distribution statistics) across multiple datasets would demonstrate whether the temperature parameter can be automatically optimized.

## Limitations

- Weak corpus evidence for temperature-controlled Balanced Softmax Cross-entropy approach to class imbalance
- Space-time positional encoding design lacks direct comparison to alternative positional encoding schemes
- Multi-scale architecture benefits not rigorously analyzed to determine necessity of all three scales

## Confidence

- **High confidence**: Core transformer architecture design and general framework for glaucoma forecasting from sequential images
- **Medium confidence**: Specific implementation of space-time positional encoding and time-aware temporal attention mechanisms
- **Low confidence**: Superiority claims for temperature-controlled Balanced Softmax Cross-entropy loss compared to standard imbalanced learning techniques

## Next Checks

1. **Ablation study on temperature scaling**: Systematically evaluate MST-former performance with τ values ranging from 1.0 to 3.0 to determine the optimal temperature for class imbalance handling and assess whether the proposed τ=2.0 is truly optimal.

2. **Comparison with standard imbalanced learning methods**: Compare temperature-controlled Balanced Softmax Cross-entropy against established imbalanced learning techniques like focal loss, class-weighted cross-entropy, and oversampling methods on the same dataset to validate the claimed superiority.

3. **Multi-scale contribution analysis**: Evaluate single-scale and two-scale variants of MST-former to determine whether all three scales are necessary for achieving state-of-the-art performance, or if simpler architectures could provide similar results with fewer parameters.