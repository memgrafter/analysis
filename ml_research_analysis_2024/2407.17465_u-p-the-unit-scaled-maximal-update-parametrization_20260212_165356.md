---
ver: rpa2
title: "u-$\u03BC$P: The Unit-Scaled Maximal Update Parametrization"
arxiv_id: '2407.17465'
source_url: https://arxiv.org/abs/2407.17465
tags:
- scaling
- training
- scale
- learning
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces u-\u03BCP, a new parametrization scheme\
  \ that combines Maximal Update Parametrization (\u03BCP) with Unit Scaling to enable\
  \ low-precision training and simplify hyperparameter search. While \u03BCP improves\
  \ stability across model sizes, it lacks practical benefits like effective hyperparameter\
  \ transfer and easy low-precision training."
---

# u-$μ$P: The Unit-Scaled Maximal Update Parametrization

## Quick Facts
- arXiv ID: 2407.17465
- Source URL: https://arxiv.org/abs/2407.17465
- Reference count: 40
- Primary result: u-μP enables stable low-precision training and simpler hyperparameter search by combining Maximal Update Parametrization with Unit Scaling

## Executive Summary
u-μP introduces a new parametrization scheme that combines Maximal Update Parametrization (μP) with Unit Scaling to address practical limitations in large-scale training. While μP provides theoretical benefits for width-invariant hyperparameter transfer, it lacks practical advantages like effective low-precision training and simplified hyperparameter search. u-μP addresses these issues by ensuring unit variance across activations, weights, and gradients, which stabilizes numerical representations and enables straightforward FP8 training without dynamic rescaling.

The key contributions include a simplified hyperparameter set that eliminates dependencies between parameters, out-of-the-box FP8 training via static scaling, improved learning rate transfer across width, and more efficient independent hyperparameter search. Experiments show u-μP models achieve equal or lower validation loss compared to μP while maintaining stable training in FP8 and scaling effectively to billion-parameter models.

## Method Summary
u-μP combines μP parametrization with unit scaling to ensure numerical stability and simplified hyperparameter transfer. The approach enforces unit variance at initialization across all tensor types (activations, weights, gradients) through a unit scaling library that modifies standard operations. Key modifications include a new embedding learning rate scaling rule (1/√fan-out) that improves width-invariant transfer, and a redesigned hyperparameter set with reduced interdependencies that enables sequential single-dimension sweeps instead of full random search. The method supports mixed-precision training with static FP8 scaling without dynamic rescaling, and demonstrates effective transfer across width, depth, and training steps.

## Key Results
- u-μP achieves equal or lower validation loss compared to standard μP on WikiText-103
- Unit variance initialization enables stable FP8 training without dynamic rescaling
- Independent hyperparameter search reaches near-optimal loss with minimal tuning
- Improved learning rate transfer across model widths (256→4096) with minimal degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: u-μP combines maximal update parametrization with unit scaling to ensure stable low-precision training
- Mechanism: By enforcing unit variance at initialization across activations, weights, and gradients, u-μP keeps tensor values centered in the floating-point range, preventing overflow/underflow in FP8
- Core assumption: Unit variance at initialization is sufficient to maintain stable training dynamics throughout
- Evidence anchors:
  - [abstract] "Unit Scaling ensures that activations, weights and gradients begin training with a scale of one"
  - [section 2.3] "Unit Scaling ensures the unit variance of activations, weights and gradients at initialization"
  - [corpus] weak (no direct mention of unit variance stability over training)
- Break condition: If unit variance drifts significantly during training, numerical instability could occur

### Mechanism 2
- Claim: u-μP improves hyperparameter transferability compared to μP
- Mechanism: Fixing the embedding layer learning rate scaling rule (from constant to 1/√fan-out) prevents degradation as model width increases
- Core assumption: The modified embedding learning rate scaling rule maintains optimal dynamics across widths
- Evidence anchors:
  - [section 4.4] "a more effective rule is cemb = 1/√fan-out"
  - [section 4.4] "Using the best proxy HPs from (a), we train many models at different widths and LRs. The best LR for width 256 is ~optimal for 4096"
  - [corpus] weak (no independent verification of embedding scaling rule)
- Break condition: If other μP hyperparameters still exhibit poor transfer despite this fix

### Mechanism 3
- Claim: u-μP enables more efficient hyperparameter search through independent optimization
- Mechanism: The reduced interdependencies between hyperparameters (compared to μP) allow sequential, single-dimension sweeps rather than full random search
- Core assumption: Hyperparameters can be treated as approximately independent for practical search purposes
- Evidence anchors:
  - [section 4.5] "u-μP's HPs are designed to admit such a strategy due to our interdependence criterion"
  - [section 5.2] "u-μP has a transfer error of 0.005" vs μP's 0.03, indicating reduced dependency
  - [corpus] weak (no comparison to alternative search strategies)
- Break condition: If hyperparameters prove more interdependent in practice than the paper suggests

## Foundational Learning

- Concept: Maximal Update Parametrization (μP)
  - Why needed here: Provides the theoretical foundation for width-invariant hyperparameter transfer
  - Quick check question: What problem does μP solve in large model training that makes it worth combining with unit scaling?

- Concept: Unit Scaling
  - Why needed here: Ensures numerical stability by maintaining unit variance across all tensors at initialization
  - Quick check question: How does unit scaling differ from dynamic scaling methods like per-tensor rescaling?

- Concept: abc-parametrization
  - Why needed here: Explains the mathematical framework underlying both μP and u-μP, showing how scaling factors relate
  - Quick check question: In abc-parametrization, what does each of the three scaling factors (aW, bW, cW) control?

## Architecture Onboarding

- Component map: μP parametrization with depth-μP scaling rules -> Unit Scaling applied to all operations -> Redesigned hyperparameter set with reduced interdependencies
- Critical path: The most important elements are the embedding learning rate scaling rule fix and the unit variance initialization, as these directly impact both numerical stability and transfer performance
- Design tradeoffs: The choice to use 1/√fan-out for embedding learning rate scaling is empirically justified but lacks theoretical backing, representing a tradeoff between performance and theoretical purity
- Failure signatures: Poor transfer of optimal hyperparameters across model widths, numerical instability in low-precision training, or the need for extensive hyperparameter sweeps despite u-μP implementation
- First 3 experiments:
  1. Verify unit variance at initialization across all tensor types using the unit scaling library
  2. Test learning rate transfer across model widths (128→256→512→1024) using a proxy model approach
  3. Implement FP8 training with simple .to(float8) casts and measure validation loss degradation compared to FP32

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does u-μP provide similar benefits for architectures beyond transformers and language models?
- Basis in paper: [inferred] The paper focuses on LLM architectures but suggests the principles "should extend to other architectures"
- Why unresolved: Limited experimental scope to transformer-based models
- What evidence would resolve it: Experiments applying u-μP to CNNs, diffusion models, or graph neural networks with comparative results against standard training

### Open Question 2
- Question: What is the theoretical explanation for the improved embedding LR scaling rule (cemb = 1/√fan-out) proposed in Section 4.4?
- Basis in paper: [explicit] The authors state "We offer no theoretical justification for our rule"
- Why unresolved: Empirical observation without mathematical proof
- What evidence would resolve it: A formal proof showing why this scaling rule maintains optimal learning rates across model widths

### Open Question 3
- Question: What causes scale growth in attention output tensors during training, and can this be prevented?
- Basis in paper: [explicit] "We leave it to future work to offer a solution to scale growth created by correlation in intermediate tensors"
- Why unresolved: The paper identifies the phenomenon but doesn't provide a solution
- What evidence would resolve it: Either a mathematical model explaining the scale growth mechanism or an effective mitigation strategy that prevents it while maintaining training performance

### Open Question 4
- Question: What is the optimal learning rate decay schedule for u-μP training?
- Basis in paper: [explicit] The authors note that "recent research has indicated that this may not be the optimal decay target" for cosine decay to 10%
- Why unresolved: The paper uses standard cosine decay but acknowledges alternatives
- What evidence would resolve it: Systematic comparison of different decay schedules (linear, cosine, step) showing optimal transfer and final performance for u-μP models across different scales

### Open Question 5
- Question: How does u-μP perform when combined with other training stability techniques like QK-norm or zero initialization?
- Basis in paper: [explicit] The authors state "We do not adopt these features in our experiments to avoid confounding effects, but we expect them to benefit u-μP"
- Why unresolved: Experimental focus on u-μP in isolation
- What evidence would resolve it: Training experiments comparing u-μP alone versus u-μP with various stability techniques, measuring both numerical stability and final model performance

## Limitations
- Theoretical justification for the embedding learning rate scaling rule (1/√fan-out) remains empirical rather than analytically derived
- Claims about "out-of-the-box" FP8 training rely on static scaling which may not handle dramatic tensor distribution changes during training
- Reduced hyperparameter interdependencies may miss subtle interactions that could improve performance in specific regimes

## Confidence
**High Confidence**: The unit variance initialization mechanism and its role in enabling stable FP8 training is well-supported by the mathematical framework and experimental results. The claim that μP's embedding learning rate scaling rule (constant) is suboptimal for width-invariant transfer is demonstrated convincingly through controlled experiments.

**Medium Confidence**: The improved hyperparameter transfer across depth and training steps beyond width scaling is supported by experiments but relies heavily on the specific μP framework assumptions. The assertion that independent hyperparameter search reaches near-optimal loss faster than full random search is plausible but would benefit from direct comparisons to alternative search strategies.

**Low Confidence**: The claim that u-μP's hyperparameter set is "dramatically simpler" than μP's is somewhat subjective and depends on how one counts interdependent parameters. The assertion that the modified embedding learning rate rule is universally better than the standard μP rule lacks theoretical justification and may not hold for all model architectures.

## Next Checks
1. **Transferability Across Architectures**: Train u-μP models using the same hyperparameters on different architectures (CNNs, MLPs, different attention mechanisms) to verify that the unit variance initialization and simplified hyperparameter set generalize beyond transformers.

2. **Long-Training Stability Analysis**: Monitor tensor statistics throughout extended training runs (beyond the 15K steps tested) to verify that unit variance at initialization translates to sustained numerical stability in FP8, particularly during later stages when gradients may become sparse or large.

3. **Hyperparameter Interaction Mapping**: Conduct a controlled experiment comparing independent single-dimension sweeps against full factorial or random search on u-μP models to quantify the actual efficiency gains and determine if any critical hyperparameter interactions are being missed.