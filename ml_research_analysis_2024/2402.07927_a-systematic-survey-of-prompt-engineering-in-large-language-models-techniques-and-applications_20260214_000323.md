---
ver: rpa2
title: 'A Systematic Survey of Prompt Engineering in Large Language Models: Techniques
  and Applications'
arxiv_id: '2402.07927'
source_url: https://arxiv.org/abs/2402.07927
tags:
- prompting
- reasoning
- llms
- prompt
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically organizes 41 prompt engineering techniques
  for large language models, categorized by application areas including reasoning,
  hallucination reduction, user interaction, code generation, and optimization. It
  provides a structured overview of recent advancements, detailing prompting methodologies,
  models, datasets, and evaluation metrics.
---

# A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications

## Quick Facts
- arXiv ID: 2402.07927
- Source URL: https://arxiv.org/abs/2402.07927
- Authors: Pranab Sahoo; Ayush Kumar Singh; Sriparna Saha; Vinija Jain; Samrat Mondal; Aman Chadha
- Reference count: 7
- Key outcome: Systematic organization of 41 prompt engineering techniques categorized by application areas including reasoning, hallucination reduction, user interaction, code generation, and optimization

## Executive Summary
This survey systematically organizes 41 prompt engineering techniques for large language models, categorized by application areas to provide a structured overview of recent advancements. The work addresses the gap in systematic organization of diverse prompt engineering methods by detailing methodologies, models, datasets, and evaluation metrics for each technique. By presenting a taxonomy diagram and summary table, the survey illuminates open challenges and opportunities in this rapidly developing field, aiming to facilitate future research and provide insights into the latest developments in prompt engineering.

## Method Summary
The survey employs a systematic literature review approach to collect and categorize 41 distinct prompt engineering techniques based on their application areas. For each technique, the authors document the prompting methodology, applications, models involved, and datasets utilized. The study creates a taxonomy diagram and summary table to highlight datasets, models, and critical points of each technique, providing a comprehensive overview of the prompt engineering landscape.

## Key Results
- Systematic organization of 41 prompt engineering techniques across five main application areas
- Detailed documentation of prompting methodologies, applications, models, and datasets for each technique
- Creation of taxonomy diagram and summary table to illuminate open challenges and opportunities
- Identification of key evaluation metrics and performance benchmarks across different techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt engineering leverages the pre-trained knowledge of LLMs without modifying model parameters, enabling task-specific behavior elicitation through task instructions.
- Mechanism: By crafting specific instructions (prompts) that guide the model to produce desired outputs, users can adapt LLMs to various downstream tasks without retraining. The model utilizes its existing knowledge to interpret and respond to these instructions.
- Core assumption: The LLM's pre-trained knowledge is sufficient and relevant for the target task when properly guided by prompts.
- Evidence anchors:
  - [abstract]: "This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters."
  - [section]: "Prompt engineering has emerged as a crucial technique for enhancing the capabilities of pre-trained large language models (LLMs) and vision-language models (VLMs). It involves strategically designing task-specific instructions, referred to as prompts, to guide model output without altering parameters."
- Break condition: The pre-trained knowledge is insufficient or misaligned with the task requirements, or the prompt fails to provide adequate guidance.

### Mechanism 2
- Claim: Different prompt engineering techniques address specific challenges in LLM reasoning and performance, such as complex reasoning, hallucination reduction, and user interaction.
- Mechanism: The survey categorizes 41 techniques by application areas, each designed to tackle particular limitations of LLMs. For example, Chain-of-Thought (CoT) prompting improves reasoning by encouraging step-by-step problem-solving, while Retrieval Augmented Generation (RAG) reduces hallucination by incorporating external knowledge.
- Core assumption: The categorization accurately reflects the distinct challenges and solutions in prompt engineering, and each technique effectively addresses its targeted problem.
- Evidence anchors:
  - [abstract]: "This survey systematically organizes 41 prompt engineering techniques for large language models, categorized by application areas including reasoning, hallucination reduction, user interaction, code generation, and optimization."
  - [section]: "We have organized prompt engineering techniques according to their application areas and provided a concise overview of the evolution of prompting techniques, spanning from zero-shot prompting to the latest advancements."
- Break condition: The categorization is incomplete or inaccurate, or a technique fails to deliver the promised improvement in its target area.

### Mechanism 3
- Claim: The systematic organization of prompt engineering techniques in the survey facilitates future research by providing a structured overview of recent advancements, methodologies, models, datasets, and evaluation metrics.
- Mechanism: By presenting a comprehensive taxonomy and summary table, the survey illuminates open challenges and opportunities in prompt engineering, enabling researchers to identify gaps, compare approaches, and build upon existing work.
- Core assumption: The survey's organization and presentation of information effectively communicates the state of the field and guides future research directions.
- Evidence anchors:
  - [abstract]: "This systematic analysis enables a better understanding of this rapidly developing field and facilitates future research by illuminating open challenges and opportunities for prompt engineering."
  - [section]: "The analysis spans applications, models, and datasets, shedding light on the strengths and limitations of each approach. Furthermore, we have added a diagram and a table to highlight the important points."
- Break condition: The survey's organization is confusing or incomplete, or it fails to highlight critical insights or gaps in the field.

## Foundational Learning

- Concept: Understanding the distinction between zero-shot, few-shot, and fine-tuning approaches.
  - Why needed here: These are fundamental concepts in prompt engineering that determine how much task-specific data is required and how the model is adapted.
  - Quick check question: What is the key difference between zero-shot and few-shot prompting in terms of the data provided to the model?

- Concept: Familiarity with different types of reasoning tasks (e.g., mathematical, commonsense, symbolic) and their challenges.
  - Why needed here: Many prompt engineering techniques are designed to improve LLM performance on specific reasoning tasks, so understanding these tasks is crucial for evaluating the techniques.
  - Quick check question: Why might an LLM struggle with complex mathematical word problems compared to simple arithmetic?

- Concept: Knowledge of evaluation metrics commonly used in prompt engineering research (e.g., accuracy, BLEU, ROUGE, F1 score).
  - Why needed here: The survey presents results using various metrics, and understanding these metrics is essential for interpreting the effectiveness of different techniques.
  - Quick check question: What is the difference between accuracy and F1 score as evaluation metrics?

## Architecture Onboarding

- Component map:
  - Prompt -> LLM -> Response
  - Prompt -> External Knowledge (RAG) -> LLM -> Response
  - Prompt -> LLM -> Evaluation Metrics -> Performance Assessment

- Critical path:
  1. Define the task and desired output
  2. Select an appropriate prompt engineering technique based on the task type and challenges
  3. Craft the prompt according to the chosen technique's methodology
  4. Provide the prompt to the LLM and generate the response
  5. Evaluate the response using relevant metrics
  6. Iterate and refine the prompt or technique as needed

- Design tradeoffs:
  - Prompt complexity vs. computational efficiency: More complex prompts (e.g., CoT, ToT) may improve reasoning but increase computational cost
  - Task specificity vs. generalizability: Highly task-specific prompts may perform well on their target task but fail to generalize to other tasks
  - External knowledge integration vs. response latency: RAG improves factuality but introduces latency due to retrieval

- Failure signatures:
  - Poor performance despite appropriate prompt engineering: Indicates insufficient pre-trained knowledge or misalignment between the prompt and the task
  - Inconsistent results across different runs: Suggests sensitivity to prompt phrasing or randomness in the LLM's generation process
  - Excessive computational cost: Implies the need for optimization or a simpler prompt engineering technique

- First 3 experiments:
  1. Implement a basic zero-shot prompt for a simple question-answering task and evaluate the LLM's performance
  2. Apply Chain-of-Thought (CoT) prompting to a complex reasoning task (e.g., mathematical word problem) and compare the results with zero-shot prompting
  3. Integrate Retrieval Augmented Generation (RAG) into a fact-based question-answering task and assess the improvement in factual accuracy compared to standard prompting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Chain-of-Table prompting be scaled to handle more complex, nested table structures with multiple interrelated tables?
- Basis in paper: [explicit] The paper notes Chain-of-Table prompting's effectiveness on simpler tabular datasets but mentions the challenge of dealing with intricate table scenarios.
- Why unresolved: While the method shows promise for basic table reasoning, it remains unclear how it would perform when tables have complex relationships or nested structures that require sophisticated SQL/DataFrame operations.
- What evidence would resolve it: Empirical testing on benchmark datasets containing multi-table schemas, self-joins, and complex relational queries, along with quantitative comparisons to traditional approaches.

### Open Question 2
- Question: What are the most effective automated methods for generating contrasting demonstrations in Contrastive Chain-of-Thought (CCoT) prompting across diverse problem types?
- Basis in paper: [explicit] The paper identifies the challenge of automatically generating contrasting valid and invalid reasoning demonstrations for different problems as an open question.
- Why unresolved: Manual creation of contrasting examples is labor-intensive and may not generalize well. Automated generation risks introducing noise or irrelevant contrasts that could degrade performance.
- What evidence would resolve it: Comparative studies of different automated generation strategies (e.g., rule-based, model-generated, or adversarial approaches) showing improvements in reasoning accuracy across multiple benchmark tasks.

### Open Question 3
- Question: How can EmotionPrompt's emotional stimuli be optimally tuned for different task domains and cultural contexts to maximize performance gains?
- Basis in paper: [explicit] The paper demonstrates EmotionPrompt's effectiveness but notes uncertainty about its optimal application across diverse tasks and contexts.
- Why unresolved: The 11 emotional stimulus sentences were developed without task-specific or cultural customization, leaving questions about their universal applicability and potential for refinement.
- What evidence would resolve it: Systematic ablation studies testing different combinations of emotional stimuli across various cultural datasets and task types, measuring performance gains and identifying optimal configurations.

## Limitations
- The selection of exactly 41 techniques appears somewhat arbitrary with no clear criteria specified for inclusion or exclusion
- Different studies use different evaluation metrics and datasets, making cross-technique comparisons challenging
- The survey's systematic nature means it may not capture very recent developments published after the literature review cutoff

## Confidence
- High Confidence: The basic premise that prompt engineering leverages pre-trained LLM knowledge through task-specific instructions is well-established and supported by extensive literature
- Medium Confidence: The categorization of techniques by application areas is useful but may not capture all nuances of technique overlap and interdisciplinary approaches
- Low Confidence: Claims about the effectiveness of specific techniques relative to one another are difficult to verify without standardized benchmarks across all surveyed methods

## Next Checks
1. **Coverage Validation**: Compare the 41 techniques against other recent surveys to identify potential gaps or omissions in technique coverage
2. **Categorization Audit**: Map each technique to multiple categories to test the exclusivity and completeness of the current taxonomy structure
3. **Replication Study**: Select 3-5 representative techniques and implement them across standardized benchmark tasks to empirically validate the claimed benefits and assess relative performance