---
ver: rpa2
title: Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful
  Evaluator of Consistency?
arxiv_id: '2404.06503'
source_url: https://arxiv.org/abs/2404.06503
tags:
- note
- human
- shot
- clinical
- genmod
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares two model designs for generating clinical SOAP
  notes from doctor-patient conversations, using PEGASUS-X Transformer models. One
  approach generates note sections independently (SPECMOD), while the other generates
  them all together in a single step (GENMOD).
---

# Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?

## Quick Facts
- **arXiv ID**: 2404.06503
- **Source URL**: https://arxiv.org/abs/2404.06503
- **Reference count**: 40
- **Primary result**: GENMOD improves consistency metrics over SPECMOD while maintaining similar ROUGE scores

## Executive Summary
This paper presents a comparison of two model designs for generating clinical SOAP notes from doctor-patient conversations using PEGASUS-X Transformer models. The study evaluates whether generating note sections independently (SPECMOD) or together in a single step (GENMOD) yields better results. Both approaches achieved similar ROUGE scores and identical factuality scores, but GENMOD demonstrated significant improvements in age, gender, and body part consistency when evaluated by humans. The research also demonstrates that Llama2 LLM can effectively evaluate note consistency with high agreement to human reviewers for age and gender consistency, though performance was lower for body part consistency.

## Method Summary
The study compared two model architectures for clinical note generation from doctor-patient conversations. SPECMOD generates each section of the SOAP note independently, while GENMOD generates all sections together in a single step. Both models used PEGASUS-X Transformer architecture with the same training data and parameters. Human evaluation assessed 100 notes (50 per model) for factuality and consistency across age, gender, and body parts. Additionally, Llama2 LLM was evaluated as an automated consistency checker against human reviewers using Cohen's Kappa statistic.

## Key Results
- GENMOD achieved 4.5% higher age consistency, 2.5% higher gender consistency, and 8.5% higher body part consistency compared to SPECMOD
- Both models achieved identical factuality scores and less than 1% difference in ROUGE scores
- LLM evaluation showed high agreement with human reviewers (Cohen Kappa of 0.79 for age, 1.00 for gender, and 0.32 for body part consistency)

## Why This Works (Mechanism)
The GENMOD approach works better because generating all sections together allows the model to maintain better contextual coherence across the entire note. When sections are generated independently in SPECMOD, there's no mechanism to ensure consistency between sections, leading to potential contradictions. The single-step generation in GENMOD enables the model to reference and maintain consistency across all sections simultaneously, resulting in more coherent clinical notes.

## Foundational Learning
- **Clinical SOAP notes structure**: Understanding the standard format (Subjective, Objective, Assessment, Plan) is essential for evaluating note quality and consistency
- **Transformer architecture fundamentals**: Knowledge of self-attention mechanisms and how they enable contextual understanding across long sequences
- **ROUGE score calculation**: Understanding how this metric measures text similarity between generated and reference notes
- **Factuality evaluation in clinical contexts**: Knowing how to assess whether generated clinical information is accurate and consistent with source conversations
- **Cohen's Kappa statistic**: Understanding this metric for measuring inter-rater agreement between human and automated evaluators
- **Clinical consistency dimensions**: Recognizing the importance of maintaining consistent patient demographics and clinical details across note sections

## Architecture Onboarding

**Component Map:**
Patient conversation audio -> Speech-to-text transcription -> Model input -> SPECMOD/GENMOD generation -> SOAP note output -> Human/LLM evaluation

**Critical Path:**
The critical path for note generation is: Conversation → Transcription → Model generation → Output note. For evaluation, the path is: Generated note → Human/LLM evaluation → Consistency scoring.

**Design Tradeoffs:**
SPECMOD offers modularity and potentially faster inference per section but sacrifices cross-section consistency. GENMOD requires more computational resources per generation but maintains better overall coherence. The choice between them depends on whether consistency or efficiency is prioritized.

**Failure Signatures:**
SPECMOD typically fails with inconsistent patient demographics across sections or contradictory clinical information. GENMOD failures are less common but may include hallucinations when the model cannot reconcile conflicting information from the conversation.

**First Experiments:**
1. Generate 100 notes with SPECMOD and manually check for cross-section inconsistencies
2. Generate 100 notes with GENMOD and compare consistency metrics to SPECMOD
3. Run both models on conversations with clear demographic information to test basic functionality

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Small sample size for human evaluation (only 100 notes total) may not provide sufficient statistical power for clinical applications
- Identical factuality scores between models despite different generation approaches raises questions about evaluation methodology
- LLM showed only fair agreement (Cohen Kappa of 0.32) for body part consistency, suggesting limitations in automated evaluation

## Confidence

**Confidence Labels:**
- **High Confidence**: ROUGE score comparisons between SPECMOD and GENMOD
- **Medium Confidence**: Human evaluation of note consistency improvements
- **Medium Confidence**: LLM evaluation methodology for age and gender consistency

## Next Checks
1. Scale human evaluation to 1000+ notes per model to validate consistency improvement claims with greater statistical power
2. Conduct inter-annotator agreement studies among multiple human reviewers to establish ground truth consistency scores
3. Test model performance across multiple clinical specialties and conversation types to assess generalizability