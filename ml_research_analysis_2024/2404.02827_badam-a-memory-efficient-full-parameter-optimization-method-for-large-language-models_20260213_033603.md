---
ver: rpa2
title: 'BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language
  Models'
arxiv_id: '2404.02827'
source_url: https://arxiv.org/abs/2404.02827
tags:
- badam
- memory
- adam
- llama
- finetuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BAdam, a memory-efficient optimization method
  for full parameter fine-tuning of large language models based on the block coordinate
  descent framework combined with Adam's update rule. The method partitions model
  parameters into blocks and updates one block at a time using Adam steps, significantly
  reducing memory consumption from approximately 18M GB to 2M + 16M/D GB for models
  with M billion parameters.
---

# BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models

## Quick Facts
- arXiv ID: 2404.02827
- Source URL: https://arxiv.org/abs/2404.02827
- Authors: Qijun Luo; Hengxu Yu; Xiao Li
- Reference count: 40
- Primary result: BAdam achieves memory-efficient full parameter fine-tuning using 2M + 16M/D GB memory for M billion parameter models

## Executive Summary
This paper introduces BAdam, a memory-efficient optimization method for full parameter fine-tuning of large language models. The method combines block coordinate descent with Adam's update rule, partitioning model parameters into blocks and updating one block at a time while storing optimizer states only for the active block. This approach significantly reduces memory consumption from approximately 18M GB to 2M + 16M/D GB for models with M billion parameters. Theoretical analysis proves convergence in the deterministic case, showing BAdam finds δ-approximate stationary points within O(δ⁻²) iterations. Experiments demonstrate BAdam achieves comparable or superior performance to Adam while using only a single RTX3090-24GB GPU for Llama 3-8B and 4×A100-80GB GPUs for Llama 3-70B, outperforming memory-efficient baselines like LoRA in both optimization capability and downstream tasks including instruction following and mathematical reasoning.

## Method Summary
BAdam partitions model parameters into D blocks and updates one block at a time using K Adam steps, significantly reducing memory consumption by storing optimizer states only for the active block. The method maintains Adam's adaptive optimization properties within each block while leveraging the block coordinate descent framework for convergence guarantees. Memory usage is reduced from storing all optimizer states for all parameters to storing them for only one block at a time, while other blocks retain only their FP16 parameter values. The approach requires reconstructing the computational graph and backward pass for each block independently, enabled by gradient checkpointing techniques.

## Key Results
- Memory consumption reduced from ~18M GB to 2M + 16M/D GB for M billion parameter models
- Achieves comparable or superior performance to full-parameter Adam training
- Outperforms LoRA in both optimization capability and downstream tasks (MT-bench, math benchmarks)
- Successfully trains Llama 3-8B on single RTX3090-24GB and Llama 3-70B on 4×A100-80GB GPUs

## Why This Works (Mechanism)

### Mechanism 1
- Partitioning model parameters into blocks and updating one block at a time reduces memory consumption by limiting the number of parameters for which optimizer states are stored simultaneously.
- BAdam stores full optimizer states (gradient, momentum, second moment) only for the active block being updated, while other blocks retain only their FP16 parameter values.
- Core assumption: The computational graph and backward pass can be reconstructed for each block independently without losing gradient information.
- Evidence: "BAdam only needs in total 2M + 16M/D memory"

### Mechanism 2
- Using Adam's update rule within each block maintains optimization capability comparable to full-parameter Adam training.
- Within each block, BAdam applies K Adam steps to minimize the objective function using block-specific gradients, momentum, and adaptive learning rates.
- Core assumption: The block coordinate descent framework converges when each block update sufficiently reduces the objective function.
- Evidence: "Theoretical analysis proves convergence in the deterministic case, showing BAdam finds δ-approximate stationary points within O(δ⁻²) iterations"

### Mechanism 3
- The BCD framework with Adam updates achieves better downstream performance than LoRA despite requiring more memory.
- By updating all parameters (not just low-rank adapters), BAdam learns higher-rank perturbations that capture more complex relationships in the data.
- Core assumption: Higher-rank parameter updates provide more expressive power than low-rank adaptations for fine-tuning tasks.
- Evidence: "downstream performance evaluation... shows that BAdam outperforms existing memory efficient baselines such as LoRA"

## Foundational Learning

- Concept: Block Coordinate Descent (BCD)
  - Why needed here: BCD allows partitioning the optimization problem into smaller subproblems that can be solved sequentially, reducing memory requirements while maintaining convergence properties.
  - Quick check question: What is the main advantage of BCD over standard gradient descent when dealing with large parameter spaces?

- Concept: Adam Optimizer
  - Why needed here: Adam provides adaptive learning rates and momentum that help optimization converge faster and more reliably than SGD, which is crucial for effective fine-tuning.
  - Quick check question: How does Adam's adaptive learning rate differ from a fixed learning rate approach?

- Concept: Mixed Precision Training
  - Why needed here: Using FP16 for model parameters and FP32 for optimizer states balances memory efficiency with numerical stability during training.
  - Quick check question: Why is it beneficial to store model parameters in FP16 but maintain optimizer states in FP32?

## Architecture Onboarding

- Component map: Parameter partitioning -> Block selection -> Adam optimization -> Memory management -> Gradient checkpointing
- Critical path: 1. Partition parameters into blocks 2. For each block: run K Adam steps using block-specific gradients 3. Clear optimizer states for inactive blocks 4. Move to next block until all blocks updated (one block-epoch) 5. Repeat until convergence
- Design tradeoffs: Block size vs. memory efficiency (smaller blocks save more memory but increase computation overhead); Number of Adam steps K vs. convergence speed (more steps per block may improve local optimization but slow overall training); Block ordering strategy (random vs. sequential ordering may affect convergence behavior)
- Failure signatures: Memory overflow (blocks are too large or D is too small); Slow convergence (K is too small or block updates are insufficient); Poor downstream performance (blocks are too small to capture necessary parameter interactions)
- First 3 experiments: 1. Verify memory savings: Compare memory usage of BAdam vs. Adam on a small model with varying D values 2. Test convergence behavior: Run BAdam with different K values on a simple task and measure training loss 3. Validate block independence: Ensure that updating one block doesn't corrupt gradients for other blocks by comparing with full-parameter training on a small scale

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BAdam perform with stochastic gradients compared to deterministic gradients?
- Basis in paper: The authors state "We provide a convergence analysis for BAdam in the deterministic case, aiming to establish that combining the block coordinate descent framework with Adam's update rule results in a convergent scheme. We consider the extension to the stochastic case as future work."
- Why unresolved: The current theoretical analysis only covers deterministic gradients, and extending it to stochastic gradients is explicitly mentioned as future work.
- What evidence would resolve it: Empirical experiments showing convergence rates and performance with stochastic gradients, or theoretical analysis extending the convergence proof to the stochastic case.

### Open Question 2
- Question: What is the optimal value for the hyperparameter K in BAdam?
- Basis in paper: The authors mention "The number K in Line 7 of Algorithm 1 is the only additional hyperparameter introduced by BAdam... We provide a detailed discussion on selecting K in Appendix B.2."
- Why unresolved: While the paper provides some guidance on selecting K, it doesn't definitively determine the optimal value, stating that "the convergence speed of choosing different K can vary across different models."
- What evidence would resolve it: Systematic experiments across various models and tasks to determine the impact of different K values on convergence speed and final performance.

### Open Question 3
- Question: How does BAdam compare to other memory-efficient methods like LoRA in terms of downstream performance for different types of tasks?
- Basis in paper: The authors state "Experimentally, we apply BAdam to finetune the Llama 3-8B and Llama 3-70B models... The results confirm BAdam's efficiency... Furthermore, the downstream performance evaluation... shows that BAdam outperforms existing memory efficient baselines such as LoRA."
- Why unresolved: While the paper shows BAdam outperforming LoRA in specific tasks (MT-bench and math benchmarks), it doesn't provide a comprehensive comparison across all types of downstream tasks.
- What evidence would resolve it: Extensive experiments comparing BAdam and LoRA on a wide range of downstream tasks, including language understanding, generation, and specialized domains.

### Open Question 4
- Question: Can BAdam be effectively extended to other optimization methods beyond Adam?
- Basis in paper: The authors mention "Apart from the chosen Adam's update rule, it is possible to propose other efficient optimization procedures for concretely implementing (1); see Section 3.4 for an ablation study where we also employ SGD's update rule."
- Why unresolved: The paper only explores BAdam with Adam and SGD update rules, leaving open the question of how other optimization methods might perform within the BCD framework.
- What evidence would resolve it: Experiments applying BAdam with various optimization methods (e.g., RMSprop, Adagrad) and comparing their performance to the original BAdam.

## Limitations

- Theoretical convergence analysis only covers deterministic gradients, not the stochastic case used in practical LLM training
- Optimal block size and number of Adam steps per block (K) remain heuristic choices without systematic optimization
- Memory savings calculations assume specific hardware constraints and don't account for variations in GPU memory bandwidth

## Confidence

- High Confidence: Memory efficiency claims (2M + 16M/D GB formula) and baseline comparisons (LoRA performance)
- Medium Confidence: Convergence theory for deterministic case and downstream performance improvements
- Low Confidence: Generalizability across different LLM architectures and tasks

## Next Checks

1. **Convergence Analysis Across K Values**: Systematically vary the number of Adam steps per block (K) on a medium-scale model (e.g., Llama 3-8B) and measure both convergence speed and final performance to validate whether the theoretical assumption of sufficient block updates holds in practice.

2. **Cross-Architecture Generalization**: Apply BAdam to a different LLM architecture (e.g., GPT-Neo, OPT) and evaluate whether the memory efficiency formula and performance benefits generalize beyond Llama models.

3. **Task Diversity Evaluation**: Test BAdam on a broader range of downstream tasks beyond instruction following and mathematical reasoning, including text classification, summarization, and code generation to validate whether optimization benefits translate across different fine-tuning objectives.