---
ver: rpa2
title: 'ScoreFusion: Fusing Score-based Generative Models via Kullback-Leibler Barycenters'
arxiv_id: '2406.19619'
source_url: https://arxiv.org/abs/2406.19619
tags:
- barycenter
- auxiliary
- distribution
- score
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ScoreFusion, a theoretically grounded method
  for fusing multiple pre-trained diffusion models via Kullback-Leibler barycenters.
  The approach addresses the challenge of generating from target distributions with
  limited data by optimally combining auxiliary models.
---

# ScoreFusion: Fusing Score-based Generative Models via Kullback-Leibler Barycenters

## Quick Facts
- arXiv ID: 2406.19619
- Source URL: https://arxiv.org/abs/2406.19619
- Reference count: 40
- Key outcome: Introduces ScoreFusion, a method for fusing pre-trained diffusion models via KL barycenters to generate from target distributions with limited data, achieving superior sample efficiency and diversity.

## Executive Summary
ScoreFusion is a theoretically grounded method for fusing multiple pre-trained diffusion models via Kullback-Leibler barycenters. It addresses the challenge of generating from target distributions with limited data by optimally combining auxiliary models. The approach learns optimal barycenter weights by solving a score-matching problem in the diffusion model framework, enabling tractable computation. Theoretical analysis provides dimension-free sample complexity bounds in total variation distance. Empirical results on MNIST demonstrate superior sample efficiency and calibration compared to baselines. On a portrait generation task using Stable Diffusion, ScoreFusion produces more diverse and underrepresented representations compared to checkpoint merging.

## Method Summary
ScoreFusion fuses multiple pre-trained diffusion models by learning optimal KL barycenter weights through a score-matching optimization problem. Instead of directly optimizing KL divergence, it recasts the problem as minimizing a score-matching loss over a time interval in the denoising diffusion framework. This converts the non-parametric density estimation problem into a tractable parametric optimization over KL barycenter weights. The method assumes auxiliary models are pre-trained via score matching, and the optimal barycenter weight is a linear function of auxiliary scores, making the optimization convex and solvable via gradient descent.

## Key Results
- Demonstrated superior sample efficiency and calibration on MNIST digit classification with limited target data
- Produced more diverse and underrepresented representations in portrait generation compared to checkpoint merging
- Provided dimension-free sample complexity bounds in total variation distance

## Why This Works (Mechanism)

### Mechanism 1
ScoreFusion learns optimal weights for KL barycenters by recasting the learning problem as score matching in denoising diffusion, making the optimization tractable even in high dimensions. This converts the problem into a linear combination of auxiliary score functions, which is convex and solvable via gradient descent. The approach relies on auxiliary models being pre-trained via score matching (diffusion models) and assumes the optimal barycenter weight is a linear function of auxiliary scores.

### Mechanism 2
ScoreFusion achieves dimension-free sample complexity by converting a non-parametric density estimation problem into a parametric optimization over KL barycenter weights. The KL barycenter of auxiliary models provides a parametric family of distributions that can approximate the target. This avoids the curse of dimensionality inherent in non-parametric density estimation with limited data, assuming the auxiliary tasks combined capture the target distribution well.

### Mechanism 3
ScoreFusion can sample from low-probability regions by interpolating between auxiliary distributions, unlike checkpoint merging which produces bimodal mixtures. The KL barycenter distribution smoothly interpolates between auxiliary distributions, allowing sampling from underrepresented regions that neither auxiliary model alone would sample from. This assumes the KL barycenter weights can be optimized to prioritize underrepresented regions in the target distribution.

## Foundational Learning

- **Concept:** KL (Kullback-Leibler) divergence and barycenters
  - **Why needed here:** KL barycenter provides the parametric family of distributions that ScoreFusion optimizes over
  - **Quick check question:** What is the mathematical form of the KL barycenter of two distributions p1 and p2 with weights λ1 and λ2?

- **Concept:** Diffusion models and score matching
  - **Why needed here:** ScoreFusion relies on auxiliary models being trained via score matching in denoising diffusion
  - **Quick check question:** How does the score-matching loss in diffusion models relate to the optimization problem in ScoreFusion?

- **Concept:** Total variation distance and generalization bounds
  - **Why needed here:** The theoretical analysis provides dimension-free sample complexity bounds in total variation distance
  - **Quick check question:** What is the relationship between KL divergence and total variation distance, and why is this relevant for the generalization bounds in ScoreFusion?

## Architecture Onboarding

- **Component map:** Auxiliary models -> ScoreFusion module -> Sampling pipeline -> Evaluation
- **Critical path:** 1. Pre-train auxiliary diffusion models on auxiliary datasets. 2. Optimize KL barycenter weights using ScoreFusion on limited target data. 3. Generate samples from the KL barycenter distribution using the optimized weights. 4. Evaluate the generated samples for fidelity and diversity.
- **Design tradeoffs:** Parametric vs. non-parametric (trades expressiveness for tractability and sample efficiency); Score matching vs. direct KL optimization (trades computational complexity for easier optimization but requires auxiliary models to be trained via score matching)
- **Failure signatures:** Poor sample quality (inadequate auxiliary model training or poor representation of target); slow convergence (inappropriate learning rate or unsuitable score-matching loss); lack of diversity (KL barycenter weights not optimized for underrepresented regions)
- **First 3 experiments:** 1. MNIST digit classification with auxiliary models trained on subsets of EMNIST digits. 2. Portrait generation using pre-trained SDXL models fine-tuned on gender- and race-homogeneous datasets. 3. Synthetic Gaussian mixtures with bimodal target and unimodal auxiliary models.

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of KL barycenter weights impact the diversity of generated samples in high-dimensional spaces? The paper discusses using KL barycenter weights to sample from low-probability regions but doesn't extensively explore diversity impacts in high-dimensional spaces. This remains unresolved due to the paper's focus on lower-dimensional settings.

### Open Question 2
Can the ScoreFusion method be extended to non-Gaussian diffusion processes, and what would be the theoretical implications? The paper assumes Gaussian diffusion processes for simplicity but doesn't explore extending to non-Gaussian cases. This is unresolved because the theoretical analysis relies on Gaussian process properties, requiring new mathematical tools for non-Gaussian extensions.

### Open Question 3
How does the sample complexity of ScoreFusion scale with the number of auxiliary models? The paper provides sample complexity bounds but doesn't explicitly analyze how the number of auxiliary models affects these bounds. This is unresolved because the theoretical results focus on a fixed number of auxiliary models, with scaling effects not discussed.

## Limitations

- Theoretical guarantees are limited to cases where the KL barycenter family can represent the target distribution
- Empirical validation is primarily demonstrated on relatively simple datasets (MNIST) and a single portrait generation task
- Computational efficiency of solving the score-matching problem is not thoroughly analyzed

## Confidence

**High Confidence:** The mechanism of recasting KL barycenter optimization as score matching in diffusion models is theoretically sound and well-supported by diffusion model literature.

**Medium Confidence:** Claims of superior sample efficiency and diversity are supported by experimental results but limited in scope, requiring broader benchmarking.

**Low Confidence:** The assertion that ScoreFusion reliably samples from low-probability regions is based on a single case study, with conditions for effective interpolation not fully characterized.

## Next Checks

1. **Scalability Test:** Evaluate ScoreFusion on a larger-scale image generation task (e.g., LSUN bedrooms) to assess performance with higher-dimensional data and compare against checkpoint merging and other fusion methods.

2. **Theoretical Bound Verification:** Conduct rigorous analysis of conditions under which dimension-free sample complexity bounds hold, characterizing similarity requirements between auxiliary and target distributions and testing bounds empirically.

3. **Generalization to Diverse Tasks:** Apply ScoreFusion to non-image domains like time-series forecasting or molecular generation to evaluate versatility and assess whether benefits extend beyond visual data to other structured data types.