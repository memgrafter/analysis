---
ver: rpa2
title: Scalable Reinforcement Learning-based Neural Architecture Search
arxiv_id: '2410.01431'
source_url: https://arxiv.org/abs/2410.01431
tags:
- search
- agent
- architecture
- random
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a reinforcement learning (RL)-based approach
  to neural architecture search (NAS) that learns to incrementally improve architectures
  rather than find a single optimal one. The authors use an RL agent trained on the
  NAS-Bench-101 and NAS-Bench-301 benchmarks to search for better neural network architectures
  through a Markov Decision Process formulation.
---

# Scalable Reinforcement Learning-based Neural Architecture Search

## Quick Facts
- arXiv ID: 2410.01431
- Source URL: https://arxiv.org/abs/2410.01431
- Authors: Amber Cassimon; Siegfried Mercelis; Kevin Mets
- Reference count: 40
- Primary result: RL agent learns to incrementally improve neural architectures, outperforming random search and achieving competitive results with state-of-the-art NAS algorithms at low query budgets

## Executive Summary
This paper proposes a reinforcement learning (RL)-based approach to neural architecture search (NAS) that learns to incrementally improve architectures rather than find a single optimal one. The authors use an RL agent trained on NAS-Bench-101 and NAS-Bench-301 benchmarks to search for better neural network architectures through a Markov Decision Process formulation. Their transformer-based agent outperforms random search and achieves competitive results with state-of-the-art NAS algorithms, particularly at low query budgets. The method shows strong scalability with increasing search space size but limited robustness to hyperparameter changes. Training times were significant, with NAS-Bench-301 requiring approximately 21 days compared to 4 days for NAS-Bench-101.

## Method Summary
The approach formulates NAS as a Markov Decision Process where architectures are states and modifications are actions. A transformer-based RL agent learns to incrementally improve architectures by selecting from neighboring architectures generated through local modifications (vertex addition/removal, edge changes, operation changes). The agent uses a 256-dimensional (NAS-Bench-101) or 512-dimensional (NAS-Bench-301) latent space representation with positional encoding and dueling heads for action and state value estimation. Reward shaping through exponential functions addresses the reward distribution problem in NAS benchmarks where most architectures cluster around similar accuracy values.

## Key Results
- Transformer-based RL agent outperforms random search baseline and achieves competitive results with state-of-the-art NAS algorithms
- Strong scalability demonstrated with increasing search space size from NAS-Bench-101 to NAS-Bench-301
- Limited robustness to hyperparameter changes, particularly reward shaping parameter α
- Significant training times: ~4 days for NAS-Bench-101 vs ~21 days for NAS-Bench-301

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The incremental improvement formulation enables effective exploration in large NAS spaces by focusing search around promising regions rather than random sampling across the entire space.
- Mechanism: By treating each architecture as a state in a Markov Decision Process and allowing the agent to iteratively improve architectures through local modifications, the search concentrates computational resources on refining good architectures rather than exploring randomly.
- Core assumption: Local modifications to neural architectures (vertex addition/removal, edge changes, operation changes) can systematically improve performance when guided by learned policies.
- Evidence anchors:
  - [abstract]: "learns to incrementally improve architectures rather than find a single optimal one"
  - [section]: "We aim to achieve this by learning a searching behaviour, rather than trying to find an optimal architecture for any given problem"
  - [corpus]: Weak evidence - no direct corpus support for incremental improvement mechanism
- Break condition: If the local neighborhood structure contains many plateaus or local optima, the incremental approach may get stuck and fail to find globally optimal architectures.

### Mechanism 2
- Claim: The transformer-based agent architecture effectively captures architectural patterns and relationships through positional encoding and latent space representations.
- Mechanism: The agent encodes architectures into 256-dimensional (NAS-Bench-101) or 512-dimensional (NAS-Bench-301) latent spaces, applies positional encoding to maintain order awareness, and uses dueling heads to compute both action values and state values for stable learning.
- Core assumption: Neural network architectures can be meaningfully represented as flattened adjacency matrices concatenated with one-hot encoded vertex labels, preserving sufficient information for the agent to learn search strategies.
- Evidence anchors:
  - [section]: "The agent is presented with at most N different architectures. Before the architectures are presented to the agent, they must be prepared...flattened adjacency matrix is then concatenated to the vertex labels"
  - [section]: "The current architecture is prepended to all possible neighbours, similar to the 'classification token' in [5]"
  - [corpus]: Weak evidence - no direct corpus support for transformer-based NAS agent architecture
- Break condition: If the encoding scheme loses critical architectural information or if the latent space dimensionality is insufficient for the complexity of the search space.

### Mechanism 3
- Claim: Reward shaping through exponential functions effectively addresses the reward distribution problem in NAS benchmarks where most architectures cluster around similar accuracy values.
- Mechanism: By applying an exponential function e^(α·R) to validation accuracy before computing reward differences, the method creates a more differentiated reward signal that helps the agent distinguish between architectures with similar baseline accuracies.
- Core assumption: The exponential transformation of accuracy values creates a reward distribution that is sufficiently spread out for effective learning, even when original accuracies cluster in a narrow range.
- Evidence anchors:
  - [section]: "we apply an exponential function, e^(α·R), to the validation accuracy of the architectures, and use this exponential accuracy in the difference calculation"
  - [section]: "we use reward shaping with α = 6" (NAS-Bench-101) and "α = 32" (NAS-Bench-301)
  - [corpus]: Weak evidence - no direct corpus support for specific reward shaping approach
- Break condition: If the exponential parameter α is poorly chosen, it could either flatten the reward distribution (making learning difficult) or create extreme values that destabilize training.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and reinforcement learning fundamentals
  - Why needed here: The entire NAS approach is formulated as an MDP where architectures are states and modifications are actions, requiring understanding of state spaces, action spaces, transition functions, and reward structures
  - Quick check question: Can you explain the difference between the state space and action space in this incremental NAS formulation?

- Concept: Graph neural networks and graph representations
  - Why needed here: Neural architectures are represented as computational graphs, and the agent must process these graph structures through adjacency matrices and vertex label encodings
  - Quick check question: How would you represent a simple neural network architecture as an adjacency matrix with vertex labels?

- Concept: Neural Architecture Search (NAS) benchmarks and evaluation
  - Why needed here: Understanding NAS-Bench-101 and NAS-Bench-301 benchmarks is crucial for evaluating agent performance and interpreting results in terms of query budgets and accuracy improvements
  - Quick check question: What is the key difference between NAS-Bench-101 and NAS-Bench-301 in terms of their search space sizes and evaluation methods?

## Architecture Onboarding

- Component map: RL agent (transformer encoder with dueling heads) -> environment (architecture search space with neighbor generation) -> reward shaping module (exponential transformation) -> performance evaluation pipeline (benchmark queries and accuracy measurements)
- Critical path: Architecture encoding → Transformer processing → Action selection → Neighbor generation → Reward computation → Experience storage → Agent training update
- Design tradeoffs: Larger latent spaces (512 vs 256 dimensions) improve representation capacity but increase computational cost; higher neighbor counts (N=100 vs N=25) provide more exploration options but may overwhelm the agent's capacity; aggressive reward shaping (α=32) improves signal differentiation but risks instability.
- Failure signatures: Training plateaus indicate poor reward shaping or insufficient exploration; random performance suggests encoding issues or improper hyperparameter tuning; slow convergence points to suboptimal γ values or inadequate neighbor sampling.
- First 3 experiments:
  1. Verify architecture encoding by visualizing latent space representations of known architectures and checking if similar architectures cluster together
  2. Test neighbor generation by implementing a small search space and manually verifying that all valid one-step modifications are correctly identified
  3. Evaluate reward shaping by plotting original vs shaped reward distributions on a subset of NAS-Bench-101 architectures to confirm improved differentiation

## Open Questions the Paper Calls Out
None

## Limitations
- The incremental improvement formulation lacks direct corpus validation for its effectiveness in NAS contexts
- The exponential reward shaping parameters (α=6 for NAS-Bench-101, α=32 for NAS-Bench-301) were chosen without systematic justification or sensitivity analysis
- Computational cost analysis is incomplete, missing critical comparisons with alternative NAS methods and hardware specifications

## Confidence
- Incremental improvement mechanism: Medium confidence (weak corpus support, promising results)
- Transformer-based agent architecture: Medium confidence (theoretical soundness, limited empirical validation)
- Reward shaping approach: Medium confidence (practical utility, poorly understood parameter sensitivity)

## Next Checks
1. Conduct a systematic ablation study varying α from 1 to 50 to determine the optimal parameter range and identify stability boundaries for both benchmarks.
2. Implement a small-scale search space (e.g., 100-1000 architectures) and manually verify that all one-step modifications are correctly identified as neighbors, checking for completeness and avoiding duplicates.
3. Visualize the latent space representations using dimensionality reduction techniques (t-SNE/UMAP) on held-out architecture samples to verify that similar architectures cluster together and the encoding preserves meaningful structural information.