---
ver: rpa2
title: 'M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence
  Ability Benchmark'
arxiv_id: '2406.05343'
source_url: https://arxiv.org/abs/2406.05343
tags:
- question
- cognitive
- intelligence
- mllms
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces M3GIA, the first cognition-inspired multilingual
  and multimodal benchmark designed to evaluate the general intelligence ability of
  large language models. It categorizes cognitive abilities into five factors based
  on the CHC model and evaluates them across six languages (English, Chinese, French,
  Spanish, Portuguese, Korean).
---

# M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability Benchmark

## Quick Facts
- arXiv ID: 2406.05343
- Source URL: https://arxiv.org/abs/2406.05343
- Authors: Wei Song; Yadong Li; Jianhua Xu; Guowei Wu; Lingfeng Ming; Kexin Yi; Weihua Luo; Houyi Li; Yi Du; Fangda Guo; Kaicheng Yu
- Reference count: 40
- Primary result: First cognition-inspired multilingual multimodal benchmark showing MLLMs approaching but not exceeding human performance

## Executive Summary
This paper introduces M3GIA, the first cognition-inspired multilingual and multimodal benchmark designed to evaluate the general intelligence ability of large language models. The benchmark categorizes cognitive abilities into five factors based on the Cattell-Horn-Carroll (CHC) model and evaluates them across six languages (English, Chinese, French, Spanish, Portuguese, Korean). The study evaluates 24 multimodal large language models (MLLMs) on 1,800 multiple-choice questions across 18 narrow question types, revealing that even the most advanced models only marginally meet human performance levels in English, with significant gaps in other languages.

## Method Summary
M3GIA is constructed using the Cattell-Horn-Carroll (CHC) model to categorize cognitive abilities into five broad factors: Fluid Intelligence (Gf), Crystallized Intelligence (Gc), Short-term Memory (Gsm), Processing Speed (Gs), and Visual Processing (Gv). The benchmark includes 1,800 multiple-choice questions across 18 narrow question types in six languages. Evaluation involves zero-shot testing of 24 MLLMs using keyword-based option extraction followed by GPT-4-turbo fallback. General Intelligence Ability (GIA) scores are calculated using Confirmatory Factor Analysis (CFA) with a latent variable model. Human participant data collection validates the benchmark's construct validity through reliability tests and CFA model specification.

## Key Results
- MLLMs achieve only marginal performance compared to human participants in English, with significant gaps in other languages
- A "winner takes all" phenomenon emerges, suggesting the development of general intelligence ability in MLLMs
- Intelligence threshold identified between 13B and 32B parameters, with performance plateaus between 7B-13B parameters

## Why This Works (Mechanism)
The benchmark works by systematically mapping cognitive psychology constructs to language model evaluation tasks. By grounding the evaluation in established cognitive models (CHC framework), the study creates a theoretically sound approach to measuring general intelligence. The multilingual design with culturally relevant questions ensures the benchmark captures language-specific cognitive processing patterns rather than just English-centric abilities. The combination of multiple cognitive factors into a unified GIA score using CFA provides a holistic measure of intelligence rather than isolated skill assessment.

## Foundational Learning

**Cognitive Factor Categorization**: Understanding the CHC model's five broad cognitive factors (Gf, Gc, Gsm, Gs, Gv) and their 18 narrow abilities - needed to interpret benchmark results and understand what cognitive skills are being measured; quick check: verify the mapping between question types and cognitive factors in the dataset.

**Confirmatory Factor Analysis (CFA)**: Statistical method for testing whether data fit a hypothesized measurement model - needed to understand how GIA scores are derived from individual cognitive factor performances; quick check: verify KMO and Bartlett's test results on human data before applying to models.

**Multilingual Question Design**: Creating culturally and linguistically appropriate questions for different languages - needed to ensure the benchmark fairly evaluates cognitive abilities across languages without English-centric bias; quick check: examine question sources and cultural context annotations in the dataset.

**Zero-shot Evaluation Protocol**: Methodology for testing models without fine-tuning on benchmark data - needed to understand how model performance is measured and compared; quick check: verify option extraction success rates across different models and question types.

## Architecture Onboarding

**Component Map**: M3GIA Dataset (1,800 questions in 6 languages) -> Zero-shot Evaluation Pipeline (keyword extraction + GPT-4 fallback) -> Cognitive Factor Scoring (per-factor accuracy) -> GIA Score Calculation (CFA model) -> Performance Analysis (human vs model comparison)

**Critical Path**: The evaluation pipeline from dataset loading through option extraction to final GIA score calculation represents the critical path. Any failure in option extraction directly impacts all downstream analyses.

**Design Tradeoffs**: The use of multiple-choice questions provides standardized evaluation but may not capture the full spectrum of cognitive abilities. The six-language coverage balances diversity with practical data collection constraints. The CFA approach enables unified scoring but requires careful model specification.

**Failure Signatures**: Option extraction failures manifest as incomplete or incorrect answer parsing. CFA model specification errors appear as poor fit indices or unrealistic factor loadings. Cultural bias in questions shows up as systematic performance differences across language groups.

**First Experiments**: 
1. Test option extraction pipeline on a small subset of questions across all 24 models to verify parsing success rates
2. Run CFA on human participant data to validate factor structure before applying to models
3. Compare per-factor performance distributions between human participants and top-performing models

## Open Questions the Paper Calls Out

**Open Question 1**: What is the precise relationship between LLM size and cognitive ability performance, particularly the observed "threshold" between 13B and 32B parameters? The paper identifies this phenomenon but doesn't explain why performance plateaus between 7B-13B parameters and then improves significantly after 32B.

**Open Question 2**: How do cultural differences specifically impact the performance of MLLMs on cognition-inspired tasks across different languages? While cultural background is noted as a factor, the paper doesn't systematically analyze how specific cultural elements differentially affect model performance.

**Open Question 3**: What are the limitations of the current five-factor cognitive model in accurately capturing the full spectrum of MLLM cognitive abilities? The paper acknowledges the model is incomplete (missing auditory and olfactory processing) but doesn't explore which specific cognitive abilities are most critical for comprehensive evaluation.

## Limitations

- Option extraction methodology may introduce bias through keyword-based rules and GPT-4-turbo fallback
- Multiple-choice format may not fully capture the breadth of cognitive abilities required for general intelligence
- Six-language coverage may not be representative of global linguistic diversity
- Cultural biases in questions may affect performance across different language groups

## Confidence

High confidence: Benchmark construction methodology, cognitive factor categorization, and dataset size are well-documented and reproducible.

Medium confidence: Observation of marginal human-level performance in English with gaps in other languages is supported by data but may be influenced by question selection and cultural factors.

Low confidence: Conclusions about intelligence thresholds between 13B and 32B parameters and emergence of general intelligence require further validation with broader model ranges.

## Next Checks

1. Replicate the option extraction process across all 24 models using exact prompt engineering details to verify parsing success rates and identify potential biases.

2. Validate GIA score calculation by independently implementing the CFA model structure with provided factor loadings, comparing results against original implementation.

3. Conduct cross-cultural validation by testing benchmark with human participants from diverse linguistic and cultural backgrounds to assess cultural biases in questions.