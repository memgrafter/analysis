---
ver: rpa2
title: 'Chatting Up Attachment: Using LLMs to Predict Adult Bonds'
arxiv_id: '2409.00347'
source_url: https://arxiv.org/abs/2409.00347
tags:
- attachment
- synthetic
- data
- human
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using synthetic data from large language models
  (LLMs) to predict adult attachment styles, a challenging problem in mental health
  due to scarce and sensitive data. The authors created synthetic interview agents
  using GPT-4 and Claude 3 Opus with diverse profiles and childhood memories, then
  trained models to predict attachment styles from the resulting transcripts.
---

# Chatting Up Attachment: Using LLMs to Predict Adult Bonds

## Quick Facts
- arXiv ID: 2409.00347
- Source URL: https://arxiv.org/abs/2409.00347
- Reference count: 40
- Primary result: LLM-generated synthetic interview data achieves ROC AUC of 0.7-0.78 for predicting adult attachment styles, comparable to models trained on scarce human data.

## Executive Summary
This paper addresses the challenge of predicting adult attachment styles—a key indicator of mental health—by leveraging synthetic data generated through large language models. Traditional methods require scarce and sensitive human interview data, limiting research and clinical applications. The authors create synthetic interview agents with diverse profiles and childhood memories, then train models to predict attachment styles from these transcripts. They find that models trained on standardized synthetic data achieve performance comparable to those trained on human data, with ROC AUC scores around 0.7-0.78. The study demonstrates that synthetic data can effectively substitute for real human training data in this domain, potentially transforming how attachment research is conducted.

## Method Summary
The researchers generated synthetic interview agents using GPT-4 and Claude 3 Opus, each with unique profiles and 10 childhood memories. These agents participated in simulated Adult Attachment Interviews (AAIs) where a question list was used to elicit responses reflecting specific attachment styles (avoidant, preoccupied, secure). Childhood memories were retrieved using Retrieval-Augmented Generation (RAG) based on cosine similarity with the current question context. Responses were embedded using OpenAI's text-embedding-3-small model, then standardized by aligning their distribution with unlabeled human data. Logistic regression, extra trees, and MLP classifiers were trained on these embeddings to predict attachment styles, with performance evaluated against a small labeled human dataset.

## Key Results
- Models trained on standardized synthetic data achieved ROC AUC scores of 0.7-0.78, comparable to those trained on human data
- Standardization of synthetic embeddings with unlabeled human data improved predictive accuracy across all classifier types
- Synthetic embeddings initially formed distinct clusters compared to human data, but simple mean alignment bridged this domain gap effectively

## Why This Works (Mechanism)

### Mechanism 1
Synthetic interview agents generate coherent, attachment-relevant narratives when given structured user profiles and childhood memories. The system uses RAG to retrieve three childhood memories aligned to the current question context, injects them into the prompt alongside the agent's profile, and constrains generation with style-specific instructions. Core assumption: The LLM can coherently weave retrieved memories into interview answers that reflect the prescribed attachment style. Evidence: Synthetic transcripts show consistent style reflection, though direct comparison to human coherence is limited.

### Mechanism 2
Standardization of synthetic embeddings with unlabeled human data reduces the domain gap between synthetic and real human response distributions. Compute the mean embedding of unlabeled human transcripts (u) and the mean of synthetic embeddings (s), then shift all synthetic embeddings by (u - s). Core assumption: The shift aligns cluster centroids without distorting within-cluster structure, allowing downstream classifiers to generalize better. Evidence: Enhanced predictive accuracy after standardization, supported by qualitative analyses of embedding alignment.

### Mechanism 3
Synthetic data from agents with varied profiles and memories can substitute for real human training data in predicting attachment styles. Train classifiers on synthetic interview embeddings; the classifiers learn to separate attachment styles because synthetic agents embed the prescribed style consistently. Core assumption: Attachment style is a stable signal in interview responses, and the synthetic agents encode it reliably. Evidence: Comparable performance between models trained on synthetic vs. human data, with synthetic models often surpassing limited human-trained models.

## Foundational Learning

- Concept: Adult Attachment Interview (AAI) protocol and attachment style categories
  - Why needed: Understanding the interview structure and style definitions is essential to design realistic agents and interpret results
  - Quick check: What are the three attachment styles used in this study and how do they differ in self/other perception?

- Concept: Retrieval-Augmented Generation (RAG) and embedding-based retrieval
  - Why needed: The system relies on RAG to pull relevant childhood memories into agent responses; understanding this ensures proper debugging and tuning
  - Quick check: How does cosine similarity between memory embeddings and question context drive memory selection?

- Concept: Embedding standardization and domain adaptation
  - Why needed: The alignment step is critical to bridge synthetic and human embedding spaces; knowing the math prevents misuse
  - Quick check: Why does adding (u - s) to synthetic embeddings shift them toward human data without recomputing synthetic embeddings?

## Architecture Onboarding

- Component map: Profile generator → User profile JSON → Memory generator → 10 childhood memories per agent → Vector DB (HuggingFace MiniLM) → Store and retrieve memories → Agent "brain" → Context update + RAG query → Language module (GPT-4/Claude 3 Opus) → Generate interview response → Embedding module (OpenAI text-embedding-3-small) → Encode responses → Standardization module → Align synthetic embeddings with unlabeled human data → Classifier (LR/Extra Trees/MLP) → Predict attachment style

- Critical path:
  1. Generate profiles and memories
  2. Store memories in vector DB
  3. For each interview turn: update context → retrieve memories → update working memory → generate response
  4. Embed responses, average per interview, optionally standardize
  5. Train classifier, evaluate on labeled human data

- Design tradeoffs:
  - Memory count (3) vs. prompt length: fewer memories reduce prompt size but may lose context richness
  - Temperature (0.5) vs. hallucination: higher temperature increases variability but risks incoherence
  - Standardization vs. raw embeddings: standardization improves alignment but adds dependency on unlabeled data

- Failure signatures:
  - Synthetic embeddings form one large cluster with no separation by attachment style → memory retrieval or style instructions are failing
  - Standardization increases variance without improving accuracy → unlabeled data distribution is too different from labeled data
  - Performance plateaus early as synthetic data increases → synthetic agents are generating overly uniform responses

- First 3 experiments:
  1. Generate synthetic interviews for one attachment style only; visualize embeddings to confirm clustering
  2. Add memory retrieval; check that retrieved memories are topically relevant to the current question
  3. Apply standardization; measure cosine similarity change between synthetic and human embeddings before and after

## Open Questions the Paper Calls Out
- How do synthetic agents' responses differ from real human responses in terms of coherence and emotional content when discussing traumatic experiences?
- Does the predictive performance of synthetic data generalize across different LLM architectures beyond GPT-4 and Claude 3 Opus?
- How does the quality and diversity of synthetic data change when agents are given more complex or nuanced childhood memories?

## Limitations
- Small labeled human dataset (n=9) raises concerns about overfitting and limits statistical power
- Human data source is not publicly available, making independent validation difficult
- Results may not generalize to broader populations or clinical settings

## Confidence
- High: Synthetic data generation process and its technical implementation
- Medium: Classifier performance comparisons between synthetic and human data
- Low: Generalization of results to broader populations and clinical settings

## Next Checks
1. Replicate the study using multiple random splits of the labeled human data to assess model stability and prevent overfitting
2. Test the standardization approach with different embedding models to verify robustness across representations
3. Conduct ablation studies removing memory retrieval or style instructions to quantify their contribution to predictive performance