---
ver: rpa2
title: Are LLMs Robust for Spoken Dialogues?
arxiv_id: '2401.02297'
source_url: https://arxiv.org/abs/2401.02297
tags:
- dialogue
- dialogues
- spoken
- response
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the robustness of Large Pre-Trained Language
  Models (LLMs) for spoken task-oriented dialogues (TODs). Due to the lack of proper
  spoken dialogue datasets, the authors transcribed a development set of spoken dialogues
  and analyzed the Automatic Speech Recognition (ASR) error patterns.
---

# Are LLMs Robust for Spoken Dialogues?

## Quick Facts
- arXiv ID: 2401.02297
- Source URL: https://arxiv.org/abs/2401.02297
- Reference count: 40
- Large Pre-Trained Language Models are not inherently robust to spoken dialogue noise, but fine-tuning on simulated ASR-error data improves robustness.

## Executive Summary
This study investigates whether Large Pre-Trained Language Models (LLMs) can handle spoken task-oriented dialogues (TODs) despite Automatic Speech Recognition (ASR) errors. The authors transcribed a small spoken dialogue dataset to characterize ASR error patterns, then simulated these errors in a large written TOD dataset to create noisy training data. They fine-tuned GPT-2 and T5 models on both clean and noisy dialogues for response generation and dialogue state tracking tasks. Evaluation on two spoken test sets (Human-Verbatim and Human-Paraphrased) showed that LLMs are not inherently robust to spoken noise, but fine-tuning on noisy dialogues slightly improved performance, particularly in response appropriateness and contextualization as judged by human annotators.

## Method Summary
The authors first transcribed a small development set of spoken dialogues using Whisper to analyze ASR error patterns. They categorized errors into insertions, deletions, and substitutions, then simulated these errors in the MultiWOZ 2.1 dataset using token-level frequency statistics. GPT-2 and T5 models were fine-tuned on both clean and noisy versions of the data for response generation and dialogue state tracking tasks respectively. The models were evaluated on two spoken test sets (Human-Verbatim and Human-Paraphrased) using Joint Goal Accuracy (JGA) for DST, perplexity for response generation, and human evaluation for response quality.

## Key Results
- LLMs are not inherently robust to spoken dialogue noise, showing performance degradation on Human-Verbatim and Human-Paraphrased test sets.
- Fine-tuning on noisy dialogues slightly improves model robustness, particularly for response generation tasks in terms of appropriateness and contextualization.
- Targeted slot-value noise injection during DST fine-tuning can increase model robustness to spoken dialogue noise.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASR error simulation transfers real spoken noise patterns into synthetic training data.
- Mechanism: The authors transcribe a small set of spoken dialogues, align predictions to ground truth, classify errors into insertions/deletions/substitutions, then inject these patterns into a large written TOD dataset using token-level frequency statistics.
- Core assumption: Error distributions from a small corpus of spoken dialogues are representative of the broader domain and can be scaled.
- Evidence anchors:
  - [abstract] "We have characterized the ASR-error types and their distributions and simulated these errors in a large dataset of dialogues."
  - [section 3.3] "We studied and categorized the transcription errors and their distribution by automatically aligning the ASR predictions and the written ground truths. The observed error pattern was then injected into a large dataset of TODs."
  - [corpus] No explicit external dataset reference; weak.
- Break condition: If the small spoken dataset contains domain-specific noise patterns that don't generalize, injection will not reflect real ASR errors.

### Mechanism 2
- Claim: Fine-tuning LLMs on noisy dialogues improves robustness to spoken noise compared to fine-tuning on clean data.
- Mechanism: The model learns to recover or tolerate substitutions, insertions, and deletions by seeing corrupted inputs during training, improving generalization to ASR-error patterns at test time.
- Core assumption: Exposure to simulated noise during training enables the model to learn noise-resilient representations.
- Evidence anchors:
  - [abstract] "fine-tuning/training such models on a proper dataset of spoken TODs can result in a more robust performance."
  - [section 4.2.2] "fine-tuning on noisy data helps the model to produce more contextualized responses for both verbatim and paraphrased spoken settings."
  - [corpus] Weak external corroboration; only one similar study cited.
- Break condition: If noise simulation does not match actual ASR distribution, robustness gains will be limited or absent.

### Mechanism 3
- Claim: Replacing slot values with ASR-error patterns during DST fine-tuning increases robustness to spoken dialogue noise.
- Mechanism: By adding noise specifically to the slot values in training, the DST model learns to handle value corruption caused by ASR errors rather than just structural noise.
- Core assumption: Most ASR errors in spoken TODs affect slot values rather than dialogue act labels.
- Evidence anchors:
  - [section 4.1] "introducing noise on slot values specifically can result in an increase in the model robustness to the observed noise in the spoken dialogues."
  - [section 3.3] "For each token in the training data, the algorithm counts the number of occurrences of the token in the data and alternates the token with the corresponding ASR error with the same ratio and distribution calculated beforehand."
  - [corpus] Weak external validation.
- Break condition: If ASR errors are distributed evenly across tokens rather than concentrated in slot values, this targeted noise injection will be ineffective.

## Foundational Learning

- Concept: Automatic Speech Recognition error types and distributions
  - Why needed here: The core simulation relies on accurate characterization of ASR error patterns to inject realistic noise.
  - Quick check question: What are the three main ASR error categories used in the study and how are they quantified?

- Concept: Token-level frequency statistics in large dialogue datasets
  - Why needed here: These statistics drive the error injection algorithm, ensuring injected errors match observed real-world ratios.
  - Quick check question: How does the study compute the likelihood of a token being replaced by an ASR error?

- Concept: Joint Goal Accuracy (JGA) metric for DST
  - Why needed here: This metric determines how well the model recovers dialogue states in the presence of noise.
  - Quick check question: What does JGA measure in dialogue state tracking tasks?

## Architecture Onboarding

- Component map: ASR pipeline (Whisper tiny) -> Transcription -> NIST alignment -> Error categorization -> Error injector module -> Token frequency table + error distribution -> Noise-augmented MultiWOZ -> Fine-tuning pipeline (T5/DST or GPT-2/response gen) -> Clean vs. noisy training data -> Evaluation pipeline (Human-Verbatim/Human-Paraphrased test sets + human evaluation)
- Critical path: ASR transcription -> Error categorization -> Injection into training data -> Fine-tuning -> Evaluation on spoken test sets
- Design tradeoffs:
  - Injection granularity: Token-level vs. sentence-level noise affects realism and training stability
  - Error distribution scaling: Small dev set errors may not represent full domain noise
  - Training data size: Noisy data doubles dataset size, increasing training time
- Failure signatures:
  - Low JGA on HV/HP despite noise training indicates injection mismatch or DST architecture limitations
  - No improvement in perplexity or human scores indicates noise simulation is ineffective
  - High similarity to ground truth despite noise training suggests model ignores injected errors
- First 3 experiments:
  1. Inject errors only at the token frequency level without domain-specific slot handling; evaluate DST JGA and response perplexity.
  2. Add slot-value-specific noise injection; compare JGA and human evaluation scores to step 1.
  3. Compare model robustness to paraphrased vs. verbatim test sets after noise fine-tuning; measure differences in error types detected by humans.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the type of speech disfluencies (pauses, repetitions, rephrasings) impact the performance of LLMs in spoken dialogues compared to ASR errors?
- Basis in paper: [inferred] The paper mentions that spoken dialogues differ from written dialogues in terms of speech disfluencies but does not explicitly study their impact separately from ASR errors.
- Why unresolved: The study focuses on ASR errors and their impact, but does not isolate or analyze the effects of speech disfluencies on LLM performance.
- What evidence would resolve it: A controlled study comparing LLM performance on spoken dialogues with and without injected speech disfluencies, separate from ASR errors.

### Open Question 2
- Question: Are there specific dialogue domains or topics where LLMs show more robustness to spoken noise than others?
- Basis in paper: [inferred] The paper uses the MultiWOZ dataset spanning multiple domains but does not analyze domain-specific robustness differences.
- Why unresolved: The study evaluates overall performance across domains without examining potential variations in robustness between different dialogue topics or domains.
- What evidence would resolve it: A comparative analysis of LLM performance on spoken dialogues across different domains within the MultiWOZ dataset or similar multi-domain datasets.

### Open Question 3
- Question: How does the size of the LLM (e.g., GPT-2 Small vs. Medium) affect its robustness to spoken noise in task-oriented dialogues?
- Basis in paper: [explicit] The paper compares GPT-2 Small and Medium models but only in the context of overall performance, not specifically regarding robustness to spoken noise.
- Why unresolved: While the paper shows that GPT-2 Medium outperforms GPT-2 Small in general, it does not analyze how model size impacts robustness to spoken dialogue noise.
- What evidence would resolve it: A systematic evaluation of multiple LLM sizes (e.g., Small, Medium, Large) on spoken dialogue tasks, comparing their robustness to noise in both DST and response generation.

## Limitations
- The study relies on a small-scale spoken dialogue dataset (42 dialogues) to characterize ASR error patterns, limiting generalizability.
- The error simulation methodology assumes token-level frequency statistics can effectively model ASR error distributions without empirical validation.
- The evaluation relies heavily on subjective human judgments for response quality, introducing potential inconsistency.

## Confidence

**High Confidence**: The finding that LLMs are not inherently robust to spoken noise is well-supported by the consistent performance drop across both HV and HP test sets. The human evaluation results showing improved appropriateness and contextualization after noise fine-tuning are also robust, as they are based on multiple annotator scores.

**Medium Confidence**: The claim that targeted slot-value noise injection improves DST robustness is supported by the results but has limited external validation. The simulation methodology's effectiveness in transferring real ASR error patterns to synthetic training data is plausible but not definitively proven.

**Low Confidence**: The assertion that fine-tuning on noisy data is the optimal approach for achieving spoken dialogue robustness is premature, given the small scale of the spoken dataset used for error characterization and the lack of comparison to alternative approaches like joint speech-text training.

## Next Checks
1. Conduct ASR transcription on a larger, more diverse spoken dialogue corpus (minimum 200 dialogues) and compare the observed error distributions to those used in the simulation. Calculate KL divergence between real and simulated error distributions to quantify similarity.
2. Apply the fine-tuned noisy models to spoken dialogues from a different domain (e.g., restaurant vs. hotel) and measure performance degradation. This will test whether the learned robustness generalizes beyond the original training domain.
3. Replicate the human evaluation protocol with a new set of annotators and a larger sample size (minimum 50 responses per condition). Calculate inter-annotator agreement scores to establish reliability of the quality assessments.