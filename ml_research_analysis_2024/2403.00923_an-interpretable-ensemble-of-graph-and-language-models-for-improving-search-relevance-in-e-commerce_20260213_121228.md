---
ver: rpa2
title: An Interpretable Ensemble of Graph and Language Models for Improving Search
  Relevance in E-Commerce
arxiv_id: '2403.00923'
source_url: https://arxiv.org/abs/2403.00923
tags:
- graph
- language
- search
- ensemble
- pp-glam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose PP-GLAM, a modular ensemble of language models
  and graph neural networks to improve search relevance in e-commerce. The model uses
  a plug-and-play framework with uniform data processing and additive explanation
  metrics to automatically decide on the inclusion of language models, GNN models,
  and inter-product behavioral signals.
---

# An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce

## Quick Facts
- **arXiv ID**: 2403.00923
- **Source URL**: https://arxiv.org/abs/2403.00923
- **Reference count**: 40
- **Primary result**: PP-GLAM achieves 4%-11% improvement over standalone language models and 20%-75% improvement over standalone GNNs on multilingual e-commerce search relevance tasks.

## Executive Summary
PP-GLAM is a modular ensemble framework that combines language models and graph neural networks to improve search relevance in e-commerce. The system uses additive explanation metrics (SHAP values) to automatically decide whether to include language model candidates, GNN candidates, and inter-product behavioral signals. The framework outperforms state-of-the-art baselines and a proprietary model on real-world multilingual, multi-regional e-commerce datasets for search relevance and irrelevant detection tasks.

## Method Summary
PP-GLAM uses a modular ensemble approach with uniform data processing pipelines that employ additive explanation metrics to independently decide whether to include language model candidates, GNN model candidates, and inter-product behavioral signals. The framework uses a Gradient-Boosted Decision Tree (GBDT) for effective model selection based on SHAP values. The system processes query-product pairs through data processing (graph construction and product denoising), model training (GBDT-based ensemble), model selection (SHAP-based), and model inference (parallel batch processing) modules.

## Key Results
- PP-GLAM improves performance of standalone LMs by 4%-11% and standalone GNNs by 20%-75% on search relevance tasks
- Achieves higher accuracy, macro-F1, and weighted F1 scores compared to proprietary baselines on multilingual datasets
- SHAP analysis demonstrates interpretable feature importance with additivity enabling efficient model selection without retraining

## Why This Works (Mechanism)

### Mechanism 1
The modular ensemble leverages both semantic and behavioral signals for improved performance. PP-GLAM uses language models to capture semantic information from queries and products while GNNs aggregate behavioral signals like clicks and purchases. The GBDT model selects the most impactful models based on SHAP values.

### Mechanism 2
SHAP values enable interpretable and efficient deployment. The additive explanations for each feature's contribution allow automatic selection of the most impactful models based on computational constraints without retraining the entire ensemble.

### Mechanism 3
Plug-and-play modularity allows easy integration of new models. The framework's modular design enables addition or removal of individual language models and GNNs without requiring full ensemble retraining.

## Foundational Learning

- **Language Models (LMs)**: Capture semantic information from queries and products; why needed: essential for understanding search relevance; quick check: What is the primary function of language models in PP-GLAM?
- **Graph Neural Networks (GNNs)**: Aggregate behavioral signals like clicks and purchases; why needed: understand user intent and product relationships; quick check: How do graph neural networks contribute to the search relevance task in PP-GLAM?
- **SHAP (SHapley Additive exPlanations)**: Provide interpretable explanations for feature contributions; why needed: enable model selection and understanding; quick check: What is the role of SHAP values in PP-GLAM's model selection process?

## Architecture Onboarding

- **Component map**: Data Processing -> Model Training -> Model Selection -> Model Inference
- **Critical path**: Query-product pair → Data Processing (graph construction, denoising) → Model Training (GBDT ensemble) → Model Selection (SHAP-based) → Model Inference (parallel batch processing)
- **Design tradeoffs**: Trades model complexity for interpretability and flexibility; ensemble approach allows integration of multiple models but increases computational requirements
- **Failure signatures**: Insufficient data for training, incompatible new models, suboptimal selection due to inaccurate SHAP values
- **First 3 experiments**:
  1. Evaluate individual LMs and GNNs performance on search relevance task
  2. Assess impact of different behavioral signals on overall performance
  3. Investigate trade-off between model complexity and performance by varying ensemble size

## Open Questions the Paper Calls Out

### Open Question 1
How does performance vary when incorporating new, more advanced language models and GNNs? The paper states PP-GLAM can incorporate new models without retraining but provides no empirical evidence on performance changes with newer models.

### Open Question 2
What is the impact of the monthly 20%-25% update rate of query-product pairs on performance and efficiency? The paper mentions this update rate but doesn't analyze its long-term effects on model performance.

### Open Question 3
How does the lack of availability of certain graphs for query-product pairs affect overall performance? The paper acknowledges this issue but doesn't provide quantitative analysis on its extent or impact.

## Limitations

- Limited empirical validation of SHAP-based model selection efficiency claims
- Heavy reliance on behavioral signals may limit generalizability to datasets with sparse behavioral data
- Insufficient analysis of actual inference time breakdown and hardware-specific optimization

## Confidence

- **High confidence**: Core ensemble architecture and performance improvements over baselines are well-demonstrated
- **Medium confidence**: SHAP-based interpretability claims are theoretically justified but lack comprehensive empirical validation
- **Low confidence**: Plug-and-play modularity assumes seamless integration without retraining but provides minimal evidence about practical challenges

## Next Checks

1. Conduct ablation studies on behavioral signals to quantify model sensitivity and identify break conditions
2. Compare SHAP-based model selection decisions against ground truth importance rankings through controlled experiments
3. Test PP-GLAM on cross-dataset generalization to evaluate performance on different e-commerce domains and product categories