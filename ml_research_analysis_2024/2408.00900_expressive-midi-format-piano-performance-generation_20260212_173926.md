---
ver: rpa2
title: Expressive MIDI-format Piano Performance Generation
arxiv_id: '2408.00900'
source_url: https://arxiv.org/abs/2408.00900
tags:
- music
- symbolic
- generation
- expressive
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a generative neural network for expressive
  piano performance in MIDI format. The model addresses the challenge of generating
  expressive music by incorporating micro-timing, polyphonic texture, dynamics, and
  sustain pedal effects.
---

# Expressive MIDI-format Piano Performance Generation

## Quick Facts
- arXiv ID: 2408.00900
- Source URL: https://arxiv.org/abs/2408.00900
- Reference count: 1
- One-line primary result: Presents a generative neural network for expressive piano performance in MIDI format that incorporates micro-timing, polyphonic texture, dynamics, and sustain pedal effects.

## Executive Summary
This paper presents a generative neural network for expressive piano performance in MIDI format. The model addresses the challenge of generating expressive music by incorporating micro-timing, polyphonic texture, dynamics, and sustain pedal effects. Key innovations include abandoning fixed grids in favor of millisecond-based durations, reinterpreting monophony and polyphony through sequential note onsets, considering sustain pedal as a crucial control parameter, and applying perceptual-based quantization of auditory features. The model uses a convolved multi-argument LSTM with attention mechanisms to generate correlated outputs. While the model shows promise in generating expressive piano pieces, it is not fully trained due to time constraints, resulting in some incoherent or random sections in the preliminary results.

## Method Summary
The method uses a convolved multi-argument LSTM with attention mechanisms to generate expressive piano performances in MIDI format. The model processes 5 input features (note value, duration in milliseconds, velocity, time shift between consecutive notes, and pedal status) through perceptual-based quantization following Weber's law. The LSTM generates correlated outputs using attention sub-modules that take previously generated terms into account, producing sequences of musical events that capture expressive timing, dynamics, and pedal usage. The system converts polyphonic music into a sequential problem by ordering note onsets by their timing, enabling LSTM-based generation.

## Key Results
- The model successfully generates expressive piano pieces with micro-timing variations and dynamic expression
- Sequential modeling of polyphonic music enables LSTM-based generation of complex textures
- Perceptual-based quantization following Weber's law creates more natural-sounding output distributions
- Preliminary results show promise but contain incoherent sections due to insufficient training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential modeling of polyphonic music by treating note onsets as sequential events can generate realistic polyphonic textures.
- Mechanism: By ordering polyphonic notes by their onset times rather than treating them as simultaneous events, the model converts polyphonic music into a sequential problem solvable by LSTM networks.
- Core assumption: There are no truly simultaneous note onsets in human performance; all notes have measurable time differences.
- Evidence anchors:
  - [abstract] "reinterpreting monophony and polyphony through sequential note onsets"
  - [section] "We claim that there is no real simultaneity of notes...since there are no simultaneous events, we can always place the notes sequentially, by their time onsets"
  - [corpus] Weak evidence - corpus neighbors focus on piano performance generation but don't specifically validate the sequential onset approach
- Break condition: If actual piano performances contain notes with identical onset times (within millisecond precision), the sequential model would lose this simultaneity information.

### Mechanism 2
- Claim: Incorporating sustain pedal as a control parameter improves the expressiveness of generated MIDI music.
- Mechanism: The sustain pedal information is treated as an additional input/output feature alongside note events, allowing the model to learn pedal-pedal relationships and pedal-note correlations.
- Core assumption: The sustain pedal is as musically important as note events and provides valuable grouping information for notes.
- Evidence anchors:
  - [abstract] "considering sustain pedal as a crucial control parameter"
  - [section] "In piano performances, there is one control parameter that plays a role as important as the notes, which is the sustain pedal"
  - [corpus] Weak evidence - corpus neighbors mention expressive performance but don't specifically address sustain pedal modeling
- Break condition: If sustain pedal usage is too irregular or doesn't follow consistent patterns across performances, the model may not learn meaningful correlations.

### Mechanism 3
- Claim: Perceptual-based quantization of auditory features (using Weber's law) improves the distribution and perceptual relevance of the model's outputs.
- Mechanism: Features like duration and velocity are quantized into uneven categories based on just noticeable differences, rather than equal divisions, to better reflect human perception.
- Core assumption: Human perception of auditory features follows Weber's law, where noticeable differences are proportional to current values.
- Evidence anchors:
  - [abstract] "applying perceptual-based quantization of auditory features"
  - [section] "Instead of equal division, like the Mel spectrogram, we divide the ranges into uneven chunks to better reflect the perceptual truth. We refer to Weber's law for just noticeable differences"
  - [corpus] No direct evidence - corpus neighbors don't discuss perceptual quantization approaches
- Break condition: If Weber's law doesn't accurately model the specific perceptual dimensions being quantized, or if the quantization boundaries don't match human perception thresholds.

## Foundational Learning

- Concept: LSTM networks and their ability to capture long-term dependencies
  - Why needed here: The model uses LSTM to generate music sequences where notes have temporal relationships that span across the piece
  - Quick check question: What problem does LSTM solve that standard RNNs cannot, particularly relevant for music generation?

- Concept: Attention mechanisms in neural networks
  - Why needed here: The model uses attention sub-modules to generate correlated outputs (note value, duration, velocity, time shift, pedal) recurrently
  - Quick check question: How does attention help generate correlated outputs rather than treating each output independently?

- Concept: Weber's law and just noticeable differences
  - Why needed here: The model uses Weber's law to inform the perceptual-based quantization of auditory features like duration and velocity
  - Quick check question: What is the mathematical relationship described by Weber's law, and how does it apply to designing perceptual quantization boundaries?

## Architecture Onboarding

- Component map: Input layer (5 features) -> Convolved multi-argument LSTM with attention mechanisms -> Output layer (5 features) -> Perceptual-based quantization layer

- Critical path: 1. Input feature preprocessing and quantization 2. Sequential LSTM processing with attention-based output generation 3. Output quantization and decoding to MIDI format

- Design tradeoffs:
  - Sequential modeling of polyphonic music sacrifices true simultaneity but enables LSTM-based generation
  - Perceptual quantization may lose some precision but improves perceptual relevance
  - Attention mechanisms increase complexity but enable correlated output generation

- Failure signatures:
  - Mechanical, non-expressive output suggests problems with input feature representation or quantization
  - Incoherent note sequences suggest LSTM attention mechanism not learning proper correlations
  - Random sustain pedal patterns suggest insufficient training data or model capacity issues

- First 3 experiments:
  1. Test sequential polyphonic generation on monophonic input to verify basic LSTM functionality
  2. Compare perceptual quantization vs equal quantization on a validation set to measure perceptual improvement
  3. Test correlated output generation by generating note values and durations separately vs with attention to verify the correlation modeling works

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can symbolic music generation systems handle the increasing diversity and hybridization of modern musical instruments that defy traditional categorization?
- Basis in paper: [explicit] "Non-categorizable instruments. As physical sound sources are more and more difficult to identify in the era of computer-based music production, the categorization of instruments become infeasible."
- Why unresolved: The paper acknowledges this limitation but doesn't propose concrete solutions for handling non-standard or hybrid instruments in symbolic generation.
- What evidence would resolve it: Research demonstrating new encoding schemes or generation approaches that can represent and generate music with novel, hybrid, or uncategorizable instruments.

### Open Question 2
- Question: What methods can be developed to handle the growing number of control parameters and effects in modern music production within symbolic generation frameworks?
- Basis in paper: [explicit] "Undefinable control events. In addition to the note events, MIDI also records the controls the performer imposed on the instruments. As the controls get multifarious, it's impossible to classify them into 128 categories."
- Why unresolved: The paper identifies the problem of expanding control events but doesn't offer solutions for encoding or generating these increasingly complex parameters.
- What evidence would resolve it: Development of new data structures or generation architectures that can effectively represent and generate complex control parameter sequences beyond the limitations of current MIDI specifications.

### Open Question 3
- Question: How can symbolic music generation models handle compositions with variable or indeterminate instrumentation while maintaining musical coherence?
- Basis in paper: [explicit] "Inconsistent styles and use of instruments. To generate symbolic music data, we need to specify how many instruments we are using and what are they... This freedom of instrument genres leaves the symbolic generative model in the dilemma of having to actively determine instruments before generating any notes."
- Why unresolved: The paper describes the challenge of variable instrumentation but doesn't propose mechanisms for models to dynamically determine appropriate instrumentation during generation.
- What evidence would resolve it: Generation systems that can successfully produce coherent music without predetermined instrumentation, automatically determining appropriate instruments or sound sources based on musical context.

## Limitations

- The model was only trained for two epochs before submission, resulting in incoherent or random sections in preliminary results
- Key architectural details (number of layers, hidden units, attention mechanism implementation) are not fully specified
- No comparative evaluation against baseline methods or human performances is provided

## Confidence

- **High confidence**: The theoretical framework and architectural design principles (sequential modeling of polyphonic music, perceptual quantization using Weber's law, attention mechanisms for correlated output generation) are well-founded and represent legitimate innovations in symbolic music generation
- **Medium confidence**: The approach of treating sustain pedal as a crucial control parameter and the reinterpretation of polyphonic music as sequential note onsets are conceptually sound but lack empirical validation in the paper
- **Low confidence**: The actual performance and expressiveness of the generated music cannot be properly evaluated given the insufficient training and lack of comparative results against baseline methods or human performances

## Next Checks

1. Complete full training of the model for sufficient epochs and conduct comprehensive evaluation using both objective metrics (MIDI-based measures of expressiveness) and subjective listening tests comparing generated music to human performances

2. Conduct ablation studies to isolate the contribution of each architectural innovation (sequential polyphonic modeling, perceptual quantization, sustain pedal modeling, attention mechanisms) by removing them individually and measuring impact on output quality

3. Implement a baseline system using traditional fixed-grid approaches and directly compare the expressiveness, naturalness, and perceptual quality of outputs against the proposed millisecond-based system