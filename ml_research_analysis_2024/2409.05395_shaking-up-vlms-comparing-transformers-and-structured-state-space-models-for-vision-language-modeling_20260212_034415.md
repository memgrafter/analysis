---
ver: rpa2
title: 'Shaking Up VLMs: Comparing Transformers and Structured State Space Models
  for Vision & Language Modeling'
arxiv_id: '2409.05395'
source_url: https://arxiv.org/abs/2409.05395
tags:
- visual
- mamba
- arxiv
- grounding
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Mamba-VL matches or outperforms Transformer-based VLMs on captioning,\
  \ VQA, and reading comprehension, but lags in visual grounding tasks, with performance\
  \ gaps widening at larger scales. This difference is attributed to Transformers\u2019\
  \ superior in-context retrieval efficiency, particularly in tasks requiring fine-grained\
  \ spatial or textual detail extraction from images."
---

# Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling

## Quick Facts
- **arXiv ID**: 2409.05395
- **Source URL**: https://arxiv.org/abs/2409.05395
- **Reference count**: 40
- **Primary result**: Mamba-VL matches or outperforms Transformer-based VLMs on captioning, VQA, and reading comprehension, but lags in visual grounding tasks

## Executive Summary
This paper presents a comprehensive comparison between Vision-Language Models (VLMs) built on Transformers versus those built on Structured State Space Models (SSMs), specifically Mamba. Through systematic experimentation across multiple benchmarks, the authors demonstrate that Mamba-VL can achieve competitive performance to Transformer-based VLMs on tasks like captioning, VQA, and reading comprehension, but shows significant limitations in visual grounding tasks. The performance gap between the architectures widens at larger scales, with Transformers maintaining advantages in retrieval-intensive scenarios. The study reveals fundamental architectural differences in how these models handle in-context retrieval and spatial reasoning.

## Method Summary
The researchers conducted a systematic comparison between Pythia (Transformer-based) and Mamba-VL (SSM-based) across four vision-language tasks: image captioning, visual question answering, document visual question answering, and visual grounding. They evaluated both base and large variants of each architecture, testing with various input resolutions (224x224 and 384x384) and employing task-aware visual encodings. The study included synthetic grounding experiments to isolate retrieval efficiency differences and tested both architectures on curated and naturally sampled datasets to assess robustness to dataset artifacts.

## Key Results
- Mamba-VL matches or exceeds Transformer performance on high-level understanding tasks (captioning, VQA, reading comprehension)
- Transformers significantly outperform Mamba-VL on visual grounding tasks requiring fine-grained spatial/textual detail extraction
- The performance gap widens at larger scales, with Pythia Large showing greater advantages over Mamba Large than Pythia Base over Mamba Base
- Task-aware visual encoding provides minimal improvement for Mamba-VL but benefits Pythia on VQA tasks
- Higher-resolution finetuning benefits both models, with Pythia gaining more on grounding benchmarks

## Why This Works (Mechanism)
The fundamental difference lies in how Transformers and SSMs process sequential information. Transformers use self-attention mechanisms that can efficiently attend to any position in the sequence, making them particularly effective at retrieval tasks where specific spatial or textual details need to be located and referenced. SSMs like Mamba process information sequentially with a fixed state size, which limits their ability to perform efficient in-context retrieval, especially when dealing with long sequences of visual tokens. This architectural constraint becomes more pronounced as model scale increases, explaining why the performance gap widens at larger sizes.

## Foundational Learning

**Attention Mechanisms**: Why needed - Understanding how Transformers perform efficient information retrieval; Quick check - Can attend to any position regardless of sequence length

**State Space Models**: Why needed - Understanding the sequential processing nature of SSMs; Quick check - Fixed state size limits context window efficiency

**Visual Grounding**: Why needed - Task requiring precise spatial and textual detail extraction; Quick check - Success depends on efficient retrieval of specific visual elements

**Tokenization**: Why needed - Understanding how visual information is converted to sequential tokens; Quick check - Different tokenization strategies affect retrieval efficiency

**Context Window**: Why needed - Understanding how much information can be processed simultaneously; Quick check - Limited window affects retrieval capabilities

## Architecture Onboarding

**Component Map**: Image Encoder -> Visual Token Processor -> Cross-modal Fusion -> Language Decoder

**Critical Path**: Visual tokens → sequential processing → cross-attention → text generation

**Design Tradeoffs**: Transformers offer superior retrieval efficiency but higher computational cost; SSMs provide faster inference but limited retrieval capabilities

**Failure Signatures**: Mamba-VL struggles with tasks requiring precise spatial detail or specific text extraction from images

**First Experiments**: 1) Compare retrieval efficiency on synthetic tasks, 2) Test performance scaling with input resolution, 3) Evaluate task-aware encoding impact

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond noting the need for further investigation into why SSMs struggle with retrieval tasks and whether alternative architectural modifications could bridge the performance gap.

## Limitations
- Performance gap between architectures widens significantly at larger scales, limiting Mamba's applicability to retrieval-intensive tasks
- Task-aware visual encoding shows minimal benefit for Mamba-VL, suggesting limited architectural flexibility
- Study focuses on specific benchmark tasks which may not capture all real-world use cases
- The exact mechanisms behind Transformers' superior retrieval efficiency remain partially unexplained

## Confidence
**High**: Mamba-VL excels at tasks relying on high-level image summaries (consistent performance advantages in captioning and VQA)
**Medium**: Transformers are more sample-efficient for retrieval-oriented tasks (synthetic grounding experiments provide supporting evidence but may not fully generalize)
**Medium**: Higher-resolution finetuning benefits both models, with Pythia gaining more on grounding benchmarks (study does not extensively explore resolution-model interplay)

## Next Checks
1. Conduct large-scale experiments to quantify the performance gap between Mamba-VL and Transformers on retrieval-intensive tasks across diverse datasets and real-world applications
2. Investigate alternative visual encoding strategies for Mamba-VL to assess whether task-aware or domain-specific encodings could improve retrieval performance
3. Explore the impact of varying input resolutions and model scales on both architectures to determine if the observed trends hold consistently across different configurations