---
ver: rpa2
title: Evaluation of Code LLMs on Geospatial Code Generation
arxiv_id: '2410.04617'
source_url: https://arxiv.org/abs/2410.04617
tags:
- code
- geospatial
- tasks
- llms
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a benchmark dataset for evaluating code generation
  models on geospatial tasks, addressing the gap in existing datasets that typically
  focus on generic programming tasks. The dataset includes 77 manually curated coding
  problems categorized by task complexity, input types, required tools, and task framing.
---

# Evaluation of Code LLMs on Geospatial Code Generation

## Quick Facts
- **arXiv ID:** 2410.04617
- **Source URL:** https://arxiv.org/abs/2410.04617
- **Reference count:** 36
- **Primary result:** Introduces benchmark dataset with 77 geospatial coding problems, finding best model achieves 45.45% pass@1 on single-step tasks but only 15.15% on multi-step tasks

## Executive Summary
This paper introduces a benchmark dataset for evaluating code generation models on geospatial tasks, addressing the gap in existing datasets that typically focus on generic programming tasks. The dataset includes 77 manually curated coding problems categorized by task complexity, input types, required tools, and task framing. Experiments on seven code generation models show varying performance, with the best model achieving 45.45% pass@1 on single-step tasks but dropping to 15.15% on multi-step tasks. The results highlight challenges in geospatial code generation, particularly with specialized libraries like MovingPandas and OSMNX, indicating significant room for improvement in models tailored for geospatial applications.

## Method Summary
The authors created a benchmark dataset of 77 manually curated geospatial coding problems, systematically categorized along four dimensions: task complexity, input types, required tools, and task framing. Each problem includes multiple test cases for automated evaluation. Seven code generation models (ranging from 7B to 8B parameters) were evaluated using greedy decoding with 4-bit quantization to enable running on consumer hardware. The evaluation framework automatically executes generated code in isolated environments and checks functional correctness against test cases. Tasks were augmented to create multiple variants from base problems, expanding coverage while maintaining manual curation quality.

## Key Results
- Best model achieved 45.45% pass@1 on single-step tasks but only 15.15% on multi-step tasks
- Models perform well on tasks using shapely and H3 libraries but fail almost completely on MovingPandas and OSMNX
- Significant performance gap between simpler and more complex geospatial reasoning tasks
- All models struggle with tasks requiring specialized geospatial knowledge and multi-step reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The dataset's multi-dimensional categorization directly exposes LLM limitations in geospatial tool usage and task complexity.
- **Mechanism:** By explicitly structuring problems along four axes (complexity, input type, tools, framing), the benchmark isolates model weaknesses that would be hidden in monolithic datasets.
- **Core assumption:** The categorization scheme accurately reflects the true dimensions of difficulty in geospatial code generation.
- **Evidence anchors:**
  - [abstract] "categorised geospatial tasks based on their complexity and required tools"
  - [section 3.1] "four-dimensional classification of problems: (1) the complexity of the task, (2) input type for geospatial information, (3) the required tools and knowledge to solve it, and (4) the framing of a task"
- **Break condition:** If real-world geospatial tasks don't align with these four dimensions, the categorization may miss critical failure modes.

### Mechanism 2
- **Claim:** Test-driven evaluation provides more reliable correctness assessment than reference-based metrics like BLEU or ROUGE.
- **Mechanism:** By requiring code to pass multiple test cases, the evaluation ensures functional correctness rather than superficial syntactic similarity to reference solutions.
- **Core assumption:** Functional correctness is the primary quality metric for geospatial code generation, not code style or structure.
- **Evidence anchors:**
  - [section 3.4] "We use automated testing to check correctness of a generated solution... functional correctness evaluation is closer to how human developers verify code correctness"
- **Break condition:** If the test cases don't cover edge cases adequately, models might exploit gaps to appear more capable than they are.

### Mechanism 3
- **Claim:** The augmentation strategy (creating multiple variants from base tasks) efficiently expands dataset coverage without requiring manual creation of entirely new problems.
- **Mechanism:** By systematically varying input formats, tools, and framing while keeping the core task constant, the approach generates diverse test scenarios from minimal human effort.
- **Core assumption:** These systematic variations capture meaningful differences in model capabilities rather than just surface-level differences.
- **Evidence anchors:**
  - [section 3.3] "We start with defining a task which falls under the first dimension... augment this task by changing the format of input parameters or framing of the task"
- **Break condition:** If the augmentations create artificial scenarios that don't reflect realistic geospatial coding challenges, the dataset loses ecological validity.

## Foundational Learning

- **Concept: Geospatial data formats and libraries**
  - Why needed here: The benchmark tests models on specific libraries (GeoPandas, OSMNX, H3, MovingPandas, shapely) and formats (GeoJSON, Shapefiles, GeoDataFrames). Understanding these is essential for interpreting results.
  - Quick check question: What is the difference between a GeoDataFrame and a regular pandas DataFrame?

- **Concept: Spatial operations and geometry**
  - Why needed here: Tasks require understanding spatial relationships (intersections, containment, geometric operations) that are fundamental to geospatial analysis.
  - Quick check question: How would you check if a point lies within a polygon using shapely?

- **Concept: Test-driven development**
  - Why needed here: The evaluation methodology relies on test cases to verify correctness. Understanding this approach is crucial for implementing and extending the benchmark.
  - Quick check question: What are the advantages of using test cases over reference solutions for code evaluation?

## Architecture Onboarding

- **Component map:** Task creation → Augmentation → Test case generation → Prompt formatting → Model generation → Environment setup → Test execution → Result aggregation
- **Critical path:** Task → Prompt formatting → Model generation → Test execution → Result aggregation. Each step must succeed for meaningful evaluation.
- **Design tradeoffs:**
  - 4-bit quantization enables running 7B models on consumer hardware but may slightly reduce performance
  - Automated test cases ensure scalability but may miss nuanced correctness issues
  - Systematic augmentation maximizes coverage but may create artificial scenarios
  - Greedy decoding simplifies evaluation but may miss better solutions that beam search could find
- **Failure signatures:**
  - Models generating placeholder functions (raise NotImplementedError) indicate complete unfamiliarity with libraries
  - Repetitive code blocks suggest hallucination rather than understanding
  - Zero test case passing for specific input formats indicates format-specific knowledge gaps
  - Significant performance drop on multi-step tasks reveals reasoning limitations
- **First 3 experiments:**
  1. **Dimension isolation test:** Run a single model on tasks from only one dimension (e.g., only GeoPandas tasks) to verify categorization effectiveness
  2. **Augmentation validation:** Compare model performance on base tasks vs. their augmented variants to ensure augmentations add meaningful difficulty
  3. **Library coverage test:** Focus on tasks using a single library (e.g., only shapely) to assess library-specific knowledge acquisition

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of code generation models vary across different geospatial libraries, and what architectural changes could improve their performance on specialized libraries like MovingPandas and OSMNX?
- **Basis in paper:** [explicit] The paper explicitly discusses the performance differences across libraries, noting that models work well with shapely and H3 but fail almost completely on MovingPandas and OSMNX, and suggests architectural changes for improvement.
- **Why unresolved:** The paper identifies the issue but does not provide solutions or detailed analysis on why models struggle with certain libraries, leaving room for further investigation.
- **What evidence would resolve it:** Comparative studies of model performance with different architectural designs or training datasets focused on specialized geospatial libraries.

### Open Question 2
- **Question:** Can finetuning existing code generation models on geospatial-specific datasets significantly improve their performance on complex multi-step geospatial tasks?
- **Basis in paper:** [inferred] The paper suggests potential for improvement through finetuning, especially for handling different input formats and complex tasks, but does not explore this approach in detail.
- **Why unresolved:** While the paper highlights the potential for improvement, it does not provide experimental results or detailed analysis on the impact of finetuning on model performance.
- **What evidence would resolve it:** Experimental results comparing model performance before and after finetuning on geospatial-specific datasets, focusing on complex multi-step tasks.

### Open Question 3
- **Question:** What are the key factors contributing to the geographic bias observed in code generation models when performing geospatial tasks, and how can these biases be mitigated?
- **Basis in paper:** [explicit] The paper references studies on geographic bias in LLMs and suggests that this is a significant challenge, but does not explore specific factors or mitigation strategies.
- **Why unresolved:** The paper acknowledges the existence of geographic bias but does not provide a detailed analysis of its causes or propose specific strategies to address it.
- **What evidence would resolve it:** Studies identifying specific factors contributing to geographic bias and experimental results on the effectiveness of different mitigation strategies.

## Limitations

- Dataset size of 77 manually curated problems limits statistical significance and generalizability of results
- 4-bit quantization may introduce performance degradation not fully accounted for in reported results
- Test-driven evaluation may miss important aspects of code quality like efficiency and best practices

## Confidence

- **High Confidence:** The dataset construction methodology and categorization scheme are well-documented and reproducible. The test-driven evaluation approach is clearly specified and technically sound.
- **Medium Confidence:** The reported performance numbers for individual models are likely accurate for the specific evaluation setup, but generalization to other contexts or larger-scale problems is uncertain.
- **Low Confidence:** Claims about the relative difficulty of different task categories (single-step vs. multi-step, library-specific challenges) lack statistical validation and may be influenced by the small dataset size.

## Next Checks

1. **Statistical Significance Testing:** Perform t-tests or non-parametric equivalents to determine if performance differences between models on single-step vs. multi-step tasks are statistically significant, given the small sample size.

2. **Ablation Study on Augmentation:** Evaluate model performance on the original 77 base tasks versus the augmented variants to quantify how much the augmentation strategy contributes to observed performance differences and whether it introduces artificial difficulty.

3. **Human Evaluation Correlation:** Conduct a small-scale human evaluation comparing model-generated code against test-driven correctness to validate whether functional correctness aligns with human assessments of code quality and usefulness.