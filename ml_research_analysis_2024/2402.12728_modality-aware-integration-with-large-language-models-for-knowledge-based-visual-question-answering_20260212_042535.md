---
ver: rpa2
title: Modality-Aware Integration with Large Language Models for Knowledge-based Visual
  Question Answering
arxiv_id: '2402.12728'
source_url: https://arxiv.org/abs/2402.12728
tags:
- knowledge
- graph
- question
- llms
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a modality-aware integration approach with
  large language models (LLMs) for knowledge-based visual question answering (KVQA).
  The method addresses challenges of LLM hallucinations and alignment of multiple
  knowledge sources.
---

# Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering

## Quick Facts
- arXiv ID: 2402.12728
- Source URL: https://arxiv.org/abs/2402.12728
- Reference count: 15
- Achieves 56.69% accuracy on OK-VQA, outperforming state-of-the-art by 2.28% with 24× fewer parameters and 2-4× faster inference

## Executive Summary
This paper addresses the challenge of knowledge-based visual question answering (KVQA) by proposing a modality-aware integration approach that leverages large language models (LLMs). The method introduces a two-stage prompting strategy to construct detailed scene graphs from images, builds coupled concept graphs with external knowledge, and employs a pseudo-siamese graph medium fusion for effective multimodal integration. The approach successfully balances intra-modal preservation with inter-modal fusion, achieving state-of-the-art performance on OK-VQA and FVQA datasets while being significantly more parameter-efficient and faster than existing methods.

## Method Summary
The proposed method involves a two-stage prompting strategy where LLMs first generate dense image captions, then extract structured triples to construct a scene graph. This scene graph is coupled with a concept graph built using external knowledge from ConceptNet. A pseudo-siamese graph neural network with medium fusion exchanges information between modalities through shared mentioned entities while preserving intra-modal characteristics. The model is trained with a combined objective of answer-targeted loss and Maximum Mean Discrepancy (MMD) loss to align representations across modalities without over-homogenization.

## Key Results
- Achieves 56.69% accuracy on OK-VQA dataset, outperforming state-of-the-art by 2.28%
- Uses 24× fewer parameters compared to previous best models
- Provides 2-4× faster inference time while maintaining superior accuracy
- Demonstrates consistent performance improvements across both OK-VQA and FVQA datasets

## Why This Works (Mechanism)

### Mechanism 1
The two-stage prompting strategy with LLMs enables dense visual scene representation that captures both spatial and object features effectively. The first stage generates a detailed caption using a visual LLM, then the second stage prompts the LLM to extract triples using predefined relations (spatial and object features), constructing a rich scene graph. Core assumption: LLMs can understand image captions and generate structured knowledge triples when properly prompted with specific relation types.

### Mechanism 2
The pseudo-siamese graph medium fusion balances intra-modal processing and inter-modal exchange effectively. Uses two graph attention networks with same architecture but different weights for each modality, then exchanges embeddings only through shared mentioned entities (mediums) to enable cross-modal fusion while preserving intra-modal information. Core assumption: Shared mentioned entities in both scene and concept graphs can serve as effective bridges for cross-modal information exchange without losing modality-specific characteristics.

### Mechanism 3
The combined training objective with answer-targeted loss and MMD loss effectively aligns modalities while maintaining prediction accuracy. Uses binary cross-entropy loss for answer prediction and MMD loss to minimize distance between representations of the same medium across modalities, controlled by hyperparameter λ. Core assumption: Mediums from different modalities should have similar representations, and controlling λ prevents over-alignment that could harm intra-modal learning.

## Foundational Learning

- Concept: Graph neural networks and message passing
  - Why needed here: The method constructs scene and concept graphs and uses graph neural networks to process and fuse information across modalities
  - Quick check question: Can you explain how message passing works in graph neural networks and how it differs from standard neural network operations?

- Concept: Large language model prompting strategies
  - Why needed here: The method relies on carefully designed prompts to extract structured knowledge from LLMs for scene graph construction
  - Quick check question: What are the key differences between zero-shot, few-shot, and chain-of-thought prompting, and when would each be most appropriate?

- Concept: Cross-modal representation learning
  - Why needed here: The method needs to align visual and textual knowledge representations effectively for reasoning
  - Quick check question: How do you measure similarity between representations from different modalities, and what are common techniques for cross-modal alignment?

## Architecture Onboarding

- Component map: Image → Visual LLM (MiniGPT-4) → Dense caption → LLM → Scene graph (triples with 12 predefined relations) → Mentioned entities + Topic entity + ConceptNet → Concept graph → Pseudo-Siamese Graph Neural Networks → Graph Medium Fusion → Prediction
- Critical path: Image → Dense caption → Scene graph construction → Concept graph construction → PS-GMF → Answer prediction
- Design tradeoffs:
  - Using LLMs for scene graph construction vs. traditional object detection: LLMs provide richer semantic understanding but may introduce hallucinations
  - Medium-based fusion vs. full graph fusion: Medium-based preserves intra-modal information better but may limit cross-modal exchange
  - MMD loss contribution: Helps alignment but too much can over-homogenize representations
- Failure signatures:
  - Low performance despite high capacity: Likely issues with prompt quality or graph construction
  - High variance in predictions: Possible instability in medium exchange or attention mechanisms
  - Slow inference: Bottleneck likely in LLM calls or graph processing layers
- First 3 experiments:
  1. Test scene graph construction quality: Input sample images, verify generated triples match expectations for spatial and object relations
  2. Test medium exchange: Check that embeddings for same mentioned entities across modalities become similar after PS-GMF layers
  3. Test ablation: Run with PS-GMF vs PSG only to verify inter-modal fusion provides performance gains as claimed

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of detailed prompt templates makes exact replication difficult and introduces variability in scene graph quality
- Hyperparameter settings for GNNs and MMD loss weighting are not provided, limiting reproducibility
- Evaluation relies solely on accuracy without statistical significance testing or confidence intervals

## Confidence

- **High confidence**: General architecture and approach is well-described and logically sound; reported speed and parameter efficiency improvements are concrete and verifiable
- **Medium confidence**: Effectiveness of medium-based fusion compared to other fusion strategies; specific implementation details that make it superior are not fully transparent
- **Low confidence**: Exact contribution of MMD loss to final performance; optimal λ value and its sensitivity are not explored or reported

## Next Checks

1. **Prompt template validation**: Reconstruct the two-stage prompting strategy using the described relations and test on sample images to verify the quality and consistency of generated scene graphs compared to ground truth or human-annotated triples

2. **Hyperparameter sensitivity analysis**: Run controlled experiments varying the λ parameter controlling MMD loss contribution to identify the optimal balance between alignment and intra-modal preservation

3. **Statistical significance testing**: Conduct t-tests or bootstrap confidence intervals on the accuracy improvements reported to determine if the 2.28% gain over baselines is statistically significant