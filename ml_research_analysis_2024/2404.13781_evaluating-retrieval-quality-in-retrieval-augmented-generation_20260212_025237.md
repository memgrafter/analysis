---
ver: rpa2
title: Evaluating Retrieval Quality in Retrieval-Augmented Generation
arxiv_id: '2404.13781'
source_url: https://arxiv.org/abs/2404.13781
tags:
- retrieval
- evaluation
- erag
- downstream
- correlation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes eRAG, a novel approach for evaluating retrieval
  models within retrieval-augmented generation (RAG) systems. eRAG leverages the large
  language model (LLM) in the RAG system itself to generate relevance labels for each
  document in the retrieval list, rather than relying on human annotations or separate
  LLM judges.
---

# Evaluating Retrieval Quality in Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2404.13781
- Source URL: https://arxiv.org/abs/2404.13781
- Authors: Alireza Salemi; Hamed Zamani
- Reference count: 40
- Key outcome: eRAG achieves 0.168-0.494 higher Kendall's τ correlation with downstream RAG performance compared to baseline evaluation methods while consuming up to 50× less GPU memory

## Executive Summary
This paper introduces eRAG, a novel approach for evaluating retrieval quality in Retrieval-Augmented Generation systems. Unlike traditional methods that rely on human annotations or external LLM judges, eRAG leverages the RAG system's own LLM to generate document relevance labels. The method processes each retrieved document individually through the LLM and applies task-specific metrics to generate per-document scores, which are then aggregated using standard ranking metrics. The approach demonstrates significantly higher correlation with downstream performance while offering substantial computational advantages.

## Method Summary
eRAG evaluates retrieval quality by using the RAG system's LLM to individually process each document in the retrieval list. For each document-query pair, the LLM generates an output that is evaluated against ground truth using task-specific metrics (EM, Accuracy, or F1). These per-document scores are aggregated using ranking metrics like MAP and NDCG to produce an overall evaluation score. The method is tested on five KILT benchmark datasets (NQ, TriviaQA, HotpotQA, FEVER, WoW) using T5-small with Fusion-in-Decoder architecture and compared against baselines including containing ground truth, KILT provenance, and LLM annotation with Mistral 7B.

## Key Results
- eRAG achieves 0.168-0.494 higher Kendall's τ correlation with downstream RAG performance compared to baseline methods
- Computational efficiency improves runtime and reduces GPU memory consumption by up to 50× compared to end-to-end evaluation
- The method shows consistent improvements across five different datasets and task types (question answering, fact verification, dialogue generation)

## Why This Works (Mechanism)

### Mechanism 1
Using the LLM itself as a judge produces relevance labels that better correlate with downstream performance than human-annotated or external LLM-judged labels. The LLM in the RAG pipeline is the final consumer of retrieved documents, so its own task-level evaluation of each document reflects the actual utility of that document for its specific downstream task. Core assumption: The same LLM used for generation can be repurposed to score individual documents without introducing significant bias or variance.

### Mechanism 2
Per-document scoring with task-specific metrics allows ranking metrics to be computed without full end-to-end generation. By feeding each retrieved document individually to the LLM and evaluating its output against ground truth, we obtain a relevance score per document, which can then be aggregated using standard ranking metrics (MAP, NDCG, etc.). Core assumption: The LLM can produce meaningful partial answers when given a single document plus query, sufficient for downstream metric calculation.

### Mechanism 3
The approach is computationally more efficient than end-to-end evaluation, saving both time and GPU memory. Since each document is scored individually, the quadratic cost of transformer inference (which scales with total sequence length) is avoided; instead, cost scales linearly with the number of documents and document length. Core assumption: The per-document inference cost is significantly less than concatenating all documents and running a single large forward pass.

## Foundational Learning

- **Concept: Kendall's tau and Spearman's rho correlation coefficients**
  - Why needed here: The paper reports improvements in Kendall's tau correlation as the primary metric for evaluating the effectiveness of eRAG versus baselines
  - Quick check question: What does a Kendall's tau of 0.5 versus 0.2 indicate about the relationship between two ranked lists?

- **Concept: End-to-end vs. per-document inference cost in transformers**
  - Why needed here: Understanding why eRAG is more efficient requires grasping the quadratic scaling of transformer inference with sequence length
  - Quick check question: If a single document is 1000 tokens and the LLM handles 8000 tokens max, how many documents can be concatenated before hitting the limit? What is the inference cost scaling difference?

- **Concept: Retrieval metrics (MAP, NDCG, MRR, Precision, Recall, Hit Rate)**
  - Why needed here: eRAG uses these to aggregate per-document relevance scores into a single evaluation score
  - Quick check question: How does Precision differ from Hit Rate when relevance labels are non-binary?

## Architecture Onboarding

- **Component map**: Retriever → Ranked document list → Per-document LLM inference → Task-specific metric scoring → Aggregation via ranking metric → Correlation with downstream RAG performance
- **Critical path**: Retriever → Per-document LLM scoring → Aggregation
- **Design tradeoffs**: Higher correlation vs. inference overhead of multiple LLM calls; choice of task metric affects label granularity; LLM size affects both correlation quality and computational cost
- **Failure signatures**: Low correlation despite high retrieval quality; high variance in per-document scores; GPU memory exhaustion during scoring; task metric incompatibility with LLM output
- **First 3 experiments**:
  1. Compare Kendall's tau of eRAG scores vs. end-to-end RAG performance on a small subset of NQ dataset with BM25 retrieval
  2. Vary number of retrieved documents (10, 25, 50) and measure correlation and runtime/memory usage
  3. Replace T5-small with T5-base in eRAG and observe changes in correlation and computational cost

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain based on the limitations of the current study.

## Limitations
- The paper lacks statistical significance testing for the reported correlation improvements
- Computational efficiency claims are based on theoretical scaling analysis rather than comprehensive empirical benchmarks
- The per-document scoring mechanism may not work well for tasks requiring multi-document reasoning
- Effectiveness may depend heavily on the specific LLM architecture and size, though only T5-small and Mistral 7B were tested

## Confidence
- Correlation superiority claim: Medium-High (strong theoretical basis, good empirical support but lacks significance testing)
- Computational efficiency claim: Medium (theoretical analysis is sound but lacks comprehensive empirical validation)
- Per-document scoring mechanism: Medium (intuitive but not thoroughly validated across diverse tasks)

## Next Checks
1. Conduct statistical significance testing (e.g., bootstrap confidence intervals) on the reported Kendall's tau correlation improvements to verify they are not due to random variation
2. Test eRAG with a larger LLM (e.g., LLaMA-2 70B) to evaluate whether correlation quality scales with model size and whether the computational efficiency advantage persists
3. Evaluate eRAG on tasks requiring multi-document reasoning (e.g., HotpotQA) to verify the per-document scoring mechanism remains effective when answers require combining information from multiple sources