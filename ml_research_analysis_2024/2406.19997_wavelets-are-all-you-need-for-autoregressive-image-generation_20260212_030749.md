---
ver: rpa2
title: Wavelets Are All You Need for Autoregressive Image Generation
arxiv_id: '2406.19997'
source_url: https://arxiv.org/abs/2406.19997
tags:
- image
- wavelet
- cant
- token
- cients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new autoregressive image generation method
  using wavelet image coding and a modified transformer model. The key innovation
  is converting images into a "wavelet language" through embedded wavelet tokenization,
  where visual information is ordered from coarse to fine details.
---

# Wavelets Are All You Need for Autoregressive Image Generation

## Quick Facts
- arXiv ID: 2406.19997
- Source URL: https://arxiv.org/abs/2406.19997
- Reference count: 40
- One-line primary result: Wavelet-based autoregressive image generation using only 6-7 tokens per image with modified transformer architecture

## Executive Summary
This paper introduces a novel autoregressive image generation method that converts images into a "wavelet language" through embedded wavelet tokenization. The approach uses discrete wavelet transforms to decompose images into sparse representations, then orders visual information from coarse to fine details in token sequences. A modified transformer architecture with custom positional encoding learns the statistical correlations between wavelet coefficients, enabling conditional image generation. The method demonstrates successful generation on MNIST and FashionMNIST datasets using class labels for guidance, with the ability to control generative diversity through stochastic sampling methods.

## Method Summary
The method consists of three main stages: wavelet preprocessing and tokenization, modified transformer processing, and image reconstruction. Images are first decomposed using discrete wavelet transforms (Haar for MNIST, bior4.4 for FashionMNIST) into low-frequency and high-frequency subbands. The wavelet coefficients are then scanned across bit-planes to create token sequences using embedded coding with 7 tokens: 'Group4x4', 'Group2x2', 'NowSignificantNeg', 'NowSignificantPos', 'Insignificant', 'NextAccuracy0', and 'NextAccuracy1'. A modified DistilGPT2 transformer processes these sequences with custom wavelet-specific positional encoding (bp, i1, i2) and class vector concatenation for conditioning. During inference, conditional next-token sampling with Top-k or Top-p methods generates new token sequences, which are then reconstructed back into images through inverse wavelet transforms.

## Key Results
- Successful image generation on MNIST and FashionMNIST datasets using only 6-7 tokens per image
- Ability to control generative diversity through Top-k and Top-p stochastic sampling methods
- Competitive results compared to diffusion models with minimal computational resources
- Class label conditioning enables guided generation of specific digit types or fashion items

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wavelet transforms convert images into sparse representations where edge and texture information concentrates in few significant coefficients
- Mechanism: Biorthogonal wavelet decomposition creates low-frequency scaling coefficients and three oriented subband sets (LH, HL, HH) that capture edges, with most energy in large-magnitude coefficients
- Core assumption: Natural images exhibit edge-sparsity with most wavelet coefficients near zero magnitude
- Evidence anchors:
  - [abstract]: "The various wavelet transforms provide the means to transform an image into a representation that captures the essence of the visual information in a sparse way."
  - [section 3.1]: "The edge information typically constitutes a small portion of a typical image, while the dual wavelet coefficients have a large absolute value only if edges intersect the support of the corresponding dual wavelets."

### Mechanism 2
- Claim: Progressive wavelet coding orders visual information from coarse to fine details, creating natural autoregressive generation sequence
- Mechanism: Bit-plane scanning from most significant to least significant bits creates sequences starting with low-resolution structure and adding finer details incrementally
- Core assumption: Human visual perception prioritizes coarse structure before fine details, making this ordering optimal for generation
- Evidence anchors:
  - [abstract]: "convert images into a 'wavelet language' through embedded wavelet tokenization, where visual information is ordered from coarse to fine details."
  - [section 3.2.1]: "ordering the information starting with the most significant bits of the most significant wavelet coefficients."

### Mechanism 3
- Claim: Modified transformer architecture learns statistical correlations between wavelet coefficients across scales
- Mechanism: Transformer processes wavelet token sequences, learning quadtree-like correlations where parent coefficients predict child coefficients across resolutions
- Core assumption: Wavelet coefficients exhibit strong statistical dependencies across scales and spatial locations learnable by transformer attention
- Evidence anchors:
  - [abstract]: "The transformer learns the significant statistical correlations within a token sequence, which are the manifestations of well-known correlations between the wavelet subbands at various resolutions."

## Foundational Learning

- Concept: Discrete Wavelet Transform (DWT) and its implementation
  - Why needed here: Understanding how 2D wavelet transforms decompose images into approximation and detail subbands is fundamental to the tokenization process
  - Quick check question: How many coefficients does a 2D DWT produce compared to the original image size?

- Concept: Bit-plane scanning and significance testing
  - Why needed here: The tokenization algorithm relies on scanning wavelet coefficients across bit-planes to identify and report significant coefficients progressively
  - Quick check question: What determines whether a coefficient is "significant" at a given bit-plane threshold?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The modified transformer processes the wavelet token sequences, requiring understanding of how attention learns dependencies in sequences
  - Quick check question: How does the transformer's positional encoding differ from standard implementations in this context?

## Architecture Onboarding

- Component map:
  Wavelet preprocessing pipeline → Tokenization engine → Modified DistilGPT2 transformer → Conditional sampling engine → Image reconstruction

- Critical path:
  1. Image → DWT → Wavelet coefficients
  2. Coefficients → Bit-plane scanning → Token sequence
  3. Token sequence + conditioning → Transformer processing
  4. Transformer output → Conditional sampling → Next token
  5. Token sequence → Inverse DWT → Generated image

- Design tradeoffs:
  - Token vocabulary size (7 tokens) vs. sequence length (can be very long)
  - Bit-plane threshold selection (uniform vs. adaptive per image)
  - Wavelet basis choice (Haar for MNIST vs. bior4.4 for FashionMNIST)
  - Conditioning method (class vector vs. text prompt via CLIP)

- Failure signatures:
  - Generation produces blurry or incomplete images → Bit-plane threshold too low or insufficient training
  - Mode collapse (only generates one class) → Conditioning vector not properly integrated
  - Artifacts in generated images → Token sequence contains invalid transitions
  - Slow generation → Transformer struggling with long sequences

- First 3 experiments:
  1. Generate MNIST digits using only the wavelet preprocessing and inverse DWT (no transformer) to verify the pipeline works end-to-end
  2. Train transformer on tokenized MNIST with class conditioning only, evaluate basic generation quality
  3. Add Top-k sampling with k=2 and compare diversity of generated samples versus greedy decoding

## Open Questions the Paper Calls Out

- How does the performance of wavelet-based autoregressive image generation compare to state-of-the-art diffusion models on large-scale datasets like ImageNet?
- What is the optimal trade-off between vocabulary size and sequence length for wavelet tokenization across different image resolutions and complexity levels?
- How does the wavelet-based approach perform for text-to-image generation compared to specialized text-conditioned diffusion models?

## Limitations

- No quantitative experimental results provided (no FID, IS, or pixel-wise metrics)
- Only demonstrated on simple datasets (MNIST and FashionMNIST)
- Scalability to complex natural images and large-scale datasets unproven
- Computational efficiency claims lack quantitative support

## Confidence

- **High Confidence**: Mathematical foundations of wavelet transforms and their sparsity properties are well-established
- **Medium Confidence**: Modified transformer architecture with custom positional encoding is theoretically sound
- **Low Confidence**: Claims about computational efficiency and competitiveness with diffusion models lack quantitative evidence

## Next Checks

1. Implement FID and IS calculations for generated images on MNIST and FashionMNIST, then compare against established autoregressive baselines and diffusion models to provide objective quality metrics.

2. Test the approach on CIFAR-10 or CIFAR-100 datasets with 32x32 or 64x64 images to evaluate whether the wavelet tokenization and transformer architecture scale to more complex natural images with varied textures and fine details.

3. Perform systematic ablation experiments removing key components (wavelet tokenization, custom positional encoding, class conditioning) to quantify their individual contributions to generation quality and diversity.