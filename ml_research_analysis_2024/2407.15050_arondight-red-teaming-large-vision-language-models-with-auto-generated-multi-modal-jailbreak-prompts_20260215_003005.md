---
ver: rpa2
title: 'Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal
  Jailbreak Prompts'
arxiv_id: '2407.15050'
source_url: https://arxiv.org/abs/2407.15050
tags:
- prompt
- vlms
- toxic
- jailbreak
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Arondight, the first comprehensive red teaming
  framework for Large Vision Language Models (VLMs) that addresses both visual and
  textual modalities while ensuring prompt diversity. The framework introduces an
  automated multi-modal jailbreak attack using a red team VLM to generate toxic images
  and a red team LLM guided by reinforcement learning to produce diverse textual prompts.
---

# Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts

## Quick Facts
- arXiv ID: 2407.15050
- Source URL: https://arxiv.org/abs/2407.15050
- Reference count: 40
- Primary result: Achieved 84.5% average attack success rate against 10 state-of-the-art VLMs across 14 prohibited scenarios

## Executive Summary
This paper presents Arondight, the first comprehensive red teaming framework for Large Vision Language Models (VLMs) that addresses both visual and textual modalities while ensuring prompt diversity. The framework introduces an automated multi-modal jailbreak attack using a red team VLM to generate toxic images and a red team LLM guided by reinforcement learning to produce diverse textual prompts. When evaluated against ten state-of-the-art VLMs including GPT-4, Arondight achieved an average attack success rate of 84.5% across all fourteen prohibited scenarios defined by OpenAI, successfully exposing significant security vulnerabilities particularly in generating toxic images and aligning multi-modal prompts.

## Method Summary
Arondight employs a two-pronged approach: a red team VLM generates toxic images using a universal prompt template, while a red team LLM creates diverse textual prompts guided by an RL agent with entropy bonuses and novelty rewards. The framework combines these multi-modal components to construct jailbreak prompts that bypass VLM safety mechanisms. The RL agent optimizes prompt generation by balancing toxicity reduction with semantic preservation, while diversity metrics ensure comprehensive security evaluation across previously unseen test cases.

## Key Results
- Achieved 84.5% average attack success rate across ten VLMs including GPT-4, Bing Chat, and Google Bard
- Successfully exposed vulnerabilities in generating toxic images and aligning multi-modal prompts
- Demonstrated effectiveness across all fourteen prohibited scenarios defined by OpenAI
- Showed significant improvement over existing single-modality red teaming approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Arondight successfully bridges the modality gap between existing LLM red teaming methods and the unique challenges posed by VLMs by incorporating toxic images into the jailbreak attack strategy.
- Mechanism: The framework uses a red team VLM to generate toxic images that, when combined with textual prompts, significantly enhance the attack success rate against VLMs. This multi-modal approach exploits the rich semantic information offered by toxic images to overcome the textual safety components of black-box VLMs.
- Core assumption: Toxic images can effectively bypass VLM safety mechanisms when paired with diverse textual prompts.
- Evidence anchors: [abstract] "Our approach builds on prior jailbreak attack strategies against black-box LLMs, creating successful attack prompts for VLMs by: (1) Probing the VLMs with testing queries, and (2) Gradually optimizing our constructed attack prompts based on testing results."
- Break condition: If VLMs develop robust multi-modal safety alignment that can effectively detect and reject both toxic images and their associated textual prompts, the attack success rate would decrease significantly.

### Mechanism 2
- Claim: The diversity-driven red team LLM enhances the comprehensiveness of VLM security evaluation by generating a wide array of diverse and previously unseen test cases.
- Mechanism: The framework integrates entropy bonuses and novelty reward metrics into the optimization objectives of the RL agent, which guides the red team LLM to create diverse textual prompts that are semantically associated with toxic images.
- Core assumption: Increasing the entropy and novelty of textual prompts will lead to more effective jailbreak attacks against VLMs.
- Evidence anchors: [abstract] "To enhance the comprehensiveness of VLM security evaluation, we integrate entropy bonuses and novelty reward metrics. These elements incentivize the RL agent to guide the red team LLM in creating a wider array of diverse and previously unseen test cases."
- Break condition: If the RL agent fails to effectively balance the entropy and novelty of textual prompts, leading to either too much randomness or insufficient diversity, the attack effectiveness would decrease.

### Mechanism 3
- Claim: The universal prompt template-driven red team VLM overcomes the limitations of existing attacks by crafting adversarial jailbreak prompts with low toxicity scores that can bypass VLM safety filters.
- Mechanism: The framework employs a word-level perturbation strategy and a prompt template correction method to generate adversarial jailbreak prompts that preserve semantic intention while reducing toxicity, enabling the generation of toxic images without triggering VLM safety mechanisms.
- Core assumption: By reducing the toxicity score of prompts through word-level perturbations and template corrections, the adversarial jailbreak prompts can effectively bypass VLM safety filters.
- Evidence anchors: [abstract] "Our approach involves crafting a universal prompt template to stimulate the red team VLM into generating toxic images."
- Break condition: If VLM safety filters become more sophisticated in detecting adversarial patterns and semantic manipulations, the effectiveness of the universal prompt template-driven red team VLM would decrease.

## Foundational Learning

- Concept: Reinforcement Learning (RL) for red teaming
  - Why needed here: RL is used to train the red team LLM to maximize the likelihood of the target VLM generating inappropriate responses by optimizing the generation process based on an evaluation function.
  - Quick check question: How does the RL agent guide the red team LLM to generate diverse and relevant textual prompts?

- Concept: Multi-modal jailbreak attacks
  - Why needed here: Multi-modal jailbreak attacks are crucial for evaluating the security of VLMs, as they cover both image and text modalities and can effectively expose vulnerabilities in the alignment of multi-modal prompts.
  - Quick check question: What are the key components of the auto-generated multi-modal jailbreak attack in Arondight?

- Concept: Diversity metrics in red teaming
  - Why needed here: Diversity metrics, such as entropy bonuses and novelty rewards, are integrated into the optimization objectives of the RL agent to incentivize the generation of a wider array of diverse and previously unseen test cases.
  - Quick check question: How do entropy bonuses and novelty rewards contribute to the generation of diverse textual prompts in Arondight?

## Architecture Onboarding

- Component map:
  - Red team VLM -> Universal prompt template -> Toxic image generation
  - Red team LLM -> RL agent (entropy bonuses, novelty rewards) -> Diverse textual prompts
  - Target VLM -> Multi-modal jailbreak prompts -> Security vulnerability evaluation

- Critical path:
  1. Create adversarial jailbreak prompts using the red team VLM
  2. Generate toxic images and text using the red team VLM and LLM
  3. Construct multimodal prompts by combining toxic images and text
  4. Select attack modes (one-shot or few-shot)
  5. Detect toxicity and evaluate the target VLM's response

- Design tradeoffs:
  - Balancing the entropy and novelty of textual prompts to ensure diversity without sacrificing relevance
  - Optimizing the word-level perturbation strategy to reduce toxicity scores while preserving semantic intention
  - Choosing appropriate attack modes (one-shot or few-shot) based on the target VLM's security mechanisms

- Failure signatures:
  - Low attack success rate against target VLMs
  - Insufficient diversity in generated textual prompts
  - Inability to bypass VLM safety filters using adversarial jailbreak prompts

- First 3 experiments:
  1. Evaluate the attack success rate of Arondight against a single VLM using both one-shot and few-shot attack modes
  2. Compare the diversity of textual prompts generated by the red team LLM with and without entropy bonuses and novelty rewards
  3. Test the effectiveness of the universal prompt template-driven red team VLM in generating toxic images that bypass VLM safety filters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Arondight perform against VLMs with different visual capabilities (e.g., those with vs. without image generation)?
- Basis in paper: [explicit] The paper mentions that VLMs with image generation capabilities may suffer from inadequate text-to-image alignment, while those without may exhibit unsatisfactory security performance when handling multi-modal inputs.
- Why unresolved: The paper does not provide specific comparisons between the performance of Arondight against VLMs with different visual capabilities.
- What evidence would resolve it: Conducting experiments to evaluate Arondight's performance against a diverse set of VLMs, including those with and without image generation capabilities, and analyzing the results to identify any performance differences.

### Open Question 2
- Question: How does the entropy bonus and novelty reward metrics in Arondight affect the diversity of generated prompts?
- Basis in paper: [explicit] The paper introduces entropy bonus and novelty reward metrics to incentivize the red team LLM to produce diverse texts and explore novelty in generated test cases.
- Why unresolved: The paper does not provide quantitative evidence or analysis of how these metrics specifically impact the diversity of generated prompts.
- What evidence would resolve it: Conducting experiments to measure the diversity of prompts generated with and without the entropy bonus and novelty reward metrics, and analyzing the results to determine the effectiveness of these metrics in promoting prompt diversity.

### Open Question 3
- Question: How does Arondight perform against VLMs with different safety alignment mechanisms?
- Basis in paper: [explicit] The paper mentions that existing VLMs may have different safety alignment mechanisms, such as stricter alignment measures for political and professional content.
- Why unresolved: The paper does not provide specific comparisons between the performance of Arondight against VLMs with different safety alignment mechanisms.
- What evidence would resolve it: Conducting experiments to evaluate Arondight's performance against a diverse set of VLMs with varying safety alignment mechanisms, and analyzing the results to identify any performance differences or vulnerabilities.

## Limitations

- The framework relies heavily on GPT-4 for both red team VLM and LLM components, raising concerns about bias and generalization
- Limited discussion of computational costs and scalability issues, particularly regarding RL agent training
- Ethical implications of generating and testing toxic content are mentioned but not thoroughly addressed

## Confidence

**High Confidence**: The core mechanism of using a red team VLM to generate toxic images and a red team LLM guided by RL to produce diverse textual prompts is well-defined and technically sound.

**Medium Confidence**: The reported attack success rate of 84.5% is based on evaluations against ten VLMs, but the paper lacks detailed statistical analysis and error margins.

**Low Confidence**: The specific implementation details of the RL training process, including hyperparameters and optimization strategies, are not fully disclosed, making it difficult to assess the reproducibility and robustness of the framework.

## Next Checks

1. Conduct a more rigorous statistical analysis of the attack success rates, including confidence intervals, p-values, and cross-validation procedures to assess the reliability and generalizability of the results across different VLM architectures.

2. Perform a comprehensive ethical impact assessment to evaluate the potential risks and benefits of the Arondight framework, including responsible disclosure practices and mitigation strategies for potential misuse.

3. Analyze the computational costs and scalability of the framework, particularly focusing on the training of the RL agent and the generation of toxic images, to assess its practical applicability in real-world scenarios.