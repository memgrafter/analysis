---
ver: rpa2
title: Cross-Fundus Transformer for Multi-modal Diabetic Retinopathy Grading with
  Cataract
arxiv_id: '2411.00726'
source_url: https://arxiv.org/abs/2411.00726
tags:
- multi-modal
- fundus
- image
- attention
- grading
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Cross-Fundus Transformer (CFT), a novel dual-stream
  transformer architecture for multi-modal diabetic retinopathy (DR) grading using
  both color fundus photography (CFP) and infrared fundus photography (IFP). The proposed
  method employs a Cross-Fundus Attention (CFA) module to capture complementary features
  between the two modalities, while utilizing both single-modality and multi-modality
  supervisions.
---

# Cross-Fundus Transformer for Multi-modal Diabetic Retinopathy Grading with Cataract

## Quick Facts
- **arXiv ID**: 2411.00726
- **Source URL**: https://arxiv.org/abs/2411.00726
- **Authors**: Fan Xiao; Junlin Hou; Ruiwei Zhao; Rui Feng; Haidong Zou; Lina Lu; Yi Xu; Juzhao Zhang
- **Reference count**: 21
- **Primary result**: Achieved state-of-the-art DR grading with 84.44% Kappa score using dual-modality fundus images

## Executive Summary
This study introduces the Cross-Fundus Transformer (CFT), a dual-stream transformer architecture that leverages both color fundus photography (CFP) and infrared fundus photography (IFP) for diabetic retinopathy grading. The key innovation is the Cross-Fundus Attention (CFA) module that captures complementary lesion visibility patterns between the two modalities, particularly beneficial for cataract patients where CFP quality is compromised. Evaluated on 1,713 paired fundus images from 616 patients, CFT achieved state-of-the-art performance with a Kappa score of 84.44%, accuracy of 73.47%, and F1 score of 65.51%, demonstrating significant improvements over single-modality approaches.

## Method Summary
The Cross-Fundus Transformer employs a dual-stream ViT-Small architecture with separate encoders for CFP and IFP inputs. Each stream processes 512×512 images using 16×16 patch embeddings. The Cross-Fundus Attention module performs cross-attention between patch features from both modalities, allowing each to attend to the most informative features from the other. The model uses weighted supervision combining single-modality losses (λ=0.6 for CFP, 0.4 for IFP) with multi-modality classification loss. Training employs Adam optimizer (1e-4 learning rate, cosine annealing, 1e-5 weight decay) for 100 epochs with data augmentation including flipping, random cropping, color jittering, and affine transformations.

## Key Results
- Achieved 84.44% Quadratic Weighted Kappa score, 73.47% accuracy, and 65.51% F1 score on clinical dataset
- Outperformed single-modality approaches by 1.38% in Kappa score
- Demonstrated particular benefit for cataract patients where CFP quality is compromised

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal attention captures complementary lesion visibility between CFP and IFP
- Mechanism: The CFA module routes CFP patches to IFP patches and vice versa, allowing each modality to attend to the most informative features from the other modality for lesions like hemorrhages and exudates
- Core assumption: Different retinal lesions have modality-specific visibility patterns (color-based vs. infrared penetration)
- Evidence anchors:
  - [abstract] "a meticulously engineered Cross-Fundus Attention (CFA) module is introduced to capture the correspondence between CFP and IFP images"
  - [section] "Lesions like retinal detachment (red arrow) is easily distinguished in CFP at PDR stage, while other lesions such as exudate (yellow box) and hemorrhages (blue box) are more easily distinguished in IFP"

### Mechanism 2
- Claim: Dual-stream architecture with modality-specific supervision improves generalization
- Mechanism: Separate MLP heads for CFP and IFP ensure each modality learns discriminative features independently, while the shared cross-attention module learns modality-invariant representations
- Core assumption: Each imaging modality has unique characteristics requiring specialized feature extraction
- Evidence anchors:
  - [section] "We adopt both the single-modality and multi-modality supervisions to maximize the overall performance for DR grading"
  - [section] "Compared with ViT models, two ViT-self attention models increase the kappa scores by 0.97% and 0.64% on CFP and IFP, respectively"

### Mechanism 3
- Claim: Pre-training on large fundus dataset accelerates convergence and improves performance
- Mechanism: Transfer learning from a model pre-trained on 200K fundus images provides robust initial representations for both CFP and IFP modalities
- Core assumption: Visual patterns in fundus imaging are transferable across different imaging modalities
- Evidence anchors:
  - [section] "we utilize a ViT model pretrained on a large fundus image dataset [19], and finetune it on our target data"
  - [section] "Experiments on a clinical dataset showed that our model achieved state-of-the-art results"

## Foundational Learning

- **Vision Transformer architecture and patch embedding**
  - Why needed here: CFT builds upon ViT as the backbone for feature extraction from fundus images
  - Quick check question: How does ViT convert a 2D image into a sequence of tokens for transformer processing?

- **Multi-head attention mechanisms**
  - Why needed here: Both the ViT backbone and the CFA module rely on multi-head attention to capture relationships between patches
  - Quick check question: What is the purpose of splitting attention into multiple heads, and how does this help capture different types of relationships?

- **Cross-modal attention fusion**
  - Why needed here: The CFA module specifically performs cross-attention between CFP and IFP features to leverage their complementary information
  - Quick check question: How does cross-attention differ from standard self-attention, and why is this important for multi-modal fusion?

## Architecture Onboarding

- **Component map**: Input → Patch embedding → ViT encoder → Class token + patch features → Single-modality MLP heads + CFA module → Fused features → Multi-modal classifier → Final prediction

- **Critical path**: Image → Patch embedding → ViT encoder → Class token + patch features → Single-modality MLP heads + CFA module → Fused features → Multi-modal classifier → Final prediction

- **Design tradeoffs**: Complexity vs. performance: dual-stream with cross-attention adds computational overhead but improves accuracy; Supervision balance: weight λ controls trade-off between single- and multi-modality learning; Fusion strategy: max-pooling chosen over mean or concatenation for better feature integration

- **Failure signatures**: Poor single-modality performance indicates issues with the ViT backbone or pre-training; Cross-attention weights becoming uniform suggests loss of complementary information; Overfitting on small datasets indicates need for stronger regularization or data augmentation

- **First 3 experiments**: Train single-stream ViT on CFP only and IFP only to establish baseline performance; Implement basic feature concatenation fusion to compare with cross-attention approach; Perform ablation study on the λ parameter to find optimal balance between single- and multi-modality supervision

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would CFT performance change with larger clinical datasets beyond 1,713 paired images?
- Basis in paper: [inferred] The authors demonstrate strong performance on their dataset but note it's clinically acquired and may be limited in size
- Why unresolved: The study only tested on one dataset of 1,713 images, making it unclear if performance gains would scale with more diverse and larger datasets
- What evidence would resolve it: Results from CFT evaluated on multiple larger clinical datasets with varying patient demographics and disease prevalence

### Open Question 2
- Question: Can CFT maintain its performance advantage when applied to other retinal diseases beyond diabetic retinopathy?
- Basis in paper: [inferred] The authors mention their study presents "a promising step towards multi-modal fundus image diagnosis" but only tested on DR
- Why unresolved: The model was specifically trained and validated only for DR grading, leaving its generalizability to other retinal conditions untested
- What evidence would resolve it: Results from CFT applied to datasets for glaucoma, macular degeneration, or other retinal pathologies

### Open Question 3
- Question: What is the optimal architecture for the Cross-Fundus Attention module when dealing with different image resolutions between CFP and IFP?
- Basis in paper: [explicit] The authors note that CFP images were captured at over 2,000×2,000 resolution while IFP images were at 768×768 resolution
- Why unresolved: The current implementation processes both modalities at the same 512×512 resolution without exploring whether the resolution mismatch affects attention mechanisms
- What evidence would resolve it: Comparative studies of CFA performance with different resizing strategies and resolution-preserving approaches

### Open Question 4
- Question: How does the Cross-Fundus Attention module compare to other attention mechanisms like self-attention or cross-attention in terms of computational efficiency and diagnostic accuracy?
- Basis in paper: [explicit] The authors compare CFT to models with self-attention and other fusion methods, but don't provide a detailed comparison of different attention mechanisms
- Why unresolved: While the paper shows CFA outperforms alternatives, it doesn't analyze the trade-offs between different attention mechanisms in depth
- What evidence would resolve it: Comprehensive benchmarking of CFA against various attention mechanisms measuring both accuracy and computational cost

## Limitations
- Clinical dataset size (1,713 paired images) represents moderate-scale evaluation that may not generalize to larger or more diverse populations
- Model was specifically trained and validated only for DR grading, leaving generalizability to other retinal conditions untested
- Cross-attention mechanism assumes consistent modality-specific lesion visibility patterns across all patients, which may not hold for all pathological presentations

## Confidence

- **High confidence**: The architectural design and implementation of the dual-stream transformer with cross-attention is technically sound and well-documented
- **Medium confidence**: The performance claims (84.44% Kappa, 73.47% accuracy) are reliable within the evaluated dataset but may not generalize to broader clinical populations
- **Medium confidence**: The interpretation that cataract cases benefit most from multi-modal fusion is supported by the data but requires further validation across different cataract severities

## Next Checks

1. Conduct external validation on an independent clinical dataset with different demographic characteristics to assess generalizability of the performance claims
2. Perform systematic ablation studies varying the λ parameter across a wider range (0.3-0.7) to confirm the robustness of the 0.6 optimal value
3. Evaluate model performance specifically on cataract severity subgroups to quantify the benefit of IFP modality across different cataract grades