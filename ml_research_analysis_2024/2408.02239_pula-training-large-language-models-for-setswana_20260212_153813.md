---
ver: rpa2
title: 'Pula: Training Large Language Models for Setswana'
arxiv_id: '2408.02239'
source_url: https://arxiv.org/abs/2408.02239
tags:
- setswana
- data
- language
- wang
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pula is a series of bilingual language models for Setswana and
  English. The work introduces the Marothodi corpus (74M tokens) and the Medupi instruction-tuning
  dataset, along with new Setswana benchmarks.
---

# Pula: Training Large Language Models for Setswana

## Quick Facts
- arXiv ID: 2408.02239
- Source URL: https://arxiv.org/abs/2408.02239
- Reference count: 28
- Pula models (1B, 3B, 8B, 14B) outperform GPT-4o and Gemini 1.5 Pro on English-Setswana translation and achieve state-of-the-art Setswana reasoning performance for their size

## Executive Summary
Pula is a series of bilingual language models for Setswana and English that address the critical shortage of high-quality Setswana language models. The work introduces the Marothodi corpus (74M tokens) and the Medupi instruction-tuning dataset, along with new Setswana benchmarks. Pula models demonstrate superior performance on translation tasks and reasoning benchmarks compared to larger commercial models, establishing new state-of-the-art results for Setswana language processing.

## Method Summary
Pula models are trained using LoRA/QLoRA fine-tuning on Llama 3.1 and Qwen 2.5 base models, with a unified training approach that combines Marothodi raw text corpus and Medupi instruction-tuning dataset. The training uses 8 H100 GPUs for 3 epochs with DeepSpeed and ZeRO Stage 3 optimization. Synthetic data generation using GPT-4o, combined with multiple translation models (NLLB-200, Gemini 1.5 Pro, Llama 3.1 405B), creates the Medupi dataset. The unified approach trains on both raw and instruction data simultaneously rather than using separate pre-training and fine-tuning stages.

## Key Results
- Pula 8B and Pula 14B outperform GPT-4o and Gemini 1.5 Pro on English-Setswana translation tasks
- Achieves state-of-the-art Setswana reasoning performance for their size on MMLU-tsn and GSM8K-tsn benchmarks
- Establishes new benchmarks for Setswana language understanding and instruction following

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data generation using GPT-4o enables high-quality Setswana text without requiring extensive human-labeled datasets
- Mechanism: GPT-4o generates diverse Setswana synopses from FineWeb seeds, producing novel text samples covering varied topics and styles
- Core assumption: GPT-4o's generation quality is high enough that synthetic samples are fluent and representative of Setswana language patterns
- Evidence anchors: [abstract] mentions synthetic LLM-generated text from GPT-4o; [section 3.2] details prompt engineering and filtering process
- Break condition: If synthetic samples drift from Setswana norms or GPT-4o's output quality degrades, the benefit diminishes

### Mechanism 2
- Claim: Combining raw text and instruction-tuning data in a single unified training mixture improves multilingual generalization and avoids catastrophic forgetting
- Mechanism: Pula trains on a blended dataset of Marothodi (raw Setswana/English) and Medupi (translated and augmented instruction data) in one stage
- Core assumption: The model can learn both language modeling and instruction-following patterns simultaneously without over-fitting
- Evidence anchors: [abstract] mentions outperforming GPT-4o on translation; [section 4] explains unified data mixture approach
- Break condition: If mixture ratio skews too heavily toward instruction data, raw language modeling capability may degrade

### Mechanism 3
- Claim: Translation-based data augmentation using multiple models diversifies instruction datasets and increases reasoning coverage without manual annotation
- Mechanism: Multiple translation models (NLLB-200, GPT-4o, Gemini 1.5 Pro, Llama 3.1 405B) convert English instruction datasets to Setswana with filtering for quality
- Core assumption: Diversity of multiple translation models compensates for individual model errors and improves overall dataset quality
- Evidence anchors: [abstract] mentions translated corpora and multiple models; [section 3.2] details filtering steps and rationale
- Break condition: If translationese or bias accumulates, or if certain models systematically underperform, instruction-following capability may degrade

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA) and QLoRA
  - Why needed here: Enables efficient fine-tuning of large models on limited hardware while reducing memory footprint and risk of overfitting
  - Quick check question: How does LoRA reduce the number of trainable parameters compared to full fine-tuning?

- Concept: Translationese and its impact on benchmark validity
  - Why needed here: The study relies on machine-translated benchmarks (MMLU-tsn, GSM8K-tsn) which may introduce artifacts that affect evaluation accuracy
  - Quick check question: What is "translationese" and why can it bias the results of translated benchmarks?

- Concept: Catastrophic forgetting and data balancing
  - Why needed here: Pula trains on both raw and instruction data; imbalanced datasets can cause the model to forget previously learned skills
  - Quick check question: How can training on a highly imbalanced dataset lead to catastrophic forgetting in LLMs?

## Architecture Onboarding

- Component map: Pre-trained base (Llama 3.2, Qwen 2.5) -> LoRA/QLoRA adapters -> Unified dataset (Marothodi + Medupi) -> DeepSpeed training -> Evaluation (BLEU/CHRF, MMLU/GSM8K, Belebele)

- Critical path:
  1. Load base model and attach LoRA/QLoRA adapters
  2. Prepare unified dataset (Marothodi + Medupi)
  3. Configure DeepSpeed training (batch size, LR, warmup)
  4. Train for 3 epochs with cosine LR schedule
  5. Evaluate on translation and reasoning benchmarks
  6. Release model weights and data

- Design tradeoffs:
  - LoRA vs full fine-tuning: memory vs flexibility
  - Single-stage vs two-stage training: efficiency vs potential specialization
  - Synthetic vs human-annotated data: scalability vs guaranteed quality

- Failure signatures:
  - Translation performance drops sharply → data imbalance or translation quality issues
  - Reasoning scores flat across model sizes → insufficient reasoning data or overfitting
  - GPU OOM during training → LoRA rank or batch size too high

- First 3 experiments:
  1. Train Pula-1B on Marothodi only (no instruction data) to establish baseline language modeling
  2. Train Pula-3B on Medupi only (no raw text) to test instruction-following capability in isolation
  3. Train Pula-8B on full mixture (Marothodi + Medupi) with different LoRA ranks (32 vs 64) to find optimal tradeoff

## Open Questions the Paper Calls Out

- Question: How do cultural and dialectal variations in Setswana affect the performance and generalization of Pula models across different regions and communities?
  - Basis in paper: [inferred] The authors note that certain cultural and contextual elements may be underrepresented in their corpora and that dialectical variations may be comparatively sparse
  - Why unresolved: The study focuses on data quantity and translation accuracy but does not evaluate regional dialectal differences or cultural context handling
  - What evidence would resolve it: Performance evaluations across different Setswana dialects, user studies with native speakers from diverse regions

- Question: What is the long-term impact of using machine-translated instruction data on Pula's ability to follow complex, nuanced instructions in Setswana?
  - Basis in paper: [explicit] The authors acknowledge that machine translation models used to create Medupi may introduce errors, biases, and "translationese"
  - Why unresolved: While the paper discusses translation quality concerns, it doesn't provide long-term studies on how these translation artifacts affect instruction-following capabilities
  - What evidence would resolve it: Comparative studies of Pula performance on human-translated vs machine-translated instruction sets, longitudinal studies on instruction-following accuracy

- Question: How would incorporating additional African languages beyond English impact Pula's multilingual capabilities and cross-lingual transfer performance?
  - Basis in paper: [inferred] The authors mention that many African languages are underrepresented in current NLP research and that Pula focuses primarily on Setswana-English bilingual capabilities
  - Why unresolved: The study deliberately limits scope to Setswana-English to reduce computational requirements, but doesn't explore how adding other African languages might affect overall model performance
  - What evidence would resolve it: Experiments training Pula variants with additional African languages, comparative analysis of cross-lingual transfer scores

## Limitations

- Synthetic data quality uncertainty: No quantitative evaluation of generated text fluency, semantic coherence, or Setswana language pattern adherence
- Translationese artifacts: Machine-translated benchmarks may not fully capture genuine Setswana reasoning capabilities due to translation quality variations
- Data distribution verification: Limited evidence that the claimed balanced approach between raw and instruction data is actually implemented

## Confidence

**High Confidence**: Pula models achieve state-of-the-art Setswana reasoning performance for their size on established benchmarks
**Medium Confidence**: The synthetic data generation approach produces sufficient quality Setswana text for effective instruction-tuning
**Medium Confidence**: The unified training approach combining raw and instruction data achieves better performance than separate pre-training and fine-tuning stages

## Next Checks

1. **Synthetic Data Quality Audit**: Sample and evaluate 100-200 synthetic Setswana texts from the GPT-4o generation pipeline using native Setswana speakers to assess fluency, coherence, and cultural appropriateness

2. **Translationese Impact Analysis**: Conduct parallel evaluations of Pula models on both original English reasoning benchmarks and their Setswana translations to measure performance degradation

3. **Data Distribution Analysis**: Analyze the actual token distribution between Marothodi raw text and Medupi instruction data in the final training mixture to verify balanced implementation