---
ver: rpa2
title: Investigating the Effect of Network Pruning on Performance and Interpretability
arxiv_id: '2409.19727'
source_url: https://arxiv.org/abs/2409.19727
tags:
- pruning
- accuracy
- interpretability
- network
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the impact of different pruning techniques\
  \ on the performance and interpretability of GoogLeNet using ImageNet validation\
  \ data. The authors apply three pruning methods\u2014unstructured, structured, and\
  \ connection sparsity (pruning input channels)\u2014with both iterative and one-shot\
  \ retraining strategies."
---

# Investigating the Effect of Network Pruning on Performance and Interpretability

## Quick Facts
- arXiv ID: 2409.19727
- Source URL: https://arxiv.org/abs/2409.19727
- Authors: Jonathan von Rad; Florian Seuffert
- Reference count: 17
- Primary result: Connection sparsity pruning achieves 80% parameter reduction while increasing Top-1 accuracy by 0.2% after retraining

## Executive Summary
This study investigates how different pruning techniques affect both performance and interpretability of GoogLeNet on ImageNet validation data. The authors compare unstructured, structured, and connection sparsity pruning methods using both iterative and one-shot retraining strategies. Results show connection sparsity pruning achieves remarkable performance, allowing significant parameter reduction while improving accuracy. The study also reveals that the Mechanistic Interpretability Score (MIS) does not correlate with pruning rate or intuitive notions of interpretability, as networks with extremely low accuracy can still achieve high MIS scores.

## Method Summary
The authors applied three pruning methods - unstructured, structured, and connection sparsity (pruning input channels) - to GoogLeNet using ImageNet validation data. Each method was tested with both iterative and one-shot retraining strategies. Training used SGD optimizer with ExponentialLR scheduler, learning rate 0.01, and minimum 50 epochs. Performance was measured by Top-1 accuracy while interpretability was assessed using the Mechanistic Interpretability Score (MIS).

## Key Results
- Connection sparsity pruning achieved 80% parameter reduction with 0.2% accuracy increase after retraining
- Iterative pruning slightly outperformed one-shot approaches but required significantly more computational resources
- No significant relationship found between pruning rate and interpretability using MIS metric
- Networks with extremely low accuracy (0.1%) still achieved high MIS scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Connection Sparsity pruning achieves higher accuracy than unstructured pruning by preserving network structure while reducing parameters.
- Mechanism: Connection Sparsity prunes input channels instead of weights or output channels, maintaining connectivity patterns that allow the network to adapt better during retraining.
- Core assumption: Input channel pruning preserves more meaningful signal paths than random weight pruning.
- Evidence anchors:
  - [abstract]: "Connection Sparsity pruning achieves remarkable performance, allowing 80% parameter reduction while increasing Top-1 accuracy by 0.2% after retraining"
  - [section]: "Our results show that Connection Sparsity pruning outperformed both Unstructured and Structured pruning methods in preserving accuracy while significantly reducing the network size"
  - [corpus]: Weak - related papers focus on different pruning strategies without directly supporting input channel pruning advantage
- Break condition: If input channels contain redundant information not essential for classification, pruning them would degrade performance regardless of preservation strategy.

### Mechanism 2
- Claim: Iterative pruning with sufficient retraining epochs yields better performance than one-shot pruning.
- Mechanism: Gradual weight reduction allows the network to continuously adapt its internal representations, preventing catastrophic forgetting.
- Core assumption: Network weights can adjust incrementally without losing critical learned features.
- Evidence anchors:
  - [abstract]: "Iterative pruning slightly outperformed one-shot approaches but required significantly more computational resources"
  - [section]: "Our results align with the findings of Frankle and Carbin [15], who proposed in the Lottery Ticket Hypothesis that iterative pruning is more effective than one-shot pruning, particularly in maintaining performance after significant pruning"
  - [corpus]: Strong - Lottery Ticket Hypothesis provides theoretical foundation for iterative approach
- Break condition: If retraining epochs are insufficient, iterative pruning may not show advantage over one-shot approaches.

### Mechanism 3
- Claim: MIS metric does not correlate with pruning rate or actual model interpretability.
- Mechanism: MIS measures perceptual similarity between filter explanations and queries, which may not capture decision-making quality.
- Core assumption: High perceptual similarity equals high interpretability in practical terms.
- Evidence anchors:
  - [abstract]: "no significant relationship was found between pruning rate and interpretability" and "networks with extremely low accuracy (0.1%) still achieved high MIS scores"
  - [section]: "Our experiments demonstrated that there is no significant relationship between interpretability and pruning rate when using the MIS as a measure"
  - [corpus]: Weak - related papers focus on pruning performance rather than interpretability metrics
- Break condition: If MIS methodology changes to incorporate decision accuracy, correlation might emerge.

## Foundational Learning

- Concept: Neural network pruning fundamentals
  - Why needed here: Understanding how different pruning strategies affect model structure and performance is central to the study
  - Quick check question: What distinguishes unstructured pruning from structured pruning at the weight/channel level?

- Concept: Mechanistic Interpretability Score methodology
  - Why needed here: The study uses MIS as the primary interpretability metric, requiring understanding of its computation and limitations
  - Quick check question: How does MIS calculate perceptual similarity between explanations and queries?

- Concept: GoogLeNet architecture specifics
  - Why needed here: Pruning effects vary significantly across architectures; knowing GoogLeNet's Inception modules is crucial
  - Quick check question: Which GoogLeNet components are most affected by input channel pruning versus output channel pruning?

## Architecture Onboarding

- Component map: GoogLeNet with pruning layers - Inception modules containing convolutional layers, pooling layers, and fully connected layers at the end. Pruning operations applied at different granularities: unstructured (individual weights), structured (entire filters), and connection sparsity (input channels).
- Critical path: Data flows through successive Inception modules, with pruning decisions made based on L1 norm magnitude. Retraining follows pruning with SGD optimizer, exponential learning rate scheduler, and 50 epochs minimum.
- Design tradeoffs: Unstructured pruning offers maximum compression but irregular sparsity patterns; structured pruning provides hardware efficiency but less compression; connection sparsity balances both but requires careful channel selection.
- Failure signatures: Accuracy degradation beyond expected levels, inconsistent MIS scores across similar pruning rates, computational inefficiency in iterative approaches.
- First 3 experiments:
  1. Apply 50% unstructured pruning without retraining
  2. Apply 80% connection sparsity pruning with iterative retraining
  3. Compare MIS scores across pruning rates for softmax layer units

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the MIS metric truly capture interpretability in a way that aligns with human understanding of decision-making processes?
- Basis in paper: [explicit] The authors note that networks with extremely low accuracy (0.1%) can still achieve high MIS scores, suggesting the metric may not align with intuitive notions of interpretability, such as understanding the basis of correct decisions.
- Why unresolved: The paper demonstrates a disconnect between MIS scores and intuitive interpretability, but doesn't explore alternative metrics or propose a solution.
- What evidence would resolve it: Development and validation of a new interpretability metric that correlates with human judgment of interpretability and accounts for decision-making quality, followed by empirical testing on pruned networks.

### Open Question 2
- Question: What is the optimal balance between pruning rate and retraining epochs for different pruning strategies to maximize both performance and computational efficiency?
- Basis in paper: [explicit] The authors found that iterative pruning with 50 epochs per retraining yielded slightly better accuracy than one-shot pruning, but at significant computational cost. Connection Sparsity pruning achieved 80% parameter reduction with 0.2% accuracy increase, but the optimal balance wasn't systematically explored.
- Why unresolved: The paper tested various hyperparameters but didn't conduct a comprehensive analysis of the trade-off between pruning effectiveness and computational resources across different pruning methods.
- What evidence would resolve it: Systematic experiments varying pruning rates, retraining epochs, and strategies across multiple network architectures, measuring both final accuracy and total computational cost.

### Open Question 3
- Question: Why does the MIS score for the softmax layer consistently show low values across all models, contrary to expectations?
- Basis in paper: [explicit] The authors found no significant correlation between class-wise accuracy and MIS scores for softmax layer units, which was unexpected given the methodology behind MIS.
- Why unresolved: The paper identifies this anomaly but doesn't investigate potential causes or implications for using MIS as an interpretability measure.
- What evidence would resolve it: Analysis of the relationship between softmax layer activations, class predictions, and MIS scores across different network architectures and pruning rates, potentially revealing methodological limitations of MIS or new insights about softmax layer interpretability.

## Limitations

- The Mechanistic Interpretability Score (MIS) methodology remains underspecified, making it difficult to assess whether the interpretability findings are robust or artifact-driven.
- The study shows that MIS can be high even for networks with near-zero accuracy (0.1%), suggesting the metric may not capture practical interpretability.
- Evidence supporting input channel pruning advantages is weak, relying primarily on the study's own results rather than external validation.

## Confidence

- Medium confidence in performance findings regarding connection sparsity pruning achieving 80% parameter reduction with 0.2% accuracy improvement, as this is directly supported by quantitative results.
- Medium confidence in iterative pruning advantage claim, which aligns with Lottery Ticket Hypothesis literature but requires careful hyperparameter tuning.
- Low confidence in interpretability findings, given the metric's apparent disconnect from practical interpretability and lack of external validation.

## Next Checks

1. Replicate the MIS metric calculation independently to verify that high scores can indeed occur for near-random networks.
2. Test whether alternative interpretability metrics (such as feature visualization consistency or decision boundary analysis) show different relationships with pruning rate.
3. Conduct ablation studies varying retraining epochs to determine the minimum requirements for iterative pruning benefits.