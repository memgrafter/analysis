---
ver: rpa2
title: 'SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert
  Grading and Automatic Grading'
arxiv_id: '2406.10421'
source_url: https://arxiv.org/abs/2406.10421
tags:
- llms
- questions
- grading
- exam
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SciEx, a benchmark of university-level computer
  science exams to evaluate large language models (LLMs) on scientific tasks. Unlike
  existing benchmarks limited to multiple-choice questions, SciEx features freeform,
  multimodal (text and image), and multilingual (English and German) questions, providing
  a more realistic assessment of LLMs' capabilities.
---

# SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading

## Quick Facts
- arXiv ID: 2406.10421
- Source URL: https://arxiv.org/abs/2406.10421
- Reference count: 35
- Current LLMs achieve only 59.4% average score on university-level computer science exams

## Executive Summary
This paper introduces SciEx, a benchmark of university-level computer science exams designed to evaluate large language models (LLMs) on scientific tasks. Unlike existing benchmarks limited to multiple-choice questions, SciEx features freeform, multimodal (text and image), and multilingual (English and German) questions, providing a more realistic assessment of LLMs' capabilities. The authors evaluate seven state-of-the-art LLMs using expert grading and propose an automatic grading scheme using LLM-as-a-judge. Results show that while current LLMs struggle with the benchmark, achieving only 59.4% on average, they perform well as graders, achieving 0.948 Pearson correlation with expert grading.

## Method Summary
The study collects university computer science exams from Karlsruhe Institute of Technology in JSON format, including multiple choice and open-ended questions with images. Seven LLMs generate answers for each question using tailored prompts. Expert lecturers grade the LLM outputs using the same criteria they apply to students, creating gold-standard assessment. The authors then implement automatic grading using LLM-as-a-judge with few-shot examples and reference answers, validating correlation with expert grading on a subset of 15 questions.

## Key Results
- Current LLMs achieve only 59.4% average score on SciEx, struggling particularly with math-related questions
- LLM-as-a-judge achieves 0.948 Pearson correlation with expert grading, demonstrating strong potential for scalable evaluation
- Stronger models like Claude and GPT-4V perform better on harder questions, suggesting human-assigned difficulty levels don't always align with LLM capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert grading provides highly reliable evaluation of LLM performance on freeform scientific exams.
- Mechanism: Lecturers who designed the exam questions grade LLM outputs using the same criteria they use for students, creating gold-standard assessment.
- Core assumption: Expert graders can distinguish between correct and incorrect answers in freeform scientific content.
- Evidence anchors:
  - [abstract] "Therefore, we make use of expert grading, i.e., having the lecturers grade the LLM output the same way they would grade student answers."
  - [section] "Expert grading by lecturers also provides an opportunity to compare LLMs' performance to university student performance in a similar setting."
- Break condition: If grading standards are inconsistent between lecturers or if expert graders cannot properly evaluate LLM outputs due to format differences.

### Mechanism 2
- Claim: LLM-as-a-judge achieves high correlation with expert grading while enabling scalable evaluation.
- Mechanism: Stronger LLMs (Mixtral, Llama3, GPT-4V) grade LLM answers using few-shot examples and reference answers to mimic expert judgment.
- Core assumption: LLMs can learn grading patterns from examples and apply them consistently to new answers.
- Evidence anchors:
  - [abstract] "Our experiments show that, although they do not perform perfectly on solving the exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with expert grading."
  - [section] "GPT-4V is the strongest grader, followed by Llama3 and Mixtral. This shows that proprietary LLMs are still stronger as judges."
- Break condition: If LLM graders over-rely on example grades or if reference answers are missing, correlation with expert grading drops significantly.

### Mechanism 3
- Claim: Exam difficulty perception differs between humans and LLMs, with stronger models excelling at harder questions.
- Mechanism: Difficulty levels assigned by lecturers don't align with LLM performance patterns, particularly for stronger models like Claude and GPT-4V.
- Core assumption: Question difficulty metrics based on human experience don't translate directly to LLM capabilities.
- Evidence anchors:
  - [section] "The stronger LLMs, i.e., Claude and GPT-4V, perform better on harder questions. This is an indication that difficulty levels from human perspective do not always align with LLMs' perspective."
  - [section] "Looking at each difficulty level independently, we observe that the ranking of the LLMs changes across different levels."
- Break condition: If difficulty levels are based on objective metrics (e.g., required reasoning depth) rather than human perception, the misalignment may disappear.

## Foundational Learning

- Concept: Freeform question evaluation
  - Why needed here: Unlike multiple-choice benchmarks, SciEx requires evaluating open-ended scientific answers where traditional exact matching fails.
  - Quick check question: Why can't we use BLEU or ROUGE scores for evaluating LLM answers on SciEx exams?

- Concept: Multimodal processing limitations
  - Why needed here: SciEx includes both text-only and image-related questions, but most LLMs can only process text, requiring different handling strategies.
  - Quick check question: How does the study handle image-related questions for text-only LLMs?

- Concept: Language model as judge methodology
  - Why needed here: Automatic grading is necessary for scalability, but requires careful prompt engineering with few-shot examples and reference answers.
  - Quick check question: What are the three different settings for selecting example questions when using LLM-as-a-judge?

## Architecture Onboarding

- Component map: Exam collection -> JSON formatting -> LLM answer generation -> Expert grading -> Automatic grading -> Analysis pipeline
- Critical path: Expert grading must complete before automatic grading can be validated against gold standard
- Design tradeoffs: Expert grading provides reliability but doesn't scale; automatic grading scales but requires validation against experts
- Failure signatures: Low Pearson correlation between LLM graders and expert grading indicates prompt engineering issues or model limitations
- First 3 experiments:
  1. Run all LLMs on text-only English questions and compare against student averages to establish baseline performance
  2. Test LLM-as-a-judge with different shot settings (same question, same exam, different exam) to find optimal configuration
  3. Compare performance differences between text-only and image-related questions to identify multimodal processing gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance gap between LLM and student performance on math-related questions be addressed, given that current models struggle with basic math tasks and mathematical proofs?
- Basis in paper: Explicit
- Why unresolved: The paper highlights that LLMs, including stronger models like GPT-4V and Claude, fail on math-related questions and struggle with mathematical proofs, even on basic tasks. This indicates a fundamental limitation in LLMs' reasoning and numerical abilities.
- What evidence would resolve it: Developing LLMs with enhanced mathematical reasoning capabilities, such as integrating symbolic math engines or specialized training on mathematical proofs, could demonstrate improved performance on math-related tasks.

### Open Question 2
- Question: What strategies can improve the performance of LLM-as-a-judge for multimodal questions, particularly those involving images, given that current models show lower correlation with human grading in these cases?
- Basis in paper: Inferred
- Why unresolved: The paper finds that while GPT-4V performs well on grading text-only questions, its performance on image-related questions is less consistent, with lower Pearson correlation to expert grading. This suggests challenges in multimodal evaluation.
- What evidence would resolve it: Testing LLM-as-a-judge with enhanced image-processing capabilities, such as fine-tuning on multimodal datasets or using hybrid models that combine text and vision expertise, could improve grading accuracy for image-related tasks.

### Open Question 3
- Question: How can the biases in LLM-as-a-judge, such as tendency to copy grades or give full points, be mitigated to ensure fair and reliable grading across diverse question types and difficulty levels?
- Basis in paper: Explicit
- Why unresolved: The paper identifies issues like Mixtral grader giving full points frequently and GPT-4V grader copying grades from examples, which can skew grading results. These biases need to be addressed for scalable and fair evaluation.
- What evidence would resolve it: Implementing calibration techniques, such as adversarial training to reduce grade-copying behavior or dynamic shot selection to balance example relevance, could demonstrate more consistent and unbiased grading performance.

## Limitations

- Expert grading process doesn't scale well and may introduce grader-specific biases
- Automatic grading validation was only performed on a subset of 15 questions rather than the full benchmark
- Multimodal aspect is limited since most evaluated LLMs can only process text

## Confidence

- **High confidence**: The finding that current LLMs struggle with university-level scientific exams, achieving only 59.4% average score. This is directly supported by the expert grading results.
- **Medium confidence**: The automatic grading correlation of 0.948 with expert grading, as this was validated on a limited question subset and may not hold across the entire benchmark.
- **Medium confidence**: The observation that stronger models like Claude and GPT-4V perform better on harder questions, though this finding is based on human-assigned difficulty levels which may not align with LLM capabilities.

## Next Checks

1. **Expand automatic grading validation**: Test the LLM-as-a-judge approach on the complete set of SciEx questions to verify that the 0.948 Pearson correlation holds across all exam questions, not just the 15-question subset used for initial validation.

2. **Cross-grader reliability assessment**: Have multiple expert graders evaluate the same LLM outputs independently to quantify inter-rater reliability and establish confidence intervals for the expert grading scores.

3. **Multimodal capability gap analysis**: Systematically compare text-only LLM performance on image-related questions versus text-only questions to quantify the exact performance penalty when models cannot access visual information, controlling for question difficulty and topic.