---
ver: rpa2
title: 'Identifying Factual Inconsistencies in Summaries: Grounding LLM Inference
  via Task Taxonomy'
arxiv_id: '2402.12821'
source_url: https://arxiv.org/abs/2402.12821
tags:
- summary
- reasoning
- factual
- text
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces three zero-shot paradigms using large language
  models (LLMs) to identify factual inconsistencies in summaries. The paradigms, Summ-NLI,
  Sent-NLI, and QG-QA, are evaluated on five diverse datasets and demonstrate that
  LLMs can effectively detect factual errors when properly prompted.
---

# Identifying Factual Inconsistencies in Summaries: Grounding LLM Inference via Task Taxonomy

## Quick Facts
- arXiv ID: 2402.12821
- Source URL: https://arxiv.org/abs/2402.12821
- Reference count: 18
- Three zero-shot LLM paradigms outperform trained NLI baselines by 2.8% macro-average for factual consistency detection in summaries.

## Executive Summary
This paper introduces three zero-shot paradigms using large language models (LLMs) to identify factual inconsistencies in summaries. The paradigms, Summ-NLI, Sent-NLI, and QG-QA, are evaluated on five diverse datasets and demonstrate that LLMs can effectively detect factual errors when properly prompted. The study shows that the appropriate paradigm design is crucial for performance, with Sent-NLI and QG-QA outperforming specialized non-LLM baselines and the basic Summ-NLI approach. Additionally, the authors propose training strategies to distill smaller, open-source LLM models that score entire summaries at once, combining the efficiency of Summ-NLI with the accuracy of Sent-NLI. These distilled models achieve state-of-the-art results, surpassing both the zero-shot approaches and specialized baselines, while being more practical for real-world use.

## Method Summary
The paper proposes three zero-shot LLM paradigms for factual inconsistency detection in summaries: Summ-NLI scores the entire summary against the source document, Sent-NLI scores smaller windows of the summary, and QG-QA generates and verifies entities. The authors also explore knowledge distillation to train smaller, open-source models (e.g., Llama-2) using reasoning traces from larger LLMs. The distilled models are trained with reasoning prompts (T-w-R) but infer without reasoning (I-wo-R) for faster inference. Evaluation is conducted on five datasets with varying summary lengths and error types, using balanced accuracy for binary classification and Pearson correlation for score-based datasets.

## Key Results
- Zero-shot LLM paradigms outperform trained NLI baselines by up to 2.8% macro-average when task taxonomy is explicitly included in prompts.
- Window-based paradigms (Sent-NLI, QG-QA) outperform summary-level scoring (Summ-NLI) for longer summaries by preventing loss of focus on scattered errors.
- Distilled smaller LLM models achieve state-of-the-art results, surpassing both zero-shot approaches and specialized baselines, while enabling faster inference.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot LLM paradigms with explicit task taxonomy outperform trained NLI baselines by up to 2.8% macro-average.
- Mechanism: Explicit error-type taxonomy in prompts guides LLM reasoning, narrowing solution space and improving factual consistency detection.
- Core assumption: LLM zero-shot reasoning is strong enough to replace or match trained models when given structured task guidance.
- Evidence anchors:
  - [abstract]: "zero-shot LLM inference could benefit from the explicit solution space depicted by the error type taxonomy, and achieves state-of-the-art performance overall, surpassing specialized non-LLM baselines, as well as recent LLM baselines."
  - [section 3]: "LLM itself is capable to resolve this task train-free under the proper paradigm design, surpassing strong trained baselines by 2.8% on average."
  - [corpus]: FMR score 0.578 on average for related factual consistency papers, suggesting active research but limited citation impact so far.
- Break condition: Taxonomy is incomplete or mismatched to dataset error types; LLM fails to follow structured reasoning instructions.

### Mechanism 2
- Claim: Window-based paradigms (Sent-NLI, QG-QA) outperform summary-level scoring (Summ-NLI) for longer summaries.
- Mechanism: Local context evaluation prevents LLM from overlooking scattered errors; aggregate window results give more accurate summary judgment.
- Core assumption: LLM reasoning degrades on long inputs due to context truncation or loss of focus.
- Evidence anchors:
  - [section 3.5]: "Summ-NLI is shown to suffer degradation on longer documents or summaries, due to its length-agnostic scoring mechanism."
  - [section 3.4]: "Sent-NLI and QG-QA obtain similar results, and both outperform the four non-LLM baselines by up to 7.6%."
  - [corpus]: 0.535-0.592 FMR range for related papers suggests windowing is an active improvement direction.
- Break condition: Window size too small (misses inter-sentence dependencies) or too large (defeats purpose of local scoring).

### Mechanism 3
- Claim: Distilling smaller open-source models with reasoning data from larger LLMs achieves both efficiency and efficacy.
- Mechanism: Training smaller models on prompts with and without reasoning bridges the gap between zero-shot accuracy and inference speed.
- Core assumption: Knowledge distillation from reasoning traces can preserve model accuracy while enabling fast inference.
- Evidence anchors:
  - [section 4.2]: "T-w-R + I-wo-R: the model receives reasoning in training, but directly yields classification labels during inference (faster inference than I-w-R)."
  - [section 4.2]: "T-w-R + I-wo-R... surpasses its counterpart (T-wo-R + I-wo-R) by 2% robustly for both ID and OOD."
  - [corpus]: Related papers have low citation counts, suggesting distillation strategies are novel and under-validated.
- Break condition: Reasoning traces are noisy or sparse; smaller model cannot generalize beyond training domains.

## Foundational Learning

- Concept: Zero-shot prompting
  - Why needed here: Enables direct LLM evaluation without labeled data; critical for rapid prototyping.
  - Quick check question: What prompt structure forces LLM to generate reasoning before final label?
- Concept: Natural Language Inference (NLI)
  - Why needed here: Provides theoretical basis for consistency scoring; NLI models are strong baselines.
  - Quick check question: How does NLI differ from pure QA-based factual verification?
- Concept: Knowledge distillation
  - Why needed here: Allows smaller models to inherit reasoning capabilities from large LLMs, improving practical utility.
  - Quick check question: What distinguishes distillation with reasoning vs. standard supervised training?

## Architecture Onboarding

- Component map: Document + Summary → (Summ-NLI / Sent-NLI / QG-QA) → Reasoning → Label → Output; for distillation: LLM reasoning → Smaller model training → Fast scorer.
- Critical path: For zero-shot: document + summary → LLM → reasoning → consistency label; for distilled: prompt generation → smaller LLM → label.
- Design tradeoffs: Zero-shot offers flexibility but high latency; distilled models offer speed but require reasoning data; windowing improves accuracy but increases cost.
- Failure signatures: Low recall on specific error types (e.g., predicate errors), high variance across runs, accuracy drops on long summaries.
- First 3 experiments:
  1. Run all three zero-shot paradigms on FRANK dataset with ChatGPT to establish baseline performance.
  2. Compare zero-shot accuracy vs. length of summary/document to identify scaling issues.
  3. Train distilled Llama-2 7B model on FRANK + DiaSumFact; evaluate on out-of-domain FRANK to test transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of zero-shot LLM paradigms (Summ-NLI, Sent-NLI, QG-QA) compare when evaluated on datasets with different types of factual errors, such as entity errors, coreference errors, predicate errors, and circumstantial errors?
- Basis in paper: [explicit] The paper mentions that the datasets used have different types of factual errors, including entity errors, coreference errors, predicate errors, and circumstantial errors. However, the paper does not provide a detailed comparison of the performance of the zero-shot LLM paradigms on these different types of errors.
- Why unresolved: The paper focuses on the overall performance of the paradigms and does not delve into the specific performance on different types of factual errors.
- What evidence would resolve it: A detailed analysis of the performance of each paradigm on different types of factual errors would provide insights into their strengths and weaknesses.

### Open Question 2
- Question: How do the zero-shot LLM paradigms perform on datasets with varying summary lengths, and what are the implications for their practical application?
- Basis in paper: [explicit] The paper mentions that the summary lengths vary significantly across the datasets used and that the performance of the paradigms can be affected by the length of the summaries.
- Why unresolved: The paper provides some insights into the performance on different summary lengths but does not explore the implications for practical application in detail.
- What evidence would resolve it: An in-depth analysis of the performance on datasets with varying summary lengths and a discussion of the practical implications would be valuable.

### Open Question 3
- Question: What are the limitations of the zero-shot LLM paradigms, and how can they be addressed in future research?
- Basis in paper: [explicit] The paper mentions that the zero-shot paradigms have limitations, such as imperfect instruction following and inconsistent answers on ambiguous cases.
- Why unresolved: The paper provides a brief overview of the limitations but does not explore potential solutions or future research directions in detail.
- What evidence would resolve it: A comprehensive discussion of the limitations and potential solutions, along with suggestions for future research, would provide a roadmap for improving the paradigms.

## Limitations
- Zero-shot LLM paradigms are highly sensitive to prompt wording and model behavior; small changes in task taxonomy or Chain-of-Thought instructions could shift performance significantly.
- The superiority of window-based approaches (Sent-NLI, QG-QA) depends on dataset characteristics—datasets with long-range dependencies may still require holistic evaluation.
- The distilled model strategy is validated only on a narrow set of datasets and model sizes; broader generalization is unproven.

## Confidence
- **High**: Zero-shot LLM paradigms outperform strong supervised NLI baselines when task taxonomy is explicitly included.
- **Medium**: Window-based paradigms improve over summary-level scoring for longer documents, but scaling behavior is not fully characterized.
- **Low**: Distilled open-source models can reliably replace large LLMs for practical deployment, given the small sample of datasets and limited ablation studies.

## Next Checks
1. Perform a prompt ablation study across the three paradigms to quantify sensitivity to taxonomy wording and reasoning instructions.
2. Test window size sensitivity in Sent-NLI/QG-QA on summaries of varying lengths to identify optimal granularity.
3. Evaluate distilled models on out-of-distribution summary tasks (e.g., multi-document summarization) to test robustness.