---
ver: rpa2
title: Graph Bottlenecked Social Recommendation
arxiv_id: '2406.08214'
source_url: https://arxiv.org/abs/2406.08214
tags:
- social
- recommendation
- graph
- gbsr
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GBSR, a graph-based social recommendation framework
  that addresses the issue of noisy social networks. The key idea is to learn a denoised
  social graph structure from an information bottleneck perspective, maximizing mutual
  information with user preferences while minimizing it with the original social graph.
---

# Graph Bottlenecked Social Recommendation

## Quick Facts
- **arXiv ID**: 2406.08214
- **Source URL**: https://arxiv.org/abs/2406.08214
- **Reference count**: 40
- **Key outcome**: Proposes GBSR, a graph-based social recommendation framework that achieves over 17.06%, 10%, and 11.27% improvements in NDCG@20 compared to the strongest baseline across three benchmark datasets.

## Executive Summary
This paper introduces GBSR, a graph-based social recommendation framework that addresses the issue of noisy social networks. The key idea is to learn a denoised social graph structure from an information bottleneck perspective, maximizing mutual information with user preferences while minimizing it with the original social graph. The proposed framework consists of preference-guided social graph refinement and HSIC-based bottleneck learning. Experimental results on three benchmark datasets demonstrate the superiority of GBSR, achieving significant improvements in NDCG@20 compared to the strongest baseline. The method also shows good generality when combined with various backbone models.

## Method Summary
GBSR is a model-agnostic social denoising framework that aims to maximize the mutual information between the denoised social graph and recommendation labels while minimizing it between the denoised social graph and the original one. The framework consists of two main components: a preference-guided social graph refinement module that calculates edge confidence scores based on user preference similarity and applies differentiable edge dropout, and an HSIC-based bottleneck regularization module that measures and minimizes the dependence between denoised and original graph node representations. The method is trained jointly using a combination of BPR loss for recommendation and HSIC regularization for social denoising.

## Key Results
- GBSR achieves over 17.06%, 10%, and 11.27% improvements in NDCG@20 compared to the strongest baseline on Douban-Book, Yelp, and Epinions datasets respectively
- The method shows good generality when combined with various backbone models including LightGCN-S, GraphRec, DiffNet++, and SocialLGN
- GBSR consistently outperforms other social denoising methods such as Rule-based, ESRF, and GDMSR across all three benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The denoised social graph preserves minimal yet sufficient information for accurate recommendations by removing redundant social relations.
- Mechanism: GBSR uses an information bottleneck approach that maximizes mutual information between the denoised social graph (S') and user-item interactions (R), while minimizing mutual information between S' and the original noisy social graph (S).
- Core assumption: Social networks contain redundant or noisy relations that harm recommendation accuracy, and user preferences can guide the identification and removal of these redundant relations.
- Evidence anchors:
  - [abstract] "GBSR is a model-agnostic social denoising framework, that aims to maximize the mutual information between the denoised social graph and recommendation labels, meanwhile minimizing it between the denoised social graph and the original one."
  - [section 3.1] "Specifically, GBSR maximize the mutual information between the denoised social graph structure S′ and interaction matrix R, meanwhile minimizing it between the denoised social graph structure S′ and the original S."
  - [corpus] Weak - the corpus papers focus on different aspects of social recommendation and don't directly address the information bottleneck denoising mechanism.
- Break condition: If the assumption that redundant social relations harm recommendation accuracy is incorrect, or if user preferences cannot effectively guide denoising, the mechanism fails.

### Mechanism 2
- Claim: Preference-guided social denoising effectively identifies which social relations are redundant by using user preference similarity as a proxy.
- Mechanism: GBSR calculates edge confidence scores based on the similarity of user preference representations (learned from user-item interactions), and uses these scores to probabilistically drop edges from the social graph.
- Core assumption: Users with similar preferences are more likely to have genuine social relations, and this similarity can be effectively captured through preference representations.
- Evidence anchors:
  - [section 3.2] "Based on social homogeneity social-connected individuals have more similar behavior similarity, we inject user preference signals into the social denoising process, i.e., users with similar preferences are more likely to have social relations."
  - [section 3.2] "For each observed social relation < a, b >, the link confidence is calculated as: w_ab = g(e_a, e_b)"
  - [corpus] Weak - the corpus papers focus on different social recommendation approaches and don't directly address preference-guided denoising.
- Break condition: If user preference similarity is not a good proxy for genuine social relations, or if the preference representations are poor quality, the mechanism fails.

### Mechanism 3
- Claim: HSIC-based bottleneck regularization effectively approximates the mutual information minimization between the denoised and original social graphs.
- Mechanism: Instead of directly computing the intractable mutual information, GBSR uses Hilbert-Schmidt Independence Criterion (HSIC) to measure dependence between node representations from the denoised and original graphs, and minimizes this measure.
- Core assumption: HSIC is a good statistical approximation of mutual information for this application, and minimizing HSIC effectively reduces redundant information while preserving essential structure.
- Evidence anchors:
  - [section 3.4] "We introduce Hilbert-Schmidt Independence Criterion (HSIC [12]) as the approximation of the minimization of I(R; S′)."
  - [section 3.4] "HSIC serves as a statistical measure of dependency [12], which is formulated as the Hilbert-Schmidt norm, assessing the cross-covariance operator between distributions within the Reproducing Kernel Hilbert Space (RKHS)."
  - [corpus] Weak - the corpus papers don't address HSIC or information bottleneck approaches for social recommendation denoising.
- Break condition: If HSIC is not a good approximation of mutual information for this application, or if the HSIC computation is unstable or ineffective, the mechanism fails.

## Foundational Learning

- Concept: Information Bottleneck Principle
  - Why needed here: GBSR is built entirely on the information bottleneck principle to guide the denoising process, so understanding this principle is essential for understanding the method.
  - Quick check question: What is the key trade-off that the information bottleneck principle seeks to optimize?

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GBSR uses GNNs as the backbone for learning user/item representations from the social and interaction graphs, and understanding GNNs is crucial for implementing and modifying the method.
  - Quick check question: What is the key difference between GCN and GNN message passing?

- Concept: Mutual Information and HSIC
  - Why needed here: GBSR relies on mutual information maximization and HSIC minimization, so understanding these concepts is essential for understanding the optimization objectives.
  - Quick check question: What is the key difference between mutual information and HSIC as measures of dependence?

## Architecture Onboarding

- Component map: Preference-guided denoising module -> HSIC bottleneck regularization -> Graph-based recommender backbone -> Combined training loop
- Critical path: The critical path is the forward pass through the preference-guided denoising, backbone recommender, and HSIC computation, followed by the backward pass to update all parameters.
- Design tradeoffs:
  - Tradeoff between denoising aggressiveness (removing more redundant relations) and recommendation accuracy (preserving useful information)
  - Tradeoff between computational cost (more complex denoising, HSIC computation) and recommendation performance
  - Tradeoff between backbone model complexity and overall system complexity
- Failure signatures:
  - Poor recommendation performance despite denoising (denoising too aggressive or guided by poor preference representations)
  - Unstable training or convergence issues (HSIC computation or regularization causing problems)
  - High computational cost or slow convergence (complex denoising or HSIC computation)
- First 3 experiments:
  1. Verify that GBSR improves over the backbone model alone on a simple dataset (e.g., LightGCN-S on Yelp)
  2. Verify that GBSR improves over other social denoising methods on a simple dataset (e.g., GDMSR on Yelp)
  3. Verify that GBSR generalizes to different backbone models (e.g., GraphRec, DiffNet++, SocialLGN on Yelp)

## Open Questions the Paper Calls Out

- How does the performance of GBSR vary when applied to different types of social networks (e.g., professional networks vs. social media platforms)?
- What is the impact of varying the observation bias parameter (ε) on the performance of GBSR?
- How does the performance of GBSR compare to other social denoising methods that do not rely on the information bottleneck principle?

## Limitations

- The paper doesn't provide ablation studies to isolate the contribution of each component (preference-guided denoising, HSIC bottleneck, and their combination)
- The method's performance on cold-start scenarios is not addressed
- The computational overhead of HSIC computation is not discussed, which could be significant for large-scale social networks

## Confidence

- Confidence in the proposed mechanism is **Medium** due to several limitations including lack of ablation studies and limited empirical validation of the information bottleneck approach
- The claim that user preference similarity effectively identifies genuine social relations has **Medium** confidence
- The superiority of GBSR over baselines is demonstrated with **High** confidence through extensive experiments on three benchmark datasets

## Next Checks

1. **Ablation Study**: Perform an ablation study to quantify the individual contributions of preference-guided denoising and HSIC bottleneck regularization.

2. **Cold-Start Evaluation**: Evaluate GBSR's performance on cold-start scenarios where users have few interactions or social connections.

3. **Scalability Analysis**: Measure the computational overhead of HSIC computation and evaluate GBSR's performance on larger, more realistic social networks.