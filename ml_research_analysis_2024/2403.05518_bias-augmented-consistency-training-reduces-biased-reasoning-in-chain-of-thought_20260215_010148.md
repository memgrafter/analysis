---
ver: rpa2
title: Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought
arxiv_id: '2403.05518'
source_url: https://arxiv.org/abs/2403.05518
tags:
- answer
- reasoning
- biased
- bias
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Chain-of-thought prompting can exhibit biased reasoning, where
  models change their explanations to rationalize answers influenced by prompt features,
  without verbalizing the bias. To address this, the paper introduces bias-augmented
  consistency training (BCT), which fine-tunes models to give consistent reasoning
  across prompts with and without biasing features, using an unsupervised approach
  that does not require ground truth reasoning labels.
---

# Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought

## Quick Facts
- **arXiv ID**: 2403.05518
- **Source URL**: https://arxiv.org/abs/2403.05518
- **Reference count**: 40
- **Primary result**: Bias-augmented consistency training (BCT) reduces biased reasoning in chain-of-thought prompts by 86% on training bias and generalizes to reduce biased reasoning on held-out biases by 37% on average.

## Executive Summary
Chain-of-thought (CoT) prompting can introduce biased reasoning, where models change their explanations to rationalize answers influenced by prompt features without verbalizing the bias. This paper introduces bias-augmented consistency training (BCT), a method that fine-tunes models to give consistent reasoning across prompts with and without biasing features. The approach is unsupervised, requiring no ground truth reasoning labels, and significantly reduces biased reasoning rates while generalizing to unseen biases and tasks.

## Method Summary
BCT works by training models to match their unbiased reasoning (generated from prompts without biasing features) to biased prompts through supervised fine-tuning. The method creates a dataset pairing biased prompts with unbiased chain-of-thought completions, then fine-tunes the model to generate consistent reasoning regardless of prompt context. This approach reduces the model's sensitivity to arbitrary context features that might influence its reasoning.

## Key Results
- BCT reduces biased reasoning by 86% on the training bias (Suggested Answer)
- The method generalizes to reduce biased reasoning on held-out biases by an average of 37%
- BCT reduces coherent biased reasoning from 27.2% to 15.1% without requiring reasoning correctness labels

## Why This Works (Mechanism)

### Mechanism 1
Consistency training forces the model to give the same reasoning across biased and unbiased prompts, thereby reducing the influence of biasing features. By training the model to match its unbiased reasoning to biased prompts, it learns to ignore or downweight biasing text, leading to more consistent behavior. This works if the model can generate coherent unbiased reasoning initially.

### Mechanism 2
The method generalizes to held-out biases because it teaches a general consistency principle rather than memorizing specific bias patterns. Training on one form of bias (Suggested Answer) and seeing generalization to others (e.g., Post Hoc, Wrong Few-Shot) suggests the model learns to be less sensitive to arbitrary context features. This assumes similar underlying mechanisms across different biases.

### Mechanism 3
BCT reduces coherent biased reasoning (internally consistent but supporting wrong answers) without needing correctness labels. By enforcing consistency in reasoning across contexts, the model is less likely to construct elaborate justifications for biased answers, even if those justifications are internally consistent. This assumes coherent biased reasoning is a distinct problem addressable by consistency training.

## Foundational Learning

- **Concept**: Chain-of-thought (CoT) prompting
  - Why needed here: The paper's method relies on models generating step-by-step reasoning, and biased reasoning occurs in this CoT context
  - Quick check question: What is the purpose of CoT prompting, and how might it introduce opportunities for biased reasoning?

- **Concept**: Supervised fine-tuning (SFT)
  - Why needed here: The core training method is SFT, where the model is fine-tuned on biased prompts paired with unbiased reasoning completions
  - Quick check question: How does SFT differ from other fine-tuning methods like RLHF, and why is it suitable for this unsupervised consistency approach?

- **Concept**: Generalization in machine learning
  - Why needed here: The paper's key finding is that training on one bias generalizes to reduce biased reasoning on held-out biases and tasks
  - Quick check question: What factors contribute to a model's ability to generalize from training data to unseen examples, and how might this apply to reducing biased reasoning?

## Architecture Onboarding

- **Component map**: Prompt generation -> Response generation -> Fine-tuning dataset creation -> Supervised fine-tuning -> Evaluation
- **Critical path**: 1) Generate unbiased prompt and get model response 2) Create biased prompt by adding biasing text 3) Pair biased prompt with unbiased response in training data 4) Perform supervised fine-tuning 5) Evaluate on held-out biases and tasks
- **Design tradeoffs**: Using CoT vs. non-CoT responses (CoT gives better bias reduction but requires coherent reasoning); proportion of BCT data vs. instruction-tuning data (more BCT increases bias reduction but may hurt general capabilities); variety of biasing features (more diversity may improve generalization but increase complexity)
- **Failure signatures**: No reduction in BRR after fine-tuning (model not learning consistency objective or biasing features too subtle); overfitting to training bias (reduces BRR on training bias but not held-out biases); degradation in general capabilities (worse at instruction-following or other tasks)
- **First 3 experiments**: 1) Train on Suggested Answer bias and evaluate BRR on same bias to confirm method works 2) Train on Suggested Answer bias and evaluate BRR on held-out biases to test generalization 3) Train with non-CoT responses and evaluate on CoT prompts to see if method transfers across reasoning styles

## Open Questions the Paper Calls Out

### Open Question 1
Does BCT with non-CoT examples generalize to reduce biased reasoning in CoT settings across a wider range of biases and tasks beyond those tested? The paper shows generalization but only tests a limited number of biases and tasks. Testing on a much larger and more diverse set would provide evidence for broader generalization.

### Open Question 2
What is the underlying mechanism that allows BCT to generalize to reduce biased reasoning from held-out biases? The paper observes the generalization effect but does not provide a detailed theoretical explanation. Understanding this mechanism could lead to more effective training methods.

### Open Question 3
Can BCT be combined with other methods to further improve reduction of biased reasoning and explanation faithfulness? The paper demonstrates BCT's effectiveness but does not explore combinations. Experimenting with BCT plus other techniques targeting reasoning correctness or consistency through different mechanisms would determine if combinations are more effective.

## Limitations
- The method assumes models can generate coherent unbiased reasoning to begin with, which may not hold for complex biases or domains
- The effectiveness may vary depending on specific implementation details of prompt generation and fine-tuning not fully specified
- The exact reasons for generalization from training bias to held-out biases remain somewhat speculative

## Confidence
- **High confidence**: The empirical results showing significant reduction in biased reasoning rates for both training and held-out biases
- **Medium confidence**: The generalization mechanism - that training on one bias improves robustness to others by teaching a general consistency principle
- **Medium confidence**: The claim that the method reduces coherent biased reasoning without requiring ground truth reasoning labels

## Next Checks
1. **Ablation study on training data composition**: Vary the proportion of BCT data versus instruction-tuning data (e.g., 25%, 50%, 75%, 100% BCT) to quantify the tradeoff between bias reduction and general capability maintenance

2. **Cross-domain generalization test**: Evaluate BCT-trained models on biases and tasks from completely different domains (e.g., from factual QA to creative writing or code generation) to test the limits of generalization

3. **Human evaluation of reasoning quality**: Conduct a human study to assess whether the model's reasoning after BCT is not only consistent but also more accurate and less likely to be influenced by irrelevant context, particularly for coherent biased reasoning