---
ver: rpa2
title: Large-scale Reinforcement Learning for Diffusion Models
arxiv_id: '2401.12244'
source_url: https://arxiv.org/abs/2401.12244
tags:
- reward
- training
- human
- figure
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a scalable reinforcement learning approach
  for fine-tuning diffusion models to improve human preference alignment, compositional
  skills, and fairness. The method reframes diffusion sampling as a multi-step Markov
  decision process, enabling RL-based optimization with arbitrary reward functions.
---

# Large-scale Reinforcement Learning for Diffusion Models

## Quick Facts
- arXiv ID: 2401.12244
- Source URL: https://arxiv.org/abs/2401.12244
- Authors: Yinan Zhang; Eric Tzeng; Yilun Du; Dmitry Kislyuk
- Reference count: 40
- Key outcome: RL fine-tuning of diffusion models achieves 80.3% human preference rate and improves fairness/compositionality

## Executive Summary
This paper introduces a scalable reinforcement learning approach for fine-tuning diffusion models to improve human preference alignment, compositional skills, and fairness. The method reframes diffusion sampling as a multi-step Markov decision process, enabling RL-based optimization with arbitrary reward functions. Experiments on millions of prompts show the approach outperforms existing methods, generating samples preferred by humans 80.3% of the time and improving skintone diversity and object compositionality. Joint optimization of multiple rewards simultaneously achieves over 80% relative performance of individually fine-tuned models across all tasks, mitigating alignment tax issues common in RLHF.

## Method Summary
The paper reframes diffusion sampling as a multi-step Markov decision process where each denoising step is treated as a sequential decision. The policy is defined as the conditional denoising distribution, and rewards are applied only at the final timestep. The approach uses policy gradient methods with trust region clipping and importance sampling to optimize the diffusion model with respect to arbitrary reward functions. To handle distribution-level rewards, the method approximates them using mini-batch statistics during training. The framework is applied to jointly optimize human preference, fairness (skintone diversity), and compositionality objectives on 1.5 million real user prompts from DiffusionDB.

## Key Results
- Human preference rate of 80.3% for RL-tuned models vs baselines
- Improved skintone diversity and object compositionality metrics
- Joint optimization achieves over 80% relative performance of individually fine-tuned models across all tasks
- Scales to millions of prompts while maintaining training stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reframing diffusion sampling as a multi-step MDP enables scalable RL optimization over arbitrary reward functions.
- Mechanism: The denoising process is decomposed into discrete timesteps where each state transition corresponds to a denoising step. The policy is defined as the conditional denoising distribution pθ(xt-1|xt, c), and rewards are applied only at the final timestep when the clean image is generated.
- Core assumption: The Markov property holds for the diffusion denoising process, meaning each denoising step depends only on the current noisy state and timestep.
- Evidence anchors:
  - [abstract]: "We reframe the iterative denoising procedure of diffusion models as a multi-step Markov decision process (MDP)"
  - [section]: "Following Black et al. [5], we reframe the iterative denoising procedure of diffusion models as a multi-step Markov decision process (MDP)"
  - [corpus]: Weak - no direct corpus evidence found supporting this specific MDP reframing claim
- Break condition: If the denoising process exhibits long-range dependencies that violate the Markov assumption, the RL formulation would fail to capture the true dynamics.

### Mechanism 2
- Claim: Distribution-level reward functions can be effectively approximated using mini-batch statistics during training.
- Mechanism: Instead of computing rewards over the full generative distribution (intractable), rewards are computed on each training minibatch and backpropagated across samples. This allows optimizing for distributional properties like fairness and diversity.
- Core assumption: Mini-batch statistics provide a reasonable approximation of the true generative distribution for the purposes of reward computation.
- Evidence anchors:
  - [section]: "We instead approximate the reward by computing it using empirical samples across mini-batches during the reinforcement learning process"
  - [section]: "it is intractable to compute the reward over the full output distribution of the model, so we compute the reward over individual minibatches"
  - [corpus]: Weak - no direct corpus evidence found supporting mini-batch approximation effectiveness
- Break condition: If the training minibatch is not representative of the true data distribution, the approximated reward would misguide the optimization.

### Mechanism 3
- Claim: Joint optimization across multiple diverse reward functions mitigates the "alignment tax" problem.
- Mechanism: By incorporating multiple reward functions (human preference, fairness, compositionality) simultaneously during fine-tuning, the model maintains performance across all tasks rather than degrading on non-optimized objectives.
- Core assumption: The multiple reward functions can be effectively balanced during joint training without one dominating the others.
- Evidence anchors:
  - [abstract]: "Joint optimization of multiple rewards simultaneously achieves over 80% relative performance of individually fine-tuned models across all tasks, mitigating alignment tax issues"
  - [section]: "While the best score for each metric is achieved by the model fine-tuned specifically for that task, our jointly-trained model is able to satisfy over 80% (relative) performance of the individually fine-tuned models across all three metrics simultaneously"
  - [corpus]: Weak - no direct corpus evidence found supporting the specific claim about alignment tax mitigation
- Break condition: If the reward functions are in direct conflict (e.g., maximizing human preference while minimizing diversity), joint optimization could lead to suboptimal performance on all objectives.

## Foundational Learning

- Concept: Markov Decision Process formulation for sequential decision making
  - Why needed here: The diffusion denoising process is inherently sequential, and reframing it as an MDP allows applying RL techniques to optimize the sampling policy
  - Quick check question: In the diffusion MDP formulation, what constitutes the "state" at each timestep?

- Concept: Policy gradient methods and advantage estimation
  - Why needed here: The REINFORCE algorithm and its variants are used to estimate gradients for optimizing the diffusion policy with respect to arbitrary reward functions
  - Quick check question: Why does the paper use a baseline (mean reward) when computing the advantage function?

- Concept: Multi-task/multi-objective optimization
  - Why needed here: The paper aims to optimize for multiple objectives simultaneously (human preference, fairness, compositionality), requiring techniques to balance competing reward signals
  - Quick check question: What happens to individually fine-tuned models on non-optimized objectives, and why does joint training help?

## Architecture Onboarding

- Component map:
  Base diffusion model (SDv2) serving as the policy π(at|st) -> Multiple reward models (human preference, fairness, compositionality) -> RL optimizer implementing clipped surrogate objective with trust region -> Pretraining loss component for stability -> Data pipeline providing millions of diverse prompts

- Critical path:
  1. Sample prompt from training distribution
  2. Generate denoising trajectory using current policy
  3. Compute rewards from all reward models
  4. Calculate advantage estimates using batch statistics
  5. Update policy using clipped surrogate objective
  6. Apply pretraining loss for stability
  7. Repeat until convergence

- Design tradeoffs:
  - Online generation vs offline dataset: Online generation with latest model enables continuous improvement but risks overfitting; offline generation is more stable but less adaptive
  - Single vs multiple rewards: Single rewards optimize for specific objectives but suffer alignment tax; multiple rewards maintain balanced performance but may converge slower
  - Pretraining loss weight: Higher weight increases stability but may slow reward optimization; lower weight risks reward hacking

- Failure signatures:
  - Reward hacking: Model generates over-detailed images with high-frequency noise to maximize reward scores
  - Overfitting: Model performance degrades on out-of-domain prompts
  - Instability: Training diverges or produces poor quality samples
  - Alignment tax: Model performance drops on non-optimized objectives

- First 3 experiments:
  1. Single reward optimization: Fine-tune with only human preference reward to establish baseline performance and check for reward hacking
  2. Distribution reward optimization: Fine-tune with fairness/diversity reward to validate mini-batch approximation effectiveness
  3. Joint optimization: Combine all three rewards to verify alignment tax mitigation and balanced performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed multi-objective RL approach scale to more than three reward functions, and what are the computational and performance trade-offs involved?
- Basis in paper: [inferred] The paper demonstrates joint optimization of three rewards (human preference, skintone diversity, compositionality) and mentions the potential for addressing "more complex compositional relationships" in future work.
- Why unresolved: The paper does not explore scaling beyond three objectives or analyze the computational overhead and potential performance degradation when adding more reward functions.
- What evidence would resolve it: Experiments training models with 4+ reward functions, measuring convergence time, computational cost (GPU hours), and relative performance on each objective compared to single-task models.

### Open Question 2
- Question: What is the impact of different normalization strategies for distribution-based rewards on the stability and effectiveness of the RL training process?
- Basis in paper: [explicit] The paper mentions that distribution-level rewards are computed over minibatches rather than the full distribution due to intractability, and uses per-batch normalization for reward values.
- Why unresolved: The paper does not compare alternative normalization strategies (e.g., moving averages, exponential smoothing) or analyze how different approaches affect training stability and final performance.
- What evidence would resolve it: Ablation studies comparing different normalization methods on the same reward functions, measuring training stability metrics (e.g., variance in reward values) and final performance on evaluation tasks.

### Open Question 3
- Question: How do the proposed RL-based fine-tuning methods perform on diffusion models with different architectures (e.g., different UNet variants or latent space dimensions) beyond Stable Diffusion v2?
- Basis in paper: [inferred] The paper uses Stable Diffusion v2 as the base model and demonstrates improvements across multiple tasks, but does not test on other diffusion model architectures.
- Why unresolved: The paper does not investigate whether the RL framework generalizes to other diffusion model architectures or latent space configurations, which would indicate broader applicability.
- What evidence would resolve it: Experiments applying the same RL framework to other diffusion models (e.g., Stable Diffusion v1.5, DALL-E 2 variants, or different latent space dimensions), measuring performance improvements across the same evaluation tasks.

## Limitations

- The MDP reformulation's empirical validation is limited, with no systematic analysis of whether the Markov assumption holds for diffusion denoising
- Mini-batch approximation for distribution-level rewards lacks theoretical guarantees and may not capture true distributional properties
- The 80% relative performance threshold for joint optimization may represent a practical limit, but the paper doesn't explore whether better optimization strategies could achieve higher performance

## Confidence

High confidence claims:
- The RL framework can be applied to diffusion models to optimize for human preference rewards
- Joint optimization of multiple rewards maintains reasonable performance across all objectives
- The approach scales to millions of prompts and produces preferred samples

Medium confidence claims:
- The MDP reformulation is the optimal way to frame diffusion sampling for RL
- Mini-batch approximation is sufficient for distribution-level reward optimization
- The 80% relative performance threshold represents a practical limit for joint optimization

Low confidence claims:
- The specific hyperparameter choices are optimal across all tasks
- The approach generalizes equally well to other diffusion model architectures
- The alignment tax is fundamentally unavoidable beyond the observed 80% threshold

## Next Checks

1. **MDP formulation validation**: Conduct systematic ablation studies varying the timestep discretization granularity and analyzing whether finer timesteps improve performance, which would support the MDP assumption. Additionally, test whether the learned policy exhibits true Markovian behavior by analyzing state transitions.

2. **Batch size sensitivity analysis**: Systematically vary the mini-batch size used for reward approximation and measure the impact on fairness/diversity metrics. Compare against theoretical expectations for Monte Carlo approximation error to quantify how well the mini-batch approach captures the true distribution.

3. **Alternative joint optimization strategies**: Implement and compare against other multi-objective optimization methods (e.g., Pareto optimization, dynamic weight scheduling) to determine whether the 80% performance threshold is fundamental or if better optimization strategies could achieve higher relative performance across all objectives.