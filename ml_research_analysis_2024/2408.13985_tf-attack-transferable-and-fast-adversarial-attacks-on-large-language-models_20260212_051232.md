---
ver: rpa2
title: 'TF-Attack: Transferable and Fast Adversarial Attacks on Large Language Models'
arxiv_id: '2408.13985'
source_url: https://arxiv.org/abs/2408.13985
tags:
- attack
- adversarial
- uni00000013
- ttack
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of current adversarial attack
  methods on large language models (LLMs), which suffer from poor transferability
  and inefficiency. The authors propose TF-Attack, a novel approach that employs an
  external LLM as a third-party overseer to identify critical units within sentences
  and introduces the concept of Importance Level to enable parallel substitutions.
---

# TF-Attack: Transferable and Fast Adversarial Attacks on Large Language Models

## Quick Facts
- **arXiv ID**: 2408.13985
- **Source URL**: https://arxiv.org/abs/2408.13985
- **Reference count**: 23
- **Primary result**: Introduces TF-Attack, achieving up to 20x faster adversarial attacks with significantly improved transferability on LLMs

## Executive Summary
TF-Attack addresses the dual challenges of poor transferability and inefficiency in current adversarial attacks on large language models. The method leverages an external LLM as an overseer to identify critical sentence units and introduces an Importance Level metric to enable parallel word substitutions. This approach significantly outperforms existing methods like TextFooler, BERT-Attack, and SDM-Attack across six benchmarks, achieving higher attack success rates while maintaining semantic similarity. The method demonstrates particular effectiveness in binary classification tasks, reducing model accuracy by over 10% while operating up to 20 times faster than competing approaches.

## Method Summary
TF-Attack employs an external large language model to identify critical units within sentences that, when modified, maximally impact model predictions. The method introduces the concept of Importance Level to prioritize and enable parallel substitutions of words or phrases, rather than sequential modifications. This strategic approach allows for more efficient exploration of the adversarial example space while maintaining semantic coherence. By leveraging the external LLM's understanding of language structure and importance, TF-Attack can generate adversarial examples that transfer effectively across different model architectures, addressing a key limitation of existing black-box attack methods.

## Key Results
- Achieves up to 20 times faster attack speed compared to baseline methods
- Consistently outperforms baselines (TextFooler, BERT-Attack, SDM-Attack) in attack success rate, modification rate, and semantic similarity
- Reduces model accuracy by over 10% on binary classification tasks
- Demonstrates minimal impact on adversarial training defenses, potentially strengthening model robustness

## Why This Works (Mechanism)
TF-Attack's effectiveness stems from its strategic identification of critical units through external LLM oversight, which provides a more sophisticated understanding of sentence structure than purely statistical approaches. The Importance Level concept enables parallel processing of substitutions, dramatically reducing the search space and computational overhead. By focusing modifications on high-importance units identified by the external LLM, the method ensures that each substitution maximally impacts model predictions while preserving semantic meaning. This targeted approach overcomes the transferability limitations of traditional gradient-based methods by exploiting language-level vulnerabilities rather than model-specific gradients.

## Foundational Learning
- **Importance Level concept**: A metric for quantifying the significance of sentence units in model predictions. *Why needed*: Enables prioritization of modifications to maximize attack impact. *Quick check*: Verify that high-importance units identified by the external LLM correlate with actual model vulnerability.
- **External LLM oversight**: Using a third-party LLM to analyze and identify critical sentence components. *Why needed*: Provides language understanding beyond statistical gradients. *Quick check*: Compare attack success rates with and without external LLM guidance.
- **Parallel substitution mechanism**: Simultaneous modification of multiple words/phrases rather than sequential changes. *Why needed*: Dramatically improves attack speed and efficiency. *Quick check*: Measure time complexity improvements versus sequential approaches.

## Architecture Onboarding
**Component map**: Input sentence -> External LLM overseer -> Importance Level scoring -> Parallel substitution candidates -> Adversarial example generation -> Model prediction testing
**Critical path**: The external LLM analysis and Importance Level scoring represent the most critical components, as they determine which units are modified and thus directly impact attack success and transferability.
**Design tradeoffs**: The method trades computational resources (external LLM API calls) for improved transferability and speed. This introduces potential latency and cost concerns but enables more sophisticated attack strategies.
**Failure signatures**: Attacks may fail when the external LLM misidentifies critical units, when parallel substitutions introduce semantic incoherence, or when target models have robust defenses against the specific modification patterns used.
**First experiments**:
1. Compare attack success rates with varying Importance Level thresholds
2. Measure transferability across different model architectures (BERT, RoBERTa, GPT variants)
3. Evaluate semantic preservation using multiple similarity metrics (BERTScore, BLEU, human evaluation)

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Reliance on external LLM oversight introduces potential cost and latency concerns
- Evaluation focuses primarily on classification tasks with limited discussion of generation or other NLP tasks
- Experiments conducted only on English text, raising questions about cross-lingual effectiveness
- Does not extensively address potential for models to adapt to or detect these attack patterns over time

## Confidence
- **High**: Improved transferability and attack success rates compared to baselines
- **Medium**: Claimed speed improvements (dependent on hardware and external LLM response times)
- **Low**: Claim that adversarial training minimally impacts model performance against these attacks

## Next Checks
1. Conduct cross-lingual experiments to verify TF-Attack's effectiveness across different languages and character encodings
2. Perform ablation studies isolating the contributions of the Importance Level concept versus external LLM oversight
3. Test the persistence of attack effectiveness against models specifically trained to detect and defend against TF-Attack patterns