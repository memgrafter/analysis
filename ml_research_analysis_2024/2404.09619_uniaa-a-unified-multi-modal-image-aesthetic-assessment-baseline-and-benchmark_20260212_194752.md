---
ver: rpa2
title: 'UNIAA: A Unified Multi-modal Image Aesthetic Assessment Baseline and Benchmark'
arxiv_id: '2404.09619'
source_url: https://arxiv.org/abs/2404.09619
tags:
- aesthetic
- image
- composition
- visual
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UNIAA, a unified multi-modal image aesthetic
  assessment framework that integrates a baseline model and a comprehensive benchmark.
  The authors address the challenge of aligning aesthetic evaluation with human perception
  by leveraging MLLMs and converting existing aesthetic datasets into unified visual
  instruction tuning data.
---

# UNIAA: A Unified Multi-modal Image Aesthetic Assessment Baseline and Benchmark

## Quick Facts
- **arXiv ID**: 2404.09619
- **Source URL**: https://arxiv.org/abs/2404.09619
- **Reference count**: 40
- **Primary result**: Introduces UNIAA, a unified MLLM framework for Image Aesthetic Assessment achieving 78.48% accuracy on aesthetic perception and competitive performance across description and assessment tasks.

## Executive Summary
This paper presents UNIAA, the first MLLM-based approach to Image Aesthetic Assessment (IAA) that unifies a baseline model and comprehensive benchmark. The authors address the challenge of aligning aesthetic evaluation with human perception by leveraging MLLMs and converting existing aesthetic datasets into unified visual instruction tuning data. The UNIAA-LLaVA model, built on the LLaVA architecture, demonstrates competitive performance across aesthetic perception, description, and assessment tasks. It achieves 78.48% accuracy on aesthetic perception, outperforming GPT-4V in several dimensions, and shows strong generalization on both in-domain and in-the-wild data. The UNIAA-Bench provides a systematic evaluation framework covering three aesthetic levels: perception, description, and assessment.

## Method Summary
UNIAA integrates a baseline model (UNIAA-LLaVA) and a comprehensive benchmark (UNIAA-Bench). The model fine-tunes LLaVA-1.5-7B on converted aesthetic data using the IAA Datasets Conversion Paradigm (IDCP), which transforms five open-source IAA datasets (AVA, AADB, PARA, PCCD, ICAA17K) into visual instruction tuning format. The training uses cosine decay learning rate schedule, AdamW optimizer, 2 epochs, batch size 128, and image resolution 336×336. The benchmark evaluates MLLMs across three levels: Perception (question-answering about aesthetic attributes), Description (generating detailed aesthetic analyses), and Assessment (zero-shot scoring). Performance is measured using accuracy for perception, completeness/preciseness/relevance scores for description, and PLCC/SRCC for assessment.

## Key Results
- UNIAA-LLaVA achieves 78.48% accuracy on aesthetic perception, outperforming GPT-4V in content/theme, color, and light dimensions
- The model demonstrates strong generalization on in-the-wild data, outperforming GPT-4V in description tasks with higher completeness (1.51 vs 1.34), preciseness (1.63 vs 1.40), and relevance (1.68 vs 1.44)
- UNIAA-LLaVA approaches junior-level human performance on AVA aesthetic scoring (0.590 PLCC vs 0.605 human junior), though it lags behind senior-level humans (0.590 vs 0.697)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLLMs align aesthetic evaluation with human perception processes by integrating visual perception and language reasoning.
- Mechanism: The UNIAA-LLaVA architecture combines a vision encoder for visual feature extraction with an LLM for language-based aesthetic reasoning, mimicking the human cognitive process of perceiving, analyzing, and describing aesthetic attributes.
- Core assumption: Human aesthetic evaluation involves both perceptual recognition of visual elements and higher-level cognitive integration of these elements into coherent aesthetic judgments.
- Evidence anchors:
  - [abstract] "We choose MLLM with both visual perception and language ability for IAA and establish a low-cost paradigm for transforming the existing datasets into unified and high-quality visual instruction tuning data, from which the UNIAA-LLaVA is trained."
  - [section 1] "We propose UNIAA, as shown in Figure 1, which to the best of our knowledge, introduces MLLM to IAA for the first time, including an IAA-Baseline and a comprehensive IAA-Bench."

### Mechanism 2
- Claim: The IAA Datasets Conversion Paradigm (IDCP) enables low-cost, scalable training data creation for aesthetic MLLMs.
- Mechanism: IDCP transforms existing IAA datasets into a unified visual instruction tuning format by converting aesthetic attributes and comments into question-answer pairs, avoiding expensive manual annotation.
- Core assumption: Existing IAA datasets contain sufficient aesthetic information that can be systematically converted into a format suitable for MLLM training.
- Evidence anchors:
  - [abstract] "We establish a low-cost paradigm for transforming the existing datasets into unified and high-quality visual instruction tuning data, from which the UNIAA-LLaVA is trained."
  - [section 3.1.1] "We select specific aesthetic information. The annotation content of existing IAA datasets can be primarily categorized into the following three types, aesthetic attributes, aesthetic comments, and aesthetic scores."

### Mechanism 3
- Claim: UNIAA-Bench provides a comprehensive evaluation framework that systematically measures MLLM aesthetic capabilities across multiple dimensions.
- Mechanism: The benchmark includes three levels - Perception (question-answering about aesthetic attributes), Description (generating detailed aesthetic analyses), and Assessment (zero-shot scoring) - each targeting different aspects of aesthetic understanding.
- Core assumption: Aesthetic ability is multidimensional and requires evaluation across perception, description, and assessment tasks to capture the full range of capabilities.
- Evidence anchors:
  - [abstract] "To further evaluate the IAA capability of MLLMs, we construct the UNIAA-Bench, which consists of three aesthetic levels: Perception, Description, and Assessment."
  - [section 4] "Inspired by human aesthetic process [24] and image-quality bench [37], we suggest a collection of integrative standards to measure the aesthetic skills of MLLMs from three perspectives: Aesthetic Perception, Aesthetic Description, and Aesthetic Assessment."

## Foundational Learning

- **Concept**: Visual Instruction Tuning
  - Why needed here: MLLMs require fine-tuning on task-specific instruction data to perform specialized tasks like aesthetic assessment.
  - Quick check question: What is the primary difference between pre-training and instruction tuning for MLLMs?

- **Concept**: Multimodal Fusion
  - Why needed here: The vision encoder and LLM must effectively combine visual features with language understanding for aesthetic judgment.
  - Quick check question: How does the UNIAA-LLaVA architecture fuse visual and language representations?

- **Concept**: Zero-shot Learning
  - Why needed here: The model must assess aesthetic quality without explicit scoring training, relying on learned aesthetic knowledge.
  - Quick check question: How does UNIAA-LLaVA perform aesthetic scoring without being trained on MOS values?

## Architecture Onboarding

- **Component map**: Vision Encoder (ViT-L) → MLP Connector → LLM (LLaMA-2-7B) → Output
- **Critical path**: Image input → Vision Encoder → Visual Features → MLP Connector → LLM → Language Output
- **Design tradeoffs**: Balancing vision encoder capacity with LLM parameters vs. computational efficiency; using converted data vs. expensive manual annotation
- **Failure signatures**: Poor performance on aesthetic perception indicates vision encoder issues; weak description ability suggests LLM integration problems
- **First 3 experiments**:
  1. Test UNIAA-LLaVA on UNIAA-QA to verify basic aesthetic perception capability
  2. Evaluate description generation on UNIAA-Describe to assess language integration
  3. Run zero-shot scoring on AVA test set to measure assessment performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the IDCP framework be effectively extended to other aesthetic domains beyond photography, such as video, AI-generated content, or art paintings?
- Basis in paper: [inferred] The authors mention that future work will focus on universal visual aesthetics including aesthetic evaluation of videos, and they note that the converted dataset from IDCP mostly comprises natural in-the-wild images, suggesting room for expanding dataset sources.
- Why unresolved: The current IDCP has only been validated on photography datasets, and its generalizability to other aesthetic domains remains unexplored.
- What evidence would resolve it: Successful application of IDCP to generate training data for MLLMs in video aesthetics, AI art evaluation, or fine art assessment, with demonstrated improvements in those specific domains.

### Open Question 2
- Question: How does the performance of UNIAA-LLaVA compare to human experts in more nuanced aesthetic dimensions, particularly composition and sentiment evaluation?
- Basis in paper: [explicit] The authors note that composition is the most challenging perceptual dimension for MLLMs, and they observe the largest gap between MLLMs and humans exists in composition evaluation, with UNIAA-LLaVA lagging behind junior-level humans (78.48% vs 79.01%) and senior-level humans (78.48% vs 87.39%).
- Why unresolved: While overall performance is reported, detailed breakdowns of how UNIAA-LLaVA performs on specific aesthetic dimensions compared to humans are not provided.
- What evidence would resolve it: Comprehensive human evaluation studies comparing UNIAA-LLaVA's assessments across all aesthetic dimensions (composition, color, light, focus, sentiment, content/theme) with expert human judgments.

### Open Question 3
- Question: What is the impact of different fine-tuning strategies on the visual aesthetic capabilities of MLLMs, such as varying the vision encoder training status or using alternative architectures?
- Basis in paper: [explicit] The authors conduct ablation studies showing that fine-tuning the vision encoder during training improves perception and assessment compared to freezing it, and they demonstrate UNIAA's effectiveness on mPLUG-OWL2 architecture.
- Why unresolved: The current study only explores a limited set of fine-tuning variations, and the optimal strategy for enhancing aesthetic capabilities in MLLMs remains unclear.
- What evidence would resolve it: Systematic comparison of multiple fine-tuning approaches (different learning rates, architectures, training durations, vision encoder configurations) on the same aesthetic benchmark, identifying the most effective strategies for aesthetic task performance.

## Limitations

- The IDCP relies on ChatGPT for question generation, introducing potential variability in the quality and consistency of converted data
- The zero-shot aesthetic scoring approach lacks direct comparison with supervised scoring methods on the same datasets
- UNIAA-LLaVA still has room for improvement, particularly in composition evaluation, indicating that current model performance does not yet match expert-level aesthetic judgment

## Confidence

- **High Confidence**: The UNIAA framework's architecture and training methodology are well-specified and reproducible. The conversion of existing datasets into visual instruction tuning data is clearly described and implemented.
- **Medium Confidence**: Performance metrics on UNIAA-Bench are reported with appropriate statistical measures (accuracy, PLCC, SRCC), but the human evaluation component introduces some variability. The claim of competitive performance with GPT-4V is supported but based on specific benchmark conditions.
- **Low Confidence**: Claims about approaching junior-level human performance are qualitative and not rigorously quantified against systematic human baseline studies. The assertion that MLLMs naturally align with human aesthetic processes is largely theoretical and lacks empirical validation beyond benchmark performance.

## Next Checks

1. **Dataset Conversion Validation**: Conduct a systematic analysis of the IDCP output quality by having independent raters evaluate a sample of generated questions and answers for accuracy, relevance, and diversity compared to the original annotations.

2. **Generalization Assessment**: Test UNIAA-LLaVA on additional out-of-distribution aesthetic datasets not used in training or the UNIAA-Bench to evaluate true generalization capabilities beyond controlled benchmark conditions.

3. **Human Baseline Comparison**: Conduct controlled experiments comparing UNIAA-LLaVA's aesthetic judgments against multiple human experts across the same image sets, measuring inter-rater agreement and systematic differences in aesthetic assessment approaches.