---
ver: rpa2
title: 'Ada-DF: An Adaptive Label Distribution Fusion Network For Facial Expression
  Recognition'
arxiv_id: '2404.15714'
source_url: https://arxiv.org/abs/2404.15714
tags:
- distributions
- label
- distribution
- samples
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Ada-DF, a dual-branch adaptive distribution
  fusion network for facial expression recognition (FER) to address the ambiguity
  problem in FER datasets. The core idea is to use an auxiliary branch to extract
  label distributions of samples, mine class distributions of emotions, and adaptively
  fuse these distributions using attention weights to train the target branch.
---

# Ada-DF: An Adaptive Label Distribution Fusion Network For Facial Expression Recognition

## Quick Facts
- arXiv ID: 2404.15714
- Source URL: https://arxiv.org/abs/2404.15714
- Reference count: 40
- Accuracy of 90.04% on RAF-DB, 65.34% on AffectNet, and 60.46% on SFEW

## Executive Summary
This paper introduces Ada-DF, a dual-branch adaptive distribution fusion network for facial expression recognition (FER) that addresses the ambiguity problem in FER datasets. The core innovation lies in using an auxiliary branch to extract label distributions of samples and mine class distributions of emotions, then adaptively fusing these distributions using attention weights to train the target branch. The framework is evaluated on three real-world datasets (RAF-DB, AffectNet, and SFEW) and demonstrates superior performance compared to state-of-the-art methods. The approach is particularly effective in handling label noise, with consistent improvements over other noisy label learning methods.

## Method Summary
Ada-DF employs a dual-branch architecture where an auxiliary branch extracts label distributions from samples while the target branch performs final emotion classification. The method mines class distributions by averaging label distributions within each emotion class, then uses attention weights to adaptively fuse label and class distributions. This fused distribution serves as supervision for the target branch during training. The framework jointly optimizes both branches using cross-entropy loss for the auxiliary branch and KL divergence loss for the target branch, with ramping functions to balance their contributions. The approach leverages the rich sentiment information in label distributions while mitigating the impact of ambiguous or mislabeled samples through class distribution mining and adaptive fusion.

## Key Results
- Achieves 90.04% accuracy on RAF-DB, 65.34% on AffectNet, and 60.46% on SFEW
- Outperforms state-of-the-art methods on all three datasets, particularly on noisy real-world data
- Demonstrates consistent improvements over other noisy label learning methods
- Shows effective handling of label ambiguity through visualization analysis

## Why This Works (Mechanism)

### Mechanism 1
The auxiliary branch generates label distributions that capture ambiguity in training samples by using an independent network branch to extract probability distributions over emotion classes for each sample. This treats the probability outputs as richer supervision than single labels, assuming that the probability outputs from a pretrained face recognition backbone contain meaningful uncertainty information about emotion labels.

### Mechanism 2
Class distribution mining removes biases from label distributions by averaging over emotion-specific samples. For each emotion class, the method computes the mean label distribution across all samples of that class, using this as a cleaner target distribution. This assumes that biases in label distributions follow a normal distribution around the true emotion class distribution.

### Mechanism 3
Adaptive distribution fusion balances robustness of class distributions with diversity of label distributions using attention weights. The method computes attention weights from both branches, normalizes them, and linearly combines label and class distributions to produce fused distributions that better fit the true emotion distributions. This assumes that attention weights effectively capture sample ambiguity and can guide the optimal fusion ratio.

## Foundational Learning

- **Label Distribution Learning (LDL)**: Needed to handle inherent label ambiguity in FER datasets due to subjective human annotations and cultural differences in emotion perception. Quick check: Why is a single label insufficient for training robust FER models?

- **Multi-task Learning**: Required to jointly optimize label distribution generation (auxiliary task) and expression classification (main task), which improves both tasks beyond just providing distributions. Quick check: How does training the auxiliary branch help the target branch beyond just providing distributions?

- **Attention Mechanisms**: Essential for identifying sample ambiguity levels and guiding the fusion of label and class distributions. Quick check: What role do attention weights play in distinguishing clear samples from ambiguous or mislabeled ones?

## Architecture Onboarding

- **Component map**: Feature Extractor -> Both Branches -> Attention Modules -> Class Distribution Mining -> Adaptive Distribution Fusion -> Target Branch Loss

- **Critical path**: Feature Extractor → Both Branches → Attention Modules → Class Distribution Mining → Adaptive Distribution Fusion → Target Branch Loss

- **Design tradeoffs**: Extra branch adds computation but provides richer supervision; attention modules add parameters but enable adaptive fusion; fixed threshold for class distributions simplifies training but may be suboptimal

- **Failure signatures**: Auxiliary branch outputs uniform distributions (no learning); attention weights collapse to near-zero or near-one values; class distributions become unreliable due to class imbalance

- **First 3 experiments**: 1) Train baseline ResNet18 on single labels only, establish performance; 2) Add auxiliary branch with label distribution extraction, compare improvements; 3) Add class distribution mining and adaptive fusion, measure final performance gains

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Ada-DF change when using different backbone architectures (e.g., ResNet18 vs. ResNet50) across different datasets with varying sizes and ambiguity levels? The paper mentions that Ada-DF performs better on RAF-DB and AffectNet using ResNet50 compared to ResNet18, but the accuracy on SFEW drops significantly with ResNet50. Comparative experiments using different backbone architectures on a variety of datasets with different sizes and ambiguity levels, along with detailed analysis, would resolve this.

### Open Question 2
Can the class distribution mining module in Ada-DF be further improved by incorporating additional facial expression-related tasks, such as facial landmark detection or action unit recognition? The paper mentions exploring more robust distributions by incorporating more additional FER-related tasks, but provides no experimental results. Experiments comparing performance with and without additional facial expression-related tasks would provide evidence.

### Open Question 3
How does the adaptive distribution fusion module in Ada-DF handle samples with extreme ambiguity or noise, and can its performance be further improved by incorporating more sophisticated attention mechanisms? The paper mentions using attention weights to balance robustness and diversity but doesn't analyze handling of extreme ambiguity or explore more sophisticated attention mechanisms. Experiments evaluating performance on samples with extreme ambiguity or noise, along with comparison of different attention mechanisms, would resolve this.

## Limitations
- Exact implementation details of attention modules remain unspecified, affecting reproducibility
- Frozen feature extractor limits end-to-end optimization and potential feature refinement
- Effectiveness depends on quality of attention weight estimation, which is not thoroughly validated across diverse scenarios

## Confidence

**High**: Ada-DF improves FER accuracy on noisy real-world datasets
**Medium**: The dual-branch architecture with adaptive fusion provides robust supervision
**Medium**: Attention weights effectively capture sample ambiguity levels

## Next Checks
1. Conduct ablation study to quantify contributions of auxiliary branch, class distribution mining, and adaptive fusion separately
2. Perform sensitivity analysis of class distribution mining threshold t across different datasets
3. Evaluate transfer learning performance to test generalization to unseen FER datasets with different annotation characteristics