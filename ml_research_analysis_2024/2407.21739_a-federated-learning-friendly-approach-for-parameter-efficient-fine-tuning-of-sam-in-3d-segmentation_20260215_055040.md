---
ver: rpa2
title: A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning
  of SAM in 3D Segmentation
arxiv_id: '2407.21739'
source_url: https://arxiv.org/abs/2407.21739
tags:
- lora
- federated
- segmentation
- medical
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently adapting the
  Segment Anything Model (SAM) for 3D medical image segmentation in a federated learning
  setting. The proposed FLAP-SAM approach combines Parameter-Efficient Fine-Tuning
  (PEFT) with federated learning by selectively fine-tuning specific components of
  SAM, namely LoRA adapters and the decoder output layers, while keeping most of the
  model frozen.
---

# A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation

## Quick Facts
- arXiv ID: 2407.21739
- Source URL: https://arxiv.org/abs/2407.21739
- Authors: Mothilal Asokan; Joseph Geo Benjamin; Mohammad Yaqub; Karthik Nandakumar
- Reference count: 38
- Key outcome: FLAP-SAM achieves 48x communication reduction and 6% Dice improvement on Fed-KiTS vs full fine-tuning

## Executive Summary
This paper proposes FLAP-SAM, a federated learning approach that efficiently adapts the Segment Anything Model (SAM) for 3D medical image segmentation by selectively fine-tuning specific components while keeping most parameters frozen. The method combines LoRA adapters with partial decoder fine-tuning to achieve significant communication efficiency gains without sacrificing segmentation performance. Experiments on Fed-KiTS, Fed-IXI, and Prostate MRI datasets demonstrate that FLAP-SAM outperforms full fine-tuning and achieves competitive results with other parameter-efficient methods while substantially reducing communication overhead.

## Method Summary
FLAP-SAM adapts SAM for federated 3D medical image segmentation by selectively fine-tuning LoRA adapters for attention layers and the decoder's final output layers (UP and HYP) while keeping the image encoder and most decoder components frozen. The approach uses LoRA with rank 32, initialized with random Gaussian matrices for A and zeros for B, and employs a hybrid loss combining cross-entropy and Dice loss. Federated averaging aggregates updates across clients with 25 communication rounds, Adam optimizer, and batch size 32. The method is evaluated on three federated datasets representing different medical imaging modalities.

## Key Results
- 48x reduction in communication overhead compared to full fine-tuning on Fed-KiTS
- 6% improvement in Dice score over full fine-tuning baseline
- 2.8x reduction in communication and parameters compared to SAMed while maintaining similar performance
- Consistent performance improvements across Fed-KiTS, Fed-IXI, and Prostate MRI datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selectively fine-tuning specific decoder components preserves SAM's zero-shot generalization while adapting to medical domain.
- Mechanism: Freezing most of SAM's parameters prevents overfitting and distortion of learned representations, while fine-tuning only LoRA adapters and decoder output layers allows task-specific adaptation.
- Core assumption: SAM's pre-trained encoder contains generalizable features that should remain unchanged.
- Evidence anchors:
  - [abstract] "retaining the parameters of the SAM model (including most of the decoder) in their original state during adaptation is beneficial because fine-tuning on small datasets tends to distort the inherent capabilities of the underlying foundation model"
  - [section 4] "Our experiments show that retaining the parameters of the SAM model (including most of the decoder) in their original state during adaptation is beneficial because fine-tuning on small datasets tends to distort the inherent capabilities of the underlying foundation model"
- Break condition: If dataset size increases significantly, full fine-tuning may become viable and potentially outperform this selective approach.

### Mechanism 2
- Claim: LoRA adapters enable parameter-efficient federated learning by reducing communication overhead.
- Mechanism: LoRA decomposes weight updates into low-rank matrices (A and B), which require fewer parameters to transmit compared to full weight matrices.
- Core assumption: The low-rank decomposition sufficiently captures the task-specific adaptation needed for medical segmentation.
- Evidence anchors:
  - [section 2] "LoRA adapters: Low-rank adaptation [9] is a promising PEFT technique widely used for adapting foundation models to downstream tasks"
  - [section 3] "The server first needs to reconstruct ∆W ℓ q,k = B ℓ q,k · A ℓ q,k and ∆W ℓ v,k = B ℓ v,k · A ℓ v,k for each ℓ and k, then performs FedAvg"
- Break condition: If rank parameter is set too low, the decomposition may not capture necessary adaptation, leading to performance degradation.

### Mechanism 3
- Claim: Fine-tuning only decoder output layers (UP and HYP) provides sufficient flexibility for medical image segmentation.
- Mechanism: The decoder output layers are responsible for transforming cross-attention features into final segmentation masks, making them critical for task adaptation.
- Core assumption: The encoder's image representations are sufficiently domain-general to be reused without fine-tuning.
- Evidence anchors:
  - [abstract] "our approach selectively finetunes certain decoder parts, reducing parameters and communication costs"
  - [section 3] "We aim to efficiently and effectively adapt the SAM for medical image segmentation tasks using limited data distributed across multiple entities. Based on this consideration, we propose to update both θLoRA as well as the final decoder output layers θMD-UP and θMD-HYP"
- Break condition: If medical images have fundamentally different visual patterns than natural images, encoder fine-tuning may become necessary.

## Foundational Learning

- Concept: Federated Learning
  - Why needed here: Medical data privacy regulations prevent centralizing patient data, making distributed training essential.
  - Quick check question: How does FedAvg aggregate model updates from multiple clients while preserving data privacy?

- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: Full fine-tuning of SAM's hundreds of millions of parameters would be computationally expensive and prone to overfitting on small medical datasets.
  - Quick check question: What are the trade-offs between LoRA and other PEFT methods like prompt tuning or adapter-based approaches?

- Concept: Low-Rank Matrix Decomposition
  - Why needed here: Enables LoRA to approximate weight updates with fewer parameters, reducing communication overhead in federated settings.
  - Quick check question: How does the rank parameter affect the expressiveness of LoRA adapters?

## Architecture Onboarding

- Component map: IE -> Cross-attention -> UP/HYP -> Segmentation Mask
- Critical path: Data → IE → Cross-attention → UP/HYP → Segmentation Mask
- Design tradeoffs:
  - Parameter efficiency vs. performance: More fine-tuning parameters improve performance but increase communication cost
  - Encoder freezing vs. fine-tuning: Freezing prevents overfitting but may limit adaptation to medical domain
  - Rank selection: Higher rank increases expressiveness but also parameter count
- Failure signatures:
  - Low Dice scores across all sites: May indicate rank parameter too low or insufficient fine-tuning capacity
  - Inconsistent performance across sites: Could suggest distribution shift between clients
  - High communication overhead: May indicate LoRA rank or fine-tuning scope too large
- First 3 experiments:
  1. Compare LoRA rank 4 vs rank 32 on Fed-KiTS dataset to find optimal balance
  2. Test encoder fine-tuning vs encoder freezing on a small subset of data
  3. Measure communication cost of LoRA parameters vs full decoder fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FLAP-SAM scale with increasing dataset size in federated settings?
- Basis in paper: [inferred] The paper focuses on small dataset scenarios and highlights the benefits of PEFT methods in such settings, but does not explore scaling behavior with larger datasets.
- Why unresolved: The paper's experiments are limited to datasets with small client-specific data splits, leaving the scalability of FLAP-SAM in larger, more diverse datasets unexplored.
- What evidence would resolve it: Experiments on federated datasets with varying sizes and distributions to compare FLAP-SAM's performance and communication efficiency against other methods.

### Open Question 2
- Question: How do different federated optimization strategies (e.g., FedProx, FedNova) impact the performance of low-rank adapters like LoRA in FLAP-SAM?
- Basis in paper: [explicit] The paper mentions that an interesting future direction would be studying the effects of various federated optimization strategies on low-rank adapters for datasets with considerable distribution shifts.
- Why unresolved: The current work uses FedAvg as the optimization strategy, and the impact of other federated optimization methods on the performance and efficiency of LoRA adapters is not explored.
- What evidence would resolve it: Comparative experiments using different federated optimization strategies to evaluate their effects on the performance and communication efficiency of FLAP-SAM.

### Open Question 3
- Question: What is the impact of varying the rank parameter of LoRA on the balance between communication efficiency and segmentation performance in larger-scale federated learning scenarios?
- Basis in paper: [explicit] The paper conducts ablation studies on the rank parameter of LoRA, showing that lower ranks reduce trainable parameters with marginal performance degradation, but this is limited to small-scale experiments.
- Why unresolved: The experiments focus on small datasets, and the trade-offs between communication efficiency and performance with different rank values in larger-scale federated learning scenarios are not addressed.
- What evidence would resolve it: Experiments with varying LoRA rank values on larger federated datasets to analyze the trade-offs between communication costs and segmentation accuracy.

## Limitations
- Limited evaluation on datasets with small client-specific data splits (330 subjects across 11 institutions)
- Does not address potential distribution shifts between client datasets
- Focuses primarily on Dice scores without extensive analysis of other segmentation metrics or failure cases

## Confidence
**High confidence**: The 48x communication cost reduction claim is well-supported by parameter count analysis and LoRA efficiency fundamentals. The selective fine-tuning mechanism preserving SAM's zero-shot capabilities is theoretically sound.

**Medium confidence**: The 6% Dice score improvement is empirically demonstrated but depends on specific dataset characteristics and training configurations. Generalization across different medical modalities shows promise but needs broader validation.

**Low confidence**: Claims about scaling to hundreds of clients or extremely heterogeneous data distributions are not directly tested. Lacks analysis of convergence speed compared to other parameter-efficient approaches.

## Next Checks
1. **Distribution shift robustness**: Evaluate FLAP-SAM on intentionally heterogeneous client datasets with varying domain characteristics to quantify performance degradation and identify breaking points for the selective fine-tuning strategy.

2. **Scalability testing**: Implement FLAP-SAM with increasing numbers of clients (10 → 50 → 100) to measure how communication costs and segmentation performance scale, particularly focusing on aggregation stability and convergence patterns.

3. **Cross-modal generalization**: Test the approach on additional medical imaging modalities (CT, X-ray, ultrasound) and anatomical structures not represented in the current datasets to assess the limits of SAM's encoder generalization when frozen.