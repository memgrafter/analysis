---
ver: rpa2
title: TIGQA:An Expert Annotated Question Answering Dataset in Tigrinya
arxiv_id: '2404.17194'
source_url: https://arxiv.org/abs/2404.17194
tags:
- dataset
- question
- tigrinya
- pages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TIGQA, an expert-annotated dataset for Tigrinya
  machine reading comprehension, featuring 2,685 question-answer pairs derived from
  537 paragraphs in educational textbooks. The dataset addresses the scarcity of high-quality
  annotated data in low-resource languages by using subject matter experts for annotation
  rather than relying on machine translation or crowd-sourced methods.
---

# TIGQA:An Expert Annotated Question Answering Dataset in Tigrinya

## Quick Facts
- arXiv ID: 2404.17194
- Source URL: https://arxiv.org/abs/2404.17194
- Reference count: 40
- Primary result: Expert-annotated Tigrinya MRC dataset with 2,685 QA pairs, human performance 87.6% EM vs. XLM-R Large 66.6% EM

## Executive Summary
This paper introduces TIGQA, the first expert-annotated question answering dataset for Tigrinya, a low-resource language. The dataset contains 2,685 question-answer pairs derived from 537 paragraphs in educational textbooks for grades 4-11. The authors demonstrate that expert annotation by subject matter teachers yields higher-quality data than machine translation approaches, with human performance at 87.6% exact match compared to the best model's 66.6% EM. The work highlights the challenges of creating high-quality datasets in low-resource languages and provides a foundation for advancing Tigrinya NLP research.

## Method Summary
The dataset was created through expert annotation of educational textbooks in Tigrinya and Biology. Four subject matter experts with teaching experience were selected and provided detailed annotation guidelines to create five question-answer pairs per paragraph. The authors initially explored machine translation feasibility but found significant quality issues. The final dataset was split into train/dev/test sets (407/65/65 paragraphs) and evaluated using human annotators and multilingual pre-trained models (XLM-R Large, AfriBERTaBase, DrQA) fine-tuned with standard MRC training procedures.

## Key Results
- Human performance: 87.6% exact match, 92.4% F1-score
- XLM-R Large performance: 66.6% exact match, 84.3% F1-score
- Machine translation error analysis revealed significant issues with untranslated words, omissions, and mistranslations
- Dataset contains 2,685 QA pairs from 537 paragraphs across 122 topics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert annotation by subject matter experts yields higher-quality question-answer pairs compared to machine translation or crowd-sourced methods.
- Mechanism: Domain experts with linguistic and pedagogical knowledge create questions and answers that are contextually relevant, accurate, and aligned with educational curriculum standards.
- Core assumption: Local experts have sufficient linguistic expertise and subject knowledge to produce high-quality annotations in low-resource languages like Tigrinya.
- Evidence anchors:
  - [abstract] "This study initially explores the feasibility of using machine translation (MT) to convert an existing dataset into a Tigrinya dataset in SQuAD format. As a result, we present TIGQA, an expert annotated educational dataset..."
  - [section 3.2] "The selection of the annotators was a meticulous process, including interviews to assess their expertise and teaching experiences. Four expert annotators were carefully selected based on their deep knowledge of the relevant subject matter..."
- Break condition: If annotators lack sufficient subject knowledge or linguistic expertise, or if there is insufficient oversight and quality control during the annotation process.

### Mechanism 2
- Claim: Using educational textbooks as source material ensures dataset relevance for educational applications.
- Mechanism: Curating paragraphs from publicly accessible Tigrinya and Biology books used in elementary and high school ensures the content is contextually appropriate and aligned with local curriculum needs.
- Core assumption: The selected textbooks represent authentic educational content that is relevant and accurate for the target student population.
- Evidence anchors:
  - [section 3.1] "To obtain high-quality and standardized data, we selected Tigrinya and Biology books from the Ethiopian Ministry of Education used in elementary school (grades four and five) and high school (grades ten and eleven) to ensure both educational relevance and authenticity."
  - [section 3.5] "Our dataset is inherently education domain-specific and is derived from student books."
- Break condition: If the source textbooks are outdated, inaccurate, or not representative of current curriculum standards.

### Mechanism 3
- Claim: Evaluating machine translation quality reveals significant limitations for creating datasets in low-resource languages.
- Mechanism: Analyzing translation errors (untranslated words, omissions, mistranslations) demonstrates that MT systems struggle with proper nouns, vocabulary, syntax, and context preservation in Tigrinya.
- Core assumption: The quality of machine translation directly impacts the performance of question-answering models trained on translated data.
- Evidence anchors:
  - [section 5.2] "We found three publicly available MT systems from English to Tigrigna and vice versa... We found that public Google Translate had fewer errors than the two then as shown in (Appendix Tables 4 and 5) for sample translation of auto and manual, we carefully analyzed by language experts..."
  - [section 5.2] "Finally, we evaluated the pre-trained model followed by (Devlin et al., 2019), a state-of-the-art QA system, using auto-translated and manually translated questions and paragraphs as input. The results revealed variations in the model's predictions for both inputs."
- Break condition: If translation technology improves significantly or if there are high-quality parallel corpora available for low-resource language pairs.

## Foundational Learning

- Concept: Machine Translation Error Analysis
  - Why needed here: Understanding the types and frequency of translation errors is crucial for evaluating the feasibility of using MT to create datasets in low-resource languages.
  - Quick check question: What are the three main categories of translation errors identified in the TIGQA paper, and how do they impact dataset quality?

- Concept: Question-Answer Dataset Construction
  - Why needed here: Knowing the different approaches to creating QA datasets (expert annotation, MT, crowd-sourcing) and their trade-offs is essential for designing effective datasets.
  - Quick check question: What are the key advantages of expert annotation over machine translation or crowd-sourcing for creating QA datasets in low-resource languages?

- Concept: Multilingual Pre-trained Models
  - Why needed here: Understanding the capabilities and limitations of multilingual models like XLM-R is important for evaluating their performance on low-resource languages.
  - Quick check question: How does the performance of XLM-R Large on TIGQA compare to human performance, and what does this gap suggest about the challenges of Tigrinya MRC?

## Architecture Onboarding

- Component map:
  Data Collection -> OCR Processing -> Paragraph Extraction -> Expert Annotation -> Quality Control -> Train/Dev/Test Split -> Model Training -> Evaluation

- Critical path: Data Collection → Expert Annotation → Quality Control → Model Training → Evaluation

- Design tradeoffs:
  - Expert annotation vs. MT: Higher quality but more expensive and time-consuming vs. cheaper but lower quality
  - Educational vs. general domain: More relevant for educational applications but potentially less diverse
  - Small vs. large dataset: Easier to create and manage but may limit model performance

- Failure signatures:
  - Low inter-annotator agreement
  - High translation error rates in MT experiments
  - Large performance gap between human and model baselines
  - Domain mismatch between source material and target application

- First 3 experiments:
  1. Evaluate the quality of OCR output by comparing a sample of extracted text to the original scanned pages
  2. Measure inter-annotator agreement by having multiple experts annotate the same sample paragraphs
  3. Fine-tune a multilingual pre-trained model (e.g., XLM-R) on TIGQA and evaluate its performance on the test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of XLM-R Large model vary when fine-tuned specifically on the TIGQA dataset compared to its performance on other multilingual QA datasets?
- Basis in paper: [inferred] The paper mentions that XLM-R Large is evaluated on TIGQA and achieves 66.6% EM and 84.3% F1-score, but does not compare its performance on other multilingual datasets.
- Why unresolved: The paper does not provide a comparative analysis of XLM-R Large's performance on TIGQA versus other multilingual QA datasets, making it unclear how well the model generalizes across different languages and domains.
- What evidence would resolve it: Conducting experiments to fine-tune XLM-R Large on TIGQA and evaluating its performance on other multilingual QA datasets would provide insights into its cross-lingual and cross-domain generalization capabilities.

### Open Question 2
- Question: What is the impact of using expert-annotated data versus machine-translated data on the performance of MRC models in low-resource languages like Tigrinya?
- Basis in paper: [explicit] The paper discusses the limitations of using machine translation for creating QA datasets in Tigrinya and highlights the benefits of expert-annotated data in terms of accuracy and contextual relevance.
- Why unresolved: While the paper provides qualitative insights into the limitations of machine-translated data, it does not quantitatively compare the performance of MRC models trained on expert-annotated versus machine-translated data.
- What evidence would resolve it: Conducting experiments to train MRC models on both expert-annotated and machine-translated datasets and comparing their performance on the same test set would provide quantitative evidence of the impact of data quality on model performance.

### Open Question 3
- Question: How does the reasoning type distribution in TIGQA compare to other QA datasets, and what implications does this have for the development of MRC models?
- Basis in paper: [explicit] The paper analyzes the reasoning types in TIGQA and finds that it requires both single-sentence and multi-sentence inference abilities, with word matching, paraphrasing, single-sentence reasoning, and multi-sentence reasoning contributing to 27.2%, 26.6%, 24.3%, and 21.9% of the questions, respectively.
- Why unresolved: The paper does not provide a comparative analysis of the reasoning type distribution in TIGQA with other QA datasets, making it unclear how the reasoning requirements in TIGQA differ from those in other datasets.
- What evidence would resolve it: Analyzing the reasoning type distribution in other QA datasets and comparing it with TIGQA would provide insights into the unique challenges posed by TIGQA and inform the development of MRC models tailored to handle different reasoning types.

## Limitations

- Dataset size is relatively small (2,685 QA pairs) compared to high-resource language datasets
- Domain is limited to educational textbooks, potentially reducing generalizability
- Human performance evaluation used only 10 samples, limiting statistical significance

## Confidence

- High confidence: Expert annotation produces higher quality data than MT in low-resource languages
- Medium confidence: Educational textbook selection ensures domain relevance
- Medium confidence: Performance gap indicates genuine dataset difficulty

## Next Checks

1. Conduct inter-annotator agreement analysis using Cohen's kappa on a larger sample of independently annotated paragraphs to quantify annotation consistency and reliability.

2. Expand the MT error analysis to include contemporary neural translation systems (e.g., GPT-4, Claude) and measure their impact on downstream MRC model performance to determine if newer systems reduce the expert annotation advantage.

3. Perform cross-domain evaluation by testing models trained on TIGQA on general-domain Tigrinya text to assess the dataset's domain specificity and potential transfer limitations.