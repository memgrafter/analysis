---
ver: rpa2
title: 'From Deception to Detection: The Dual Roles of Large Language Models in Fake
  News'
arxiv_id: '2409.17416'
source_url: https://arxiv.org/abs/2409.17416
tags:
- news
- fake
- llms
- detection
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores the dual roles of large language models (LLMs)
  in fake news generation and detection. The study investigates whether LLMs can generate
  biased fake news, their ability to detect fake news compared to traditional models,
  and the effectiveness of LLM-generated explanations.
---

# From Deception to Detection: The Dual Roles of Large Language Models in Fake News

## Quick Facts
- arXiv ID: 2409.17416
- Source URL: https://arxiv.org/abs/2409.17416
- Authors: Dorsaf Sallami; Yuan-Chen Chang; Esma Aïmeur
- Reference count: 6
- Key outcome: Large language models exhibit dual roles in fake news generation and detection, with larger models showing better detection abilities but struggling to identify fake news they generate themselves, while users benefit from LLM-generated explanations.

## Executive Summary
This paper investigates the dual roles of large language models (LLMs) in both generating and detecting fake news. The study examines whether LLMs can generate biased fake news, their comparative detection abilities against traditional models, and the effectiveness of LLM-generated explanations for users. Results reveal that while some models strictly adhere to safety protocols, others readily produce fake news across various biases. Larger models generally exhibit superior detection capabilities, but LLM-generated fake news proves harder to detect than human-written versions, particularly when containing fictitious citations.

## Method Summary
The research employs a two-pronged approach: first, generating fake news using biased prompts across seven LLMs (Phi-3, Gemma-1.1, Mistral, Llama-3, C4AI, Zephyr-orpo, GPT-4) via HuggingChat/ChatGPT; second, evaluating detection performance on both human-created and LLM-generated fake news against a fine-tuned BERT baseline. Human evaluation of LLM-generated explanations was conducted with 20 participants rating explanations on helpfulness, clarity, accuracy, relevance, and comprehensiveness. The study uses recent real and fake headlines from Snopes (March-May 2024) as evaluation data.

## Key Results
- Some LLMs can generate biased fake news while others strictly adhere to safety protocols
- Larger models generally exhibit superior detection abilities compared to smaller ones
- Users benefit from LLM-generated explanations, with 40% reporting changed opinions after exposure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger LLMs show higher accuracy in fake news detection due to increased parameter count enabling more nuanced language understanding.
- Mechanism: Greater model capacity allows deeper contextual embeddings, which help distinguish subtle cues between real and fake news, especially when fake news includes sophisticated techniques like fabricated citations.
- Core assumption: Model size correlates with detection capability and models can leverage their learned representations to identify misinformation.
- Evidence anchors:
  - [abstract] "larger models generally exhibit superior detection abilities"
  - [section] "We observe a positive correlation between the size of the model and its efficacy in accurately identifying fake news"
  - [corpus] Weak - corpus focuses on different detection architectures, not size comparisons
- Break condition: If training data quality is poor or models are trained on narrow domains, larger size may not improve detection performance.

### Mechanism 2
- Claim: LLM-generated fake news with fictitious citations is harder to detect because models interpret citations as credibility signals.
- Mechanism: LLMs associate citations with authoritative sources during training, so fabricated but plausible citations can override textual inconsistencies, leading to misclassification as real news.
- Core assumption: Models have learned to treat cited sources as reliability indicators, and they cannot independently verify citation authenticity.
- Evidence anchors:
  - [section] "LLM-generated fake news articles that cite fictitious studies to be misclassified as real news"
  - [section] "there was a significant reduction in the successful detection rate for most models when fake news articles cited fake studies"
  - [corpus] Weak - corpus doesn't address citation-based deception
- Break condition: If models develop citation verification capabilities or receive explicit training to identify fabricated sources.

### Mechanism 3
- Claim: LLMs perform worse at detecting fake news they themselves generate due to lack of self-awareness and contextual understanding of their own output patterns.
- Mechanism: Models generate text based on learned patterns without tracking their own output, so they cannot recognize the stylistic and logical inconsistencies in their own fake news as deceptive.
- Core assumption: Generation and evaluation are separate processes without shared self-referential memory or evaluation frameworks.
- Evidence anchors:
  - [section] "models struggle significantly when tasked with identifying fake news that they themselves have generated"
  - [section] "a critical blind spot in the capabilities of LLMs, where their advanced generative abilities may not be matched by equally robust evaluative abilities"
  - [corpus] Weak - corpus focuses on external detection methods, not self-detection
- Break condition: If models implement memory of their own outputs or separate evaluation modules specifically trained to detect their own generation patterns.

## Foundational Learning

- Concept: Fake news detection in NLP
  - Why needed here: Understanding baseline approaches helps evaluate LLM performance improvements and limitations
  - Quick check question: What are the key features traditional fake news detectors use that LLMs might or might not leverage?

- Concept: Bias propagation in language models
  - Why needed here: Critical for understanding how and why models generate biased fake news and how this affects detection
  - Quick check question: How do training data biases manifest in generated content, and what safeguards can mitigate this?

- Concept: Explainability in AI systems
  - Why needed here: Evaluating the effectiveness of LLM-generated explanations requires understanding what makes explanations useful for human decision-making
  - Quick check question: What criteria distinguish helpful explanations from unhelpful ones in the context of fake news detection?

## Architecture Onboarding

- Component map: Generation pipeline → Detection pipeline → Explanation generation → Human evaluation framework
- Critical path: Fake news generation → detection by multiple LLMs → comparison with BERT baseline → explanation quality assessment → human evaluation
- Design tradeoffs: Model size vs. inference cost vs. detection accuracy; explanation detail vs. clarity vs. user comprehension
- Failure signatures: High inconclusive rates, self-detection failures, citation-based misclassification, selective bias generation
- First 3 experiments:
  1. Test generation of biased content across all models with varying prompt structures to map safety protocol boundaries
  2. Evaluate detection accuracy on human vs. LLM-generated fake news to identify capability gaps
  3. Compare explanation quality scores across models to identify optimal explanation length and detail for user comprehension

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs' abilities to generate fake news vary across different types of biases?
- Basis in paper: [explicit] The paper found that some LLMs can generate fake news with various biases while others refuse, with LLMs like Llama-3 showing reluctance to generate content related to certain biases like disability and gender, but not for social status or physical appearance.
- Why unresolved: The paper only tested a limited set of biases and did not explore the full range of potential biases that could be exploited.
- What evidence would resolve it: Testing a wider range of biases and observing the LLMs' responses to each would provide a clearer picture of their bias-generating capabilities.

### Open Question 2
- Question: What are the key factors that contribute to an LLM's effectiveness in detecting fake news, beyond model size?
- Basis in paper: [explicit] The paper notes that Gemma-1.1, a smaller model, outperformed larger models in fake news detection, suggesting that factors other than size play a role.
- Why unresolved: The paper did not investigate the specific factors that contribute to an LLM's detection effectiveness.
- What evidence would resolve it: Conducting a detailed analysis of the training methodologies, data quality, and architectural features of effective models would help identify the key factors.

### Open Question 3
- Question: How can LLMs be improved to better detect fake news they themselves generate?
- Basis in paper: [explicit] The paper found that LLMs struggle to detect fake news they generate, especially when fictitious citations are included.
- Why unresolved: The paper did not explore potential solutions or techniques to enhance LLMs' self-detection capabilities.
- What evidence would resolve it: Developing and testing methods such as self-consistency checks, adversarial training, or enhanced contextual understanding could provide insights into improving self-detection.

## Limitations
- Small dataset size (20 real and 30 fake headlines) may not provide sufficient statistical power
- Reliance on binary yes/no detection outputs without considering confidence scores
- Limited human evaluation with only 20 participants
- Potential data contamination issues with evaluation data possibly overlapping training data
- Focus on binary detection without exploring more sophisticated approaches like claim verification

## Confidence
- High Confidence Claims:
  - Some models strictly adhere to safety protocols while others can generate biased fake news
  - Larger models generally exhibit superior detection abilities
  - Users benefit from LLM-generated explanations in identifying fake news

- Medium Confidence Claims:
  - LLM-generated fake news is less likely to be detected than human-written ones
  - Models struggle significantly when detecting fake news they themselves have generated
  - Citation-based deception reduces detection rates

- Low Confidence Claims:
  - 40% of participants changed their opinions after exposure to explanations (based on small sample size)
  - Specific comparative detection rates between models (limited by small dataset)
  - Exact mechanisms of why self-generated content is harder to detect

## Next Checks
1. **Dataset Validation**: Replicate the study with a significantly larger and more diverse dataset (minimum 100 real and 100 fake news articles) from multiple sources to ensure robust statistical analysis and generalizability of results.

2. **Model Contamination Assessment**: Conduct a comprehensive analysis to verify that evaluation data does not overlap with training data for any of the tested models, particularly for GPT-4 and other models trained on web-scale data.

3. **Multi-Modal Detection Comparison**: Extend the detection framework to include multi-modal approaches (text + image analysis) and compare performance against the pure text-based LLM detection to assess whether LLMs' advantages persist in more comprehensive detection scenarios.