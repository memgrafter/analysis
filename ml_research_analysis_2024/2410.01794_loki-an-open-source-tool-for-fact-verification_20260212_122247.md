---
ver: rpa2
title: 'Loki: An Open-Source Tool for Fact Verification'
arxiv_id: '2410.01794'
source_url: https://arxiv.org/abs/2410.01794
tags:
- loki
- claim
- fact-checking
- evidence
- claims
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Loki is an open-source, human-in-the-loop fact verification tool
  that breaks the task into five modular steps: claim decomposition, check-worthiness
  filtering, query generation, evidence retrieval, and claim verification. It balances
  automation with human judgment, optimized for latency, robustness, and cost efficiency
  at a commercially usable level.'
---

# Loki: An Open-Source Tool for Fact Verification

## Quick Facts
- arXiv ID: 2410.01794
- Source URL: https://arxiv.org/abs/2410.01794
- Authors: Haonan Li; Xudong Han; Hao Wang; Yuxia Wang; Minghan Wang; Rui Xing; Yilin Geng; Zenan Zhai; Preslav Nakov; Timothy Baldwin
- Reference count: 16
- Primary result: Open-source fact verification tool achieving precision up to 0.93 and recall up to 0.88 with human-in-the-loop design

## Executive Summary
Loki is an open-source, human-in-the-loop fact verification tool that breaks the task into five modular steps: claim decomposition, check-worthiness filtering, query generation, evidence retrieval, and claim verification. It balances automation with human judgment, optimized for latency, robustness, and cost efficiency at a commercially usable level. Using GPT-4o and the Serper API, Loki achieves competitive accuracy with precision up to 0.93 and recall up to 0.88, processing claims in an average of 8.32 seconds. Its modular, multilingual design supports multiple LLM APIs and flexible deployment via Python library, web interface, or multimodal inputs. Released under an MIT license, Loki aims to provide accessible, reliable fact-checking assistance for journalists, moderators, and general users.

## Method Summary
Loki implements a five-step pipeline where claims are first decomposed from text, filtered for check-worthiness, converted into search queries, used to retrieve evidence from web sources via the Serper API, and finally verified against the evidence using GPT-4o. Each component is implemented as a Python class with functions that interact with LLMs to perform core tasks. The system employs asynchronous processing with asyncio for concurrent execution, prompt engineering with structured JSON outputs for consistent parsing, and human-in-the-loop verification at multiple levels to provide transparency and build user trust. The modular design allows individual components to be optimized or replaced as needed, supporting extensibility and multilingual adaptation through prompt modifications.

## Key Results
- Achieves precision up to 0.93 and recall up to 0.88 on benchmark datasets
- Processes claims in an average of 8.32 seconds
- Demonstrates cost efficiency through asynchronous parallelism and optimized LLM usage
- Provides multi-level evidence presentation for user trust and transparency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Loki achieves competitive accuracy with precision up to 0.93 and recall up to 0.88 by combining human-in-the-loop verification with optimized LLM-based components.
- Mechanism: The five-step pipeline allows each stage to focus on a specific subtask, reducing cognitive load on both LLM and human user. Asynchronous parallelism and retries maximize throughput and robustness while controlling costs.
- Core assumption: Breaking down fact-checking into modular steps improves efficiency and accuracy over end-to-end automation.
- Evidence anchors: Abstract states "Loki achieves competitive accuracy with precision up to 0.93 and recall up to 0.88, processing claims in an average of 8.32 seconds." Section describes each component implemented as Python class with LLM interactions.
- Break condition: If claim decomposition fails to preserve semantic intent, subsequent steps may propagate errors.

### Mechanism 2
- Claim: Loki's human-in-the-loop design provides transparency and user trust by presenting evidence and rationales at four distinct levels.
- Mechanism: Instead of single verdict, Loki presents overall credibility score, claim-level analysis, evidence-level insight, and detailed evidence breakdown to help users understand reasoning.
- Core assumption: Users prefer structured evidence over opaque verdicts, especially journalists and content moderators.
- Evidence anchors: Section states "Loki presents critical information and insights at each step, to assist users in making well-informed decisions." Design principles include transparency and trust through multi-level information breakdown.
- Break condition: If evidence snippets are too long or irrelevant, users may become overwhelmed or lose trust.

### Mechanism 3
- Claim: Loki's modular, multilingual design supports flexible deployment and component substitution for different domains.
- Mechanism: Each component is a separate Python class with pluggable LLM APIs, allowing developers to swap in better models. Supports asynchronous processing to reduce latency.
- Core assumption: Modularity and language adaptability are essential for practical, commercial-grade fact-checking tools.
- Evidence anchors: Section notes "Loki's modular design enables high extensibility. Each component can be individually optimized or replaced." Also supports multilingual scenarios through prompt modifications.
- Break condition: If component interfaces are not well-defined, integration of new models may break the pipeline.

## Foundational Learning

- Concept: Asynchronous processing with asyncio
  - Why needed here: Allows concurrent LLM calls and web queries, reducing overall latency and improving throughput.
  - Quick check question: How does asyncio help Loki handle multiple fact-checking requests simultaneously?

- Concept: Prompt engineering with structured JSON outputs
  - Why needed here: Ensures LLM outputs are consistently parseable and can be integrated into subsequent pipeline steps.
  - Quick check question: Why does Loki prompt LLMs to return results in JSON format?

- Concept: Check-worthiness filtering
  - Why needed here: Filters out vague, ambiguous, or opinion-based claims to focus verification resources on factual statements.
  - Quick check question: What type of claims does Loki's Checkworthiness Identifier filter out?

## Architecture Onboarding

- Component map: Decomposer → Checkworthiness Identifier → Query Generator → Evidence Retriever → Claim Verifier
- Critical path: Claim → Decomposer → Checkworthiness Identifier → Query Generator → Evidence Retriever → Claim Verifier → UI display
- Design tradeoffs:
  - Human-in-the-loop vs. full automation: Balances accuracy with user trust
  - LLM-based vs. traditional NLP: LLMs are more robust but costlier
  - Asynchronous vs. synchronous: Faster but more complex to implement
- Failure signatures:
  - High latency or timeouts: Check asyncio queue limits and API rate limits
  - Low precision: Inspect claim decomposition or query generation prompts
  - Missing evidence: Verify web API availability and query formulation
- First 3 experiments:
  1. Test Decomposer with short and long input texts to verify claim extraction quality
  2. Evaluate Checkworthiness Identifier on a mix of factual and opinion-based claims
  3. Measure end-to-end latency and cost for a batch of 10 claims using different LLM APIs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Loki compare when using different LLM APIs or locally deployed models?
- Basis in paper: [explicit] The paper mentions that Loki supports multiple LLM APIs as well as locally deployed models, but does not provide comparative performance data.
- Why unresolved: The paper focuses on Loki's performance using GPT-4o and the Serper API, but does not explore the impact of using alternative LLMs or local models on accuracy, latency, or cost.
- What evidence would resolve it: Comparative experiments evaluating Loki's performance across various LLM APIs (e.g., GPT-3.5, Claude, Llama) and local models, measuring accuracy, latency, and cost for each configuration.

### Open Question 2
- Question: How does Loki's fact-checking accuracy vary across different domains and languages?
- Basis in paper: [explicit] The paper mentions that Loki supports multilingual fact-checking and evaluates it on datasets covering various domains, but does not provide detailed performance breakdowns by domain or language.
- Why unresolved: While the paper shows overall performance metrics, it does not analyze how Loki's accuracy changes when dealing with specific domains (e.g., science, politics) or languages other than English and Chinese.
- What evidence would resolve it: Detailed performance analysis of Loki across different domains and languages, including precision, recall, and F1-score breakdowns for each domain and language.

### Open Question 3
- Question: How does Loki's performance scale with longer documents or larger volumes of text?
- Basis in paper: [inferred] The paper mentions that Loki can decompose long texts into individual claims, but does not discuss how performance is affected by the length of the input document or the number of claims to be verified.
- Why unresolved: The paper focuses on Loki's performance with individual claims but does not explore how the system handles very long documents or high volumes of text, which could impact latency and accuracy.
- What evidence would resolve it: Experiments testing Loki's performance with progressively longer documents and higher volumes of text, measuring accuracy, latency, and resource consumption as input size increases.

## Limitations
- Dependency on third-party APIs (GPT-4o, Serper API) introduces potential reliability and cost issues affecting scalability
- Human-in-the-loop design means performance partially depends on user engagement and judgment quality
- Accuracy metrics based on specific datasets; generalization to other domains or claim types remains uncertain
- Modular design creates integration complexity that could impact maintenance and updates

## Confidence

**High Confidence**: The technical architecture description, including the five-step pipeline and modular Python class design, is well-specified and consistent throughout the paper. The reported performance metrics and latency measurements appear reliable based on the provided evaluation methodology.

**Medium Confidence**: The claims about user trust and transparency benefits from the human-in-the-loop approach are supported by design principles but lack empirical user studies to validate their effectiveness in practice. The multilingual capabilities are described conceptually but without comprehensive testing across multiple languages.

**Low Confidence**: The scalability and cost efficiency claims for commercial deployment are not fully substantiated with real-world usage data or long-term monitoring results. The system's robustness to adversarial or highly complex claims is not thoroughly evaluated.

## Next Checks

1. **Cross-dataset validation**: Test Loki on additional fact-checking datasets (e.g., FEVER, LIAR) to assess generalization beyond the reported Factcheck-Bench and FacTool-QA datasets, measuring any degradation in precision and recall.

2. **User study on transparency**: Conduct a controlled experiment comparing user trust and decision quality when using Loki's multi-level evidence presentation versus traditional single-verdict fact-checking tools, with at least 30 participants from target user groups (journalists, moderators).

3. **Long-term API dependency analysis**: Run Loki continuously for one month using rate-limited API quotas to measure actual costs, failure rates, and system behavior under sustained usage conditions, documenting any performance degradation or error patterns.