---
ver: rpa2
title: Semi Supervised Heterogeneous Domain Adaptation via Disentanglement and Pseudo-Labelling
arxiv_id: '2406.14087'
source_url: https://arxiv.org/abs/2406.14087
tags:
- target
- domain
- data
- source
- labelled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles semi-supervised heterogeneous domain adaptation
  (SSHDA), where source and target domains differ in data modality (e.g., optical
  vs. radar in remote sensing).
---

# Semi Supervised Heterogeneous Domain Adaptation via Disentanglement and Pseudo-Labelling

## Quick Facts
- arXiv ID: 2406.14087
- Source URL: https://arxiv.org/abs/2406.14087
- Authors: Cassio F. Dantas; Raffaele Gaetano; Dino Ienco
- Reference count: 27
- Key outcome: SHeDD improves SSHDA in remote sensing, achieving up to 6 F1-score points over baselines in challenging cross-modality transfers.

## Executive Summary
This paper addresses semi-supervised heterogeneous domain adaptation (SSHDA) where source and target data differ in modality (e.g., optical vs. radar). SHeDD disentangles domain-invariant task-relevant features from domain-specific ones, uses pseudo-labeling with consistency regularization, and is end-to-end without relying on pre-trained models. Experiments on RESISC45-Euro and EuroSat-MS-SAR show SHeDD outperforms baselines and state-of-the-art, especially in challenging cross-modality scenarios.

## Method Summary
SHeDD is an end-to-end deep learning framework for SSHDA that uses separate encoders for each modality to preserve preprocessing advantages, disentangles features into domain-invariant and domain-specific components via orthogonality regularization, and leverages pseudo-labeling with consistency regularization on unlabelled target data. The model is trained jointly with classification, domain discrimination, and orthogonality losses, and does not rely on pre-trained models.

## Key Results
- SHeDD achieves up to 6 F1-score points improvement over baselines in cross-modality adaptation.
- Outperforms state-of-the-art SS-HIDA in both RESISC45-Euro and EuroSat-MS-SAR benchmarks.
- Visualizations confirm better-separated class embeddings compared to baselines.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHeDD's feature disentanglement improves cross-modality transfer by separating domain-invariant from domain-specific features.
- Mechanism: The architecture splits the embedding vector into two halves: `zinv` (domain-invariant) fed to the task classifier, and `zspe` (domain-specific) fed to the domain classifier. The orthogonality loss enforces separation, ensuring task-relevant features are preserved across modalities.
- Core assumption: Domain-specific features hinder cross-modality transfer, and enforcing orthogonality preserves useful information while discarding modality noise.
- Evidence anchors:
  - [abstract]: "SHeDD is designed to effectively disentangle domain-invariant representations, relevant for the downstream task, from domain-specific information, that can hinder the cross-modality transfer."
  - [section 3.1]: "To further enforce disentanglement between domain-invariant and domain-specific information, we enforce orthogonality between the two embedding types for any given input sample (source and target, labelled and unlabelled)."
  - [corpus]: Weak/no direct evidence; the concept is standard in domain adaptation literature but not validated here.
- Break condition: If orthogonality regularization over-penalizes useful shared features, performance degrades. This could happen if modalities share subtle but important differences that are misclassified as domain-specific.

### Mechanism 2
- Claim: Consistency regularization via pseudo-labeling boosts generalization on unlabelled target data.
- Mechanism: Unlabelled target samples are augmented and pseudo-labels are generated from the classifier's predictions. The model is trained to make consistent predictions on both original and augmented versions, using a confidence threshold to filter unreliable labels.
- Core assumption: The model's predictions on unlabelled data are reliable enough to serve as training signals when augmented views are consistent.
- Evidence anchors:
  - [abstract]: "Additionally, SHeDD adopts an augmentation-based consistency regularization mechanism that takes advantages of reliable pseudo-labels on the unlabelled target samples to further boost its generalization ability on the target domain."
  - [section 3.2]: "To fully exploit the available target unlabelled data... employ an unsupervised lossà la FixMatch... that enforces consistency between predictions obtained from the unlabeled sample xu and its augmentation xˆu via pseudo-labelling procedure."
  - [corpus]: The mechanism aligns with FixMatch literature, but specific ablation evidence for SHeDD is missing.
- Break condition: If the confidence threshold is too low, noisy pseudo-labels corrupt training; if too high, too few samples are used, underutilizing unlabelled data.

### Mechanism 3
- Claim: Using separate encoders for each modality preserves modality-specific preprocessing advantages while enabling joint learning.
- Mechanism: SHeDD uses one encoder for source data and one for target data, each specialized to its modality's structure, but both feed into shared classifiers for task and domain discrimination.
- Core assumption: Heterogeneous modalities benefit from dedicated feature extraction before alignment, rather than forcing a single shared encoder.
- Evidence anchors:
  - [section 3]: "A given input data x is firstly encoded by its matching backbone and the obtained embedding vector z = g(x) ∈ R2D is then split..."
  - [section 4]: "While SHeDD employs distinct per-domain encoders, SS-HIDA shared a portion of its encoder between the two domains."
  - [corpus]: The claim is supported by comparison to SS-HIDA, but the specific performance gain from dual encoders is not directly isolated in the ablation study.
- Break condition: If the modalities are too similar, dual encoders may overfit and waste capacity; a shared encoder might suffice.

## Foundational Learning

- Concept: Domain adaptation and transfer learning
  - Why needed here: SHeDD operates in a semi-supervised heterogeneous domain adaptation setting, where knowledge must transfer from a labelled source to a scarcely labelled target with different data modalities.
  - Quick check question: What is the difference between unsupervised and semi-supervised domain adaptation?

- Concept: Feature disentanglement
  - Why needed here: The paper explicitly disentangles domain-invariant and domain-specific features to prevent modality-specific noise from hurting cross-domain generalization.
  - Quick check question: How does orthogonality regularization help in separating two types of features?

- Concept: Consistency regularization and pseudo-labelling
  - Why needed here: SHeDD leverages unlabelled target data by enforcing prediction consistency between original and augmented samples, using pseudo-labels as training targets.
  - Quick check question: What role does the confidence threshold play in pseudo-labelling?

## Architecture Onboarding

- Component map:
  Source encoder -> Target encoder -> Embedding split -> zinv to task classifier, zspe to domain classifier -> Augmentation module

- Critical path:
  1. Input → modality-specific encoder → embedding split.
  2. zinv → task classifier (supervised loss + pseudo-label consistency).
  3. zspe → domain classifier (domain discrimination + orthogonality).
  4. Backpropagation updates all components.

- Design tradeoffs:
  - Dual encoders vs. shared encoder: Dual encoders handle modality differences better but double parameter count.
  - Orthogonality loss weight: Too high may discard useful shared features; too low may not disentangle enough.
  - Pseudo-label threshold: High threshold ensures quality but reduces utilization of unlabelled data.

- Failure signatures:
  - Poor target performance despite good source accuracy: Domain-specific features leaking into zinv or insufficient pseudo-label quality.
  - Instability during training: Overly aggressive augmentation or confidence threshold causing gradient spikes.
  - Degraded performance on highly similar modalities: Dual encoders may overfit; consider shared encoder.

- First 3 experiments:
  1. Run SHeDD on a simple RGB→MS transfer (e.g., EuroSat-MS-SAR) with 50 target labels per class; verify orthogonality loss decreases over epochs.
  2. Replace dual encoders with a single shared encoder; compare target F1-score to quantify benefit of modality-specific encoding.
  3. Remove pseudo-labeling consistency loss; observe drop in unlabelled target performance to confirm its contribution.

## Open Questions the Paper Calls Out
- The paper mentions plans to extend SHeDD to multi-source domain adaptation as a future direction, suggesting potential performance differences compared to single-source settings.
- The authors suggest adapting SHeDD to structured tasks such as semantic segmentation or object recognition, but no evaluation on pixel-level or object-level tasks is conducted.
- The paper does not explore whether deeper or task-specific architectures (e.g., EfficientNet, domain-specific encoders) improve results, leaving backbone influence unanalyzed.

## Limitations
- Limited generalization scope: All experiments focus on remote sensing imagery with specific modality pairs; broader applicability is not directly tested.
- Ablation depth: Detailed ablation studies isolating each mechanism's contribution are missing.
- Implementation specifics: Critical hyperparameters are not fully specified, making exact reproduction challenging.

## Confidence
- SHeDD architecture and training pipeline: High
- Performance gains on tested benchmarks: Medium-High (strong results but narrow domain scope)
- General claims about disentanglement benefits: Medium (mechanism plausible but not fully isolated in ablations)

## Next Checks
1. Reproduce SHeDD on a held-out subset of RESISC45-Euro with varying numbers of target labels (5, 10, 50 per class) to verify robustness claims.
2. Conduct an ablation removing the orthogonality loss to quantify its specific contribution to performance gains.
3. Test SHeDD on a non-remote sensing heterogeneous adaptation task (e.g., RGB→depth or text→image) to assess cross-domain generalization.