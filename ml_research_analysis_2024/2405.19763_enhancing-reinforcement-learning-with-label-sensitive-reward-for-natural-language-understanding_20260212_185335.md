---
ver: rpa2
title: Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language
  Understanding
arxiv_id: '2405.19763'
source_url: https://arxiv.org/abs/2405.19763
tags:
- rlhf
- rllr
- reward
- rationales
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the objective mismatch issue in RLHF for NLU
  tasks, where RLHF prioritizes rationale quality over label correctness. To solve
  this, the authors propose RLLR (Reinforcement Learning with Label-sensitive Reward),
  which trains reward models on label-sensitive pairs (correct vs.
---

# Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding

## Quick Facts
- arXiv ID: 2405.19763
- Source URL: https://arxiv.org/abs/2405.19763
- Reference count: 26
- Primary result: RLLR outperforms SFT by 1.54% and RLHF by 0.69% in accuracy across 8 NLU tasks

## Executive Summary
This paper addresses a fundamental objective mismatch in reinforcement learning from human feedback (RLHF) for natural language understanding (NLU) tasks. In standard RLHF, reward models are trained to prioritize rationale quality over label correctness, leading to suboptimal performance on classification tasks. The authors propose RLLR (Reinforcement Learning with Label-sensitive Reward), which trains reward models on both rationale-sensitive and label-sensitive pairs, explicitly capturing the semantic features that distinguish correct from incorrect labels. By incorporating rationales into supervised fine-tuning and using mixed rewards during reinforcement learning, RLLR achieves higher accuracy while maintaining rationale quality comparable to RLHF.

## Method Summary
RLLR addresses the objective mismatch in RLHF for NLU by training reward models on both rationale-sensitive and label-sensitive pairs. The method begins with supervised fine-tuning using Chain-of-Thought prompting with GPT-4-generated rationales for correct labels. Then, rationales are generated for incorrect labels to create label-sensitive pairs, which are used to train a label-sensitive reward model alongside the standard rationale-sensitive reward model. During reinforcement learning, a mixed reward function balances both components using a threshold λ, allowing the model to prioritize label correctness initially while maintaining rationale quality. The method is evaluated across 5 foundation models (3B-13B parameters) on 8 NLU tasks, showing consistent improvements over both SFT and RLHF baselines.

## Key Results
- RLLR outperforms SFT by 1.54% and RLHF by 0.69% in accuracy across 8 NLU tasks
- RLLR achieves rationale quality comparable to RLHF while improving label accuracy
- The method shows consistent improvements across 5 foundation models ranging from 3B to 13B parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training reward models on label-sensitive pairs directly addresses the objective mismatch issue in RLHF for NLU tasks.
- Mechanism: By generating rationales for both correct and incorrect labels, the reward model learns to prioritize label correctness over rationale quality during reinforcement learning.
- Core assumption: The label-sensitive pairs effectively capture the semantic features necessary for accurate label prediction.
- Evidence anchors: Weak or missing evidence explicitly.

### Mechanism 2
- Claim: Integrating rationales into supervised fine-tuning enhances the model's comprehension abilities for NLU tasks.
- Mechanism: By providing step-by-step explanations for predictions, rationales improve the model's reasoning and understanding of the task.
- Core assumption: The rationales generated by GPT-4 accurately reflect the reasoning process for the given labels.
- Evidence anchors: Weak or missing evidence explicitly.

### Mechanism 3
- Claim: Using mixed rewards from both label-sensitive and rationale-sensitive reward models balances label accuracy and rationale quality.
- Mechanism: By combining rewards based on a threshold, the model prioritizes label correctness initially and then focuses on rationale quality as training progresses.
- Core assumption: The threshold λ effectively balances the contributions of the two reward models.
- Evidence anchors: Weak or missing evidence explicitly.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is the foundation for the proposed RLLR method, which aims to improve upon the objective mismatch issue in RLHF for NLU tasks.
  - Quick check question: What are the two main stages of RLHF, and how do they contribute to the objective mismatch issue in NLU tasks?

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: CoT prompting is used to enhance the model's reasoning and understanding by providing step-by-step explanations for predictions.
  - Quick check question: How does CoT prompting improve the model's performance on complex reasoning tasks, and why is it particularly relevant for NLU tasks?

- Concept: Preference Ranking and Bradley-Terry Model
  - Why needed here: The Bradley-Terry model is used to define the preference probability for training the reward models on label-sensitive and rationale-sensitive pairs.
  - Quick check question: How does the Bradley-Terry model work in the context of preference ranking, and why is it suitable for training reward models in RLLR?

## Architecture Onboarding

- Component map: Supervised Fine-Tuning (SFT) with rationales -> Reward Model Training with label-sensitive and rationale-sensitive pairs -> Reinforcement Learning with mixed rewards -> Policy Model (initialized from SFT model)

- Critical path: Generate rationales for correct and incorrect labels -> Train SFT model with rationales -> Sample comparison data from SFT model -> Train reward models on label-sensitive and rationale-sensitive pairs -> Train policy model using mixed rewards in reinforcement learning

- Design tradeoffs: Increased computational requirements due to generating rationales and training multiple reward models; potential trade-off between label accuracy and rationale quality when using mixed rewards; reliance on GPT-4 for generating rationales and evaluating quality

- Failure signatures: Poor performance on NLU tasks due to ineffective rationales or reward models; imbalanced contributions from label-sensitive and rationale-sensitive reward models; insufficient diversity in generated rationales for incorrect labels

- First 3 experiments: 1) Evaluate the performance of reward models trained on label-sensitive pairs versus rationale-sensitive pairs on a hold-out dataset; 2) Compare the accuracy and rationale quality of the RLLR method against the SFT and RLHF baselines on a range of NLU tasks; 3) Analyze the impact of different threshold values λ on the balance between label accuracy and rationale quality in the RLLR MIXED method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the RLLR method's performance scale effectively with even larger models beyond 13B parameters?
- Basis in paper: [inferred] The paper demonstrates effectiveness on models ranging from 3B to 13B parameters but does not test on larger models like 30B or 70B.
- Why unresolved: The paper only tested models up to 13B parameters. Scaling to much larger models may reveal new challenges or benefits not observed in smaller models.
- What evidence would resolve it: Experiments applying RLLR to models like 30B, 65B, or 70B parameters showing whether performance gains persist, diminish, or improve with scale.

### Open Question 2
- Question: How does RLLR perform when applied to tasks outside the tested NLU domains, such as mathematical reasoning or code generation?
- Basis in paper: [explicit] The paper tests on 8 NLU tasks but explicitly states "The compatibility of RLLR with RL-free methods such as DPO, PRO, and RRHF remains unexplored."
- Why unresolved: The method was only validated on NLU tasks. Different task types may have different reward structures and objective alignment challenges.
- What evidence would resolve it: Testing RLLR on mathematical reasoning datasets (like MATH) and code generation tasks (like HumanEval) to measure performance improvements.

### Open Question 3
- Question: What is the long-term stability of RLLR-trained models compared to RLHF-trained models during extended deployment?
- Basis in paper: [inferred] The paper presents static performance metrics but doesn't evaluate model behavior over time or under distribution shift.
- Why unresolved: The paper focuses on immediate performance gains but doesn't address how models maintain their improvements over extended use or when encountering out-of-distribution data.
- What evidence would resolve it: Longitudinal studies tracking model performance over weeks/months of deployment, including robustness tests with adversarial examples and distribution-shifted data.

## Limitations

- The method relies heavily on GPT-4 for rationale generation and quality evaluation, creating potential brittleness if GPT-4's outputs or preferences change
- The computational overhead from generating rationales for both correct and incorrect labels, plus training two reward models, may limit practical deployment
- The study focuses on English NLU tasks with classification objectives, leaving unclear whether results generalize to other languages, task types, or generation tasks

## Confidence

- **High confidence**: Claims about RLLR achieving higher accuracy than SFT (1.54%) and RLHF (0.69%) across 8 NLU tasks with 5 foundation models
- **Medium confidence**: Claims about generating rationales comparable to RLHF in quality
- **Low confidence**: Claims about the mechanism by which label-sensitive pairs specifically capture nuanced semantic features

## Next Checks

1. **Reward Model Ablation Study**: Train separate reward models using only label-sensitive pairs, only rationale-sensitive pairs, and mixed pairs, then evaluate their accuracy on a held-out test set of label-sensitive pairs to quantify how much each component contributes to learning correct label distinctions.

2. **Cross-Model Consistency Check**: Apply RLLR to a diverse set of foundation models including both encoder-decoder (e.g., T5) and decoder-only (e.g., LLaMA2) architectures across additional NLU tasks (e.g., natural language inference with different datasets) to verify the robustness of improvements beyond the reported models.

3. **Human Evaluation Validation**: Conduct human evaluations of rationale quality and correctness using multiple annotators to validate the GPT-4-based evaluation, measuring inter-annotator agreement and comparing human preferences with the automated evaluation scores reported in Table 5.