---
ver: rpa2
title: Sketch Decompositions for Classical Planning via Deep Reinforcement Learning
arxiv_id: '2412.08574'
source_url: https://arxiv.org/abs/2412.08574
tags:
- move
- stack
- unstack
- plan
- rooma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a deep reinforcement learning approach to
  learn sketch decompositions for classical planning problems. The method replaces
  explicit feature pools and combinatorial solvers with neural networks that learn
  general policies in modified planning problems where successor states are defined
  by IW(k) reachability.
---

# Sketch Decompositions for Classical Planning via Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2412.08574
- Source URL: https://arxiv.org/abs/2412.08574
- Reference count: 40
- Near-perfect coverage (90-100%) on 11 classical planning domains using learned width-k decompositions

## Executive Summary
This paper presents a deep reinforcement learning approach to learn sketch decompositions for classical planning problems. The method replaces explicit feature pools and combinatorial solvers with neural networks that learn general policies in modified planning problems where successor states are defined by IW(k) reachability. The learned policies define safe, acyclic, width-bounded decompositions that solve test instances through greedy sequences of IW(k) searches. Experiments across 11 domains show near-perfect coverage and linear or quadratic scaling of subgoal counts with problem size, with plan lengths close to optimal. The decompositions, though represented as neural networks rather than rules, can often be interpreted logically, revealing interpretable substructure in domains like Delivery, Gripper, Childsnack, Miconic, and Spanner.

## Method Summary
The method uses actor-critic deep reinforcement learning with GNN policy and value functions to learn general policies that implicitly encode width-k sketch decompositions. The key innovation is replacing the standard successor state set N(s) with Nk(s), defined as states reachable via IW(k) searches from s. During training, the GNN learns to select subgoal states that are reachable within k-width, forming decompositions that can be evaluated greedily using IW(k). The approach is trained on small instances and tested on larger ones, with stochastic sampling of subgoals during evaluation to improve robustness. The method is compared against baseline approaches using coverage, subgoal count, plan length, and plan quality metrics.

## Key Results
- Near-perfect coverage (90-100%) across 11 domains with k=1 and k=2
- Subgoal counts scale linearly with problem size for many domains, quadratically for others
- Plan lengths close to optimal, though slightly longer than LAMA in some cases
- Learned decompositions can be interpreted logically, revealing domain structure
- Consistent performance across multiple training runs with early stopping

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The learned neural policy implicitly encodes a width-k sketch decomposition without explicit rule-based representation.
- Mechanism: By modifying the successor state space from direct actions to states reachable via IW(k), the DRL policy learns to select subgoal states that are reachable within k-width. These subgoals form a decomposition that can be evaluated greedily using IW(k).
- Core assumption: GNNs can represent the necessary decision boundaries to approximate width-k sketch decompositions even though they cannot express exact width-0 (general policies) for some domains.
- Evidence anchors:
  - [abstract]: "The learned policies define safe, acyclic, width-bounded decompositions"
  - [section 4]: "safe, acyclic sketch decompositions with approximate width k > 0 for a class of problems P ∈ Q can be derived from general policies over a slightly different class of problems Pk ∈ Q k"
  - [corpus]: No direct evidence; related work shows GNNs struggle with exact width-0 policies but succeed with width-2 sketches in some domains.
- Break condition: If the GNN architecture lacks sufficient expressivity for the domain's feature interactions, the learned policy will fail to generalize or produce unsafe decompositions.

### Mechanism 2
- Claim: Sampling subgoal states stochastically from the learned policy's distribution yields robust, generalizable decompositions.
- Mechanism: Instead of deterministically picking the highest-probability subgoal, sampling according to π(s'|s) allows the system to explore alternative decomposition paths, avoiding overfitting to narrow subgoal chains and improving coverage on test instances.
- Core assumption: The policy's learned probability distribution captures meaningful uncertainty about subgoal selection that benefits exploration during test-time search.
- Evidence anchors:
  - [section 5.1]: "The subgoal states Gπk(s) are sampled stochastically according to (3)"
  - [section 5.1]: "we will analyze plots showing the number of subgoals resulting from the learned decomposition Gπk(s) as a function of relevant parameters"
  - [corpus]: No direct evidence; sampling is a common RL technique for exploration but its benefit here is inferred from experimental setup.
- Break condition: If the policy's probabilities are near-deterministic (high confidence), stochastic sampling provides little benefit and may add variance without improving results.

### Mechanism 3
- Claim: Replacing N(s) with Nk(s) extends the effective reasoning depth of the GNN without changing its architecture, enabling width-k reasoning.
- Mechanism: By redefining the successor set to include states reachable via IW(k), the GNN can "jump" multiple steps in the original problem, effectively simulating multi-step planning within a single policy decision. This bypasses the GNN's limited message-passing depth.
- Core assumption: The IW(k) procedure can be treated as a black-box that expands the reachable state space for the GNN policy without requiring the GNN to model multi-step transitions directly.
- Evidence anchors:
  - [section 4]: "replacing the set of successor states N(s) in P ∈ Q with the set Nk(s) defined in (1)"
  - [section 5.1]: "The subgoal states Gπk(s) are sampled stochastically according to (3)"
  - [corpus]: No direct evidence; this is a novel adaptation of GNN-based policy learning to planning decompositions.
- Break condition: If IW(k) is computationally expensive relative to the problem size, the overhead of repeatedly running IW(k) during training and testing may outweigh the benefits of the learned decomposition.

## Foundational Learning

- Concept: Classical planning and width-based search (IW(k))
  - Why needed here: The method relies on IW(k) both as a subroutine for defining successor states and as the planner used to execute the learned decomposition. Understanding width, novelty, and the IW algorithm is essential to grasp how decompositions are formed and evaluated.
  - Quick check question: What is the difference between the width of a problem and its effective width, and why does this distinction matter for this approach?

- Concept: Generalized planning and general policies
  - Why needed here: The connection between general policies (rules mapping states to state transitions) and sketch decompositions is central to the method. The learned neural policy is a continuous approximation of such a general policy.
  - Quick check question: How does a general policy differ from a classical state-to-action policy, and why is this distinction important for learning decompositions?

- Concept: Graph Neural Networks (GNNs) and their expressive limits
  - Why needed here: The learned policies are represented by GNNs, and the method's success depends on the GNN's ability to capture the relevant features for subgoal selection. Knowing the limits of GNNs in planning domains helps explain when the method will or won't work.
  - Quick check question: What are the known limitations of GNNs in representing certain types of planning problems, and how does the use of IW(k) help overcome these?

## Architecture Onboarding

- Component map: Training MDPs -> GNN Policy Network -> IW(k) Module -> Actor-Critic RL Loop -> Evaluation Module
- Critical path: Training → Policy Learning (via RL with IW(k) expansions) → Evaluation (greedy IW(k) sequence using learned policy)
- Design tradeoffs:
  - Using IW(k) during training increases computational cost but extends reasoning depth
  - Stochastic vs. greedy subgoal selection trades determinism for robustness
  - GNN size and depth affect expressivity but increase training time and overfitting risk
- Failure signatures:
  - Low coverage on test instances indicates poor generalization or unsafe decompositions
  - High variance in subgoal counts suggests policy uncertainty or ambiguous subgoal definitions
  - Long plan lengths relative to LAMA indicate the decomposition introduces inefficiencies
- First 3 experiments:
  1. Train and evaluate on a simple domain (e.g., Delivery with 2 packages) to verify basic functionality and coverage.
  2. Compare stochastic vs. greedy subgoal selection on a medium-sized domain to assess robustness benefits.
  3. Test the effect of varying k (1 vs. 2) on coverage and plan quality in a domain with known width-2 solutions (e.g., Blocksworld).

## Open Questions the Paper Calls Out

- Question: How can the plan quality of SIWπ(k) be improved without sacrificing the decomposition's interpretability or generality?
- Basis in paper: [inferred] The paper notes that SIWπ(k) plans are often longer than LAMA's, despite covering more domains, and suggests two potential fixes: preferring high-probability, nearby Nk-successors, or incorporating action costs into the DRL objective.
- Why unresolved: These are only suggested as future directions; the paper does not experiment with either approach, nor does it evaluate their effects on interpretability or generalization.
- What evidence would resolve it: Empirical comparison of SIWπ(k) variants with cost-aware training or distance-weighted selection, showing both improved plan length and maintained decomposition structure.

## Limitations

- The approach's reliance on IW(k) computations during both training and testing introduces significant computational overhead
- Interpretability claims are based on qualitative observations rather than systematic analysis across domains
- Computational complexity analysis is not explicitly provided, making scalability claims uncertain

## Confidence

- High confidence: Coverage results (90-100%) and basic methodology description
- Medium confidence: Claims about interpretability of learned decompositions and comparison to baseline methods
- Low confidence: Claims about computational efficiency and scalability guarantees

## Next Checks

1. Analyze the computational complexity of IW(k) subroutines across different domain sizes and k values to quantify the actual overhead compared to standard planning methods
2. Conduct systematic experiments to verify the interpretability claims by comparing learned subgoals against hand-crafted logical rules in each domain
3. Test the learned policies' robustness to noisy or imperfect training data by introducing controlled variations in training instances and measuring performance degradation