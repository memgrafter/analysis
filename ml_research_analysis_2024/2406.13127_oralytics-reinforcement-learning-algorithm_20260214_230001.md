---
ver: rpa2
title: Oralytics Reinforcement Learning Algorithm
arxiv_id: '2406.13127'
source_url: https://arxiv.org/abs/2406.13127
tags:
- participant
- algorithm
- effect
- section
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes the design of the Oralytics reinforcement
  learning algorithm for promoting healthy oral self-care behaviors through personalized
  intervention prompts delivered via a mobile app. The algorithm uses a Bayesian contextual
  bandit framework with Thompson sampling and a Bayesian linear regression reward
  model.
---

# Oralytics Reinforcement Learning Algorithm

## Quick Facts
- arXiv ID: 2406.13127
- Source URL: https://arxiv.org/abs/2406.13127
- Authors: Anna L. Trella; Kelly W. Zhang; Stephanie M. Carpenter; David Elashoff; Zara M. Greer; Inbal Nahum-Shani; Dennis Ruenger; Vivek Shetty; Susan A. Murphy
- Reference count: 7
- Primary result: Designed and deployed a Bayesian contextual bandit algorithm for personalized oral self-care intervention prompts via mobile app

## Executive Summary
This paper describes the design of the Oralytics reinforcement learning algorithm for promoting healthy oral self-care behaviors through personalized intervention prompts delivered via a mobile app. The algorithm uses a Bayesian contextual bandit framework with Thompson sampling and a Bayesian linear regression reward model. Key design decisions include using full pooling across participants, weekly update cadence, a smooth posterior sampling approach, and a reward function that incorporates both immediate brushing outcomes and costs to model delayed effects. The algorithm was developed using prior data, domain expertise, and extensive simulation experiments across 12 environment variants, with final hyperparameters tuned to optimize average and 25th percentile outcomes. The finalized algorithm was deployed in a clinical trial from fall 2023 to summer 2024.

## Method Summary
The Oralytics algorithm employs a Bayesian contextual bandit framework with Thompson sampling to deliver personalized intervention prompts for oral self-care behaviors. The algorithm uses a Bayesian linear regression reward model with full pooling across participants, updated weekly at 4:04 AM PST. The state features include time of day, exponential averages of OSCB and engagement prompts over the past 7 days, prior day app engagement, and an intercept. The reward function incorporates both immediate brushing outcomes and a cost term that penalizes participants receiving too many prompts, modeling delayed negative effects. The algorithm was developed through extensive simulation experiments using prior dental health studies and pilot data, with final hyperparameters (ξ1 = 80, ξ2 = 40, slope value B = 0.515) tuned to optimize performance across 12 simulation environment variants.

## Key Results
- Bayesian contextual bandit algorithm successfully designed and deployed in clinical trial (fall 2023 - summer 2024)
- Full pooling approach outperformed no-pooling alternatives across all simulation variants
- Weekly update cadence selected after finding minimal performance difference compared to daily updates
- Smooth posterior sampling with generalized logistic function improves replicability
- Cost-aware reward function effectively models delayed intervention effects without requiring full MDP modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm's personalization improves brushing behavior by adapting intervention prompts to individual response patterns.
- Mechanism: Uses Bayesian contextual bandit framework with Thompson sampling to balance exploration and exploitation. The algorithm learns which prompts work best for each participant by updating posterior distributions weekly based on observed OSCB outcomes.
- Core assumption: Individual brushing behavior has predictable patterns that can be learned from limited data (140 decision points per participant).
- Evidence anchors:
  - [abstract] "The algorithm uses a Bayesian contextual bandit framework with Thompson sampling and a Bayesian linear regression reward model."
  - [section 2.5] "We chose a Bayesian contextual bandit algorithm framework" and "Thompson sampling algorithms are stochastic (action selections are randomized with probabilities depending on the posterior distribution), which helps the algorithm explore and learn better"
- Break condition: If participant behavior is too noisy or irregular, the limited data per participant may not support reliable personalization.

### Mechanism 2
- Claim: The reward function design enables the algorithm to learn delayed effects of interventions while maintaining computational tractability.
- Mechanism: Incorporates a cost term (Ci,t) that penalizes the reward when participants receive too many prompts, approximating delayed negative effects that would normally require full MDP modeling.
- Core assumption: The cost term effectively proxies for real delayed effects on future treatment responsiveness without requiring complex state transition modeling.
- Evidence anchors:
  - [section 2.13.2] "By including a cost on selecting action 1, we can move from an always zero model of delayed effects of sending an engagement prompt (selecting action 1) used by the contextual bandit algorithm, to a more realistic setting in which there is some non-negative delayed effect of sending an engagement prompt, captured by our cost term Ci,t."
  - [section 2.13] "Ci,t := ξ1I[ ¯Bi,t > b]I[ ¯Ai,t > a1] +ξ2I[ ¯Ai,t > a2] if Ai,t = 1"
- Break condition: If the cost term parameters are poorly tuned, the algorithm may either over-penalize (reducing effectiveness) or under-penalize (missing delayed effects).

### Mechanism 3
- Claim: Full pooling across participants improves learning efficiency by sharing information while maintaining sufficient personalization.
- Mechanism: Uses full-pooling Bayesian linear regression where all participants share the same model parameters (α0, α1, β), updated weekly using data from all participants.
- Core assumption: Participants share enough similarity in their response patterns that pooling improves learning without sacrificing meaningful personalization.
- Evidence anchors:
  - [section 2.10] "We found through our experiments that full-pooling algorithms outperformed no-pooling algorithms across all variants of the simulation environment"
  - [section 2.6.1] "Through experiments (Section 3.4.1), we made the final decision to use an algorithm that does full pooling (clustering with cluster sizeN = 70)"
- Break condition: If participants are highly heterogeneous, full pooling may prevent the algorithm from learning individual-specific patterns.

## Foundational Learning

- Concept: Bayesian Linear Regression with Action Centering
  - Why needed here: Provides a tractable way to model the reward function while handling the binary action space and enabling personalization
  - Quick check question: How does action centering help the algorithm distinguish between baseline reward and treatment advantage?

- Concept: Thompson Sampling in Contextual Bandits
  - Why needed here: Enables probabilistic action selection that balances exploration of new prompt strategies with exploitation of known effective ones
  - Quick check question: Why does using a smooth rho function instead of a hard threshold improve replicability?

- Concept: Cost-Aware Reward Design
  - Why needed here: Allows the bandit algorithm to approximate delayed negative effects without requiring full MDP modeling
  - Quick check question: How does the cost term Ci,t encode the belief that too many prompts reduce future effectiveness?

## Architecture Onboarding

- Component map: Toothbrush sensor data -> Cloud server (Bayesian RL algorithm) -> Posterior distribution calculation -> Action schedule generation -> Mobile app delivery -> OSCB measurement collection
- Critical path: Toothbrush sensor data -> cloud processing -> algorithm update -> schedule generation -> app delivery -> OSCB measurement
- Design tradeoffs: Full pooling vs. no pooling (efficiency vs. personalization), weekly vs. daily updates (computational load vs. learning speed), smooth vs. hard thresholds (exploration vs. exploitation)
- Failure signatures: Participants receiving too many/few prompts, algorithm not updating, app not delivering scheduled actions, sensor data not transmitting
- First 3 experiments:
  1. Test algorithm behavior with synthetic data where treatment effects are known to verify learning
  2. Simulate app opening issue to verify fallback scheduling mechanism works correctly
  3. Run algorithm with different cost term parameters to observe impact on action selection patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal frequency of reinforcement learning updates (daily vs. weekly) for maximizing oral self-care behaviors while minimizing computational burden?
- Basis in paper: [explicit] The paper compares daily and weekly update cadences in simulations and finds little difference in outcomes, but decides on weekly updates to simplify after-study analyses and reduce computational burden.
- Why unresolved: While the paper provides experimental results, it acknowledges that the choice may depend on specific study conditions and computational constraints not fully explored.
- What evidence would resolve it: A head-to-head comparison of daily vs. weekly updates in a real-world clinical trial, measuring both oral self-care outcomes and computational efficiency.

### Open Question 2
- Question: How does the smoothing allocation function's slope parameter (b) affect the algorithm's ability to personalize interventions and maintain statistical power for after-study analyses?
- Basis in paper: [explicit] The paper discusses choosing the slope parameter (b) for the smoothing allocation function, balancing between concentration of randomization probabilities and maintaining power for statistical inference.
- Why unresolved: The optimal slope value likely depends on the specific context and may require further exploration in different clinical trial settings.
- What evidence would resolve it: A systematic study varying the slope parameter across multiple clinical trials, analyzing its impact on personalization effectiveness and statistical inference quality.

### Open Question 3
- Question: What is the optimal prior sampling period duration for balancing warm-start benefits with early learning opportunities in reinforcement learning algorithms for digital health interventions?
- Basis in paper: [explicit] The paper compares a longer prior sampling period (until the 15th participant) with a shorter one (until the 5th participant) and finds minimal differences in outcomes, but the choice was made based on domain expert consultation.
- Why unresolved: The optimal duration may depend on factors like recruitment rate, participant heterogeneity, and the specific intervention context, which were not fully explored in the study.
- What evidence would resolve it: A series of simulations or real-world trials varying the prior sampling period duration, measuring both learning efficiency and intervention effectiveness across different recruitment scenarios and participant populations.

## Limitations

- Personalization effectiveness depends on assumptions about participant behavior regularity that may not hold across diverse populations
- Cost term used to model delayed effects is a simplification that may not capture complex temporal dependencies in intervention effectiveness
- Full pooling assumes sufficient similarity across participants, which could limit learning for highly heterogeneous individuals
- Deployment environment (mobile app with sensor data) introduces potential technical failure modes not fully characterized

## Confidence

- High confidence: The core Bayesian contextual bandit framework with Thompson sampling is well-established and the algorithm architecture is clearly specified
- Medium confidence: The reward function design and hyperparameter tuning process is described but relies on simulation assumptions that may not fully reflect real-world behavior
- Medium confidence: The full pooling approach is supported by experiments but may not generalize to populations with higher heterogeneity

## Next Checks

1. **Behavioral heterogeneity test**: Run the algorithm on participant subgroups with known behavioral differences to assess whether full pooling limits personalization effectiveness
2. **Cost term sensitivity analysis**: Systematically vary ξ1 and ξ2 parameters to quantify their impact on action selection patterns and overall effectiveness
3. **Real-world deployment monitoring**: Track algorithm performance metrics (exploration rate, reward improvement) during the clinical trial to validate simulation predictions