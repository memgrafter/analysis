---
ver: rpa2
title: 'Learning Scalable Model Soup on a Single GPU: An Efficient Subspace Training
  Strategy'
arxiv_id: '2407.03641'
source_url: https://arxiv.org/abs/2407.03641
tags:
- mehl-soup
- soup
- fine-tuned
- memory
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory and computational inefficiency
  of Learned-Soup model averaging methods for combining fine-tuned models. The authors
  propose MEHL-Soup(+), which formulates model averaging as a hyperplane optimization
  problem and uses block coordinate gradient descent to learn mixing coefficients
  efficiently.
---

# Learning Scalable Model Soup on a Single GPU: An Efficient Subspace Training Strategy

## Quick Facts
- arXiv ID: 2407.03641
- Source URL: https://arxiv.org/abs/2407.03641
- Authors: Tao Li; Weisen Jiang; Fanghui Liu; Xiaolin Huang; James T. Kwok
- Reference count: 40
- Key outcome: MEHL-Soup(+) reduces memory usage by >13× and achieves 9× speedup in soup construction while improving test accuracy over Learned-Soup(+)

## Executive Summary
This paper addresses the memory and computational inefficiency of Learned-Soup model averaging methods for combining fine-tuned models. The authors propose MEHL-Soup(+), which formulates model averaging as a hyperplane optimization problem and uses block coordinate gradient descent to learn mixing coefficients efficiently. This approach only requires loading a mini-batch of fine-tuned models at each iteration and building a computational graph on the combined model, significantly reducing memory usage. Experiments on various ViT models show that MEHL-Soup(+) outperforms Learned-Soup(+) in test accuracy while reducing memory usage by more than 13× and achieving 9× speedup in soup construction. The method also exhibits lower sensitivity to top-performing fine-tuned models compared to Greedy-Soup.

## Method Summary
MEHL-Soup(+) introduces a memory-efficient approach to model soup training by formulating the problem as hyperplane optimization. Instead of loading all fine-tuned models simultaneously, it uses block coordinate gradient descent (BCGD) to update mixing coefficients in mini-batches. At each iteration, only a subset of models is loaded into memory while the remaining coefficients are kept fixed. The method extends to MEHL-Soup+ with a layer-wise mixing scheme that assigns individual coefficients to each layer. This allows for more flexible model combinations by relaxing the solution space from a convex hull to a hyperplane spanned by the fine-tuned models.

## Key Results
- Memory usage reduced by more than 13× compared to Learned-Soup(+)
- Soup construction speed improved by 9×
- Test accuracy improved on ImageNet and CIFAR datasets
- Lower sensitivity to top-performing fine-tuned models compared to Greedy-Soup

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MEHL-Soup(+) reduces memory usage by loading only a mini-batch of fine-tuned models at each iteration instead of all models simultaneously.
- Mechanism: Block Coordinate Gradient Descent (BCGD) is used to update mixing coefficients. At each iteration, only a randomly selected mini-batch of models is loaded into memory, while the remaining models' coefficients remain fixed. This reduces memory usage from O(KD) to O(bD), where K is the number of fine-tuned models and b is the mini-batch size.
- Core assumption: The subspace optimization problem remains solvable when updating coefficients in blocks rather than all at once.
- Evidence anchors:
  - [abstract]: "At each iteration, MEHL-Soup only needs to load a few fine-tuned models and build a computational graph with one combined model."
  - [section 3.3]: "We randomly select and update a block of variables by gradient descent at each iteration while keeping the remaining variables fixed."
  - [corpus]: Weak evidence. Corpus neighbors focus on memory-efficient LLM training but don't directly address the specific block-coordinate approach used here.
- Break condition: If the mini-batch size b is too small, convergence may slow significantly. If too large, memory constraints are not sufficiently alleviated.

### Mechanism 2
- Claim: The hyperplane optimization formulation allows for extrapolated mixing coefficients, which improves performance compared to constrained convex hull methods.
- Mechanism: Instead of constraining mixing coefficients to sum to 1 and lie in [0,1] (convex hull), the method formulates the optimization on a hyperplane spanned by the fine-tuned models. This allows coefficients to extrapolate beyond the convex hull, providing more flexible model combinations.
- Core assumption: Extrapolation in the weight space can lead to better generalization than interpolation within the convex hull.
- Evidence anchors:
  - [abstract]: "This relaxes the solution space from a convex hull to a hyperplane spanned by the fine-tuned modelsθk's."
  - [section 3.1]: "recent studies [5,35] show that interpolation within the convex hull may lead to sub-optimal performance, and extrapolation is more general and can perform better."
  - [corpus]: Weak evidence. While corpus papers discuss memory efficiency, none directly address the benefits of weight extrapolation in model soups.
- Break condition: If extrapolation leads to numerical instability or if the fine-tuned models are too dissimilar, performance may degrade.

### Mechanism 3
- Claim: The layer-wise extension (MEHL-Soup+) provides more precise model averaging by assigning individual mixing coefficients to each layer.
- Mechanism: Instead of using the same mixing coefficients across all layers, the layer-wise approach assigns separate coefficients to each layer. This allows the method to capture layer-specific contributions from different fine-tuned models.
- Core assumption: Different layers in neural networks contribute differently to the final performance, and fine-tuned models may excel at different layers.
- Evidence anchors:
  - [abstract]: "We further extend MEHL-Soup to MEHL-Soup+ in a layer-wise manner."
  - [section 3.1]: "To further enhance the representation ability of HL-Soup, we introduce a layer-wise mixing scheme called HL-Soup+"
  - [corpus]: Weak evidence. No direct support in corpus, but layer-wise approaches are common in neural network optimization.
- Break condition: If the number of layers is very large, the increased parameter space for coefficients may lead to overfitting or computational inefficiency.

## Foundational Learning

- Concept: Subspace optimization and hyperplane learning
  - Why needed here: The method relies on formulating model averaging as a subspace learning problem where the combined model lies in a hyperplane spanned by fine-tuned models.
  - Quick check question: What is the mathematical difference between optimizing over a convex hull versus a hyperplane in the context of model averaging?

- Concept: Block Coordinate Gradient Descent (BCGD)
  - Why needed here: BCGD is the core algorithmic technique that enables memory-efficient learning by updating only a subset of variables at each iteration.
  - Quick check question: How does the convergence rate of BCGD compare to full gradient descent when the batch size is small?

- Concept: Computational graph construction and memory management
  - Why needed here: Understanding how computational graphs are built and managed is crucial for appreciating the memory savings achieved by only building one graph for the combined model.
  - Quick check question: What is the typical memory overhead of building a computational graph for a neural network, and how does this scale with the number of models?

## Architecture Onboarding

- Component map:
  Fine-tuned model loader -> Combined model builder -> Coefficient optimizer -> Layer-wise coefficient manager (for MEHL-Soup+) -> Validation evaluator

- Critical path:
  1. Load mini-batch of fine-tuned models
  2. Build computational graph for combined model
  3. Compute gradients for selected coefficients
  4. Update coefficients
  5. Construct new combined model
  6. Evaluate on validation set

- Design tradeoffs:
  - Mini-batch size vs. convergence speed: Larger batches improve convergence but reduce memory savings
  - Number of inner iterations vs. communication overhead: More inner iterations reduce parameter passing but may overfit to mini-batch
  - Layer-wise vs. model-wise: Layer-wise provides better performance but increases parameter count

- Failure signatures:
  - Memory overflow: Indicates mini-batch size too large for available GPU memory
  - Slow convergence: May indicate learning rate too low or mini-batch size too small
  - Degraded performance: Could result from poor initialization or insufficient training epochs

- First 3 experiments:
  1. Verify memory usage reduction: Compare peak memory usage of MEHL-Soup vs. Learned-Soup with varying numbers of fine-tuned models
  2. Test convergence behavior: Plot validation loss vs. iteration count for different mini-batch sizes
  3. Validate layer-wise benefits: Compare performance of MEHL-Soup vs. MEHL-Soup+ on a small dataset with few fine-tuned models

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but the methodology raises several important research directions.

## Limitations
- The paper only demonstrates effectiveness on ViT and ResNet architectures, leaving open questions about performance on other vision architectures.
- No analysis of how the method scales to extremely large-scale models like GPT-3, GPT-4, or other LLMs for language tasks.
- Limited exploration of the relationship between extrapolation coefficients and model interpretability or robustness.

## Confidence
- High confidence in memory reduction and speed improvement claims due to direct measurability
- Medium confidence in accuracy improvement claims due to lack of ablation studies
- Low confidence in scalability analysis for very large numbers of fine-tuned models

## Next Checks
1. **Convergence analysis**: Run MEHL-Soup with varying mini-batch sizes (b=1, b=5, b=10) on the same dataset to empirically verify the convergence-speed tradeoff and identify the optimal batch size for different memory constraints.

2. **Ablation study**: Compare MEHL-Soup against a version that uses full gradient descent (loading all models) but with the same hyperplane formulation to isolate the benefits of block coordinate descent versus the subspace optimization approach.

3. **Robustness test**: Generate fine-tuned models with deliberately different performance characteristics (some significantly worse than others) and test whether MEHL-Soup maintains its robustness advantage over Greedy-Soup in these extreme scenarios.