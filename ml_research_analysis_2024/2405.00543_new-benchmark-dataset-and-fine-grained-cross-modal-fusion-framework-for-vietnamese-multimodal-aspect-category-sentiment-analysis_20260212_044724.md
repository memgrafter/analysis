---
ver: rpa2
title: New Benchmark Dataset and Fine-Grained Cross-Modal Fusion Framework for Vietnamese
  Multimodal Aspect-Category Sentiment Analysis
arxiv_id: '2405.00543'
source_url: https://arxiv.org/abs/2405.00543
tags:
- sentiment
- multimodal
- image
- dataset
- aspect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ViMACSA, a new Vietnamese multimodal dataset
  for aspect-category sentiment analysis, consisting of 4,876 text-image pairs with
  fine-grained annotations for both text and image in the hotel domain. To leverage
  the fine-grained information in the dataset, the authors propose a Fine-Grained
  Cross-Modal Fusion Framework (FCMF) that learns intra- and inter-modality interactions
  and fuses them into a unified multimodal representation.
---

# New Benchmark Dataset and Fine-Grained Cross-Modal Fusion Framework for Vietnamese Multimodal Aspect-Category Sentiment Analysis

## Quick Facts
- arXiv ID: 2405.00543
- Source URL: https://arxiv.org/abs/2405.00543
- Reference count: 40
- Primary result: Introduces ViMACSA dataset with 4,876 text-image pairs and proposes FCMF framework achieving 79.73% F1 score

## Executive Summary
This paper introduces ViMACSA, a new Vietnamese multimodal dataset for aspect-category sentiment analysis in the hotel domain. The dataset contains 4,876 text-image pairs with fine-grained annotations for both modalities. To leverage this rich annotation structure, the authors propose a Fine-Grained Cross-Modal Fusion Framework (FCMF) that captures both intra-modality and inter-modality interactions to create unified multimodal representations. Experiments demonstrate that FCMF outperforms existing state-of-the-art models on the ViMACSA benchmark.

## Method Summary
The paper addresses Vietnamese multimodal aspect-category sentiment analysis by creating a new dataset and developing a specialized fusion framework. ViMACSA provides fine-grained annotations at both text and image levels, enabling detailed analysis of aspect-category sentiment relationships. The proposed FCMF framework employs a dual-stream architecture that processes text and image inputs separately before applying sophisticated fusion mechanisms to capture cross-modal interactions. The framework learns both modality-specific features and their interrelationships to produce enhanced multimodal representations for sentiment classification.

## Key Results
- FCMF achieves highest F1 score of 79.73% on ViMACSA dataset
- Proposed framework outperforms state-of-the-art multimodal sentiment analysis models
- Demonstrates effectiveness of fine-grained cross-modal fusion for Vietnamese language processing

## Why This Works (Mechanism)
The framework succeeds by explicitly modeling both intra-modality (within text, within image) and inter-modality (between text and image) interactions. This fine-grained approach allows the model to capture subtle sentiment cues that might be present in only one modality or reinforced across both. The dual-stream architecture with attention-based fusion mechanisms enables the model to weigh the importance of different features from each modality dynamically based on the specific aspect-category being analyzed.

## Foundational Learning
- Vietnamese language processing challenges: Why needed - Vietnamese has unique linguistic features including diacritics and compound words that affect sentiment interpretation; Quick check - Validate model performance on clean vs. noisy Vietnamese text
- Multimodal sentiment analysis: Why needed - Combining text and image provides richer context for sentiment understanding; Quick check - Compare multimodal vs. unimodal baselines
- Fine-grained annotation: Why needed - Detailed aspect-category level annotations enable more precise sentiment analysis; Quick check - Evaluate performance across different aspect categories

## Architecture Onboarding

Component map: Text Encoder -> Intra-text Attention -> Image Encoder -> Intra-image Attention -> Cross-modal Fusion -> Sentiment Classifier

Critical path: Text/Image encoding → Intra-modality attention → Cross-modal fusion → Classification

Design tradeoffs:
- Dual-stream vs. unified architecture: Chosen for better modality-specific feature extraction
- Attention-based fusion vs. simple concatenation: Provides dynamic weighting of cross-modal features
- Fine-grained vs. coarse annotations: Enables more precise sentiment analysis but requires more complex annotation process

Failure signatures:
- Poor performance on aspect categories with subtle visual cues
- Sensitivity to Vietnamese text quality (misspellings, abbreviations)
- Potential overfitting due to limited dataset size

First experiments:
1. Baseline unimodal text-only model
2. Baseline unimodal image-only model
3. Simple early fusion baseline (concatenation before classification)

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Dataset size of 4,876 samples may limit generalizability
- Exclusive focus on hotel domain constrains broader applicability
- Limited experimental validation without ablation studies or unimodal comparisons

## Confidence
- Dataset creation and annotations: High
- FCMF framework architecture: Medium
- Experimental results and comparisons: Medium
- Claims about Vietnamese language challenges: Low

## Next Checks
1. Conduct ablation studies on FCMF to measure the individual impact of intra-modality and inter-modality components on performance
2. Evaluate the dataset size sensitivity by testing model performance across different train/validation splits to assess robustness
3. Perform qualitative analysis of failure cases to better understand model limitations and the specific challenges posed by Vietnamese language characteristics