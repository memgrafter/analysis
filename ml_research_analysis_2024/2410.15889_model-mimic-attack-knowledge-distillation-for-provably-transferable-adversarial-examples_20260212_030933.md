---
ver: rpa2
title: 'Model Mimic Attack: Knowledge Distillation for Provably Transferable Adversarial
  Examples'
arxiv_id: '2410.15889'
source_url: https://arxiv.org/abs/2410.15889
tags:
- adversarial
- attack
- student
- distillation
- black-box
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a knowledge distillation-based black-box adversarial
  attack method that provides provable guarantees on success. The approach iteratively
  trains surrogate models on expanding datasets collected by querying the black-box
  teacher model.
---

# Model Mimic Attack: Knowledge Distillation for Provably Transferable Adversarial Examples

## Quick Facts
- arXiv ID: 2410.15889
- Source URL: https://arxiv.org/abs/2410.15889
- Reference count: 33
- This paper proposes a knowledge distillation-based black-box adversarial attack method that provides provable guarantees on success.

## Executive Summary
This paper introduces MMAttack, a novel black-box adversarial attack method that combines knowledge distillation with iterative training of surrogate models to generate transferable adversarial examples. The approach iteratively trains student models on expanding datasets collected by querying the black-box teacher model, then uses white-box attacks (PGD) on the student to generate adversarial examples. The method provides theoretical guarantees that under bounded gradient assumptions, transferable adversarial examples will be found within finite iterations. Experiments on CIFAR-10 and CIFAR-100 demonstrate superior performance compared to state-of-the-art black-box attack methods, achieving significantly lower average query numbers while maintaining high success rates.

## Method Summary
MMAttack is a black-box adversarial attack method that uses iterative knowledge distillation to train surrogate models on expanding datasets collected from the target model. The approach starts with a hold-out dataset to collect the teacher model's predictions, then trains a student model using soft-label distillation. Adversarial examples are generated via PGD attacks on the student model, and successful examples that transfer to the teacher are added to the training dataset for the next iteration. This process continues until transferable adversarial examples are found. The method proves that under bounded gradient assumptions, transferable examples will be found within finite iterations.

## Key Results
- MMAttack with SmallCNN architecture achieves an average query number (AQN) of 32.8 for CIFAR-10, significantly outperforming ZOO (205.1), NES (168.9), and Square attack (79.7).
- For CIFAR-100, MMAttack achieves an AQN of 24 with 97.4% success rate, outperforming MCG (45.1 AQN, 96.7% success) and NP-Attack (67.6 AQN, 98.6% success).
- The approach provides provable guarantees that transferable adversarial examples will be found within finite iterations under bounded gradient assumptions.

## Why This Works (Mechanism)

### Mechanism 1
Knowledge distillation iteratively improves the surrogate model's ability to mimic the black-box teacher model, increasing the likelihood of generating transferable adversarial examples. The algorithm collects the teacher's predictions on a hold-out dataset and uses these as soft labels to train the student model. Each iteration expands the training dataset with newly generated adversarial examples, allowing the student to better approximate the teacher's decision boundary. Core assumption: The student model has sufficient learning capacity to accurately mimic the teacher model on the expanding dataset.

### Mechanism 2
The PGD-based white-box attack on the student model generates adversarial examples that are more likely to transfer to the teacher model as the student becomes a better functional copy. PGD performs iterative gradient ascent on the student model to find adversarial examples within a δ-neighborhood. As the student model better approximates the teacher's decision boundary through distillation, the adversarial examples generated on the student become more transferable to the teacher. Core assumption: The gradient of the difference between student and teacher models is bounded in the δ-neighborhood of the target point.

### Mechanism 3
The iterative expansion of the training dataset with successful adversarial examples creates a self-reinforcing cycle that guarantees finding transferable examples within finite iterations. Each iteration adds adversarial examples that transfer from the student to the teacher into the training dataset. This creates a convergent sequence of adversarial examples that approach a point where the student and teacher make identical misclassifications. Core assumption: The sequence of adversarial examples generated for the student model is bounded and converges to a point where both models make the same misclassification.

## Foundational Learning

- **Knowledge distillation and its application to model compression**: The attack relies on creating a student model that mimics the teacher's behavior using soft-label distillation. Quick check: What is the difference between soft-label and hard-label knowledge distillation, and why is soft-label used in this attack?

- **Projected Gradient Descent (PGD) and adversarial example generation**: PGD is the white-box attack method used to generate adversarial examples on the student model. Quick check: How does PGD differ from basic gradient ascent in finding adversarial examples, and what role does the projection step play?

- **Transferability of adversarial examples between models**: The core of the attack is generating examples on the student that transfer to the black-box teacher. Quick check: What factors influence the transferability of adversarial examples between different model architectures?

## Architecture Onboarding

- **Component map**: Teacher model (black-box) -> Student model -> PGD attack module -> Query manager
- **Critical path**: 1. Initialize student model with teacher's predictions on hold-out dataset 2. Train student to mimic teacher using soft-label distillation 3. Generate adversarial examples on student using PGD 4. Test if examples transfer to teacher 5. If successful, return example; if not, add to training dataset and repeat
- **Design tradeoffs**: Student model complexity vs. query efficiency: Simpler models require fewer queries but may have limited learning capacity; Number of PGD iterations vs. convergence speed: More iterations increase success rate but also computational cost; Dataset size vs. training time: Larger initial datasets improve mimicry but increase setup time
- **Failure signatures**: High AQN values: Indicates poor transferability between student and teacher models; Low ASR after many iterations: Suggests the student model cannot adequately mimic the teacher's decision boundary; Unstable convergence: Adversarial examples oscillate without approaching a transferable solution
- **First 3 experiments**: 1. Compare AQN and ASR using different student model architectures (e.g., SmallCNN vs ResNet18) on CIFAR-10 2. Vary the initial dataset size |D(S1)| and number of adversarial examples l to find optimal hyperparameters 3. Test the impact of PGD parameters (step size α, number of iterations M) on attack success and query efficiency

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the Model Mimic Attack's performance scale with increasing complexity of the teacher model architecture? The paper only tests a limited set of architectures and does not systematically explore the relationship between teacher model complexity and attack success.

- **Open Question 2**: Can the transferability guarantees of Theorem 4.1 be extended to hard-label distillation scenarios? The paper explicitly states that the transferability guarantee is for soft-label distillation and mentions the need for estimating probability of transferability for hard-label cases.

- **Open Question 3**: How does the choice of white-box attack method impact the overall efficiency and success of the Model Mimic Attack? The paper only tests PGD and does not explore how different white-box attack methods affect the distillation process and final attack success.

## Limitations
- **Bounded Gradient Assumption**: The theoretical guarantee relies on the critical assumption that the gradient of the difference between student and teacher models is bounded in the δ-neighborhood, which may not hold for complex models.
- **Model Capacity Requirement**: The approach assumes the student model has "enough learning capabilities" to accurately mimic the teacher, which may fail for highly complex teacher models or significantly simpler student architectures.
- **Dataset Coverage**: The iterative expansion strategy requires sufficient coverage of the input space through the hold-out dataset, and if the initial dataset is too small or unrepresentative, the student may fail to capture important regions of the teacher's decision boundary.

## Confidence
- **High Confidence**: The empirical evaluation results showing MMAttack outperforming state-of-the-art methods on CIFAR-10 and CIFAR-100 datasets.
- **Medium Confidence**: The theoretical convergence proof under bounded gradient assumptions, though the practical validity of this assumption for real neural networks remains uncertain.
- **Medium Confidence**: The knowledge distillation mechanism for improving surrogate model fidelity, which is well-grounded in literature but the specific iterative expansion strategy is novel.

## Next Checks
1. **Gradient Bound Validation**: Empirically measure the sup|∇fi(x')|F values across different δ-neighborhoods for various teacher-student model pairs to assess the validity of the bounded gradient assumption.
2. **Model Capacity Sensitivity**: Systematically vary the student model architecture complexity and measure the impact on AQN and ASR to identify the minimum model capacity required for successful attacks.
3. **Dataset Size Scaling**: Test the attack performance with varying initial hold-out dataset sizes to determine the minimum dataset requirements for effective knowledge distillation.