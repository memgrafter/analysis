---
ver: rpa2
title: Multimodal Cross-Document Event Coreference Resolution Using Linear Semantic
  Transfer and Mixed-Modality Ensembles
arxiv_id: '2404.08949'
source_url: https://arxiv.org/abs/2404.08949
tags:
- event
- real
- pairs
- coreference
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses cross-document event coreference resolution
  (CDCR), where the goal is to determine whether distinct mentions of events across
  multiple documents refer to the same underlying occurrence. The authors propose
  a multimodal approach that integrates visual and textual cues using a simple linear
  mapping between vision and language models, aiming to leverage event-centric images
  to improve resolution when language is ambiguous.
---

# Multimodal Cross-Document Event Coreference Resolution Using Linear Semantic Transfer and Mixed-Modality Ensembles

## Quick Facts
- arXiv ID: 2404.08949
- Source URL: https://arxiv.org/abs/2404.08949
- Reference count: 0
- Key outcome: This paper proposes multimodal approaches for cross-document event coreference resolution, demonstrating that integrating visual and textual cues improves resolution performance, with an ensemble approach establishing an upper limit of 91.9 CoNLL F1 on ECB+ ECR.

## Executive Summary
This paper addresses the challenge of cross-document event coreference resolution (CDCR) by proposing multimodal approaches that integrate visual and textual information. The authors augment the ECB+ benchmark with event-centric images and introduce three methods: a fused model with fine-tuning, a novel linear mapping method without fine-tuning, and an ensemble approach based on mention pair difficulty. Their results show that multimodal information is particularly useful for resolving semantically ambiguous event mentions, establishing new baselines on both ECB+ and AIDA Phase 1 datasets.

## Method Summary
The paper proposes a multimodal approach to cross-document event coreference resolution that combines visual and textual cues. The core method involves using linear semantic transfer (Lin-Sem) to map between vision and language models without fine-tuning, alongside fused modality models that integrate images and text through standard fine-tuning. The authors augment ECB+ and AIDA Phase 1 datasets with event-centric images, encode them using various vision models (ViT, BEiT, SWIN, CLIP), and train pairwise scorers to determine coreference likelihood. They also introduce an ensemble approach that categorizes mention pairs by difficulty and assigns different models to handle each category.

## Key Results
- Linear semantic transfer achieves competitive performance with fused models while being more computationally efficient
- The ensemble approach based on mention pair difficulty achieves state-of-the-art results (91.9 CoNLL F1) on ECB+ ECR
- Multimodal information particularly improves resolution of semantically ambiguous mentions and pronominal coreference
- The approach establishes novel baselines on AIDA Phase 1 dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear semantic transfer preserves cross-modal semantic equivalence between event mentions.
- Mechanism: The linear mapping function (M_LLM→V and M_V→LLM) learned via ridge regression minimizes the squared distance between concatenated text and image representations, thereby preserving semantic information across modalities.
- Core assumption: The bidirectional map exists and is approximately linear between heterogeneous image and text embedding spaces.
- Evidence anchors:
  - [abstract] "A novel approach to multimodal cross document event coreference (MM-CDCR) including a low-compute, bidirectional linear semantic transfer technique (Lin-Sem) based on semantic equivalence across modalities"
  - [section] "Since a closed-form solution to analytically derive the mapping function M_V↔LLM is not always feasible... we propose a parameter-efficient linear-mapping technique Lin-Sem"
  - [corpus] Weak - the paper shows performance improvements but doesn't provide direct corpus-level evidence of semantic preservation
- Break condition: If the embedding spaces are fundamentally incompatible (e.g., different dimensional semantics) or the ridge regression overfits, the linear mapping will fail to preserve semantic equivalence.

### Mechanism 2
- Claim: Ensembling different models based on mention pair difficulty improves overall performance.
- Mechanism: By categorizing mention pairs as "easy" or "hard" based on semantic and discourse-level similarity scores, and assigning different models to handle each category, the system can leverage the strengths of each model type.
- Core assumption: The difficulty categorization is valid and the models perform consistently better on their assigned categories.
- Evidence anchors:
  - [abstract] "An ensemble approach based on splitting mention pairs by semantic and discourse-level difficulty"
  - [section] "These combined semantic and discourse similarity scores were then bucketed into easy and hard semantic transfer categories based on the means of coreferring and non-coreferring samples"
  - [corpus] Weak - the paper shows improved results but doesn't provide corpus-level validation of the difficulty categorization
- Break condition: If the difficulty categorization is inaccurate or the models don't perform as expected on their assigned categories, the ensemble approach will not improve performance.

### Mechanism 3
- Claim: Multimodal information is particularly useful for resolving semantically ambiguous event mentions.
- Mechanism: Visual cues like faces, background scenes, and damaged buildings provide disambiguating information that text alone cannot capture, especially for pronominal coreference and misleading lexical overlap.
- Core assumption: The images are relevant to the event mentions and contain sufficient disambiguating information.
- Evidence anchors:
  - [abstract] "Our results demonstrate the utility of multimodal information in ECR for certain challenging coreference problems"
  - [section] "Within true positives (TP), linearly-mapped models... tended to correctly retrieve a higher proportion of hard pairs compared to the text-only and fused models"
  - [corpus] Weak - the paper provides examples but doesn't provide corpus-level statistics on the impact of multimodal information
- Break condition: If the images are irrelevant or don't contain sufficient disambiguating information, the multimodal approach will not improve performance.

## Foundational Learning

- Concept: Event Coreference Resolution
  - Why needed here: Understanding the task of linking event mentions across documents is fundamental to understanding the problem being solved.
  - Quick check question: What is the difference between event coreference resolution and entity coreference resolution?

- Concept: Multimodal Learning
  - Why needed here: The paper proposes a multimodal approach that integrates visual and textual cues, so understanding multimodal learning is crucial.
  - Quick check question: What are the advantages and disadvantages of using multimodal information compared to unimodal information?

- Concept: Linear Transformations
  - Why needed here: The paper uses linear transformations to map between image and text embedding spaces, so understanding linear transformations is essential.
  - Quick check question: What are the conditions under which a linear transformation can preserve semantic information?

## Architecture Onboarding

- Component map: Text Encoder (Longformer) -> Image Encoder (ViT, BEiT, SWIN, CLIP) -> Linear Mapper (Lin-Sem) -> Pairwise Scorer (MLP) -> Difficulty Categorizer -> Ensembler

- Critical path: Text/Image Encoding → Linear Mapping → Pairwise Scoring → Ensembling

- Design tradeoffs:
  - Linear mapping vs. full fine-tuning: Linear mapping is more computationally efficient but may not capture complex relationships
  - Fused modality vs. separate modalities: Fused modality may capture more information but is more computationally expensive
  - Difficulty categorization vs. no categorization: Difficulty categorization may improve performance but requires additional computation

- Failure signatures:
  - Poor performance on hard mention pairs: May indicate the linear mapping or difficulty categorization is not effective
  - Inconsistent performance across different modalities: May indicate the models are not learning effectively
  - High computational cost: May indicate the fused modality approach is not efficient

- First 3 experiments:
  1. Evaluate the performance of the linear mapping on a small dataset to validate its effectiveness
  2. Compare the performance of the ensemble approach to a single model approach to validate its effectiveness
  3. Analyze the impact of multimodal information on different types of mention pairs (e.g., pronominal coreference vs. lexical overlap) to understand its strengths and weaknesses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of linear semantic transfer compare to fine-tuned multimodal models on coreference resolution tasks outside the event domain?
- Basis in paper: [explicit] The paper demonstrates that linear semantic transfer between vision and language models performs competitively with fused multimodal models for event coreference resolution, but does not explore other coreference tasks.
- Why unresolved: The study focuses specifically on event coreference resolution and does not provide evidence for the generalizability of linear semantic transfer to other coreference tasks.
- What evidence would resolve it: Experiments comparing linear semantic transfer to fine-tuned multimodal models on entity coreference resolution or other coreference tasks would provide evidence for the broader applicability of this approach.

### Open Question 2
- Question: What is the impact of using different image generation models on the performance of multimodal coreference resolution?
- Basis in paper: [explicit] The paper uses Stable Diffusion for image generation, but does not explore the impact of using other image generation models.
- Why unresolved: The study only uses one image generation model and does not provide evidence for the sensitivity of multimodal coreference resolution to the choice of image generation model.
- What evidence would resolve it: Experiments comparing the performance of multimodal coreference resolution using different image generation models (e.g., DALL-E, Midjourney) would provide evidence for the impact of image generation model choice.

### Open Question 3
- Question: How does the performance of linear semantic transfer scale with the size of the training data?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of linear semantic transfer on a relatively small dataset (ECB+), but does not explore its performance on larger datasets.
- Why unresolved: The study does not provide evidence for the scalability of linear semantic transfer to larger datasets, which may be more representative of real-world scenarios.
- What evidence would resolve it: Experiments comparing the performance of linear semantic transfer on datasets of varying sizes would provide evidence for its scalability and potential limitations.

## Limitations
- Limited validation of the linear mapping's semantic preservation capability
- Lack of corpus-level statistics on multimodal information's impact
- No comparison with state-of-the-art cross-modal retrieval methods
- Potential domain-specific limitations of scraped internet images

## Confidence

**High Confidence:**
- Performance improvements on ECB+ and AIDA Phase 1 datasets
- Utility of multimodal information for resolving semantically ambiguous mentions
- Computational efficiency of linear mapping compared to full fine-tuning

**Medium Confidence:**
- Effectiveness of difficulty-based ensembling
- Generalization of results to other coreference resolution tasks
- Robustness of the linear mapping across different embedding spaces

**Low Confidence:**
- Semantic preservation through linear mapping
- Corpus-level impact of multimodal information
- Effectiveness of difficulty categorization

## Next Checks

1. **Linear Mapping Validation:** Conduct controlled experiments to measure semantic preservation directly by testing the linear mapping on known semantically equivalent event mentions across modalities, rather than relying solely on downstream coreference performance.

2. **Difficulty Categorization Analysis:** Perform detailed corpus-level analysis to validate the difficulty categorization by examining the distribution of correctly resolved mention pairs across difficulty levels and testing the categorization's consistency across different models.

3. **Cross-Modal Retrieval Comparison:** Compare the linear mapping approach with state-of-the-art cross-modal retrieval methods (e.g., CLIP-based retrieval) to establish whether the proposed method offers advantages in computational efficiency or performance for the specific task of event coreference resolution.