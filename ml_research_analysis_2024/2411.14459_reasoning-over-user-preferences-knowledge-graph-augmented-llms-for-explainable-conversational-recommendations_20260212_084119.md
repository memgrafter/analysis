---
ver: rpa2
title: 'Reasoning over User Preferences: Knowledge Graph-Augmented LLMs for Explainable
  Conversational Recommendations'
arxiv_id: '2411.14459'
source_url: https://arxiv.org/abs/2411.14459
tags:
- user
- preference
- compass
- preferences
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of explainable user preference
  modeling in conversational recommender systems (CRSs). Current CRS approaches typically
  represent preferences as latent vectors, lacking transparency.
---

# Reasoning over User Preferences: Knowledge Graph-Augmented LLMs for Explainable Conversational Recommendations

## Quick Facts
- arXiv ID: 2411.14459
- Source URL: https://arxiv.org/abs/2411.14459
- Reference count: 40
- Primary result: COMPASS improves CRS performance with 13.76%-44.76% HR@10 gains using interpretable preference summaries

## Executive Summary
This paper addresses the critical challenge of explainable user preference modeling in conversational recommender systems, where current approaches rely on opaque latent vectors. The authors propose COMPASS, a framework that leverages large language models (LLMs) and knowledge graphs (KGs) to generate interpretable, human-readable preference summaries. The system uses a two-stage training process: first aligning KG structures with natural language through graph entity captioning, then fine-tuning the LLM to reason over user preferences from dialogue histories and KG-augmented context. The framework demonstrates substantial performance improvements across multiple CRS models while providing the added benefit of transparency in preference modeling.

## Method Summary
COMPASS introduces a novel approach to explainable conversational recommendations by combining LLMs with knowledge graphs. The method employs a two-stage training process: graph entity captioning pre-training aligns KG structures with natural language descriptions, followed by knowledge-aware instruction fine-tuning to optimize the LLM for reasoning over user preferences from dialogue histories. The framework generates textual preference summaries that integrate with existing CRS models through an adaptive gating mechanism. The approach is evaluated on ReDial and INSPIRED datasets with IMDB-scraped knowledge graphs, using Llama3.1-8B as the base LLM with R-GCN for entity embeddings and LoRA for efficient fine-tuning.

## Key Results
- COMPASS achieves 13.76% to 44.76% relative improvement in HR@10 across various CRS baseline models
- The framework generates interpretable preference summaries that enhance recommendation performance
- Experimental results show consistent improvements across multiple metrics including HR@50, NDCG@10, NDCG@50, MRR@10, and MRR@50

## Why This Works (Mechanism)
The approach works by bridging the gap between latent preference representations and human-interpretable reasoning. By leveraging LLMs' natural language capabilities and augmenting them with structured knowledge graph information, the system can generate coherent explanations for user preferences while maintaining recommendation accuracy. The two-stage training process ensures that the model learns both the structural relationships in the KG and how to apply this knowledge to real conversational contexts.

## Foundational Learning

**Knowledge Graph Embeddings (R-GCN)**: Graph neural networks that learn entity representations by aggregating neighborhood information. Why needed: To capture relational structure in the KG for preference reasoning. Quick check: Verify entity embeddings capture known movie relationships.

**LoRA Fine-tuning**: Parameter-efficient fine-tuning method that inserts low-rank adapters into pre-trained models. Why needed: To enable efficient LLM adaptation without full parameter updates. Quick check: Monitor adapter weight changes during fine-tuning.

**Graph Entity Captioning**: Task of generating natural language descriptions from graph structures. Why needed: To align KG structures with natural language for preference explanation. Quick check: Evaluate caption quality against ground truth descriptions.

**Adaptive Gating Mechanism**: Technique to dynamically weight and integrate multiple information sources. Why needed: To combine preference summaries with baseline CRS models effectively. Quick check: Analyze gating weights for different user scenarios.

**Knowledge-aware Instruction Tuning**: Fine-tuning approach that teaches models to reason with structured knowledge. Why needed: To enable LLM to generate preference summaries from dialogue and KG context. Quick check: Test model's ability to answer knowledge-based questions.

## Architecture Onboarding

**Component Map**: Dialogue History -> KG Entity Linking -> Preference Encoder -> LLM with LoRA Adapters -> Preference Summary -> Adaptive Gating -> CRS Model

**Critical Path**: User dialogue → Entity linking → Preference summary generation → Adaptive gating integration → Recommendation output

**Design Tradeoffs**: The framework trades computational overhead of KG integration and LLM inference for improved explainability and performance. The two-stage training approach balances between general language understanding and domain-specific preference reasoning.

**Failure Signatures**: Poor entity linking quality degrades preference reasoning; insufficient KG coverage limits contextual understanding; suboptimal adaptive gating weights reduce integration effectiveness with baseline CRS models.

**First 3 Experiments**:
1. Test entity linking accuracy on dialogue-movie entity pairs
2. Evaluate preference summary quality with human judges
3. Measure baseline CRS performance with and without preference summary integration

## Open Questions the Paper Calls Out
None

## Limitations
- Requires access to proprietary knowledge graphs (IMDB-scraped) and baseline CRS models not publicly available
- Ground truth preference summary generation process using GPT-4 lacks detailed prompt specifications
- Evaluation relies heavily on automated metrics without sufficient human validation of preference summary quality

## Confidence
**High Confidence**: Core methodology of using LLM fine-tuning for preference reasoning over dialogue and KG context is technically sound and well-motivated.

**Medium Confidence**: Performance improvements are substantial but depend on KG integration quality and adaptive gating mechanism effectiveness.

**Low Confidence**: Exact impact of preference summaries on CRS performance cannot be fully assessed without access to baseline implementations and integration details.

## Next Checks
1. Implement and evaluate knowledge graph entity linking accuracy from dialogue mentions to KG entities
2. Conduct human evaluation study to validate quality, coherence, and factual consistency of generated preference summaries
3. Perform ablation experiments removing KG context to isolate contribution of KG-augmented reasoning versus dialogue-based preference modeling