---
ver: rpa2
title: Self-training Language Models for Arithmetic Reasoning
arxiv_id: '2407.08400'
source_url: https://arxiv.org/abs/2407.08400
tags:
- training
- self-training
- online
- methods
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the potential of self-training language\
  \ models for arithmetic reasoning without requiring new annotated data. The authors\
  \ experiment with two variants of self-training\u2014offline and online\u2014using\
  \ automated feedback based on the correctness of model predictions."
---

# Self-training Language Models for Arithmetic Reasoning

## Quick Facts
- arXiv ID: 2407.08400
- Source URL: https://arxiv.org/abs/2407.08400
- Reference count: 28
- Primary result: Self-training improves arithmetic reasoning by +25.9% with online methods vs +13.9% with offline methods

## Executive Summary
This paper investigates self-training for improving language models' arithmetic reasoning capabilities without requiring new annotated data. The authors compare offline and online self-training approaches using automated correctness feedback, evaluating both supervised fine-tuning and three preference optimization methods (DPO, KTO, IPO) across six arithmetic reasoning datasets. Results demonstrate that both self-training variants significantly improve model performance, with online self-training achieving larger gains. The study reveals that preference optimization methods are particularly effective in online settings, maintaining stability and improving performance on out-of-distribution problems.

## Method Summary
The method employs self-training where models generate predictions for known arithmetic problems, and only correct predictions are used as training data. Two variants are explored: offline (single round of data collection) and online (continuous generation of training data). The approach uses FLAN-CALCFORMER models (3B parameters) and compares supervised fine-tuning with three preference optimization methods. Automated feedback is provided through correctness verification against ground truth answers, creating a loop where the model reinforces its own correct reasoning patterns without requiring new human annotations.

## Key Results
- Online self-training achieves +25.9% improvement in correct results compared to +13.9% for offline self-training
- Preference optimization methods outperform supervised fine-tuning in online settings due to superior stability
- Both offline and online self-training significantly improve initial model performance without new data
- PO methods maintain original capabilities while improving on out-of-distribution problems like GSM8K

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-training improves arithmetic reasoning by exposing models to automated feedback on prediction correctness, allowing iterative refinement without new data
- Mechanism: The model generates predictions for known problems, and only correct predictions are used as training data, creating a loop where the model reinforces its own correct reasoning patterns
- Core assumption: Automated correctness feedback (comparing predictions to ground truth) is sufficient to improve reasoning capabilities
- Evidence anchors:
  - [abstract] "we explore the potential of improving models' reasoning capabilities without new data, merely using automated feedback to the validity of their predictions in arithmetic reasoning (self-training)"
  - [section] "we use the trained model to generate training data... The generated data consists of the original input prompt (xi) and associated model predictions (yi) in the form of a chain-of-thought sequence containing the model's final result at the end. For each prompt, we generate 16 predictions using sampled generation. Annotations of correct results then allow us to automatically annotate each prediction for either being correct (yOKi), or incorrect (yNOKi)"
- Break condition: If correctness feedback is unreliable or noisy, the model may reinforce incorrect reasoning patterns instead of improving.

### Mechanism 2
- Claim: Preference optimization methods outperform supervised fine-tuning in online self-training because they maintain stability and robustness when training on the model's own predictions
- Mechanism: PO methods like DPO, KTO, and IPO optimize for relative preferences between correct and incorrect predictions, which helps prevent catastrophic forgetting and maintains original capabilities while improving
- Core assumption: The stability advantage of PO methods translates to better performance when the model trains on its own outputs compared to SFT
- Evidence anchors:
  - [abstract] "in online self-training, preference optimization methods largely outperform supervised training thanks to their superior stability and robustness on unseen types of problems"
  - [section] "Online Self-training... reveals much larger differences between methods. Supervised fine-tuning (SFT) improves accuracy on simple one-step and two-step datasets (MAWPS, SV AMP, and ASDiv-A) but substantially degrades performance on out-of-distribution GSM8K and AQuA-RAT... Contrary to the SFT, PO methods deliver significant improvements compared to both the base model and their offline variants"
- Break condition: If the model's predictions become too poor quality, even PO methods may struggle to maintain stability.

### Mechanism 3
- Claim: The actuality of self-training feedback (online vs offline) is crucial for effectiveness, with online training showing larger improvements
- Mechanism: Online training provides immediate feedback to the model's current predictions, allowing for more responsive and relevant learning compared to offline training where feedback is based on older model states
- Core assumption: More recent feedback leads to better learning outcomes in self-training scenarios
- Evidence anchors:
  - [abstract] "both variants allow to significantly improve the initial model without any new data. In the offline variant, similar improvements can be achieved by both supervised and preference optimization methods. However, the online variant reveals crucial issues in scaling the supervised training to autonomous settings. On the contrary, preference optimization methods can robustly persist the original capabilities even in autonomous self-training while reaching further improvements"
  - [section] "the difference in average improvement between our best-performing offline (+13.9%) and online method (+25.9%) indicates that the actuality of self-training feedback is a crucial factor of self-training effectivity"
- Break condition: If the model's predictions change too rapidly, online feedback may become inconsistent and harm learning.

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: The paper uses models that generate step-by-step reasoning before producing answers, which is essential for understanding how self-training affects the reasoning process
  - Quick check question: What is the difference between a model that outputs just an answer versus one that shows its reasoning steps?

- Concept: Supervised fine-tuning (SFT)
  - Why needed here: SFT is one of the training methods compared in the experiments, forming the baseline for comparison with preference optimization methods
  - Quick check question: How does SFT differ from standard language model pre-training in terms of training objective?

- Concept: Preference optimization
  - Why needed here: The paper compares three PO methods (DPO, KTO, IPO) against SFT, making understanding these methods crucial for interpreting results
  - Quick check question: What is the key difference between preference optimization and standard supervised learning in terms of training data?

## Architecture Onboarding

- Component map: Model → Prediction Generator → Feedback Annotator → Training Loop → Updated Model
- Critical path: Generate predictions → Evaluate correctness → Select correct predictions → Train model → Repeat
- Design tradeoffs: Offline training is faster but less responsive; online training is slower but more effective
- Failure signatures: Degradation on out-of-distribution datasets, forgetting of response formats, production of incomprehensible rationales
- First 3 experiments:
  1. Implement offline self-training with SFT on a small arithmetic dataset and measure improvement
  2. Add preference optimization methods to the offline training pipeline and compare results
  3. Convert the offline pipeline to online training and observe stability and performance differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of self-training vary across different model scales and architectures?
- Basis in paper: [inferred] The paper experiments with a 3-billion-parameter CALCFORMER model but does not explore whether results generalize to larger models or different architectures like GPT-style transformers
- Why unresolved: The study focuses on a single model architecture (CALCFORMER) and size, leaving open whether the observed improvements from self-training would scale similarly or differently with larger models or alternative architectures
- What evidence would resolve it: Systematic experiments comparing self-training effectiveness across multiple model scales (e.g., 1B, 7B, 70B parameters) and architectures (e.g., GPT-style vs decoder-only) using the same methodology and datasets

### Open Question 2
- Question: What are the long-term effects and potential diminishing returns of continuous online self-training?
- Basis in paper: [explicit] The paper mentions that three subsequent epochs of self-training showed diminishing returns in previous work (Parisi et al., 2022), but does not explore this in their own experiments or investigate the long-term stability of online self-training
- Why unresolved: The experiments focus on initial improvements from self-training but do not investigate whether benefits plateau or degrade over extended training periods, or whether the model's reasoning capabilities stabilize or degrade with prolonged autonomous training
- What evidence would resolve it: Extended online self-training experiments tracking model performance over hundreds or thousands of training steps, measuring both accuracy gains and potential degradation in reasoning quality or robustness to new problem types

### Open Question 3
- Question: How does self-training with preference optimization methods perform on non-arithmetic reasoning tasks?
- Basis in paper: [inferred] The paper demonstrates success with arithmetic reasoning tasks that allow automated feedback, but does not explore whether the observed advantages of PO methods over supervised training generalize to other reasoning domains where feedback may be less objective
- Why unresolved: The study is limited to arithmetic reasoning where correctness can be automatically verified, leaving open whether the superiority of preference optimization methods would extend to domains like commonsense reasoning or logical deduction where feedback is more subjective
- What evidence would resolve it: Experiments applying the same self-training methodology to diverse reasoning tasks (e.g., commonsense QA, logical deduction, scientific reasoning) with appropriate feedback mechanisms, comparing PO methods against supervised training across these domains

## Limitations

- The approach relies on automated correctness feedback, which may not capture the full complexity of arithmetic reasoning errors
- Experiments use a specific model family (FLAN-CALCFORMER) with a relatively small parameter count (3B), limiting generalizability to larger models
- Evaluation focuses on six specific arithmetic datasets, which may not represent the full diversity of reasoning challenges
- The paper doesn't address potential degradation in non-arithmetic capabilities or explore long-term stability beyond immediate training cycles

## Confidence

- High Confidence: The core finding that self-training improves arithmetic reasoning without new data is well-supported by experimental results across multiple datasets and methods
- Medium Confidence: The claim that preference optimization methods outperform SFT in online settings is supported by results but requires further validation across different model sizes and problem domains
- Low Confidence: The assertion that actuality of feedback is the crucial factor in self-training effectiveness lacks detailed ablation studies to isolate this effect from other potential factors

## Next Checks

1. Cross-model validation: Replicate the online self-training experiments with larger models (e.g., 7B, 13B parameters) to verify if observed improvements and method preferences scale consistently with model size

2. Capability retention test: Implement comprehensive evaluations measuring non-arithmetic capabilities before and after self-training to quantify potential negative transfer or capability degradation

3. Feedback mechanism ablation: Design controlled experiments varying the frequency and recency of feedback updates in online training to isolate the contribution of feedback actuality from other factors like training duration or data volume