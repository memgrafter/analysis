---
ver: rpa2
title: 'Get more for less: Principled Data Selection for Warming Up Fine-Tuning in
  LLMs'
arxiv_id: '2405.02774'
source_url: https://arxiv.org/abs/2405.02774
tags:
- data
- selection
- dataset
- arxiv
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of selecting unlabeled open data
  to pre-fine-tune a pre-trained language model for subsequent fine-tuning, aiming
  to minimize reliance on costly domain-specific data while achieving target performance.
  The key idea is to prioritize samples that nudge the pre-training distribution closer
  to the target distribution, rather than simply matching the target distribution.
---

# Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs

## Quick Facts
- arXiv ID: 2405.02774
- Source URL: https://arxiv.org/abs/2405.02774
- Authors: Feiyang Kang; Hoang Anh Just; Yifan Sun; Himanshu Jahagirdar; Yuanzhi Zhang; Rongxing Du; Anit Kumar Sahu; Ruoxi Jia
- Reference count: 40
- One-line primary result: GOT-D reduces toxicity in GPT-2 by 30% with 10K samples and improves average performance across 8 domain tasks by 1.13% with 150K samples.

## Executive Summary
This paper addresses the challenge of selecting unlabeled open data to pre-fine-tune pre-trained language models for subsequent fine-tuning, aiming to minimize reliance on costly domain-specific data while achieving target performance. The key innovation is GOT-D, which uses Optimal Transport (OT) distance to measure distributional discrepancy between candidate and target datasets, then selects samples that most effectively shift the pre-training distribution toward the target distribution. This approach is shown to be optimal for fine-tuning tasks under conditions of light fine-tuning, and the method scales efficiently to millions of samples within a single GPU hour.

## Method Summary
GOT-D leverages Optimal Transport distance to quantify the distributional discrepancy between a candidate dataset (approximating pre-training data) and target task data. The method computes the OT distance and extracts gradients that indicate which samples most effectively pull the pre-training distribution closer to the target. These gradients guide the selection of the most impactful samples for pre-fine-tuning. The approach is grounded in the theoretical insight that when a pre-trained model undergoes light fine-tuning, the effective data distribution is a weighted combination of pre-training and fine-tuning data, making distribution-nudging more effective than direct matching.

## Key Results
- Reduces toxicity level of GPT-2 by 30% with only 10K samples
- Improves average performance across 8 domain-specific tasks by 1.13% with 150K samples
- Scales to millions of samples within a single GPU hour, significantly faster than existing techniques
- Consistently outperforms other selection methods across diverse tasks (NLU, NLG, zero-shot) with models up to 2.7B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting samples that pull the pre-training distribution closer to the target distribution is more effective than direct distribution matching when fine-tuning a pre-trained model.
- Mechanism: The fine-tuned model's effective data distribution is a weighted combination of the pre-training and fine-tuning data (λ·DU + (1-λ)·DP). Minimizing OT distance between this combined distribution and the target distribution is more aligned with improving downstream task performance than minimizing OT distance between fine-tuning data alone and the target distribution.
- Core assumption: The model undergoes "light fine-tuning" (single epoch or few epochs) with a small data budget (N(DU) ≪ N(DP)), making the weighted ratio λ reasonably small.
- Evidence anchors:
  - [abstract]: "our key idea is to select data that nudges the pre-training distribution closer to the target distribution."
  - [section]: "our key idea is to prioritize samples that most effectively shift the pre-training distribution closer to the target data distribution."
- Break condition: If fine-tuning is extensive (many epochs, large learning rate) or the fine-tuning data budget is comparable to the pre-training data, the weighted combination effect diminishes and direct distribution matching may become more effective.

### Mechanism 2
- Claim: The gradient of the OT distance with respect to the probability mass of samples in the candidate dataset provides the direction that most rapidly decreases the OT distance to the target distribution.
- Mechanism: By solving the OT problem between the candidate dataset (proxy for pre-training) and the target data, we obtain a dual solution that encodes the gradient. Samples with the largest negative gradients, when increased in presence, will most rapidly decrease the OT distance to the target task.
- Core assumption: The candidate dataset approximately matches the distribution of the pre-training data (OT(DP, DS) ≤ ε for small ε).
- Evidence anchors:
  - [abstract]: "We measure the distance between the candidate and target datasets using the Optimal Transport (OT) distance. The direction that pulls one distribution to another can be found through the gradient of the distance."
  - [section]: "The partial differentiation in Eq. equation 4 is the gradient∇DS OT(DS, DR) of the OT distance w.r.t. the probability mass of each sample in DS."
- Break condition: If the candidate dataset does not well-approximate the pre-training distribution (ε is large), the gradient direction may not effectively pull towards the target distribution.

### Mechanism 3
- Claim: The proposed method (GOT-D) scales efficiently to millions of samples within a single GPU hour, making it practical for large-scale language data selection.
- Mechanism: By leveraging optimization techniques like entropy regularization, momentum, and parallel GPU computations, the dual solution of the OT problem can be efficiently computed for large datasets. The selection process only requires solving a single OT problem, ranking gradients, and selecting top samples.
- Core assumption: Modern OT solvers (e.g., ott-jax) with GPU acceleration can handle the computational load for millions of samples.
- Evidence anchors:
  - [abstract]: "Moreover, our proposed method is significantly faster than existing techniques, scaling to millions of samples within a single GPU hour."
  - [section]: "By integrating optimization techniques like entropy regularization (Cuturi, 2013) and momentum (Sutskever et al., 2013) and leveraging parallel GPU computations, we can efficiently calculate the dual solution of OT for datasets comprising millions of samples, completing the selection within a few minutes on a single GPU."
- Break condition: If the dataset size grows beyond the capability of the OT solver or available GPU memory, or if the embedding computation becomes a bottleneck, the scalability may be compromised.

## Foundational Learning

- Concept: Optimal Transport (OT) Distance
  - Why needed here: OT distance is used as a principled measure to quantify the distributional discrepancy between the candidate dataset (proxy for pre-training) and the target data, which correlates with model performance on the target task.
  - Quick check question: What is the mathematical definition of OT distance and how does it differ from other distributional discrepancy measures like KL divergence or MMD?

- Concept: Kantorovich-Rubinstein Duality
  - Why needed here: This duality provides an upper bound on the difference between training and validation loss of a model in terms of the OT distance between training and validation data, forming the theoretical foundation for data selection based on OT distance.
  - Quick check question: How does the Kantorovich-Rubinstein duality relate the OT distance between two distributions to the difference in model performance when trained on one and evaluated on the other?

- Concept: Gradient of OT Distance
  - Why needed here: The gradient of the OT distance with respect to the probability mass of samples in the candidate dataset gives the direction that most rapidly decreases the OT distance to the target distribution, guiding the data selection process.
  - Quick check question: How is the gradient of the OT distance encoded in the dual solution of the OT problem, and how can it be efficiently computed for large-scale datasets?

## Architecture Onboarding

- Component map: Pre-trained model -> Candidate dataset (DS) -> Embedding (distilled-BERT) -> OT solver (ott-jax) -> Gradient extraction -> Selected samples -> Pre-fine-tuning -> Fine-tuning on target data
- Critical path: Embedding → OT computation → Gradient extraction → Data selection
- Design tradeoffs:
  - Embedding space choice (e.g., distilled-BERT vs. sentence-transformer) affects both quality and speed
  - Selection budget vs. performance gain tradeoff
  - Single vs. multiple domain candidate datasets
- Failure signatures:
  - Poor performance: Selected data too similar to pre-training data, candidate dataset not representative of pre-training, embedding space not discriminative enough
  - Slow runtime: Embedding computation bottleneck, OT solver unable to handle dataset size, insufficient GPU memory
  - Numerical instability: Extreme selection ratios, OT solver convergence issues
- First 3 experiments:
  1. Ablation study on embedding space (distilled-BERT vs. sentence-transformer) to assess impact on selection quality and runtime
  2. Sensitivity analysis on selection budget (e.g., 5K, 20K, 40K samples) to find optimal tradeoff for a given task
  3. Comparison of single vs. multiple domain candidate datasets to evaluate the effect of domain relevance test on selection quality

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely on strong assumptions about "light fine-tuning" (single epoch, small learning rate, N(DU) ≪ N(DP))
- Assumes candidate dataset approximates pre-training distribution, which may not hold for all pre-trained models
- Limited to cases where embedding-based distributional matching is meaningful - may not generalize well to tasks where semantic similarity is not well-captured by chosen embedding space

## Confidence
- **High confidence**: Computational efficiency claims (scaling to millions of samples within GPU hour) are well-supported by modern OT solver integration with GPU acceleration. Comparative performance results across multiple tasks and model sizes are empirically validated.
- **Medium confidence**: Theoretical foundation connecting OT distance minimization to downstream performance relies on Kantorovich-Rubinstein duality under specific fine-tuning conditions. Mathematical framework is sound but practical impact of violating conditions needs more validation.
- **Medium confidence**: Claim that nudging pre-training distribution is more effective than direct distribution matching is supported by theoretical analysis and empirical results, but mechanism's robustness across diverse fine-tuning scenarios requires further investigation.

## Next Checks
1. **Robustness testing**: Evaluate GOT-D performance when fine-tuning data budget approaches or exceeds pre-training data size (N(DU) ≫ N(DP)), testing boundaries of "light fine-tuning" assumption.

2. **Distribution approximation analysis**: Systematically vary quality of candidate dataset's approximation to pre-training distribution (controlled ε values) to quantify impact on selection effectiveness and identify breaking points.

3. **Embedding space sensitivity**: Conduct comprehensive ablation study across different embedding methods (distilled-BERT, sentence-transformers, domain-specific embeddings) to determine which embedding spaces provide most robust distributional alignment for various task types.