---
ver: rpa2
title: 'InversionView: A General-Purpose Method for Reading Information from Neural
  Activations'
arxiv_id: '2405.17653'
source_url: https://arxiv.org/abs/2405.17653
tags:
- information
- activation
- query
- digits
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces InversionView, a method for interpreting\
  \ information encoded in neural network activations by sampling from a decoder conditioned\
  \ on those activations. The core idea is to treat the information in an activation\
  \ as its preimage\u2014the set of inputs that produce similar activations\u2014\
  and explore this preimage through conditional generation."
---

# InversionView: A General-Purpose Method for Reading Information from Neural Activations

## Quick Facts
- arXiv ID: 2405.17653
- Source URL: https://arxiv.org/abs/2405.17653
- Reference count: 40
- One-line primary result: Introduces a method to interpret neural activations by sampling from a decoder conditioned on those activations, validated across four case studies including character counting, IOI, 3-digit addition, and factual recall.

## Executive Summary
This paper introduces InversionView, a method for interpreting information encoded in neural network activations by sampling from a decoder conditioned on those activations. The core idea is to treat the information in an activation as its preimage—the set of inputs that produce similar activations—and explore this preimage through conditional generation. Across four case studies (character counting, IOI, 3-digit addition, and factual recall), the authors demonstrate that InversionView can reveal what information is encoded at each activation site, how it changes across layers, and what algorithm the model implements. For example, in the character counting task, the method uncovers that the model first counts target character occurrences in layer 0 and then moves this count information to another position in layer 1. In IOI, it confirms that certain attention heads copy subject or object names as hypothesized by prior work. The approach is validated with causal interventions and shown to scale to larger models like GPT-2 XL. The authors also provide a proof of concept for automated interpretation using LLMs. Limitations include reliance on a black-box decoder and the need for careful threshold selection.

## Method Summary
InversionView interprets neural activations by treating them as preimages and sampling from a trained decoder conditioned on those activations. The method involves training a decoder model on activation-input pairs, then using it to generate samples for specific query activations. By analyzing commonalities in the generated samples within a distance threshold, researchers can infer what information is encoded at each activation site. The approach is validated through causal interventions and shown to work across different tasks and model sizes.

## Key Results
- Reveals clear information contained in activations, including basic token information, counts, relative positions, and abstract knowledge
- Demonstrates layer-wise information flow, such as count information moving from layer 0 to layer 1 in the character counting task
- Confirms prior hypotheses about IOI circuits, showing specific attention heads copy subject/object names
- Shows scalability to GPT-2 XL (1.5B parameters) while maintaining interpretability quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Information encoded in neural activations can be revealed by sampling from a decoder conditioned on those activations.
- **Mechanism**: The preimage of an activation (the set of inputs producing similar activations) embodies the information content. A trained decoder generates inputs from this preimage, allowing human inspection of commonalities to infer encoded information.
- **Core assumption**: Activations sensitive to certain input changes encode information about those changes; the preimage captures all such information.
- **Evidence anchors**:
  - [abstract] "we argue that this information is embodied by the subset of inputs that give rise to similar activations"
  - [section] "InversionView aims at providing a direct way of reading out the information encoded in an activation"
  - [corpus] Weak - related works focus on activations but not conditional sampling approach
- **Break condition**: If decoder fails to generate diverse samples from the preimage, or if preimage contains inputs with conflicting information making interpretation ambiguous.

### Mechanism 2
- **Claim**: Information abstraction and forgetting across layers can be observed through changes in preimage composition.
- **Mechanism**: As information flows through layers, certain details are preserved while others are abstracted. Comparing preimages across activation sites reveals which information persists and which is discarded.
- **Core assumption**: Different layers implement different levels of abstraction, and this process is reflected in how preimages change.
- **Evidence anchors**:
  - [abstract] "In these studies, we show that InversionView can reveal clear information contained in activations, including basic information about tokens appearing in the context, as well as more complex information, such as the count of certain tokens, their relative positions, and abstract knowledge"
  - [section] "comparing the ϵ-preimage before and after the MLP in layer 0, we find that the MLP makes the count information more prominent"
  - [corpus] Weak - related works don't explicitly address layer-wise abstraction through preimage analysis
- **Break condition**: If model doesn't implement clear abstraction patterns, or if preimages remain too similar across layers to distinguish information changes.

### Mechanism 3
- **Claim**: Causal verification can confirm hypotheses about information flow generated through InversionView.
- **Mechanism**: Activation patching can test whether specific components causally affect predictions, validating the information flow patterns inferred from preimage analysis.
- **Core assumption**: If a component is necessary for routing specific information, patching it with alternative activations will change model predictions accordingly.
- **Evidence anchors**:
  - [abstract] "We also provide causally verified circuits to confirm the decoded information"
  - [section] "We causally verified our hypothesis using activation patching [53, 21] on (position, head output) pairs"
  - [corpus] Weak - related works use patching but not specifically to verify preimage-based hypotheses
- **Break condition**: If activation patching fails to confirm hypotheses, suggesting either incorrect preimage interpretation or that model uses different mechanisms than inferred.

## Foundational Learning

- **Concept**: Vector representation geometry and distance metrics
  - Why needed here: Understanding how activations encode information requires knowledge of how vectors represent different inputs and how distances between them reflect information similarity
  - Quick check question: If two activations have small distance, what does this imply about the information they encode?

- **Concept**: Conditional generation and autoregressive models
  - Why needed here: The decoder must generate inputs conditioned on activations, requiring understanding of how language models can be conditioned on additional context
  - Quick check question: How does conditioning a decoder on activations differ from standard language model generation?

- **Concept**: Causal intervention and activation patching
  - Why needed here: Verifying hypotheses about information flow requires understanding how to causally test component importance
  - Quick check question: What does it mean if patching an activation has no effect on model output?

## Architecture Onboarding

- **Component map**: Cached activations -> Trained decoder -> Distance metric -> Sampling configuration -> Filtered samples -> Human analysis -> Causal verification
- **Critical path**: 
  1. Cache activations from probed model
  2. Train decoder on activation-input pairs
  3. Sample from decoder conditioned on query activation
  4. Filter samples by distance threshold
  5. Analyze commonalities in filtered samples
  6. Verify hypotheses with causal interventions

- **Design tradeoffs**:
  - Decoder capacity vs. training efficiency
  - Temperature/noise settings vs. sample diversity
  - Distance threshold vs. preimage size
  - Activation site selection vs. interpretability

- **Failure signatures**:
  - Decoder generates samples far from query activation
  - Preimages contain conflicting or ambiguous information
  - Causal interventions fail to confirm hypotheses
  - Interpretation becomes too complex or contradictory

- **First 3 experiments**:
  1. Character counting task on small transformer to validate basic mechanism
  2. IOI circuit analysis on GPT-2 small to test scalability
  3. 3-digit addition to explore complex algorithmic patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can InversionView be scaled to very large language models (e.g., GPT-4 class) while maintaining interpretability quality?
- Basis in paper: [inferred] The paper states "We focus on models up to 1.5B parameters; scaling the technique to large models is an interesting problem for future work" and notes limitations regarding larger models.
- Why unresolved: The current implementation relies on training a decoder model that needs to learn an inverse mapping from activations to inputs. For very large models, this mapping becomes more complex and requires more computational resources. The paper only tests up to GPT-2 XL (1.5B parameters).
- What evidence would resolve it: Successful application of InversionView to models like GPT-4 (with 1T+ parameters) while maintaining comparable interpretability quality to smaller models, or empirical evidence showing where and why the method breaks down at scale.

### Open Question 2
- Question: How does the choice of distance metric D(·, ·) affect the geometry of the preimage and the resulting interpretations?
- Basis in paper: [explicit] "The geometry, however, in general could be nonisotropic and treating each dimension equally could be sub-optimal. We leave the exploration of this to future work." The paper uses different metrics (Euclidean, normalized Euclidean, cosine) but doesn't systematically explore this choice.
- Why unresolved: Different distance metrics can capture different aspects of representation similarity, and the paper uses different metrics across different tasks without providing guidance on when to use which. The impact on interpretation quality is not systematically studied.
- What evidence would resolve it: Systematic comparison showing how different distance metrics affect the preimage geometry, the completeness of sampling, and the resulting interpretations across various tasks and model types.

### Open Question 3
- Question: What is the optimal trade-off between decoder model complexity and interpretability quality?
- Basis in paper: [explicit] "We use GPT-2 Medium as the backbone model for decoder" for the factual recall task, while using a small 2-layer transformer for other tasks. The paper notes that "Preliminary experiments revealed that it is necessary to restrict the number of activation sites" due to computational constraints.
- Why unresolved: The paper uses different decoder architectures across tasks without systematic justification. There's an implicit trade-off between decoder capacity (which affects the completeness of the preimage sampling) and computational efficiency that is not explored.
- What evidence would resolve it: Empirical study showing how decoder model size, architecture, and training data affect the quality of interpretations (e.g., completeness of preimage sampling, faithfulness of generated samples) across various tasks and model sizes.

### Open Question 4
- Question: Can the automated interpretability pipeline using LLMs be made reliable and consistent enough for practical use?
- Basis in paper: [explicit] "Despite some flaws, the outcome is informative in general, suggesting this as a promising direction for further speeding up hypothesis generation." The paper presents a proof of concept but acknowledges issues like inconsistent outputs and verbose interpretations.
- Why unresolved: The current LLM-based interpretation is described as having "some problems" including spurious claims, inconsistency between runs, and verbosity. The paper doesn't provide solutions to these issues.
- What evidence would resolve it: Development of a standardized prompt engineering approach that produces consistent, concise, and accurate interpretations across different tasks and activation sites, with quantifiable measures of reliability and accuracy compared to human interpretations.

## Limitations
- Heavy dependence on decoder quality - poor decoder training leads to misleading interpretations
- Critical choice of distance threshold ε affects which inputs are considered part of the preimage
- Interpretation process remains somewhat subjective, relying on human pattern recognition
- Substantial computational resources required for training decoders and generating samples

## Confidence
- **High confidence**: The core mechanism of using conditional generation to explore preimages is well-supported by empirical results across all four case studies. The character counting and IOI experiments show clear, interpretable patterns.
- **Medium confidence**: The scalability claims to larger models like GPT-2 XL are supported but require more extensive validation across diverse model architectures and tasks.
- **Medium confidence**: The automated interpretation using LLMs shows promise but needs systematic evaluation to assess reliability compared to human interpretation.
- **Low confidence**: The method's performance on highly abstract or multi-modal information encoding remains untested.

## Next Checks
1. **Cross-architecture validation**: Apply InversionView to a BERT-based model on a masked language modeling task to test generalizability beyond GPT-style transformers and synthetic datasets.

2. **Decoder fidelity analysis**: Systematically vary decoder architecture capacity and training data size to quantify how these factors affect the quality and interpretability of generated samples, establishing minimum requirements for reliable analysis.

3. **Automated vs. human interpretation comparison**: Design a benchmark where both human experts and LLM-based interpreters analyze the same activation sites, measuring agreement rates and identifying systematic differences in interpretation quality.