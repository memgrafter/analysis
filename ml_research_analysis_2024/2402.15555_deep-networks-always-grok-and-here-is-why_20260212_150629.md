---
ver: rpa2
title: Deep Networks Always Grok and Here is Why
arxiv_id: '2402.15555'
source_url: https://arxiv.org/abs/2402.15555
tags:
- training
- figure
- local
- complexity
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that grokking\u2014delayed generalization\
  \ in deep neural networks\u2014is a widespread phenomenon, not limited to specific\
  \ controlled settings. The authors show that grokking manifests as \"delayed robustness,\"\
  \ where networks become robust to adversarial examples long after interpolating\
  \ training data."
---

# Deep Networks Always Grok and Here is Why

## Quick Facts
- arXiv ID: 2402.15555
- Source URL: https://arxiv.org/abs/2402.15555
- Authors: Ahmed Imtiaz Humayun; Randall Balestriero; Richard Baraniuk
- Reference count: 40
- This paper demonstrates that grokking—delayed generalization in deep neural networks—is a widespread phenomenon, not limited to specific controlled settings.

## Executive Summary
This paper reveals that grokking, traditionally observed as delayed generalization after training data interpolation, is actually a universal phenomenon in deep networks that manifests as "delayed robustness" to adversarial examples. The authors introduce a novel local complexity measure based on the density of linear regions (spline partition regions) in the input space, which serves as a task-agnostic progress measure for training. Through extensive experiments across various architectures and datasets, they show that grokking occurs during a "region migration" phase when linear regions migrate from training points toward the decision boundary, creating a robust input space partition. The paper demonstrates that increasing model depth, width, or batch size expedites grokking, while batch normalization and weight decay can delay or remove it entirely.

## Method Summary
The paper introduces a local complexity (LC) measure that quantifies the density of linear regions around specific points in the input space by counting neuron hyperplane intersections within local neighborhoods. Using cross-polytopal frames centered on data points with configurable radius and dimensionality, the method tracks how the network's partition evolves during training. The LC dynamics reveal three distinct phases: initial descent (parameter optimization reducing complexity), ascent (accumulation of nonlinearities around data points), and region migration (second descent where linear regions migrate from training points toward the decision boundary). This LC measure serves as a progress metric that predicts grokking onset by identifying when region migration occurs, which precedes both generalization and robustness improvements.

## Key Results
- Grokking manifests as delayed robustness to adversarial examples, occurring post-interpolation across diverse architectures and datasets
- Local complexity exhibits three-phase dynamics (initial descent, ascent, region migration) that predict grokking timing
- Increasing model depth, width, or batch size accelerates grokking by expediting region migration
- Batch normalization and weight decay can prevent or delay grokking by interfering with LC dynamics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Delayed robustness occurs because neural networks undergo a phase transition where linear regions migrate from training points toward the decision boundary, creating a robust input space partition.
- Mechanism: During training, the density of linear regions around training points initially increases (ascent phase), then decreases as regions migrate away from training points and concentrate around the decision boundary (second descent/ region migration phase). This migration creates larger linear regions around training data and accumulates nonlinearities near the decision boundary, resulting in both generalization and robustness.
- Core assumption: The local complexity measure based on linear region density accurately reflects the network's functional smoothness and expressivity around training points and the decision boundary.
- Evidence anchors:
  - [abstract] "We provide the first evidence that, for classification problems, the linear regions undergo a phase transition during training whereafter they migrate away from the training samples (making the DNN mapping smoother there) and towards the decision boundary (making the DNN mapping less smooth there)."
  - [section] "Grokking occurs post phase transition as a robust partition of the input space thanks to the linearization of the DNN mapping around the training points."
  - [corpus] Weak - related papers discuss grokking phenomena but don't specifically address the linear region migration mechanism.
- Break condition: If the local complexity measure fails to correlate with training dynamics or if region migration does not precede robustness, the mechanism breaks down.

### Mechanism 2
- Claim: The double descent behavior of local complexity (first descent, ascent, second descent) is causally linked to the emergence of delayed generalization and robustness.
- Mechanism: The first descent represents initial parameter optimization reducing complexity. The ascent phase accumulates nonlinearities around data points, creating high expressivity but poor generalization. The second descent (region migration) reorganizes the partition, reducing complexity around training points while concentrating it at the decision boundary, enabling both generalization and robustness.
- Core assumption: The three-phase LC dynamics are universal across different architectures and datasets, and the second descent specifically triggers grokking.
- Evidence anchors:
  - [abstract] "We develop an analytical explanation for the emergence of both delayed generalization and delayed robustness based on the local complexity of a DNN's input-output mapping."
  - [section] "We see three distinct phases in the dynamics of local complexity: The first descent... The ascent phase... The second descent phase or region migration phase..."
  - [corpus] Weak - corpus neighbors discuss grokking but don't provide evidence for the specific three-phase LC dynamics mechanism.
- Break condition: If networks can achieve generalization and robustness without undergoing the described LC double descent pattern, the mechanism fails.

### Mechanism 3
- Claim: Batch normalization prevents grokking by explicitly adapting the partition to keep boundaries close to training data, preventing region migration.
- Mechanism: Batch normalization dynamically updates normalization parameters for each mini-batch, centering and scaling layer pre-activations. This forces partition boundaries to remain near training data rather than migrating toward the decision boundary, thus preventing the robust partition formation necessary for grokking.
- Core assumption: Batch normalization's effect on partition geometry directly prevents the LC dynamics that lead to region migration.
- Evidence anchors:
  - [section] "In Appendix B, we show that at each layer ℓ of a DN, BN explicitly adapts the partition so that the partition boundaries are as close to the training data as possible."
  - [section] "Batch normalization removes grokking. In Appendix B, we show that at each layer ℓ of a DN, BN explicitly adapts the partition so that the partition boundaries are as close to the training data as possible."
  - [corpus] Weak - related papers don't specifically address batch normalization's effect on partition geometry and grokking.
- Break condition: If batch normalization can be modified to allow region migration while retaining its benefits, the mechanism breaks.

## Foundational Learning

- Concept: Spline partition theory and continuous piecewise affine spline operators
  - Why needed here: The entire analysis framework relies on viewing deep networks as continuous piecewise affine splines with linear regions forming a partition of the input space.
  - Quick check question: Can you explain how a ReLU network creates a piecewise affine mapping and what constitutes a "linear region" in this context?

- Concept: Local complexity as a measure of functional smoothness
  - Why needed here: Local complexity quantifies the density of linear regions around specific points, serving as the progress measure that reveals training dynamics and predicts grokking onset.
  - Quick check question: How does counting neuron hyperplane intersections in a local neighborhood provide an approximation of the network's local complexity?

- Concept: VC-dimension and expressive power relationships
  - Why needed here: The paper connects local complexity to the network's expressive power and its ability to form decision boundaries, linking geometric properties to learning capacity.
  - Quick check question: How does the number of linear regions in a network's partition relate to its VC-dimension and classification capacity?

## Architecture Onboarding

- Component map:
  Input space sampling (cross-polytopal frames) -> Layerwise complexity computation (hyperplane intersections) -> Progress tracking (LC monitoring) -> Visualization (SplineCam)

- Critical path: Sample neighborhoods → Embed through network layers → Count hyperplane intersections → Track LC dynamics → Identify phase transitions → Predict grokking

- Design tradeoffs:
  - Radius r vs. accuracy: Larger r increases coverage but reduces locality; r ≤ 0.014 recommended for deeper networks
  - Dimensionality P vs. computation: Higher P gives better coverage but increases computation; P=25 often sufficient
  - Train vs. test samples: Both needed to distinguish memorization from generalization

- Failure signatures:
  - LC remains constant throughout training (no phase transitions)
  - No correlation between LC dynamics and generalization/robustness
  - Batch normalization present prevents LC double descent
  - Overparameterization eliminates LC ascent phase

- First 3 experiments:
  1. Implement LC computation for a simple MLP on MNIST with varying depths, tracking LC vs. accuracy to observe the three-phase dynamics.
  2. Add batch normalization to the same setup and verify that LC no longer shows the double descent pattern and grokking is prevented.
  3. Train with varying weight decay to observe its effect on LC dynamics and grokking timing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical explanation for why the second descent (region migration) occurs so late in the training process?
- Basis in paper: [explicit] The authors state "it is not clear why that migration occurs so late in the training process, and we hope to study that in future research."
- Why unresolved: The paper provides empirical evidence of region migration but lacks a theoretical justification for the timing of this phenomenon.
- What evidence would resolve it: A theoretical model explaining the training dynamics that predicts when region migration should occur, validated against empirical observations.

### Open Question 2
- Question: How do different optimizers (beyond Adam) affect the local complexity dynamics and grokking behavior?
- Basis in paper: [explicit] "There can be possible connections between region migration and neural collapse which are not explored in this paper."
- Why unresolved: The paper primarily uses Adam optimizer and does not explore the effects of other optimizers on grokking.
- What evidence would resolve it: Empirical studies comparing local complexity dynamics and grokking across multiple optimizers (SGD, RMSprop, etc.) under controlled conditions.

### Open Question 3
- Question: What is the relationship between region migration and neural collapse phenomena?
- Basis in paper: [explicit] "There can be possible connections between region migration and neural collapse (Papyan et al., 2020) which are not explored in this paper."
- Why unresolved: The paper mentions this potential connection but does not investigate it.
- What evidence would resolve it: Experimental analysis showing correlation or causation between region migration dynamics and neural collapse patterns across different architectures and datasets.

## Limitations

- The local complexity measure, while intuitive, lacks theoretical guarantees for approximating the true partition complexity, particularly for deep networks where exact computation is intractable.
- The mechanism connecting region migration to robustness is primarily correlational - while the timing aligns, causal evidence is limited.
- The paper doesn't fully explore why some datasets/architectures show clearer grokking than others, suggesting potential confounding factors not addressed.

## Confidence

- Delayed robustness as universal grokking manifestation: High
- Three-phase LC dynamics as training progress measure: Medium
- Region migration mechanism for robustness: Medium
- Batch normalization preventing grokking: Medium

## Next Checks

1. Conduct ablation studies on initialization schemes and optimizer choices to determine if LC dynamics and grokking timing are robust to these factors.
2. Implement exact LC computation for small networks to validate the cross-polytopal frame approximation method and establish error bounds.
3. Design experiments to directly test the causal relationship between region migration and robustness by artificially constraining linear region movement during training.