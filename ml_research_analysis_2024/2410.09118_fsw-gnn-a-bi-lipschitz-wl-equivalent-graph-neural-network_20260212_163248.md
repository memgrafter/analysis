---
ver: rpa2
title: 'FSW-GNN: A Bi-Lipschitz WL-Equivalent Graph Neural Network'
arxiv_id: '2410.09118'
source_url: https://arxiv.org/abs/2410.09118
tags:
- graph
- graphs
- fsw-gnn
- same
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FSW-GNN, a novel graph neural network (GNN)
  architecture that is both Weisfeiler-Lehman (WL) equivalent and bi-Lipschitz with
  respect to standard graph metrics. The key innovation is using Fourier Sliced-Wasserstein
  (FSW) embeddings for message aggregation, which provides bi-Lipschitz properties.
---

# FSW-GNN: A Bi-Lipschitz WL-Equivalent Graph Neural Network

## Quick Facts
- arXiv ID: 2410.09118
- Source URL: https://arxiv.org/abs/2410.09118
- Reference count: 33
- Primary result: FSW-GNN achieves competitive performance on standard graph learning tasks and significantly outperforms other MPNNs on long-range tasks by maintaining bi-Lipschitz properties.

## Executive Summary
This paper introduces FSW-GNN, a novel graph neural network architecture that combines Weisfeiler-Lehman (WL) equivalence with bi-Lipschitz continuity. The key innovation is using Fourier Sliced-Wasserstein (FSW) embeddings for message aggregation, which preserves distances between graphs in the embedding space. Unlike standard summation-based MPNNs that can map WL-separable graphs to nearly identical features, FSW-GNN maintains the metric structure of graphs. The method is theoretically proven to be bi-Lipschitz with respect to both the DS metric and Tree Mover's Distance (TMD), and achieves competitive performance on standard benchmarks while significantly outperforming other MPNNs on long-range tasks that require deep message passing.

## Method Summary
FSW-GNN is a graph neural network that uses Fourier Sliced-Wasserstein (FSW) embeddings for both message aggregation and graph-level readout. The architecture performs T iterations of message passing where each node's features are updated using an FSW embedding of its neighbors' features, processed through MLPs. The final graph embedding is obtained by applying another FSW embedding to the node features followed by an MLP. The FSW embedding transforms multisets into Euclidean space while preserving distances, ensuring that graph distances in the embedding space remain proportional to original graph distances. The method is proven to be both WL-equivalent (can distinguish any pair of graphs that the WL test can distinguish) and bi-Lipschitz continuous with respect to standard graph metrics.

## Key Results
- FSW-GNN achieves competitive performance on standard graph learning tasks including Peptides-func, Peptides-struct, and MolHIV datasets
- Significantly outperforms other MPNNs on long-range tasks, particularly the NeighborsMatch task from Alon & Yahav (2020)
- Maintains stable performance with increasing message-passing depth, avoiding the Holder exponent deterioration seen in standard MPNNs
- Theoretical guarantees of WL-equivalence and bi-Lipschitz properties with respect to DS metric and Tree Mover's Distance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: FSW-GNN maintains bi-Lipschitz properties by using Fourier Sliced-Wasserstein (FSW) embeddings for message aggregation, unlike standard summation-based MPNNs.
- **Mechanism**: FSW embeddings transform multisets into Euclidean space while preserving distances, ensuring that graph distances in the embedding space remain proportional to original graph distances.
- **Core assumption**: FSW embeddings are bi-Lipschitz for multisets, and the WL equivalence of FSW-GNN ensures that zero distances in the graph metric correspond to zero distances in the embedding.
- **Evidence anchors**:
  - [abstract] "Unlike standard summation-based MPNNs that can map WL-separable graphs to nearly identical features, FSW-GNN maintains distances between graphs in the embedding space."
  - [section] "The FSW embedding maps input multisets {x1, . . . ,xn}, with x1, . . . ,xn ∈ Rd, to output vectors z = (z1, . . . , zm) ∈ Rm."

### Mechanism 2
- **Claim**: FSW-GNN avoids the deterioration of the Holder exponent seen in standard MPNNs as depth increases, maintaining stable performance in deep message-passing scenarios.
- **Mechanism**: Standard MPNNs are lower-Holder with an exponent that worsens with depth, causing long-range dependencies to be poorly captured. FSW-GNN's bi-Lipschitz property prevents this exponential degradation, preserving long-range information.
- **Core assumption**: The bi-Lipschitz property of FSW-GNN is maintained for any finite number of message-passing iterations.
- **Evidence anchors**:
  - [abstract] "Empirically, FSW-GNN achieves competitive performance on standard graph learning tasks and significantly outperforms other MPNNs on long-range tasks, which require deep message passing."
  - [section] "This improvement is attributed to FSW-GNN's bi-Lipschitz property, which avoids the deterioration of the Holder exponent seen in standard MPNNs as depth increases."

### Mechanism 3
- **Claim**: FSW-GNN is WL-equivalent, meaning it can distinguish any pair of graphs that the Weisfeiler-Lehman test can distinguish.
- **Mechanism**: The FSW-GNN architecture, with appropriate MLP sizes and number of iterations, ensures that the graph embedding is injective on WL-distinguishable graphs, providing strong separation power.
- **Core assumption**: The FSW-GNN uses sufficient dimensions for the FSW embeddings and linear functions for the MLPs to achieve WL equivalence.
- **Evidence anchors**:
  - [section] "Theorem 3.2 (Informal). Consider the FSW-GNN architecture for input graphs in G≤N(Rd), with T = N iterations, where Φ(t), Ψ are just linear functions, and all features (except for input features) are of dimension m ≥ 2N d + 2. Then for Lebesgue almost every choice of model parameters, the graph embedding defined by the architecture is WL equivalent."

## Foundational Learning

- **Concept**: Weisfeiler-Lehman (WL) graph isomorphism test
  - **Why needed here**: Understanding the WL test is crucial because FSW-GNN's performance is compared to WL-equivalent MPNNs, and the paper discusses WL metrics and WL equivalence.
  - **Quick check question**: What is the main limitation of standard MPNNs in terms of graph separation, and how does WL equivalence address this limitation?

- **Concept**: Bi-Lipschitz continuity
  - **Why needed here**: Bi-Lipschitz continuity ensures that distances in the graph space are preserved in the embedding space, which is a key property of FSW-GNN.
  - **Quick check question**: How does bi-Lipschitz continuity differ from Lipschitz continuity, and why is it important for graph neural networks?

- **Concept**: Fourier Sliced-Wasserstein (FSW) embedding
  - **Why needed here**: FSW embeddings are the core innovation in FSW-GNN, providing the bi-Lipschitz property and handling multisets of varying sizes.
  - **Quick check question**: Describe the three steps of the FSW embedding process and explain how it differs from standard sorting-based embeddings.

## Architecture Onboarding

- **Component map**: Input graphs -> FSW embeddings (EFSW) -> Node updates via MLPs (Φ(t)) -> Graph-level readout via FSW embedding (EGlob) and MLP (Ψ) -> Final graph embedding
- **Critical path**: Input graphs → FSW embeddings → Node updates via MLPs → Graph-level readout via FSW embedding and MLP → Final graph embedding
- **Design tradeoffs**: FSW-GNN trades computational complexity for bi-Lipschitz properties and improved long-range performance. The FSW embeddings are more expensive than standard summation-based aggregations but provide better distance preservation.
- **Failure signatures**: If the FSW-GNN fails to outperform standard MPNNs on long-range tasks, it may indicate that the bi-Lipschitz property is not being maintained due to improper parameter choices or insufficient dimensions.
- **First 3 experiments**:
  1. **NeighborsMatch task**: Test FSW-GNN's ability to handle long-range dependencies by varying the problem radius r and comparing accuracy to standard MPNNs.
  2. **Ring graph transfer task**: Evaluate FSW-GNN's performance on a synthetic long-range task with a ring graph topology and varying message-passing depths.
  3. **Over-smoothing analysis**: Measure the Mean Average Distance (MAD) metric for FSW-GNN and standard MPNNs on the Ring task to assess over-smoothing behavior and its correlation with performance.

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the precise rate of distortion growth for FSW-GNN as depth increases?
  - **Basis in paper**: [inferred] The paper conjectures that the bi-Lipschitz distortion of FSW-GNN should grow exponentially with depth T, but notes that empirical results suggest a much slower rate.
  - **Why unresolved**: The theoretical analysis hasn't been completed to determine the exact relationship between depth and distortion.
  - **What evidence would resolve it**: Formal proof of the distortion growth rate and experimental validation showing the actual distortion behavior across different depths.

- **Open Question 2**: How does FSW-GNN compare to graph rewiring methods and spectral filter approaches for long-range tasks?
  - **Basis in paper**: [explicit] The paper mentions that FSW-GNN performs well with the given graph topology, unlike methods that reduce radius or add global information.
  - **Why unresolved**: No direct comparison experiments were conducted between FSW-GNN and these alternative approaches.
  - **What evidence would resolve it**: Benchmark experiments comparing FSW-GNN with graph rewiring and spectral filter methods on the same long-range tasks.

## Limitations

- Theoretical guarantees rely on specific conditions (proper FSW parameters, sufficient dimensions) that may not hold in practice
- Computational complexity is higher than standard MPNNs due to FSW embeddings
- Limited ablation studies on the impact of different hyperparameters and architectural choices

## Confidence

- **High**: The core architectural innovation (FSW embeddings) is clearly described and the theoretical framework is well-established
- **Medium**: Empirical results on standard benchmarks, though the paper lacks extensive ablation studies
- **Low**: Claims about avoiding oversmoothing in deep message passing, as the theoretical analysis of Holder exponent deterioration is limited

## Next Checks

1. **Ablation on FSW parameters**: Systematically vary the FSW embedding parameters (dimension m, number of slices) to determine the minimal requirements for maintaining bi-Lipschitz properties and WL equivalence.

2. **Comparative oversmoothing analysis**: Measure the Mean Average Distance (MAD) energy for FSW-GNN and standard MPNNs across multiple deep message-passing scenarios to verify the claimed stability.

3. **Scaling study**: Evaluate FSW-GNN's performance on graphs with varying sizes and densities to assess how the bi-Lipschitz property scales with graph complexity.