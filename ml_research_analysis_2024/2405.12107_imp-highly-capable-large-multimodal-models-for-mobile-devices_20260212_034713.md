---
ver: rpa2
title: 'Imp: Highly Capable Large Multimodal Models for Mobile Devices'
arxiv_id: '2405.12107'
source_url: https://arxiv.org/abs/2405.12107
tags:
- arxiv
- lmms
- language
- visual
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying large multimodal
  models (LMMs) on resource-constrained devices like mobile phones. The authors propose
  Imp, a family of highly capable LMMs with 2B-4B parameters, designed to overcome
  the computational limitations of existing large-scale LMMs.
---

# Imp: Highly Capable Large Multimodal Models for Mobile Devices
## Quick Facts
- arXiv ID: 2405.12107
- Source URL: https://arxiv.org/abs/2405.12107
- Reference count: 40
- Primary result: Imp models achieve 73.2 average score on six LMM benchmarks while running at 13 tokens/second on mobile devices

## Executive Summary
This paper introduces Imp, a family of highly capable large multimodal models (LMMs) designed specifically for mobile deployment. By systematically exploring the design space of lightweight LMMs through controlled experiments on model architecture, training strategy, and training data, the authors develop 2B-4B parameter models that outperform existing lightweight LMMs and even surpass state-of-the-art 13B-scale models. The Imp-3B model achieves an average score of 73.2 on six commonly-used LMM benchmarks and runs efficiently on Qualcomm Snapdragon 8Gen3 mobile chips at approximately 13 tokens per second.

## Method Summary
The authors develop Imp models through a two-stage training scheme: multimodal alignment pretraining on 558K image-caption pairs, followed by multimodal instruction tuning on an augmented 1M mixed dataset for 2 epochs with LoRA finetuning (rank 256). The models use lightweight LLMs (Phi-2, Qwen-1.5, or Phi-3) combined with SigLIP-SO400M/14@384 visual encoder. For mobile deployment, they apply 4-bit quantization and reduce image resolution to 196x196 pixels, enabling efficient inference on Snapdragon 8Gen3 chips.

## Key Results
- Imp-3B achieves an average score of 73.2 on six LMM benchmarks, outperforming other lightweight models of similar size
- The model runs at approximately 13 tokens per second on Qualcomm Snapdragon 8Gen3 mobile chip
- Imp models outperform state-of-the-art LMMs at the 13B scale while using significantly less training data
- Replacing CLIP-ViT with SigLIP provides consistent performance improvements across all benchmarks

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Systematic design space exploration of model architecture, training strategy, and training data leads to improved capability in lightweight LMMs.
- Mechanism: By evaluating the impact of each design choice independently (LLM selection, visual encoder choice, finetuning mechanism, training epochs, data augmentation), the authors identify optimal combinations that maximize performance without increasing model size.
- Core assumption: The scaling law relationship observed in larger models also applies to smaller models, and design choices that work for larger models can be transferred to smaller ones with appropriate adjustments.
- Evidence anchors:
  - [abstract] "we conduct a systematic study for lightweight LMMs from the aspects of model architecture, training strategy, and training data"
  - [section] "we conduct a thorough study to investigate the impact of design choices of the lightweight LMMs in controlled settings"
  - [corpus] Weak evidence - corpus focuses on deployment rather than systematic design exploration
- Break condition: If the scaling law relationship doesn't hold for smaller models, or if design choices that work for larger models don't transfer effectively to smaller ones.

### Mechanism 2
- Claim: Using SigLIP as visual encoder instead of CLIP provides significant performance improvements for lightweight LMMs.
- Mechanism: SigLIP's shape-optimized ViT architecture with larger image resolution (729 vs 576 visual tokens) provides more fine-grained visual representations, while large-scale image-text contrastive learning facilitates better generalization.
- Core assumption: The increased number of visual tokens and improved model architecture in SigLIP directly translates to better performance in downstream LMM tasks.
- Evidence anchors:
  - [section] "Replacing CLIP-ViT with SigLIP brings consistent performance improvement on all the benchmarks"
  - [section] "can be explained by the synergistic effect of improved model capability and increased visual tokens (576 in CLIP vs. 729 in SigLIP)"
  - [corpus] Weak evidence - corpus focuses on deployment rather than visual encoder comparisons
- Break condition: If the increased computational cost of processing more visual tokens outweighs the performance benefits, or if the improved architecture doesn't translate to better representations.

### Mechanism 3
- Claim: Augmenting training data with OCR/chart-oriented and GPT4V-annotated datasets significantly improves performance on specific tasks.
- Mechanism: The addition of OCR and chart-oriented datasets improves performance on tasks requiring text understanding in images, while GPT4V-annotated captioning and conversation data provide high-quality instruction-tuning examples that enhance general capabilities.
- Core assumption: High-quality synthetic data generated by state-of-the-art LMMs (GPT4V) can effectively augment training data and improve model capabilities.
- Evidence anchors:
  - [section] "we introduce DVQA, ChartQA, DocVQA, AI2D, and InfographicVQA... which are human annotated VQA datasets that focus on reasoning about of OCR and chart in images"
  - [section] "we utilize three typical GPT4V-annotated datasets, namely ShareGPT-4V, LAION-GPT-V, and ALLaVA"
  - [section] "Both the captioning and conversation data facilitate the model capability"
- Break condition: If the quality of GPT4V-annotated data is inconsistent or if the model overfits to the synthetic data patterns.

## Foundational Learning
- Concept: Multimodal alignment pretraining
  - Why needed here: This stage learns the alignment between visual embeddings and word embeddings, which is crucial for the model to understand the relationship between visual and language modalities
  - Quick check question: What is the purpose of keeping the visual encoder and LLM frozen during multimodal alignment pretraining?

- Concept: LoRA (Low-Rank Adaptation) finetuning
  - Why needed here: LoRA allows efficient finetuning of large models by modifying only a small number of parameters, making it suitable for resource-constrained scenarios
  - Quick check question: How does LoRA differ from full-parameter finetuning in terms of GPU memory requirements and performance?

- Concept: Vision transformer architectures
  - Why needed here: Understanding ViT-based visual encoders is crucial for appreciating the differences between CLIP and SigLIP, and how they impact LMM performance
  - Quick check question: What are the key architectural differences between CLIP-ViT and SigLIP that contribute to SigLIP's improved performance?

## Architecture Onboarding
- Component map: Visual Encoder (SigLIP-SO400M/14@384) -> Multimodal Connector (2-layer MLP) -> Large Language Model (Phi-2, Qwen-1.5, or Phi-3) -> Text output
- Critical path: Visual input → Visual Encoder → Multimodal Connector → LLM → Text output
- Design tradeoffs:
  - Model size vs. performance (2B-4B parameters)
  - Visual resolution vs. inference speed (384x384 vs 196x196)
  - Training data quantity vs. quality (1M samples vs fewer high-quality samples)
- Failure signatures:
  - Poor performance on OCR tasks suggests insufficient OCR/chart-oriented training data
  - Slow inference indicates need for model compression or resolution reduction
  - Incoherent responses may indicate issues with multimodal alignment
- First 3 experiments:
  1. Compare SigLIP vs CLIP visual encoders with identical training setups
  2. Evaluate different LoRA ranks (128, 256, 512) on validation set
  3. Test the impact of training epoch count (1, 2, 3) on final performance

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do different lightweight LMMs perform on Chinese language tasks, and what architectural choices impact multilingual capabilities?
- Basis in paper: [explicit] The paper notes that Imp-2B (Qwen-1.5) performs significantly better on Chinese MMBCN benchmark compared to Imp-3B (Phi-2) and Imp-4B (Phi-3), despite all models using the same training data which is purely in English.
- Why unresolved: The paper only provides a brief observation about this phenomenon without detailed analysis of why Qwen-1.5's multilingual capabilities are better inherited by the LMM.
- What evidence would resolve it: Comparative studies of different lightweight LLMs (Qwen, Phi, etc.) fine-tuned into LMMs, with detailed analysis of their multilingual performance on various language benchmarks.

### Open Question 2
- Question: What is the optimal balance between training data quantity and quality for lightweight LMMs, and how does this compare to larger models?
- Basis in paper: [explicit] The paper states that Imp models outperform MiniCPM-V-3B and Mini-Gemini-2B while using much less training data, suggesting that for lightweight LMMs, quality is more important than quantity.
- Why unresolved: The paper doesn't provide a detailed analysis of how different amounts and qualities of training data impact the performance of lightweight LMMs versus larger models.
- What evidence would resolve it: Systematic experiments varying both the quantity and quality of training data for different sized LMMs, measuring performance across various benchmarks.

### Open Question 3
- Question: How do different quantization techniques and model compression methods impact the performance and latency of lightweight LMMs on mobile devices?
- Basis in paper: [explicit] The paper explores 16/8/4-bit quantization on mobile devices and finds that 4-bit or 8-bit quantization leads to subtle performance and latency degradation compared to 16-bit.
- Why unresolved: The paper only explores basic quantization techniques and doesn't investigate more advanced compression methods or their impact on different aspects of model performance.
- What evidence would resolve it: Comprehensive evaluation of various compression techniques (1-bit, 1.58-bit, pruning, etc.) on lightweight LMMs, measuring their impact on accuracy, latency, and memory usage across different mobile hardware.

## Limitations
- The models are primarily evaluated on English-language benchmarks, limiting generalizability to other languages
- Mobile deployment optimizations are only tested on a single device (Snapdragon 8Gen3), with no evaluation across different hardware architectures
- The training dataset composition, particularly the GPT4V-annotated data, is not fully disclosed, making it difficult to assess potential biases

## Confidence
- High Confidence: The systematic design space exploration methodology and the core finding that SigLIP outperforms CLIP as a visual encoder for lightweight LMMs
- Medium Confidence: The claim that Imp-3B surpasses state-of-the-art LMMs at the 13B scale and the mobile deployment optimizations
- Low Confidence: The generalizability of the design choices to other lightweight model families and the long-term stability of quantized models

## Next Checks
1. Test the Imp-3B quantized model across multiple mobile device architectures (different chipsets, iOS vs Android) to verify portability of deployment optimizations
2. Evaluate Imp-3B on multilingual benchmarks to assess language generalization capabilities beyond English-centric training data
3. Conduct extended inference sessions on mobile devices to measure memory usage patterns and potential performance degradation under continuous operation