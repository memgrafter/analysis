---
ver: rpa2
title: Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up
  Questions
arxiv_id: '2408.00727'
source_url: https://arxiv.org/abs/2408.00727
tags:
- medical
- i-medrag
- queries
- question
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: i-MedRAG improves medical question-answering performance by enabling
  large language models to iteratively ask follow-up questions and retrieve additional
  information across multiple rounds. Unlike conventional RAG systems that conduct
  a single retrieval step, i-MedRAG uses LLMs to dynamically generate queries based
  on previous information-seeking attempts, with each iteration refining the search
  process.
---

# Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions

## Quick Facts
- arXiv ID: 2408.00727
- Source URL: https://arxiv.org/abs/2408.00727
- Authors: Guangzhi Xiong; Qiao Jin; Xiao Wang; Minjia Zhang; Zhiyong Lu; Aidong Zhang
- Reference count: 40
- One-line primary result: i-MedRAG achieves 69.68% accuracy on MedQA, setting a new state-of-the-art without fine-tuning or few-shot examples

## Executive Summary
i-MedRAG is a novel approach that improves medical question-answering performance by enabling large language models to iteratively ask follow-up questions and retrieve additional information across multiple rounds. Unlike conventional RAG systems that conduct a single retrieval step, i-MedRAG uses LLMs to dynamically generate queries based on previous information-seeking attempts, with each iteration refining the search process. The method significantly enhances GPT-3.5's performance on MedQA, achieving state-of-the-art accuracy of 69.68% without requiring fine-tuning or few-shot examples. Experiments with multiple LLMs (GPT-3.5, Llama-3.1-8B) and medical datasets (MedQA, MMLU-Med) demonstrate consistent improvements.

## Method Summary
The i-MedRAG framework enables iterative retrieval-augmented generation by prompting LLMs to generate follow-up queries based on previous information-seeking history. The system operates through multiple rounds where each iteration uses previous query-answer pairs to inform the next round of retrieval. Rather than including all retrieved documents in the LLM context, the system uses LLM-generated answers to the follow-up queries as the information-seeking history, addressing context length limitations. The framework has been tested with multiple LLMs including GPT-3.5 and Llama-3.1-8B across various medical datasets including MedQA and MMLU-Med.

## Key Results
- i-MedRAG achieves 69.68% accuracy on MedQA, significantly outperforming baseline methods including CoT and conventional RAG
- The method demonstrates consistent improvements across multiple LLMs (GPT-3.5, Llama-3.1-8B) and medical datasets
- Performance scales with the number of iterations and queries per iteration, with optimal configurations varying by task complexity
- Case studies show i-MedRAG successfully identifying drugs and mechanisms through iterative information-seeking chains

## Why This Works (Mechanism)

### Mechanism 1
Iterative querying breaks down complex multi-step reasoning into manageable sub-queries that can be answered with RAG. The system prompts the LLM to generate follow-up queries in multiple iterations, where each iteration uses previous query-answer pairs to inform the next round of retrieval, creating a reasoning chain. Core assumption: LLMs can decompose complex medical questions into simpler sub-questions that, when answered sequentially, lead to correct final answers.

### Mechanism 2
The iterative approach improves retrieval relevance by progressively narrowing down the search scope based on previous results. Initial broad queries retrieve general information, while subsequent iterations use the accumulated context to generate more specific queries that target precise information needs. Core assumption: Information retrieved in early iterations provides sufficient context for generating more focused follow-up queries.

### Mechanism 3
The system overcomes the context length limitation of LLMs by using query-answer pairs instead of entire retrieved documents. Rather than including all retrieved documents in the LLM context, the system uses LLM-generated answers to the follow-up queries as the information-seeking history. Core assumption: LLM-generated answers to follow-up queries are sufficiently informative to guide subsequent query generation and final answer synthesis.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The system builds upon RAG by adding iterative query generation capabilities, so understanding basic RAG is essential.
  - Quick check question: What are the two main components of a RAG system and how do they interact?

- Concept: Prompt Engineering for Chain-of-Thought Reasoning
  - Why needed here: The system relies on carefully crafted prompts to guide LLM behavior in generating follow-up queries, similar to CoT prompting.
  - Quick check question: How does Chain-of-Thought prompting differ from standard prompting in terms of the reasoning process it encourages?

- Concept: Context Window Management in LLMs
  - Why needed here: The system's design choice to use query-answer pairs instead of full documents is motivated by context length limitations.
  - Quick check question: What are the typical context window sizes for GPT-3.5 and Llama-3.1-8B, and why does this matter for RAG systems?

## Architecture Onboarding

- Component map:
  - LLM module (GPT-3.5 or Llama-3.1-8B) -> Text retriever (MedCPT) -> Medical corpora (Textbooks and Statpearls) -> Query generator (iterative LLM prompting) -> Answer synthesizer (final LLM prompt with information-seeking history)

- Critical path:
  1. Receive medical question
  2. Generate initial queries
  3. Retrieve documents for each query
  4. Generate answers for each query
  5. Update information-seeking history
  6. Repeat steps 2-5 for m iterations
  7. Generate final answer using original question and all query-answer pairs

- Design tradeoffs:
  - More iterations → potentially better answers but higher cost and latency
  - More queries per iteration → broader information coverage but increased complexity
  - Using query-answer pairs vs. full documents → reduced context usage but potential information loss

- Failure signatures:
  - If query generation fails, the system will produce irrelevant follow-up queries
  - If retrieval fails, the LLM will have insufficient information to generate meaningful answers
  - If answer synthesis fails, the final answer will not incorporate the retrieved information effectively

- First 3 experiments:
  1. Compare accuracy on MedQA with 1 vs. 2 iterations using GPT-3.5 to verify the benefit of additional iterations
  2. Test the effect of varying the number of queries per iteration (1, 2, 3, 4, 5) on GPT-3.5 performance
  3. Evaluate the system's performance on a simpler dataset like MMLU-Med to confirm generalizability across medical question complexities

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of iterations and queries per iteration for i-MedRAG across different medical tasks and LLM models? The paper discusses that different LLMs can have different hyperparameter settings for optimal performance, and even for the same LLM, optimal hyperparameters can vary based on the medical questions being processed. Systematic experiments testing i-MedRAG across a wide range of medical tasks with varying complexity levels, using multiple LLM models, to identify patterns in optimal hyperparameter settings would resolve this.

### Open Question 2
How can i-MedRAG be adapted to incorporate few-shot demonstrations to further improve performance? The paper mentions that few-shot CoT prompting performs better than zero-shot, but adapting such strategies to i-MedRAG is non-trivial due to the dynamic nature of the reasoning process with external corpora. Development and testing of few-shot adaptation methods specifically designed for iterative RAG systems would provide evidence.

### Open Question 3
What are the error patterns and failure modes of i-MedRAG compared to conventional RAG and other prompting methods? While the paper shows overall performance improvements, it doesn't characterize the types of errors i-MedRAG makes or the specific scenarios where it underperforms. Comprehensive error analysis categorizing failure modes and comparing these patterns across i-MedRAG, conventional RAG, and baseline prompting methods across diverse medical question types would help identify areas for improvement.

## Limitations
- The system's effectiveness relies heavily on the LLM's ability to generate useful follow-up queries, with limited analysis of query quality or cases where query generation fails
- Evaluation focuses primarily on USMLE-style multiple-choice questions, which may not generalize to broader clinical scenarios requiring free-text responses or complex decision-making
- The paper acknowledges high cost and hyperparameter selection challenges but doesn't provide detailed analysis of when and why the system fails

## Confidence

- **High confidence**: The 69.68% accuracy on MedQA represents a genuine improvement over baseline RAG methods, supported by multiple experiments and datasets
- **Medium confidence**: The claim that iterative querying improves clinical reasoning is supported by case studies but lacks systematic analysis of query quality or failure modes
- **Low confidence**: The paper's assertion that the method "effectively captures the dependencies among multi-step clinical reasoning processes" is demonstrated through specific examples but not validated across diverse clinical reasoning patterns

## Next Checks

1. **Query Quality Analysis**: Systematically evaluate the quality of generated follow-up queries across different iterations and medical domains to identify patterns of success and failure in the query generation process.

2. **Generalization Testing**: Test i-MedRAG on clinical scenarios requiring free-text responses and complex decision-making beyond multiple-choice questions to assess real-world applicability.

3. **Comparative Analysis**: Compare i-MedRAG's performance against domain-specific medical LLMs (like Med-PaLM or PubMedGPT) to determine whether the iterative RAG approach provides advantages beyond general-purpose LLMs with retrieval augmentation.