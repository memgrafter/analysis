---
ver: rpa2
title: How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft
  Biometrics, and Explainability
arxiv_id: '2401.13641'
source_url: https://arxiv.org/abs/2401.13641
tags:
- chatgpt
- face
- images
- image
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the potential of ChatGPT, based on the GPT-4
  multimodal LLM, for face biometrics tasks including verification, soft-biometrics
  estimation, and explainability. Using popular benchmarks and comparing with state-of-the-art
  methods, the research found that ChatGPT achieved promising results in face verification
  (e.g., 94% accuracy on LFW) and soft-biometrics estimation (e.g., 96% gender accuracy
  on MAAD-Face), although generally lower than specialized models.
---

# How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability

## Quick Facts
- arXiv ID: 2401.13641
- Source URL: https://arxiv.org/abs/2401.13641
- Reference count: 40
- ChatGPT achieved promising results in face verification (94% accuracy on LFW) and soft-biometrics estimation (96% gender accuracy on MAAD-Face), though generally lower than specialized models

## Executive Summary
This study evaluates ChatGPT (GPT-4 with vision capabilities) for face biometrics tasks including verification, soft-biometrics estimation, and explainability. Using multiple public benchmarks, the research found that while ChatGPT achieved promising results in several tasks, its performance was generally lower than specialized models. The model demonstrated particular strength in gender classification and face verification on standard datasets, while showing limitations in age estimation and small face recognition. Notably, ChatGPT provided explanations for its decisions, enhancing transparency in the biometric process.

## Method Summary
The study used zero-shot evaluation of ChatGPT API (GPT-4 with vision) on multiple face biometrics tasks. Researchers tested face verification, soft-biometrics estimation (gender, age, ethnicity, 47 facial attributes), and explainability using public benchmarks including LFW, QUIS-CAMPI, TinyFaces, BUPT-BalancedFace, CFP-FP, AgeDB, ROF, and MAAD-Face. Images were processed in two formats: single comparison images and 4x3 matrix layouts. Results were compared against state-of-the-art models like ArcFace, AdaFace, FairFace, and ResNet-50. Custom prompts were engineered for each task type, and confidence values were collected from model responses.

## Key Results
- ChatGPT achieved 94% accuracy on LFW face verification and 88.88% on QUIS-CAMPI
- Gender classification accuracy reached 96% on MAAD-Face dataset
- Performance varied significantly with image quality and presentation format, showing particular challenges with small faces (62% on TinyFaces)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT's performance varies significantly based on image quality and presentation format.
- Mechanism: The model processes images differently when presented as individual comparisons versus a matrix, leading to changes in accuracy due to additional face detection steps.
- Core assumption: The high-resolution image processing capabilities are affected by how images are arranged and the additional computational load of detecting multiple faces.
- Evidence anchors:
  - [abstract] "performance varied across different scenarios, with challenges such as demographic bias and image quality affecting outcomes."
  - [section] "we observed that ChatGPT always provided a confidence value of 100% on the response, even in instances of failure."
  - [corpus] "Weak - no direct mention of image format effects on performance."

### Mechanism 2
- Claim: ChatGPT can generalize facial attribute recognition without explicit training on biometric tasks.
- Mechanism: The model leverages its broad pretraining on diverse image and text data to identify facial attributes through pattern matching and contextual understanding.
- Core assumption: The pretraining corpus included sufficient examples of facial features and attributes to enable zero-shot recognition.
- Evidence anchors:
  - [abstract] "ChatGPT achieved promising results in face verification...and soft-biometrics estimation...although generally lower than specialized models."
  - [section] "ChatGPT shows promising results and utility for tasks with no prior training."
  - [corpus] "Weak - no direct mention of zero-shot facial attribute recognition capabilities."

### Mechanism 3
- Claim: ChatGPT provides explainability in its decisions through reasoning about facial features.
- Mechanism: The model generates textual explanations by describing the facial features it uses to make decisions, even when those decisions are incorrect.
- Core assumption: The model's training included learning to generate explanations for its outputs, not just the outputs themselves.
- Evidence anchors:
  - [abstract] "The study highlighted ChatGPT's ability to provide explainability in its decisions, enhancing transparency in automatic face recognition systems."
  - [section] "ChatGPT demonstrates its ability to rationalize decisions based on image features."
  - [corpus] "Weak - no direct mention of explainability features in the corpus."

## Foundational Learning

- Concept: Zero-shot learning capabilities
  - Why needed here: Understanding how ChatGPT can perform tasks without explicit training on biometric data is crucial for evaluating its potential applications.
  - Quick check question: How does ChatGPT's zero-shot performance compare to models specifically trained on facial recognition tasks?

- Concept: Multimodal processing limitations
  - Why needed here: Recognizing the constraints of processing both text and images helps explain performance variations across different scenarios.
  - Quick check question: What specific image quality thresholds cause significant drops in ChatGPT's performance?

- Concept: Bias in large language models
  - Why needed here: Understanding how demographic biases manifest in ChatGPT's outputs is essential for evaluating its fairness and reliability.
  - Quick check question: How does ChatGPT's performance vary across different demographic groups compared to specialized models?

## Architecture Onboarding

- Component map: ChatGPT API with GPT-4 Vision model -> prompt engineering layer -> image preprocessing (single vs matrix) -> evaluation framework for face verification and soft-biometrics tasks
- Critical path: Image input → Prompt processing → Model inference → Output parsing → Evaluation metric calculation
- Design tradeoffs: Matrix vs. individual image presentation (cost vs. accuracy), prompt complexity (detailed vs. concise), image resolution (quality vs. API limitations)
- Failure signatures: Inconsistent confidence scores, significant performance drops with image quality changes, demographic bias in outputs
- First 3 experiments:
  1. Compare matrix vs. individual image presentation accuracy on LFW dataset
  2. Test performance across different image quality levels using TinyFaces dataset
  3. Evaluate demographic bias by testing across different ethnic and gender groups in BUPT database

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ChatGPT for face verification change when provided with a larger number of face comparisons per image matrix (e.g., beyond the 4x3 configuration tested)?
- Basis in paper: [explicit] The paper tested a 4x3 matrix configuration but mentions the potential for larger matrices given the size recommendations for optimal image processing.
- Why unresolved: The study only evaluated a specific matrix size (4x3), leaving the impact of larger matrices on performance unexplored.
- What evidence would resolve it: Comparative experiments using different matrix sizes (e.g., 5x4, 6x5) to assess the impact on accuracy and processing efficiency.

### Open Question 2
- Question: Can fine-tuning ChatGPT on a dataset specifically designed for face biometrics tasks improve its performance compared to the zero-shot learning approach used in this study?
- Basis in paper: [inferred] The paper discusses the potential of ChatGPT for face biometrics but acknowledges its lower performance compared to specialized models, suggesting room for improvement.
- Why unresolved: The study focused on evaluating ChatGPT's capabilities without any task-specific training, leaving the impact of fine-tuning unexplored.
- What evidence would resolve it: Experiments comparing the performance of the original ChatGPT with a fine-tuned version on a face biometrics dataset, measuring improvements in accuracy and robustness.

### Open Question 3
- Question: How does ChatGPT's performance on face verification tasks compare to other large language models like Google Bard or LLaMA when provided with similar prompts and image configurations?
- Basis in paper: [explicit] The paper mentions that Google Bard was initially considered but discarded due to limitations, highlighting the need for comparative analysis with other LLMs.
- Why unresolved: The study only evaluated ChatGPT, leaving a gap in understanding its relative performance compared to other models in the field.
- What evidence would resolve it: Comparative experiments using ChatGPT, Google Bard, and LLaMA with identical prompts and image configurations to assess their relative strengths and weaknesses in face biometrics tasks.

## Limitations
- Performance generally underperforms specialized models like ArcFace and FairFace, particularly in challenging scenarios like age estimation and small face recognition
- Zero-shot evaluation approach may underestimate true capabilities as ChatGPT wasn't optimized for biometric tasks
- Consistent 100% confidence scores raise concerns about reliability assessment and may not correlate with actual accuracy

## Confidence

| Claim | Confidence |
|-------|------------|
| Face Verification Claims | Medium |
| Soft-Biometrics Claims | Low-Medium |
| Explainability Claims | Medium |

## Next Checks
1. **Prompt Engineering Optimization:** Systematically test different prompt structures and instruction formats to determine if performance can be improved beyond the baseline zero-shot results.
2. **Demographic Bias Analysis:** Conduct comprehensive testing across diverse demographic groups to quantify and characterize any biases in ChatGPT's biometric decisions.
3. **Confidence Score Validation:** Design experiments to test whether ChatGPT's consistent 100% confidence scores correlate with actual accuracy, and develop methods to estimate true confidence levels.