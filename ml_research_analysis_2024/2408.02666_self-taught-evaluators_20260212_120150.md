---
ver: rpa2
title: Self-Taught Evaluators
arxiv_id: '2408.02666'
source_url: https://arxiv.org/abs/2408.02666
tags:
- data
- training
- synthetic
- instruction
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Self-Taught Evaluators is a method that trains LLM-as-a-Judge\
  \ models without any human-annotated preference data. The approach iteratively generates\
  \ synthetic preference pairs and uses the model itself to judge and label them,\
  \ progressively improving the evaluator\u2019s accuracy."
---

# Self-Taught Evaluators

## Quick Facts
- arXiv ID: 2408.02666
- Source URL: https://arxiv.org/abs/2408.02666
- Reference count: 23
- One-line primary result: Self-Taught Evaluators trains LLM-as-a-Judge models without human-labeled preference data, achieving 88.3% accuracy on RewardBench (88.7% with majority voting)

## Executive Summary
Self-Taught Evaluators introduces a method to train LLM-as-a-Judge models without any human-annotated preference data. The approach iteratively generates synthetic preference pairs and uses the model itself to judge and label them, progressively improving the evaluator's accuracy. Starting from Llama3-70B-Instruct, the method improves accuracy from 75.4 to 88.3 (88.7 with majority voting) on RewardBench, matching or outperforming top reward models trained with human-labeled data and surpassing commonly used LLM judges like GPT-4.

## Method Summary
The method trains an LLM-as-a-Judge through iterative self-improvement without human annotations. It begins with unlabeled instructions, generates contrasting model outputs by creating semantically similar but distinct modified instructions, and uses the current model to produce chain-of-thought reasoning traces and final judgments. Correct judgments are retained through rejection sampling and used to fine-tune a new model. This process repeats iteratively, with each round using the improved predictions to generate better training data, effectively creating an automatic curriculum that progressively enhances the evaluator's capabilities.

## Key Results
- Starting from Llama3-70B-Instruct with 75.4% accuracy on RewardBench, Self-Taught Evaluators improves to 88.3% accuracy (88.7% with 32-sample majority voting)
- Matches or outperforms reward models derived from the same Llama-3-70B-Instruct model that uses human annotations
- Outperforms commonly used LLM judges like GPT-4 on RewardBench evaluation
- Demonstrates that high-quality evaluators can be trained without expensive human-labeled preference data

## Why This Works (Mechanism)

### Mechanism 1
The iterative self-training loop progressively improves the LLM-as-a-Judge by using its own correct judgments to train a new model. At each iteration, the model generates synthetic preference pairs and then judges them. Correct judgments are retained and used to fine-tune a new model. This process repeats, with the model improving its ability to generate and identify correct judgments over time. The core assumption is that the model can generate meaningful synthetic preference pairs and identify at least some correct judgments in early iterations.

### Mechanism 2
Generating synthetic preference pairs by modifying instructions and responses ensures that one response is clearly inferior, providing a clear ground truth for training. For each instruction, a baseline response is generated. Then, a modified instruction is created, and a response to that modified instruction is generated. This response is expected to be poor for the original instruction, creating a clear preference pair. The core assumption is that the modified instruction is sufficiently different from the original that the response to it is clearly inferior for the original instruction.

### Mechanism 3
Using majority voting with multiple samples from the model at inference time improves the accuracy of the LLM-as-a-Judge by reducing the impact of individual errors. At inference time, the model generates multiple judgments for each pair. The final judgment is the most common verdict among these samples, reducing the impact of individual errors. The core assumption is that the model's errors are not systematic and can be averaged out by majority voting.

## Foundational Learning

- Concept: Pairwise preference learning
  - Why needed here: The model needs to learn to compare two responses and determine which is better, which is the core task of an LLM-as-a-Judge.
  - Quick check question: Can you explain the difference between pairwise preference learning and scalar reward learning?

- Concept: Synthetic data generation
  - Why needed here: The model needs training data, but human-labeled data is expensive. Synthetic data generation allows the model to create its own training data.
  - Quick check question: What are the advantages and disadvantages of using synthetic data compared to human-labeled data?

- Concept: Iterative training and self-improvement
  - Why needed here: The model needs to improve its ability to generate meaningful synthetic preference pairs and identify correct judgments over time. Iterative training allows the model to learn from its own outputs.
  - Quick check question: How does iterative training differ from traditional supervised learning, and what are the potential benefits and risks?

## Architecture Onboarding

- Component map: Instruction selection -> Response pair construction -> Judgment annotation -> Iterative training -> Inference with majority voting
- Critical path: 1. Select instructions 2. Generate synthetic preference pairs 3. Sample judgments and retain correct ones 4. Fine-tune model on retained judgments 5. Repeat steps 2-4 for multiple iterations 6. Use majority voting at inference time
- Design tradeoffs:
  - Synthetic data vs. human-labeled data: Synthetic data is cheaper but may not capture all nuances of human judgment
  - Iterative training vs. one-shot training: Iterative training allows for self-improvement but requires more computation
  - Majority voting vs. single judgment: Majority voting improves accuracy but increases inference cost
- Failure signatures:
  - Model fails to generate meaningful synthetic preference pairs: Check the quality of the initial model and the instruction selection process
  - Model fails to identify correct judgments: Check the sampling parameters and the rejection sampling process
  - Model overfits to synthetic data: Check the diversity of the synthetic data and the fine-tuning process
- First 3 experiments:
  1. Test the instruction selection process by manually reviewing a sample of selected instructions
  2. Test the response pair construction process by manually reviewing a sample of synthetic preference pairs
  3. Test the judgment annotation process by manually reviewing a sample of retained judgments and comparing them to the synthetic preference pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Self-Taught Evaluator method scale to smaller LLMs (e.g., 7B or 13B parameters) compared to the 70B Llama-3 model used in the experiments?
- Basis in paper: The paper uses a 70B Llama-3-Instruct model and does not explore performance on smaller models, which is noted as a limitation.
- Why unresolved: The paper only tested the method on a 70B parameter model, leaving the question of whether the approach works effectively on smaller, more resource-constrained models unanswered.
- What evidence would resolve it: Experiments showing the performance of the Self-Taught Evaluator on smaller models (e.g., 7B, 13B) across the same benchmarks (RewardBench, MT-Bench) compared to the 70B baseline.

### Open Question 2
- Question: What is the impact of using different LLM-as-a-Judge models for generating synthetic judgments on the final evaluator performance?
- Basis in paper: The paper mentions using different models (Mixtral 22Bx8 Instruct and Llama-3-70B-Instruct) to generate synthetic data and judgments, with the best performance achieved using Mixtral to judge Mixtral-generated responses.
- Why unresolved: While the paper shows that different combinations of models for generating responses and judgments affect performance, it does not fully explore the space of possible model combinations or explain why certain combinations work better.
- What evidence would resolve it: A systematic study varying the LLM-as-a-Judge model used for generating judgments (e.g., GPT-4, different Llama versions) and analyzing the resulting evaluator performance.

### Open Question 3
- Question: How does the Self-Taught Evaluator method perform on single-response evaluation tasks, as opposed to the pairwise comparison approach used in the experiments?
- Basis in paper: The paper explicitly states that it only investigated pairwise evaluation and suggests that evaluating single responses is left for future work.
- Why unresolved: The paper focuses on pairwise comparison (A vs B) and does not explore whether the method can be adapted to score individual responses on a scale (e.g., 1-10), which is a common evaluation paradigm.
- What evidence would resolve it: Experiments adapting the Self-Taught Evaluator to score single responses and comparing its performance to pairwise evaluation and other single-response scoring methods.

## Limitations

- The method relies heavily on the initial model's ability to generate meaningful synthetic preference pairs and correctly identify them in early iterations
- The approach assumes that majority voting will consistently improve accuracy, but this may not hold if the model develops systematic biases during training
- The reliance on synthetic data raises questions about whether the learned preferences generalize to real human preferences beyond the RewardBench benchmark

## Confidence

- **High confidence**: The overall methodology of iterative self-training with synthetic preference pairs is sound and well-supported by the empirical results showing improvement from 75.4 to 88.3 accuracy on RewardBench
- **Medium confidence**: The claim that this matches or outperforms reward models trained with human data is supported by benchmark comparisons, though the relative performance on different benchmarks varies
- **Medium confidence**: The effectiveness of majority voting for inference is supported by the reported improvement to 88.7 accuracy, but the robustness of this approach across different types of evaluation tasks needs further validation

## Next Checks

1. Test the method's robustness by applying it to a different initial model (e.g., starting from Llama2-70B instead of Llama3-70B) to verify that the improvement is not specific to the initial model choice
2. Conduct ablation studies on the synthetic data generation process by varying the instruction modification strategy and measuring its impact on final evaluation accuracy
3. Evaluate the trained evaluator on human-annotated preference data not used in training to verify that the synthetic training successfully captures human preferences beyond the RewardBench benchmark