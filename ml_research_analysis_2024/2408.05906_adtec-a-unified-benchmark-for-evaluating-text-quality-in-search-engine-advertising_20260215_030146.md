---
ver: rpa2
title: 'AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising'
arxiv_id: '2408.05906'
source_url: https://arxiv.org/abs/2408.05906
tags:
- dataset
- data
- text
- task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces AdTEC, the first public benchmark for evaluating\
  \ the quality of search engine advertising texts across multiple dimensions relevant\
  \ to practical advertising operations. The authors define five tasks\u2014AD ACCEPTABILITY,\
  \ AD CONSISTENCY, AD PERFORMANCE ESTIMATION, A3 RECOGNITION, and AD SIMILARITY\u2014\
  based on real-world advertising workflows, and construct a high-quality Japanese\
  \ dataset using operational data from advertising agencies."
---

# AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising

## Quick Facts
- arXiv ID: 2408.05906
- Source URL: https://arxiv.org/abs/2408.05906
- Authors: Peinan Zhang; Yusuke Sakai; Masato Mita; Hiroki Ouchi; Taro Watanabe
- Reference count: 40
- Primary result: First public benchmark for evaluating search engine advertising text quality across five tasks using real operational data from advertising agencies

## Executive Summary
This paper introduces AdTEC, the first public benchmark for evaluating the quality of search engine advertising texts across multiple dimensions relevant to practical advertising operations. The authors define five tasks—AD ACCEPTABILITY, AD CONSISTENCY, AD PERFORMANCE ESTIMATION, A3 RECOGNITION, and AD SIMILARITY—based on real-world advertising workflows, and construct a high-quality Japanese dataset using operational data from advertising agencies. Experiments with fine-tuned encoder models, zero-/few-shot large language models, and human evaluators show that while PLMs have reached practical usage levels in several tasks, humans still outperform in certain domains, highlighting significant room for improvement. The benchmark provides a challenging testbed for advancing research in ad text evaluation and bridging the fields of advertising and NLP.

## Method Summary
The AdTEC benchmark defines five tasks for evaluating ad text quality: three direct evaluation tasks (AD ACCEPTABILITY for fluency/acceptability, AD CONSISTENCY for semantic consistency with landing pages, AD PERFORMANCE ESTIMATION for predicted performance) and two indirect evaluation tasks (A3 RECOGNITION for identifying advertising appeals, AD SIMILARITY for semantic similarity between ads). The dataset was constructed from operational data of advertising agencies, containing 47,413 samples with annotations from industry experts and curated keyword-based labels. Models were evaluated using fine-tuned encoder models (Tohoku BERT, Waseda RoBERTa, XLM-RoBERTa) and zero-/few-shot large language models (CALM27b, ELYZA7b, GPT-3.5, GPT-4), with human evaluators providing gold standard performance baselines.

## Key Results
- Multi-task evaluation framework captures diverse quality dimensions with practical relevance to advertising workflows
- Hierarchical data splitting prevents leakage and ensures robust generalization across advertiser structures
- Real operational data provides more practical evaluation than synthetic benchmarks, revealing domain-specific challenges
- Humans still outperform models in certain domains, indicating significant room for improvement in ad text evaluation
- The benchmark successfully bridges advertising and NLP research fields, providing a challenging testbed for future work

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task evaluation framework captures diverse quality dimensions of ad text
- Mechanism: By defining five tasks (AD ACCEPTABILITY, AD CONSISTENCY, AD PERFORMANCE ESTIMATION, A3 RECOGNITION, AD SIMILARITY), the benchmark evaluates fluency, consistency, predicted performance, advertising appeals, and semantic similarity simultaneously
- Core assumption: Real-world advertising quality assessment requires multiple complementary perspectives
- Evidence anchors:
  - [abstract] "defines five tasks for evaluating the quality of ad texts"
  - [section] "we defined five tasks: three direct evaluation tasks... and two indirect evaluation tasks"
  - [corpus] Weak - related papers focus on single-dimension evaluations (factuality, fairness, long text quality)
- Break condition: If ad quality assessment in practice only requires one or two dimensions, the multi-task framework adds unnecessary complexity

### Mechanism 2
- Claim: Hierarchical data splitting prevents leakage and improves generalization
- Mechanism: Dataset split by client, campaign, and account structure ensures training, development, and testing sets contain disjoint advertiser representations
- Core assumption: Ad text expressions follow hierarchical patterns tied to advertiser structure
- Evidence anchors:
  - [section] "split the data, considering the ad hierarchical structure... ensuring that clients do not overlap"
  - [section] "For the AD PERFORMANCE ESTIMATION, we used the delivery structure, with the non-overlapping layer as the campaign"
  - [corpus] Weak - no corpus evidence found about hierarchical splitting in advertising datasets
- Break condition: If ad expressions are not strongly tied to advertiser structure, hierarchical splitting provides minimal benefit over random splitting

### Mechanism 3
- Claim: Real operational data provides more practical evaluation than synthetic benchmarks
- Mechanism: Dataset constructed from actual advertising agency operations rather than generated examples, capturing authentic quality criteria and patterns
- Core assumption: Practical advertising operations differ significantly from academic assumptions about ad quality
- Evidence anchors:
  - [abstract] "building a Japanese dataset based on the practical operational experiences of advertising agencies"
  - [section] "constructed a high-quality Japanese dataset from the operational data"
  - [corpus] Weak - related papers use synthetic or academic datasets rather than operational data
- Break condition: If academic assumptions about ad quality align well with practical requirements, the operational data advantage diminishes

## Foundational Learning

- Concept: Pre-trained Language Models (PLMs) fine-tuning
  - Why needed here: Tasks require domain-specific understanding of advertising terminology and quality criteria
  - Quick check question: Can a BERT model trained on general text distinguish between acceptable and unacceptable ad texts without advertising-specific fine-tuning?

- Concept: Few-shot learning with Large Language Models
  - Why needed here: Evaluates whether LLMs can understand ad quality without extensive domain-specific training
  - Quick check question: Does GPT-4 achieve competitive performance on AD SIMILARITY without any task-specific fine-tuning?

- Concept: Multi-label classification
  - Why needed here: A3 RECOGNITION requires identifying multiple advertising appeals in single ad texts
  - Quick check question: Can the model output multiple A3 labels simultaneously, or does it only select the single best label?

## Architecture Onboarding

- Component map: Data preprocessing -> Fine-tuned PLMs (BERT, RoBERTa, XLM-RoBERTa) -> Zero-shot LLMs (CALM27b, ELYZA7b, GPT-3.5, GPT-4) -> Human evaluators -> Evaluation metrics -> Analysis dashboard
- Critical path: Data preprocessing -> Model training/validation -> Task evaluation -> Performance analysis -> Benchmark release
- Design tradeoffs: Fine-tuned models achieve higher accuracy but require training resources; zero-shot LLMs provide flexibility but lower performance; human evaluation provides gold standard but doesn't scale
- Failure signatures: Poor performance on AD CONSISTENCY indicates inability to detect semantic mismatches; low correlation in AD PERFORMANCE ESTIMATION suggests numerical prediction difficulties; failure on A3 RECOGNITION indicates misunderstanding of advertising appeal concepts
- First 3 experiments:
  1. Fine-tune BERT on AD ACCEPTABILITY task with 10% data to establish baseline performance
  2. Test GPT-4 zero-shot on AD SIMILARITY with 3-shot examples to compare with fine-tuned models
  3. Conduct human evaluation on 100 random samples across all tasks to establish human performance benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using different tokenizer types (BPE vs. Unigram) on model performance for ad text evaluation tasks?
- Basis in paper: [explicit] The paper compares models with different tokenizers (Tohoku BERT with MeCab+BPE vs. Waseda RoBERTa with Juman+++Unigram) and finds performance differences across tasks.
- Why unresolved: The paper does not provide a systematic analysis of how tokenizer choice affects performance specifically for ad text evaluation tasks, which have unique characteristics compared to general NLP tasks.
- What evidence would resolve it: A controlled experiment varying only the tokenizer type while keeping other factors constant, with detailed analysis of tokenization effects on ad-specific phenomena like A3 recognition and ad similarity.

### Open Question 2
- Question: How do cultural differences in advertising practices affect the generalizability of the benchmark across different countries?
- Basis in paper: [explicit] The paper mentions that Japanese advertising tends to be more cluttered compared to US/European practices and that search queries differ culturally, suggesting potential limitations of the Japanese-focused dataset.
- Why unresolved: The paper only discusses these cultural differences qualitatively and does not test the benchmark's performance on datasets from other countries or cultural contexts.
- What evidence would resolve it: Replicating the benchmark with ad text datasets from different countries, particularly comparing results between Japanese, US, and European advertising data to identify cross-cultural variations in task difficulty.

### Open Question 3
- Question: What is the optimal sampling strategy for creating pseudo-similar pairs in the AD SIMILARITY task?
- Basis in paper: [explicit] The paper uses a 9:1 ratio of pseudo-similar to pseudo-dissimilar pairs sampled from the same ad group, but acknowledges this was determined through preliminary results without systematic optimization.
- Why unresolved: The paper does not explore alternative sampling strategies or ratios, nor does it analyze how the choice of sampling method affects model performance or the difficulty of the task.
- What evidence would resolve it: Systematic experiments varying the ratio of similar to dissimilar pairs, sampling from different hierarchical levels, and analyzing the effect on model performance and label distribution.

### Open Question 4
- Question: How does the performance of the benchmark tasks correlate with actual advertising outcomes like CTR or conversion rates?
- Basis in paper: [explicit] The paper mentions that AD PERFORMANCE ESTIMATION task uses a simulated score based on past delivery history, but does not validate this against actual business metrics or explore correlations between task performance and real-world outcomes.
- Why unresolved: The paper focuses on task-specific metrics without establishing a clear connection to practical advertising success metrics that would validate the benchmark's real-world relevance.
- What evidence would resolve it: Longitudinal studies tracking how improvements in benchmark task performance translate to improvements in actual advertising metrics like CTR, conversion rates, and ROI across multiple advertising campaigns.

## Limitations

- Japanese-language focus restricts applicability to non-Japanese advertising markets and limits cross-linguistic generalizability
- Dataset relies on operational data from a single advertising agency, potentially introducing systematic biases in how ad quality is defined and labeled
- Hierarchical data splitting approach lacks empirical validation showing significant improvement over simpler random splitting methods

## Confidence

- High Confidence: The multi-task evaluation framework captures diverse quality dimensions of ad text
- Medium Confidence: Hierarchical data splitting prevents leakage and improves generalization
- Medium Confidence: Real operational data provides more practical evaluation than synthetic benchmarks

## Next Checks

1. Cross-linguistic validation: Test whether models trained on AdTEC transfer effectively to search engine advertising text quality evaluation in other languages, measuring performance degradation and identifying language-specific challenges.

2. Splitting strategy comparison: Conduct an ablation study comparing hierarchical splitting against random splitting with identical dataset sizes to quantify the actual benefit of the hierarchical approach on model generalization.

3. Synthetic vs. operational data comparison: Create a parallel synthetic benchmark with similar task definitions and evaluate whether models trained on operational data significantly outperform those trained on high-quality synthetic examples across all five tasks.