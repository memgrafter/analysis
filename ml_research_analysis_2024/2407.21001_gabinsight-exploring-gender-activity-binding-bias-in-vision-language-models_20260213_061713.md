---
ver: rpa2
title: 'GABInsight: Exploring Gender-Activity Binding Bias in Vision-Language Models'
arxiv_id: '2407.21001'
source_url: https://arxiv.org/abs/2407.21001
tags:
- bias
- gender
- activity
- vlms
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies and quantifies a gender-activity binding\
  \ bias in vision-language models (VLMs), where VLMs tend to associate activities\
  \ with expected genders due to sample selection bias in training data. To measure\
  \ this bias, the authors introduce the GAB dataset\u2014approximately 5500 AI-generated\
  \ images covering diverse activities with different gender combinations (expected/unexpected\
  \ performer, with/without the other gender present)."
---

# GABInsight: Exploring Gender-Activity Binding Bias in Vision-Language Models

## Quick Facts
- **arXiv ID**: 2407.21001
- **Source URL**: https://arxiv.org/abs/2407.21001
- **Reference count**: 40
- **Primary result**: VLMs show 13.2% average accuracy drop in gender-activity binding tasks

## Executive Summary
This paper identifies and quantifies gender-activity binding bias in vision-language models (VLMs), where models tend to associate activities with expected genders due to sample selection bias in training data. The authors introduce the GAB datasetâ€”approximately 5500 AI-generated images covering diverse activities with different gender combinations. Benchmarking 12 VLMs reveals significant performance drops when unexpected genders perform activities, with text encoders showing particular bias toward expected gender stereotypes.

## Method Summary
The study creates the GAB dataset using DALL-E 3 with prompt enhancement techniques, generating ~5500 images across diverse activities and gender combinations. Images are evaluated for quality using FID, SSIM, and LPIPS metrics. Twelve pre-trained VLMs (CLIP variants, Eva models, Flava, Align, COCA, AltCLIP) are benchmarked on text-to-image and image-to-text retrieval tasks. The experiments measure accuracy drops between expected and unexpected gender scenarios, with particular attention to scenes with one or two persons present.

## Key Results
- VLMs experience an average 13.2% drop in retrieval accuracy when the activity performer is the unexpected gender
- Text encoders show significant bias towards expected genders for gender-stereotyped activities
- VLMs perform near-random (approximately 50% accuracy) in text-to-image retrieval tasks, indicating poor activity understanding
- The bias is most pronounced in scenes with both genders present

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLMs show reduced retrieval accuracy when the unexpected gender performs a biased activity in scenes with both genders present.
- Mechanism: The model associates the activity with the gender that is more frequently co-occurring in the training data, leading to a binding bias that overrides the visual evidence of the unexpected gender performing the activity.
- Core assumption: The shared embedding space is influenced more by textual co-occurrence patterns than by visual features in complex scenes.
- Evidence anchors:
  - [abstract] "VLMs experience an average 13.2% drop in retrieval accuracy when confronted with gender-activity binding bias."
  - [section] "Performance Drop Due to Gender-Activity Binding Bias: ...the majority of models display a substantial decline in retrieval accuracy when the action is performed by an individual of the unexpected gender."
  - [corpus] Weak evidence - related works focus on general bias evaluation but don't directly address activity-gender binding in retrieval tasks.
- Break condition: If the model's image encoder prioritizes visual features over textual co-occurrence patterns, the accuracy drop would be minimal or absent.

### Mechanism 2
- Claim: Text encoders in VLMs exhibit a significant bias towards the expected gender for gender-stereotyped activities.
- Mechanism: During training, the text encoder learns to associate certain activities with specific genders based on the frequency of these associations in the training corpus, creating an inherent bias that persists during inference.
- Core assumption: The text encoder's training objective reinforces gender stereotypes present in the training data.
- Evidence anchors:
  - [abstract] "text encoders show significant bias towards expected genders"
  - [section] "we determine the frequency at which the embedding of a gender-neutral sentence ... is closer to the embedding of 'a e is <doing activity>' than the embedding of 'a u is <doing activity>'"
  - [corpus] Moderate evidence - works like [40] discuss gender bias in image search but don't specifically address text encoder bias in activity-gender associations.
- Break condition: If the text encoder is trained with balanced gender representations for all activities, the bias would be significantly reduced.

### Mechanism 3
- Claim: VLMs perform near-random in text-to-image retrieval tasks, indicating poor activity understanding.
- Mechanism: The model fails to bind gender and activity information from the text to select the correct image, suggesting that the text embeddings do not effectively encode the activity-gender relationship.
- Core assumption: The shared embedding space does not adequately represent the compositional relationship between gender and activity for retrieval tasks.
- Evidence anchors:
  - [abstract] "text-to-image retrieval, indicating poor activity understanding"
  - [section] "VLMs achieve an accuracy of approximately 50%, indicating that their performance is nearly random in this task"
  - [corpus] Weak evidence - related works focus on general VLM capabilities but don't specifically address text-to-image retrieval for activity-gender binding.
- Break condition: If the model can accurately perform text-to-image retrieval with gender-neutral prompts, it would indicate better activity understanding.

## Foundational Learning

- Concept: Cosine similarity in embedding space
  - Why needed here: The retrieval tasks rely on comparing cosine similarities between text and image embeddings to find matches.
  - Quick check question: If two embeddings have a cosine similarity of 0.9, are they more or less similar than embeddings with a cosine similarity of 0.5?

- Concept: Zero-shot learning in VLMs
  - Why needed here: The experiments evaluate VLMs without fine-tuning, relying on their pre-trained ability to understand and associate concepts.
  - Quick check question: What is the main advantage of using zero-shot learning in evaluating VLM biases?

- Concept: Sample selection bias
  - Why needed here: The paper attributes the gender-activity binding bias to sample selection bias in the training data.
  - Quick check question: How does sample selection bias differ from other forms of bias in machine learning datasets?

## Architecture Onboarding

- Component map: Text encoder -> Shared embedding space <- Image encoder
- Critical path:
  1. Input text or image
  2. Encode using appropriate encoder
  3. Compare embeddings using cosine similarity
  4. Retrieve most similar item from opposite modality
- Design tradeoffs:
  - Larger models may capture more nuanced relationships but are more computationally expensive
  - Balancing text and image encoder performance to ensure fair comparison
  - Trade-off between model size and inference speed for real-world applications
- Failure signatures:
  - Random performance in text-to-image retrieval tasks
  - Significant accuracy drop in image-to-text retrieval when unexpected gender performs activity
  - Consistent bias towards expected gender in text encoder evaluations
- First 3 experiments:
  1. Evaluate text-to-image retrieval accuracy with gender-neutral prompts
  2. Test image-to-text retrieval with single-gender scenarios to establish baseline
  3. Analyze embedding distances between expected and unexpected gender/activity pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact source of gender-activity binding bias in vision-language models (VLMs) - is it primarily due to training data biases or the model architecture itself?
- Basis in paper: [explicit] The paper states "we plan to address this limitation in future work by examining the sources of bias in publicly available training datasets for VLMs and analyzing the origins of these biases within those datasets."
- Why unresolved: The paper acknowledges that the source of bias in training data cannot be directly investigated due to limited accessibility to training sets, especially for models with unpublished datasets.
- What evidence would resolve it: Detailed analysis of publicly available training datasets used by VLMs, comparing activity-gender associations in training data versus model predictions, and controlled experiments varying training data composition.

### Open Question 2
- Question: How do VLMs' text and image encoders individually contribute to gender-activity binding bias, and can they be separately debiased?
- Basis in paper: [explicit] The paper shows "VLMs achieve an accuracy of approximately 50%, indicating that their performance is nearly random in this task" for text-to-image retrieval, suggesting text encoders internalize more bias than image encoders.
- Why unresolved: While the paper demonstrates differential performance between text and image encoders, it doesn't provide a detailed analysis of how each component contributes to the overall bias or methods to separately address each component's bias.
- What evidence would resolve it: Ablation studies isolating text and image encoder performance on gender-activity binding tasks, followed by targeted debiasing interventions for each component and measurement of their individual effects.

### Open Question 3
- Question: How generalizable is gender-activity binding bias to other social dimensions like race, age, or intersectional identities?
- Basis in paper: [explicit] The paper states "we plan to explore how VLMs perform across these dimensions using the experiments outlined in this study" for race and age biases.
- Why unresolved: The current study focuses exclusively on gender bias, and while the methodology could be extended to other dimensions, no empirical results exist for these other social categories.
- What evidence would resolve it: Replication of the GAB dataset and experiments using race and age as the primary social dimensions, with comparison of bias patterns and severity across different social categories.

## Limitations
- The study uses AI-generated images rather than real-world photographs, which may not fully capture real-world complexity
- The evaluation relies on cosine similarity in embedding space, which may not capture all nuances of gender-activity associations
- Limited investigation into the specific sources of bias within training datasets due to accessibility constraints

## Confidence

**High Confidence**: The 13.2% average accuracy drop in image-to-text retrieval when unexpected gender performs activities (supported by direct experimental measurements across 12 models)

**Medium Confidence**: The claim that text encoders show significant bias toward expected genders (based on embedding similarity analysis, though the specific thresholds for "significant" bias could be debated)

**Medium Confidence**: The assertion that VLMs perform near-random in text-to-image retrieval (supported by ~50% accuracy, though this could be influenced by the specific prompt formulations used)

## Next Checks

1. **Real Image Validation**: Replicate the key experiments using a subset of real-world images with controlled gender-activity pairings to verify if the bias patterns hold across image generation methods

2. **Cross-Dataset Consistency**: Test the same VLMs on established bias benchmarks like COCO and FairFace to assess whether the gender-activity binding bias is consistent across different datasets and evaluation paradigms

3. **Temporal Analysis**: Evaluate whether the magnitude of gender-activity binding bias correlates with the training data timeline of each VLM, potentially revealing whether more recent models show reduced bias