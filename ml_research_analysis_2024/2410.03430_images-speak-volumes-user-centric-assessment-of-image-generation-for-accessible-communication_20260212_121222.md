---
ver: rpa2
title: 'Images Speak Volumes: User-Centric Assessment of Image Generation for Accessible
  Communication'
arxiv_id: '2410.03430'
source_url: https://arxiv.org/abs/2410.03430
tags:
- images
- image
- group
- people
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated seven text-to-image models for generating
  accessible illustrations for easy-to-read (E2R) texts. Using 80 reference images
  from a German E2R gallery, models were benchmarked on automatic metrics (FID, CLIP,
  TIFA) and human evaluation (prompt coherence, correctness, bias, suitability).
---

# Images Speak Volumes: User-Centric Assessment of Image Generation for Accessible Communication

## Quick Facts
- arXiv ID: 2410.03430
- Source URL: https://arxiv.org/abs/2410.03430
- Authors: Miriam Anschütz; Tringa Sylaj; Georg Groh
- Reference count: 15
- Seven text-to-image models were evaluated for generating accessible illustrations for easy-to-read (E2R) texts, with DALL-E-3 and Midjourney achieving highest human ratings

## Executive Summary
This study evaluated seven text-to-image models for generating accessible illustrations for easy-to-read (E2R) texts using 80 reference images from a German E2R gallery. The research combined automatic metrics (FID, CLIP, TIFA) with human evaluation and a user study with seven target-group participants. DALL-E-3 and Midjourney achieved the highest human ratings, while open-source models struggled with body parts and complex motions. Results show models can produce suitable images but require human oversight, especially for nuanced accessibility needs. This is a key step toward automating accessible image creation, though limitations remain in model accuracy and inclusivity.

## Method Summary
The study generated 2,217 images from 80 reference images using seven T2I models (four open-source, three closed-source) with prompts "Cartoon picture of {title} - {description}". Images were evaluated using automatic metrics (FID for distribution similarity, CLIP for image-text alignment, TIFA for faithfulness via visual question answering) and human evaluation on four scales (prompt coherence, correctness, bias, suitability). A user study with seven participants from the target group provided feedback on preferred images. The approach combined quantitative benchmarks with qualitative assessment to evaluate both technical performance and real-world accessibility impact.

## Key Results
- DALL-E-3 and Midjourney achieved highest human ratings for prompt coherence, correctness, bias, and suitability for target group
- Open-source models (especially SD 1.4) struggled with body parts and complex motions like hands and faces
- Automatic metrics showed moderate correlation with human judgments but missed critical aspects of image suitability
- User study confirmed DALL-E-3 images were most preferred, though models require human oversight for nuanced accessibility needs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Image generation models can close the gap between generic image databases and the need for customized images in easy-to-read (E2R) texts by producing quickly available, flexible, and cost-effective illustrations.
- Mechanism: The study benchmarks seven text-to-image (T2I) models, generating 2,217 images from 80 reference images and evaluating them across multiple metrics, including automatic metrics (FID, CLIP, TIFA) and human evaluation. The evaluation shows that some models, particularly DALL-E-3 and Midjourney, produce images that are suitable for E2R texts, although human oversight is still required.
- Core assumption: T2I models can generate images that align with the prompts and are suitable for the target group, even if they don't perfectly match the reference images.
- Evidence anchors:
  - [abstract] "Our research is an important step toward facilitating the creation of accessible information for E2R creators and tailoring accessible images to the target group's needs."
  - [section] "Closed-source models like DALL-E-3 and Midjourney and the open-source Stable Diffusion 3 have shown impressive performance in creating these images, sometimes creating even more favorable images than the gold-standard references."
- Break condition: If the models consistently fail to generate images that are suitable for the target group or require excessive human oversight, making them impractical for large-scale use.

### Mechanism 2
- Claim: Automatic metrics like FID, CLIP, and TIFA can effectively evaluate the quality of generated images for accessible communication, but they may not fully capture the nuances of suitability for the target group.
- Mechanism: The study uses FID to compare the distribution of generated images to the reference images, CLIP to evaluate the alignment between images and descriptions, and TIFA to assess the faithfulness of images to the image descriptions through visual question answering. While these metrics provide valuable insights, they may not fully capture the suitability of images for the target group, which is why human evaluation is also necessary.
- Core assumption: Automatic metrics can provide a good initial assessment of image quality, but they may not capture all aspects of suitability for the target group.
- Evidence anchors:
  - [section] "The second aspect of our evaluation is how well the generated images follow the image descriptions. For this, we evaluate two different metrics. The first metric is Contrastive Language-Image Pre-training (CLIP), which is trained to determine if an image and a text are paired together (Radford et al., 2021)."
  - [section] "TIFA (Hu et al., 2023) also evaluates the fit between an image and its description, similar to the CLIP score, but chooses a different approach: visual question answering."
- Break condition: If the automatic metrics consistently fail to align with human judgments or if they miss critical aspects of image suitability for the target group.

### Mechanism 3
- Claim: Human evaluation is crucial for assessing the quality of generated images for accessible communication, as it can capture aspects that automatic metrics may miss, such as correctness, bias, and suitability for the target group.
- Mechanism: The study includes a human evaluation of 560 images, where an expert in German Easy language reviews the images based on four scales: prompt coherence, correctness, bias, and suitability for the target group. This evaluation provides valuable insights into the strengths and weaknesses of the T2I models that automatic metrics may not capture.
- Core assumption: Human evaluators can provide a more nuanced assessment of image quality, especially for aspects that are important for the target group.
- Evidence anchors:
  - [section] "While TIFA scores have a high correlation with human judgments (Hu et al., 2023), automatic metrics can't cover all evaluation aspects. Especially for the overall correctness and simplicity of the images, there is currently no metric available. Therefore, we added a human evaluation of our generated images."
  - [section] "The human annotator could choose between four possible answers to the questions: no/indeterminable, partly, mostly, and yes. We mapped these answers to a numerical scale between 0 (answer no) and 3 (answer yes)."
- Break condition: If human evaluators consistently disagree with each other or if their judgments are not reliable or representative of the target group's preferences.

## Foundational Learning

- Concept: Easy-to-read (E2R) texts and their guidelines
  - Why needed here: Understanding the purpose and requirements of E2R texts is crucial for evaluating the suitability of generated images for this specific use case.
  - Quick check question: What are the key characteristics of E2R texts, and how do images play a role in making them more accessible?

- Concept: Text-to-image (T2I) models and their capabilities
  - Why needed here: Familiarity with T2I models, their architectures, and their limitations is essential for understanding the study's approach and results.
  - Quick check question: What are the main differences between open-source and closed-source T2I models, and how do these differences affect their performance in generating images for E2R texts?

- Concept: Automatic evaluation metrics for image generation
  - Why needed here: Understanding the strengths and limitations of automatic metrics like FID, CLIP, and TIFA is crucial for interpreting the study's findings and their implications for accessible communication.
  - Quick check question: How do FID, CLIP, and TIFA differ in their approach to evaluating image quality, and what are their respective strengths and limitations?

## Architecture Onboarding

- Component map: Reference dataset (80 images) -> T2I models (7 models) -> Image generation (up to 4 images per model/prompt) -> Automatic evaluation (FID, CLIP, TIFA) -> Human evaluation (560 images) -> User study (7 participants)

- Critical path: Generate images using T2I models → Calculate automatic metrics → Conduct human evaluation → Perform user study with target group

- Design tradeoffs: Mix of open-source and closed-source models provides comprehensive evaluation but may introduce biases; focus on cartoon-style images may not suit all E2R contexts

- Failure signatures: Models consistently generating unsuitable images; automatic metrics failing to align with human judgments; user study not providing representative feedback

- First 3 experiments:
  1. Generate images using a single T2I model and evaluate them using automatic metrics to assess initial performance
  2. Conduct a small-scale human evaluation of generated images to gather initial feedback on target group suitability
  3. Perform a pilot user study with a small group of participants from the target group to test methodology and gather preliminary feedback

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Small sample size (80 reference images, 7 user study participants) limits statistical power
- Focus on German E2R texts and cartoon-style images restricts generalizability
- Open-source model performance variability and absence of prompt engineering optimization introduce uncertainty
- Models require human oversight for nuanced accessibility needs, limiting automation potential

## Confidence
- Human evaluation findings (DALL-E-3/Midjourney superiority): High
- Automatic metric correlations with human judgment: Medium
- User study preferences: Medium (small sample size)
- Generalizability across languages/domains: Low

## Next Checks
1. Replicate user study with larger, more diverse participant pool across different accessibility needs
2. Test additional T2I models with optimized prompts for E2R contexts
3. Validate findings across multiple languages and non-cartoon visual styles to assess generalizability