---
ver: rpa2
title: 'From pixels to planning: scale-free active inference'
arxiv_id: '2407.20292'
source_url: https://arxiv.org/abs/2407.20292
tags:
- learning
- level
- states
- figure
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces renormalising generative models (RGMs) as
  discrete state-space models that can handle classification, compression, and planning
  across different scales. The core idea is to generalise Markov decision processes
  by including paths as latent variables and applying renormalisation group techniques
  to achieve scale-free hierarchical representations.
---

# From pixels to planning: scale-free active inference

## Quick Facts
- arXiv ID: 2407.20292
- Source URL: https://arxiv.org/abs/2407.20292
- Authors: Karl Friston; Conor Heins; Tim Verbelen; Lancelot Da Costa; Tommaso Salvatori; Dimitrije Markovic; Alexander Tschantz; Magnus Koudahl; Christopher Buckley; Thomas Parr
- Reference count: 14
- Primary result: RGMs achieve scale-free learning across classification, compression, and planning tasks with 95.1% MNIST accuracy

## Executive Summary
This paper introduces renormalising generative models (RGMs) as discrete state-space models that generalize Markov decision processes by incorporating paths as latent variables and applying renormalisation group techniques. The core innovation enables scale-free hierarchical representations that achieve efficient learning and inference across space and time through recursive coarse-graining. The approach successfully demonstrates applications in MNIST digit classification (95.1% accuracy), video compression and generation including chaotic systems, and Atari-like game playing through structure learning and inductive inference.

## Method Summary
RGMs employ variational inference with active learning via expected free energy minimization, Bayesian model selection for structure learning, and renormalisation group techniques to create hierarchical discrete state-space models. The method uses blocking transformations with singular value decomposition to quantize continuous data, enabling efficient exact inference through conditional independence assumptions among latent factors.

## Key Results
- Achieves 95.1% accuracy on MNIST digit classification through progressive structure learning
- Successfully compresses and generates video sequences including chaotic systems
- Demonstrates planning capabilities in Atari-like environments through structure learning and inductive inference
- Shows sample efficiency by selecting informative data for learning and avoiding overfitting through Bayesian model selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RGMs achieve scale-free learning by recursively applying block transformations that preserve mutual information while reducing dimensionality
- Mechanism: The blocking transformation groups local states, applies singular value decomposition to identify a compressed basis, and retains only the most informative singular variates. This process repeats hierarchically, creating a multi-scale representation where each level generates the initial conditions and paths for the next lower level.
- Core assumption: Conditional independence among latent factors ensures that each state has only one parent, enabling efficient sum-product operations.
- Evidence anchors:
  - [abstract] "generalise Markov decision processes by including paths as latent variables and applying renormalisation group techniques to achieve scale-free hierarchical representations"
  - [section] "Each application of the block transformation creates a likelihood mapping (D) from the states at a higher level to the lower level"
  - [corpus] Weak evidence - corpus focuses on discrete state-space models but doesn't explicitly discuss mutual information preservation during blocking
- Break condition: If conditional independence fails or if blocking creates co-parents, the efficient sum-product operations break down and the model becomes computationally intractable.

### Mechanism 2
- Claim: Active learning via expected free energy minimization ensures sample efficiency by selecting only informative data that reduces uncertainty.
- Mechanism: When updating Dirichlet parameters, the model evaluates expected free energy for each potential update. Only updates that decrease expected free energy are accepted, effectively implementing a Bayesian model selection that accumulates counts only when they maximize mutual information between outcomes and hidden states.
- Core assumption: Expected free energy accurately predicts the information gain from including a particular training example.
- Evidence anchors:
  - [abstract] "The RGM architecture naturally captures compositional structure and provides a unified framework for perception, generation, and decision-making"
  - [section] "Active learning has a specific meaning in this paper. It implies that the updating of Dirichlet counts depends on expected free energy"
  - [corpus] Weak evidence - corpus discusses learning but doesn't explicitly address expected free energy-based sample selection
- Break condition: If the expected free energy calculation is inaccurate or if the model cannot distinguish between informative and redundant examples, sample efficiency degrades and the model may overfit or underfit.

### Mechanism 3
- Claim: The hierarchical structure with temporal separation enables semi-Markovian processes that capture long-range dependencies.
- Mechanism: Higher levels generate sequences of sequences by producing initial states and paths for lower levels. This creates a separation of temporal scales where belief updates at higher levels occur more slowly than at lower levels, allowing the model to represent events of increasing temporal depth.
- Core assumption: The recursive composition of RG operators maintains the functional form of dynamics across scales.
- Evidence anchors:
  - [abstract] "furnishing models of paths or orbits; i.e., events of increasing temporal depth and itinerancy"
  - [section] "A more intuitive view —of the latitude afforded by temporal renormalisation —is that successively higher levels encode sequences of sequences"
  - [corpus] Moderate evidence - corpus discusses hierarchical temporal models but doesn't explicitly link to semi-Markovian properties
- Break condition: If the temporal separation breaks down or if the recursive composition fails to maintain consistent dynamics, the model loses its ability to capture long-range dependencies.

## Foundational Learning

- Concept: Renormalisation Group Theory
  - Why needed here: Provides the mathematical framework for scale-free hierarchical representations and ensures consistency across different levels of abstraction
  - Quick check question: How does the RG operator preserve relevant information while coarse-graining data?

- Concept: Variational Inference
  - Why needed here: Enables approximate Bayesian inference by minimizing variational free energy, which serves as both a bound on model evidence and an objective function for learning
  - Quick check question: What is the relationship between variational free energy and the Kullback-Leibler divergence?

- Concept: Dirichlet Distributions
  - Why needed here: Provides the conjugate prior for categorical distributions used in the generative model, enabling efficient Bayesian updating of transition probabilities
  - Quick check question: How do Dirichlet parameters encode the sufficient statistics for posterior beliefs?

## Architecture Onboarding

- Component map: Hierarchical levels with factors (state groups) connected by likelihood mappings (A, D, E tensors) and transition tensors (B), where states generate initial conditions and paths for subordinate levels recursively
- Critical path: Structure learning → Active learning → Model inversion → Planning as inference
- Design tradeoffs: Discrete state-space enables exact inference but limits expressiveness compared to continuous models; blocking transformations provide compression but may lose fine-grained information
- Failure signatures: Poor classification accuracy suggests inadequate structure learning; slow convergence indicates suboptimal blocking transformations; inability to generate plausible sequences suggests missing temporal structure
- First 3 experiments:
  1. Implement the MNIST digit classification pipeline with progressive structure learning and active learning
  2. Test the video compression pipeline on synthetic video sequences with known dynamics
  3. Apply the RGM to a simple Atari-like game with sparse rewards and evaluate planning performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can renormalising generative models (RGMs) be extended to continuous state-space models while preserving their renormalisation properties and computational efficiency?
- Basis in paper: [explicit] The paper mentions that converting continuous state-space models into master equations could enable learning discrete state-space models using renormalisation procedures, but then replacing the Dirichlet parameterisation with forms parameterised in terms of rate constants and probability fluxes among compartments or states.
- Why unresolved: The paper only suggests this as a possibility without providing concrete implementation details or demonstrating the effectiveness of such an approach.
- What evidence would resolve it: Successful implementation of continuous state-space RGMs with demonstrated improvements in sample efficiency and computational speed compared to traditional continuous models, along with clear parameter interpretation and biophysical relevance.

### Open Question 2
- Question: What is the relationship between the brittleness of RGMs due to their minimal complexity and their ability to generalise to novel, unseen data?
- Basis in paper: [explicit] The paper explicitly discusses this as a limitation, comparing it to learning to ride a bike - the model only learns from specific starting positions and sequences that lead to rewards, making it unable to handle arbitrary starting states.
- Why unresolved: The paper identifies this as a potential issue but does not provide empirical evidence of how this brittleness manifests in practice or potential solutions to improve generalisation.
- What evidence would resolve it: Empirical studies comparing RGM performance on novel data distributions versus traditional models, along with analysis of how the brittleness affects real-world applicability and potential architectural modifications to improve robustness.

### Open Question 3
- Question: How can the computational efficiency of RGMs be maintained while scaling to more complex, higher-dimensional data while preserving their renormalisation properties?
- Basis in paper: [inferred] The paper demonstrates successful applications on MNIST, video, and audio data, but these are relatively low-dimensional compared to modern large-scale datasets. The efficiency claims are based on sample efficiency and compression, but scalability is not explicitly addressed.
- Why unresolved: The paper does not address the computational challenges of applying RGMs to large-scale, high-dimensional data or how the renormalisation group approach scales with increasing complexity.
- What evidence would resolve it: Successful implementation of RGMs on large-scale datasets (e.g., ImageNet, large video datasets) with demonstrated computational efficiency and maintained performance, along with analysis of how the renormalisation group approach handles increased dimensionality.

## Limitations
- MNIST classification accuracy of 95.1% is respectable but not state-of-the-art, with unclear comparison baselines
- Video compression and generation results lack quantitative metrics for proper evaluation
- Atari-like game playing results presented without detailed performance comparisons to established reinforcement learning methods

## Confidence
- High confidence: The mathematical framework of renormalising generative models and their hierarchical composition is well-specified and internally consistent
- Medium confidence: The active learning mechanism via expected free energy minimization is theoretically sound but practical implementation details may vary
- Low confidence: The sample efficiency claims and avoidance of overfitting through Bayesian model selection require more rigorous empirical validation

## Next Checks
1. Implement controlled experiments comparing RGM performance against standard active inference models on identical datasets to verify claimed sample efficiency gains
2. Conduct ablation studies removing the renormalisation group hierarchy to quantify the contribution of scale-free representations to overall performance
3. Test the model's robustness to noise and missing data by systematically degrading input quality and measuring performance degradation curves