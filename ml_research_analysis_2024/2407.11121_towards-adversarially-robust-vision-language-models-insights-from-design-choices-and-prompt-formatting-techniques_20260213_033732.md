---
ver: rpa2
title: 'Towards Adversarially Robust Vision-Language Models: Insights from Design
  Choices and Prompt Formatting Techniques'
arxiv_id: '2407.11121'
source_url: https://arxiv.org/abs/2407.11121
tags:
- adversarial
- robustness
- image
- apgd
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work systematically evaluates how different design choices
  in Vision-Language Models (VLMs) affect their robustness to white-box adversarial
  attacks on image inputs. Experiments with multiple VLMs reveal that: (1) vision
  encoders trained on diverse datasets provide marginal robustness gains, only against
  weaker attacks; (2) higher image resolution does not consistently improve robustness;
  (3) increasing language model size does not enhance robustness to image-based attacks;
  and (4) ensembles of vision encoders remain vulnerable if the weakest encoder is
  attacked.'
---

# Towards Adversarially Robust Vision-Language Models: Insights from Design Choices and Prompt Formatting Techniques

## Quick Facts
- arXiv ID: 2407.11121
- Source URL: https://arxiv.org/abs/2407.11121
- Authors: Rishika Bhagwatkar; Shravan Nayak; Reza Bayat; Alexis Roger; Daniel Z Kaplan; Pouya Bashivan; Irina Rish
- Reference count: 21
- Primary result: Prompt formatting techniques can significantly improve VLM robustness to image-based adversarial attacks without additional training

## Executive Summary
This work systematically evaluates how different design choices in Vision-Language Models (VLMs) affect their robustness to white-box adversarial attacks on image inputs. Experiments with multiple VLMs reveal that: (1) vision encoders trained on diverse datasets provide marginal robustness gains, only against weaker attacks; (2) higher image resolution does not consistently improve robustness; (3) increasing language model size does not enhance robustness to image-based attacks; and (4) ensembles of vision encoders remain vulnerable if the weakest encoder is attacked. The study also introduces prompt formatting techniques to enhance robustness without additional training. By rephrasing questions and indicating the possibility of adversarial perturbations, significant improvements in model robustness against strong image-based attacks (e.g., Auto-PGD) are achieved, particularly for Visual Question Answering tasks. These findings provide actionable insights for building more robust VLMs, especially for safety-critical applications.

## Method Summary
The study evaluates adversarial robustness of VLMs through controlled experiments using white-box attacks (FGSM, PGD, APGD) with ℓ∞ perturbations (ϵ ∈ {4/255, 8/255, 16/255}). Multiple VLMs (LLaVA-7B, LLaVA-13B, CLIP, SigLIP, DINOv2, ImageNet encoders) are tested across COCO, Flickr30k, VQAv2, TextVQA, OK-VQA, and VizWiz datasets. The approach involves no additional training; instead, it focuses on evaluating existing architectures under attack and testing prompt formatting techniques. Prompt variations include original prompts, adversarial cues (AC), adversarial possibility (AP), and random prompts to assess their impact on robust accuracy.

## Key Results
- Vision encoders trained on diverse datasets provide only marginal robustness gains, effective only against weaker attacks
- Higher image resolution does not consistently improve adversarial robustness across tasks
- Increasing language model size (7B to 13B parameters) does not enhance robustness to image-based attacks
- Ensemble vision encoders remain vulnerable if the weakest encoder is attacked
- Prompt formatting techniques (rephrasing questions and indicating adversarial perturbations) significantly improve robust accuracy against strong attacks like Auto-PGD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt formatting improves robustness by shifting the model's attention toward adversarial vulnerability awareness
- Mechanism: When the prompt explicitly suggests or states the image might be adversarially perturbed, the model adjusts its internal processing to be more cautious, improving robustness especially against strong attacks like APGD
- Core assumption: The language model component in the VLM can interpret and act upon the semantic content of the prompt to adjust its inference strategy
- Evidence anchors:
  - [abstract] "By rephrasing questions and suggesting potential adversarial perturbations, we demonstrate substantial improvements in model robustness against strong image-based attacks such as Auto-PGD."
  - [section] "From the results presented in Fig. 1 and Table 16 in Appendix C, it is evident that indicating the possibility of adversarial perturbations (AP prompt) assists the model significantly more than explicitly stating that the image is perturbed (AC prompt)."
  - [corpus] Weak or missing. No corpus papers directly test this prompt-awareness mechanism.
- Break condition: If the LLM fails to process prompt semantics correctly, or if the adversarial perturbation is so severe that no linguistic cue can compensate for corrupted visual features

### Mechanism 2
- Claim: Increasing the resolution of vision encoders does not reliably improve adversarial robustness
- Mechanism: Higher resolution may improve clean accuracy by providing more detailed features, but adversarial attacks can still corrupt these features in a way that degrades robustness
- Core assumption: The adversarial attack can exploit vulnerabilities regardless of resolution, and robustness gains are not proportional to feature richness
- Evidence anchors:
  - [section] "Increasing the resolution of image encoders did not correlate with enhanced adversarial robustness, suggesting that benefits seen in the clean accuracy do not extend to improved robustness."
  - [section] "Based on Table 3, while increasing resolution enhances robustness against stronger attacks for high-resolution CLIP models (on most tasks), the effectiveness of the increased resolution in SigLIP models appears to be task-dependent."
  - [corpus] Weak or missing. No corpus papers directly compare resolution effects on robustness.
- Break condition: If the model architecture changes so that higher resolution features are more resilient to perturbation, or if attack methods become resolution-specific

### Mechanism 3
- Claim: Ensemble vision encoders remain vulnerable if the weakest encoder is attacked
- Mechanism: The ensemble model's robustness is limited by the most vulnerable component; attacking only the weakest encoder can compromise the entire system's output
- Core assumption: The ensemble architecture does not implement defensive voting or thresholding to reject outputs from compromised encoders
- Evidence anchors:
  - [section] "Our results revealed that using multiple vision encoders does not guarantee robustness; rather, knowledge about the most vulnerable encoder is enough to compromise the entire system."
  - [section] "Results in Table 5 show that attacking only DINOv2 is sufficient to compromise the model under stronger attacks, despite the other vision encoder providing clean inputs."
  - [corpus] Weak or missing. No corpus papers directly test ensemble vulnerability to targeted encoder attacks.
- Break condition: If the ensemble architecture implements robust fusion strategies or if all encoders are equally strong

## Foundational Learning

- Concept: Adversarial robustness in VLMs
  - Why needed here: This work focuses on improving the model's resistance to adversarial attacks, which is central to safe deployment in safety-critical applications
  - Quick check question: What is the difference between clean accuracy and robust accuracy in adversarial settings?

- Concept: Vision-Language Model (VLM) architecture
  - Why needed here: The study evaluates how different architectural choices (vision encoder, language model, resolution, ensemble) affect robustness
  - Quick check question: How do VLMs typically align visual and textual representations?

- Concept: Prompt formatting techniques
  - Why needed here: The research introduces novel prompt-based approaches to enhance robustness without additional training
  - Quick check question: How can modifying the text prompt affect a VLM's interpretation of adversarial images?

## Architecture Onboarding

- Component map: Input image and text prompt -> Vision encoder -> Mapping network -> Language model -> Output
- Critical path:
  1. Input image and text prompt are processed
  2. Vision encoder extracts visual features from image
  3. Mapping network aligns visual features with language model embedding space
  4. Language model generates output conditioned on combined embeddings and prompt
  5. Adversarial robustness is evaluated by measuring performance under perturbed inputs

- Design tradeoffs:
  - Vision encoder choice: Tradeoff between clean accuracy gains and robustness improvements
  - Resolution: Higher resolution may improve clean accuracy but not necessarily robustness
  - Language model size: Larger models don't necessarily improve robustness to image-based attacks
  - Ensemble approach: Can improve performance but may not improve robustness if weakest encoder is attacked

- Failure signatures:
  - Significant drop in accuracy when images are adversarially perturbed
  - Inconsistent performance improvements across different tasks when using higher resolution or larger models
  - Vulnerability of ensemble models when weakest encoder is targeted

- First 3 experiments:
  1. Evaluate clean vs robust accuracy of a VLM under FGSM, PGD, and APGD attacks with varying perturbation magnitudes
  2. Test the effect of different prompt formats (original, AC, AP, random) on robust accuracy for a specific VLM
  3. Compare robustness of VLMs with different vision encoders (CLIP, SigLIP, DINOv2, ImageNet) under the same attack conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the adversarial robustness of VLMs change when using ensemble methods that include a robust vision encoder (e.g., one trained with adversarial training)?
- Basis in paper: [inferred] The paper shows that attacking the weakest vision encoder in an ensemble is sufficient to compromise the entire system. However, it does not explore scenarios where the ensemble includes a robust encoder.
- Why unresolved: The study focused on using standard encoders without explicit adversarial training. The impact of including robust encoders in an ensemble remains unexplored.
- What evidence would resolve it: Experiments comparing the robustness of ensembles with and without a robust encoder, tested against various adversarial attacks, would provide insights into the effectiveness of such configurations.

### Open Question 2
- Question: To what extent can prompt formatting techniques improve robustness against multimodal adversarial attacks that target both image and text inputs?
- Basis in paper: [explicit] The paper demonstrates that prompt formatting can enhance robustness against image-based attacks but does not investigate multimodal attacks.
- Why unresolved: The study's scope was limited to image perturbations, leaving the effectiveness of prompt formatting against combined image and text attacks untested.
- What evidence would resolve it: Testing VLMs with prompt formatting techniques under multimodal adversarial attacks (simultaneously perturbing images and text) would determine the broader applicability of this approach.

### Open Question 3
- Question: How do different types of prompt modifications (e.g., rephrasing, expanding, or adding adversarial hints) affect robustness across various VQA datasets with different characteristics?
- Basis in paper: [explicit] The paper explores prompt formatting for VQA tasks but focuses on a limited set of datasets and prompt strategies.
- Why unresolved: The study did not extensively analyze how prompt modifications perform across diverse VQA datasets with varying complexity and domain.
- What evidence would resolve it: Systematic evaluation of prompt formatting techniques across a wide range of VQA datasets would reveal which strategies are most effective for different types of questions and contexts.

## Limitations

- The findings on prompt formatting effectiveness are limited to white-box attacks and may not generalize to black-box or multimodal attack scenarios
- The study focuses on specific VLM architectures (CLIP, SigLIP, LLaVA variants) and may not apply to all VLM designs
- The mechanism by which prompt formatting improves robustness remains speculative without direct empirical validation

## Confidence

**High Confidence**: The finding that increasing language model size does not enhance robustness to image-based attacks is well-supported with consistent results across multiple model sizes (7B and 13B parameters).

**Medium Confidence**: The ensemble vulnerability finding is reasonably supported, but the generalizability depends on the specific fusion strategy employed. The paper's observation holds for the tested ensemble configuration but may not apply universally.

**Low Confidence**: The prompt formatting mechanism lacks direct empirical validation. While effectiveness is demonstrated, the proposed explanation about semantic interpretation by the language model remains a hypothesis without rigorous testing.

## Next Checks

1. **Mechanism validation**: Design an ablation study that isolates the language model's role by testing prompt formatting effectiveness when visual features are completely corrupted versus mildly perturbed.

2. **Architecture generalization**: Replicate the resolution robustness experiments across additional VLM architectures (beyond CLIP and SigLIP) to determine if the task-dependent effects are architecture-specific or more general.

3. **Attack type robustness**: Evaluate prompt formatting effectiveness against black-box and transfer-based attacks to determine if the robustness gains extend beyond white-box scenarios.