---
ver: rpa2
title: ASR Error Correction using Large Language Models
arxiv_id: '2409.09554'
source_url: https://arxiv.org/abs/2409.09554
tags:
- n-best
- correction
- decoding
- error
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving Automatic Speech
  Recognition (ASR) transcriptions through error correction (EC) using Large Language
  Models (LLMs). The study proposes leveraging ASR N-best lists as richer input for
  EC models, providing more contextual information for the correction process.
---

# ASR Error Correction using Large Language Models

## Quick Facts
- arXiv ID: 2409.09554
- Source URL: https://arxiv.org/abs/2409.09554
- Authors: Rao Ma; Mengjie Qian; Mark Gales; Kate Knill
- Reference count: 40
- One-line primary result: ASR error correction using N-best lists improves WER by up to 7.8% on standard datasets

## Executive Summary
This paper addresses the challenge of improving Automatic Speech Recognition (ASR) transcriptions through error correction (EC) using Large Language Models (LLMs). The study proposes leveraging ASR N-best lists as richer input for EC models, providing more contextual information for the correction process. The paper introduces novel decoding strategies, including constrained decoding based on N-best lists or ASR lattices, to enhance model robustness and alignment with the original utterance. Additionally, it explores the generalization ability of EC models across different ASR systems and domains, extending to zero-shot error correction using LLMs like ChatGPT.

## Method Summary
The approach involves fine-tuning T5-base models on augmented erroneous transcriptions from LibriSpeech, then evaluating on three standard datasets (LibriSpeech, TED-LIUM3, Artie Bias). The method uses ASR N-best lists (top 5 or 10 hypotheses) as input to the error correction model, implementing various decoding strategies including unconstrained, constrained, closest mapping, and lattice-constrained decoding. Zero-shot prompting with ChatGPT (GPT-3.5 and GPT-4) is also evaluated using N-best hypotheses. The evaluation measures Word Error Rate (WER) reduction compared to baseline ASR systems across different decoding strategies.

## Key Results
- Fine-tuned T5 with N-best lists achieves WER reductions of 6.4% on LibriSpeech and 7.8% on TED-LIUM3 using Conformer-Transducer outputs
- Constrained decoding based on N-best lists shows mixed results, improving performance on some datasets but degrading on others
- Zero-shot ChatGPT error correction shows limited effectiveness, with GPT-4 failing to produce meaningful improvements and GPT-3.5 showing only modest gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Using ASR N-best lists as input to error correction models provides richer contextual information, leading to better error detection and correction performance.
- **Mechanism**: The N-best list contains multiple transcription hypotheses ranked by likelihood, allowing the error correction model to leverage alternative sequences that may be closer to the correct transcription. This provides more semantic and phonetic cues for the model to identify and correct errors.
- **Core assumption**: The ASR N-best hypotheses are diverse enough to contain meaningful alternatives that can guide the error correction process.
- **Evidence anchors**: [abstract]: "We propose building high-performance EC models using ASR N-best lists which should provide more contextual information for the correction process." [section]: "The rationale behind this is that the N-best list contains alternative sequences that have a strong possibility of being the correct transcription, thus providing valuable cues for the EC model during predictions."

### Mechanism 2
- **Claim**: Constrained decoding based on N-best lists or ASR lattices improves model robustness and alignment with the original utterance.
- **Mechanism**: By restricting the output space to hypotheses within the N-best list or lattice, the error correction model is forced to generate outputs that are more consistent with the original speech content. This prevents the model from generating completely unrelated or incorrect corrections.
- **Core assumption**: The correct transcription is more likely to be found within the ASR N-best list or lattice than in the unconstrained generation space.
- **Evidence anchors**: [abstract]: "We introduce a constrained decoding approach based on the N-best list or an ASR lattice." [section]: "When an N-best list is used, we can constrain the model to allow it to generate from a limited space."

### Mechanism 3
- **Claim**: Zero-shot error correction using large language models can generalize across different ASR systems and domains without requiring retraining.
- **Mechanism**: LLMs like ChatGPT have been pre-trained on vast amounts of text data, giving them strong language understanding capabilities. When provided with ASR N-best hypotheses as context, they can leverage this knowledge to identify and correct errors without needing task-specific training.
- **Core assumption**: The language understanding capabilities learned during LLM pre-training are transferable to the ASR error correction task.
- **Evidence anchors**: [abstract]: "This concept is further extended to zero-shot error correction using LLMs, such as ChatGPT." [section]: "By fine-tuning these pre-trained large language models (LLMs), the implicit knowledge acquired from vast amounts of text data can be effectively transferred to the target error correction task."

## Foundational Learning

- **Concept: Automatic Speech Recognition (ASR)**
  - Why needed here: Understanding the ASR process and its limitations is crucial for designing effective error correction methods.
  - Quick check question: What are the main sources of errors in ASR systems, and how do they typically manifest in transcriptions?

- **Concept: Language Models and Transformers**
  - Why needed here: The error correction models are built using transformer-based architectures, requiring knowledge of how these models process and generate text.
  - Quick check question: How do transformer models use attention mechanisms to capture contextual information in sequences?

- **Concept: Decoding Strategies in NLP**
  - Why needed here: Different decoding approaches (e.g., unconstrained, constrained, beam search) significantly impact the performance of error correction models.
  - Quick check question: What are the tradeoffs between exploring a larger search space versus constraining the output to more likely candidates?

## Architecture Onboarding

- **Component map**: ASR system -> N-best list generator -> Error correction model (T5/ChatGPT) -> Decoding module -> Evaluation pipeline

- **Critical path**: 1. ASR generates N-best hypotheses and lattices; 2. Error correction model processes N-best input; 3. Decoding module applies chosen strategy; 4. Output is evaluated against reference transcriptions

- **Design tradeoffs**:
  - Model size vs. inference speed: Larger models may achieve better performance but are slower
  - N-best list size: Larger N provides more context but increases computational cost
  - Constrained vs. unconstrained decoding: Constrained decoding may improve robustness but could limit correction potential

- **Failure signatures**:
  - Degradation in performance when applying model to out-of-domain data
  - Limited improvement when ASR system already has low baseline WER
  - Over-reliance on formatting differences in N-best hypotheses (e.g., punctuation, capitalization)

- **First 3 experiments**:
  1. Compare error correction performance using 1-best vs. 5-best vs. 10-best input hypotheses
  2. Evaluate the impact of different decoding strategies (unconstrained, constrained, closest mapping) on correction accuracy
  3. Test the generalization ability of the error correction model on outputs from different ASR systems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal N-best list size for ASR error correction using LLMs, and how does it vary across different ASR architectures?
- Basis in paper: [explicit] The paper extensively analyzes N-best list sizes (1-best to 10-best) for both fine-tuning and zero-shot approaches, finding that increasing N generally helps but doesn't always guarantee proportional benefits.
- Why unresolved: The study shows diminishing returns beyond certain N values, but doesn't identify a universal optimal size across all ASR systems.
- What evidence would resolve it: Systematic experiments comparing error correction performance across multiple ASR architectures (Transducer, LAS, RNN-T) with varying N-best list sizes on diverse datasets.

### Open Question 2
- Question: How does data contamination affect zero-shot ASR error correction performance with different LLMs?
- Basis in paper: [explicit] The paper develops a data contamination quiz method and finds varying levels of contamination across different LLMs and datasets, with GPT-4 showing some contamination on certain datasets.
- Why unresolved: While contamination levels are measured, the paper doesn't quantify the direct impact on error correction performance.
- What evidence would resolve it: Comparative experiments measuring error correction performance on contaminated vs. uncontaminated test sets across multiple LLMs.

### Open Question 3
- Question: What are the limitations of zero-shot ASR error correction with LLMs for high-performance ASR systems like Whisper?
- Basis in paper: [explicit] The paper demonstrates that zero-shot methods struggle with Whisper outputs compared to other ASR systems, particularly on TED-LIUM3 dataset.
- Why unresolved: The paper identifies this limitation but doesn't fully explain the underlying reasons or propose solutions.
- What evidence would resolve it: In-depth analysis of N-best list characteristics from high-performance ASR systems and their impact on LLM error correction capabilities.

## Limitations

- Performance highly sensitive to input ASR N-best list quality, with significant degradation when using Whisper outputs
- Constrained decoding shows inconsistent improvements across datasets, suggesting it may not be universally beneficial
- Zero-shot ChatGPT error correction demonstrates limited effectiveness with minimal to no improvement across tested configurations

## Confidence

**High confidence**: Using N-best lists as input can improve error correction performance when hypotheses are diverse and ASR outputs are reasonable (supported by WER reductions of 6.4% on LibriSpeech and 7.8% on TED-LIUM3).

**Medium confidence**: Constrained decoding effectiveness varies significantly across datasets and ASR systems, with mixed performance results.

**Low confidence**: Zero-shot LLM error correction capabilities are limited, with minimal to no improvement observed across all tested configurations.

## Next Checks

1. Test cross-dataset generalization: Evaluate the fine-tuned T5 model on ASR outputs from completely different domains (e.g., medical transcription or conversational speech) to assess true generalization capabilities beyond the tested datasets.

2. Analyze N-best list diversity correlation: Quantitatively measure the relationship between N-best list diversity metrics (Cross WER, hypothesis uniqueness) and subsequent error correction performance to validate the assumption that diverse N-best lists lead to better corrections.

3. Benchmark against specialized error correction systems: Compare the proposed approach against dedicated error correction models that don't use N-best lists to determine whether the N-best input provides sufficient additional value to justify the increased computational complexity.