---
ver: rpa2
title: A survey on the impacts of recommender systems on users, items, and human-AI
  ecosystems
arxiv_id: '2407.01630'
source_url: https://arxiv.org/abs/2407.01630
tags:
- users
- ecosystem
- recommender
- diversity
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews the impact of recommender systems
  across four human-AI ecosystems, standardising fragmented terminology and methodologies.
  It introduces a unified taxonomy classifying outcomes (concentration, diversity,
  discrimination, echo chamber, filter bubble, homogenisation, polarisation, radicalisation,
  volume, content degradation) and their analytical levels (individual, item, ecosystem).
---

# A survey on the impacts of recommender systems on users, items, and human-AI ecosystems

## Quick Facts
- arXiv ID: 2407.01630
- Source URL: https://arxiv.org/abs/2407.01630
- Reference count: 40
- Primary result: Unified taxonomy and methodology categorisation reveals recommender systems amplify popularity bias and concentration while increasing individual diversity but reducing ecosystem diversity

## Executive Summary
This survey systematically reviews the impact of recommender systems across four human-AI ecosystems (social media, online retail, urban mapping, generative AI), standardising fragmented terminology and methodologies. It introduces a unified taxonomy classifying outcomes (concentration, diversity, discrimination, echo chamber, filter bubble, homogenisation, polarisation, radicalisation, volume, content degradation) and their analytical levels (individual, item, ecosystem). The survey also harmonises measurement approaches and identifies methodological gaps, particularly the scarcity of controlled studies in social media, urban mapping, and generative AI. Findings reveal that recommenders amplify popularity bias and concentration across ecosystems while increasing individual diversity but reducing ecosystem diversity. In generative AI, self-consuming training loops cause content degradation. The work highlights ethical concerns such as discrimination and filter bubbles, and calls for standardised datasets, transparent governance, and public-interest algorithmic alternatives to mitigate negative impacts.

## Method Summary
The survey conducted a qualitative systematic review of 154 articles from multiple disciplines, gathered via Google Scholar, Web of Science, EBSCO, JSTOR, ArXiv, journal browsing, reference cross-checking, and expert input. The methodology involved iterative consensus-building taxonomy development through independent classification by two coders per article, followed by group discussion and refinement. The process defined a preliminary taxonomy by sampling and classifying a small set of relevant articles, then iteratively refined it. Candidate articles were collected via keyword searches and journal browsing, screened for relevance, and assigned to two coders for independent classification. Classifications were discussed within ecosystem teams, disagreements resolved, and preliminary classifications presented to the full group for iterative improvement.

## Key Results
- Recommender systems amplify popularity bias and concentration across all four ecosystems studied
- Individual-level diversity increases while ecosystem-level diversity decreases
- Generative AI suffers content degradation due to self-consuming training loops
- Limited controlled studies in social media, urban mapping, and generative AI create methodological gaps
- Discrimination and filter bubble effects represent significant ethical concerns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey's unified taxonomy harmonises fragmented terminology across ecosystems, enabling systematic comparison and knowledge accumulation.
- Mechanism: By abstracting user, item, interaction, and recommendation components, the taxonomy provides a consistent framework that transcends ecosystem-specific jargon, allowing researchers to classify outcomes (e.g., concentration vs. popularity bias) under a single label.
- Core assumption: Terminological fragmentation significantly hinders cross-ecosystem research and that a unified notation reduces this barrier.
- Evidence anchors:
  - [abstract] "This is a crucial contribution to the literature because terminologies vary substantially across disciplines and ecosystems, hindering comparison and accumulation of knowledge in the field."
  - [section] "We define all outcomes on the basis of four key components of human–recommender interactions: the user, the item, the interaction, and the recommendation."
  - [corpus] Weak: Corpus contains survey and taxonomy papers but none directly comparing fragmented terminologies across ecosystems.

### Mechanism 2
- Claim: Categorising methodologies into empirical vs simulation, and controlled vs observational, clarifies causal inference capabilities across ecosystems.
- Mechanism: This two-level categorisation distinguishes studies based on data source (real vs synthetic) and experimental design (causal vs associative), making it easier to assess the strength of evidence for each outcome.
- Core assumption: The distinction between controlled and observational studies is meaningful for causal inference and that this distinction is consistently applied across ecosystems.
- Evidence anchors:
  - [abstract] "We categorise the methodologies employed to assess the outcomes of user-recommender interactions – empirical, simulation, observational, and controlled studies."
  - [section] "Controlled studies enable researchers to control for various factors and conditions, allowing the isolation of the effect produced by a specific intervening variable."
  - [corpus] Weak: Corpus lacks direct comparisons of methodological rigor across ecosystems; most papers focus on methodology within single ecosystems.

### Mechanism 3
- Claim: Distinguishing individual, item, and ecosystem levels of analysis reveals trade-offs between diversity at different scales.
- Mechanism: By separating outcomes into three analytical levels, the survey uncovers patterns where individual-level diversity gains may coincide with ecosystem-level diversity losses, a nuance missed when using aggregate measures alone.
- Core assumption: Outcomes can be meaningfully classified at three distinct levels and that these levels capture different causal dynamics.
- Evidence anchors:
  - [abstract] "We categorise the outcomes at the individual, item, and ecosystem levels."
  - [section] "For example, we show that an increase in individual diversity can coexist with a decrease in ecosystem diversity."
  - [corpus] Weak: Corpus lacks explicit examples of trade-offs between individual and ecosystem diversity across multiple ecosystems.

## Foundational Learning

- Concept: Feedback loop in recommender systems
  - Why needed here: The survey repeatedly references the "human-AI feedback loop" as a core dynamic; understanding it is essential for grasping why outcomes like concentration and homogenisation emerge.
  - Quick check question: What are the four components of the human-AI feedback loop as defined in the survey?

- Concept: Causal inference and SUTVA (Stable Unit Treatment Value Assumption)
  - Why needed here: The survey emphasises the difficulty of controlled experiments in social media and urban mapping due to SUTVA violations; engineers need to understand this to design valid experiments.
  - Quick check question: Why do interactions between users in social media and urban mapping ecosystems violate SUTVA?

- Concept: Diversity metrics (Shannon entropy, Gini index, coverage)
  - Why needed here: The survey harmonises multiple diversity measures across ecosystems; engineers must know how these metrics differ and when to use each.
  - Quick check question: How does Shannon entropy differ from the Gini index in measuring diversity?

## Architecture Onboarding

- Component map:
  Data ingestion -> Taxonomy engine -> Methodology categoriser -> Measurement harmoniser -> Analysis dashboard

- Critical path:
  1. Ingest study metadata and full-text
  2. Parse outcome definitions and measures
  3. Apply taxonomy to classify each outcome
  4. Categorise methodology
  5. Harmonise measures using unified notation
  6. Generate cross-ecosystem comparisons and visualisations

- Design tradeoffs:
  - Granularity vs. usability: More granular outcome subcategories improve precision but increase classification complexity
  - Manual vs. automated coding: Manual coding ensures accuracy but limits scalability; automated NLP tagging speeds processing but risks misclassification
  - Ecosystem specificity vs. generality: Ecosystem-specific measures capture nuance but hinder cross-ecosystem comparison

- Failure signatures:
  - Inconsistent outcome classification across coders → taxonomy ambiguity
  - Missing or ambiguous measure definitions → harmonisation errors
  - Overlaps between analytical levels → trade-off insights become unreliable

- First 3 experiments:
  1. Validate taxonomy by having two independent coders classify a sample of 10 papers; measure inter-rater reliability
  2. Test measure harmonisation by mapping Gini index and Shannon entropy formulas to a common notation; verify consistency
  3. Simulate methodology categorisation on a mixed set of empirical and simulation studies; check for correct controlled/observational assignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific technological components (e.g., collaborative filtering, deep learning architectures) within real-world recommenders contribute to concentration, discrimination, and homogenisation outcomes?
- Basis in paper: [explicit] The paper highlights that while simulation studies often use simplified models (e.g., collaborative filtering), real platforms employ sophisticated blends of multiple paradigms. It suggests assessing how different components shape observed outcomes.
- Why unresolved: Platform algorithms are typically undisclosed, making it difficult to isolate the causal impact of individual components. Existing studies often cannot differentiate between algorithmic effects and user behaviour.
- What evidence would resolve it: Controlled experiments with platform access that systematically vary individual algorithmic components while holding others constant, or transparent platform audits revealing algorithmic structure.

### Open Question 2
- Question: Under which conditions do recommender systems contribute to radicalisation of users' opinions, and how can this be empirically distinguished from radicalisation driven by user preferences alone?
- Basis in paper: [explicit] The paper notes that empirical studies on YouTube find little evidence of recommender-driven radicalisation, while simulation studies show consistent polarisation. It questions whether outcomes are caused by algorithms or user behaviour.
- Why unresolved: Observational studies cannot establish causality due to confounding factors. Controlled experiments are rare due to platform access limitations and the complexity of preventing cross-group interactions.
- What evidence would resolve it: Large-scale controlled experiments that isolate algorithmic influence from user choice, or natural experiments where recommender parameters change unexpectedly.

### Open Question 3
- Question: What adoption rate thresholds exist for recommender systems in urban mapping that trigger concentration of traffic and emissions, and how do these thresholds vary across cities and services?
- Basis in paper: [explicit] The paper identifies that navigation services may trigger traffic concentration and increased emissions only when adoption rates exceed specific thresholds, but this has been primarily studied in urban mapping.
- Why unresolved: Most studies focus on individual outcomes or aggregate effects without examining critical adoption thresholds. Limited data availability and ethical constraints hinder systematic investigation.
- What evidence would resolve it: Empirical studies across multiple cities measuring traffic patterns at varying adoption rates, or controlled simulations that systematically vary adoption levels while monitoring system-wide impacts.

## Limitations
- The taxonomy may not capture all emergent outcomes or methodological nuances across rapidly evolving ecosystems
- Consensus-based coding is subject to inter-reviewer variability and potential blind spots in less-studied domains
- Reliance on published literature may underrepresent negative or null results, introducing publication bias
- The survey does not empirically validate its taxonomy against real-world data

## Confidence
- High confidence: The classification of outcomes (concentration, diversity, discrimination, etc.) and their analytical levels (individual, item, ecosystem) is well-supported by the literature and consistent with established definitions
- Medium confidence: The methodology categorisation (empirical vs simulation, controlled vs observational) is useful but may not fully capture the complexity of causal inference in social systems
- Low confidence: The claim that the unified taxonomy will significantly improve cross-ecosystem synthesis depends on its adoption by the broader research community, which is not guaranteed

## Next Checks
1. Replicate the taxonomy classification on a new set of 20 papers not used in the original survey; measure inter-rater reliability and coverage of all ecosystems and methodologies
2. Conduct a meta-analysis of the effect sizes for concentration and diversity outcomes across ecosystems to test the claim that individual diversity gains coincide with ecosystem diversity losses
3. Implement the taxonomy in a live recommender system (e.g., a news or music recommender) and track whether it improves outcome measurement consistency and causal inference