---
ver: rpa2
title: 'KG-CTG: Citation Generation through Knowledge Graph-guided Large Language
  Models'
arxiv_id: '2404.09763'
source_url: https://arxiv.org/abs/2404.09763
tags:
- text
- citation
- knowledge
- generation
- papers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents KG-CTG, a method for generating citation text
  using Large Language Models (LLMs) guided by knowledge graphs. The approach fine-tunes
  LLMs (LLaMA, Alpaca, Vicuna) on a dataset of computer science research papers from
  S2ORC, and incorporates knowledge graph relations derived from the source and target
  paper abstracts.
---

# KG-CTG: Citation Generation through Knowledge Graph-guided Large Language Models

## Quick Facts
- arXiv ID: 2404.09763
- Source URL: https://arxiv.org/abs/2404.09763
- Reference count: 32
- Alpaca with knowledge graphs achieves 14.02 METEOR score, improving over Alpaca without knowledge graphs by 33.14%

## Executive Summary
KG-CTG introduces a novel approach for citation text generation by incorporating knowledge graphs into Large Language Model (LLM) prompts. The method fine-tunes LLaMA, Alpaca, and Vicuna models on a computer science subset of the S2ORC dataset, then enhances them with knowledge graph relations extracted from paper abstracts using PL-Marker. The approach demonstrates significant performance improvements, with Alpaca achieving the best results when knowledge graphs are included in the generation process.

## Method Summary
The KG-CTG approach fine-tunes three LLMs (LLaMA, Alpaca, Vicuna) on 79,588 training samples from the computer science subset of S2ORC. Knowledge graphs are constructed using PL-Marker to capture entity relationships from source and target paper abstracts. These knowledge graphs are incorporated into LLM prompts during fine-tuning using QLora optimization, which employs Low Rank Adapters with 4-bit quantization to minimize GPU memory usage. The models are evaluated using standard text generation metrics including METEOR and ROUGE scores.

## Key Results
- Alpaca with knowledge graphs achieves 14.02 METEOR score, 12.63 Rouge-1, 1.54 Rouge-2, and 10.71 Rouge-L
- Incorporating knowledge graphs improves Alpaca's performance by 33.14% in METEOR and 36.98% in Rouge-1
- Knowledge graph-guided models consistently outperform models without knowledge graph incorporation across all three tested LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge graphs improve citation text quality by enriching contextual understanding between source and target papers
- Mechanism: PL-Marker constructs a knowledge graph capturing entities and their relationships from abstracts of source and target papers. This graph is incorporated into LLM prompts, providing structured relational data that guides more contextually relevant citations
- Core assumption: Structured relationships in the knowledge graph provide additional semantic cues that LLMs can leverage for better citation coherence
- Evidence anchors: [abstract] "incorporating the knowledge graph relations of the papers in the prompt for the LLM to better learn the relationship between the papers"; [section 3.2] "The knowledge graph provides structured information about the papers, including key concepts, entities, and their relationships"

### Mechanism 2
- Claim: Domain-specific fine-tuning on computer science papers improves citation generation performance
- Mechanism: Models are fine-tuned on 79,588 computer science samples from S2ORC, learning patterns and language specific to scientific citation contexts
- Core assumption: Domain-specific fine-tuning provides better contextual grounding than general-purpose LLMs for scientific citation tasks
- Evidence anchors: [abstract] "To assess how well our model is performing, we have used a subset of standard S2ORC dataset, which only consists of computer science academic research papers in the English Language"

### Mechanism 3
- Claim: QLora optimization enables efficient fine-tuning of large LLMs with limited GPU resources
- Mechanism: QLora uses Low Rank Adapters (LoRA) to backpropagate gradients through frozen, 4-bit quantized pretrained models, reducing memory usage while maintaining performance
- Core assumption: LoRA adapters can effectively capture task-specific knowledge without full model fine-tuning
- Evidence anchors: [section 5.1] "For fine-tuning the Large Language Models (LLMs), we employed QLora [13] to minimize GPU usage. By back propagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA), QLora is an effective method for maximizing memory utilization"

## Foundational Learning

- Concept: Knowledge Graph Construction
  - Why needed here: Provides structured relational information between source and target papers that guides LLM generation
  - Quick check question: What tool is used to construct knowledge graphs from paper abstracts in this approach?

- Concept: Large Language Model Fine-Tuning
  - Why needed here: Adapts general-purpose LLMs to the specific task of scientific citation text generation
  - Quick check question: How many training samples were used to fine-tune the models?

- Concept: Evaluation Metrics for Text Generation
  - Why needed here: Quantifies the quality of generated citations using standard NLP metrics
  - Quick check question: Which metric measures the longest common subsequence between generated and reference text?

## Architecture Onboarding

- Component map: S2ORC dataset → preprocessing → abstract + introduction + conclusion extraction → PL-Marker → knowledge graph construction → prompt formation → LLaMA/Alpaca/Vicuna → QLora fine-tuning → evaluation (METEOR, ROUGE-N, ROUGE-L)
- Critical path: Data preprocessing → Knowledge graph construction → Prompt formation → LLM inference → Evaluation
- Design tradeoffs:
  - Using LoRA vs full fine-tuning: Memory efficiency vs potential performance ceiling
  - Including knowledge graphs vs text-only: Better context vs increased prompt complexity
  - Computer science domain only vs broader coverage: Specialized performance vs generalizability
- Failure signatures:
  - Degraded metrics when knowledge graphs are removed
  - Memory errors during fine-tuning (QLora issues)
  - Generated citations that don't reference target paper content
- First 3 experiments:
  1. Compare Alpaca with and without knowledge graphs on a small validation set
  2. Test different LoRA rank values to find memory-performance balance
  3. Evaluate Vicuna vs Alpaca with identical fine-tuning settings

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Performance gains measured only within computer science domain, raising generalizability concerns to other fields
- Knowledge graph construction relies entirely on PL-Marker's entity and relation extraction capabilities without validation of semantic capture quality
- While 33.14% METEOR improvement appears substantial, absolute numbers (14.02 vs baseline) remain relatively low, suggesting significant room for improvement

## Confidence
- High confidence: The methodology for incorporating knowledge graphs into prompts is clearly specified and follows established NLP practices
- Medium confidence: The reported performance improvements are likely real but may not generalize beyond the tested domain
- Low confidence: The claim that knowledge graphs provide "substantial" improvements is not well-supported by absolute metric values

## Next Checks
1. Test the trained models on citation generation tasks from biomedical or social science domains to assess domain transfer capability
2. Conduct a human evaluation study comparing citation quality with and without knowledge graph incorporation, focusing on factual accuracy and relevance
3. Experiment with different knowledge graph construction tools (beyond PL-Marker) to determine if the improvements are tool-specific or generalizable to the knowledge graph approach