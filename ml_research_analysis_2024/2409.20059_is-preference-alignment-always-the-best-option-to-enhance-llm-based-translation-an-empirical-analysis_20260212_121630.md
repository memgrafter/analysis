---
ver: rpa2
title: Is Preference Alignment Always the Best Option to Enhance LLM-Based Translation?
  An Empirical Analysis
arxiv_id: '2409.20059'
source_url: https://arxiv.org/abs/2409.20059
tags:
- chosen
- neural
- metrics
- alignment
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether preference alignment techniques
  consistently improve LLM-based machine translation quality. Through extensive experiments
  comparing Contrastive Preference Optimization (CPO) to Supervised Fine-Tuning (SFT)
  on high-quality data, the research finds that while CPO consistently outperforms
  SFT on the alignment metric with neural evaluation metrics, it leads to instability
  across downstream evaluation metrics, particularly between neural and lexical ones.
---

# Is Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis

## Quick Facts
- arXiv ID: 2409.20059
- Source URL: https://arxiv.org/abs/2409.20059
- Reference count: 26
- Key outcome: Preference alignment techniques like CPO don't consistently improve LLM translation quality and can introduce metric instability

## Executive Summary
This study investigates whether preference alignment techniques consistently improve LLM-based machine translation quality. Through extensive experiments comparing Contrastive Preference Optimization (CPO) to Supervised Fine-Tuning (SFT), the research finds that while CPO consistently outperforms SFT on neural alignment metrics, it leads to instability across downstream evaluation metrics, particularly between neural and lexical ones. The study also demonstrates that using the base model's own translations for candidate generation achieves performance comparable to using multiple external systems while ensuring better metric consistency.

## Method Summary
The study fine-tunes the ALMA-13B-LoRA model using both CPO and SFT objectives on preference datasets derived from multiple candidate systems (base model, GPT-4, and references) and mono-system approaches. The CPO method incorporates both preference and SFT terms, while SFT uses only the traditional supervised objective. Preference data is generated by top-p sampling K=50 candidates per source, scoring with quality estimators, and selecting chosen/rejected pairs. The model is evaluated on WMT'22 and WMT'23 test sets using neural metrics (xCOMET-QE, CometKiwi) and lexical metric (chrF), with additional metrics in appendix.

## Key Results
- CPO consistently outperforms SFT on the alignment metric (xCOMET-QE) across language pairs
- CPO alignment on neural metrics negatively impacts lexical metrics for out-of-English translations
- Mono-system preference data (using only base model outputs) achieves comparable performance to multi-system approaches while improving metric consistency
- Optimal mono-system performance occurs when rejected options average around 90% and chosen options around 105% of base model quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preference-based alignment using CPO outperforms SFT on neural metrics due to the inclusion of a rejection term that better captures quality gradients.
- Mechanism: CPO combines a preference term with an SFT term, optimizing both the chosen translation and the gap between chosen and rejected candidates, thereby improving the model's ability to discriminate between high and low-quality translations.
- Core assumption: The rejection option provides meaningful signal for gradient updates that SFT alone cannot capture.
- Evidence anchors:
  - [abstract]: "CPO consistently outperforms Supervised Fine-Tuning (SFT) on high-quality data with regard to the alignment metric"
  - [section 4.1]: "The inclusion of the reject option seems to offer additional benefits over the traditional SFT objective in this context"
  - [corpus]: Weak - no direct corpus evidence linking rejection term to improved discrimination

### Mechanism 2
- Claim: Multi-system preference data introduces metric instability due to the inclusion of translations from different system distributions.
- Mechanism: When combining translations from the base model, GPT-4, and references, the model learns preferences across heterogeneous distributions, leading to inconsistent behavior when evaluated with different metrics.
- Core assumption: Translations from different systems have distinct statistical properties that conflict when used for joint preference learning.
- Evidence anchors:
  - [section 4.2]: "removing GPT-4 has the strongest negative impact on downstream xCOMET-QE" and "candidate system choice affects alignment effectiveness and downstream metric consistency"
  - [section 4.1]: "aligning on neural metrics negatively impacts lexical metrics" and "candidate system choice affects alignment effectiveness"
  - [corpus]: Weak - no direct corpus evidence showing distribution differences between GPT-4 and base model outputs

### Mechanism 3
- Claim: Mono-system preference data allows for controlled optimization of chosen/rejected quality gaps, leading to better downstream consistency.
- Mechanism: By using only the base model's outputs for preference data, the model can be fine-tuned with carefully controlled quality differences between chosen and rejected candidates, avoiding the distribution mismatch issues of multi-system approaches.
- Core assumption: The base model's own translations can provide sufficient diversity in quality levels to enable effective preference learning.
- Evidence anchors:
  - [section 5.1]: "using candidate translations all originating from the same system distribution, specifically the base model, can be an effective strategy for gaining more control over preference-based fine-tuning"
  - [section 5.2]: "optimal test performance was obtained with rejected options average around 90% (∆ = −10%) of the base model's quality, and chosen options averaging around 105% (∆ = +5%)"
  - [corpus]: Weak - no direct corpus evidence showing base model can generate sufficient quality diversity

## Foundational Learning

- Concept: Quality-aware decoding strategies (MBR, N-best reranking)
  - Why needed here: Understanding the relationship between quality-informed decoding and quality-informed fine-tuning is crucial for contextualizing the paper's approach
  - Quick check question: What is the key difference between quality-aware decoding and quality-aware fine-tuning?

- Concept: Preference optimization (DPO, CPO)
  - Why needed here: The paper's main contribution relies on understanding how preference optimization techniques differ from traditional fine-tuning approaches
  - Quick check question: How does CPO differ from DPO in terms of the learning objective?

- Concept: Neural vs lexical evaluation metrics
  - Why needed here: The paper's findings about metric inconsistency depend on understanding the different properties of neural and lexical metrics
  - Quick check question: Why might neural metrics show different sensitivity to fine-tuning than lexical metrics?

## Architecture Onboarding

- Component map: Preference data generator -> Alignment model (CPO/SFT) -> Evaluation pipeline -> Quality estimator
- Critical path:
  1. Generate candidate translations (K=50 top-p sampled)
  2. Score candidates with quality estimator
  3. Select chosen/rejected pairs based on metric
  4. Train alignment model with CPO or SFT
  5. Evaluate on held-out test data
- Design tradeoffs:
  - Multi-system provides higher quality candidates but introduces distribution mismatch
  - Mono-system provides better control but may lack diversity
  - CPO provides better optimization but is more sensitive to preference settings
  - SFT is more stable but may not capture preference signals as effectively
- Failure signatures:
  - Poor downstream performance despite improved alignment metric
  - High variance in results across different language pairs
  - Degradation of lexical metrics when optimizing neural metrics
  - Sensitivity to choice of candidate systems
- First 3 experiments:
  1. Compare CPO vs SFT on mono-system data with controlled chosen/rejected quality gaps
  2. Test sensitivity to number of candidate translations (K parameter)
  3. Evaluate impact of different quality estimators on downstream consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties of the chosen and rejected translation qualities maximize downstream performance when using mono-system alignment?
- Basis in paper: [explicit] The paper states that optimal performance was obtained with rejected options averaging around 90% of the base model's quality and chosen options averaging around 105% of the base model's quality, but does not specify exact quality distributions or thresholds.
- Why unresolved: The paper only provides approximate percentages for optimal quality ranges and does not explore the full spectrum of quality distributions or their effects on downstream performance.
- What evidence would resolve it: Systematic experiments varying the quality distributions of chosen and rejected translations across the full spectrum, with detailed analysis of their impact on downstream evaluation metrics.

### Open Question 2
- Question: How do different alignment metrics (neural vs lexical) interact during training and evaluation, and why do they sometimes show contradictory results?
- Basis in paper: [explicit] The paper observes that CPO alignment on neural metrics negatively impacts lexical metrics for out-of-English translations, while SFT does not produce such effects, but does not explain the underlying mechanism.
- Why unresolved: The paper identifies the phenomenon but does not investigate the root causes of the metric inconsistencies or the relationship between neural and lexical metrics during training.
- What evidence would resolve it: Detailed analysis of metric behavior during training, including correlation studies and ablation experiments to understand the relationship between neural and lexical metrics.

### Open Question 3
- Question: How do different candidate system combinations affect the stability and robustness of preference-based alignment?
- Basis in paper: [explicit] The paper shows that removing systems from the candidate pool significantly affects performance, particularly for CPO, but does not systematically explore all possible system combinations or their effects on robustness.
- Why unresolved: The paper only examines the impact of removing individual systems from a three-system pool and does not investigate the effects of different system combinations or their impact on alignment robustness.
- What evidence would resolve it: Comprehensive experiments testing various combinations of candidate systems and analyzing their impact on alignment stability and downstream performance consistency.

## Limitations

- Experimental scope limited to six languages and ten language directions with English-centric evaluation
- Preference data generation relies on a single base model (ALMA-13B) and quality estimator (xCOMET-QE)
- Mono-system approach requires careful calibration of chosen/rejected quality gaps that may not generalize easily
- Results may not extend to specialized domains or different model architectures

## Confidence

- **High confidence**: The finding that CPO outperforms SFT on the alignment metric (xCOMET-QE) is well-supported by experimental results across multiple language pairs and conditions
- **Medium confidence**: The claim that mono-system preference data can achieve comparable performance to multi-system approaches while improving metric consistency is supported but requires careful parameter tuning
- **Medium confidence**: The conclusion that preference alignment is not always beneficial for translation quality is well-supported, though the specific conditions under which it fails need further exploration

## Next Checks

1. **Domain Transfer Test**: Evaluate whether the mono-system approach maintains metric consistency when applied to specialized domains (e.g., legal, medical, or technical translation) that may have different quality distributions than the FLORES-200-based dataset used in the experiments.

2. **Model Architecture Generalization**: Test whether the observed benefits of CPO over SFT and the advantages of mono-system approaches extend to other LLM architectures beyond ALMA-13B, such as different parameter sizes or training methodologies.

3. **Quality Estimator Sensitivity**: Investigate whether using alternative quality estimators (beyond xCOMET-QE) affects the stability and effectiveness of preference alignment, particularly for the mono-system approach that relies heavily on accurate quality ranking.