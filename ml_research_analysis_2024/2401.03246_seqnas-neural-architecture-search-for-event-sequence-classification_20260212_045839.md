---
ver: rpa2
title: 'SeqNAS: Neural Architecture Search for Event Sequence Classification'
arxiv_id: '2401.03246'
source_url: https://arxiv.org/abs/2401.03246
tags:
- search
- architecture
- neural
- architectures
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SeqNAS, a novel neural architecture search
  (NAS) method specifically designed for event sequence classification. The authors
  develop a new search space that incorporates commonly used building blocks like
  multi-head self-attention, convolutions, and recurrent cells, tailored for handling
  event sequence data.
---

# SeqNAS: Neural Architecture Search for Event Sequence Classification

## Quick Facts
- arXiv ID: 2401.03246
- Source URL: https://arxiv.org/abs/2401.03246
- Reference count: 40
- Introduces a novel NAS method for event sequence classification that outperforms state-of-the-art methods

## Executive Summary
SeqNAS introduces a specialized neural architecture search method designed specifically for event sequence classification tasks. The authors develop a tailored search space incorporating multi-head self-attention, convolutions, and recurrent cells, and employ sequential Bayesian Optimization with ensemble teacher models for knowledge distillation. Experiments on six diverse datasets demonstrate superior performance compared to existing NAS methods and established sequence classification architectures.

## Method Summary
SeqNAS employs sequential Bayesian Optimization to search through a carefully designed space of architectures suitable for event sequence data. The search space includes commonly used building blocks like multi-head self-attention, convolutional layers, and recurrent cells. The method leverages previously trained models as an ensemble of teachers to enhance knowledge distillation during the search process. The approach is evaluated across six diverse event sequence classification datasets, with performance comparisons against both state-of-the-art NAS methods and popular hand-designed architectures.

## Key Results
- SeqNAS outperforms state-of-the-art NAS methods on six event sequence classification datasets
- The proposed method achieves better accuracy than popular hand-designed architectures for sequence classification
- Authors release NAS-Bench Event Sequences, a benchmark dataset with 3200 trained architectures and their scores

## Why This Works (Mechanism)
The effectiveness of SeqNAS stems from its specialized search space design that incorporates building blocks particularly suited for event sequence data. The sequential Bayesian Optimization framework efficiently navigates this space while the ensemble teacher approach improves knowledge transfer during architecture evaluation. This combination allows the method to discover architectures that better capture temporal dependencies and event patterns specific to sequence classification tasks.

## Foundational Learning
- Neural Architecture Search: Automated method for discovering optimal neural network architectures
  - Why needed: Manual architecture design is time-consuming and requires expert knowledge
  - Quick check: Compare search time and final performance against random search baseline
- Sequential Bayesian Optimization: Sequential model-based optimization technique
  - Why needed: Efficiently explores architecture space with limited computational budget
  - Quick check: Evaluate convergence speed compared to other optimization methods
- Knowledge Distillation: Technique for transferring knowledge from larger models to smaller ones
  - Why needed: Improves training efficiency and performance of searched architectures
  - Quick check: Measure performance difference with and without ensemble teachers

## Architecture Onboarding

**Component Map**
SeqNAS follows the path: Search Space Design -> Sequential Bayesian Optimization -> Knowledge Distillation -> Performance Evaluation

**Critical Path**
The critical path involves defining the search space, performing architecture search via Bayesian optimization, training candidate architectures with knowledge distillation, and evaluating their performance on validation data.

**Design Tradeoffs**
The authors balance search space expressiveness against computational efficiency, choosing building blocks that are both powerful and computationally tractable. The ensemble teacher approach trades additional computation for improved knowledge transfer.

**Failure Signatures**
Potential failure modes include getting stuck in local optima during search, overfitting to the training data within the search space, or selecting architectures that perform well on validation data but generalize poorly to new event sequences.

**3 First Experiments**
1. Run SeqNAS with a reduced search space to establish baseline performance
2. Compare sequential Bayesian Optimization against random search in the same space
3. Evaluate the impact of ensemble teacher size on final architecture performance

## Open Questions the Paper Calls Out
None

## Limitations
- Search space design may not capture all potentially useful architectural patterns for event sequence data
- Effectiveness of ensemble teacher approach lacks theoretical justification
- Computational cost of running 3200 architectures for the benchmark dataset is substantial

## Confidence

High confidence: Core methodology is well-defined, experimental setup follows standard practices, benchmark dataset release is concrete and verifiable, comparison methodology is sound

Medium confidence: Performance claims are supported by experiments but statistical significance across all comparisons is not explicitly reported, six datasets provide diversity but may not represent full breadth of tasks

Low confidence: Long-term impact of benchmark dataset cannot be assessed at this stage, generalizability to different event sequence domains remains unknown

## Next Checks

1. Conduct ablation studies to isolate contributions of individual components (search space design, sequential Bayesian Optimization, ensemble teacher approach)

2. Test SeqNAS on additional event sequence datasets with different characteristics (longer/shorter sequences, higher dimensionality, different event distributions)

3. Evaluate computational efficiency compared to other NAS methods in terms of both search time and resource utilization