---
ver: rpa2
title: Inroads to a Structured Data Natural Language Bijection and the role of LLM
  annotation
arxiv_id: '2401.07190'
source_url: https://arxiv.org/abs/2401.07190
tags:
- work
- language
- training
- task
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores using sequence-to-sequence transformers for\
  \ structured data \u2194 natural language bijection. The author fine-tunes t5 models\
  \ on the WebNLG dataset for two tasks: data-to-sentence (d2s) and sentence-to-data\
  \ (s2d), including a multi-task variant."
---

# Inroads to a Structured Data Natural Language Bijection and the role of LLM annotation

## Quick Facts
- arXiv ID: 2401.07190
- Source URL: https://arxiv.org/abs/2401.07190
- Authors: Blake Vente
- Reference count: 40
- One-line primary result: Multi-task training improves structured data ↔ natural language performance, but LLM-annotated synthetic data does not

## Executive Summary
This paper explores using sequence-to-sequence transformers for structured data ↔ natural language bijection, specifically fine-tuning t5 models on the WebNLG dataset for data-to-sentence and sentence-to-data tasks. The multi-task variant achieves superior performance on the data-to-sentence task (F1 0.771) compared to single-task specialists (F1 0.692), suggesting that cross-task knowledge generalization improves results. However, adding ~4500 LLM-annotated records from WikiBio does not substantially change automatic metric performance, possibly due to model size limitations or distributional differences in the corpora. The paper also documents a "Palatul" glitch token indicating hallucination issues that require further addressing.

## Method Summary
The study fine-tunes t5-small and t5-base models on the WebNLG bidirectional dataset using both single-task and multi-task objectives with control prompts. Models are trained for 5 epochs with learning rate 2e-4 and batch sizes of 64 (t5-small) or 32 (t5-base), with interlaced training for multi-task variants. An additional experiment incorporates ~4500 LLM-annotated synthetic records from the WikiBio corpus. Evaluation uses BLEU, BERTScore, RougeL, F1 (semantic parsing), and Edit Distance metrics. Data preprocessing involves cleaning Unicode/ASCII and serializing RDF triples with vertical bars and semicolons.

## Key Results
- Multi-task t5-small outperforms single-task specialist t5-small with F1 of 0.771 vs 0.692 on data-to-sentence task
- Adding ~4500 LLM-annotated records does not substantially change automatic metric performance compared to same model without synthetic data
- The "Palatul" glitch token indicates hallucination issues in the model generation
- t5-base multi-task underperforms t5-base single-task specialist, suggesting optimal ratio between training set size, fine-tuning size, and model size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task training improves performance by reusing vocabulary across tasks.
- Mechanism: When training on both data-to-sentence and sentence-to-data tasks, the model learns shared vocabulary that is re-used when computing labels, leading to improved F1 on the data-to-sentence task.
- Core assumption: The vocabulary learned from one task transfers effectively to the other task.
- Evidence anchors:
  - [abstract] "This further suggests that even with the same network, 're-using' the same data in a different way may lead to higher performance in some metrics."
  - [section] "This would explain why the converse isn't true: RDF label vocabulary is a subset of the text vocabulary."
- Break condition: If the vocabulary overlap between tasks is minimal or the tasks require completely different encoding schemes.

### Mechanism 2
- Claim: Synthetic data augmentation does not improve performance due to model size limitations.
- Mechanism: The t5-small model lacks sufficient learning capacity to effectively incorporate the additional ~4500 LLM-annotated records, leading to no substantial change in automatic metric performance.
- Core assumption: The model size is a bottleneck for learning from the augmented data.
- Evidence anchors:
  - [abstract] "This may be due to a learning capacity bottleneck on account of model size, and decreases observed may be due to distributional differences in the corpora."
  - [section] "The augmented variant performed slightly worse in all tasks. Perhaps this was due to not enough learning capacity in the t5-small base model."
- Break condition: If the model size is increased or the synthetic data is more closely aligned with the training corpus distribution.

### Mechanism 3
- Claim: The "Palatul" glitch token indicates hallucination issues in the model.
- Mechanism: The model falls into repetitive generation cycles, producing the same word "Palatul" repeatedly, even with parameters set to prevent such behavior.
- Core assumption: The model's decoding mechanism can get stuck in loops producing repetitive outputs.
- Evidence anchors:
  - [section] "Curiously, it was isolated to a particular word Palatul, Romainan for 'palace'. This behavior diminished but was present in some of the trained models nonetheless."
  - [section] "The 'Palatul' issue shows that the 'Hallucination problem' still requires addressing."
- Break condition: If the model's architecture or training process is modified to prevent such repetitive generation.

## Foundational Learning

- Concept: Sequence-to-sequence transformers
  - Why needed here: The paper uses t5 models, which are sequence-to-sequence transformers, for the data-to-sentence and sentence-to-data tasks.
  - Quick check question: What are the key components of a sequence-to-sequence transformer model?

- Concept: Fine-tuning pre-trained models
  - Why needed here: The paper fine-tunes pre-trained t5 models on the WebNLG dataset for the specific tasks.
  - Quick check question: What is the difference between training a model from scratch and fine-tuning a pre-trained model?

- Concept: Multi-task learning
  - Why needed here: The paper explores the effects of training a single model on both data-to-sentence and sentence-to-data tasks.
  - Quick check question: How does multi-task learning differ from single-task learning in terms of model architecture and training process?

## Architecture Onboarding

- Component map: t5-small/t5-base models -> WebNLG dataset preprocessing -> fine-tuning (single/multi-task) -> evaluation (BLEU, BERTScore, RougeL, F1, Edit Distance)
- Critical path: Data preprocessing -> model training (5 epochs) -> evaluation with multiple metrics
- Design tradeoffs: Smaller t5-small with multi-task training vs larger t5-base with single-task training, balancing model capacity against task complexity
- Failure signatures: "Palatul" repetitive generation cycles, lower performance on some metrics with multi-task training, no improvement with synthetic data augmentation
- First 3 experiments:
  1. Train t5-small on data-to-sentence task only and evaluate performance.
  2. Train t5-small on both data-to-sentence and sentence-to-data tasks (multi-task) and compare performance.
  3. Train t5-small with multi-task objective and incorporate LLM-annotated WikiBio data, then evaluate performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does multi-task training consistently improve performance across different model sizes for structured data ↔ natural language bijection tasks?
- Basis in paper: [explicit] The paper shows t5-small multi-task outperforms specialist, but t5-base multi-task underperforms specialist
- Why unresolved: The results show contradictory patterns between model sizes, suggesting an optimal ratio between training set size, fine-tuning size, and model size
- What evidence would resolve it: Systematic experiments varying model sizes, training set sizes, and multi-task ratios to identify conditions where multi-task training helps

### Open Question 2
- Question: Why does adding synthetic LLM-annotated data not improve performance?
- Basis in paper: [explicit] Adding ~4500 LLM-annotated WikiBio records did not change performance compared to same model without synthetic data
- Why unresolved: The paper suggests possible causes (model size bottleneck, distributional differences) but doesn't definitively explain the mechanism
- What evidence would resolve it: Controlled experiments varying model sizes and analyzing distributional differences between synthetic and original data

### Open Question 3
- Question: What causes the "repetition loops" or "hallucination" problem in the generated text?
- Basis in paper: [explicit] The paper documents repetitive generation cycles, particularly with the word "Palatul"
- Why unresolved: The paper observes the problem but doesn't identify root causes or effective solutions
- What evidence would resolve it: Analysis of when and why these loops occur, and testing interventions to prevent them

### Open Question 4
- Question: Would human evaluation provide different insights than automatic metrics for this task?
- Basis in paper: [inferred] The paper mentions automatic evaluation limitations and suggests future work with human evaluation
- Why unresolved: The paper relies entirely on automatic metrics (BLEU, BERTScore, RougeL, F1) which have known limitations
- What evidence would resolve it: Direct comparison between human and automatic evaluation results on the same outputs

## Limitations

- The paper relies on a single dataset (WebNLG) and model architecture (t5), which may not generalize to other structured data formats or larger language models
- The LLM-annotated synthetic data evaluation is limited, with only ~4500 additional records tested on the smaller t5-small model
- The "Palatul" hallucination issue lacks systematic analysis of its frequency, conditions that trigger it, or comprehensive solutions
- The paper does not provide statistical significance testing for performance differences between model variants

## Confidence

**High Confidence**: The observation that t5-small with multi-task training outperforms t5-base with single-task training on the d2s task (F1 0.771 vs 0.692) is well-supported by the experimental results and demonstrates a clear, reproducible finding.

**Medium Confidence**: The explanation that multi-task performance gains stem from vocabulary reuse across tasks is plausible given the evidence but lacks direct validation through ablation studies or controlled experiments that would isolate the vocabulary transfer mechanism.

**Low Confidence**: The claim that synthetic data augmentation failed due to model size limitations is speculative, as the paper only tested one model size (t5-small) and did not explore whether larger models or better-aligned synthetic data would yield different results.

## Next Checks

1. **Vocabulary Transfer Validation**: Design an experiment that systematically varies the vocabulary overlap between training tasks (e.g., using controlled datasets with different lexical distributions) to directly test whether the observed multi-task benefits are indeed driven by shared vocabulary reuse as claimed.

2. **Synthetic Data Scaling Study**: Replicate the synthetic data augmentation experiment using t5-base or larger models with substantially more LLM-generated examples (10K-50K records) and perform controlled studies varying the quality and distribution alignment of synthetic data to determine whether the negative results are truly model-capacity-limited or due to other factors.

3. **Hallucination Characterization Protocol**: Implement systematic monitoring during training to track the frequency and conditions triggering the "Palatul" repetition glitch, including metrics on repetition length, trigger patterns in input data, and effects of different decoding strategies, to establish whether this is an isolated incident or indicative of broader generation stability issues.