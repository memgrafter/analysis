---
ver: rpa2
title: LLMs for Extremely Low-Resource Finno-Ugric Languages
arxiv_id: '2410.18902'
source_url: https://arxiv.org/abs/2410.18902
tags:
- languages
- translation
- language
- data
- livonian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a comprehensive approach to developing large\
  \ language models (LLMs) for three extremely low-resource Finno-Ugric languages:\
  \ V\xF5ro, Livonian, and Komi. The authors cover the entire LLM development cycle,\
  \ from data collection and continued pre-training to instruction tuning and evaluation."
---

# LLMs for Extremely Low-Resource Finno-Ugric Languages

## Quick Facts
- arXiv ID: 2410.18902
- Source URL: https://arxiv.org/abs/2410.18902
- Reference count: 40
- Primary result: Instruction-tuned models outperform or match strong proprietary baselines on automatic benchmarks for Livonian and Komi, while showing comparable performance for Võro.

## Executive Summary
This paper presents a comprehensive approach to developing large language models for three extremely low-resource Finno-Ugric languages: Võro, Livonian, and Komi. The authors cover the entire LLM development cycle, from data collection and continued pre-training to instruction tuning and evaluation. Key contributions include creating multilingual base and instruction-tuned models, developing evaluation benchmarks (including a novel multi-turn conversational benchmark), and conducting human evaluation. The instruction-tuned models outperform or match strong proprietary baselines (GPT-3.5-turbo and GPT-4-turbo) on automatic benchmarks for Livonian and Komi, while showing comparable performance for Võro. Human evaluation reveals that the models significantly outperform proprietary ones in terms of naturalness across all three languages, with higher helpfulness for Komi.

## Method Summary
The method employs a two-stage continued pre-training approach using Llama-2 7B as the base model. Stage 1 continues pre-training on high-resource languages (Estonian, Finnish, English, Latvian, Russian), while Stage 2 focuses on the extremely low-resource Finno-Ugric languages. Instruction tuning is performed using translated Alpaca-style instructions, with an optional translation-tuning step using parallel data. The models are evaluated using automatic benchmarks (SIB-SMUGRI, BELEBELE-SMUGRI, FLORES-SMUGRI) and human evaluation on a novel multi-turn conversational benchmark.

## Key Results
- Instruction-tuned models match or outperform GPT-3.5-turbo and GPT-4-turbo on automatic benchmarks for Livonian and Komi
- Models show comparable performance to proprietary baselines for Võro across all automatic benchmarks
- Human evaluation reveals significantly higher naturalness scores compared to proprietary models for all three languages
- Komi models achieve higher helpfulness scores in human evaluation compared to GPT models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual transfer from related higher-resourced languages improves model performance on extremely low-resource target languages.
- Mechanism: Continued pre-training on Estonian and Finnish provides linguistic knowledge that transfers to Võro and Livonian through shared vocabulary, grammar, and orthography.
- Core assumption: The model can effectively leverage shared linguistic features across related languages during continued pre-training.
- Evidence anchors:
  - [abstract] "During pre-training and instruction-tuning, we make use of cross-lingual transfer from related higher-resourced languages and parallel translation data."
  - [section 3.1] "Stage 1: Learning supporting languages... continue pre-training Llama-2 7B on higher-resource languages Estonian, Finnish, English, Latvian, and Russian."
  - [section 5.1] "Stage 1 continued pre-training on high-resource supporting languages shows notable improvements in SIB-SMUGRI for Võro and Livonian compared to Llama-2-7B."

### Mechanism 2
- Claim: Translation-tuned LLMs can serve as viable alternatives to external translation systems for creating instruction data in extremely low-resource languages.
- Mechanism: Fine-tuning the base model on translation data enables it to translate English/Alpaca-style instructions into target languages, eliminating dependency on external MT systems.
- Core assumption: The translation capabilities learned during fine-tuning are sufficient to produce usable translations for instruction data.
- Evidence anchors:
  - [section 5.2] "Automatic metrics indicate that instructions translated using our translation-tuned LLM achieve results comparable to those produced by the external system Neurotõlge."

### Mechanism 3
- Claim: Small amounts of translation instruction data improve the model's ability to generate responses in the correct target language.
- Mechanism: Including 250 translation examples per direction during instruction-tuning teaches the model to recognize and respond in the appropriate language context.
- Core assumption: The model already has the underlying language and translation capabilities from base pre-training, and the instruction data provides the language-switching awareness.
- Evidence anchors:
  - [section 3.2] "Translation instructions... We augment the general instructions with translation task instructions for Võro, Livonian, and Komi, using 250 examples per direction."
  - [section 5.2] "Incorporating a small set of translation instructions... does not lead to clear and consistent improvements in the discriminative benchmarks... However, there is a notable improvement in the translation benchmark."

## Foundational Learning

- Concept: Continued pre-training vs. training from scratch
  - Why needed here: Continued pre-training leverages existing knowledge in a pre-trained model while adapting to new languages, requiring less data than training from scratch.
  - Quick check question: Why might continued pre-training be more effective than training from scratch for extremely low-resource languages?

- Concept: Curriculum learning through staged pre-training
  - Why needed here: Training on higher-resource languages first builds general linguistic capabilities before focusing on extremely low-resource languages.
  - Quick check question: What is the benefit of training on supporting languages before the target extremely low-resource languages?

- Concept: Instruction tuning for cross-lingual transfer
  - Why needed here: High-quality instructions in English/Finnish/Estonian can transfer knowledge to the target languages through fine-tuning.
  - Quick check question: How does instruction tuning with high-resource language instructions benefit low-resource language performance?

## Architecture Onboarding

- Component map: Base model (Llama-2 7B) → Stage 1 pre-training (supporting languages) → Stage 2 pre-training (target XLR languages) → Instruction tuning (with/without translation instructions) → Translation tuning (optional)
- Critical path: Base model → Stage 2 pre-training → Instruction tuning → Evaluation
- Design tradeoffs: Using external translation systems vs. translation-tuned LLM for instruction creation; including translation instructions vs. relying on base model capabilities
- Failure signatures: Low perplexity on target languages but poor benchmark performance suggests vocabulary coverage issues; poor naturalness in human evaluation suggests language-specific generation issues
- First 3 experiments:
  1. Evaluate perplexity on held-out validation set after each pre-training stage to ensure language learning
  2. Compare zero-shot instruction following performance on target languages with and without translation instructions
  3. Test translation quality from XLR languages to high-resource languages before and after translation tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal amount of parallel translation data to include during the pre-training stage for extremely low-resource languages?
- Basis in paper: [inferred] The paper mentions that Stage 2 + parallel data results in minimal improvements in benchmarks and perplexity, with the exception of translation tasks from the XLR languages showing larger gains. However, the authors conclude that the inclusion of parallel data has a limited impact or that their benchmarks are insufficiently sensitive to capture these effects.
- Why unresolved: The paper does not conduct a systematic study on the impact of varying amounts of parallel data during pre-training. They only test one configuration (up to 1% of the character budget).
- What evidence would resolve it: A comprehensive ablation study varying the amount of parallel data (e.g., 0%, 0.1%, 0.5%, 1%, 2%, 5%) during pre-training and measuring the impact on downstream tasks would provide insights into the optimal amount of parallel data.

### Open Question 2
- Question: How do different curriculum learning strategies for introducing XLR languages during pre-training affect model performance?
- Basis in paper: [inferred] The paper mentions using Unimax with N=4 for sampling Võro, Komi, and Livonian during Stage 2 pre-training, but does not explore alternative curriculum learning strategies or compare different N values systematically.
- Why unresolved: The paper only tests one curriculum learning approach (Unimax with N=4) and does not compare it to other strategies like proportional sampling, random sampling, or different N values.
- What evidence would resolve it: A systematic comparison of different curriculum learning strategies during pre-training, measuring their impact on perplexity and downstream task performance, would reveal the most effective approach.

### Open Question 3
- Question: What is the impact of instruction quality on the performance of instruction-tuned models for extremely low-resource languages?
- Basis in paper: [explicit] The paper mentions that instructions for XLR languages are machine-translated from Alpaca-style instructions, and that some of these instructions were of low quality, potentially affecting the overall performance and reliability of the fine-tuned models.
- Why unresolved: The paper does not conduct a systematic study on the impact of instruction quality on model performance. They only use machine-translated instructions without evaluating how the quality of these translations affects the model's ability to follow instructions in the target languages.
- What evidence would resolve it: A study comparing model performance using instructions of varying quality (e.g., human-translated instructions vs. machine-translated instructions with different BLEU scores) would reveal the impact of instruction quality on downstream task performance.

## Limitations

- Extreme data scarcity for target languages limits model development and evaluation quality
- Translation tuning mechanism lacks comprehensive validation through direct human evaluation of translation quality
- Evaluation benchmarks may not fully capture nuanced capabilities needed for real-world applications
- Human evaluation was limited to 10 prompts and may not represent full range of model capabilities

## Confidence

**High Confidence:**
- The methodology for creating instruction-tuned models for extremely low-resource languages is technically sound and reproducible
- Human evaluation results showing superior naturalness of the models compared to proprietary baselines are robust
- The multi-turn conversational benchmark is a novel and valuable contribution to the field

**Medium Confidence:**
- Automatic benchmark results showing comparable or better performance to proprietary models (particularly for Livonian and Komi)
- The effectiveness of translation instructions in improving target language generation
- Cross-lingual transfer effectiveness from supporting languages to target languages

**Low Confidence:**
- The claim that translation-tuned LLMs are viable alternatives to external MT systems for instruction creation (limited validation)
- The generalizability of results beyond the specific Finno-Ugric languages studied
- The long-term sustainability and scalability of this approach for other extremely low-resource language families

## Next Checks

1. **Direct Translation Quality Validation**: Conduct comprehensive human evaluation of translation quality from the translation-tuned Llama-SMUGRI-translate model on a diverse set of test sentences from each XLR language to high-resource languages, comparing against both the external MT system (Neurotõlge) and GPT-4-turbo. This should include fluency, adequacy, and consistency metrics with 50+ samples per language direction.

2. **Zero-Shot Generalization Test**: Evaluate the instruction-tuned models on out-of-distribution tasks and domains not seen during training (e.g., creative writing, technical instructions, cultural knowledge questions) to assess their true generalization capabilities beyond the benchmark tasks. This should include both automatic metrics and human evaluation.

3. **Cross-Lingual Transfer Analysis**: Conduct ablation studies by training models with different supporting language combinations (e.g., only Baltic languages, only Slavic languages, only Finno-Ugric languages) to quantify the specific contribution of each language family to target language performance. This would help validate the mechanism of cross-lingual transfer and identify optimal supporting language choices for future extremely low-resource language modeling efforts.