---
ver: rpa2
title: Curiosity & Entropy Driven Unsupervised RL in Multiple Environments
arxiv_id: '2401.04198'
source_url: https://arxiv.org/abs/2401.04198
tags:
- higher
- more
- environments
- exploration
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work improves upon unsupervised RL in multiple environments
  by introducing five modifications: entropy-based trajectory sampling, dynamic alpha,
  higher KL divergence threshold, curiosity-driven exploration, and alpha-percentile
  sampling on curiosity. The primary findings show that dynamic alpha and higher KL
  divergence threshold significantly improve performance over the baseline.'
---

# Curiosity & Entropy Driven Unsupervised RL in Multiple Environments

## Quick Facts
- arXiv ID: 2401.04198
- Source URL: https://arxiv.org/abs/2401.04198
- Authors: Shaurya Dewan; Anisha Jain; Zoe LaLena; Lifan Yu
- Reference count: 4
- Key outcome: This work improves upon unsupervised RL in multiple environments by introducing five modifications: entropy-based trajectory sampling, dynamic alpha, higher KL divergence threshold, curiosity-driven exploration, and alpha-percentile sampling on curiosity. The primary findings show that dynamic alpha and higher KL divergence threshold significantly improve performance over the baseline. Curiosity-driven exploration particularly enhances learning in high-dimensional environments like Ant by encouraging diverse experiences, while showing limited benefits in simpler environments like Grid World. The best results were achieved with a combination of curiosity, decreasing alpha, and higher KL divergence threshold, especially in more complex environments. Sampling trajectories using a PDF did not improve performance due to the small sample space. Overall, the study demonstrates promising directions for further research in unsupervised RL across multiple environments.

## Executive Summary
This work addresses the challenge of unsupervised reinforcement learning across multiple environments by introducing five key modifications to improve sample efficiency and exploration. The authors propose entropy-based trajectory sampling, dynamic adjustment of the alpha parameter, increased KL divergence threshold, curiosity-driven exploration, and alpha-percentile sampling specifically for curiosity-driven trajectories. These modifications aim to enhance the diversity of experiences and improve learning efficiency in both simple and complex environments.

The experimental results demonstrate that dynamic alpha and higher KL divergence threshold provide significant performance improvements over the baseline approach. Curiosity-driven exploration shows particular promise in high-dimensional environments like Ant, where it encourages diverse experiences that lead to better learning outcomes. However, the effectiveness of these modifications varies significantly across different environment complexities, with simpler environments like Grid World showing limited benefits from curiosity-driven exploration.

## Method Summary
The paper introduces five modifications to the unsupervised RL framework to improve learning efficiency across multiple environments. First, entropy-based trajectory sampling prioritizes trajectories with higher entropy, assuming they contain more diverse and informative experiences. Second, dynamic alpha adjusts the temperature parameter during training, starting high to encourage exploration and decreasing over time to focus on optimal policies. Third, a higher KL divergence threshold allows for more exploration before constraining policy updates.

The fourth modification introduces curiosity-driven exploration using a bonus reward mechanism based on prediction error of future states. This encourages the agent to visit novel states and experience diverse scenarios. Finally, alpha-percentile sampling specifically targets curiosity-driven trajectories, prioritizing those with higher curiosity values for training.

The authors evaluate these modifications across four environments: Grid World, Cartpole, LunarLander, and Ant. They conduct ablation studies to isolate the effects of each modification and test various combinations to identify the most effective approach. The performance is measured using average returns over training episodes and comparison to the baseline unsupervised RL method.

## Key Results
- Dynamic alpha and higher KL divergence threshold significantly improve performance over baseline in multiple environments
- Curiosity-driven exploration enhances learning in high-dimensional environments like Ant by encouraging diverse experiences
- The combination of curiosity, decreasing alpha, and higher KL divergence threshold yields the best results, especially in complex environments
- Sampling trajectories using a PDF did not improve performance due to the small sample space
- Curiosity-driven exploration shows limited benefits in simpler environments like Grid World

## Why This Works (Mechanism)
The proposed modifications work by addressing key challenges in unsupervised RL: exploration efficiency and sample diversity. Dynamic alpha allows the agent to start with broad exploration and gradually focus on promising policies as training progresses. Higher KL divergence threshold prevents premature convergence by allowing more exploration before constraining policy updates. Curiosity-driven exploration directly incentivizes visiting novel states through prediction error-based rewards, creating a feedback loop that encourages diverse experience collection. Entropy-based trajectory sampling ensures that more informative trajectories (those with higher state diversity) are prioritized during training. Together, these mechanisms create a more efficient exploration process that balances novelty seeking with policy refinement.

## Foundational Learning
- Unsupervised Reinforcement Learning: Why needed - Enables agents to learn useful behaviors without external rewards, crucial for real-world applications where reward signals are sparse or unavailable. Quick check - Agent can develop meaningful policies using only intrinsic motivation signals.
- KL Divergence in Policy Optimization: Why needed - Measures the difference between old and new policies, preventing destructive policy updates while allowing sufficient exploration. Quick check - Policy updates remain stable while still improving over time.
- Entropy Maximization: Why needed - Encourages exploration by preventing premature convergence to suboptimal deterministic policies. Quick check - Agent maintains stochastic policies that explore the state space effectively.
- Curiosity-Driven Exploration: Why needed - Provides intrinsic motivation for visiting novel states when external rewards are unavailable. Quick check - Agent consistently visits new states and experiences diverse scenarios.
- Alpha Parameter in PPO: Why needed - Controls the trade-off between exploration and exploitation by adjusting the entropy regularization term. Quick check - Policy balance between randomness and exploitation can be smoothly adjusted during training.

## Architecture Onboarding

Component Map:
Unsupervised RL Agent -> Curiosity Module -> Entropy Calculator -> KL Divergence Monitor -> Alpha Scheduler -> Trajectory Sampler

Critical Path:
1. Agent collects trajectories in environment
2. Curiosity module computes prediction errors for each state transition
3. Entropy calculator measures trajectory diversity
4. KL divergence monitor tracks policy changes
5. Alpha scheduler adjusts exploration temperature
6. Trajectory sampler selects training samples based on entropy and curiosity

Design Tradeoffs:
- Exploration vs. Exploitation: Higher alpha and KL thresholds favor exploration but may slow convergence
- Computational Overhead: Curiosity module and entropy calculations add processing requirements
- Sample Efficiency: More sophisticated sampling improves learning but requires more computation per step
- Hyperparameter Sensitivity: Dynamic alpha and KL thresholds require careful tuning for different environments

Failure Signatures:
- Premature convergence: Low KL divergence, high alpha, low curiosity scores
- Chaotic learning: High KL divergence, very low alpha, inconsistent returns
- Poor sample efficiency: Low entropy trajectories dominate training, limited curiosity exploration
- Computational bottleneck: Curiosity calculations slow down training significantly

First Experiments:
1. Test baseline unsupervised RL performance across all four environments to establish reference points
2. Evaluate individual curiosity module impact by comparing prediction error distributions across environments
3. Measure entropy-based sampling effectiveness by comparing trajectory diversity metrics before and after implementation

## Open Questions the Paper Calls Out
None

## Limitations
- Limited scope of experiments across only four environments may not generalize to other complex domains
- Performance gains from curiosity-driven exploration show high variability depending on environment complexity
- The approach may not be universally applicable across all types of environments and tasks
- Computational overhead of the combined approach versus individual modifications needs evaluation

## Confidence
- High confidence in the technical implementation of entropy-based trajectory sampling and alpha-percentile sampling
- Medium confidence in the dynamic alpha and higher KL divergence threshold improvements
- Low confidence in the generalization of curiosity-driven exploration benefits across different environment types

## Next Checks
1. Test the proposed modifications across a broader range of environments with varying state and action spaces to assess generalization capabilities
2. Conduct ablation studies comparing the proposed curiosity mechanism against established intrinsic motivation approaches like RND and ICM to establish relative effectiveness
3. Evaluate the sample efficiency and computational overhead of the combined approach versus individual modifications to determine practical deployment considerations