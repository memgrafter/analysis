---
ver: rpa2
title: 'Adversarial purification for no-reference image-quality metrics: applicability
  study and new methods'
arxiv_id: '2404.06957'
source_url: https://arxiv.org/abs/2404.06957
tags:
- adversarial
- image
- attacks
- metrics
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the applicability of adversarial purification
  methods from image classifiers to no-reference image quality assessment (IQA) metrics.
  The authors apply 10 different adversarial attacks on three IQA metrics (Linearity,
  MetaIQA, SPAQ) and evaluate 16 purification defenses.
---

# Adversarial purification for no-reference image-quality metrics: applicability study and new methods

## Quick Facts
- arXiv ID: 2404.06957
- Source URL: https://arxiv.org/abs/2404.06957
- Reference count: 40
- Primary result: DiffPure-based methods achieve highest SROCC scores for defending IQA metrics against adversarial attacks

## Executive Summary
This study investigates the applicability of adversarial purification methods from image classifiers to no-reference image quality assessment (IQA) metrics. The authors apply 10 different adversarial attacks on three IQA metrics (Linearity, MetaIQA, SPAQ) and evaluate 16 purification defenses. They propose two new methods: DiffPure+Unsharp, combining diffusion purification with unsharp masking, and an FCN filter for unrestricted color attacks. Results show that DiffPure-based methods achieve the highest Spearman correlation coefficient (SROCC) of defended metric values while maintaining image quality, with DiffPure+Unsharp reaching 0.750 SROCC. The FCN filter demonstrates superior performance against unrestricted color attacks.

## Method Summary
The study applies 10 adversarial attacks to three IQA metrics (Linearity, MetaIQA, SPAQ) using the NIPS 2017 dataset. For each attack-metric combination, 16 purification methods are evaluated, including 12 baseline methods (resize, flip, blur, JPEG, median filter, MPRNet, Real-ESRGAN, DiffPure variants) and 4 proposed methods (DiffPure+Unsharp, FCN filter for AdvCF). The authors measure defense effectiveness using SROCC between metric scores on clean and purified images, relative gain scores, and quality metrics (PSNR/SSIM). The proposed FCN filter is a compact 3-layer convolutional network trained specifically to reverse color filtering attacks.

## Key Results
- DiffPure-based methods achieve highest SROCC scores for defended metric values while maintaining image quality
- DiffPure+Unsharp reaches 0.750 SROCC, the best performance among all tested methods
- Simple geometric transformations like rotation and flipping effectively defeat attacks, with flipping achieving best gain score
- FCN filter demonstrates superior performance specifically against unrestricted color attacks (AdvCF)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion-based purification methods can effectively neutralize adversarial perturbations in IQA metrics by gradually merging perturbations with added noise and then reversing the diffusion process.
- Mechanism: The DiffPure method introduces Gaussian noise to the adversarial image through forward diffusion, allowing adversarial perturbations to blend with the noise. During the reverse diffusion process, both the noise and perturbations are removed, resulting in a purified image that better reflects the original quality.
- Core assumption: Adversarial perturbations behave similarly to Gaussian noise during the diffusion process and can be effectively removed through the reverse diffusion.
- Evidence anchors:
  - [abstract]: "DiffPure-based methods achieve the highest Spearman correlation coefficient (SROCC) of defended metric values while maintaining image quality"
  - [section]: "DiffPure [28] diffusion model shown to be effective in purifying adversarial images. A small amount of noise is introduced to the adversarial example through forward diffusion. The diffusion process continues until an optimally computed time step is reached. Subsequently, the model reverses this diffusion to recover a purified, clean image."
- Break condition: If adversarial perturbations have specific frequency or spatial characteristics that don't blend well with Gaussian noise during diffusion, or if the perturbations are too strong for the diffusion process to effectively remove them.

### Mechanism 2
- Claim: Simple geometric transformations like rotation and flipping can effectively defeat certain types of adversarial attacks on IQA metrics.
- Mechanism: These transformations alter the spatial location of adversarial perturbations, making them less effective at manipulating the metric scores. The study found that flipping achieved the best Gain score among all purification methods.
- Core assumption: Adversarial perturbations are not invariant to spatial transformations and their effectiveness diminishes when the image is geometrically transformed.
- Evidence anchors:
  - [abstract]: "Results show that simple transformations like rotation and flipping can effectively defeat attacks"
  - [section]: "Flip 0.158 0.077 12.402 0.162 0.728 36.67" - showing Flip had the best Gain score
- Break condition: If attackers optimize their perturbations specifically to be invariant to common geometric transformations, or if the IQA metric is designed to be robust to such transformations.

### Mechanism 3
- Claim: FCN-based filters can effectively defend against unrestricted color attacks that traditional purification methods cannot handle.
- Mechanism: The proposed FCN filter uses a compact convolutional neural network trained specifically to recover original images from color-attacked versions. It learns to map color-filtered images back to their original state by optimizing for Mean Squared Error during training.
- Core assumption: A relatively simple FCN architecture with 3 convolutional layers can learn to reverse color filtering attacks effectively.
- Evidence anchors:
  - [abstract]: "The FCN filter demonstrates superior performance against unrestricted color attacks"
  - [section]: "We propose a new defence method based on a compact, fully convolutional neural network (FCN filter). Our model consists of three convolutional layers that apply 64, 16, and 3 filters and preserve the original image dimensions."
- Break condition: If color attacks become too complex for the FCN to learn effective reversal mappings, or if attackers use color transformations that significantly alter image content beyond what the FCN was trained to handle.

## Foundational Learning

- Concept: Adversarial attacks and their classification
  - Why needed here: Understanding the different types of attacks (restricted vs unrestricted, additive vs perceptual) is crucial for selecting appropriate defense mechanisms and interpreting results
  - Quick check question: What distinguishes restricted adversarial attacks from unrestricted ones in the context of IQA metrics?

- Concept: No-reference image quality assessment (IQA) metrics
  - Why needed here: The study focuses on defending IQA metrics, so understanding how these metrics work and what they measure is essential for evaluating defense effectiveness
  - Quick check question: How do no-reference IQA metrics differ from full-reference metrics in terms of input requirements and vulnerability to attacks?

- Concept: Purification techniques and their evaluation
  - Why needed here: The study tests various purification methods, so understanding how they work and how their effectiveness is measured is critical for interpreting results
  - Quick check question: What are the key evaluation metrics used to assess the effectiveness of adversarial purification methods?

## Architecture Onboarding

- Component map:
  Attack generation module -> Purification module -> Evaluation module -> Dataset module

- Critical path:
  1. Generate adversarial images using attack methods on clean images
  2. Apply purification methods to adversarial images
  3. Evaluate defense effectiveness using multiple metrics
  4. Analyze results to identify most effective defenses

- Design tradeoffs:
  - Computational efficiency vs defense effectiveness (e.g., DiffPure is effective but slow)
  - Defense generality vs attack-specific optimization (e.g., FCN filter for AdvCF vs general methods)
  - Image quality preservation vs metric score stability

- Failure signatures:
  - High Gain scores but low SROCC indicating metrics are being manipulated
  - Good SSIM/PSNR but poor metric score preservation
  - Inconsistent performance across different attack types

- First 3 experiments:
  1. Test the baseline performance of all 16 purification methods against a simple FGSM attack
  2. Evaluate the FCN filter specifically against the AdvCF unrestricted color attack
  3. Compare the performance of DiffPure with and without the Unsharp add-on on strong iterative attacks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of adversarial purification methods on the perceptual quality of images across different types of IQA metrics (e.g., no-reference vs. full-reference)?
- Basis in paper: [explicit] The paper evaluates the efficiency of 16 purification methods against 10 attacks on three IQA metrics (Linearity, MetaIQA, and SPAQ) and discusses the trade-offs between defending against adversarial attacks and maintaining image quality.
- Why unresolved: The study focuses on no-reference metrics and provides results for Linearity, but the effects on full-reference metrics and the general applicability across different types of IQA metrics remain unexplored.
- What evidence would resolve it: Comparative analysis of purification methods on both no-reference and full-reference IQA metrics, assessing the changes in perceptual quality and correlation with subjective scores.

### Open Question 2
- Question: How do adversarial purification methods perform in real-world scenarios where the presence of an attack is unknown?
- Basis in paper: [explicit] The paper mentions that defenses do not know if an input image is perturbed or not and checks the Linearity performance after all defenses on clean images, but it does not explore the performance in scenarios with unknown attacks.
- Why unresolved: The study does not simulate real-world conditions where attacks might be present but undetected, leaving the effectiveness of defenses in such scenarios untested.
- What evidence would resolve it: Testing purification methods on a dataset with a mix of clean and adversarial images without prior knowledge of which images are attacked, measuring the ability to maintain image quality and metric stability.

### Open Question 3
- Question: Can provable defenses be developed for IQA metrics to ensure robustness against adversarial attacks?
- Basis in paper: [explicit] The paper concludes with a discussion on the limitations of empirical attacks and defenses, suggesting that developing provable defenses for IQA metrics would be a promising direction for creating robust metrics.
- Why unresolved: The study focuses on empirical defense mechanisms, and while it suggests the potential for provable defenses, it does not explore the development or effectiveness of such methods.
- What evidence would resolve it: Research into mathematical frameworks or algorithms that can guarantee the robustness of IQA metrics against a wide range of adversarial attacks, validated through theoretical proofs and empirical testing.

## Limitations

- The study's findings are based primarily on synthetic adversarial attacks rather than real-world image distortions, limiting real-world applicability
- DiffPure-based methods, while effective, are computationally intensive and may not be practical for real-time applications
- The FCN filter's effectiveness is limited to the specific AdvCF attack it was trained on, raising questions about generalization to other attack types

## Confidence

- **High Confidence**: The finding that simple geometric transformations (rotation, flipping) can effectively defeat certain adversarial attacks is well-supported by the experimental data and aligns with established adversarial defense literature.
- **Medium Confidence**: The superior performance of DiffPure-based methods is supported by the results, but the computational complexity and potential overfitting to the specific attack types used in the study introduce uncertainty about real-world applicability.
- **Medium Confidence**: The effectiveness of the proposed FCN filter against AdvCF attacks is demonstrated, but its generalization to other unrestricted attacks and its performance with limited training data are not fully explored.

## Next Checks

1. **Generalization Test**: Evaluate the proposed methods against a broader range of real-world image distortions (compression artifacts, sensor noise, etc.) beyond the synthetic adversarial attacks used in the study.

2. **Efficiency Analysis**: Conduct a thorough computational efficiency analysis of DiffPure-based methods, including runtime benchmarks and memory usage comparisons with other purification techniques.

3. **Transferability Assessment**: Test the FCN filter's performance against other unrestricted attacks and different image datasets to assess its generalization capabilities beyond the specific AdvCF attack it was trained on.