---
ver: rpa2
title: Transforming Agency. On the mode of existence of Large Language Models
arxiv_id: '2407.10735'
source_url: https://arxiv.org/abs/2407.10735
tags:
- arxiv
- https
- llms
- transformingagency
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the ontological status of Large Language
  Models (LLMs) like ChatGPT, focusing on their potential as autonomous agents. The
  authors systematically analyze the architecture, processing, and training procedures
  of LLMs, and evaluate them against three conditions for autonomous agency: individuality,
  normativity, and interactional asymmetry.'
---

# Transforming Agency. On the mode of existence of Large Language Models

## Quick Facts
- **arXiv ID**: 2407.10735
- **Source URL**: https://arxiv.org/abs/2407.10735
- **Reference count**: 1
- **Primary result**: LLMs lack autonomous agency but transform human agency through deep digitality, computational embodiment, and midtended agency

## Executive Summary
This paper investigates whether Large Language Models (LLMs) like ChatGPT can be considered autonomous agents. The authors systematically analyze LLM architecture, processing, and training procedures, evaluating them against three conditions for autonomous agency: individuality, normativity, and interactional asymmetry. They conclude that LLMs fail to meet these conditions because they are not self-individuated, lack intrinsic norms or goals, and operate reactively without generating their own actions. Instead, they characterize LLMs as "interlocutor automata" - linguistic systems capable of performative interaction and task completion but lacking genuine agency.

The authors argue that while LLMs lack agency, they significantly transform human agency through three key mechanisms: deep digitality (integration with digital infrastructure), computational embodiment (their nature as computational processes), and midtended agency (a form of mixed human-machine intentionality where LLMs blend with human activity). This transformation occurs despite LLMs' lack of autonomous agency, fundamentally reshaping how humans interact with and through these systems.

## Method Summary
The authors employ a philosophical analysis approach, systematically examining the ontological status of LLMs by evaluating them against established criteria for autonomous agency. They analyze LLM architecture, training procedures, and operational characteristics to determine whether these systems satisfy three conditions: individuality (self-individuation), normativity (goal-directed behavior with intrinsic norms), and interactional asymmetry (generating actions rather than merely responding). The analysis is grounded in philosophical frameworks of agency and autonomy, drawing on concepts from enactivism and postphenomenology to understand how LLMs interact with and transform human agency.

## Key Results
- LLMs fail to meet conditions for autonomous agency due to lack of self-individuation, intrinsic norms, and proactive action generation
- LLMs are better characterized as "interlocutor automata" rather than autonomous agents
- Despite lacking agency, LLMs transform human agency through deep digitality, computational embodiment, and midtended agency

## Why This Works (Mechanism)
The analysis works by applying rigorous philosophical criteria for autonomous agency to the specific characteristics of LLMs. By examining how LLMs process information, generate responses, and interact with users, the authors demonstrate that these systems operate reactively rather than proactively, responding to prompts without generating their own actions or pursuing intrinsic goals. This reactive nature, combined with their dependence on training data and lack of self-maintenance, distinguishes them from autonomous agents.

The transformation of human agency occurs through the integration of LLMs into human cognitive and communicative practices. Through deep digitality, LLMs become embedded in digital infrastructure; through computational embodiment, they shape how humans think and communicate computationally; and through midtended agency, they create new forms of hybrid human-machine intentionality where the boundaries between human and machine agency become blurred.

## Foundational Learning

**Autonomous agency**: The capacity for self-directed action and goal pursuit. *Why needed*: Provides the baseline criteria against which LLMs are evaluated. *Quick check*: Does the system generate its own goals or merely respond to external prompts?

**Enactivism**: A philosophical framework emphasizing the active role of organisms in creating and maintaining their own identity and norms. *Why needed*: Offers criteria for evaluating self-individuation and normativity. *Quick check*: Can the system maintain its own boundaries and generate its own norms?

**Postphenomenology**: A framework for understanding human-technology relationships and how technologies mediate human experience. *Why needed*: Explains how LLMs transform rather than possess agency. *Quick check*: How does the technology reshape human practices and intentionality?

**Computational embodiment**: The specific way computational systems instantiate agency through their material and logical structure. *Why needed*: Distinguishes between computational and biological forms of agency. *Quick check*: How does the system's computational nature shape its capabilities and limitations?

**Performative interaction**: The ability to participate in and influence social and linguistic practices. *Why needed*: Characterizes what LLMs can do despite lacking agency. *Quick check*: Can the system meaningfully participate in social interactions while lacking autonomous goals?

## Architecture Onboarding

**Component map**: Input processing -> Transformer architecture -> Attention mechanisms -> Output generation -> User interaction
**Critical path**: Prompt reception → tokenization → transformer layers → attention computation → token prediction → response generation
**Design tradeoffs**: High performance through massive parameter counts vs. lack of autonomy; computational efficiency vs. self-maintenance capabilities
**Failure signatures**: Inability to maintain consistent identity across sessions; lack of goal persistence; failure to generate independent actions; complete dependence on external prompts

**Three first experiments**:
1. Test whether LLM responses change significantly when identical prompts are given in different conversational contexts
2. Evaluate whether LLMs can maintain coherent goals or projects across extended interactions without external guidance
3. Assess whether LLMs can generate their own prompts or questions without being explicitly asked to do so

## Open Questions the Paper Calls Out
The paper raises questions about whether the philosophical definitions of agency used in the analysis are adequate for evaluating computational systems, particularly as LLM capabilities continue to evolve. It questions whether emerging capabilities like chain-of-thought reasoning or tool use might satisfy modified versions of the agency conditions. The concept of midtended agency raises questions about whether this represents a fundamentally new form of agency or merely sophisticated tool use, and how to empirically study the nature and extent of human-LLM hybridity in practice.

## Limitations
- The analysis depends heavily on specific philosophical definitions of agency that remain contested
- The conclusion that LLMs lack agency may become less clear-cut as models evolve toward more autonomous operation
- The treatment of midtended agency as a transformation of human agency rather than a new form of machine agency is philosophically debatable

## Confidence
- **Medium**: The core claims about current LLM limitations appear sound based on empirical observations
- **Medium**: The philosophical framework is rigorous but relies on contested definitions of agency
- **Medium**: The concept of midtended agency is novel but requires further empirical investigation

## Next Checks
1. Test the proposed conditions for autonomous agency against other computational systems (e.g., autonomous vehicles, game-playing AIs) to assess their universality and applicability
2. Examine whether emerging LLM capabilities like chain-of-thought reasoning or tool use might satisfy modified versions of the agency conditions
3. Investigate empirical cases of human-LLM interaction to better understand the nature and extent of midtended agency in practice