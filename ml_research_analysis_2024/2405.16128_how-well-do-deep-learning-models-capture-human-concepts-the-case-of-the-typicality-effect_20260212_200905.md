---
ver: rpa2
title: How Well Do Deep Learning Models Capture Human Concepts? The Case of the Typicality
  Effect
arxiv_id: '2405.16128'
source_url: https://arxiv.org/abs/2405.16128
tags:
- typicality
- vision
- language
- human
- category
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how well deep learning models capture human
  concepts by evaluating their alignment with human typicality judgments. Researchers
  tested 8 language models, 10 vision models, and one multimodal model across 27 categories.
---

# How Well Do Deep Learning Models Capture Human Concepts? The Case of the Typicality Effect

## Quick Facts
- **arXiv ID:** 2405.16128
- **Source URL:** https://arxiv.org/abs/2405.16128
- **Reference count:** 9
- **Primary result:** Language models (particularly MiniLM) show better alignment with human typicality judgments than vision models

## Executive Summary
This study investigates the alignment between deep learning models and human conceptual representations by examining how well different ML models capture the typicality effect - the cognitive phenomenon where some category members are perceived as better examples than others. Researchers evaluated 8 language models, 10 vision models, and one multimodal model across 27 categories using human-generated typicality scores. The findings reveal that language models, especially MiniLM, demonstrate substantially better alignment with human typicality judgments compared to vision models, with correlations ranging from 0.429 to 0.096 versus 0.146 to -0.007 for vision models. The study also shows that combining language and vision models improves prediction accuracy, with the AlexNet + MiniLM pairing achieving a 0.4995 correlation.

## Method Summary
The researchers tested deep learning models' ability to capture human concepts by evaluating their alignment with typicality judgments across 27 categories. They collected human typicality scores through crowdsourcing and compared these against model-generated similarity scores. The evaluation included 8 language models, 10 vision models, and one multimodal model. For vision models, researchers created a new dataset of 14-15 images per category, while language models were evaluated using word embeddings. Model performance was assessed using Spearman correlation between model-generated similarity scores and human typicality judgments. The study also examined whether combining language and vision models could improve typicality prediction, finding that such combinations generally outperformed individual models.

## Key Results
- Language models showed substantially better alignment with human typicality judgments than vision models, with the best language model (MiniLM) achieving a Spearman correlation of 0.429 versus 0.146 for the best vision model
- Combining language and vision models improved typicality predictions, with the AlexNet + MiniLM pairing achieving a 0.4995 correlation
- The multimodal CLIP ViT model performed well, particularly when using the vision portion of the model
- Individual model performance varied significantly, with language models generally outperforming vision models across categories

## Why This Works (Mechanism)
None provided in the source material.

## Foundational Learning

**Typicality Effect**: The cognitive phenomenon where some category members are perceived as better examples than others. Why needed: This is the core psychological concept being measured to evaluate model alignment with human conceptual representations. Quick check: Verify understanding by recognizing that robins are typically considered better examples of "birds" than penguins.

**Spearman Correlation**: A non-parametric measure of rank correlation that assesses how well the relationship between two variables can be described using a monotonic function. Why needed: Used to measure alignment between model-generated similarity scores and human typicality judgments. Quick check: Confirm that Spearman correlation values range from -1 to +1, with higher values indicating stronger monotonic relationships.

**Embeddings**: Numerical representations of words, images, or other data that capture semantic relationships in vector space. Why needed: Models use embeddings to represent and compare concepts, which are then evaluated against human judgments. Quick check: Understand that similar concepts have embeddings that are closer together in the vector space.

## Architecture Onboarding

**Component Map**: Data Collection -> Human Typicality Scores -> Model Similarity Scores -> Spearman Correlation Analysis -> Performance Evaluation

**Critical Path**: The most important sequence is: collecting human typicality judgments → generating model similarity scores → computing Spearman correlations → analyzing performance differences between model types.

**Design Tradeoffs**: The study chose to use Spearman correlation (non-parametric) rather than Pearson correlation (parametric), which is appropriate for ordinal data but may miss linear relationships. They also limited evaluation to 27 categories rather than a broader concept set, trading comprehensiveness for manageability.

**Failure Signatures**: Poor performance indicates either the model fails to capture human conceptual structure or the evaluation method is insufficient. Vision models' consistently lower performance suggests fundamental challenges in capturing abstract conceptual properties from visual data alone.

**3 First Experiments**:
1. Test whether model performance correlates with model size or training data diversity
2. Evaluate cross-linguistic performance using non-English language models
3. Examine whether fine-tuning models on typicality-relevant data improves alignment

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Limited set of 27 categories may not represent full diversity of human concepts
- Focus on English-language models and concepts may limit generalizability
- Image dataset of 14-15 images per category may be insufficient for robust judgments
- Correlation metrics capture monotonic relationships but may miss complex alignment patterns
- Did not investigate the role of model size, training data diversity, or architectural differences

## Confidence

**High confidence in**:
- Methodology for measuring typicality alignment using human-generated scores
- Relative performance differences between language and vision models
- Finding that combining language and vision models improves predictions

**Medium confidence in**:
- Absolute magnitude of correlations reported
- Generalizability of findings across broader concept domains
- Specific ranking of individual model performances

## Next Checks
1. **Scale and Diversity Validation**: Replicate the study using a substantially larger and more diverse set of categories (e.g., 100+ categories spanning different conceptual domains) to test generalizability

2. **Cross-Lingual and Cultural Validation**: Conduct parallel studies using non-English language models and culturally diverse concept sets to assess cross-cultural generalizability of typicality effects

3. **Architectural and Training Analysis**: Systematically vary model architectures, sizes, and training datasets to identify which factors most strongly predict typicality alignment with human judgments