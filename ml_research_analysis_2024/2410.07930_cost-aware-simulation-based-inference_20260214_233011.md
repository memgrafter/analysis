---
ver: rpa2
title: Cost-aware simulation-based inference
arxiv_id: '2410.07930'
source_url: https://arxiv.org/abs/2410.07930
tags:
- cost
- sampling
- cost-aware
- which
- density
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces cost-aware simulation-based inference (SBI)
  methods that significantly reduce computational costs of popular SBI approaches
  like neural posterior estimation (NPE), neural likelihood estimation (NLE), and
  approximate Bayesian computation (ABC). The key idea is to use self-normalised importance
  sampling with a cost-aware proposal distribution that encourages sampling from cheaper
  parameter regions.
---

# Cost-aware simulation-based inference

## Quick Facts
- arXiv ID: 2410.07930
- Source URL: https://arxiv.org/abs/2410.07930
- Reference count: 40
- This paper introduces cost-aware simulation-based inference (SBI) methods that significantly reduce computational costs of popular SBI approaches like neural posterior estimation (NPE), neural likelihood estimation (NLE), and approximate Bayesian computation (ABC).

## Executive Summary
This paper presents a novel approach to reduce the computational cost of simulation-based inference by incorporating simulation cost information into the sampling process. The method uses self-normalized importance sampling with a cost-aware proposal distribution that encourages sampling from parameter regions with lower simulation costs. By weighting the prior with a penalty function of the simulation cost and correcting for bias with importance weights, the approach achieves 25-85% reduction in simulation time while maintaining comparable posterior accuracy to standard methods. The technique is particularly effective when simulation costs vary significantly across parameter values.

## Method Summary
The method introduces cost-aware simulation-based inference by modifying the sampling procedure of standard SBI approaches. Instead of sampling directly from the prior, parameters are drawn from a cost-aware proposal distribution obtained through rejection sampling. This proposal is constructed by dividing the prior by a penalty function of the simulation cost, then normalized using self-normalized importance sampling to correct for the induced bias. The approach can be applied to various SBI methods including NPE, NLE, and ABC, and includes a multiple importance sampling variant that combines proposals with different penalty strengths to balance computational efficiency and posterior accuracy.

## Key Results
- Demonstrated 25-85% reduction in simulation time across epidemiology models (SIR variants) and radio propagation models
- Maintained comparable posterior accuracy (measured by MMD) to standard SBI methods while achieving significant computational gains
- Multiple importance sampling variant improved robustness when posterior mass is distributed across both high and low cost regions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cost-aware sampling reduces total computational time by biasing parameter selection toward cheaper simulation regions.
- Mechanism: Rejection sampling uses a cost-aware proposal distribution that downweights parameters with high simulation costs, then applies self-normalized importance sampling to correct the bias.
- Core assumption: The cost function c(θ) is known or can be estimated, and varies significantly across parameter space.
- Evidence anchors:
  - [abstract] "This is achieved through a combination of rejection and self-normalised importance sampling, which significantly reduces the number of expensive simulations needed."
  - [section 3.1] "Instead of sampling independently from the target π(θ), we propose to use a cost-aware proposal distribution"
- Break condition: If c(θ) is constant or nearly constant across parameter space, the cost-aware approach offers minimal benefit over standard SBI.

### Mechanism 2
- Claim: The cost-efficiency tradeoff is controlled by the penalty function g(c(θ)), balancing reduced cost against potential loss in posterior accuracy.
- Mechanism: Different choices of g (e.g., g(z) = z, z², z³) create different acceptance probabilities in rejection sampling, affecting both computational gain (CG) and effective sample size (ESS).
- Core assumption: The ratio gmin/gmax can be tuned to achieve a balance between cost reduction and statistical efficiency.
- Evidence anchors:
  - [section 3.2] "we also introduce the notion of computational gain (CG), which is the ratio of the expected cost of the downstream task using Monte Carlo to that of using cost-aware sampling"
  - [section 3.3] "we select g such that the quantity CG×ESS is above or close to 1"
- Break condition: If g is chosen such that gmin/gmax is too small, the effective sample size becomes too low, degrading posterior estimates significantly.

### Mechanism 3
- Claim: Multiple importance sampling combines cost-aware proposals with different penalty strengths to capture both high-cost and low-cost regions effectively.
- Mechanism: Using J components with different g functions (including the original prior), the method ensures samples from expensive regions are not completely lost while still benefiting from cost reduction.
- Core assumption: The posterior mass is distributed across both high and low cost regions, requiring a mixture approach for accurate inference.
- Evidence anchors:
  - [section 3.3] "we consider J importance distributions, which include the target π1 = π and J − 1 cost-aware proposals"
  - [section 4.1] "we found n1 = . . . = nJ and J = 4 to work well"
- Break condition: If the posterior is entirely concentrated in the cheapest region, multiple importance sampling provides no advantage over using a single strong penalty function.

## Foundational Learning

- Concept: Importance sampling and self-normalized weights
  - Why needed here: The method relies on reweighting samples from a proposal distribution to estimate expectations under the target distribution, correcting for the bias introduced by cost-aware sampling.
  - Quick check question: How do you compute normalized weights in self-normalized importance sampling when you don't know the normalization constants of either the target or proposal?

- Concept: Rejection sampling with acceptance probability
  - Why needed here: The cost-aware proposal is sampled via rejection sampling, requiring the acceptance probability to be computed without knowing the normalization constant of the proposal.
  - Quick check question: What is the acceptance probability for rejection sampling when the proposal is π(θ)/g(c(θ)) and the target is π(θ)?

- Concept: Computational gain (CG) and effective sample size (ESS)
  - Why needed here: These metrics quantify the cost-efficiency tradeoff, guiding the choice of penalty function g to balance reduced simulation cost against statistical efficiency.
  - Quick check question: How is CG defined, and what bounds exist on its value given constraints on g?

## Architecture Onboarding

- Component map: Cost function estimation -> Cost-aware proposal sampling (rejection sampling) -> SBI method (NPE/NLE/ABC) with weighted samples -> Multiple importance sampling variant
- Critical path: 1. Estimate or obtain cost function c(θ) 2. Choose penalty function g based on CG×ESS 3. Sample parameters from cost-aware proposal via rejection sampling 4. Run simulator for each sampled parameter 5. Apply SBI method with importance weights
- Design tradeoffs: Stronger penalty functions (larger exponents) reduce cost more but also reduce ESS; multiple importance sampling improves robustness but increases complexity; cost estimation accuracy directly impacts proposal quality.
- Failure signatures: If ESS becomes too small (weights are highly variable), posterior estimates will have high variance; if cost function is poorly estimated, the method may sample inefficiently; if the posterior lies entirely in high-cost regions, cost reduction will be minimal.
- First 3 experiments:
  1. Implement cost-aware ABC on the Gamma simulator with known cost function, compare to standard ABC in terms of MMD vs cost
  2. Add cost estimation via GP regression to the Gamma experiment, test how estimation error affects performance
  3. Implement multiple importance sampling for NPE on the temporal SIR model, compare single vs multiple g functions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for selecting penalty functions g when the posterior lies in high-cost regions of the parameter space?
- Basis in paper: [inferred] The paper acknowledges that performance degrades when the true posterior lies in computationally expensive regions, and recommends using multiple importance sampling components but does not provide a systematic approach for this scenario.
- Why unresolved: The paper only suggests using higher powers of g (e.g., g(z) = z^3) as components but doesn't provide theoretical guidance on how to optimally balance components when the posterior concentrates in expensive regions.
- What evidence would resolve it: Systematic experiments comparing different combinations of penalty functions when the posterior is known to be in high-cost regions, or a theoretical framework for adaptive penalty function selection based on posterior location.

### Open Question 2
- Question: How does cost-aware SBI perform in high-dimensional parameter spaces where simulation cost varies across multiple parameters?
- Basis in paper: [explicit] The paper states "As importance sampling suffers in high dimensions, our method may not work as well if there are many parameters that affect the cost function."
- Why unresolved: The experimental evaluation focuses on models with at most 4 parameters, and the paper does not provide theoretical analysis of how performance scales with dimensionality.
- What evidence would resolve it: Empirical studies on models with varying numbers of parameters (e.g., 10, 20, 50 parameters) where simulation cost varies across dimensions, and theoretical analysis of the curse of dimensionality for cost-aware SBI.

### Open Question 3
- Question: Can cost-aware SBI be extended to optimization-based SBI methods like minimum distance estimation or simulated moments?
- Basis in paper: [explicit] The paper explicitly states "A limitation of our approach is that it does not apply to optimisation-based SBI methods such as minimum distance estimation... Developing cost-aware versions of these methods is an interesting avenue for future work."
- Why unresolved: The paper only applies the approach to sampling-based methods and does not attempt to extend it to optimization-based methods.
- What evidence would resolve it: A successful implementation of cost-aware sampling for at least one optimization-based SBI method, demonstrating computational savings while maintaining parameter estimation accuracy.

## Limitations
- Method performance depends heavily on accurate cost function estimation, which can be challenging when simulation costs vary complexly across parameter space
- The approach is limited to sampling-based SBI methods and cannot be directly applied to optimization-based methods like minimum distance estimation
- Performance degrades significantly when the posterior concentrates in high-cost regions of parameter space

## Confidence
- Theoretical framework: High
- Experimental results: Medium-High
- Scalability to high dimensions: Low
- Performance with poor cost estimation: Low

## Next Checks
1. **Cost estimation sensitivity**: Systematically vary the amount of initial data used for GP-based cost estimation and measure the impact on computational gain and posterior accuracy across all three SBI methods (NPE, NLE, ABC).

2. **Dimensionality scaling**: Test the method on higher-dimensional parameter spaces (e.g., 10+ dimensions) using synthetic simulators with known cost structures to evaluate how computational gains scale with problem complexity.

3. **Cost function smoothness**: Compare performance across simulators with different cost function characteristics (smooth vs. discontinuous, correlated vs. uncorrelated with posterior) to identify when the method breaks down or provides minimal benefit.