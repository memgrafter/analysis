---
ver: rpa2
title: Accelerating Convergence in Bayesian Few-Shot Classification
arxiv_id: '2405.01507'
source_url: https://arxiv.org/abs/2405.01507
tags:
- classification
- descent
- bayesian
- convergence
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a mirror-descent-based variational inference
  method for few-shot classification using Gaussian processes. The approach transforms
  the non-conjugate inference problem into an optimization with conjugate computation,
  leveraging non-Euclidean geometry to accelerate convergence and maintain parameterization
  invariance.
---

# Accelerating Convergence in Bayesian Few-Shot Classification

## Quick Facts
- arXiv ID: 2405.01507
- Source URL: https://arxiv.org/abs/2405.01507
- Authors: Tianjun Ke; Haoqun Cao; Feng Zhou
- Reference count: 37
- Key outcome: Mirror-descent variational inference for few-shot classification achieves 65.45% 1-shot and 78.38% 5-shot accuracy on CUB dataset

## Executive Summary
This paper introduces a mirror-descent-based variational inference method for few-shot classification using Gaussian processes, addressing the computational challenges of non-conjugate Bayesian inference. The approach transforms non-conjugate problems into optimization tasks with conjugate computations by leveraging non-Euclidean geometry, resulting in faster convergence and improved uncertainty quantification. The method demonstrates competitive accuracy across multiple datasets (mini-ImageNet, Omniglot, EMNIST, CUB) while maintaining parameterization invariance.

## Method Summary
The proposed method reformulates variational inference for non-conjugate likelihood functions as a mirror descent optimization problem in the parameter space. By introducing a non-Euclidean geometry through a mirror map, the algorithm achieves second-order optimization characteristics in the inner loop, accelerating convergence compared to first-order methods. The approach uses an isotropic Gaussian posterior approximation and performs conjugate computations within each optimization step, making it suitable for few-shot learning scenarios where computational efficiency is critical.

## Key Results
- Achieves 65.45% 1-shot and 78.38% 5-shot accuracy on CUB dataset
- Lowest Expected Calibration Error (ECE) of 0.005 on CUB 5-shot classification
- Competitive performance across multiple datasets: mini-ImageNet, Omniglot, EMNIST
- Demonstrated faster convergence compared to first-order variational inference methods

## Why This Works (Mechanism)
The method works by transforming the non-conjugate variational inference problem into an optimization framework using mirror descent. The key insight is that by choosing an appropriate mirror map, the non-conjugate likelihood can be handled through conjugate computations within each optimization step. This approach leverages the geometry of the parameter space to accelerate convergence, achieving second-order optimization characteristics through first-order operations. The isotropic Gaussian posterior assumption simplifies the conjugate updates while maintaining computational tractability.

## Foundational Learning

**Gaussian Processes**: Probabilistic models that define distributions over functions, essential for few-shot learning as they provide uncertainty estimates. Why needed: GP priors allow incorporation of prior knowledge about function smoothness and similarity. Quick check: Verify understanding of kernel functions and their role in GP modeling.

**Variational Inference**: Approximate Bayesian inference method that converts integration problems into optimization by minimizing KL divergence. Why needed: Enables scalable inference for complex models where exact Bayesian inference is intractable. Quick check: Understand mean-field approximation and ELBO optimization.

**Mirror Descent**: Optimization algorithm that uses Bregman divergences instead of Euclidean distance, providing better convergence in non-convex spaces. Why needed: Enables handling of non-conjugate likelihoods through geometric transformations. Quick check: Compare convergence rates between mirror descent and gradient descent.

## Architecture Onboarding

**Component Map**: Input data -> Feature extractor -> GP kernel -> Mirror map transformation -> Variational optimization -> Posterior prediction

**Critical Path**: The optimization loop where mirror descent updates are computed using conjugate updates for each data point, with the mirror map transforming the parameter space to enable efficient conjugate computations.

**Design Tradeoffs**: The isotropic Gaussian posterior assumption simplifies computation but may limit expressiveness for complex data distributions. The mirror map choice affects convergence speed but requires careful selection to maintain computational efficiency.

**Failure Signatures**: Poor convergence may indicate inappropriate mirror map selection or violation of the isotropic posterior assumption. Overconfident predictions suggest insufficient regularization in the optimization process.

**First Experiments**: 
1. Verify convergence on synthetic data with known ground truth
2. Compare calibration metrics (ECE) against baseline methods
3. Test sensitivity to mirror map hyperparameters

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on complex datasets like mini-ImageNet remains competitive but not state-of-the-art
- Theoretical runtime efficiency gains lack empirical validation through wall-clock time comparisons
- Isotropic Gaussian posterior assumption may limit flexibility for complex data distributions
- Uncertainty quantification claims based on single dataset metrics require broader validation

## Confidence

**Theoretical foundation and convergence analysis**: High
**Empirical accuracy results**: Medium
**Uncertainty quantification claims**: Medium
**Runtime efficiency improvements**: Low

## Next Checks

1. Conduct runtime efficiency benchmarks comparing wall-clock time against first-order variational inference methods across multiple datasets
2. Test the method on larger-scale few-shot learning benchmarks (e.g., tiered-ImageNet) to evaluate scalability
3. Perform extensive hyperparameter sensitivity analysis to verify the claimed robustness across different settings