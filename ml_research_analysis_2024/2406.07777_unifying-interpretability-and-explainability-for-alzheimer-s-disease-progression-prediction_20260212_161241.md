---
ver: rpa2
title: Unifying Interpretability and Explainability for Alzheimer's Disease Progression
  Prediction
arxiv_id: '2406.07777'
source_url: https://arxiv.org/abs/2406.07777
tags:
- shap
- ipfc
- value
- plot
- cognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of interpretability and explainability
  in Alzheimer's disease progression prediction models. The authors propose a unified
  framework that combines an interpretable reinforcement learning (RL) model with
  the post-hoc explainability method SHAP.
---

# Unifying Interpretability and Explainability for Alzheimer's Disease Progression Prediction

## Quick Facts
- arXiv ID: 2406.07777
- Source URL: https://arxiv.org/abs/2406.07777
- Reference count: 40
- This work addresses the challenge of interpretability and explainability in Alzheimer's disease progression prediction models.

## Executive Summary
This study proposes a unified framework that combines an interpretable reinforcement learning (RL) model with the post-hoc explainability method SHAP to predict Alzheimer's disease (AD) progression. The interpretable RL model uses differential equations to encode causal relationships between brain regions, amyloid accumulation, and cognition, while SHAP provides transparent feature attributions. The framework successfully predicts 10-year cognition trajectories and reveals insights into the relative importance of different biomarkers, though it surprisingly downplays the role of amyloid accumulation compared to clinical expectations.

## Method Summary
The approach combines an interpretable RL model based on differential equations with SHAP for explainability. The RL agent predicts 10-year cognition trajectories from baseline data using a reward function that balances cognitive load and energetic cost. Four RL algorithms (TRPO, PPO, DDPG, SAC) are trained and compared. SHAP is then applied to attribute feature importance to the RL agent's decisions, providing both global and patient-level explanations. The ADNI dataset provides longitudinal patient data for training and evaluation.

## Key Results
- Only TRPO among the four RL algorithms successfully models AD progression, while PPO, DDPG, and SAC fail to produce accurate predictions.
- SHAP analysis reveals that all methods fail to properly capture the importance of amyloid accumulation, despite it being a key biomarker for AD.
- The framework successfully merges predictive accuracy with transparency, offering insights into AD progression modeling for healthcare applications.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The framework succeeds because it unifies an interpretable mechanistic model with a post-hoc explainability method, thereby enabling both accurate disease progression prediction and transparent feature attribution.
- **Mechanism:** The interpretable RL model encodes causal relationships between brain region size, amyloid accumulation, information processing, and cognition using differential equations. The RL agent optimizes a reward balancing cognitive load and energetic cost. SHAP then attributes feature importance to the RL agent's actions without requiring retraining.
- **Core assumption:** The differential equations accurately represent AD pathophysiology, and SHAP can faithfully attribute importance to RL decisions despite the black-box nature of the policy network.
- **Evidence anchors:**
  - [abstract]: "Our approach combines interpretability with explainability to provide insights into the key factors influencing AD progression, offering both global and individual, patient-level analysis."
  - [section 3]: "We use SHAP [10] for its consistency with human intuition. SHAP is a model-agnostic framework built on Shapley values [39] that has roots in cooperative game theory."
  - [corpus]: Weak. Corpus shows many AD prediction models, but few explicitly combine interpretable mechanistic models with post-hoc explainability like SHAP.

### Mechanism 2
- **Claim:** Only TRPO performs well because its trust region constraint stabilizes policy updates in the sensitive DE simulator environment.
- **Mechanism:** TRPO maximizes policy improvement under a KL-divergence constraint, ensuring that each update stays within a "trust region" of the current policy. This prevents large, destabilizing updates that would violate the DE dynamics. Other RL algorithms (PPO, DDPG, SAC) make larger, less constrained updates, leading to divergence from realistic AD trajectories.
- **Core assumption:** The DE simulator environment is highly sensitive to policy updates, such that large steps lead to unrealistic or unstable predictions.
- **Evidence anchors:**
  - [section 5]: "Results show that TRPO was able to model the cognitive decline quite closely while PPO, DDPG, and SAC failed to do so, on both MMSE and ADAS13 predictions."
  - [section 3.2]: "The RL agent aims to calculate the optimal information processing in each brain region... With λ as a parameter controlling the trade-off between the mismatch and the cost, the agent's goal is to maximize the reward."
  - [corpus]: Weak. While TRPO's stability is well-documented in RL literature, direct evidence linking its trust region constraint to DE simulator stability is absent from the corpus.

### Mechanism 3
- **Claim:** SHAP reveals that all methods fail to capture amyloid's importance because the mechanistic model does not properly encode its causal role in cognition decline.
- **Mechanism:** SHAP quantifies feature importance by measuring the marginal contribution of each input state to the RL agent's action. Despite amyloid accumulation being a known AD biomarker, the model's DEs and reward structure do not sufficiently link amyloid to cognition, so SHAP assigns it low importance. This exposes a flaw in the mechanistic model rather than the explainability method.
- **Core assumption:** Amyloid accumulation is causally important for AD progression, and its underrepresentation in the model's DEs and reward function explains the SHAP findings.
- **Evidence anchors:**
  - [section 5.2]: "The minimal feature attribution to amyloid by all RL methods seems at first surprising as it goes against some existing clinical research suggesting that the accumulation of Aβ is a significant contributor to AD progression [11]."
  - [section 6]: "Our IXRL framework further supports evidence that amyloid is more of a downstream pathologic marker and not a key cause of neurodegeneration and cognitive decline."
  - [corpus]: Weak. The corpus contains many AD prediction models, but few explicitly validate the causal role of amyloid through combined mechanistic modeling and explainability.

## Foundational Learning

- **Concept: Reinforcement Learning fundamentals**
  - Why needed here: Understanding how RL agents learn policies through interaction with an environment is crucial for grasping the model's optimization process and why certain algorithms (like TRPO) are more suitable.
  - Quick check question: What is the difference between on-policy and off-policy RL algorithms, and why might this distinction matter for stability in a DE simulator?

- **Concept: Differential equations in disease modeling**
  - Why needed here: The interpretable model relies on DEs to encode causal relationships between brain region size, amyloid, activity, and cognition. Understanding how DEs model dynamic systems is essential for interpreting the model's predictions.
  - Quick check question: How do differential equations model the propagation of amyloid between brain regions, and what assumptions underlie this modeling choice?

- **Concept: Shapley values and feature attribution**
  - Why needed here: SHAP uses Shapley values to quantify the contribution of each input feature to the model's output. Understanding this concept is key to interpreting the global and local explanations generated by the framework.
  - Quick check question: How does SHAP calculate the contribution of a feature to a model's prediction, and what are the computational challenges of applying it to high-dimensional RL outputs?

## Architecture Onboarding

- **Component map:** ADNI dataset -> Interpretable RL Model (Differential equations + RL agent) -> SHAP explainability -> Feature importance and predictions
- **Critical path:** (1) Preprocess ADNI data -> (2) Train interpretable RL model -> (3) Apply SHAP to trained model -> (4) Analyze feature importance and model predictions
- **Design tradeoffs:**
  - **Interpretability vs. Flexibility:** The mechanistic model is interpretable but may oversimplify AD complexity. More flexible models (e.g., deep learning) could capture more nuances but sacrifice transparency.
  - **SHAP vs. Other XAI Methods:** SHAP provides consistent feature attribution but can be computationally expensive for high-dimensional RL outputs. Simpler methods (e.g., feature importance) may be faster but less reliable.
- **Failure signatures:**
  - **Poor Prediction Accuracy:** If the RL agent fails to predict cognition trajectories accurately, it suggests the mechanistic model or reward function is flawed.
  - **Misleading SHAP Explanations:** If SHAP attributes high importance to irrelevant features or low importance to known biomarkers, it indicates issues with the RL model's decision-making or SHAP's applicability.
  - **Instability in Training:** If the RL agent fails to converge or produces unrealistic predictions, it suggests the DE simulator is too sensitive or the algorithm's update rule is too aggressive.
- **First 3 experiments:**
  1. **Reproduce TRPO Results:** Train the interpretable RL model with TRPO on the ADNI dataset and verify it predicts cognition trajectories accurately.
  2. **Apply SHAP to TRPO:** Generate global and local SHAP explanations for the TRPO model and analyze feature importance rankings.
  3. **Compare with Other RL Algorithms:** Train the model with PPO, DDPG, and SAC, and compare their prediction accuracy and SHAP explanations to TRPO.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do the explanations generated by SHAP change when the interpretable model incorporates additional brain regions or biomarkers?
- **Basis in paper:** [inferred] The authors note that the model focuses on two brain regions and suggest that incorporating more regions or biomarkers could be a direction for future work. They also raise the question of whether the importance of features would remain the same with a different model or reward function.
- **Why unresolved:** The current study uses a limited model scope and doesn't explore how explanations might shift with expanded feature sets or model complexity.
- **What evidence would resolve it:** Comparing SHAP explanations from models with varying numbers of brain regions or incorporating different biomarkers (e.g., tau accumulation, neuroinflammation) to see if feature importance rankings change.

### Open Question 2
- **Question:** Why does the RL model that accurately predicts AD progression fail to attribute significance to amyloid accumulation in its decision-making process?
- **Basis in paper:** [explicit] The authors note that despite amyloid being a pathological hallmark of AD, the best-performing RL method (TRPO) does not consider it an important feature. They suggest this could indicate amyloid is more of a downstream marker than a key cause of neurodegeneration.
- **Why unresolved:** The study identifies this as a potential limitation but doesn't investigate the underlying reasons for this discrepancy between clinical expectations and model behavior.
- **What evidence would resolve it:** Investigating the model's internal representations and decision pathways to understand why amyloid features don't influence predictions, or testing alternative reward functions that might better capture amyloid's role.

### Open Question 3
- **Question:** How would different XAI techniques or incorporating clinician feedback improve the interpretability and explainability of the RL model?
- **Basis in paper:** [explicit] The authors suggest that better explainability tools and insights from clinicians and domain experts could enhance understanding of AD progression modeling. They also mention it would be interesting to investigate different XAI techniques.
- **Why unresolved:** The study uses SHAP as the primary explainability method and doesn't explore alternative techniques or validate findings with clinical experts.
- **What evidence would resolve it:** Comparing SHAP explanations with those from other XAI methods (e.g., LIME, counterfactual explanations) and conducting user studies with clinicians to assess which explanations are most useful and trustworthy for clinical decision-making.

## Limitations
- The mechanistic model's reliance on differential equations may oversimplify the complex, multifactorial nature of AD pathophysiology.
- SHAP's application to high-dimensional RL outputs may introduce computational challenges and potential attribution errors.
- The study's focus on a specific subset of ADNI data (hippocampal and prefrontal cortex) may limit generalizability to other brain regions or disease stages.

## Confidence
- **High:** The comparative analysis of RL algorithms, with TRPO's superior performance well-supported by the results.
- **Medium:** The framework's effectiveness in merging interpretability and explainability, requiring further validation across diverse datasets and patient populations.
- **Low:** The conclusion regarding amyloid's role in AD progression, as it challenges existing clinical understanding and may require additional mechanistic validation.

## Next Checks
1. Test the framework on a broader range of ADNI data, including multiple brain regions and disease stages, to assess generalizability.
2. Compare the interpretable RL model's predictions with those of state-of-the-art deep learning models to quantify the trade-off between interpretability and predictive accuracy.
3. Conduct a sensitivity analysis of SHAP attributions to different feature subsets and model architectures to ensure robustness and reliability of explanations.