---
ver: rpa2
title: 'Structurally Prune Anything: Any Architecture, Any Framework, Any Time'
arxiv_id: '2403.18955'
source_url: https://arxiv.org/abs/2403.18955
tags:
- pruning
- channels
- obspa
- graph
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Structurally Prune Anything (SPA), a framework-agnostic
  neural network pruning method that can handle any architecture, framework, and training
  stage. SPA leverages ONNX to build a standardized computational graph and automatically
  detects coupled channels through mask propagation.
---

# Structurally Prune Anything: Any Architecture, Any Framework, Any Time

## Quick Facts
- arXiv ID: 2403.18955
- Source URL: https://arxiv.org/abs/2403.18955
- Authors: Xun Wang; John Rachwan; Stephan GÃ¼nnemann; Bertrand Charpentier
- Reference count: 36
- One-line primary result: SPA achieves framework-agnostic structured pruning across any architecture, framework, and training stage, with OBSPA setting state-of-the-art results without fine-tuning or calibration data.

## Executive Summary
Structurally Prune Anything (SPA) introduces a framework-agnostic approach to neural network pruning that works across any architecture, framework, and training stage. The method leverages ONNX to create standardized computational graphs and automatically detects coupled channels through mask propagation. SPA groups these channels and applies various importance scoring criteria to enable structured pruning. The authors introduce Optimal Brain SPA (OBSPA) for pruning without fine-tuning, achieving state-of-the-art results on CIFAR-10, CIFAR-100, and ImageNet without requiring calibration data.

## Method Summary
SPA converts models from any framework to ONNX format, builds a computational graph, and uses mask propagation to automatically detect coupled channels that must be pruned together. These coupled channels are grouped and importance scores are calculated using various criteria (L1, SNIP, OBS, etc.). The method supports pruning before training, after training with fine-tuning, and without fine-tuning. For the latter setting, OBSPA applies layer-wise OBS importance scores to coupled channels and uses reconstruction to update remaining weights without calibration data. The pruned model is then converted back to the original framework if needed.

## Key Results
- SPA achieves framework-agnostic pruning across diverse architectures (ResNet, MobileNet, ViT) and frameworks (PyTorch, TensorFlow, MXNet)
- OBSPA achieves state-of-the-art pruning performance without fine-tuning or calibration data on CIFAR-10, CIFAR-100, and ImageNet
- SPA maintains competitive performance to existing methods while supporting pruning at any training stage
- The method successfully handles various pruning criteria including L1, SNIP, and OBS through group-level importance estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SPA can prune models from any framework by converting them to ONNX and building a computational graph.
- Mechanism: ONNX provides a standardized computational graph representation that captures operator sequencing, data connections, and data shapes. This graph is independent of the source framework, enabling framework-agnostic pruning.
- Core assumption: ONNX conversion preserves all necessary information for structured pruning, and the computational graph contains sufficient information to detect coupled channels.
- Evidence anchors:
  - [abstract] "SPA leverages a standardized computational graph and ONNX representation to prune diverse neural network architectures without the need for manual intervention."
  - [section] "We establish a computational graph using the ONNX framework for pruning. The adoption of ONNX offers several notable advantages..."
- Break condition: If ONNX conversion fails to preserve critical structural information, or if the computational graph doesn't capture dependencies between channels.

### Mechanism 2
- Claim: SPA automatically detects coupled channels through mask propagation rules defined for ONNX operators.
- Mechanism: SPA creates a mask for a target channel and propagates it through the computational graph using predefined rules for each operator type. This identifies all channels that must be pruned together due to dimensional constraints.
- Core assumption: The mask propagation rules accurately capture all dependencies between channels across different operator types.
- Evidence anchors:
  - [section] "SPA employs a group-level importance estimation method, which groups dependent computational operators, estimates their importance, and prunes unimportant coupled channels."
  - [section] "Given the computational graph, we employ a mask propagation technique which intuitively aims at finding all the coupled channels for any target channel within any source node."
- Break condition: If new operator types are introduced that don't have propagation rules defined, or if the rules don't capture all coupling patterns.

### Mechanism 3
- Claim: SPA achieves state-of-the-art pruning without fine-tuning by using OBSPA with data-free calibration.
- Mechanism: OBSPA applies layer-wise OBS importance scores to coupled channels, then uses a reconstruction approach to update remaining weights without requiring calibration data from the original training distribution.
- Core assumption: The OBS importance scores can be aggregated across coupled channels and still provide effective pruning decisions without fine-tuning.
- Evidence anchors:
  - [abstract] "In the context of the latter, we introduce Optimal Brain SPA (OBSPA), an algorithm that achieves state-of-the-art pruning results needing neither fine-tuning nor calibration data."
  - [section] "For the more challenging pruning without fine-tuning setting, we propose a new algorithm, Optimal Brain SPA (OBSPA)."
- Break condition: If the data-free calibration approach fails to preserve model functionality, or if the layer-wise application doesn't work well for coupled channels.

## Foundational Learning

- Concept: Computational graphs and their role in neural network analysis
  - Why needed here: SPA fundamentally relies on computational graphs to detect dependencies and perform structured pruning across frameworks
  - Quick check question: What are the three types of nodes in a computational graph, and how do they differ from dependency graph nodes?

- Concept: Mask propagation for dependency detection
  - Why needed here: SPA uses mask propagation to automatically discover coupled channels that must be pruned together
  - Quick check question: How does mask propagation work for a convolution layer when you mask one output channel?

- Concept: Group-level importance estimation
  - Why needed here: SPA aggregates importance scores across coupled channels to enable structured pruning
  - Quick check question: Why is it necessary to normalize importance scores across different groups?

## Architecture Onboarding

- Component map:
  - ONNX conversion module -> Computational graph builder -> Mask propagation engine -> Group formation module -> Importance scoring system -> Pruning executor -> Framework conversion back

- Critical path:
  1. Convert source model to ONNX
  2. Build computational graph
  3. Apply mask propagation to detect coupled channels
  4. Group coupled channels
  5. Calculate importance scores using chosen criterion
  6. Prune least important channels
  7. Convert back to original framework (optional)

- Design tradeoffs:
  - ONNX vs direct framework analysis: ONNX provides framework independence but adds conversion overhead
  - Mask propagation vs manual analysis: Automatic but requires comprehensive rule definitions
  - Group-level vs individual channel pruning: Maintains structure but may be less precise
  - Data-driven vs data-free calibration: Better performance vs broader applicability

- Failure signatures:
  - Conversion errors: ONNX conversion fails or produces incorrect graph
  - Missing rules: Mask propagation doesn't handle new operator types
  - Coupling errors: Incorrectly identified coupled channels break model
  - Performance degradation: Pruned model performs significantly worse than expected
  - Framework incompatibility: Converted model doesn't work in target framework

- First 3 experiments:
  1. Convert a simple PyTorch model (e.g., ResNet-18) to ONNX and verify the computational graph structure
  2. Test mask propagation on a residual block to verify coupled channel detection
  3. Apply SPA-L1 to a pre-trained model and verify that pruned channels can be successfully removed without breaking the model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of aggregation operator (AGG) and normalization operator (NORM) in SPA's scoring function affect the final pruning performance across different architectures?
- Basis in paper: [explicit] The paper mentions that the best choice of AGG and NORM function is not fixed over different models and can be regarded as hyper-parameters that need to be tuned.
- Why unresolved: The paper does not provide empirical evidence on the impact of different AGG and NORM combinations on pruning performance.
- What evidence would resolve it: Conducting systematic experiments varying AGG and NORM operators across diverse architectures and comparing the pruning results would provide insights into their impact.

### Open Question 2
- Question: Can SPA's mask propagation method be extended to handle dynamic computational graphs, such as those found in RNNs or transformers with variable sequence lengths?
- Basis in paper: [inferred] The paper demonstrates SPA's effectiveness on static architectures like CNNs and ViTs, but does not explore its applicability to dynamic architectures.
- Why unresolved: The current mask propagation rules are designed for static computational graphs, and their extension to dynamic architectures is not explored.
- What evidence would resolve it: Implementing and evaluating SPA on dynamic architectures like RNNs or transformers with variable sequence lengths would determine its applicability.

### Open Question 3
- Question: How does the performance of OBSPA compare to other state-of-the-art data-free pruning methods on larger-scale datasets or more complex architectures?
- Basis in paper: [explicit] The paper demonstrates OBSPA's superiority over DFPC on CIFAR datasets and shows promising results on ImageNet-1k, but does not compare it to other data-free methods on larger scales.
- Why unresolved: The comparison of OBSPA to other data-free methods is limited to specific datasets and architectures.
- What evidence would resolve it: Conducting comprehensive experiments comparing OBSPA to other data-free methods on larger-scale datasets and more complex architectures would provide a clearer picture of its performance.

## Limitations
- The effectiveness of mask propagation rules for detecting coupled channels depends on comprehensive rule definitions for all operator types
- Data-free calibration in OBSPA may not preserve model functionality as well as traditional fine-tuning approaches
- ONNX conversion may fail to preserve critical structural information for certain complex or custom architectures

## Confidence
- **High confidence**: The general framework approach and computational graph methodology (Claim: SPA can prune models from any framework using ONNX) - supported by multiple experimental results across different architectures and frameworks
- **Medium confidence**: The automatic detection of coupled channels through mask propagation (Claim: SPA automatically detects coupled channels) - conceptually sound but implementation details are not fully specified
- **Medium confidence**: The state-of-the-art results achieved by OBSPA without fine-tuning (Claim: OBSPA achieves SOTA without calibration data) - results are promising but lack comparison to strong fine-tuning baselines

## Next Checks
1. **Mask Propagation Verification**: Test mask propagation on increasingly complex architectures (e.g., ResNet-50, MobileNetV2) to verify that all coupled channels are correctly identified across different operator combinations.

2. **Framework Conversion Robustness**: Attempt to convert and prune models from less common frameworks or with custom operators to identify edge cases where ONNX conversion may fail or lose critical information.

3. **Data-Free Calibration Analysis**: Compare OBSPA performance against fine-tuning baselines on the same pruned architectures to quantify the tradeoff between calibration data requirements and final model accuracy.