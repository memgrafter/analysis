---
ver: rpa2
title: 'uOttawa at LegalLens-2024: Transformer-based Classification Experiments'
arxiv_id: '2410.21139'
source_url: https://arxiv.org/abs/2410.21139
tags:
- legal
- subtask
- language
- task
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents methods for the LegalLens-2024 shared task
  focused on detecting legal violations in unstructured text and associating them
  with affected individuals. Two subtasks were addressed: Legal Named Entity Recognition
  (L-NER) and Legal Natural Language Inference (L-NLI).'
---

# uOttawa at LegalLens-2024: Transformer-based Classification Experiments

## Quick Facts
- arXiv ID: 2410.21139
- Source URL: https://arxiv.org/abs/2410.21139
- Authors: Nima Meghdadi; Diana Inkpen
- Reference count: 10
- Primary result: 86.3% F1 for L-NER and 88.25% F1 for L-NLI (validation)

## Executive Summary
This paper presents transformer-based approaches for the LegalLens-2024 shared task, addressing two subtasks: Legal Named Entity Recognition (L-NER) and Legal Natural Language Inference (L-NLI). The authors developed a DeBERTa-based model integrated with spaCy for L-NER, achieving 86.3% F1-score, and a combined RoBERTa+CNN model for L-NLI, achieving 88.25% F1 on validation data. The results demonstrate the effectiveness of transformer models in the legal domain while highlighting challenges in generalization to unseen cases. The source code is publicly available on GitHub.

## Method Summary
The authors employed two distinct approaches for the LegalLens-2024 shared task. For L-NER, they utilized the spaCy pipeline configured with a deberta-v3-base model and a transition-based parser for entity recognition. For L-NLI, they developed a combined model architecture integrating the ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli model with a custom-built CNN model for keyword detection. The models were trained on the provided LegalLens dataset with separate training, validation, and test splits for each subtask.

## Key Results
- L-NER subtask achieved 86.3% F1-score using DeBERTa-based spaCy pipeline
- L-NLI subtask achieved 88.25% F1-score on validation data using RoBERTa+CNN combined model
- L-NLI performance dropped to 72.4% F1 on hidden test set, indicating generalization challenges
- Pre-trained NLI models outperformed vanilla models and LLMs in the legal domain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DeBERTa with spaCy pipeline achieves strong NER performance in legal domain
- Mechanism: DeBERTa provides contextual embeddings while spaCy handles tokenization and entity recognition workflow
- Core assumption: Legal text requires both deep contextual understanding and robust tokenization pipeline
- Evidence anchors:
  - [abstract] "For subtask A, we utilized the spaCy library, while for subtask B, we employed a combined model incorporating RoBERTa and CNN. Our results were 86.3% in the L-NER subtask"
  - [section] "Our training utilizes the SpaCy pipeline configured with a transformer model and a transition-based parser for NER tasks. The deberta-v3-base model has been selected for the main transformer architecture"
  - [corpus] Weak - corpus neighbors don't specifically discuss spaCy integration with DeBERTa
- Break condition: Performance degrades if tokenization errors compound with contextual embedding misalignment

### Mechanism 2
- Claim: Combined RoBERTa+CNN model improves NLI classification through complementary feature extraction
- Mechanism: RoBERTa captures semantic context while CNN detects local keyword patterns for better classification
- Core assumption: Legal NLI benefits from both global context understanding and local pattern recognition
- Evidence anchors:
  - [abstract] "For subtask B, we developed a RoBERTa-based model combined with a CNN-based model" achieving "88.25% in the L-NLI subtask"
  - [section] "Our combined model architecture integrates the ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli model with a custom-built CNN model for keyword detection"
  - [corpus] Weak - corpus neighbors don't discuss RoBERTa+CNN combinations for legal NLI
- Break condition: Model performance plateaus if CNN keyword detection doesn't add complementary information beyond RoBERTa's context

### Mechanism 3
- Claim: Fine-tuning pre-trained NLI models outperforms vanilla models on legal NLI tasks
- Mechanism: Transfer learning from general NLI datasets provides better starting point than training from scratch
- Core assumption: Legal NLI task benefits from knowledge transfer rather than learning from scratch
- Evidence anchors:
  - [abstract] "Our results were 86.3% in the L-NER subtask and 88.25% in the L-NLI subtask"
  - [section] "We found that pre-trained NLI models can perform significantly better than vanilla models and LLMs. Falcon 7B and RoBERTa base are the best-performing models for LLMs and vanilla models"
  - [corpus] Weak - corpus neighbors don't provide comparative analysis of pre-trained vs vanilla models
- Break condition: Fine-tuning degrades performance if legal domain differences are too large from general NLI training data

## Foundational Learning

- Concept: Named Entity Recognition fundamentals
  - Why needed here: Understanding how NER systems identify and classify legal entities is crucial for interpreting model architecture choices
  - Quick check question: What are the four entity types in the LegalLens dataset and how do they differ from general NER tasks?

- Concept: Natural Language Inference principles
  - Why needed here: Understanding entailment, contradiction, and neutral relationships is essential for interpreting NLI task requirements
  - Quick check question: How do premise-hypothesis relationships differ in legal texts versus general domain texts?

- Concept: Transformer architecture basics
  - Why needed here: Both DeBERTa and RoBERTa are transformer-based models, understanding their architecture helps in interpreting model choices
  - Quick check question: What key architectural difference between DeBERTa and standard BERT enables better performance?

## Architecture Onboarding

- Component map: DeBERTa-based spaCy pipeline (L-NER) → RoBERTa+CNN combined model (L-NLI) → Fully connected softmax layer for classification
- Critical path: Tokenization → Contextual embedding generation → Feature extraction (CNN for L-NLI) → Classification decision
- Design tradeoffs: SpaCy integration provides robust tokenization but adds complexity; CNN addition improves keyword detection but increases model size
- Failure signatures: Low F1 scores indicate tokenization issues (L-NER) or inadequate context-keyword balance (L-NLI)
- First 3 experiments:
  1. Test baseline DeBERTa performance without spaCy integration
  2. Evaluate standalone RoBERTa vs combined RoBERTa+CNN model
  3. Test different CNN filter sizes to optimize keyword detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can transformer models be improved to better generalize to unseen legal cases and handle nuanced legal language?
- Basis in paper: [explicit] The paper states that despite using robust models, generalizing to unseen cases proved challenging, particularly with nuanced legal language.
- Why unresolved: The authors acknowledge this limitation but do not propose specific solutions beyond mentioning that more advanced methods like attention mechanisms may help.
- What evidence would resolve it: Experiments comparing different transformer architectures, attention mechanisms, and domain adaptation techniques on diverse legal datasets would provide evidence for effective generalization strategies.

### Open Question 2
- Question: What are the relative contributions of RoBERTa and CNN components in the combined model for L-NLI, and how can their integration be optimized?
- Basis in paper: [inferred] The authors mention that the CNN improved phrase detection but do not provide detailed ablation studies or analysis of each component's contribution.
- Why unresolved: The paper presents a combined model but lacks detailed analysis of individual component performance and optimal integration strategies.
- What evidence would resolve it: Detailed ablation studies, performance metrics for individual components, and experiments with different integration strategies (e.g., varying CNN filter sizes, attention mechanisms between components) would clarify their relative contributions.

### Open Question 3
- Question: How would incorporating large language models like GPT-4 affect performance on legal reasoning tasks compared to the transformer models used in this study?
- Basis in paper: [explicit] The authors suggest that future work should explore leveraging LLMs like GPT-4 to improve legal reasoning.
- Why unresolved: The paper only uses RoBERTa and DeBERTa models and does not test GPT-4 or similar LLMs, leaving this as a future direction.
- What evidence would resolve it: Direct comparison experiments using GPT-4 and other LLMs on the same LegalLens tasks would provide empirical evidence of their effectiveness for legal reasoning compared to the current transformer models.

## Limitations
- Domain transfer limitations evidenced by significant performance gap (88.25% to 72.4% F1) between validation and hidden test sets for L-NLI
- Architecture transparency gaps with underspecified CNN component details for L-NLI model
- Legal domain specificity concerns regarding how legal-specific language patterns and jurisdictional variations might affect long-term model performance

## Confidence

**High Confidence**: The core approach of using transformer models for legal domain tasks is well-supported, with the DeBERTa+spaCy combination achieving 86.3% F1 for L-NER. The general architecture design principles (combining contextual embeddings with pattern recognition) are sound.

**Medium Confidence**: The specific performance metrics are reliable for the reported validation sets, but the significant drop on hidden test data suggests medium confidence in real-world generalization. The choice of pre-trained models over vanilla alternatives is supported but could benefit from more comparative analysis.

**Low Confidence**: The CNN architecture details for L-NLI are insufficiently specified, and the analysis of why certain models (like Falcon 7B) underperform is not deeply explored. The paper doesn't adequately address potential biases in the legal dataset or how different legal domains might require different modeling approaches.

## Next Checks

1. **Generalization Test**: Evaluate the L-NLI model on cross-jurisdictional legal texts to assess how well it handles legal language variations across different regions or legal systems, addressing the validation-to-test performance gap.

2. **Architecture Ablation Study**: Systematically remove the CNN component from the L-NLI model and compare performance with standalone RoBERTa to quantify the actual contribution of keyword detection versus contextual understanding alone.

3. **Legal Entity Distribution Analysis**: Conduct a detailed error analysis on the L-NER results, specifically examining which entity types (violation, violation by, violation on, law) have the lowest precision/recall to identify systematic weaknesses in the spaCy+DeBERTa pipeline.