---
ver: rpa2
title: 'SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder'
arxiv_id: '2402.07370'
source_url: https://arxiv.org/abs/2402.07370
tags:
- face
- identity
- target
- swapping
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the instability and identity leakage issues
  in face swapping models caused by the conventional seesaw game training scheme.
  The proposed Shape Agnostic Masked AutoEncoder (SAMAE) introduces a self-supervised
  training approach that eliminates the seesaw game and leverages a clear ground truth
  through self-reconstruction.
---

# SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder

## Quick Facts
- arXiv ID: 2402.07370
- Source URL: https://arxiv.org/abs/2402.07370
- Authors: Jaeseong Lee; Junha Hyung; Sohyun Jeong; Jaegul Choo
- Reference count: 40
- Primary result: Introduces SAMAE framework that eliminates seesaw game instability and achieves state-of-the-art face swapping performance through self-supervised reconstruction

## Executive Summary
This paper presents SelfSwapper, a novel face swapping framework that addresses critical instability and identity leakage issues in conventional models through a self-supervised masked autoencoder approach. The proposed Shape Agnostic Masked AutoEncoder (SAMAE) eliminates the unstable seesaw game training by leveraging self-reconstruction with clear ground truth supervision. By masking facial regions and learning disentangled identity and non-identity features, SAMAE effectively mitigates identity leakage while maintaining high-quality attribute preservation. The framework introduces additional techniques including perforation confusion and random mesh scaling to address shape misalignment and volume discrepancy problems, establishing new state-of-the-art performance across multiple quantitative metrics.

## Method Summary
SelfSwapper introduces a self-supervised training approach that replaces the conventional seesaw game with self-reconstruction as the primary learning signal. The framework employs a Shape Agnostic Masked AutoEncoder (SAMAE) that masks facial regions during training, forcing the model to learn disentangled identity and non-identity features from partial observations. This masking strategy directly addresses identity leakage by preventing the model from relying on complete facial information during reconstruction. The method incorporates perforation confusion to handle shape misalignment and random mesh scaling to address volume discrepancy issues, creating a comprehensive solution that outperforms existing approaches in preserving both identity and non-identity attributes across various quantitative metrics.

## Key Results
- Achieves state-of-the-art performance in Identity Similarity, Identity Consistency, Head Pose, and FID scores compared to baseline methods
- Effectively mitigates identity leakage through masked reconstruction while preserving non-identity attributes
- Demonstrates superior quantitative performance across multiple evaluation metrics while maintaining stable training without seesaw game instability

## Why This Works (Mechanism)
The SAMAE framework succeeds by fundamentally restructuring the training paradigm from adversarial seesaw games to self-supervised reconstruction. By masking facial regions during training, the model is forced to learn robust identity representations from incomplete information, which naturally prevents identity leakage into the non-identity feature space. The self-reconstruction objective provides clear ground truth supervision that eliminates the instability inherent in discriminator-based training schemes. The perforation confusion technique introduces controlled shape misalignment during training, making the model robust to geometric variations, while random mesh scaling addresses volume discrepancies by teaching the model to handle different face sizes and proportions.

## Foundational Learning

3D Morphable Models (3DMM)
- Why needed: Provides the geometric foundation for face representation and manipulation in the swapping pipeline
- Quick check: Verify the model correctly extracts and applies 3DMM parameters for face alignment and warping

Masked Autoencoders
- Why needed: Enables self-supervised learning by reconstructing masked inputs, forcing feature disentanglement
- Quick check: Confirm the masking strategy effectively prevents identity leakage while maintaining reconstruction quality

Perforation Confusion
- Why needed: Introduces controlled shape misalignment during training to improve robustness to geometric variations
- Quick check: Validate that the perforation patterns create sufficient shape variation without breaking identity recognition

## Architecture Onboarding

Component Map:
Input Face -> 3DMM Alignment -> SAMAE Encoder -> Disentangled Features -> SAMAE Decoder -> Output Face

Critical Path:
The core processing pipeline follows: input face preprocessing → 3DMM-based geometric alignment → SAMAE encoding of disentangled features → masked reconstruction → final output generation. The critical path emphasizes the encoder-decoder structure where disentanglement occurs through the masking mechanism.

Design Tradeoffs:
The primary tradeoff involves masking ratio versus reconstruction quality - higher masking prevents identity leakage more effectively but may reduce reconstruction fidelity. The framework also trades computational complexity for stability by avoiding adversarial training components.

Failure Signatures:
Common failure modes include: insufficient masking leading to identity leakage, excessive masking causing poor reconstruction quality, and geometric misalignment from inaccurate 3DMM parameter estimation.

First 3 Experiments:
1. Baseline comparison with standard autoencoder without masking
2. Ablation study varying masking ratios to find optimal trade-off
3. Evaluation of shape alignment robustness with and without perforation confusion

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Theoretical justification for self-reconstruction as sufficient supervision for feature disentanglement remains underexplored
- Computational overhead from 3DMM-based warping and mesh scaling may limit practical deployment
- Limited qualitative assessment leaves room for subjective evaluation discrepancies

## Confidence

| Claim | Confidence |
|-------|------------|
| Self-reconstruction provides sufficient supervision for disentanglement | Medium |
| Masking strategy effectively prevents identity leakage | Medium |
| Perforation confusion and mesh scaling are essential components | Low-Medium |
| Establishes new state-of-the-art performance | High (based on quantitative metrics) |

## Next Checks

1. Test SAMAE's robustness to extreme occlusions and non-frontal face angles beyond current dataset scope
2. Conduct ablation studies isolating contributions of masking versus autoencoder architecture to identity disentanglement
3. Evaluate model performance when integrated with real-time or low-compute hardware for practical deployment feasibility