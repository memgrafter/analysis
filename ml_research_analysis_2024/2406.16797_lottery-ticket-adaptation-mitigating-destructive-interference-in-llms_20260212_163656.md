---
ver: rpa2
title: 'Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs'
arxiv_id: '2406.16797'
source_url: https://arxiv.org/abs/2406.16797
tags:
- lota
- task
- tasks
- arxiv
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of adapting large language models
  (LLMs) to new tasks while avoiding destructive interference between tasks, which
  can lead to catastrophic forgetting. The proposed solution, Lottery Ticket Adaptation
  (LoTA), is a sparse adaptation method that identifies and optimizes only a sparse
  subnetwork of the model, rather than modifying all the weights.
---

# Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs

## Quick Facts
- **arXiv ID**: 2406.16797
- **Source URL**: https://arxiv.org/abs/2406.16797
- **Reference count**: 40
- **Key outcome**: LoTA achieves 19.0% AlpacaEval 2 winrate for instruction following, outperforming LoRA's 15.3% while mitigating catastrophic forgetting

## Executive Summary
The paper addresses the challenge of adapting large language models to new tasks while avoiding destructive interference between tasks, which can lead to catastrophic forgetting. The proposed solution, Lottery Ticket Adaptation (LoTA), is a sparse adaptation method that identifies and optimizes only a sparse subnetwork of the model, rather than modifying all weights. LoTA works by first fine-tuning the model to calibrate a sparsity mask, then extracting this mask based on the magnitude of the updates, and finally fine-tuning the model using only the sparse subnetwork. This approach mitigates catastrophic forgetting and enables effective multi-task adaptation.

## Method Summary
LoTA identifies a sparse subnetwork ("lottery ticket") through three phases: mask calibration (normal fine-tuning), mask extraction (selecting top-magnitude weight changes), and sparse adaptation (fine-tuning only selected parameters). The method extracts a sparsity mask from the task vector by thresholding based on magnitude, then fine-tunes using only the selected weights. LoTTO extends this to sequential tasks by constraining mask extraction to exclude weights important for previous tasks, reducing interference between tasks.

## Key Results
- LoTA achieves 19.0% length-controlled AlpacaEval 2 winrate for instruction following, outperforming LoRA's 15.3%
- LoTA maintains good performance across tasks, avoiding catastrophic forgetting during multi-task adaptation
- LoTA enables efficient model merging across dissimilar tasks, achieving 38.5% task-average performance compared to 36.7% for merging FFT models

## Why This Works (Mechanism)

### Mechanism 1
LoTA reduces destructive interference by restricting weight updates to a sparse subnetwork identified via magnitude-based thresholding. During mask calibration, the model is fine-tuned normally, and the difference between fine-tuned and original weights forms a task vector. LoTA selects the largest-magnitude entries (90% sparsity) as the mask, and subsequent training updates only these entries, freezing the rest. This confines updates and limits interference between tasks requiring different parameter modifications.

### Mechanism 2
LoTA preserves task performance during sequential fine-tuning by maintaining disjoint sparse masks for each task. LoTTO extends LoTA by calibrating a mask for the new task that excludes weights important for previous tasks. This ensures new task adaptation does not overwrite parameters critical for earlier tasks, reducing catastrophic forgetting. The disjointness of masks directly limits interference.

### Mechanism 3
LoTA enables efficient model merging by training inherently sparse task vectors, eliminating the need for post-hoc sparsification. Since LoTA produces sparse task vectors during fine-tuning, merging two LoTA models requires only combining their sparse masks without additional compression. This avoids the performance degradation seen when post-hoc sparsification is applied to dense task vectors before merging.

## Foundational Learning

- **Concept**: Lottery Ticket Hypothesis (LTH)
  - Why needed here: LoTA is based on the idea that sparse subnetworks ("winning tickets") can match dense model performance. Understanding LTH explains why confining updates to a sparse subnetwork is viable.
  - Quick check question: If a subnetwork is identified by large-magnitude weight changes, what assumption must hold about the distribution of task-relevant updates?

- **Concept**: Catastrophic Forgetting in Sequential Fine-tuning
  - Why needed here: LoTA and LoTTO are designed to mitigate forgetting when adapting a model to multiple tasks in sequence. Knowing how forgetting arises clarifies why sparse, disjoint masks help.
  - Quick check question: If two tasks require updates to the same parameters, what will happen during sequential fine-tuning without interference mitigation?

- **Concept**: Model Merging via Parameter Arithmetic
  - Why needed here: LoTA's ability to merge models trained on dissimilar tasks relies on understanding how task vectors can be combined. This concept underpins the merging experiments.
  - Quick check question: Why might post-hoc sparsification degrade performance when merging dense task vectors?

## Architecture Onboarding

- **Component map**: Base model → Fine-tune (calibration) → Compute task vector → Threshold to create mask → Reset weights → Fine-tune with sparse mask
- **Critical path**: 1) Fine-tune base model for T iterations 2) Compute Δ = wF - wP 3) Threshold Δ to create mask m 4) Reset weights to wP 5) Fine-tune using only w ⊙ m
- **Design tradeoffs**: Sparsity ratio vs. performance (higher sparsity reduces interference but risks underfitting), mask calibration time vs. compute (more calibration data improves mask quality but increases cost), LoTTO mask generalization (using mask from one task on others can help forgetting but may hurt task-specific performance)
- **Failure signatures**: Performance collapse during sequential fine-tuning (likely due to overlapping masks), underfitting on new tasks (likely due to overly aggressive sparsity), poor merging results (likely due to insufficient mask disjointness)
- **First 3 experiments**: 1) Single-task fine-tuning with varying sparsity ratios (0%, 50%, 90%, 95%) to observe performance vs. interference trade-off 2) Sequential fine-tuning (Task A → Task B) comparing LoTA vs. FFT vs. LoRA to measure forgetting 3) Model merging of LoTA vs. FFT task vectors on dissimilar tasks to compare post-hoc sparsification impact

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the impact of LoTA's sparsity level on its performance across different tasks and model sizes?
- **Basis in paper**: The paper states that LoTA's performance degrades gracefully as the sparsity level increases, but it does not provide a comprehensive analysis of the impact of sparsity across different tasks and model sizes.
- **Why unresolved**: The paper only evaluates LoTA with a fixed sparsity level of 90% and does not explore the optimal sparsity level for different tasks or model sizes.
- **What evidence would resolve it**: Experiments evaluating LoTA's performance with varying sparsity levels across different tasks and model sizes.

### Open Question 2
- **Question**: How does LoTA's performance compare to other parameter-efficient fine-tuning methods, such as Prompt Tuning and Prefix Tuning?
- **Basis in paper**: The paper focuses on comparing LoTA to Full Fine-tuning (FFT) and Low-Rank Adaptation (LoRA), but it does not provide a comprehensive comparison with other parameter-efficient fine-tuning methods.
- **Why unresolved**: The paper does not evaluate LoTA against a wide range of parameter-efficient fine-tuning methods, limiting the understanding of its relative performance.
- **What evidence would resolve it**: Experiments comparing LoTA's performance to other parameter-efficient fine-tuning methods on a diverse set of tasks and model sizes.

### Open Question 3
- **Question**: How does LoTA's performance scale with the size of the model and the dataset?
- **Basis in paper**: The paper evaluates LoTA on models up to 8 billion parameters and datasets of varying sizes, but it does not provide a detailed analysis of how LoTA's performance scales with model and dataset size.
- **Why unresolved**: The paper does not investigate the relationship between LoTA's performance and the size of the model or the dataset, limiting the understanding of its scalability.
- **What evidence would resolve it**: Experiments evaluating LoTA's performance on models and datasets of different sizes, and analyzing the scaling behavior of LoTA's performance.

## Limitations
- The sparsity threshold of 90% is fixed across all tasks, but the optimal threshold likely varies by task complexity and dataset size
- While LoTA shows improvements on held-out tasks after multi-task fine-tuning, the mechanism by which disjoint masks prevent interference remains theoretical - experiments show correlation but not causation
- The LoTTO extension's effectiveness depends heavily on the assumption that different tasks use mostly non-overlapping parameters, which may not hold for closely related tasks

## Confidence

- **High confidence**: LoTA's core mechanism of magnitude-based mask extraction and sparse adaptation is clearly specified and produces measurable performance improvements over baselines on the tested tasks
- **Medium confidence**: The claim that LoTA mitigates catastrophic forgetting is supported by experimental results, but the mechanism (disjoint masks) is not directly validated through ablation studies or parameter overlap analysis
- **Medium confidence**: The model merging results show LoTA enables merging without post-hoc sparsification, but the performance comparison with alternative merging strategies is limited

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary the sparsity threshold (50%, 70%, 90%, 95%) across tasks to determine optimal values and identify breaking points where performance collapses
2. **Mask overlap quantification**: For LoTTO experiments, measure the actual parameter overlap between masks for different tasks to directly validate whether disjoint masks reduce interference
3. **Breaking condition tests**: Design experiments where tasks require overlapping parameters (e.g., fine-tuning on similar instruction sets) to test LoTTO's failure modes and identify when interference cannot be avoided through sparse adaptation