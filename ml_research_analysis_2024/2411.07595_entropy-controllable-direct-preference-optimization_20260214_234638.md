---
ver: rpa2
title: Entropy Controllable Direct Preference Optimization
arxiv_id: '2411.07595'
source_url: https://arxiv.org/abs/2411.07595
tags:
- distribution
- mode-seeking
- h-dpo
- coverage
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses a failure mode in Direct Preference Optimization
  (DPO) where minimizing reverse KL divergence can lead to mode-covering rather than
  mode-seeking behavior, potentially hurting performance. The authors propose H-DPO,
  which modifies DPO by introducing an entropy-controllable regularizer that allows
  for sharper distributions and more effective mode-seeking fitting.
---

# Entropy Controllable Direct Preference Optimization

## Quick Facts
- arXiv ID: 2411.07595
- Source URL: https://arxiv.org/abs/2411.07595
- Reference count: 31
- Key outcome: H-DPO improves DPO performance across multiple tasks by introducing entropy-controllable regularizer

## Executive Summary
The paper addresses a failure mode in Direct Preference Optimization (DPO) where minimizing reverse KL divergence can lead to mode-covering rather than mode-seeking behavior, potentially hurting performance. The authors propose H-DPO, which modifies DPO by introducing an entropy-controllable regularizer that allows for sharper distributions and more effective mode-seeking fitting. This is achieved by adjusting the coefficient α on entropy in the loss function. Experiments on a Zephyr-7B model fine-tuned with UltraFeedback show that H-DPO with α < 1 outperforms standard DPO across multiple tasks.

## Method Summary
H-DPO modifies the standard DPO loss function by introducing a coefficient α that controls the entropy term. The loss function becomes JH-DPO = Ex∼D,y∼π[r(x,y)] + αβH(π) − βH(π,πref), where α adjusts the entropy regularization. By setting α < 1, the method encourages sharper distributions that better capture individual modes of multimodal targets. The implementation requires only a minor modification to the DPO loss calculation, scaling the log probability ratio term by αβ instead of β.

## Key Results
- H-DPO with α < 1 outperforms standard DPO (α = 1) on GSM8K (28.83 vs 26.40)
- H-DPO shows improved performance on HumanEval (29.63 vs 28.77), MMLU-Pro (32.30 vs 31.83), and IFEval (60.93 vs 59.63)
- H-DPO improves coverage in pass@k evaluations and provides better control over diversity through temperature scaling
- The method is simple to implement, requiring only a minor modification to the DPO loss calculation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing reverse KL divergence can fail to achieve mode-seeking fitting because it preserves the variance of the target distribution.
- Mechanism: The reverse KL divergence integrates only over regions where the policy distribution is non-zero, and its formulation inherently preserves the variance of the target distribution. When fitting a unimodal distribution to a multimodal target, this variance preservation causes the fitted distribution to cover all modes instead of seeking a single mode.
- Core assumption: The target distribution has modes sufficiently far apart that a single mode capture would require significantly reduced variance compared to the full target distribution.
- Evidence anchors:
  - [abstract] "minimizing reverse KL divergence could fail to capture a mode of the reference distribution, which may hurt the policy's performance"
  - [section] "As shown in Figure 1, consider fitting a unimodal distribution to a multimodal distribution... reverse KL minimization can fail at mode-seeking fitting due to its nature of preserving variance"
  - [corpus] No direct corpus evidence found for this specific mechanism claim.
- Break condition: When the target distribution is unimodal or when modes are close enough that preserving variance doesn't prevent mode-seeking.

### Mechanism 2
- Claim: Adjusting the entropy coefficient α in the loss function allows control over the sharpness of the resulting policy distribution.
- Mechanism: By decomposing reverse KL into entropy and cross-entropy components, the coefficient α can be modified to either increase or decrease the entropy term. Setting α < 1 reduces entropy, leading to sharper (more deterministic) distributions that can better capture individual modes of multimodal targets.
- Core assumption: The entropy term in the loss function directly controls the variance/sharpness of the fitted distribution.
- Evidence anchors:
  - [abstract] "H-DPO, which allows for control over the entropy of the resulting policy, enhancing the distribution's sharpness"
  - [section] "we can adjust the entropy with α. The objective function for DPO with entropy adjustment is shown below: JH-DPO = Ex∼D,y∼π[r(x,y)]−βDα(π||πref) = Ex∼D,y∼π[r(x,y)] +αβH(π)−βH(π,πref)"
  - [corpus] No direct corpus evidence found for this specific mechanism claim.
- Break condition: When α is set too low, causing over-sharpening and loss of important modes.

### Mechanism 3
- Claim: Sharpened distributions trained with lower α values are more robust to temperature scaling at inference time.
- Mechanism: When a model is trained with lower α (sharper distributions), it maintains low entropy even when temperature scaling is applied at inference. This means the model can use higher temperatures without losing performance, effectively trading off between diversity and accuracy more effectively than standard DPO.
- Core assumption: Temperature scaling during inference is a common practice, and models trained with standard DPO (α=1) require lower temperatures to maintain performance.
- Evidence anchors:
  - [abstract] "By setting α less than 1, it encourages the entropy to be reduced so that achieves mode-seeking fitting more successfully"
  - [section] "In contrast, our proposed method trains the language model using an objective function aimed at sharpening the distribution, ensuring that this sharper distribution aligns with the objective function"
  - [corpus] No direct corpus evidence found for this specific mechanism claim.
- Break condition: When the inference task requires high diversity regardless of temperature scaling.

## Foundational Learning

- Concept: KL Divergence (forward and reverse)
  - Why needed here: Understanding the difference between forward and reverse KL is crucial for grasping why standard DPO might fail at mode-seeking and how H-DPO addresses this.
  - Quick check question: What is the key difference between forward KL and reverse KL divergence in terms of how they handle regions of zero probability?

- Concept: Entropy and its role in probability distributions
  - Why needed here: The core innovation of H-DPO relies on understanding how entropy affects distribution sharpness and how controlling it can improve mode-seeking behavior.
  - Quick check question: How does reducing entropy in a probability distribution affect its shape and the likelihood of generating samples from different regions?

- Concept: Reinforcement Learning from Human Feedback (RLHF) pipeline
  - Why needed here: H-DPO is positioned as an improvement over RLHF and DPO, so understanding the full RLHF pipeline helps contextualize the problem being solved.
  - Quick check question: What are the three main steps in the RLHF pipeline, and where does DPO fit in this process?

## Architecture Onboarding

- Component map: Data → Model Forward Pass → Log Probability Calculation → H-DPO Loss (with α scaling) → Backpropagation → Parameter Update
- Critical path: The modified loss calculation with α scaling is the only change from standard DPO, making the implementation straightforward
- Design tradeoffs: Lower α values (sharper distributions) improve mode-seeking but may reduce diversity. Higher α values maintain diversity but may fail to capture individual modes effectively. The optimal α depends on the specific task and desired balance between accuracy and diversity.
- Failure signatures: If α is set too low, the model may become overly deterministic and miss important modes. If α is too high, the model behaves like standard DPO and may fail at mode-seeking. Performance degradation on pass@k metrics or unexpected temperature sensitivity can indicate incorrect α settings.
- First 3 experiments:
  1. Implement H-DPO by modifying the DPO loss calculation to scale the log probability ratio term by αβ instead of β. Test with α values of 0.8, 0.9, 0.95, 1.0, 1.1, and 1.2.
  2. Evaluate each α setting on a simple multimodal distribution fitting task (like the Gaussian mixture example) to verify mode-seeking behavior.
  3. Run the full evaluation suite on GSM8K, HumanEval, MMLU-Pro, and IFEval to measure performance differences across α values.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal α value for different tasks and model sizes in H-DPO, and can it be determined automatically?
- Basis in paper: [explicit] The authors mention that "the need to adjust α is a limitation of this method, and automating the search of appropriate α values for each task can be a focus of future research"
- Why unresolved: The paper manually tunes α for each task but doesn't provide a systematic method for determining optimal values across different domains or model scales
- What evidence would resolve it: Experiments showing optimal α values across diverse tasks and model sizes, plus a proposed automated selection method with empirical validation

### Open Question 2
- Question: How does H-DPO's performance compare to other divergence-based methods like f-divergence regularization (Wang et al., 2024a) across the full spectrum of α values?
- Basis in paper: [inferred] The paper mentions that Wang et al. (2024a) uses f-divergence but doesn't directly compare H-DPO to this approach
- Why unresolved: The authors only compare H-DPO to standard DPO, leaving questions about its relative effectiveness compared to other divergence-based regularization methods
- What evidence would resolve it: Head-to-head comparison of H-DPO against f-divergence methods across multiple benchmarks and α values

### Open Question 3
- Question: Does H-DPO's entropy control mechanism lead to better performance when combined with temperature scaling during inference, and what is the optimal temperature-α relationship?
- Basis in paper: [explicit] The authors note that "even with a smaller α, diversity could be increased if a higher temperature is used" and show that α affects optimal temperature choice
- Why unresolved: While the paper demonstrates temperature-α interactions, it doesn't systematically explore the optimal temperature for each α value or the benefits of combining both techniques
- What evidence would resolve it: Comprehensive experiments mapping optimal temperature-α combinations across tasks and measuring performance gains from combined optimization

## Limitations

- Empirical results are based on a single experimental setup (Zephyr-7B on UltraFeedback), limiting generalizability
- Theoretical claims about reverse KL divergence's variance preservation properties lack rigorous mathematical proof
- The method's performance on larger models or different domains remains unknown

## Confidence

- **High Confidence**: The core claim that modifying the entropy coefficient in DPO loss allows control over distribution sharpness is well-supported by the loss function derivation and basic experiments.
- **Medium Confidence**: The claim that reverse KL divergence inherently preserves variance and causes mode-covering behavior is intuitively sound but lacks rigorous proof or comprehensive empirical validation.
- **Low Confidence**: The claim about improved temperature scaling robustness at inference time is mentioned but not empirically validated in the paper.

## Next Checks

1. **Synthetic Distribution Validation**: Test H-DPO on a controlled synthetic task where a unimodal distribution must fit a multimodal target (like the Gaussian mixture example). Measure KL divergence, entropy, and mode-seeking behavior across different α values to verify the theoretical claims about variance preservation and mode coverage.

2. **Temperature Scaling Ablation**: Conduct a systematic study of inference performance across different temperature values for models trained with various α settings. Measure accuracy, diversity metrics, and pass@k scores at temperatures ranging from 0.1 to 2.0 to validate the claim about temperature robustness and the trade-off between accuracy and diversity.

3. **Cross-Domain Generalization**: Evaluate H-DPO on at least two additional domains beyond code and reasoning (e.g., summarization, dialogue) using different base models (Llama, Gemma) and datasets. Compare performance across α values to determine if the benefits generalize beyond the specific experimental setup and identify optimal α ranges for different task types.