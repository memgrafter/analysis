---
ver: rpa2
title: 'STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start
  Active Learning'
arxiv_id: '2402.13468'
source_url: https://arxiv.org/abs/2402.13468
tags:
- learning
- active
- stencil
- data
- submodular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents STENCIL, a submodular mutual information (SMI)
  based weak supervision approach for cold-start active learning in class-imbalanced
  NLP tasks. STENCIL addresses the challenge of improving rare-class performance in
  scenarios where no initial labeled data exists by leveraging a small set of text
  exemplars of the rare class.
---

# STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning

## Quick Facts
- arXiv ID: 2402.13468
- Source URL: https://arxiv.org/abs/2402.13468
- Reference count: 17
- Improves rare-class F1 score by 17%-40% in cold-start active learning

## Executive Summary
STENCIL addresses the challenge of cold-start active learning in class-imbalanced NLP tasks by using submodular mutual information (SMI) to guide the selection of weakly labeled rare-class instances from unlabeled data. The method leverages a small set of text exemplars of the rare class to identify additional instances that are likely to belong to the rare class, which are then strongly annotated and used to train a classifier. Experiments on three text classification datasets demonstrate that STENCIL achieves 10%-18% overall accuracy improvement and 17%-40% rare-class F1 score gains compared to common active learning methods, with as few as 15-25 exemplar instances.

## Method Summary
STENCIL uses submodular mutual information maximization to select instances from unlabeled data that are semantically similar to a provided exemplar set of rare-class instances. The method employs four different SMI instantiations (FLQMI, FLVMI, LOGDETMI, GCMI) that optimize different aspects of the selection process, including cross-similarity and diversity. Selected instances are assumed to be weakly labeled as the rare class and are then strongly annotated by human experts. The approach is particularly effective in cold-start scenarios where traditional active learning methods struggle due to the lack of initial labeled data and severe class imbalance.

## Key Results
- Achieves 10%-18% improvement in overall accuracy compared to baseline active learning methods
- Improves rare-class F1 score by 17%-40% on YouTube Spam, SMS Spam, and Twitter Sentiment datasets
- Effective with small exemplar sets (15-25 instances) and single-round selection
- Different SMI variants perform best on different datasets: GCMI excels on SMS, LOGDETMI on YouTube

## Why This Works (Mechanism)

### Mechanism 1
- Claim: STENCIL improves rare-class performance by maximizing submodular mutual information between selected instances and exemplar query set.
- Mechanism: The selection process iteratively picks unlabeled instances that have high semantic similarity to rare-class exemplars, effectively creating a weakly labeled rare-class subset that is then strongly annotated.
- Core assumption: Small exemplar sets (15-25 instances) contain sufficient semantic information to guide selection of additional rare-class instances.
- Evidence anchors:
  - [abstract]: "STENCIL uses this set to guide the active learning selection by maximizing the Submodular Mutual Information between the set of selected instances and the exemplar set."
  - [section]: "Conducting selection in this way effectively returns a set of instances that are assumed to be weakly labeled as the rare class, which are then strongly labeled by an annotator."
  - [corpus]: Weak - related work focuses on general cold-start active learning but doesn't specifically address exemplar-guided selection.
- Break condition: If exemplars poorly represent the rare class distribution, selected instances may not improve rare-class performance.

### Mechanism 2
- Claim: Single-round selection achieves comparable results to multi-round methods by incorporating prior knowledge via exemplars.
- Mechanism: By using exemplars to guide initial selection, STENCIL bypasses the need for multiple rounds of selection and retraining that traditional cold-start methods require.
- Core assumption: Prior knowledge can be effectively encoded in exemplar sets that capture rare-class characteristics.
- Evidence anchors:
  - [abstract]: "STENCIL offers a sound and complimentary strategy for the cold-start round of class-imbalanced active learning"
  - [section]: "Due to the nature of cold-start scenarios, such gradual build-up is natural as the active learning strategy learns more about the data landscape before capitalizing on selecting rare-class instances. However, such capitalization can be conducted earlier in cases where prior knowledge can be incorporated"
  - [corpus]: Weak - related work discusses multi-round cold-start methods but doesn't compare single-round exemplar-guided approaches.
- Break condition: When prior knowledge is insufficient or outdated, single-round selection may perform worse than iterative approaches.

### Mechanism 3
- Claim: Different SMI instantiations (FLQMI, FLVMI, LOGDETMI, GCMI) provide complementary selection strategies based on representation and diversity.
- Mechanism: Each SMI variant optimizes different aspects - FLQMI focuses on cross-similarity between selected and exemplar sets, LOGDETMI emphasizes diversity, GCMI captures high similarity in rare instances.
- Core assumption: Different SMI functions capture different useful properties for rare-class selection.
- Evidence anchors:
  - [section]: "LOGDETMI accounts for diversity through its determinants, which is advantageous in cold-start settings where a good coverage of the data is required. GCMI's effectiveness is particularly notable in situations where rare instances exhibit high similarity, as observed in the SMS dataset."
  - [section]: "we see that the GCMI variant of STENCIL performs the best on the SMS dataset, the LOGDETMI variant performs the best on the YouTube dataset, and the LOGDETMI variant exhibits the best overall accuracy"
  - [corpus]: Weak - related work doesn't explore SMI variants for active learning.
- Break condition: If the dataset characteristics don't align with the strengths of a particular SMI variant, performance may suffer.

## Foundational Learning

- Concept: Submodular functions and their properties (monotonicity, diminishing returns)
  - Why needed here: STENCIL relies on submodular mutual information maximization, which requires understanding submodular function properties to implement greedy selection algorithms.
  - Quick check question: What is the approximation guarantee of the greedy algorithm for maximizing monotone submodular functions?

- Concept: Mutual information and its generalization via submodular functions
  - Why needed here: SMI extends Shannon entropy-based mutual information to submodular functions, allowing STENCIL to measure information overlap between selected instances and exemplars.
  - Quick check question: How does submodular mutual information differ from Shannon mutual information in terms of optimization properties?

- Concept: Active learning principles and cold-start scenarios
  - Why needed here: Understanding why traditional active learning methods struggle with class imbalance and cold-start settings explains STENCIL's design motivation.
  - Quick check question: What are the key challenges in applying active learning to class-imbalanced datasets?

## Architecture Onboarding

- Component map:
  - Unlabeled dataset -> Featurization (GloVe embeddings) -> Similarity computation -> SMI instantiation -> Greedy selection -> Annotated instances -> LSTM classifier -> Performance evaluation

- Critical path:
  1. Load unlabeled data and exemplars
  2. Compute embeddings and similarity matrices
  3. Instantiate chosen SMI function
  4. Run greedy selection algorithm
  5. Annotate selected instances
  6. Fine-tune model on labeled data
  7. Evaluate performance

- Design tradeoffs:
  - Single-round vs multi-round selection: Faster but requires good exemplars
  - Exemplar size vs performance: Diminishing returns beyond 15-25 instances
  - SMI variant choice: Different variants optimize for different properties (representation vs diversity)

- Failure signatures:
  - Poor rare-class performance: Exemplars don't represent true rare-class distribution
  - Low overall accuracy: Selected instances don't cover general data distribution well
  - High variance across runs: Embedding/similarity computation instability or small dataset size

- First 3 experiments:
  1. Run STENCIL with GCMI variant on SMS dataset using default exemplar set
  2. Compare STENCIL performance against baseline methods (Random, Entropy, Badge)
  3. Test STENCIL with different exemplar set sizes (20%, 40%, 60%, 80%, 100%) to observe diminishing returns

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the experimental results and discussion, several important questions emerge:

1. How do different submodular mutual information functions (FLVMI, FLQMI, LOGDETMI, GCMI) perform across different types of class imbalance distributions and dataset characteristics?

2. What is the optimal size and composition of the exemplar query set for maximizing STENCIL's performance?

3. How does STENCIL's performance compare to traditional active learning methods that use multiple rounds of selection and retraining in the cold-start setting?

## Limitations

- Results are specific to text classification and may not generalize to other NLP tasks or domains
- The effectiveness heavily depends on the quality and representativeness of the exemplar query sets
- Single-round selection assumes good prior knowledge, which may not always be available in practice

## Confidence

Medium - The experimental results are well-documented and show consistent improvements across three datasets, but the method has only been tested on text classification tasks and relies heavily on the quality of exemplar sets.

## Next Checks

1. Test STENCIL on additional NLP tasks (e.g., relation extraction, named entity recognition) to assess domain generalizability
2. Evaluate performance with different embedding methods (BERT, RoBERTa) to check embedding sensitivity
3. Conduct ablation studies on exemplar set size and quality to quantify their impact on final performance