---
ver: rpa2
title: 'Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws'
arxiv_id: '2410.11820'
source_url: https://arxiv.org/abs/2410.11820
tags:
- data
- training
- loss
- arxiv
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an online data selection algorithm for pretraining
  large language models, which dynamically adjusts the data mixture over different
  domains based on domain-specific scaling laws that predict the learning potential
  of each domain. The method, called Adaptive Data Optimization (ADO), does not require
  additional computational steps or proxy models, making it more scalable and easier
  to integrate into existing training pipelines.
---

# Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws

## Quick Facts
- arXiv ID: 2410.11820
- Source URL: https://arxiv.org/abs/2410.11820
- Reference count: 40
- Primary result: ADO achieves comparable or better performance than prior methods on pretraining LLMs while maintaining computational efficiency across different model scales

## Executive Summary
This paper introduces Adaptive Data Optimization (ADO), an online data selection algorithm for pretraining large language models. ADO dynamically adjusts the data mixture over different domains based on domain-specific scaling laws that predict the learning potential of each domain. Unlike prior methods that rely on proxy models or additional computational steps, ADO fits per-domain scaling laws online during training and uses these to guide data selection decisions. The method is evaluated on the Pile dataset and shows improved validation loss on higher-quality datasets like SlimPajama and FineWeb, while maintaining computational efficiency across different model scales.

## Method Summary
ADO operates by fitting per-domain scaling laws online during training to estimate each domain's learning potential. Every 1,000 training steps starting from step 5,000, ADO performs grid search to find optimal parameters (α, β, ε) for the scaling law equation. It then calculates credit assignment scores for each domain based on the difference between observed and predicted losses, normalizes these scores, and uses a clipping function to ensure minimum probability for each domain. The resulting probabilities determine the data mixture for the next training period. The method uses a Transformer-based architecture with SwiGLU MLP layers, RMS normalization, and rotary embeddings, trained at 124M and 1.3B parameter scales.

## Key Results
- ADO improves validation loss on SlimPajama and FineWeb datasets compared to prior methods
- The method maintains computational efficiency by avoiding proxy models and additional computational steps
- Performance improvements are demonstrated across different model scales (124M and 1.3B parameters)
- Zero-shot downstream performance on benchmarks like HellaSwag, WinoGrande, and PIQA shows competitive results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ADO's dynamic data mixture adjustment improves validation loss on higher-quality datasets (SlimPajama, FineWeb) while maintaining computational efficiency.
- Mechanism: ADO fits per-domain scaling laws online during training, estimating each domain's learning potential through credit assignment scores. This allows the algorithm to allocate more training data to domains that offer higher learning potential at any given training stage.

### Mechanism 2
- Claim: ADO achieves computational efficiency by avoiding proxy models and additional computational steps.
- Mechanism: Instead of using separate proxy models for data selection, ADO integrates data selection directly into the main training loop through online scaling law fitting. This eliminates the need for additional forward passes or model training steps while still providing adaptive data selection capabilities.

## Foundational Learning

### Scaling Laws
- Why needed: Scaling laws predict how loss decreases with training steps for each domain, enabling estimation of learning potential
- Quick check: Verify that loss curves follow predictable patterns that can be captured by the scaling law equation

### Online Parameter Fitting
- Why needed: Real-time adaptation to changing learning dynamics during training requires continuous updating of scaling law parameters
- Quick check: Confirm that parameter updates lead to improved loss predictions over time

### Credit Assignment
- Why needed: To quantify the value of each domain's contribution to overall learning progress
- Quick check: Ensure credit scores correlate with actual performance improvements when domains are emphasized

## Architecture Onboarding

### Component Map
- Training loop -> Data sampling -> Domain loss tracking -> Scaling law fitting -> Credit assignment -> Probability normalization -> Next period data mixture

### Critical Path
The critical path involves the online fitting of scaling laws every 1,000 steps, which directly influences the data mixture for the subsequent training period. The accuracy of scaling law predictions determines the quality of credit assignment scores and ultimately the effectiveness of data selection.

### Design Tradeoffs
- Frequency of scaling law fitting vs. computational overhead
- Complexity of scaling law model vs. fitting accuracy
- Minimum probability clipping vs. flexibility in data mixture adjustment
- Grid search resolution vs. fitting time

### Failure Signatures
- Poor scaling law forecasts due to incorrect parameter initialization or bounds
- Inadequate data mixture adjustment leading to suboptimal performance
- Overfitting to specific domains due to insufficient exploration of the data space
- Computational bottlenecks from frequent scaling law fitting

### 3 First Experiments
1. Verify scaling law fitting accuracy by comparing predicted vs. actual loss curves
2. Test credit assignment score calculation with controlled domain performance variations
3. Validate probability normalization and clipping by monitoring domain selection frequencies

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset specificity limits generalizability beyond the Pile dataset
- Implementation complexity requires careful tuning of multiple components
- Computational overhead from frequent online scaling law fitting
- Limited testing on domains not represented in the training data

## Confidence

**High Confidence**: ADO achieves comparable or better performance than prior methods on the Pile dataset with improved validation loss on SlimPajama and FineWeb.

**Medium Confidence**: ADO's scalability and ease of integration into existing training pipelines, while plausible due to the elimination of proxy models, is tempered by implementation complexity and computational overhead considerations.

**Low Confidence**: Generalizability of ADO to datasets beyond the Pile and performance on underrepresented domains remains uncertain due to limited experimental validation.

## Next Checks

1. **Scaling Law Fitting Sensitivity Analysis**: Conduct experiments to determine how sensitive ADO's performance is to the frequency of scaling law fitting and the grid search ranges for hyperparameters (α, β, ε). Test different fitting frequencies and compare the impact on validation loss and downstream performance.

2. **Cross-Dataset Generalization**: Evaluate ADO on datasets with different domain compositions, such as C4, Common Crawl, or domain-specific corpora. Compare performance to both random data selection and fixed data mixture baselines to assess whether ADO's advantages transfer beyond the Pile dataset.

3. **Implementation Detail Verification**: Reproduce ADO with detailed logging of the credit assignment score calculation, probability clipping, and scaling law fitting process. Verify that the implementation matches the intended mechanism by comparing domain probability trajectories against expected learning potential patterns.