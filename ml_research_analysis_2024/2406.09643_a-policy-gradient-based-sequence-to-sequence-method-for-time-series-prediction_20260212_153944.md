---
ver: rpa2
title: A Policy Gradient-Based Sequence-to-Sequence Method for Time Series Prediction
arxiv_id: '2406.09643'
source_url: https://arxiv.org/abs/2406.09643
tags:
- e-02
- e-01
- prediction
- e-03
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the exposure bias and error accumulation problems
  in sequence-to-sequence (S2S) models for multi-step-ahead time series prediction.
  The authors propose a novel policy gradient-based S2S (PG-S2S) method that uses
  reinforcement learning to dynamically select the most beneficial inputs for the
  decoder from a pool of auxiliary models and the decoder itself.
---

# A Policy Gradient-Based Sequence-to-Sequence Method for Time Series Prediction

## Quick Facts
- arXiv ID: 2406.09643
- Source URL: https://arxiv.org/abs/2406.09643
- Authors: Qi Sima; Xinze Zhang; Yukun Bao; Siyue Yang; Liang Shen
- Reference count: 40
- Key outcome: PG-S2S outperforms conventional training methods with lower RMSE, MAPE, and SMAPE across five datasets and different RNN structures.

## Executive Summary
This paper addresses exposure bias and error accumulation in sequence-to-sequence models for multi-step-ahead time series prediction by proposing a novel policy gradient-based approach. The method uses reinforcement learning to dynamically select the most beneficial inputs for the decoder from a pool of auxiliary models and the decoder itself, rather than relying on fixed strategies like teacher forcing. Experimental results demonstrate significant performance improvements over conventional training methods across various real-world and simulated datasets.

## Method Summary
The proposed PG-S2S method constructs an auxiliary model pool with well-trained models to produce input candidates, then uses a trainable policy network optimized via policy gradients to select the best inputs for maximizing long-term prediction performance. The approach employs asynchronous training where RNN parameters are frozen during policy updates and vice versa, preventing instability. An ϵ-greedy exploration strategy helps the agent avoid local optima during training by balancing exploitation of learned policies with random exploration.

## Key Results
- PG-S2S achieves lower RMSE, MAPE, and SMAPE compared to teacher forcing, scheduled sampling, and professor forcing across all tested datasets
- The method generalizes well across different RNN structures including LSTM and GRU
- Performance improvements are consistent across various time series types (temperature, PM2.5, electricity demand, etc.) and prediction horizons

## Why This Works (Mechanism)

### Mechanism 1
Reinforcement learning dynamically selects more accurate inputs than fixed strategies like teacher forcing or free running. The agent observes decoder hidden states and selects from auxiliary models or the decoder itself, maximizing cumulative reward defined by prediction accuracy and model rank. Core assumption: The agent can learn to discern when auxiliary models outperform the decoder, even without knowing future true values.

### Mechanism 2
Asynchronous training prevents policy updates from destabilizing RNN parameter updates, enabling stable learning. During policy gradient updates, RNN parameters are frozen; during RNN updates, policy parameters are frozen, alternating optimization. Core assumption: The policy and RNN objectives are sufficiently decoupled that alternating updates converge to a good solution.

### Mechanism 3
Policy gradient learning with ϵ-greedy exploration avoids local optima in input selection policy. During training, the agent selects actions based on learned policy with probability (1-ϵ) and random actions with probability ϵ, encouraging exploration. Core assumption: Random exploration is sufficient to discover better input selection strategies than greedy exploitation alone.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The input selection problem is modeled as a sequential decision problem where each action (selecting a model) affects future states and rewards.
  - Quick check question: What are the four components of an MDP, and how are they defined in this paper's context?

- Concept: Policy Gradient Methods
  - Why needed here: The agent's policy is parameterized and optimized using gradient ascent to maximize expected cumulative reward.
  - Quick check question: How does the REINFORCE algorithm update policy parameters, and what role does the reward-to-go play?

- Concept: Exposure Bias
  - Why needed here: The paper explicitly addresses exposure bias as a key motivation for the proposed approach.
  - Quick check question: What is exposure bias, and how does teacher forcing create this problem during inference?

## Architecture Onboarding

- Component map: Encoder RNN → Policy Network → Decoder RNN → Reward Calculation; Auxiliary Models (MSVR, MLP) provide input candidates
- Critical path: Historical observations → Encoder → Hidden state → Policy Network → Model selection → Input to Decoder → Prediction → Reward calculation
- Design tradeoffs: Auxiliary models add complexity but provide better inputs; asynchronous training stabilizes learning but requires careful hyperparameter tuning; ϵ-greedy exploration balances exploitation and exploration
- Failure signatures: Poor performance on validation data despite good training performance (overfitting to training policy); agent consistently selecting same model regardless of hidden state (policy not learning); reward not improving over training epochs (policy not learning)
- First 3 experiments:
  1. Train with only teacher forcing and free running baselines to establish baseline performance
  2. Implement PG-S2S with one auxiliary model and compare against baselines
  3. Add second auxiliary model and tune ϵ-greedy exploration rate to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of PG-S2S compare to ensemble methods that combine multiple auxiliary models without reinforcement learning? The paper mentions that PG-S2S outperforms "Teach_MSVR" and "Teach_MLP" methods, but doesn't compare against ensemble methods that combine multiple auxiliary models without RL.

### Open Question 2
How sensitive is the PG-S2S performance to the choice and number of auxiliary models in the pool? While the paper acknowledges variability in model performance, it doesn't systematically explore how different combinations or numbers of auxiliary models affect PG-S2S performance.

### Open Question 3
How does the computational complexity of PG-S2S compare to traditional training methods during inference? The paper focuses on training performance improvements but doesn't discuss inference-time computational overhead from the policy network and auxiliary model pool.

## Limitations

- Asynchronous training approach lacks direct corpus support for its effectiveness in this specific context
- Reliance on ϵ-greedy exploration for avoiding local optima is weakly supported by related literature
- Paper doesn't specify key hyperparameters like exploration rate schedule or reward function balancing parameter

## Confidence

- **High Confidence**: The overall methodology of using policy gradients for input selection is well-established in RL literature and the problem formulation (MDP for input selection) is sound
- **Medium Confidence**: The asynchronous training approach and its claimed benefits for stabilizing learning, as well as the effectiveness of ϵ-greedy exploration in this specific context
- **Low Confidence**: The specific implementation details of the reward function components and their optimal weighting, as well as the scalability of the approach to larger model pools

## Next Checks

1. Conduct ablation studies to isolate the contribution of asynchronous training versus synchronous training with policy gradient updates
2. Test the sensitivity of performance to different exploration rate schedules (linear decay, exponential decay) and compare with other exploration strategies like Boltzmann exploration
3. Validate the method on a larger and more diverse set of time series datasets, including those with longer prediction horizons and higher noise levels, to assess robustness and generalizability