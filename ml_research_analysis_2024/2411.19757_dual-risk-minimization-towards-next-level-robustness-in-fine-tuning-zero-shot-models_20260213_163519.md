---
ver: rpa2
title: 'Dual Risk Minimization: Towards Next-Level Robustness in Fine-tuning Zero-Shot
  Models'
arxiv_id: '2411.19757'
source_url: https://arxiv.org/abs/2411.19757
tags:
- features
- performance
- fine-tuning
- concept
- descriptions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes dual risk minimization (DRM) to improve robustness
  of fine-tuned zero-shot vision models. DRM combines empirical risk minimization
  (ERM) with worst-case risk minimization (WRM) using concept descriptions from LLMs
  as proxies for core visual features.
---

# Dual Risk Minimization: Towards Next-Level Robustness in Fine-tuning Zero-Shot Models

## Quick Facts
- arXiv ID: 2411.19757
- Source URL: https://arxiv.org/abs/2411.19757
- Reference count: 38
- Outperforms state-of-the-art baselines on ImageNet (75.9→77.1), WILDS-iWildCam (47.1→51.8), and WILDS-FMoW (50.7→53.1)

## Executive Summary
This paper addresses the challenge of improving robustness in fine-tuned zero-shot vision models. The authors propose dual risk minimization (DRM), a framework that combines empirical risk minimization (ERM) with worst-case risk minimization (WRM). DRM leverages concept descriptions generated by LLMs to capture core visual features and balance expected and worst-case performance. The method significantly outperforms existing approaches on standard benchmarks, establishing new state-of-the-art results in out-of-distribution generalization.

## Method Summary
DRM combines ERM and WRM using dual prompts: default text prompts for standard classification and concept descriptions for worst-case risk estimation. GPT-4 generates concept descriptions focusing on visual features (e.g., "a large, tawny cat with a muscular build"). These descriptions are encoded and used to create soft labels that pull image representations toward core features during training. Affinity normalization between images and concept descriptions creates balanced soft labels. The model is fine-tuned using a weighted combination of ERM and WRM losses, with predictions combined at inference.

## Key Results
- Improves ImageNet robustness (75.9→77.1 accuracy)
- Increases WILDS-iWildCam performance (47.1→51.8 accuracy)
- Enhances WILDS-FMoW accuracy (50.7→53.1 accuracy)
- Establishes new state-of-the-art on all tested benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual risk minimization improves robustness by balancing expected performance with worst-case performance across domains.
- Mechanism: DRM combines empirical risk minimization (ERM) with worst-case risk minimization (WRM). ERM uses default prompts for standard classification, while WRM uses concept descriptions to estimate and minimize worst-case risk via soft labels that represent core visual features.
- Core assumption: Core visual features are essential for robust predictions across domains, and preserving them improves out-of-distribution performance.
- Evidence anchors:
  - [abstract] "DRM balances two crucial aspects of model robustness: expected performance and worst-case performance"
  - [section 3] "we propose a new principle called dual risk minimization (DRM) which combines ERM with worst-case risk minimization (WRM)"
  - [corpus] Weak - related work mentions distortion risk measures but not directly applicable to DRM
- Break condition: If concept descriptions fail to accurately capture core features or if worst-case risk estimation is unreliable.

### Mechanism 2
- Claim: Concept descriptions from LLMs serve as effective proxies for core visual features that define class identity.
- Mechanism: GPT-4 generates short text descriptions focusing on visual features (e.g., "a large, tawny cat with a muscular build"). These descriptions are encoded and used to create soft labels that pull image representations toward core features during training.
- Core assumption: LLM-generated descriptions accurately capture the essential visual characteristics of each class without including context or environment.
- Evidence anchors:
  - [abstract] "We utilize core-feature descriptions generated by LLMs to induce core-based zero-shot predictions"
  - [section 4.1] "The description for cougar, for instance, is 'a large, tawny cat with a muscular build and a small head.'"
  - [corpus] Weak - related work on language models but not specifically on vision-language feature preservation
- Break condition: If LLM descriptions include non-core features or if the text encoder fails to map descriptions to appropriate visual representations.

### Mechanism 3
- Claim: Affinity normalization between images and concept descriptions creates reliable soft labels for worst-case risk estimation.
- Mechanism: Raw affinities between images and concept descriptions contain artifact terms that vary across classes. Min-max normalization per class reduces this variation, creating balanced soft labels where each class has at least one image with maximum affinity.
- Core assumption: Without normalization, artifact terms would create imbalanced soft labels that harm learning.
- Evidence anchors:
  - [section 4.2] "To mitigate the impact of artifact terms, we perform a simple min-max normalization on ξ(x, y) = exp(Aθ0(x, tcd y )/τ)"
  - [section 4.2] "This effectively adjusts the affinity range of each class, reducing the difference in the artifact terms of different classes"
  - [corpus] Weak - no direct corpus evidence for affinity normalization technique
- Break condition: If normalization fails to adequately reduce artifact term variation or if the normalization itself introduces bias.

## Foundational Learning

- Concept: Dual risk minimization (DRM) - combining ERM with WRN for balanced robustness
  - Why needed here: Single-risk approaches either overfit to spurious correlations (ERM) or sacrifice too much accuracy for worst-case performance (WRM)
  - Quick check question: Why can't we just use ERM or just use WRM for robust fine-tuning?

- Concept: Concept descriptions as feature representations
  - Why needed here: Standard text prompts include context and environment, but DRM needs prompts that isolate core visual features
  - Quick check question: How do concept descriptions differ from standard text prompts used in zero-shot classification?

- Concept: Affinity normalization for soft label creation
  - Why needed here: Raw image-text affinities contain artifact terms that vary across classes, creating imbalanced learning targets
  - Quick check question: What problem does min-max normalization solve in the context of creating soft labels from image-text affinities?

## Architecture Onboarding

- Component map: CLIP model (image encoder + text encoder) → dual prompts (default + concept descriptions) → dual classifiers → DRM loss (ERM + WRM regularization) → combined inference
- Critical path: Image → Image encoder → Default prompt text embedding → Classifier → Prediction; Image → Image encoder → Concept description text embedding → Soft labels → WRM regularization
- Design tradeoffs: Using LLM-generated concept descriptions adds dependency on external models but provides better feature preservation; dual classifiers increase inference cost but improve robustness
- Failure signatures: Poor OOD performance despite good ID performance suggests inadequate worst-case risk minimization; poor ID performance suggests over-regularization from WRM
- First 3 experiments:
  1. Train with only ERM component (λ=0) to establish baseline performance
  2. Train with only WRM component (λ→∞) to assess worst-case performance without ERM
  3. Train with DRM using different λ values to find optimal balance between expected and worst-case performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DRM perform when the LLM-generated concept descriptions contain significant errors or omissions?
- Basis in paper: [inferred] The paper acknowledges that GPT-4 may not always generate useful concept descriptions, particularly in specialized domains like medical imaging, and that the vast number of descriptions makes manual verification impractical.
- Why unresolved: The paper only tested DRM with GPT-4-generated descriptions and did not evaluate performance when descriptions were intentionally corrupted or when using descriptions from different sources or manual creation.
- What evidence would resolve it: Experiments comparing DRM performance using accurate vs. corrupted concept descriptions, or using descriptions from domain experts versus LLM-generated ones.

### Open Question 2
- Question: What is the theoretical relationship between the λ parameter in DRM and the trade-off between expected and worst-case performance?
- Basis in paper: [explicit] The paper states that DRM balances expected and worst-case performance and that λ controls this balance, but provides only empirical evidence showing DRM is "fairly insensitive" to λ choices without theoretical analysis.
- Why unresolved: The paper provides empirical results showing performance across different λ values but does not offer theoretical justification for why λ has this particular effect or what the optimal relationship should be.
- What evidence would resolve it: Theoretical analysis connecting λ to the trade-off between expected and worst-case risks, possibly through bounds or convergence guarantees.

### Open Question 3
- Question: How does DRM's performance degrade when applied to CLIP models pre-trained on domain-specific data versus general image-text pairs?
- Basis in paper: [inferred] The paper notes that CLIP models may not perform optimally across all domains, particularly in less common areas, and that DRM's effectiveness depends on the pre-training data breadth.
- Why unresolved: All experiments used general CLIP models pre-trained on broad image-text pairs, with no evaluation on CLIP models fine-tuned for specific domains or trained from scratch on domain-specific data.
- What evidence would resolve it: Experiments comparing DRM performance on domain-specific CLIP models versus general CLIP models across various specialized domains.

## Limitations
- Reliance on GPT-4 for concept descriptions introduces dependency on external LLM performance
- Concept descriptions not provided, making exact reproduction difficult
- Affinity normalization technique lacks detailed validation across different datasets
- Assumption about LLM descriptions capturing core features not systematically validated

## Confidence

- **High Confidence**: The empirical results showing DRM's improvement over baselines on multiple benchmarks (ImageNet, WILDS-iWildCam, WILDS-FMoW). The dual risk minimization framework is logically sound and well-motivated.

- **Medium Confidence**: The mechanism by which concept descriptions from LLMs capture core visual features. While the performance gains support this claim, the paper does not provide systematic analysis of what makes descriptions effective.

- **Low Confidence**: The robustness of the affinity normalization approach across different datasets and whether it generalizes beyond the specific implementation details used in this paper.

## Next Checks

1. **Concept Description Analysis**: Systematically evaluate the quality of GPT-4 generated concept descriptions by comparing them to human annotations and testing whether they consistently capture core visual features across diverse classes.

2. **Cross-Dataset Generalization**: Apply DRM to additional vision datasets with different characteristics (e.g., medical imaging, satellite imagery) to test whether the dual risk minimization approach generalizes beyond the tested benchmarks.

3. **Ablation of Normalization Technique**: Remove the affinity normalization step and evaluate its impact on performance across different λ values to quantify its contribution to DRM's effectiveness.