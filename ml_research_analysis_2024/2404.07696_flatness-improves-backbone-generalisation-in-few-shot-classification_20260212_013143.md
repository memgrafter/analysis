---
ver: rpa2
title: Flatness Improves Backbone Generalisation in Few-shot Classification
arxiv_id: '2404.07696'
source_url: https://arxiv.org/abs/2404.07696
tags:
- backbone
- training
- learning
- fine-tuning
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using sharpness-aware training objectives (SAM)
  for backbone training in few-shot classification (FSC). The authors theoretically
  show that flat minima lead to better generalization in FSC by bounding the expected
  generalization gap on target domains.
---

# Flatness Improves Backbone Generalisation in Few-shot Classification

## Quick Facts
- arXiv ID: 2404.07696
- Source URL: https://arxiv.org/abs/2404.07696
- Authors: Rui Li; Martin Trapp; Marcus Klasson; Arno Solin
- Reference count: 40
- This paper proposes using sharpness-aware training objectives (SAM) for backbone training in few-shot classification (FSC)

## Executive Summary
This paper investigates the impact of backbone training on few-shot classification performance, proposing that flatness of minima in the loss landscape is crucial for good generalization. The authors theoretically demonstrate that flat minima lead to better generalization in FSC by bounding the expected generalization gap on target domains. They introduce a training protocol combining SAM-based backbone training with fine-tuning, which achieves state-of-the-art results on the Meta-Dataset benchmark, outperforming existing methods in 10 out of 13 domains.

## Method Summary
The authors propose using Sharpness-Aware Minimization (SAM) for backbone training in few-shot classification. SAM minimizes both the loss value and the sharpness of the loss landscape, leading to flatter minima that generalize better. The theoretical analysis shows that flat minima reduce the generalization gap between source and target domains. Empirically, SAM-trained backbones are evaluated with various adaptation methods including fine-tuning, knowledge distillation, and late fusion. The complete training protocol combines SAM-based backbone training with fine-tuning as an information fusion strategy, which performs competitively against more complex methods.

## Key Results
- SAM-trained backbones outperform ERM-trained ones across multiple adaptation methods
- Fine-tuning is shown to be an effective information fusion strategy, competitive with knowledge distillation and late fusion
- The complete training protocol achieves state-of-the-art results on Meta-Dataset benchmark, outperforming existing methods in 10 out of 13 domains
- The approach is conceptually simpler than many existing few-shot learning methods while maintaining superior performance

## Why This Works (Mechanism)
The paper establishes a theoretical connection between flatness of minima and generalization in few-shot classification. Flat minima are less sensitive to small perturbations in the input or model parameters, making the learned representations more robust to domain shifts between source and target tasks. In few-shot classification, where only a few examples are available for adaptation, this robustness is crucial for preventing overfitting to the limited training data. The sharpness-aware training objective explicitly optimizes for flatness, leading to better generalization when adapting to new tasks.

## Foundational Learning
- **Sharpness-Aware Minimization (SAM)**: An optimization technique that minimizes both the loss value and the sharpness of the loss landscape. Needed to find flatter minima that generalize better. Quick check: Verify that SAM requires computing gradients twice per iteration (at current weights and perturbed weights).
- **Few-shot Classification (FSC)**: A learning paradigm where models must classify examples from new classes with only a few labeled examples per class. Needed to evaluate backbone generalization. Quick check: Confirm that typical FSC settings use 1-5 shots per class.
- **Meta-Dataset Benchmark**: A collection of diverse image datasets used to evaluate few-shot learning methods. Needed for comprehensive evaluation across multiple domains. Quick check: Verify that Meta-Dataset contains 10+ diverse image classification datasets.

## Architecture Onboarding

Component Map:
Pre-training Dataset -> Backbone Network -> SAM Optimization -> Feature Extractor -> Adaptation Method -> Classification Head

Critical Path:
Backbone pre-training (with SAM) -> Feature extraction -> Adaptation (fine-tuning/knowledge distillation/late fusion) -> Classification

Design Tradeoffs:
- SAM vs ERM: SAM provides better generalization but requires twice the computation per iteration
- Fine-tuning vs Knowledge Distillation: Fine-tuning is simpler but may require more careful hyperparameter tuning
- Single-stage vs Multi-stage training: The proposed method uses a two-stage approach (backbone pre-training + adaptation) which separates representation learning from task-specific adaptation

Failure Signatures:
- Poor performance on target domains may indicate that the source and target domains are too dissimilar
- Overfitting during fine-tuning can occur when the number of shots is extremely limited
- Computational overhead from SAM may be prohibitive for very large-scale applications

3 First Experiments:
1. Train a ResNet backbone on MiniImageNet using both SAM and ERM, then evaluate on 5-way 5-shot tasks
2. Compare fine-tuning, knowledge distillation, and late fusion on a multi-domain FSC benchmark
3. Analyze the effect of backbone pre-training on target domain performance using linear evaluation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications for future research are evident from their findings.

## Limitations
- The theoretical analysis assumes the target domain is close to the source domain, which may not always hold in practice
- SAM requires twice the computation per iteration compared to standard ERM training, increasing training time
- The paper does not extensively explore memory usage and computational efficiency during inference for different fusion strategies

## Confidence
High: SAM's effectiveness across different adaptation methods and domains
Medium: Theoretical analysis connecting flatness to generalization
Medium: Fine-tuning as a strong baseline for information fusion in multi-domain settings

## Next Checks
1. Test SAM-trained backbones on out-of-distribution few-shot tasks to assess robustness beyond the assumption of close target domains
2. Conduct an ablation study on the computational trade-offs between SAM and ERM training, including training time and memory usage
3. Compare the proposed approach with more recent few-shot learning methods that incorporate advanced techniques like self-supervised learning or meta-learning