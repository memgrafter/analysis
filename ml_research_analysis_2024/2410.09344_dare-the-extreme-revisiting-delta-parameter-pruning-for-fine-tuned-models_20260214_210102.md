---
ver: rpa2
title: 'DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned Models'
arxiv_id: '2410.09344'
source_url: https://arxiv.org/abs/2410.09344
tags:
- pruning
- dare
- performance
- darex-q
- regularization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of delta-parameter pruning (DPP),
  which aims to reduce the storage size of fine-tuned models by pruning the differences
  between fine-tuned and pre-trained model weights. The authors focus on improving
  the DARE method, a randomized DPP approach that struggles with high pruning rates
  or large delta parameters.
---

# DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned Models

## Quick Facts
- arXiv ID: 2410.09344
- Source URL: https://arxiv.org/abs/2410.09344
- Reference count: 40
- Primary result: DAREx-q and DAREx-L2 methods significantly improve delta-parameter pruning performance, especially at high pruning rates (>30%)

## Executive Summary
This paper addresses delta-parameter pruning (DPP), which reduces storage size of fine-tuned models by pruning differences between fine-tuned and pre-trained weights. The authors improve upon the DARE method by introducing DAREx-q, which modifies the rescaling factor for better high-rate performance, and DAREx-L2, which combines DARE with AdamR L2 regularization. The work demonstrates that importance-based pruning can outperform random-based methods when delta parameters are large, with particular success in decoder models and at extreme pruning rates up to 99%.

## Method Summary
The paper introduces two key improvements to the DARE delta-parameter pruning method. DAREx-q modifies the rescaling factor in the original DARE algorithm to achieve better performance at high pruning rates without requiring labeled data. DAREx-L2 combines DARE with AdamR, an in-training method that applies L2 regularization to delta parameters before pruning. This combination is particularly effective for structural pruning and enables maintaining model performance even at extreme pruning rates of 99%. The methods can be combined with parameter-efficient fine-tuning techniques like LoRA.

## Key Results
- DAREx-q consistently outperforms vanilla DARE across various datasets and models, with gains exceeding 30% on COLA and SST2 for encoder models
- DAREx-L2 significantly improves performance compared to DARE without L2 fine-tuning, achieving results close to original fine-tuned models even at 99% pruning rates
- The paper demonstrates that importance-based DPP methods can outperform random-based methods when delta parameters are large
- Decoder models show even greater improvements with DAREx methods compared to encoder models

## Why This Works (Mechanism)
The improved performance stems from better handling of delta parameter magnitudes during pruning. DAREx-q's modified rescaling factor prevents the disproportionate elimination of large delta parameters that occurs in vanilla DARE at high pruning rates. DAREx-L2's L2 regularization during fine-tuning reduces delta parameter variance, making the subsequent pruning more stable and effective. This is particularly important for structural pruning where entire weight matrices need to be eliminated. The combination addresses the core challenge that delta parameters can vary widely in magnitude, and traditional importance-based pruning fails when these variations are extreme.

## Foundational Learning

**Delta-Parameter Pruning (DPP)**: A compression technique that reduces fine-tuned model size by pruning only the weight differences (deltas) between fine-tuned and pre-trained models. Needed to minimize storage costs while maintaining performance. Quick check: Verify that pruned delta weights can be effectively reconstructed from pre-trained weights.

**DARE (Delta-Parameter Pruning with Adaptive Rescaling)**: A randomized DPP method that applies importance-based pruning to delta parameters. Needed as baseline for delta pruning. Quick check: Confirm that DARE maintains reasonable performance at moderate pruning rates.

**AdamR with L2 Regularization**: An in-training method that applies L2 regularization to delta parameters during fine-tuning before pruning. Needed to stabilize large delta parameters for more effective pruning. Quick check: Measure delta parameter magnitude reduction after L2 regularization.

## Architecture Onboarding

**Component Map**: Pre-trained Model -> Fine-tuning (with/without L2) -> Delta Parameter Calculation -> DARE/DAREx-q/DAREx-L2 Pruning -> Compressed Model

**Critical Path**: The critical sequence is fine-tuning → delta calculation → pruning method application → evaluation. The pruning method (DARE, DAREx-q, or DAREx-L2) determines final performance, with DAREx-L2 requiring the additional L2 fine-tuning step.

**Design Tradeoffs**: DAREx-q trades off some potential accuracy for broader applicability without labeled data, while DAREx-L2 trades training time and complexity for superior performance at extreme pruning rates. Random-based pruning (DARE) is simpler but less effective when delta parameters are large.

**Failure Signatures**: Performance degradation occurs primarily at high pruning rates (>30%) with vanilla DARE, especially when delta parameters are large. Structural pruning without L2 regularization shows significant accuracy drops. Random pruning fails to maintain performance when important delta parameters are incorrectly pruned.

**First Experiments**: 1) Compare vanilla DARE vs DAREx-q on SST-2 at 50% pruning rate. 2) Evaluate DAREx-L2 at 90% pruning rate on COLA. 3) Test importance-based vs random pruning when delta parameters are artificially enlarged.

## Open Questions the Paper Calls Out
- How do DAREx methods perform on tasks beyond text classification, such as question answering, summarization, and code generation?
- What is the impact of these pruning methods on longer sequences and complex reasoning tasks?
- How do these methods scale to larger model sizes and different architectures beyond BERT and RoBERTa base models?
- What is the practical applicability of DAREx-q without labeled data in real-world scenarios?

## Limitations
- Evaluation primarily focuses on text classification tasks with BERT and RoBERTa base models, limiting generalizability
- The data-free applicability of DAREx-q is claimed but not thoroughly empirically validated
- Does not explore impact on longer sequences or complex reasoning tasks
- Limited investigation of scalability to larger model sizes and different architectures

## Confidence

*High confidence*: Technical description of DAREx-q and DAREx-L2 methods is clear and well-documented. Consistent improvement over vanilla DARE at high pruning rates (>30%) is demonstrated across multiple datasets and model types. Effectiveness of DAREx-L2 for structural pruning is well-supported.

*Medium confidence*: Claim that importance-based DPP outperforms random-based methods when delta parameters are large needs more rigorous validation. Generalization to other model families and tasks remains uncertain.

*Low confidence*: Practical applicability of DAREx-q without labeled data lacks comprehensive empirical validation.

## Next Checks
1. Evaluate DAREx-q and DAREx-L2 on diverse task types beyond text classification, including question answering, summarization, and code generation.

2. Conduct experiments with larger model sizes (BERT/RoBERTa large, GPT-style models) and different architectures (LLaMA, Mistral) to verify scalability.

3. Perform ablation studies specifically testing the data-free claim of DAREx-q by implementing a pipeline that applies the method without access to validation data.