---
ver: rpa2
title: Provably Efficient Off-Policy Adversarial Imitation Learning with Convergence
  Guarantees
arxiv_id: '2405.16668'
source_url: https://arxiv.org/abs/2405.16668
tags:
- off-policy
- policy
- reward
- updates
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the theoretical properties of off-policy adversarial
  imitation learning (AIL) algorithms, which aim to learn from expert demonstrations
  without requiring on-policy data for reward updates. The authors propose an algorithm
  that combines a KL divergence regularized model-based policy update with an off-policy
  projected gradient ascent reward update.
---

# Provably Efficient Off-Policy Adversarial Imitation Learning with Convergence Guarantees

## Quick Facts
- arXiv ID: 2405.16668
- Source URL: https://arxiv.org/abs/2405.16668
- Reference count: 40
- One-line primary result: Proves that off-policy adversarial imitation learning with data reuse from o(√K) recent policies achieves sublinear regret while maintaining convergence guarantees

## Executive Summary
This paper studies off-policy adversarial imitation learning (AIL) algorithms that can learn from expert demonstrations without requiring on-policy data for reward updates. The authors propose a novel algorithm that combines a KL divergence regularized model-based policy update with an off-policy projected gradient ascent reward update. They prove that reusing samples from the o(√K) most recent policies during reward updates does not undermine convergence guarantees, and that the distribution shift error is dominated by the benefits of having more data available. The algorithm achieves sublinear regret for both policy and reward updates, with an optimal number of policies to consider growing as the state space size increases. Experiments on MiniGrid and MuJoCo tasks demonstrate improved sample efficiency compared to on-policy AIL methods.

## Method Summary
The algorithm operates by collecting trajectories using the current policy, updating the transition model and Q-function estimates, and then updating the policy using mirror descent with KL divergence regularization. For reward updates, it samples data from a replay buffer containing the N most recent policies and performs projected gradient ascent on the reward parameters. The key innovation is that it reuses off-policy data from the o(√K) most recent policies while maintaining convergence guarantees through conservative policy updates that limit total variation distance between consecutive policies.

## Key Results
- Proves that reusing samples from o(√K) most recent policies does not undermine convergence guarantees
- Shows that distribution shift error is dominated by the benefits of having more data available
- Achieves sublinear regret for both policy and reward updates
- Experimental results on MiniGrid and MuJoCo tasks demonstrate improved sample efficiency compared to on-policy AIL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reusing samples from the o(√K) most recent policies does not undermine convergence guarantees in off-policy AIL.
- Mechanism: The algorithm limits distribution shift by bounding the total variation distance between consecutive policies. This bounded shift ensures that the objective function used for reward updates remains close to the original on-policy objective, preserving convergence.
- Core assumption: The policy update algorithm (KL divergence regularized mirror descent) is conservative enough to keep consecutive policies close in total variation distance.
- Evidence anchors:
  - [abstract]: "reusing samples generated by the o(√K) most recent policies... does not undermine the convergence guarantees"
  - [section 4.1]: "policy update algorithm should limit the total variation distance between consecutive policies"
  - [corpus]: Weak evidence. No direct support for o(√K) bound in corpus.
- Break condition: If the policy update becomes too aggressive (large step size σ), the total variation distance grows beyond O(AHσ), breaking the distribution shift bound and convergence guarantees.

### Mechanism 2
- Claim: The distribution shift error induced by off-policy updates is dominated by the benefits of having more data available.
- Mechanism: By treating the mixture distribution of past policies as a single policy (via Corollary 4.9), the algorithm effectively has N times more data to evaluate this single policy, reducing estimation error compared to on-policy methods.
- Core assumption: The convex combination of state-action visitation distributions from multiple policies corresponds to a single policy's visitation distribution.
- Evidence anchors:
  - [abstract]: "the distribution shift error induced by off-policy updates is dominated by the benefits of having more data available"
  - [section 4.2]: "Theorem 4.8 demonstrates that we can interpret our use of off-policy data as samples from a single policy"
  - [corpus]: No direct evidence. The claim is novel to this paper.
- Break condition: If N becomes too large (e.g., N = √K), the distribution shift error term (H⁴A²(N-1)²K) dominates the regret bound, eliminating the sample efficiency advantage.

### Mechanism 3
- Claim: There exists an optimal number of recent policies N* to consider during reward updates that balances policy regret and distribution shift error.
- Mechanism: The regret bound contains competing terms: policy regret decreases with more data (larger N), but distribution shift error increases quadratically with N. The optimal N* minimizes the total regret.
- Core assumption: The state space size S dominates other parameters (H, A) in most practical settings.
- Evidence anchors:
  - [section 4.1]: "Proposition 4.6 shows that there is an optimal scaling for the number of recent policies to consider during the reward updates"
  - [section 5.1]: Experimental results show N=32 performs well across different grid sizes
  - [corpus]: Weak evidence. No direct support for optimal N* scaling in corpus.
- Break condition: If the action space A or horizon H dominates S, the optimal N* may scale differently or become negligible, changing the theoretical and practical benefits of the approach.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and state-action visitation distributions
  - Why needed here: The algorithm relies on estimating and bounding differences between state-action visitation distributions of different policies to control distribution shift.
  - Quick check question: What is the relationship between a policy's state-action visitation distribution and the underlying MDP's transition dynamics?

- Concept: KL divergence and mirror descent optimization
  - Why needed here: The policy update uses KL divergence regularized mirror descent to ensure conservative updates that keep consecutive policies close.
  - Quick check question: How does the KL divergence regularization coefficient σ affect the conservativeness of policy updates?

- Concept: Projected gradient ascent and convex optimization
  - Why needed here: The reward update uses projected gradient ascent on a convex reward parameter space, with convergence guarantees based on standard optimization theory.
  - Quick check question: What conditions ensure that projected gradient ascent converges to a stationary point for a convex objective?

## Architecture Onboarding

- Component map: Policy Update Module -> Reward Update Module -> Data Management -> Estimation Module
- Critical path: Collect B trajectories using current policy πₖ → Update transition model and Q-function estimates → Update policy πₖ₊₁ using mirror descent → Sample data from replay buffer containing N most recent policies → Update reward parameters µₖ₊₁ using projected gradient ascent → Repeat
- Design tradeoffs:
  - Larger N provides more data but increases distribution shift error
  - Smaller step size σ makes policy updates more conservative but may slow convergence
  - Number of trajectories B per policy affects estimation accuracy vs. update frequency
  - Choice between tabular vs. function approximation affects scalability and theoretical guarantees
- Failure signatures:
  - High variance in reward estimates suggests insufficient data or poor policy mixing
  - Degrading performance on expert tasks indicates distribution shift exceeding acceptable bounds
  - Oscillating policy behavior may indicate step sizes (σ, η) are too large
  - Slow convergence suggests conservative parameters need adjustment
- First 3 experiments:
  1. Run on-policy AIL (N=1) baseline to establish performance floor and identify key hyperparameters
  2. Test off-policy AIL with small N (N=2,4) to verify distribution shift remains bounded and sample efficiency improves
  3. Sweep N values on a small MDP to find optimal N* and verify the trade-off between data quantity and distribution shift

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal distribution {βn}N n=1 for weighting past policies in off-policy reward updates?
- Basis in paper: [explicit] The authors assume a uniform distribution and leave finding the optimal distribution to future work.
- Why unresolved: The paper only considers uniform weighting and does not explore other distributions or their impact on performance.
- What evidence would resolve it: Empirical studies comparing different weighting distributions (e.g., exponential decay, adaptive weighting) on various tasks and their effects on regret bounds and sample efficiency.

### Open Question 2
- Question: How does the algorithm perform when N = K (using all past policies) instead of N = o(√K)?
- Basis in paper: [inferred] The authors mention that while N = o(√K) is required for theoretical guarantees, N = K works well in practice.
- Why unresolved: The theoretical analysis only covers the case of N = o(√K), and the paper does not provide empirical results for N = K.
- What evidence would resolve it: Empirical studies comparing the performance of the algorithm with N = K to the theoretical bounds and other values of N on various tasks.

### Open Question 3
- Question: How does the algorithm perform with neural network function approximation in linear MDPs?
- Basis in paper: [explicit] The authors extend their results to linear MDPs but note that the theoretical results do not apply when using neural networks.
- Why unresolved: The theoretical analysis is for tabular and linear MDPs, and the paper does not provide empirical results for neural network function approximation in linear MDPs.
- What evidence would resolve it: Empirical studies comparing the performance of the algorithm with neural network function approximation in linear MDPs to the theoretical bounds and other function approximation methods.

## Limitations

- Theoretical analysis relies on strong assumptions including tabular MDPs, bounded rewards, and access to infinite expert demonstrations
- The o(√K) scaling for the number of policies to reuse is derived theoretically but may not be optimal in practice
- The algorithm requires model-based components (transition estimation) which can be challenging in complex environments

## Confidence

- High confidence: The convergence guarantees for the algorithm components (policy updates via mirror descent, reward updates via projected gradient ascent)
- Medium confidence: The claim that distribution shift error is dominated by data benefits, as this depends on the specific scaling relationships and may vary by environment
- Low confidence: The practical applicability of the o(√K) bound for policy reuse, given that experiments use a fixed N=32 regardless of K

## Next Checks

1. Test the algorithm on larger, continuous state-space environments to verify if the theoretical bounds hold and if the distribution shift remains controlled
2. Experiment with varying N values beyond the theoretically suggested o(√K) scaling to empirically determine optimal policy reuse in practice
3. Implement an ablation study isolating the impact of the conservative policy updates versus the off-policy data reuse to quantify their individual contributions to sample efficiency