---
ver: rpa2
title: 'Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information
  Imbalance in Contrastive Vision-Language Models'
arxiv_id: '2404.07983'
source_url: https://arxiv.org/abs/2404.07983
tags:
- modality
- information
- bias
- object
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the modality gap and object bias in contrastive
  vision-language models (VLMs) like CLIP, which are widely used but still poorly
  understood. The authors first clarify that a larger modality gap (geometric separation
  between image and text embeddings) does not clearly harm downstream performance
  due to confounding factors, but evidence suggests it might still be beneficial to
  close it.
---

# Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Models

## Quick Facts
- arXiv ID: 2404.07983
- Source URL: https://arxiv.org/abs/2404.07983
- Reference count: 40
- Primary result: Modality gap and object bias in VLMs stem from information imbalance between images and captions

## Executive Summary
This paper investigates two key phenomena in contrastive vision-language models (VLMs) like CLIP: the modality gap and object bias. The authors show that both effects arise from information imbalance between images and their corresponding captions, where images contain more information than their text descriptions. Through systematic experiments, they demonstrate that this imbalance forces the image encoder to focus on caption-likely features (typically objects), making perfect alignment impossible and widening the modality gap. The paper provides both theoretical insights and practical experiments on synthetic and real datasets to validate these findings.

## Method Summary
The authors conduct a comprehensive analysis of VLMs by first measuring the modality gap and object bias using formal definitions and metrics. They create synthetic datasets (MAD) with controlled information content and imbalance to validate their hypotheses, and complement these with experiments on real-world datasets like CC12M. The methodology includes post-hoc gap-closing experiments, analysis of embedding dimensions driving the gap, and controlled training experiments that manipulate information imbalance. They also introduce the MOAD metric for measuring object bias and systematically study the relationship between gap size, object bias, and downstream performance.

## Key Results
- The modality gap is driven by only a few embedding dimensions, and post-hoc gap-closing does not improve performance due to different neighborhood structures between modalities
- Object bias in VLMs does not directly harm attribute task performance; instead, improvements on object tasks correlate with improvements on attribute tasks
- Both the modality gap and object bias stem from information imbalance between images and captions, confirmed through synthetic and real dataset experiments
- The gap is a by-product of the model's effort to control logit entropy under uncertainty

## Why This Works (Mechanism)
The paper's core mechanism is that information imbalance between visual and textual modalities creates competing optimization pressures in VLMs. When images contain more information than their captions, the image encoder must selectively focus on features that are most likely to appear in captions (typically objects), while the text encoder can freely encode all available information. This creates an asymmetric embedding space where perfect alignment is impossible, forcing the model to increase uniformity in the joint space, which widens the modality gap. The model's attempt to balance these competing objectives results in both the observed gap and the object bias.

## Foundational Learning
- **Contrastive learning objective**: Required to understand how VLMs learn joint embeddings - quick check: verify the model maximizes agreement between matched image-text pairs while minimizing agreement between mismatched pairs
- **Modality gap**: The geometric separation between image and text embeddings in the joint space - quick check: measure using average distance between matched pairs
- **Information imbalance**: The asymmetry in information content between images and their captions - quick check: quantify by comparing entropy or feature count between modalities
- **Object bias**: The tendency of VLMs to prioritize object features over other visual attributes - quick check: measure using MOAD metric on object vs. attribute recognition tasks
- **Logit entropy control**: How models manage uncertainty in predictions - quick check: monitor entropy values during training to observe gap-widening behavior

## Architecture Onboarding

Component map: Image encoder -> Joint embedding space <- Text encoder

Critical path: Image/text input → Encoder (ViT/Transformer) → Projection head → Normalized embeddings → Contrastive loss

Design tradeoffs: The model must balance alignment (bringing matched pairs close) with uniformity (spreading embeddings in the space), with information imbalance creating an inherent tension between these objectives.

Failure signatures: Large modality gaps, object-centric embeddings that neglect attributes, poor cross-modal retrieval performance despite good within-modal performance.

First experiments:
1. Measure the modality gap and object bias on a pre-trained VLM using the provided metrics
2. Create a simple synthetic dataset with controlled information imbalance and train a VLM to observe gap formation
3. Test post-hoc gap-closing techniques and measure their impact on downstream performance

## Open Questions the Paper Calls Out
None

## Limitations
- Correlation studies on downstream performance cannot establish causation regarding modality gap impact (Medium confidence)
- Object bias analysis doesn't account for potential task-specific encoding strategies
- Practical mitigation strategies for information imbalance require further validation
- Findings may not generalize to non-contrastive VLMs or different training objectives

## Confidence
- Modality gap driven by few dimensions: High confidence
- Information imbalance causes both gap and object bias: High confidence (synthetic experiments provide strong evidence)
- Gap closing doesn't improve performance: Medium confidence (confounding factors not fully controlled)
- Object bias doesn't harm attribute performance: Medium confidence (correlation vs. causation not fully established)

## Next Checks
1. Test whether the few dimensions driving the modality gap remain consistent across different VLM architectures and scales
2. Validate the information imbalance hypothesis by training VLMs with controlled text augmentation to reduce caption-information deficit
3. Examine whether the observed correlations between object and attribute task performance hold when controlling for dataset-specific biases and annotation quality