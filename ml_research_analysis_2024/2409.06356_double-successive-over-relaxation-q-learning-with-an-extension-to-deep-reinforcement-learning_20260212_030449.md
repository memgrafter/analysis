---
ver: rpa2
title: Double Successive Over-Relaxation Q-Learning with an Extension to Deep Reinforcement
  Learning
arxiv_id: '2409.06356'
source_url: https://arxiv.org/abs/2409.06356
tags:
- q-learning
- algorithm
- proposed
- learning
- sorql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a double successive over-relaxation (DSOR)
  Q-learning algorithm to address the slow convergence and overestimation bias in
  standard Q-learning. The key idea is to combine the successive over-relaxation technique
  with the double Q-learning approach, making the algorithm model-free and less biased.
---

# Double Successive Over-Relaxation Q-Learning with an Extension to Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2409.06356
- Source URL: https://arxiv.org/abs/2409.06356
- Authors: Shreyas S R
- Reference count: 36
- One-line primary result: DSOR Q-learning outperforms standard Q-learning, double Q-learning, and SOR Q-learning on benchmark tasks, with deep RL extension showing superiority on Atari games.

## Executive Summary
This paper proposes a double successive over-relaxation (DSOR) Q-learning algorithm that addresses the slow convergence and overestimation bias issues in standard Q-learning. The key innovation combines successive over-relaxation with double Q-learning, creating a model-free algorithm that tracks the relaxation factor online via stochastic approximation. The algorithm is validated in both tabular and deep RL settings, demonstrating superior performance on CartPole, LunarLander, multi-armed bandit problems, and Atari games compared to existing algorithms.

## Method Summary
The DSOR Q-learning algorithm maintains two Q-estimators (Q_A and Q_B) and updates them using a relaxation factor w that is tracked online through a stochastic approximation loop. The relaxation parameter w is updated based on empirical self-loop frequencies without requiring explicit model knowledge. In the deep RL extension, this framework is implemented using neural networks with replay buffers and target networks. The algorithm uses epsilon-greedy exploration with diminishing step sizes, and the relaxation factor is bounded to ensure contraction properties. Experimental evaluation compares DSORQL against QL, DQL, and SORQL across multiple benchmark environments.

## Key Results
- DSOR Q-learning demonstrates faster convergence than standard Q-learning and SORQL on tabular benchmarks
- The algorithm shows reduced overestimation bias compared to SORQL through the double estimator mechanism
- DSORDQN outperforms DQN, SORDQN, DDQN, and Rainbow on average reward across tested Atari games
- Performance improvements are consistent across CartPole, LunarLander, and multi-armed bandit problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The relaxation factor w is tracked online using an SA update, enabling model-free SOR Q-learning without explicit knowledge of transition probabilities.
- Mechanism: An auxiliary stochastic approximation loop updates w_n via: w_{n+1} = w_n + α(n)(1/(1-γ min_{i,a} p̂_{ia}) - w_n) where p̂_{ia} are empirical self-loop frequencies. This ensures w_n → w^* a.s.
- Core assumption: Empirical self-loop frequencies p̂_{ia} converge to true p_{ia} under sufficient exploration.
- Evidence anchors:
  - [abstract]: "successive relaxation factor in the above equation depends on the transition probability, which makes it not entirely model-free."
  - [section II]: "Using the strong law of large numbers p̂_{ia} → p_{ia}, ∀i,a as n → ∞ w.p.1."
  - [corpus]: No direct evidence found; weak anchor.
- Break condition: If exploration is insufficient, p̂_{ia} may not converge, causing w_n to misestimate w^* and slow convergence.

### Mechanism 2
- Claim: DSORQL reduces overestimation bias by combining double Q-learning with SOR relaxation.
- Mechanism: DSORQL maintains two estimators Q_A and Q_B. Updates use the partner estimator for bootstrapping: Q_A^{n+1}(i,a) = (1-β_n)Q_A^n(i,a) + β_n[w(r + γ Q_B^n(j,b^*)) + (1-w)Q_B^n(i,c^*)]. The double-estimator structure decorrelates action selection and evaluation, lowering bias.
- Core assumption: The two estimators have sufficiently different sampling noise so that E[Q_B(j,b^*)] ≤ max_i E[Q_B(j,i)].
- Evidence anchors:
  - [abstract]: "Theoretically and empirically, this algorithm is shown to be less biased than SORQL."
  - [section IV.A]: "Lemma 1, [11]" shows double Q-learning yields non-positive bias.
  - [corpus]: No direct evidence found; weak anchor.
- Break condition: If both estimators converge to the same biased values, the decorrelation effect vanishes and overestimation bias re-emerges.

### Mechanism 3
- Claim: The contraction factor of the SOR operator U_w is 1-w+wγ < γ, enabling faster convergence than standard Q-learning.
- Mechanism: By choosing w > 1, the modified Bellman operator contracts faster. The SA iterates track U_w instead of U, so error decays at rate 1-w+wγ rather than γ.
- Core assumption: w remains bounded above by w^* to preserve contractivity.
- Evidence anchors:
  - [abstract]: "SOR Q-learning, which introduces a relaxation factor to speed up convergence, addresses this issue..."
  - [section II]: "The contraction factor of the map U_w is 1-w+wγ. It is interesting to note that for a suitable SOR parameter w, one can show that 1-w+wγ < γ."
  - [corpus]: No direct evidence found; weak anchor.
- Break condition: If w exceeds w^*, the operator may cease to be a contraction and convergence is lost.

## Foundational Learning

- Concept: Contraction mapping theorem
  - Why needed here: To prove that the SOR Bellman operator has a unique fixed point and that iterative application converges.
  - Quick check question: What is the contraction factor of U_w in terms of w and γ?

- Concept: Stochastic approximation theory
  - Why needed here: To establish almost sure convergence of SA updates for both Q-values and w_n.
  - Quick check question: Which conditions of Lemma 1 ensure convergence under diminishing step sizes?

- Concept: Bias-variance tradeoff in Q-learning
  - Why needed here: To understand why overestimation occurs and how double estimators mitigate it.
  - Quick check question: How does Jensen's inequality lead to overestimation in standard Q-learning?

## Architecture Onboarding

- Component map: Replay buffer -> Q-networks (Q_A, Q_B) -> Target networks -> SA loop for w_n -> Output policy
- Critical path:
  1. Collect sample (s,a,r,s').
  2. With 50% prob, update Q_A using Q_B for bootstrapping; else update Q_B using Q_A.
  3. Update w_n asynchronously via SA.
  4. For deep RL: compute loss and gradient step; periodically sync target nets.
- Design tradeoffs:
  - w > 1 speeds convergence but risks instability if too large.
  - Double estimators reduce bias but double memory/computation.
  - SA step size α(n) must decay slowly enough for w_n tracking but fast enough for convergence.
- Failure signatures:
  - Overestimation persists: check decorrelation quality of Q_A, Q_B.
  - w_n diverges: likely step-size too large or exploration insufficient.
  - Deep RL diverges: replay buffer too small or target update too infrequent.
- First 3 experiments:
  1. Run DSORQL on single-state MDP with known w to verify tracking.
  2. Compare DSORQL vs SORQL on multi-armed bandit for bias reduction.
  3. Train DSORDQN on CartPole with varying w to find optimal relaxation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact convergence rate of the double successive over-relaxation (DSOR) Q-learning algorithm compared to standard Q-learning, double Q-learning, and successive over-relaxation Q-learning (SORQL)?
- Basis in paper: [inferred] The paper claims DSOR Q-learning converges faster than standard Q-learning and SORQL, but does not provide explicit convergence rate analysis.
- Why unresolved: The paper focuses on establishing convergence under boundedness assumptions but does not quantify the speed of convergence or compare it to other algorithms in terms of convergence rate.
- What evidence would resolve it: Theoretical analysis proving DSOR Q-learning has a faster contraction rate than standard Q-learning and SORQL, or empirical experiments comparing the number of iterations needed to reach a target performance threshold across algorithms.

### Open Question 2
- Question: Can the boundedness assumption (A1, A2, A3) on iterates in the tabular setting be relaxed or proven without assuming bounded iterates?
- Basis in paper: [explicit] The paper explicitly states that convergence is shown under the assumption that iterates are bounded (A1, A2, A3).
- Why unresolved: The boundedness assumption is a common requirement in stochastic approximation theory, but proving convergence without it would be more general and theoretically significant.
- What evidence would resolve it: A proof that the iterates of DSOR Q-learning remain bounded without assuming it a priori, or a convergence proof that does not require boundedness.

### Open Question 3
- Question: How does the choice of the successive over-relaxation (SOR) parameter w affect the bias-variance tradeoff in DSOR Q-learning?
- Basis in paper: [explicit] The paper discusses that w > 1 reduces overestimation bias and mentions ablation studies varying w, but does not analyze the bias-variance tradeoff.
- Why unresolved: While the paper shows DSOR Q-learning reduces overestimation bias, it does not explore how different values of w affect both bias and variance, or provide guidance on selecting w for optimal performance.
- What evidence would resolve it: Theoretical analysis of how w affects the bias and variance of the Q-value estimates, or empirical studies showing the impact of different w values on both bias and variance across tasks.

### Open Question 4
- Question: How does DSORDQN perform compared to other advanced deep RL algorithms like Rainbow in terms of sample efficiency and computational complexity?
- Basis in paper: [explicit] The paper compares DSORDQN to Rainbow on Atari games but does not provide a detailed comparison of sample efficiency or computational complexity.
- Why unresolved: The paper shows DSORDQN outperforms Rainbow on average reward, but does not analyze how many samples or how much computation each algorithm requires to achieve its performance.
- What evidence would resolve it: Experiments measuring the number of environment interactions or training steps required to reach a target performance, and computational profiling comparing the per-update cost of DSORDQN and Rainbow.

## Limitations

- Empirical validation relies on a limited set of environments (CartPole, LunarLander, and a subset of Atari games) that may not capture algorithm performance across diverse problem domains.
- Theoretical analysis focuses on tabular settings with boundedness assumptions, leaving uncertainty about convergence properties under function approximation in deep RL.
- The paper does not provide systematic sensitivity analysis of the relaxation parameter w or guidance on optimal selection across different environments.

## Confidence

- **High Confidence**: The mechanism by which the double estimator structure reduces overestimation bias is well-established in prior literature and logically extends to the DSOR framework.
- **Medium Confidence**: The convergence analysis for the tabular case is mathematically rigorous, but its applicability to deep RL settings with function approximation remains less certain.
- **Low Confidence**: The empirical superiority of DSORDQN on Atari games is demonstrated, but the sample size of tested games is small, and comparisons to newer, more advanced algorithms are limited.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Conduct a grid search over the relaxation parameter w and step sizes α(n) to identify optimal configurations across multiple environments, including those not tested in the original paper.

2. **Robustness to Function Approximation**: Evaluate DSORDQN on a wider variety of Atari games and continuous control tasks (e.g., MuJoCo) to assess whether the convergence guarantees observed in tabular settings hold under neural network approximation.

3. **Bias and Variance Decomposition**: In simple tabular MDPs with known optimal values, measure both bias and variance of Q-value estimates for DSORQL, DQL, and SORQL to quantify the specific contribution of the double estimator and relaxation components.