---
ver: rpa2
title: Activation Scaling for Steering and Interpreting Language Models
arxiv_id: '2410.04962'
source_url: https://arxiv.org/abs/2410.04962
tags:
- activation
- vectors
- intervention
- token
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes activation scaling as a method to steer and
  interpret language models. The approach learns multiplicative scalars to scale activation
  vectors, aiming to be effective (flipping predictions between correct and incorrect
  tokens), faithful (leaving unrelated tokens unaffected), and minimal (affecting
  few locations).
---

# Activation Scaling for Steering and Interpreting Language Models

## Quick Facts
- arXiv ID: 2410.04962
- Source URL: https://arxiv.org/abs/2410.04962
- Reference count: 11
- Key outcome: Activation scaling learns multiplicative scalars to scale activation vectors, achieving comparable effectiveness and faithfulness to additive steering vectors while using far fewer parameters, enabling interpretable localization of model components.

## Executive Summary
This paper introduces activation scaling as a method for steering and interpreting language models by learning multiplicative scalars that scale activation vectors. The approach aims to be effective (flipping predictions between correct and incorrect tokens), faithful (leaving unrelated tokens unaffected), and minimal (affecting few locations). Using gradient-based optimization with a three-term objective, activation scaling achieves comparable effectiveness and faithfulness to additive steering vectors but with significantly fewer parameters, enabling interpretable localization of model components. Dynamic variants generalize to variable-length prompts, and the method offers a principled framework connecting steerability and interpretability.

## Method Summary
Activation scaling learns multiplicative scalars to scale activation vectors at specific intervention points in transformer models. The method optimizes a three-term objective balancing effectiveness (maximizing logit differences between correct and incorrect tokens), faithfulness (minimizing KL divergence from the original distribution), and minimality (penalizing the ℓ1 norm of intervention parameters). Dynamic variants make scalars functions of activation vectors themselves, enabling generalization to variable-length prompts. The approach is evaluated on synthetic tasks (Country-Capital Conflicts and Indirect Object Identification) and compared against additive steering vectors, demonstrating comparable performance with far fewer parameters.

## Key Results
- Activation scaling achieves comparable effectiveness and faithfulness to additive steering vectors while using far fewer parameters (single scalar vs. full vector per location)
- Dynamic activation scaling generalizes to variable-length prompts by learning scalar functions of activation vectors
- Visualization of learned activation scalars reveals interpretable localization of task-relevant model components
- The three-term objective successfully balances effectiveness, faithfulness, and minimality, with parameter count serving as a natural interpretability metric

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Activation scaling achieves steering by scaling the magnitude of activation vectors without altering their direction.
- **Mechanism:** The method learns multiplicative scalars that adjust the signed magnitude of activation vectors at specific locations. This amplifies or attenuates the existing steering directions encoded in the model, effectively strengthening or weakening the influence of task-relevant components.
- **Core assumption:** Some model components are highly specialized for certain task-relevant computations, and scaling their magnitude is sufficient to flip predictions between correct and incorrect tokens.
- **Evidence anchors:** [abstract] "activation scaling only modifies the signed magnitude of activation vectors to strengthen, weaken, or reverse the steering directions already encoded in the model."
- **Break condition:** If the steering directions are not encoded in the magnitude of activation vectors, or if the model requires changes in activation vector direction rather than magnitude for effective steering.

### Mechanism 2
- **Claim:** The three-term objective ensures interventions are effective, faithful, and minimal simultaneously.
- **Mechanism:** The objective combines an effectiveness term that maximizes logit differences between correct and incorrect tokens, a faithfulness term that minimizes KL divergence from the original distribution, and a minimality term that penalizes the ℓ1 norm of intervention parameters to encourage sparsity.
- **Core assumption:** A successful intervention must balance effectiveness (flipping predictions), faithfulness (not affecting unrelated tokens), and minimality (affecting few locations).
- **Evidence anchors:** [abstract] "we establish a three-term objective: a successful intervention should flip the correct with the wrong token and vice versa (effectiveness), and leave other tokens unaffected (faithfulness), all while being sparse (minimality)."
- **Break condition:** If the hyperparameters λF and λM are not properly tuned, one objective may dominate and compromise the others, or if the ℓ1 regularization is too strong, the intervention may become ineffective.

### Mechanism 3
- **Claim:** Dynamic activation scaling generalizes to variable-length prompts by making scalars functions of activation vectors.
- **Mechanism:** Each scalar is defined as a learned function of the corresponding activation vector, allowing the intervention to adapt to different token positions across varying-length prompts. This replaces fixed position-based scalars with context-dependent ones.
- **Core assumption:** Task-relevant activation vectors have identifiable patterns that can be captured by functions mapping activations to scalars, enabling generalization beyond fixed prompt templates.
- **Evidence anchors:** [abstract] "make activation scalars a learnable function of the activation vectors themselves to generalize to varying-length prompts."
- **Break condition:** If the function g(s)_l cannot adequately capture the relationship between activation vectors and task relevance across different prompt lengths, or if the model's behavior is too context-dependent for fixed functions to generalize.

## Foundational Learning

- **Concept:** Multi-head self-attention mechanism
  - **Why needed here:** Understanding how attention heads process information is crucial for interpreting where activation scaling interventions are most effective and how they influence the model's predictions.
  - **Quick check question:** How does the multi-head attention mechanism compute attention scores and use them to weight value vectors for each token position?

- **Concept:** Autoregressive language modeling and softmax probability distribution
  - **Why needed here:** The steering objective relies on manipulating logit differences between correct and incorrect tokens, which are transformed into probabilities via softmax. Understanding this relationship is essential for interpreting effectiveness metrics.
  - **Quick check question:** How does the softmax function transform logits into a probability distribution over the vocabulary, and how do logit differences affect the predicted probabilities?

- **Concept:** Gradient-based optimization and hyperparameter tuning
  - **Why needed here:** The method uses gradient-based optimization to learn intervention parameters, and proper hyperparameter tuning (learning rate, initialization, regularization strength) is critical for achieving the desired balance between effectiveness, faithfulness, and minimality.
  - **Quick check question:** What role do the hyperparameters λF, λM, and the margin m play in the three-term objective, and how do they affect the learned intervention?

## Architecture Onboarding

- **Component map:** Transformer blocks with multi-head attention and MLP layers → Layer normalization → Residual connections → Projection (unembedding) matrix → Intervention points at attention output, MLP output, and post-residual locations across all layers and token positions

- **Critical path:**
  1. Forward pass through the model to obtain activation vectors at specified intervention points
  2. Computation of the three-term objective using the current intervention parameters
  3. Backward pass to compute gradients of the objective with respect to intervention parameters
  4. Parameter update using gradient-based optimization (Adam)
  5. Evaluation of effectiveness, faithfulness, and minimality on train and test sets
  6. Visualization of learned activation scalars to identify task-relevant components

- **Design tradeoffs:**
  - ACTIV SCALAR vs STEER VEC: ACTIV SCALAR uses fewer parameters (single scalar per location vs full vector) but may be less expressive if direction changes are needed
  - Fixed vs dynamic scalars: Fixed scalars are simpler but don't generalize to varying prompt lengths; dynamic scalars generalize but require learning functions instead of scalars
  - Effectiveness vs faithfulness: Stronger interventions may flip predictions more reliably but could affect unrelated tokens; weaker interventions may be more faithful but less effective
  - Minimality vs performance: More sparse interventions are more interpretable but may sacrifice steering performance

- **Failure signatures:**
  - Intervention parameters remain near zero: ℓ1 regularization too strong or learning rate too low
  - Test set performance much worse than train set: overfitting due to too many parameters or insufficient regularization
  - Intervention doesn't flip predictions: effectiveness term too weak, margin too small, or intervention points not task-relevant
  - Intervention affects unrelated tokens: faithfulness term too weak or effectiveness term too strong
  - Dynamic scalars don't generalize: function g(s)_l insufficiently expressive or not properly trained on diverse prompt lengths

- **First 3 experiments:**
  1. Implement ACTIV SCALAR on GPT2-Small for the CCC task with fixed hyperparameters (λF=1, λM=1, m=0) on a single data point, visualizing learned scalars to verify they highlight task-relevant token positions
  2. Compare ACTIV SCALAR vs STEER VEC on the same task and model, measuring effectiveness, faithfulness, and parameter count to quantify the efficiency tradeoff
  3. Implement DYNSCALAR for the same task, training on prompts of length 10-20 and testing on held-out lengths to verify generalization capability, visualizing how the learned functions adapt to different token positions

## Open Questions the Paper Calls Out
- **Open Question 1:** Can activation scaling be effectively applied to larger, more complex language models beyond GPT2-XL and Pythia-1.4B, and how does its performance scale with model size?
- **Open Question 2:** How does the performance of activation scaling compare to other interpretability methods like activation patching or direct logit attribution when applied to tasks beyond synthetic benchmarks?
- **Open Question 3:** Can the dynamic version of activation scaling (DYNSCALAR) be further improved by incorporating more sophisticated functions to map activation vectors to scalars, and how would this impact its generalization performance?
- **Open Question 4:** How does the choice of hyperparameters, such as the learning rate, number of epochs, and initialization of intervention parameters, affect the performance and interpretability of activation scaling?

## Limitations
- The method's performance on real-world NLP tasks beyond synthetic benchmarks remains unverified, raising questions about practical applicability
- The core assumption that magnitude-only scaling suffices for effective steering may not generalize to more complex tasks requiring direction changes
- The method has not been thoroughly tested on larger models beyond GPT2-XL and Pythia-1.4B, limiting understanding of scalability

## Confidence
- **High confidence:** The technical implementation of activation scaling and the three-term objective optimization are well-specified and reproducible
- **Medium confidence:** The effectiveness of activation scaling on the CCC and IOI synthetic tasks, while demonstrated, may not translate to more complex reasoning or generation tasks
- **Low confidence:** The generalization capability of dynamic activation scaling to truly variable-length prompts and its performance on diverse task types beyond the synthetic benchmarks is not yet established

## Next Checks
1. Apply activation scaling to a real-world NLP task (e.g., sentiment analysis or natural language inference) beyond the synthetic CCC/IOI benchmarks to verify whether magnitude-only interventions remain effective on semantically rich tasks.
2. Implement a variant of activation scaling that can modify both direction and magnitude of activation vectors, then compare effectiveness on tasks where pure magnitude scaling might be insufficient to determine if direction changes are sometimes necessary.
3. Systematically vary initialization schemes, learning rates, and the balance between the three objective terms across multiple random seeds to establish the stability of the method and identify failure modes when hyperparameter tuning is suboptimal.