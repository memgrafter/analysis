---
ver: rpa2
title: Exploring the Capabilities of Large Multimodal Models on Dense Text
arxiv_id: '2405.06706'
source_url: https://arxiv.org/abs/2405.06706
tags:
- text
- images
- lmms
- dense
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the DT-VQA dataset to evaluate large multimodal
  models (LMMs) on dense text understanding tasks. The dataset includes 170k question-answer
  pairs from 30k images covering scenes, tables, and product labels, focusing on challenges
  like high information density, complex layouts, and varied text types.
---

# Exploring the Capabilities of Large Multimodal Models on Dense Text

## Quick Facts
- arXiv ID: 2405.06706
- Source URL: https://arxiv.org/abs/2405.06706
- Authors: Shuo Zhang; Biao Yang; Zhang Li; Zhiyin Ma; Yuliang Liu; Xiang Bai
- Reference count: 40
- Primary result: Introduces DT-VQA dataset and shows LMMs struggle with dense text; prompt engineering and fine-tuning significantly improve performance.

## Executive Summary
This paper addresses the challenge of dense text understanding in large multimodal models (LMMs) by introducing the DT-VQA dataset, a comprehensive benchmark with 170k question-answer pairs from 30k images covering scenes, tables, and product labels. The authors evaluate 13 LMMs, revealing significant performance gaps, especially in product images. They propose prompt engineering and fine-tuning strategies that yield substantial improvements, highlighting the need for specialized datasets and methods to advance LMMs' text understanding capabilities.

## Method Summary
The authors construct the DT-VQA dataset using OCR-based automatic labeling, covering scenes, tables, and product labels. They evaluate LMMs (GPT-4V, Gemini, Qwen-VL, Monkey) using accuracy, ANLS, and a new AccANLS metric. Two improvement strategies are proposed: (1) prompt engineering by adding a targeted instruction to constrain output format, and (2) downstream fine-tuning on automatically labeled dense text data. Experiments are conducted on Qwen-VL and Monkey with AdamW optimizer and learning rate 5e-6.

## Key Results
- LMMs struggle with dense text, especially in product images; accuracy often falls short of traditional OCR models.
- Prompt engineering yields an average 7.6% improvement on ANLS for 13 LMMs.
- Fine-tuning Qwen-VL and Monkey on DT-VQA improves AccANLS by 16.5% and 12.2%, respectively.
- Higher input resolution (e.g., 448px in Qwen-VL) correlates with better OCR performance in dense text.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt engineering significantly improves LMMs' text extraction accuracy from dense images.
- Mechanism: Adding a targeted instruction at the beginning of the question ("This is a question related to text in an image, please provide a word or phrase from the image text as an answer") constrains the model's output format, shifting focus from multimodal reasoning to direct text extraction.
- Core assumption: The base LMMs already encode OCR-level capabilities, but their default generation strategy dilutes performance with overly long, contextually broad responses.
- Evidence anchors:
  - [abstract] "Prompt engineering brings an average improvement of 7.6% on the ANLS metric for 13 LMMs"
  - [section] "This modification enables the model to generate shorter format answers and focus more on the textual information within the image."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.523. No strong anchor found for OCR-focused LMMs; assume limited prior work.
- Break condition: If the LMM's underlying OCR capability is weak, prompt engineering cannot compensate for missing recognition features.

### Mechanism 2
- Claim: Downstream fine-tuning on automatically labeled dense text data yields large performance gains.
- Mechanism: Fine-tuning Qwen-VL and Monkey on DT-VQA's automatically labeled training set aligns the model's visual-text fusion weights to the specific statistical distribution of dense text layouts.
- Core assumption: Even imperfect synthetic labels capture the key correlation between dense layout features and textual content extraction, enabling effective gradient updates.
- Evidence anchors:
  - [abstract] "Downstream fine-tuning results in an improvement of 16.5% and 12.2% on the AccANLS evaluation metric for Qwen-VL and Monkey, respectively."
  - [section] "We adopt the model scale configurations of Qwen-VL [3] and Monkey [16]...we utilize the AdamW optimizer [21] with a learning rate set to 5e-6"
  - [corpus] No corpus evidence for synthetic-label fine-tuning efficacy; inference based on standard transfer learning theory.
- Break condition: If the synthetic labels contain systematic biases that dominate training, fine-tuning may worsen generalization to real-world images.

### Mechanism 3
- Claim: Higher input resolution improves dense text recognition accuracy.
- Mechanism: Models like Qwen-VL (448px) and Monkey (sliding window) process finer-grained visual features, enabling better OCR-level character segmentation and recognition in cluttered layouts.
- Core assumption: The text density in DT-VQA images exceeds the resolution capacity of standard 224px models, so increasing resolution exposes more discriminative features.
- Evidence anchors:
  - [abstract] "Qwen-VL [3] introduces a position-aware visual language adapter that compresses image feature lengths, raising the input resolution to 448 for the first time."
  - [section] "We find that increasing input resolution [3,16] is a promising approach to address densely arranged text"
  - [corpus] No direct corpus support; assumption aligns with known trade-offs in vision-language model design.
- Break condition: If computational constraints force severe down-sampling, resolution gains may be negated.

## Foundational Learning

- Concept: Levenshtein-based similarity metrics (ANLS)
  - Why needed here: DT-VQA's answers are short, variable-length text snippets; simple string match would be too strict.
  - Quick check question: If the predicted answer is "2g" and the ground truth is "2 g", what is the ANLS score assuming threshold τ=0.5?

- Concept: Instruction-tuned multimodal alignment
  - Why needed here: LMMs must map visual features to text generation space while following layout-aware text extraction instructions.
  - Quick check question: How does the Q-Former in BLIP-2 help in this mapping compared to naive linear projection?

- Concept: Dataset curation for OCR-heavy tasks
  - Why needed here: Standard vision-language datasets underrepresent dense text; DT-VQA fills that gap to enable model specialization.
  - Quick check question: Why does combining scenes, tables, and product labels improve model generalization vs. single-domain datasets?

## Architecture Onboarding

- Component map:
  - Image (resized to model's native resolution) -> Vision backbone -> Visual token stream -> Adapter (position-aware or sliding-window) -> LLM context -> Answer generation -> ANLS/AccANLS evaluation

- Critical path: Image → Vision encoder → Adapter → LLM context → Answer generation → Metric evaluation

- Design tradeoffs:
  - Resolution vs. compute: Higher resolution improves OCR accuracy but increases memory cost.
  - OCR injection vs. end-to-end: Explicit OCR tokens boost precision; pure end-to-end risks hallucination.
  - Fine-tuning vs. prompt engineering: Fine-tuning yields higher gains but is costly; prompt engineering is cheap but model-dependent.

- Failure signatures:
  - Low ANLS but high accuracy: Model generates correct answer but with extra text (e.g., "The answer is 2g" vs "2g").
  - Low accuracy: Model fails to locate or recognize text.
  - High AccANLS but poor human readability: Model outputs overly terse or incomplete phrases.

- First 3 experiments:
  1. Compare ANLS vs. accuracy on a held-out dense text sample to confirm metric mismatch.
  2. Test prompt variants (e.g., "extract the text content only") on Qwen-VL to find optimal phrasing.
  3. Fine-tune Monkey on 10% of DT-VQA training set and measure validation loss to gauge early overfitting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LMMs on dense text images vary across different text types (e.g., scene text, tables, product labels) and what are the specific challenges associated with each type?
- Basis in paper: [explicit] The paper states that models perform relatively better in tables and scene text images, where the boundaries between text are distinct, compared to product images, and that current models face greater challenges in scenarios with denser text.
- Why unresolved: The paper provides a general overview of the performance differences across text types but does not delve into the specific challenges associated with each type or provide a detailed analysis of the factors contributing to these differences.
- What evidence would resolve it: A detailed analysis of the performance of LMMs on different text types, including a breakdown of the specific challenges associated with each type (e.g., text density, layout complexity, font variations, noise), and an exploration of the factors contributing to the performance differences.

### Open Question 2
- Question: What are the most effective prompt engineering strategies for improving the performance of LMMs on dense text images, and how do these strategies differ across different LMM architectures?
- Basis in paper: [explicit] The paper mentions that prompt engineering brings an average improvement of 7.6% on the ANLS metric for 13 LMMs and suggests that designing prompts that are more suitable for each model is a problem that needs further exploration.
- Why unresolved: The paper does not provide a comprehensive analysis of the most effective prompt engineering strategies or explore how these strategies differ across different LMM architectures.
- What evidence would resolve it: A systematic evaluation of different prompt engineering strategies, including a comparison of their effectiveness across different LMM architectures, and an analysis of the factors that influence the effectiveness of these strategies (e.g., model architecture, training data, task complexity).

### Open Question 3
- Question: How does the performance of LMMs on dense text images compare to traditional text detection and recognition models, and what are the advantages and disadvantages of each approach?
- Basis in paper: [explicit] The paper mentions that LMMs' accuracy often falls short of traditional text detection and recognition models when dealing with text-related questions that do not require complex reasoning.
- Why unresolved: The paper does not provide a detailed comparison of the performance of LMMs and traditional text detection and recognition models on dense text images, nor does it explore the advantages and disadvantages of each approach.
- What evidence would resolve it: A comprehensive evaluation of the performance of LMMs and traditional text detection and recognition models on dense text images, including a comparison of their accuracy, efficiency, and robustness, and an analysis of the advantages and disadvantages of each approach in different scenarios.

## Limitations

- Data Quality and Representativeness: The DT-VQA dataset is automatically constructed using OCR-based labeling, which introduces potential errors in ground-truth annotations. The claim that it captures "diverse real-world scenarios" is not empirically validated beyond the three domains (scenes, tables, product labels). There is no mention of out-of-distribution testing or human verification of labels.
- Generalization of Proposed Methods: The paper demonstrates prompt engineering and fine-tuning on specific LMMs (Qwen-VL, Monkey), but does not test these methods on a broader set of models or on datasets outside DT-VQA. The reported improvements may be dataset-specific and not indicative of general performance gains across varied dense text scenarios.
- Resolution as a Proxy for OCR Quality: The paper attributes performance differences partly to input resolution, but does not control for other architectural differences between models (e.g., OCR token injection, attention mechanisms). The claim that "increasing input resolution" is a "promising approach" is observational and not experimentally isolated from confounding factors.

## Confidence

- Prompt Engineering Efficacy: Medium
  - Rationale: Improvements are demonstrated, but only on a subset of models and without ablation studies on prompt phrasing or comparison to alternative strategies (e.g., chain-of-thought prompting).
- Fine-tuning Gains: Medium
  - Rationale: Results are promising, but the training procedure (automatic labeling, dataset splits) lacks detail for full reproducibility. No analysis of overfitting or validation on external data.
- Resolution as a Key Factor: Low
  - Rationale: The paper does not isolate resolution from other architectural differences; the claim is correlative, not causative.
- DT-VQA as a Comprehensive Benchmark: Low
  - Rationale: Dataset construction method and domain coverage are not independently verified; no discussion of potential biases in OCR-based labeling.

## Next Checks

1. **Human Evaluation of Annotations**: Recruit annotators to manually verify a random sample of DT-VQA question-answer pairs. Calculate inter-annotator agreement and error rates to quantify dataset quality.

2. **Cross-Dataset Generalization**: Evaluate prompt-engineered and fine-tuned models on an independent dense text dataset (e.g., OCR-VQA or DocVQA). Measure performance drop to assess robustness and generalization.

3. **Ablation Study on Model Architectures**: Compare Qwen-VL and Monkey at matched input resolutions, with and without OCR token injection, to isolate the effect of resolution versus architectural features on dense text performance.