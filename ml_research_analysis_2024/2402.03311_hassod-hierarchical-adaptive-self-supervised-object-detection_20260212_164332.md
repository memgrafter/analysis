---
ver: rpa2
title: 'HASSOD: Hierarchical Adaptive Self-Supervised Object Detection'
arxiv_id: '2402.03311'
source_url: https://arxiv.org/abs/2402.03311
tags:
- object
- hassod
- objects
- detection
- self-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HASSOD introduces a hierarchical adaptive clustering approach to
  self-supervised object detection, addressing limitations in prior methods that focus
  on only a few prominent objects per image and neglect object composition. By grouping
  regions into object masks based on self-supervised visual representations, HASSOD
  adaptively determines the appropriate number of objects per image, improving coverage
  of potential objects in natural scenes.
---

# HASSOD: Hierarchical Adaptive Self-Supervised Object Detection

## Quick Facts
- arXiv ID: 2402.03311
- Source URL: https://arxiv.org/abs/2402.03311
- Reference count: 40
- One-line primary result: Achieves state-of-the-art self-supervised object detection with Mask AR of 22.5 on LVIS and 26.0 on SA-1B while using 1/5 the images and 1/12 the iterations of prior methods

## Executive Summary
HASSOD addresses limitations in self-supervised object detection by introducing hierarchical adaptive clustering that better handles scenes with multiple objects and object composition. Unlike prior methods that focus on only a few prominent objects per image, HASSOD adaptively determines the appropriate number of objects based on semantic feature similarity, improving coverage of potential objects in natural scenes. The method also identifies hierarchical levels (whole, part, subpart) through coverage analysis between masks, enhancing interpretability and performance.

## Method Summary
HASSOD employs a two-stage discover-and-learn process for self-supervised object detection. First, it generates initial pseudo-labels using hierarchical adaptive clustering on unlabeled images with DINO ViT-B/8 features, determining the appropriate number of objects per image through iterative merging of adjacent regions based on feature similarity. Second, it trains an object detector using a Cascade Mask R-CNN with a hierarchical level prediction head, incorporating Mean Teacher self-training with adaptive targets where the teacher model provides progressively more reliable supervision as training progresses.

## Key Results
- Improves Mask AR from 20.2 to 22.5 on LVIS dataset
- Improves Mask AR from 17.0 to 26.0 on SA-1B dataset
- Requires only 1/5 of the images and 1/12 of the iterations compared to prior work

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical adaptive clustering improves object coverage by adaptively determining the number of objects per image based on semantic feature similarity
- **Mechanism:** The method iteratively merges adjacent regions with highest feature similarity until a threshold is reached, allowing more objects in heterogeneous scenes and fewer in homogeneous ones
- **Core assumption:** Visual representations from self-supervised models preserve semantic similarity that correlates with object boundaries
- **Evidence anchors:**
  - [abstract] "HASSOD employs a hierarchical adaptive clustering strategy to group regions into object masks based on self-supervised visual representations, adaptively determining the number of objects per image."
  - [section 3.1] "By adjusting the threshold for terminating the clustering process, HASSOD is capable of effectively determining the appropriate number of objects per image, thus better leveraging the learning signals in images with multiple objects."
- **Break condition:** If visual features don't preserve semantic boundaries or similarity doesn't correlate with objectness, merging won't produce meaningful object masks

### Mechanism 2
- **Claim:** Identifying hierarchical levels (whole/part/subpart) through coverage analysis improves detection performance and interpretability
- **Mechanism:** Masks are organized into a tree structure based on coverage relations; root nodes are "whole" objects, direct children are "parts," and descendants are "subparts," enabling composition-aware detection
- **Core assumption:** Coverage relations between masks reliably indicate compositional hierarchy (e.g., a wheel is part of a car)
- **Evidence anchors:**
  - [abstract] "Furthermore, HASSOD identifies the hierarchical levels of objects in terms of composition, by analyzing coverage relations between masks and constructing tree structures."
  - [section 3.2] "Using all such coverage relations, we can construct a forest of trees that contain all masks in an image. Ultimately, the roots of all trees in this image are considered as 'whole' objects, their direct children are 'part' objects, and all the remaining descendants are 'subpart' objects."
- **Break condition:** If coverage relations don't reflect true compositional hierarchy or if objects have complex overlapping structures, the tree construction may misrepresent relationships

### Mechanism 3
- **Claim:** Mean Teacher adaptation with adaptive targets provides smoother, more efficient training than multi-round self-training
- **Mechanism:** A teacher-student framework where the teacher provides evolving supervision, with loss weights gradually shifting from initial noisy pseudo-labels to teacher predictions as the teacher becomes more reliable
- **Core assumption:** The teacher model becomes progressively more accurate through exponential moving average updates, making its predictions superior to initial pseudo-labels over time
- **Evidence anchors:**
  - [abstract] "Lastly, we abandon the inefficient multi-round self-training process utilized in prior methods and instead adapt the Mean Teacher framework from semi-supervised learning, which leads to a smoother and more efficient training process."
  - [section 3.3] "Different from standard Mean Teacher, our method employs adaptive training targets, as we gradually adjust the loss weights for the two branches. This is because the initial pseudo-labels may not effectively cover all possible objects, while the teacher model will progressively improve as a better source of supervision."
- **Break condition:** If teacher updates are too slow or noisy, the adaptive target schedule may converge to suboptimal supervision sources

## Foundational Learning

- **Concept:** Self-supervised visual representation learning (e.g., DINO)
  - Why needed here: Provides semantic features without human labels for clustering regions into object masks
  - Quick check question: How does DINO ensure that its features capture object boundaries without explicit supervision?

- **Concept:** Agglomerative hierarchical clustering
  - Why needed here: Groups adjacent image patches into semantically coherent object masks based on feature similarity
  - Quick check question: What determines when the merging process should stop in the clustering algorithm?

- **Concept:** Exponential moving average (EMA) in teacher-student frameworks
  - Why needed here: Smooths teacher model updates to provide stable training targets for the student model
  - Quick check question: How does EMA help prevent the teacher from providing noisy or unstable supervision?

## Architecture Onboarding

- **Component map:**
  DINO ViT-B/8 → Patch merging → Hierarchical adaptive clustering → Pseudo-labels with hierarchical levels → Cascade Mask R-CNN with hierarchical level prediction head → Mean Teacher self-training

- **Critical path:**
  1. Extract DINO features from unlabeled images
  2. Perform hierarchical adaptive clustering to generate initial pseudo-labels
  3. Train object detector on pseudo-labels with hierarchical level prediction
  4. Apply Mean Teacher with adaptive targets for self-training refinement

- **Design tradeoffs:**
  - Multiple merging thresholds vs. single threshold: Ensemble improves coverage but increases computation
  - Small patch size (8×8) vs. larger patches: Better pseudo-label quality but more regions to process
  - Hierarchical level prediction vs. standard detection: Better interpretability but added complexity

- **Failure signatures:**
  - Poor pseudo-label quality: Check merging threshold selection and post-processing steps
  - Teacher-student instability: Verify EMA update rate and adaptive target schedule
  - Hierarchical misclassification: Examine coverage threshold and tree construction logic

- **First 3 experiments:**
  1. Vary θmerge thresholds to observe impact on pseudo-label quantity and quality
  2. Test different patch sizes in DINO backbone to optimize feature locality
  3. Compare adaptive vs. fixed target weights in Mean Teacher to measure training efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HASSOD's performance scale when trained on datasets significantly larger than MS-COCO, such as ImageNet?
- Basis in paper: The paper notes that HASSOD is trained on MS-COCO (0.24 million images), which is only 1/5 of ImageNet used by prior work. It mentions that training on ImageNet would lead to a stronger detector and leaves large-scale training as a future direction.
- Why unresolved: The authors did not conduct experiments with larger datasets due to limited computational resources. They hypothesize that using ImageNet for detector training would improve performance, but this remains untested.
- What evidence would resolve it: Conducting experiments training HASSOD on ImageNet or similar large-scale datasets and comparing the performance to the current MS-COCO-trained model.

### Open Question 2
- Question: Can HASSOD's hierarchical level predictions be improved to better align with human perception, especially for objects with complex compositions?
- Basis in paper: The paper acknowledges that HASSOD's learned object hierarchical levels may not be perfectly aligned with human perception, leading to over-segmentation or under-segmentation of objects. Examples include treating overlapping players as a single object or failing to recognize a motorcycle as a whole entity.
- Why unresolved: The self-supervised nature of HASSOD limits its ability to align with human perception without additional supervision. The paper suggests that further human supervision would be necessary to correct these mistakes.
- What evidence would resolve it: Incorporating human supervision or developing new methods to improve the alignment of HASSOD's hierarchical predictions with human perception, followed by evaluating the improved model on complex scenes.

### Open Question 3
- Question: How does HASSOD perform in detecting and segmenting objects in domains outside natural images, such as medical imaging or satellite imagery?
- Basis in paper: The paper mentions that HASSOD's ability to distinguish semantically different parts within objects could be advantageous in medical imaging for identifying tissues or anatomical structures. It also suggests potential applications in manufacturing quality control.
- Why unresolved: The paper primarily evaluates HASSOD on natural image datasets (Objects365, LVIS, SA-1B) and does not explore its performance in specialized domains.
- What evidence would resolve it: Training and evaluating HASSOD on datasets from specialized domains like medical imaging or satellite imagery, and comparing its performance to domain-specific models or human experts.

## Limitations

- Hierarchical adaptive clustering relies heavily on DINO feature quality, with no ablation studies on feature quality impact
- Coverage-based hierarchical level identification may not accurately represent compositional hierarchy in complex scenes with occlusions or abstract relationships
- Mean Teacher adaptation's effectiveness depends on proper scheduling of adaptive targets, but sensitivity analysis for these parameters is not provided

## Confidence

- **High Confidence:** Mask AR improvements on LVIS (20.2→22.5) and SA-1B (17.0→26.0) with significantly reduced training iterations and image count
- **Medium Confidence:** The hierarchical adaptive clustering approach's general effectiveness, though specific implementation details are limited
- **Low Confidence:** The specific impact of hierarchical level prediction on downstream performance and the robustness of coverage-based hierarchy construction

## Next Checks

1. Conduct ablation studies varying DINO feature quality (different pre-trained models) to quantify its impact on clustering quality and final detection performance
2. Test the hierarchical level prediction head's contribution by comparing performance with and without this component while controlling for other variables
3. Analyze the coverage threshold sensitivity by evaluating performance across a range of θcov values to determine optimal settings for different dataset characteristics