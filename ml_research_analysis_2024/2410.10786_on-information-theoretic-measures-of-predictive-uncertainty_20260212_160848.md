---
ver: rpa2
title: On Information-Theoretic Measures of Predictive Uncertainty
arxiv_id: '2410.10786'
source_url: https://arxiv.org/abs/2410.10786
tags:
- uncertainty
- measures
- predictive
- posterior
- predicting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a unifying framework for information-theoretic
  measures of predictive uncertainty in machine learning, addressing the lack of consensus
  on how to quantify uncertainty. The framework categorizes measures based on two
  factors: the predicting model and the approximation of the true predictive distribution,
  deriving nine distinct uncertainty measures that include both known and newly introduced
  ones.'
---

# On Information-Theoretic Measures of Predictive Uncertainty

## Quick Facts
- arXiv ID: 2410.10786
- Source URL: https://arxiv.org/abs/2410.10786
- Authors: Kajetan Schweighofer; Lukas Aichberger; Mykyta Ielanskyi; Sepp Hochreiter
- Reference count: 40
- Primary result: Framework categorizes uncertainty measures based on predicting model and true distribution approximation, finding alignment crucial for in-distribution data while aleatoric uncertainty often outperforms epistemic for out-of-distribution detection.

## Executive Summary
This paper addresses the lack of consensus on how to quantify predictive uncertainty in machine learning by proposing a unifying framework that categorizes information-theoretic uncertainty measures based on two key factors: the predicting model (single model, average model, or model sampled from posterior) and the approximation of the true predictive distribution. The framework derives nine distinct uncertainty measures, including both known and newly introduced ones, and provides a systematic way to understand their relationships and assumptions.

The authors conduct extensive experiments across multiple datasets and tasks, demonstrating that aligning the uncertainty measure with the predicting model is crucial for in-distribution data, particularly in selective prediction and misclassification detection. Surprisingly, they find that aleatoric uncertainty measures often outperform epistemic uncertainty measures for out-of-distribution detection, contrary to common assumptions in the literature. The study also reveals that the disentanglement between measures varies substantially between in-distribution and out-of-distribution data, with strong correlations among measures on in-distribution data that weaken significantly for out-of-distribution scenarios.

## Method Summary
The paper proposes a framework that decomposes predictive uncertainty into aleatoric (model-dependent) and epistemic (model-mismatch dependent) components. The framework considers three predicting model choices (single model A, average model B, model sampled from posterior C) and three true distribution approximations (single model 1, posterior predictive 2, all models weighted by posterior 3), creating nine uncertainty measures. The authors evaluate these measures using deep ensembles trained on CIFAR10, CIFAR100, SVHN, Tiny-ImageNet, and LSUN datasets, testing selective prediction, misclassification detection, OOD detection, and active learning tasks. Monte Carlo sampling is used for posterior expectations, and performance is measured using AUARC, AUROC, and accuracy metrics.

## Key Results
- Aligning uncertainty measure with predicting model significantly improves selective prediction and misclassification detection on in-distribution data
- Aleatoric uncertainty measures outperform epistemic measures for out-of-distribution detection
- Strong correlations among measures on in-distribution data weaken substantially for out-of-distribution scenarios
- Mutual information performs best as active learning acquisition function, while random baselines are surprisingly effective for small datasets

## Why This Works (Mechanism)

### Mechanism 1
Aligning the uncertainty measure with the predicting model significantly improves selective prediction and misclassification detection performance. When the uncertainty measure's assumptions match how the model actually makes predictions, the measure better captures the relevant sources of uncertainty. For a single fixed model (A), epistemic uncertainty (EU) dominates because the model has limited knowledge. For model averaging (B/C), total uncertainty (TU) dominates because the combination of models captures more information.

### Mechanism 2
Aleatoric uncertainty (AU) measures often outperform epistemic uncertainty (EU) measures for out-of-distribution (OOD) detection. AU measures capture the inherent uncertainty in the predicting model's distribution, which tends to increase more dramatically when encountering data from different distributions. EU measures, which capture knowledge gaps, may not distinguish OOD data as effectively since the model's fundamental uncertainty about its own predictions increases more reliably.

### Mechanism 3
The disentanglement between uncertainty measures varies substantially between in-distribution and out-of-distribution data. On ID data, measures that share similar assumptions (e.g., all using a single predicting model) become highly correlated because they capture similar aspects of uncertainty. On OOD data, the different assumptions lead to more diverse uncertainty patterns, causing measures to become less correlated as they capture different aspects of the distributional mismatch.

## Foundational Learning

- Concept: Information-theoretic entropy and its role in quantifying uncertainty
  - Why needed here: The paper builds its framework on decomposing entropy into aleatoric and epistemic components, so understanding basic entropy concepts is essential for grasping the measures
  - Quick check question: What does high entropy in a predictive distribution indicate about the model's certainty?

- Concept: Bayesian model averaging and posterior predictive distributions
  - Why needed here: The framework considers different predicting models including single models, Bayesian model averages, and models sampled from the posterior, so understanding these concepts is crucial
  - Quick check question: How does Bayesian model averaging differ from using a single model for prediction?

- Concept: Kullback-Leibler divergence and cross-entropy
  - Why needed here: The paper uses KL divergence and cross-entropy as fundamental building blocks for decomposing uncertainty, so familiarity with these measures is important
  - Quick check question: What does KL divergence between two distributions measure?

## Architecture Onboarding

- Component map: Predicting models (A: single, B: average, C: posterior sample) → True distribution approximations (1: single, 2: posterior predictive, 3: all models) → Nine uncertainty measures (TU/AU/EU variants)
- Critical path: 1) Select predicting model based on inference strategy, 2) Choose true distribution approximation based on available information, 3) Compute the corresponding measure using Monte Carlo sampling for posterior expectations, 4) Use the measure for the target task
- Design tradeoffs: Single model approaches (A) are computationally efficient but may miss uncertainty from model uncertainty; averaging approaches (B/C) capture more uncertainty but require more computation; approximation choice 1 is simple but biased, while choice 3 is comprehensive but computationally expensive
- Failure signatures: Poor performance when measure and predicting model are misaligned; unexpectedly high correlation between measures on OOD data suggesting insufficient disentanglement; EU measures failing on OOD detection suggesting aleatoric uncertainty dominates in distributional shift scenarios
- First 3 experiments:
  1. Reproduce selective prediction results comparing TU(A3) vs TU(B/C3) to verify aligning measure with predicting model matters
  2. Test OOD detection comparing AU(B/C) vs EU measures to confirm aleatoric outperforms epistemic
  3. Compute rank correlations between measures on both ID and OOD data to observe the disentanglement patterns

## Open Questions the Paper Calls Out

### Open Question 1
How do different posterior sampling methods (global vs local) impact the effectiveness of aligning uncertainty measures with the predicting model? The paper compares Deep Ensembles (global) with Laplace Approximation and MC Dropout (local) and finds different patterns in measure effectiveness, but doesn't fully explain why this occurs or when each approach is optimal.

### Open Question 2
Under what conditions do epistemic uncertainty measures become more effective than aleatoric/total uncertainty measures for out-of-distribution detection? The authors observe that epistemic uncertainty measures generally underperform aleatoric measures for OOD detection, except for one dataset pairing (TIN/SVHN), but don't characterize the properties that make this pairing different.

### Open Question 3
How does the size of the training dataset affect the reliability of uncertainty measures as acquisition functions in active learning? The paper observes that random sampling performs surprisingly well for small datasets and that epistemic measures underperform for MNIST/FMNIST, suggesting model uncertainty dominates early learning, but doesn't establish a theoretical framework for when different uncertainty measures become informative as data accumulates.

## Limitations

- The framework assumes clean separation between predicting model and true distribution approximations, but in practice these are often entangled in complex ways
- Results are primarily shown for image classification tasks and may not generalize to regression, structured prediction, or other domains
- The paper doesn't provide detailed runtime comparisons or discuss memory requirements for maintaining deep ensembles across all experiments

## Confidence

**High confidence**: The core finding that aligning uncertainty measures with predicting models improves performance on in-distribution data is well-supported by consistent results across multiple tasks and datasets. The observation that aleatoric uncertainty outperforms epistemic for OOD detection is robust across experimental conditions.

**Medium confidence**: The claim about disentanglement patterns varying between ID and OOD data is supported by correlation analysis, but the qualitative interpretation of "highly correlated blocks" could benefit from additional quantitative metrics.

**Low confidence**: The suggestion that random baselines work well for small datasets is based on limited experiments with only MNIST and FMNIST, and the implications for other domains or larger dataset sizes are unclear.

## Next Checks

1. Apply the framework to non-image domains (e.g., NLP or tabular data) to test whether the alignment principle holds across different data modalities and task types

2. Measure and compare the actual computational costs (runtime and memory) of each uncertainty measure across different implementations to validate the claimed efficiency tradeoffs

3. Test how performance degrades when the posterior approximation is poor (e.g., using fewer ensemble members or variational approximations) to understand the robustness of each measure to posterior estimation quality