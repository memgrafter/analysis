---
ver: rpa2
title: 'Interplay between Federated Learning and Explainable Artificial Intelligence:
  a Scoping Review'
arxiv_id: '2411.05874'
source_url: https://arxiv.org/abs/2411.05874
tags:
- data
- federated
- learning
- methods
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This scoping review maps the intersection of federated learning
  (FL) and explainable artificial intelligence (XAI), focusing on publications that
  address the impact of FL on model interpretability and post-hoc explanations. Out
  of 37 studies meeting the inclusion criteria, only one explicitly and quantitatively
  analyzed the influence of FL on model explanations, revealing a significant research
  gap.
---

# Interplay between Federated Learning and Explainable Artificial Intelligence: a Scoping Review

## Quick Facts
- **arXiv ID:** 2411.05874
- **Source URL:** https://arxiv.org/abs/2411.05874
- **Reference count:** 40
- **Key outcome:** Only one study quantitatively analyzed FL's impact on explanations, revealing a major research gap in federated-XAI interplay.

## Executive Summary
This scoping review systematically maps the intersection of federated learning (FL) and explainable AI (XAI) by analyzing 37 peer-reviewed studies published between 2019 and April 2023. The review reveals that while FL can achieve accuracy similar to centralized models when properly synchronized, aggregating interpretability metrics across nodes dilutes node-specific insights. The study highlights that explanation methods can safeguard FL against malicious nodes, but empirical research on the bidirectional impact of FL and XAI remains scarce. The review calls for more structured, quantitative studies and transparent reporting practices to understand when and how FL affects model interpretability.

## Method Summary
The study conducted a systematic scoping review following PRISMA guidelines, searching multiple databases (IEEEXplore, Google Scholar, PubMed, Scopus, Web of Science) for papers published 2019-2023. Dual independent screening and extraction were performed using predefined inclusion criteria: studies must propose FL+XAI methods, report FL+XAI experiments, or assess FL's impact on explanations. Data extraction included study details, XAI nomenclature, FL type, data characteristics, and interplay descriptions. Disagreements were resolved through discussion, though exact resolution criteria were unspecified.

## Key Results
- Only one study explicitly and quantitatively analyzed FL's impact on model explanations
- Aggregating interpretability metrics across FL nodes creates generalized global insights but dilutes node-specific patterns
- Several studies proposed FL algorithms incorporating explanation methods to detect and mitigate malicious nodes
- Studies using established FL libraries or following reporting guidelines are in the minority
- Major research gap exists in understanding under which conditions FL affects interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FL can achieve accuracy similar to centralized models when synchronization is optimized, while preserving privacy through distributed training
- Mechanism: FL trains local models on each client's data, then aggregates model updates centrally without sharing raw data, allowing model performance to match centralized learning if aggregation and synchronization are well-designed
- Core assumption: Model updates from different clients can be aggregated without significant loss of information or performance
- Evidence anchors:
  - [abstract] states that FL can achieve accuracy similar to centralized models when synchronization is optimized
  - [section] describes how the central server aggregates local model updates without sharing raw data
- Break condition: If client data distributions are highly non-IID, aggregation may lead to poor performance or bias

### Mechanism 2
- Claim: Aggregating interpretability metrics across FL nodes creates generalized global insights at the expense of node-specific patterns being diluted
- Mechanism: When interpretability metrics (e.g., feature importance, attention weights) are averaged across clients, the resulting global explanation is less specific to any one client's data distribution
- Core assumption: Interpretability metrics are aggregatable without loss of local meaning
- Evidence anchors:
  - [abstract] reports that aggregation of interpretability metrics across FL nodes created generalized global insights at the expense of node-specific patterns being diluted
  - [section] explains that attention weights from multiple clients are aggregated into a global saliency map, reducing the visibility of individual client contributions
- Break condition: If node-specific insights are critical for the application, global aggregation may obscure important local behaviors

### Mechanism 3
- Claim: Explanation methods can be integrated into FL algorithms to safeguard against malicious nodes and improve model robustness
- Mechanism: By computing feature importance or Shapley values locally and using them to detect anomalies or filter malicious updates, the FL process can reject harmful contributions and personalize models
- Core assumption: Explanation methods can reliably detect malicious behavior without exposing sensitive data
- Evidence anchors:
  - [abstract] mentions that several studies proposed FL algorithms incorporating explanation methods to safeguard the learning process against defaulting or malicious nodes
  - [section] details how SHAP values and LIME are used to detect and mitigate malicious attacks in FL
- Break condition: If explanation methods themselves leak sensitive information or are computationally prohibitive, their integration may not be feasible

## Foundational Learning

- Concept: Federated Learning (FL) fundamentals
  - Why needed here: Understanding FL's distributed training and aggregation mechanics is essential to reason about how model updates and explanations interact
  - Quick check question: In FL, where does the central server aggregate model updates without accessing raw data?

- Concept: Explainable AI (XAI) taxonomy
  - Why needed here: Distinguishing between interpretable models (inherently understandable) and post-hoc explanations (computed after training) is crucial for mapping the interplay
  - Quick check question: What is the key difference between algorithmic transparency and local explanations in XAI?

- Concept: Feature importance methods (e.g., SHAP, LIME)
  - Why needed here: These methods are central to quantifying the impact of features in FL contexts and detecting malicious behavior
  - Quick check question: How do SHAP values help in identifying influential features in a model?

## Architecture Onboarding

- Component map: Clients -> Central server -> Aggregation protocol
- Critical path:
  1. Clients train local models and compute explanations
  2. Explanations are aggregated (or kept local for privacy)
  3. Central server updates global model and monitors for anomalies
- Design tradeoffs:
  - Aggregation vs. privacy: Aggregating explanations globally increases insight but may expose sensitive patterns
  - Computational cost: Computing explanations locally adds overhead but protects privacy
  - Model performance vs. interpretability: Aggregating metrics may improve generalization but reduce local interpretability
- Failure signatures:
  - Poor performance: Aggregation fails due to non-IID data
  - Privacy breach: Local explanations reveal sensitive patterns when shared
  - Malicious attacks: Explanation-based filters fail to detect bad actors
- First 3 experiments:
  1. Simulate FL on a small dataset with two clients; compare centralized vs. federated accuracy
  2. Apply SHAP locally on each client; aggregate and compare to centralized SHAP
  3. Introduce a malicious client; test if explanation-based filters detect and mitigate the attack

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does federated learning (FL) fundamentally alter the structure of machine learning models compared to centralized training, and if so, how does this impact interpretability?
- Basis in paper: [explicit] The review notes that FL can produce better generalizing models at the cost of missing some interpretable insights from local nodes, and that aggregating feature relevance or attention weights across nodes can lead to more generalized but less interpretable global insights
- Why unresolved: While the review highlights this potential trade-off, it does not provide a comprehensive analysis of the specific structural changes in models caused by FL and their direct impact on interpretability. The evidence is primarily qualitative and based on a limited number of studies
- What evidence would resolve it: Rigorous experimental studies comparing the internal structures of models trained via FL and centralized learning, coupled with quantitative assessments of interpretability metrics before and after FL training, would be needed to definitively answer this question

### Open Question 2
- Question: Can federated learning (FL) and explainable artificial intelligence (XAI) be effectively combined to improve both model accuracy and interpretability without compromising privacy?
- Basis in paper: [explicit] The review identifies a research gap in studies that quantitatively analyze the impact of FL on explanations, suggesting that the interplay between FL and XAI is not fully understood. It also notes that explanation methods designed for federated-trained ML models were limited to feature aggregation, control of information sharing, and counterfactual explanations in VFL
- Why unresolved: While some studies have explored combining FL and XAI, there is a lack of comprehensive research on how to effectively integrate these technologies to achieve both accuracy and interpretability while maintaining privacy. The review highlights the need for more structured and transparent research practices in this area
- What evidence would resolve it: Studies that propose and evaluate novel FL methods specifically designed to enhance interpretability while preserving privacy, along with quantitative assessments of their impact on model accuracy and explainability, would help resolve this question

### Open Question 3
- Question: How can federated learning (FL) be adapted to preserve local interpretability patterns while still achieving global model performance?
- Basis in paper: [explicit] The review discusses how aggregating feature relevance or attention weights across nodes can lead to more generalized global insights but can also result in sacrificing some degree of localized interpretability, as reported in the case of an attention mechanism where the contribution of individual client models in the aggregated saliency map was diluted
- Why unresolved: While the review identifies this challenge, it does not provide concrete solutions for how FL can be adapted to retain node-specific insights without compromising global model integrity. The evidence is primarily based on a limited number of studies and qualitative observations
- What evidence would resolve it: Research that proposes and evaluates novel aggregation strategies or federated explanation techniques that balance global model performance with the preservation of local interpretability patterns would help resolve this question

## Limitations

- Only one study quantitatively analyzed FL's impact on explanations, limiting empirical validation of key claims
- Terminological ambiguity between "explainable" and "interpretable" may have excluded relevant studies
- The review lacks specification of sample sizes, data distributions, and statistical significance for aggregation effects

## Confidence

- **High Confidence:** FL can achieve accuracy similar to centralized models when synchronization is optimized
- **Medium Confidence:** Aggregating interpretability metrics across FL nodes dilutes node-specific patterns, based on one study's findings
- **Low Confidence:** Explanation methods reliably detect malicious nodes without privacy leaks, as this claim lacks quantitative validation in the corpus

## Next Checks

1. Replicate the single study that quantitatively analyzed FL's impact on explanations using different datasets and aggregation methods
2. Conduct a controlled experiment comparing local vs. global explanation aggregation on non-IID data to measure information loss
3. Test explanation-based malicious node detection across multiple attack scenarios and privacy threat models