---
ver: rpa2
title: Prompt-based Visual Alignment for Zero-shot Policy Transfer
arxiv_id: '2406.03250'
source_url: https://arxiv.org/abs/2406.03250
tags:
- visual
- prompt
- domains
- image
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of zero-shot policy transfer
  in reinforcement learning by mitigating domain bias in visual observations. The
  proposed method, Prompt-based Visual Alignment (PVA), uses a pretrained visual-language
  model as an explicit semantic constraint to train a visual aligner that maps images
  from multiple domains to a unified domain.
---

# Prompt-based Visual Alignment for Zero-shot Policy Transfer

## Quick Facts
- **arXiv ID**: 2406.03250
- **Source URL**: https://arxiv.org/abs/2406.03250
- **Authors**: Haihan Gao; Rui Zhang; Qi Yi; Hantao Yao; Haochen Li; Jiaming Guo; Shaohui Peng; Yunkai Gao; QiCheng Wang; Xing Hu; Yuanbo Wen; Zihao Zhang; Zidong Du; Ling Li; Qi Guo; Yunji Chen
- **Reference count**: 28
- **Primary result**: PVA achieves superior zero-shot generalization performance on unseen domains compared to state-of-the-art methods, while requiring limited access to cross-domain data.

## Executive Summary
This work addresses the challenge of zero-shot policy transfer in reinforcement learning by mitigating domain bias in visual observations. The proposed method, Prompt-based Visual Alignment (PVA), uses a pretrained visual-language model as an explicit semantic constraint to train a visual aligner that maps images from multiple domains to a unified domain. PVA employs prompt tuning to generate domain-adaptive instance-conditional prompts that accurately describe semantic information in input images. Experiments on a vision-based autonomous driving task with the CARLA simulator demonstrate that PVA achieves superior zero-shot generalization performance on unseen domains compared to state-of-the-art methods, while requiring limited access to cross-domain data.

## Method Summary
PVA is a three-stage framework for zero-shot policy transfer that leverages a pretrained visual-language model (CLIP) as an explicit semantic constraint. In the first stage, domain-adaptive instance-conditional prompts are generated through prompt tuning, capturing global, domain-specific, and instance-conditional information. The second stage trains a visual aligner to map images from multiple domains to a unified domain while preserving semantic content described by the prompts. Finally, an RL agent is trained using the aligned images to learn a robust policy. PVA aims to learn unified cross-domain representations under limited access to cross-domain data, improving zero-shot generalization performance on unseen domains.

## Key Results
- PVA achieves superior zero-shot generalization performance on unseen domains compared to state-of-the-art methods
- PVA requires limited access to cross-domain data (100 images from 2 different domains)
- PVA demonstrates robust policy transfer in vision-based autonomous driving tasks with the CARLA simulator

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The visual aligner maps images from multiple training domains to a unified domain by aligning visual embeddings with semantic text prompts.
- Mechanism: The visual aligner takes an image from any domain and transforms it into a target domain while preserving the semantic content described by the learned prompt. This is done by optimizing the aligner with loss terms that match the visual features of the transformed image with the text embedding of the corresponding prompt.
- Core assumption: The CLIP model provides a meaningful and consistent embedding space where images and text descriptions of the same semantic content have high cosine similarity.
- Evidence anchors:
  - [abstract] "we leverage the semantic information contained in a text sequence as an explicit constraint to train a visual aligner"
  - [section 4.2] "With the tuned prompts obtained in the first stage, we train a prompt-based visual aligner gθ to align the domain-specific information"
- Break condition: If CLIP embeddings are not semantically meaningful, the visual aligner will not learn to preserve content and will fail to generalize across domains.

### Mechanism 2
- Claim: Prompt tuning generates domain-adaptive instance-conditional prompts that accurately describe semantic information in input images.
- Mechanism: The prompt is composed of three learnable parts: global (task-level), domain-specific, and instance-conditional. Each part is optimized with a contrastive loss that aligns it with the corresponding visual embedding, enabling the prompt to capture information at different granularities.
- Core assumption: Separating the prompt into domain-shared, domain-specific, and instance-conditional components allows for fine-grained semantic descriptions that improve the visual aligner's performance.
- Evidence anchors:
  - [section 4.1.1] "To fulfill this target, we separate the prompt into different parts to depict domain-shared, domain-specific, and instance-conditional level information respectively"
  - [section 4.1.2] "Tune the global and domain-specific prompts: Given image I of domain c, we aim to match the corresponding prompt P c I with image I contrasting to prompt P i I, i ≠ c of other domains"
- Break condition: If the prompt components are not properly tuned, the semantic descriptions will be inaccurate, leading to poor visual alignment and degraded policy transfer performance.

### Mechanism 3
- Claim: Using explicit semantic constraints from the VLM enables learning unified cross-domain representations with limited multi-domain data.
- Mechanism: The visual aligner is trained to minimize the difference between the visual embedding of the aligned image and the text embedding of the prompt. This forces the aligner to preserve semantic content during domain mapping, reducing domain bias and improving generalization.
- Core assumption: The semantic information contained in the text prompt is sufficient to constrain the visual aligner and ensure the preservation of task-relevant content across domains.
- Evidence anchors:
  - [abstract] "PV A can learn unified cross-domain representation under limited access to cross-domain data"
  - [section 4.2] "Aside from matching the global visual semantic information with the prompt P u I, we also want the local features of the output image to match the text description"
- Break condition: If the semantic information in the prompt is not sufficient to describe the relevant content, the visual aligner may discard important features, leading to poor policy performance in the target domain.

## Foundational Learning

- Concept: Visual-Language Models (VLMs) like CLIP
  - Why needed here: VLMs provide a shared embedding space for images and text, allowing the use of text prompts as explicit semantic constraints for visual alignment.
  - Quick check question: How does CLIP map images and text into a common embedding space, and what is the significance of the cosine similarity between these embeddings?

- Concept: Prompt Tuning
  - Why needed here: Prompt tuning allows the generation of domain-adaptive instance-conditional prompts that accurately describe the semantic content of input images, improving the visual aligner's performance.
  - Quick check question: What are the three components of the prompt in PVA, and how are they optimized to capture different levels of semantic information?

- Concept: Reinforcement Learning in Visual Domains
  - Why needed here: The ultimate goal of PVA is to improve the zero-shot policy transfer performance of RL agents in visual-based tasks by mitigating domain bias in the observations.
  - Quick check question: How does domain bias in visual observations affect the performance of RL agents, and why is zero-shot transfer important in real-world applications?

## Architecture Onboarding

- Component map: CLIP -> Prompt Learner -> Visual Aligner -> RL Agent
- Critical path:
  1. Prompt tuning stage: Generate accurate prompts for each image.
  2. Visual alignment stage: Train the visual aligner to map images to a unified domain using the prompts.
  3. Robust policy training stage: Train the RL agent using the aligned images.

- Design tradeoffs:
  - Using CLIP as the VLM vs. training a custom model: CLIP is pretrained on a large dataset, providing a good starting point for semantic alignment, but may not be optimal for the specific task.
  - Number of domains used for training: More domains may improve generalization, but also increase the complexity of the visual aligner and the amount of data required.

- Failure signatures:
  - Poor prompt tuning: Inaccurate prompts lead to poor visual alignment and degraded policy performance.
  - Overfitting of the visual aligner: The aligner may overfit to the training domains and fail to generalize to unseen domains.
  - Insufficient semantic information in prompts: The prompts may not capture all relevant information, leading to loss of important features during alignment.

- First 3 experiments:
  1. Verify that the prompt learner can generate accurate prompts for images from different domains by checking the cosine similarity between the prompt embeddings and the corresponding image embeddings.
  2. Test the visual aligner's ability to map images from the training domains to the unified domain by visualizing the aligned images and comparing them to the original images.
  3. Evaluate the zero-shot policy transfer performance of the RL agent on unseen domains by measuring the average reward achieved in these domains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PVA scale with increasing numbers of training domains beyond the two used in the experiments?
- Basis in paper: [explicit] The paper only evaluates PVA with two training domains (ClearNoon and HardRainNoon).
- Why unresolved: The paper does not explore scenarios with more than two training domains, leaving the scalability question unanswered.
- What evidence would resolve it: Experiments evaluating PVA with varying numbers of training domains (e.g., 3, 5, 10) to determine performance trends and potential saturation points.

### Open Question 2
- Question: What is the impact of different types of weather conditions on PVA's performance, particularly extreme conditions not included in the training set?
- Basis in paper: [inferred] The paper tests PVA on several weather conditions but does not explore the limits of its generalization to extreme or novel weather types.
- Why unresolved: The paper does not include experiments with extreme weather conditions (e.g., heavy fog, snow) to assess PVA's robustness.
- What evidence would resolve it: Testing PVA on a wide range of extreme weather conditions to evaluate its performance and identify potential failure modes.

### Open Question 3
- Question: How does PVA compare to other domain adaptation methods when the training data is highly imbalanced across domains?
- Basis in paper: [inferred] The paper uses balanced datasets for training but does not address scenarios where training data is imbalanced.
- Why unresolved: The paper does not explore the impact of imbalanced training data on PVA's performance relative to other methods.
- What evidence would resolve it: Experiments comparing PVA and other domain adaptation methods using datasets with varying levels of imbalance to assess robustness and performance.

### Open Question 4
- Question: What is the computational overhead of PVA compared to other state-of-the-art methods, and how does it scale with image resolution?
- Basis in paper: [explicit] The paper mentions training times but does not provide a detailed comparison of computational overhead with other methods.
- Why unresolved: The paper lacks a comprehensive analysis of PVA's computational efficiency and scalability.
- What evidence would resolve it: Detailed benchmarking of PVA's computational requirements (e.g., memory usage, inference time) across different image resolutions and comparisons with other methods.

## Limitations
- Evaluation limited to a single autonomous driving task with controlled domain variations
- Method requires access to a small number of images from target domains during training
- Visual aligner's architecture details are not fully specified, making exact reproduction challenging

## Confidence
- **High confidence**: The mechanism of using CLIP embeddings as semantic constraints is well-established
- **Medium confidence**: The three-stage framework (prompt tuning → visual alignment → policy training) is logically sound but relies on specific implementation details
- **Medium confidence**: Claims about superior zero-shot generalization are supported by CARLA experiments but limited to specific domain types

## Next Checks
1. Test PVA on a different RL domain (e.g., robotic manipulation) with more diverse domain shifts to assess generalizability
2. Evaluate performance when only minimal cross-domain data is available (e.g., 10-20 images per domain) to test robustness
3. Compare with alternative semantic alignment methods that don't rely on CLIP to isolate the contribution of the specific VLM choice