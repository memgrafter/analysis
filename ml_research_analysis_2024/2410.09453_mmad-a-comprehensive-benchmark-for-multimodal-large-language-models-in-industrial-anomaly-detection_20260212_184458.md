---
ver: rpa2
title: 'MMAD: A Comprehensive Benchmark for Multimodal Large Language Models in Industrial
  Anomaly Detection'
arxiv_id: '2410.09453'
source_url: https://arxiv.org/abs/2410.09453
tags:
- mllms
- image
- anomaly
- arxiv
- defect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MMAD is the first benchmark evaluating multimodal large language
  models (MLLMs) for industrial anomaly detection. The dataset contains 39,672 multiple-choice
  questions covering 7 key subtasks (e.g., defect classification, localization, analysis)
  for 8,366 industrial images from 38 product categories.
---

# MMAD: A Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection

## Quick Facts
- arXiv ID: 2410.09453
- Source URL: https://arxiv.org/abs/2410.09453
- Authors: Xi Jiang; Jian Li; Hanqiu Deng; Yong Liu; Bin-Bin Gao; Yifeng Zhou; Jialin Li; Chengjie Wang; Feng Zheng
- Reference count: 25
- Primary result: Commercial MLLMs achieved 74.9% accuracy on industrial anomaly detection tasks, outperforming open-source models but falling short of industrial standards

## Executive Summary
MMAD introduces the first comprehensive benchmark for evaluating Multimodal Large Language Models (MLLMs) in industrial anomaly detection. The dataset contains 39,672 multiple-choice questions across 7 subtasks for 8,366 industrial images from 38 product categories. Commercial models like GPT-4o achieved 74.9% accuracy, significantly outperforming open-source alternatives but still not meeting industrial quality inspection standards. The evaluation reveals that MLLMs struggle particularly with anomaly-related questions compared to object-related ones, and even specialized industrial models underperform due to overfitting. The study explores two training-free enhancement strategies—RAG and expert agents—to improve performance.

## Method Summary
The MMAD benchmark was constructed using a data generation pipeline where GPT-4V captions were first generated for industrial images, followed by question generation and manual filtering to ensure quality. The evaluation uses a 1-shot setting with template images, testing MLLMs across 7 subtasks including defect classification, localization, and analysis. Performance is measured using multiple-choice accuracy, with comparisons between commercial models (GPT-4o, Gemini 1.5), open-source models (InternVL2, Qwen-VL, LLaVA), and specialized industrial models (AnomalyGPT). Two enhancement strategies were explored: retrieval-augmented generation (RAG) and expert agents that combine multiple models with visualization tools.

## Key Results
- Commercial models achieved 74.9% average accuracy, significantly outperforming open-source models
- MLLMs showed particular weakness in anomaly-related tasks compared to object-related tasks
- Model performance scales with size, with the largest models showing 23.37% improvement over smallest
- Even specialized industrial models like AnomalyGPT underperformed due to overfitting
- Performance degrades with too many normal samples due to information overload

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4o's superior performance stems from its ability to integrate visual and language cues through large-scale pretraining on diverse data.
- Mechanism: The model leverages cross-modal attention to align textual descriptions of defects with visual features, enabling robust classification and localization across varied industrial scenarios.
- Core assumption: Pretraining data included sufficient industrial defect examples to learn relevant visual-language associations.
- Evidence anchors:
  - [abstract]: "The commercial models performed the best, with the average accuracy of GPT-4o models reaching 74.9%."
  - [section]: "Current MLLMs are challenging to evaluate directly on IAD tasks because existing publicly available datasets only contain visual perception annotations and category labels, lacking rich semantic annotations."
  - [corpus]: Weak evidence - no direct mention of GPT-4o training data composition.
- Break condition: If pretraining data lacks sufficient industrial defect examples, performance degrades significantly on fine-grained anomaly detection.

### Mechanism 2
- Claim: Retrieval-Augmented Generation (RAG) improves performance by providing domain-specific knowledge that compensates for MLLMs' lack of industrial expertise.
- Mechanism: Domain knowledge about normal product characteristics and defect types is indexed and retrieved based on the query image, then injected into the prompt to guide reasoning.
- Core assumption: The retrieved knowledge is relevant and sufficiently comprehensive to cover the tested scenarios.
- Evidence anchors:
  - [abstract]: "We further explore two training-free performance enhancement strategies to help models improve in industrial scenarios, highlighting their promising potential for future research."
  - [section]: "Experts, with the assistance of large language models, first summarize the existing IAD datasets. For each category, they summarize the characteristics of normal samples and the features of each possible anomaly."
  - [corpus]: Weak evidence - no direct evaluation of RAG performance in related papers.
- Break condition: If domain knowledge is incomplete or retrieval fails to match relevant information, RAG provides no benefit.

### Mechanism 3
- Claim: Larger model scale enables better fine-grained understanding necessary for distinguishing subtle defects from normal variations.
- Mechanism: Increased parameter count allows more complex visual feature extraction and more nuanced language reasoning, critical for tasks like defect localization and description.
- Core assumption: Model scaling laws apply to industrial anomaly detection as they do to general vision tasks.
- Evidence anchors:
  - [section]: "As shown in the left panel of Figure 5, performance significantly improves with increasing model size, with the average accuracy difference between the largest and smallest models reaching 23.37%."
  - [section]: "Among the general open-source MLLMs, earlier models like Qwen-VL-Chat and LLaVA-1.5 underperform compared to newer models like LLaVA-OneVision and MiniCPM-V2.6, indicating that advancements in general capabilities benefit performance on IAD tasks."
  - [corpus]: No direct evidence in corpus papers about scaling effects on industrial anomaly detection.
- Break condition: If scaling reaches a point where additional parameters don't improve fine-grained feature discrimination, further increases provide diminishing returns.

## Foundational Learning

- Concept: Multimodal alignment between visual and language representations
  - Why needed here: MLLMs must map visual defect features to textual descriptions for accurate classification and localization
  - Quick check question: How does cross-modal attention enable the model to connect visual defect patterns with their textual descriptions?

- Concept: Industrial defect taxonomy and characteristics
  - Why needed here: Understanding defect types (scratches, cracks, discoloration) and their visual signatures is essential for accurate detection
  - Quick check question: What are the distinguishing visual features between a scratch and a crack in industrial products?

- Concept: Anomaly detection paradigms (supervised vs. unsupervised vs. zero-shot)
  - Why needed here: MMAD uses multiple-shot learning with template images, requiring understanding of how template-based comparison works
  - Quick check question: How does providing a normal template image help the model identify anomalies in the query image?

## Architecture Onboarding

- Component map:
  - Data generation pipeline: Visual prompts → GPT-4V caption generation → Question generation → Manual filtering
  - Benchmark evaluation: Multiple-choice questions across 7 subtasks → Accuracy calculation
  - Enhancement strategies: RAG module (retrieval + injection) and Expert Agent (model + visualization)
  - Testing framework: 1-shot, 0-shot, few-shot settings with template images

- Critical path: Data generation → Benchmark evaluation → Performance analysis → Enhancement exploration
- Design tradeoffs:
  - Multiple-choice format simplifies evaluation but may not capture full model capabilities
  - Manual filtering ensures quality but introduces scalability constraints
  - 1-shot setting balances realism with evaluation complexity
- Failure signatures:
  - Low accuracy across all subtasks suggests fundamental limitations in visual understanding
  - Poor performance on anomaly tasks specifically indicates weakness in fine-grained defect recognition
  - Performance drop with additional normal samples suggests context confusion
- First 3 experiments:
  1. Evaluate a new MLLM on the 1-shot setting across all 7 subtasks to establish baseline
  2. Test the same model with RAG enhancement to measure knowledge integration benefits
  3. Compare performance with and without template images to assess multi-image understanding capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific industrial knowledge or fine-grained understanding do MLLMs lack that prevents them from performing as effectively as human quality inspectors in anomaly detection?
- Basis in paper: Explicit - The paper states that MLLMs struggle with industrial anomaly detection, particularly in fine-grained industrial knowledge and multi-image understanding, and that they require further supplementation with IAD knowledge.
- Why unresolved: The paper identifies the problem but doesn't specify which exact types of industrial knowledge (e.g., specific defect types, product characteristics, quality standards) would most improve performance, nor does it detail what constitutes "fine-grained understanding" in this context.
- What evidence would resolve it: A detailed analysis comparing the specific types of errors made by MLLMs versus humans, identifying which industrial concepts or defect characteristics MLLMs consistently miss, and systematic testing of MLLM performance when provided with targeted industrial knowledge inputs.

### Open Question 2
- Question: How does the performance of MLLMs in industrial anomaly detection scale with model size, and what is the theoretical upper limit for their performance in this domain?
- Basis in paper: Explicit - The paper observes that performance significantly improves with increasing model size (e.g., InternVL2 series), but the trend slows and questions remain about cost-effectiveness and practical limits.
- Why unresolved: While the paper shows scaling trends, it doesn't establish whether there's a plateau point where additional parameters no longer improve IAD performance, or whether architectural changes might be more beneficial than parameter scaling.
- What evidence would resolve it: Extensive scaling experiments across multiple orders of magnitude in model size, combined with architectural ablation studies, to determine whether IAD performance has fundamental scaling limits and what architectural modifications might overcome them.

### Open Question 3
- Question: What is the optimal approach for MLLMs to utilize multiple normal samples in industrial anomaly detection, and why does performance degrade with too many samples?
- Basis in paper: Explicit - The paper finds that while 1-shot performance improves over 0-shot, adding more normal samples beyond 1-2 doesn't help and can even hurt performance due to information overload.
- Why unresolved: The paper observes the phenomenon but doesn't explain the underlying mechanism—whether it's due to attention mechanisms being overwhelmed, context length limitations, or MLLMs lacking the ability to effectively summarize and compare multiple normal samples.
- What evidence would resolve it: Controlled experiments varying the number and selection method of normal samples, analysis of attention patterns and internal representations when processing multiple samples, and testing whether specialized prompting strategies or architectural modifications could enable effective multi-sample comparison.

## Limitations
- Commercial model evaluation through APIs may introduce variability and access limitations
- Manual filtering process introduces potential bias and limits scalability of the benchmark
- 1-shot setting with template images may not fully represent industrial inspection diversity

## Confidence
- High confidence: Benchmark construction methodology and dataset statistics are well-documented and reproducible
- Medium confidence: Performance comparisons between models are valid, but API-based evaluation of commercial models may introduce variability
- Medium confidence: The identified performance gap between anomaly and object-related tasks is supported by data, though the underlying causes require further investigation

## Next Checks
1. Replicate performance evaluation using identical prompts and settings across all MLLMs to verify the reported accuracy differences, particularly focusing on the open-source model results
2. Analyze error patterns by manually reviewing a sample of questions where all models performed poorly to identify common failure modes and potential dataset biases
3. Test enhancement strategies (RAG and Expert Agent) with controlled experiments to measure their actual impact on performance across different task types and model sizes