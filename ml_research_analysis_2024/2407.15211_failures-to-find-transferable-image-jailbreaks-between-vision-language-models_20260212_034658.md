---
ver: rpa2
title: Failures to Find Transferable Image Jailbreaks Between Vision-Language Models
arxiv_id: '2407.15211'
source_url: https://arxiv.org/abs/2407.15211
tags:
- vlms
- image
- transfer
- siglip
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work systematically evaluated the transferability of image-based
  jailbreaks across 40+ vision-language models (VLMs). Despite extensive efforts,
  including attacking individual models, model ensembles, and closely related model
  variants, gradient-based image jailbreaks showed minimal transfer between different
  VLMs.
---

# Failures to Find Transferable Image Jailbreaks Between Vision-Language Models

## Quick Facts
- arXiv ID: 2407.15211
- Source URL: https://arxiv.org/abs/2407.15211
- Reference count: 40
- 40+ vision-language models tested; gradient-based image jailbreaks showed minimal transfer between different VLMs

## Executive Summary
This work systematically evaluated the transferability of image-based jailbreaks across 40+ vision-language models (VLMs). Despite extensive efforts, including attacking individual models, model ensembles, and closely related model variants, gradient-based image jailbreaks showed minimal transfer between different VLMs. Transfer was only partially observed between models with identical initializations and overlapping training data or between different checkpoints of the same model. These findings indicate that VLMs exhibit greater robustness to transferable adversarial attacks compared to unimodal models, suggesting fundamental differences in how multimodal systems process visual and textual inputs.

## Method Summary
The researchers optimized image jailbreaks using gradient descent to maximize the probability of harmful-yet-helpful responses across various VLMs. They tested transferability by attacking individual VLMs, ensembles of 8 VLMs, and highly similar VLMs (identical initializations, checkpoints). The optimization targeted cross-entropy loss between generated and target responses, evaluated using Claude 3 Opus Harmful-Yet-Helpful Score. Experiments spanned 40+ open-parameter VLMs with diverse vision backbones (CLIP, SigLIP, DINOv2+SigLIP) and language models (Vicuna, Llama 2/3, Mistral, Gemma, Phi-3).

## Key Results
- Gradient-based image jailbreaks showed minimal transfer between diverse VLMs
- Transfer only occurred between identically-initialized VLMs with different training data or between checkpoints of the same model
- Successful transfer required attacking larger ensembles of highly similar VLMs
- VLMs demonstrated greater robustness to transferable adversarial attacks than unimodal models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLMs resist transferable adversarial attacks due to fundamentally different multimodal integration compared to unimodal models.
- Mechanism: Visual and textual inputs are integrated in a manner that creates non-transferable representations; when attacks optimized on one VLM are transferred to another, the multimodal integration circuits differ sufficiently to break the attack.
- Core assumption: The integration of vision backbones and language models in VLMs creates unique adversarial surfaces that are not shared across different architectures.
- Evidence anchors:
  - [abstract] "suggesting that VLMs may be more robust to gradient-based transfer attacks"
  - [section] "our experimental results are suggestive... the mechanism by which outputs of the vision backbone are injected into the language model play a critical role in successful transfer"
  - [corpus] Weak - no direct mention of multimodal integration as a defense mechanism.

### Mechanism 2
- Claim: The vast degrees of freedom in image optimization (256^512x512x3 ≈ 1e2000000) prevent effective transfer.
- Mechanism: The enormous optimization space means that even when attacking ensembles, the attack space is too unconstrained to find transferable perturbations; attacks find local minima specific to each model.
- Core assumption: Reducing degrees of freedom through constraints, regularization, or more sophisticated optimization will enable transfer.
- Evidence anchors:
  - [section] "This would explain why each individual VLM was jailbroken rapidly and why jailbreaking 8 VLMs simultaneously took no longer than jailbreaking 1 VLM"
  - [section] "This conjecture suggests that improvements on constraints, regularization or optimization may be necessary to obtain reliable transfer"
  - [corpus] Weak - no direct discussion of degrees of freedom as a defense mechanism.

### Mechanism 3
- Claim: Transfer succeeds only between "highly similar" VLMs with identical initializations and overlapping training data or checkpoints.
- Mechanism: When VLMs share identical vision backbones, language models, and MLP connectors, and differ only in training data or optimizer steps, their representations remain sufficiently aligned for transfer.
- Core assumption: "Highly similar" means identical architecture, initialization, and minimal differences in training procedure.
- Evidence anchors:
  - [abstract] "Only two settings display partially successful transfer: between identically-pretrained and identically-initialized VLMs with slightly different VLM training data, and between different training checkpoints of a single VLM"
  - [section] "This strongly suggests that the mechanism by which outputs of the vision backbone are injected into the language model play a critical role in successful transfer"
  - [corpus] Weak - no direct mention of identical initialization requirements.

## Foundational Learning

- Concept: Adversarial transferability in unimodal models
  - Why needed here: Understanding why text jailbreaks and image classifier attacks transfer helps identify why VLM attacks don't
  - Quick check question: Why do adversarial examples typically transfer between image classifiers?

- Concept: Multimodal representation learning
  - Why needed here: VLMs combine visual and textual features; understanding this integration is key to understanding attack surfaces
  - Quick check question: How do VLMs typically fuse visual and textual representations?

- Concept: Ensemble attack methods
  - Why needed here: The paper tests ensemble attacks; understanding how they work in unimodal settings provides context
  - Quick check question: What is the theoretical basis for why ensemble attacks might improve transferability?

## Architecture Onboarding

- Component map:
  - Vision backbone (CLIP, SigLIP, DINOv2+SigLIP)
  - Language backbone (Vicuna, Llama 2/3, Mistral, Gemma, Phi-3)
  - MLP connector between vision and language
  - Training stages (1-stage vs 2-stage)
  - Safety alignment modules

- Critical path:
  1. Initialize image (random noise or natural image)
  2. Compute cross-entropy loss across ensemble
  3. Backpropagate gradients to image
  4. Update image with Adam
  5. Evaluate transfer to target models

- Design tradeoffs:
  - More similar models in ensemble → better transfer but less generalization
  - More diverse models in ensemble → worse transfer but more robust attack
  - 1-stage training → potentially more transferrable attacks
  - 2-stage training → potentially more robust against attacks

- Failure signatures:
  - Cross-entropy drops on attacked models but not on target models
  - Claude 3 Opus scores remain low on target models
  - No improvement with longer optimization or larger ensembles

- First 3 experiments:
  1. Optimize image against single VLM, test transfer to all others
  2. Optimize image against ensemble of 8 VLMs, test transfer
  3. Optimize image against "highly similar" VLMs, test transfer to target

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural differences between VLMs and unimodal models account for the reduced transferability of adversarial attacks?
- Basis in paper: [explicit] The authors state that VLMs exhibit greater robustness to transferable adversarial attacks compared to unimodal models and suggest this indicates "fundamental differences in how multimodal systems process visual and textual inputs."
- Why unresolved: The paper identifies the phenomenon but does not investigate the mechanistic differences between VLMs and unimodal models that cause this difference in transferability.
- What evidence would resolve it: Comparative analysis of activation patterns, attention mechanisms, or feature representations between VLMs and unimodal models during adversarial attacks could reveal architectural differences responsible for the robustness.

### Open Question 2
- Question: How do specific choices in image optimization constraints affect the transferability of jailbreaks across VLMs?
- Basis in paper: [inferred] The authors note that images have "too many degrees of freedom" (256^512×512×3 ≈ 1e2000000) compared to text-based attacks, suggesting that optimization constraints might be crucial for transferability.
- Why unresolved: The paper used standard optimization approaches but did not systematically explore how different constraints, regularization techniques, or optimization methods might affect transferability.
- What evidence would resolve it: Systematic comparison of different optimization approaches (e.g., varying constraints, regularization, initialization methods) and their effects on transferability across VLMs would identify optimal strategies.

### Open Question 3
- Question: What defines "highly similar" VLMs mathematically, and can this be used to predict transferability?
- Basis in paper: [explicit] The authors demonstrate that attacking larger ensembles of "highly similar" VLMs produces transferable jailbreaks, but acknowledge they "lack a mathematical definition of 'highly similar'."
- Why unresolved: While the paper shows that similarity matters for transferability, it does not establish quantitative criteria for what makes VLMs "highly similar" or how to measure this similarity.
- What evidence would resolve it: Development of quantitative similarity metrics based on architectural features, training data overlap, parameter distributions, or other measurable characteristics that correlate with transferability success.

## Limitations
- Focus on gradient-based adversarial attacks may not represent the full space of potential jailbreak strategies
- Reliance on Claude 3 Opus Harmful-Yet-Helpful Score as an oracle introduces potential bias
- 40+ VLMs tested may not capture all architectural variations or future developments

## Confidence
- High Confidence: Empirical finding that gradient-based image jailbreaks show minimal transfer between diverse VLMs
- Medium Confidence: Proposed mechanisms (multimodal integration differences, degrees of freedom) explaining why transfer fails
- Low Confidence: Broader claim that VLMs are inherently more robust to transferable adversarial attacks than unimodal models

## Next Checks
1. Test alternative attack methods (human-engineered or non-gradient-based) to determine if lack of transfer is fundamental to VLM architecture or specific to gradient-based approaches.

2. Conduct ablation studies systematically varying vision backbone, language model, and connector components to test whether multimodal integration is the key factor preventing transfer.

3. Compute and compare internal representations (using canonical correlation analysis or centered kernel alignment) between models that do and don't show transfer to identify representation similarity patterns.