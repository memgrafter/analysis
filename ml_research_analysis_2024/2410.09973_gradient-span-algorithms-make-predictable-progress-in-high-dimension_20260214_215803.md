---
ver: rpa2
title: Gradient Span Algorithms Make Predictable Progress in High Dimension
arxiv_id: '2410.09973'
source_url: https://arxiv.org/abs/2410.09973
tags:
- random
- have
- covariance
- function
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proves that gradient span algorithms (GSAs) applied to
  scaled Gaussian random functions (GRFs) in high dimensions exhibit asymptotically
  deterministic behavior, regardless of initialization or realization. This explains
  the observed phenomenon in machine learning where different training runs of large
  models result in approximately equal cost curves despite random initialization.
---

# Gradient Span Algorithms Make Predictable Progress in High Dimension

## Quick Facts
- arXiv ID: 2410.09973
- Source URL: https://arxiv.org/abs/2410.09973
- Authors: Felix Benning; Leif Döring
- Reference count: 40
- Primary result: Gradient span algorithms exhibit asymptotically deterministic behavior on scaled Gaussian random functions in high dimensions

## Executive Summary
This paper proves that gradient span algorithms applied to scaled Gaussian random functions in high dimensions exhibit asymptotically deterministic behavior, regardless of initialization or realization. This explains the observed phenomenon in machine learning where different training runs of large models result in approximately equal cost curves despite random initialization. The authors define GSAs as a general class of first-order optimization algorithms that select the next point from the span of previous gradients, and prove convergence to deterministic limits for function values and gradient norms.

## Method Summary
The paper analyzes gradient span algorithms (GSAs) - a general class of first-order optimization algorithms that select the next point from the span of previous gradients. The method involves proving convergence of the optimization path on scaled Gaussian random functions with isotropic covariance. The proof uses an inductive approach with previsible orthonormal coordinate systems to analyze conditional distributions, leveraging isotropy to show that directional derivatives become independent in high dimensions, enabling the law of large numbers to apply.

## Key Results
- Gradient span algorithms applied to scaled Gaussian random functions converge to asymptotically deterministic behavior in high dimensions
- Function values and gradient norms along the optimization path converge in probability to deterministic limits as dimension increases
- Halting times become asymptotically deterministic
- Results hold for a wide range of algorithms including gradient descent, momentum methods, and conjugate gradient

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In high dimensions, the optimization path along Gaussian random functions becomes asymptotically deterministic regardless of initialization.
- Mechanism: Directional derivatives in orthogonal directions become independent Gaussian variables due to isotropy. The law of large numbers causes the sum of squared directional derivatives to converge to its expected value, making the gradient effectively deterministic in direction and magnitude.
- Core assumption: The random function must be (non-stationary) isotropic Gaussian random function with smooth covariance and strictly positive definite derivatives.
- Evidence anchors:
  - [abstract] "we prove that all 'gradient span algorithms' have asymptotically deterministic behavior on scaled Gaussian random functions as the dimension tends to infinity"
  - [section 2.2] Definition of (non-stationary) isotropic Gaussian random functions with specific covariance structure
  - [section 3] Discussion showing gradient norms don't collapse while function values do
- Break condition: If the isotropy assumption is violated, or if the covariance function is not smooth enough, or if the derivatives are not strictly positive definite.

### Mechanism 2
- Claim: The covariance matrices of directional derivatives develop a sparse block structure that enables dimension-free analysis.
- Mechanism: The proof constructs a previsible orthonormal coordinate system that captures the span of previous gradients. In this coordinate system, the covariance matrix develops a block structure where blocks corresponding to orthogonal directions have a special form. The scaling by 1/N keeps the "chaotic" parts bounded while the law of large numbers applies to the structured parts.
- Core assumption: The existence of a coordinate system that can be constructed previsibly and that exploits the isotropy structure.
- Evidence anchors:
  - [section 5.1] "Idea 2: Finding sparsity in the covariance matrices with a custom coordinate system"
  - [section 5.3] Detailed construction of previsible orthonormal coordinate systems
  - [section 5.4.3] Analysis showing block matrix structure emerges
- Break condition: If the algorithm doesn't use the most recent gradient asymptotically, or if the coordinate system cannot be constructed previsibly.

### Mechanism 3
- Claim: Previsible sampling allows treating random inputs as deterministic when computing conditional distributions.
- Mechanism: Since the evaluation points are selected based on previous gradient information, they are previsible with respect to the filtration generated by the function values and gradients. This allows applying conditional sampling theory to treat the random inputs as deterministic when computing conditional expectations and covariances.
- Core assumption: The evaluation points must be measurable with respect to the filtration generated by previous observations.
- Evidence anchors:
  - [section 2.4.1] "Previsible Sampling" lemma showing how to handle random inputs
  - [section 5.1] "Idea 3: Treating the random input with care"
  - [section 5.4.3] Application of previsible sampling in the induction proof
- Break condition: If the algorithm uses information not contained in the previsible filtration, or if the function is not sufficiently smooth.

## Foundational Learning

- Concept: Gaussian random fields and their covariance structure
  - Why needed here: The entire analysis relies on the Gaussian assumption to compute conditional distributions and covariances of derivatives
  - Quick check question: What is the relationship between the covariance of directional derivatives and the derivatives of the covariance function?

- Concept: Isotropy and its implications for directional derivatives
  - Why needed here: Isotropy ensures that directional derivatives in orthogonal directions are uncorrelated, which is crucial for the independence that enables the law of large numbers argument
  - Quick check question: How does isotropy affect the covariance structure of directional derivatives?

- Concept: Previsible sampling and conditional distributions
  - Why needed here: Since the optimization path depends on previous evaluations, we need tools to handle conditional distributions of random functions evaluated at random points
  - Quick check question: Why can we treat previsible random inputs as deterministic when computing conditional expectations?

## Architecture Onboarding

- Component map: Gradient Span Algorithm (GSA) -> Gaussian Random Function -> Previsible Coordinate System -> Conditional Distribution Analysis

- Critical path: 1) Define the gradient span algorithm and verify it's previsible, 2) Construct the previsible orthonormal coordinate system, 3) Analyze the conditional distribution of the next function value and gradient given previous information, 4) Prove convergence of the conditional mean and covariance, 5) Apply continuous mapping to get convergence of the optimization path.

- Design tradeoffs: The choice between stationary and non-stationary isotropy affects the generality of the result. The requirement for strictly positive definite derivatives is strong but can potentially be relaxed. The 1/N scaling is crucial for keeping the "chaos" bounded while allowing the structured parts to converge.

- Failure signatures: If the algorithm doesn't use the most recent gradient asymptotically, the coordinate system won't capture the optimization dynamics properly. If the covariance function isn't smooth enough, the conditional distributions won't be well-defined. If the function isn't isotropic, the directional derivatives won't be independent.

- First 3 experiments:
  1. Implement a simple gradient descent on a high-dimensional Gaussian random function and verify the convergence of the optimization path across different initializations
  2. Test the previsible coordinate system construction on a known Gaussian random function and verify the block structure of the covariance matrix
  3. Implement the conditional distribution calculation and verify the convergence of the conditional mean and covariance as dimension increases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the strict positive definiteness assumption be removed from the main theorem?
- Basis in paper: [explicit] The paper discusses this in the outlook section, mentioning two potential approaches: using generalized matrix inverses or a perturbation argument
- Why unresolved: The paper states this as a possible generalization but doesn't provide a proof or complete methodology for either approach
- What evidence would resolve it: A rigorous proof showing that either generalized inverses converge appropriately or that the perturbation approach yields the same limiting behavior as N approaches infinity

### Open Question 2
- Question: Can the Gaussian assumption be relaxed to non-Gaussian random functions while maintaining predictable progress?
- Basis in paper: [explicit] The paper mentions this as "G2" in the outlook section, noting that uncorrelated directional derivatives are sufficient for independence in the Gaussian case but not in non-Gaussian cases
- Why unresolved: The paper acknowledges this as a potential generalization but states that uncorrelated squares no longer guarantee the law of large numbers applies in the non-Gaussian case
- What evidence would resolve it: A proof showing that predictable progress holds for a specific class of non-Gaussian random functions (e.g., sub-Gaussian or with specific moment conditions)

### Open Question 3
- Question: What is the optimal algorithm for high-dimensional problems, and how can we find it?
- Basis in paper: [explicit] The paper mentions this as "E2" in the outlook section, suggesting that an explicit representation of limiting information could allow for meta-optimization over the optimizer
- Why unresolved: The paper provides a theoretical framework but doesn't derive explicit representations of limiting function values or provide methods for algorithm optimization
- What evidence would resolve it: A methodology for deriving explicit representations of limiting values and a framework for comparing different gradient span algorithms to identify optimal ones for specific problem classes

### Open Question 4
- Question: How can component-wise learning rates (like Adam) be analyzed within this framework?
- Basis in paper: [explicit] The paper mentions this as "E4" in the outlook section, noting that component-wise learning rates cannot easily be expressed in a dimension-free manner
- Why unresolved: The paper's proof relies on continuity assumptions that component-wise learning rates violate, and it's unclear how to extend the analysis to such algorithms
- What evidence would resolve it: A theoretical framework or proof showing that algorithms with component-wise learning rates exhibit predictable progress, or empirical evidence with theoretical justification explaining why they behave similarly to algorithms covered by the current framework

## Limitations

- The main result requires the covariance of (fN, ∇fN) to be strictly positive definite, which may not hold for all Gaussian random functions
- The proof techniques rely heavily on isotropy and smoothness assumptions that may not generalize to non-isotropic or non-smooth functions common in practice
- The 1/N scaling is crucial for the proof but may not be optimal for practical applications

## Confidence

- High confidence in the asymptotic determinism result for isotropic Gaussian random functions in high dimensions
- Medium confidence in the applicability to practical deep learning scenarios due to the idealized assumptions
- Medium confidence in the dimensional scaling requirements, as the proof requires careful balancing between the 1/N scaling and the dimensionality

## Next Checks

1. Test the convergence rate empirically by running gradient span algorithms on high-dimensional Gaussian random functions with varying smoothness parameters and verify the asymptotic behavior predicted by the theory.

2. Investigate the impact of violating the isotropy assumption by constructing anisotropic Gaussian random functions and analyzing how the optimization path behavior differs from the isotropic case.

3. Examine the dimensional scaling more carefully by computing the critical dimension threshold where the asymptotic behavior becomes apparent, and validate this against the theoretical bounds derived in the paper.