---
ver: rpa2
title: Contamination Report for Multilingual Benchmarks
arxiv_id: '2410.16186'
source_url: https://arxiv.org/abs/2410.16186
tags:
- should
- answer
- data
- contamination
- authors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates multilingual benchmark contamination in seven
  popular large language models (LLMs) using the Black Box test methodology. The authors
  assess whether commonly used multilingual datasets (XNLI, XQuAD, XStory Cloze, XCOPA,
  XLSum, FLORES, and PAWS-X) are contaminated in both base and instruction-tuned versions
  of Mistral-7B, Llama-3.1-8B, Gemma-2-9B, and Aya-23-8B.
---

# Contamination Report for Multilingual Benchmarks

## Quick Facts
- arXiv ID: 2410.16186
- Source URL: https://arxiv.org/abs/2410.16186
- Authors: Sanchit Ahuja; Varun Gumma; Sunayana Sitaram
- Reference count: 40
- Key outcome: Nearly all evaluated multilingual models show contamination across almost all tested benchmarks, with only four instances showing no contamination

## Executive Summary
This paper evaluates multilingual benchmark contamination in seven popular large language models using the Black Box test methodology. The authors assess whether commonly used multilingual datasets (XNLI, XQuAD, XStory Cloze, XCOPA, XLSum, FLORES, and PAWS-X) are contaminated in both base and instruction-tuned versions of Mistral-7B, Llama-3.1-8B, Gemma-2-9B, and Aya-23-8B. The study finds that newer LLM versions, despite being larger and trained on more data, are more likely to include benchmark datasets in their training data, with contamination occurring primarily during the pre-training phase and persisting after instruction tuning.

## Method Summary
The study uses Oren et al.'s Black Box test methodology to detect contamination by comparing model performance on canonical versus shuffled dataset orderings. The evaluation uses 5000 data points across all languages, split into 48 shards with 768 permutations per shard. The test determines if models exhibit a statistically significant preference for canonical ordering through log probability scoring and p-value analysis. The study runs on 8× H100 GPUs for 362 hours, evaluating base and instruction-tuned versions of four models across seven multilingual benchmarks.

## Key Results
- Nearly all evaluated models show contamination across almost all tested benchmarks
- Only four instances showed no contamination across all model-dataset combinations
- Contamination primarily occurs during pre-training and persists after instruction tuning
- Newer LLM versions are more likely to be contaminated due to larger pre-training corpora

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Black Box test reliably detects multilingual benchmark contamination by comparing canonical versus shuffled dataset orderings
- Mechanism: If a model has memorized benchmark data, its log probability scores will be significantly higher for the canonical order than for randomly shuffled orders. The statistical difference (p-value) below the threshold indicates contamination
- Core assumption: The test set exhibits exchangeability, meaning the joint distribution remains unchanged under reordering
- Evidence anchors:
  - [abstract]: "By comparing a model's performance on the canonical order versus shuffled orders, this method determines if the model exhibits a statistically significant preference for the original order."
  - [section]: "If a model has been exposed to a benchmark dataset, it will likely develop a bias toward the canonical ordering of examples... over randomly shuffled versions of the same dataset."
- Break condition: If the dataset lacks exchangeability (e.g., highly structured or ordered data), the test will fail to provide valid contamination signals

### Mechanism 2
- Claim: Contamination primarily occurs during pre-training and persists after instruction tuning
- Mechanism: During pre-training, the model ingests vast web data that often includes benchmark datasets. Even after instruction tuning, the memorized content remains, leading to inflated evaluation scores
- Core assumption: Pre-training data sources are reused and expanded across model versions, so contamination accumulates over time
- Evidence anchors:
  - [abstract]: "Our findings suggest that contamination occurs during the pre-training phase and persists after post-training."
  - [section]: "Given that the pre-training corpus for these models is typically expanded and reused, it is likely that future versions will also ingest these datasets."
- Break condition: If future model versions implement strict data filtering or deduplication, contamination may not persist post-instruction tuning

### Mechanism 3
- Claim: Larger and newer LLM versions are more likely to be contaminated because their pre-training corpora are larger and more diverse
- Mechanism: As models scale up, they ingest more web data, increasing the probability that benchmark datasets are included. This trend is evident across evaluated models
- Core assumption: The relationship between model size and contamination likelihood is monotonic
- Evidence anchors:
  - [abstract]: "This indicates that newer LLM versions, despite being larger and trained on more data, are more likely to include benchmark datasets in their training data."
  - [section]: "Our findings suggest that newer versions LLMs, despite being larger and trained on more data, are more likely to include benchmark datasets in their training data."
- Break condition: If model creators implement robust data deduplication or contamination detection pipelines, the trend may reverse

## Foundational Learning

- Concept: Exchangeability in probability theory
  - Why needed here: The Black Box test relies on exchangeability to detect contamination by comparing canonical vs. shuffled orderings
  - Quick check question: What does it mean for a dataset to be exchangeable, and why is this property critical for contamination detection?

- Concept: Statistical significance testing
  - Why needed here: The method uses p-values to determine whether performance differences between orderings are statistically significant
  - Quick check question: How is the significance threshold (1/(1 + r)) derived, and what does a p-value below this threshold imply?

- Concept: Pre-training vs. post-training phases
  - Why needed here: Understanding when contamination occurs (pre-training vs. instruction tuning) informs mitigation strategies
  - Quick check question: Why does contamination detected in base models often persist after instruction tuning?

## Architecture Onboarding

- Component map: Multilingual benchmarks -> Black Box test -> Contamination detection results
- Critical path: 
  1. Load benchmark datasets and split into shards
  2. Generate r = 768 permutations per shard
  3. Evaluate model on both canonical and shuffled orders
  4. Compute p-values and compare to significance threshold
  5. Report contamination status
- Design tradeoffs:
  - Shard size vs. statistical power: Smaller shards reduce computation but may weaken detection sensitivity
  - Number of permutations (r) vs. runtime: Higher r increases accuracy but is computationally expensive
  - Choice of benchmarks vs. coverage: Evaluating more datasets increases detection breadth but also runtime
- Failure signatures:
  - False positives: Datasets lacking exchangeability may be incorrectly flagged as contaminated
  - False negatives: Insufficient permutations or shards may miss subtle contamination
  - Performance variance: Inconsistent log probabilities across shards may indicate instability
- First 3 experiments:
  1. Run Black Box test on a small synthetic dataset with known contamination to validate detection logic
  2. Evaluate a base model on one benchmark to confirm contamination status before instruction tuning
  3. Compare contamination results between base and instruction-tuned versions of the same model to confirm persistence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does contamination vary across different multilingual benchmark types (e.g., text classification vs. generation tasks)?
- Basis in paper: [inferred] The paper tests seven different multilingual benchmarks but doesn't analyze contamination patterns across task types
- Why unresolved: The paper provides aggregate contamination results but doesn't analyze whether certain task types are more susceptible to contamination than others
- What evidence would resolve it: Comparative analysis of contamination rates across different benchmark categories (classification, generation, reasoning, etc.)

### Open Question 2
- Question: What is the relationship between model size and contamination susceptibility?
- Basis in paper: [inferred] The paper tests models of different sizes (7B, 8B, 9B parameters) but doesn't analyze contamination patterns relative to model scale
- Why unresolved: While the paper tests various model sizes, it doesn't investigate whether larger models show different contamination patterns than smaller ones
- What evidence would resolve it: Correlation analysis between model parameter count and contamination rates across benchmarks

### Open Question 3
- Question: How does contamination manifest differently across languages within the same benchmark?
- Basis in paper: [inferred] The paper tests multilingual benchmarks but doesn't analyze contamination variation across individual languages
- Why unresolved: The paper treats each benchmark as a whole rather than examining whether contamination affects certain languages more than others
- What evidence would resolve it: Language-specific contamination analysis showing which languages show higher contamination rates within each benchmark

## Limitations
- The Black Box test assumes exchangeability of test datasets, which may not hold for all evaluated benchmarks
- Computational requirements (8× H100 GPUs for 362 hours) create significant barriers to independent verification
- The study doesn't explore potential mitigation strategies or alternative contamination detection methods

## Confidence
- Contamination detection methodology: Medium - The Black Box test is well-established, but its assumptions about dataset exchangeability aren't explicitly validated for all evaluated benchmarks
- Persistence of contamination post-instruction tuning: High - Direct comparisons between base and instruction-tuned models show consistent contamination patterns
- Trend of increasing contamination with model size: Medium - While the data supports this trend, the sample size of models is limited, and alternative explanations (e.g., evolving data collection practices) aren't ruled out

## Next Checks
1. Validate the exchangeability assumption for each evaluated benchmark by testing whether random shuffling affects model performance distributions
2. Replicate contamination detection results using a subset of models and datasets on more accessible computational resources (e.g., fewer GPUs or smaller shard sizes)
3. Test whether implementing data deduplication or filtering during pre-training would reduce contamination rates in newer model versions