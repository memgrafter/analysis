---
ver: rpa2
title: 'Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving'
arxiv_id: '2410.22313'
source_url: https://arxiv.org/abs/2410.22313
tags:
- planning
- driving
- senna
- image
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Senna, an autonomous driving system that integrates
  a large vision-language model (Senna-VLM) with an end-to-end model (Senna-E2E) to
  achieve structured planning from high-level decisions to low-level trajectory prediction.
  Senna decouples high-level planning decisions from low-level trajectory prediction,
  with Senna-VLM generating planning decisions in natural language and Senna-E2E predicting
  precise trajectories.
---

# Senna: Bridging Large Vision-Language Models and End-to-End Autonomous Driving

## Quick Facts
- **arXiv ID**: 2410.22313
- **Source URL**: https://arxiv.org/abs/2410.22313
- **Reference count**: 40
- **Primary result**: Senna achieves 27.12% reduction in average planning error and 33.33% reduction in collision rate over models without pre-training

## Executive Summary
Senna is an autonomous driving system that combines a large vision-language model (Senna-VLM) for high-level planning decisions with an end-to-end model (Senna-E2E) for precise trajectory prediction. The system decouples high-level planning from low-level execution, with Senna-VLM generating planning decisions in natural language while Senna-E2E predicts trajectories. A three-stage training strategy with planning-oriented Q&As enhances the LVLM's driving performance while preserving commonsense reasoning. Extensive experiments on nuScenes and DriveX datasets demonstrate state-of-the-art planning accuracy with significant improvements in safety metrics.

## Method Summary
Senna integrates a large vision-language model with an end-to-end trajectory predictor through a structured planning approach. The system takes multi-view camera images, user instructions, and navigation commands as input. Senna-VLM processes the visual input through a driving vision adapter that compresses image tokens while maintaining spatial understanding, then generates high-level planning decisions in natural language. These decisions are formatted as meta-actions and passed to Senna-E2E, which predicts precise trajectories. The system uses a three-stage training strategy (mix pre-training, driving fine-tuning, planning fine-tuning) with automatically-generated planning-oriented Q&As covering scene description, traffic signal detection, VRU identification, motion prediction, and meta-action planning.

## Key Results
- Achieves 27.12% reduction in average planning error compared to models without pre-training
- Reduces collision rate by 33.33% over baseline models
- Planning performance improves significantly with dataset size without showing diminishing returns
- Maintains commonsense reasoning while achieving superior driving-specific planning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Senna's structured planning approach improves safety by separating high-level reasoning from low-level trajectory prediction
- Mechanism: Large Vision-Language Models (Senna-VLM) handle high-level planning decisions in natural language, leveraging commonsense reasoning and scene understanding, while an end-to-end model (Senna-E2E) focuses on precise trajectory prediction. This separation reduces the learning complexity for the end-to-end model and prevents LVLM limitations in numerical predictions from affecting safety-critical decisions.
- Core assumption: Natural language planning decisions are sufficient for guiding precise trajectory prediction
- Evidence anchors:
  - [abstract] "Senna decouples high-level planning decisions from low-level trajectory prediction"
  - [section] "Predicting language-based planning decisions allows LVLMs to fully utilize their knowledge and common sense from pre-trained language tasks, so as to generate reasonable decisions and avoid issues with precise numerical predictions"
  - [corpus] Weak evidence - no direct citations, but conceptually supported by related work on LVLM-AD and HERMES
- Break condition: If the natural language decisions become too vague or fail to capture necessary numerical constraints for safe driving

### Mechanism 2
- Claim: Multi-view image encoding with a driving vision adapter enables efficient spatial understanding
- Mechanism: Senna-VLM uses surround-view images encoded through a driving vision adapter that compresses image tokens while maintaining spatial understanding. The adapter maps image features to the LLM feature space and reduces token count to prevent model collapse, while multi-view prompts help the model differentiate perspectives.
- Core assumption: Compressed image tokens preserve sufficient spatial information for driving scene understanding
- Evidence anchors:
  - [section] "Senna-VLM employs an efficient multi-image encoding strategy along with a carefully designed surround-view prompt, enabling accurate perception and spatial understanding of driving scenarios"
  - [section] "we introduce a Driving Vision Adapter module. this module not only functions similarly to previous studies which maps image features to the LLM feature space but also performs additional encoding and compression of the image features"
  - [corpus] Weak evidence - no direct citations, but supported by VLM-AD's multi-modal approach
- Break condition: If compression reduces image tokens below the threshold where spatial relationships become ambiguous

### Mechanism 3
- Claim: Planning-oriented Q&A training improves LVLM's driving-specific reasoning while preserving commonsense
- Mechanism: Senna-VLM is trained with automatically-generated planning-oriented Q&As covering scene description, traffic signal detection, VRU identification, motion prediction, and meta-action planning. A three-stage training strategy (mix pre-training, driving fine-tuning, planning fine-tuning) optimizes performance for driving tasks without losing general commonsense knowledge.
- Core assumption: Automatically-generated Q&As capture sufficient driving complexity for effective training
- Evidence anchors:
  - [abstract] "we introduce planning-oriented QAs alongside a three-stage training strategy, which enhances Senna-VLM's planning performance while preserving commonsense"
  - [section] "We design a series of planning-oriented Q&As for training Senna-VLM, which do not require human annotation and can be produced at scale entirely through an auto-labeling pipeline"
  - [corpus] Weak evidence - no direct citations, but conceptually supported by VLM-AD's supervision approach
- Break condition: If auto-generated Q&As miss critical edge cases or introduce biases that compromise safety

## Foundational Learning

- Concept: Multi-modal representation learning
  - Why needed here: Senna needs to integrate visual information (surround-view images) with textual information (natural language planning decisions) to make driving decisions
  - Quick check question: How does Senna handle the different modalities (vision and language) in its architecture?

- Concept: Structured planning decomposition
  - Why needed here: The system needs to understand why breaking down driving decisions into high-level planning and low-level execution improves performance and safety
  - Quick check question: What are the specific advantages of separating high-level planning decisions from low-level trajectory prediction?

- Concept: Vision-language model fine-tuning strategies
  - Why needed here: Understanding how the three-stage training strategy (mix pre-training, driving fine-tuning, planning fine-tuning) optimizes the LVLM for driving tasks
  - Quick check question: Why does Senna use a three-stage training strategy instead of standard pre-training and fine-tuning?

## Architecture Onboarding

- Component map: Multi-view images → Driving Vision Adapter → LLM → Meta-action Encoder → Senna-E2E → Trajectory output
- Critical path: Multi-view images → Driving Vision Adapter → LLM → Meta-action Encoder → Senna-E2E → Trajectory output
- Design tradeoffs: Using LVLMs for high-level decisions trades off some precision for better commonsense reasoning; compressing image tokens improves efficiency but risks losing spatial detail; auto-generated Q&As enable scalability but may miss edge cases
- Failure signatures: Model collapse during training (too many image tokens), poor spatial understanding (insufficient image compression), unsafe planning decisions (inadequate Q&A coverage), slow inference (inefficient multi-image encoding)
- First 3 experiments:
  1. Test different numbers of image tokens (32, 64, 128, 256) to find optimal balance between efficiency and spatial understanding
  2. Evaluate ablation of different planning-oriented Q&A types to identify which contribute most to planning accuracy
  3. Compare Senna's performance with and without pre-training on DriveX dataset to measure transfer learning benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Senna scale with increasing dataset sizes beyond the evaluated 800k training clips in DriveX?
- Basis in paper: [inferred] The paper states "Senna's planning performance improves significantly as the dataset size increases, without any signs of diminishing returns" and acknowledges "Given the limited data, we believe that Senna has not yet reached its performance ceiling, and there is still room for improvement as the training data continues to grow."
- Why unresolved: The current evaluation only covers up to 800k clips, leaving the scaling behavior at larger dataset sizes unexplored.
- What evidence would resolve it: Training and evaluating Senna on progressively larger subsets of DriveX or other datasets (e.g., 1M, 2M, 5M clips) and measuring the corresponding changes in planning accuracy, collision rates, and other metrics to determine if the improvement curve plateaus or continues to grow.

### Open Question 2
- Question: Can smaller vision-language models (e.g., 2B parameters) maintain Senna's performance while significantly improving inference speed for real-time autonomous driving applications?
- Basis in paper: [inferred] The paper acknowledges "during deployment, the inference speed of large models may hinder the system from meeting the real-time requirements of autonomous driving" and suggests this "could potentially be mitigated in the future by utilizing smaller vision-language models, such as a 2B model, to reduce inference latency."
- Why unresolved: The current implementation uses Vicuna-v1.5-7b, and the paper only provides latency comparisons with other models but doesn't explore the performance trade-offs of using smaller LVLMs.
- What evidence would resolve it: Replacing the 7B Vicuna model with progressively smaller LVLMs (2B, 1B, 500M parameters) while keeping the same architecture and training strategy, then measuring the impact on planning accuracy, collision rates, and inference latency to find the optimal balance between performance and speed.

### Open Question 3
- Question: How does Senna perform in long-tail, rare scenarios that were not well-represented in the training data?
- Basis in paper: [explicit] The paper mentions "end-to-end autonomous driving demonstrates strong planning capabilities with large-scale data but still struggles in complex, rare scenarios due to limited commonsense" and positions Senna as addressing this limitation through LVLM integration.
- Why unresolved: While the paper shows Senna outperforms baselines on standard metrics, it doesn't specifically analyze performance on rare or edge-case scenarios that challenge autonomous driving systems.
- What evidence would resolve it: Creating or identifying a benchmark dataset containing rare driving scenarios (unusual weather conditions, complex multi-agent interactions, rare traffic situations) and evaluating Senna's performance on these cases compared to traditional end-to-end models and other LVLM-based approaches, particularly measuring success rates and safety metrics in these challenging situations.

## Limitations

- LVLM planning reliability uncertain for complex edge cases in real-world driving scenarios
- Image token compression trade-off may reduce spatial detail below safe thresholds
- Auto-generated Q&A approach may miss critical safety scenarios not well-represented in training data

## Confidence

- **High Confidence**: The core architectural approach of separating high-level planning from low-level trajectory prediction is well-supported by the described mechanisms and baseline comparisons.
- **Medium Confidence**: The three-stage training strategy and planning-oriented Q&A generation are conceptually sound but lack extensive empirical validation across diverse driving scenarios.
- **Low Confidence**: The safety implications of using compressed image tokens and auto-generated Q&As for critical driving decisions require further real-world testing.

## Next Checks

1. **Edge Case Robustness**: Test Senna's performance on challenging driving scenarios not well-represented in training data, such as unusual weather conditions, complex urban intersections, and rare traffic situations to verify safety claims.

2. **Token Compression Sensitivity**: Systematically evaluate the impact of different image token compression ratios (e.g., 16, 32, 64, 128 tokens) on both planning accuracy and spatial understanding to identify the optimal balance.

3. **Real-world Deployment Validation**: Conduct on-road testing to compare Senna's performance against human drivers in terms of safety metrics (collision rate, near-miss events) and comfort metrics (smoothness of maneuvers) to validate simulation results.