---
ver: rpa2
title: Finetuning LLMs for Comparative Assessment Tasks
arxiv_id: '2409.15979'
source_url: https://arxiv.org/abs/2409.15979
tags:
- comparative
- assessment
- probabilities
- comparisons
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scalability challenge in automated natural
  language generation (NLG) assessment, where pairwise comparisons used for comparative
  assessment scale quadratically with the number of items. The authors propose finetuning
  large language models (LLMs) using soft pairwise probabilities derived from true
  score differences, enabling the model's output to align with target distributions
  used in product-of-experts frameworks.
---

# Finetuning LLMs for Comparative Assessment Tasks

## Quick Facts
- arXiv ID: 2409.15979
- Source URL: https://arxiv.org/abs/2409.15979
- Authors: Vatsal Raina; Adian Liusie; Mark Gales
- Reference count: 12
- Primary result: State-of-the-art performance on USMLE response time (RMSE 23.2) and difficulty estimation (RMSE 0.291) using soft finetuning with only 4N comparisons

## Executive Summary
This paper addresses the scalability challenge in automated natural language generation assessment by proposing a soft finetuning approach that reduces the quadratic scaling problem of pairwise comparisons. The method uses soft probabilities derived from true score differences to train LLMs, enabling alignment with product-of-experts frameworks while requiring only 4N comparisons instead of N². Experimental results on USMLE and CMCQRD datasets demonstrate state-of-the-art performance, achieving RMSE of 23.2 on response time estimation and 0.291 on difficulty estimation.

## Method Summary
The approach finetunes LLMs using soft pairwise probabilities generated by scaling true score differences with a sigmoid function controlled by parameter γ. This soft training aligns the model's output distribution with the assumptions of product-of-experts frameworks, specifically the Bradley-Terry model. The method uses soft binary cross entropy loss where labels are soft probabilities rather than hard binary decisions. The finetuned model then performs pairwise comparisons, with results combined using the PoE-BT framework to produce final item scores. The approach maintains high performance while requiring only 4N comparisons instead of the N² needed for full pairwise assessment.

## Key Results
- Achieved RMSE of 23.2 on USMLE response time estimation, surpassing all BEA 2024 shared task submissions
- Obtained RMSE of 0.291 on difficulty estimation for CMCQRD dataset
- Outperformed zero-shot and hard finetuning methods across both datasets
- Maintained high performance with only 4N comparisons instead of N², demonstrating computational efficiency
- Showed consistent improvement over baseline methods in both Spearman's ρ and Pearson's r metrics

## Why This Works (Mechanism)

### Mechanism 1
Finetuning with soft probabilities under the Bradley-Terry distribution reduces distributional mismatch between true and assumed pairwise probabilities. By scaling true score differences with a sigmoid function to create soft training probabilities, the LLM learns to output probabilities that naturally align with the PoE-BT framework's assumed distribution, avoiding the binary decision bias that causes misalignment. Core assumption: The Bradley-Terry model's sigmoid assumption reasonably approximates the true underlying distribution of pairwise preferences.

### Mechanism 2
Soft finetuning enables efficient comparative assessment by reducing the number of comparisons needed while maintaining performance. By training with soft probabilities that push values outside the sigmoid saturation region (controlled by γ), the model learns more discriminative representations that require fewer comparisons per item to achieve accurate rankings. Core assumption: Operating outside the sigmoid saturation region preserves more information about score differences while still being compatible with the PoE framework.

### Mechanism 3
The product-of-experts framework effectively combines pairwise comparison probabilities into coherent item scores. The PoE-BT framework assumes pairwise score differences follow a sigmoid distribution conditioned on LLM probabilities, then finds scores that maximize the joint probability across all comparisons. Core assumption: The Bradley-Terry model's assumption about score difference distributions is valid for the task domain.

## Foundational Learning

- Concept: Pairwise comparison scaling
  - Why needed here: Understanding how N items require N² comparisons for full pairwise assessment explains why efficient methods are crucial
  - Quick check question: For 100 items, how many pairwise comparisons are needed in the naive approach? (Answer: 4950)

- Concept: Product of experts (PoE) framework
  - Why needed here: The scoring method that combines multiple pairwise probability estimates into final item scores
  - Quick check question: What mathematical operation does PoE use to combine expert probabilities? (Answer: Multiplication of probability densities)

- Concept: Sigmoid function properties
  - Why needed here: Critical for understanding how soft probabilities are generated and why saturation regions matter
  - Quick check question: What range of input values to a sigmoid function produces outputs between 0.25 and 0.75? (Answer: Approximately -1.1 to 1.1)

## Architecture Onboarding

- Component map: Data preprocessing -> Soft probability conversion -> Finetuning -> Inference -> PoE scoring -> Evaluation
- Critical path: Data → Soft probability conversion → Finetuning → Inference → PoE scoring → Evaluation
- Design tradeoffs:
  - Soft vs hard finetuning: Soft preserves distributional alignment but requires careful γ tuning; hard is simpler but may misalign with PoE assumptions
  - γ selection: Higher γ values preserve more information but risk pushing probabilities toward 0.5; lower γ values are more stable but may stay in saturation
  - Comparison efficiency: 4N comparisons trade off some accuracy for computational efficiency vs full N² comparisons
- Failure signatures:
  - Poor performance with few comparisons: Likely γ too high or PoE assumption mismatch
  - No improvement from finetuning: Possible training instability or learning rate issues
  - Overfitting to training distribution: Soft probabilities may not generalize if training set is unrepresentative
- First 3 experiments:
  1. Run zero-shot baseline with GPT4o mini and Llama-3.1-8B to establish performance floor
  2. Test hard finetuning with γ=0 to verify basic finetuning works
  3. Sweep γ values {0.1, 0.5, 1.0, 2.0, 5.0, 10.0} to find optimal balance between information preservation and saturation avoidance

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed soft finetuning approach scale with larger LLMs and more diverse datasets? The paper mentions they only experimented with the smallest models due to computational budget constraints, leaving the performance on larger models unexplored. Experimental results comparing the approach on larger LLM variants and more diverse datasets would resolve this.

### Open Question 2
What is the optimal value of γ across different datasets and tasks, and how sensitive is the performance to this hyperparameter? The paper found γ = 5.0 works well but only tested a limited range of values. Systematic hyperparameter search across multiple datasets and tasks would resolve this question.

### Open Question 3
How does the soft finetuning approach compare to other parameter-efficient finetuning methods like LoRA, P-Tuning, or prefix tuning for comparative assessment tasks? The paper uses QLoRA but doesn't compare it against other parameter-efficient methods. Comparative experiments using different methods would determine which approach works best for comparative assessment.

## Limitations

- The approach relies heavily on the Bradley-Terry model's sigmoid assumption, which may not hold for all domains
- All experiments are conducted on medical multiple-choice question datasets, with no evidence for performance on other NLG assessment domains
- The optimal γ parameter value (5.0) was only tested on limited datasets and may not generalize across different model architectures and domains

## Confidence

**High Confidence Claims:**
- Soft finetuning with Bradley-Terry probabilities outperforms zero-shot and hard finetuning on USMLE and CMCQRD datasets
- The PoE-BT framework effectively combines pairwise probabilities into coherent item scores when properly aligned with LLM outputs
- The 4N comparison efficiency maintains strong performance relative to full N² comparisons

**Medium Confidence Claims:**
- The distributional alignment mechanism is the primary driver of performance improvements
- The approach generalizes to non-medical NLG assessment tasks
- γ = 5.0 is near-optimal across different model architectures and domains

**Low Confidence Claims:**
- The approach will maintain performance gains when scaled to datasets with thousands of items
- The computational efficiency gains will translate to real-world deployment scenarios with strict latency requirements
- The soft finetuning strategy is robust to different LLM architectures beyond the tested models

## Next Checks

1. **Cross-Domain Validation**: Apply the soft finetuning approach to a non-medical NLG assessment task (e.g., story generation quality, code generation correctness) and measure whether the performance gains translate. Compare against zero-shot and hard finetuning baselines using the same γ = 5.0 parameter.

2. **Distributional Analysis**: For a subset of comparisons from the USMLE dataset, empirically estimate the true pairwise probability distribution and compare it against the Bradley-Terry sigmoid assumption. Use this to quantify the actual distributional mismatch that soft finetuning addresses.

3. **γ Sensitivity Sweep**: Conduct a systematic hyperparameter search across γ values {0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0} on both USMLE and CMCQRD datasets, measuring not just final performance but also the entropy of the soft probability distributions to understand the information preservation-saturation tradeoff more precisely.