---
ver: rpa2
title: Transformer based Multitask Learning for Image Captioning and Object Detection
arxiv_id: '2403.06292'
source_url: https://arxiv.org/abs/2403.06292
tags:
- image
- object
- detection
- captioning
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TICOD, a Transformer-based multitask learning
  framework that jointly trains image captioning and object detection. The model leverages
  a shared Swin Transformer backbone and combines losses from both tasks to improve
  image representations for captioning.
---

# Transformer based Multitask Learning for Image Captioning and Object Detection

## Quick Facts
- arXiv ID: 2403.06292
- Source URL: https://arxiv.org/abs/2403.06292
- Authors: Debolena Basak; P. K. Srijith; Maunendra Sankar Desarkar
- Reference count: 39
- Primary result: TICOD achieves 3.65% BERTScore improvement for image captioning through joint training with object detection

## Executive Summary
This paper proposes TICOD, a Transformer-based multitask learning framework that jointly trains image captioning and object detection. The model leverages a shared Swin Transformer backbone and combines losses from both tasks to improve image representations for captioning. By jointly optimizing the two tasks, TICOD achieves a 3.65% improvement in BERTScore for image captioning compared to task-specific baselines on the MS-COCO dataset, while maintaining comparable object detection performance. The multitask approach enables complementary information sharing between the tasks, leading to better visual understanding.

## Method Summary
TICOD uses a shared Swin Transformer backbone with a Feature Pyramid Network (FPN) neck for both tasks. The object detection head uses Cascade R-CNN framework with Region Proposal Network (RPN), RoI pooling, classification, and bounding box regression. The captioning head uses a GPT-2 decoder with cross-attention mechanism. The model is trained end-to-end using a joint loss function that combines object detection loss (classification + bounding box regression) and captioning loss (cross-entropy) with a tunable λ parameter to balance task contributions.

## Key Results
- 3.65% improvement in BERTScore for image captioning compared to task-specific baselines
- Maintains comparable object detection performance while improving captioning
- Single backbone approach eliminates error propagation issues from two-stage methods
- End-to-end training improves efficiency compared to cached feature approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training of image captioning and object detection tasks improves image representations for captioning by sharing complementary information.
- Mechanism: The shared Swin Transformer backbone learns richer features by being optimized for both tasks simultaneously. Object detection's need for precise localization and classification helps the backbone extract more discriminative features, which benefits the captioning task's need for semantic understanding.
- Core assumption: The tasks are complementary - object detection's focus on local object features and image captioning's need for global semantic context can mutually reinforce each other.
- Evidence anchors:
  - [abstract]: "By jointly optimizing the two tasks, TICOD achieves a 3.65% improvement in BERTScore for image captioning compared to task-specific baselines... The multitask approach enables complementary information sharing between the tasks, leading to better visual understanding."
  - [section]: "The key idea is multitask learning across object detection and image captioning that enables the model to develop a better representation learning capability. This shared representation learning enables the model to leverage the knowledge gained from each task to effectively align the backbone representations, enhancing the overall learning capacity."
  - [corpus]: Weak - the corpus neighbors don't directly address multitask learning benefits for image captioning specifically.

### Mechanism 2
- Claim: The multitask loss function with a tunable lambda parameter allows balancing the contributions of each task to optimize overall performance.
- Mechanism: By combining the object detection loss (classification + bounding box regression) and captioning loss (cross-entropy) with a weighting factor λ, the model can be trained to prioritize one task over the other while still learning useful representations for both.
- Core assumption: There exists an optimal λ value that balances the two tasks to maximize the performance of the primary task (image captioning) while maintaining acceptable performance on the auxiliary task (object detection).
- Evidence anchors:
  - [abstract]: "Our approach utilizes a transformer-based architecture that enables end-to-end network integration for image captioning and object detection and performs both tasks jointly."
  - [section]: "For training our model, we use the joint multitask learning objective function, which combine the image captioning and object detection losses as LT (θ, ϕ, ψ) = LO(θ, ψ) + λ · LC(θ, ϕ) where λ is the weightage to be given to the captioning loss."
  - [corpus]: Weak - corpus doesn't specifically address multitask loss balancing.

### Mechanism 3
- Claim: Using a single Swin Transformer backbone for both tasks eliminates the error propagation and inefficiency issues of two-stage approaches that use separate feature extractors.
- Mechanism: Instead of first extracting features with a pre-trained object detector and then using those features for captioning, the single backbone is trained end-to-end to extract features optimized for both tasks simultaneously, avoiding errors from mismatched pre-training objectives.
- Core assumption: End-to-end training of a single backbone is more efficient and effective than using pre-extracted features from task-specific models.
- Evidence anchors:
  - [section]: "Most mainstream image captioning models [1,5] use a two-step training method, which firstly extracts image regional features using a pre-trained object detector like Faster R-CNN [26], then feeds these feature vectors into an encoder-decoder framework for image captioning. However, this approach has a few inherent shortcomings: 1) the object detection model in the first step is trained on specific visual datasets like the Visual Genome, and the visual representation is not optimized towards the image captioning dataset used (commonly MS-COCO). This may lead to an error propagation problem if the object detector fails to recognize certain important visual information [21], 2) the time-intensive nature of extracting region features causes state-of-the-art models to rely on cached visual features (usually pre-computed) for training and evaluation, imposing constraints on model designs and resulting in run-time inference inefficiencies during prediction [33,31,13]."
  - [corpus]: Weak - corpus doesn't directly address two-stage vs. end-to-end approaches.

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanism
  - Why needed here: The Swin Transformer backbone uses self-attention to capture relationships between image patches at different scales, which is crucial for both object detection (localizing objects) and image captioning (understanding context).
  - Quick check question: How does the shifted window approach in Swin Transformer help capture global context while maintaining computational efficiency?

- Concept: Multitask learning and joint optimization
  - Why needed here: Understanding how to design and train models that can learn multiple related tasks simultaneously is fundamental to this work's approach.
  - Quick check question: What are the potential benefits and drawbacks of multitask learning compared to training separate models for each task?

- Concept: Object detection frameworks (Faster R-CNN, Cascade R-CNN)
  - Why needed here: The object detection component uses these frameworks, and understanding their components (RPN, RoI pooling, FPN) is necessary to understand how the multitask model works.
  - Quick check question: How does the Feature Pyramid Network (FPN) help detect objects at different scales?

## Architecture Onboarding

- Component map: Raw image -> Swin Transformer -> Feature maps -> FPN -> Multi-scale features -> RPN -> Object proposals -> RoI pooling -> Classification + BBox regression AND Feature maps -> GPT-2 (with cross-attention) -> Caption generation

- Critical path: 1. Image → Swin Transformer → Feature maps 2. Feature maps → FPN → Multi-scale features 3. Multi-scale features → RPN → Object proposals 4. Object proposals + features → RoI pooling → Classification + BBox regression 5. Feature maps → GPT-2 (with cross-attention) → Caption generation

- Design tradeoffs:
  - Shared vs. separate backbones: Shared reduces parameters and enables information sharing but may limit task-specific optimization
  - λ value: Higher λ prioritizes captioning performance but may hurt object detection; lower λ does the opposite
  - Backbone size (Swin-T vs. Swin-B): Larger backbones have more capacity but are more computationally expensive

- Failure signatures:
  - Caption quality degrades significantly: May indicate λ is too low or backbone isn't learning good representations for captioning
  - Object detection mAP drops significantly: May indicate λ is too high or object detection components aren't being trained adequately
  - Training instability: May indicate λ is poorly chosen or learning rates need adjustment

- First 3 experiments:
  1. Ablation study: Train with only captioning head active (λ=0) to establish baseline performance
  2. Ablation study: Train with only object detection head active to establish baseline performance
  3. Multitask training: Train with both heads active and vary λ values (0.1, 0.2, 0.5, 1.0) to find optimal balance and measure improvements in both tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of λ (weight for captioning loss) affect the trade-off between image captioning and object detection performance in TICOD?
- Basis in paper: [explicit] The paper mentions that λ is chosen to maximize evaluation scores of both tasks and that different values are used for TICOD-small (λ=0.1) and TICOD-large (λ=0.2).
- Why unresolved: The paper only tests a limited range of λ values and doesn't provide a comprehensive analysis of how different λ values impact the performance of both tasks across different model sizes.
- What evidence would resolve it: A systematic study varying λ across a wider range of values and analyzing the resulting performance on both image captioning and object detection tasks for different model sizes.

### Open Question 2
- Question: Can the proposed multitask learning approach be extended to other vision-language tasks beyond image captioning and object detection?
- Basis in paper: [inferred] The paper focuses on image captioning and object detection, but the authors mention that the framework is customizable and can be extended to newer specialized models.
- Why unresolved: The paper doesn't explore the applicability of the multitask learning approach to other vision-language tasks or provide insights into potential challenges or benefits of such extensions.
- What evidence would resolve it: Experiments applying the multitask learning framework to other vision-language tasks like visual question answering or image-text retrieval, and analyzing the performance and potential benefits.

### Open Question 3
- Question: How does the proposed approach compare to other methods that use pre-trained vision-language models like CLIP for image captioning?
- Basis in paper: [explicit] The paper mentions that ClipCap uses CLIP encoder and GPT2 decoder for image captioning, but doesn't provide a direct comparison with TICOD.
- Why unresolved: The paper doesn't provide a comprehensive comparison of TICOD with other state-of-the-art methods that leverage pre-trained vision-language models for image captioning.
- What evidence would resolve it: A direct comparison of TICOD with methods like ClipCap and other recent approaches that use pre-trained vision-language models, evaluating performance on standard image captioning benchmarks.

## Limitations

- Limited ablation studies on λ values prevent understanding of optimal balance between tasks
- Absolute BERTScore values not provided, making practical significance unclear
- No comparison with state-of-the-art single-task models to validate multitask benefits

## Confidence

- **High confidence**: The mechanism of shared backbone learning complementary features is well-supported by the architecture description and aligns with established multitask learning principles.
- **Medium confidence**: The claim of 3.65% BERTScore improvement is based on the reported results, but lacks absolute baseline values and comprehensive ablation studies.
- **Low confidence**: The assertion that end-to-end training with a single backbone is inherently more efficient than two-stage approaches lacks empirical comparison of training/inference times.

## Next Checks

1. **Ablation study**: Run experiments with λ values at 0.1, 0.5, 1.0, and 2.0 to quantify the trade-off between captioning and object detection performance and identify the optimal balance point.

2. **Efficiency analysis**: Measure and compare training time, inference time, and memory usage between TICOD and a two-stage approach using a pre-trained object detector followed by a separate captioning model.

3. **Cross-dataset generalization**: Evaluate the model's performance on a different dataset (e.g., Flickr30k) to assess whether the multitask learning benefits transfer beyond the MS-COCO domain.