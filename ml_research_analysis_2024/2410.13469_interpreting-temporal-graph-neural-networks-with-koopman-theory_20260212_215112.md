---
ver: rpa2
title: Interpreting Temporal Graph Neural Networks with Koopman Theory
arxiv_id: '2410.13469'
source_url: https://arxiv.org/abs/2410.13469
tags:
- time
- koopman
- nodes
- dynamics
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Koopman-theory-based interpretability approach
  for spatiotemporal graph neural networks (STGNNs). The method leverages dynamic
  mode decomposition (DMD) and sparse identification of nonlinear dynamics (SINDy)
  to identify the most relevant spatial and temporal patterns in temporal graphs that
  drive model decisions.
---

# Interpreting Temporal Graph Neural Networks with Koopman Theory

## Quick Facts
- arXiv ID: 2410.13469
- Source URL: https://arxiv.org/abs/2410.13469
- Reference count: 40
- Key outcome: Koopman-theory-based interpretability approach for STGNNs achieving F1 scores 0.31-0.50 for temporal explanations and AUC scores 0.54-0.95 for node-level explanations

## Executive Summary
This paper introduces a Koopman-theory-based interpretability approach for spatiotemporal graph neural networks (STGNNs). The method leverages dynamic mode decomposition (DMD) and sparse identification of nonlinear dynamics (SINDy) to identify the most relevant spatial and temporal patterns in temporal graphs that drive model decisions. The approach is demonstrated on a Graph Convolutional Recurrent Network (GCRN) trained to classify dissemination processes, where it successfully identifies infection times and infected nodes. Experimental results show F1 scores between 0.31-0.50 for temporal explanations (compared to baseline of 0.004-0.03), AUC scores of 0.54-0.95 for node-level explanations, and AUC scores of 0.60-0.90 for edge-level explanations across five datasets. The method provides both instance-level and model-level insights, revealing that the GCRN learns to implicitly count infected nodes over time. The approach is generalizable to other STGNNs and offers a principled way to interpret complex spatiotemporal dynamics in graph-based models.

## Method Summary
The method modifies a Graph Convolutional Recurrent Network (GCRN) to learn a Koopman observable representation, where node embeddings evolve approximately linearly in the embedding space. The GCRN is trained with auxiliary losses that enforce linear dynamics between consecutive embeddings (ℓobs) and reconstruction accuracy (ℓrec). Once trained, the method applies Dynamic Mode Decomposition (DMD) to extract temporal patterns from the embedding dynamics, identifying significant changes that drive model decisions. SINDy is then used to identify the most important edges by learning sparse regression weights that represent edge importance in the nonlinear dynamics reconstruction. The approach provides instance-level explanations by identifying relevant time points and nodes, as well as model-level insights by revealing the learned dynamics.

## Key Results
- F1 scores of 0.31-0.50 for temporal explanations (vs. baseline 0.004-0.03) across five datasets
- AUC scores of 0.54-0.95 for node-level explanations identifying infected nodes
- AUC scores of 0.60-0.90 for edge-level explanations identifying infection paths
- Reveals GCRN learns to implicitly count infected nodes over time through model-level analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Koopman operator theory transforms nonlinear spatiotemporal dynamics into a linear representation, enabling simpler interpretation of complex STGNN behavior.
- Mechanism: By training the GCRN to represent a Koopman observable, the embeddings ht evolve approximately linearly (ht+1 ≈ Kht), allowing DMD to extract interpretable temporal and spatial patterns.
- Core assumption: The auxiliary losses (ℓobs and ℓrec) successfully encourage the GCRN to learn a linear embedding dynamics that preserves the model's accuracy.
- Evidence anchors:
  - [abstract] "Inspired by Koopman theory, which allows a simpler description of intricate, nonlinear dynamical systems, we introduce an explainability approach for temporal graphs."
  - [section 3.1] "Thanks to the auxiliary losses in (16), we expect the dynamics of the embeddings of the node ht,n and of the whole TG ht to be approximately linear."
- Break condition: If the auxiliary losses fail to enforce linear dynamics in the embeddings, DMD analysis will not reveal meaningful patterns.

### Mechanism 2
- Claim: DMD modes capture the most relevant temporal patterns driving model decisions by identifying significant changes in the embedding dynamics.
- Mechanism: The time weight wt(t) is derived from the derivative of DMD modes, highlighting times when important input features occur that influence the model's output.
- Core assumption: When |λi| ≃ 1, the corresponding DMD mode reflects crucial input features for the downstream task.
- Evidence anchors:
  - [section 3.2.1] "When |λi| ≃ 1, its dynamics reflect crucial input features, both on its topology and its time dependency, meaning that the i-th mode explains the model's output."
  - [section 4.3] "We further verify hypothesis 1 with the Mann-Whitney U test, which shows that the distribution fgt of w(i)t around the ground truth and the distribution fr at random times are statistically very different."
- Break condition: If DMD modes are dominated by noise or irrelevant patterns, the time weights will not correlate with actual infection times.

### Mechanism 3
- Claim: SINDy identifies the most important edges by learning sparse regression weights that represent edge importance in the nonlinear dynamics reconstruction.
- Mechanism: The SINDy library Θ is constructed using terms involving neighboring nodes (HnHm), and the regression weights ξn,j measure how important each edge is for the node embedding dynamics.
- Core assumption: The edge importance can be captured by considering only monomials up to third degree involving neighboring nodes.
- Evidence anchors:
  - [section 3.2.2] "Due to the GCN layer, each node's state is a nonlinear combination of the neighbouring nodes. Therefore, we can consider as nonlinearities only those terms involving couples of neighbouring nodes."
  - [section 4.3] "AUC(3)edge is almost always larger than AUC(2)edge, as expected, since adding more nonlinearities into the library Θ makes SINDy more expressive."
- Break condition: If important edges involve higher-order interactions not captured by the third-degree monomials, SINDy will miss them.

## Foundational Learning

- Concept: Koopman operator theory
  - Why needed here: Provides the theoretical foundation for transforming nonlinear spatiotemporal dynamics into a linear representation that can be interpreted using DMD.
  - Quick check question: What is the key advantage of using Koopman operators for analyzing complex dynamical systems?

- Concept: Dynamic Mode Decomposition (DMD)
  - Why needed here: Enables extraction of interpretable temporal and spatial patterns from the linear embedding dynamics of the STGNN.
  - Quick check question: How does DMD identify the most relevant modes in a dynamical system?

- Concept: Sparse Identification of Nonlinear Dynamics (SINDy)
  - Why needed here: Provides a method to identify the most important edges in the spatiotemporal graph by learning sparse regression weights.
  - Quick check question: What is the key principle behind SINDy's ability to discover governing equations from data?

## Architecture Onboarding

- Component map:
  GCRN encoder -> Koopman operator K -> DMD analysis module -> SINDy analysis module -> MLP classifier

- Critical path: Input temporal graph → GCRN encoder → Linear embedding dynamics (via K) → DMD/SINDy analysis → Interpretability results

- Design tradeoffs:
  - Tradeoff between embedding dimensionality f and computational complexity of DMD analysis
  - Tradeoff between expressiveness of SINDy library Θ and risk of overfitting
  - Tradeoff between the strength of regularization parameters α and β and model accuracy

- Failure signatures:
  - Low F1 scores for temporal explanations indicate DMD is not capturing relevant patterns
  - Low AUC scores for spatial explanations indicate SINDy is not identifying important edges/nodes
  - Significant drop in classification accuracy indicates the Koopman regularizer is too strong

- First 3 experiments:
  1. Train GCRN with Koopman regularizer on a small dataset (e.g., Highschool) and verify classification accuracy is maintained
  2. Apply DMD analysis to the trained model's embeddings and visualize the top DMD modes to check for meaningful patterns
  3. Apply SINDy analysis to identify important edges and compare with ground truth edge importance scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Koopman-based explainability methods vary across different types of STGNN architectures beyond GCRN?
- Basis in paper: [explicit] The paper mentions that "the proposed approach can be extended to other STGNNs" but only demonstrates results on GCRN
- Why unresolved: The experiments only tested on GCRN, leaving uncertainty about generalizability to other architectures like graph attention networks or temporal graph attention networks
- What evidence would resolve it: Direct comparison of the Koopman-based methods on multiple STGNN architectures using the same datasets and evaluation metrics

### Open Question 2
- Question: What is the theoretical relationship between the choice of nonlinear functions in the SINDy library and the accuracy of edge-level explanations?
- Basis in paper: [explicit] The authors note that "the difficult and somewhat arbitrary part of SINDy is the choice of the library of nonlinearities Θ" and only test up to third-degree monomials
- Why unresolved: The paper doesn't systematically explore how different function libraries affect explanation quality, particularly for complex graph structures
- What evidence would resolve it: Systematic experiments varying the SINDy library composition (polynomial degree, trigonometric functions, etc.) and measuring impact on AUC edge scores

### Open Question 3
- Question: How sensitive are the Koopman-based explanations to hyperparameter choices like dimensionality reduction (f) and threshold values (δ)?
- Basis in paper: [explicit] The authors perform grid search over hyperparameters and note that "the F1 scores depend on the threshold δ and the window size ω"
- Why unresolved: While hyperparameters are tuned, the paper doesn't analyze sensitivity or provide guidelines for selecting these values in new applications
- What evidence would resolve it: Robustness analysis showing how explanation quality varies across hyperparameter ranges and identifying critical parameters that most affect performance

## Limitations
- The method's performance heavily depends on the assumption that auxiliary losses successfully enforce linear dynamics in the embedding space
- SINDy approach is limited to capturing interactions up to third-degree monomials, potentially missing higher-order interactions
- The method requires ground truth explanations for validation, which may not be available in real-world applications

## Confidence

- Mechanism 1 (Koopman theory transformation): High - Well-established theoretical foundation with empirical validation
- Mechanism 2 (DMD temporal pattern extraction): Medium - Relies on specific assumptions about Koopman mode magnitudes
- Mechanism 3 (SINDy edge identification): Medium - Limited by choice of nonlinear function library and degree of monomials

## Next Checks
1. Test the method's robustness to different embedding dimensionalities by varying f and measuring the impact on explanation quality and model accuracy
2. Validate the approach on a real-world dataset without ground truth explanations to assess practical utility
3. Compare the Koopman-based approach with alternative explainability methods (e.g., attention-based or gradient-based) on the same tasks to establish relative performance