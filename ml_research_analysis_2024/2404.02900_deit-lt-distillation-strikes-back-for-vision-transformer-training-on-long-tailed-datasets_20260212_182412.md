---
ver: rpa2
title: DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed
  Datasets
arxiv_id: '2404.02900'
source_url: https://arxiv.org/abs/2404.02900
tags:
- deit-lt
- classes
- training
- distillation
- tail
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DeiT-LT, a method to train Vision Transformers
  (ViTs) from scratch on long-tailed datasets, which are common in real-world applications.
  The key challenge is that ViTs struggle with long-tailed data due to their lack
  of inductive bias and reliance on large-scale pre-training.
---

# DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets

## Quick Facts
- arXiv ID: 2404.02900
- Source URL: https://arxiv.org/abs/2404.02900
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on long-tailed datasets without pre-training

## Executive Summary
This paper addresses the challenge of training Vision Transformers (ViTs) on long-tailed datasets, where ViTs typically struggle due to their lack of inductive bias and reliance on large-scale pre-training. DeiT-LT introduces a novel distillation approach that leverages out-of-distribution images and re-weighted loss functions to improve ViT performance on tail classes. By distilling knowledge from CNN teachers and creating specialized expert tokens within the ViT architecture, the method achieves significant improvements across multiple long-tailed benchmark datasets.

## Method Summary
DeiT-LT trains ViTs from scratch on long-tailed data by distilling knowledge from a CNN teacher using out-of-distribution images and re-weighting the distillation loss to focus on tail classes. The method creates two expert tokens within the ViT: one for head classes and one for tail classes. A flat CNN teacher trained via Sharpness Aware Minimization (SAM) induces low-rank generalizable features across all ViT blocks. The approach eliminates the need for pre-training while achieving state-of-the-art performance on long-tailed datasets.

## Key Results
- Achieves state-of-the-art performance on CIFAR-10 LT, CIFAR-100 LT, ImageNet-LT, and iNaturalist-2018
- Creates two specialized expert tokens within ViT: CLS for head classes, DIST for tail classes
- Eliminates need for pre-training while maintaining competitive performance
- Demonstrates significant improvements in tail class accuracy compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
Out-of-distribution (OOD) distillation from a CNN teacher induces local, CNN-like feature locality in early ViT blocks. Strong augmentations create OOD samples that the teacher CNN predicts poorly on, but the ViT student learns to mimic these incorrect predictions, effectively internalizing the teacher's inductive bias (locality). Core assumption: Learning to mimic incorrect teacher predictions on OOD data still transfers useful inductive biases to the student.

### Mechanism 2
Distilling from flat CNN teachers trained via Sharpness Aware Minimization (SAM) induces low-rank, generalizable features in the DIST token across all ViT blocks. SAM training converges to flat minima, producing low-rank features. Distilling these features into the ViT student transfers this characteristic, leading to more generalizable features for tail classes. Core assumption: Low-rank features learned by the teacher via SAM are generalizable and can be effectively transferred to the ViT student through distillation.

### Mechanism 3
The divergence of CLS and DIST tokens during training creates two specialized experts within the ViT: CLS for head classes and DIST for tail classes. Since the DIST token learns from the teacher's predictions (which differ from ground truth due to OOD distillation), it develops representations specialized for tail classes, while the CLS token remains focused on head classes. Core assumption: Having separate expert tokens for head and tail classes is more effective than a single classifier for long-tailed data.

## Foundational Learning

- Concept: Vision Transformers (ViTs) and their lack of inductive bias compared to CNNs.
  - Why needed here: Understanding why ViTs struggle with long-tailed data is crucial for appreciating the need for DeiT-LT's innovations.
  - Quick check question: What is the key difference between ViTs and CNNs that makes ViTs require more data for effective training?

- Concept: Knowledge distillation and its application in DeiT.
  - Why needed here: DeiT-LT builds upon the DeiT framework, so understanding how knowledge distillation works in this context is essential.
  - Quick check question: In DeiT, what are the two types of tokens in the ViT architecture, and how do they learn?

- Concept: Long-tailed data distributions and their challenges.
  - Why needed here: The entire motivation for DeiT-LT is to address the challenges of training ViTs on long-tailed datasets.
  - Quick check question: What is the primary challenge when training models on long-tailed datasets, particularly for minority (tail) classes?

## Architecture Onboarding

- Component map:
  Image patches -> Linear layer -> ViT Backbone (CLS, DIST tokens) -> CNN Teacher -> Augmentation -> Loss computation -> Backpropagation

- Critical path:
  1. Prepare input image with strong augmentations
  2. Process through ViT backbone to get CLS and DIST token representations
  3. Compute loss: CE loss for CLS token against ground truth, DRW loss for DIST token against teacher predictions on OOD images
  4. Backpropagate and update ViT weights

- Design tradeoffs:
  - Using a smaller CNN teacher with weak augmentations vs. a larger teacher with strong augmentations (DeiT-LT vs. DeiT)
  - Focusing DIST token on tail classes vs. trying to improve tail performance through other means (e.g., re-weighting, expert networks)
  - Inducing local features vs. maintaining ViT's global attention capabilities

- Failure signatures:
  - If the DIST token does not show improved performance on tail classes compared to the CLS token
  - If the overall accuracy on head classes degrades significantly compared to baselines
  - If the model overfits to the training data, particularly for tail classes

- First 3 experiments:
  1. Compare the performance of DeiT-LT with vanilla DeiT on a small-scale long-tailed dataset (e.g., CIFAR-10 LT) to verify the effectiveness of OOD distillation
  2. Analyze the attention maps of CLS and DIST tokens on tail class images to confirm their specialization
  3. Vary the strength of augmentations used for OOD distillation to find the optimal balance between creating informative OOD samples and maintaining a meaningful distillation signal

## Open Questions the Paper Calls Out

The paper explicitly states their goal is to develop a generic technique for training ViTs across domains and modalities on long-tailed data without requiring any external supervision, and they mention specialized domains like medicine and satellite imagery as examples where pre-training might not generalize well. However, experiments were only conducted on standard computer vision datasets (CIFAR, ImageNet-LT, iNaturalist) which don't represent the extreme domain shifts found in medical or satellite imagery.

## Limitations

- Key implementation details remain unspecified, particularly the exact augmentation strategy for OOD distillation which is critical for the method's effectiveness
- The DRW loss implementation and its integration with the distillation process lack specificity
- Ablation studies do not isolate the individual contributions of each proposed component (OOD distillation, DRW loss, SAM-trained teacher)

## Confidence

- High Confidence: The overall experimental results showing DeiT-LT outperforming baseline methods on long-tailed datasets
- Medium Confidence: The mechanism by which OOD distillation induces CNN-like local features in early ViT blocks
- Low Confidence: The claim that distilling from SAM-trained flat CNN teachers specifically induces low-rank generalizable features that benefit tail classes

## Next Checks

1. Run ablation studies with each component (OOD distillation, DRW loss, SAM teacher) separately to quantify their individual contributions to tail class performance improvements

2. Generate and compare attention maps for CLS and DIST tokens on head vs. tail class examples to empirically verify their claimed specialization

3. Systematically vary the CNN teacher's accuracy on OOD samples to determine the minimum threshold needed for effective distillation, testing the break condition for Mechanism 1