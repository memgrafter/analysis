---
ver: rpa2
title: Towards Zero-Shot, Controllable Dialog Planning with LLMs
arxiv_id: '2410.05821'
source_url: https://arxiv.org/abs/2410.05821
tags:
- user
- dialog
- node
- goal
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CTS-LLM, a zero-shot, controllable dialog
  planning method for Large Language Models (LLMs) in task-oriented dialog systems.
  The approach addresses the challenges of planning towards overarching dialog goals
  and avoiding hallucination in sensitive domains.
---

# Towards Zero-Shot, Controllable Dialog Planning with LLMs

## Quick Facts
- arXiv ID: 2410.05821
- Source URL: https://arxiv.org/abs/2410.05821
- Authors: Dirk Väth; Ngoc Thang Vu
- Reference count: 15
- Key outcome: CTS-LLM significantly outperforms RL-based CTS agents in simulation (p<0.0001) and user studies (9.71% higher task success rate, p<0.05)

## Executive Summary
This paper introduces CTS-LLM, a zero-shot dialog planning method that uses Large Language Models to navigate task-oriented dialog graphs without retraining. The approach addresses the challenge of planning towards overarching dialog goals while avoiding hallucination in sensitive domains. By combining semantic search with LLM-based filtering and path planning, the method achieves higher task success rates than state-of-the-art RL-based approaches while reducing computational resource requirements and enabling real-time interactions.

## Method Summary
CTS-LLM employs a dialog policy that traverses predefined dialog graphs using LLM decisions for path planning while restricting outputs to predefined graph node texts. The method uses semantic search to pre-filter goal nodes, then applies LLM-based post-filtering with justifications to select the most relevant nodes. The system determines the longest shared path prefix across candidates to minimize user clarifications. Interaction mode classification, intent classification, and goal node filtering are handled by the same LLM through in-context examples, enabling zero-shot adaptation without training cycles.

## Key Results
- CTS-LLM significantly outperforms state-of-the-art RL-based CTS agents in simulation (p<0.0001) across all three domains
- User study on REIMBURSE-en domain shows 9.71% higher task success rate compared to RL-based agent (p<0.05)
- The approach reduces computational resource requirements and enables real-time dialog interactions (~18s per question vs ~34s for RL)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based path planning outperforms RL-based CTS agents in simulation across all three domains
- Mechanism: Combines fast semantic search with LLM-based post-filtering to retrieve and rank goal nodes, computing longest shared path prefix to reduce clarifications
- Core assumption: LLM can accurately filter relevant goal nodes from pre-retrieved candidates
- Evidence: [abstract] "significantly outperform state-of-the-art CTS agents (p<0.0001; Barnard Exact test) in simulation"
- Break condition: If LLM filtering accuracy drops below ~80% recall, dialog success falls below RL baseline

### Mechanism 2
- Claim: Zero-shot adaptation eliminates retraining cost and enables real-time dialog interactions
- Mechanism: Delegates multiple classification tasks to single LLM with in-context examples, reducing inference time from ~34s to ~18s per question
- Core assumption: Single LLM can handle multiple tasks without catastrophic interference
- Evidence: [abstract] "significantly improves task-success compared to state-of-the-art RL-based CTS agent" and "reduces computational resource requirements"
- Break condition: If inference time exceeds ~25s or GPU memory exceeds ~27GB, real-time interaction becomes infeasible

### Mechanism 3
- Claim: Controllable dialog flow is maintained by outputting only predefined graph node texts
- Mechanism: Policy traverses graph node-by-node, using LLM only for planning decisions, never for user-facing content
- Core assumption: Restricting LLM output to planning decisions ensures dialog controllability
- Evidence: [abstract] "adhering to CTS controllability aspects" and "avoid hallucination"
- Break condition: If LLM occasionally outputs node texts or deviates from graph structure, controllability is compromised

## Foundational Learning

- Concept: Graph traversal algorithms (BFS, DFS, path prefix computation)
  - Why needed here: Must efficiently find longest shared path prefix across goal candidates to minimize user clarifications
  - Quick check question: Given paths [A,B,C,D] and [A,B,E,F], what is the longest shared prefix? (Answer: [A,B])

- Concept: Semantic similarity search and embedding models
  - Why needed here: Fast retrieval of candidate goal nodes based on user utterance similarity before LLM filtering
  - Quick check question: If k=15 pre-filtering retrieves nodes with recall 0.84, what fraction of relevant nodes might still be missed? (Answer: ~16%)

- Concept: In-context learning and few-shot prompting
  - Why needed here: LLM-based classifiers operate without training data, relying on provided examples
  - Quick check question: How many in-context examples are typically needed for stable intent classification? (Answer: 3-5 examples based on ablation study)

## Architecture Onboarding

- Component map: User → Interaction Mode Classifier → (Guided Mode → Intent Classifier → Navigation) OR (Free Mode → Goal Node Filter → Navigation) → Dialog State Tracker → Graph
- Critical path: User utterance → interaction mode detection → (goal node retrieval + LLM filtering) → longest prefix computation → node output → user response → repeat until goal reached
- Design tradeoffs: Single LLM reduces hardware requirements but may create interference; pre-filtering reduces LLM input size but adds complexity
- Failure signatures: Low recall in goal filtering (dialog success drops), high latency in response time (>25s), or incorrect interaction mode classification (unnecessary clarifications)
- First 3 experiments:
  1. Test interaction mode classification accuracy on 100 diverse user utterances
  2. Benchmark goal node filter recall with k=5, 10, 15, 20 candidates
  3. Measure full dialog success rate against CTS simulator for 50 dialogs per domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CTS-LLM performance scale with graph size and complexity?
- Basis in paper: [explicit] The paper mentions "decouples the slow filtering step from the graph size" and discusses resource usage
- Why unresolved: Only evaluated on three domains with relatively small graphs (up to 123 nodes)
- What evidence would resolve it: Systematic evaluation on dialog graphs with varying sizes (500, 1000, 5000 nodes) measuring success rates and resource usage

### Open Question 2
- Question: Can CTS-LLM be effectively extended to handle multimodal user inputs?
- Basis in paper: [inferred] Focuses solely on text-based dialog planning without exploring multimodal capabilities
- Why unresolved: Doesn't investigate multimodal inputs or required modifications
- What evidence would resolve it: Implementation and evaluation of multimodal CTS-LLM on dialog tasks with voice or image inputs

### Open Question 3
- Question: How does CTS-LLM perform in domains with high linguistic variability or domain-specific terminology?
- Basis in paper: [explicit] Evaluates on three domains but doesn't analyze performance in highly variable or specialized domains
- Why unresolved: Performance on domains with complex language patterns or jargon is unknown
- What evidence would resolve it: Evaluation on domains known for high linguistic variability (legal, medical) comparing performance metrics

## Limitations
- Evaluation limited to three specific CTS domains with small graphs (up to 123 nodes)
- User diversity limited in main study (n=12 participants)
- LLM components rely on specific prompting strategies that may not transfer to other dialog systems
- Absolute resource requirements (GPU memory, inference time) not fully characterized across hardware configurations

## Confidence

- Dialog success improvement over RL baseline: **High** - supported by simulation (p<0.0001) and user study (p<0.05)
- Zero-shot adaptation eliminating retraining: **Medium** - demonstrated operationally but runtime benchmarks limited
- Controllability through graph restriction: **Medium** - theoretically sound but no direct hallucination prevention measurements

## Next Checks
1. Test the full system with 100+ diverse user utterances across multiple demographic groups to verify interaction mode classification robustness
2. Measure absolute GPU memory usage and inference latency on different hardware configurations (CPU, various GPU models) to quantify real-time feasibility claims
3. Conduct A/B testing with 50+ users per condition comparing CTS-LLM against multiple RL baselines across all three domains to confirm generalizability of success rate improvements