---
ver: rpa2
title: Resurrecting Old Classes with New Data for Exemplar-Free Continual Learning
arxiv_id: '2405.19074'
source_url: https://arxiv.org/abs/2405.19074
tags:
- task
- learning
- drift
- adversarial
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in exemplar-free continual
  learning (EFCIL), focusing on the challenge of feature drift estimation when starting
  with small initial tasks. Existing methods like SDC [56] struggle in such scenarios
  because their drift estimation relies on current task samples that may be distant
  from old class prototypes in the embedding space.
---

# Resurrecting Old Classes with New Data for Exemplar-Free Continual Learning

## Quick Facts
- arXiv ID: 2405.19074
- Source URL: https://arxiv.org/abs/2405.19074
- Authors: Dipam Goswami; Albin Soutif--Cormerais; Yuyang Liu; Sandesh Kamath; Bartłomiej Twardowski; Joost van de Weijer
- Reference count: 40
- Primary result: State-of-the-art exemplar-free continual learning performance with up to 4.2% accuracy improvement over existing methods

## Executive Summary
This paper addresses catastrophic forgetting in exemplar-free continual learning (EFCIL) by introducing Adversarial Drift Compensation (ADC). The core insight is that existing drift estimation methods struggle when starting with small initial tasks because current task samples may be distant from old class prototypes. ADC generates adversarial samples that act as pseudo-exemplars, moving current task data closer to old prototypes in the old feature space. These adversarial samples enable more accurate drift estimation between old and new feature spaces, leading to state-of-the-art performance on multiple benchmarks including CIFAR-100, TinyImageNet, and Stanford Cars.

## Method Summary
ADC generates adversarial samples by perturbing current task data to move their embeddings closer to old class prototypes in the old feature space. These adversarial samples act as pseudo-exemplars, allowing for more accurate drift estimation from old to new feature space. The method exploits continual adversarial transferability, where adversarial samples crafted on the old model still behave similarly on the new model due to knowledge distillation. ADC is evaluated on several benchmarks with 5 or 10 task settings and achieves significant performance improvements over existing methods.

## Key Results
- Achieves 4.2% accuracy improvement over existing methods on CIFAR-100 5-task settings
- Outperforms state-of-the-art EFCIL methods across CIFAR-100, TinyImageNet, ImageNet-Subset, CUB-200, and Stanford Cars benchmarks
- Demonstrates effectiveness particularly in small-start settings where existing methods struggle
- Shows consistent performance gains across both 5-task and 10-task experimental setups

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Adversarial samples generated in the old feature space can reliably estimate prototype drift when transferred to the new feature space.
- **Mechanism**: Targeted adversarial attacks move new task samples' embeddings close to old prototypes in the old feature space. These adversarial samples, when passed through the new model, still behave similarly due to continual adversarial transferability, enabling drift estimation without exemplars.
- **Core assumption**: The adversarial perturbation that works in the old feature space will still misclassify the sample as the target class in the new feature space due to knowledge distillation preserving feature structure.
- **Evidence anchors**:
  - [abstract]: "We exploit the fact that adversarial samples are transferable from the old to the new feature space in a continual learning setting."
  - [section 3.2]: "We hypothesize that the pseudo-exemplars behave like the original exemplars in the feature space, and thus we exploit them to measure the drift."
  - [corpus]: Weak - no direct citations support continual adversarial transferability in CIL settings.
- **Break condition**: If knowledge distillation fails to preserve enough feature similarity between old and new models, adversarial samples may no longer behave consistently across feature spaces.

### Mechanism 2
- **Claim**: Moving current task samples closer to old prototypes in the old feature space improves drift estimation accuracy compared to using raw current samples.
- **Mechanism**: By adversarially perturbing current samples to minimize distance to old prototypes in the old feature space, the resulting adversarial samples become pseudo-exemplars that better represent old class distributions, leading to more accurate drift computation.
- **Core assumption**: The correlation between distance to old prototype in old feature space and distance to oracle prototype in new feature space (shown in Fig. 2) holds consistently across tasks and datasets.
- **Evidence anchors**:
  - [section 3.2]: "We analyze in Fig. 2 that there exist a correlation between the distance to the old prototype in the old feature space and the distance to the oracle prototype in the new feature space."
  - [section 3.2]: "This motivates us to leverage current task samples so that their distance to the old prototype in the old feature space is even smaller, which could in turn improve the drift estimation."
  - [corpus]: Weak - no corpus evidence supports the correlation assumption.
- **Break condition**: If feature drift becomes too extreme between tasks, the correlation may break down, making adversarial samples poor proxies for old class representations.

### Mechanism 3
- **Claim**: Using adversarial samples as pseudo-exemplars provides computational efficiency compared to data inversion methods for exemplar-free continual learning.
- **Mechanism**: Generating adversarial samples requires only a few optimization iterations per class, whereas data inversion methods require training generative models or iterative optimization to reconstruct realistic images from embeddings.
- **Core assumption**: The computational overhead of generating adversarial samples is significantly lower than that of data inversion while providing comparable drift estimation quality.
- **Evidence anchors**:
  - [abstract]: "The generation of these images is simple and computationally cheap."
  - [section 3.2]: "This generation of adversarial samples is computationally cheaper and much faster (only a few iterations) compared to data-inversion methods [55] which inverts embeddings to realistic images."
  - [corpus]: Weak - no corpus evidence compares computational costs of these approaches.
- **Break condition**: If the number of classes or samples per class becomes very large, the iterative adversarial generation process may become computationally prohibitive.

## Foundational Learning

- **Concept**: Catastrophic forgetting in neural networks
  - Why needed here: The paper addresses catastrophic forgetting specifically in exemplar-free continual learning, where models must learn new classes without storing old data.
  - Quick check question: What happens to a neural network's performance on old tasks when trained on new tasks without any mechanism to prevent forgetting?

- **Concept**: Feature drift and semantic drift
  - Why needed here: The paper's core contribution is addressing feature drift estimation, where class prototypes move in the embedding space after learning new tasks.
  - Quick check question: Why does the position of old class prototypes in feature space change after training on new classes?

- **Concept**: Adversarial examples and transferability
  - Why needed here: The method generates adversarial samples to act as pseudo-exemplars, relying on the concept of adversarial transferability across different models.
  - Quick check question: What property of adversarial examples allows them to potentially work across different neural network models?

## Architecture Onboarding

- **Component map**: Old model → Adversarial sample generation → New model training → Drift estimation → Prototype compensation → Classification
- **Critical path**: Old model → Adversarial sample generation → New model training → Drift estimation → Prototype compensation → Classification
- **Design tradeoffs**:
  - Accuracy vs. computation: More adversarial iterations improve drift estimation but increase computation
  - Perturbation strength vs. realism: Stronger perturbations move samples closer to prototypes but create less realistic images
  - Number of samples vs. coverage: More samples per class improve drift estimation but increase memory and computation
- **Failure signatures**:
  - If prototypes drift too much between tasks, adversarial samples may become poor proxies
  - If knowledge distillation is weak, adversarial transferability may fail
  - If the adversarial perturbation is too weak, samples won't approximate old class distributions well
- **First 3 experiments**:
  1. Implement adversarial sample generation and verify that samples move closer to target prototypes in old feature space
  2. Test continual adversarial transferability by checking if adversarial samples maintain target class classification in new feature space
  3. Compare drift estimation quality using adversarial samples versus raw current samples on a simple benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do adversarial samples generated by ADC compare in visual quality and perceptual similarity to original images versus samples generated by traditional adversarial attack methods?
- Basis in paper: [explicit] The paper mentions that ADC perturbations are perceptible in CIFAR-100 and TinyImageNet but not in higher-resolution datasets, and notes that ADC doesn't apply ℓ2 or ℓ∞-norm restrictions unlike conventional attacks.
- Why unresolved: The paper only qualitatively observes perturbation visibility but doesn't quantify perceptual similarity or conduct user studies comparing ADC perturbations to traditional adversarial attacks.
- What evidence would resolve it: Perceptual studies measuring image similarity (e.g., SSIM, LPIPS) between original and adversarial samples from ADC versus traditional attacks, plus human evaluation of visual quality.

### Open Question 2
- Question: Does ADC maintain its performance advantage when applied to online continual learning scenarios where task boundaries are not available during training?
- Basis in paper: [explicit] The paper acknowledges that ADC requires task boundary access to trigger drift computation and states it would be challenging to apply in online continual learning settings.
- Why unresolved: The paper only discusses the theoretical difficulty but doesn't experiment with or adapt ADC for online learning scenarios.
- What evidence would resolve it: Experimental results showing ADC performance on online continual learning benchmarks compared to methods designed for that setting.

### Open Question 3
- Question: What is the relationship between ADC's adversarial transferability effectiveness and the similarity between old and new feature spaces?
- Basis in paper: [explicit] The paper observes that continual adversarial transferability works because old and new models are "not entirely different" due to knowledge distillation, and notes that drift estimation quality decreases as the backbone drifts more.
- Why unresolved: The paper doesn't systematically analyze how feature space similarity (measured by metrics like cosine similarity of embeddings) correlates with adversarial transferability effectiveness.
- What evidence would resolve it: Quantitative analysis showing correlation between feature space similarity metrics and ADC's drift estimation accuracy across different datasets and learning scenarios.

## Limitations
- Effectiveness depends on continual adversarial transferability, which is asserted but not empirically validated
- No systematic evaluation of failure cases when feature drift becomes extreme or knowledge distillation is insufficient
- Computational efficiency claims compared to data inversion methods lack quantitative benchmarks
- Correlation between old-space and new-space distances is presented without statistical validation

## Confidence

- **High confidence**: ADC improves performance over baseline methods (empirical results clearly show this)
- **Medium confidence**: ADC's improvements stem primarily from better drift estimation via adversarial samples (mechanism plausible but not definitively isolated)
- **Low confidence**: Adversarial samples reliably estimate drift across all task and dataset configurations (no systematic evaluation of failure cases)

## Next Checks

1. **Transferability ablation**: Train models where knowledge distillation is weakened or removed, then measure ADC performance degradation to quantify reliance on adversarial transferability.
2. **Correlation validation**: Perform statistical tests on the distance correlation between old-space and new-space prototypes across multiple task transitions and datasets.
3. **Failure case analysis**: Systematically test ADC on scenarios with extreme feature drift (e.g., highly dissimilar tasks) to identify conditions where adversarial samples fail as pseudo-exemplars.