---
ver: rpa2
title: '"What is the value of {templates}?" Rethinking Document Information Extraction
  Datasets for LLMs'
arxiv_id: '2410.15484'
source_url: https://arxiv.org/abs/2410.15484
tags:
- questions
- datasets
- document
- dataset
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: K2Q is a new dataset for training and evaluating large language
  models on visually rich document understanding tasks. It transforms five existing
  key information extraction datasets into a prompt-response format using over 100
  diverse templates, enabling complex questions with multiple entities and both extractive
  and boolean types.
---

# "What is the value of {templates}?" Rethinking Document Information Extraction Datasets for LLMs

## Quick Facts
- arXiv ID: 2410.15484
- Source URL: https://arxiv.org/abs/2410.15484
- Reference count: 40
- Primary result: K2Q dataset improves LLM performance on visually rich document understanding by up to 40% over simple templates

## Executive Summary
This paper introduces K2Q, a new dataset designed to train and evaluate large language models on visually rich document understanding tasks. K2Q transforms five existing key information extraction datasets into a prompt-response format using over 100 diverse templates, enabling complex questions with multiple entities and both extractive and boolean types. Experiments demonstrate that training models on K2Q significantly improves performance compared to simple templates, leading to more grounded, document-aligned responses. The work highlights the value of dataset-specific, intricate question generation for robust model development in real-world document understanding applications.

## Method Summary
K2Q is created by transforming five key information extraction datasets (CORD, Docile, Kleister Charity, Ad-Buy, Reg. Form) into a prompt-response format using over 100 diverse templates per dataset. These templates generate questions spanning multiple entities and both extractive and boolean types. The method involves curating dataset-specific templates, populating them with entity values from source datasets, and generating prompt-response pairs. Three trainable models (Donut, Pix2Struct, DocLLM) are fine-tuned on K2Q and compared against training on simpler templates, with evaluation using Average Normalized Levenshtein Similarity (ANLS) and analysis of model robustness to template changes.

## Key Results
- Models trained on K2Q achieve up to 40% improvement in performance compared to those trained on simple templates
- K2Q-trained models show increased robustness to template changes and better generalization to diverse query patterns
- Enhanced contextualization in K2Q questions leads to more grounded responses, with answers more likely to be present in the document

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dataset-specific, intricate question templates improve model robustness by exposing it to a wider variety of real-world query patterns.
- Mechanism: By designing templates that incorporate multiple entities, domain-specific jargon, and both extractive and boolean formats, the model learns to handle the complexity and diversity of real-world document queries rather than just simple, repetitive ones.
- Core assumption: Training data diversity directly correlates with downstream model robustness on held-out distributions.
- Evidence anchors:
  - [abstract] "creating diverse and intricate KIE questions enhances the performance and robustness of VRDU models"
  - [section] "we compare how models trained on simple templates perform when tested on K2Q templates, and vice versa"
  - [corpus] Weak: Corpus shows related works but no direct evidence of template complexity driving robustness.

### Mechanism 2
- Claim: K2Q's balance between templated and human-annotated datasets achieves high diversity without prohibitive manual effort.
- Mechanism: Human intervention at the template design level (rather than at each document) allows for a diverse set of questions while scaling efficiently, resulting in a dataset 6-7 times larger than fully human-curated alternatives like DocVQA or DUDE.
- Core assumption: Template-level human input can sufficiently approximate the diversity of fully human-curated datasets for effective model training.
- Evidence anchors:
  - [abstract] "K2Q is derived from five datasets... using a plethora of bespoke templates"
  - [section] "Our work addresses this by applying human intervention at the dataset level rather than at the document level"
  - [corpus] Missing: No direct corpus evidence comparing template-based vs human-curated diversity.

### Mechanism 3
- Claim: Enhanced contextualization in K2Q questions leads to more grounded model responses, regardless of correctness.
- Mechanism: Questions in K2Q are framed using entities present in the document, guiding the model to generate answers also present in the document, thus increasing the likelihood of grounded (and verifiable) responses.
- Core assumption: Question framing that references document entities improves the model's ability to stay grounded in the document context.
- Evidence anchors:
  - [abstract] "we find that creating diverse and intricate KIE questions enhances... groundedness"
  - [section] "our error analysis suggests that questions in K2Q provide enhanced contextualization compared to simple ones, resulting in more grounded answers from models"
  - [corpus] Missing: No corpus evidence directly linking question framing to groundedness.

## Foundational Learning

- Concept: Visually Rich Document Understanding (VRDU)
  - Why needed here: VRDU is the core domain; understanding its tasks (like KIE) is essential to grasp the problem K2Q addresses.
  - Quick check question: What are the two main input modalities in VRDU tasks, and how do they interact?

- Concept: Key Information Extraction (KIE)
  - Why needed here: KIE is the specific VRDU task K2Q focuses on; knowing what entities are and how they're annotated is key.
  - Quick check question: In the context of KIE, what is the difference between a line item and a standalone entity?

- Concept: Template-based Data Generation
  - Why needed here: K2Q relies on templates to generate diverse questions; understanding this approach is critical for dataset design.
  - Quick check question: What is a potential drawback of using a single, simple template like "What is the value for the {key}?" across all entities?

## Architecture Onboarding

- Component map:
  Template Designer (human) -> Template Populator -> Question Generator -> Model Trainer -> Evaluator

- Critical path: Template Designer → Template Populator → Question Generator → Model Trainer → Evaluator

- Design tradeoffs:
  - Diversity vs. Manual Effort: Richer templates increase diversity but require more human design time.
  - Template Specificity vs. Reusability: Dataset-specific templates are more natural but less reusable across domains.
  - Extractive vs. Boolean Questions: Including both increases complexity but better reflects real-world usage.

- Failure signatures:
  - Low model performance despite high dataset size: Templates may be too complex or not representative of real queries.
  - High ANLS but low groundedness: Model may be overfitting to template patterns rather than document content.
  - Disproportionate boolean vs. extractive performance: Template balance may be off.

- First 3 experiments:
  1. Train and evaluate on SD (simple templates) vs. K2Q to measure the impact of template complexity.
  2. Compare model performance on extractive vs. boolean questions within K2Q to assess template balance.
  3. Analyze groundedness and error types for models trained on K2Q vs. SD to evaluate contextualization benefits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using LLM-generated templates on the diversity and quality of questions in K2Q compared to human-curated templates?
- Basis in paper: [explicit] The paper mentions that using LLMs to generate questions is a promising research avenue and suggests that a future iteration of K2Q could use an LLM to augment the set of templates.
- Why unresolved: The paper does not provide any experimental results or analysis on the use of LLM-generated templates.
- What evidence would resolve it: Conducting experiments comparing the performance of models trained on K2Q with LLM-generated templates versus human-curated templates, and evaluating the diversity and quality of questions generated by each method.

### Open Question 2
- Question: How does the performance of K2Q-trained models generalize to real-world applications and diverse document types beyond the datasets used in the study?
- Basis in paper: [inferred] The paper discusses the importance of robust document understanding in real-world applications and mentions that K2Q contains questions that reflect the intricacies of document understanding, such as co-references and disambiguation of similar entities.
- Why unresolved: The paper does not provide any empirical evidence on the generalization of K2Q-trained models to real-world applications or diverse document types.
- What evidence would resolve it: Evaluating the performance of K2Q-trained models on a diverse set of real-world documents and tasks, and comparing their performance to models trained on other datasets or methods.

### Open Question 3
- Question: What are the optimal strategies for data cleaning and error handling in KIE datasets to ensure high-quality training data for generative models?
- Basis in paper: [explicit] The paper discusses the data cleaning methods applied to each dataset in K2Q and mentions that cleaning the OCR entity values is necessary to enable generative models trained on K2Q to produce more natural responses.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of different data cleaning strategies on model performance or a discussion of optimal error handling techniques.
- What evidence would resolve it: Conducting experiments comparing the performance of models trained on KIE datasets with different data cleaning strategies and error handling techniques, and analyzing the impact on model robustness and generalization.

## Limitations

- The study's reliance on human-designed templates raises questions about scalability and potential bias in template creation
- Evaluation focuses on ANLS and groundedness metrics, which may not fully capture practical utility in real-world scenarios
- Computational costs and trade-offs of fine-tuning models on the larger K2Q dataset are not explored

## Confidence

- **High Confidence:** The claim that K2Q improves model performance and robustness compared to simpler templates is well-supported by experimental results, including quantitative metrics like ANLS and qualitative error analysis.
- **Medium Confidence:** The assertion that template-level human intervention achieves sufficient diversity without prohibitive manual effort is plausible but lacks direct empirical comparison to fully human-curated datasets.
- **Low Confidence:** The claim that enhanced contextualization in K2Q questions leads to more grounded responses is weakly supported, as the paper does not provide direct evidence linking question framing to groundedness.

## Next Checks

1. Conduct a human evaluation study to assess the practical utility of models trained on K2Q in real-world document understanding tasks.
2. Perform an ablation study to quantify the impact of template diversity on model performance, comparing K2Q to a dataset with manually curated questions.
3. Analyze the computational trade-offs of fine-tuning models on K2Q, including training time, memory usage, and inference latency, to evaluate scalability for production use.