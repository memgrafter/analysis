---
ver: rpa2
title: Predictions from language models for multiple-choice tasks are not robust under
  variation of scoring methods
arxiv_id: '2403.00998'
source_url: https://arxiv.org/abs/2403.00998
tags:
- language
- methods
- human
- arxiv
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically compares different methods of deriving
  item-level predictions of language models for multiple-choice tasks. It compares
  scoring methods for answer options based on free generation of responses, various
  probability-based scores, a Likert-scale style rating method, and embedding similarity.
---

# Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods

## Quick Facts
- arXiv ID: 2403.00998
- Source URL: https://arxiv.org/abs/2403.00998
- Authors: Polina Tsvilodub; Hening Wang; Sharon Grosch; Michael Franke
- Reference count: 5
- Primary result: LLM predictions vary significantly across different scoring methods for multiple-choice tasks

## Executive Summary
This paper systematically compares five different methods for deriving item-level predictions of language models on multiple-choice tasks: free generation, string scoring, label scoring, rating aggregation, and embedding similarity. Using a case study on pragmatic language interpretation with four different LLMs (GPT-3.5-turbo, GPT-davinci, LLaMA-2, FLAN-T5), the authors find that LLM predictions are not robust under variation of method choice, both within a single LLM and across different LLMs. This variability creates pronounced researcher degrees of freedom in reporting results, which can lead to accumulation of false or overstated findings and biased reporting.

## Method Summary
The study compares five different scoring methods for assessing LLM performance on multiple-choice tasks using experimental items from Hu et al. (2023) covering pragmatic language interpretation phenomena. The methods include free generation of responses, various probability-based scores (string scoring, label scoring, prior correction), a Likert-scale style rating method, and embedding similarity. Four LLMs were tested: GPT-3.5-turbo-instruct, text-davinci-002, LLaMA-2, and FLAN-T5-XL. Performance was evaluated using accuracy (proportion of target choices) and goodness-of-fit to human data (log-likelihood).

## Key Results
- Different scoring methods produce significantly different accuracy results for the same LLM and task
- The best-performing method varies across LLMs, with free generation excelling for GPT models but failing for FLAN-T5
- Some methods (rating aggregation, embedding similarity) consistently underperform compared to probability-based approaches
- Variability in method performance creates researcher degrees of freedom that can lead to biased result reporting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM prediction methods produce different results because they extract different types of model-internal information (raw probabilities vs. embedding similarity vs. free generations).
- Mechanism: Each method captures a different latent dimension of the model's decision-making process, and these dimensions do not perfectly align across tasks or models.
- Core assumption: The internal representations and probability distributions used by LLMs are not invariant under method variation.
- Evidence anchors:
  - [abstract] "LLM predictions are not robust under variation of method choice, both within a single LLM and across different LLMs."
  - [section] "We found that different methods of assessing model performance give different results for different models and conditions."

### Mechanism 2
- Claim: Different scoring methods create researcher degrees of freedom, allowing selective reporting of favorable results.
- Mechanism: Researchers can choose the method that best fits their narrative, leading to inflated or biased performance claims.
- Core assumption: Method variability is known but not consistently controlled or reported.
- Evidence anchors:
  - [abstract] "As this variability entails pronounced researcher degrees of freedom in reporting results, knowledge of the variability is crucial to secure robustness of results and research integrity."

### Mechanism 3
- Claim: Model architecture and training affect which scoring method is most reliable.
- Mechanism: Some architectures (e.g., GPT with RLHF fine-tuning) are better suited for free generation, while others (e.g., decoder-only models) work better with string scoring.
- Core assumption: Training objectives and model design influence the fidelity of different scoring approaches.
- Evidence anchors:
  - [section] "Based on the obtained results, we can recommend not using rating approaches... The embedding-based method also performed generally worse. Free generation gave excellent results for GPT-instruct..."

## Foundational Learning

- Concept: Probabilistic scoring (log probabilities, surprisal)
  - Why needed here: The paper compares string-scoring, label-scoring, and prior correction methods that rely on token-level probabilities.
  - Quick check question: What is the difference between "option probability" and "average negative surprisal" in string scoring?

- Concept: Embedding similarity and cosine distance
  - Why needed here: The embedding-similarity method uses cosine similarity between input and answer option embeddings.
  - Quick check question: How does the embedding-similarity score differ from probability-based scores?

- Concept: Pragmatic language interpretation
  - Why needed here: The case study focuses on tasks requiring understanding of non-literal meaning, which may be sensitive to scoring method.
  - Quick check question: Why might pragmatic tasks be more variable across scoring methods than factual recall tasks?

## Architecture Onboarding

- Component map: Experimental items -> Five scoring methods -> Four LLMs -> Evaluation metrics
- Critical path: 1) Load experimental items 2) Compute scores via all five methods for each item and LLM 3) Aggregate results by method and model 4) Compare accuracy and goodness-of-fit to human performance
- Design tradeoffs:
  - Free generation: High interpretability but expensive and variable
  - String scoring: Fast and consistent but may miss nuanced meaning
  - Label scoring: More robust to token-level noise but less flexible
  - Rating aggregation: Human-like but computationally heavy
  - Embedding similarity: Language-agnostic but less aligned with LLM internals
- Failure signatures:
  - High variance in accuracy across methods for the same model
  - Low correlation between method scores and human judgments
  - Inconsistent best-performing method across tasks or models
- First 3 experiments:
  1. Replicate the main result: compare accuracy of all five methods on the pragmatic dataset
  2. Vary the prompt template slightly and measure method sensitivity
  3. Test a new scoring method (e.g., chain-of-thought probability) and compare to existing methods

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions but identifies several limitations and directions for future work, including the need to address full distributional predictions for the whole set of options rather than single target options, and investigating effects of factors like LLM size, architecture, and training data.

## Limitations
- Results are limited to a specific dataset focused on pragmatic language interpretation and may not generalize to other task types
- Only English-language materials were tested, limiting generalizability to other languages and cultural contexts
- The study doesn't quantify computational costs of different methods at scale

## Confidence

- **High confidence**: The core finding that different scoring methods produce varying results for the same model and task is well-supported by the empirical data across multiple conditions and models.
- **Medium confidence**: The claim about researcher degrees of freedom leading to biased reporting is plausible but requires empirical validation in actual research practices.
- **Low confidence**: The recommendation against certain methods (rating aggregation, embedding similarity) is based on this single dataset and may not generalize to all LLM architectures or task types.

## Next Checks

1. Cross-dataset validation: Test all five scoring methods on a diverse set of multiple-choice tasks (factual recall, logical reasoning, creative generation) to assess generalizability of performance patterns.

2. Prompt sensitivity analysis: Systematically vary prompt templates, temperature settings, and instructions across all scoring methods to quantify method robustness to prompt engineering.

3. Statistical power analysis: Conduct simulations to determine the minimum number of items needed for each scoring method to detect true performance differences, accounting for method-specific variance.